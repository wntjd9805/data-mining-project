Reinforcement learning (RL) aims to maximize reward through control policies, but current algorithms are data inefficient. Multi-task RL has the potential to improve sample efficiency by sharing data across tasks. However, effectively sharing data remains a challenge. This paper introduces the concept of retroactive relabeling of experience to improve data efficiency. Previous relabeling methods have focused on relabeling past trials based on successful goals, but they are not suitable for more general reward functions. This paper formalizes relabeling techniques under inverse RL and proposes two algorithms, MaxEnt RL and MaxEnt inverse RL, for optimizing multi-task objectives. The paper demonstrates the effectiveness of these algorithms in complex locomotion and manipulation tasks.