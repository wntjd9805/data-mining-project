This paper introduces a chart-based approach to 3D object reconstruction by effectively fusing global and local information from visual and haptic modalities. The proposed approach leverages graph convolutional networks (GCN) to represent a 3D object as a collection of disjoint mesh surface elements called charts. By combining RGB images and high spatial resolution tactile readings, the approach predicts local charts at touch sites and uses vision information to predict global charts to close the surface. To evaluate the approach, a simulated dataset of interactions between a robotic hand and four classes of objects is created, containing ground truth 3D objects, RGB images, and touch readings. Results show that combining visual and tactile cues outperforms single modality baselines, demonstrating the effectiveness of the proposed approach in leveraging the complementarity of both modalities. Additionally, increasing the number of grasps improves the 3D shape reconstruction quality, and the touch readings not only enhance reconstruction at the touch site but also reduce error in the surrounding area. The code for the system is publicly available for reproducible experimental comparison.