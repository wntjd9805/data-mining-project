Optimal Transport (OT) has become an effective tool in solving various machine learning problems. It has been shown that OT achieves state-of-the-art performance in applications such as data alignment/integration, domain adaptation, model interpolation/combination, and natural language processing. However, there are two main limitations in the current use of OT in machine learning: the lack of algorithms for finding the transport plan/map over continuous domains and the reliance on samples rather than the actual marginal distributions. This paper focuses on the problem of statistical OT over continuous domains and aims to develop consistent estimators for the optimal transport plan/map with a dimension-free sample complexity. The authors propose a novel approach that reformulates the statistical OT problem in terms of kernel mean embeddings, which allows for dimension-free sample complexity and provides implicit smoothness. The use of Maximum Mean Discrepancy (MMD) based regularization is also discussed. The results show that the proposed methodology can recover the optimal transport plan and map with a dimension-free sample complexity, and a representer theorem guarantees a fully kernelized and convex formulation for estimation. The approach can be applied to solve OT problems in non-standard domains using universal kernels, and an ADMM based solver can be used for efficient computation. Empirical results validate the effectiveness of the proposed approach.