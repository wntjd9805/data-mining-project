The goal of this paper is to explore the interaction between the semantics of tasks and individual concepts in the brain. The authors propose a computational model that predicts brain activity based on task semantics and stimuli, and they show that this model outperforms those that do not account for task semantics. The authors also demonstrate the ability of their model to generalize to unseen tasks and stimuli, allowing for zero-shot predictions of brain recordings. This research has implications for both neuroscience and artificial intelligence, providing insights into how the brain processes meaning and informing the development of AI models that can adapt to new tasks. The authors contribute a methodology for representing task semantics and predicting brain activity, as well as making their code publicly available for future research. They also highlight the significance of zero-shot learning in testing generalization beyond experimental stimuli and tasks.