This paper proposes the Evolving Graphical Planner (EGP), an autonomous agent that navigates through unseen environments based on natural language instructions. EGP dynamically constructs a graphical map of the environment and incorporates a global planning module for action selection. The agent uses a structured representation of the environment's layout and operates directly on raw sensory inputs. The paper introduces a novel method for training the planning modules using imitation learning and proxy graphs to address scalability challenges. Experimental results show that EGP outperforms state-of-the-art models on benchmark datasets for 3D navigation with instructions. The implementation is available on GitHub.