Building interpretable systems for natural language understanding is crucial in domains like healthcare and finance, and generating natural language explanations for predictions has shown promise in improving interpretability. This paper proposes a principled probabilistic framework called ELV for text classification, where natural language explanations are treated as latent variables. Unlike existing approaches, this framework jointly trains an explanation generator and an explanation-augmented prediction model, requiring only a few annotated explanations to guide the generation process. Additionally, the ELV framework is extended for semi-supervised learning, leveraging explanations as implicit logic rules to label unlabeled data. The effectiveness of the proposed approach is demonstrated through extensive experiments on relation extraction and sentiment analysis tasks in both supervised and semi-supervised settings.