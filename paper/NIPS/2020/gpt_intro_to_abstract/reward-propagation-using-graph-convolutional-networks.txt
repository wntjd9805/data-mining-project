Reinforcement learning (RL) algorithms aim to optimize a policy based on expected cumulative rewards. Developing a well-designed reward function is crucial for the success of RL algorithms, but it can be challenging. This paper addresses the problem by leveraging the probabilistic inference view of RL and formulating the problem as a directed graph analogous to hidden Markov models. Using Graph Convolutional Networks, the authors propose a scalable and flexible implementation that propagates reward information through message passing. The approach is evaluated in various domains, demonstrating its effectiveness compared to existing methods.