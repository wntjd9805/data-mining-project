Combining reinforcement learning with search (RL+Search) has achieved significant success in perfect-information games. However, existing RL+Search algorithms do not work in imperfect-information games due to the assumptions they make. This paper introduces ReBeL, a general RL+Search framework that converges to a Nash equilibrium in two-player zero-sum games. ReBeL expands the notion of "state" to include the probabilistic belief distribution of all agents, based on their observations and policies. The algorithm trains a value network and a policy network for these expanded states through self-play reinforcement learning. Experimental results demonstrate the effectiveness of ReBeL in large-scale games, including heads-up no-limit Texas hold'em poker and Liar's Dice. ReBeL approximates a Nash equilibrium in imperfect-information games, making it a promising approach for AI in this domain.