Conditioning, the existence of multiple input signals, plays a crucial role in various computer science applications. In some cases, conditioning can be intuitive, such as using language features to drive text-to-speech processes. However, other forms of conditioning may be less intuitive, such as changing the weights of normalization layers based on desired style in Style GANs. This paper focuses on the concept of hypernetworks, which have shown state-of-the-art results in various benchmarks due to their ability to adapt to different inputs. The paper compares hypernetworks to standard embedding methods and examines their respective complexities and capabilities for approximating target functions. The central contributions of the paper include the establishment of a lower bound on trainable parameters for approximating smooth functions, a comparison of complexities between the two methods, a demonstration of the smaller number of trainable parameters in hypernetworks, and experimental validation of these observations. Based on the findings, it is concluded that hypernetworks exhibit modularity, making them a promising approach in computer science applications.