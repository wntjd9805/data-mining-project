Spatial relations, such as "laptop on table," are important for both humans and intelligent agents like robots. Humans use spatial relations to navigate their environment and complete daily tasks, while robots need to understand spatial relations for tasks like navigation and object manipulation. However, understanding spatial relations is not a simple task, as it involves factors beyond relative positions, such as frames of reference and commonsense. Previous rule-based systems have limited success in capturing the complexity of spatial relations, leading researchers to explore data-driven approaches. In this paper, we introduce Rel3D, a large-scale dataset of human-annotated spatial relations in 3D, which allows for the extraction of rich geometric and semantic information. We also propose a minimally contrastive data collection method to reduce dataset bias and evaluate models' understanding of spatial relations. With Rel3D, we demonstrate the effectiveness of using 3D information for predicting spatial relations and show that training models on minimally contrastive examples leads to better sample efficiency. Overall, our work contributes to the study of spatial relation recognition and provides a valuable resource for training and evaluating models in this area.