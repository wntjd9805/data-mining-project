Multi-agent imitation learning (MAIL) has gained significant attention and has shown promising results in various tasks. However, applying MAIL to real-world problems is challenging due to the need to consider the policies of other agents in a Markov game with unknown reward functions. In this paper, we address the challenges of sample efficiency and scalability in MAIL. We propose a Bayesian approach that learns a stable reward function for more efficient policy search and faster convergence. Additionally, we introduce a new multi-type mean field approximation that effectively gathers information from other agents. We develop a new algorithm called Bayesian multi-type mean field multi-agent imitation learning (BM3IL) and demonstrate its performance through benchmarking in different scenarios, including real-world transportation networks.