In recent years, the size of machine-learning models and datasets has been rapidly increasing, leading to the need for distributed computation on multiple machines to accelerate training. This paper focuses on Stochastic Gradient Descent (SGD), commonly used for training large-scale deep neural networks. The state-of-the-art work in communication-efficient SGD is called QSparse-local-SGD, which combines message compression and infrequent synchronization techniques to reduce communication overhead. However, QSparse-local-SGD fails to converge at high compression ratios. To address this, the paper introduces a new algorithm called Communication-efficient SGD with Error Reset (CSER), which utilizes a novel technique called error reset and partial synchronization to achieve better convergence and scalability. Experimental results show that CSER significantly outperforms existing approaches, accelerating distributed training by nearly 10× for CIFAR-100 and 4.5× for ImageNet.