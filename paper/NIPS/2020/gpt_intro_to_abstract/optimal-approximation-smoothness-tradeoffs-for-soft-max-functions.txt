Soft-max functions are widely used in computer science and machine learning applications for selecting options based on their values. The exponential function has been the dominant choice, but there may be other soft-max functions that perform well under certain criteria. This paper explores the tradeoff between approximation guarantee and smoothness of soft-max functions, with a focus on Lipschitz continuity. The exponential mechanism is shown to achieve the lowest possible Lipschitz constant among all approximate soft-max functions, using 𝑝-norm and Rényi divergence as distance measures. However, it cannot guarantee approximations in the worst case. A new soft-max function called PLSOFTMAX is introduced, which achieves a Lipschitz constant of 𝑂(1/𝛿) and is also 𝛿-approximate in the worst case. The paper also examines other properties and applications of these soft-max functions, including differentially private submodular maximization, incentive compatible mechanisms, and multiclass classification using deep neural networks.