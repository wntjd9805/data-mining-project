This paper focuses on learning the target signal in a linear regression model. It introduces a generalized ridge regression estimator and investigates its properties in high dimensions. The paper analyzes the estimator in the proportional limit and explores the risk, the "negative ridge" phenomenon, and the optimal weighting matrix. The contributions of the work include deriving the prediction risk, characterizing principal component regression, analyzing the optimal regularization strength, and determining the optimal weighting matrix. The findings demonstrate the benefits of weighted regularization and provide insights into the behavior of the estimator in different scenarios.