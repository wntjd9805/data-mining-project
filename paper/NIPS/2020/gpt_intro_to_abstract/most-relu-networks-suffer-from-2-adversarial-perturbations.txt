Adversarial examples in machine learning have garnered significant attention since the seminal paper by Szegedy et al. Various attacks and defense methods have been developed, but their existence and discoverability remain unclear. This paper sheds new light on this phenomenon and demonstrates that for certain network architectures, an adversarial example can be found via simple algorithms such as gradient descent. The authors show that for most choices of weights and examples, an adversarial example exists at a certain Euclidean distance. Furthermore, they prove that gradient flow or gradient descent with small steps is guaranteed to find these adversarial examples. This result implies that unless restrictions are imposed on the weights or examples, adversarial examples should be expected. This paper presents the first result that shows the existence of adversarial examples based on the Euclidean distance for a large class of networks and distributions, as well as the first result that guarantees the discovery of such perturbations using gradient-based algorithms.