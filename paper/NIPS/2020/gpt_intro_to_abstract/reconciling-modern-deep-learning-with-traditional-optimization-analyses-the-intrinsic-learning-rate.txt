The training of modern neural networks involves Stochastic Gradient Descent (SGD) with a suitable learning rate schedule. Previous analysis shows that SGD approaches a stationary point of the training loss with small learning rates. However, it is known that using only small learning rates or large batch sizes may lead to worse generalization. In this paper, we incorporate the effects of normalization in the SGD view to study the interaction between Batch Normalization (BN), weight decay, and learning rate schedule. We observe that, with BN and weight decay, a neural network will eventually reach an equilibrium distribution in the function space, which only depends on the product of the learning rate and weight decay and is independent of the training history. We make theoretical analyses and empirical observations to support our findings, including the identification of a new "intrinsic LR" parameter and the challenge to the belief that large initial learning rates are necessary for good generalization. We also propose the Fast Equilibrium Conjecture, which suggests that the number of steps for reaching equilibrium scales inversely to the intrinsic LR, providing a new perspective on the effectiveness of BN in deep learning.