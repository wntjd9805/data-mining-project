The field of deep reinforcement learning has seen success in solving sequential decision-making problems, but the high sample complexity of model-free RL algorithms has limited their applicability in real-world environments. Model-based RL algorithms, which learn a predictive model of the environment, have emerged as a sample-efficient alternative. However, these algorithms are not robust to changes in dynamics, making them unreliable in real-world scenarios. In this paper, we propose a new model-based RL algorithm called trajectory-wise multiple choice learning (T-MCL) that can approximate the multi-modal distribution of transition dynamics in an unsupervised manner. Our algorithm utilizes a novel loss function, trajectory-wise oracle loss, to learn a multi-headed dynamics model and incorporates context learning for online adaptation to unseen environments. We also introduce adaptive planning to select actions based on the most accurate prediction head. Experimental results demonstrate the effectiveness of T-MCL compared to existing model-based RL methods, showing superior generalization performance in control tasks.