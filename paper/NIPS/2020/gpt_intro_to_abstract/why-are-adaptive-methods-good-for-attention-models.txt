Stochastic gradient descent (SGD) is a widely used algorithm for training neural networks, but certain tasks require the use of adaptive variants of SGD due to the distribution of stochastic gradients. In this paper, we investigate the convergence of optimization methods under heavy-tailed noise and propose a novel algorithm, ACClip, that outperforms traditional methods such as Adam on BERT training tasks. We empirically show that in tasks where Adam outperforms SGD, the stochastic gradient noise is heavy-tailed, while in tasks where SGD outperforms Adam, the noise is well concentrated. We establish the convergence of clipped gradient methods under heavy-tailed noise conditions and prove their theoretically optimal rates. We also introduce a novel adaptive-threshold coordinate-wise clipping algorithm that further improves performance on BERT training tasks.