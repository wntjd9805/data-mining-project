Many existing training algorithms for robust classification are vulnerable to powerful adversaries, highlighting the need for developing classifiers with provable worst-case guarantees. Randomized smoothing methods have emerged as a promising approach, constructing smoothed classifiers with certified robustness by introducing noise on the inputs. However, most methods use Gaussian noise for smoothing, which is suboptimal in high-dimensional spaces. In this paper, we propose a general framework of adversarial certification using non-Gaussian smoothing noises, based on a new functional optimization perspective. Our framework unifies existing methods and allows for more general smoothing distributions and attacks. We develop a new family of distributions that outperforms existing approaches for different attack norms and demonstrate the superiority of our method on CIFAR-10 and ImageNet datasets.