Computations in the brain are implemented through the dynamic evolution of activity in large populations of neurons. While this perspective has shed light on computational mechanisms underlying various tasks, little is known about how a single neural population can learn multiple computations without interference or forgetting. Based on recent experimental results, we hypothesize that organizing population dynamics into orthogonal or shared subspaces can enable robust multi-task computations. To test this hypothesis, we propose a novel algorithm for continual multi-task learning in recurrent neural networks (RNNs). Our approach modifies the stochastic gradient descent update to preserve network dynamics within subspaces used for previously learned tasks and encourages interference-free dynamics in orthogonal subspaces for new tasks. Through experiments on networks performing multiple tasks, we demonstrate the effectiveness of our learning algorithm in facilitating sequential training. The development of better approaches in continual learning for RNNs will contribute to understanding the organization of dynamics across tasks and provide insights into multi-task computation in the brain.