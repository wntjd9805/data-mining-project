In this paper, we investigate the theoretical properties of sliced probability divergences, which are increasingly popular in the field of implicit generative modeling. While these divergences have been successfully applied in practical applications, their theoretical properties are not well-understood. Existing results are limited to specific cases, such as the Sliced-Wasserstein distance. Therefore, our goal is to bridge this gap by exploring the topological and statistical implications of the slicing operation used in these divergences. We establish several topological properties of sliced divergences, including their metric properties and convergence implications. Additionally, we derive statistical properties, demonstrating that the sample complexity of sliced divergences is proportional to the sample complexity of the base divergence for one-dimensional measures and does not depend on the dimensionality. However, we also identify limitations when approximating the expectation over random projections and discuss their impact on high-dimensional performance. Through multiple examples, including Sliced-Cram√©r distance and Sliced-Sinkhorn divergence, we illustrate the applicability of our theoretical findings. Numerical experiments on synthetic and real data support our theoretical analysis.