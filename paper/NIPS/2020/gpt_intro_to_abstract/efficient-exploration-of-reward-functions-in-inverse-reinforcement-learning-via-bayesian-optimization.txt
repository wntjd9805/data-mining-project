Inverse reinforcement learning (IRL) is a challenging problem in computer science, as it involves inferring the reward function of a reinforcement learning (RL) agent from its observed behavior. Existing approaches, such as Bayesian IRL, are computationally expensive or suffer from overfitting. In this paper, we propose a novel approach that uses Bayesian optimization (BO) to minimize the negative log-likelihood of expert demonstrations with respect to reward functions. By applying a œÅ-projection, we address the issue of policy invariance and leverage standard stationary kernels for covariance representation. Experimental results demonstrate the effectiveness of our BO-IRL algorithm in capturing the correlation structure of the reward space and outperforming state-of-the-art methods.