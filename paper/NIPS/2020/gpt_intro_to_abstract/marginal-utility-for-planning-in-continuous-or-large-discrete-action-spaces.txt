Sample-based planning is a popular choice for decision-making scenarios where finding the best action is the objective. However, in domains with large or continuous action spaces, exhaustive search becomes infeasible due to the extensive number of samples required. Monte Carlo tree search (MCTS) algorithms, like UCT, offer a solution by balancing action sampling and exploration. Nevertheless, they may not encounter effective actions when confronted with very large action spaces. Yee et al. introduced KR-UCT for continuous spaces and stochastic outcomes but used hand-coded action generators. Lee et al. later combined KR-UCT with reinforcement learning to learn an action generator. However, relying solely on a good policy for action generation did not guarantee satisfactory planning. In this paper, we propose a novel approach that trains a neural network model to explicitly generate sets of candidate actions for effective sample-based planning in large action spaces. Our approach optimizes for the marginal utilities of the generated actions, which measure the increase in value compared to previously generated actions and promote diversity. We evaluate our approach in curling, a stochastic domain with continuous state and action spaces, as well as a location game with a large discrete action space. Comparisons against trained policies, hand-coded agents, and other optimization objectives demonstrate the superiority of our generator for sample-based planning.