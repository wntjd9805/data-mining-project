This paper introduces several doubly robust off-policy value and gradient estimators for deterministic policies in reinforcement learning (RL) settings. The authors analyze the mean-squared error (MSE) convergence rate of each estimator and propose efficient estimators that do not deteriorate with horizon. These results are particularly important for RL applications in healthcare where experimentation is limited. The paper also addresses the challenges of evaluating and learning deterministic policies with continuous actions, providing a comprehensive comparison of existing estimators.