Due to the need for extensive training data and expensive computing resources, many companies use pre-trained deep neural network models provided by third-parties. However, this reliance on pre-trained models has led to a security threat known as neural backdoor attacks, in which the networks behave improperly under specific input conditions. Previous studies have focused on fixed trigger patterns for these attacks, but we argue that dynamic triggers that vary from input to input are more effective and difficult to detect. In this paper, we propose an input-aware dynamic backdoor attack method that generates unique triggers conditioned on the input image. We demonstrate the effectiveness of our approach through experiments on various image recognition datasets, showing a high attack success rate and the ability to bypass state-of-the-art defense methods. We also investigate potential countermeasures against our attack, including image regularization and network inspection, and show that our backdoor remains persistent and stealthy.