The seminal paper by Jordan, Kinderlehrer, and Otto has greatly influenced our understanding of sampling algorithms. This paper presents the JKO scheme, which interprets the evolution of marginal distributions of a Langevin diffusion as a gradient flow of a Kullback-Leibler divergence over the Wasserstein space of probability measures. This optimization perspective on Markov Chain Monte Carlo (MCMC) has not only enhanced our understanding of algorithms based on Langevin diffusions but has also led to the discovery of new MCMC algorithms inspired by optimization techniques. In this paper, we focus on the discretization of the Wasserstein gradient flow and introduce a new perspective on the Stein Variational Gradient Descent (SVGD) algorithm, viewing it as the kernelized gradient flow of the chi-squared divergence. We provide theoretical results showing exponential convergence of this gradient flow under certain conditions and introduce a new sampling algorithm called Laplacian Adjusted Wasserstein Gradient Descent (LAWGD), which exhibits fast convergence regardless of the Poincar√© constant. We demonstrate the effectiveness of LAWGD on mixtures of Gaussians and highlight its potential in advancing the state-of-the-art for sampling algorithms.