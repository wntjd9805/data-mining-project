Recent advancements in Natural Language Processing (NLP) have emphasized the importance of model interpretability and controllability in improving the transparency and output control of black-box neural networks. Existing approaches in this area mainly rely on non-probabilistic methods or complex Reinforcement Learning (RL)-based hard structures. While these methods have their benefits, they lack the expressive power and flexibility of probabilistic approaches. In this paper, we propose a novel approach that combines the advantages of relaxed training and graphical models, specifically focusing on Conditional Random Field (CRF) models. Our approach incorporates the Gumbel-Softmax method to relax the CRF sampling process, allowing for more stable and fine-grained pathwise gradients. We apply this approach as an inference model in a structured variational autoencoder for learning latent templates that control sentence structures. Experimental results show that our Gumbel-CRF approach provides lower-variance gradients, improved text modeling performance, and enhanced interpretability and controllability in tasks such as paraphrase generation and data-to-text generation.