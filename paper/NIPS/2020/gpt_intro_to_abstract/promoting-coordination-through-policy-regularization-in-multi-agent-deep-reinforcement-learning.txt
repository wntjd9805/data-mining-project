Multi-Agent Reinforcement Learning (MARL) is a challenging branch of Reinforcement Learning (RL) that involves training an agent to maximize its expected return by interacting with an environment containing other learning agents. The use of a Centralized Training and Decentralized Execution (CTDE) procedure is a popular framework for MARL. However, this approach relies on agents stumbling upon coordinated joint actions by chance, which may not always occur. In this paper, we propose two coordination-promoting inductive biases, referred to as TeamReg and CoachReg, to improve the efficiency of discovering successful behaviors in scenarios where coordinated strategies cannot be easily engineered. We present a simple Markov Game to motivate our proposition and validate our approaches by conducting experiments on cooperative tasks in the multi-agent particle environment. Our results demonstrate that coordination can significantly accelerate multi-agent learning, and our proposed objectives contribute to enhancing the overall performance of the agents in a variety of scenarios.