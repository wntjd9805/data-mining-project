Machine learning (ML) frameworks like PyTorch and TensorFlow are widely used in the ML community, but they require all code to be written in their specific domain-specific languages (DSLs). This limitation makes it challenging to apply ML to new domains that use existing tools written in different languages. To address this issue, researchers have either created new DSLs to simplify the rewriting process or added differentiation as a first-class construct in programming languages. However, these approaches still require code rewriting and have limited support for cross-language differentiation. This paper introduces Enzyme, a compiler plugin that operates on LLVM IR and provides efficient automatic differentiation for code written in various languages. The paper also presents interfaces for integrating Enzyme with PyTorch, TensorFlow, and Julia, as well as support for multisource AD and static libraries. The study demonstrates significant performance gains when running AD after optimization on a machine learning benchmark suite, achieving state-of-the-art performance.