This paper addresses the problem of minimizing a function with a smooth convex form using Stochastic Gradient Descent (SGD) in the presence of heavy-tailed noise in the stochastic gradient. The existing convergence theory for SGD in expectation is not sufficient to describe the behavior of the method for heavy-tailed distributions. To illustrate this, a simple example of stochastic optimization problem is considered, and the performance of SGD is compared to a modified version called clipped-SGD. Clipped-SGD, which updates the parameter by clipping the stochastic gradient, shows robustness to heavy-tailed noise and does not compromise the convergence rate.