Monotonicity is an important property in machine learning predictions for various practical applications, particularly for scenarios involving fairness and security concerns. Incorporating monotonicity into ML models can improve interpretability and increase generalization ability. However, effectively and flexibly incorporating monotonicity into complex neural networks is a current challenge. Existing approaches include hand-designed monotonic architectures, which can be restrictive and complex, and heuristic monotonic regularization, which does not guarantee the desired monotonic response. In this paper, we propose a new paradigm that combines the advantages of both approaches by leveraging arbitrary neural architectures and mathematically verifying the monotonicity using optimization-based techniques. Our method demonstrates better performance and flexibility in learning monotonic functions on challenging datasets.