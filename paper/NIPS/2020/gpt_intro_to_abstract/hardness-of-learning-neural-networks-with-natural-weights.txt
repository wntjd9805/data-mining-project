Neural networks have demonstrated remarkable practical success in various domains but their theoretical understanding is still lacking. There are known computational difficulties in learning neural network models, even for simple architectures. In recent years, researchers have attempted to bridge the gap between theoretical hardness results and empirical success by exploring different assumptions and properties of neural networks. This paper investigates the learnability of networks with random weights and networks with weights possessing natural properties. The results show that under various natural weights distributions, learning neural networks is hard. This implies that there is no generic property that allows for efficient learning in such networks. The paper considers networks with depth 2 and single output neurons, as well as random convolutional networks, and demonstrates the hardness of learning in both cases.