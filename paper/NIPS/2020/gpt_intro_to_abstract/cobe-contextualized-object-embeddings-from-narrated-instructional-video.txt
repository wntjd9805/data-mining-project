The field of object detection has seen significant advancements in recent years, driven by the use of larger labeled datasets and more complex models. However, existing datasets still define objects at a coarse level, failing to capture variations in appearance and contextual information. In this paper, we propose a model called Contextualized OBject Embeddings (COBE) that learns from instructional videos with narrations to recognize contextual cues and object states. By mapping object instances to contextual word representations, COBE can capture detailed descriptions of objects and their surroundings. We evaluate COBE on multiple datasets and demonstrate its superior performance compared to state-of-the-art detectors, even in scenarios with semantic and appearance differences. Furthermore, COBE shows promising results in zero-shot and few-shot learning, generalizing to novel classes and scenarios.