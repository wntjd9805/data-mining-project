Reinforcement learning (RL) has the potential to enhance decision-making in various domains such as robotics, computer systems, recommender systems, and user interfaces. However, RL agents typically require extensive exploration to discover optimal decision policies, leading to inefficiency. To address this, imitation learning (IL) techniques have been explored, where an RL agent learns from oracle policies to achieve high performance without requiring global exploration. Existing IL techniques mostly focus on a single oracle policy, but in practice, multiple oracle policies may be available, each with its own strengths and desirable behaviors for different situations. This paper investigates how RL agents can leverage domain knowledge from multiple suboptimal oracle policies. The authors propose a max-aggregated baseline as a benchmark for policy performance and introduce a novel IL algorithm called MAMBA (Max-aggregation of Multiple Baselines) that combines the oracles to create a new policy that outperforms all individual oracles. They provide theoretical foundations for designing IL algorithms with multiple oracles, present regret-based performance guarantees for MAMBA, and demonstrate its effectiveness through empirical evaluations.