Bayesian neural networks (BNNs) aim to combine the strengths of neural networks with the probabilistic framework of Bayesian statistics for uncertainty quantification. However, exact inference in BNNs is analytically intractable, leading to the use of approximate inference techniques. This paper investigates the extent to which the successes and failures of BNNs can be attributed to the approximation method used. The authors focus on the flexibility of the predictive mean and variance functions in approximated BNNs, providing both theoretical analyses and empirical evidence. They find that certain approximating families fail to capture the exact posterior predictive uncertainty, leading to a lack of "in-between uncertainty." Additionally, they demonstrate that deep approximate BNNs using mean-field Gaussian or Monte Carlo dropout distributions can universally approximate any continuous function and non-negative function, respectively. However, they highlight that appropriate predictive means and variances may not be found when optimizing the evidence lower bound. This is supported by empirical evidence in the low-dimensional, small data regime. The authors also provide a case study on a real-world dataset, showing the importance of in-between uncertainty in the posterior predictive.