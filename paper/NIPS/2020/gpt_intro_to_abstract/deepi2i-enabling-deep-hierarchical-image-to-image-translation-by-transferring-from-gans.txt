Most image-to-image (I2I) networks suffer from a high-resolution bottleneck, limiting their ability to extract abstract semantic information for successful translation between classes with significant shape changes. To address this limitation, this paper proposes a deep hierarchical I2I translation framework that combines feature representations at various levels of abstraction. The proposed method, DeepI2I, extends the state-of-the-art BigGAN model to I2I translation by adding an encoder with the same architecture as the discriminator. It also introduces a knowledge transfer method that leverages pre-trained GANs to initialize the weights of the I2I model. Experimental results on multiple datasets, including animal faces and foods, demonstrate the effectiveness of the proposed framework, as well as the benefits of knowledge transfer and the use of an adaptor network to align the encoder and generator layers.