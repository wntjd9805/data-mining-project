This paper addresses the stochastic multi-armed bandit (MAB) problem, where a decision-maker selects from a set of arms and receives uncertain rewards over a time horizon. Traditionally, the number of arms is assumed to be small compared to the time horizon. However, in practical applications such as drug development and recommendation engines, there may be a large number of arms. The paper introduces a subsampled upper confidence bound (SS-UCB) algorithm that improves performance in the many-armed regime. Empirical results show that the greedy algorithm performs well in this regime as well. Theoretical analysis demonstrates high probability of concentrating attention on arms with high mean rewards. The paper concludes that in the many-armed regime, using greedy algorithms may be preferable in practical applications.