Reinforcement learning (RL) problems are distinguished from contextual bandit problems by their longer horizons and state-dependent transitions. In RL, actions taken early on can greatly impact the future, requiring the consideration of future state transitions along with immediate rewards. In contrast, contextual bandit problems treat each time step as independent, with the action chosen based on the current state. This paper examines the sample complexity of RL problems in relation to the planning horizon, addressing the question of whether a lower bound that depends polynomially on the horizon can be proven. Using the Online Trajectory Synthesis algorithm, the authors demonstrate that the sample complexity can be bounded by a logarithmic function of the horizon. These findings suggest that the perceived differences between long horizon RL and contextual bandit problems may not be due to the horizon dependence.