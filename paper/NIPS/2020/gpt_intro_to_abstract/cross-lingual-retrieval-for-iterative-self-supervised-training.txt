In this paper, we propose a novel self-supervised pretraining method called CRISS for multilingual sequence generation. We utilize the encoder outputs of a multilingual denoising autoencoder as a language agnostic representation to retrieve parallel sentence pairs. By training the model on these retrieved sentence pairs in an iterative manner, we improve its sentence retrieval and translation capabilities. We present empirical results that demonstrate the effectiveness of our approach, outperforming previous state-of-the-art methods in unsupervised machine translation and sentence retrieval. Additionally, we show that our pretraining method can enhance the performance of supervised machine translation. This paper provides an overview of related work, introduces language agnostic representations, describes the details of CRISS, evaluates its performance on various tasks, conducts ablation studies, and concludes with final remarks.