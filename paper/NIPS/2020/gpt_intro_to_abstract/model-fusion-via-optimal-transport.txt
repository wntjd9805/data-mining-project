This paper explores the fusion of two different neural networks into a single child network in a one-shot manner, without the need for retraining. The authors propose a layer-wise approach that aligns the neurons and weights of the parent models using optimal transport. The resulting method outperforms vanilla averaging, even when merging networks with different weights or trained for slightly different tasks. The authors demonstrate the potential of this approach in various scenarios, including adaptation to personal training data, parameter fusion, federated learning, and decentralized learning applications. The improved model fusion techniques have the potential to improve privacy and reduce communication costs.