Neural networks have been found to perform well when formed into ensembles of models with different hyperparameters. This diversity in ensembles is beneficial as it allows for complementary errors on held-out data. While various methods have been proposed to enhance diversity in neural network ensembles, this paper focuses on exploiting the diversity induced by combining networks with different hyperparameters. The authors aim to improve upon deep ensembles in terms of robustness and uncertainty quantification, as well as surpass batch ensembles in efficiency. A stratiÔ¨Åcation scheme is developed to achieve this, along with a parameterization combining the approaches of batch ensembles and self-tuning networks. Through experiments, the authors demonstrate the effectiveness of their approach in improving performance.