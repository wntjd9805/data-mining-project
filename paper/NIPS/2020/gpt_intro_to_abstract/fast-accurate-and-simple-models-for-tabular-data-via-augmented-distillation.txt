Modern AutoML tools have achieved good out-of-the-box accuracy on diverse datasets by using extensive model ensembling. However, the resulting predictors can be large, slow, opaque, and expensive to deploy. This paper introduces FAST-DAD, a technique to produce fast-and-accurate models through distillation with augmented data. By increasing the amount of data available for distillation, the student's approximation of the teacher can be improved, resulting in higher accuracy on test data. The contributions of this paper include a model-agnostic distillation technique, a maximum pseudolikelihood model for tabular data, Gibbs sampling for dataset augmentation, and a comprehensive benchmark for distillation strategies in tabular data.