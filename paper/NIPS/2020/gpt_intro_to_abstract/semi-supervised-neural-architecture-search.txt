Neural Architecture Search (NAS) has been successfully used in various tasks such as image classification and language modeling. NAS consists of a controller that generates new architectures and an evaluator that trains and evaluates candidate architectures. However, obtaining high-quality architecture-accuracy pairs for training the controller is expensive and time-consuming. To address this issue, we propose SemiNAS, a semi-supervised approach that leverages a large number of unlabeled architectures. SemiNAS trains an initial accuracy predictor, uses it to predict the accuracy of unlabeled architectures, and incorporates the generated architecture-accuracy pairs into the training data. We apply SemiNAS to the NAO algorithm, a gradient optimization-based NAS method, but it can be easily adapted to other NAS algorithms. Experimental results show that SemiNAS achieves similar accuracy to conventional NAS methods while significantly reducing computational costs and outperforms one-shot NAS in terms of accuracy. It achieves promising results in image classification and text-to-speech tasks, surpassing human-designed models.