Recent advances in technology have led to the availability of large amounts of complex high dimensional data in computer vision and machine learning applications. However, high dimensionality poses challenges such as confusion of algorithms with irrelevant dimensions and increased computational time and memory. To address these issues, researchers have explored techniques for representing high-dimensional data in lower dimensions, particularly in the context of subspace clustering. While conventional subspace clustering algorithms assume linear subspaces, many real-world datasets exhibit non-linear manifolds. To accommodate this, various approaches have incorporated projections and kernel tricks to express non-linearity. More recently, deep subspace clustering (DSC) methods have been proposed, which utilize deep learning to learn unsupervised nonlinear mappings that project data into a latent space with linear subspaces. Although data augmentation has proven beneficial in deep learning, current DSC frameworks are unable to fully utilize it. In this paper, we propose a modified DSC model that incorporates data augmentation and investigate the challenge of conveying the property of consistent outputs for similar data points without ground-truth labels. Specifically, we optimize a consistency loss based on temporal ensembling to ensure that the model maps plausible transformations of existing samples to consistent subspaces. We also present a simple yet effective method for finding efficient augmentation policies using a greedy search. Experimental results using mean Silhouette scores demonstrate the impact of different augmentation policies on the performance of our proposed model.