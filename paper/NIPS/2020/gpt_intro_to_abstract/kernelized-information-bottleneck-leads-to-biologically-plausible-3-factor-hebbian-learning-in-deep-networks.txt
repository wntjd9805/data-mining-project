Supervised learning in deep networks is commonly done using backpropagation, but this algorithm cannot explain learning in the brain due to various reasons. While alternatives like feedback alignment can solve some issues, they still require a backward pass and labeled data. Layer-wise update rules, on the other hand, remove the need for weight transport and a backward pass, and there are indications that they can be as effective as backprop. However, ensuring biological plausibility remains a challenge. In this paper, we propose a biologically plausible layer-wise learning rule inspired by the information bottleneck principle. We use the Hilbert-Schmidt independence criterion (HSIC) and modify it to create plausible HSIC (pHSIC), which incorporates neuronal limitations. Our experiments show that pHSIC-based learning rules perform almost as well as backpropagation on various datasets.