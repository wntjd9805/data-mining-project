Reward shaping is a common approach in reinforcement learning (RL) to enhance sample efficiency by incorporating domain knowledge into additional rewards. While existing methods mainly focus on generating shaping rewards based on potential values, they often assume that these rewards are completely helpful, disregarding the subjectivity and cognitive bias involved in the transformation of human knowledge into numeric values. In this paper, we propose a novel approach that aims to adaptively utilize a given shaping reward function, maximizing its beneficial aspects while ignoring the detrimental ones. We formulate the problem as a bi-level optimization, where the lower level optimizes the policy for shaping rewards maximization, and the upper level optimizes a parameterized shaping weight function for maximizing the expected accumulative true reward. We provide formal results for computing the gradient of the expected true reward with respect to the weight function parameters and propose three learning algorithms to solve the bi-level optimization problem. Extensive experiments conducted in cart-pole and MuJoCo environments demonstrate the effectiveness of our algorithms in identifying the quality of shaping rewards and adaptively utilizing them, even transforming harmful shaping rewards into beneficial ones for improved and accelerated policy optimization.