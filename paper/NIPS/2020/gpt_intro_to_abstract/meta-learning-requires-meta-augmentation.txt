Data augmentation is a crucial technique in machine learning to improve performance in various fields such as computer vision, speech recognition, and natural language processing. While it is known to be effective, its impact on performance can be significant. In this paper, we focus on the application of data augmentation in meta-learning, a framework for efficient learning of new tasks from a limited number of examples. We propose a unified framework for meta-data augmentation and provide an information theoretic view on how it prevents overfitting. Our contributions include modifications to handle two forms of overfitting, memorization overfitting and learner overfitting, and we demonstrate the importance of meta-augmentation through experiments on various benchmarks and meta-learning algorithms.