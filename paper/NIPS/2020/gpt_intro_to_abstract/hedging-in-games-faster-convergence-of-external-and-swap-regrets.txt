Online algorithms for regret minimization are essential in machine learning applications that require real-time sequential decision making. Several algorithms have been developed to minimize regret, including Hedge / Multiplicative Weights, Mirror Decent, and Follow the Regularized / Perturbed Leader. The average regret decay rate for these algorithms in an adversarial environment is well understood. However, it remains unclear whether the regret of each player can decay faster when players use these algorithms in a repeated game setting. Previous research has shown that if players in a multiplayer game run an algorithm satisfying the RVU (Regret bounded by Variation in Utilities) property, then the regret of each player decays at a rate of O(1/T^3/4). This paper presents the BM-Optimistic-Hedge algorithm, which achieves a significantly lower average swap regret than O(1/T) in the repeated game setting. The algorithm combines the external-to-internal reduction method with the optimistic Hedge algorithm and shows improved convergence to a correlated equilibrium. Additionally, the paper analyzes regret minimization in two-player games and demonstrates that optimistic Hedge can achieve a lower average regret of O(1/T^5/6), while vanilla Hedge cannot outperform the O(1/T^3/4) bound. The analysis of optimistic Hedge includes a lemma that bounds the trajectory length of strategy movements based on cost vectors, allowing for the estimation of individual regrets. The paper also provides lower bounds for vanilla Hedge, using simple games and the zero-sum Matching Pennies game to showcase different learning rate scenarios. These bounds demonstrate that at least one player must have regret at least O(âˆšT) at some point.