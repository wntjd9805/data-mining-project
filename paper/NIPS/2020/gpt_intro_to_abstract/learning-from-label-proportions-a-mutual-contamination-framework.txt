Learning from label proportions (LLP) is a weak supervision setting for classification where training data consists of bags with unlabeled instances and annotated proportions of each class. Various methods have been developed for LLP, but the theoretical foundations and algorithmic consistency have been lacking. In this paper, we propose a statistical framework based on mutual contamination models (MCMs) and use it to establish an empirical objective for LLP, prove generalization error bounds, and demonstrate universal consistency with respect to the balanced error rate. Additionally, we introduce a novel experimental setting to address limitations of previous comparisons.