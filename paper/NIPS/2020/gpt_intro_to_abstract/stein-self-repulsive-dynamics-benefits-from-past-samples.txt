Drawing samples from complex un-normalized distributions is a fundamental challenge in statistics and machine learning. Markov Chain Monte Carlo (MCMC) and variational inference have been proposed as approximate sampling methods, but MCMC is criticized for its slow convergence rate and high auto-correlation. In this paper, we introduce a different method called Stein variational gradient descent (SVGD), which evolves particles in parallel to match the distribution of interest. We address the limitation of SVGD requiring an infinite number of chains by proposing a variant called "single-chain SVGD" that combines repulsive dynamics with gradient-based MCMC. We provide theoretical analysis and empirical evaluations that demonstrate the effectiveness of our method in improving uncertainty estimation and increasing the effective sample size.