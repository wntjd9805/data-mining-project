This paper focuses on the analysis of machine learning systems with a fixed number of parameters N trained on a dataset of size n. Traditional statistical learning theory typically examines the limits of either the dataset size or the model complexity, assuming that one dimension is much smaller than the other. However, in practice, as data size increases, the model becomes more complex. This paper examines the double asymptotic regime where both n and N are constants, which is a more realistic scenario but also more challenging to analyze. The authors investigate the behavior of random Fourier features and random feature maps in this regime and provide a precise characterization of their asymptotics. They also explore the effect of the relative model complexity N/n on the performance of machine learning algorithms and identify a phase transition and double descent phenomenon. Theoretical results are supported by empirical evaluations on real-world datasets.