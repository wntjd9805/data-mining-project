Machine learning is being widely used in various domains, such as healthcare, banking, and manufacturing. With the increasing transparency of predictive models, end-users tend to use them prescriptively, seeking guidance on how to adapt their features to improve future outcomes. However, models optimized for accuracy may not accurately reflect post-modification outcomes, leading to potentially detrimental actions. This paper presents a learning framework for organizations aiming to deploy transparent and responsible predictive models. The framework focuses on achieving both predictive accuracy and safe decision-making by controlling the tradeoff between accuracy and decision quality. The proposed approach, called lookahead regularization, models user actions and penalizes models that do not generate confident improvements in outcomes. The framework includes an uncertainty model trained through importance weighting to handle covariate shift and estimate accurate intervals for decision outcomes. Experimental results using synthetic data, wine quality prediction, and diabetes progression prediction demonstrate the effectiveness and tradeoffs of the approach.