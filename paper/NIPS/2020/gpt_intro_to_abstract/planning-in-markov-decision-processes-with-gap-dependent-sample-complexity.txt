In this paper, we consider the task of Monte-Carlo planning in reinforcement learning. We propose an algorithm in the fixed confidence setting, where the goal is to recommend a good action to be taken by the agent in a given state. We measure the quality of the action recommendation by its simple regret and provide an algorithm that returns an action with a regret bounded by a given value. We prove that the sample complexity of our algorithm is bounded in high probability. We also compare our algorithm with other planning algorithms in terms of sample complexity guarantees. Additionally, we discuss the use of Monte-Carlo Tree Search in planning and its potential for improving policy learning. The sample complexity of state-of-the-art MCTS algorithms is still largely unknown, and we aim to contribute to understanding this aspect.