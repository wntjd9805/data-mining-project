The goal in statistical learning is to minimize the expected risk associated with a given loss function in order to learn hypotheses that generalize well. Generalization bounds, which provide an upper bound on the expected risk based on the empirical risk, are essential in machine learning. Concentration inequalities, such as Chernoff's and McDiarmid's inequalities, are commonly used techniques to control the deviation between population and empirical averages. However, in certain applications where mean performance might not be the best objective to optimize, alternative measures of risk, such as coherent risk measures, are of interest. One popular coherent risk measure is Conditional Value at Risk (CVAR), which has been widely studied in machine learning. While concentration inequalities for CVAR have been derived, there is a lack of results in the statistical learning setting. This paper fills this gap by providing a sharp PAC-Bayesian generalization bound for minimizing CVAR in the learning setting.