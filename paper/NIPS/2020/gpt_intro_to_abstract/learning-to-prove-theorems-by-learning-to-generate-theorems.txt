Automated theorem proving involves generating proofs automatically using a formal language. Deep learning has shown promise in learning search heuristics for theorem provers, but the reliance on human-written theorems and proofs limits scalability. To address this, we propose training a theorem prover using synthetic data generated by a neural theorem generator called "MetaGen". The generator uses inference rules and existing theorems to construct new theorems and proofs. We parameterize the generator with deep networks to generate synthetic theorems that are similar to human-written ones. We instantiate our approach in Metamath and Holophrasm, and experiments show that synthetic data from MetaGen improves theorem proving on Metamath.