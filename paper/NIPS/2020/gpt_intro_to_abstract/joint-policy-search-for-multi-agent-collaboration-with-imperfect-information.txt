This paper explores the limitations of deep reinforcement learning in multi-agent collaborative games with imperfect information, where the traditional approach of improving the policy of the current agent leads to sub-optimal equilibria. To address this issue, the authors propose a novel approach called Joint Policy Search (JPS) that focuses on sparse policy changes and introduces a policy-change density metric. The JPS approach is proven to be computationally efficient and outperforms existing methods in collaborative games. The authors also demonstrate the applicability of JPS in Contract Bridge bidding, where it achieves significant improvements over the state-of-the-art program Wbridge5.