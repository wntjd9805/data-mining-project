Deep generative models, such as variational autoencoders (VAEs), have limitations in their factorial posterior assumption and inability to offer exact density estimation. To address these limitations, normalizing flows (NFs) have emerged as a valuable approach, providing smooth and invertible transformations with tractable Jacobians. NFs complement VAEs by transforming a simple base distribution into a more accurate representation of the true posterior. Recent developments in NFs have aimed at creating deeper and more complex transformations, but this comes with the risk of overfitting and slower training. In this paper, we propose gradient boosted normalizing flows (GBNF), a wider approach that improves the expressiveness of density estimators and posterior approximations. GBNF utilizes gradient boosting to iteratively add new NF components to the model, resulting in a mixture structure with the advantages of boosting and simplified training objectives. GBNF improves performance without increasing the complexity of transformations, and it does not slow down prediction and sampling processes. Our experiments demonstrate that GBNF produces image modeling results on par with state-of-the-art NFs and improves density estimation for complex, multi-modal data.