In recent years, there has been a growing interest in self-supervised visual representation learning as a solution to the manual annotation bottleneck. Early approaches focused on using "pretext" tasks with automated ground-truth generation. However, the performance of these learned representations has been inferior to supervised counterparts. In the past six months, there have been significant advancements in self-supervised learning, particularly in instance discrimination. This paper aims to uncover the reasons behind these gains and evaluate the invariances encoded by the learned representations. The results indicate that recent gains primarily come from occlusion invariances, although there is still a gap in viewpoint and category instance invariance compared to supervised approaches. The paper proposes leveraging natural video data for the instance discrimination task and demonstrates improved viewpoint invariance, category instance invariance, occlusion invariance, and object recognition performance. The paper also presents the contrastive learning framework and discusses the choice of transformation functions for self-supervised learning.