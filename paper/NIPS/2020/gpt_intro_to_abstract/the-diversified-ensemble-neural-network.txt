Deep neural networks (DNNs) have shown expressive representation power but suffer from the overfitting issue, making them less popular on small datasets compared to decision tree-based methods. In ensemble learning, diversity among modules is desirable for more stable predictions, but it is not well explored in neural networks. This study aims to develop a computationally efficient and stable neural network-based ensemble model by maximizing module diversity. The proposed diversified ensemble layer can be used with different network architectures and incurs minimal time overhead. The contributions include a principled technique for diversity, an adaptive learning procedure, and experimental results demonstrating improved accuracy with low extra time and space requirements. The proposed ensemble layer can be easily applied to various network architectures.