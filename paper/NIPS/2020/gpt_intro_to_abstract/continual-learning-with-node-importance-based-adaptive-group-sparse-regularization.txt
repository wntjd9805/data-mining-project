Continual learning, or lifelong learning, is a well-known challenge in the field of machine learning. The goal of continual learning is to find a balance between stability and plasticity, where a model can efficiently learn new tasks without forgetting previously learned tasks. In this paper, we focus on regularization-based methods, which aim to use neural networks effectively and potentially combine with other approaches. We propose a new method called Adaptive Group Sparsity based Continual Learning (AGS-CL), which controls both plasticity and stability by using two node-wise group sparsity-based penalties as regularization terms. We demonstrate the effectiveness of AGS-CL through extensive experiments on various benchmarks in supervised and reinforcement learning, showcasing its superiority over state-of-the-art baselines. We also analyze the stability-plasticity trade-off, evaluate memory usage, and highlight the applicability of AGS-CL to larger networks. Additionally, we present results in the pure continual learning setting in Atari games, where past tasks cannot be relearned.