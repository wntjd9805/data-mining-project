Video generation poses unique challenges compared to image generation, including the need to generate multiple images with accurate continuity in time and appropriate scene composition. Mode collapse is also a more severe problem in video synthesis. To address these challenges, this paper proposes a novel patch-VAE formulation for video generation that explicitly models the patch distribution. This approach allows for faithful reconstruction and generation of novel samples, avoiding mode collapse. A hierarchical patch-VAE is then introduced, using a multi-scale decoder and a single encoder. The VAE's KL constraint is applied only to the activation map of the single encoder, and the different scales of the decoder are trained sequentially. Experimental results demonstrate that the proposed method surpasses previous approaches in terms of diversity and quality of video generation. Key contributions are the patch-VAE formulation, the patch VAE-GAN method, and the ability to unconditionally generate videos from a single sample.