The determination of optimal step sizes in stochastic gradient descent is a major challenge in deep learning optimization. Previous approaches have used line search methods, but they have not fully analyzed the shape of the loss function in the direction of the update step. In this paper, we empirically examine the shape of the loss function in deep learning scenarios and propose a simple line search optimizer that constructs its own learning rate schedule based on parabolic approximations. We provide a comprehensive performance analysis and compare our method to other optimization techniques. Additionally, we investigate the convergence properties and explore the relationship between exact line searches and our approach. We support our empirical findings with a convergence analysis. The empirical loss function is defined as the average over batches, and we consider the loss function in the negative gradient direction for optimization. Our work is motivated by the assumption that the position of the minimum of the line function is a good estimator of the position of the minimum of the empirical loss function. We empirically analyze this assumption in further detail.