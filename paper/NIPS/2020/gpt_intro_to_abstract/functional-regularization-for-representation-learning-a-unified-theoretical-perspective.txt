Advancements in machine learning have led to the development of large prediction models that require a significant amount of labeled data to achieve effective learning. However, the high costs associated with labeling data have resulted in the popularity of unsupervised representation learning techniques using unlabeled data. These techniques involve learning a representation function and a prediction function over the representation for the target prediction task. Various auxiliary tasks, such as auto-encoders and manifold learning, have been used with unlabeled data to aid representation learning. While these approaches have achieved impressive empirical performance, there is a lack of theoretical studies focused on understanding them. This paper aims to improve the theoretical understanding of the benefits of learning representations through auxiliary tasks by analyzing the sample complexity of labeled and unlabeled data. The authors propose a unified perspective that views different representation learning approaches as imposing a learnable regularization on the representation. They present a PAC-style discriminative framework that bounds the sample complexities of labeled and unlabeled data under different assumptions. The analysis demonstrates that functional regularization with unlabeled data can reduce the labeled data required for the prediction task, and the authors provide concrete examples using auto-encoders and masked self-supervision to showcase the application of their framework.