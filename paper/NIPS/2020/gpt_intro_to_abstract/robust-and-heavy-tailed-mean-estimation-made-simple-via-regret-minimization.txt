Learning from high-dimensional data in the presence of outliers is a challenging task in statistics and machine learning. Outliers can arise from various sources, including random corruptions and malicious tampering. Handling outliers in high dimensions poses computational difficulties, as classical robust estimators suffer from worst-case computational hardness and naive computationally-efficient algorithms have suboptimal error rates. Recent works have made progress in developing efficient algorithms with optimal error rates for high-dimensional robust statistics, but many algorithmic questions remain unanswered. This paper revisits the problem of estimating the mean of a distribution from samples in two robust settings: robust mean estimation and heavy-tailed mean estimation. The authors propose a unified treatment of iterative methods for these problems and address fundamental questions regarding their algorithmic connections and rigorous analysis. The main results include reductions of both robust and heavy-tailed mean estimation to a spectral sample reweighing problem and the analysis of well-known algorithms, such as FILTER and gradient descent, for these tasks. The framework captures state-of-the-art algorithms for robust mean estimation and demonstrates their efficiency in terms of running time and error rates.