This paper presents a problem in non-convex optimization, aiming to minimize a general non-convex function. Due to the NP-hard nature of finding the global minimum, the paper focuses on finding an Îµ-approximate first-order stationary point. The paper explores gradient-based algorithms in two scenarios: when the gradient of the function is accessible and when only a stochastic estimator is available. The paper also introduces the concept of (L0, L1)-smoothness, which is a more realistic assumption for function smoothness compared to standard L-smoothness. The paper provides a comprehensive analysis of iteration complexities for (L0, L1)-smooth objectives and proposes a unified framework that includes various clipping-based algorithms. The convergence rate and performance of these algorithms are evaluated through experiments on different tasks.