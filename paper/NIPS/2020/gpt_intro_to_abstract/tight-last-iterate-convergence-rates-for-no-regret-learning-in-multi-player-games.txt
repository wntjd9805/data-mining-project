In the setting of multi-agent online learning, this paper studies the convergence of players' actions to a coarse correlated equilibrium (CCE) in games. The paper focuses on establishing last-iterate convergence rates for monotone games, including smooth and bilinear games. The optimistic gradient (OG) algorithm is analyzed and shown to exhibit last-iterate convergence to a Nash equilibrium in smooth, monotone games at a rate of O(1/pT). This result is significant as it provides a positive answer to a question about the convergence rate of last-iterate actions when players follow a no-regret learning algorithm with constant step size. The paper also presents a lower bound for any algorithm belonging to the class of p-SCLI algorithms, which includes OG. The proof of the upper bound for OG uses a new technique called adaptive potential functions, while the proof of the lower bound reduces to a question about the spectral radius of a family of polynomials. The paper focuses on the unconstrained setting and leaves the constrained setting for future work.