This paper introduces the concept of Output-Constrained Bayesian Neural Networks (OC-BNNs) as a solution for incorporating functional knowledge expressed as output constraints in machine learning models. The authors propose a sampling-based prior that assigns probability mass to BNN parameters based on how well the output satisfies the specified constraints. The OC-BNN framework is applicable to all black-box BNN inference algorithms and allows for the enforcement of various types of output constraints, such as known input-output relationships or critical desiderata. The paper presents a formal framework, formulates the prior, and demonstrates proof-of-concepts on toy simulations as well as real-world datasets in the domains of clinical action prediction, racial fairness in recidivism prediction, and recourse in credit scoring.