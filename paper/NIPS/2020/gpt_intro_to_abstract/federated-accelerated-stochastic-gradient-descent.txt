In this paper, we focus on the importance of leveraging distributed computing resources and decentralized data for large-scale machine learning applications. Communication is often the bottleneck for parallelization in both data-center and cross-device federated settings. We study the distributed stochastic optimization problem and propose a principled acceleration for Federated Averaging (FEDAVG), called Federated Accelerated Stochastic Gradient Descent (FEDAC), which improves convergence rate and communication efficiency. We analyze the stability and acceleration tradeoff in distributed settings and extend the results of previous work to more general or strongly convex objectives. Our empirical results demonstrate the efficiency of FEDAC compared to baseline methods in highly infrequent communication scenarios.