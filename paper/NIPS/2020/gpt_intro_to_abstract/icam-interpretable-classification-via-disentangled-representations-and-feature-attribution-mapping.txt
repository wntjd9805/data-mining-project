Brain images are valuable for studying behavior and neurological diseases, but analyzing these images is challenging due to their variability and complexity. Traditional approaches use global templates to compare brain data, but this ignores cortical heterogeneity and limits interpretation. Deep learning has shown promise for brain imaging tasks, but there is a need for more accountable networks, especially in the medical domain. In this paper, we propose ICAM, a framework that combines image-to-image translation with a Variational Autoencoder (VAE) and Generative Adversarial Network (GAN) to improve feature attribution in brain imaging. ICAM allows for simultaneous classification and generation of meaningful feature attribution maps by disentangling class-relevant attributes from class-irrelevant content. We demonstrate the effectiveness of ICAM through qualitative and quantitative validation on multiple datasets, including Human Connectome Project, Alzheimerâ€™s Disease Neuroimaging Initiative, and UK Biobank. Our code is publicly available on GitHub for multi-class classification and regression tasks.