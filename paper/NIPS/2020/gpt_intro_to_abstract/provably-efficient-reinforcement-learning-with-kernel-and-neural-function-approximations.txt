Reinforcement learning (RL) algorithms combined with modern function approximators have been successful in various application problems. However, there is a lack of theoretical guidance in designing computationally and statistically efficient RL algorithms when using powerful function approximators. Existing theoretical treatments are limited to the tabular setting, and there is a disconnect between theory and practical applications. Recent work has addressed efficient exploration in the function approximation setting but mostly with linear models, which are rigid and often not satisfied in practice. This work aims to design RL algorithms that incorporate powerful nonlinear function approximators, such as neural networks or kernel functions, while achieving computational and statistical efficiency. The proposed algorithm provides an affirmative answer to this question and focuses on the episodic Markov decision process setting. The algorithms utilize kernel functions or overparameterized neural networks to represent the value function and incorporate a UCB bonus term to each iterate. The algorithms guarantee polynomial runtime and sample complexity without additional assumptions on the data-generating model. Regret bounds are derived for both the kernel and neural network settings, depending on the complexity of the state space. This work presents the first provably efficient framework for RL with kernel and neural network function approximations.