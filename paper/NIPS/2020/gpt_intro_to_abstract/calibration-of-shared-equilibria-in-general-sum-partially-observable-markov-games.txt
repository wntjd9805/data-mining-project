Multi-agent learning in partially observable settings presents challenges, but the use of a single shared policy network across all agents has proven to be an efficient training mechanism. This network takes individual agent observations as input and outputs individual agent actions, enabling decentralized execution. Additionally, agents can include agent-specific information in their observations to introduce heterogeneity. In this paper, we explore the nature of equilibria learned by agents using a shared policy from a game theoretic standpoint, showing that they are symmetric pure Nash equilibria of a higher-level game. We introduce the concept of Shared equilibria and propose CALSHEQ, a novel dual-RL-based algorithm for calibrating shared equilibria to externally specified targets. CALSHEQ includes an RL calibrator that optimally balances agent types to meet calibration targets, avoiding the need for repeated training. Through experiments, we demonstrate that CALSHEQ outperforms a Bayesian optimization baseline.