Hierarchical Clustering (HC) is a fundamental problem in data analysis, with applications ranging from phylogenetics to community detection. However, the lack of a global objective function has hindered the development of HC algorithms. To address this issue, Dasgupta introduced a discrete cost function that corresponds to meaningful hierarchical partitions in the data. In this paper, we propose HYPHC, an end-to-end differentiable model for HC that leverages hyperbolic embeddings and a continuous analogue of the lowest common ancestor (LCA). We show that HYPHC produces good clustering quality, both theoretically and empirically. We also demonstrate the scalability, flexibility, and integration into machine learning pipelines of our approach. Moreover, HYPHC outperforms existing discrete and continuous methods on various HC benchmarks and improves accuracy in a downstream classification task.