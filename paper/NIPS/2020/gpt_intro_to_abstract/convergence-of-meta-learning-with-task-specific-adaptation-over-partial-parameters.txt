Meta-learning has gained significant attention in the field of machine learning, especially with the integration of deep neural networks. Unlike traditional learning approaches, meta-learning aims to use prior knowledge and datasets from a task ensemble to quickly learn new tasks with limited data. This paper focuses on optimization-based meta-learning algorithms, specifically the model-agnostic meta-learning (MAML) algorithm. However, the MAML algorithm can have high computational and memory costs due to the Hessian update in the outer loop. To address this, a simplified version called almost no inner loop (ANIL) has been proposed, which updates only a small subset of parameters in the inner loop. While previous work has shown empirical results of ANIL's effectiveness, this paper aims to provide theoretical analysis of ANIL, including its convergence rate, computational complexity, and hyper-parameter selection. The authors also introduce new techniques to capture the unique properties of ANIL.