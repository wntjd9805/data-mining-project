Recent interest in computer science has focused on object-centric generative models of images and videos, which explicitly represent the composition of multiple objects or regions in their latent structure. These models allow for the segmentation of input videos or images into objects and the generation of new ones. While existing approaches treat objects as 2D spatial mixtures or 2.5D stacks of sprites, they do not capture the underlying physical structure of the world, which consists of solid 3D objects situated in 3D space. In this paper, we propose a novel object-centric generative model that explicitly models the 3D scenes depicted in videos. Our model is trained purely from unannotated monocular videos and learns to decompose videos into multiple 3D foreground objects and a 3D background, as well as generate videos showing coherent scenes. We achieve this by designing a generative model that represents videos as the view from a camera moving through a 3D scene. We use a single latent embedding that captures the space of allowable scene structures, and a structured decoder that processes objects independently and compositionally. This allows for the efficient representation of a distribution of videos without needing to separately model all possible combinations of objects, shapes, textures, and poses. Additionally, by treating objects and the background as 3D, we automatically capture variation in appearance due to viewpoint changes. We conduct extensive experiments on datasets of videos with stationary and moving objects and show that our method can decompose videos into constituent objects and background, determine the 3D structure, and generate coherent videos. Overall, our contribution is the first object-centric generative model of videos that explicitly reasons in 3D space and learns to decompose scenes into 3D objects and generate new, plausible samples.