AI systems are becoming increasingly important in decision-making processes, leading to a greater emphasis on the development of trustworthy AI. One key aspect of trustworthy AI is the ability to produce consistently correct outputs for the same input. However, as AI models are periodically retrained, there is no guarantee that different generations of the model will maintain this consistency. Inconsistent behavior in AI systems can be exploited by attackers and compromise safety. In this paper, we focus on classification models and define consistency as the ability to make consistent predictions across successive model generations for the same input. We introduce the concept of correct-consistency, which refers to the ability to consistently produce correct predictions for the same inputs. We then analyze the effect of consistency and correct-consistency on users' trust using various scenarios. Previous studies have mentioned consistency but have not formally defined or measured it. We investigate how ensemble learning can improve consistency and correct-consistency in deep learning classifiers, both theoretically and empirically. We present several contributions, including formally defining consistency and correct-consistency, providing a theoretical explanation of how ensembles can improve consistency, proving the benefits of ensemble learning, proposing a dynamic snapshot ensemble learning with pruning algorithm, and conducting experiments to demonstrate the effectiveness of our approach.