Deep neural networks (DNN) have been widely used in machine learning tasks, but they are vulnerable to adversarial attacks where imperceptible perturbations can fool the classifiers. Adversarial machine learning research aims to design stronger attacks and more robust neural networks. This paper introduces AdvFlow, a black-box adversarial attack that utilizes pre-trained normalizing flows to generate adversarial examples. By modeling the probability distribution of adversarial examples using flow-based methods, and solving the black-box optimization problem, AdvFlow generates perturbations that maintain the structure of the data, making them difficult to detect. The paper proves a lemma about the generated perturbations and demonstrates the effectiveness of AdvFlow in terms of detectability, success rate, and transferability compared to existing methods. The contributions of this paper include the introduction of AdvFlow, the proof of a lemma about the generated perturbations, and the demonstration of its performance against adversarial training defense techniques.