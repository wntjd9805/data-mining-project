Many applications today rely on large and complex neural network models, necessitating the efficient use of ML accelerators like GPUs and TPUs. To optimize the execution of these models on specific devices, compilers such as TensorFlow XLA, Glow, MLIR, and AutoTVM are used to map high-level computational graphs to device-executable operations. However, current compiler optimization approaches suffer from sub-optimal configurations and lack of joint optimizations across tasks. Existing RL-based approaches outperform human experts and heuristics but require significant computational resources and lack generalizability. To address these limitations, this paper proposes an end-to-end deep RL method called GO, which consists of an inductive graph-embedding network and a policy network for optimization decisions. GO can be trained on a set of computation graphs and transferred to new graphs, allowing for faster convergence with fewer resources. Additionally, a novel recurrent attention mechanism is introduced to jointly optimize multiple dependent graph optimization tasks.