The widespread use of automatic decision-making systems has raised concerns about their potential for unfair behavior. As a result, machine learning models are often required to meet fairness requirements to prevent racist or sexist decisions. There are various methods in the literature to generate fair models with respect to sensitive attributes, such as gender or ethnicity. These methods can be categorized into three families: post-processing of pre-trained models to make them fair, enforcing fairness during training, and modifying the data representation before employing standard machine learning methods. However, using the same model for different tasks can lead to unexpected unfair behavior. In this paper, we propose a multitask learning framework to create a fair representation that generalizes well to unseen tasks. We measure fairness using demographic parity and enforce fairness by imposing constraints on different distances between subgroups. We demonstrate empirically and theoretically that our method transfers well to new tasks with small unfairness. Our results imply that the learned representation can be used to learn fair models for new tasks without additional fairness constraints. We discuss related work, describe our proposed method, analyze its generalization properties, compare it with baselines on real-world datasets, and conclude with future research directions.