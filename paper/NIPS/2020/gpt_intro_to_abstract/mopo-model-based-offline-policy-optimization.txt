Recent advances in machine learning have led to significant successes in scaling deep neural networks to large realistic datasets in various domains, such as computer vision, natural language processing, and robot learning. However, reinforcement learning (RL) methods struggle to scale to real-world applications due to their reliance on costly online trial-and-error. In contrast, pre-recorded datasets in domains like autonomous driving and healthcare can be large and diverse. This paper aims to address the challenge of designing RL algorithms that can learn from diverse, static datasets in order to enable more practical RL training in the real world and enhance generalization. The authors argue that offline RL algorithms should be able to leave the data support of the offline dataset and learn a better policy, as the provided batch dataset is usually sub-optimal and the target task may differ from the tasks performed in the batch data. To investigate this, the authors hypothesize that model-based RL methods have the potential to enable generalization, as they receive more supervision, are trained with supervised learning, and have well-developed uncertainty estimation techniques. A proof-of-concept experiment is conducted, evaluating two state-of-the-art off-policy model-based and model-free algorithms on offline RL tasks. The findings support the hypothesis and suggest that model-based methods are well-suited for batch RL. However, there is still room for improvement, particularly in incorporating a reward penalty based on the model's error estimation. Therefore, the paper introduces an offline model-based RL algorithm that optimizes a policy in an uncertainty-penalized Markov Decision Process (MDP) by penalizing the reward function with an estimate of the model's error. The authors demonstrate that this approach outperforms existing methods on offline RL benchmarks and problems that require generalization to out-of-distribution states.