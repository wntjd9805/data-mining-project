This paper investigates the vulnerability of deep neural networks to adversarial examples, which are small perturbations of the input signal that can cause significant changes in the model output. The authors explore the strategies of adversarial training and detecting perturbations to defend against adversarial attacks. The paper also addresses open questions regarding the existence of robust models, the amount of training data required, and the computational feasibility of learning robust models. The authors propose a new variant of spectral norm regularization and demonstrate through empirical evaluations that adversarial perturbations align with dominant singular vectors and that adversarial training and data-dependent spectral norm regularization result in more linear models around data.