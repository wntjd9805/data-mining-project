In this paper, we address the issue of temporal inconsistency in applying image processing algorithms to videos. While task-specific video processing algorithms can improve temporal coherence, there is a need for a generic framework that can ensure strong temporal consistency across different tasks. We propose a novel approach that transforms an image processing algorithm into its video processing counterpart, resulting in a temporally consistent video. Previous work has explored general frameworks for improving temporal consistency but may not hold in practice. Our framework utilizes the Deep Video Prior by training a convolutional network on videos to ensure consistency among corresponding patches in video frames. We do not enforce handcrafted temporal regularization but instead leverage the network's prior to correct flickering artifacts similar to noise in the temporal domain. Our method does not require training on a large-scale dataset and demonstrates better temporal consistency and less performance degradation compared to state-of-the-art methods.