The major bottleneck of applying machine learning to practical problems is the high cost associated with data and labeling. Meta-learning, or learning to learn, aims to reduce the sample complexity of machine learning methods by leveraging a collection of tasks sampled from a distribution. This paper focuses on the domain randomized search (DRS) approach to meta-learning, where the meta-learned information is used as an initialization for iterative optimization. The authors rigorously prove the effectiveness of DRS as a meta-learning algorithm and explore the trade-off between modeling accuracy and computational ease. Empirical, sample complexity, and theoretical analyses are conducted to compare DRS with the representative meta-learning algorithm MAML in the context of meta-reinforcement learning and meta-linear regression.