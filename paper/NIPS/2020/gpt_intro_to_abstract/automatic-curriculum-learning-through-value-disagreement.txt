Model-free reinforcement learning (RL) has achieved remarkable success in various games and control tasks, but a major limitation is its high sample complexity. Humans, on the other hand, are able to learn efficiently from significantly fewer samples. This paper investigates how to endow RL agents with the ability to learn efficiently by employing a curriculum learning approach. While most curriculum learning research has focused on the order of data in supervised learning, this paper focuses on organizing the presentation of goals in RL. The goal is to improve the sample efficiency of goal-conditioned RL by proposing challenging yet solvable goals for the agent. The proposed approach, called Value Disagreement based Sampling (VDS), selects goals based on the epistemic uncertainty of the value function. Empirical results on challenging sparse-reward tasks demonstrate that VDS improves the sample efficiency of RL algorithms. The code for VDS is publicly available.