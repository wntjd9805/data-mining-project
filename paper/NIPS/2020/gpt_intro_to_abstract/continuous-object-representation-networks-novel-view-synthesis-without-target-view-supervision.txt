In this paper, we introduce Continuous Object Representation Networks (CORNs), a neural object representation that can generate novel views of a scene from as few as two source images per object. CORNs use a continuous, differentiable function to map local and global features to 3D world coordinates, allowing for multi-view consistency and fast inference of novel views from a single image. We evaluate CORNs on various 3D computer vision tasks and show that despite being self-supervised, CORN performs competitively compared to state-of-the-art approaches that require more images and direct supervision. Our method has applications in novel view synthesis, 3D model reconstruction, and out-of-domain view synthesis.