Reinforcement learning (RL) faces challenges in evaluating new policies before deployment, particularly when historical data collected from a different policy is used. This problem, known as off-policy evaluation (OPE), is difficult to solve due to the exponential growth of variance in unbiased estimators. Recent advances in RL have introduced algorithms that use function approximation and marginalized importance weights to improve evaluation accuracy. However, these methods still have limitations, such as bias and neglecting important data components. In this paper, we propose a unified method that addresses these problems by combining existing approaches. Our method provides double robustness and quantifies the misspecification of function classes while making effective use of all data components. We also explore the application of our method to two RL problems, reliable off-policy policy optimization and efficient exploration.