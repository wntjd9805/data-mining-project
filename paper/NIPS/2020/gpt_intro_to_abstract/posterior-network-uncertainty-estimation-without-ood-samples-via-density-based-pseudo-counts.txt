This paper focuses on the importance of quantifying uncertainty in neural network predictions for reliable machine learning. In sensitive domains such as robotics, finance, and medicine, trust in AI systems is crucial for granting them autonomy. AI systems that are aware of their uncertainty can adapt to new situations and avoid making decisions in unknown or unsafe conditions. However, traditional neural networks often exhibit overconfident predictions, even when faced with significantly different data. It is necessary for these networks to differentiate between aleatoric and epistemic uncertainty, which represent data and knowledge uncertainty, respectively. Aleatoric uncertainty is inherent in the data itself, such as the 50/50 chance of a fair coin landing on heads. Epistemic uncertainty arises from a lack of knowledge about unseen data, like an image of an unknown object or an outlier in the data.