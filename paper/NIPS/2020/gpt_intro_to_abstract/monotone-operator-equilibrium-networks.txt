Recent advancements in deep learning have highlighted the effectiveness of implicit-depth networks, where features are generated through the solution of implicitly defined equations, rather than explicitly iterating through nonlinear layers. This includes models like the Neural ODE and the Deep Equilibrium (DEQ) Model, which have shown promising results in sequence modeling and improved memory efficiency compared to traditional deep networks. However, achieving stable convergence in these models requires extensive tuning and regularization, and the uniqueness and existence of solutions are not guaranteed. In this paper, we introduce a new class of implicit-depth equilibrium model called the Monotone Operator Equilibrium Network (monDEQ), which guarantees stable convergence to a unique fixed point. Our model is based on the theory of monotone operators and establishes the existence and uniqueness of the equilibrium point through parameterization and backpropagation using the implicit function theorem. We further demonstrate the practical implementation of monDEQ and the use of operator splitting methods, achieving superior performance compared to other models in terms of accuracy and efficiency.