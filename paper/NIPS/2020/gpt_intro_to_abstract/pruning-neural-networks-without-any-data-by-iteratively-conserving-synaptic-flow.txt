Network pruning is a widely studied topic in computer science, with applications in both practical deployment and theoretical understanding of neural networks. Conventionally, pruning algorithms have focused on compressing pre-trained models, but recent research has shown that there are sparse subnetworks, known as winning tickets, in randomly-initialized neural networks that can achieve similar test accuracy as the original network. However, the challenge lies in efficiently identifying these winning ticket subnetworks at initialization without training or looking at the data. In this paper, we investigate the limitations of existing pruning algorithms at initialization, propose strategies to overcome these limitations, and introduce a novel data-agnostic algorithm called Iterative Synaptic Flow Pruning (SynFlow) that achieves state-of-the-art pruning performance across various models and datasets. Our contributions include studying layer-collapse, conserving synaptic saliency, explaining the pruning of large layers, and demonstrating the effectiveness of iterative magnitude pruning in avoiding layer-collapse.