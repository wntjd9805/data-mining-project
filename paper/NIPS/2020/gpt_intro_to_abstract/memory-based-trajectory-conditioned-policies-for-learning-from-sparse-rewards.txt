This paper addresses the challenge of reasoning over long horizons with sparse rewards in deep reinforcement learning (DRL) algorithms. The authors propose a novel trajectory-conditioned policy that can imitate diverse demonstration trajectories and exploit diverse past experiences to indirectly drive exploration. They compare their approach with existing methods on various sparse-reward RL tasks and achieve superior performance on hard-exploration Atari games without using expert demonstrations or resetting to arbitrary states. The effectiveness of their method is demonstrated on multiple benchmarks.