Machine learning (ML) systems used for high-stakes decision-making often suffer from biases due to training on biased data, resulting in biased outcomes. The fairness community has focused on designing fair classifiers, but their robustness has been largely overlooked. Evaluating fairness on sampled datasets can be unreliable due to biased samples and noisy attributes, which are especially prevalent in the fairness domain. We examine the performance of an optimized preprocessing algorithm on ProPublica's COMPAS dataset and find that while it achieves fairness on the unweighted training set, it fails to do so on the weighted set. This motivates our goal to design a fair classifier that is robust to perturbations. We propose a min-max objective to find a classifier that is fair not only with respect to the training distribution but also for the weighted combinations of the training dataset. We develop an iterative algorithm based on online learning to converge to such a fair and robust solution. Experimental results on standard datasets demonstrate the trade-off between fairness robustness and accuracy of these classifiers.