Choosing the right inductive bias for a machine learning model is crucial for good generalization. Model selection, which involves identifying suitable inductive biases for a given dataset, can be facilitated by computing the marginal likelihood (ML) using Bayesian inference. However, computing the ML for complex models like neural networks is challenging. This paper explores the connection between the ML and the sum of predictive log likelihoods of datapoints conditioned on preceding data in the dataset, offering a family of estimators that depend on predictions sampled from the posterior. The proposed estimator family is studied in linear models, revealing its theoretical properties and potential interpretation as a measure of training speed. The utility of the estimator is demonstrated through empirical evaluations and its effectiveness in approximating the marginal likelihood of a model. The paper also explores the application of the estimator to deep neural networks (DNNs) and finds that it is predictive of both final test accuracy and the weight assigned to the model in a linear model combination trained with gradient descent, shedding light on the generalization performance of DNNs.