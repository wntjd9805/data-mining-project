In this paper, we address the problem of reasoning in knowledge graphs (KGs) using full first-order logic (FOL) queries. We propose a method called Beta Embedding (BETAE) that models both entities and queries as probabilistic distributions. We design neural logical operators to support all FOL operators, including negation. Our approach effectively captures the uncertainty of queries and achieves state-of-the-art performance in handling arbitrary conjunctive queries. Experimental results on standard KG datasets demonstrate the effectiveness and scalability of our model. The project website with data and code can be accessed at http://snap.stanford.edu/betae.