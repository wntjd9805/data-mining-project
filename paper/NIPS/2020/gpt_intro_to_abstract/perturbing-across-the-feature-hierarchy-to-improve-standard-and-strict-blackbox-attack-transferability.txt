The paper discusses the need for improvement in adversarial methods for deep neural networks (DNNs) in situations where access to the target model is restricted. The focus is on the blackbox transfer-based adversarial threat model, where adversarial examples are created using a substitute whitebox model and then used to attack the target blackbox model. The specific goal is targeted adversarial attacks, which aim to induce the target model to output a specific class. The paper proposes an improved method called the Feature Distribution Attack (FDA), which optimizes adversarial noise across the intermediate feature space of the whitebox model. Experimental results show that the FDA method achieves higher transferability and success rates compared to traditional methods. The generated noise also provides a useful prior direction for query-based attacking methods.