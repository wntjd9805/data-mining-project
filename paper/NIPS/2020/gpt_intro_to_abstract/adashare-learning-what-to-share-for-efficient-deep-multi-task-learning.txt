Multi-task learning (MTL) is a popular approach that aims to solve multiple related tasks simultaneously. It offers benefits such as reduced training and inference time, improved generalization, and higher prediction accuracy. However, a key challenge in MTL is determining the optimal sharing of parameters across tasks. Existing approaches rely on hand-designed architectures, which are not scalable for deep neural networks with numerous layers and a large number of tasks. Recent advancements have introduced soft-parameter sharing, where task-specific networks are combined with feature sharing/fusion. While this approach achieves reasonable accuracy, it is computationally and memory-intensive as the model size increases with the number of tasks. In this paper, we propose AdaShare, a novel and differentiable approach for efficient MTL. AdaShare uses a task-specific policy to selectively execute layers in a single multi-task network, effectively learning the sharing pattern. We employ Gumbel Softmax Sampling and standard back-propagation for joint learning of the policy and network parameters. Our approach outperforms state-of-the-art methods in terms of accuracy and parameter efficiency, demonstrating its effectiveness in various MTL benchmarks.