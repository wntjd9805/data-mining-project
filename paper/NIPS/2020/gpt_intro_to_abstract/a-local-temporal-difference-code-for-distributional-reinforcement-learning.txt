This paper introduces the concept of distributional RL algorithms, which go beyond traditional reinforcement learning by considering the entire distribution of future rewards instead of just their expected values. The authors propose a neurally plausible method called Expectile TD learning, which converges to the expectiles of the value distribution. However, the non-locality of distributional TD algorithms presents a challenge for implementing them in neural systems. To overcome this, the authors propose an ensemble of independent units that perform traditional and local TD backups, which can recover the value distribution. The ensemble has selectivities along three dimensions: reward magnitude, temporal discount factors, and explicit memory about past outcomes. The authors demonstrate that their code also allows for the computation of the temporal evolution of the immediate reward distribution and shows a connection to predictive representations.