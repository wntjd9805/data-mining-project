The introduction discusses the challenge of training large convolutional neural networks (CNNs) on resource-constrained edge devices, and the potential of federated learning (FL) to address this challenge. The paper proposes a framework called Group Knowledge Transfer (FedGKT) that combines the benefits of FL and model parallelism-based split learning (SL) to train large CNNs on edge devices efficiently. FedGKT reformulates FL as an alternating minimization approach and incorporates knowledge distillation to improve the performance of the server model. The framework is evaluated using various datasets and shows comparable accuracy to existing FL methods while significantly reducing the computational power and parameters required on edge devices.