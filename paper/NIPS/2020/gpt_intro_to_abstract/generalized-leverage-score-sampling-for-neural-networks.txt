The introduction presents the use of kernel methods, specifically kernel ridge regression, in machine learning problems. Despite its effectiveness, the computation of kernel ridge regression becomes costly with large datasets. To address this issue, the random Fourier features sampling method has been proposed, which approximates the kernel function. This paper follows the approach of leverage score sampling and generalizes it to a broader class of kernels. Additionally, this work introduces the concept of leverage score theory into neural network training, establishing an equivalence between training a regularized neural network and kernel ridge regression. The main contributions of this work include generalizing leverage score sampling, connecting it with neural network training, and proving the equivalence between training a regularized neural network and kernel ridge regression under different initialization methods.