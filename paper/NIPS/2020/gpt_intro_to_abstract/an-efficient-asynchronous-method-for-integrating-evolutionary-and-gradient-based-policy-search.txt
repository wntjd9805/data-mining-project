Reinforcement Learning (RL) algorithms, combined with deep learning, have shown impressive performance in various environments such as playing video games and solving continuous control tasks. On the other hand, Evolutionary methods, another popular policy search algorithm, have also yielded compatible results. These two branches of policy search algorithms have different properties in terms of sample efficiency and stability. While DRL is sample efficient but sensitive to hyperparameters, ES is relatively stable but requires more learning steps. Although they are often considered as competitive approaches, few studies have attempted to combine them. Recently, some works have introduced methods to integrate the useful gradient information from DRL into ES. In this paper, we propose a novel asynchronous framework that efficiently combines ES and DRL, along with effective asynchronous update schemes. We evaluate the proposed framework and update schemes on a continuous control benchmark, demonstrating superior performance and time efficiency compared to previous approaches. Our contributions include the proposal of the asynchronous framework and multiple asynchronous update methods, as well as showcasing the time and sample efficiency of our method.