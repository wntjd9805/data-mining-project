Recent research has highlighted the importance of robustness in machine learning models. Adversarial examples, which involve manipulating natural images with subtle perturbations to cause misclassifications, and out-of-distribution transforms, which involve common corruptions and perturbations in natural images, have exposed the lack of stability in these models. In our paper, we introduce the concept of boundary thickness as a measure of neural network robustness. Thick decision boundaries improve robustness, while thin decision boundaries lead to overfitting. We demonstrate that common methods used to improve robustness also increase boundary thickness. Additionally, we propose a novel training scheme called noisy mixup, which involves augmenting data with random noise, to improve robustness against image imperfections. Overall, boundary thickness is a reliable metric associated with model robustness, and ensuring a thick boundary can enhance robustness in various ways. Code for reproducing our results is available.