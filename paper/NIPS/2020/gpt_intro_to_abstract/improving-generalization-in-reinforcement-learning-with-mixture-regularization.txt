Deep Reinforcement Learning (RL) has shown remarkable progress in tackling challenging tasks such as board games, video games, and robotics control. However, deep RL agents struggle with poor generalization to unseen environments, which hinders their deployment in real applications. The limited diversity of training environments is a major cause of this generalization gap. Although previous approaches have attempted to address this issue through data augmentation techniques, they only perturb the observations within the state feature space, resulting in limited diversity and limited generalization performance. In this work, we propose mixreg, a method that trains RL agents on a mixture of observations from different training environments. Inspired by the success of mixup in supervised learning, mixreg generates augmented observations by combining two randomly sampled observations from the collected batch. This approach effectively increases the training data diversity and improves generalization performance. Additionally, mixreg imposes piece-wise linearity regularization to enhance the learned policy and value functions. We evaluate mixreg on the Procgen Benchmark and demonstrate its superiority over existing data augmentation techniques and regularization methods. Our work contributes to the understanding of the importance of training data diversity in RL generalization and provides a strong baseline for future studies.