The adversarial vulnerability of deep neural networks (DNNs) has been extensively studied, showing that attackers can generate imperceptible perturbations to deceive DNN models. This poses security threats to systems relying on DNNs. Existing attacks can be categorized into white-box and black-box settings, with black-box attacks relying on queries to the victim models. However, practical attacks under a no-box threat model, where the attacker has no access to the victim model or its parameters, have not been explored. In this paper, we investigate no-box attacks against DNNs on computer vision models. We propose using auto-encoders to learn discriminative features with limited data, and demonstrate that adversarial examples crafted using these models transfer effectively to various open-source victim models. Our experiments also show the effectiveness of the generated no-box adversarial examples, and highlight their intrinsic differences compared to white-box/black-box examples.