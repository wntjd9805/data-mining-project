Deep learning has achieved remarkable success in various machine learning tasks. However, conventional optimization and machine learning theory are unable to fully explain the success of deep learning due to the nonconvex and over-parameterized nature of neural networks. Recent research has focused on studying over-parameterized neural networks in the neural tangent kernel (NTK) regime, which characterizes the training dynamics of kernel regression. This analysis has shown fast convergence rates for over-parameterized neural networks trained with stochastic gradient descent, as well as good generalization error for wide enough neural networks. Despite these theoretical results, the NTK-based analysis has limitations and does not perfectly match empirical observations, particularly when the network weights deviate from their initialization or when regularizers are involved. In this paper, we address this by using a mean-field analysis to study the neural tangent kernel and demonstrate that two-layer neural networks trained with noisy gradient descent and weight decay can still exhibit similar training dynamics as kernel methods. We also establish generalization bounds for these networks, handling explicit regularization, and provide a proof technique for continuous distributions.