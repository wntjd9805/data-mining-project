Tuning regularization hyperparameters is crucial for achieving state-of-the-art performance in challenging datasets. However, the process of manually adapting these hyperparameters can be time-consuming and complex. In this paper, we propose an automatic approach to hyperparameter optimization by formulating it as a bilevel optimization problem. We introduce a novel architecture called ∆-STN, which addresses issues in training structured hypernetworks (STNs) and improves stability and convergence. We also linearize the best-response hypernetwork to yield an affine approximation, encouraging accurate approximation of the Jacobian. Our empirical evaluations demonstrate the superiority of ∆-STNs over baseline methods in various tasks.