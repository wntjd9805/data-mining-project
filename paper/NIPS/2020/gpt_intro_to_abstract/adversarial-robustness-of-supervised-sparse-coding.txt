With the increasing prevalence of machine learning applications, there is a growing concern about the vulnerability of deployed models to adversarial attacks. These attacks involve imperceptible perturbations to the input that can disrupt otherwise well-performing systems. Much research has been dedicated to devising defense strategies and sophisticated attacks. This paper focuses on understanding adversarial examples from a theoretical standpoint, particularly in the context of the supervised sparse coding model. The authors explore the interplay between the expressivity and stability of the representation map and the notion of margin in the feature space. They provide a robustness certificate and a generalization bound for this model, considering assumptions that are milder and more easily quantifiable from data. The analysis of the Lipschitz continuity of the adversarial loss in the presence of these assumptions is particularly challenging. The authors also extend their results to other supervised learning problems and provide a robustness certificate for multiclass classification.