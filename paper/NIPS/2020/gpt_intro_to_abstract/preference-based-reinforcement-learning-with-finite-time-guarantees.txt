Preferences play a crucial role in reinforcement learning (RL) when numerical reward values are not readily available or difficult to design. Preference-based Reinforcement Learning (PbRL) is a framework that solves RL problems using preferences, but its theoretical understanding is still largely open. This paper proposes an efficient algorithm for PbRL with finite-time guarantees, addressing the limitations of previous methods. The algorithm combines synthetic reward functions, reward-free exploration, and dueling bandit algorithms to perform policy search. Experimental results demonstrate the superiority of the proposed algorithm over previous baselines in simulated environments.