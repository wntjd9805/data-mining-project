Deep networks have played a crucial role in advancing machine learning tasks, but their effectiveness relies heavily on supervised learning with large labeled datasets. Self-supervised learning (SSL) aims to address this limitation by utilizing unlabeled data to define surrogate tasks for network training. Contrastive learning (CL) is a specific SSL technique that treats instances as classes and seeks to learn an invariant instance representation by generating pairs of examples and training an encoder with a contrastive loss. While the design of positive pairs in CL has been extensively explored, the design of negative pairs has received less attention. In this paper, we propose a general algorithm for generating diverse positive and challenging negative pairs by leveraging adversarial examples. We demonstrate that adversarial data augmentation improves SSL performance and introduce a novel procedure called Contrastive Learning with Adversarial Examples (CLAE) to train SSL models. Our experiments show that CLAE enhances the performance of several CL baselines across different datasets.