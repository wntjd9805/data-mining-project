Deep learning often requires a large amount of labeled data, but semi-supervised learning (SSL) is a promising approach that uses unlabeled data to mitigate this weakness. Consistency training, a method within SSL, regularizes model predictions to be robust to small changes in input examples or hidden states. This paper investigates the role of noise injection in consistency training and proposes substituting traditional noise injection methods with high-quality data augmentation methods, referred to as Unsupervised Data Augmentation (UDA), to improve consistency training. UDA is evaluated on various language and vision tasks, demonstrating significant improvements over state-of-the-art models, outperforming existing semi-supervised learning methods, and achieving high accuracy even with limited labeled data. The paper also analyzes the theoretical foundations of UDA and its compatibility with transfer learning.