Deep convolutional neural networks (CNNs) have achieved great success in various visual tasks, but deploying these networks on mobile devices requires adjusting the depth, width, and image resolution to reduce memory and latency. While existing works have focused on scaling only one of these dimensions, this paper explores a compound scaling method for designing tiny networks based on the baseline EfÔ¨ÅcientNet-B0. Through experiments and observations, the authors find that resolution and depth are more important than width for retaining performance in smaller architectures. They propose a tiny formula for optimizing the three dimensions, leveraging Gaussian process regression on frontier models. The resulting tiny networks, called TinyNets, achieve high accuracy with significantly fewer FLOPs compared to traditional networks. The proposed formula can be applied to other architectures like ResNet to generate small yet effective networks. This work contributes to the study of simultaneously twisting resolution, depth, and width in neural networks to enable efficient deployment on mobile devices.