In this paper, we address the challenge of training control policies on high-dimensional, continuous systems using policy gradient (PG) methods. While PG methods have shown success in this domain, they require extensive interaction with the world, which can be costly in some cases. To tackle this issue, we propose a novel framework for lifelong reinforcement learning (RL) that leverages prior experience during the training process of each task. Our algorithm, lifelong PG: faster training without forgetting (LPG-FTW), learns a shared repository of knowledge and task-specific mappings, resulting in high-performing policies with less experience compared to independent task learning. We also demonstrate theoretical guarantees for the convergence of LPG-FTW to a specific approximation of the multi-task objective.