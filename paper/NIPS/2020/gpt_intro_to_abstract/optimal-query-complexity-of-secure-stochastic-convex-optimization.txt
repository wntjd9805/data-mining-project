Optimization is a crucial tool in decision making and machine learning, but the iterative nature of modern optimization techniques introduces the risk of information leak. In this paper, we focus on the secure stochastic convex optimization problem, where the goal is to optimize accuracy while maintaining privacy. We define accuracy and privacy using PAC notions and aim to characterize the trade-offs between the two using query complexity. Our main results include the lower and upper bounds of query complexity for secure stochastic convex optimization, with logarithmic factors compressed in the bounds. We show matching upper and lower bounds for both function error and point error measures. Our bounds provide a complexity price for securing privacy compared to non-secure bounds. We also contribute a general lower bound template based on information-theoretical analysis and present a secure learning protocol that guarantees privacy.