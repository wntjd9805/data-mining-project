Pretrained language models like BERT have become popular for various NLP tasks but face limitations when dealing with long texts. Sliding window solutions sacrifice the ability for distant tokens to interact, hindering performance in complex tasks. This paper introduces the CogLTX framework, inspired by human working memory, which reasons over concatenations of key sentences. The framework utilizes a process called MemRecall to identify relevant text blocks and introduces a judge BERT model to score block relevance. Experimental results show that CogLTX outperforms or achieves comparable performance to state-of-the-art models on multiple tasks, with constant memory consumption regardless of text length.