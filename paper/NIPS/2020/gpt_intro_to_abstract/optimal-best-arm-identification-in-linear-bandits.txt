The stochastic linear bandit is a sequential decision-making problem that extends the classical stochastic Multi-Armed Bandit problem by assuming that the average reward is a linear function of the arm. While regret minimization in stochastic linear bandits has been extensively studied, the pure exploration problem (best-arm identification) with a finite set of arms has received less attention. Existing algorithms for pure exploration exhibit scalability issues and lack simplicity and practicality. In this paper, we present a new algorithm for best-arm identification in linear bandits with a finite set of arms that achieves the information-theoretical lower bound for sample complexity. We also study the pure exploration problem in linear bandits with a continuous set of arms and propose an algorithm that matches the lower bound order-wise.