In this paper, we propose a solution to the challenges of content control and catastrophic forgetting in deep learning architectures for natural language generation. We define content control as the ability to command the network to include or exclude specific words, phrases, topics, styles, or sentiments in its output. We also introduce an alternative to fine-tuning neural language models that avoids the high cost and limited generalizability associated with overwriting model weights. Instead, we propose a framework for combining separate models of natural language and high-level command responses to produce desired linguistic output. This framework allows us to interpret and control the hidden activations of a pretrained neural network without modifying the pretrained model. Our approach is biologically inspired by the inhibitory interactions observed in human neural pathways and can be applied to other neural network architectures and domains beyond text generation.