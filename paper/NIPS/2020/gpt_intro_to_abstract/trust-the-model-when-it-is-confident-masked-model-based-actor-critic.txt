Deep reinforcement learning (RL) has demonstrated significant accomplishments in solving complex decision-making problems. However, current model-free RL methods often require a large number of interactions with the environment to learn a good policy. On the other hand, model-based RL (MBRL) enhances sample efficiency by using a fitted model to approximate the true dynamics of the environment. While MBRL has shown promise in limited interaction scenarios, the challenge lies in determining when to trust the model, especially in settings with nonlinear and noisy dynamics. Existing deep MBRL methods tend to perform poorly in low-data regimes, long-horizon model rollouts, and complex, noisy dynamics. In this paper, we propose the Masked Model-based Actor-Critic (M2AC) algorithm, which reduces the influence of model errors through a masking mechanism. We derive theoretical results that provide bounds on the discrepancy between the true return and the masked model rollout value, motivating the design of M2AC. Experimental results demonstrate that M2AC achieves high sample efficiency and robustness across various tasks in continuous control.