Reinforcement learning (RL) is a framework for sequential decision-making under uncertainty. In RL, an agent interacts with an environment by taking actions, receiving rewards, and transitioning to new states. The goal of the agent is to maximize its total reward in the long run. Dealing with the unknown rewards and transition probabilities is a challenge in RL, and optimistic reinforcement learning algorithms have been successful in balancing the exploration versus exploitation trade-off. Optimistic algorithms maintain plausible models of the world and select actions to maximize returns. This paper introduces a new framework for studying optimistic algorithms, demonstrating a strong connection between model-optimistic and value-optimistic approaches. The framework can be applied to various RL algorithms and extends to incorporate linear function approximation.