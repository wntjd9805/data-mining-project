This paper introduces the use of attention mechanisms in recurrent neural networks (RNNs) to capture and process long-term dependencies in sequential data. While traditional RNNs struggle with large timescales due to unstable information propagation, attention mechanisms provide a way to dynamically access past states and inputs, improving learning and computation over long sequences. However, the understanding of gradient scaling properties in the presence of attention is currently limited, and attending over long sequences can be computationally expensive. To address these issues, the authors propose a formal analysis of gradient propagation in self-attentive systems and present a family of screening mechanisms to reduce complexity and memory usage while maintaining good gradient propagation. Numerical experiments are conducted to demonstrate the effectiveness of this approach. The paper provides guarantees for attention mechanism development and offers insights into the trade-offs between recurrence and attention.