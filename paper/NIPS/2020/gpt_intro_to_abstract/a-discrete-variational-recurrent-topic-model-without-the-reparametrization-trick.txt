This paper explores the use of neural variational inference (NVI) and recurrent topic modeling for capturing both syntactic and semantic/thematic word dynamics in natural language processing (NLP). While short-term memory architectures have improved syntactic dependencies, capturing thematic dependencies remains challenging. This paper presents a model that maintains word-level, discrete topic assignments without requiring reparametrization. The authors analyze the model and demonstrate the benefits of their modeling decisions. They also highlight the importance of priors and probabilistic models in neural methods. The paper concludes with the availability of code, scripts, and models for further exploration.