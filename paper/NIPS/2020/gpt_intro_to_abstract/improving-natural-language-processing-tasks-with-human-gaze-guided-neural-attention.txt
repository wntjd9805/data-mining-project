Neural attention mechanisms have been successfully used in computer vision tasks, allowing neural networks to focus only on relevant information. The addition of human gaze information has proven to be beneficial in various cases, but its integration into natural language processing (NLP) tasks has been under-explored due to data scarcity. To overcome this issue, we propose a hybrid text saliency model (TSM) that combines a cognitive model of reading behavior with human gaze supervision. We pre-train a BiLSTM network with a Transformer using synthetic training examples generated by the E-Z Reader model, and then refine the weights using a small amount of human gaze data. Additionally, we propose a joint modeling approach that integrates TSM predictions into an attention layer, allowing for flexible adaptation to different NLP tasks. Our approach outperforms the state of the art in paraphrase generation and sentence compression, demonstrating the potential of combining cognitive and data-driven models for gaze integration in NLP tasks.