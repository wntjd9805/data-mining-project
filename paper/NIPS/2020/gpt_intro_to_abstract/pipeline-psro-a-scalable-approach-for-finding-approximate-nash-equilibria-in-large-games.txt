A long-standing goal in artificial intelligence and algorithmic game theory is to develop a general algorithm for finding approximate Nash equilibria in large imperfect-information two-player zero-sum games. While previous methods, such as AlphaStar and OpenAI Five, have demonstrated expert-level performance in large imperfect-information video games, they are not game-theoretically principled and do not guarantee convergence to an approximate Nash equilibrium. In this paper, we introduce Policy Space Response Oracles (PSRO), a game-theoretic reinforcement learning algorithm based on the Double Oracle algorithm that guarantees convergence to an approximate Nash equilibrium. However, PSRO may not scale well to large games due to its sequential nature. Two existing parallelization approaches, Deep Cognitive Hierarchies (DCH) and Rectified PSRO, have limitations and fail to converge reliably in random normal form games. To address these shortcomings, we propose Pipeline PSRO (P2SRO), a scalable PSRO-based method that maintains a hierarchical pipeline of reinforcement learning workers. P2SRO achieves convergence guarantees and outperforms existing methods in various imperfect information games. Additionally, we introduce an open-source environment for Barrage Stratego and demonstrate the state-of-the-art performance of P2SRO on this game.