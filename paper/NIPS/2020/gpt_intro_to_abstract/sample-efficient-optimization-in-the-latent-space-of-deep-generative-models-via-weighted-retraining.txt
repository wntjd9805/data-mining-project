This paper presents a two-stage approach called latent space optimization (LSO) that addresses the challenges of optimizing objective functions over high-dimensional and structured input spaces, as well as expensive function evaluations. In the first stage, a generative model is trained to map low-dimensional continuous space to the input space, constructing a continuous analog of the optimization problem. In the second stage, the objective function is optimized over this learned latent space using a surrogate model. The authors identify two decoupling issues in previous LSO methods and propose data weighting and periodic retraining of the generative model as solutions. Their empirical results demonstrate the effectiveness of weighted retraining in improving LSO's performance across various domains and generative models.