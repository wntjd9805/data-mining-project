In this paper, we present a novel knowledge distillation method called task-oriented feature distillation (TOFD) to address the issue of explosive growth of model parameters and computation in deep neural networks. TOFD aims to distill the task-oriented information from teachers to students by utilizing auxiliary classifiers to capture the task-oriented information from all features. We also introduce an orthogonal loss to prevent the loss of important information in the feature resizing layers. Extensive experiments on various neural networks and datasets demonstrate the superior performance of TOFD compared to state-of-the-art distillation methods, with significant accuracy improvements observed. This work contributes to the advancement of efficient knowledge distillation techniques.