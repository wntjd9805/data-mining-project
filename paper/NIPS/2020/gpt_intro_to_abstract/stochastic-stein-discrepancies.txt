Markov chain Monte Carlo (MCMC) methods are commonly used in Bayesian inference and probabilistic inference to estimate complex integrals. However, these methods can be computationally burdensome due to the need to cycle through large datasets or factors. To address this, scalable approximate MCMC methods have been developed, which use small subsamples of data to generate new sample points. While these methods reduce computational time, they introduce biases that affect the accuracy of the estimates. To assess the quality of these approximate MCMC outputs, computable Stein discrepancies (SDs) have been developed. These discrepancies quantify the discrepancy between sample and target expectations and track sample convergence to the target. In this paper, the authors propose stochastic Stein discrepancies (SSDs), which are based on subsampling data points. They prove that SSDs inherit the convergence-tracking properties of standard SDs. The authors also apply SSDs to analyze a scalable stochastic variant of the Stein variational gradient descent (SVGD) algorithm for particle-based variational inference. The results show that SSDs deliver accurate inferences with significantly fewer datapoint accesses compared to standard SDs.