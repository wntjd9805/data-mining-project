The backpropagation algorithm (BP) is widely used in deep learning. However, it has limitations including non-local weight updates, making parallelization difficult, and a problematic biological implementation. To address these issues, alternative training algorithms have been developed but are not widely adopted due to limited task performance and disappointing results under synaptic asymmetry. In this paper, we introduce Direct Feedback Alignment (DFA) and demonstrate its applicability in state-of-the-art settings across various tasks. Our results set new standards for learning without weight transport and show that challenging tasks can be effectively tackled under synaptic asymmetry.