Reinforcement learning (RL) has been successful in learning complex behaviors without human supervision, but its application to practically useful environments is limited by the need for a well-shaped reward function. Sparse reward environments, where agents receive rewards only upon task completion, provide a more general approach but pose challenges for training. Model-based RL has gained interest for its sample efficiency and use of learned models for planning actions. However, current model-based approaches require dense reward signals. In this paper, we propose PlanGAN, a model-based algorithm designed for sparse-reward, multi-goal environments. We utilize an ensemble of Generative Adversarial Networks (GANs) to generate plausible trajectories and develop a novel planning algorithm. Experimental results demonstrate that PlanGAN achieves comparable performance to leading model-free methods with improved sample efficiency. This paper contributes the first model-based method explicitly designed for multi-goal, sparse reward environments.