We introduce a class of implicit neural representations that parameterize functions satisfying a set of equations. These representations are defined by a functional that takes spatial coordinates as input and can handle derivatives of the function. Our goal is to learn a neural network that can map these coordinates to a desired quantity while satisfying the defined constraint. We demonstrate the wide applicability of these representations across various scientific fields, such as image and audio processing, 3D shape representation, and solving boundary value problems. We highlight the benefits of continuous parameterization, such as improved memory efficiency and the ability to compute gradients and higher-order derivatives analytically. Existing architectures, such as ReLU-based multilayer perceptrons, lack the capability to represent fine details and derivatives accurately, motivating the use of MLPs with periodic activation functions. Our contributions include a continuous implicit neural representation using periodic activation functions, an initialization scheme, and applications in various domains.