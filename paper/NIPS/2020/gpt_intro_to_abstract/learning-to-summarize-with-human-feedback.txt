Large-scale language model pretraining has become common in natural language processing tasks. However, a misalignment exists between the objective of maximizing the log probability of human-written text during fine-tuning and the goal of generating high-quality outputs. This misalignment is attributed to various factors such as the inability to differentiate important and unimportant errors, incentivizing low-quality demonstrations, and degradation of performance due to distributional shift during sampling. To address these issues, this paper focuses on training language models for abstractive English text summarization using human feedback. The authors propose a method that combines reward learning and reinforcement learning to train the models. The results demonstrate that training with human feedback outperforms strong baselines, generalizes well to new domains, and produces high-quality summaries. The authors also release a human feedback dataset for further research. Ultimately, these methods aim to overcome misalignment issues and improve the behavior of AI systems in alignment with human expectations and tasks.