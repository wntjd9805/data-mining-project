Classical supervised learning systems are designed to learn from a single set of classes, while humans acquire knowledge by continually building upon previous knowledge when learning new tasks. This disparity has led to the development of continual learning systems, which consume a sequence of tasks with their own sets of classes. However, state-of-the-art continual learning systems often suffer from catastrophic forgetting, where knowledge from previous tasks is lost when learning new ones. While research in this area has primarily focused on feed-forward neural networks, little attention has been given to continual learning of recurrent networks. This paper explores the application of continual learning to captioning tasks using recurrent networks, specifically LSTM networks. The paper also introduces a new benchmark for continual image captioning and proposes modifications to weight-regularization and knowledge distillation approaches for their application to recurrent LSTM networks and transient task vocabularies. Experimental evaluation and comparisons with baselines demonstrate the effectiveness of the proposed approach.