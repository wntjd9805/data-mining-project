This paper investigates the possibility of pretraining spatiotemporal models for action recognition using unlabeled videos by capturing cross-modal information from audio and video. The authors argue that the cost of manually labeling large video datasets is prohibitive, and the selection of suitable label spaces for action recognition is still a matter of debate. They propose three approaches that leverage the complementary nature of audio and video to perform cross-modal prediction: Multi-Head Deep Clustering (MDC), Concatenation Deep Clustering (CDC), and Cross-Modal Deep Clustering (XDC). Experimental results show that all three cross-modal methods yield representations that generalize better to downstream tasks compared to within-modality approaches. XDC, in particular, outperforms the other methods and demonstrates that self-supervised video representation learning can surpass fully-supervised pretraining for action recognition.