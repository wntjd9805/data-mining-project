Graph neural networks (GNNs) have shown promising results on various graph data applications but are susceptible to adversarial attacks. However, existing attack setups do not reflect real-world scenarios, where attackers have limited access to nodes and lack knowledge about the underlying model. In this paper, we propose a new type of black-box adversarial attack for node classification tasks with restricted access, where attackers can only manipulate a subset of nodes. We focus on the node selection strategy and exploit the structural inductive biases of GNNs as an effective source of attack information. By generalizing the gradient-norm into a model-independent importance score, we demonstrate that attacking nodes with high importance scores significantly increases classification loss. We also propose a greedy correction procedure for calculating importance scores and show the effectiveness of our method on three benchmark datasets. Overall, our contributions include the proposal of a new black-box attack setup, the exploitation of GNN structural biases for attacks, the analysis of discrepancy between loss and mis-classification rate, and the empirical validation of our method.