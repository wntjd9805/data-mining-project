Regret minimization is essential for online learning and decision-making in uncertain and unpredictable environments. Follow the regularized leader (FTRL) is a widely studied class of no-regret algorithms that adapt optimally to unpredictable environments. In multi-agent environments, FTRL-based learning processes can achieve stronger regret guarantees as the sequence of play becomes more predictable. However, there is a disconnect between the game-theoretic solution concept of Nash equilibrium and the convergence of no-regret dynamics. This paper explores the dynamics of FTRL and their implications for convergence to Nash equilibrium, establishing a dichotomy between the treatment of mixed and pure Nash equilibria. The analysis reveals that no interior Nash equilibrium can be asymptotically stable under FTRL dynamics, and only strict Nash equilibria survive in the long run. The paper also examines the boundary behavior of different classes of FTRL dynamics and shows that entropy-like regularizers lead to asymptotically stable sets that contain at least one pure strategy profile.