Temporal data plays a crucial role in many machine learning applications, such as electronic health records and financial transaction ledgers. However, handling variable-number events in continuous time poses a challenge. The combination of temporal point processes (TPP) and recurrent neural networks (RNN) has shown promise in addressing this issue, but suffers from limited parallelization. In this paper, we propose a novel approach using triangular maps and normalizing flows to design flexible TPP models without relying on RNNs. Our contributions include a new parametrization for classic TPPs, the introduction of TriTPP as a non-recurrent TPP class that enables faster sampling, and a differentiable relaxation for non-differentiable TPP loss functions. These advancements facilitate efficient parallel likelihood computation and sampling, and offer a new variational inference scheme for Markov jump processes.