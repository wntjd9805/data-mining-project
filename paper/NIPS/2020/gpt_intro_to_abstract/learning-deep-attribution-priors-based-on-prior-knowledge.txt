Recent advances in machine learning have led to the development of complex models that are difficult for humans to interpret. To address this issue, there has been a recent focus on model interpretability, particularly in the area of feature attribution methods. These methods assign a numeric value to each input feature to indicate its importance for a given prediction. However, it remains a challenge to understand why a feature was assigned a specific attribution value. Previous work has attempted to address this problem by regularizing model training, but these methods require domain-specific knowledge and lack human interpretability. Additionally, overparameterization of deep learning models has hindered their adoption in domains with limited data. In this paper, we propose the deep attribution prior (DAPr) framework, which trains deep neural networks to learn the relationship between meta-features (sets of information about each feature) and global feature importance values. This framework aims to achieve greater model accuracy and interpretability by biasing the prediction model to focus on more relevant features. We apply our method to synthetic and real-world biological datasets, showing improved performance in tasks with limited training data and increased insight into deep model behavior.