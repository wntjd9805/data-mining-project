The expressive power of feedforward neural networks has been extensively studied, with a focus on the tradeoffs between depth and width. Deeper networks tend to perform better due to their compositional expressibility. While depth separation has been shown between depth 2 and 3, it remains an open problem for any constant greater than 2. Similarly, in the context of threshold circuits, depth separation between depth 3 and some constant is also an open problem. This paper explores the relationship between depth separation in neural networks and threshold circuits, showing that a depth-separation result in neural networks would imply depth separation in threshold circuits. The authors also investigate the impact of weight magnitude on the expressiveness of neural networks, demonstrating that networks with bounded weights can approximate functions expressed by networks with arbitrarily large weights. Overall, this work provides insights into the limitations and possibilities of depth separation and weight magnitude for neural networks.