Depth is a significant factor in the expressive power of neural networks, but it often leads to a slowdown in learning due to unstable gradient norms. Randomly initialized layer weights with i.i.d. entries from a zero-mean distribution are commonly used, but they cannot avoid spectral instabilities as the network depth increases. Recent advancements in neural architectures, such as residual connections and batch normalization, have allowed for the training of deep networks with standard initialization schemes. This paper aims to bridge the gap between these observations by studying the effect of architectural enhancements on the spectral properties of randomly initialized networks and their impact on gradient-based optimization algorithms. The focus is particularly on the rank of hidden layer activations and its preservation in batch-normalized networks. The study demonstrates that batch normalization stabilizes the rank, allowing for the training of deep networks, while un-normalized networks experience rank collapse and lose information about the input. Theoretical proofs and empirical evidence support the importance of rank in gradient-based learning.