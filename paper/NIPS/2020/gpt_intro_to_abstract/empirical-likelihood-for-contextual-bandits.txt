Contextual Bandits are widely used in practical applications, but off-policy evaluation for estimating the value of any policy requires more data than supervised learning. This paper explores the use of empirical likelihood, a nonparametric maximum likelihood approach, to find tight confidence intervals on counterfactual estimates. The authors introduce a computationally tractable estimator that has low bias and experimentally demonstrate its performance exceeding popular alternatives. The paper also describes an asymptotically exact confidence interval that is narrow and empirically approaches nominal coverage. Furthermore, the authors use this confidence interval to construct a robust counterfactual learning objective and highlight several innovations in their approach.