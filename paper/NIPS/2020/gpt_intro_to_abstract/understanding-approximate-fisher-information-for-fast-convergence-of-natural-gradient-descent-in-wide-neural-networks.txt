Natural gradient descent (NGD) is a method used to improve the convergence speed of the gradient method. However, the computational cost of computing the inverse of the Fisher information matrix (FIM) in NGD is high. To address this issue, various approximation methods have been proposed that reduce the computational cost, allowing NGD to be used in large-scale models with many parameters, such as deep neural networks (DNNs). These approximations, such as layer-wise block diagonal approximations and unit-wise approximations, have been shown empirically to improve convergence compared to conventional first-order gradient descent. However, there is a lack of theoretical evidence on the convergence of approximate NGD. In this paper, we extend the analysis of gradient descent dynamics in infinitely-wide DNNs to NGD and examine the dynamics of NGD with approximate FIMs. Surprisingly, we find that these approximations achieve the same fast convergence as NGD with the exact FIM. We also demonstrate that the dynamics of approximate NGD methods are the same in the function space but differ in the parameter space and lead to different global minima. Additionally, our experiments show that the predictions of models trained using approximate NGD are comparable to those of exact NGD. This paper provides a systematic understanding of NGD with approximate Fisher information for deep learning.