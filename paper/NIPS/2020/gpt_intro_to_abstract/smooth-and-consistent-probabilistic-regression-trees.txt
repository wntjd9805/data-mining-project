Classification and regression trees (CART) and ensemble methods based on them, such as Random Forests and Gradient Boosted Trees, have been widely utilized in regression problems and machine learning competitions. However, standard decision/regression trees may struggle to adapt to the smoothness of the link functions and noise in input data. In this study, we propose Probabilistic Regression trees (PR trees) that address these issues by incorporating smooth predictions based on probability functions. These trees can naturally adapt to noisy input, preserve interpretability, and exhibit consistency. We also extend PR trees to Random Forests and Gradient Boosted Trees, demonstrating their benefits in terms of performance, interpretability, and robustness to noise. Although these additional properties come at a computational cost, we provide insights into mitigating such costs. Section 2 discusses related work, followed by the introduction of PR trees and their inference in Section 3. The consistency of PR trees is presented in Section 3, and experimental results on PR trees, as well as their extension to Random Forests and Gradient Boosted Trees, are discussed in Section 4. Finally, the paper concludes in Section 5.