Graph neural networks (GNNs) have become popular for obtaining meaningful node representations in graph-structured data. These models aggregate information from neighboring nodes, encoding graph-relational information into the representation and achieving good performance on various tasks. However, existing GNN structures have limitations in capturing long-range dependencies. To address this, recurrent GNNs have been proposed, which iteratively perform graph convolutional aggregation until convergence. This paper introduces the Implicit Graph Neural Network (IGNN) framework to improve the evaluation and training of recurrent GNNs. The framework is based on rigorous mathematical analysis and incorporates a novel projected gradient method for efficient training. Additionally, IGNN is extended to heterogeneous network settings. Comparative evaluations demonstrate that IGNN effectively captures long-range dependencies and outperforms state-of-the-art GNN models across a wide range of tasks. The paper provides an overview of related work, discusses the IGNN framework, and presents empirical comparisons and results.