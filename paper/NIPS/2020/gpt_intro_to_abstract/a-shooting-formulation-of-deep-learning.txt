Deep neural networks (DNNs) and optimal control (OC) are closely related, with the control variable corresponding to the parameters of the DNN. This requires a definition of a control cost, or a norm on the control variable. In this paper, we explore the implications of such a control cost in the context of DNN parameterization, focusing on continuous formulations inspired by neural ordinary differential equations (ODEs). We investigate a time-varying parameterization approach using regularization to ensure well-posed estimation and regularity of the resulting flow. We propose a shooting formulation for DNNs, optimizing over the initial conditions of critical networks instead of directly optimizing over the set of time-dependent parameters. We introduce the Hamiltonian particle-ensemble parameterization, a finite set of particles representing the space of optimal network parameters. Our contributions include the UpDown model, explicit shooting equations, and the demonstration of good performance on prediction tasks.