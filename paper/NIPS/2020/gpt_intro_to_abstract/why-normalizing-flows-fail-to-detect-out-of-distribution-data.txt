Normalizing flows have been considered promising for out-of-distribution (OOD) detection due to their simplicity and ability to provide exact likelihoods. However, it has been observed that these flows often assign higher likelihood to OOD data compared to the data used for training. The reasons for this failure in OOD detection are not fully understood. In this paper, we investigate the inductive biases of flow models and how they hinder OOD detection. Our findings include: flows primarily learn based on local pixel correlations rather than semantic content, which makes it difficult to detect data with anomalous semantics; flows can increase likelihood for all structured images simultaneously through specific mechanisms; and by modifying the architectural details, flows can learn transformations specific to the target data and improve OOD detection. We also demonstrate that training flows on high-level features extracted from image datasets containing semantic information can improve OOD detection. Code for our experiments can be found on our GitHub repository.