In this paper, we address the problem of saddle point optimization where the objective function is convex-concave and L-smooth, and the feasible sets are convex and compact. We also consider the stochastic minimax problem in which the objective is sampled from a finite set of random variables. We focus on cases where the feasible set is complex and projecting onto it is computationally expensive. We introduce a projection-free algorithm called Mirror-Prox Conditional Gradient Sliding (MPCGS) that leverages the idea of proximal point iterations and combines it with Mirror-Prox and conditional gradient sliding techniques. Our algorithm requires a minimal number of gradient evaluations and linear optimizations to guarantee suboptimality error. We extend this algorithm to the stochastic setting with Mirror-Prox Stochastic Conditional Gradient Sliding (MPSCGS), making it the first projection-free algorithm for convex-strongly-concave saddle point problems in the stochastic setting. We conduct experiments to validate our theoretical analysis and demonstrate the superiority of our methods over existing projection-free and projection-based methods for complex feasible sets.