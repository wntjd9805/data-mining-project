The control of uncertain dynamical systems in Reinforcement Learning (RL) and continuous control has been a challenging task. While there have been successes in sequential decision making tasks, the focus on provably correct methods in both learning and approximation in unknown environments is relatively new. This paper introduces a kernelized nonlinear regulator (KNR) model for the online nonlinear control problem, where the objective is to optimize a sequence of cost functions without prior knowledge of the dynamics. The authors propose the Lower ConÔ¨Ådence-based Continuous Control (LC3) algorithm, which achieves a O(T) regret bound. The algorithm does not have explicit dependencies on the dimension of the system dynamics and provides improved rates for small losses. The paper also presents empirical evaluations and addresses technical challenges related to the extension to Reinforcement Learning.