Reinforcement learning (RL) faces the challenge of policy evaluation, particularly in off-policy evaluation (OPE) scenarios where deploying policies in real environments is costly or risky. To estimate the value of a target policy using a dataset of experience gathered by other policies, behavior-agnostic OPE methods have been developed. This paper focuses on the "DICE" family of distribution correction estimators which model the ratio between the propensity of the target policy and the likelihood of appearing in the logged data. Although these estimators have different derivations, they share a similar minimax optimization structure. The authors establish that the previous DICE formulations are equivalent to regularized Lagrangians of the same linear program (LP), with a focus on the dual form (d-LP) for off-policy evaluation. They explore key choices in translating the d-LP into a stable minimax optimization problem and analyze the consequences of these choices both theoretically and empirically. The results provide valuable insights for improving OPE methods.