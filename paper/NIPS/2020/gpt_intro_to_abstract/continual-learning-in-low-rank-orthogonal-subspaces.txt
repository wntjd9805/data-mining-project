This paper focuses on the unique challenges posed by the sequential arrival of multiple tasks in continual learning. The main challenge is catastrophic forgetting, where the model forgets previously acquired knowledge when updating parameters for the current task. Existing approaches in neural networks address this issue through regularization-based approaches, modular approaches, and memory-based approaches. However, these approaches still result in some level of forgetting. Therefore, this paper proposes a new approach called ORTHOG-SUBSPACE, which learns different tasks in different vector subspaces, ensuring orthogonality between subspaces to prevent interference. This approach significantly reduces forgetting in shallower networks and improves performance in deeper networks compared to existing baselines.