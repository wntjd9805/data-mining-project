This paper introduces ISTA-NAS, a methodology for neural architecture search (NAS) that formulates the problem as a sparse coding problem. The authors propose a compressed search space where each point represents a sparse solution and perform a gradient-based search with inherent sparsity constraint. They alternate between differentiable search and architecture recovery, resulting in a sparse and efficient network for training. Furthermore, they develop a one-stage framework that aligns the search and evaluation phases under the target-net settings. Experimental results demonstrate the effectiveness and efficiency of ISTA-NAS, achieving state-of-the-art performances in a single run on CIFAR-10 and ImageNet datasets.