Understanding human behaviors and creating virtual humans that act like real people has been a challenging goal in computer vision and graphics. Human motion synthesis plays a crucial role in achieving this goal and has applications in animation, gaming, and virtual reality. While data-driven approaches have made significant progress in producing realistic motions, physics-based methods face challenges in imitating highly agile motions and long-term motions. In this paper, we propose a Residual Force Control (RFC) approach that introduces external forces to compensate for the dynamics mismatch between the humanoid model and real humans. We also present a dual-policy control framework for synthesizing multi-modal long-term human motions without task guidance or user input. Experimental results demonstrate that our approach outperforms state-of-the-art methods in terms of learning speed and motion quality, and is capable of imitating highly agile motions like ballet dance. Additionally, our approach successfully learns from a large-scale human motion dataset and generates diverse long-term motions.