Deep Neural Networks (DNNs) are widely used in machine learning applications, but optimizing them is a non-convex problem that often leads to finding local minima using gradient methods. However, different minima can have varying properties, and finding a specific type or class of minimum is important in many cases. This paper introduces Ridge Rider (RR), a method that aims to find diverse minima by iteratively following orthogonal directions of negative curvature from a saddle point. RR is less greedy than standard SGD methods and can be used in various situations to find diverse solutions. The paper presents theoretical guarantees and showcases the effectiveness of RR in different machine learning problems.