Deep neural networks have been highly successful in approximating complex functions, with grid-structured convolutional and chain-structured recurrent networks being particularly effective. However, these networks primarily capture pairwise dependencies between variables. In this paper, we introduce the concept of factor graph neural networks (FGNN) that extend the capabilities of graph neural networks by capturing dependencies over multiple variables using factor graph structures. Factor graphs provide a natural way to specify and learn dependencies among multiple variables, and FGNNs can be used to learn both the inference algorithm and latent variables simultaneously. We demonstrate that FGNNs can exactly parameterize the Max-Product Belief Propagation algorithm and showcase their practical potential through experiments on various inference problems.