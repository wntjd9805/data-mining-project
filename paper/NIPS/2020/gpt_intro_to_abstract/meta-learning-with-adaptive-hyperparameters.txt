Few-shot learning is a subfield of artificial intelligence that aims to train AI models to generalize well with only a few training examples. Meta-learning, or learning-to-learn, is a technique that investigates prior knowledge from previous tasks to facilitate rapid learning of new tasks. Gradient-based meta-learning algorithms, such as model-agnostic meta-learning (MAML), have shown promise in enabling fast adaptation to new tasks with few updates. However, the training strategy for fast adaptation is often overlooked, with most approaches relying on conventional optimization algorithms. In this paper, we propose a novel approach called Adaptive Learning of hyperparameters for Fast Adaptation (ALFA) that focuses on the inner-loop optimization process. Instead of solely optimizing the initialization, ALFA dynamically generates learning rates and weight decay coefficients for each inner-loop update step, resulting in better training and generalization compared to conventional approaches. Through experiments, we demonstrate that ALFA achieves improved few-shot classification accuracy, even from a random initialization. Additionally, ALFA can be combined with existing meta-learning approaches to further enhance learning performance.