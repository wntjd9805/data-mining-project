Reinforcement learning (RL) has shown great potential as a general-purpose learning framework for artificial intelligence problems, but model-free RL approaches are not yet sample efficient, limiting their applicability in real-world tasks. To address this issue, model-based RL has emerged as a promising direction for improving sample efficiency. This paper introduces a novel model-based framework called BrIdging Reality and Dream (BIRD), which performs differentiable planning on imaginary trajectories and enables adaptive generalization to reality through optimizing mutual information between imaginary and real trajectories. Experimental results on visual control benchmarks demonstrate that BIRD achieves state-of-the-art performance in terms of sample efficiency, and an ablation study confirms the superiority of BIRD's mutual information maximization approach.