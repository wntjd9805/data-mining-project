Due to the increasing availability of low-cost sensors, the fusion of data from different sources for classification or regression has become a central problem in machine learning. This paper introduces deep multimodal fusion, which integrates multiple modalities using end-to-end neural integration. The paper categorizes fusion methods into aggregation-based fusion and alignment-based fusion, and proposes a new approach called Channel-Exchanging-Network (CEN). CEN dynamically exchanges channels between sub-networks for fusion, balancing inter-modal fusion and intra-modal processing. The paper presents the design, theories, and evaluations of CEN in the context of semantic segmentation and image translation. Results show that CEN outperforms existing fusion methods in terms of performance.