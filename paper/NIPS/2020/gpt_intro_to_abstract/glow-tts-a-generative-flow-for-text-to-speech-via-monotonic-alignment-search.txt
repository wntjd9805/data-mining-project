Text-to-speech (TTS) is a task in which speech is generated from text, and deep-learning-based TTS models have shown great success. However, deploying autoregressive TTS models in real-time services is challenging due to their long inference time and lack of robustness in certain cases. To address these limitations, parallel TTS models have been proposed, such as FastSpeech, which can synthesize speech faster and with reduced failure cases. However, these models rely on external aligners for training, which can impact their performance. In this work, we propose Glow-TTS, a flow-based generative model for parallel TTS that internally learns its own alignment. By combining flows and dynamic programming, Glow-TTS efficiently searches for the most probable monotonic alignment between text and speech. We demonstrate that enforcing hard monotonic alignments enables robust TTS for long utterances, while employing flows enables fast, diverse, and controllable speech synthesis. Glow-TTS can generate mel-spectrograms 15.7 times faster than Tacotron 2 (an autoregressive TTS model) while achieving comparable performance and exhibits superior performance to Tacotron 2 for longer input utterances. Additionally, we show that the model can be extended to a multi-speaker setting with minimal modifications.