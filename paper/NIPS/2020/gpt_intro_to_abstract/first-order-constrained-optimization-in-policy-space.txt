Abstract:This paper addresses the issue of safety in Deep Reinforcement Learning (DRL) algorithms, which often focus on performance improvement without considering the potential negative consequences of an agent's behavior. The authors propose the First Order Constrained Optimization in Policy Space (FOCOPS) algorithm, which aims to find the best constraint-satisfying policy update. FOCOPS utilizes a two-step approach, first finding the optimal policy in the nonparametric policy space and then projecting it back into the parametric policy space. The algorithm achieves better performance and approximate constraint satisfaction compared to state-of-the-art approaches, with the advantage of being simple to implement. The proposed method is tested on challenging high-dimensional continuous control tasks.