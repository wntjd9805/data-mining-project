Meta-learning aims to leverage knowledge learned from a large set of training tasks to improve efficiency in solving new tasks. One approach to achieve this is by reusing or repurposing task-agnostic modules, which can reduce overfitting, enhance interpretability, and enable deployment on limited-resource devices. However, current meta-learning methods are often inadequate for tasks with few data and long adaptation periods, such as few-shot text-to-speech synthesis (TTS). In this paper, we propose an automatic module learning approach that utilizes hierarchical Bayesian modeling and exploits the statistical property of shrinkage. We demonstrate the effectiveness of our approach in low-data, long-adaptation domains and show that it outperforms non-modular baselines in predictive power. Our findings also provide insights into network structures in computer vision and TTS. Overall, our method offers a promising solution for modular meta-learning and facilitates the identification of task-specific modules while preventing overfitting and improving predictive performance.