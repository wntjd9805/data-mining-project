Learning to optimize (L2O) is a subfield of meta learning that aims to replace manually designed optimizers with learned optimizers. These learned optimizers, typically modeled as recurrent neural networks, can be applied to train other machine learning models. However, training L2O models is challenging, and the learned optimizers often suffer from poor generalization. This paper introduces a toolkit of novel training techniques to improve L2O models. These techniques include a progressive training scheme to balance the trade-off between truncation bias and gradient explosion, and off-policy imitation learning to stabilize training. Extensive experiments demonstrate that incorporating these techniques significantly reduces training instability and improves performance and generalization of trained models. The results highlight the importance of training existing simple baselines better to evaluate progress in the field of L2O.