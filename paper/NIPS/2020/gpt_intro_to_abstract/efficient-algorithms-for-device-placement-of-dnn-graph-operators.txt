Deep Neural Networks (DNNs) have become widely used in various applications, such as image classification, translation, language modeling, and video captioning. As DNNs continue to grow in size and complexity, model parallelism has become increasingly important. Model-parallel inference, where the model is split across multiple devices, is beneficial due to memory limitations, potential for parallel execution, and device compatibility. Similarly, model-parallel training splits the model across devices to improve throughput. In this paper, we present efficient algorithms for partitioning DNN models in both inference and training scenarios, optimizing for latency and throughput respectively. We propose approaches for both non-pipelined and pipelined settings, solving the device placement optimization problem using Integer Programming and Dynamic Programming methods. Our evaluation shows that our algorithms result in efficient and optimal model splits, outperforming other techniques.