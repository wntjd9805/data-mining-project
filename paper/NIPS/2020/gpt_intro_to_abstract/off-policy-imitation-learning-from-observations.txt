Imitation Learning (IL) in reinforcement learning (RL) has been extensively studied for learning complex tasks with the help of expert demonstrations. Unlike traditional RL, IL can learn solely from expert guidance, making it crucial for practical applications in robotics where demonstrations are more accessible than accurate reward functions. However, conventional IL assumes access to both expert states and actions, which may not be feasible in the real world. To address this, Learning from Observations (LfO) has been introduced to learn from observations without expert actions. LfO is a more challenging setting due to the lack of fine-grained action guidance, but it is also a more practical approach that can utilize previously unused resources and potentially achieve advanced artificial intelligence. Distribution matching has been a popular approach for LfD and LfO, but traditional methods suffer from inefficient sampling strategies that require on-policy interactions. In this paper, we propose a LfO approach called OPOLO that improves sample efficiency through off-policy learning. By deriving an upper-bound of the LfO objective and combining it with a regularization term, we achieve state-of-the-art performance and sample efficiency. Extensive experiments on popular benchmarks validate the effectiveness of OPOLO.