Multi-stage training is widely used in natural language processing (NLP) applications, where word embeddings or contextual representations are first learned from unsupervised corpus and then fine-tuned on supervised tasks. However, many fundamental questions about multi-stage learning remain unanswered, such as the contribution of pretraining data, detection of false transfer, and tracing back problematic examples. This paper focuses on quantitatively measuring the influence of pretraining data on the end model and proposes a novel approach to estimate influence scores in multi-stage training without additional retraining. The proposed technique effectively determines the impact of pretraining data on the fine-tuned model and allows for assessing its benefit on the fine-tuning task.