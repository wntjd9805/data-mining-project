This paper introduces a minimax optimization problem that considers a more general case where the objective function is Âµ-strongly-concave in one variable but possibly nonconvex in the other variable. The problem formulation includes various machine learning applications such as robust optimization and adversarial training. Existing work has focused on the convex-concave case of the problem, but this paper aims to address the more general case. The paper proposes a novel algorithm called Stochastic Recursive gradiEnt Descent Ascent (SREDA) that reduces the number of stochastic gradient evaluations and achieves optimal complexity. The effectiveness of the algorithm is demonstrated through experiments on a robust optimization problem.