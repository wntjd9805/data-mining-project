Large-scale transfer learning has become widely used in deep learning, achieving state-of-the-art performance in natural language processing and related fields. However, the deployment of large models requires significant resources and has high environmental costs. Sparsity induction, specifically through pruning methods, offers a solution to reduce the memory footprint of neural networks with minimal loss of accuracy. While magnitude pruning is effective in standard supervised learning, it is less useful in transfer learning. Therefore, this paper introduces movement pruning, which considers weight changes during fine-tuning, and applies it to pretrained language representations (BERT) on various fine-tuning tasks. The results show significant improvements over magnitude pruning in highly sparse regimes, with 95% performance compared to the original BERT model using only 5% of the encoder's weight. Movement pruning also demonstrates greater adaptability to the end-task compared to magnitude pruning.