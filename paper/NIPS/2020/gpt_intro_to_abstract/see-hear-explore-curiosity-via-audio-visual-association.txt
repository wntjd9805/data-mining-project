Reinforcement learning (RL) has achieved many successes by maximizing extrinsic rewards, but formulating reward functions in real-world settings is difficult and requires significant human engineering. In contrast, humans are driven by intrinsic motivation, such as curiosity, to explore the world even in the absence of rewards. This paper focuses on curiosity-based exploration using future prediction, where an exploration policy receives rewards for actions that lead to differences between the real future and the future predicted by a forward dynamics model. However, putting this curiosity formulation into practice is challenging due to the open research problem of learning and modeling forward dynamics, handling multiple possible futures, incorporating physics, and determining the appropriate prediction space. Inspired by human exploration, the authors propose See Hear Explore (SHE), a curiosity formulation that encourages the agent to explore novel associations between sensory modalities. The authors demonstrate the effectiveness of SHE in several Atari games, showing that it enables more efficient exploration, is more sample-efficient, and is more robust to noise compared to existing curiosity baselines. In addition, experiments on area exploration in a realistic simulator further validate the superiority of SHE. The contributions of this paper include introducing a novel formulation of curiosity that searches for multimodal associations, outperforming existing curiosity approaches on Atari benchmark tasks, and highlighting the importance of multimodality in exploring complex unknown environments.