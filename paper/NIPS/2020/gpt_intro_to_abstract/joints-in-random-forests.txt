Decision Trees (DTs) and Random Forests (RFs) are widely used machine learning models for predictive analysis. While Deep Neural Networks dominate in areas such as image and text data, DTs and RFs are the go-to models for tabular datasets. However, these models are usually seen as purely predictive, lacking the ability to capture the overall joint distribution of the data. In this paper, the authors explore the potential for DTs and RFs to be interpreted as generative models, allowing for a more comprehensive understanding of the input space. They introduce the concept of Generative Decision Trees (GeDTs) and Generative Forests (GeFs), which offer the advantages of both discriminative and generative models. The authors demonstrate that GeDTs and GeFs can handle missing features and outliers more effectively than traditional DTs and RFs, and provide consistent predictions in the presence of incomplete data. They also propose a practical implementation of the generative interpretation that can be easily integrated into existing DT and RF learning algorithms.