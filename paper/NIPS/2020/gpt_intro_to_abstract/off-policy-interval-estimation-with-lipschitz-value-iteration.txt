Reinforcement learning (RL) has become widely used in various domains, but its success relies on large amounts of data, which may not be available in real-world tasks. Off-policy evaluation (OPE), estimating the expected reward of a target policy using observational data, holds promise for data-efficient RL algorithms. However, existing OPE methods primarily focus on point estimation, which can be unreliable and inadequate for high-stakes applications. To address this, we propose a general optimization-based framework for provably correct off-policy interval estimation. Our algorithm, based on Lipschitz functions, computes upper and lower bounds of the expected reward and guarantees convergence and efficiency. Experimental results demonstrate the tightness and provable correctness of our interval estimation.