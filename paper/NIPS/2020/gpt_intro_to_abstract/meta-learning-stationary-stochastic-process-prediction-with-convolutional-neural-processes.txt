In this paper, we address the problem of incorporating inductive biases into machine learning models for achieving good generalization performance. Specifically, we focus on the inductive biases of stationarity and translation equivariance (TE) in tasks such as time-series, image-based sampling, and spatio-temporal modeling. We propose a new framework called Convolutional Neural Processes (ConvNPs) that extends the capabilities of existing models, such as Conditional Neural Processes (CNPs) and Neural Processes (NPs), by incorporating TE and enabling the modeling of complex joint distributions. We introduce a simplified maximum-likelihood training procedure for ConvNPs and demonstrate their effectiveness in various experiments, including toy time-series, image-based sampling, and real-world environmental data sets. Our contributions include the development of ConvNPs, the proposal of a simplified training procedure, and the demonstration of ConvNPs' usefulness in practical applications.