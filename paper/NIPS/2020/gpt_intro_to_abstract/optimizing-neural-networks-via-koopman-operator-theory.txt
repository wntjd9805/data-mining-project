This paper explores the use of Koopman operator theory (KOT) to learn and predict the dynamics present in the training of artificial neural networks (NNs), a process typically viewed as a black box. The authors propose the concept of "Koopman training" and investigate its potential benefits compared to traditional NN training approaches. By leveraging KOT, which is a linear theory for nonlinear dynamical systems, the authors demonstrate that the computations involved in evolving NN weights/biases using Koopman operators can be considerably cheaper than standard NN training methods. The authors present successful applications of Koopman training to feedforward, fully connected NNs and discuss its versatility and power. They also highlight future problems of interest and the potential for more advanced KOT methods to further extend their results.