This paper explores the challenges faced by reinforcement learning algorithms in maximizing cumulative reward with limited environmental data and imperfect policy and value function representation. It examines techniques commonly used in large-scale RL settings, such as entropy regularization and value smoothing, which have shown success in improving algorithm stability and accuracy. However, the theoretical understanding of these techniques is still incomplete. The paper contributes by conducting error propagation analysis and demonstrating the implications of entropy regularization and value smoothing on actor-critic and value-based RL algorithms. It also introduces a new smooth Bellman operator for formalizing the value smoothing technique and discusses how entropy regularization reduces overestimation errors. Additionally, the paper presents error bounds for an abstract algorithm that combines smoothing, regularization, and neural network function approximation.