In recent years, there has been a significant increase in the application of machine learning algorithms in various domains, leading to a profound impact on people's lives. However, these algorithms are susceptible to bias towards certain individuals or groups. For example, facial recognition algorithms have been found to exhibit bias based on race and gender, resulting in potential implications for law enforcement. To address this issue, the field of fairness in machine learning has emerged, with various definitions of fairness and debiasing techniques being proposed. While pre-, in-, and post-processing methods have been commonly used, they may not be suitable for the fine-tuning of large pretrained models. In this paper, we introduce the concept of intra-processing methods for debiasing neural networks, which involve updating or augmenting the weights of the original model based on a target task dataset. We propose three baseline algorithms and adapt an existing in-processing algorithm for this intra-processing setting. We also compare our methods with existing in- and post-processing techniques and demonstrate the effectiveness of intra-processing for fine-tuning use cases. Overall, our contributions include the exploration of intra-processing algorithms, a study on the sensitivity of these techniques to initial conditions, and the comparison of different algorithms across various fairness constraints and datasets.