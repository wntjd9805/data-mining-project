One of the intriguing phenomena in contemporary machine learning is the extraordinary success of unregularized and overparameterized learning algorithms. These algorithms have the ability to memorize and fit random data, yet they demonstrate remarkable performance in generalizing to unseen samples when trained on real-life data. This phenomenon is often attributed to the implicit-regularization of the algorithms, which refers to their preference for certain structured solutions. Understanding this implicit regularization poses interesting challenges such as identifying the implicit bias of a learning algorithm, analyzing the rate of convergence towards biased solutions, and exploring the role of regularization in generalization in modern-day machine learning. This paper focuses on stochastic convex optimization (SCO) using the SGD optimization algorithm as a test-bed to explore the role of regularization and its relation to generalization. The paper presents several constructions that demonstrate the absence of distribution-independent and distribution-dependent implicit biases in SGD. Additionally, the paper investigates the implicit bias in constant-dimensional problems and provides insights into the role of specific regularizers.