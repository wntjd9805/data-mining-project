This paper introduces MulMON (Multi-View and Multi-Object Network), an unsupervised method for learning object-centric scene representations from multiple views. MulMON addresses the limitations of single-view methods by iteratively updating latent object representations for a scene over multiple views, maintaining object correspondence across views. The method predicts the appearance and object segmentations of a scene from unobserved viewpoints and accurately captures 3D scene information. Experimental results demonstrate that MulMON resolves spatial ambiguities, achieves inter- and intra-object disentanglement, and provides new functionality such as viewpoint-queried object-segmentation.