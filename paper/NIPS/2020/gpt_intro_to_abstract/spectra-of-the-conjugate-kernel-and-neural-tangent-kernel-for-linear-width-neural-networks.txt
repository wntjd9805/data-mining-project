Recent progress in our theoretical understanding of neural networks has connected their training and generalization to two associated kernel matrices: the Conjugate Kernel (CK) and the Neural Tangent Kernel (NTK). In this paper, we apply techniques of random matrix theory to derive an exact asymptotic characterization of the eigenvalue distributions of the CK and NTK at random initialization in a multi-layer feedforward network. We demonstrate that the eigenvalue distributions of both matrices converge to deterministic limits, depending on the limiting eigenvalue distribution of the training data. Additionally, we examine the spectral evolutions of the CK and NTK during training and observe elongations in their bulk eigenvalue distributions and the emergence of isolated principal components that are highly predictive of the training labels. Our findings provide insights into the training and generalization properties of neural networks and expand on previous work that has focused on the spectral properties of the CK and NTK.