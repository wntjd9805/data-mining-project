This paper introduces the concept of continual learning (CL) or lifelong learning in computer science. It discusses the objectives of CL, including knowledge accumulation, transfer of knowledge, and learning a sequence of dissimilar and similar tasks simultaneously. The paper highlights the limitations of existing CL techniques and proposes a novel TCL model called CAT (Continual learning with Forgetting Avoidance and knowledge Transfer). CAT uses a knowledge base to retain previous knowledge, identifies similar and dissimilar tasks, and employs task masks and knowledge transfer attention for effective learning. Empirical evaluation demonstrates that CAT outperforms existing baseline models in the proposed problem.