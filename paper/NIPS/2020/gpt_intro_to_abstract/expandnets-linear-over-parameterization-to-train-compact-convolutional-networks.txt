Convolutional neural networks (CNNs) have achieved remarkable success in various tasks, but the trend of designing larger networks raises challenges in memory and computation. Compact networks, although sufficient in many cases, are difficult to train from scratch and knowledge transfer has been the popular approach. In this paper, we propose an alternative approach of training compact neural networks by expanding linear layers instead of using nonlinearity. We introduce three expansion strategies for convolutions in CNNs and demonstrate their effectiveness in tasks like image classification, object detection, and image segmentation. Our experiments show that over-parameterization plays a crucial role in achieving better performance. We also analyze the benefits of linear over-parameterization through experiments studying generalization, gradient confusion, and the loss landscape.