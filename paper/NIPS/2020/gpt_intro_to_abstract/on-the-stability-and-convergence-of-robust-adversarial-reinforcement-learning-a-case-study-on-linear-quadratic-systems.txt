Reinforcement learning (RL) often struggles to generalize from simulation to the real world, but robust adversarial RL (RARL) is a promising approach that addresses this issue. RARL trains a protagonist and an adversary, with the protagonist learning to perform control tasks in the presence of disturbances generated by the adversary. However, there is currently no established baseline for evaluating the robustness of policies learned through RARL. In this paper, we explore the effectiveness of RARL in a linear quadratic (LQ) setting, a fundamental model for robust control. We examine the stability and convergence of RARL in the LQ case and propose a new double-loop natural policy gradient algorithm that guarantees robust stability and convergence. Our findings offer insights into RARL from a rigorous robust control perspective and demonstrate the importance of stability in continuous control tasks.