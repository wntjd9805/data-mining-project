The success of deep learning relies on large neural networks that out-scale the training dataset, challenging conventional learning theory. Regularization methods have been essential in deep learning and can take various forms, such as invariances to transformations and prior regularities within and between layers of neural networks. The most common approach to regularization is regularized risk minimization (RRM), but optimizing model weights while balancing accurate prediction and faithful regularization can be challenging. This paper proposes a novel approach called ProxNet, which leverages the idea of proximal mapping to optimize hidden layer outputs directly instead of indirectly through the predictor. ProxNet provides modularity, improves regularization effectiveness, and can be connected to meta-learning. Experimental results demonstrate that ProxNet outperforms state-of-the-art prediction models.