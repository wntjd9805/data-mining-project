The introduction of this computer science paper discusses the importance of learning from large unlabeled datasets in machine learning techniques, as annotating data is costly and infeasible at scale. It explores different approaches to self-supervised visual learning, including general architectures for feature representation learning and leveraging pseudo-labels generated from proxy tasks. The paper presents a different strategy that involves using a trained vision system to generate the proxy task for learning visual representations. The authors focus on the bootstrapping of visual representation learning by using a pre-existing contour detector to define the task objective. They propose a system that combines contour detection and contrastive feature learning to train a convolutional neural network (CNN) for semantic embeddings. The system leverages a small amount of annotated data to learn from a larger pool of unlabeled data, and the results show competitive performance in learning transferable representations and applications such as semantic region search and instance tracking in video.