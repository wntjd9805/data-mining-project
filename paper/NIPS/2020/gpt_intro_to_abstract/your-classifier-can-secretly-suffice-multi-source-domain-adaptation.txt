The task of supervised learning for classification assumes that the training and testing data are sampled from the same distributions, leading to state-of-the-art results. However, real-world deployment of these models often yields suboptimal results due to the distribution shift between the training data and the target domain. Unsupervised Domain Adaptation methods aim to transfer knowledge from a labeled source dataset to an unlabeled target dataset under a domain shift. One popular strategy is to learn a domain-invariant latent space using statistical discrepancy minimization or adversarial objectives. However, the performance of Single-Source Domain Adaptation (SSDA) methods is limited by the choice of the source dataset. Multi-Source Domain Adaptation (MSDA) has gained interest, where multiple labeled source domains are used to transfer knowledge to the target domain. MSDA faces the challenge of domain and category shift between each pair of source domains. In this work, we propose a different approach to MSDA by leveraging the implicit alignment of deep models' latent features. We train domain-specific classifiers and observe that by enforcing classifier agreement on the class label for each input instance, the domains align without requiring an explicit alignment loss. We name this approach Self-supervised Implicit Alignment (SImpAl) and demonstrate successful MSDA on benchmark datasets.