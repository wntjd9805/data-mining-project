The introduction of this computer science paper discusses the fascination with deep learning algorithms and their ability to generalize to unseen data. It highlights the issue of overfitting and the tendency for algorithms to memorize labels rather than generalize. The paper introduces the long tail theory, which suggests that memorization is necessary for achieving close-to-optimal generalization error in long-tailed datasets. The paper aims to empirically validate this theory by examining which training examples are memorized and their utility, as well as the influence of memorized examples on test accuracy. The paper presents experimental results that support the long tail theory and discusses related questions regarding the choice of architecture and computational efficiency.