Many applications in artificial intelligence (AI) require utilizing previous data and experience to improve effectiveness and efficiency on new tasks. Meta-learning has been successful in this regard, aiming to leverage data from available tasks to learn model parameters or representations that can be used to perform well on new unseen tasks, particularly when limited data and computational power are available at the test time. Model-Agnostic Meta-Learning (MAML) has been one of the most successful formulations in meta-learning, training model parameters to perform well on new tasks with only a few gradient-based updates. However, MAML is applicable only to continuous domains. This paper extends the methodology of MAML to the discrete setting, focusing on submodular maximization, an essential class of discrete optimization. The proposed framework reduces computation load and can be applied to various recommendation tasks. The paper presents deterministic and randomized meta-greedy algorithms to solve the resulting meta-learning problem, providing analytical guarantees for their performance. The proposed framework and algorithms are experimentally evaluated in movie recommendation and ride-sharing problems, demonstrating their effectiveness in quickly adapting to new tasks with reduced computation.