One-class learning, also known as one-class classification, has various applications including information retrieval, anomaly detection, and image/video-based applications. This paper introduces a novel approach to one-class learning that does not rely on auto-encoders or adversarial training techniques. The proposed method utilizes a new loss function called the one-class loss, which incorporates negative log likelihood and a novel regularization method called holistic regularization. This regularization aims to avoid biasing certain features in model building, which is crucial since the distribution and location of anomalies or negative data are unknown. The paper presents empirical evaluations showing the effectiveness of the proposed method, named HRN (H-Regularization with 2-Norm instance-level normalization), compared to eleven state-of-the-art baselines. Additionally, the authors discuss the potential impact of holistic one-class learning on other learning paradigms such as positive and unlabeled learning, open-world learning, and continual learning.