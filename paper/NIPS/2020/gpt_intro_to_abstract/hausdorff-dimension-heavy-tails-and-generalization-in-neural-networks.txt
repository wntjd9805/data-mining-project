This paper introduces a novel approach to analyzing the generalization properties of stochastic gradient descent (SGD) in deep learning problems. The authors consider the trajectories of the optimization algorithm as a Feller process, which is a broad class of Markov processes that includes many important stochastic processes. They define a new notion called "uniform Hausdorff dimension" to measure the complexity of the trajectories and show that the generalization error can be controlled by this dimension, which is smaller than the Euclidean dimension. The results suggest that heavier-tailed processes achieve smaller generalization error, providing an implicit regularization effect. The authors also propose an efficient method for estimating the Hausdorff dimension and validate their theory through experiments on various neural networks.