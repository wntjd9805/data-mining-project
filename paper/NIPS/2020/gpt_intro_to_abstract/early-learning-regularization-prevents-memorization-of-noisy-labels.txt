Deep neural networks have become indispensable for classification tasks, but the availability of large curated datasets with manually verified labels is limited. This paper addresses the issue of training deep neural networks on datasets with lower quality annotations, which contain label noise. The authors observe that when trained on noisy labels, deep neural networks initially fit the training data with clean labels before memorizing examples with false labels. To counteract the influence of noisy labels, the paper introduces a novel framework that utilizes the early-learning phenomenon. The authors propose a technique that incorporates target probabilities estimated from model outputs using semi-supervised learning techniques, effectively achieving robustness to noisy labels. Experimental results on standard benchmarks and real-world datasets validate the proposed methodology's effectiveness.