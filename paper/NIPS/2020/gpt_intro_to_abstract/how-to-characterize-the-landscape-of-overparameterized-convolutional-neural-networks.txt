In this paper, we address the contradiction in the understanding of the landscape of deep neural networks (DNNs) in parameter space. We propose investigating the loss landscape in the feature distribution space instead of the parameter space. We introduce a technique called neural network grafting (NNG) to compare feature distributions learned by different DNNs. We find that feature distributions learned by DNNs with the same architecture but different initializations are almost the same throughout the training process. This indicates that viewing DNNs in the aspect of feature distributions simplifies their understanding. We reparameterize DNNs with respect to the feature function distributions and show that the loss landscape becomes convex in the feature distribution space. This has significant implications for DNN optimization, as training algorithms converge to a solution that is a stationary point of the convex reformulation. Studying DNNs in the feature distribution space provides valuable insights and simplifies their characterization.