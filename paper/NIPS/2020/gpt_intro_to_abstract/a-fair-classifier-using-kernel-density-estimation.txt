In recent years, there has been a significant increase in interest and application of machine learning algorithms, particularly in decision-making processes that have a direct impact on people's lives and rights. One crucial factor in the design of these algorithms is ensuring fairness, specifically in regard to sensitive attributes such as race, sex, age, and religion. Previous works have developed various metrics to measure fairness, including group fairness, individual fairness, and causality-based fairness. However, many existing algorithms struggle with quantifying fairness in a differentiable manner, hindering their effectiveness. To address this issue, this paper introduces a novel approach based on kernel density estimation (KDE) to directly compute fairness measures without relying on a fairness proxy. The proposed KDE-based framework not only provides accurate distribution estimates but also enables the use of standard gradient descent for optimization. Experimental results on both synthetic and real datasets demonstrate that the algorithm achieves a higher accuracy-fairness tradeoff compared to existing methods. Additionally, it offers improved training stability compared to adversarial learning-based frameworks.