Deep Neural Networks (DNNs) have shown remarkable progress in various real-world applications, such as computer vision, speech recognition, natural language processing, and recommendation systems. Residual Networks (ResNets) with residual connections have emerged as a breakthrough in network architecture, demonstrating superior generalization abilities compared to traditional Feedforward Networks (FFNets). Despite their success, the reason behind the exceptional generalization abilities of ResNets remains largely unknown. This paper aims to investigate the Neural Tangent Kernels (NTKs) associated with deep feedforward and residual networks to better understand their generalization abilities. The authors analyze the behavior of the NTKs as the width and depth of the networks increase, providing both asymptotic and nonasymptotic bounds. The findings reveal that deep residual networks exhibit a different behavior from deep feedforward networks, justifying their advantages in terms of generalization abilities. Numerical results are provided to support the claims, and the work is compared to related research.