Reinforcement learning involves evaluating the value function of a target policy, either through on-policy or off-policy sampling. Conventional algorithms used in the on-policy setting have guaranteed convergence, but in the more popular off-policy setting, these algorithms can diverge under linear function approximation. To address this issue, gradient-based TD algorithms have been developed, with the TD with gradient correction (TDC) algorithm being widely used. However, these algorithms suffer from large variance due to stochastic samples from a dynamic environment. Previous approaches have used variance reduction techniques to address this issue, but they have only been applied to i.i.d. samples and the on-policy setting. This paper aims to develop two variance-reduced TDC algorithms for both i.i.d. samples and Markovian samples in the off-policy setting. The convergence rates and sample complexities of these algorithms are analyzed, and experiments show their superior performance compared to conventional TDC and variance-reduced TD algorithms. This analysis requires new bounding techniques to establish the reported sample complexities.