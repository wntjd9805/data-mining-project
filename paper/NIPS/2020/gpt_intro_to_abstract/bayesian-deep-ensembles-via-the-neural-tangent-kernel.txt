In this paper, we explore the relationship between deep ensembles and Bayesian inference in the context of neural networks. While deep ensembles have shown promising empirical performance in uncertainty quantification and out-of-distribution robustness, their underlying Bayesian justification remains unclear. We connect deep ensembles to Bayesian inference using recent developments linking Gaussian processes and wide neural networks. We propose a modification to standard neural network training that yields an exact posterior sample for the network outputs in the infinite width limit. By ensembling these modified baselearners, we obtain a posterior predictive approximation, thus creating a Bayesian deep ensemble. We also discuss the use of randomised priors in ensembles to approximate posterior interpretation, particularly in the presence of observation noise. Our methods build on the Neural Tangent Kernel framework and provide a novel approach for uncertainty quantification in neural networks.