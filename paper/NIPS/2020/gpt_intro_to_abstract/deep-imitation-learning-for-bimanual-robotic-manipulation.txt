Bimanual manipulation is an essential capability for robots to successfully perform real-world tasks. While there has been significant progress in single-arm manipulation, bimanual manipulation tasks have received less attention. Existing work in this area often relies on classical control methods, which require explicit models of the environment and its dynamics. However, constructing accurate models is challenging due to complex interactions between the two arms/hands and the manipulated object. In this paper, we propose a deep imitation learning framework, called Hierarchical Deep Relational Imitation Learning (HDR-IL), to address the challenges of bimanual manipulation. Our framework incorporates relational information and uses recurrent graph neural networks to capture robot-robot and robot-object interactions. We present experimental results on a table-lifting task, demonstrating that our proposed framework outperforms baseline approaches by a significant margin.