Adversarial attacks on sensor streams in complex machine learning systems can create vulnerabilities, making it crucial to detect when an attack has occurred and which sensors have been compromised. While previous work has focused on detecting anomalies or attacks from single samples, we propose a method that looks for sequences of samples showing anomalous behavior. We frame the problem as distribution shift detection, emphasizing the importance of identifying shifts in distributions across sensors. To address the computational challenge, we develop a test statistic based on Fisher divergence that allows us to consider all features simultaneously and reduces the amount of computation needed. We also extend our approach to handle time-series data. Our contributions include the definition of the problem, the development of conditional distribution hypothesis tests, practical algorithms, and a score-based test statistic.