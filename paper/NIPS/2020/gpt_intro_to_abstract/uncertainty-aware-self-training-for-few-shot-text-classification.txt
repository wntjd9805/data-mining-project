Deep neural networks have shown great success in various applications, but the lack of labeled data poses a challenge for training these networks. Acquiring large amounts of labeled data is expensive and time-consuming, and human labeling is often not feasible due to data access and privacy constraints. Recent advances in pre-training have helped to mitigate this issue, but fine-tuning with task-specific labeled data is still necessary for optimal performance. Semi-supervised learning (SSL) has emerged as a promising approach to address this problem by leveraging large amounts of unlabeled data in addition to limited labeled data. In this paper, we propose an uncertainty-aware self-training framework for few-shot text classification using pre-trained language models like BERT. We introduce three core components, namely masked model dropout for uncertainty estimation, sample selection based on teacher uncertainty, and confident learning to emphasize low variance examples. Our experiments demonstrate that our framework significantly improves the performance of BERT for few-shot text classification without the need for auxiliary resources.