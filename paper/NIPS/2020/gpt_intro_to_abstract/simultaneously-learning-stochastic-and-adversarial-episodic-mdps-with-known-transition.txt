This paper addresses the problem of learning episodic Markov Decision Processes (MDPs) and focuses on minimizing regret, the difference between the learner's total loss and that of the optimal fixed policy. The existing state-of-the-art algorithms perform well in either the adversarial or stochastic loss setting, but not both. This paper introduces a new algorithm that achieves the best of both worlds by combining techniques from the adversarial and stochastic settings. The algorithm achieves logarithmic regret in the stochastic setting and worst-case robustness in the adversarial setting. The paper also introduces a new regularizer and presents a novel analysis that handles the non-decomposable and non-diagonal Hessian of the regularizer. Although the paper only considers the case with known transition, it provides insights for solving the general case with unknown transition.