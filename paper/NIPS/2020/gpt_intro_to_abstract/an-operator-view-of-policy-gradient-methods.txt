This paper introduces the concept of model-free reinforcement learning algorithms, which aim to learn a policy that maximizes the sum of rewards from samples generated through the agent's interactions with the environment. The algorithms fall into two categories: value-based methods, where the agent predicts the value of an action and chooses the one with the highest value, and policy-based methods, where the agent learns a distribution over actions in each state. Past works have explored connections between these views, but often only in relation to the optimal policy and without considering training dynamics. This paper proposes a new perspective that connects policy-gradient methods to value-based ones by introducing a policy improvement operator and a projection operator. The authors also present a lower bound on policy performance and explore the use of Î±-divergences in imitation learning.