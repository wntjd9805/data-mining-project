In this paper, we examine the concept of generalization in Bayesian deep learning from a probabilistic perspective. We argue that generalization depends on the support and inductive biases of a model, rather than on parameter counting. We illustrate this perspective with different models and discuss the importance of choosing priors that induce reasonable inductive biases for the problem at hand. We also analyze the properties of priors over functions induced by priors over the weights of neural networks and connect these results to tempering. Additionally, we show that the recent phenomenon of generalization in neural networks is not specific to them, as Gaussian processes can also achieve similar results. Finally, we propose a method called MultiSWAG inspired by deep ensembles, which marginalizes within basins of attraction and improves performance compared to traditional Bayesian approaches.