The advances in machine learning have been driven by the availability of large datasets collected by various organizations. In our society, individuals own electronic devices that collect massive amounts of data, which, when used collaboratively, can lead to transformative insights. However, this data is often bound to the device it is captured on, either due to communication efficiency or privacy constraints. Decentralized machine learning offers a solution by enabling collaborative processing of this data, allowing nodes to train a model by minimizing a loss function on their joint dataset. Communication in this paradigm occurs in a peer-to-peer fashion without centralized coordination. The introduction of decentralized optimization algorithms in deep learning has sparked interest in communication compression techniques to reduce bandwidth requirements. While existing compression algorithms in centralized deep learning retain model quality, decentralized optimization algorithms require additional hyperparameter tuning. In this paper, we study a specific class of low-rank compressors for decentralized optimization that do not require tuning, inspired by previous work. We validate these compressors on image classification and language modeling tasks, demonstrating competitive performance without the need for additional hyperparameter tuning. Overall, our approach simplifies the transition from centralized to decentralized learning without compromising convergence or performance.