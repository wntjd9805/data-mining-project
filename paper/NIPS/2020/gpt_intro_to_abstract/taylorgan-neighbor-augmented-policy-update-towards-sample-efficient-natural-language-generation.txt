Generative adversarial networks (GAN) have been successful in various applications, including image generation and unsupervised style transfer. In the field of natural language generation (NLG), GANs face challenges due to the non-differentiable nature of passing discrete tokens. Existing solutions, such as using score function-based gradient estimators, suffer from poor sample efficiency and credit assignment problems. In this paper, we propose TaylorGAN, a novel unsupervised NLG technique, which improves the efficiency and accuracy of the estimator by incorporating approximated rewards of sequences. Our experiments show that TaylorGAN achieves state-of-the-art performance without the need for maximum likelihood pre-training or additional variance reduction techniques.