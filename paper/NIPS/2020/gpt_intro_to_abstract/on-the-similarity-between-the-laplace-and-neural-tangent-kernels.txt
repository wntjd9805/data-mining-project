Neural networks with a large number of parameters have proven to be effective in various tasks, despite having fewer training examples. These networks are equivalent to kernel regression using Neural Tangent Kernels (NTK) in the limit of infinite model size. NTK methods offer insights into the convergence and generalization abilities of neural networks. Recent experiments have shown that NTK performs similarly to, or even better than, neural networks in practice. This raises the question of whether NTK differs significantly from standard kernels. Experimental evidence suggests that NTK is particularly effective for fully connected networks, outperforming the Gaussian kernel. In this paper, we demonstrate theoretically and experimentally that NTK closely resembles the Laplace kernel, a standard tool in machine learning. We show that NTK and the Laplace kernel exhibit similar behavior on the hypersphere, suggesting the same dynamics in gradient descent. Our results provide insights into the properties of NTK and its potential application in analyzing neural networks.