This paper introduces the problem of online learning with dynamics, where machine learning systems interact with their environment and their decisions affect the data collected. The authors propose a counterfactual notion of regret called Policy Regret, which measures the performance of a learner by comparing it to a policy deployed from the beginning of time. They address the gap in previous works by studying the learnability of online learning problems with arbitrary dynamical systems, providing sufficient conditions and upper bounds for policy regret. Their approach considers general policy classes and loss functions, and they provide lower bounds to show the tightness of their results. The authors also apply their framework to various previously studied problems and obtain tight regret bounds for new examples.