Active perception, which involves gathering observations to reduce uncertainty about a hidden variable, is a crucial capability for intelligent agents. In multi-agent active perception, a team of autonomous agents cooperatively collects observations to infer the value of the hidden variable. This problem is often encountered in application domains such as search and rescue robotics, sensor networks, and distributed hypothesis testing. The main challenge lies in determining how each agent should act independently during the decentralized phase to maximize the informativeness of the collected observations. This problem can be formalized as a decentralized partially observable Markov decision process (Dec-POMDP), which is a model of sequential multi-agent decision-making under uncertainty. In this paper, we propose a method to convert the centralized prediction reward into a decentralized one, allowing for the application of standard Dec-POMDP algorithms to solve multi-agent active perception problems. We demonstrate the empirical usefulness of our approach with improved scalability compared to existing techniques.