Despite the success of deep learning in various domains, feed-forward neural networks are often criticized for being difficult to interpret. In this paper, we propose network architectures derived from optimization algorithms, allowing for functional interpretation. We introduce a machine learning paradigm where we learn the parameters of a parametric objective function and design an optimization algorithm for efficient minimization. This approach has proven successful for solving inverse imaging problems and allows for the use of domain-specific priors within deep models. Existing approaches are limited in incorporating complex image priors, and our paper addresses this issue by introducing a general algorithmic framework. We adopt a flexible point of view, considering non-cooperative games and utilizing the Moreau-Yosida regularization technique. Unrolling the resulting optimization algorithm leads to a trainable network architecture that captures a wide range of image priors. Our approach offers improvements in efficiency and flexibility and demonstrates competitive performance in imaging tasks with fewer parameters. We present practical techniques for training optimization-driven layers.