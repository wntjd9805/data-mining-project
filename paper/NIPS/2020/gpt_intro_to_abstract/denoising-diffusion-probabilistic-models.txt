This paper introduces diffusion probabilistic models, also known as diffusion models, which are parameterized Markov chains trained using variational inference to generate high-quality samples matching the data. The transitions of the diffusion chain reverse a diffusion process, gradually adding noise to the data in the opposite direction of sampling until the signal is destroyed. The authors demonstrate that diffusion models can produce better samples than other generative models and establish an equivalence between diffusion models and denoising score matching and annealed Langevin dynamics. However, the models' log likelihoods are not competitive, primarily due to the description of imperceptible image details. The authors provide a refined analysis of this phenomenon and show that the sampling procedure of diffusion models resembles progressive decoding and vastly generalizes autoregressive decoding.