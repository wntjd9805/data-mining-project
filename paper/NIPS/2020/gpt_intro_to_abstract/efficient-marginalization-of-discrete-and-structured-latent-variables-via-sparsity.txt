Neural latent variable models are useful for analyzing high-dimensional data, but training them with discrete variables can be challenging. Current approaches rely on Monte Carlo methods, but these suffer from high variance or introduce bias and assumptions. In this paper, we propose an alternative method that avoids sampling and instead evaluates the sum with less computation. We introduce a strategy that parameterizes the discrete distribution with sparse mappings, such as sparsemax and SparseMAP. These sparse distributions allow us to eliminate certain assignments of the latent variable, reducing the need for expensive computations. We demonstrate the effectiveness of our approach in three different applications and provide a thorough analysis and comparison to existing methods. Our strategy combines the accuracy of exact marginalization with the efficiency of single-sample estimators.