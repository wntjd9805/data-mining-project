This paper presents a new approach to training large neural networks by utilizing consumer-grade hardware through volunteer computing. The authors propose a Decentralized Mixture of Experts (DMoE) layer that is designed for training with vast amounts of unreliable hardware. They describe a framework for training large neural networks composed of DMoE layers and evaluate the efficiency and reliability of this approach through formal guarantees and experiments. The authors also provide the PyTorch source code for reproducing their results. By leveraging volunteer computing, this approach offers a cost-effective solution for scaling up neural network models beyond the capabilities of traditional supercomputers.