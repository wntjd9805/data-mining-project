This paper introduces the concept of normalization-activation layers in deep networks and explores the use of automated machine learning to design these layers. The authors argue that the existing separate design of normalization layers and activation functions may not be optimal and propose a unification approach. They formulate the layer as a tensor-to-tensor computation graph and develop novel rejection protocols to filter out ineffective candidate layers. Their method, called EvoNorms, discovers a set of novel layers that outperform popular layers such as BatchNorm-ReLU. The authors highlight the contributions of their work in terms of the novel approach, rejection protocols, generalization capabilities, and insights into the design of normalization and activation layers.