Reinforcement learning (RL) is a powerful form of learning where agents interact with their environment to maximize a reward signal. However, RL models typically assume precise access and updates to value functions, which is not realistic in neural circuits with limited precision. In this paper, we examine how agents with limited computational resources can represent and utilize values effectively. We propose a novel algorithm called Dynamic Resource Allocator (DRA) that combines resource allocation and learning to optimize performance. We demonstrate the effectiveness of DRA in standard RL tasks and model-based planning.