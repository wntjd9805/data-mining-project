In this paper, we investigate the role of gates in deep neural networks (DNNs) with rectified linear unit (ReLU) activations. We focus on the fact that the output of a ReLU activation can be expressed as the product of its pre-activation input and a gating signal. We examine the changes in these gates and the corresponding active sub-networks during training, and explore the potential valuable information contained in the gates of a trained DNN. Our goal is to study the impact of gates in DNNs trained with gradient descent (GD), and make claims about the fundamental nature of active sub-networks and their contribution to generalization.