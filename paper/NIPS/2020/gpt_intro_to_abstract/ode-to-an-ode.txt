Neural ODEs are continuous extensions of deep neural network architectures that use ordinary differential equations (ODEs) to govern the evolution of intermediate activations. These ODEs are parameterized by Î¸ and can be approximated using numerical integration techniques. Neural ODEs have been successfully applied in various machine learning tasks, but they can suffer from training instabilities and vanishing/exploding gradients. To address these challenges, previous works have proposed improvements based on orthogonal/Hermitian matrices. However, these approaches are limited to discrete settings. In this paper, we present a new paradigm called ODEtoODE, which extends neural ODE algorithms to continuous systems. In ODEtoODE, the time-dependent parameters of the main flow evolve according to a matrix flow on the orthogonal group. We show that this approach improves the stability and effectiveness of training neural ODE architectures, leading to better downstream models. We provide convergence results and demonstrate the superiority of our method in reinforcement learning and image classification tasks.