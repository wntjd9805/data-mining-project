The increasing use of automated decision making aided by machine learning models has highlighted the importance of credibility in fields like drug discovery. Credibility refers to the user's trust in a model's output, and providing interpretability is a requirement set by the European regulatory framework for the application of machine learning models. One approach to interpretability is attribution, which assigns credit to individual input features based on their importance to the model's prediction. Attribution methods have been extensively studied in image and text domains, but their application to graph-valued data remains relatively unexplored. In this paper, we present a benchmarking suite and evaluate the performance of attribution methods on graph neural network (GNN) models using accuracy, faithfulness, consistency, and stability as evaluation metrics. Our results show that CAM applied to GCNs is the best performing attribution method for GNNs, while GradCAM is optimal for graph datasets containing only adjacency information. Our contributions include the benchmarking suite and insights on the performance of attribution methods in GNNs.