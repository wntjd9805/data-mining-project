Reinforcement learning (RL) aims to maximize expected total reward by taking actions according to a policy in a stochastic environment. The actor-critic (AC) algorithm, which combines a critic to estimate the value function and an actor to update the policy, has been proposed to address the limitations of policy gradient (PG) methods. Existing studies have focused on the asymptotic convergence and sample complexity of AC and natural actor-critic (NAC) algorithms. However, these studies often assume access to the stationary distribution and use single-sample estimators, which may not be practical or sample-efficient. In this paper, we propose online AC and NAC algorithms that operate on a single sample path and use Markovian mini-batches for updates. We characterize the convergence rates and sample complexities of both algorithms, showing that mini-batch AC and NAC outperform existing methods. Our analysis introduces novel techniques for handling bias error and demonstrates the advantages of the Markovian mini-batch update. Overall, this is the first theoretical study to show that AC and NAC have better convergence rates than PG and NPG algorithms.