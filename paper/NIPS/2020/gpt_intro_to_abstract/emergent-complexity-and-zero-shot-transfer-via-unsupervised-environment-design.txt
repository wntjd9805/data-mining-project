Reinforcement learning problems in computer science often require the design of a distribution of tasks and environments to evaluate and train effective policies. However, designing an appropriate distribution of environments presents challenges due to the complexity of the real world and the impracticality of enumerating all relevant edge cases. In this paper, we propose an automated approach called Unsupervised Environment Design (UED) to address this issue. By supplying an underspecified environment with free parameters, our method constructs distributions of fully specified environments for training policies. We introduce the Protagonist Antagonist Induced Regret Environment Design (PAIRED) algorithm, which combines an environment-generating adversary and an antagonist agent to create challenging yet solvable environments for the protagonist agent. Our results demonstrate that PAIRED outperforms existing approaches in generating complex behaviors and achieving high zero-shot transfer performance in novel environments. This paper formalizes the problem of UED, presents the PAIRED algorithm, showcases its effectiveness, and connects the framework to classical decision theory.