In recent years, there has been a growing demand for deep learning (DL), leading to the development of DL frameworks such as Caffe2, MXNet, PyTorch, and TensorFlow. These frameworks allow users to easily implement GPU-based neural network computations using high-level APIs. However, current DL frameworks face two major challenges in runtime task scheduling: scheduling overhead and lack of inter-operator parallelism. To address these challenges, this paper introduces Nimble, a new DL execution engine that schedules GPU tasks to run in parallel with minimal scheduling overhead. Nimble achieves this through an ahead-of-time (AoT) scheduling technique that leverages detailed information about the computation graph and input shape. Additionally, Nimble employs automatic multi-stream execution to execute multiple GPU tasks in parallel. Experimental results show that Nimble significantly improves the speed of inference and training compared to PyTorch, as well as outperforms state-of-the-art inference systems.