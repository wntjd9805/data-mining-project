Powerful deep neural networks (DNNs) often require significant computational resources, making them impractical for resource-constrained devices. This has led to efforts to design hardware-friendly deep networks that reduce the reliance on computationally expensive multiplication operations. Previous approaches, such as ShiftNet, DeepShift, and AdderNet, have demonstrated the feasibility of using alternative operations like spatial shift, bit-wise shift, and addition in place of multiplications. In this paper, we go further by exploring the idea of using additions and logical bit-shifts as a replacement for multiplications, inspired by hardware design practices. We present a new deep model called ShiftAddNet, which replaces convolutional and fully-connected layers with bit-shift and add layers, resulting in energy-efficient inference and training algorithms. We show that ShiftAddNet offers expressive capabilities comparable to standard DNNs while achieving significant energy savings. Our contributions include the development of training and inference algorithms for ShiftAddNet, as well as extensive experiments demonstrating its compactness, accuracy, efficiency, and robustness.