The introduction of novel architectures for neural machine translation (NMT) has greatly advanced the field. While the self-attention method has set the standard for NMT, non-intrusive extensions, such as using sub-word units and exploiting monolingual data, offer a practical approach to enhance existing models without extensive changes. In this paper, we propose Data Diversification, a simple yet effective method that involves training multiple models on both backward and forward translation tasks to generate diverse synthetic training data. Our method achieves state-of-the-art performance in various translation tasks, outperforming other related methods and demonstrating its compatibility with ensembles of models.