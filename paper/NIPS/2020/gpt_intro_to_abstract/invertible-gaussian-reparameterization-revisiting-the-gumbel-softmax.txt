This paper introduces an alternative family of distributions called Invertible Gaussian Reparameterization (IGR) for achieving relaxation in machine learning tasks involving optimization problems over discrete stochastic components. The authors propose that IGR is a more natural, flexible, and easily extendable method compared to the widely used Gumbel-Softmax (GS) distribution. They demonstrate that IGR enables the use of the reparameterization trick on distributions with countably infinite support, allows nonparametric uses, and provides closed form KL divergence evaluation. Furthermore, experimental results show that IGR outperforms GS in various settings.