The remarkable success of neural networks has generated significant interest in understanding their behavior. While previous papers have proven the learnability of linear models, which can be realized by a linear classifier on top of a fixed embedding, these results do not explain the superior performance of neural networks in practice. In this paper, we explore how far neural network theory can extend beyond linear models by demonstrating the small error achieved by neural networks trained with gradient descent on a specific family of distributions. We show that this family cannot be approximated by any polynomial-size linear model, highlighting the superior capability of neural networks compared to linear methods.