In this paper, we address the issue of computational efficiency and overthinking in pretrained language models (PLMs) in the field of Natural Language Processing (NLP). We propose a novel Patience-based Early Exit (PABEE) mechanism that allows models to dynamically stop inference. Our approach incorporates an internal classifier with each layer of a PLM and stops inference when the intermediate predictions of the internal classifiers remain unchanged for a predefined patience value. We demonstrate that our method improves accuracy compared to conventional inference and outperforms existing prediction probability distribution-based exit criteria. Additionally, PABEE can enhance inference speed and adversarial robustness without significant additional effort in terms of model size and training time. We validate our approach using the GLUE benchmark and also conduct experiments on image classification tasks to verify its generalization ability. Our contributions include the proposal of PABEE as an effective inference mechanism and empirical results showcasing its ability to improve efficiency and accuracy in various tasks.