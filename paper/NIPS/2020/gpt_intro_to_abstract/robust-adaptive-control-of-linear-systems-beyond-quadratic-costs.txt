Reinforcement learning has shown significant success in various domains, but its application in real industrial problems is limited due to two key issues: the need for a large amount of interaction data that cannot always be simulated, and reliance on trial-and-error and random exploration. In this paper, we address these challenges by considering the control of an unknown linear system to maximize a bounded reward function in a critical setting where mistakes must be avoided at all costs. We assume partial knowledge of the system dynamics and propose a data-driven approach to estimate the parameters. We also introduce a robust decision-making framework that takes into account uncertainty in the model, and develop a robust Model Predictive Control (MPC) algorithm to solve the control objective. We further extend the framework to accommodate multiple modeling assumptions and demonstrate its applicability in numerical experiments.