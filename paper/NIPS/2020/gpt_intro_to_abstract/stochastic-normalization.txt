Training deep networks from scratch requires large amounts of data, but collecting such data is challenging. Pre-trained deep networks provide a solution to this problem, allowing practitioners to benefit from deep learning even with small amounts of data. However, fine-tuning pre-trained networks on small datasets often leads to overfitting. To address this, the authors propose StochNorm, a two-branch module that refactors the Batch Normalization (BN) layer commonly used in deep networks. StochNorm introduces explicit and implicit regularization techniques to mitigate overfitting by using mini-batch statistics and pre-trained moving statistics, respectively. The authors compare StochNorm with existing fine-tuning methods, showing its effectiveness in combating overfitting with limited data and its potential complementarity with other methods.