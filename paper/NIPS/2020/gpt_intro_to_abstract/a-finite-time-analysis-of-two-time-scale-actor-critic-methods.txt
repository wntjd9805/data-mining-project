Actor-Critic (AC) methods combine actor-only and critic-only methods in reinforcement learning, and have achieved success in various applications. Actor-only methods optimize a parameterized policy function using gradient ascent, while critic-only methods focus on learning a value function based on the Bellman equation. By combining these methods, the actor can estimate the policy gradient based on the approximate value function provided by the critic. The two time-scale actor-critic algorithm updates the actor and critic simultaneously, with the actor changing more slowly than the critic. While asymptotic convergence has been analyzed, a finite-time analysis is missing in the literature. In this paper, we provide the first finite-time analysis of the two time-scale actor-critic algorithm, showing convergence to an approximate stationary point of the non-concave performance function. We also present a new proof framework that characterizes the estimation error more tightly, removing unnecessary factors introduced by previous techniques.