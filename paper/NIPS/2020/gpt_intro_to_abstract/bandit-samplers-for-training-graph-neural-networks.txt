Graph neural networks have become a powerful tool for representation learning of graph data in irregular or non-euclidean domains. These networks have been successful in various learning tasks and applications, but they face challenges in terms of heavy floating point operations and large memory footprints. In this paper, we propose novel variance reduced samplers for training graph neural networks and attentive graph neural networks. We formulate the optimization of the samplers as a bandit problem and derive two bandit algorithms. We demonstrate through theoretical analysis and empirical experiments that our approach outperforms existing methods in terms of convergence and sample variance.