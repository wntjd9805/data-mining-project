Reinforcement Learning (RL) agents are achieving increasing success on a growing range of tasks. However, the focus has largely been on single-agent environments and fully-cooperative games. This paper explores the challenge of multi-agent cooperation in a decentralized training setting, where agents have misaligned objectives. The authors propose a new approach that allows agents to learn incentive functions, which can shape the behavior of other agents and optimize their own extrinsic objectives. The concept is illustrated with the example of an Escape Room game, where standard RL approaches fail to achieve positive rewards. The paper outlines the contributions of this work, including the creation of an agent that learns incentive functions, the derivation of gradients for updating incentive functions and policies, the demonstration of convergence to mutual cooperation in a matrix game, and the success of the agent in solving complex social dilemma problems.