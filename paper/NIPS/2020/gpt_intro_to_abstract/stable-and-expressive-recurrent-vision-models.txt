In this paper, we address the memory bottleneck issue faced by recurrent Convolutional Neural Networks (CNNs) trained using the "back-propagation through time" (BPTT) algorithm. We propose a solution inspired by memory-efficient approximations to BPTT and develop a learning algorithm called "Contractor-RBP" (C-RBP) that achieves constant memory complexity. Our approach combines the stability of convergent dynamical systems with model expressivity. We introduce a constraint called the Lipschitz-Constant Penalty (LCP) to train recurrent CNNs to be both stable and expressive. Experimental results show that recurrent CNNs trained with C-RBP learn difficult visual tasks, generalize better, and require fewer parameters compared to BPTT-trained models. Additionally, our C-RBP trained model outperforms feedforward approaches on the MS-COCO Panoptic Segmentation challenge while staying within the memory capacity of a standard GPU.