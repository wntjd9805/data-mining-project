Hypothesis testing of discrete distributions is widely used in data-based decision making and machine learning algorithms. The goal of a goodness-of-fit test is to determine whether an unknown distribution fits a known reference distribution. In this paper, we focus on measuring the distance between distributions using the L1 or L2 norm and determining the optimal separation rate between the two distributions. We also investigate how the constraint of local differential privacy affects the optimal separation rate. We consider a range of privacy mechanisms, including non-interactive and interactive mechanisms, and provide efficient and statistically optimal test procedures. Our results show that interactive mechanisms can achieve faster rates than non-interactive mechanisms. We provide optimal rates for general discrete distributions, including those with polynomial or exponentially decreasing tails, and prove the optimality of our separation rates in most cases. All results are valid with a finite number of samples.