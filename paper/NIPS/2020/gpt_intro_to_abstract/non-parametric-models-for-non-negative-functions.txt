Linear models have been highly effective in supervised and unsupervised learning problems, thanks to their richness and flexibility. However, there are applications where it is necessary to learn functions with constrained outputs, such as non-negativity or convexity. In this paper, we propose a class of models that have non-negative or convex outputs and share the key properties of linear models. These models can be used in empirical risk minimization with convex risks and can be evaluated, differentiated, and integrated easily. We also provide a representer theorem and a convex finite-dimensional dual formulation for the learning problem. We demonstrate that our model is a universal approximator and outperforms commonly used generalized linear models. Additionally, we present algorithms for density estimation, regression with Gaussian heteroscedastic errors, and multiple quantile regression, and compare their performance with standard techniques through simulations.