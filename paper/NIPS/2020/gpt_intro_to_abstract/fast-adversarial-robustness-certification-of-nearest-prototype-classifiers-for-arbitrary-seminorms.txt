The paper discusses the problem of adversarial robustness in classifiers and the limitations of current defenses against adversarial attacks. The authors propose extending the study of robustness guarantees to Nearest Prototype Classifiers (NPCs), which are considered interpretable machine learning models. They analyze the adversarial robustness properties of NPCs in terms of the hypothesis margin and show that it can be computed in constant time during inference. They also prove that this margin is a tight lower bound on the magnitude of an adversarial attack, making it the first robustness guarantee that holds for an arbitrary seminorm. Experimental results demonstrate the effectiveness of the derived robustness certificate compared to other methods, particularly in terms of computation speed. The paper concludes with a discussion and outlook on the presented results.