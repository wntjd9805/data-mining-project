Few-shot learning is a difficult problem in computer science due to limited training samples. Meta-learning offers potential solutions by training an algorithm that can adapt quickly to new tasks with few training data. The support/query episodic training strategy, commonly used in meta-algorithms like MAML and ProtoNet, has been shown to improve generalization. However, there is a lack of theoretical analysis on how this strategy impacts generalization. In this paper, we study the generalization error bounds of meta-algorithms trained with the support/query scheme using stability analysis. We derive a generalization bound that is independent of the sample size of each task, which may seem counterintuitive but can be explained by the difference between support/query training and traditional meta-training strategies. We also introduce a new training strategy, leave-one-out, and compare it with support/query training. Our results provide theoretical support for the generalization of modern meta-learning algorithms in few-shot learning settings. Experimental results further validate our findings.