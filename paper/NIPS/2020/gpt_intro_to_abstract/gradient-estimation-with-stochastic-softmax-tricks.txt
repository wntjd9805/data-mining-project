This paper addresses the challenge of computing gradients in deep learning by introducing a relaxed gradient estimation framework for discrete distributions, with a focus on latent variable models. The authors propose stochastic softmax tricks (SSTs) as a unified framework for designing structured relaxations of combinatorial distributions. These relaxations aim to improve interpretability, incorporate problem-specific constraints, and enhance generalization in deep learning models. The authors demonstrate the effectiveness of their approach in the Neural Relational Inference (NRI) and L2X frameworks, showing improved performance and interpretability compared to baselines.