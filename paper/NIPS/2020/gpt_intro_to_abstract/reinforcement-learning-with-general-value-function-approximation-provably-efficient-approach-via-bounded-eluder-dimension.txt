In this paper, we study reinforcement learning (RL) and its applications in various domains. RL involves an agent interacting with an unknown environment to maximize cumulative rewards. The environment is modeled as a Markov decision process (MDP), and the agent's goal is to collect sufficient information to act optimally. The performance of the agent is measured by regret, the difference between its rewards and those of an optimal agent. While RL algorithms with function approximation using deep neural networks (DNNs) have achieved success, there are no theoretical guarantees. We address this by developing a computationally and statistically efficient Q-learning algorithm that works with general value function approximators, without the need for feature extractors. Our theoretical guarantees justify the effectiveness of practical algorithms like deep Q-learning, and our algorithm provides comparable regret bounds to linear RL algorithms.