Recent years have seen significant advancements in reinforcement learning (RL) in various applications, such as strategy games, Go, autonomous driving, and security. These successes have led to an increased interest in multi-agent RL (MARL), particularly in terms of theoretical research. MARL involves multiple decision-makers with potentially conflicting objectives, making it challenging to develop effective algorithms. One approach is model-based MARL, which estimates an empirical model using data and then finds optimal policies using planning. However, there are few rigorous theoretical justifications for these methods. In this paper, we focus on two-player discounted zero-sum Markov games and investigate the performance of the model-based MARL approach in terms of sample complexity. We show that this approach can achieve near-optimal sample complexity for finding Nash equilibrium (NE) policies. Our results provide theoretical support for the efficiency of the model-based approach in MARL.