Learning good image representations is a crucial challenge in computer vision, as it enables efficient training on downstream tasks. Various training approaches have been proposed, with contrastive methods being among the most effective. These methods rely on reducing the distance between representations of augmented views from the same image (positive pairs) and increasing the distance between representations of augmented views from different images (negative pairs). However, handling negative pairs poses challenges, requiring large batch sizes, memory banks, or customized mining strategies. Additionally, the effectiveness of contrastive methods heavily depends on the choice of image augmentations. In this paper, we introduce Bootstrap Your Own Latent (BYOL), a novel algorithm for self-supervised learning of image representations. BYOL achieves superior performance compared to state-of-the-art contrastive methods, without the need for negative pairs. It employs iterative bootstrapping of network outputs as targets for enhanced representation learning. Moreover, BYOL exhibits improved robustness to the choice of image augmentations, potentially attributed to the absence of negative pairs. We evaluate BYOL on ImageNet and other vision benchmarks, demonstrating its superior performance under different evaluation protocols. Our contributions include introducing BYOL as a self-supervised representation learning method, showcasing its state-of-the-art performance on ImageNet, and demonstrating its resilience to changes in batch size and image augmentations.