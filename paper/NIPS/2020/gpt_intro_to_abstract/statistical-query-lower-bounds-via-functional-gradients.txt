This paper explores the computational complexity of fitting a single nonlinear activation to a joint distribution on Rn Ã— R in the context of deep learning. The problem is referred to as ReLU regression for the ReLU activation and agnostically learning a ReLU, and as learning a halfspace for the sign activation with Boolean labels. The paper provides statistical-query lower bounds for learning broad classes of nonlinear activations, considering the simplest setting when the marginal distribution is Gaussian. The results include exponential lower bounds for ReLU regression, sigmoid activation, and halfspaces. These bounds hold for general statistical queries and have implications for learning monomials and Lipschitz functions. The approach of the paper involves a reduction-based approach and the use of functional gradient descent via the Frank-Wolfe method. The results highlight the relationship between agnostically learnable concept classes and the learnability of monotone, Lipschitz functions.