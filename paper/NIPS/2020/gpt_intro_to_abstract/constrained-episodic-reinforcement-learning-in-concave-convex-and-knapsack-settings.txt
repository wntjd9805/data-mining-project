Standard reinforcement learning (RL) approaches focus on maximizing a scalar reward, but in many cases, this is inadequate as the desired properties of agent behavior can be better described using constraints. For instance, autonomous vehicles should not only reach their destination but also adhere to safety, fuel efficiency, and human comfort constraints. Similarly, robots should not only fulfill their tasks but also limit wear and tear by controlling motor torque. In this paper, we examine constrained episodic reinforcement learning, which encompasses various applications. Our approach emphasizes efficient exploration, leading to reduced sample complexity. We introduce extensions for maximizing concave objectives under convex constraints and addressing reinforcement learning under hard constraints. Our approach leverages the principle of optimism under uncertainty to explore efficiently, and we provide the first regret guarantees in the episodic setting for concave-convex and knapsack settings. We also present empirical comparisons with previous works and showcase significant improvements achieved through our algorithmic innovations.