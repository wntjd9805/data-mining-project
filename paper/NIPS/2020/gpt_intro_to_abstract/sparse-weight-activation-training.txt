Convolutional Neural Networks (CNNs) are widely used in computer vision tasks such as object recognition, detection, and image restoration. However, training CNNs requires significant computational and memory resources. To address this challenge, both software and hardware approaches have been proposed. This paper introduces Sparse Weight Activation Training (SWAT), a technique that extends the sparsification approach and significantly reduces training time by introducing sparsity in both the forward and backward passes. SWAT is suitable for emerging hardware platforms with support for sparse matrix operations, such as the recently announced Ampere GPU architecture. The paper presents an empirical sensitivity analysis of sparsity induction approaches, introduces the SWAT training algorithm, and provides an empirical evaluation demonstrating its effectiveness on complex models and datasets.