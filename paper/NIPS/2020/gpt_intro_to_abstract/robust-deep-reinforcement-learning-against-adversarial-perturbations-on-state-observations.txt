Deep reinforcement learning (DRL) has seen considerable success in solving complex tasks using deep neural networks (DNNs) as function approximators. However, the presence of adversarial examples in DNNs and successful attacks on DRL algorithms raise concerns about their robustness. This paper focuses on studying robust DRL algorithms that can handle uncertainty in state observations due to sensor errors or equipment inaccuracy. The authors propose a modified Markov decision process (MDP) called state-adversarial MDP (SA-MDP) to model perturbations in state observations and analyze its fundamental properties. They also introduce a robust policy regularizer based on the total variation distance or KL-divergence to improve robustness in RL algorithms. Experimental results demonstrate the effectiveness of the proposed method in enhancing robustness under strong attacks on state observations.