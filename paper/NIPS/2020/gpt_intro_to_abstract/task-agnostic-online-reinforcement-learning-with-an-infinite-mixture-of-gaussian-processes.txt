This paper introduces a model-based reinforcement learning (RL) method for solving nonstationary online problems with unknown task boundaries and the number of tasks. The proposed method utilizes an infinite mixture to model system dynamics, with each type of dynamics represented by a Gaussian Process (GP). The use of GPs allows for fast adaptation to new tasks without the need for a pre-trained model. The method also incorporates sequential variational inference and a transition prior based on the Dirichlet Process to handle temporal dependencies of dynamics and improve task shift detection. The effectiveness of the approach is demonstrated in various non-stationary environments.