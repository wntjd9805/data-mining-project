Q-learning is a successful reinforcement learning algorithm used to find the optimal action-value function. However, it suffers from large overestimation of the action-value function, resulting in poor performance. To address this issue, double Q-learning was introduced, which uses two Q-estimators to continuously change the roles of estimating the maximum Q-function value and updating. Despite its empirical success, there is limited theoretical understanding of double Q-learning. This paper provides the first finite-time analysis of double Q-learning with both synchronous and asynchronous implementations. The results show that double Q-learning achieves accurate global optima with high probability and provide insights into its convergence rate compared to vanilla Q-learning. The analysis develops new techniques to handle the two random path updates in double Q-learning and provides mathematical bounds on the convergence.