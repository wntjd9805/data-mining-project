We introduce the problem of minimizing a functional over a parameterized family of probability measures. The measures are defined over a common ground set and are obtained through a push-forward mapping from a latent space. Traditional optimization algorithms, such as Stochastic Gradient Descent (SGD), are not suitable for this problem due to the complex optimization landscape induced by the neural-network mappings. Instead, we propose Sinkhorn Natural Gradient (SiNG), an algorithm that performs steepest descent using the Sinkhorn divergence as the metric. SiNG is invariant to reparameterization and can be computed efficiently. We derive the exact update rule for SiNG and provide an expression for the Sinkhorn information matrix. Our experiments demonstrate the superiority of SiNG compared to SGD-type solvers in terms of both efficacy and efficiency.