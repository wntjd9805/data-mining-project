Recent advancements in technology and regulations have enabled the collection of extensive data on sequential decision trajectories and associated rewards in various domains, such as healthcare and product recommendations. This presents unique opportunities for utilizing off-policy methods to improve sequential decision-making. However, off-policy policy evaluation (OPE) faces the challenge of estimating counterfactual rewards, which requires causal reasoning and assumptions of sequential ignorability. These assumptions are often violated in real-world OPE problems where the behavior policy is unknown and decisions depend on unlogged features correlated with future outcomes. In this paper, we propose a framework for quantifying the impact of unobserved confounders on OPE estimates and developing worst-case bounds on the performance of an evaluation policy. We demonstrate the impact of confounding on OPE accuracy and introduce a model for scenarios where unobserved confounders only directly affect a single decision. We develop bounds on expected cumulative rewards under this single-decision confounding model and provide empirical evidence of the effectiveness of our approach. Our findings highlight the importance of considering confounding in OPE and offer insights into the robustness of policy selection.