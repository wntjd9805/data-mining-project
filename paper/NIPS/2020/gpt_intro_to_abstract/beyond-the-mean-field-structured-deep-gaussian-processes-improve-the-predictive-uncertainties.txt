Gaussian Processes (GPs) are widely used in various applications due to their ability to learn distributions over unknown functions. However, standard GPs have limited flexibility and often require expert knowledge to come up with appropriate features or kernel functions. Deep Gaussian Processes (DGPs) offer a more flexible alternative by learning non-linear feature representations through GP cascades. While DGPs provide more flexibility, they require inference techniques such as Monte Carlo sampling or approximate inference due to the lack of closed-form solutions. In this paper, we propose a new class of variational families for DGPs that fulfill two important requirements: marginalization of global latent variables and capture of correlations between latent GP models. We introduce a novel inference scheme and demonstrate its performance in terms of accurate predictions and well-calibrated uncertainty estimates.