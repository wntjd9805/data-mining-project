Reinforcement learning algorithms have been proposed for high-risk applications such as improving treatments for type 1 diabetes and sepsis. Safe and/or Seldonian RL algorithms provide high-confidence guarantees that these applications will not cause undesirable behavior. However, existing safe RL algorithms assume that training data is free from anomalies, which is often not the case in real-world applications. This paper analyzes the robustness of Seldonian RL algorithms to data perturbations and proposes a new measure called Î±-security to quantify the robustness of the safety test component. The analysis reveals that current safety test mechanisms can be violated when even just one data point is corrupted, leading to the proposal of a more robust algorithm. Experimental results support the theoretical analysis, and the findings are applicable to scenarios requiring confidence intervals around importance sampling estimates. The work also addresses the broader interests of the community in safety definitions and limitations, as well as the study of robustness to data corruption attacks in high-confidence methods.