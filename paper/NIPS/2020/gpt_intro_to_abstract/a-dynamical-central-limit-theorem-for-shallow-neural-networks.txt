Theoretical analyses of neural networks aim to understand their computational and statistical advantages seen in practice. This paper focuses on the interplay between the deviations from the mean-field limit and the gradient flow dynamics of neural networks. The authors prove a dynamical Central Limit Theorem (CLT) to capture how the fluctuations away from the mean-field limit evolve as a function of 1/2-scale for all finite times. They also examine the long-time behavior of the fluctuations, showing that in certain scenarios, the fluctuations are controlled by the error of Monte-Carlo resampling from the limiting measure. The paper considers both unregularized and regularized cases, providing insights into the convergence and behavior of neural networks in these setups. Empirical results in a teacher-student model complement the asymptotic predictions.