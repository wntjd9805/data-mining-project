This paper aims to explore the learning of representations from multimodal experiences in a self-supervised manner without manual annotation. The considered modalities are visual, audio, and language, which are easily obtained from unlabelled videos. The goal is to create a versatile network that can handle any of the three modalities, respect their specific characteristics, enable comparison even when modalities are not seen together during training, and effectively process dynamic videos and static images. The network design involves embedding each modality into a vector space using backbone networks and constructing a modality embedding graph. The network is trained through self-supervised contrastive learning on unlabelled videos. The learned MultiModal Versatile (MMV) networks are evaluated on various downstream tasks, achieving state-of-the-art performance for self-supervised approaches and reducing the gap with supervised approaches. The contributions of this work involve investigating different modality embedding graphs, proposing a self-supervised training strategy, introducing a deflation approach for ingesting static images, and demonstrating the superiority of the learned representations in multiple tasks.