Reinforcement Learning (RL) has shown promising results in decision-making tasks, such as solving Atari games and Go. However, RL methods face the challenge of "curse-of-dimensionality." This paper aims to address the problem of learning the optimal Q-function in a data-efficient manner when it has a lower-dimensional structure. The authors propose a universal representation of the Q-function that allows for designing a data-efficient learning algorithm without requiring prior knowledge of features. They develop a novel spectral representation of the Q-function and introduce a parametric family of Q-functions parameterized by rank. They also propose a sample-efficient RL method that removes the dependence on the dimensions of the state and action spaces by exploiting the low-rank structure of the Q-function. This method significantly improves the sample complexity compared to existing approaches.