This paper introduces the concept of over-parametrized models in machine learning, specifically focusing on deep neural networks. These models have interesting statistical and computational properties, including highly expressive interpolation of training data and the ability for local minimizers to have good generalization properties. However, the challenge lies in designing algorithms that can avoid getting trapped in saddle-points during the training process. The paper analyzes two standard algorithms, perturbed stochastic gradient descent (PSGD) and stochastic cubic-regularized Newton's method (SCRN), and shows that under certain assumptions, both algorithms can effectively escape saddle-points and converge to local minimizers. Theoretical results are compared to existing empirical and theoretical works, and the best-known oracle complexity is obtained for escaping saddle-points using vanilla PSGD algorithm. The paper also presents results for the zeroth-order version of PSGD and discusses the potential benefits of using second-order methods, such as SCRN. Overall, the findings contribute to the understanding and improvement of optimization algorithms for over-parametrized machine learning models.