Network pruning is a technique that reduces the computational cost of inference without compromising accuracy. There are two branches of network pruning: unstructured pruning, which prunes individual weights, and structured pruning, which prunes neurons or channels. While unstructured pruning reduces more weights, it requires special hardware or libraries for speedup. This paper focuses on structured pruning at the neuron level, which can be implemented on common GPU/CPU devices. The authors propose a novel regularizer called polarization that separates pruned and preserved neurons more clearly, improving the reasonability of the pruning process. The effectiveness of polarization pruning is verified on CIFAR and ImageNet datasets, achieving state-of-the-art results.