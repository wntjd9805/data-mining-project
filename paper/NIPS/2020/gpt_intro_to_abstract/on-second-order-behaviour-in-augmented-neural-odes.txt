Residual Networks (ResNets) have been crucial for enhancing the capabilities of neural networks by allowing them to reach extreme depths. This paper introduces a new class of models called Neural Ordinary Differential Equations (NODEs), which consider the limit of the skip layers used in ResNets. NODEs naturally give rise to an ODE that can be optimized using black-box ODE solvers, making them suitable for learning and modeling complex systems with unknown dynamics. While many variants of NODEs have been proposed, none of them have explored second-order behaviors, despite the prevalence of second-order laws in scientific dynamical systems. To address this gap, the paper introduces Second Order Neural ODEs (SONODEs) and studies their optimization and properties. The study also extends the analysis to Augmented Neural ODEs (ANODEs) and compares SONODEs and ANODEs on various second-order dynamical systems. The results demonstrate that SONODEs have beneficial inductive biases in this setting. The paper focuses on low-dimensional physical systems with known analytic solutions to facilitate the analysis of these models. The code for the experiments is available online.