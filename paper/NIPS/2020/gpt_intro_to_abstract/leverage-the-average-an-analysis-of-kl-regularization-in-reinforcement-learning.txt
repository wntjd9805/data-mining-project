This paper introduces the use of Kullback-Leibler (KL) regularization in Reinforcement Learning (RL) algorithms. The authors build upon previous work on regularization by Bregman divergences and show that using KL regularization implicitly averages the successive estimates of the q-function in the approximate dynamic programming scheme. They provide a performance bound that exhibits a linear dependency on the time horizon, unlike traditional ADP algorithms. The authors also study the interplay between KL regularization and entropy regularization, and demonstrate the wide applicability of their framework to existing RL algorithms. They complement their analysis with empirical studies in a deep RL setting, considering the challenges posed by neural networks.