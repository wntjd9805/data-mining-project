Transfer learning is becoming increasingly important in addressing learning problems with limited data. One effective method for transfer learning involves the use of task-specific parameters and a common low-dimensional representation. This approach has seen success in computer vision and classical machine learning domains, as well as in other areas such as protein engineering. However, there is a lack of understanding of the statistical principles underlying transfer learning, particularly in terms of the number of samples needed to learn a shared feature representation and improve prediction on a new task. This paper presents a two-stage empirical risk minimization procedure for learning a new task with a shared representation and explores the sampling complexity of this procedure. The findings demonstrate that the excess risk of prediction scales based on the complexity of the shared representation, task-specific maps, and task diversity. The paper also introduces a problem-agnostic definition of task diversity and provides general-purpose bounds for learning task-specific structure and shared feature representation. The framework is applied to different multi-task learning models to obtain transfer learning guarantees.