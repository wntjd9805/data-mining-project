This paper focuses on the problem of continual learning, where a neural network model needs to learn a sequence of tasks without access to previous task data. The main challenge in continual learning is the phenomenon of "catastrophic forgetting," where the model's performance on older tasks degrades as it learns new tasks. This paper investigates the stability-plasticity dilemma and explores how different training regimes, such as dropout regularization, batch size, and learning rate, can affect catastrophic forgetting. The authors empirically demonstrate that simple techniques, like dropout and tuning learning rate schedules and batch sizes, can outperform more complex algorithms for continual learning. They also show that these techniques can be integrated with existing methods. Overall, this work highlights the importance of selecting appropriate hyperparameters and suggests that neural networks can be more effective for continual learning than previously believed.