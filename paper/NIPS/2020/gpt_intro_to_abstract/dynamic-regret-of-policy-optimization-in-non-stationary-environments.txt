Classical reinforcement learning (RL) literature often evaluates algorithms based on their performance compared to a fixed policy in hindsight. However, in modern RL problems with dynamic and non-stationary environments, comparing against a single policy is insufficient. This paper focuses on two examples of RL in non-stationary environments: continual RL and meta RL. The authors propose two model-free policy optimization algorithms, POWER and POWER++, and provide dynamic regret analysis for both algorithms. The regret bounds demonstrate the adaptive near-optimality of the algorithms in slow-changing environments. This work contributes to the study of generalizability of RL algorithms in non-stationary settings.