Reinforcement Learning (RL) is a key component of machine learning that goes beyond prediction and regression by incorporating sequential decision making. The core problem in RL is solving Bellman's equation iteratively using noisy samples. Stochastic Approximation (SA) algorithms are commonly used to solve such fixed-point equations, but existing literature only provides finite-sample convergence guarantees under certain conditions. In this paper, we address the question of whether we can derive finite-sample convergence guarantees for SA when the norm of contraction is arbitrary and the noise scales affinely with the squared-norm of the current iterate. We present novel techniques that allow us to derive finite-sample error bounds for SA under general norm contraction and handle unbounded noise. The effectiveness of our theoretical results is demonstrated through the V-trace algorithm for solving the policy evaluation problem in reinforcement learning. Additionally, our approach also recovers existing state-of-the-art error bounds for Q-learning.