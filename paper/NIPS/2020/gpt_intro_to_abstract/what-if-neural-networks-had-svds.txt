This paper introduces a novel algorithm called FastH, which aims to increase core utilization and reduce idle cores in the computation of matrix inversion in Neural Networks. The algorithm achieves this by increasing the degree of parallelization of an underlying matrix multiplication. FastH retains the same time complexity as the sequential algorithm from a previous technique, but reduces the number of sequential operations. Experimental results show that FastH is significantly faster than existing algorithms. The code for FastH is available on GitHub.