Attribution methods have gained popularity for explaining the behavior of Deep Neural Networks (DNNs), especially in domains like medical imaging and safety-critical tasks. However, recent work has shown that attribution methods are vulnerable to adversarial perturbations, which raises concerns about their reliability in high-stakes settings. In this paper, we investigate the vulnerability of attribution methods and propose Smooth Surface Regularization (SSR) as a technique to enhance the robustness of gradient-based attributions. We also propose a stochastic post-processing method as an alternative to SSR. Our experimental results on real data and large-scale models demonstrate the effectiveness of these techniques in mitigating attribution attacks and highlight the significance of model geometry in such attacks. Proofs, implementation details, and code availability are provided in the appendix.