Generative Adversarial Networks (GANs) have been successful in generating objects, but struggle with generating objects that satisfy specific structural constraints. In this paper, we propose Constrained Adversarial Networks (CANs) as an extension of GANs that can generate valid structures with high probability. CANs achieve this by penalizing the generator for generating invalid objects during training. We use semantic loss (SL) to implement this penalty term, which turns the constraints into a differentiable loss function. CANs can handle complex constraints, such as reachability on graphs, by embedding the candidate configurations in a compact space. The constraints are embedded directly into the generator, allowing for efficient sampling of high-quality structures. We also introduce the ability for CANs to dynamically switch constraints on and off during inference. Our empirical study demonstrates that CANs generate structures that are likely to be valid and coherent with the training data.