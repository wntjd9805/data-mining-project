This paper introduces BNP, a novel extension of Neural Process (NP), which is a class of stochastic processes defined by parametric neural networks. BNP utilizes the bootstrap technique to induce functional uncertainty in NP models. By constructing multiple resampled datasets and combining their predictions, BNP naturally introduces functional uncertainty. This approach offers several benefits, including robustness under model-data mismatch. Experimental results on various datasets demonstrate that BNP outperforms existing NP models, particularly in scenarios where the test data differs significantly from the training data.