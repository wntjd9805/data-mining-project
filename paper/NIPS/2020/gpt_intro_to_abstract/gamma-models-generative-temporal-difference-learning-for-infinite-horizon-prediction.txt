This paper introduces the concept of an infinite-horizon predictive model, named the -model, which combines the advantages of both model-based and model-free reinforcement learning. The -model predicts over an infinite horizon using a geometrically-distributed timestep weighting and can be trained with a generative analogue of temporal difference learning. The paper discusses the benefits of the -model, including constant-time prediction and the ability to reuse the model for new tasks within the same environment. The paper also explores the implications of probabilistic prediction horizons and the potential for fine-tuned interpolation and value estimation. Furthermore, the paper demonstrates that the precise timing of state encounters is not necessary for decision-making.