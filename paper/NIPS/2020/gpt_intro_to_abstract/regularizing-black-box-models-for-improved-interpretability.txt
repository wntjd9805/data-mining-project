Complex learning-based systems are becoming increasingly influential in our daily lives, necessitating the need for clear explanations of their behavior. Model interpretability, although having various definitions, is important for understanding and monitoring these systems. Local explanations have emerged as a popular tool in interpretability. This paper focuses on local explanations and proposes a strategy called Explanation-based Optimization (EXPO) that combines the advantages of interpretable models and post-hoc explanation systems. EXPO achieves this by adding an interpretability regularizer to the model's loss function, allowing for control over the trade-off between prediction accuracy and explanation quality. Unlike existing approaches, EXPO's regularizers are differentiable and model-agnostic, providing flexibility without explicit constraints on the model family. Empirical results show that EXPO improves explanation quality while slightly enhancing test accuracy, and a user study demonstrates that participants find EXPO-regularized models more useful. This work contributes a differentiable and model-agnostic interpretability regularizer and provides empirical evidence of its effectiveness.