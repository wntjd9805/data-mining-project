This paper introduces a new method called Implicit Rank-Minimizing Autoencoder (IRMAE) for minimizing the rank/dimensionality of the latent code in an autoencoder. The authors propose inserting extra linear layers between the encoder and decoder of a standard autoencoder, which are trained jointly with the rest of the autoencoder using backpropagation. This results in the system learning representations with a low effective dimensionality. The authors empirically demonstrate the regularization behavior of IRMAE on a synthetic dataset and show its superior performance compared to a standard deterministic autoencoder and comparable performance to a variational autoencoder on various generative tasks and a downstream classification task.