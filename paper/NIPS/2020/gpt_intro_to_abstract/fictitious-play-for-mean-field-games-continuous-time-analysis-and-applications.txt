Learning in games has a long history, but learning in the midst of a large number of players remains a challenging problem. Although machine learning techniques such as Reinforcement Learning (RL) have achieved success in interactions with a handful of players, extending them to a large population is still out of reach. To address this, the concept of Mean Field Games (MFGs) has been introduced, inspired by the economic literature on games with a continuum of players. In MFGs, all players are identical, anonymous, and have symmetric interests, allowing for tractable models. Most existing MFG literature assumes the representative player to be fully informed about the game dynamics, but this assumption limits scalability. To overcome this limitation, stochastic methods based on neural network approximations have been proposed. In this paper, we present a generic and scalable simulation-based learning algorithm that computes approximate Nash equilibria for MFGs, building upon the Fictitious Play scheme. We analyze the convergence of Fictitious Play for MFGs, provide convergence results for finite horizon and discounted monotone MFGs, and also consider MFGs with common noise. Our analysis introduces the notion of exploitability as a metric for evaluating the convergence towards a Nash equilibrium, and we demonstrate the performance of our algorithm through empirical experiments on various MFG settings.