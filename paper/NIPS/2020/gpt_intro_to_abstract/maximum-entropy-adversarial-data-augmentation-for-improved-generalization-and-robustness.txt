Deep neural networks can achieve good performance when trained and tested on data from the same distribution. However, in practice, mismatches between training and testing domains, small corruptions to data distributions, and adversarial attacks can cause significant performance degradation. Adversarial data augmentation has been proposed as a way to improve model robustness by generating fictitious target distributions that resemble unforeseen data shifts. However, existing methods are insufficient in synthesizing large data shifts. To address this issue, we propose a regularization technique based on the Information Bottleneck principle. We aim to maximize the mutual information between the input and latent distribution to generate "hard" adversarial perturbations. We develop an efficient maximum-entropy regularizer and show its improvement over existing methods on standard benchmarks.