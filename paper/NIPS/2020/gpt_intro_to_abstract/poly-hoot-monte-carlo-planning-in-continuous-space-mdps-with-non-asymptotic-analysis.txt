Monte-Carlo tree search (MCTS) has been successful in deterministic games, but lacks compatibility with continuous action spaces. This paper introduces a new MCTS method for continuous domains that guarantees non-asymptotic convergence. The analysis of MCTS in finite spaces is challenging due to non-stationary value estimates and rewards. Additionally, finding a balance between fine-grained samples and accurate estimations in continuous domains is difficult. The proposed algorithm integrates MCTS with a continuous-armed bandit strategy and adaptively partitions the action space. The new algorithm, named POLY-HOOT, has theoretical guarantees of convergence to an optimal solution at a polynomial rate. The enhanced bandit strategy and algorithm design contribute to the field of bandit problems and continuous space MDPs, providing theoretical justifications and improved performance compared to existing methods.