The modern machine learning paradigm is shifting towards parallelization and decentralization to improve training speed and address time-sensitive real-world problems. Distributed learning algorithms implemented in parameter-server frameworks have been developed to make use of large-scale computing units. However, two major drawbacks have been identified: Byzantine attacks and communication burden. Byzantine attacks can mislead the model updating process, while the high communication burden arises from transmitting gradient vectors between the parameter-server and workers. This paper proposes a coding-theoretic framework called Election Coding to enhance the robustness of the signed stochastic gradient descent method with majority vote (SIGNSGD-MV) to Byzantine attacks. The framework uses two coding schemes (random Bernoulli codes and deterministic algebraic codes) to estimate the majority vote accurately. The proposed algorithms are implemented and tested on machine learning architectures, demonstrating their superior tolerance to Byzantine attacks compared to conventional uncoded methods.