Deep learning has been successful in solving complex tasks and has shown similarities to the brain's learning processes. However, there are debates regarding the extent of similarity between the backpropagation algorithm used in deep learning and biological learning algorithms. Backpropagation has certain features, such as the requirement for feedback weights to be tied to feedforward weights, which may not directly correspond to biological implementations. Additionally, backpropagation utilizes derivatives of forward-pass nonlinearities during the feedback pass, indicating that feedback pathways require knowledge of the state of feedforward pathways. The issue of credit assignment, or the communication of appropriate learning signals to neurons upstream of behavioral outputs, is still unresolved in biological circuits. This paper proposes a learning paradigm that addresses the credit assignment problem in a biologically plausible manner. The approach includes local plasticity rules for updating feedforward synaptic weights, feedback connections to propagate target output information to upstream neurons, and meta-learning to optimize feedback weights, feedforward weight initializations, and rates of synaptic plasticity. The results show that meta-learned deep networks can successfully perform useful weight updates in non-readout layers, and feedback with local learning rules can sometimes outperform gradient descent as a learning algorithm within an individual's lifetime.