Labeled data is often scarce in machine learning practice, leading to a need for methods that can combine and transfer knowledge across datasets and domains. The concept of distance between datasets is crucial in these paradigms, but quantifying it efficiently and in a principled manner remains an open problem. Current approaches often rely on heuristics or strong assumptions, lacking guarantees and scalability. In this work, we propose an alternative notion of distance using optimal transport distances, allowing for comparison of datasets with unrelated or disjoint label sets. Our approach has the potential for various applications and we provide algorithmic strategies for scaling up computation. Empirical evidence demonstrates the predictive power of our distance measure in transfer learning across domains, tasks, and data modalities.