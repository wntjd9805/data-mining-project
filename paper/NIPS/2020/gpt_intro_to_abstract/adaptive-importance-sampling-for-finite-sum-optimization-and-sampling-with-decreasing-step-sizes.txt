Functions of the form f : Rd â†’ R are prevalent in machine learning and statistics, and are often optimized or sampled using stochastic gradient descent or stochastic gradient Langevin dynamics. These methods rely on stochastic estimates of the gradient, which can be improved by reducing the variance. In this paper, we focus on importance sampling as a variance reduction technique. We propose an algorithm called Avare that achieves sub-linear dynamic regret for both SGD and SGLD. Our algorithm uses a new mini-batch estimator that combines sampling without replacement and importance sampling, and we empirically validate its performance. We argue that decreasing step sizes are suitable for variance reduction, as they converge to optimal solutions exponentially fast in the early stages and are useful at later stages when the noise from the stochastic gradient dominates.