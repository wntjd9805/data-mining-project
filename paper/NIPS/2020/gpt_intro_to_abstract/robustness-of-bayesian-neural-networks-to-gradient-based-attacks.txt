Adversarial attacks, small perturbations of test inputs that can lead to misclassifications in deep Neural Networks (NN), have raised concerns about the security and robustness of machine learning models. While Bayesian Neural Networks (BNNs) have been suggested as a more robust paradigm, the source of their robustness and its general applicability are not well understood mathematically. In this paper, we demonstrate a remarkable property of BNNs: in a large data limit, the gradients of the expected loss function of a BNN with respect to the input points vanish. Our analysis reveals that adversarial attacks for deterministic NNs arise from the low-dimensional support of the data generating distribution, while BNNs, by averaging over nuisance dimensions, achieve zero expected gradient of the loss and are thus immune to gradient-based attacks. We support our theoretical findings with experiments on various BNN architectures, showing decreased gradients with more samples from the BNN posterior. Additionally, we demonstrate the failure of popular gradient-based attack strategies on BNNs, and show that high accuracy in BNNs is correlated with high robustness to adversarial attacks. This paper provides a theoretical framework, proof, and empirical evidence for the adversarial robustness of BNNs.