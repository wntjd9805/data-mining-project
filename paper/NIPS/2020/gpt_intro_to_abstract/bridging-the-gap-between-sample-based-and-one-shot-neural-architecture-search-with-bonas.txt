Designing deep network architectures for different tasks and datasets is a cumbersome and time-consuming process. Neural architecture search (NAS) has emerged as a promising approach to automating this process, with competitive results achieved in various application areas such as natural language processing and computer vision. However, optimization in NAS is challenging due to the vast search space and the computational expense of evaluating architecture performance. This paper introduces BONAS, a sample-based NAS algorithm combined with weight-sharing, aiming to improve efficiency and overcome limitations of previous methods. BONAS leverages Bayesian optimization, graph convolutional networks, and a novel Bayesian sigmoid regressor to select candidate architectures in the search phase, and constructs a super-network for simultaneous evaluation of multiple architectures in the query phase. Experimental results demonstrate the effectiveness of BONAS in achieving competitive models efficiently across different benchmarks, bridging the gap between sample-based and one-shot NAS methods.