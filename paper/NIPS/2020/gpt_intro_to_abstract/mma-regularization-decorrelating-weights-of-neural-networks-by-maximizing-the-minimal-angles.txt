Neural networks often contain redundant neurons or filters, which can result in limited feature space directions and poor generalization performance. In this paper, we propose a method called MMA regularization that penalizes the minimum angles between pairwise weight vectors in each layer, aiming to promote angular diversity. We employ the Tammes problem, which seeks to maximize the minimum distance between points on a unit sphere, to model the criterion of uniformity. We develop a numerical optimization method to obtain approximate solutions for the Tammes problem and demonstrate that the MMA regularization is effective in improving the generalization performance of neural networks. Experimental results on various tasks support the efficacy of MMA regularization. Overall, this paper makes three main contributions: proposing a numerical method for the Tammes problem, introducing the MMA regularization method, and demonstrating its effectiveness through experiments.