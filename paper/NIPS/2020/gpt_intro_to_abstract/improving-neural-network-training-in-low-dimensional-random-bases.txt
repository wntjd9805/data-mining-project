Despite the success of Stochastic Gradient Descent (SGD) in finding minima in deep learning networks, recent evidence suggests that not all gradient directions are necessary for effective optimization. Methods have been developed to reduce model redundancy and improve computational efficiency. This paper revisits optimization in low-dimensional random subspaces and proposes a new approach that employs smaller independent random projections at each step. Experimental results show improved accuracy in various network architectures. The paper also includes a schematic illustration of random subspace optimization and presents comparison results with other optimization methods.