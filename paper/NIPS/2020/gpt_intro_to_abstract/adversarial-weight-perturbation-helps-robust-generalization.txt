Deep neural networks (DNNs) have achieved significant success in various fields, but their vulnerability to adversarial examples poses a significant challenge to their robustness. Adversarial Training (AT) has been proposed as an effective defense technique, incorporating adversarial examples into the training process. However, the robustness achieved by AT is still unsatisfactory, with a significant gap between training and test performance. In this paper, we explore the weight loss landscape under adversarial training using on-the-fly generated adversarial examples and establish a strong connection between the flatness of the weight loss landscape and the robust generalization gap. Motivated by this observation, we propose Adversarial Weight Perturbation (AWP), a regularization technique that explicitly controls the flatness of the weight loss landscape. AWP demonstrates consistent improvements in the adversarial robustness of state-of-the-art methods and can be easily incorporated into existing adversarial training approaches.