Knowledge Graphs (KGs) are widely used in knowledge-driven applications such as question answering, medical diagnosis, and recommendation systems. KGs consist of relational facts represented as triplets. To enhance downstream tasks, embedding-based methods have been developed to learn low-dimensional vector representations of entities and relations in KGs. However, existing models mainly focus on capturing short-term semantic information within individual triplets and overlook the information among multiple triplets. To address this limitation, relational paths, which capture the composition of relations and long-term dependency of triplets, are introduced. This paper proposes a novel approach called Interstellar, a recurrent network that leverages relational paths to learn both short-term and long-term information in KGs. The approach formulates the problem as a neural architecture search (NAS) problem and designs a domain-specific search space. A hybrid-search algorithm is introduced to efficiently search for the optimal recurrent architectures. Experimental results on entity alignment and link prediction tasks demonstrate the effectiveness of the searched models and the efficiency of the search algorithm.