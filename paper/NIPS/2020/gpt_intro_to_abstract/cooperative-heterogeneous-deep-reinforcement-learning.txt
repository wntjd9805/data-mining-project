Deep reinforcement learning (DRL) has shown great success in solving challenging real-world problems, but existing algorithms still face challenges such as sample complexity, instability, and temporal credit assignment problems. This paper introduces a framework called CHDRL that combines on-policy and off-policy methods with evolutionary algorithms to achieve high sample efficiency and stability in continuous control tasks. Three mechanisms are employed: hierarchical policy transfer, local-global memory replay, and loosely coupled hierarchical framework. Experimental results demonstrate the superiority of the proposed framework over state-of-the-art baselines. The paper also reviews the proximal policy optimization (PPO), soft actor-critic (SAC), and evolutionary algorithms, providing the necessary background for the proposed framework.