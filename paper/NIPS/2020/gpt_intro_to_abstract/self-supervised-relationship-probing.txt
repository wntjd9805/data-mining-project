Visual relationships in images, represented as scene graphs, are crucial for complex reasoning tasks in computer vision. However, current visual relationship models heavily rely on human-annotated labels, resulting in a limited representation of relationships. In contrast, natural language processing has made significant progress in contextualized language models using self-supervised pretraining objectives. This paper proposes a novel framework, called self-supervised relationship probing (SSRP), that discovers dependencies between objects in the model's representation space. The framework consists of three modules: modeling implicit intra-modal relationships, cross-modal learning, and explicit representation of relationships as latent variables. The modules are trained using self-supervision with masked language modeling and contrastive learning. The main contribution of this work is the SSRP framework, which addresses the limitations of existing visual relationship models and demonstrates improved performance on vision and vision-language understanding tasks.