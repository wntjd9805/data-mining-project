Reinforcement learning is a field that focuses on sequential decision making problems, where an agent interacts with an environment and aims to improve its strategy based on feedback. One important tradeoff in this field is the exploration vs. exploitation tradeoff, where the agent must balance between trying new strategies and exploiting those that are already known to be effective. One common form of partial feedback in reinforcement learning is the "bandit" feedback, where the agent only observes the cost of its chosen action without any information about the performance of other actions. This paper introduces the bandit linear control problem, where a learning agent controls a linear dynamical system under stochastic noise, convex cost functions, and bandit feedback. The paper presents an efficient bandit algorithm for strongly convex and smooth cost functions that achieves optimal regret rates. The algorithm is based on a reparameterization of the control problem and a reduction technique for addressing bandit convex optimization. The techniques can also be extended to weakly convex costs without the assumption of stochastic noise.