Visual object detection has made significant progress in recent years, mainly due to the advancement of Convolutional Neural Networks (CNNs). However, the process of obtaining precise bounding-box annotations for large-scale training data is time-consuming and labor-intensive. As a result, researchers have turned to weakly-supervised object detection (WSOD) which aims to train object detectors with only image-level category labels. Most previous methods for WSOD are based on Multiple Instance Learning (MIL). However, there is still a performance gap between weakly-supervised and fully-supervised detectors. This paper proposes a Comprehensive Attention Self-Distillation (CASD) approach for WSOD training. CASD balances feature learning among objects by computing comprehensive attention from multiple transformations and feature layers. It also enforces consistent spatial supervision through self-distillation on the WSOD network. CASD achieves state-of-the-art results on standard benchmarks and outperforms other methods.