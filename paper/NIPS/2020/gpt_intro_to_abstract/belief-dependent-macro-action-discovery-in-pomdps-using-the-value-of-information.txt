Partially observable Markov decision processes (POMDPs) pose challenges in planning under uncertainty, particularly in terms of the exponentially growing belief space and complexity in computing an optimal POMDP policy. Previous approaches have used macro-actions to reduce planning complexity, but they have limitations in terms of being hand-coded or learned without guarantees. In this paper, we propose a method for generating belief-dependent, variable-length macro-actions using a point-based representation of the POMDP value function. We introduce a value of information (VoI) function to selectively act open-loop when VoI is low, resulting in bounded regret compared to the optimal policy. We analyze the trade-offs in macro-action-based POMDP planning and present empirical results that demonstrate the effectiveness of our approach.