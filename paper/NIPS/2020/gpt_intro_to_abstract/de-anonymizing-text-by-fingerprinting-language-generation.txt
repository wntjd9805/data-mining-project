Machine learning models are widely used and can contain sensitive information, making them a potential target for security attacks. In this paper, we focus on the side-channel vulnerabilities in ML models using nucleus sampling as a case study. We first demonstrate that the series of nucleus sizes generated when generating English-language word sequences can be used as a fingerprint to identify the content. Next, we show that popular implementations of nucleus sampling have an information leak that allows an attacker to infer the nucleus size and fingerprint the input text. We propose a fingerprint matching algorithm that can accurately identify the typed sequence from billions of candidates. This technique can be used to de-anonymize text in various applications. We conclude by discussing mitigation strategies and the importance of secure coding practices in ML models. Responsible disclosure of our findings has been made to the relevant engineering team.