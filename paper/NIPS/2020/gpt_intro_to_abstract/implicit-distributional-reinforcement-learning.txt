Model-free reinforcement learning (RL) has gained significant attention in addressing complex real-world sequential decision making tasks. With the use of deep neural networks, model-free deep RL algorithms have been successfully implemented in various domains, including game playing and robotic control. This paper focuses on the distributional perspective for deep RL problems, which models the full distribution of the discounted cumulative return of a chosen action at a state, rather than just its expectation. Several algorithms have been proposed to capture the skewness and multimodality in state-action value distributions. However, there are discrepancies between theory and implementation, motivating the introduction of QR-DQN that incorporates quantile regression techniques. This paper integrates the distributional action-value learning framework with stochastic policy to leverage the strengths of both approaches. A deep generator network is used to model the distribution of the cumulative return, and a semi-implicit actor is introduced to capture complex distributional properties. A twin-delayed structure is proposed to mitigate the overestimation issue, and a novel solution is presented to stabilize the training process and improve performance. The contributions of this paper include incorporating the distributional idea with the stochastic policy setting, introducing the twin-delayed structure on deep generator networks, and improving the flexibility of the policy by using a semi-implicit actor.