Attention layers are widely used in various domains such as image synthesis, natural language processing, symbolic mathematics, and music modeling. However, attention layers have high computational and memory cost, which limits their scalability. Existing alternatives, such as sparse attention, have shown promise but still have limitations in terms of performance, strict assumptions, or efficiency. In this paper, we propose a novel approach called SMYRF that approximates attention using balanced clustering. We provide an efficient algorithm for solving the optimization problem of attention biclustering and demonstrate that SMYRF layers can be drop-in replacements for pre-trained models. Our experiments show that SMYRF attention layers offer significant improvements in performance, memory usage, and speed without the need for re-training. We also show that SMYRF can be easily interchanged with dense layers before and after training. Additionally, we demonstrate the scalability of SMYRF for GANs and provide code and pre-trained models for further research.