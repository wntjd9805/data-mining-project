This paper introduces a gradient-based meta-learning algorithm, called Continual-MAML (C-MAML), for efficient and online continual learning. The algorithm utilizes a replay buffer and optimizes a meta-objective to mitigate forgetting. A modification to C-MAML, named La-MAML, is proposed, which incorporates modulation of per-parameter learning rates (LRs) to pace the learning of a model across tasks and time. The scalability, robustness, and favorable performance of the algorithm are demonstrated on several benchmarks of varying complexity.