In recent years, supervised learning has made significant progress in real-world applications, relying on human annotations for training deep neural networks. However, supervised learning has limitations in capturing the rich structure of the underlying features and label acquisition can be time-consuming and expensive. In contrast, unsupervised learning aims to characterize the feature distribution without manual annotations. Contrastive learning, which uses a contrastive loss function to encourage invariant features, has become a core method in unsupervised learning. However, existing contrastive learning methods only penalize the incompatibility of each individual query-key pair. To address this, we propose Joint Contrastive Learning (JCL), which considers the dependencies among different query-key pairs to encourage similarity consistency within specific instances. Our contributions include introducing a novel loss formulation that allows for end-to-end training, providing theoretical interpretations of the loss, and demonstrating the advantages of JCL in feature learning and its performance on various benchmarks.