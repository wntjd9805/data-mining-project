Reinforcement learning (RL) is a field in computer science where an agent interacts with an unknown environment to maximize its cumulative reward. Exploration mechanisms are crucial in RL algorithms to effectively solve tasks with long horizons and sparse reward signals. While there have been empirical successes in combining deep RL methods with exploration strategies, the theoretical understanding of exploration in RL is limited. In this paper, we focus on the reward-free RL setting, where the agent explores the environment without a pre-specified reward function and then plans to find a near-optimal policy based on the collected data. We study RL with linear function approximation in the reward-free setting and provide both a provably efficient algorithm and a hardness result. Our results demonstrate the importance of feature encoding and highlight the potential exponential difficulty of reward-free RL compared to standard RL. We also explore the power of simulators in improving the sample complexity of reward-free RL. This work contributes to the theoretical understanding of exploration and planning in RL, particularly in the reward-free setting.