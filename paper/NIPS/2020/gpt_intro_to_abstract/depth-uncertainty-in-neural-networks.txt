Deep learning models have become widely adopted, but they often lack robust uncertainty estimates, which is crucial for real-world applications where training data may not match the distribution of observations. Existing methods for building uncertainty-aware neural networks have implementation complexity, high computational cost, and weak performance. In this paper, we propose Depth Uncertainty Networks (DUNs), which treat the depth of a neural network as a random variable to perform Bayesian Model Averaging. DUNs leverage the overparametrization of a single deep network to generate diverse explanations of the data. Our experiments show that DUNs offer implementation simplicity, cheap deployment, and calibrated uncertainty in predictive performance, out-of-distribution detection, and robustness to corruptions.