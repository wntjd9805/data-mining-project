The sampling of probability distributions speciﬁed up to a normalization constant is an important problem in both machine learning and statistical mechanics. While classical stochastic sampling methods such as Markov Chain Monte Carlo (MCMC) or Langevin Dynamics (LD) can suffer from slow mixing times there is a growing interest in using normalizing ﬂows in order to learn the transformation of a simple prior distribution to the given target distribution. Here we propose a generalized and combined approach to sample target densities: Stochastic Normalizing Flows (SNF) – an arbitrary sequence of deterministic invertible functions and stochastic sampling blocks. We show that stochasticity overcomes expressivity limitations of normalizing ﬂows resulting from the invertibility constraint, whereas trainable transformations between sampling steps improve efﬁciency of pure MCMC/LD along the ﬂow. By invoking ideas from non-equilibrium statistical mechanics we derive an efﬁcient training procedure by which both the sampler’s and theﬂow’s parameters can be optimized end-to-end, and by which we can compute exact importance weights without having to marginalize out the randomness of the stochastic blocks. We illustrate the representational power, sampling efﬁciency and asymptotic correctness of SNFs on several benchmarks including applications to sampling molecular systems in equilibrium. 