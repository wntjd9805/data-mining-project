Classical optimal transport problem seeks a transportation map that preserves the total mass between two probability distributions, requiring their masses to be equal.This may be too restrictive in some applications such as color or shape matching, since the distributions may have arbitrary masses and/or only a fraction of the total mass has to be transported. In this paper, we address the partial Wasserstein andGromov-Wasserstein problems and propose exact algorithms to solve them. We showcase the new formulation in a positive-unlabeled (PU) learning application.To the best of our knowledge, this is the ﬁrst application of optimal transport in this context and we ﬁrst highlight that partial Wasserstein-based metrics prove effective in usual PU learning settings. We then demonstrate that partial Gromov-Wasserstein metrics are efﬁcient in scenarii in which the samples from the positive and the unlabeled datasets come from different domains or have different features. 