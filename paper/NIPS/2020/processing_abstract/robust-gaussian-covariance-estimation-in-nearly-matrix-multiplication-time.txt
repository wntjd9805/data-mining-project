Robust covariance estimation is the following, well-studied problem in high di-mensional statistics: given N samples from a d-dimensional Gaussian N (0, Σ), but where an ε-fraction of the samples have been arbitrarily corrupted, out-put (cid:98)Σ minimizing the total variation distance between N (0, Σ) and N (0, (cid:98)Σ).This corresponds to learning Σ in a natural afﬁne-invariant variant of the Frobe-nius norm known as the Mahalanobis norm. Previous work of [CDGW19] demonstrated an algorithm that, given N = Ω(d2/ε2) samples, achieved a near-optimal error of O(ε log 1/ε), and moreover, their algorithm ran in time (cid:101)O(T (N, d) log κ/ poly(ε)), where T (N, d) is the time it takes to multiply a d × N matrix by its transpose, and κ is the condition number of Σ. When ε is rela-tively small, their polynomial dependence on 1/ε in the runtime is prohibitively large. In this paper, we demonstrate a novel algorithm which achieves the same statistical guarantees, but which runs in time (cid:101)O(T (N, d) log κ). In particular our runtime has no dependence on ε. When Σ is reasonably conditioned, our runtime matches that of the fastest algorithm for covariance estimation without outliers, up to poly-logarithmic factors, showing that we can get robustness essentially “for free.” 