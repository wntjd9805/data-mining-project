Bayesian optimization (BO) is a popular framework for optimizing black-box functions. In many applications, the objective function can be evaluated at mul-tiple ﬁdelities to enable a trade-off between the cost and accuracy. To reduce the optimization cost, many multi-ﬁdelity BO methods have been proposed. Despite their success, these methods either ignore or over-simplify the strong, complex correlations across the ﬁdelities. While the acquisition function is therefore easy and convenient to calculate, these methods can be inefﬁcient in estimating the objective function. To address this issue, we propose Deep Neural Network Multi-Fidelity Bayesian Optimization (DNN-MFBO) that can ﬂexibly capture all kinds of complicated relationships between the ﬁdelities to improve the objective function es-timation and hence the optimization performance. We use sequential, ﬁdelity-wiseGauss-Hermite quadrature and moment-matching to compute a mutual information based acquisition function in a tractable and highly efﬁcient way. We show the advantages of our method in both synthetic benchmark datasets and real-world applications in engineering design. 