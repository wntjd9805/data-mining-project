The search for efﬁcient, sparse deep neural network models is most prominently performed by pruning: training a dense, overparameterized network and removing parameters, usually via following a manually-crafted heuristic. Additionally, the recent Lottery Ticket Hypothesis conjectures that, for a typically-sized neural net-work, it is possible to ﬁnd small sub-networks which, when trained from scratch on a comparable budget, match the performance of the original dense counterpart. We revisit fundamental aspects of pruning algorithms, pointing out missing ingredients in previous approaches, and develop a method, Continuous Sparsiﬁcation, which searches for sparse networks based on a novel approximation of an intractable (cid:96)0 regularization. We compare against dominant heuristic-based methods on pruning as well as ticket search – ﬁnding sparse subnetworks that can be successfully re-trained from an early iterate. Empirical results show that we surpass the state-of-the-art for both objectives, across models and datasets, including VGG trained onCIFAR-10 and ResNet-50 trained on ImageNet. In addition to setting a new stan-dard for pruning, Continuous Sparsiﬁcation also offers fast parallel ticket search, opening doors to new applications of the Lottery Ticket Hypothesis. 