Despite being the workhorse of deep learning, the backpropagation algorithm is no panacea. It enforces sequential layer updates, thus preventing efﬁcient paral-lelization of the training process. Furthermore, its biological plausibility is being challenged. Alternative schemes have been devised; yet, under the constraint of synaptic asymmetry, none have scaled to modern deep learning tasks and architec-tures. Here, we challenge this perspective, and study the applicability of DirectFeedback Alignment (DFA) to neural view synthesis, recommender systems, geo-metric learning, and natural language processing. In contrast with previous studies limited to computer vision tasks, our ﬁndings show that it successfully trains a large range of state-of-the-art deep learning architectures, with performance close toﬁne-tuned backpropagation. When a larger gap between DFA and backpropagation exists, like in Transformers, we attribute this to a need to rethink common practices for large and complex architectures. At variance with common beliefs, our work supports that challenging tasks can be tackled in the absence of weight transport. 