The current paper studies the problem of agnostic Q-learning with function approx-imation in deterministic systems where the optimal Q-function is approximable by a function in the class F with approximation error δ ≥ 0. We propose a novel recursion-based algorithm and show that if δ = O (cid:0)ρ/ (cid:1), then one can ﬁnd the optimal policy using O(dimE) trajectories, where ρ is the gap between the optimal Q-value of the best actions and that of the second-best actions and dimE is the Eluder dimension of F. Our result has two implications: dimE√ 1. In conjunction with the lower bound in [Du et al., 2020], our upper bound (cid:1) is necessary and sufﬁcient for suggests that the condition δ = (cid:101)Θ (cid:0)ρ/ algorithms with polynomial sample complexity. dimE√ 2. In conjunction with the obvious lower bound in the tabular case, our upper bound suggests that the sample complexity (cid:101)Θ (dimE) is tight in the agnostic setting.Therefore, we help address the open problem on agnostic Q-learning proposed in [Wen and Van Roy, 2013]. We further extend our algorithm to the stochastic reward setting and obtain similar results. 