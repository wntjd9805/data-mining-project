Deep reinforcement learning (DRL) algorithms and evolution strategies (ES) have been applied to various tasks, showing excellent performances. These have the opposite properties, with DRL having good sample efﬁciency and poor stability, while ES being vice versa. Recently, there have been attempts to combine these algorithms, but these methods fully rely on synchronous update scheme, making it not ideal to maximize the beneﬁts of the parallelism in ES. To solve this chal-lenge, asynchronous update scheme was introduced, which is capable of good time-efﬁciency and diverse policy exploration. In this paper, we introduce an Asyn-chronous Evolution Strategy-Reinforcement Learning (AES-RL) that maximizes the parallel efﬁciency of ES and integrates it with policy gradient methods. Specif-ically, we propose 1) a novel framework to merge ES and DRL asynchronously and 2) various asynchronous update methods that can take all advantages of asyn-chronism, ES, and DRL, which are exploration and time efﬁciency, stability, and sample efﬁciency, respectively. The proposed framework and update methods are evaluated in continuous control benchmark work, showing superior performance as well as time efﬁciency compared to the previous methods. 