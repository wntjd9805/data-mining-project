Invertible neural networks based on coupling ﬂows (CF-INNs) have various ma-chine learning applications such as image synthesis and representation learning.However, their desirable characteristics such as analytic invertibility come at the cost of restricting the functional forms. This poses a question on their repre-sentation power: are CF-INNs universal approximators for invertible functions?Without a universality, there could be a well-behaved invertible transformation that the CF-INN can never approximate, hence it would render the model class unreliable. We answer this question by showing a convenient criterion: a CF-INN is universal if its layers contain afﬁne coupling and invertible linear functions as special cases. As its corollary, we can afﬁrmatively resolve a previously unsolved problem: whether normalizing ﬂow models based on afﬁne coupling can be uni-versal distributional approximators. In the course of proving the universality, we prove a general theorem to show the equivalence of the universality for certain diffeomorphism classes, a theoretical insight that is of interest by itself. 