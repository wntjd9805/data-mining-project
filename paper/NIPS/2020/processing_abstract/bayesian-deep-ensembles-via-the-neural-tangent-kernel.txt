We explore the link between deep ensembles and Gaussian processes (GPs) through the lens of the Neural Tangent Kernel (NTK): a recent development in understand-ing the training dynamics of wide neural networks (NNs). Previous work has shown that even in the inﬁnite width limit, when NNs become GPs, there is noGP posterior interpretation to a deep ensemble trained with squared error loss.We introduce a simple modiﬁcation to standard deep ensembles training, through addition of a computationally-tractable, randomised and untrainable function to each ensemble member, that enables a posterior interpretation in the inﬁnite width limit. When ensembled together, our trained NNs give an approximation to a posterior predictive distribution, and we prove that our Bayesian deep ensembles make more conservative predictions than standard deep ensembles in the inﬁnite width limit. Finally, using ﬁnite width NNs we demonstrate that our Bayesian deep ensembles faithfully emulate the analytic posterior predictive when available, and can outperform standard deep ensembles in various out-of-distribution settings, for both regression and classiﬁcation tasks. 