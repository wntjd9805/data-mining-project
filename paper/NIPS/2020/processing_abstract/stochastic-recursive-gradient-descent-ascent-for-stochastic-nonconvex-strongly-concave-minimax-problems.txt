We consider nonconvex-concave minimax optimization problems of the form minx maxy∈Y f (x, y), where f is strongly-concave in y but possibly nonconvex in x and Y is a convex and compact set. We focus on the stochastic setting, where we can only access an unbiased stochastic gradient estimate of f at each iteration. This formulation includes many machine learning applications as special cases such as robust optimization and adversary training. We are interested inﬁnding an O(ε)-stationary point of the function Φ(·) = maxy∈Y f (·, y). The most popular algorithm to solve this problem is stochastic gradient decent ascent, which requires O(κ3ε−4) stochastic gradient evaluations, where κ is the condition number. In this paper, we propose a novel method called Stochastic Recursive gradiEnt Descent Ascent (SREDA), which estimates gradients more efﬁciently using variance reduction. This method achieves the best known stochastic gradient complexity of O(κ3ε−3), and its dependency on ε is optimal for this problem. 