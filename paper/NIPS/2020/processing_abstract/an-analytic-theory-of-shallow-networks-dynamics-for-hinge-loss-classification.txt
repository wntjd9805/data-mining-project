Neural networks have been shown to perform incredibly well in classiﬁcation tasks over structured high-dimensional datasets. However, the learning dynamics of such networks is still poorly understood. In this paper we study in detail the training dynamics of a simple type of neural network: a single hidden layer trained to perform a classiﬁcation task. We show that in a suitable mean-ﬁeld limit this case maps to a single-node learning problem with a time-dependent dataset determined self-consistently from the average nodes population. We specialize our theory to the prototypical case of a linearly separable data and a linear hinge loss, for which the dynamics can be explicitly solved in the inﬁnite dataset limit. This allows us to address in a simple setting several phenomena appearing in modern networks such as slowing down of training dynamics, crossover between rich and lazy learning, and overﬁtting. Finally, we assess the limitations of mean-ﬁeld theory by studying the case of large but ﬁnite number of nodes and of training samples. 