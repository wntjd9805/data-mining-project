In this paper we study the problem of escaping from saddle points and achieving second-order optimality in a decentralized setting where a group of agents collabo-rate to minimize their aggregate objective function. We provide a non-asymptotic (ﬁnite-time) analysis and show that by following the idea of perturbed gradient descent, it is possible to converge to a second-order stationary point in a number of iterations which depends linearly on dimension and polynomially on the accuracy of second-order stationary point. Doing this in a communication-efﬁcient manner requires overcoming several challenges, from identifying (ﬁrst order) stationary points in a distributed manner, to adapting the perturbed gradient framework with-out prohibitive communication complexity. Our proposed Perturbed DecentralizedGradient Tracking (PDGT) method consists of two major stages: (i) a gradient-based step to ﬁnd a ﬁrst-order stationary point and (ii) a perturbed gradient descent step to escape from a ﬁrst-order stationary point, if it is a saddle point with sufﬁ-cient curvature. As a side beneﬁt of our result, in the case that all saddle points are non-degenerate (strict), the proposed PDGT method ﬁnds a local minimum of the considered decentralized optimization problem in a ﬁnite number of iterations. 