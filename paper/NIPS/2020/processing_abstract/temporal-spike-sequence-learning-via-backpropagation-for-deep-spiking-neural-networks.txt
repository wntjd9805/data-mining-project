Spiking neural networks (SNNs) are well suited for spatio-temporal learning and implementations on energy-efﬁcient event-driven neuromorphic processors. How-ever, existing SNN error backpropagation (BP) methods lack proper handling of spiking discontinuities and suffer from low performance compared with the BP methods for traditional artiﬁcial neural networks. In addition, a large number of time steps are typically required to achieve decent performance, leading to high la-tency and rendering spike based computation unscalable to deep architectures. We present a novel Temporal Spike Sequence Learning Backpropagation (TSSL-BP) method for training deep SNNs, which breaks down error backpropagation across two types of inter-neuron and intra-neuron dependencies and leads to improved temporal learning precision. It captures inter-neuron dependencies through presy-naptic ﬁring times by considering the all-or-none characteristics of ﬁring activities, and captures intra-neuron dependencies by handling the internal evolution of each neuronal state in time. TSSL-BP efﬁciently trains deep SNNs within a much short-ened temporal window of a few steps while improving the accuracy for various image classiﬁcation datasets including CIFAR10. 