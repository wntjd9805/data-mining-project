Contemporary machine learning applications often involve classiﬁcation tasks with many classes. Despite their extensive use, a precise understanding of the statistical properties and behavior of classiﬁcation algorithms is still missing, especially in modern regimes where the number of classes is rather large. In this paper, we take a step in this direction by providing the ﬁrst asymptotically precise analysis of linear multiclass classiﬁcation. Our theoretical analysis allows us to precisely character-ize how the test error varies over different training algorithms, data distributions, problem dimensions as well as number of classes, inter/intra class correlations and class priors. Speciﬁcally, our analysis reveals that the classiﬁcation accuracy is highly distribution-dependent with different algorithms achieving optimal per-formance for different data distributions and/or training/features sizes. Unlike linear regression/binary classiﬁcation, the test error in multiclass classiﬁcation relies on intricate functions of the trained model (e.g., correlation between some of the trained weights) whose asymptotic behavior is difﬁcult to characterize. This challenge is already present in simple classiﬁers, such as those minimizing a square loss. Our novel theoretical techniques allow us to overcome some of these chal-lenges. The insights gained may pave the way for a precise understanding of other classiﬁcation algorithms beyond those studied in this paper.