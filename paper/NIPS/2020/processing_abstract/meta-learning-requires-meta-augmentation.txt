Meta-learning algorithms aim to learn two components: a model that predicts targets for a task, and a base learner that updates that model when given exam-ples from a new task. This additional level of learning can be powerful, but it also creates another potential source of overﬁtting, since we can now overﬁt in either the model or the base learner. We describe both of these forms of meta-learning overﬁtting, and demonstrate that they appear experimentally in common meta-learning benchmarks. We introduce an information-theoretic framework of meta-augmentation, whereby adding randomness discourages the base learner and model from learning trivial solutions that do not generalize to new tasks. We demonstrate that meta-augmentation produces large complementary beneﬁts to recently proposed meta-regularization techniques. 