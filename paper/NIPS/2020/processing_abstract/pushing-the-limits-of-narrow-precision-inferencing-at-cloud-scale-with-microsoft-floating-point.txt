In this paper, we explore the limits of Microsoft Floating Point (MSFP), a new class of datatypes developed for production cloud-scale inferencing on custom hardware.Through the co-evolution of hardware design and algorithms, MSFP16 incurs 3× lower cost compared to Bﬂoat16 and MSFP12 has 4× lower cost compared to INT8 while delivering a comparable or better accuracy. MSFP incurs negligible impact to accuracy (<1%), requires no changes to the model topology, and is integrated with a mature cloud production pipeline. MSFP supports various classes of deep learning models including CNNs, RNNs, and Transformers without modiﬁcation. Finally, we characterize the accuracy and implementation of MSFP and demonstrate its efﬁcacy on a number of production scenarios, including models that power major online scenarios such as web search, question-answering, and image classiﬁcation. 