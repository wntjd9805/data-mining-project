Efﬁcient exploration is one of the main challenges in reinforcement learning (RL).Most existing sample-efﬁcient algorithms assume the existence of a single reward function during exploration. In many practical scenarios, however, there is not a single underlying reward function to guide the exploration, for instance, when an agent needs to learn many skills simultaneously, or multiple conﬂicting objectives need to be balanced. To address these challenges, we propose the task-agnosticRL framework: In the exploration phase, the agent ﬁrst collects trajectories by exploring the MDP without the guidance of a reward function. After exploration, it aims at ﬁnding near-optimal policies for N tasks, given the collected trajectories augmented with sampled rewards for each task. We present an efﬁcient task-agnostic RL algorithm, UCBZERO, that ﬁnds ε-optimal policies for N arbitrary tasks after at most ˜O(log(N )H 5SA/ε2) exploration episodes, where H is the episode length, S is the state space size, and A is the action space size. We also provide an Ω(log(N )H 2SA/ε2) lower bound, showing that the log dependency onN is unavoidable. Furthermore, we provide an N -independent sample complexity bound of UCBZERO in the recently proposed reward-free setting, a statistically easier setting where the ground truth reward functions are known. 