We present a method for provably defending any pretrained image classiﬁer against (cid:96)p adversarial attacks. This method, for instance, allows public vision API providers and users to seamlessly convert pretrained non-robust classiﬁcation services into provably robust ones. By prepending a custom-trained denoiser to any off-the-shelf image classiﬁer and using randomized smoothing, we effectively create a new classiﬁer that is guaranteed to be (cid:96)p-robust to adversarial examples, without modifying the pretrained classiﬁer. Our approach applies to both the white-box and the black-box settings of the pretrained classiﬁer. We refer to this defense as denoised smoothing, and we demonstrate its effectiveness through extensive experimentation on ImageNet and CIFAR-10. Finally, we use our approach to provably defend the Azure, Google, AWS, and ClarifAI image classiﬁcation APIs.Our code replicating all the experiments in the paper can be found at: https://github.com/microsoft/denoised-smoothing