While 2D generative adversarial networks have enabled high-resolution image syn-thesis, they largely lack an understanding of the 3D world and the image formation process. Thus, they do not provide precise control over camera viewpoint or object pose. To address this problem, several recent approaches leverage intermediate voxel-based representations in combination with differentiable rendering. However, existing methods either produce low image resolution or fall short in disentangling camera and scene properties, e.g., the object identity may vary with the viewpoint.In this paper, we propose a generative model for radiance ﬁelds which have recently proven successful for novel view synthesis of a single scene. In contrast to voxel-based representations, radiance ﬁelds are not conﬁned to a coarse discretization of the 3D space, yet allow for disentangling camera and scene properties while degrading gracefully in the presence of reconstruction ambiguity. By introducing a multi-scale patch-based discriminator, we demonstrate synthesis of high-resolution images while training our model from unposed 2D images alone. We systematically analyze our approach on several challenging synthetic and real-world datasets. Our experiments reveal that radiance ﬁelds are a powerful representation for generative image synthesis, leading to 3D consistent models that render with high ﬁdelity. 