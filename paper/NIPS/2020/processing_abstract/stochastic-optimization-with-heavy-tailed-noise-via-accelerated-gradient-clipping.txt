In this paper, we propose a new accelerated stochastic ﬁrst-order method called clipped-SSTM for smooth convex stochastic optimization with heavy-tailed dis-tributed noise in stochastic gradients and derive the ﬁrst high-probability complexity bounds for this method closing the gap in the theory of stochastic optimization with heavy-tailed noise. Our method is based on a special variant of acceleratedStochastic Gradient Descent (SGD) and clipping of stochastic gradients. We extend our method to the strongly convex case and prove new complexity bounds that out-perform state-of-the-art results in this case. Finally, we extend our proof technique and derive the ﬁrst non-trivial high-probability complexity bounds for SGD with clipping without light-tails assumption on the noise. 