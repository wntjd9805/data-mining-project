Recent research has seen several advances relevant to black-box variational infer-ence (VI), but the current state of automatic posterior inference is unclear. One such advance is the use of normalizing ﬂows to deﬁne ﬂexible posterior densities for deep latent variable models. Another direction is the integration of Monte-Carlo methods to serve two purposes; ﬁrst, to obtain tighter variational objectives for optimization, and second, to deﬁne enriched variational families through sam-pling. However, both ﬂows and variational Monte-Carlo methods remain relatively unexplored for black-box VI. Moreover, on a pragmatic front, there are several optimization considerations like step-size scheme, parameter initialization, and choice of gradient estimators, for which there is no clear guidance in the literature.In this paper, we postulate that black-box VI is best addressed through a careful combination of numerous algorithmic components. We evaluate components relat-ing to optimization, ﬂows, and Monte-Carlo methods on a benchmark of 30 models from the Stan model library. The combination of these algorithmic components signiﬁcantly advances the state-of-the-art "out of the box" variational inference. 