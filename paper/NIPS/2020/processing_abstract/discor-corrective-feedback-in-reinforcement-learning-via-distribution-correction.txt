Deep reinforcement learning can learn effective policies for a wide range of tasks, but is notoriously difÔ¨Åcult to use due to instability and sensitivity to hyperparame-ters. The reasons for this remain unclear. In this paper, we study how RL methods based on bootstrapping-based Q-learning can suffer from a pathological interac-tion between function approximation and the data distribution used to train theQ-function: with standard supervised learning, online data collection should induce corrective feedback, where new data corrects mistakes in old predictions. With dy-namic programming methods like Q-learning, such feedback may be absent. This can lead to potential instability, sub-optimal convergence, and poor results when learning from noisy, sparse or delayed rewards. Based on these observations, we propose a new algorithm, DisCor, which explicitly optimizes for data distributions that can correct for accumulated errors in the value function. DisCor computes a tractable approximation to the distribution that optimally induces corrective feedback, which we show results in reweighting samples based on the estimated accuracy of their target values. Using this distribution for training, DisCor results in substantial improvements in a range of challenging RL settings, such as multi-task learning and learning from noisy reward signals. 