Learning object-centric representations of multi-object scenes is a promising ap-proach towards machine intelligence, facilitating high-level reasoning and control from visual sensory data. However, current approaches for unsupervised object-centric scene representation are incapable of aggregating information from multiple observations of a scene. As a result, these “single-view” methods form their repre-sentations of a 3D scene based only on a single 2D observation (view). Naturally, this leads to several inaccuracies, with these methods falling victim to single-view spatial ambiguities. To address this, we propose The Multi-View and Multi-ObjectNetwork (MulMON)1—a method for learning accurate, object-centric represen-tations of multi-object scenes by leveraging multiple views. In order to sidestep the main technical difﬁculty of the multi-object-multi-view scenario—maintaining object correspondences across views—MulMON iteratively updates the latent ob-ject representations for a scene over multiple views. To ensure that these iterative updates do indeed aggregate spatial information to form a complete 3D scene understanding, MulMON is asked to predict the appearance of the scene from novel viewpoints during training. Through experiments we show that MulMON better-resolves spatial ambiguities than single-view methods—learning more accu-rate and disentangled object representations—and also achieves new functionality in predicting object segmentations for novel viewpoints. 