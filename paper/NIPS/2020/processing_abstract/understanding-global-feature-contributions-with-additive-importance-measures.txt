Understanding the inner workings of complex machine learning models is a long-standing problem and most recent research has focused on local interpretability.To assess the role of individual input features in a global sense, we explore the perspective of deﬁning feature importance through the predictive power associated with each feature. We introduce two notions of predictive power (model-based and universal) and formalize this approach with a framework of additive importance measures, which uniﬁes numerous methods in the literature. We then proposeSAGE, a model-agnostic method that quantiﬁes predictive power while accounting for feature interactions. Our experiments show that SAGE can be calculated efﬁciently and that it assigns more accurate importance values than other methods. 