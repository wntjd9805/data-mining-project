Second-order information, in the form of Hessian- or Inverse-Hessian-vector prod-ucts, is a fundamental tool for solving optimization problems. Recently, there has been signiﬁcant interest in utilizing this information in the context of deep neural networks; however, relatively little is known about the quality of existing approximations in this context. Our work examines this question, identiﬁes issues with existing approaches, and proposes a method called WoodFisher to compute a faithful and efﬁcient estimate of the inverse Hessian.Our main application is to neural network compression, where we build on the classic Optimal Brain Damage/Surgeon framework. We demonstrate thatWoodFisher signiﬁcantly outperforms popular state-of-the-art methods for one-shot pruning. Further, even when iterative, gradual pruning is allowed, our method results in a gain in test accuracy over the state-of-the-art approaches, for standard image classiﬁcation datasets such as ImageNet ILSVRC. We examine how our method can be extended to take into account ﬁrst-order information, as well as illustrate its ability to automatically set layer-wise pruning thresholds and perform compression in the limited-data regime. The code is available at the following link, https://github.com/IST-DASLab/WoodFisher. 