The idea of slicing divergences has been proven to be successful when comparing two probability measures in various machine learning applications including gener-ative modeling, and consists in computing the expected value of a ‘base divergence’ between one-dimensional random projections of the two measures. However, the topological, statistical, and computational consequences of this technique have not yet been well-established. In this paper, we aim at bridging this gap and derive various theoretical properties of sliced probability divergences. First, we show that slicing preserves the metric axioms and the weak continuity of the divergence, implying that the sliced divergence will share similar topological properties. We then precise the results in the case where the base divergence belongs to the class of integral probability metrics. On the other hand, we establish that, under mild condi-tions, the sample complexity of a sliced divergence does not depend on the problem dimension. We ﬁnally apply our general results to several base divergences, and illustrate our theory on both synthetic and real data experiments. 