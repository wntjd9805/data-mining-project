Monte-Carlo planning, as exempliﬁed by Monte-Carlo Tree Search (MCTS), has demonstrated remarkable performance in applications with ﬁnite spaces. In this paper, we consider Monte-Carlo planning in an environment with continuous state-action spaces, a much less understood problem with important applications in control and robotics. We introduce POLY-HOOT, an algorithm that augmentsMCTS with a continuous armed bandit strategy named Hierarchical OptimisticOptimization (HOO) (Bubeck et al., 2011). Speciﬁcally, we enhance HOO by using an appropriate polynomial, rather than logarithmic, bonus term in the upper conﬁ-dence bounds. Such a polynomial bonus is motivated by its empirical successes in AlphaGo Zero (Silver et al., 2017b), as well as its signiﬁcant role in achieving theoretical guarantees of ﬁnite space MCTS (Shah et al., 2019). We investigate, for the ﬁrst time, the regret of the enhanced HOO algorithm in non-stationary bandit problems. Using this result as a building block, we establish non-asymptotic con-vergence guarantees for POLY-HOOT: the value estimate converges to an arbitrarily small neighborhood of the optimal value function at a polynomial rate. We further provide experimental results that corroborate our theoretical ﬁndings. 