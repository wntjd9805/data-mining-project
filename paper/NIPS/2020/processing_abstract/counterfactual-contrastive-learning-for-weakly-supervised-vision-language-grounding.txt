Weakly-supervised vision-language grounding aims to localize a target moment in a video or a speciﬁc region in an image according to the given sentence query, where only video-level or image-level sentence annotations are provided during training. Most existing approaches employ the MIL-based or reconstruction-based paradigms for the WSVLG task, but the former heavily depends on the quality of randomly-selected negative samples and the latter cannot directly optimize the visual-textual alignment score. In this paper, we propose a novel CounterfactualContrastive Learning (CCL) to develop sufﬁcient contrastive training between coun-terfactual positive and negative results, which are based on robust and destructive counterfactual transformations. Concretely, we design three counterfactual trans-formation strategies from the feature-, interaction- and relation-level, where the feature-level method damages the visual features of selected proposals, interaction-level approach confuses the vision-language interaction and relation-level strategy destroys the context clues in proposal relationships. Extensive experiments on ﬁve vision-language grounding datasets verify the effectiveness of our CCL paradigm. 