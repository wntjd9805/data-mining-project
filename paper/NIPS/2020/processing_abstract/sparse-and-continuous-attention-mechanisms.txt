Exponential families are widely used in machine learning; they include many distributions in continuous and discrete domains (e.g., Gaussian, Dirichlet, Poisson, and categorical distributions via the softmax transformation). Distributions in each of these families have ﬁxed support. In contrast, for ﬁnite domains, there has been recent work on sparse alternatives to softmax (e.g. sparsemax and α-entmax), which have varying support, being able to assign zero probability to irrelevant categories. This paper expands that work in two directions: ﬁrst, we extend α-entmax to continuous domains, revealing a link with Tsallis statistics and deformed exponential families. Second, we introduce continuous-domain attention mechanisms, deriving efﬁcient gradient backpropagation algorithms for α ∈ {1, 2}.Experiments on attention-based text classiﬁcation, machine translation, and visual question answering illustrate the use of continuous attention in 1D and 2D, showing that it allows attending to time intervals and compact regions. 