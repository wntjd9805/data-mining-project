Reward decomposition, which aims to decompose the full reward into multiple sub-rewards, has been proven beneﬁcial for improving sample efﬁciency in re-inforcement learning. Existing works on discovering reward decomposition are mostly policy dependent, which constrains diversiﬁed or disentangled behavior be-tween different policies induced by different sub-rewards. In this work, we propose a set of novel policy-independent reward decomposition principles by constraining uniqueness and compactness of different state representations relevant to different sub-rewards. Our principles encourage sub-rewards with minimal relevant features, while maintaining the uniqueness of each sub-reward. We derive a deep learning algorithm based on our principle, and refer to our method as RD2, since we learn reward decomposition and disentangled representation jointly. RD2 is evaluated on a toy case, where we have the true reward structure, and chosen Atari environments where the reward structure exists but is unknown to the agent to demonstrate the effectiveness of RD2 against existing reward decomposition methods. 