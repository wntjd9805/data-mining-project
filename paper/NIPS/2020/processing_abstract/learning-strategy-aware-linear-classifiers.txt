We address the question of repeatedly learning linear classiﬁers against agents who are strategically trying to game the deployed classiﬁers, and we use theStackelberg regret to measure the performance of our algorithms. First, we show that Stackelberg and external regret for the problem of strategic classiﬁcation are strongly incompatible: i.e., there exist worst-case scenarios, where any sequence of actions providing sublinear external regret might result in linear Stackelberg re-gret and vice versa. Second, we present a strategy-aware algorithm for minimizing the Stackelberg regret for which we prove nearly matching upper and lower regret bounds. Finally, we provide simulations to complement our theoretical analysis.Our results advance the growing literature of learning from revealed preferences, which has so far focused on “smoother” assumptions from the perspective of the learner and the agents respectively. 