While large scale pre-trained language models such as BERT [5] have achieved great success on various natural language understanding tasks, how to efﬁciently and effectively incorporate them into sequence-to-sequence models and the cor-responding text generation tasks remains a non-trivial problem. In this paper, we propose to address this problem by taking two different BERT models as the encoder and decoder respectively, and ﬁne-tuning them by introducing simple and lightweight adapter modules, which are inserted between BERT layers and tuned on the task-speciﬁc dataset. In this way, we obtain a ﬂexible and efﬁcient model which is able to jointly leverage the information contained in the source-side and target-side BERT models, while bypassing the catastrophic forgetting problem. Each com-ponent in the framework can be considered as a plug-in unit, making the frameworkﬂexible and task agnostic. Our framework is based on a parallel sequence decoding algorithm named Mask-Predict [8] considering the bi-directional and conditional independent nature of BERT, and can be adapted to traditional autoregressive decoding easily. We conduct extensive experiments on neural machine translation tasks where the proposed method consistently outperforms autoregressive baselines while reducing the inference latency by half, and achieves 36.49/33.57 BLEU scores on IWSLT14 German-English/WMT14 German-English translation. When adapted to autoregressive decoding, the proposed method achieves 30.60/43.56BLEU scores on WMT14 English-German/English-French translation, on par with the state-of-the-art baseline models. 