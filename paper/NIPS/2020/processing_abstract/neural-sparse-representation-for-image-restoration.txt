Inspired by the robustness and efÔ¨Åciency of sparse representation in sparse coding based image restoration models, we investigate the sparsity of neurons in deep net-works. Our method structurally enforces sparsity constraints upon hidden neurons.The sparsity constraints are favorable for gradient-based learning algorithms and attachable to convolution layers in various networks. Sparsity in neurons enables computation saving by only operating on non-zero components without hurting accuracy. Meanwhile, our method can magnify representation dimensionality and model capacity with negligible additional computation cost. Experiments show that sparse representation is crucial in deep neural networks for multiple image restora-tion tasks, including image super-resolution, image denoising, and image compres-sion artifacts removal. Code is available at https://github.com/ychfan/nsr. 