Normalizing ﬂows are deep generative models that allow efﬁcient likelihood cal-culation and sampling. The core requirement for this advantage is that they are constructed using functions that can be efﬁciently inverted and for which the deter-minant of the function’s Jacobian can be efﬁciently computed. Researchers have introduced various such ﬂow operations, but few of these allow rich interactions among variables without incurring signiﬁcant computational costs. In this paper, we introduce Woodbury transformations, which achieve efﬁcient invertibility via the Woodbury matrix identity and efﬁcient determinant calculation via Sylvester’s determinant identity. In contrast with other operations used in state-of-the-art normalizing ﬂows, Woodbury transformations enable (1) high-dimensional in-teractions, (2) efﬁcient sampling, and (3) efﬁcient likelihood evaluation. Other similar operations, such as 1x1 convolutions, emerging convolutions, or periodic convolutions allow at most two of these three advantages. In our experiments on multiple image datasets, we ﬁnd that Woodbury transformations allow learning of higher-likelihood models than other ﬂow architectures while still enjoying their efﬁciency advantages. 