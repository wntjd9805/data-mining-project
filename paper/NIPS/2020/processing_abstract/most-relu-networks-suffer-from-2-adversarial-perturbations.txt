We consider ReLU networks with random weights, in which the dimension de-creases at each layer. We show that for most such networks, most examples x admit an adversarial perturbation at an Euclidean distance of O, where d is the input dimension. Moreover, this perturbation can be found via gradient ﬂow, as well as gradient descent with sufﬁciently small steps. This result can be seen as an explanation to the abundance of adversarial examples, and to the fact that they are found via gradient descent. (cid:16) (cid:107)x(cid:107)√ d (cid:17) 