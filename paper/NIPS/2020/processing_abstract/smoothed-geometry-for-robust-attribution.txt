Feature attributions are a popular tool for explaining the behavior of Deep NeuralNetworks (DNNs), but have recently been shown to be vulnerable to attacks that produce divergent explanations for nearby inputs. This lack of robustness is especially problematic in high-stakes applications where adversarially-manipulated explanations could impair safety and trustworthiness. Building on a geometric understanding of these attacks presented in recent work, we identify Lipschitz continuity conditions on models’ gradients that lead to robust gradient-based attributions, and observe that the smoothness of the model’s decision surface is related to the transferability of attacks across multiple attribution methods.To mitigate these attacks in practice, we propose an inexpensive regularization method that promotes these conditions in DNNs, as well as a stochastic smoothing technique that does not require re-training. Our experiments on a range of image models demonstrate that both of these mitigations consistently improve attribution robustness, and conﬁrm the role that smooth geometry plays in these attacks on real, large-scale models. 