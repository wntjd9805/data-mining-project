We study high-conﬁdence behavior-agnostic off-policy evaluation in reinforcement learning, where the goal is to estimate a conﬁdence interval on a target policy’s value, given only access to a static experience dataset collected by unknown be-havior policies. Starting from a function space embedding of the linear program formulation of the Q-function, we obtain an optimization problem with generalized estimating equation constraints. By applying the generalized empirical likelihood method to the resulting Lagrangian, we propose CoinDICE, a novel and efﬁcient algorithm for computing conﬁdence intervals. Theoretically, we prove the obtained conﬁdence intervals are valid, in both asymptotic and ﬁnite-sample regimes. Em-pirically, we show in a variety of benchmarks that the conﬁdence interval estimates are tighter and more accurate than existing methods.2 