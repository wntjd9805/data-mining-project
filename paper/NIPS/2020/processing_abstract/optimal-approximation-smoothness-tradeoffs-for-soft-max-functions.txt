A soft-max function has two main efÔ¨Åciency measures: (1) approximation - which corresponds to how well it approximates the maximum function, (2) smoothness- which shows how sensitive it is to changes of its input. Our goal is to identify the optimal approximation-smoothness tradeoffs for different measures of approx-imation and smoothness. This leads to novel soft-max functions, each of which is optimal for a different application. The most commonly used soft-max function, called exponential mechanism, has optimal tradeoff between approximation mea-sured in terms of expected additive approximation and smoothness measured with respect to R√©nyi Divergence. We introduce a soft-max function, called piecewise linear soft-max, with optimal tradeoff between approximation, measured in terms of worst-case additive approximation and smoothness, measured with respect to‚Ñìùëû-norm. The worst-case approximation guarantee of the piecewise linear mech-anism enforces sparsity in the output of our soft-max function, a property that is known to be important in Machine Learning applications [14, 12] and is not sat-isÔ¨Åed by the exponential mechanism. Moreover, the ‚Ñìùëû-smoothness is suitable for applications in Mechanism Design and Game Theory where the piecewise linear mechanism outperforms the exponential mechanism. Finally, we investigate an-other soft-max function, called power mechanism, with optimal tradeoff between expected multiplicative approximation and smoothness with respect to the R√©nyiDivergence, which provides improved theoretical and practical results in differen-tially private submodular optimization. 