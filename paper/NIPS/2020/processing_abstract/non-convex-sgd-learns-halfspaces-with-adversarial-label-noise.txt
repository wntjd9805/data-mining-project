We study the problem of agnostically learning homogeneous halfspaces in the distribution-speciﬁc PAC model. For a broad family of structured distributions, including log-concave distributions, we show that non-convex SGD efﬁciently converges to a solution with misclassiﬁcation error O(opt) + (cid:15), where opt is the misclassiﬁcation error of the best-ﬁtting halfspace. In sharp contrast, we show that optimizing any convex surrogate inherently leads to misclassiﬁcation error ofω(opt), even under Gaussian marginals. 