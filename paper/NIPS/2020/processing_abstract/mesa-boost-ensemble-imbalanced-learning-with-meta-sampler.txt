Imbalanced learning (IL), i.e., learning unbiased models from class-imbalanced data, is a challenging problem. Typical IL methods including resampling and reweighting were designed based on some heuristic assumptions. They often suffer from unstable performance, poor applicability, and high computational cost in com-plex tasks where their assumptions do not hold. In this paper, we introduce a novel ensemble IL framework named MESA. It adaptively resamples the training set in iterations to get multiple classiﬁers and forms a cascade ensemble model. MESA di-rectly learns the sampling strategy from data to optimize the ﬁnal metric beyond fol-lowing random heuristics. Moreover, unlike prevailing meta-learning-based IL solu-tions, we decouple the model-training and meta-training in MESA by independently train the meta-sampler over task-agnostic meta-data. This makes MESA generally applicable to most of the existing learning models and the meta-sampler can be efﬁciently applied to new tasks. Extensive experiments on both synthetic and real-world tasks demonstrate the effectiveness, robustness, and transferability of MESA.Our code is available at https://github.com/ZhiningLiu1998/mesa. 