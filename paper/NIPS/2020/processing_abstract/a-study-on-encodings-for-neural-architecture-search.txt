Neural architecture search (NAS) has been extensively studied in the past few years.A popular approach is to represent each neural architecture in the search space as a directed acyclic graph (DAG), and then search over all DAGs by encoding the adjacency matrix and list of operations as a set of hyperparameters. Recent work has demonstrated that even small changes to the way each architecture is encoded can have a signiﬁcant effect on the performance of NAS algorithms [22, 24].In this work, we present the ﬁrst formal study on the effect of architecture encodings for NAS, including a theoretical grounding and an empirical study. First we formally deﬁne architecture encodings and give a theoretical characterization on the scalability of the encodings we study. Then we identify the main encoding-dependent subroutines which NAS algorithms employ, running experiments to show which encodings work best with each subroutine for many popular algorithms. The experiments act as an ablation study for prior work, disentangling the algorithmic and encoding-based contributions, as well as a guideline for future work. Our results demonstrate that NAS encodings are an important design decision which can have a signiﬁcant impact on overall performance.1 