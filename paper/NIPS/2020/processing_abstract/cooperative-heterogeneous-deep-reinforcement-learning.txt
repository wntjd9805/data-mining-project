Numerous deep reinforcement learning agents have been proposed, and eachIn this work, we present a Cooperative of them has its strengths and ﬂaws.Heterogeneous Deep Reinforcement Learning (CHDRL) framework that can learn a policy by integrating the advantages of heterogeneous agents. Speciﬁcally, we propose a cooperative learning framework that classiﬁes heterogeneous agents into two classes: global agents and local agents. Global agents are off-policy agents that can utilize experiences from the other agents. Local agents are either on-policy agents or population-based evolutionary algorithms (EAs) agents that can explore the local area effectively. We employ global agents, which are sample-efﬁcient, to guide the learning of local agents so that local agents can beneﬁt from sample-efﬁcient agents and simultaneously maintain their advantages, e.g., stability. Global agents also beneﬁt from effective local searches. Experimental studies on a range of continuous control tasks from the Mujoco benchmark show that CHDRL achieves better performance compared with state-of-the-art baselines. 