Exploration in multi-agent reinforcement learning is a challenging problem, es-pecially in environments with sparse rewards. We propose a general method for efﬁcient exploration by sharing experience amongst agents. Our proposed algo-rithm, called Shared Experience Actor-Critic (SEAC), applies experience sharing in an actor-critic framework by combining the gradients of different agents. We evaluate SEAC in a collection of sparse-reward multi-agent environments and ﬁnd that it consistently outperforms several baselines and state-of-the-art algorithms by learning in fewer steps and converging to higher returns. In some harder environ-ments, experience sharing makes the difference between learning to solve the task and not learning at all. 