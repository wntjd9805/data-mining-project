Learning from Observations (LfO) is a practical reinforcement learning scenario from which many applications can beneﬁt through the reuse of incomplete re-sources. Compared to conventional imitation learning (IL), LfO is more chal-lenging because of the lack of expert action guidance. In both conventional IL and LfO, distribution matching is at the heart of their foundation. Traditional distribution matching approaches are sample-costly which depend on on-policy transitions for policy learning. Towards sample-efﬁciency, some off-policy solu-tions have been proposed, which, however, either lack comprehensive theoretical justiﬁcations or depend on the guidance of expert actions. In this work, we pro-pose a sample-efﬁcient LfO approach which enables off-policy optimization in a principled manner. To further accelerate the learning procedure, we regulate the policy update with an inverse action model, which assists distribution matching from the perspective of mode-covering. Extensive empirical results on challenging locomotion tasks indicate that our approach is comparable with state-of-the-art in terms of both sample-efﬁciency and asymptotic performance.