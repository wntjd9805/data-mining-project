Discrete structures play an important role in applications like program language modeling and software engineering. Current approaches to predicting complex structures typically consider autoregressive models for their tractability, with some sacriﬁce in ﬂexibility. Energy-based models (EBMs) on the other hand offer a more ﬂexible and thus more powerful approach to modeling such distributions, but require partition function estimation. In this paper we propose ALOE, a new algo-rithm for learning conditional and unconditional EBMs for discrete structured data, where parameter gradients are estimated using a learned sampler that mimics local search. We show that the energy function and sampler can be trained efﬁciently via a new variational form of power iteration, achieving a better trade-off betweenﬂexibility and tractability. Experimentally, we show that learning local search leads to signiﬁcant improvements in challenging application domains. Most no-tably, we present an energy model guided fuzzer for software testing that achieves comparable performance to well engineered fuzzing engines like libfuzzer. 