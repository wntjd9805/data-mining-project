Recent advances in Generative Adversarial Networks (GANs) have led to their widespread adoption for the purposes of generating high quality synthetic imagery.While capable of generating photo-realistic images, these models often produce unrealistic samples which fall outside of the data manifold. Several recently proposed techniques attempt to avoid spurious samples, either by rejecting them after generation, or by truncating the model’s latent space. While effective, these methods are inefﬁcient, as a large fraction of training time and model capacity are dedicated towards samples that will ultimately go unused. In this work we propose a novel approach to improve sample quality: altering the training dataset via instance selection before model training has taken place. By reﬁning the empirical data distribution before training, we redirect model capacity towards high-density regions, which ultimately improves sample ﬁdelity, lowers model capacity requirements, and signiﬁcantly reduces training time. Code is available at https://github.com/uoguelph-mlrg/instance_selection_for_gans. 