We introduce a framework for inference in general state-space hidden Markov models (HMMs) under likelihood misspeciﬁcation. In particular, we leverage the loss-theoretic perspective of Generalized Bayesian Inference (GBI) to deﬁne generalised ﬁltering recursions in HMMs, that can tackle the problem of inference under model misspeciﬁcation. In doing so, we arrive at principled procedures for robust inference against observation contamination by utilising the β-divergence.Operationalising the proposed framework is made possible via sequential MonteCarlo methods (SMC), where most standard particle methods, and their associated convergence results, are readily adapted to the new setting. We apply our approach to object tracking and Gaussian process regression problems, and observe improved performance over both standard ﬁltering algorithms and other robust ﬁlters. 