Detecting out-of-distribution (OOD) data is crucial for robust machine learning systems. Normalizing ﬂows are ﬂexible deep generative models that often surpris-ingly fail to distinguish between in- and out-of-distribution data: a ﬂow trained on pictures of clothing assigns higher likelihood to handwritten digits. We investigate why normalizing ﬂows perform poorly for OOD detection. We demonstrate thatﬂows learn local pixel correlations and generic image-to-latent-space transforma-tions which are not speciﬁc to the target image datasets, focusing on ﬂows based on coupling layers. We show that by modifying the architecture of ﬂow coupling layers we can bias the ﬂow towards learning the semantic structure of the target data, improving OOD detection. Our investigation reveals that properties that enable ﬂows to generate high-ﬁdelity images can have a detrimental effect on OOD detection. 