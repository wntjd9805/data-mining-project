We propose an estimator and conﬁdence interval for computing the value of a policy from off-policy data in the contextual bandit setting. To this end we apply empirical likelihood techniques to formulate our estimator and conﬁdence interval as simple convex optimization problems. Using the lower bound of our conﬁdence interval, we then propose an off-policy policy optimization algorithm that searches for policies with large reward lower bound. We empirically ﬁnd that both our estimator and conﬁdence interval improve over previous proposals in ﬁnite sample regimes. Finally, the policy optimization algorithm we propose outperforms a strong baseline system for learning from off-policy data. 