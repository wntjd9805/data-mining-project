The objective of this paper is visual-only self-supervised video representation learn-ing. We make the following contributions: (i) we investigate the beneﬁt of adding semantic-class positives to instance-based Info Noise Contrastive Estimation (In-foNCE) training, showing that this form of supervised contrastive learning leads to a clear improvement in performance; (ii) we propose a novel self-supervised co-training scheme to improve the popular infoNCE loss, exploiting the com-plementary information from different views, RGB streams and optical ﬂow, of the same data source by using one view to obtain positive class samples for the other; (iii) we thoroughly evaluate the quality of the learnt representation on two different downstream tasks: action recognition and video retrieval. In both cases, the proposed approach demonstrates state-of-the-art or comparable performance with other self-supervised approaches, whilst being signiﬁcantly more efﬁcient to train, i.e. requiring far less training data to achieve similar performance. 