Because of the lack of expertise, to gain beneﬁts from their data, average users have to upload their private data to cloud servers they may not trust. Due to legal or privacy constraints, most users are willing to contribute only their encrypted data, and lack interests or resources to join deep neural network (DNN) training in cloud.To train a DNN on encrypted data in a completely non-interactive way, a recent work proposes a fully homomorphic encryption (FHE)-based technique implementing all activations by Brakerski-Gentry-Vaikuntanathan (BGV)-based lookup tables.However, such inefﬁcient lookup-table-based activations signiﬁcantly prolong private training latency of DNNs.In this paper, we propose, Glyph, a FHE-based technique to fast and accurately train DNNs on encrypted data by switching between TFHE (Fast Fully Homo-morphic Encryption over the Torus) and BGV cryptosystems. Glyph uses logic-operation-friendly TFHE to implement nonlinear activations, while adopts vectorial-arithmetic-friendly BGV to perform multiply-accumulations (MACs). Glyph fur-ther applies transfer learning on DNN training to improve test accuracy and reduce the number of MACs between ciphertext and ciphertext in convolutional layers.Our experimental results show Glyph obtains state-of-the-art accuracy, and re-duces training latency by 69% ∼ 99% over prior FHE-based privacy-preserving techniques on encrypted datasets. 