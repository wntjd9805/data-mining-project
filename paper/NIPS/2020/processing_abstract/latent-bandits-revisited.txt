A latent bandit is a bandit problem where the learning agent knows reward distri-butions of arms conditioned on an unknown discrete latent state. The goal of the agent is to identify the latent state, after which it can act optimally. This setting is a natural midpoint between online and ofﬂine learning, where complex models can be learned ofﬂine and the agent identiﬁes the latent state online. This is of high practical relevance, for instance in recommender systems. In this work, we propose general algorithms for latent bandits, based on both upper conﬁdence bounds and Thompson sampling. The algorithms are contextual, and aware of model uncertainty and misspeciﬁcation. We provide a uniﬁed theoretical analysis of our algorithms, which have lower regret than classic bandit policies when the number of latent states is smaller than actions. A comprehensive empirical study showcases the advantages of our approach. 