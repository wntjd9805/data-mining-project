Deep learning algorithms are well-known to have a propensity for ﬁtting the training data very well and often ﬁt even outliers and mislabeled data points. Such ﬁtting requires memorization of training data labels, a phenomenon that has attracted signiﬁcant research interest but has not been given a compelling explanation so far. A recent work of Feldman [13] proposes a theoretical explanation for this phenomenon based on a combination of two insights. First, natural image and data distributions are (informally) known to be long-tailed, that is have a signiﬁcant fraction of rare and atypical examples. Second, in a simple theoretical model such memorization is necessary for achieving close-to-optimal generalization error when the data distribution is long-tailed. However, no direct empirical evidence for this explanation or even an approach for obtaining such evidence were given.In this work we design experiments to test the key ideas in this theory. The experiments require estimation of the inﬂuence of each training example on the accuracy at each test example as well as memorization values of training examples.Estimating these quantities directly is computationally prohibitive but we show that closely-related subsampled inﬂuence and memorization values can be estimated much more efﬁciently. Our experiments demonstrate the signiﬁcant beneﬁts of memorization for generalization on several standard benchmarks. They also provide quantitative and visually compelling evidence for the theory put forth in [13]. 