This paper studies minimax optimization problems minx maxy f (x, y), where f (x, y) is mx-strongly convex with respect to x, my-strongly concave with respect to y and (Lx, Lxy, Ly)-smooth. Zhang et al.[42] provided the fol-lowing lower bound of the gradient complexity for any ﬁrst-order method: (cid:16)(cid:113) Lx mxΩ+L2 xy mxmy+ Ly my (cid:17) ln(1/(cid:15)). This paper proposes a new algorithm with (cid:17) (cid:16)(cid:113) Lx mx ln (1/(cid:15))+ Ly my+ L·Lxy mxmy (cid:17) (cid:16)(cid:112)L2/mxmy ln3 (1/(cid:15)) gradient complexity upper bound ˜O, whereL = max{Lx, Lxy, Ly}. This improves over the best known upper bound˜O by Lin et al. [24]. Our bound achieves linear con-vergence rate and tighter dependency on condition numbers, especially whenLxy (cid:28) L (i.e., when the interaction between x and y is weak). Via reduction, our new bound also implies improved bounds for strongly convex-concave and convex-concave minimax optimization problems. When f is quadratic, we can further improve the upper bound, which matches the lower bound up to a small sub-polynomial factor. 