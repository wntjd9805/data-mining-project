Although Q-learning is one of the most successful algorithms for ﬁnding the best action-value function (and thus the optimal policy) in reinforcement learning, its im-plementation often suffers from large overestimation of Q-function values incurred by random sampling. The double Q-learning algorithm proposed in Hasselt (2010) overcomes such an overestimation issue by randomly switching the update between two Q-estimators, and has thus gained signiﬁcant popularity in practice. However, the theoretical understanding of double Q-learning is rather limited. So far only the asymptotic convergence has been established, which does not characterize how fast the algorithm converges. In this paper, we provide the ﬁrst non-asymptotic (i.e., ﬁnite-time) analysis for double Q-learning. We show that both synchronous and asynchronous double Q-learning are guaranteed to converge to an (cid:15)-accurate 1−ω (cid:19) neighborhood of the global optimum by taking ˜Ω (cid:18)(cid:16) (cid:17) 1 (cid:17) 1+ω 1 (1−γ)6(cid:15)2 (cid:16) 1 1−γ iterations, where ω ∈ (0, 1) is the decay parameter of the learning rate, and γ is the discount factor. Our analysis develops novel techniques to derive ﬁnite-time bounds on the difference between two inter-connected stochastic processes, which is new to the literature of stochastic approximation. 