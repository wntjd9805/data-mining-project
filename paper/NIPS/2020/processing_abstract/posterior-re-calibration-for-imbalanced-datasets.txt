Neural Networks can perform poorly when the training label distribution is heav-ily imbalanced, as well as when the testing data differs from the training dis-tribution. In order to deal with shift in the testing label distribution, which im-balance causes, we motivate the problem from the perspective of an optimalBayes classiﬁer and derive a post-training prior rebalancing technique that can be solved through a KL-divergence based optimization. This method allows aﬂexible post-training hyper-parameter to be efﬁciently tuned on a validation set and effectively modify the classiﬁer margin to deal with this imbalance. We fur-ther combine this method with existing likelihood shift methods, re-interpreting them from the same Bayesian perspective, and demonstrating that our method can deal with both problems in a uniﬁed way. The resulting algorithm can be conveniently used on probabilistic classiﬁcation problems agnostic to underlying architectures. Our results on six different datasets and ﬁve different architectures show state of art accuracy, including on large-scale imbalanced datasets such as iNaturalist for classiﬁcation and Synthia for semantic segmentation. Please see https://github.com/GT-RIPL/UNO-IC.git for implementation.