We consider a general adversarial multi-armed blocking bandit setting where each played arm can be blocked (unavailable) for some time periods and the reward per arm is given at each time period adversarially without obeying any distribution.The setting models scenarios of allocating scarce limited supplies (e.g., arms) where the supplies replenish and can be reused only after certain time periods. Weﬁrst show that, in the optimization setting, when the blocking durations and rewards are known in advance, ﬁnding an optimal policy (e.g., determining which arm per round) that maximises the cumulative reward is strongly NP-hard, eliminating the possibility of a fully polynomial-time approximation scheme (FPTAS) for the problem unless P = NP. To complement our result, we show that a greedy algorithm that plays the best available arm at each round provides an approximation guarantee that depends on the blocking durations and the path variance of the rewards. In the bandit setting, when the blocking durations and rewards are not known, we design two algorithms, RGA and RGA-META, for the case of bounded duration an path variation. In particular, when the variation budget BT is known in advance,T (2 ˜D + K)BT ) dynamic approximate regret. On the otherRGA can achieve O( hand, when BT is not known, we show that the dynamic approximate regret ofRGA-META is at most O((K + ˜D)1/4 ˜B1/2T 3/4) where ˜B is the maximal path variation budget within each batch of RGA-META (which is provably in order of o(T ). We also prove that if either the variation budget or the maximal blocking duration is unbounded, the approximate regret will be at least Θ(T ). We also show that the regret upper bound of RGA is tight if the blocking durations are bounded above by an order of O(1). (cid:113)√ 