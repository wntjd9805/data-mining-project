This work focuses on off-policy evaluation (OPE) with function approximation in inﬁnite-horizon undiscounted Markov decision processes (MDPs). For MDPs that are ergodic and linear (i.e. where rewards and dynamics are linear in some known features), we provide the ﬁrst ﬁnite-sample OPE error bound for a model-based approach, extending existing results beyond the episodic and discounted cases.In a more general setting, when the feature dynamics are approximately linear and for arbitrary rewards, we propose a new heuristic approach for estimating stationary distributions with function approximation. Namely, we formulate this problem as ﬁnding the maximum-entropy distribution subject to matching feature expectations under empirical dynamics. We show that this results in an exponential-family distribution whose sufﬁcient statistics are the features, paralleling maximum-entropy approaches in supervised learning. We demonstrate the effectiveness of the proposed OPE approaches in multiple environments. 