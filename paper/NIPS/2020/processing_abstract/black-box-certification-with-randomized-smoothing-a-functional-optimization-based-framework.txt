Randomized classiﬁers have been shown to provide a promising approach for achieving certiﬁed robustness against adversarial attacks in deep learning. How-ever, most existing methods only leverage Gaussian smoothing noise and only work for ℓ2 perturbation. We propose a general framework of adversarial certiﬁca-tion with non-Gaussian noise and for more general types of attacks, from a uniﬁed functional optimization perspective. Our new framework allows us to identify a key trade-off between accuracy and robustness via designing smoothing distribu-tions and leverage it to design new families of non-Gaussian smoothing distribu-tions that work more efﬁciently for different ℓp settings, including ℓ1, ℓ2 and ℓ∞ attacks. Our proposed methods achieve better certiﬁcation results than previous works and provide a new perspective on randomized smoothing certiﬁcation. 