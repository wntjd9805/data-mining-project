We formulate the problem of neural network optimization as Bayesian ﬁltering, where the observations are backpropagated gradients. While neural network op-timization has previously been studied using natural gradient methods which are closely related to Bayesian inference, they were unable to recover standard opti-mizers such as Adam and RMSprop with a root-mean-square gradient normalizer, instead getting a mean-square normalizer. To recover the root-mean-square nor-malizer, we ﬁnd it necessary to account for the temporal dynamics of all the other parameters as they are optimized. The resulting optimizer, AdaBayes, adaptively transitions between SGD-like and Adam-like behaviour, automatically recoversAdamW, a state of the art variant of Adam with decoupled weight decay, and has generalisation performance competitive with SGD. 