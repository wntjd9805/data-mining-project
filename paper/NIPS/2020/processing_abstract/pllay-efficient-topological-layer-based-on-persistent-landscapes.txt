We propose PLLay, a novel topological layer for general deep learning models based on persistence landscapes, in which we can efﬁciently exploit the underlying topological features of the input data structure. In this work, we show differentia-bility with respect to layer inputs, for a general persistent homology with arbitraryﬁltration. Thus, our proposed layer can be placed anywhere in the network and feed critical information on the topological features of input data into subsequent layers to improve the learnability of the networks toward a given task. A task-optimal structure of PLLay is learned during training via backpropagation, without requiring any input featurization or data preprocessing. We provide a novel adap-tation for the DTM function-based ﬁltration, and show that the proposed layer is robust against noise and outliers through a stability analysis. We demonstrate the effectiveness of our approach by classiﬁcation experiments on various datasets. 