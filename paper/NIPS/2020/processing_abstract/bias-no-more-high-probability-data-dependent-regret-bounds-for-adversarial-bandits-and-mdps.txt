We develop a new approach to obtaining high probability regret bounds for online learning with bandit feedback against an adaptive adversary. While existing ap-proaches all require carefully constructing optimistic and biased loss estimators, our approach uses standard unbiased estimators and relies on a simple increasing learning rate schedule, together with the help of logarithmically homogeneous self-concordant barriers and a strengthened Freedman’s inequality.Besides its simplicity, our approach enjoys several advantages. First, the obtained high-probability regret bounds are data-dependent and could be much smaller than the worst-case bounds, which resolves an open problem asked by Neu [31].Second, resolving another open problem of Bartlett et al. [12] and Abernethy andRakhlin [1], our approach leads to the ﬁrst general and efﬁcient algorithm with a high-probability regret bound for adversarial linear bandits, while previous methods are either inefﬁcient or only applicable to speciﬁc action sets. Finally, our approach can also be applied to learning adversarial Markov Decision Processes and provides the ﬁrst algorithm with a high-probability small-loss bound for this problem. 