We describe a procedure for explaining neurons in deep representations by iden-tifying compositional logical concepts that closely approximate neuron behavior.Compared to prior work that uses atomic labels as explanations, analyzing neurons compositionally allows us to more precisely and expressively characterize their behavior. We use this procedure to answer several questions on interpretability in models for vision and natural language processing. First, we examine the kinds of abstractions learned by neurons. In image classiﬁcation, we ﬁnd that many neurons learn highly abstract but semantically coherent visual concepts, while other polyse-mantic neurons detect multiple unrelated features; in natural language inference (NLI), neurons learn shallow lexical heuristics from dataset biases. Second, we see whether compositional explanations give us insight into model performance: vision neurons that detect human-interpretable concepts are positively correlated with task performance, while NLI neurons that ﬁre for shallow heuristics are negatively cor-related with task performance. Finally, we show how compositional explanations provide an accessible way for end users to produce simple “copy-paste” adversarial examples that change model behavior in predictable ways. 