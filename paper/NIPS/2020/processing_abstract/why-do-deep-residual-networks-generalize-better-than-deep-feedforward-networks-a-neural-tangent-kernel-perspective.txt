Deep residual networks (ResNets) have demonstrated better generalization per-formance than deep feedforward networks (FFNets). However, the theory behind such a phenomenon is still largely unknown. This paper studies this fundamental problem in deep learning from a so-called “neural tangent kernel” perspective.Speciﬁcally, we ﬁrst show that under proper conditions, as the width goes to inﬁn-ity, training deep ResNets can be viewed as learning reproducing kernel functions with some kernel function. We then compare the kernel of deep ResNets with that of deep FFNets and discover that the class of functions induced by the kernel ofFFNets is asymptotically not learnable, as the depth goes to inﬁnity. In contrast, the class of functions induced by the kernel of ResNets does not exhibit such degeneracy. Our discovery partially justiﬁes the advantages of deep ResNets over deep FFNets in generalization abilities. Numerical results are provided to support our claim.