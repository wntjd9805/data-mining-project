Multiplication (e.g., convolution) is arguably a cornerstone of modern deep neural networks (DNNs). However, intensive multiplications cause expensive resource costs that challenge DNNs’ deployment on resource-constrained edge devices, driv-ing several attempts for multiplication-less deep networks. This paper presentedShiftAddNet, whose main inspiration is drawn from a common practice in energy-efﬁcient hardware implementation, that is, multiplication can be instead performed with additions and logical bit-shifts. We leverage this idea to explicitly parameterize deep networks in this way, yielding a new type of deep network that involves only bit-shift and additive weight layers. This hardware-inspired ShiftAddNet immedi-ately leads to both energy-efﬁcient inference and training, without compromising the expressive capacity compared to standard DNNs. The two complementary operation types (bit-shift and add) additionally enable ﬁner-grained control of the model’s learning capacity, leading to more ﬂexible trade-off between accuracy and (training) efﬁciency, as well as improved robustness to quantization and prun-ing. We conduct extensive experiments and ablation studies, all backed up by ourFPGA-based ShiftAddNet implementation and energy measurements. Compared to existing DNNs or other multiplication-less models, ShiftAddNet aggressively re-duces over 80% hardware-quantiﬁed energy cost of DNNs training and inference, while offering comparable or better accuracies. Codes and pre-trained models are available at https://github.com/RICE-EIC/ShiftAddNet. 