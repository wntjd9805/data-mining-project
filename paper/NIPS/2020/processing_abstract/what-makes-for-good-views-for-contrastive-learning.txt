Contrastive learning between multiple views of the data has recently achieved state of the art performance in the ﬁeld of self-supervised representation learning.Despite its success, the inﬂuence of different view choices has been less studied. In this paper, we use theoretical and empirical analysis to better understand the impor-tance of view selection, and argue that we should reduce the mutual information (MI) between views while keeping task-relevant information intact. To verify this hypothesis, we devise unsupervised and semi-supervised frameworks that learn effective views by aiming to reduce their MI. We also consider data augmenta-tion as a way to reduce MI, and show that increasing data augmentation indeed leads to decreasing MI and improves downstream classiﬁcation accuracy. As a by-product, we achieve a new state-of-the-art accuracy on unsupervised pre-training for ImageNet classiﬁcation (73% top-1 linear readout with a ResNet-50)1. 