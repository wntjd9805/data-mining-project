This work develops central limit theorems for cross-validation and consistent esti-mators of its asymptotic variance under weak stability conditions on the learning algorithm. Together, these results provide practical, asymptotically-exact conﬁ-dence intervals for k-fold test error and valid, powerful hypothesis tests of whether one learning algorithm has smaller k-fold test error than another. These results are also the ﬁrst of their kind for the popular choice of leave-one-out cross-validation.In our real-data experiments with diverse learning algorithms, the resulting inter-vals and tests outperform the most popular alternative methods from the literature. 