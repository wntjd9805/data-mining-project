Normalizing ﬂows (NFs) have become a prominent method for deep generative models that allow for an analytic probability density estimation and efﬁcient syn-thesis. However, a ﬂow-based network is considered to be inefﬁcient in parameter complexity because of reduced expressiveness of bijective mapping, which renders the models unfeasibly expensive in terms of parameters. We present an alternative parameterization scheme called NanoFlow, which uses a single neural density estimator to model multiple transformation stages. Hence, we propose an efﬁcient parameter decomposition method and the concept of ﬂow indication embedding, which are key missing components that enable density estimation from a single neural network. Experiments performed on audio and image models conﬁrm that our method provides a new parameter-efﬁcient solution for scalable NFs with signiﬁcant sublinear parameter complexity. 