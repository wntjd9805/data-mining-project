Determining whether inputs are out-of-distribution (OOD) is an essential building block for safely deploying machine learning models in the open world. However, previous methods relying on the softmax conﬁdence score suffer from overconﬁ-dent posterior distributions for OOD data. We propose a uniﬁed framework forOOD detection that uses an energy score. We show that energy scores better distin-guish in- and out-of-distribution samples than the traditional approach using the softmax scores. Unlike softmax conﬁdence scores, energy scores are theoretically aligned with the probability density of the inputs and are less susceptible to the overconﬁdence issue. Within this framework, energy can be ﬂexibly used as a scoring function for any pre-trained neural classiﬁer as well as a trainable cost function to shape the energy surface explicitly for OOD detection. On a CIFAR-10 pre-trained WideResNet, using the energy score reduces the average FPR (at TPR 95%) by 18.03% compared to the softmax conﬁdence score. With energy-based training, our method outperforms the state-of-the-art on common benchmarks. 