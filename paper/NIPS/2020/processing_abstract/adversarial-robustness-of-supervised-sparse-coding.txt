Several recent results provide theoretical insights into the phenomena of adversarial examples. Existing results, however, are often limited due to a gap between the simplicity of the models studied and the complexity of those deployed in practice.In this work, we strike a better balance by considering a model that involves learning a representation while at the same time giving a precise generalization bound and a robustness certiﬁcate. We focus on the hypothesis class obtained by combining a sparsity-promoting encoder coupled with a linear classiﬁer, and show an interesting interplay between the expressivity and stability of the (supervised) representation map and a notion of margin in the feature space. We bound the robust risk (to (cid:96)2-bounded perturbations) of hypotheses parameterized by dictionaries that achieve a mild encoder gap on training data. Furthermore, we provide a robustness certiﬁcate for end-to-end classiﬁcation. We demonstrate the applicability of our analysis by computing certiﬁed accuracy on real data, and compare with other alternatives for certiﬁed robustness. 