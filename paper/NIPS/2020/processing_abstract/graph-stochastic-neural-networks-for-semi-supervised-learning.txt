Graph Neural Networks (GNNs) have achieved remarkable performance in the task of the semi-supervised node classiﬁcation. However, most existing models learn a deterministic classiﬁcation function, which lack sufﬁcient ﬂexibility to explore better choices in the presence of kinds of imperfect observed data such as the scarce labeled nodes and noisy graph structure. To improve the rigidness and inﬂexibility of deterministic classiﬁcation functions, this paper proposes a novel framework named Graph Stochastic Neural Networks (GSNN), which aims to model the uncertainty of the classiﬁcation function by simultaneously learning a family of functions, i.e., a stochastic function. Speciﬁcally, we introduce a learnable graph neural network coupled with a high-dimensional latent variable to model the distribution of the classiﬁcation function, and further adopt the amortised variational inference to approximate the intractable joint posterior for missing labels and the latent variable. By maximizing the lower-bound of the likelihood for observed node labels, the instantiated models can be trained in an end-to-end manner effectively. Extensive experiments on three real-world datasets show thatGSNN achieves substantial performance gain in different scenarios compared with state-of-the-art baselines. 