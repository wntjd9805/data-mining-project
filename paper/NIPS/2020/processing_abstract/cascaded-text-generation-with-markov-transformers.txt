The two dominant approaches to neural text generation are fully autoregressive models, using serial beam search decoding, and non-autoregressive models, using parallel decoding with no output dependencies. This work proposes an autore-gressive model with sub-linear parallel time generation. Noting that conditional random ﬁelds with bounded context can be decoded in parallel, we propose an efﬁcient cascaded decoding approach for generating high-quality output. To param-eterize this cascade, we introduce a Markov transformer, a variant of the popular fully autoregressive model that allows us to simultaneously decode with speciﬁc autoregressive context cutoffs. This approach requires only a small modiﬁcation from standard autoregressive training, while showing competitive accuracy/speed tradeoff compared to existing methods on ﬁve machine translation datasets. 