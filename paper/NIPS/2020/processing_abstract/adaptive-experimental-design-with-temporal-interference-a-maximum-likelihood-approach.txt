Suppose an online platform wants to compare a treatment and control policy, e.g., two different matching algorithms in a ridesharing system, or two different in-ventory management algorithms in an online retail site. Standard experimental approaches to this problem are biased (due to temporal interference between the policies), and not sample efﬁcient. We study optimal experimental design for this setting. We view testing the two policies as the problem of estimating the steady state difference in reward between two unknown Markov chains (i.e., policies).We assume estimation of the steady state reward for each chain proceeds via non-parametric maximum likelihood, and search for consistent (i.e., asymptotically unbiased) experimental designs that are efﬁcient (i.e., asymptotically minimum variance). Characterizing such designs is equivalent to a Markov decision problem with a minimum variance objective; such problems generally do not admit tractable solutions. Remarkably, in our setting, using a novel application of classical martin-gale analysis of Markov chains via Poisson’s equation, we characterize efﬁcient designs via a succinct convex optimization problem. We use this characterization to propose a consistent, efﬁcient online experimental design that adaptively samples the two Markov chains. 