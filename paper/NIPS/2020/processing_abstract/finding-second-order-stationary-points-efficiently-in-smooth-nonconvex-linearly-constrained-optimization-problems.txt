This paper proposes two efﬁcient algorithms for computing approximate second-order stationary points (SOSPs) of problems with generic smooth non-convex objective functions and generic linear constraints. While ﬁnding (approximate)SOSPs for the class of smooth non-convex linearly constrained problems is com-putationally intractable, we show that generic problem instances in this class can be solved efﬁciently. Speciﬁcally, for a generic problem instance, we show that certain strict complementarity (SC) condition holds for all Karush-Kuhn-Tucker (KKT) solutions. Based on this condition, we design an algorithm named Successive Negative-curvature grAdient Projection (SNAP), which per-forms either conventional gradient projection or some negative curvature based projection steps to ﬁnd SOSPs. SNAP is a second-order algorithm that requires}) iterations to compute an (ϵG, ϵH )-SOSP, where eO hides eO(max{1/ϵ2 the iteration complexity for eigenvalue-decomposition. Building on SNAP, we propose a ﬁrst-order algorithm, named SNAP+, that requires O(1/ϵ2.5) iterationsϵ)-SOSP. The per-iteration computational complexities of our al-to compute (ϵ, gorithms are polynomial in the number of constraints and problem dimension. To the best of our knowledge, this is the ﬁrst time that ﬁrst-order algorithms with polynomial per-iteration complexity and global sublinear rate are designed to ﬁndSOSPs of the important class of non-convex problems with linear constraints (al-most surely).G, 1/ϵ3H√ 