We present a ﬂexible framework for learning predictive models that approximately satisfy the equalized odds notion of fairness. This is achieved by introducing a general discrepancy functional that rigorously quantiﬁes violations of this criterion.This differentiable functional is used as a penalty driving the model parameters towards equalized odds. To rigorously evaluate ﬁtted models, we develop a formal hypothesis test to detect whether a prediction rule violates this property, the ﬁrst such test in the literature. Both the model ﬁtting and hypothesis testing leverage a resampled version of the sensitive attribute obeying equalized odds, by construction.We demonstrate the applicability and validity of the proposed framework both in regression and multi-class classiﬁcation problems, reporting improved performance over state-of-the-art methods. Lastly, we show how to incorporate techniques for equitable uncertainty quantiﬁcation—unbiased for each group under study—to communicate the results of the data analysis in exact terms. 