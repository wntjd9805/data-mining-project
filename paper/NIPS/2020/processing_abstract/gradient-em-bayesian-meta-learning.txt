Bayesian meta-learning enables robust and fast adaptation to new tasks with un-certainty assessment. The key idea behind Bayesian meta-learning is empiricalBayes inference of hierarchical model. In this work, we extend this framework to include a variety of existing methods, before proposing our variant based on gradient-EM algorithm. Our method improves computational efﬁciency by avoid-ing back-propagation computation in the meta-update step, which is exhausting for deep neural networks. Furthermore, it provides ﬂexibility to the inner-update opti-mization procedure by decoupling it from meta-update. Experiments on sinusoidal regression, few-shot image classiﬁcation, and policy-based reinforcement learning show that our method not only achieves better accuracy with less computation cost, but is also more robust to uncertainty. 