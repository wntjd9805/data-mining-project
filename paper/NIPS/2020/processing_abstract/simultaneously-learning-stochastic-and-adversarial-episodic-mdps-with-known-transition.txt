√This work studies the problem of learning episodic Markov Decision Processes with known transition and bandit feedback. We develop the ﬁrst algorithm with a“best-of-both-worlds” guarantee: it achieves O(log T ) regret when the losses are stochastic, and simultaneously enjoys worst-case robustness with (cid:101)O(T ) regret even when the losses are adversarial, where T is the number of episodes. MoreC) regret in an intermediate setting where the losses generally, it achieves (cid:101)O( are corrupted by a total amount of C. Our algorithm is based on the Follow-the-Regularized-Leader method from Zimin and Neu [26], with a novel hybrid regularizer inspired by recent works of Zimmert et al. [27, 29] for the special case of multi-armed bandits. Crucially, our regularizer admits a non-diagonal Hessian with a highly complicated inverse. Analyzing such a regularizer and deriving a particular self-bounding regret guarantee is our key technical contribution and might be of independent interest.√ 