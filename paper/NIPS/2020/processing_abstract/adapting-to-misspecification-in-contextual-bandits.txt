A major research direction in contextual bandits is to develop algorithms that are computationally efﬁcient, yet support ﬂexible, general-purpose function approx-imation. Algorithms based on modeling rewards have shown strong empirical performance, yet typically require a well-speciﬁed model, and can fail when this assumption does not hold. Can we design algorithms that are efﬁcient and ﬂexible, yet degrade gracefully in the face of model misspeciﬁcation? We introduce a new family of oracle-efﬁcient algorithms for ε-misspeciﬁed contextual bandits that adapt to unknown model misspeciﬁcation—both for ﬁnite and inﬁnite action settings. Given access to an online oracle for square loss regression, our algorithm attains optimal regret and—in particular—optimal dependence on the misspeciﬁ-cation level, with no prior knowledge. Specializing to linear contextual bandits with inﬁnite actions in d dimensions, we obtain the ﬁrst algorithm that achieves the optimal ˜O(d dT ) regret bound for unknown ε.On a conceptual level, our results are enabled by a new optimization-based per-spective on the regression oracle reduction framework of Foster and Rakhlin [20], which we believe will be useful more broadly.T + ε√√ 