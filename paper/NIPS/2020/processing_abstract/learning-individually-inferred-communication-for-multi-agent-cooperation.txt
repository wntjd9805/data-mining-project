Communication lays the foundation for human cooperation. It is also crucial for multi-agent cooperation. However, existing work focuses on broadcast communi-cation, which is not only impractical but also leads to information redundancy that could even impair the learning process. To tackle these difﬁculties, we proposeIndividually Inferred Communication (I2C), a simple yet effective model to enable agents to learn a prior for agent-agent communication. The prior knowledge is learned via causal inference and realized by a feed-forward neural network that maps the agent’s local observation to a belief about who to communicate with.The inﬂuence of one agent on another is inferred via the joint action-value func-tion in multi-agent reinforcement learning and quantiﬁed to label the necessity of agent-agent communication. Furthermore, the agent policy is regularized to better exploit communicated messages. Empirically, we show that I2C can not only reduce communication overhead but also improve the performance in a variety of multi-agent cooperative scenarios, comparing to existing methods. 