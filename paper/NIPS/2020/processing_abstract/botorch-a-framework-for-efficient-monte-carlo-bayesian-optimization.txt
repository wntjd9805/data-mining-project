Bayesian optimization provides sample-efﬁcient global optimization for a broad range of applications, including automatic machine learning, engineering, physics, and experimental design. We introduce BOTORCH, a modern programming frame-work for Bayesian optimization that combines Monte-Carlo (MC) acquisition functions, a novel sample average approximation optimization approach, auto-differentiation, and variance reduction techniques. BOTORCH’s modular design facilitates ﬂexible speciﬁcation and optimization of probabilistic models written in PyTorch, simplifying implementation of new acquisition functions. Our ap-proach is backed by novel theoretical convergence results and made practical by a distinctive algorithmic foundation that leverages fast predictive distributions, hardware acceleration, and deterministic optimization. We also propose a novel“one-shot” formulation of the Knowledge Gradient, enabled by a combination of our theoretical and software contributions. In experiments, we demonstrate the improved sample efﬁciency of BOTORCH relative to other popular libraries. 