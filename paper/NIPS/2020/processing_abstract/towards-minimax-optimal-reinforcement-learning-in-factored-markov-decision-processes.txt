We study minimax optimal reinforcement learning in episodic factored Markov decision processes (FMDPs), which are MDPs with conditionally independent transition components. Assuming the factorization is known, we propose two model-based algorithms. The ﬁrst one achieves minimax optimal regret guarantees for a rich class of factored structures, while the second one enjoys better com-putational complexity with a slightly worse regret. A key new ingredient of our algorithms is the design of a bonus term to guide exploration. We complement our algorithms by presenting several structure-dependent lower bounds on regret forFMDPs that reveal the difﬁculty hiding in the intricacy of the structures. 