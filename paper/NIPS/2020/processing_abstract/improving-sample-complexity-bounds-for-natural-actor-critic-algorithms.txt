The actor-critic (AC) algorithm is a popular method to ﬁnd an optimal policy in rein-forcement learning. In the inﬁnite horizon scenario, the ﬁnite-sample convergence rate for the AC and natural actor-critic (NAC) algorithms has been established recently, but under independent and identically distributed (i.i.d.) sampling and single-sample update at each iteration. In contrast, this paper characterizes the con-vergence rate and sample complexity of AC and NAC under Markovian sampling, with mini-batch data for each iteration, and with actor having general policy class approximation. We show that the overall sample complexity for a mini-batch AC to attain an (cid:15)-accurate stationary point improves the best known sample complexity of AC by an order of O((cid:15)−1 log(1/(cid:15))), and the overall sample complexity for a mini-batch NAC to attain an (cid:15)-accurate globally optimal point improves the exist-ing sample complexity of NAC by an order of O((cid:15)−2/ log(1/(cid:15))). Moreover, the sample complexity of AC and NAC characterized in this work outperforms that of policy gradient (PG) and natural policy gradient (NPG) by a factor of O((1 − γ)−3) and O((1 − γ)−4(cid:15)−2/ log(1/(cid:15))), respectively. This is the ﬁrst theoretical study establishing that AC and NAC attain orderwise performance improvement over PG and NPG under inﬁnite horizon due to the incorporation of critic. 