Consider an oracle which takes a point x and returns the minimizer of a convex function f in an (cid:96)2 ball of radius r around x. It is straightforward to show that roughly r−1 log 1 (cid:15) calls to the oracle sufﬁce to ﬁnd an (cid:15)-approximate minimizer of f in an (cid:96)2 unit ball. Perhaps surprisingly, this is not optimal: we design an accelerated algorithm which attains an (cid:15)-approximate minimizer with roughly r−2/3 log 1 (cid:15) oracle queries, and give a matching lower bound. Further, we implement ball optimization oracles for functions with locally stable Hessians using a variant ofNewton’s method and, in certain cases, stochastic ﬁrst-order methods. The resulting algorithm applies to a number of problems of practical and theoretical import, improving upon previous results for logistic and (cid:96)∞ regression and achieving guarantees comparable to the state-of-the-art for (cid:96)p regression. 