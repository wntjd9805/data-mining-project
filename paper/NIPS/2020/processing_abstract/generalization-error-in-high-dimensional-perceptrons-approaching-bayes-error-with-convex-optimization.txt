We consider a commonly studied supervised classiﬁcation of a synthetic dataset whose labels are generated by feeding a one-layer neural network with random i.i.d inputs. We study the generalization performances of standard classiﬁers in the high-dimensional regime where α = n/d is kept ﬁnite in the limit of a high dimension d and number of samples n. Our contribution is three-fold: First, we prove a formula for the generalization error achieved by (cid:96)2 regularized classiﬁers that minimize a convex loss. This formula was ﬁrst obtained by the heuristic replica method of statistical physics. Secondly, focussing on commonly used loss functions and optimizing the (cid:96)2 regularization strength, we observe that while ridge regression performance is poor, logistic and hinge regression are surprisingly able to approach the Bayes-optimal generalization error extremely closely. As α→ ∞ they lead to Bayes-optimal rates, a fact that does not follow from predictions of margin-based generalization error bounds. Third, we design an optimal loss and regularizer that provably leads to Bayes-optimal generalization error. 