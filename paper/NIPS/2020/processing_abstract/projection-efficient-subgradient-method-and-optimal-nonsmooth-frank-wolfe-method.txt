We consider the classical setting of optimizing a nonsmooth Lipschitz continuous convex function over a convex constraint set, when having access to a (stochastic)ﬁrst-order oracle (FO) for the function and a projection oracle (PO) for the con-straint set. It is well known that to achieve ε-suboptimality in high-dimensions,Θ(ε−2) FO calls are necessary [64]. This is achieved by the projected subgradient method (PGD) [11]. However, PGD also entails O(ε−2) PO calls, which may be computationally costlier than FO calls (e.g. nuclear norm constraints). Improving this PO calls complexity of PGD is largely unexplored, despite the fundamental nature of this problem and extensive literature. We present ﬁrst such improvement.This only requires a mild assumption that the objective function, when extended to a slightly larger neighborhood of the constraint set, still remains Lipschitz and accessible via FO. In particular, we introduce MOPES method, which carefully combines Moreau-Yosida smoothing and accelerated ﬁrst-order schemes. This is guaranteed to ﬁnd a feasible ε-suboptimal solution using only O(ε−1) PO calls and optimal O(ε−2) FO calls. Further, instead of a PO if we only have a linear mini-mization oracle (LMO, à la Frank-Wolfe) to access the constraint set, an extension of our method, MOLES, ﬁnds a feasible ε-suboptimal solution using O(ε−2) LMO calls and FO calls—both match known lower bounds [54], resolving a question left open since [84]. Our experiments conﬁrm that these methods achieve signiﬁcant speedups over the state-of-the-art, for a problem with costly PO and LMO calls. 