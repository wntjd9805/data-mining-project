In this work, we focus on the task of learning and representing dense correspon-dences in deformable object categories. While this problem has been considered before, solutions so far have been rather ad-hoc for speciﬁc object types (i.e., humans), often with signiﬁcant manual work involved. However, scaling the ge-ometry understanding to all objects in nature requires more automated approaches that can also express correspondences between related, but geometrically differ-ent objects. To this end, we propose a new, learnable image-based representation of dense correspondences. Our model predicts, for each pixel in a 2D image, an embedding vector of the corresponding vertex in the object mesh, therefore estab-lishing dense correspondences between image pixels and 3D object geometry. We demonstrate that the proposed approach performs on par or better than the state-of-the-art methods for dense pose estimation for humans, while being conceptually simpler. We also collect a new in-the-wild dataset of dense correspondences for animal classes and demonstrate that our framework scales naturally to the new deformable object categories. 