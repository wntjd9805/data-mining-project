The existence of adversarial examples capable of fooling trained neural network classiﬁers calls for a much better understanding of possible attacks to guide the development of safeguards against them. This includes attack methods in the chal-lenging non-interactive blackbox setting, where adversarial attacks are generated without any access, including queries, to the target model. Prior attacks in this setting have relied mainly on algorithmic innovations derived from empirical obser-vations (e.g., that momentum helps), lacking principled transferability guarantees.In this work, we provide a theoretical foundation for crafting transferable adver-sarial examples to entire hypothesis classes. We introduce Adversarial ExampleGames (AEG), a framework that models the crafting of adversarial examples as a min-max game between a generator of attacks and a classiﬁer. AEG provides a new way to design adversarial examples by adversarially training a generator and a classiﬁer from a given hypothesis class (e.g., architecture). We prove that this game has an equilibrium, and that the optimal generator is able to craft adversarial examples that can attack any classiﬁer from the corresponding hypothesis class.We demonstrate the efﬁcacy of AEG on the MNIST and CIFAR-10 datasets, outper-forming prior state-of-the-art approaches with an average relative improvement of 29.9% and 47.2% against undefended and robust models (Table 2 & 3) respectively. 