As language models become more powerful, training and evaluation are increas-ingly bottlenecked by the data and metrics used for a particular task. For example, summarization models are often trained to predict human reference summaries and evaluated using ROUGE, but both of these metrics are rough proxies for what we really care about—summary quality. In this work, we show that it is possible to signiﬁcantly improve summary quality by training a model to optimize for human preferences. We collect a large, high-quality dataset of human comparisons be-tween summaries, train a model to predict the human-preferred summary, and use that model as a reward function to ﬁne-tune a summarization policy using reinforce-ment learning. We apply our method to a version of the TL;DR dataset of Reddit posts [63] and ﬁnd that our models signiﬁcantly outperform both human reference summaries and much larger models ﬁne-tuned with supervised learning alone. Our models also transfer to CNN/DM news articles [22], producing summaries nearly as good as the human reference without any news-speciﬁc ﬁne-tuning.2 We con-duct extensive analyses to understand our human feedback dataset and ﬁne-tuned models.3 We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing ROUGE according to humans. We hope the evidence from our paper motivates machine learning researchers to pay closer attention to how their training loss affects the model behavior they actually want. 