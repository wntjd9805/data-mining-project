Most of the work on interpretable machine learning has focused on designing either inherently interpretable models, which typically trade-off accuracy for in-terpretability, or post-hoc explanation systems, whose explanation quality can be unpredictable. Our method, EXPO, is a hybridization of these approaches that regularizes a model for explanation quality at training time. Importantly, these regularizers are differentiable, model agnostic, and require no domain knowledge to deﬁne. We demonstrate that post-hoc explanations for EXPO-regularized models have better explanation quality, as measured by the common ﬁdelity and stability metrics. We verify that improving these metrics leads to signiﬁcantly more useful explanations with a user study on a realistic task. 