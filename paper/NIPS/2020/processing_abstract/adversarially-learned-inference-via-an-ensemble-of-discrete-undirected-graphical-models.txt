Undirected graphical models are compact representations of joint probability dis-tributions over random variables. To solve inference tasks of interest, graphical models of arbitrary topology can be trained using empirical risk minimization.However, to solve inference tasks that were not seen during training, these models (EGMs) often need to be re-trained. Instead, we propose an inference-agnostic adversarial training framework which produces an inﬁnitely-large ensemble of graphical models (AGMs). The ensemble is optimized to generate data within the GAN framework, and inference is performed using a ﬁnite subset of these models. AGMs perform comparably with EGMs on inference tasks that the latter were speciﬁcally optimized for. Most importantly, AGMs show signiﬁcantly bet-ter generalization to unseen inference tasks compared to EGMs, as well as deep neural architectures like GibbsNet and VAEAC which allow arbitrary conditioning.Finally, AGMs allow fast data sampling, competitive with Gibbs sampling fromEGMs. 