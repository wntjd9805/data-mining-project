Stein Variational Gradient Descent (SVGD), a popular sampling algorithm, is often described as the kernelized gradient ﬂow for the Kullback-Leibler divergence in the geometry of optimal transport. We introduce a new perspective on SVGD that instead views SVGD as the (kernelized) gradient ﬂow of the chi-squared diver-gence which, we show, exhibits a strong form of uniform exponential ergodicity under conditions as weak as a Poincar´e inequality. This perspective leads us to propose an alternative to SVGD, called Laplacian Adjusted Wasserstein GradientDescent (LAWGD), that can be implemented from the spectral decomposition of the Laplacian operator associated with the target density. We show that LAWGD exhibits strong convergence guarantees and good practical performance. 