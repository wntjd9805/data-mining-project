Existing Neural Architecture Search (NAS) methods either encode neural archi-tectures using discrete encodings that do not scale well, or adopt supervised learning-based methods to jointly learn architecture representations and optimize architecture search on such representations which incurs search bias. Despite the widespread use, architecture representations learned in NAS are still poorly under-stood. We observe that the structural properties of neural architectures are hard to preserve in the latent space if architecture representation learning and search are coupled, resulting in less effective search performance. In this work, we ﬁnd empirically that pre-training architecture representations using only neural archi-tectures without their accuracies as labels improves the downstream architecture search efﬁciency. To explain this ﬁnding, we visualize how unsupervised archi-tecture representation learning better encourages neural architectures with similar connections and operators to cluster together. This helps map neural architectures with similar performance to the same regions in the latent space and makes the transition of architectures in the latent space relatively smooth, which considerably beneﬁts diverse downstream search strategies. 