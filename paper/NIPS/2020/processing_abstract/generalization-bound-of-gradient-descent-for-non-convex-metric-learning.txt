Metric learning aims to learn a distance measure that can beneﬁt distance-based methods such as the nearest neighbor (NN) classiﬁer. While considerable efforts have been made to improve its empirical performance and analyze its generaliza-tion ability by focusing on the data structure and model complexity, an unresolved question is how choices of algorithmic parameters, such as the number of training iterations, affect metric learning as it is typically formulated as an optimization problem and nowadays more often as a non-convex problem. In this paper, we theoretically address this question and prove the agnostic Probably ApproximatelyCorrect (PAC) learnability for metric learning algorithms with non-convex objective functions optimized via gradient descent (GD); in particular, our theoretical guar-antee takes the iteration number into account. We ﬁrst show that the generalizationPAC bound is a sufﬁcient condition for agnostic PAC learnability and this bound can be obtained by ensuring the uniform convergence on a densely concentrated subset of the parameter space. We then show that, for classiﬁers optimized via GD, their generalizability can be guaranteed if the classiﬁer and loss function are bothLipschitz smooth, and further improved by using fewer iterations. To illustrate and exploit the theoretical ﬁndings, we ﬁnally propose a novel metric learning method called Smooth Metric and representative Instance LEarning (SMILE), designed to satisfy the Lipschitz smoothness property and learned via GD with an early stopping mechanism for better discriminability and less computational cost of NN. 