We present a transductive learning algorithm that takes as input training examples from a distribution ğ‘ƒ and arbitrary (unlabeled) test examples, possibly chosen by an adversary. This is unlike prior work that assumes that test examples are small perturbations of ğ‘ƒ . Our algorithm outputs a selective classiï¬er, which abstains from predicting on some examples. By considering selective transductive learning, we give the ï¬rst nontrivial guarantees for learning classes of bounded VC dimension with arbitrary train and test distributionsâ€”no prior guarantees were known even for simple classes of functions such as intervals on the line. In particular, for any function in a class ğ¶ of bounded VC dimension, we guarantee a low test error rate and a low rejection rate with respect to ğ‘ƒ . Our algorithm is efï¬cient given an Empirical Risk Minimizer (ERM) for ğ¶. Our guarantees hold even for test examples chosen by an unbounded white-box adversary. We also give guarantees for generalization, agnostic, and unsupervised settings. 