Machine learning on tiny IoT devices based on microcontroller units (MCU) is appealing but challenging: the memory of microcontrollers is 2-3 orders of magni-tude smaller even than mobile phones. We propose MCUNet, a framework that jointly designs the efﬁcient neural architecture (TinyNAS) and the lightweight inference engine (TinyEngine), enabling ImageNet-scale inference on microcon-trollers. TinyNAS adopts a two-stage neural architecture search approach thatﬁrst optimizes the search space to ﬁt the resource constraints, then specializes the network architecture in the optimized search space. TinyNAS can automatically handle diverse constraints (i.e. device, latency, energy, memory) under low search costs. TinyNAS is co-designed with TinyEngine, a memory-efﬁcient inference library to expand the search space and ﬁt a larger model. TinyEngine adapts the memory scheduling according to the overall network topology rather than layer-wise optimization, reducing the memory usage by 3.4×, and accelerating the inference by 1.7-3.3× compared to TF-Lite Micro [3] and CMSIS-NN [28].MCUNet is the ﬁrst to achieves >70% ImageNet top1 accuracy on an off-the-shelf commercial microcontroller, using 3.5× less SRAM and 5.7× less Flash compared to quantized MobileNetV2 and ResNet-18. On visual&audio wake words tasks,MCUNet achieves state-of-the-art accuracy and runs 2.4-3.4× faster than Mo-bileNetV2 and ProxylessNAS-based solutions with 3.7-4.1× smaller peak SRAM.Our study suggests that the era of always-on tiny machine learning on IoT devices has arrived. 