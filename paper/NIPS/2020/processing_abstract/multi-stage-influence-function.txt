Multi-stage training and knowledge transfer, from a large-scale pretraining task to various ﬁnetuning tasks, have revolutionized natural language processing and computer vision resulting in state-of-the-art performance improvements. In this paper, we develop a multi-stage inﬂuence function score to track predictions from a ﬁnetuned model all the way back to the pretraining data. With this score, we can identify the pretraining examples in the pretraining task that contribute most to a prediction in the ﬁnetuning task. The proposed multi-stage inﬂuence function generalizes the original inﬂuence function for a single model in (Koh &Liang, 2017), thereby enabling inﬂuence computation through both pretrained andﬁnetuned models. We study two different scenarios with the pretrained embeddingsﬁxed or updated in the ﬁnetuning tasks. We test our proposed method in various experiments to show its effectiveness and potential applications. 