The Frank-Wolfe algorithm is a classic method for constrained optimization prob-lems. It has recently been popular in many machine learning applications because its projection-free property leads to more efﬁcient iterations. In this paper, we study projection-free algorithms for convex-strongly-concave saddle point problems with complicated constraints. Our method combines Conditional Gradient Sliding withMirror-Prox and shows that it only requires ˜O(1/ (cid:15)) gradient evaluations and˜O(1/(cid:15)2) linear optimizations in the batch setting. We also extend our method to the stochastic setting and propose ﬁrst stochastic projection-free algorithms for saddle point problems. Experimental results demonstrate the effectiveness of our algorithms and verify our theoretical guarantees.√ 