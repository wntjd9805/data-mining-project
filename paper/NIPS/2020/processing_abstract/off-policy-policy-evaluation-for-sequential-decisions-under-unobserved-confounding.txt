When observed decisions depend only on observed features, off-policy policy evalu-ation (OPE) methods for sequential decision problems can estimate the performance of evaluation policies before deploying them. However, this assumption is fre-quently violated due to unobserved confounders, unrecorded variables that impact both the decisions and their outcomes. We assess robustness of OPE methods under unobserved confounding by developing worst-case bounds on the performance of an evaluation policy. When unobserved confounders can affect every decision in an episode, we demonstrate that even small amounts of per-decision confounding can heavily bias OPE methods. Fortunately, in a number of important settings found in healthcare, policy-making, and technology, unobserved confounders may directly affect only one of the many decisions made, and inﬂuence future decisions/rewards only through the directly affected decision. Under this less pessimistic model of one-decision confounding, we propose an efﬁcient loss-minimization-based proce-dure for computing worst-case bounds, and prove its statistical consistency. On simulated healthcare examples—management of sepsis and interventions for autis-tic children—where this is a reasonable model, we demonstrate that our method invalidates non-robust results and provides meaningful certiﬁcates of robustness, allowing reliable selection of policies under unobserved confounding. 