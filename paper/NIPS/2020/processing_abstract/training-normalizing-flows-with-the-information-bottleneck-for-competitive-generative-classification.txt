The Information Bottleneck (IB) objective uses information theory to formulate a task-performance versus robustness trade-off. It has been successfully applied in the standard discriminative classiﬁcation setting. We pose the question whether the IB can also be used to train generative likelihood models such as normalizingﬂows. Since normalizing ﬂows use invertible network architectures (INNs), they are information-preserving by construction. This seems contradictory to the idea of a bottleneck. In this work, ﬁrstly, we develop the theory and methodology ofIB-INNs, a class of conditional normalizing ﬂows where INNs are trained using the IB objective: Introducing a small amount of controlled information loss allows for an asymptotically exact formulation of the IB, while keeping the INN’s gen-erative capabilities intact. Secondly, we investigate the properties of these models experimentally, speciﬁcally used as generative classiﬁers. This model class offers advantages such as improved uncertainty quantiﬁcation and out-of-distribution de-tection, but traditional generative classiﬁer solutions suffer considerably in clas-siﬁcation accuracy. We ﬁnd the trade-off parameter in the IB controls a mix of generative capabilities and accuracy close to standard classiﬁers. Empirically, our uncertainty estimates in this mixed regime compare favourably to conventional generative and discriminative classiﬁers. Code: github.com/VLL-HD/IB-INN 