This paper considers the problem of designing optimal algorithms for reinforce-ment learning in two-player zero-sum games. We focus on self-play algorithms which learn the optimal policy by playing against itself without any direct super-vision. In a tabular episodic Markov game with S states, A max-player actions and B min-player actions, the best existing algorithm for ﬁnding an approximateNash equilibrium requires ˜O(S2AB) steps of game playing, when only highlight-ing the dependency on (S, A, B). In contrast, the best existing lower bound scales as Ω(S(A + B)) and has a signiﬁcant gap from the upper bound. This paper closes this gap for the ﬁrst time: we propose an optimistic variant of the Nash Q-learning algorithm with sample complexity ˜O(SAB), and a new Nash V-learning algorithm with sample complexity ˜O(S(A + B)). The latter result matches the information-theoretic lower bound in all problem-dependent parameters except for a polynomial factor of the length of each episode. In addition, we present a computational hardness result for learning the best responses against a ﬁxed op-ponent in Markov games—a learning objective different from ﬁnding the Nash equilibrium. 