By chaining a sequence of differentiable invertible transformations, normalizingﬂows (NF) provide an expressive method of posterior approximation, exact density evaluation, and sampling. The trend in normalizing ﬂow literature has been to devise deeper, more complex transformations to achieve greater ﬂexibility. We propose an alternative: Gradient Boosted Normalizing Flows (GBNF) model a density by successively adding new NF components with gradient boosting. Under the boosting framework, each new NF component optimizes a weighted likelihood objective, resulting in new components that are ﬁt to the suitable residuals of the previously trained components. The GBNF formulation results in a mixture model structure, whose ﬂexibility increases as more components are added. Moreover,GBNFs offer a wider, as opposed to strictly deeper, approach that improves existingNFs at the cost of additional training—not more complex transformations. We demonstrate the effectiveness of this technique for density estimation and, by coupling GBNF with a variational autoencoder, generative modeling of images.Our results show that GBNFs outperform their non-boosted analog, and, in some cases, produce better results with smaller, simpler ﬂows. 