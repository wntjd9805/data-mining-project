Ofﬂine reinforcement learning (RL) refers to the problem of learning policies entirely from a large batch of previously collected data. This problem setting offers the promise of utilizing such datasets to acquire policies without any costly or dangerous active exploration. However, it is also challenging, due to the distribu-tional shift between the ofﬂine training data and those states visited by the learned policy. Despite signiﬁcant recent progress, the most successful prior methods are model-free and constrain the policy to the support of data, precluding generaliza-tion to unseen states. In this paper, we ﬁrst observe that an existing model-basedRL algorithm already produces signiﬁcant gains in the ofﬂine setting compared to model-free approaches. However, standard model-based RL methods, designed for the online setting, do not provide an explicit mechanism to avoid the ofﬂine setting’s distributional shift issue. Instead, we propose to modify the existing model-based RL methods by applying them with rewards artiﬁcially penalized by the uncertainty of the dynamics. We theoretically show that the algorithm maxi-mizes a lower bound of the policy’s return under the true MDP. We also characterize the trade-off between the gain and risk of leaving the support of the batch data.Our algorithm, Model-based Ofﬂine Policy Optimization (MOPO), outperforms standard model-based RL algorithms and prior state-of-the-art model-free ofﬂineRL algorithms on existing ofﬂine RL benchmarks and two challenging continuous control tasks that require generalizing from data collected for a different task. 