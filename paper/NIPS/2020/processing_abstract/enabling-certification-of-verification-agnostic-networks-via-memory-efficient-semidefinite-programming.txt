Convex relaxations have emerged as a promising approach for verifying desirable properties of neural networks like robustness to adversarial perturbations. Widely used Linear Programming (LP) relaxations only work well when networks are trained to facilitate veriﬁcation. This precludes applications that involve veriﬁcation-agnostic networks, i.e., networks not specially trained for veriﬁcation. On the other hand, semideﬁnite programming (SDP) relaxations have successfully be applied to veriﬁcation-agnostic networks, but do not currently scale beyond small networks due to poor time and space asymptotics. In this work, we propose a ﬁrst-order dual SDP algorithm that (1) requires memory only linear in the total number of network activa-tions, (2) only requires a ﬁxed number of forward/backward passes through the net-work per iteration. By exploiting iterative eigenvector methods, we express all solver operations in terms of forward and backward passes through the network, enabling efﬁcient use of hardware like GPUs/TPUs. For two veriﬁcation-agnostic networks on MNIST and CIFAR-10, we signiﬁcantly improve ` veriﬁed robust accuracy from 1% 40% respectively. We also demonstrate tight veriﬁcation of a quadratic stability speciﬁcation for the decoder of a variational autoencoder. 88% and 6%ÑÑ 8 