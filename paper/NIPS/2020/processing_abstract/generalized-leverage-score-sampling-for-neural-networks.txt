Leverage score sampling is a powerful technique that originates from theoretical computer science, which can be used to speed up a large number of fundamental questions, e.g. linear regression, linear programming, semi-deﬁnite programming, cutting plane method, graph sparsiﬁcation, maximum matching and max-ﬂow. Re-cently, it has been shown that leverage score sampling helps to accelerate kernel methods [Avron, Kapralov, Musco, Musco, Velingker and Zandieh 17].In this work, we generalize the results in [Avron, Kapralov, Musco, Musco, Vel-ingker and Zandieh 17] to a broader class of kernels. We further bring the leverage score sampling into the ﬁeld of deep learning theory.• We show the connection between the initialization for neural network train-ing and approximating the neural tangent kernel with random features.• We prove the equivalence between regularized neural network and neural tan-gent kernel ridge regression under the initialization of both classical randomGaussian and leverage score sampling. 