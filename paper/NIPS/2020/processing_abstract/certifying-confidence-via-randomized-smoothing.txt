Randomized smoothing has been shown to provide good certiﬁed-robustness guar-antees for high-dimensional classiﬁcation problems. It uses the probabilities of predicting the top two most-likely classes around an input point under a smoothing distribution to generate a certiﬁed radius for a classiﬁer’s prediction. However, most smoothing methods do not give us any information about the conﬁdence with which the underlying classiﬁer (e.g., deep neural network) makes a prediction. In this work, we propose a method to generate certiﬁed radii for the prediction conﬁdence of the smoothed classiﬁer. We consider two notions for quantifying conﬁdence: average prediction score of a class and the margin by which the average prediction score of one class exceeds that of another. We modify the Neyman-Pearson lemma (a key theorem in randomized smoothing) to design a procedure for computing the certiﬁed radius where the conﬁdence is guaranteed to stay above a certain threshold.Our experimental results on CIFAR-10 and ImageNet datasets show that using information about the distribution of the conﬁdence scores allows us to achieve a signiﬁcantly better certiﬁed radius than ignoring it. Thus, we demonstrate that extra information about the base classiﬁer at the input point can help improve certiﬁed guarantees for the smoothed classiﬁer. Code for the experiments is available at https://github.com/aounon/cdf-smoothing. 