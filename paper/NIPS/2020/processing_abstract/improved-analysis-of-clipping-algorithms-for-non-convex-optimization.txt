Gradient clipping is commonly used in training deep neural networks partly due to its practicability in relieving the exploding gradient problem. Recently, Zhang et al. [2020a] show that clipped (stochastic) Gradient Descent (GD) converges faster than vanilla GD/SGD via introducing a new assumption called (L0, L1)-smoothness, which characterizes the violent ﬂuctuation of gradients typically en-countered in deep neural networks. However, their iteration complexities on the problem-dependent parameters are rather pessimistic, and theoretical justiﬁcation of clipping combined with other crucial techniques, e.g. momentum acceleration, are still lacking. In this paper, we bridge the gap by presenting a general frame-work to study the clipping algorithms, which also takes momentum methods into consideration. We provide convergence analysis of the framework in both deter-ministic and stochastic setting, and demonstrate the tightness of our results by comparing them with existing lower bounds. Our results imply that the efﬁciency of clipping methods will not degenerate even in highly non-smooth regions of the landscape. Experiments conﬁrm the superiority of clipping-based methods in deep learning tasks. 