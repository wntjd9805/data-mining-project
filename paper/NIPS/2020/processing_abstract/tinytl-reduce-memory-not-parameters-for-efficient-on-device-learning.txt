On-device learning enables edge devices to continually adapt the AI models to new data, which requires a small memory footprint to ﬁt the tight memory constraint of edge devices. Existing work solves this problem by reducing the number of trainable parameters. However, this doesn’t directly translate to memory saving since the major bottleneck is the activations, not parameters. In this work, we present Tiny-Transfer-Learning (TinyTL) for memory-efﬁcient on-device learning.TinyTL freezes the weights while only learns the bias modules, thus no need to store the intermediate activations. To maintain the adaptation capacity, we introduce a new memory-efﬁcient bias module, the lite residual module, to reﬁne the feature extractor by learning small residual feature maps adding only 3.8% memory overhead. Extensive experiments show that TinyTL signiﬁcantly saves the memory (up to 6.5×) with little accuracy loss compared to ﬁne-tuning the full network. Compared to ﬁne-tuning the last layer, TinyTL provides signiﬁcant accuracy improvements (up to 34.1%) with little memory overhead. Furthermore, combined with feature extractor adaptation, TinyTL provides 7.3-12.9× memory saving without sacriﬁcing accuracy compared to ﬁne-tuning the full Inception-V3. 