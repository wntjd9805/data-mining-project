We study the eigenvalue distributions of the Conjugate Kernel and Neural TangentKernel associated to multi-layer feedforward neural networks. In an asymptotic regime where network width is increasing linearly in sample size, under random ini-tialization of the weights, and for input samples satisfying a notion of approximate pairwise orthogonality, we show that the eigenvalue distributions of the CK andNTK converge to deterministic limits. The limit for the CK is described by iterating the Marcenko-Pastur map across the hidden layers. The limit for the NTK is equiv-alent to that of a linear combination of the CK matrices across layers, and may be described by recursive Ô¨Åxed-point equations that extend this Marcenko-Pastur map.We demonstrate the agreement of these asymptotic predictions with the observed spectra for both synthetic and CIFAR-10 training data, and we perform a small simulation to investigate the evolutions of these spectra over training. 