We study the role of L2 regularization in deep learning, and uncover simple relations between the performance of the model, the L2 coefﬁcient, the learning rate, and the number of training steps. These empirical relations hold when the network is overparameterized. They can be used to predict the optimal regularization parameter of a given model. In addition, based on these observations we propose a dynamical schedule for the regularization parameter that improves performance and speeds up training. We test these proposals in modern image classiﬁcation settings.Finally, we show that these empirical relations can be understood theoretically in the context of inﬁnitely wide networks. We derive the gradient ﬂow dynamics of such networks, and compare the role of L2 regularization in this context with that of linear models. 