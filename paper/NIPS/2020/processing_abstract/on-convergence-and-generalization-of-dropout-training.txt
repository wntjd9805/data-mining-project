We study dropout in two-layer neural networks with rectiÔ¨Åed linear unit (ReLU) ac-tivations. Under mild overparametrization and assuming that the limiting kernel can separate the data distribution with a positive margin, we show that dropout training with logistic loss achieves (cid:15)-suboptimality in test error in O(1/(cid:15)) iterations. 