Recent theoretical works have characterized the dynamics of wide shallow neural networks trained via gradient descent in an asymptotic mean-ﬁeld limit when the width tends towards inﬁnity. At initialization, the random sampling of the parame-ters leads to deviations from the mean-ﬁeld limit dictated by the classical CentralLimit Theorem (CLT). However, since gradient descent induces correlations among the parameters, it is of interest to analyze how these ﬂuctuations evolve. In this work, we derive a dynamical CLT to prove that the asymptotic ﬂuctuations around the mean limit remain bounded in mean square throughout training. The upper bound is given by a Monte-Carlo resampling error, with a variance that depends on the 2-norm of the underlying measure, which also controls the generalization error. This motivates the use of this 2-norm as a regularization term during training.Furthermore, if the mean-ﬁeld dynamics converges to a measure that interpolates the training data, we prove that the asymptotic deviation eventually vanishes in theCLT scaling. We also complement these results with numerical experiments. 