Abstract 1.

Introduction
We present Diffusion in Style, a simple method to adapt
Stable Diffusion to any desired style, using only a small set of target images. It is based on the key observation that the style of the images generated by Stable Diffusion is tied to the initial latent tensor. Not adapting this initial latent tensor to the style makes fine-tuning slow, expensive, and impractical, especially when only a few target style images are available. In contrast, fine-tuning is much easier if this initial latent tensor is also adapted. Our Diffusion in Style is orders of magnitude more sample-efficient and faster. It also generates more pleasing images than existing approaches, as shown qualitatively and with quantitative comparisons.
Generating images of a specific style using large-scale text-to-image models, such as Stable Diffusion [24], is an attractive idea, owing to the high quality of the output im-ages. However, enforcing a coherent style on the generated images is not straightforward. Describing the style in the input textual prompt is often insufficient to obtain images in the desired style. As a consequence, the model needs to be fine-tuned. Yet, current approaches for fine-tuning Stable
Diffusion to a particular style suffer from one or more of the following limitations: results can be far from aesthetically pleasing [40], results may not match the desired style pre-cisely [38, 39], the method may require impractical amounts of data and computational resources [46, 47], or fine-tuned
models may undergo catastrophic forgetting [36].
To generate images, Stable Diffusion uses a U-Net [25] to progressively denoise a tensor in the latent space of a
Variational Auto-Encoder (VAE) [13]. This latent tensor is initially sampled from a standard Gaussian distribution. The
U-Net is conditioned on a textual prompt, preprocessed by a CLIP text encoder [23], to iteratively denoise the noisy latent tensor. Finally, the denoised latent tensor is passed through the VAE decoder to obtain the generated image.
We observe empirically that the initial latent tensors in-fluence the style and layout of generated images. Images generated with the same initial latent tensor and different textual prompts often lead to images with shared attributes, such as similar colors, brightness, and object positioning.
We therefore hypothesize that the standard Gaussian distri-bution, from which the initial latent tensors are sampled, prevents generating images in a desired style.
We propose Diffusion in Style, a new method for adapt-ing Stable Diffusion to a target style. The key idea behind
Diffusion in Style is to start the denoising process with style-relevant initial latent tensors. We obtain the style-specific distribution of initial latent tensors by simply estimating the element-wise mean and standard deviation of the latent en-codings of a small set of target style images. This leaves us, in a second step, with a simple fine-tuning that requires orders of magnitude fewer images and/or training iterations than the previous approaches. Diffusion in Style generates vi-sually pleasing results and does not suffer from catastrophic forgetting. The highlights of Diffusion in Style are: (1) To our knowledge, it is the first method that modifies the initial latent distribution for style adaptation. (2) Diffusion in Style requires only a small amount of images from the target style, typically 50 to 200. This opens the door to many practical applications where thousands of images of the desired style might not be available. Through minor modifications presented in Section 6, our method can also work with as few as 3 target style images. (3) Diffusion in Style is computationally efficient. Fine-tuning the U-Net on the style-specific distribution takes less than 20 minutes on a Tesla V100 GPU.
We evaluate Diffusion in Style quantitatively and qual-itatively, and compare it to existing alternatives: prompt engineering, classical fine-tuning [36], LoRA-based fine-tuning [10, 42], and state-of-the-art image translation [2].
As presented in Figures 6, 7, and 8, Diffusion in Style consis-tently outputs better qualitative results than prior art. 2.