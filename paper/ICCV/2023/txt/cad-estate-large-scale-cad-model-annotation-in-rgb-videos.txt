Abstract ular Scan2CAD benchmark. https://github.com/google-research/cad-estate .
The dataset is available at
We propose a method for annotating videos of com-plex multi-object scenes with a globally-consistent 3D rep-resentation of the objects. We annotate each object with a CAD model from a database, and place it in the 3D coordinate frame of the scene with a 9-DoF pose trans-formation. Our method is semi-automatic and works on commonly-available RGB videos, without requiring a depth sensor. Many steps are performed automatically, and the tasks performed by humans are simple, well-specified, and require only limited reasoning in 3D. This makes them fea-sible for crowd-sourcing and has allowed us to construct a large-scale dataset by annotating real-estate videos from
YouTube. Our dataset CAD-Estate offers 101k instances of 12k unique CAD models placed in the 3D representations of 20k videos. In comparison to Scan2CAD, the largest ex-isting dataset with CAD model annotations on real scenes,
CAD-Estate has 7× more instances and 4× more unique
CAD models. We showcase the benefits of pre-training a
Mask2CAD model on CAD-Estate for the task of automatic 3D object reconstruction and pose estimation, demonstrat-ing that it leads to performance improvements on the pop-1.

Introduction
Semantic 3D scene understanding from images and videos is a major topic in 3D scene understanding, cru-cial for many computer vision applications, ranging from robotics to AR/VR scenarios. The final goal is to detect all objects in the scene, recognize their class, reconstruct their 3D shape, as well as their pose within the overall scene co-ordinate frame. With the advances of scalable deep learning techniques, the field has progressed from reconstructing the 3D shape of one object in a simple image with trivial back-ground [32, 50, 40, 33, 8, 17], to limited reasoning about ob-ject arrangements in simple multi-object scenes [36, 18, 23], and finally to unrestricted multi-object 3D reconstruction in complex real-world scenes [49, 30, 42, 12]. This evolution has been dependent on the availability of ever larger and more diverse data sets for training and evaluation [3, 10, 6, 44, 47, 7, 27, 15, 16]
Existing datasets for Semantic 3D scene understanding
Dataset
SUN RGB-D [44]
PASCAL 3D+ [51]
IKEA [28]
Pix3D [47]
ABO [9]
Objectron [1]
CO3D [37]
Replica [46]
Matterport3D [6]
Scan2CAD [3]
CAD-Estate (Ours)
Type of data image image image image image video video video video video video
Sensor type Multi-object
RGB-D
RGB
RGB
RGB
RGB
RGB
RGB
RGB-D++
RGB-D
RGB-D
RGB
✓
∼
✗
✗
✗
✗
✗
✓
✓
✓
✓
Annotation type 3D box
CAD
CAD
CAD
CAD 3D box object point cloud labels on scene point cloud labels on scene point cloud
CAD
CAD
Requires 3D reasoning Total # objects yes yes limited limited no yes no yes yes yes limited 64.6k 36k 759 10k 6.3k 17k 19k
∼3k 50.8k 14.2k 101k
Table 1. Real 3D scene understanding datasets and their attributes. ‘Multi-object’: whether there is more than one annotated object in the same image/video. ‘Annotation type’: what constitute the annotation for an object. ’Requires 3D reasoning’: whether annotators need to reason in 3D. ’Total’: number object instances with annotations. fall broadly in two categories: synthetic and acquired from real images/videos. The former [27, 15, 16, 45, 41] feature artificial 3D scenes that are manually designed by human artists, and then rendered into synthetic images. While these datasets are relatively large, their images/videos expose a domain gap to real imagery [52, 39, 48, 19, 38, 35].
Acquired datasets [10, 3, 44, 46, 6] annotate 3D objects on real images and videos (Table 2). Such datasets have been limited in size and diversity so far, partly due to limi-tations in their annotation process. They rely on specialized equipment to capture depth images (RGB-D) in order to get a high-quality 3D point cloud reconstruction of the scene.
Humans then annotate objects on this 3D point cloud. How-ever, it is very expensive and cumbersome to go and physi-cally acquire RGB-D videos in the real world, which limits the number of scenes captured, as well as their variety (e.g.
RGB-D sensors struggle outdoors due to sunlight, fail on glossy surfaces, and they have limited depth range). More-over, annotating on 3D point clouds requires expert annota-tors able to reason in 3D.
In this paper, we present the CAD-Estate dataset, which annotates real videos of complex scenes from Real Estate 10k [53] with globally-consistent 3D representations of the objects within them. For each object we find a similar
CAD model from a database, and place it in the 3D coordi-nate frame of the scene with a 9-DoF pose transformation.
We designed a semi-automatic approach which works on commonly-available RGB videos, without requiring a depth sensor, thereby opening the door to annotating many videos readily available on the web. In our approach many steps are performed automatically, and the tasks performed by hu-mans are simple, well-specified, and require only very lim-ited reasoning in 3D. This makes them feasible for crowd sourcing, enabling to distribute work to a large pool of an-notators. In turn, this has allowed us to construct a truly large-scale data set.
CAD-Estate contains 100,882 instances of 12,024 unique CAD models, covering 19,512 videos (Sec. 4). The models span 49 categories, 28 of which with more than 100 objects annotated. In comparison, the largest existing dataset with CAD model annotations on real multi-object scenes (Scan2CAD [3]) has 7× fewer objects (14,225), 4× fewer unique CAD models, 2× fewer categories with more than 100 objects (14) and 13× fewer videos (1,506).
In our experiments, we show that pre-training a modern model for automatic 3D object reconstruction and pose es-timation [23] on CAD-Estate improves performance on the popular Scan2CAD benchmark [3]. Moreover, we estab-lish baseline performance on our own test set, and provide ablation experiments to validate various choices of our an-notation pipeline. 2.