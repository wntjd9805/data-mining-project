Abstract
Cross-modal Unsupervised Domain Adaptation aims to exploit the complementarity of 2D-3D data to overcome the lack of annotation in an unknown domain. However, the training of these methods relies on access to target samples, meaning the trained model only works in a spe-cific target domain.
In light of this, we propose cross-modal learning under bird’s-eye view for Domain Gener-alization (DG) of 3D semantic segmentation, called BEV-DG. DG is more challenging because the model cannot ac-cess the target domain during training, meaning it needs to rely on cross-modal learning to alleviate the domain gap. Since 3D semantic segmentation requires the clas-sification of each point, existing cross-modal learning is directly conducted point-to-point, which is sensitive to the misalignment in projections between pixels and points. To this end, our approach aims to optimize domain-irrelevant representation modeling with the aid of cross-modal learn-ing under bird’s-eye view. We propose BEV-based Area-to-area Fusion (BAF) to conduct cross-modal learning un-der bird’s-eye view, which has a higher fault tolerance for point-level misalignment. Furthermore, to model domain-irrelevant representations, we propose BEV-driven Domain
Contrastive Learning (BDCL) with the help of cross-modal learning under bird’s-eye view. We design three domain generalization settings based on three 3D datasets, and
BEV-DG significantly outperforms state-of-the-art competi-tors with tremendous margins in all settings. 1.

Introduction
Semantic segmentation of LiDAR point clouds is funda-mental for numerous vision applications, such as robotics, autonomous driving and virtual reality. Given a LiDAR
‡Corresponding Author
Figure 1. DG results of methods under different levels of point-to-pixel misalignment. The models are trained on A2D2 and Se-manticKITTI datasets, and tested on nuScenes dataset. Area-to-area methods significantly outperform point-to-point methods un-der each level of misalignment. Moreover, point-to-point methods degrade more dramatically with the increasing misalignment. frame, the goal is to separate each point in the point cloud into a cluster with corresponding semantic labels.
Recently, some 3D semantic segmentation approaches continuously refresh the performance leader-boards on sev-eral benchmark datasets [2, 3, 4, 7, 9]. Nevertheless, the training and testing data for these approaches originate from identical datasets (domains). As each dataset has a dif-ferent configuration of LiDAR sensors, these methods can significantly degrade under domain shift. Specifically, due to the number of laser beams varying from LiDAR to Li-DAR, the obtained point cloud is also quite diverse in terms of density (resolution), which results in a tremendous do-main gap. To improve the generalization of the model,
some Unsupervised Domain Adaptation (UDA) methods
[14, 21, 25, 34, 36] are proposed for point cloud seman-tic segmentation in a single-modal or cross-modal manner.
However, the training of these UDA methods relies on the target domain data, which makes them only generalize well to a specific target domain.
To this end, we are focused on investigating Domain
Generalization (DG) for 3D semantic segmentation. Com-pared to UDA, DG is more challenging as it can not ac-cess the target domain during training, and the model should generalize well to an unseen target domain. Currently, many cross-modal UDA methods [14, 21, 25, 36] are proposed for 3D semantic segmentation. To mitigate the negative effect of domain shift, they utilize cross-modal learning to prompt information interaction between two modalities (image and point cloud). The mechanism behind this idea is that if one modality is sensitive to one type of shift while the other is robust, and the robust modality can guide the sensitive modality. In light of this, we solve the DG problem using cross-modal learning on multi-modal data.
However, current cross-modal learning achieves cross-modal matching through a point-to-point manner, wherein the 2D pixels and 3D points are matched using pre-existing projections. Due to the inaccuracy of the extrinsic calibra-tion between LiDAR and camera [6], there is a more or less point-level misalignment in the projections. As a result, existing cross-modal UDA methods degrade significantly when extending them to DG task. Unlike UDA, which al-lows fine-tuning on the target domain, the target domain is unavailable in the DG setting. Thus these point-to-point cross-modal UDA methods are more sensitive to inaccurate cross-modal matching caused by point-level misalignment, as seen in Fig. 1. Moreover, to model domain-irrelevant representations, some cross-modal UDA methods [25, 36] introduce adversarial learning, which is highly responsive to hyperparameters and challenging to train.
To tackle these concerns, we propose cross-modal learn-ing under BEV for domain generalization of 3D seman-tic segmentation, which is inspired by 3D object detection methods [16, 35, 39] that use the additional bird’s-eye view of one modality (point cloud) to better the target posture and boundary. For different modalities (image and point cloud), with the help of an auxiliary bird’s-eye view, we allevi-ate the cross-modal matching error caused by point-to-pixel misalignment and optimize the domain-irrelevant represen-tation modeling. Specifically, we first propose BEV-based
Area-to-area Fusion (BAF). Instead of conducting cross-modal learning point-to-point, we divide the point cloud and its corresponding image into areas with the help of a unified
BEV space. And then, based on point-to-pixel projections, we match areas from two modalities to conduct area-to-area fusion. The cross-modal matching between areas has a higher fault tolerance for point-level misalignment. Be-cause two projected point and pixel are more likely to be lo-cated in the same area than sharing the same accurate loca-tion. In this way, we significantly mitigate the influence of point-level misalignment and achieve accurate cross-modal learning in an area-to-area manner.
Furthermore, BEV-driven Domain Contrastive Learning (BDCL) is proposed to optimize domain-irrelevant repre-sentation modeling. First, with the aid of cross-modal learn-ing under bird’s-eye view, we generate the BEV feature map in a voxelized manner. This process is greatly af-fected by point cloud density, which makes the BEV feature map highly domain-relevant. Thus, using the BEV feature map to drive contrastive learning can provide stronger su-pervision for learning domain-irrelevant features. However, domain attribute, i.e., LiDAR configuration, is reflected in the global density of the point cloud. Therefore, we pro-pose Density-maintained Vector Modeling (DVM) to trans-form the BEV feature map into a global vector that main-tains density perception. Then, we build contrastive learn-ing that constrains consistency between BEV vectors before and after changing domain attributes. Moreover, as the BEV vectors contain domain-retentive multi-modal information,
BDCL can push both 2D and 3D networks to learn domain-irrelevant features jointly.
We can summarize our contributions as follows:
• We propose BEV-DG for domain generalization of 3D semantic segmentation. With the aid of cross-modal learn-ing under bird’s-eye view, we optimize domain-irrelevant representation modeling in a constraint manner.
• To relive the cross-modal learning from the suffering of misalignment in point-to-pixel projections, we propose
BEV-based area-to-area fusion. The accurate area-to-area cross-modal learning under bird’s-eye view can more effi-ciently promote the information interaction between modal-ities to confront the domain shift.
• We propose BEV-driven domain contrastive learning, where the Density-maintained Vector Modeling (DVM) is introduced to generate the global vector that sufficiently embodies domain attributes. Furthermore, with the help of
Density Transfer (DT), we build contrastive learning based on these vectors, pushing 2D and 3D networks to learn domain-irrelevant features jointly.
• We design three generalization settings based on three 3D datasets and provide the results of some competitors by extending cross-modal UDA methods to the DG setting.
Comprehensive experimental results illustrate that BEV-DG consistently outperforms both the baseline and state-of-the-art approaches across all evaluated generalization scenarios. 2.