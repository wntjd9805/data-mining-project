Abstract
We present an image formation model and optimization procedure that combines the advantages of neural radi-ance ﬁelds and structured light imaging. Existing depth-supervised neural models rely on depth sensors to accurately capture the scene’s geometry. However, the depth maps re-covered by these sensors can be prone to error, or even fail outright. Instead of depending on the ﬁdelity of processed depth maps from a structured light system, a more princi-pled approach is to explicitly model the raw structured light images themselves. Our proposed approach enables the esti-mation of high-ﬁdelity depth maps, including for objects with complex material properties (e.g., partially-transparent sur-faces). Besides computing depth, the raw structured light im-ages also confer other useful radiometric cues, which enable predicting surface normals and decomposing scene appear-ance in terms of a direct, indirect, and ambient component.
We evaluate our framework quantitatively and qualitatively on a range of real and synthetic scenes, and decompose scenes into their constituent components for novel views. 1.

Introduction 3D scene reconstruction lies at the center of ﬁelds like pho-togrammetry, robotics, and digital preservation. However, reconstructing scenes from 2D image supervision alone is under-constrained and classical approaches struggle in tex-tureless regions [25], where ﬁnding correspondences be-tween images is hard. Recent neural rendering techniques like NeRF [18] and other variants [29] are good at novel-view synthesis, but they, too, struggle to reconstruct geometry in scenes with low-texture regions or from few input views.
Many depth cameras alleviate these issues by introducing their own lighting into the scene [9, 24]. For example, active stereo systems (e.g., Intel RealSense [14]) use a projector to illuminate the scene with an (often unknown) light pattern, which adds texture to help solve the stereo correspondence problem. Coded structured light systems use known light patterns to solve correspondences using as few as one camera viewpoint. Such active depth-sensing devices are found in many smartphones and tablets [1, 14, 39], and unlock new (a) Structured light image (b) Ambient image (c) Direct component (d) Indirect component (e) Disparity map (f) Normal map
Figure 1. Scene decomposition of a real scene from a novel view-point. Our proposed framework uses the raw measurements from a single infrared camera on an Intel RealSense to generate a volu-metric representation of the scene. The images (a–f) synthesized from a novel camera viewpoint show the different representations of shape and appearance recovered using our proposed framework.
VR and AR applications. However, these sensors can also fail to reliably estimate depth, especially in cases where light misbehaves [10, 22], e.g., due to light traveling many different paths before reaching a particular camera pixel.
We propose a volumetric image formation model and cor-responding optimization procedure designed to synthesize structured light images under a known projection pattern.
Given a set of raw structured light and ambient-only images captured from different viewpoints, our proposed framework reconstructs scenes through a neural volume rendering pro-cedure [18], recovering a representation of a scene’s shape and appearance from only a few input views. Beyond re-covering the geometry of challenging scenes (e.g., scenes containing translucent objects), our image formation model takes advantage of additional radiometric cues present in the raw structured light images, to solve for normals and
separate images into direct, indirect, and ambient compo-nents; see Figure 1. Through a wide range of experiments on real and synthetic datasets, we explore the advantages of our proposed framework, and provide comparisons to both
NeRF [18] and depth-supervised NeRF baselines [8].
In summary, we provide the following contributions:
• a physically-based neural volume rendering model for multi-view structured light imaging, incorporating shad-ing cues that inform normals and the separation of direct and indirect components;
• an implementation on a widely-available commercial sys-tem, an Intel RealSense camera [14], leading to reliable depth reconstruction performance when compared to base-line approaches and the original RealSense depth;
• a demonstration that our model allows us to tackle new problems with structured light cameras, such as recover-ing geometry through partially transparent surfaces and through ﬁne meshes. 2.