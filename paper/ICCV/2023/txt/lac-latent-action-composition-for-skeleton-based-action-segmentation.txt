Abstract
Skeleton-based action segmentation requires recogniz-ing composable actions in untrimmed videos. Current ap-proaches decouple this problem by first extracting local vi-sual features from skeleton sequences and then processing them by a temporal model to classify frame-wise actions.
However, their performances remain limited as the visual features cannot sufficiently express composable actions. In this context, we propose Latent Action Composition (LAC)1, a novel self-supervised framework aiming at learning from synthesized composable motions for skeleton-based action segmentation. LAC is composed of a novel generation mod-ule towards synthesizing new sequences. Specifically, we design a linear latent space in the generator to represent primitive motion. New composed motions can be synthe-sized by simply performing arithmetic operations on latent representations of multiple input skeleton sequences. LAC leverages such synthesized sequences, which have large di-versity and complexity, for learning visual representations of skeletons in both sequence and frame spaces via contrastive learning. The resulting visual encoder has a high expressive power and can be effectively transferred onto action segmen-tation tasks by end-to-end fine-tuning without the need for additional temporal models. We conduct a study focusing on transfer-learning and we show that representations learned from pre-trained LAC outperform the state-of-the-art by a large margin on TSU, Charades, PKU-MMD datasets. 1.

Introduction
Human-centric activity recognition is a crucial task in real-world video understanding. In this context, skeleton data that can be represented by 2D or 3D human keypoints plays
*Corresponding author. 1Project website: https://walker1126.github.io/LAC/ an important role, as it is complementary to other modalities such as RGB [31, 7, 27, 23, 22, 48, 36, 63, 4] and optical flow [32, 25]. As the human skeleton modality has witnessed a tremendous boost in robustness w.r.t. content changes re-lated to camera viewpoints and subject appearances, the study of recognizing activities directly from 2D/3D skele-tons has gained increasing attention [20, 19, 5, 70, 50, 11, 55, 73, 9, 38, 21, 74]. While aforementioned approaches have achieved remarkable success, such approaches often focus on trimmed videos containing single actions, which constitutes a highly simplified scenario. Deviating from this, in this work, we tackle the challenging setting of action seg-mentation in untrimmed videos based on skeleton sequences.
In untrimmed videos, activities are composable i.e., mo-tion performed by a person generally comprises multiple actions (co-occurrence), each with the duration of a few seconds. Towards modeling long-term dependency among different actions, expressive skeleton features are required.
Current approaches [33, 46, 45, 16] obtain such features through visual encoder such as AGCNs [50] pre-trained on trimmed datasets. However, due to the limited motion in-formation in the trimmed samples, the performance of such features in classifying complex actions is far from satisfac-tory. Towards addressing this issue, we propose to construct synthesized composable skeleton data for training a more ef-fective visual encoder, endowed with strong representability of subtle action details for action segmentation.
In this paper, we propose Latent Action Composition (LAC), a novel framework aiming at leveraging synthesized composable motion data for self-supervised action represen-tation learning. As illustrated in Fig. 1 (left), as opposed to current self-supervised approaches [33, 46, 45, 16], LAC learns action representations in two steps: a first action com-position step is then followed by a contrastive learning step.
Action composition is a novel initialization step to train a generative module that can generate new skeleton sequences by combining multiple videos. As high-level motions are
Figure 1. General pipeline of LAC. Firstly, in the representation learning stage (left), we propose (i) a novel action generation module to combine skeletons of multiple videos (e.g., ‘Walking’ and ‘Drinking’ shown in the top and bottom respectively). We then adopt a (ii) contrastive module to pre-train a visual encoder by learning data augmentation invariant representations of the generated skeletons in both video space and frame space. Secondly (right), the pre-trained visual encoder is evaluated by transferring to action segmentation tasks. difficult to combine directly by the joint coordinates (e.g.,
‘drink’ and ‘sitdown’), LAC incorporates a novel Linear
Action Decomposition (LAD) mechanism within an autoen-coder. LAD seeks to learn an action dictionary to express subtle motion distribution in a discrete manner. Such action dictionary incorporates an orthogonal basis in the latent en-coding space, containing two sets of directions. The first set named ‘Static’ includes directions representing static information of the skeleton sequence, e.g., viewpoints and body size. The other set named ‘Motion’ includes directions representing temporal information of the skeleton sequence, e.g., the primitive dynamics of the action performed by the subject. The new skeleton sequence is generated via a lin-ear combination of the learned ‘Static’ and ‘Motion’ direc-tions. We adopt motion retargeting to train the autoencoder and the dictionary using skeleton sequences with ‘Static’ and ‘Motion’ information built from 3D synthetic data [30].
Once the action dictionary is constructed, in the following contrastive learning step, ‘Static’/‘Motion’ information and action labels are not required and composable motions can be generated from any multiple input skeleton sequences by combining their latent ‘Motion’ sets.
The contrastive learning step aims at training a skeleton visual encoder such as UNIK [73] in a self-supervised man-ner, without the need for action labels (see Fig. 1 (middle)).
It is designed for the resulting visual encoder to be able to maximize the similarity of different skeleton sequences, that are obtained via data augmentation from the same original sequence, across large-scale datasets. Unlike current meth-ods [18, 29, 47, 29, 57, 37, 43, 74] that perform contrastive learning for the video-level representations, we perform con-trastive learning additionally on the frame space to finely maximize the per-frame similarities between the positive samples. Subsequently, the so-trained frame-level skeleton visual encoder is transferred and retrained on action segmen-tation datasets [16, 53].
To assess the performance of LAC, we train the skele-ton visual encoder on the large-scale dataset Posetics [73] and we evaluate the quality of the learned skeleton repre-sentations (see Fig. 1 (right)) by fine-tuning onto unseen action segmentation datasets (e.g., TSU [16], Charades [53],
PKU-MMD [12]). Experimental analyses confirm that ac-tion composition and contrastive learning can significantly increase the expressive power of the visual encoder. The fine-tuning results outperform state-of-the-art accuracy.
In summary, the contributions of this paper include the following. (i) We introduce LAC, a novel generative and contrastive framework, streamlined to synthesize complex motions and improve the skeleton action representation ca-pability. (ii) In the generative step, we introduce a novel
Linear Action Decomposition (LAD) mechanism to repre-sent high-level motion features thanks to an orthogonal basis.
The motions for multiple skeleton sequences can thus be linearly combined by latent space manipulation. (iii) In the contrastive learning step, we propose to learn the skeleton representations in both, video and frame space to improve generalization onto frame-wise action segmentation tasks. (iv) We conduct experimental analysis and show that pre-training LAC on Posetics and transferring it onto an unseen target untrimmed video dataset represents a generic and ef-fective methodology for action segmentation. 2.