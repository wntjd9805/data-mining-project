Abstract
We introduce PointOdyssey, a large-scale synthetic dataset, and data generation framework, for the train-ing and evaluation of long-term fine-grained tracking al-gorithms. Our goal is to advance the state-of-the-art by placing emphasis on long videos with naturalistic motion.
Toward the goal of naturalism, we animate deformable characters using real-world motion capture data, we build 3D scenes to match the motion capture environments, and we render camera viewpoints using trajectories mined via structure-from-motion on real videos. We create combinato-rial diversity by randomizing character appearance, motion profiles, materials, lighting, 3D assets, and atmospheric ef-fects. Our dataset currently includes 104 videos, averaging 2,000 frames long, with orders of magnitude more corre-spondence annotations than prior work. We show that exist-ing methods can be trained from scratch in our dataset and outperform the published variants. Finally, we introduce modifications to the PIPs point tracking method, greatly widening its temporal receptive field, which improves its performance on PointOdyssey as well as on two real-world benchmarks. Our data and code are publicly available at: https://pointodyssey.com 1.

Introduction
In a variety of computer vision tasks, large-scale anno-tated datasets have provided a highway for the development
† Corresponding author of accurate models. In this paper, we aim to provide such a highway for the task of fine-grained long-range tracking.
The goal of fine-grained long-range tracking is: given any pixel coordinate in any frame of a video, track the corre-sponding world surface point for as long as possible.
While there exist multiple generations of datasets tar-geting fine-grained short-range tracking (i.e., optical flow)
[6, 13, 40], and annually updated datasets targeting several forms of coarse-grained long-range tracking (i.e., single-object tracking [20], multi-object tracking [32], video ob-ject segmentation [45]), there are only a handful of works at the intersection of fine-grained and long-range tracking.
Harley et al. [25] and Doersch et al. [16], train fine-grained trackers on unrealistic synthetic data (FlyingTh-ings++ [40, 25] and Kubric-MOVi-E [24]), consisting of random objects moving in random directions on ran-dom backgrounds, and test on real-world videos with sparse human-provided annotations (BADJA [8] and TAP-Vid [16]). While it is interesting that generalization to real video emerges from these models, the use of such simplistic training data precludes the learning of long-range tempo-ral context, and scene-level semantic awareness. We argue that long-range point tracking should not be treated as an extension of optical flow, where naturalism might indeed be discarded without ill effect [50]. Pixels in real video may move somewhat unpredictably, but they take a jour-ney which reflects a variety of modellable factors, including camera shake, object-level motions and deformations, and multi-object relationships such as physical and social inter-actions. Realizing the grand scope of this problem, both in our data and in our methods, is critical for progress.
MPI Sintel [13]
Flyingthings++ [40, 25]
Kubric [24]
TAP-Vid-Kinetics [16]
TAP-Vid-DAVIS [16]
PointOdyssey
Resolution
Frame rate
Avg. trajectory count
Avg. span of trajectories
Avg. frames per video
Training frames
Validation frames
Test frames
Total point annotations
Depth & normals
Segmentation masks
Retargeted motion
Scene randomization
Multiple views
Continuous
Object-object interaction
Human-object interaction
Human-human interaction 436 × 1024 24 436 × 1024 4% 50 1064
-564 7×108
✓
✓
×
×
×
✓
✓
✓
✓ 540 × 960 8 1,024 100% 8 21818 4248 2247 3×108
✓
✓
×
×
×
✓
×
×
× 256 × 256 8
Flexible 100% 24
Flexible
---✓
✓
×
✓
×
✓
✓
×
×
≥ 720 × 1280 25 26.3 30% 250
--297K 8×107
×
×
×
×
×
×
×
✓
✓ 1080 × 1920 25 21.7 30% 67
--1999 4×105
×
×
×
×
×
✓
×
✓
✓ 540 × 960 30 18,700 100% 2,035 166K 24K 26K 4.9×1010
✓
✓
✓
✓
✓
✓
✓
✓
✓
Table 1: Comparison of point tracking datasets. PointOdyssey is larger, has longer videos, and includes trajectories which reflect interactions between the objects and the scene. Note that the TAP-Vid datasets are real-world, with sparse human annotations, and are typically reserved for testing [16], whereas most synthetic datasets provide train/test splits.
We propose PointOdyssey, a large-scale synthetic dataset for the training and evaluation of long-term fine-grained tracking. Our dataset aims to provide the complexity, di-versity, and naturalism of real-world video, with pixel-perfect annotation only possible in simulation. Besides the length of our videos, the key aspects differentiating our work from prior synthetic datasets are (1) we use motions, scene layouts, and camera trajectories mined from real-world videos and motion captures (as opposed to being ran-dom or hand-designed), and (2) we use domain randomiza-tion on a wider range of scene attributes, including environ-ment maps, lighting, human and animal bodies, camera tra-jectories, and materials (similar to Shen et al. [48]). Thanks to progress in the availability of high-quality assets and ren-dering tools, we are also able to deliver better photo-realism than possible in years past.
The motion profiles in our data come from large-scale motion-capture datasets of humans and animals [38, 34].
We use these captures to drive humanoids and animals in outdoor scenes, producing realistic long-range trajectories.
In outdoor scenes, we pair these actors with 3D assets ran-domly scattered on the ground plane, which react to the ac-tors according to physics (e.g., being kicked away as the feet collide with the objects). To produce realistic indoor scenes, we use motion captures of indoor scenes [67, 66], and man-ually replicate the capture environments in our simulator, allowing us to re-render the exact motions and interactions, and preserve their scene-aware nature. Finally, we import camera trajectories computed from real video [35], and at-tach additional cameras to the synthetic humans’ heads, giv-ing challenging multi-view data of the scenes. Our capture-driven approach is in contrast to the mostly random motion patterns used in Kubric [24] and FlyingThings [40]. We hope that our data will encourage the development of track-ing methods which use scene-level cues to provide strong priors on tracking, pushing past the tradition of relying en-tirely on bottom-up cues such as feature-matching.
Our data’s visual diversity stems from a large set of simu-lated assets: 42 humanoid shapes with artist-made textures, 7 animals, 1K+ object/background textures, 1K+ objects, 20 unique 3D scenes, and 50 environment maps. We ran-domize the scene lighting to achieve a wide range of dark and bright scenes. We also render dynamic fog and smoke effects into our scenes, introducing a form of partial occlu-sion entirely missing from FlyingThings and Kubric.
PointOdyssey unlocks a variety of new challenges, one of them being: how to use long-range temporal context.
Since prior datasets have only included short videos for training (< 30 frames, see Table 1), existing models only exploit similarly short temporal context. For example, the current state-of-the-art method Persistent Independent Par-ticles (PIPs) [25], uses an 8-frame temporal window when tracking. As a step toward leveraging arbitrarily long tem-poral context, we propose some modifications to PIPs [25], greatly widening its 8-frame temporal window, and incor-porating a template-update mechanism. Experimental re-sults show that our method achieves higher tracking accu-racy than all existing methods, both on the PointOdyssey test set and on real-world benchmarks.
In summary, the main contribution of this paper is
PointOdyssey, a large-scale synthetic dataset for long-term point tracking, which aims to reflect the challenges—and opportunities—of real-world fine-grained tracking. The dataset, and the code for the simulation engine, are avail-able at: https://pointodyssey.com
2.