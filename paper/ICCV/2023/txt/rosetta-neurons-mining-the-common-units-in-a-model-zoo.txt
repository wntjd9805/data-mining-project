Abstract 1.

Introduction
Do different neural networks, trained for various vision tasks, share some common representations? In this paper, we demonstrate the existence of common features we call
“Rosetta Neurons” across a range of models with differ-ent architectures, different tasks (generative and discrimi-native), and different types of supervision (class-supervised, text-supervised, self-supervised). We present an algorithm for mining a dictionary of Rosetta Neurons across sev-eral popular vision models: Class Supervised-ResNet50,
DINO-ResNet50, DINO-ViT, MAE, CLIP-ResNet50, Big-GAN, StyleGAN-2, StyleGAN-XL. Our ﬁndings suggest that certain visual concepts and structures are inherently em-bedded in the natural world and can be learned by differ-ent models regardless of the speciﬁc task or architecture, and without the use of semantic labels. We can visualize shared concepts directly due to generative models included in our analysis. The Rosetta Neurons facilitate model-to-model translation enabling various inversion-based manip-ulations, including cross-class alignments, shifting, zoom-ing, and more, without the need for specialized training.
* Equal contribution.
One of the key realizations of modern machine learn-ing is that models trained on one task end up being useful for many other, often unrelated, tasks. This is evidenced by the success of backbone pretrained networks and self-supervised training regimes. In computer vision, the pre-vailing theory is that neural network models trained for var-ious vision tasks tend to share the same concepts and struc-tures because they are inherently present in the visual world.
However, the precise nature of these shared elements and the technical mechanisms that enable their transfer remain unclear.
In this paper, we seek to identify and match units that express similar concepts across different models. We call them Rosetta 1 Neurons (see ﬁg. 1). How do we ﬁnd them, considering it is likely that each model would express them differently? Additionally, neural networks are usually over-parameterized, which suggests that multiple neurons can
Project page, code and models: https://yossigandelsman. github.io/rosetta_neurons 1The Rosetta Stone is an ancient Egyptian artifact, a large stone in-scribed with the same text in three different languages. It was the key to deciphering Egyptian hieroglyphic script. The original stone is on public display at the British Museum in London.
Figure 2: Visualization of all the concepts for one class. An example of the set of all concepts emerging for ImageNet “Tench” class by matching the ﬁve discriminative models from Table 2 and clustering within StyleGAN-XL. GAN heatmaps are visualized over one generated image. express the same concept (synonyms). The layer and chan-nel that express the concept would also differ between mod-els. Finally, the value of the activation is calibrated dif-ferently in each. To address these challenges, we care-fully choose the matching method we use. We found that post ReLU/GeLU values tend to produce distinct activation maps, thus these are the values we match. We compare units from different layers between the models while carefully normalizing the activation maps to overcome these differ-ences. To address synonym neurons, we also apply our matching method on a model with itself and cluster units together according to the matches.
We search for Rosetta Neurons across eight differ-ent models: Class Supervised-ResNet50 [13], DINO-ResNet50, DINO-ViT [4], MAE [12], CLIP-ResNet50 [24],
BigGAN [3], StyleGAN-2 [15], StyleGAN-XL [29]. We apply the models to the same dataset and correlate different units of different models. We mine the Rosetta neurons by clustering the highest correlations. This results in the emer-gence of model-free global representations, dictated by the data.
Fig. 2 shows an example image and all the activation maps from the discovered Rosetta Neurons. The activa-tion maps include semantic concepts such as the person’s head, hand, shirt, and ﬁsh as well as non-semantic concepts like contour, shading, and skin tone. In contrast to the cel-ebrated work of Bau et al. on Network Dissection [2, 1], our method does not rely on human annotations or semantic segmentation maps. Therefore, we allow for the emergence of non-semantic concepts.
The Rosetta Neurons allow us to translate from one model’s “language” to another. One particularly useful type of model-to-model translation is from discriminative mod-els to generative models as it allows us to easily visualize the Rosetta Neurons. By applying simple transformations to the activation maps of the desired Rosetta Neurons and optimizing the generator’s latent code, we demonstrate re-alistic edits. Additionally, we demonstrate how GAN inver-sion from real image to latent code improves when the opti-mization is guided by the Rosetta Neurons. This can be fur-ther used for out-of-distribution inversion, which performs image-to-image translation using a regular latent-to-image
GAN. All of these edits usually require specialized training
[8, 14, 38]), but we leverage the Rosetta Neurons to (e.g. perform them with a ﬁxed pre-trained model.
The contributions of our paper are as follows:
• We show the existence of Rosetta Neurons that share the same concepts across different models and training regimes.
• We develop a method for matching, normalizing, and clustering activations across models. We use this method to curate a dictionary of visual concepts.
• The Rosetta Neurons enables model-to-model trans-lation that bridges the gap between representations in generative and discriminative models.
• We visualize the Rosetta Neurons and exploit them as handles to demonstrate manipulations to generated im-ages that otherwise require specialized training. 2.