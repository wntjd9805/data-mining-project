Abstract
Unsupervised learning visible-infrared person re-identification (USL-VI-ReID) is an extremely important and challenging task, which can alleviate the issue of expensive cross-modality annotations.
Existing works focus on handling the cross-modality discrepancy under unsupervised conditions. However, they ignore the fact that
USL-VI-ReID is a cross-modality retrieval task with the hierarchical discrepancy, i.e., camera variation and modal-ity discrepancy, resulting in clustering inconsistencies and ambiguous cross-modality label association. To address these issues, we propose a hierarchical framework to learn grand unified representation (GUR) for USL-VI-ReID.
The grand unified representation lies in two aspects: 1)
GUR adopts a bottom-up domain learning strategy with a cross-memory association embedding module to explore the information of hierarchical domains, i.e., intra-camera, inter-camera, and inter-modality domains, learning a unified and robust representation against hierarchical discrepancy. 2) To unify the identities of the two modalities, we develop a cross-modality label unification module that constructs a cross-modality affinity matrix as a bridge for propagating labels between two modalities. Then, we utilize the homogeneous structure matrix to smooth the propagated labels, ensuring that the label structure within one modality remains unchanged. Extensive experiments demonstrate that our GUR framework significantly outper-forms existing USL-VI-ReID methods, and even surpasses some supervised counterparts. 1.

Introduction
Person re-identification (ReID) aims at matching the same person images captured by non-overlapping cameras
[13, 16]. This technology has been widely investigated due to its significance for social security. Most existing ReID
â€ Corresponding Author.
Figure 1. Illustration of hierarchical discrepancy in USL-VI-ReID with two cameras within each modality as an example. Circles and hexagons represent the sample points of the same person from in-frared and visible modalities, respectively. Different colors repre-sent different cameras and modalities. The inter-camera variation and inter-modality discrepancy collectively result in clustering in-consistencies and ambiguous cross-modality label association. models concentrate on the single-modality image match-ing task with RGB images captured by visible cameras.
However, visible cameras cannot capture enough informa-tion under poor illumination conditions [54]. Hence, visible infrared person re-identification (VI-ReID) has emerged to match person images captured by visible and infrared cam-eras for the 24-hour surveillance system [35, 42, 55].
Existing VI-ReID methods have achieved remarkable performance with deep learning methods [56, 51, 49, 50].
However, the success mainly profits from supervised learn-ing over massive human-labeled data, which is more time-consuming and expensive than manual annotations in single-modality ReID [21, 43]. Recently, unsupervised learning visible infrared person re-identification (USL-VI-ReID) [21, 43, 31] has been proposed to alleviate the issue of expensive cross-modality annotations.
In USL-VI-ReID, unsupervised settings and hierarchi-cal discrepancies in both inter-camera and inter-modality make it more challenging and different from unsupervised single-modality ReID. The inter-camera variation and inter-modality discrepancy collectively form the hierarchical dis-crepancy, which complicates the learning of the USL-VI-ReID model, e.g., leading to clustering inconsistencies and ambiguous cross-modality label association, as illustrated in Fig. 1. The variations between the cameras of the two modalities are different. Visible and infrared cameras have different sensitivities to light. In general, RGB cameras are more susceptible to light and other factors compared with
IR cameras. Large variations may make identities split and small variations may enable identities to merge, leading to inconsistent cluster numbers of the two modalities and sig-nificantly increasing the difficulty of cross-modality label association. More importantly, the hierarchical discrepancy is not simply camera variation plus modality discrepancy, but a complex misalignment of features and cross-modality labels, hindering the retrieval of the same person across dif-ferent modalities. We will show that our approach signif-icantly alleviates clustering inconsistencies in the experi-ments. For better cross-modality retrieval performance, it is desirable to handle the aforementioned hierarchical dis-crepancy. Existing methods [21, 43, 31] for USL-VI-ReID usually focus on solving the problem of modality discrep-ancy. However, they ignored the hierarchical discrepancy, hindering further improvement.
To handle the hierarchical discrepancy in USL-VI-ReID, we put forward a novel grand unified representation (GUR) learning framework to explore the information of hierar-chical domains. GUR adopts a bottom-up domain learn-ing strategy with a cross-memory association embedding (CAE) and cross-modality label unification (CLU) module.
The bottom-up domain learning strategy consists of intra-camera training, inter-camera and inter-modality training.
At the inter-camera and inter-modality training stage, a
CAE module is developed to calculate the association prob-ability embedding between a pedestrian image and each memory item of one domain, and collect the association probabilities of camera or modality of all domains as the unified probability embedding for clustering. To further as-sociate the cross-modality identities, we introduce a CLU module to construct a top-k heterogeneous affinity matrix as the bridge for propagating labels between two modalities and use the homogeneous structure matrix to smooth the propagated labels, ensuring that the label structure within one modality remains unchanged. Finally, with the above bottom-up domain learning strategy with the CAE module and CLU module, our method learns a unified representa-tion, achieving both camera- and modality-invariant prop-erties.
The main contributions are summarized as follows:
- We propose a novel unsupervised learning framework that adopts a bottom-up domain learning strategy with cross-memory association embedding. This enables the model to learn unified representation which is ro-bust against hierarchical discrepancy.
- We design a cross-modality label unification module to propagate and smooth labels between two modalities with heterogeneous affinity matrix and homogeneous structure matrix, respectively, unifying the identities across the two modalities.
- Extensive experiments on the SYSU-MM01 and
RegDB datasets demonstrate that our GUR frame-work significantly outperforms existing USL-VI-ReID methods, and even surpasses some supervised coun-terparts, further narrowing the gap between supervised and unsupervised VI-ReID. 2.