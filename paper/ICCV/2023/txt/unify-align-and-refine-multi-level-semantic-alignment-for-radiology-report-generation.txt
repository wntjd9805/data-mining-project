Abstract (cid:21)(cid:6)(cid:22)(cid:24)(cid:2)(cid:13)(cid:11)(cid:11)(cid:10)(cid:18)(cid:10)(cid:16)(cid:20)(cid:24)(cid:1)(cid:12)(cid:6)(cid:18)(cid:6)(cid:8)(cid:20)(cid:10)(cid:18)(cid:13)(cid:19)(cid:20)(cid:13)(cid:8)(cid:19) (cid:21)(cid:7)(cid:22)(cid:24)(cid:4)(cid:6)(cid:8)(cid:14)(cid:24)(cid:17)(cid:11)(cid:24)(cid:1)(cid:18)(cid:17)(cid:19)(cid:19)(cid:23)(cid:5)(cid:17)(cid:9)(cid:6)(cid:15)(cid:24)(cid:3)(cid:16)(cid:20)(cid:10)(cid:18)(cid:6)(cid:8)(cid:20)(cid:13)(cid:17)(cid:16)(cid:19)
Automatic radiology report generation has attracted enor-mous research interest due to its practical value in reducing the workload of radiologists. However, simultaneously es-tablishing global correspondences between the image (e.g.,
Chest X-ray) and its related report and local alignments between image patches and keywords remains challenging.
To this end, we propose an Unify, Align and then Reﬁne (UAR) approach to learn multi-level cross-modal alignments and introduce three novel modules: Latent Space Uniﬁer (LSU), Cross-modal Representation Aligner (CRA) and Text-to-Image Reﬁner (TIR). Speciﬁcally, LSU uniﬁes multimodal data into discrete tokens, making it ﬂexible to learn common knowledge among modalities with a shared network. The modality-agnostic CRA learns discriminative features via a set of orthonormal basis and a dual-gate mechanism ﬁrst and then globally aligns visual and textual representations under a triplet contrastive loss. TIR boosts token-level local align-ment via calibrating text-to-image attention with a learnable mask. Additionally, we design a two-stage training proce-dure to make UAR gradually grasp cross-modal alignments at different levels, which imitates radiologists’ workﬂow: writing sentence by sentence ﬁrst and then checking word by word. Extensive experiments and analyses on IU-Xray and MIMIC-CXR benchmark datasets demonstrate the supe-riority of our UAR against varied state-of-the-art methods. 1.

Introduction
Automatic radiology report generation, as a potential intelligent assistant to relieve radiologists from the heavy workload, has attracted a surge of research interests in re-cent years [19, 30, 18, 24, 7, 35, 61, 67]. Mainstream methods adopt the de facto encoder-decoder framework, where a medical image (e.g., chest X-ray) is ﬁrst encoded as latent representations via convolutional neural networks
Continuous Visual Signal
Discrete Textual Data
Visual
Encoder
Textual
Encoder
Gap (cid:10)(cid:3)(cid:11)(cid:12)(cid:1)(cid:2)(cid:8)(cid:2)(cid:12)(cid:1)(cid:4)(cid:9)(cid:5)(cid:2)(cid:8)(cid:5)(cid:7)(cid:6)
Diffuse bilateral coarse interstitial markings  are unchanged. No focal consolidation,  pleural effusion, pneumothoraces. 
Cardiomediastinal silhouette is within  normal limits. Degenerative changes of the shoulder. Soft tissues are unremarkable.
Figure 1. Challenges of modeling cross-modal alignments in radi-ology report generation. At the encoding stage, it is intractable to globally align visual and textual semantics due to (a) their different characteristics (i.e., continuous signals vs. discrete data) and (b) the lack of cross-modal interactions. (c) During decoding, capturing the ﬁne-grained alignment between keywords and image patches is difﬁcult because of the data deviation problem. (CNNs) [50, 15], and then further decoded into a radiology report comprised of natural language sentences by recurrent neural networks (RNNs) [16, 10] or fully-attentive networks like Transformer [55]. The key problems in this task are twofold: 1) how to obtain comprehensive information asso-ciated with the input medical image and 2) how to accurately establish cross-modal alignments (CMA), e.g., matching generated words with their corresponding image regions.
This paper targets at improving CMA in automatic ra-diology report generation, which however, is hindered by three factors shown in Figure 1. First of all, continuous visual signals and discrete text data have the very differ-ent characteristics, resulting in semantic inconsistency and modality-independent encoding pipelines in common prac-tices. Secondly, as illustrated in Figure 1 (b), the vision and text modalities are usually encoded via different backbones without cross-modal interactions [7, 67, 43], leading to dis-parities in the representation space. Thirdly, as shown in Fig-ure 1 (c), there exists the so-called “data deviation” problem,
where important details in the report (e.g., abnormalities) are sparse. As a result, the above three factors pose challenges to learning global and local CMA between radiographs and related reports. To this end, although there has emerged pro-gresses for improving either global CMA [7, 61, 67, 43] or local CMA [19, 60, 68, 8], how to exploit multi-level CMA to enhance radiology report generation is underexplored.
Considering the above issues, we propose a Unify, Align and Reﬁne (UAR) framework to facilitate multi-level CMA for generating faithful and believable radiology reports.
Within UAR, we design a Latent Space Uniﬁer (LSU) to tokenize images into discrete visual tokens with a discrete variational autoencoder (dVAE) [44]. By doing so, LSU uni-ﬁes vision and text modalities into discrete tokens, making it ﬂexible to design a shared network that seamlessly pro-cesses both modalities to learn common knowledge [59, 66].
Next, we introduce a modality-agnostic Cross-modal Rep-resentation Aligner (CRA) to learn global CMA. In imple-mentation, CRA not only learns discriminative visual and textual features based on a set of orthonormal basis and a dual-gate mechanism, but also globally aligns both type of features under the supervision of a triplet contrastive loss [48]. What’s more, we improve local CMA by inte-grating Transformer [55] with our proposed Text-to-Image
Reﬁner (TIR). Different from the vanilla attention mecha-nism in Transformer, TIR additionally contains a learnable mask to re-calibrate text-to-image attention activations. TIR constrains the learnable mask with an auxiliary loss to focus word prediction on useful visual information. Last but not least, we design a two-stage training procedure to make the full model grasp CMA at different levels gradually.
We conduct experiments on two widely-adopted radiol-ogy report generation benchmarks, i.e., IU-Xray [12] and
MIMIC-CXR [20]. The results demonstrate that our UAR outperforms state-of-the-art methods by a large margin, e.g., with up to 1.9% and 15% absolute improvements in terms of BLEU-4 [42] and CIDEr [56] on IU-Xray, respectively.
Ablation study is also carried out to understand the effect of each module of our approach.
In brief, our contributions are three-fold:
• To facilitate multi-level cross-modal alignments, we propose a Unify, Align and Reﬁne framework with three modules: Latent Space Uniﬁer (LSU), Cross-modal Repre-sentation Aligner (CRA), and Text-to-Image Reﬁner (TIR).
• LSU uniﬁes vision and text modalities into discrete to-kens, based on which CRA learns to align the global seman-tics of both modalities whereas TIR encourages the token-level text-to-image alignment.
• Extensive experiments and analyzes on IU-Xray and
MIMIC-CXR datasets validate the superiority and effective-ness of our approach, which sets state-of-the-art performance and generates radiology reports accurately. 2.