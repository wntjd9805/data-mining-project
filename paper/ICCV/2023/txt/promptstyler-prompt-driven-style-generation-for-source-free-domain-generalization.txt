Abstract
In a joint vision-language space, a text feature (e.g., from
“a photo of a dog”) could effectively represent its relevant im-age features (e.g., from dog photos). Also, a recent study has demonstrated the cross-modal transferability phenomenon of this joint space. From these observations, we propose
PromptStyler which simulates various distribution shifts in the joint space by synthesizing diverse styles via prompts without using any images to deal with source-free domain generalization. The proposed method learns to generate a variety of style features (from “a S∗ style of a”) via learn-able style word vectors for pseudo-words S∗. To ensure that learned styles do not distort content information, we force style-content features (from “a S∗ style of a [class]”) to be located nearby their corresponding content features (from
“[class]”) in the joint vision-language space. After learning style word vectors, we train a linear classifier using synthe-sized style-content features. PromptStyler achieves the state of the art on PACS, VLCS, OfficeHome and DomainNet, even though it does not require any images for training. 1.

Introduction
Deep neural networks are usually trained with the assump-tion that training and test data are independent and identically distributed, which makes them vulnerable to substantial dis-tribution shifts between training and test data [23, 52]. This susceptibility is considered as one of the major obstacles to their deployment in real-world applications. To enhance their robustness to such distribution shifts, Domain Adaptation (DA) [2, 24, 32, 33, 54, 56, 57, 68] has been studied; it aims at adapting neural networks to a target domain using target domain data available in training. However, such a target domain is often latent in common training scenarios, which considerably limits the application of DA. Recently, a body of research has addressed this limitation by Domain Gener-alization (DG) [3, 5, 21, 29, 35, 37, 74] that aims to improve model’s generalization capability to any unseen domains. It has been a common practice in DG to utilize multiple source domains for learning domain-invariant features [61, 69], but
Figure 1: Motivation of our method. (a) Text features could effectively represent various image styles in a joint vision-language space. (b) PromptStyler synthesizes diverse styles in a joint vision-language space via learnable style word vectors for pseudo-words S∗ without using any images. it is unclear which source domains are ideal for DG, since arbitrary unseen domains should be addressed. Furthermore, it is costly and sometimes even infeasible to collect and annotate large-scale multi-source domain data for training.
We notice that a large-scale pre-trained model might have already observed a great variety of domains and thus can be used as an efficient proxy of actual multiple source domains.
From this perspective, we raised a question “Could we fur-ther improve model’s generalization capability by simulating various distribution shifts in the latent space of such a large-scale model without using any source domain data?” If this
Figure 2: Important factors in the proposed method. PromptStyler learns style word vectors for pseudo-words S∗ which lead to diverse style features (from “a S∗ style of a”) while preserving content information encoded in style-content features (from
“a S∗ style of a [class]”). Lstyle and Lcontent are the loss functions used for maximizing style diversity and content consistency in a hyperspherical joint vision-language space (e.g., CLIP [50] latent space). is possible, DG will become immensely practical by effec-tively and efficiently exploiting such a large-scale model.
However, this approach is much more challenging since any actual data of source and target domains are not accessible but only the target task definition (e.g., class names) is given.
In this paper, we argue that large-scale vision-language models [26, 50, 64] could shed light on this challenging source-free domain generalization. As conceptually illus-trated in Figure 1(a), text features could effectively represent their relevant image features in a joint vision-language space.
Despite the modality gap between two modalities in the joint space [39], a recent study has demonstrated the cross-modal transferability phenomenon [67]; we could train a classifier using text features while running an inference with the classi-fier using image features. This training procedure meets the necessary condition for the source-free domain generaliza-tion, i.e., source domain images are not required. Using such a joint vision-language space, we could simulate various distribution shifts via prompts without any images.
We propose a prompt-driven style generation method, dubbed PromptStyler, which synthesizes diverse styles via learnable word vectors to simulate distribution shifts in a hyperspherical joint vision-language space. PromptStyler is motivated by the observation that a shared style of images could characterize a domain [27, 74] and such a shared style could be captured by a learnable word vector for a pseudo-word S∗ using CLIP [50] with a prompt (“a painting in the style of S∗”) [17]. As shown in Figure 1(b), our method learns a style word vector for S∗ to represent each style.
To effectively simulate various distribution shifts, we try to maximize style diversity as illustrated in Figure 2. Specifi-cally, our method encourages learnable style word vectors to result in orthogonal style features in the hyperspherical space, where each style feature is obtained from a style prompt (“a S∗ style of a”) via a pre-trained text encoder. To prevent learned styles from distorting content information, we also consider content consistency as illustrated in Figure 2. Each style-content feature obtained from a style-content prompt (“a S∗ style of a [class]”) is forced to be located closer to its corresponding content feature obtained from a content prompt (“[class]”) than the other content features.
Learned style word vectors are used to synthesize style-content features for training a classifier; these synthesized features could simulate images of known contents with di-verse unknown styles in the joint space. These style-content features are fed as input to a linear classifier which is trained by a classification loss using contents (“[class]”) as their class labels. At inference time, an image encoder extracts image features from input images, which are fed as input to the trained classifier. Note that the text and image encoders are derived from the same pre-trained vision-language model (e.g., CLIP [50]); the text encoder is only involved in training and the image encoder is only involved at inference time.
The proposed method achieves state-of-the-art results on PACS [34], VLCS [15], OfficeHome [60] and Domain-Net [48] without using any actual data of source and target domains. It takes just ∼30 minutes for the entire training us-ing a single RTX 3090 GPU, and our model is ∼2.6× smaller and ∼243× faster at inference compared with CLIP [50].
Our contributions are summarized as follows:
• This work is the first attempt to synthesize a variety of styles in a joint vision-language space via prompts to effectively tackle source-free domain generalization.
• This paper proposes a novel method that effectively sim-ulates images of known contents with diverse unknown styles in a joint vision-language space.
• PromptStyler achieves the state of the art on domain generalization benchmarks without using any images.
Setup
DA
DG
Source-free DA
Source-free DG
Source Target Task Definition
✓
✓ – –
✓ –
✓ –
✓
✓
✓
✓
Table 1: Different requirements in each setup. Source-free
DG only assumes the task definition (i.e., what should be predicted) without requiring source and target domain data. 2.