Abstract
In this paper, we present StyleLipSync, a style-based per-sonalized lip-sync video generative model that can generate identity-agnostic lip-synchronizing video from arbitrary au-dio. To generate a video of arbitrary identities, we leverage expressive lip prior from the semantically rich latent space of a pre-trained StyleGAN, where we can also design a video consistency with a linear transformation. In contrast to the previous lip-sync methods, we introduce pose-aware masking that dynamically locates the mask to improve the naturalness over frames by utilizing a 3D parametric mesh predictor frame by frame. Moreover, we propose a few-shot lip-sync adaptation method for an arbitrary person by in-troducing a sync regularizer that preserves lip-sync gener-alization while enhancing the person-specific visual infor-mation. Extensive experiments demonstrate that our model can generate accurate lip-sync videos even with the zero-shot setting and enhance characteristics of an unseen face using a few seconds of target video through the proposed adaptation method. 1.

Introduction
In the past few years, advances in deep learning have al-tered the dynamics of video creation. Now, users can easily make and edit videos with the help of deep learning.
In particular, the task of generating a talking head video has received great interest due to its various practical uses. It can be applied in many applications such as film dubbing into a different language, face-to-face live chats, and vir-tual avatars in games and videos. Thus, a lot of prior works
[21, 34, 28, 46, 45, 27] have been studied to generate a talk-ing head video that has accurate lip shapes according to ar-bitrary audio inputs.
Most of the prior works mainly focus on enhancing syn-chronization between lip shapes and audio input. Some of the previous methods [46, 9, 34] use intermediate structural
⋆Equal contribution. representations such as landmarks and 3D models. They predicted the representations from the audio input and syn-thesized a talking head video of a target person. How-ever, they suffered from inaccurate lip-sync results since such representations are too sparse to produce fine-grained details in lip-syncing. Recently, another line of methods
[28, 27] mapped input audio to latent space and leveraged it to construct the mouth region of the target identity. While it achieves satisfactory results in lip-syncing, it generated blurry lower faces which are visually implausible. Further-more, most methods only consider synthesizing frame-by-frame, lacking temporal consistency at the video level.
In this paper, we propose StyleLipSync, a style-based lip-sync video generative model that can generate identity-agnostic lip-synchronizing video from the arbitrary au-dio input. Our model consists of the following compo-nents. First, different from a previous masking method
[21, 28, 27, 6] which masks the entire lower half face, we propose Pose-aware Masking. We analyze that the previous masking method cause unpleasant artifacts and unnatural jaw moving in the generated videos. To circumvent this, we utilize a 3D face mesh predictor [11, 23] and generate lip masks with consideration of pose information and fa-cial semantics such as jaw shape. Second, our image de-coder is based on a style-based generator, namely Style-GAN [18, 19, 17]. StyleGANs have demonstrated their effectiveness in various facial generative tasks, including face editing[1, 2], face enhancement [42], and video gen-eration [35, 24]. As a pre-trained StyleGAN already con-tains expressive and diverse face priors in style latent space
[1], we leverage it to synthesize the high-fidelity lip region of the target person. Furthermore, thanks to the continu-ous and linear nature of the latent space [18, 13, 32], we linearly manipulate the style codes using the audio input to generate lip-synced video frames. Additionally, we pro-pose Style-aware Masked Fusion to effectively adopt a skip-connection to our decoder, which helps to preserve the 2D structure of the image and improves lip fidelity. Finally, we propose a Moving-average based Latent Smoothing module that makes the latent trajectory smoother for enhancing the
temporal consistency in the synthesized talking head video.
While our model can synthesize a talking head video of the target person, there is a slight identity gap between a generated video and the target person. The gap can be no-ticeable, for example, in racial faces, which are relatively scarce in the training data. One approach to addressing this issue is to fine-tune the generator on a few seconds of video of the target person to create a personalized model.
Several fine-tuning methods [30, 25, 37] have already been demonstrated and widely adopted by the industry to achieve product-level quality. However, we analyze that simply fine-tuning the generator loses its ability to generalize to ar-bitrary audio inputs, which is critical for generating talking head videos. Therefore, to minimize the side effect, we pro-pose a sync regularizer enforcing the audio generalization performance. The key idea is to leverage the audio from the training data, not from the target video. Specifically, we not only optimize the generator to reconstruct the target video but also synthesize the video corresponding to the randomly sampled audio from the training data and maintain a sync correlation between the synthesized video and the audio. As a result, we obtain a personalized lip-sync generative model that can synthesize a video of the target person for arbitrary audio. Our contributions are summarized as follows:
• We present StyleLipSync, a lip-sync video generative model which generates lip-synchronizing video in-the-wild of 256 × 256 resolution with accurate and natural lip movement from a given masked video frames, au-dio segment, and single reference image.
• We additionally propose a few-shot adaptation method for entirely unseen faces, which uses only a few sec-onds of video by introducing a sync regularizer to maintain audio generalization.
• Experimental results demonstrate that StyleLipSync achieves state-of-the-art performance in terms of lip-sync and visual quality, even with the zero-shot setting. 2.