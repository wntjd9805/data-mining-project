Abstract
This paper examines the problems of severe image-text misalignment and high redundancy in the widely-used large-scale Vision-Language Pre-Training (VLP) datasets.
To address these issues, we propose an efficient and straightforward Vision-Language learning algorithm called
TL;DR, which aims to compress the existing large VLP data into a small, high-quality set. Our approach consists of two major steps. First, a codebook-based encoder-decoder cap-tioner is developed to select representative samples. Sec-ond, a new caption is generated to complement the origi-nal captions for selected samples, mitigating the text-image misalignment problem while maintaining uniqueness. As the result, TL;DR enables us to reduce the large dataset into a small set of high-quality data, which can serve as an al-ternative pre-training dataset. This algorithm significantly speeds up the time-consuming pretraining process. Specifi-cally, TL;DR can compress the mainstream VLP datasets at a high ratio, e.g., reduce well-cleaned CC3M dataset from 2.82M to 0.67M (∼24%) and noisy YFCC15M from 15M to 2.5M (∼16.7%). Extensive experiments with three pop-ular VLP models over seven downstream tasks show that
VLP model trained on the compressed dataset provided by
TL;DR can perform similar or even better results compared with training on the full-scale dataset1. 1.

Introduction
The recent “scale-is-everything” viewpoint has become a widely accepted notion in the Vision-language Pre-training (VLP) communtity [40, 7, 34, 17, 1]. According to this view, the scale of the data has increased from the original tens of thousands-level (e.g., COCO [26] and VG [20]) to millions-level (e.g., CC3M [40] and CC12M [7]), and even up to billions-level (e.g., YFCC100M [43], WIT400M [34], and LAION400M [39]). Approaches [57, 34, 17] trained on these large-scale data show remarking performance im-*Corresponding Author. 1https://github.com/showlab/datacentric.vlp
Figure 1: Does using more data really lead to better per-formance in VLP? Instead of training on the full-scale
CC3M dataset, we delete data with low image-text match-ing score. We find that BLIP [22] model pretrained on 50% reserved data even obtains better result than full-scale dataset on downstream COCO retrieval [26]. This obser-vation exposes there exists serious misalignment between text&visual modalities and data redundancy in dataset. provement in various downstream tasks.
However, simply scaling-up data brings two critical chal-lenges: i. Larger image-text datasets lead to more training cost (e.g., Pretraining CoCa takes about 5 days on 2,048
CloudTPUv4 chips [57]) and storage overhead, which is difficult to afford. ii. Obtaining high-quality VLP data re-quires massive data and well-designed collecting/filtering pipeline, which is expensive. For instance, the CC3M [40] data was obtained after filtering 5 billion collected images.
These challenges are daunting and may impede the partici-pation of numerous researchers in the VLP community.
In this study, we stop hunting for larger-scale data blindly and ask an important question: Does employing a larger dataset always result in better performance in VLP?
To explore and answer this question, we begin with a simple experiment. First, we utilize a pre-trained BLIP [22] model to calculate the Image-Text Matching (ITM) scores for all samples in the clean CC3M dataset. Subsequently, we re-move a portion of the samples with the lowest ITM scores and evaluate the transfer learning results, as shown in Fig-ure 1. Surprisingly, discarding 50% of the samples slightly improves performance. This remarkable finding challenges the prevailing belief that employing larger amounts of data invariably leads to superior VLP outcomes.
Data Type
Year
Image 2017 2022
Image 2020 Multi-modality
Multi-modality
-Method
Generation/Selection
Dataset Distillation [51]
Generation
Data Pruning [41]
Selection
Neural Data Server [53]
Selection
TL;DR (ours)
Generation+Selection
Table 1: Data-efficient learning methods. "Large-scale" means that the methods are effective when used on datasets that are very large in size. The "task agnostic" means that the methods can be used regardless of the specific downstream task, and without any prior exposure to the associated data.
Compression Ratio↑ 99%-99.99% 20%-30% 94%-98% 75%-90%
Supervision
Class Label
Class Label
Image-text Pairs
Image-text Pairs
Task Agnostic
✗
✗
✗
✓
Large-scale
✗
✓
✓
✓
This experiment suggests removing certain data points can actually improve the model’s ability to learn and gen-eralize. Moreover, considering the performance improve-ments after removing the low ITM score data, we can in-fer the existence of significant misalignment between the textual and visual modalities in many text-image data pairs (see Figure 7 and the supplementary material for more evi-dences). These discoveries present promising potentiality to enhance the performance of models that depend on a smaller volume of VLP data.
Driven by above analysis and recent advance in dataset pruning [41], we present a simple, effective and scalable al-gorithm called TL;DR that aims to improve data efficiency for visual-language pretraining. The TL;DR has a power-ful codebook-based captioner, which contains a visual en-coder, a look-up codebook and a text decoder. Here is how it works: First, TL;DR feeds each image into the visual en-coder and determines the corresponding codes of the image by measuring the similarity between the codebook and the embedding generated by the encoder. Given a large pool of image-text pairs, TL;DR clusters the samples based on their image corresponding codes and selects a representative sub-set of samples from each cluster. Then, TL;DR further re-fines the caption of the selected samples via text decoder to reduce text-image misalignment. By doing so, TL;DR is able to significantly reduce the size of the training dataset while maintaining the high quality.
In this work, we employ TL;DR on widely-used CC3M,
CC12M, YFCC100M and LAION400M datasets and eval-uate small size data on three widely-used frameworks in-cluding CLIP [34], ViLT [19], and BLIP [22] for data efficiency pretraining with seven representative visual-language downstream tasks. The results show that, with only 10% − 25% data obtained by TL;DR, frameworks achieve similar or even better performance compared with the full-scale dataset. We hope our findings can inspire the community to reconsider data efficiency for VLP rather than blindly utilizing increasingly massive datasets. 2.