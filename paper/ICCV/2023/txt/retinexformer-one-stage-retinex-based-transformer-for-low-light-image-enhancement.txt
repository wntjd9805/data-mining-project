Abstract
When enhancing low-light images, many deep learning algorithms are based on the Retinex theory. However, the
Retinex model does not consider the corruptions hidden in the dark or introduced by the light-up process. Besides, these methods usually require a tedious multi-stage training pipeline and rely on convolutional neural networks, show-In ing limitations in capturing long-range dependencies. this paper, we formulate a simple yet principled One-stage
Retinex-based Framework (ORF). ORF first estimates the illumination information to light up the low-light image and then restores the corruption to produce the enhanced image.
We design an Illumination-Guided Transformer (IGT) that utilizes illumination representations to direct the modeling of non-local interactions of regions with different lighting conditions. By plugging IGT into ORF, we obtain our al-gorithm, Retinexformer. Comprehensive quantitative and qualitative experiments demonstrate that our Retinexformer significantly outperforms state-of-the-art methods on thir-teen benchmarks. The user study and application on low-light object detection also reveal the latent practical values of our method. Code is available at https://github. com/caiyuanhao1998/Retinexformer 1.

Introduction
Low-light image enhancement is an important yet chal-lenging task in computer vision. It aims to improve the poor visibility and low contrast of low-light images and restore the corruptions (e.g., noise, artifact, color distortion, etc.) hidden in the dark or introduced by the light-up process.
These issues challenge not only human visual perception but also other vision tasks like nighttime object detection.
Hence, a large number of algorithms have been proposed for low-light image enhancement. However, these existing algorithms have their own drawbacks. Plain methods like histogram equalization and gamma correction tend to pro-duce undesired artifacts because they barely consider the
*Haoqian Wang and Yulun Zhang are corresponding authors
Figure 1. Our Retinexformer significantly outperforms state-of-the-art Retinex-based deep learning methods including DUPE (DeepUPE [49]), ReNet (RetinexNet [54]), KinD [66], and
RUAS [30] on six low-light image enhancement benchmarks. illumination factors. Traditional cognition methods rely on the Retinex theory [27] that assumes the color image can be decomposed into two components, i.e., reflectance and illu-mination. Different from plain methods, traditional meth-ods focus on illumination estimation but usually introduce severe noise or distort color locally because these methods assume that the images are noise- and color distortion-free.
This is inconsistent with real under-exposed scenes.
With the development of deep learning, convolutional neural networks (CNNs) have been applied in low-light im-age enhancement. These CNN-based methods are mainly divided into two categories. The first category directly em-ploys a CNN to learn a brute-force mapping function from the low-light image to its normal-light counterpart, thereby ignoring human color perception. This kind of methods lack interpretability and theoretically proven properties. The second category is inspired by the Retinex theory. These methods [54, 65, 66] usually suffer from a multi-stage train-ing pipeline. They employ different CNNs to decompose the color image, denoise the reflectance, and adjust the illu-mination, respectively. These CNNs are first trained inde-pendently and then connected together to be finetuned end-to-end. The training process is tedious and time-consuming.
In addition, these CNN-based methods show limitations in capturing long-range dependencies and non-local self-similarity, which are critical for image restoration. The re-cently rising deep learning model, Transformer, may pro-vide a possibility to address this drawback of CNN-based methods. However, directly applying original vision Trans-formers for low-light image enhancement may encounter an issue. The computational complexity is quadratic to the in-put spatial size. This computational cost may be unafford-able. Due to this limitation, some CNN-Transformer hybrid algorithms like SNR-Net [57] only employ a single global
Transformer layer at the lowest spatial resolution of a U-shaped CNN. Thus, the potential of Transformer for low-light image enhancement still remains under-explored.
To cope with the above problems, we propose a novel method, Retinexformer, for low-light image enhancement.
Firstly, we formulate a simple yet principled One-stage
Retinex-based Framework (ORF). We revise the original
Retinex model by introducing perturbation terms to the re-flectance and illumination for modeling the corruptions.
Our ORF estimates the illumination information and uses it to light up the low-light images. Then ORF employs a corruption restorer to suppress noise, artifacts, under-/over-exposure, and color distortion. Different from previous
Retinex-based deep learning frameworks that suffer from a tedious multi-stage training pipeline, our ORF is trained end-to-end in a one-stage manner. Secondly, we propose an Illumination-Guided Transformer (IGT) to model the long-range dependencies. The key component of IGT is
Illumination-Guided Multi-head Self-Attention (IG-MSA).
IG-MSA exploits the illumination representations to direct the computation of self-attention and enhance the interac-tions between regions of different exposure levels. Finally, we plug IGT into ORF as the corruption restorer to de-rive our method, Retinexformer. As shown in Fig. 1, our
Retinexformer surpasses state-of-the-art (SOTA) Retinex-based deep learning methods by large margins on various datasets. Especially on SID [9], SDSD [48]-indoor, and
LOL-v2 [59]-synthetic, the improvements are over 6 dB.
Our contributions can be summarized as follows:
• We propose the first Transformer-based algorithm,
Retinexformer, for low-light image enhancement.
• We formulate a one-stage Retinex-based low-light en-hancement framework, ORF, that enjoys an easy one-stage training process and models the corruptions well.
• We design a new self-attention mechanism, IG-MSA, that utilizes the illumination information as a key clue to guide the modeling of long-range dependences.
• Quantitative and qualitative experiments show that our
Retinexformer outperforms SOTA methods on thirteen datasets. The results of user study and low-light detec-tion also suggest the practical values of our method. 2.