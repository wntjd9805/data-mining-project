Abstract
In digital photography, autofocus is a key feature that aids high-quality image capture, and modern approaches use the phase patterns arising from dual-pixel sensors as important focus cues. However, dual-pixel data is prone to multiple error sources in its image capturing process, in-cluding lens shading or distortions due to the inherent opti-cal characteristics of the lens. We observe that, while these degradations are hard to model using prior knowledge, they are correlated with the spatial position of the pixels within the image sensor area, and we propose a learning-based autofocus model with positional encodings (PE) to capture these patterns. Specifically, we introduce RoI-PE, which en-codes the spatial position of our focusing region-of-interest (RoI) on the imaging plane. Learning with RoI-PE allows the model to be more robust to spatially-correlated degra-dations. In addition, we also propose to encode the current focal position of lens as lens-PE, which allows us to sig-nificantly reduce the computational complexity of the auto-focus model. Experimental results clearly demonstrate the effectiveness of using the proposed position encodings for automatic focusing based on dual-pixel data. 1.

Introduction
Automatically focusing to the region of interest is a fun-damental problem in digital photography. When an object of interest is not in focus, the captured image will contain defocus blur, which significantly degrades perceptual im-age quality. Furthermore, an autofocus module usually ap-pears early in the image signal processing (ISP) pipeline of a camera, making this type of blur propagate errors that are difficult to remove.
Existing works on autofocus (AF) mainly fall into two categories: contrast detection autofocus (CDAF) or phase detection autofocus (PDAF). CDAF approach first defines a sharpness metric of an image region of interest (RoI) and then moves the lens back and forth to achieve maximum sharpness. Since CDAF methods require a large number of observations and physical lens movements while searching, they are notoriously slow and power-consuming. Such limi-tations in contrast-based AF algorithms led to an innovative image sensor design, i.e. phase pixels (dual/quad-pixels), which provides an important cue for focus estimation.
PDAF approach determines that a region is in-focus by comparing the disparity between the left and right dual-pixel image. Since the amount of defocus is correlated with the disparity, many methods pre-calibrate the relationship between disparity and focus distance to make the predic-tion of the lens position in a single shot. However, these pre-calibrated predictions can be error-prone due to many physical / optical constraints, such as lens shading, geomet-rical distortions due to optical refraction of the camera lens, or extra variations when combined with optical zoom. Note that PDAF can be formulated as an extremely narrow (sub-pixel level) baseline stereo problem, and it is well known that depth estimation is error-prone with a narrow base-line [14, 26]. Moreover, recent smartphone cameras use smaller pixels compared to DSLRs, which makes them even more sensitive to signal noise and distortions. Individually modeling all error sources is impractical, and we propose a learning-based approach to tackle these problems.
In this paper, we present a novel framework for auto-focus that better leverages the dual-pixel data. We focus on two aspects: 1) improving AF accuracy by handling spatially-varying distortions and 2) improving efficiency to be practically applicable even on low-end smartphones.
First, we speculate that the phase statistic of the dual-pixel data is different w.r.t. its spatial position relative to the full image sensor plane that receives light. Our motivation is illustrated in Fig. 1, where we observe that the same object located at the same depth can have different disparities w.r.t. the coordinates projected on the imaging plane. We also cal-culate the absolute disparity of our full training dataset by moving the lens from index 0∼49 for the patches that have the same ground truth depth (with focal index 10). Ideally, the calculated disparity values should be the same regard-less of the relative position in the image sensor plane1, but 1The ideal setting assumes a thin-lens approximation. To be precise, the disparity values of different position in the imaging plane can be different due to the field curvature of the thick (real-world) lens, meaning that the
4MB SRAM (with float32 precision).
Our contributions can be summarized as follows:
• We propose two positional encodings for camera auto-focus, RoI-PE and lens-PE, that effectively capture the complex characteristics of dual-pixel data.
• The proposed model significantly improves the focus-ing accuracy and the computational complexity.
• We thoroughly analyze the effects of spatial position in the dual-pixel data for various problem settings. 2.