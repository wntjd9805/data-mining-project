Abstract
The quadratic computation complexity of self-attention has been a persistent challenge when applying Transformer models to vision tasks. Linear attention, on the other hand, offers a much more efficient alternative with its linear com-plexity by approximating the Softmax operation through carefully designed mapping functions. However, current linear attention approaches either suffer from significant performance degradation or introduce additional compu-tation overhead from the mapping functions. In this paper, we propose a novel Focused Linear Attention module to achieve both high efficiency and expressiveness. Specifi-cally, we first analyze the factors contributing to the per-formance degradation of linear attention from two perspec-the focus ability and feature diversity. To over-tives: come these limitations, we introduce a simple yet effective mapping function and an efficient rank restoration mod-ule to enhance the expressiveness of self-attention while maintaining low computation complexity. Extensive ex-periments show that our linear attention module is appli-cable to a variety of advanced vision Transformers, and achieves consistently improved performances on multiple benchmarks. Code is available at https://github. com/LeapLabTHU/FLatten-Transformer. 1.

Introduction
Recent years have witnessed the vast development of
Transformer and self-attention in the field of computer vi-sion. With the advent of Vision Transformer [11, 39], self-attention techniques have shown great potential in a variety of vision tasks including image classification [41, 43, 30, 46], semantic segmentation [6, 49], object detec-tion [4, 61, 22], and multi-modal tasks [35, 31].
However, applying Transformer to vision models is a non-trivial task. Unlike lightweight convolution neural net-works [37, 16, 44, 33], the quadratic computation com-plexity O(n2) with respect to sequence length n leads to high computation costs when employing self-attention with
*Equal contribution.
†Corresponding Author.
Figure 1. Difference between Softmax attention and Linear at-tention. Q, K, V ∈ RN×d denote query, key and value matrix respectively. Softmax attention compels to compute the pairwise similarity between queries and keys, and results in the complexity of O(N 2d). Linear attention manages to decouple the Softmax operation with proper approximation and change the computation order by computing K T V first, which leads to the complexity of
O(N d2). Considering that channel dimension d is usually smaller than token number N in modern vision Transformer designs, e.g., d = 64, N = 196 in DeiT [39] and d = 32, N = 49 in Swin Trans-former [24], linear attention modules practically save the overall computation cost while can also enjoy the benefits of a larger re-ceptive field and higher throughput. a global receptive field. Previous works have sought to mit-igate this challenge by confining the global receptive field to a smaller region, such as designing sparse global atten-tion patterns [41, 46] or applying smaller attention win-dows [24, 17]. Albeit effective, these methods are either prone to disregarding informative features in other regions due to their attention patterns or inevitably sacrifice the abil-ity to model long-range dependencies.
Linear attention, on the other hand, has been considered a simple yet effective alternative to address the computa-tion dilemma by reducing the general complexity. Early research leverages a locally-sensitive hashing scheme [21] that compresses the computation complexity from O(n2) to
O(nlog(n)). Nevertheless, it introduces a large constant be-fore the complexity term, which makes it still unaffordable under common cases. More recent studies have noticed that the utilization of Softmax function in the self-attention op-eration practically compels a pairwise computation between all queries and keys, resulting in the predominant O(n2) complexity. To tackle this, several approaches adopt sim-ple activation functions [19, 38] or tailored mapping func-tions [7, 26] to approximate the original Softmax function.
As illustrated in Fig. 1, by changing the computation or-der from (query·key)·value to query·(key·value), the overall computation complexity can be reduced to O(n). However, compared to Softmax attention, current linear attention ap-proaches still suffer from severe performance drop and may involve additional computation overhead from the mapping function, thereby constraining their practical application.
In this paper, we target on the limitations of current linear attention approaches and propose a novel Focused
Linear Attention module, which achieves both high ef-ficiency and expressiveness. Specifically, we undertake a dual-pronged analysis of the factors contributing to the per-formance decline in linear attention and subsequently pro-pose corresponding solutions. First, the distribution of at-tention weight in the former linear attention modules is rel-atively smooth, lacking the focus ability to address the most informative features. As a remedy, we propose a simple mapping function to adjust the feature direction of queries and keys, making the attention weights more distinguish-able. Second, we notice that the diminished rank of the attention matrix curtails the diversity of features in linear attention. To address this, we propose a rank restoration module by applying an additional depthwise convolution (DWC) to the original attention matrix, which helps to re-store the matrix rank and keeps the output feature of differ-ent positions diversified. Leveraging these improved tech-niques, our module demonstrates comparable or superior performance to its Softmax counterparts, while enjoying the benefits of low computation complexity.
We empirically validate the effectiveness of our mod-ule on image classification, semantic segmentation, and ob-ject detection tasks using five advanced vision Transformer models. The results demonstrate consistent improvements over all baselines and other linear attention approaches. 2.