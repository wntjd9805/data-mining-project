Abstract
We propose MAMo, a novel memory and attention frame-work for monocular video depth estimation. MAMo can augment and improve any single-image depth estimation networks into video depth estimation models, enabling them to take advantage of the temporal information to predict more accurate depth. In MAMo, we augment model with memory which aids the depth prediction as the model streams through the video. Specifically, the memory stores learned visual and displacement tokens of the previous time instances. This allows the depth network to cross-reference relevant features from the past when predicting depth on the current frame. We introduce a novel scheme to con-tinuously update the memory, optimizing it to keep tokens that correspond with both the past and the present visual information. We adopt attention-based approach to process memory features where we first learn the spatio-temporal relation among the resultant visual and displacement mem-ory tokens using self-attention module. Further, the output features of self-attention are aggregated with the current vi-sual features through cross-attention. The cross-attended features are finally given to a decoder to predict depth on the current frame. Through extensive experiments on several benchmarks, including KITTI, NYU-Depth V2, and
DDAD, we show that MAMo consistently improves monoc-ular depth estimation networks and sets new state-of-the-art (SOTA) accuracy. Notably, our MAMo video depth esti-mation provides higher accuracy with lower latency, when comparing to SOTA cost-volume-based video depth models. 1.

Introduction
Depth plays a fundamental role in 3D perception. There-fore, accurate depth estimation is critical in various appli-cations, such as autonomous driving, AR/VR, and robotics.
While it is possible to measure depth using LiDAR or Time-of-Flight (ToF) sensors, these sensors are expensive, con-sume a lot of power, require extensive calibration, and can-not generate reliable measurements for certain surfaces. On
*Qualcomm AI Research, an initiative of Qualcomm Technologies, Inc.
Figure 1. Our proposed MAMo (bottom) enables video depth es-timation efficiently in a streaming fashion, by leveraging memory and attention. Monocular depth estimation fails to leverage tem-poral information (top), while existing cost-volume-based video depth models are computationally expensive (middle). For in-stance, for each inference, they require multiple image warping operations as well as significant memory usage and heavy compu-tation to construct the cost volume(s). the other hand, inferring depth from camera images has recently become an cost-efficient and promising alterna-tive. Traditional approaches [42, 14, 35] utilize stereo vi-sion and/or structure-from-motion to estimate depth, which, however, have limited accuracy. By leveraging deep learn-ing, researchers have achieved significantly more accurate image-based depth estimation [11, 13, 3, 38, 62].
Using deep neural networks to infer depth from a sin-gle camera image, i.e., monocular depth estimation,1 has 1In this paper, we refer to depth estimation based on a single image as monocular depth estimation and depth estimation using consecutive frames captured by the same monocular camera as video depth estimation.
been one of the most popular choices. Monocular depth es-timation, however, only predicts depth based on individual images and does not utilize the temporal information from videos, which are almost always available in many appli-cations, e.g., autonomous driving, AR/VR. More recently, researchers have proposed various ways to leverage multi-ple frames for depth estimation. One common approach is to utilize a cost volume (or multiple cost volumes), which is used to evaluate depth hypotheses and can be embedded into a deep learning architecture. Cost volumes have en-abled considerable boost in performance at the expense of high computational complexity and memory usage. Other works propose video depth estimation models without cost volumes, by leveraging recurrent network [63, 36], optical flow [12, 60], and/or attention [8, 55]. While these models can be more computationally efficient as compared to cost volumes, they have not been shown to provide SOTA ac-curacy. Moreover, existing video depth estimation methods do not incorporate the latest developments from monocu-lar depth architectures and as a result, they can underper-form SOTA monocular depth estimation models despite us-ing more information.
In this paper, we propose a novel approach, MAMo, for video depth estimation, which leverages memory and atten-tion to make use of the key temporal information contained in a video. MAMo can be combined with any monocular network (e.g., NeWCRFs [62], PixelFormer [2]) to perform video depth estimation in a streaming fashion. As such, it is complementary to any existing and future developments in monocular depth estimation. Furthermore, it improves depth estimation accuracy being significantly compute effi-cient compared to cost volumes.
Fig. 1 (bottom) provides a high-level outline of our pro-posed MAMo framework. We introduce a memory to aug-ment the depth estimation process as the network goes through the video frames, which maintains learned visual and displacement tokens storing useful information from a set of consecutive previous frames. These tokens are cross-referenced using a cross-attention approach when the net-work derives the depth for the current input frame.
We propose a novel update scheme for memory mod-ule to effectively retain the relevant information from past frames. More specifically, when performing a memory up-date, we first predict depths using the current frame and a synthesized version of it warped from the previous frame using optical flow, respectively. We compare and mini-mize the difference between the two predictions, and back-propagate the gradients to update the memory, with the depth network’s weights frozen. Since the memory tokens are used to cross-attend the respective visual features dur-ing the two forward passes, they are updated to capture fea-tures that is shared across the current frame and the warped previous frame, i.e., the equivariant (w.r.t. motion) features across the current and the previous frames. As we will show, our proposed memory update is more effective compared to sliding window style concatenating
Our main contributions are summarized as follows:
• We introduce MAMo, a novel memory and attention based framework for video depth estimation. MAMo can be combined with any monocular depth network, enabling it to utilize the temporal information to pre-dict more accurate depth.
• In MAMo, we augment model with memory to retain tokens that capture useful information from the previ-ous frames. These tokens are used to assist depth pre-diction of the current input frame, via cross-attention.
• We propose a novel memory update scheme to effec-tively retain the relevant information from past frames.
Specifically, the memory tokens are updated to encode (motion) equivariant features across the current frame and the previous frame.
• We additionally incorporate careful designs to further improve the video depth estimation performance, such as carrying over decoder features from the previous time step.
• We conduct extensive experiments on common depth estimation datasets: KITTI [17], NYU Depth V2 [46], and DDAD [20]. We show that MAMo not only con-sistently improves latest monocular depth networks, but also outperforms existing SOTA video depth esti-mation methods. It is also significantly more efficient as compared to approaches that use cost volumes. 2.