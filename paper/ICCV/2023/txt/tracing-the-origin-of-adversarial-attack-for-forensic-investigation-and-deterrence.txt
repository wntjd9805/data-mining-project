Abstract
Deep neural networks are vulnerable to adversarial at-tacks. In this paper, we take the role of investigators who want to trace the attack and identify the source, that is, the particular model which the adversarial examples are gener-ated from. Techniques derived would aid forensic investiga-tion of attack incidents and serve as deterrence to potential attacks. We consider the buyers-seller setting where a ma-chine learning model is to be distributed to various buyers and each buyer receives a slightly different copy with the same functionality. A malicious buyer generates adversar-ial examples from a particular copy Mi and uses them to attack other copies. From these adversarial examples, the investigator wants to identify the source Mi. To address this problem, we propose a two-stage separate-and-trace framework. The model separation stage generates multiple copies of a model for the same classification task. This pro-cess injects unique features into each copy so that adversar-ial examples generated have distinct and traceable features.
We give a parallel structure which pairs a unique tracer with the original classification model in each copy and a variational autoencoder (VAE)-based training method to achieve this goal. The tracing stage takes in adversarial ex-amples and a few candidate models, and identifies the likely source. Based on the unique features induced by the tracer, we could effectively trace the potential adversarial copy by considering the output logits from each tracer. Empirical results show that it is possible to trace the origin of the ad-versarial example and the mechanism can be applied to a wide range of architectures and datasets. 1.

Introduction
Deep learning models are vulnerable to adversarial at-tacks. By introducing specific perturbations on input sam-*Corresponding author.
Figure 1: Buyers-seller setting. The seller has multiple models Mi, i ∈ [1, m] that are to be distributed to differ-ent buyers. A malicious buyer batt attempts to attack the victim buyers bvics by generating the adversarial examples with his own model Matt. ples, the network model could be misled to give wrong predictions even when the perturbed sample looks visually close to the clean image [4, 8, 21, 27]. There are many exist-ing works on defending against such attacks [9, 12, 16, 20].
Unfortunately, although current defenses could mitigate the attack to some extent, the threat is still far from being com-In this paper, we look into the foren-pletely eliminated. sic aspect: from the adversarial examples, can we deter-mine which model the adversarial examples were generated from? Techniques derived could aid forensic investigation of attack incidents and provide deterrence to future attacks.
We consider a buyers-seller setting [28], which is sim-ilar to the buyers-seller setting in digital copyright protec-tion [19].
Buyers-seller Setting. Under this setting, the seller S dis-tributes m classification models Mi, i ∈ [1, m] to different buyers bi’s as shown in Fig. 1. These models are trained for a same classification task using a same training dataset. The models are made accessible to the buyer as black boxes, for instance, the models could be embedded in hardware such as FPGA and ASIC, or are provided in a Machine Learn-ing as a Service (MLaaS) platform. Hence, the buyer only has black-box access, which means that he can only query the model for the hard label. In addition, we assume that the buyers do not know the training datasets. The seller has
full knowledge and thus has white-box access to all the dis-tributed models.
Attack and Traceability. A malicious buyer wants to at-tack other victim buyers. The malicious buyer does not have direct access to other models and thus generates the examples from his own model and then deploys the found examples. For example, the malicious buyer might gen-erate an adversarial example of a road sign using its self-driving vehicle, and then physically defaces the road sign to trick passing vehicles. Now, as forensic investigators who have obtained the defaced road sign, we want to understand where the adversarial example is generated from and trace the model used in generating the example.
Proposed Framework. There are two stages in our so-lution: model separation and origin tracing. During the model separation stage, given a classification task, we want to generate multiple models that have high accuracy on the classification task and yet are sufficiently different for trac-ing. In other words, we want to proactively enhance differ-ences among the models in order to facilitate tracing. To achieve that, we propose a parallel network structure that pairs a unique tracer with the original classification model.
The role of the tracer is to modify the output, so as to induce the attacker to generate adversarial examples with unique features. We give a variational autoencoder (VAE)-based training method for training the tracer.
During the tracing stage, given m different classification models Mi, i ∈ [1, m] and the found adversarial example, we want to determine which model is most likely used in generating the adversarial example. This is achieved by ex-ploiting the different tracers that are earlier embedded into the parallel models. Our proposed method compares the output logits of those tracers to identify the source.
In a certain sense, traceability is similar to neural net-work watermarking and can be viewed as a stronger form of watermarking. Neural network watermarking schemes [2] attempt to generate multiple models so that an investigator can trace the source of a modified copy. In traceability, the investigator can trace the source based on the generated ad-versarial examples.
Contributions. 1. We point out a new aspect in defending against adver-sarial attacks, that is, tracing the origin of adversarial samples among multiple classifiers. Techniques derived would aid forensic investigation of attack incidents and provide deterrence to future attacks. 2. We propose a framework to achieve traceability in the buyers-seller setting. The framework consists of two stages: a model separation stage, and a tracing stage.
The model separation stage generates multiple “well-separated” models and this is achieved by a parallel network structure that pairs a tracer with the classifier.
The tracing mechanism exploits the characteristics of the paired tracers to decide the origin of the given adversar-ial examples. 3. We investigate the effectiveness of the separation and the subsequent tracing. Experimental studies show that the proposed mechanism can effectively trace to the source.
For example, the tracing accuracy achieves more than 97% when applying to “ResNet18-CIFAR10” task for 5 distributed models. We also observe a clear separation of the source tracer’s logits distribution, from the non-source’s logits distribution (e.g. Fig. 4a). 2.