Abstract
In this paper, we address the problem of weakly super-vised Audio-Visual Video Parsing (AVVP), where the goal is to temporally localize events that are audible or visible and simultaneously classify them into known event cate-gories. This is a challenging task, as we only have access to the video-level event labels during training but need to predict event labels at the segment level during evaluation.
Existing multiple-instance learning (MIL) based methods use a form of attentive pooling over segment-level predic-tions. These methods only optimize for a subset of most discriminative segments that satisfy the weak-supervision constraints, which miss identifying positive segments. To address this, we focus on improving the proportion of pos-itive segments detected in a video. To this end, we model the number of positive segments in a video as a latent vari-able and show that it can be modeled as Poisson binomial distribution over segment-level predictions, which can be computed exactly. Given the absence of fine-grained super-vision, we propose an Expectation-Maximization approach to learn the model parameters by maximizing the evidence lower bound (ELBO). We iteratively estimate the minimum positive segments in a video and refine them to capture more positive segments. We conducted extensive experiments on
AVVP tasks to evaluate the effectiveness of our proposed ap-proach, and the results clearly demonstrate that it increases the number of positive segments captured compared to ex-isting methods. Additionally, our experiments on Temporal
Action Localization (TAL) demonstrate the potential of our method for generalization to similar MIL tasks. 1.

Introduction
Event detection and localization in videos is an impor-tant task for video understanding. Event localization using visual frames has attracted a lot of attention [49, 50, 36, 35, 22, 18, 4, 41, 28, 33, 23, 13]. These methods rely only on visual cues and ignore a crucial modality - audio, which plays an integral part in human perception. To address this, there has been increased attention on audio-visual event de-tection [39, 43], where an event could occur in either of the modalities. Furthermore, audio and visual modalities could aid each other in better event localization.
In this paper, we explore the problem of weakly-supervised Audio-Visual Video Parsing (AVVP) task, where the goal is to detect and localize events using only video-level event labels. Such a formulation is attractive as it forgoes the need for expensive and tedious fine-grained labeling. However, this is a challenging problem due to the absence of segment-level labels during training and the re-quirement to process multi-modal (audio-visual) data in un-constrained videos with varying scene content.
Most previous works [38, 43, 15, 20, 26, 3] use instance-level MIL technique [12] with attentive pooling to model the video-level labels from the segment-level predictions.
Such models are then trained to optimize for the video-level labels. These models are then used to identify the posi-tive segments from segment-level predictions. While re-cent models have shown promising results, they often fail to identify all positive segments of an event due to the MIL-based objective as it implicitly prioritizes the most discrim-inative segments that satisfy weak supervision constraints.
Consequently, the model selects only a subset of the most discriminative incomplete set of positive segments, which are sufficient for correctly classifying the video.
Recent work by Wu et.al [43] focuses on reducing the uncertainty due to the absence of the modality labels by es-timating them from a model trained with video-level weak labels. Inspired by this, we focus on improving temporal localization by capturing all of the positive segments. To achieve this, we explicitly model the number of positive segments in a video (z) and optimize this along with the
MIL objective. We show that the number of positive seg-ments in a video follows the Poisson binomial distribution over segment-level event probabilities, which can be com-puted exactly. We can use the weak-supervision constraint that a positive video must contain at least one positive seg-ment (z ≥ 1) to provide supervision for z and train the model. However, this approach also faces issues similar to that of other MIL-based techniques in accurately localizing events temporally. To overcome the challenge of weak la-bels, we propose an iterative optimization approach using the Expectation-Maximization (EM) algorithm by model-ing the number of positive segments (z) as the latent vari-able. In E-Step, we employ the trained model to estimate the number of positive segments in a video. In the M-step, we optimize for the video-level labels through classifica-tion. Here, we propose the MIL objective using our Poisson binomial formulation with weakly-supervised constraints.
We iteratively optimize for the model parameters and esti-mate the minimum number of positive segments in a video to improve the performance of the task.
We fix our network architecture to that of the HAN [38] and show that our carefully-designed training strategy yields performance gains. We evaluate our approach on the weakly-supervised AVVP task and show that our proposed method achieves state-of-the-art performance. Here, our method achieves improvement in terms of recall and pre-cision, which indicates that it is able to better capture the positive segments. Moreover, while architectural changes may constrain a method to the particular task at hand, inno-vative training strategies can generalize to multiple related tasks. To validate this, we evaluate our approach on the
Temporal Action Localization (TAL) task, which involves a substantially higher number of instances per video com-pared to AVVP. We achieve state-of-the-art results, which show that our proposed approach can generalize effectively to other related tasks as well. Our contributions are:
• We propose to explicitly model and maximize the number of positive segments in a video to improve localization under weak supervision. We show that the number of positive segments follows the Poisson binomial distribution and can be computed exactly from segment-level probabilities in a fully differen-tiable manner.
• Given the absence of explicit supervision on the num-ber of positives within a video, we propose an EM-based optimization and iteratively optimize for the pro-posed Poisson binomial-based MIL loss to boost the total number of positive segments using a model under weak supervision.
• Our experiments on the AVVP task show that our pro-posed approach consistently performs favorably over the state-of-the-art methods on various metrics. Addi-tionally, our experimentation on TAL demonstrates its potential for generalization to similar tasks. 2.