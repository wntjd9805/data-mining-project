Abstract
Knowledge distillation aims to learn a lightweight stu-dent network from a pre-trained teacher network. In prac-tice, existing knowledge distillation methods are usually infeasible when the original training data is unavailable due to some privacy issues and data management consid-erations. Therefore, data-free knowledge distillation ap-proaches proposed to collect training instances from the In-ternet. However, most of them have ignored the common distribution shift between the instances from original train-ing data and webly collected data, affecting the reliability of the trained student network. To solve this problem, we propose a novel method dubbed “Knowledge Distillation between Different Distributions” (KD3), which consists of three components. Speciﬁcally, we ﬁrst dynamically select useful training instances from the webly collected data ac-cording to the combined predictions of teacher network and student network. Subsequently, we align both the weighted features and classiﬁer parameters of the two networks for knowledge memorization. Meanwhile, we also build a new contrastive learning block called MixDistribution to gen-erate perturbed data with a new distribution for instance alignment, so that the student network can further learn a distribution-invariant representation.
Intensive experi-ments on various benchmark datasets demonstrate that our proposed KD3 can outperform the state-of-the-art data-free knowledge distillation approaches. 1.

Introduction
In recent years, advanced deep neural networks (DNNs) have signiﬁcantly succeeded in many computer vision
*Corresponding authors: Chen Gong (chen.gong@njust.edu.cn), Shuo
Chen (shuo.chen.ya@riken.jp).
ﬁelds [19, 21]. However, those excellent DNNs usually have excess learning parameters, which may incur unaf-fordable computation and memory burdens for resource-limited intelligent devices. To address this problem, model compression algorithms have been developed to constrict heavy DNNs into portable ones, mainly including the net-work pruning [28], network quantization [35], and knowl-edge distillation [23].
Most existing compression algorithms are data-driven and rely on massive original training data that is usually inaccessible in the real world. For example, the large-scale
ImageNet [12] requires 138GB of storage and is too heavy to transfer among devices, yet the ResNet34 [22] trained on
ImageNet only needs 85MB memory and can be shared at a relatively low cost. Besides, users may be more willing to share pre-trained models than their personal data, such as photos and travel records. As a result, existing data-driven algorithms for model compression frequently fail to deal with large DNNs in practical applications.
To address this issue, data-free model compression meth-ods have received wide attention in recent studies [6, 10, 13, 16]. Among these methods, data-free knowledge distil-lation has shown encouraging results, which only requires a pre-trained large network (a.k.a. a teacher network) to learn a compact network (a.k.a. a student network). Exist-ing data-free knowledge distillation methods train student network with the guidance of teacher network through the generated pseudo data [7, 45, 50] or real-world data col-lected from the Internet [6]. Generally, the performance of the student networks trained on synthetic data might be suboptimal due to the ﬂawed or distorted synthetic images.
In comparison, the student networks using real-world data from the Internet usually achieve better performance, espe-cially on the tasks involving complicated natural images.
Current data-free knowledge distillation methods [6] that
Original 
Instances (unavailable)
Desired 
Instances
Undesired Instances  with Deviated Styles
Undesired Instances  with Deviated Concepts
Webly Collected Data (available)
D e n s i t y
Sample space
Figure 1. The illustration of distribution shift between the webly collected data and original data, where the original data consists of realistic images of animals. Nevertheless, the webly collected data may include cartoon and sketch images of animals, and even some non-animal images. train student network with data from the Internet (i.e., we-bly collected data) seek to select conﬁdent instances from the collected data, so that they can provide correctly labeled images for training student network. However, the webly collected data and original data may have different distribu-tions, and existing methods usually ignore the distribution shift (e.g., the image style and image category) between them, as shown in Fig. 1. For example, when we are in-terested in classifying various real-world animals and enter
“cat” into the image search engines, we may obtain the im-ages of “cartoon cat” or “cat food”. Apparently, the former is with different styles of cat images, and the latter is even unrelated to our interested animal classiﬁcation task. The student network trained on the webly collected data will in-evitably suffer from distribution shift when it is evaluated on the unseen test data. This makes the performance of stu-dent network trained on the webly collected data obviously lower than that using the original data. Consequently, it is critically important to alleviate the distribution shift be-tween the webly collected data and original data.
To this end, we propose a new data-free approach called
Knowledge Distillation between Different Distributions (KD3) to learn a student network by utilizing the plentiful data collected from the Internet with speciﬁc considerations on the distribution shift. More speciﬁcally, we ﬁrst select the webly collected instances with the similar distribution to original data by dynamically combining the predictions of teacher network and student network during the train-ing phase. After that, to exhaustively learn the informa-tion of teacher network, we share the classiﬁer of teacher network with student network and conduct a weighted fea-ture alignment. In this way, we can encourage student net-work to mimic the feature extraction of teacher network.
Furthermore, a new contrastive learning block MixDistribu-tion is designed to control the statistics (i.e., the mean and variance) of instances, so that we can generate perturbed instances with the new distribution. The student network is encouraged to produce consistent features for the un-perturbed and perturbed instances to learn the distribution-invariant representation, which can generalize to the previ-ously unseen test data. As a result, the student network that precisely mimics teacher network can produce the features that are consistent with teacher network. Finally, these fea-tures fed into the shared classiﬁer can make the predictions as accurate as the corresponding teacher network. Thanks to effectively resolving the distribution shift between the we-bly collected data and original data, our KD3 ﬁnally learns an accurate and lightweight student network, which can achieve comparable performance to those student networks trained on the original data. The contributions of our pro-posed KD3 are summarized as follows:
• We propose a new data-free knowledge distillation method termed KD3, which dynamically selects useful training instances from the Internet by alleviating the distribution shift between the original data and webly collected data.
• We design a weighted feature alignment strategy and a new contrastive learning block to closely match stu-dent network with teacher network in the feature space, so that the student network can successfully learn use-ful knowledge from teacher network for the unseen original data.
• Intensive experiments on multiple benchmarks demon-strate that our KD3 can outperform the state-of-the-art data-free knowledge distillation approaches. 2.