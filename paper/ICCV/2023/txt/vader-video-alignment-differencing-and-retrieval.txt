Abstract
We propose VADER, a spatio- temporal matching, align-ment, and change summarization method to help fight misin-formation spread via manipulated videos. VADER matches and coarsely aligns partial video fragments to candidate videos using a robust visual descriptor and scalable search over adaptively chunked video content. A transformer- based alignment module then refines the temporal localization of the query fragment within the matched video. A space- time comparator module identifies regions of manipulation be-tween aligned content, invariant to any changes due to any residual temporal misalignments or artifacts arising from non- editorial changes of the content. Robustly matching video to a trusted source enables conclusions to be drawn on video provenance, enabling informed trust decisions on content encountered. Code and data are available at https://github.com/AlexBlck/vader 1.

Introduction
Fake news and misinformation are major societal prob-lems, much of it caused by manipulated videos shared to misrepresent events. Detecting manipulation presents only a partial solution; most edited content is not misinforma-tion [22]. Many emerging solutions, therefore, focus on provenance - determining the origins of content, what was changed, and by whom [43, 6]. Securely embedding such an audit trail inside video metadata can help consumers make better trust decisions on content, and is the basis of recent open standards (e.g. C2PA) [11, 1]. However, such metadata can be trivially stripped or replaced.
We propose VADER; a robust technique matching video fragments to a trusted database of original videos (such as those maintained by journalism or publishing organizations, per C2PA). Once matched and temporally aligned, differ-ences between the query-result pair are visualized to high-light any manipulation of the video (ignoring artifacts due to benign, i.e., non-editorial transformation due to renditions).
Any provenance metadata associated with the original may also be displayed. Our technical contributions are: 1. Scalable video fragment (R)etrieval. We learn a ro-bust self-supervised video clip descriptor invariant to various transformations of visual content. Video content is adap-tively chunked and represented via these descriptors, which are aggregated and ranked to match and coarsely localize short video fragments within a large corpus of videos. 2. Fine-grained temporal query (A)lignment. We propose a Transformer based alignment method that performs fine-grained (i.e., frame level) localization of the matched video with the query fragment. 3. Space-time (D)ifferencing to visualize manipulation.
We learn a spatio-temporal model that ‘compares’ the aligned query and matched video in order to identify regions of ma-nipulation. The resulting heatmap ignores any discrepancies introduced due to residual frame misalignments or visual artifacts introduced by non-editorial changes such as quality, resolution, or format changes. 4. Video manipulation dataset. We contribute a dataset of 1042 professionally mANipulated videos and mAsK annota-tIoNs (ANAKIN) to train and evaluate VADER.
We show VADER’s modular design meets three desirable properties we propose for practical video attribution: scal-ability in search; fine-grained localization of the query in the returned clip; and visualization of areas of manipulation.
We evaluate all three properties on our new dataset of ma-nipulated video content (ANAKIN) in addition to standard video benchmarks (VCDB [33] and Kinetics-600 [37]). We believe VADER is a promising step toward addressing fake news by helping users make more informed trust decisions on the video content they view online. 2.