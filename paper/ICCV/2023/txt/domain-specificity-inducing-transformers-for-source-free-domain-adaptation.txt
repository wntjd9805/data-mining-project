Abstract
Conventional Domain Adaptation (DA) methods aim to learn domain-invariant feature representations to improve the target adaptation performance. However, we moti-vate that domain-specificity is equally important since in-domain trained models hold crucial domain-specific prop-erties that are beneficial for adaptation. Hence, we propose to build a framework that supports disentanglement and learning of domain-specific factors and task-specific factors in a unified model. Motivated by the success of vision trans-formers in several multi-modal vision problems, we find that queries could be leveraged to extract the domain-specific factors. Hence, we propose a novel Domain-Specificity in-ducing Transformer (DSiT) framework 1 for disentangling and learning both domain-specific and task-specific factors.
To achieve disentanglement, we propose to construct novel
Domain-Representative Inputs (DRI) with domain-specific information to train a domain classifier with a novel domain token. We are the first to utilize vision transformers for do-main adaptation in a privacy-oriented source-free setting, and our approach achieves state-of-the-art performance on single-source, multi-source, and multi-target benchmarks. 1.

Introduction
Machine learning models often fail to generalize to unseen domains due to the discrepancy between training (source) and test (target) data distributions (i.e. domain shift). This results in poor deployment performance and is also critical for applications like autonomous driving [9] or medical imaging [12]. Unsupervised domain adapta-tion (DA) techniques seek to address this challenging sce-nario by transferring task-specific knowledge from a labeled source domain to an unlabeled target domain. However, DA works [15] require concurrent access to source and target data. Such a constraint is highly impractical since data tends to be proprietary and cannot be easily shared. Hence, we fo-cus on Source-Free DA [31] that operates under a practical
*Equal Contribution 1Project Page: http://val.cds.iisc.ac.in/DSiT-SFDA/
Figure 1. A. We induce domain-specificity by disentangling domain- and task-specific factors within the model. A task-label-destructive transform produces novel Domain-Representative In-puts (DRI) to learn domain-specific factors via domain classifica-tion. B. Conventional DA methods preserve domain-invariance, resulting in only task-oriented clusters in the feature space. C.
Our proposed disentanglement ensures that different domains are well-clustered. setting of sharing only the source model between a vendor and a client, without any data-sharing.
Conventional Domain Adaptation works [15] aim to learn task-discriminative features that are domain-invariant, i.e. indistinguishable w.r.t. domain shift. Intuitively, if the feature distributions of the source and target domain are similar, then a low error on the source domain translates to the target, shown theoretically by [3]. But domain-invariance does not always result in optimal target perfor-mance because a specific domain might be far away from the support of a domain-invariant model [14]. Further, su-pervised in-domain trained models (where train and test datasets come from the same domain) usually perform bet-ter as they hold useful domain-specific properties. Thus, we motivate the concept of domain-specificity to improve the target adaptation performance.
In source-free DA, while adapting a model to a new
target domain, the major problem is to preserve the task knowledge from the source domain. Prior works address this by aligning the target model feature space with that of the source [37]. However, we argue that it is equally impor-tant for a model to learn domain-specific information while preserving task-specific knowledge. Hence, in our work, we seek a solution to an important question, “How do we develop a framework that enables us to improve domain-specificity while also retaining task-specificity?”.
Thus, we seek a framework that supports the disentan-glement of task-specific factors and domain-specific fac-tors, thereby allowing us better control over them. A well-disentangled framework would allow the learning of both domain-specific and task-specific factors in the model si-multaneously, as shown in Fig. 1A. This not only yields better performance but also ensures that the different do-mains are well-clustered in the feature space of the model (Fig. 1C) compared to the baseline (Fig. 1B). However, the key question remains, “How do we devise a method for the disentanglement and learning of domain-specific and task-specific factors?”
Recently, transformers have demonstrated remarkable performance in several vision tasks [26, 39]. They con-tain a multi-head self-attention mechanism that attends to all image patches and provides a global context. Motivated by this fundamental difference of a global context, we ex-plore the possibility of a disentanglement framework with transformers as domain information is inherently a global, higher-order statistic [8] that may not be adequately cap-tured in CNNs. Inspired by the multi-modal works [21] that utilize queries to extract domain-specific information from a particular modality, we propose to enable the disentan-glement of domain-specificity through the query weights as part of our novel Domain-Specificity inducing Transformer (DSiT) framework.
Concretely, we induce domain-specificity by updat-ing only the query weights via domain classifier training (Fig. 1A). The remaining weights are updated via task classifier training. To further inculcate the disentangle-ment, we train the domain-classifier with novel Domain-Representative Inputs (DRI) where a task-label-destructive transform removes the task-specific information. We also propose a novel domain-specificity disentanglement crite-rion to evaluate the disentanglement of domain-specific and task-specific factors and demonstrate the disentanglement of our proposed DSiT.
We outline the contributions of our work as follows:
• We investigate and provide insights on how domain-specificity can be leveraged to improve DA. To this end, we propose a novel, unified Domain-Specificity inducing Transformer (DSiT) to disentangle and learn task-specific and domain-specific factors.
• We utilize query weights to enable the disentanglement in DSiT with a novel training algorithm that well sup-ports our insights. We also introduce novel Domain-Representative Inputs (DRI) to further enhance the dis-entanglement.
• We define a novel domain-specificity disentanglement criterion to determine if the domain-specific and task-specific factors are well-disentangled.
• We achieve state-of-the-art performance on source-free benchmarks across single-source, multi-source, and multi-target DA while also introducing the first source-free DA benchmarks for transformers. 2.