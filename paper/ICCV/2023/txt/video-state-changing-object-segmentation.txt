Abstract
Daily objects commonly experience state changes. For example, slicing a cucumber changes its state from whole to sliced. Learning about object state changes in Video
Object Segmentation (VOS) is crucial for understanding and interacting with the visual world. Conventional VOS benchmarks do not consider this challenging yet crucial problem. This paper makes a pioneering effort to introduce a weakly-supervised benchmark on Video State-Changing
Object Segmentation (VSCOS). We construct our VSCOS benchmark by selecting state-changing videos from existing datasets. In advocate of an annotation-efficient approach to-wards state-changing object segmentation, we only annotate the first and last frames of training videos, which is different from conventional VOS. Notably, an open-vocabulary set-ting is included to evaluate the generalization to novel types of objects or state changes. We empirically illustrate that state-of-the-art VOS models struggle with state-changing objects and lose track after the state changes. We analyze the main difficulties of our VSCOS task and identify three technical improvements, namely, fine-tuning strategies, rep-resentation learning, and integrating motion information.
Applying these improvements results in a strong baseline for segmenting state-changing objects consistently. Our benchmark and baseline methods are publicly available at https://github.com/venom12138/VSCOS. 1.

Introduction
Object state changes are common in the real world. For example, when slicing a cucumber, the cucumber’s state changes from whole to sliced. Humans learn commonsense knowledge about actions and associated objects by memo-rizing the state change in a certain time period [12]. Un-derstanding state changes in visual perception tasks, for
*Equal contribution.
Figure 1. We propose Video State-Changing Object Segmentation (VSCOS), which is significantly more challenging than conven-tional VOS. State-of-the-art VOS model XMem [2] performance drops from 89.5 J &F (Jaccard & F-Score) to 66.7 J &F on our benchmark, because it fails to associate drastically changing object appearance. Best viewed in color with zoom. example, video object segmentation (VOS), is also crucial for autonomous agents to interact safely and efficiently with objects. In the example of slicing a cucumber, without state change knowledge, an autonomous agent might not know how to pick up and cut the cucumber such that it becomes slices. However, objects under state changes are largely ig-nored in previous VOS research. Existing VOS benchmarks tend to focus on normal objects, while overseeing the sig-nificantly more difficult state-changing ones with shifting appearances.
This work investigates this under-explored problem of object state change in VOS. To the best of our knowledge, we are the first to formally define the task of Video State-Changing Object Segmentation (VSCOS). VSCOS aims to predict pixel-wise masks of state-changing objects in each frame of the video, given the first frame mask as reference.
In an effort to facilitate research on the VSCOS task, our first contribution is to construct a dedicated benchmark that reveals the failure of existing VOC methods and identifies
easily extended to different video datasets. Doing so also encourages the advancement of weakly-supervised VSCOS methods. Based on the previous discovery that the key diffi-culty in VSCOS is the association, our setting provides two annotated frames for each training video, namely, the first and last frames. At test time, only the first frame mask is provided for online inference.
Another principle is that we propose an open-vocabulary setting alongside our conventional setting. This setting aims to test the models’ generalization to novel state changes and objects that are previously unseen in training. Our open-vocabulary setting simulates a practical scenario where the trained model may encounter new types of objects under seen state changes, new types of state changes on seen objects, or even completely novel state changes and objects. This setting advocates models that do not overfit to categories seen in training, but generalize to the complex scenarios in the open world.
Based on our proposed VSCOS benchmark, we investi-gate how to adapt any existing VOS models to enable robust segmentation for state-changing objects, and propose our baseline method. Our baseline contains three components centered around solving the key difficulty of segmentation association before and after the state changes. First, we design an effective fine-tuning method that explicitly tack-les the association problem with cycle consistency and a teacher-student loss. Our fine-tuning strategy significantly improves VSCOS performance, while avoiding training in-stability and trivial solutions. Then, we point out a promising direction in improving feature representation for VSCOS.
Specifically, the features for the object region before and after the state changes should be aligned, while both should be distinguished from the background feature. As an ini-tial approach, we adapt Contrastive Random Walk [7] to be an auxiliary loss and it demonstrates a noticeable perfor-mance improvement. Finally, we explore whether motion information in the form of optical flow could assist VSCOS in connecting the states before and after the changes. We design a simple approach to fuse flow features into VOS models and also observe a minor improvement. Here we do not claim that our baseline method is necessarily an optimal strategy, but it points out key research directions for VS-COS including fine-tuning, feature learning, and integrating motion information.
We further analyze the results of our baseline method on VSCOS from different perspectives. For example, we investigate the contribution of different design decisions, the performance comparison for different action categories, as well as the different phenomena in different sets of the open-vocabulary setting. From these experiments, we draw empir-ical conclusions on how to improve VSCOS performance.
Finally, we observe and categorize key failure cases of our baseline model and the main difficulties of our VSCOS task.
Figure 2. Visualization of our pilot study. On an example of cutting a cucumber, we apply a state-of-the-art VOS model XMem [2].
Provided with the first frame mask as a reference (top row), the model segments the whole cucumber but omits the sliced pieces.
Similarly, provided with the last frame mask as a reference (bottom row), the model segments the sliced pieces but loses track of the whole part of the cucumber. This shows the state-of-the-art model segments the object in each individual state but fails to associate the segmentation when the state change happens. This key difficulty motivates us to derive the setting for our VSCOS task. Best viewed in color with zoom. the key challenges of the VSCOS task. As shown in the example in Figure 1, on conventional VOS benchmarks (e.g.,
DAVIS-2017 [11]), the model is expected to segment nor-mal objects that do not experience major state or appearance changes like camels. Therefore, state-of-the-art VOS meth-ods (e.g., XMem [2] or DeAOT [23]) could satisfactorily segment these objects in videos, by matching the current frame’s visual appearance to the reference frames or previ-ous frames’ predictions. By contrast, on our benchmark, the state-changing objects have large appearance changes. Given the first frame mask as reference, the model fails to coher-ently segment both the whole cucumber and the slices before and after the cutting action, which is notably more challeng-ing. Correspondingly, the performance of the state-of-the-art
VOS model XMem drops from 89.5 J &F on DAVIS-2017 to 66.7 J &F on our benchmark. This result highlights the significance of learning about object state changes in VOS.
We further show in Figure 2 that state-of-the-art VOS methods fail on our VSCOS benchmark, because they lose track of objects when state changes happen. This phe-nomenon indicates that the state-of-the-art VOS models lack the understanding of the identity of objects experiencing state changes. Therefore, we highlight a key difficulty of our
VSCOS task as that the existing VOS models can segment the object in each individual state reasonably given the cor-rect reference, but cannot associate the segmentation before and after the state change.
Our benchmark also possesses several desirable proper-ties, as we have constructed it following two crucial prin-ciples. Primarily, our VSCOS benchmark should be es-tablished in an annotation-efficient fashion, so it could be
Dataset
Perspective
Annotated Length (h)
# of Action Categories
Segmentation Label Open-Vocabulary Setting
[1]
[8]
ChangeIt [15]
VSCOS (Ours)
Third-person
Egocentric
Third-person
Egocentric 1 3 48 4 7 14 44 271
✗
✗
✗
✓
✗
✗
✗
✓
Table 1. Comparison of dataset statistics between our VSCOS benchmark and previous work. Our VSCOS benchmark features the most varied action categories, as well as fine-grained segmentation labels and open-vocabulary settings.
We also discuss the limitations of our approach and future work.
To summarize, our contributions are three-fold: (1) We propose a crucial yet under-investigated problem of Video State-Changing Object Segmentation (VSCOS). (2) We annotate a state-changing VOS dataset and build our VSCOS benchmark. Our benchmark is annotation effi-cient and contains an open-vocabulary setting to evaluate the model’s generalization capability. (3) We identify the key difficulty of our task: as-sociating the object segmentation before and after state changes. Based on this observation, we establish a model-agnostic baseline method that adapts existing VOS models for VSCOS. Our baseline method points out key research directions for the VSCOS task. We present and analyze the baseline results, as well as the key challenges of our benchmark. 2.