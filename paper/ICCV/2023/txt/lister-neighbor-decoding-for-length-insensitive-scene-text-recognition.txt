Abstract
The diversity in length constitutes a significant charac-teristic of text. Due to the long-tail distribution of text lengths, most existing methods for scene text recognition (STR) only work well on short or seen-length text, lack-ing the capability of recognizing longer text or perform-ing length extrapolation. This is a crucial issue, since the lengths of the text to be recognized are usually not given in advance in real-world applications, but it has not been ad-equately investigated in previous works. Therefore, we pro-pose in this paper a method called Length-Insensitive Scene
TExt Recognizer (LISTER), which remedies the limitation regarding the robustness to various text lengths. Specifi-cally, a Neighbor Decoder is proposed to obtain accurate character attention maps with the assistance of a novel neighbor matrix regardless of the text lengths. Besides, a
Feature Enhancement Module is devised to model the long-range dependency with low computation cost, which is able to perform iterations with the neighbor decoder to enhance the feature map progressively. To the best of our knowl-edge, we are the first to achieve effective length-insensitive scene text recognition. Extensive experiments demonstrate that the proposed LISTER algorithm exhibits obvious supe-riority on long text recognition and the ability for length ex-trapolation, while comparing favourably with the previous state-of-the-art methods on standard benchmarks for STR (mainly short text)1. 1.

Introduction
Scene text recognition (STR) is a popular topic in the computer vision community [40, 41, 42, 34, 54, 18, 36, 48, 5], which aims at extracting machine-readable sym-bols from scene text images. Recently, a variety of works have pushed forward the recognition performance from the perspective of arbitrary-shaped text [42, 28, 57], com-bining language models [42, 54, 18, 36, 47], etc. How-ever, sequence length, as a vital characteristic of text, is 1https://github.com/AlibabaResearch/AdvancedLiterateMachinery
Figure 1. Different attention mechanisms for STR. Î± in green is the attention maps produced by: (a) complicated RNN in a serial way (Transformer decoder does similar), (b) pre-defined learnable limited queries in a parallel way, (c) the proposed novel neighbor matrix in a serial but simple and efficient way. rarely discussed. In fact, instances of long text occur fre-quently in websites, compound words, text lines, codes and multi-lingual scenarios [7]. As revealed in previous stud-ies [19, 49], existing methods cannot handle images with long text very well. We regard it as a key issue worthy of in-depth investigation. Concretely, we ought to systemati-cally analyze the performance of existing methods on text of different lengths and explore an effective way to realize length-insensitive STR.
According to our observation, decoders in scene text recognition models are directly associated with the objec-tive of text prediction, thus they are highly likely to play an important role in length-insensitive STR. In previous works, there are three types of text decoders: CTC [20, 40]-based decoder, serial attention decoder [42, 33, 5], and parallel attention decoder [49, 54, 18]. The CTC-based decoder makes dense prediction on the feature sequence, and then re-organizes the text by a pre-defined rule. It is efficient, robust to long text recognition, and can be applied to multi-line recognition [53, 50], but has some trouble in feature learning [30, 22]. The serial attention decoder (Fig. 1(a)) adopts RNN or the Transformer decoder to predict charac-ters step by step, which is slow in inference, and may en-ible. Image patches or token features, including the noisy background, are all fed into the Transformer layers, which is a little bit redundant and expensive for GPU memory [58], especially for a large input size. Besides, the absolute posi-tional encoding in Transformer layers also restricts the abil-ity for length-insensitive STR.
Aware of the challenges above, we propose a Length-Insensitive Scene TExt Recognizer (LISTER), which in-corporates a novel robust Neighbor Decoder (ND) and a lightweight Feature Enhancement Module (FEM). In ND, the attention mechanism is still utilized to ensure the effec-tiveness. Like the serial attention-based decoder, we regard the decoding process as a linked list data structure, so each character can be aligned by its previous neighbor regardless of its absolute position in the string. However, ND relies on a novel neighbor matrix where the next neighbor (charac-ter) locations of all the points in the feature map are indi-cated in a soft way, as shown in Fig. 1(c), which is simple but effective for length-insensitive text decoding. To model the long-range feature dependency with low computation cost, we propose to feed only the aligned character features to the Transformer layers and then enhance the whole fea-ture map in FEM. The self-attention layers adopt the sliding window [6] to fit arbitrary-length sequences. Our contribu-tions are summarized as follows: 1) A length-insensitive scene text recognizer, LISTER, is proposed with neighbor decoder as its core module.
To the best of our knowledge, it is the first attempt to realize effective length-insensitive text recognition. 2) We propose a Feature Enhancement Module that en-hance the whole feature map by Transformer layers with low computation cost. 3) Through extensive experiments, we prove that LIS-TER is on par with the previous state-of-the-arts on the common STR benchmarks, while outperforming them on long text. Besides, LISTER also achieves excellent performance in terms of length extrapolation. 2.