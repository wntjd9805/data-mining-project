Abstract
State-of-the-art visual grounding models can achieve high detection accuracy, but they are not designed to dis-tinguish between all objects versus only certain objects of interest. In natural language, in order to specify a partic-ular object or set of objects of interest, humans use deter-miners such as “my”, “either” and “those”. Determiners, as an important word class, are a type of schema in nat-ural language about the reference or quantity of the noun.
Existing grounded referencing datasets place much less em-phasis on determiners, compared to other word classes such as nouns, verbs and adjectives. This makes it difficult to de-velop models that understand the full variety and complex-ity of object referencing. Thus, we have developed and re-leased the DetermiNet dataset 1, which comprises 250,000 synthetically generated images and captions based on 25 determiners. The task is to predict bounding boxes to iden-tify objects of interest, constrained by the semantics of the given determiner. We find that current state-of-the-art vi-sual grounding models do not perform well on the dataset, highlighting the limitations of existing models on reference and quantification tasks. 1.

Introduction
Humans combine visual and linguistic cues to perform object localization, referencing and quantification tasks on a daily basis. For example, when someone says “pass me a cup”, we first locate any cups present, and then select one cup based on other criterias, such as the nearest or cleanest one. Deep learning models [5, 9, 10, 11, 16, 19, 29, 37, 39, 41] can localize object impressively to achieve the first part of the task. However, the ability to deal with a variety of complex referencing and quantification to achieve the sec-ond part of the tasks has yet to be properly investigated.
*Equal contribution 1https://github.com/clarence-lee-sheng/
DetermiNet contains the dataset and code
A determiner is an English part-of-speech (word class) that quantifies or references the noun following it. For in-stance, the determiner in “my apple” versus “your apple” takes reference from different owners. The number of ap-ples being referenced differs for “some apples” versus “all apples”. Such semantic differences are succinctly captured by determiners, and not by other word classes.
Determiners like “a”, “the” and “my” are ubiquitous and among the most common English words [1, 22]. Most chil-dren learn to use determiners at a near-mastery level by 3 years of age [3, 6]. Since determiners play an important role in the semantics of a phrase, they are distinctly classified in natural language processing libraries [26, 35].
Unlike numerous nouns, verbs and adjectives, there are only about 50 determiners in the English language [22].
Nevertheless, determiners can be highly complex, and a hardcoded or fixed-rule approach to using or understand-ing determiners simply will not work. For instance, take the determiner “some” – in its simplest form, “some” refers to a relatively small number or quantity. However, this can be highly noun-specific and context-specific, e.g. the absolute physical quantities for “add some salt” versus “drink some water” are very different. Furthermore, determiners that de-scribe ownership or possession, such as “my” and “your”, are highly context-dependent and dynamic, as possession can change on the fly, e.g. after handing over an object. In general, there are many such subtleties and complexities for determiners. Hence, a learning-based approach is needed, along with suitable training data.
If state-of-the-art models could learn a schema of de-terminers [33, 20, 34], it could facilitate flexible combina-tion in novel contexts [21, 17, 28] and improve visual rea-soning. However, existing vision-language models such as
CLIP [31] and BLIP-2 [23] do not capture the semantic or-ganization of determiners well (see Supplementary Mate-rial), and there is no visual grounding dataset that focuses on Determiners. Existing grounded referring expression datasets [4, 13, 15, 18, 27, 36, 38] exclusively focus on “the” and “a”, making an unambiguous reference to a specific sin-gle object. Some examples include “bottle with a lid”, “the
blue truck in the bottom right corner” and “a bird that is close to the baby in a pink shirt”. In other words, existing datasets focus on the noun, verb and adjective aspects of referring expressions, with “the” and “a” as the main deter-miners used.
Hence, as a first step towards bridging this gap, we devel-oped the DetermiNet diagnostic dataset [15] to benchmark current state-of-the-art (SOTA) algorithms on their poten-tial for learning determiner concepts. As with CLEVR [15], good performance on DetermiNet is not an end-goal in it-self, as knowledge of the dataset generation process can be used to hand-craft toy models that will not generalize to real-world determiner usage. The dataset uses a bounding box localization task, set in a highly-constrained instruction task context, and deals only with simplified determiner def-initions. Even with all these simplifications, we find that
SOTA methods do not perform well.
DetermiNet contains 250,000 synthetic images and cap-tions covering 25 determiners. The images are designed with the premise of two avatars interacting at a table with objects. The captions consist of a determiner followed by a noun; the task context is that the viewer is asking the avatar in the image to “pass me {determiner noun}”.
The task is to choose a set of objects that is consistent with the given {determiner noun} pair. Examples are “those apples” or “either orange”. Beyond just object detection, the task tests the ability to understand the logical semantics that define various determiners (see Fig. 1), such as select-ing the correct number of requested objects. Simply return-ing all or random instances of the queried noun would not lead to high performance. Since the focus of DetermiNet is on the logical schema of determiners, high levels of visual realism and diversity are not crucial for benchmarking the ability of algorithms to learn determiners.
Finally, we analyze the performance of SOTA models that were pre-trained to perform visual grounding, so as to see if SOTA deep learning models can learn to understand the logical schema governing determiners.
In summary, our contributions are as follows: 1. We developed DetermiNet, the first large-scale diag-nostic dataset covering the determiners word class, with 250,000 examples across 25 determiners from all four main types of determiners (Articles, Possessives,
Demonstratives and Quantifiers). 2. We show that the core task of learning determiners is very challenging – even an oracle model struggles to learn the determiner schema from a few hundred ex-amples and requires a large dataset. 3. We find that state-of-the-art visually-grounded mod-els show only moderate results on DetermiNet, hence much more work is needed to perform well on the end-to-end task. 2.