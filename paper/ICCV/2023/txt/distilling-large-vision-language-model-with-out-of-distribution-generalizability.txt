Abstract
Large vision-language models have achieved outstanding performance, but their size and computational requirements make their deployment on resource-constrained devices and time-sensitive tasks impractical. Model distillation, the pro-cess of creating smaller, faster models that maintain the per-formance of larger models, is a promising direction towards the solution. This paper investigates the distillation of vi-sual representations in large teacher vision-language models into lightweight student models using a small- or mid-scale dataset. Notably, this study focuses on open-vocabulary out-of-distribution (OOD) generalization, a challenging problem that has been overlooked in previous model distillation liter-ature. We propose two principles from vision and language modality perspectives to enhance student’s OOD generaliza-tion: (1) by better imitating teacher’s visual representation space, and carefully promoting better coherence in vision-language alignment with the teacher; (2) by enriching the teacher’s language representations with informative and fine-grained semantic attributes to effectively distinguish between different labels. We propose several metrics and conduct extensive experiments to investigate their techniques. The results demonstrate significant improvements in zero-shot and few-shot student performance on open-vocabulary out-of-distribution classification, highlighting the effectiveness of our proposed approaches. Code released at this link. 1.

Introduction
In recent years, there has been a significant growth and development of large vision language models (large VLMs) such as CLIP [39], GLIP [28], OFA [49], SimVLM [53],
BEiT [50], Florence [60], and Flamingo [1], which have been pretrained on massive amounts of internet-scale data.
These models have demonstrated enormous potential in a wide range of downstream applications, including clas-sification/detection, image-to-text generation, and vision-language reasoning, especially for the out-of-distribution (OOD) samples and open-vocabulary settings. Despite their
* Equal contributions. Corresponding authors: xul012@ucsd.edu, yuf026@ucsd.edu potential, the large model sizes, high computational resource requirements, and inefficient inference speed of these mod-els restrict their deployment on mobile and IoT edge de-vices [21, 44], as well as in scenarios like robotic control [4] that require rapid network inference. Thus, it would be ideal if we can obtain small and compact models that still possess strong generalizability towards diverse open-set concepts encountered in the real world.
We note that the internet-scale data has endowed founda-tion models with well-aligned vision and language represen-tation spaces across diverse domains and datasets. Ideally, if such representation spaces can be perfectly distilled into smaller models, the resulting models can naturally possess similar open-set OOD generalization ability as their larger counterparts. However, it is usually not the case, as demon-strated in our later experiments. Therefore, we ask the fol-lowing question: How to effectively distill representation spaces of large vision-language teacher models to benefit
OOD generalization of smaller student models?
In this study, we investigate the principles and tech-niques for distilling visual representations from large vision-language models using small- to mid-scale datasets, with a specific focus on out-of-distribution (OOD) generalization.
Although large-scale datasets with image-text pairs exist, as a pioneer work in this direction, we argue that small- to mid-scale datasets has many practical scenarios for vision application researchers (e.g., for robotics), because this is more flexible, allowing for faster research and development cycles with fewer resource dependencies. Additionally, as we shall see, extensive experiments yield a deeper under-standing of the representation spaces of vision-language models, which can be beneficial for future tasks like distill-ing image-based foundation models for detection [22, 49], segmentation [24, 10], and other data modalities like 3D geometries [58, 36, 29]. To keep study focused and fun-damental, we also choose the image classification task to explore many possible strategies.
Based on extensive experimental results, we propose to preserve the internal structure of the representation spaces.
Specifically, we propose techniques to maintain the relation-ship between the visual and language representation spaces of the teacher models and a novel strategy to enhance the
(a) (b)
Figure 1: (a) Illustration of better teacher-student visual space alignments, motivated by our finding that achieving precise matching between teacher and student’s high-dimensional visual spaces is inherently challenging. While the student cheesecake image feature Sc2 minimizes its distance to the teacher’s cheesecake image feature, it is even closer to the teacher’s pizza image feature. This discrepancy results in poor coherence in visual space and vision-language alignment with the teacher, causing classification mistakes. By replacing the “minimizing distance” requirement with the “relative distance” requirement, which encourages student visual features to be closer to their corresponding teacher visual features than other teacher visual features, we greatly enhance visual space and vision-language alignment coherency with the teacher, thereby improving OOD generalization. (b) UMAP embeddings of student visual features and text features before and after language representation enrichment with LLMs. Different colors denote different OOD classes on tiered-ImageNet. Language representation enrichment confers better clusterings of image features around their corresponding text features, enhancing OOD generalization. text representation used during distillation. Across our study, we make the following contributions: 1) We motivate the ability for students to generalize towards out-of-distribution (OOD) concepts by designing several metrics that measure the visual representation space con-sistency and the vision-language alignment consistency be-tween the student and the teacher vision-language model. 2) We find that by better imitating teacher’s visual represen-tation space, and carefully promoting better coherence in vision-language alignment with the teacher, we substantially strengthen student’s OOD generalization ability. 3) We further improve student’s OOD generalization ability by enriching teacher’s language representations with more informative, finegrained, and meaningful semantic attributes to effectively distinguish between different labels. 4) We conduct a thorough experimental analysis to under-stand the efficacy and impact of our techniques on students’
OOD generalization ability. 2.