Abstract
Object understanding in egocentric visual data is ar-guably a fundamental research topic in egocentric vision.
However, existing object datasets are either non-egocentric or have limitations in object categories, visual content, and annotation granularities.
In this work, we intro-duce EgoObjects, a large-scale egocentric dataset for fine-grained object understanding.
Its Pilot version contains over 9K videos collected by 250 participants from 50+ countries using 4 wearable devices, and over 650K ob-ject annotations from 368 object categories. Unlike prior datasets containing only object category labels, EgoObjects also annotates each object with an instance-level identifier, and includes over 14K unique object instances. EgoOb-jects was designed to capture the same object under diverse background complexities, surrounding objects, distance, lighting and camera motion. In parallel to the data collec-tion, we conducted data annotation by developing a multi-stage federated annotation process to accommodate the growing nature of the dataset. To bootstrap the research on
EgoObjects, we present a suite of 4 benchmark tasks around the egocentric object understanding, including a novel in-stance level- and the classical category level object detec-tion. Moreover, we also introduce 2 novel continual learn-ing object detection tasks. The dataset and API are avail-able at https://github.com/facebookresearch/EgoObjects. 1.

Introduction
Object understanding tasks, such as classification and detection, are arguably fundamental research topics in com-puter vision. Enormous amount of advances achieved so
far have been accelerated by the availability of large-scale datasets, such as ImageNet [14], COCO [28], LVIS [18],
Open Images [24] and Objectron [1]. Those datasets often contain images captured from a third-person or exocentric viewpoint and curated from given sources (e.g. Flicker).
Albeit the large volume, they often only capture individ-ual object instances in a single image or video, and do not capture the same object under diverse settings, which are important for fine-grained object understanding task, such
In contrast, object un-as instance-level object detection. derstanding tasks in egocentric vision processes visual data containing objects captured from a first-person or egocen-tric viewpoint. The approaches to those tasks have wide applications in augmented reality and robotics, such as ro-bustly anchoring virtual content at a real world object un-der various conditions (e.g. background, lighting, distance), and are often required to perform well from the egocen-tric viewpoint and distinguish objects at both category- (e.g. mug vs kettle) and instance level (e.g. my mug vs your mug) under various conditions. Therefore, there are clear gaps in adopting existing exocentric datasets for egocentric object understanding.
On the other hand, several egocentric datasets contain-ing object annotations have been built. A family of such datasets are focused on capturing human activities and hand-object interactions. Ego4D [17] contains a large num-ber of egocentric videos of human activities. However, ac-cording to the PACO-Ego4D [39] which mines the objects from Ego4D, there are only 75 object categories with at least 20 samples, and each object instance often only ap-pears in one video. Epic-Kitchens-100 [11] contains over 700 videos depicting human activities in the kitchen, but only annotates objects within the kitchen. HOI4D [30] is collected for category-level human-object interaction, and only contains 800 different object instances from 16 cat-egories. There are several other datasets that are more object-centric, including TREK-150 [15], FPHA [16] and
CO3D [41], but only contain objects from a limited set of categories (<50). Objects there are often captured in a single setup or few setups with limited variations in sur-rounding objects, background, distances and camera mo-tions. Moreover, semantic granularity of the object anno-tations are often limited at category-level, and object in-stances from the same category are not distinguished, which impedes the development of instance-level object under-standing approaches. Therefore, there are still significant gaps with existing egocentric datasets in the dataset scale, visual content variations around individual objects, object semantic diversity, and instance-level object annotation.
To address these gaps, we introduce EgoObjects, a new large-scale egocentric video dataset for fine-grained object understanding (Figure 1). Unlike prior egocentric datasets which are limited to a small dataset scale, a specific do-main or a small number of object categories, EgoObjects includes a large number of videos containing objects from hundreds of object categories commonly seen in the house-holds and offices worldwide. For video capture, 4 wear-able devices with various field-of-view are used, including
Vuzix Blade smart glasses1, Aria glasses2, Ray-Ban Sto-ries smart glasses3 and mobile phones with ultra-wide lens4, which provide representative media formats of egocentric visual data. Each main object is captured in multiple videos with different choices of nearby secondary objects, back-ground complexity, lighting, viewing distance and camera motion. We annotate both the main and secondary objects in the sampled frames with bounding boxes, category level semantic labels and instance-level object identifiers (ID). In current Pilot version release, it contains over 9, 200 videos of over 30 hours collected by 250 participants from 50+ countries and regions, and 654K object annotations with 368 object categories and 14K unique object instance IDs from 3.8K hours of annotator efforts. To our best knowl-edge, EgoObjects is the largest egocentric video dataset of objects in terms of object categories, videos with object an-notations, and object instances captured in multiple condi-tions. Comparisons between EgoObjects and other datasets can be seen in Table 1.
To bootstrap the research on EgoObjects, we introduce 4 benchmark tasks spanning over both non-continual learning and continual learning settings. For non-continual learning setting, we include a novel instance-level object detection task, largely under-explored previously due to the lack of a dataset with object ID annotations, as well as conventional category-level object detection task. For continual learning setting, we present novel object detection tasks at instance-and category level. Evaluations of different approaches to all tasks are also presented to establish the baseline bench-marks. In particular, for instance-level object detection task, a novel target-aware instance detection approach is pro-posed and validated to outperform a baseline target-agnostic object detection method.
To summarize, we make the following contributions.
• We created a large-scale egocentric dataset for object un-derstanding, which features videos captured by various wearable devices at worldwide locations, objects from a diverse set of categories commonly seen in indoor envi-ronments, and videos of the same object instance captured under diverse conditions.
• We proposed a multi-stage federated annotation process for the continuously growing dataset to accompany the 1https://www.vuzix.com 2https://about.meta.com/realitylabs/projectaria 3https://www.meta.com/glasses 4Participants are asked to hold mobile phone close to their eyes to sim-ulate egocentric viewpoints
Exocentric
Egocentric
Objectron CO3D BOP Epic-K∗. HOI4D Ego4D∗∗ 300∗ 9 45 75 859 int’l. 89
-#category
#participant int’l.
#image
#instance
#bbox inst ID device 4M 17K
-✗
M 50
-1.5M 330K 20M 19K 89
-✗
M
--38M
✗
✓
PC,K G 16 9 2.4M 23.9K 800
-✗
K,I 17K 50K
✗
G,V,Z,W,PP R,A,V,M
EgoObjects 368+ 250 int’l. 114K+ 14K+ 654K+
✓
Table 1: Comparing EgoObjects with other datasets. For
EgoObjects, we report statistics of the current Pilot version, which is estimated to account for 10% of the full dataset (thus the “+” notation). ∗Epic-Kitchen-100 [11] only contain object categories in the kitchen. ∗∗Ego4D statistics are reported by the PACO-Ego4D [39], which annotates objects in the Ego4D [17]. Abbre-viation for devices: M=Mobile, K=Kinect, A=Aria, G=GoPro,
PC=Primesense Carmine, I=Intel RealSense, V=Vuzix Blade,
R=Ray-Ban Stories, PP=Pupil, Z=Zetronix zShades, W=Weeview. parallel data collection at scale. Rich annotations at video level (e.g. location, background description) and object-level (e.g. bounding box, object instance ID, category level semantic label) are collected from 3.8K hours of hu-man annotator efforts.
• We introduced 4 benchmark tasks on EgoObjects, in-cluding the novel instance-level and the conventional category-level object detection tasks as well as their continual learning variants. We evaluated multiple ap-proaches on all tasks, and also proposed a novel target-aware approach for instance-level object detection task. 2.