Abstract
Stable Diffusion v1.5
Stable Diffusion v2.1
Despite thousands of researchers, engineers, and artists actively working on improving text-to-image generation mod-els, systems often fail to produce images that accurately align with the text inputs. We introduce TIFA (Text-to-Image
Faithfulness evaluation with question Answering), an auto-matic evaluation metric that measures the faithfulness of a generated image to its text input via visual question answer-ing (VQA). Speciﬁcally, given a text input, we automatically generate several question-answer pairs using a language model. We calculate image faithfulness by checking whether existing VQA models can answer these questions using the generated image. TIFA is a reference-free metric that allows for ﬁne-grained and interpretable evaluations of generated images. TIFA also has better correlations with human judg-ments than existing metrics. Based on this approach, we introduce TIFA v1.0, a benchmark consisting of 4K diverse text inputs and 25K questions across 12 categories (object, counting, etc.). We present a comprehensive evaluation of ex-isting text-to-image models using TIFA v1.0 and highlight the limitations and challenges of current models. For instance, we ﬁnd that current text-to-image models, despite doing well on color and material, still struggle in counting, spatial relations, and composing multiple objects. We hope our benchmark will help carefully measure the research progress in text-to-image synthesis and provide valuable insights for further research.1 1.

Introduction
While we welcome artistic freedom when we commis-sion art from artists, images produced by deep generative models [44, 46, 43, 47, 61] should conform closely to our requests. Despite the advances in generative models, it is still challenging for models to produce images faithful to users’ intentions [40, 11, 30, 35, 36]. For example, current 1Correspondance to <Yushi Hu: yushihu@uw.edu>. All data and a pip-installable evaluation package are available on the project page.
Text Input: A person sitting on a horse in air over gate in grass  with people and trees in background.
GPT-3 generated + verified QAs (pre-generated in TIFA v1.0 benchmark)
Question: what is the animal?     Answer inferred from text: horse
VQA:
Horse
Horse   Question: is there a gate?            Answer inferred from text: yes
VQA:
No
Yes   Question: is the horse in air?      Answer inferred from text: yes
VQA:
No
. . .
Accuracy on 14 questions
Yes
TIFA 71.4
<
Fine-Grained         Accurate         Interpretable 100.0 v.s. other automatic image-text align metrics
CLIP (image)
CLIPScore
CLIP (text)
SPICE
Caption sim. (caption, text input)
CLIP: 21.3  SPICE: 11.8 
CLIP: 24.3  SPICE: 22.2 
>
Figure 1. Illustration of how TIFA works, and comparison with the widely-used CLIPScore and SPICE metrics. Given the text input,
TIFA uses GPT-3 to generate several question-answer pairs, and a QA model ﬁlters them (3 out of 14 questions for this text input are shown). TIFA measures whether VQA models can accurately answer these questions given the generated image. In this example,
TIFA indicates that the image generated by Stable Diffusion v2.1 is better than that by v1.5, while CLIP and SPICE yield the opposite result. The text input is from the MSCOCO validation set. models often fail to compose multiple objects [40, 11, 35], bind attributes to the wrong objects [11], and struggle in gen-erating visual text [36]. Today, there are efforts to address
TIFA
Text input: An empty  bedroom with a flat screen 
TV sitting on a dresser
LM
Question generation  by LM
Answer by LM 
Filtered by QA
TIFA v1.0
Q: Is there a TV in the room?
VQA 
Model
LM
QA
A: yes are p m o
C
VQA: no
Text input: An empty bedroom with a  flat screen TV sitting on a dresser
Q: What room is this? 
C: kitchen, bedroom, bathroom, office 
A: bedroom
Q: Is there a dresser? 
C: yes, no 
A: yes
. . .
VQA tools mPLUG
BLIP-2
. . .
Text-to-Image 
Model
Unfaithfulness discovered: No TV!
（a) Overview of TIFA
（b) TIFA v1.0 benchmark
Figure 2. (a) Overview of how TIFA evaluates the faithfulness of a synthesized image. TIFA uses a language model (LM), a question-answering (QA) model, and a visual-question-answering (VQA) model. Given a text input, we generate several question-answer pairs with the LM and then ﬁlter them via the QA model. To evaluate the faithfulness of a synthesized image to the text input, a VQA model answers these visual questions using the image, and we check the answers for correctness. (b) TIFA v1.0 benchmark. While TIFA is applicable to any text prompt, to allow direct comparison across different studies, and for ease of use, we introduce the TIFA v1.0 benchmark, a repository of text inputs along with pre-generated question-answer tuples with answer choices. To evaluate a text-to-image model, a user ﬁrst produces the images for the text inputs in TIFA v1.0 and then performs VQA with our provided tools on generated images to compute TIFA. these challenges: researchers are imposing linguistic struc-ture with diffusion guidance to produce images with multiple objects [11]; others are designing reward models trained us-ing human feedback to better align generations with user intention [30]. However, progress is difﬁcult to quantify without accurate and interpretable evaluation measures that explain when and how models struggle.
A critical bottleneck, therefore, is the lack of reliable automatic evaluation metrics for text-to-image generation faithfulness. One of the popular metrics is CLIPScore [17], which measures the cosine similarity between the CLIP em-beddings [42] of the text input and the generated image.
However, since CLIP is not effective at counting objects [42], or reasoning compositionally [37], CLIPScore is unreliable and often inaccurate. Another family of evaluation metrics uses image captions, in which an image captioning model
ﬁrst converts the image into text, and then the image caption is evaluated by comparing it against the text input. Un-fortunately, using captioning models is insufﬁcient since they might decide to ignore salient information in images or focus on other non-essential image regions [24]; for ex-ample, a captioning model might say that the images in
Figure 1 are “a ﬁeld of grass with trees in the background”.
Moreover, evaluating text (caption) generation is inherently challenging [23, 26]. Another recent text-to-image evalua-tion is DALL-Eval [6], which employs object detection to determine if the objects in the texts are in the generated im-ages. However, this approach only works on synthesized text and measures faithfulness along the limited axes of objects, counting, colors, and spatial relationships but misses activi-ties, geolocation, weather, time, materials, shapes, sizes, and other potential categories we often ask about when we recall images from memory [29].
To address the above challenges, we introduce TIFA, a new metric to evaluate text-to-image generation faithfulness.
Our approach is illustrated in Figure 2. Given a repository of text inputs, we automatically generate question-answer pairs for each text via a language model (here, GPT-3 [3]). A question-answering (QA) system (here, UniﬁedQA [25]) is subsequently used to verify and ﬁlter these question-answer pairs. To evaluate a generated image, we use a visual-question-answering (VQA) system (here, mPLUG-large [31], BLIP-2 [32], etc.) to answer the questions given the generated image. We measure the image’s faithfulness to the text input as the accuracy of the answers generated by the VQA system. While the accuracy of TIFA is depen-dent on the accuracy of the VQA model, our experiments show that TIFA has much higher correlation with human judgments than CLIPScore (Spearman’s ⇢ = 0.60 vs. 0.33) and captioning-based approaches (Spearman’s ⇢ = 0.60 vs. 0.34). Additionally, since the LMs and VQA models will continue to improve, we hypothesize that TIFA will continue to be more reliable over time. Also, our metrics can automat-ically detect when elements are missing in the generation: in Figure 2, TIFA detects that the generated image does not contain a TV.
To promote the use of our new evaluation metric, we release TIFA v1.0, a large-scale text-to-image generation benchmark containing 4K diverse text inputs, sampled from the MSCOCO captions [34], DrawBench [47], Par-tiPrompts [61], and PaintSkill [6]. Each input comes with a pre-generated set of question-answer pairs, resulting in 25K questions covering 4.5K distinct elements. These questions have been automatically generated and pre-ﬁltered using a question-answering model. This benchmark also comes with different VQA models [57, 28, 58, 32, 31, 21] that can be used to evaluate generative models and can be eas-ily extended to use future VQA models when they become available.
We conduct a comprehensive evaluation of current text-to-image models using TIFA v1.0. Thanks to TIFA’s ability to detect ﬁne-grained unfaithfulness in images, we ﬁnd that current state-of-the-art models are good at rendering com-mon objects, animals, and colors, but still struggle in com-posing multiple objects, reasoning about spatial relations, and binding the correct activity for each entity. In addition, our ablation experiments show that TIFA is robust to dif-ferent VQA models. Future researchers can use TIFA v1.0 to compare their text-to-image models’ faithfulness across different studies. Also, future generative models may focus on addressing the weaknesses of current models that TIFA discovered. In addition, with TIFA, users can customize evaluations with their own text inputs and questions [10]; for example, a future TIFA benchmark could focus on counting or scene text. 2.