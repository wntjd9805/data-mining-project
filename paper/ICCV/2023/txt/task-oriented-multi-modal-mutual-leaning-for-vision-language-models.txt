Abstract
Prompt learning has become one of the most efficient paradigms for adapting large pre-trained vision-language models to downstream tasks. Current state-of-the-art meth-ods, like CoOp and ProDA, tend to adopt soft prompts to learn an appropriate prompt for each specific task. Re-cent CoCoOp further boosts the base-to-new generalization performance via an image-conditional prompt. However, it directly fuses identical image semantics to prompts of dif-ferent labels and significantly weakens the discrimination among different classes as shown in our experiments. Mo-tivated by this observation, we first propose a class-aware text prompt (CTP) to enrich generated prompts with label-related image information. Unlike CoCoOp, CTP can effec-tively involve image semantics and avoid introducing extra ambiguities into different prompts. On the other hand, in-stead of reserving the complete image representations, we propose text-guided feature tuning (TFT) to make the im-age branch attend to class-related representation. A con-trastive loss is employed to align such augmented text and image representations on downstream tasks.
In this way, the image-to-text CTP and text-to-image TFT can be mutu-ally promoted to enhance the adaptation of VLMs for down-stream tasks. Extensive experiments demonstrate that our method outperforms the existing methods by a significant margin. Especially, compared to CoCoOp, we achieve an average improvement of 4.03% on new classes and 3.19% on harmonic-mean over eleven classification benchmarks. 1.

Introduction
Recently, large vision-language models (VLM), such as
CLIP [33] and ALIGN [15], which employ language as
*Equal contribution. † Interns at Baidu VIS. ‡ Corresponding authors.
Figure 1: Comparisons between CoCoOp [50] and our method. The cosine distance between the positive and the negative prompts, which quantifies the class discrimination, and the average accuracy on benchmarks are reported. supervision signal instead of discrete labels, have shown impressive generalization performance in a wide range of downstream vision tasks. Their multi-modal interaction na-ture delivers open-vocabulary support and achieves amaz-ing zero-shot classification performance. Despite their im-pressive transferable abilities, as discussed in [25], it is es-sential to re-activate specific representation capabilities for optimal performance in certain downstream tasks. Consid-ering their hundreds of millions or billions of parameters, attempting to fine-tune the entire model is impractical and even jeopardizes the well-established representation space
[14]. To this end, many recent studies have centred on the efficient and effective adaptation of pre-trained and frozen large VLMs for the specific downstream tasks [28, 51, 50].
Prompt, a simple, compact, and viable strategy, has be-come the leading solution for deploying large pre-trained
VLMs into certain downstream tasks. CLIP [33] utilizes hand-crafted prompts to achieve impressive zero- and few-shot classification performance. Nevertheless, manually-designed prompts require significant domain knowledge and can be highly time-consuming and sub-optimal for spe-cific downstream tasks. To address this problem, later stud-ies [28, 51] adopt soft prompts to learn an appropriate text prompt via optimizing a contrastive loss on different text la-bels. CoCoOp [50] further highlights the limitations of such static soft prompts and proposes learning image-dependent prompts conditioned on individual instances rather than fixed prompts. It achieves great performance gains on un-seen classes by adding high-level image embedding to text prompts. However, compared to CoOp with static prompts,
CoCoOp essentially fuses identical image semantics with different text labels, leading to inevitable learning ambigu-ity and resulting in an average performance drop of 2.22% on base classes on 11 datasets (see Table 1). For example, it may associate the dog image semantics with a prompt that references the [class] of a cat. When using the cosine dis-tance to measure the differences between the positive and negative text prompts, as shown in Fig. 1, CoCoOp holds low distance values, suggesting that it brings significant learning ambiguities to text prompts. Therefore, we argue that text prompts should not only condition on distinct input images for better generalization abilities, but also adapt to different classes to eliminate the potential ambiguities.
To achieve this goal, we propose Class-aware Text
Prompts (CTP), which leverages label-related image infor-mation to generate finer prompts. Specifically, we first con-tact learnable context vectors and each class label to model the initial prompt sentences. Then we leverage these class prompt sentences to query their corresponding image re-gions and representations. Corresponding related image features are subsequently added to initial class prompt sen-tences to produce the final text prompts. In this way, gen-erated image-dependent and class-aware prompts can bet-ter concentrate on the image information in a more precise manner. As shown in Fig. 1, our method enjoys better dis-crimination between positive and negative prompts and con-sistently outperforms CoCoOp on 11 classification datasets.
On the other hand, we identify a critical problem in these text prompt-based strategies: the image branch is ignored and not adjusted to specific downstream tasks. As shown in
Fig. 2 (CoCoOp), on the task of identifying birds, the out-put image feature, without further tuning, can be distracted to leaves of the same color. Similarly, it also wrongly high-lights the beer foam that is of a similar shape to recognize golf balls. Since the final recognition is jointly inferred by both text and image branches, such an issue may de-grade the classification performance. Thus it is necessary
Figure 2: Comparisons of attention map visualization for
CoCoOp and our method on ImageNet. Our method obtains better average accuracy of both base and new classes across 11 datasets by paying attention to task-related regions. to tune the image features further so that the image branch can focus more on the tasks-related representation. We then propose Text-guided Feature Tuning (TFT), which lever-ages encoded text embedding to guide image representation more on task-related regions. As shown in Fig. 2 (ours), our method successfully focuses on task-related regions, i.e., birds and golf balls. We then leverage the contrastive loss function to further align class-aware text embedding and text-guided image features on certain downstream tasks.
In summary, we propose a new task-oriented multi-modal mutual-learning method, which well-integrates our designed class-aware text prompts and text-guided fea-ture tuning for fast adaptation of frozen VLMs on down-Image features can help construct image-stream tasks. dependant class-aware text prompts, leading to more dis-criminative text embedding. Simultaneously, improved text embedding can further guide the image branch attending to class-related representation. In this way, these two differ-ent modality branches can be tightly coupled and mutual-beneficial across the whole training process. Our main con-tributions are summarized in the following.
• We propose class-aware text prompts which generate prompts based on task-relevant image semantics in-stead of complete visual information. In this way, we improve the classification accuracy of unseen classes without introducing extra learning ambiguities.
• We propose text-guided feature tuning which enforces image branch to pay more attention to the task-related representation. As a result, the model avoids deviating attention to the task-irrelevant regions of the image.
• Benefiting from our mutual learning strategy, our
method achieves SOTA results on four downstream tasks. Especially, ours significantly outperforms ex-isting methods on the base-to-new generalization task. 2.