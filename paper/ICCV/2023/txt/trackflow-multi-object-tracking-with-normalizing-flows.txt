Abstract
The field of multi-object tracking has recently seen a renewed interest in the good old schema of tracking-by-detection, as its simplicity and strong priors spare it from the complex design and painful babysitting of tracking-by-attention approaches.
In view of this, we aim at extend-ing tracking-by-detection to multi-modal settings, where a comprehensive cost has to be computed from heterogeneous information e.g., 2D motion cues, visual appearance, and pose estimates. More precisely, we follow a case study where a rough estimate of 3D information is also available and must be merged with other traditional metrics (e.g., the
IoU). To achieve that, recent approaches resort to either simple rules or complex heuristics to balance the contri-bution of each cost. However, i) they require careful tuning of tailored hyperparameters on a hold-out set, and ii) they imply these costs to be independent, which does not hold in reality. We address these issues by building upon an elegant probabilistic formulation, which considers the cost of a can-didate association as the negative log-likelihood yielded by a deep density estimator, trained to model the conditional joint probability distribution of correct associations. Our experiments, conducted on both simulated and real bench-marks, show that our approach consistently enhances the performance of several tracking-by-detection algorithms. 1.

Introduction
Real-time multi-person tracking in crowded real-world scenarios is a challenging and difficult problem with appli-cations ranging from autonomous driving to visual surveil-lance. Indeed, the work done to create a reliable tracker that can function in every environment is noteworthy.
The most successful methods currently available in literature can be broadly grouped into three main cat-tracking-by-egories: regression [26, 36, 51], and tracking-by-attention [56, 85,
In tracking-by-detection, bounding boxes are com-83]. puted independently for each frame and associated with tracking-by-detection [10, 5, 39], tracks in subsequent steps. Tracking-by-regression unifies detection and motion analysis, with a single module that simultaneously locates the bounding boxes and their dis-placement w.r.t. the previous frame. Finally, in tracking-by-attention, an end-to-end deep tracker based on self-attention [79] manages the life-cycle of a set of track pre-dictions through the video sequence.
Although the two latter paradigms have recently sparked the research interest, tracking-by-detection still proves to be competitive [88, 11], under its simplicity, reliability, and the emergence of super-accurate object detectors [27]. In light of these considerations, we aim to strengthen tracking-by-detection algorithms by enriching the information they usually leverage – i.e., the displacement between estimated and actual bounding boxes [82, 88] – with additional cues.
Indeed, as shown by several works of multi-modal track-ing [14, 87], the visual domain is just one of the possi-ble sources that may contribute. The pose of the skele-ton [16], the depth maps [62, 18] and even thermal measure-ments [45] are concepts that can gain further robustness, as they encode a deeper understanding of the scene. In par-ticular, as humans move and interact in a three-dimensional space, one of the goals of this work is to provide the tracker with the (predicted) distance from the camera, thus resem-bling what is generally acknowledged as “2.5D”. To achieve that, we train a per-istance distance deep regressor on MOT-Synth [25], a recently released synthetic dataset displaying immense variety in scenes, lightning/weather conditions, pedestrians’ appearance, and behaviors.
However, the fusion of multi-modal representations poses a big question: how to weigh the contribution of each input domain to the overall cost? It represents a crucial step, as its design directly impacts the subsequent assignment op-timization problem: in this respect, existing works resort to handwritten formulas and heuristics e.g., DeepSORT [82] computes two different cost matrices and combines them through a weighted sum. Notably, the authors of [62] build upon a probabilistic formulation, which recasts the cost ci,j as the likelihood of the event “the i-th detection belongs to the j-th tracklet”. Afterward, it is about estimating a den-sity function on top of correct associations, termed inliers.
Although these fusing approaches may appear reasonable, they hide several practical and conceptual pitfalls:
• They introduce additional hyperparameters, which re-quire careful tuning on a separate validation set and hence additional labeled data.
• A single choice of these hyperparameters cannot fit dif-ferent scenes perfectly, as these typically display dif-ferent dynamics in terms of pedestrians’ motion and spatial density, the camera’s position/motion, and light-ing/weather conditions. Therefore, the right trade-off is likely to be scenario-dependent;
• Common approaches (e.g., a simple weighted summa-tion) assume the input modalities to be independent, thus overlooking their interactions.
We propose to take into account the weaknesses mentioned above through a dedicated parametric density estimator – termed TrackFlow – tasked to summarize several input costs/displacements in a single output metric, e.g., the prob-ability that a specific detection D belongs to a particular track T . As we strive to approximate the underlying condi-tional probability distribution P(D ∈ T | T ) over the input costs, we borrow the estimator from the world of deep gen-erative models, in particular from the literature of Normal-izing Flow models [21, 22, 41]. In fact, these models rep-resent a flexible and effective tool to perform density esti-mation. Moreover, we would like to emphasize the reliance of such a module on an additional context-level representa-tion, which we provide in order to inform the model about scene-level peculiarities. This way, the computation of the likelihood is also conditioned on visual cues of the scene, which we assume may be unobserved during evaluation.
Extensive experiments on MOTSynth [25], MOT17 [57], and MOT20 [17] show that the naive cost metric – i.e., the 2D intersection between predicted and candidate bounding boxes – can be replaced by the score provided by our ap-proach, with a remarkable performance gain in exchange. 2.