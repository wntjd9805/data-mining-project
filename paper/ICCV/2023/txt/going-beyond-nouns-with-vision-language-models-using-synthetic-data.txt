Abstract
Large-scale pre-trained Vision & Language (VL) mod-els have shown remarkable performance in many applica-tions, enabling replacing a fixed set of supported classes with zero-shot open vocabulary reasoning over (almost ar-bitrary) natural language prompts. However, recent works have uncovered a fundamental weakness of these models.
For example, their difficulty to understand Visual Language
Concepts (VLC) that go ‘beyond nouns’ such as the mean-ing of non-object words (e.g., attributes, actions, relations, states, etc.), or difficulty in performing compositional rea-soning such as understanding the significance of the or-der of the words in a sentence.
In this work, we investi-gate to which extent purely synthetic data could be lever-aged to teach these models to overcome such shortcomings without compromising their zero-shot capabilities. We con-tribute Synthetic Visual Concepts (SyViC) - a million-scale synthetic dataset and data generation codebase allowing to generate additional suitable data to improve VLC under-standing and compositional reasoning of VL models. Addi-tionally, we propose a general VL finetuning strategy for effectively leveraging SyViC towards achieving these im-provements. Our extensive experiments and ablations on
VL-Checklist, Winoground, and ARO benchmarks demon-strate that it is possible to adapt strong pre-trained VL mod-els with synthetic data significantly enhancing their VLC understanding (e.g. by 9.9% on ARO and 4.3% on VL-Checklist) with under 1% drop in their zero-shot accuracy. 1.

Introduction
There have been impressive advances in the performance of zero-shot recognition through the use of large-scale pre-trained Vision & Language (VL) models [45, 20, 50, 28, 15, 57, 29, 13]. However, these VL models still face some important challenges in understanding Visual Lan-guage Concepts (VLC) beyond object nouns (e.g., recog-*Equal contribution. Project page: https://synthetic-vic.github.io/
†Work partially done while interning at the MIT-IBM Watson AI Lab.
Figure 1. Overview of our proposed synthetic set: we place differ-ent objects in a scene and change their position, color, size, and material. We further emphasize on human-level interactions, sam-pling a wide set of body poses and behaviors to cover transitive and intransitive human actions. nizing attributes, relations, states) and in terms of com-positional reasoning capabilities (i.e.., understanding sub-tle changes in meaning due to small changes in word or-der). Recently, several benchmark tests have been devised to demonstrate the extent to which these models lack these capabilities [51, 62, 59] 1. As noted in several recent works [59, 62, 9], this behavior of VL models is likely due to the contrastive pre-training prevalent for all of them and likely inducing ‘bag-of-objects’ kind of representations (for both images and text alike). Indeed, for (even large) random batches of paired image-text samples, the collection of ob-jects (nouns) in the image (or text) is likely to uniquely de-termine the image (or text) in the batch, making contrastive batch losses focus on the objects (nouns) while regarding 1Please also see supplementary material for the expanded set of results of [62] including results for all the most recent open-sourced VL models, all exhibiting poor VLC understanding performance.
other details (attributes, relations, states, word order, etc.) as unnecessary. Intuitively, this impairs VLC understand-ing and compositional reasoning of the resulting model.
Given the above, a natural question to ask is what is the most effective way to ‘fix’ the VL models to improve their
VLC understanding and compositional reasoning perfor-mance? An approach proposed in concurrent works [9, 59], advocates for the use of text augmentation, using language tools to teach a model the importance of non-noun words by manipulating them (e.g., replacing them with incorrect al-ternatives) and adding the resulting texts to the same batch.
Although effective, such augmentation techniques are only easy on the text side and are much harder and prohibitively expensive on the image side. Indeed, finding, collecting, or generating real image samples sharing the objects but dif-fering in their composition, attributes, relations, or states is very difficult. Although significant progress has been achieved with text-based editing [16, 21, 37, 5, 23, 3, 38], these methods are relatively slow (leveraging diffusion) and not sufficiently stable to allow effective use for augmenta-tion in training pipelines. In this work, therefore, we pro-pose an orthogonal route – VL data synthesis for fixing VL models by targeted demonstration. Specifically, we pro-pose enhancing the VLC and compositionality aspects of the generated visual and text data, in turn using this data for finetuning VL models teaching them to pay closer atten-tion to these aspects. Moreover, besides being largely free and infinitely scalable, synthetic data has an additional ad-vantage – it can also be free from privacy concerns always accompanying real data.
Besides the inherent challenges of realistic data simula-tion, building synthetic data that can be effectively used to improve VLC and compositionality aspects of VL models pre-trained on massive real data poses additional technical challenges. Unlike the majority of prior work focusing on synthetic visual data generation, we need not only to gen-erate images, but also the text that describes compositional items in a scene. We generate synthetic videos that lever-age realistic physical 3D simulation [11] including diverse 3D environments and different 3D objects, human motions, and actions assets [1, 35, 41, 44], added interaction with ob-jects, and different camera viewpoints. Every frame of these videos is accompanied by rich metadata, allowing using lan-guage grammar for generating detailed descriptive captions of any instantaneous scene in each video. These captions, in turn, allow collecting diverse image-text pairs samples con-trasting which one to another highlights to the model the importance of the compositional items in the text captions (e.g. different viewpoints or different frames in the same video share objects but may strongly differ in the VLC and other compositional items). While motion assets were used by previous works to generate synthetic data [55, 54], the vi-sual data was not accompanied by textual captions and was not designed with the need to highlight compositionality in mind. We contribute Synthetic Visual Concepts (SyViC) – a large (million-scale) generated synthetic VL dataset with rich textual captions, easily extensible through our data syn-thesis code together with all the already generated million-scale synthetic data used in this paper (Figure 1).
In addition to the data synthesis pipeline, we also of-fer a strategy for effectively leveraging the generated syn-thetic data, while avoiding forgetting real data alignment and losing the strong a-priori zero-shot capabilities of the model. We propose and extensively ablate a combination of domain adaptation by stylization [63], parameter efficient fine-tuning [17], long captions handling, and model averag-ing methods [56] to reduce forgetting, as well as examine the effect of different aspects of data synthesis and fine-tuning choices on the gains in VLC and compositionality understanding.
Our contributions can be summarized as follows: (i) we contribute SyViC – a million-scale synthetic dataset with rich textual captions, intended for improving VLC under-standing and compositional reasoning in VL models, as well as the methodology and the generation codebase 2 for its synthesis and potentially extensibility; (ii) an effec-tive general VL model finetuning strategy enabling effective leveraging of SyViC data for enhancing the aforementioned aspects of strong pre-trained VL models without sacrificing their zero-shot capabilities; (iii) experimental results and extensive ablation study showing significant (over 10% in some cases) improvement in VLC understanding and com-positional reasoning respectively, measured on all the recent
VL-Checklist, ARO, and Winoground benchmarks and val-idated on the most popular CLIP [45] model and its deriva-tives (e.g. the most recent CyCLIP [15]).
For supplemental materials, readers are referred to the associated arXiv document at [arXiv:2303.17590]. 2.