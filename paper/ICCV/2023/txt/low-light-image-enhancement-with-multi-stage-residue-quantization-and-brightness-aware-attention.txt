Abstract
Low-light image enhancement (LLIE) aims to recover il-lumination and improve the visibility of low-light images.
Conventional LLIE methods often produce poor results be-cause they neglect the effect of noise interference. Deep learning-based LLIE methods focus on learning a map-ping function between low-light images and normal-light images that outperforms conventional LLIE methods. How-ever, most deep learning-based LLIE methods cannot yet fully exploit the guidance of auxiliary priors provided by normal-light images in the training dataset. In this paper, we propose a brightness-aware network with normal-light
* Equal Contribution, † Corresponding Author priors based on brightness-aware attention and residual-quantized codebook. To achieve a more natural and re-alistic enhancement, we design a query module to obtain more reliable normal-light features and fuse them with low-light features by a fusion branch. In addition, we propose a brightness-aware attention module to further improve the robustness of the network to the brightness. Extensive ex-perimental results on both real-captured and synthetic data show that our method outperforms existing state-of-the-art methods. 1.

Introduction
Low-light images suffer from extremely limited visibil-ity and noise interference, which often have serious effects on the performance of many downstream tasks [6, 8, 29,
30]. Professional photographers can work with exposure time, aperture, and ISO settings to capture more informa-tion related to the image. However, in the meantime, mo-tion blur degradation and noise amplification are often in-evitable. Therefore, computational photography methods known as low-light image enhancement (LLIE) [9, 39, 43] have received increasing attention in recent years. Con-ventional LLIE methods, including histogram equalization methods [15, 25, 26], gamma correction methods [12, 27], and Retinex-based methods [14, 19, 39, 43], generally ne-glect noise degeneration, resulting in unsatisfactory perfor-mance on real low-light images. LLIE methods based on deep learning aim to learn a mapping function between low-light images and normal-light images and outperform con-ventional LLIE methods by a large margin [41, 45, 47].
However, most deep learning-based LLIE methods focus on learning the mapping function but ignore the guidance of the auxiliary priors provided by normal-light images in the training dataset, leading to unpleasant artifacts or dis-torted colors in the enhanced images, as shown in Figure 1.
Recently, several image restoration methods [4, 11, 38, 48] propose learning textures and details under the guid-ance of vector-quantized (VQ) codebook priors. VQ-based methods, e.g. VQ-VAE [24, 32] and VQ-GAN [7], usu-ally have two stages. Specifically, in the first stage, a high-quality codebook is learned with self-reconstruction of high-quality labels and aims to record high-level semantic information and provide auxiliary priors to guide the learn-ing of the second stage. The decoders of VQ-based net-works store rich, high-quality textures and details simulta-neously. However, it is unsatisfactory to employ directly
VQ-based methods in LLIE. These VQ-based methods pro-pose to construct a codebook in high-level feature space by downsampling the high-quality image with factors of 8, 16, or 32 and select one codebook item to represent high-level features, which makes image details lost and network train-ing unstable.
To improve important details, residual quantized VAE (RQ-VAE) [16] proposes the selection of multiple code-book items to accurately represent characteristics using a residual quantization strategy. In the second stage, the low-quality images are mapped into high-level semantic space, and the codebook items closest to the high-level features are selected to be inputted to the decoder of the first stage for generating output. Taking into account the gap between low-quality and high-quality images, some VQ-based meth-ods adopt distillation loss to allow the second-stage encoder to mimic one of the first stages. However, since these meth-ods select the nearest codebook items relying on similari-ties between the feature vectors of the low-quality images and the high-quality codebook items, the quantized features are still limited and unreliable. Furthermore, the short-cut connection popularly used in the encoder-decoder structure (e.g., U-Net [31]) cannot be used in VQ-based networks, resulting in further significant detail loss.
To address these challenges, we propose a novel low-light image enhancement (LLIE) method based on VQ-VAE with a three-stage framework in this paper. In the first stage, we learn a normal-light decoder and a more hierarchical and expressive normal-light codebook by residual quanti-zation [16]. However, constructing a more expressive code-book also means that it becomes more difficult to select the correct items from the codebook. Therefore, in the sec-ond stage, not only should the learned features from the low-light images approximate the normal-light image fea-tures by distillation loss, but also should be calculated the similarities between the low-light features and query items to select the codebook items. To avoid the loss of details caused by downsampling, we propose the third stage to pro-tect valuable details and refine the enhanced results by a fu-sion branch that fuses features of the pre-trained low-light encoder and normal-light decoder on different scales.
In addition, a novel brightness-aware attention module is pro-posed to dynamically learn the brightness and textures of images and is integrated into the fusion branch. The contri-butions of this paper are listed below.
• We propose a novel low-light image enhancement method based on VQ-VAE with a three-stage frame-work. To our knowledge, our proposed method is the first VQ-based method for LLIE.
• We construct a more hierarchical and expressive code-book by residual quantization. In addition, we design a query module to bridge the gap between low-light features and the normal-light codebook.
• To avoid the image details lost by the downsampling operation, we propose a fusion branch fusing low-light features and normal-light priors at different scales.
• We design a brightness-aware attention module that learns a brightness map to modulate features to im-prove the robustness of the network to the brightness.
• Extensive experimental results on several popular datasets show that our proposed method outperforms several existing state-of-the-art LLIE methods. 2.