Abstract
Answering visual queries is a complex task that requires both visual processing and reasoning. End-to-end models, the dominant approach for this task, do not explicitly differ-entiate between the two, limiting interpretability and gener-alization. Learning modular programs presents a promising alternative, but has proven challenging due to the difficulty of learning both the programs and modules simultaneously.
We introduce ViperGPT, a framework that leverages code-generation models to compose vision-and-language models into subroutines to produce a result for any query. ViperGPT utilizes a provided API to access the available modules, and composes them by generating Python code that is later ex-ecuted. This simple approach requires no further training, and achieves state-of-the-art results across various complex visual tasks. 1.

Introduction
How many muffins can each kid in Figure 1 (top) eat for it to be fair? To answer this, we might 1) find the children and the muffins in the image, 2) count how many there are of each, and 3) reason that ‘fair’ implies an even split, hence divide. People find it natural to compositionally combine individual steps together to understand the visual world.
Yet, the dominant approach in the field of computer vision remains end-to-end models, which do not inherently lever-age this compositional reasoning.
Although the field has made large progress on individual tasks such as object recognition and depth estimation, end-to-end approaches to complex tasks must learn to implicitly perform all tasks within the forward pass of a neural net-work. Not only does this fail to make use of the advances in fundamental vision tasks at different steps, it does not make use of the fact that computers can perform mathemat-ical operations (e.g., division) easily without machine learn-ing. We cannot trust neural models to generalize system-atically to different numbers of muffins or children. End-to-end models also produce fundamentally uninterpretable decisions – there is no way to audit the result of each step
*Equal contribution. Order determined via coin flip and may be listed either way. to diagnose failure. As models grow increasingly data and compute-hungry, this approach grows increasingly unten-able. We would like to perform new tasks without additional training by recombining our existing models in new ways.
What limits us from creating such modular systems for more complex tasks?
In previous years, the pioneering works of Neural Module Networks [2, 28, 20] attempted to decompose tasks into simpler modules. By training end-to-end with modules rearranged in different ways for different problems, the hope was that each module would learn their appropriate function and thereby become reusable. How-ever, numerous issues made this approach difficult to extend to the real world. In particular, program generation relied on hand-tuned natural language parsers [2], or otherwise re-quired reinforcement learning from scratch and were thus difficult to optimize [20, 28]. In each case, program gener-ation was highly domain-limited. Furthermore, learning the perceptual models jointly with the program generator made training even more difficult, often failing to produce the in-tended modular structure [3, 49].
In this work, we present ViperGPT1, a framework that overcomes these bottlenecks by leveraging code generat-ing large language models (e.g. GPT-3 Codex [9]) to flexi-bly compose vision models based on any textual query that defines the task. It creates customized programs for each query that take images or videos as argument and return the result of the query for that image or video. We show that providing Codex an API exposing various visual capabili-ties (e.g. find, compute_depth), just as one might provide an engineer, is sufficient for the creation of these programs.
The model’s prior training on code enables it to reason about how to use these functions and implement the relevant logic. Our results demonstrate that this simple approach de-livers remarkable zero-shot performance (i.e. without ever training on task specific images).
Our simple approach enjoys many benefits: it is 1) inter-pretable, as all the steps are explicit as code function calls with intermediate values that can be inspected; 2) logical, as it explicitly uses built-in Python logical and mathematical operators; 3) flexible, as it can easily incorporate any vision or language module, only requiring the specification of the 1We name our method after a snake because it executes Python code.
Figure 1. In-the-wild results. Given a visual input and a query, ViperGPT synthesizes a program, then executes it with the Python interpreter in order to produce the final answer. This figure shows both the generated code, and the result of intermediate variables during the execution. By composing pretrained modules, ViperGPT obtains answers that are both correct and interpretable for open-world queries.
associated module be added to the API; 4) compositional, decomposing tasks into smaller sub-tasks performed step-by-step; 5) adaptable to advances in the field, as improve-ments in any of the used modules will result in a direct im-provement in our approach’s performance; 6) training-free, as it does not require to re-train (or finetune) a new model for every new task; and finally, 7) general, as it unifies all tasks into one system.
In summary, our contributions are: 1. We propose a simple framework for solving complex visual queries by integrating code-generation models into vision with an API and the Python interpreter, with the benefits above. 2. We achieve state-of-the-art zero-shot results across tasks in visual grounding, image question answer-ing, and video question-answering, showing this inter-pretability aids performance rather than hindering it. 3. To promote research in this direction, we develop a
Python library enabling rapid development for pro-gram synthesis for visual tasks, which will be open-sourced upon publication. 2.