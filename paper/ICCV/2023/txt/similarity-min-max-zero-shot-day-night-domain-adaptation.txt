Abstract
Low-light conditions not only hamper human visual ex-perience but also degrade the model’s performance on downstream vision tasks. While existing works make re-markable progress on day-night domain adaptation, they rely heavily on domain knowledge derived from the task-specific nighttime dataset. This paper challenges a more complicated scenario with border applicability, i.e., zero-shot day-night domain adaptation, which eliminates re-liance on any nighttime data. Unlike prior zero-shot adap-tation approaches emphasizing either image-level transla-tion or model-level adaptation, we propose a similarity min-max paradigm that considers them under a unified frame-work. On the image level, we darken images towards mini-mum feature similarity to enlarge the domain gap. Then on the model level, we maximize the feature similarity between the darkened images and their normal-light counterparts for better model adaptation. To the best of our knowledge, this work represents the pioneering effort in jointly optimiz-ing both levels, resulting in a significant improvement of model generalizability. Extensive experiments demonstrate our method’s effectiveness and broad applicability on vari-ous nighttime vision tasks, including classification, seman-tic segmentation, visual place recognition, and video action recognition. Our project page is available at https:// red-fairy.github.io/ZeroShotDayNightDA-Webpage/. 1.

Introduction
Deep neural networks are sensitive to insufficient illumi-nation, and such deficiency has posed significant threats to safety-critical computer vision applications. Intuitively, in-sufficient illumination can be handled by low-light enhance-ment methods [23, 30, 34, 56, 60, 63], which aim at restoring low-light images to normal-light. However, enhancement models do not necessarily benefit downstream high-level vi-sion tasks as they are optimized for human visual perception
*Corresponding author.
Figure 1. Left: Illustration of our similarity min-max framework for zero-shot day-night domain adaptation. Right: Our framework achieves state-of-the-art results on multiple downstream high-level vision tasks without seeing real nighttime images during training. and neglect the need for machine vision.
Much existing literature has focused on improving ma-chine vision performance at night through domain adap-tation. By aligning the distribution statistics between the nighttime and daytime datasets through image transla-tion [2, 12, 45], self-supervised learning [52, 53], or multi-stage algorithms [10, 46, 47], these methods have greatly improved models’ performance in nighttime environments.
The primary assumption of domain adaptation is that the target domain data is readily available. Nevertheless, ob-taining data from the task-specific target domain may be challenging in extreme practical application scenerios such as deep-space exploration and deep-sea analysis.
To reduce the requirement on target domain data, zero-shot domain adaptation has emerged as a promising re-search direction, where adaptation is performed without ac-cessing the target domain. Regarding day-night domain adaptation, the primary challenge is learning illumination-robust representations generalizable to both day and night modalities. To accomplish this goal under zero-shot con-straints, Lengyel et al. [29] proposed a color invariant con-volution for handling illumination changes. Cui et al. [8] designed a Reverse ISP pipeline and generated synthetic nighttime images with pseudo labels. However, image-level methods simply consider synthetic nighttime as pseudo-labeled data and overlook model-level feature extraction; model-level methods focus on adjusting model architecture but neglect image-level nighttime characteristics. Neither is effective enough capture the illumination-robust representa-tions that could bridge the complex day-night domain gap.
From this point of view, we devise a similarity min-max framework that involves two levels, as illustrated in Fig-ure 1. On the image level, we generate a synthetic night-time domain that shares minimum feature similarity with the daytime domain to enlarge the domain gap. On the model level, we learn illumination-robust representations by maximizing the feature similarity of images from the two domains for better model adaptation.
Intuitive as it seems, solving this bi-level optimization problem is untrivial. Directly solving it may yield unsat-isfactory results, e.g., meaningless images filled with zero values or identical features given all inputs. Therefore, we develop a stable training pipeline that can be considered a sequential operation on both the image and the model. Re-garding the image, we propose an exposure-guided module to perform reliable and controllable nighttime image syn-thesis. Regarding the model, we align the representation of images from day and night domains through multi-task contrastive learning. Finally, our model achieves day-night adaptation without seeing real nighttime images.
Our framework can serve as a plug-and-play remedy to existing daytime models. To verify its effectiveness, we conduct extensive experiments on multiple high-level night-time vision tasks, including classification, semantic seg-mentation, visual place recognition, and video action recog-nition. Results on various benchmarks demonstrate our su-periority over the state-of-the-art.
Our contributions are summarized as follows:
• We propose a similarity min-max framework for zero-shot day-night domain adaptation. Feature similar-ity between the original and darkened images is min-imized by image-level translation and maximized by model-level adaptation. In this way, model’s perfor-mance in nighttime is improved without accessing real nighttime images.
• We develop a stable training pipeline to solve this bi-level optimization problem. On the image level, we propose an exposure-guided module to perform reli-able and controllable nighttime image synthesis. On the model level, we align the representation of images from day and night domains through multi-task con-trastive learning.
• Our framework universally applies to various night-time high-level vision tasks. Experiments on classi-fication, semantic segmentation, visual place recogni-tion, and video action recognition demonstrate the su-periority of our method. 2.