Abstract
Visual object tracking is a fundamental research topic with a broad range of applications. Benefiting from the rapid development of Transformer, pure Transformer track-ers have achieved great progress. However, the feature learning of these Transformer-based trackers is easily dis-turbed by complex backgrounds. To address the above lim-itations, we propose a novel foreground-background distri-bution modeling transformer for visual object tracking (F-BDMTrack), including a fore-background agent learning (FBAL) module and a distribution-aware attention (DA2) module in a unified transformer architecture. The proposed
F-BDMTrack enjoys several merits. First, the proposed
FBAL module can effectively mine fore-background infor-mation with designed fore-background agents. Second, the
DA2 module can suppress the incorrect interaction between foreground and background by modeling fore-background distribution similarities. Finally, F-BDMTrack can extract discriminative features under ever-changing tracking sce-narios for more accurate target state estimation. Extensive experiments show that our F-BDMTrack outperforms previ-ous state-of-the-art trackers on eight tracking benchmarks. 1.

Introduction
Visual object tracking(VOT) aims to locate the position of a class-agnostic target in a video sequence given the tar-get in the first frame, which is a fundamental and essential research topic in computer vision. Due to its great appli-cation potential (such as video surveillance [9, 49, 9], anti-UAV tracking [64, 25, 29], and automatic driving [47, 41, 15]), VOT has attracted substantial attention and has been developed tremendously [56, 31, 17]. However, as a video processing task, visual object tracking still faces various challenges including deformation, motion blur, and suscep-tibility to background interference [51, 27, 24].
*Equal Contribution
â€ Corresponding Author
Figure 1. Response map comparisons among different attention mechanisms. Both (a) and (b) have incorrect similarities between the target (green square patch) and background distractor (yellow triangle patch) due to similar appearance, causing messy response map. Differently, in our proposed DA2 module (c), the similarity between target and distractor is suppressed, since the similarity is obtained according to their fore-background distributions.
To deal with the above challenges, numerous approaches have been proposed [23, 2, 3, 59, 10, 5, 62]. These meth-ods can be generally divided into three main paradigms, including CNN-based [8, 52, 58, 35, 3, 13], hybrid CNN-Transformer [7, 59, 36, 50, 42], and pure Transformer trackers [5, 62, 32, 57]. CNN-based trackers first extract features from the template and search region separately through a shared convolutional neural network (CNN), and then the target state is estimated by calculating cross-correlation [2, 58, 52] between features in template and search regions, or learning a discriminative correlation fil-ter [12, 3, 13]. With the successful development of trans-former in the computer vision field, an increasing number of hybrid CNN-Transformer trackers [7, 59, 42] have been proposed. These trackers also leverage CNN to extract fea-tures separately, but adopt transformer to realize the feature interaction between the template and search region, which can alleviate the loss of discriminative foreground informa-tion. However, for the above two paradigm trackers, there is no interaction between the template and the search re-gion when extracting features. These target-unaware feature extraction ways have limited target-background discrimina-tive power, especially when the target category is not seen
in the training dataset. To this end, pure Transformer track-ers [5, 62, 32, 57] are proposed to build interaction between template and search region along with the feature extrac-tion. Thanks to the strong representation and interaction capability of ViT/SwinT variants [14, 38, 55], pure Trans-former trackers have achieved remarkable improvements.
Despite the success of the above pure Transformer track-ers, the feature learning of these methods are easily dis-turbed by complex backgrounds due to insufficient con-sideration of fore-background relationship. To make pure
Transformer architectures more suitable for discriminative feature learning in visual object tracking, there are two core (1) Fore-background points that need to be considered. information mining. Previous methods [5, 22, 21] usu-ally mine fore-background information from the template according to the given bounding box (bbox). However, the tracking target can be continuously changing, making it difficult to learn target discriminative features for cur-rent search region, if only guided by fore-background in-formation of the template. Therefore, it is necessary to propose a effective way to mine fore-background informa-tion for both template and search region. (2) Target-aware feature interaction. Most of popular Transformer track-ers [62, 10, 57] adopt the plain attention mechanism to build interaction between template and search region. Since at-tention scores are obtained by appearance similarity, the template features will mistakenly aggregate background in-formation and further affect features of search region during feature interaction, resulting messy response map (Figure 1 (a)). Although some methods [22, 42] introduce additional target embeddings to enhance the target information, they could still be disturbed by distractors, making features of the search region have limited target discriminative power.
And the response map is also unsatisfactory (Figure 1 (b)).
Therefore, it is urgent to design a new attention mechanism to achieve better target-aware feature interaction.
Motivated by the above discussion, we propose a novel Foreground-