Abstract
Deep learning models have a risk of utilizing spurious clues to make predictions, such as recognizing actions based on the background scene. This issue can severely degrade the open-set action recognition performance when the testing samples have different scene distributions from the train-ing samples. To mitigate this problem, we propose a novel method, called Scene-debiasing Open-set Action Recogni-tion (SOAR), which features an adversarial scene reconstruc-tion module and an adaptive adversarial scene classiﬁcation module. The former prevents the decoder from reconstruct-ing the video background given video features, and thus helps reduce the background information in feature learning.
The latter aims to confuse scene type classiﬁcation given video features, with a speciﬁc emphasis on the action fore-ground, and helps to learn scene-invariant information. In addition, we design an experiment to quantify the scene bias.
The results indicate that the current open-set action recog-nizers are biased toward the scene, and our proposed SOAR method better mitigates such bias. Furthermore, our exten-sive experiments demonstrate that our method outperforms state-of-the-art methods, and the ablation studies conﬁrm the effectiveness of our proposed modules. 1.

Introduction
Recent years have witnessed signiﬁcant progress in ac-tion recognition [10, 69, 70, 41, 20, 77, 79, 26, 72]. Yet, most works follow a closed-set paradigm, where both train-ing and testing videos belong to a set of pre-deﬁned action categories. This limits their application as the real world is naturally open with unknown actions. Open-set recogni-tion is proposed to identify unknown samples from known ones while maintaining classiﬁcation performance on known samples [57, 31, 4]. It is challenging due to missing knowl-edge of the unknown world. Moreover, deep models are
*Work done during an internship at Wormpex AI Research.
†Corresponding author.
Figure 1. Scene-biased open-set action recognizers fail in two typi-cal scenarios: known actions in unfamiliar scenes, and unknown actions in familiar scenes. The former leads to low precision on open-set detection, while the latter leads to low recall. Our method focuses on mitigating the scene bias to improve OSAR. found to rely on spurious information to make predictions, e.g., classify images using local textures [24, 44] and rec-ognize actions using background scene [40, 13]. This not only hurts the performance under the closed-set setting when training and testing sets are not independent and identically distributed, but also severely degrades the open-set recogni-tion performance, as the distribution of the open-set testing set is unknown.
Open-set action recognition (OSAR) is especially vul-nerable to the spurious information for two main reasons: (1) current benchmark datasets are found to be severely bi-ased, and action classiﬁcation using non-action information (e.g., scene, object, or human) achieves high accuracy [40]; (2) without a speciﬁc module design, the model tends to fo-cus on static information learning instead of temporal action modeling [83, 16, 59, 68, 52].
This paper focuses on mitigating the scene bias in OSAR: we speculate that current OSAR methods are biased toward the scene, and the performance degrades when the testing set exhibits different scene distributions from the training set. Speciﬁcally, existing methods may fail in two typical scenarios: known action in unfamiliar scene and unknown action in familiar scene, as illustrated in Fig. 1. For the
former scenario, a scene-biased recognizer would falsely rec-ognize the action as unknown given the scene is unfamiliar to the training set, and lowers the OSAR precision. For the latter scenario, a scene-biased recognizer may falsely recog-nize the unknown action as known if a familiar scene has appeared during training, which further lowers the OSAR recall. Consequently, the two above situations degrade the overall OSAR performance. To verify our speculations, a quantitative scene bias analysis experiment is carried out in Sec. 3, and the results reveal a strong correlation between the testing scene distribution shift and OSAR performance.
To mitigate scene bias, we propose a Scene-debiasing
Open-set Action Recognition method (SOAR), which fea-tures an adversarial scene reconstruction module (AdRecon) and an adaptive adversarial scene classiﬁcation module (AdaScls). As shown in Fig. 3, we formulate the OSAR task as an uncertainty estimation problem, where the recent evidential deep learning is leveraged to quantify the second-order prediction uncertainty [58, 1, 3, 39]. To mitigate scene bias, AdRecon promotes the backbone to reduce scene infor-mation by applying adversarial learning between a decoder and the backbone. Meanwhile, AdaScls encourages the back-bone to learn scene-invariant feature by preventing a scene classiﬁer from predicting the scene type of input videos.
Speciﬁcally, for AdRecon, our intuition stems from the observation that reconstruction autoencoders prioritize re-constructing the low-frequency part of the input [29], which typically corresponds to the static scene in the video domain.
Therefore, we regard the decoder that takes as input video feature and reconstructs the video as a scene information extractor. By applying adversarial learning between the de-coder and the encoder, AdRecon promotes the encoder (i.e., the feature backbone) to reduce scene information within the output feature. Furthermore, to reduce the noise from re-constructing the foreground motion, we propose background estimation and uncertainty-guided reconstruction to make the decoder focus on background scene reconstruction, thus preserving motion information during adversarial learning.
For AdaScls, instead of only conducting video-level ad-versarial scene classiﬁcation as in [13], we propose to adap-tively apply weights on the background and foreground lo-cations: higher weights on the action foreground and lower weights on the background scene, where the background and foreground locations are determined by the learned spatio-temporal uncertainty map. As a result, AdaScls prioritizes debiasing on the foreground, and promotes scene-invariant action feature learning.
Extensive experiments performed on UCF101 [63],
HMDB51 [38] and MiTv2 [45] demonstrate the effective-ness of our proposed modules, and show our SOAR achieves state-of-the-art OSAR performance. Besides, quantita-tive scene bias analysis experiments reveal that our SOAR achieves the lowest scene bias compared to previous arts.
To summarize, our contributions are threefold:
• We design a quantitative experiment to analyze the scene bias of current OSAR methods. The results reveal a strong correlation between testing scene distribution shift and
OSAR performances. Our SOAR achieves the lowest scene bias while outperforming state-of-the-art OSAR methods, demonstrating the effectiveness of our debias method.
• We propose an adversarial scene reconstruction module.
By preventing a decoder from reconstructing the video background from the extracted feature, AdRecon forces the backbone to reduce scene information from the feature while preserving motion information.
• We propose an adaptive adversarial scene classiﬁcation module, which prevents a scene classiﬁcation head from predicting the scene type of the video. Beneﬁting from additional guidance from the learned uncertainty map,
AdaScls promotes effective scene-invariant feature learn-ing. 2.