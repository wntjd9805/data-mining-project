Abstract
Vector Quantisation (VQ) is experiencing a comeback in machine learning, where it is increasingly used in represen-tation learning. However, optimizing the codevectors in ex-isting VQ-VAE is not entirely trivial. A problem is codebook collapse, where only a small subset of codevectors receive gradients useful for their optimisation, whereas a majority of them simply “dies off” and is never updated or used. This limits the effectiveness of VQ for learning larger codebooks in complex computer vision tasks that require high-capacity representations. In this paper, we present a simple alterna-tive method for online codebook learning, Clustering VQ-VAE (CVQ-VAE). Our approach selects encoded features as anchors to update the “dead” codevectors, while optimis-ing the codebooks which are alive via the original loss. This strategy brings unused codevectors closer in distribution to the encoded features, increasing the likelihood of being cho-sen and optimized. We extensively validate the generaliza-tion capability of our quantiser on various datasets, tasks (e.g. reconstruction and generation), and architectures (e.g.
VQ-VAE, VQGAN, LDM). CVQ-VAE can be easily inte-grated into the existing models with just a few lines of code. 1.

Introduction
Vector Quantisation (VQ) [12] is a basic building block of many machine learning techniques. It is often used to help learning unsupervised representations for vision and language tasks, including data compression [1, 39, 36], recognition [26, 3, 44, 24, 23], and generation [37, 31, 11, 32, 47, 34, 33]. VQ quantises continuous feature vectors into a discrete space by embedding them to the closest vec-tors in a codebook of representatives or codevectors. Quan-tisation has been shown to simplify optimization problems by reducing a continuous search space to a discrete one.
Despite its success, VQ has some drawbacks when ap-plied to deep networks [37]. One of them is that quanti-sation stops gradients from back-propagating to the code-vectors. This has been linked to codebook collapse [36], (a) VQ-VAE [37]
Usage: 9.96% (b) SQ-VAE [36]
Usage: 49.02% (c) CVQ-VAE
Usage: 100% (d) Codebook Perplexity (e) Reconstruction error
Figure 1: Codebook usage and reconstruction error. The setting is the same as VQ-VAE [37], except for the differ-ent quantisers. All models are trained and evaluated on the
CIFAR10 [20] dataset. VQ-VAE has many “dead” vectors (green points) which are not used. CVQ-VAE updates these unoptimized vectors by using online sampled feature an-chors, leading to a 100% usage of the codebook. CVQ-VAE achieves substantially higher codebook perplexity and bet-ter reconstruction results than with the ﬁxed initialization. which means that only a small subset of active codevec-tors are optimized alongside the learnable features, while the majority of them are not used at all (see the green
“dead” points in Fig. 1(a)). As a result, many recent meth-ods [11, 10, 44, 32, 6, 47] fail to utilise the full expressive power of a codebook due to the low codevector utilisation, especially when the codebook size is large. This signiﬁ-cantly limits VQ’s effectiveness.
To tackle this issue, we propose a new alternative quan-tiser called Clustering VQ-VAE (CVQ-VAE). We observe that classical clustering algorithms, such as reﬁned initial-ization k-means [4] and k-means++ [2], use a dynamic clus-ter initialization approach. For example, k-means++ ran-domly selects a data point as the ﬁrst cluster centre, and
then chooses the next new centre based on a weighted prob-ability calculated from the distance to the previous cen-tres. Analogously, CVQ-VAE dynamically initializes un-optimized codebooks by resampling them from the learned features (Fig. 2). This simple approach can avoid codebook collapse and signiﬁcantly enhance the usage of larger code-books by enabling optimization of all codevectors (achiev-ing 100% codebook utilisation in Fig. 1(c)).
While CVQ-VAE is inspired by previous dynamic clus-ter initialization techniques [4, 2], its implementation in deep networks requires careful consideration. Unlike tradi-tional clustering algorithms [25, 4, 14, 2] where source data points are ﬁxed, in deep networks features and their corre-sponding codevectors are mutually and incrementally op-timized. Thus, simply sampling codevectors from a single snapshot of features would not work well because any mini-batch used for learning cannot capture the true data distribu-tion, as demonstrated in our ofﬂine version in Tab. 3. To ﬁx this issue, we propose to compute running averages of the encoded features across different training mini-batches and use these to improve the dynamic reinitialization of the col-lapsed codevectors. This operation is similar to an online feature clustering method that calculates average features across different training iterations (Fig. 2). While this may seem a minor change, it leads to a very signiﬁcant improve-ment in terms of performance (Fig. 1(e)).
As a result of these changes, CVQ-VAE signiﬁcantly outperforms the previous models VQ-VAE [37] and SQ-VAE [36] on various datasets under the same setting, and with no other changes except for swapping in the new quan-tiser. Moreover, we conduct thorough ablation experiments on variants of the method to demonstrate the effectiveness of our design and analyse the importance of various design factors. Finally, we incorporate CVQ-VAE into large mod-els (e.g. VQ-GAN [11] and LDM [32]) to further demon-strate its generality and potential in various applications. 2.