Abstract
As a new paradigm in machine learning, self-supervised learning (SSL) is capable of learning high-quality represen-tations of complex data without relying on labels. In addition to eliminating the need for labeled data, research has found that SSL improves the adversarial robustness over supervised learning since lacking labels makes it more challenging for adversaries to manipulate model predictions. However, the extent to which this robustness superiority generalizes to other types of attacks remains an open question.
We explore this question in the context of backdoor at-tacks. Specifically, we design and evaluate CTRL, an embar-rassingly simple yet highly effective self-supervised backdoor attack. By only polluting a tiny fraction of training data (
≤ 1%) with indistinguishable poisoning samples, CTRL causes any trigger-embedded input to be misclassified to the ad-versary’s designated class with a high probability ( 99%) at inference time. Our findings suggest that SSL and su-pervised learning are comparably vulnerable to backdoor attacks. More importantly, through the lens of CTRL, we study the inherent vulnerability of SSL to backdoor attacks.
With both empirical and analytical evidence, we reveal that the representation invariance property of SSL, which benefits adversarial robustness, may also be the very reason making
SSL highly susceptible to backdoor attacks. Our findings also imply that the existing defenses against supervised backdoor attacks are not easily retrofitted to the unique vulnerability of
SSL. Code is available at: https://github.com/meet-cjli/CTRL
≥ 1.

Introduction
As a new machine learning paradigm, self-supervised learning (SSL) has gained tremendous advances recently [4, 12, 7]. Without requiring data labeling or human anno-tations, SSL is able to learn high-quality representations of complex data and enable a range of downstream tasks.
In particular, contrastive learning, one dominant SSL ap-proach [4, 12, 7, 5, 14], performs representation learning by
Figure 1: Comparison of supervised and self-supervised backdoor attacks; self-supervised backdoor attacks influence the label space only indirectly through the representations. aligning the features1 of the same sample under varying data augmentations (e.g., random cropping) while separating the features of different samples. In many tasks, contrastive learning has attained performance comparable to supervised learning [12]. Meanwhile, besides obviating the reliance on data labeling, SSL also benefits the robustness to adversar-ial perturbation, label corruption, and data distribution shift by making it more challenging for the adversary to influ-ence model predictions directly [17, 58]. However, whether this robustness benefit generalizes to other malicious attacks remains an open question.
In this work, we explore this question in the context of backdoor attacks, in which the adversary plants “back-doors” functions into target models during training and ac-tivates such backdoors at inference. Recent work has ex-plored new ways to inject backdoors into SSL-trained mod-els [37, 28, 2, 21]; however, the existing attacks appear to be significantly less effective than their supervised counterparts: they either work for specific, pre-defined inputs only [21, 28] or succeed with a low probability (e.g., 2% on ImageNet-100 [37]). These observations raise a set of intriguing and
≤ 1Below we use the terms “feature” and “representation” exchangeably.
critical questions:
RQ1 – Is SSL comparably vulnerable to backdoor attacks as supervised learning?
RQ2 – If so, what makes it highly vulnerable?
RQ3 – What are the implications of this vulnerability?
Our Work. This work represents a solid step toward answering these questions.
RA1 – We present CTRL2, a simple yet highly effective self-supervised backdoor attack. Compared with the exist-ing attacks, (i) CTRL assumes that the adversary is able to pollute a tiny fraction of training data yet without any con-trol of the training process; (ii) it defines the “trigger” as an augmentation-insensitive perturbation in the spectral space of inputs and generates poisoning data indistinguishable from clean data; (iii) it aims to force all trigger-embedded inputs to be misclassified to the adversary’s designated class at inference. With evaluation on benchmark models and datasets, we show that SSL is also highly vulnerable to backdoor attacks. For instance, by poisoning 1% of the training data, CTRL achieves 99% attack success rate on
≥
CIFAR-10. This level of vulnerability is comparable to what are observed in supervised backdoor attacks.
≤
RA2 – Through the lens of CTRL, we study the inherent vulnerability of SSL. Intuitively, CTRL exploits data augmen-tation and contrastive loss, two essential ingredients of SSL
[4, 12], which together entail the representation invariance property: different augmented views of the same input share similar representations. Given the overlap between the aug-mented views of trigger-embedded and target-class inputs, enforcing representation invariance naturally entangles them in the feature space, as illustrated in Figure 1, incurring the risk of backdoor attacks. This mechanism fundamentally dif-fers from supervised backdoor attacks [46, 54, 33], which di-rectly associate the trigger pattern with the target class in the label space, while the representations of trigger-embedded and target-class inputs are not necessarily aligned [42].
RA3 – Moreover, we discuss the challenges to defending against self-supervised backdoor attacks. We find that ex-isting defenses against supervised backdoor attacks are not easily retrofitted to the unique vulnerability of SSL. For in-stance, SCAN [42], a state-of-the-art defense, detects trigger-embedded inputs based on the statistical properties of their representations; however, it is ineffective against CTRL, due to the inherent entanglement between the representations of trigger-embedded and target-class inputs.
Our Contributions. This work establishes a strong base-line for comprehending the inherent vulnerability of SSL to backdoor attacks. By employing innovative techniques and insights, our study contributes to the field in the following ways. 2CTRL: Contrastive TRojan Learning.
We present CTRL, a simple yet effective self-supervised backdoor attack, which greatly reduces the gap between the attack effectiveness of supervised and self-supervised backdoor attacks. Leveraging CTRL, we show that SSL is highly susceptible to backdoor attacks. Our findings imply that the benefit of SSL for adversarial robustness superiority may not generalize to trojan attacks.
With both empirical and analytical evidence, we reveal that (i) self-supervised backdoor attacks may function by en-tangling the representations of trigger-embedded and target-class inputs; (ii) the representation invariance property of
SSL, which benefits adversarial robustness, may also account for the vulnerability of SSL to backdoor attacks.
We evaluate SSL on some existing defenses and point out several promising directions for further research. 2.