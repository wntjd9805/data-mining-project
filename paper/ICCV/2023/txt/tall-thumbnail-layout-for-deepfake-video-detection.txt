Abstract
The growing threats of deepfakes to society and cyberse-curity have raised enormous public concerns, and increas-ing efforts have been devoted to this critical topic of deep-fake video detection. Existing video methods achieve good performance but are computationally intensive. This paper introduces a simple yet effective strategy named Thumb-nail Layout (TALL), which transforms a video clip into a pre-defined layout to realize the preservation of spa-tial and temporal dependencies. Specifically, consecutive frames are masked in a fixed position in each frame to im-prove generalization, then resized to sub-images and rear-ranged into a pre-defined layout as the thumbnail. TALL is model-agnostic and extremely simple by only modify-ing a few lines of code.
Inspired by the success of vi-sion transformers, we incorporate TALL into Swin Trans-former, forming an efficient and effective method TALL-Swin. Extensive experiments on intra-dataset and cross-dataset validate the validity and superiority of TALL and
SOTA TALL-Swin. TALL-Swin achieves 90.79% AUC on the challenging cross-dataset task, FaceForensics++ → Celeb-DF. The code is available at https://github.com/ rainy-xu/TALL4Deepfake. 1.

Introduction
Deepfakes generate and manipulate facial appearances to deceive viewers through generation techniques [61, 48].
With the remarkable success of generative adversarial net-works [14, 27], deepfake products have become photo-realistic that humans can not distinguish. These deepfake products [21, 49] may be misused for malicious purposes, leading to severe trust issues and security problems, such as financial fraud, identity theft, and celebrity imperson-*This work was done when she was a student in CRIPAC.
†Corresponding author.
Figure 1. The AUC and FLOPs trade-off of different back-bones.
Image-level backbones with TALL enjoy comparable accuracy-cost trade-offs with the 3DCNN and video transformer family on the unseen Celeb-DF dataset. All models with the same setting are trained on the FF++ (HQ) dataset. ation [53, 40]. The rapid development of social media ex-acerbates the abuse of deepfakes. Therefore, it is crucial to develop advanced detection methods to protect the data privacy of individual users.
Most previous image-based methods [22, 69] perform well on intra-dataset, but their generalizability needs to be improved. Recent research has focused on video-based methods to detect deepfake by modeling spatio-temporal dependencies. There are subtle spatio-temporal inconsis-tencies between frames since the deepfake algorithms are executed frame by frame. The core of video-level ap-proaches for deepfake detection is capturing inconsistencies through temporal modeling. Existing deepfake video detec-tion methods generally follow two directions. Some meth-ods [15, 16] use two-branch networks or modules to learn spatial and temporal information separately and then fuse them. However, these two-branch approaches may frag-ment spatiotemporal cooperation and lead to subtle artifacts being neglected. Others directly use classic temporal mod-els such as LSTM and 3D-CNNs. These methods are com-putationally intensive. The current rise of transformers for vision task backbones has prompted the emergence of cor-responding deepfake detection methods. They are accom-panied by significant computational complexity that makes them challenging to deploy and use, despite breakthroughs in performance. To enjoy benefits from both image and video methods, we are curious to see whether it is possi-ble to append information about the temporal dimension to the image dimension.
This work develops a simple yet effective Thumbnail
Layout (TALL) for deepfake detection by spatio-temporal modeling. TALL is computationally cheap and retains both temporal and spatial information. In detail, we use dense sampling to extract multiple clips in the video and then ran-domly select four consecutive frames in the video segment.
Subsequently, a block is masked at a fixed position in each frame. Finally, the frames are resized as sub-image and se-quentially rearranged into a pre-defined layout as a thumb-nail, which has the same size as the clip frames. As shown in Figure 1, TALL brings two advantages compared to the previous spatio-temporal modeling methods for deepfake detection: (1) TALL contains local and global contextual deepfake patterns. (2) TALL is a model-agnostic method for spatio-temporal modeling deepfake patterns at zero compu-tation and zero parameters.
Furthermore, we discover that the better temporal model-ing capabilities backbone has, the better performance TALL achieves. Based on the proposed TALL, we complement a baseline for video deepfake detection based on Swin
Transformer [36], called TALL-Swin. We validate TALL-Swin on four popular benchmark datasets, including Face-Forensics++, Celeb-DF, DFDC, and DeeperForensics. Our method gains a remarkable improvement over the state-of-the-art approaches. The main contributions of our paper are summarized as follows:
• We provide a new perspective for an efficient strategy for video deepfake detection called Thumbnail Layout (TALL), which incorporates both spatial-temporal de-pendencies, and allows the model to capture spatial-temporal inconsistencies.
• We propose a spatio-temporal modeling method called
TALL-Swin, which efficiently captures the inconsis-tencies between deepfake video frames. 2.