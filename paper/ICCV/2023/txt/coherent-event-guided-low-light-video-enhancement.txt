Abstract 1.

Introduction
With frame-based cameras, capturing fast-moving scenes without suffering from blur often comes at the cost of low
SNR and low contrast. Worse still, the photometric con-stancy that enhancement techniques heavily relied on is frag-ile for frames with short exposure. Event cameras can record brightness changes at an extremely high temporal resolution.
For low-light videos, event data are not only suitable to help capture temporal correspondences but also provide alter-native observations in the form of intensity ratios between consecutive frames and exposure-invariant information. Mo-tivated by this, we propose a low-light video enhancement method with hybrid inputs of events and frames. Specifi-cally, a neural network is trained to establish spatiotemporal coherence between visual signals with different modalities and resolutions by constructing correlation volume across space and time. Experimental results on synthetic and real data demonstrate the superiority of the proposed method compared to the state-of-the-art methods.
∗Corresponding author 1 https://sherrycattt.github.io/EvLowLight
Capturing fast-moving scenes without introducing blurry artifacts (Figure 1 (c)) is challenging, especially in an envi-ronment with insufficient illumination. A fast shutter speed helps to freeze motion (Figure 1 (a)), but it also causes exces-sive noise and low contrast. A popular choice in professional photography, such as sports recording and filmmaking, is to place large fill lights to allow sufficient illumination (Fig-ure 1 (d)), however, they are limited in their portability and power requirements in uncontrolled environments.
Video enhancement aims at improving the degraded quality, in which the key is to exploit the temporal co-herence [27, 59].
Its performance heavily depends on the quality of image-based optical flow estimation that as-sumes spatiotemporally small translation and brightness con-stancy; however, it becomes fragile for low-light frames whose features such as edges are less distinctive and are contaminated by noise (Figure 1 (e)). Despite recent ad-vances in learning-based low-light video enhancement meth-ods [63, 49, 20, 30, 5], it remains challenging to improve the quality of frames capturing fast-moving scenes where the pixel displacements are large (Figure 1 (f)).
Event cameras asynchronously record logarithmic bright-ness changes with a high dynamic range, low latency, and low power cost [39], which raise promising directions for low-light imaging with events [64, 67]. Their unique advan-tage of high temporal resolution in the order of microseconds benefits motion estimation (Figure 2), providing reliable tem-poral coherence between frames in fast-moving scenes for video enhancement. They are free of the notion of exposure time and thus do not suffer from the well-known trade-off between strong blur using long exposure and low SNR using short exposure, which remains hard to be handled by existing deblurring or multi-exposure fusion methods. In this paper, we propose to utilize the high temporal resolution and high dynamic range information from events to guide low-light video enhancement. Specifically, motion is jointly estimated from events and frames for capturing temporal coherence as guidance to warp and integrate multimodal observations according to the same scene points for noise reduction.
However, it is non-trivial due to three types of misalign-ment: i) Modality. Events asynchronously record bright-ness changes, while frames synchronously record absolute brightness. They are inherently different modalities, whose gap is further increased by noise from their mechanisms in low-light conditions, making their translation [64] or fu-sion [37, 36] hard (Figure 1 (g)). ii) Sensor. Hybrid sensors with precisely aligned events and frames have low spatial resolutions (e.g., 346 × 260 [14]). Although hybrid camera systems [48, 47] have high-resolution frames (e.g., 2448 × 2048 [67]), online registration for each scene (with different depths and arbitrary lighting) that is important in obtaining stable results becomes fragile in low-light conditions since features for matching are too weak to be precisely extracted (Figure 1 (a) vs. (b)). iii) Temporal resolution. Pixels corre-sponding to the same scene points in events and frames are recorded in different temporal resolutions, between which the established correspondences should be robust to inaccu-racy in motion estimation caused by limited photons.
To overcome the above challenges, the paper proposes to establish the spatiotemporal coherence between events and frames by following strategies:
• A multimodal coherence modeling module that estab-lishes multiscale all-pair coherence between events and frames in the feature space to compensate for modality misalignment in sensing ability and the sensor-level misalignment under low-light conditions.
• A temporal coherence propagation module that sam-ples features of consecutive events and frames cor-responding to the same scene point for realizing coherence-aware aggregation in the local displacement field to improve the SNR of the reconstructed video.
The modules newly designed above extract comple-mentary information from events and frames, enable a misalignment-robust hybrid-imaging setting, and integrate
Figure 2. Comparison of optical flow estimation. Optical flow (d) and (e) are estimated from (a) low-light frames and (b) events by using the state-of-the-art methods [45] and [9], respectively, while (f) is estimated by the proposed joint optical flow estimation module from both (a) and (b). information across time for effective denoising. They com-pose our contribution to the first event guided low-light video enhancement method that captures high-quality videos in fast-moving scenes with short exposure. The superior per-formances against state-of-the-art methods for synthetic and real data make it potentially useful as an alternative to fill lights for low-light photography. 2.