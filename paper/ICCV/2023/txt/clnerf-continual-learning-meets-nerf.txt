Abstract
Novel view synthesis aims to render unseen views given a set of calibrated images. In practical applications, the coverage, appearance or geometry of the scene may change over time, with new images continuously being captured.
Efﬁciently incorporating such continuous change is an open challenge. Standard NeRF benchmarks only involve scene coverage expansion. To study other practical scene changes, we propose a new dataset, World Across Time (WAT), con-sisting of scenes that change in appearance and geometry over time. We also propose a simple yet effective method,
CLNeRF, which introduces continual learning (CL) to Neu-ral Radiance Fields (NeRFs). CLNeRF combines gener-ative replay and the Instant Neural Graphics Primitives (NGP) architecture to effectively prevent catastrophic forget-ting and efﬁciently update the model when new data ar-rives. We also add trainable appearance and geometry embeddings to NGP, allowing a single compact model to handle complex scene changes. Without the need to store historical images, CLNeRF trained sequentially over mul-tiple scans of a changing scene performs on-par with the upper bound model trained on all scans at once. Com-pared to other CL baselines CLNeRF performs much better across standard benchmarks and WAT. The source code, a demo, and the WAT dataset are available at https:
//github.com/IntelLabs/CLNeRF. 1.

Introduction
Neural Radiance Fields (NeRFs) have emerged as the pre-eminent method for novel view synthesis. Given images of a scene from multiple views, NeRFs can effectively interpolate between them. However, in practical applications (e.g., city rendering [32]), the scene may change over time, resulting in a gradually revealed sequence of images with new scene coverage (new city blocks), appearance (lighting or weather) and geometry (new construction). Learning continually from such sequential data is an important problem.
Naive model re-training on all revealed data is expensive, millions of images may need to be stored for large scale systems [32]. Meanwhile, updating the model only on new
data leads to catastrophic forgetting [22], i.e., old scene geometry and appearances can no longer be recovered (see
Fig. 1).
Inspired by the continual learning literature for image classiﬁcation [7], this work studies continual learning in the context of NeRFs to design a system that can learn from a sequence of scene scans without forgetting while requiring minimal storage.
Replay is one of the most effective continual learning algorithms; it trains models on a blend of new and historical data. Experience replay [5] explicitly stores a tiny portion of the historical data for replay, while generative replay [30] synthesizes replay data using a generative model (e.g., a
GAN [9]) trained on historical data. Experience replay is more widely used in image classiﬁcation, since generative models are often hard to train, perform poorly on high res-olution images, and introduce new model parameters. In contrast, NeRFs excel at generating high-resolution images, making them ideal candidates for generative replay.
Motivated by this synergy between advanced NeRF mod-els and generative replay, we propose CLNeRF which com-bines generative replay with Instant Neural Graphics Prim-itives (NGP) [24] to enable efﬁcient model updates and to prevent forgetting without the need to store historical images.
CLNeRF also introduces trainable appearance and geometry embeddings into NGP so that various scene changes can be handled by a single model. Unlike classiﬁcation-based continual learning methods whose performance gap to the upper bound model is still non-negligible [30], the synergy between continual learning and advanced NeRF architec-tures allows CLNeRF to achieve a similar rendering quality as the upper bound model (see Fig. 1).
Contributions: (1) We study the problem of continual learn-ing in the context of NeRFs. We present World Across Time (WAT), a practical continual learning dataset for NeRFs that contains scenes with real-world appearance and geometry changes over time. (2) We propose CLNeRF, a simple yet effective continual learning system for NeRFs with minimal storage and memory requirements. Extensive experiments demonstrate the superiority of CLNeRF over other continual learning approaches on standard NeRF datasets and WAT. 2.