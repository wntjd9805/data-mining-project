Abstract
The commonly adopted detect-then-match approach to registration finds difficulties in the cross-modality cases due to the incompatible keypoint detection and inconsistent fea-ture description. We propose, 2D3D-MATR, a detection-free method for accurate and robust registration between images and point clouds. Our method adopts a coarse-to-fine pipeline where it first computes coarse correspon-dences between downsampled patches of the input im-age and the point cloud and then extends them to form dense correspondences between pixels and points within the patch region. The coarse-level patch matching is based on transformer which jointly learns global contextual con-straints with self-attention and cross-modality correlations with cross-attention. To resolve the scale ambiguity in patch matching, we construct a multi-scale pyramid for each im-*Equal contribution.
†Corresponding author: kevin.kai.xu@gmail.com. age patch and learn to find for each point patch the best matching image patch at a proper resolution level. Ex-tensive experiments on two public benchmarks demonstrate that 2D3D-MATR outperforms the previous state-of-the-art P2-Net by around 20 percentage points on inlier ra-tio and over 10 points on registration recall. Our code and models are available at https://github.com/ minhaolee/2D3DMATR. 1.

Introduction
The inter-modality registration between images and point clouds finds applications in many computer vision tasks, e.g., 3D reconstruction, camera relocalization, SLAM and AR. It aims at estimating a rigid transformation that aligns a scene point cloud into the camera coordinates of an image capturing the same scene. The typical pipeline of 2D-3D registration is to first extract correspondences between
pixels and points and then adopt robust pose estimators such as PnP-RANSAC [25, 16] to recover the alignment transfor-mation. Therefore, the accuracy of the putative correspon-dences is the crux of a successful registration.
Following the intra-modality correspondence methods for stereo images [12, 32, 44, 14] or point clouds [18, 9, 2, 21], 2D-3D matching methods [15, 37, 51] usually adopt a detect-then-match approach where 2D and 3D keypoints are first detected independently in the image and the point cloud, respectively, and then matched based on their as-sociated descriptors. Such method, however, suffers from two difficulties. First, 2D and 3D keypoints are detected in different visual domains. While 2D keypoint detection is based on texture and color information, 3D detection is hinged on local geometry. This makes the detection of re-peatable keypoints difficult. Second, 2D and 3D descriptors encode different visual information, which hampers extract-ing consistent descriptors for matching pixels and points.
As a consequence, existing 2D-3D matching methods often lead to too low inlier ratio to be practically usable.
Recently, detection-free approach has received increas-ing attention in both stereo matching [41, 27, 54, 46] and point cloud registration [53, 38]. Saving the step of keypoint detection, it achieves high-quality correspondence with a coarse-to-fine pipeline: It first establishes coarse correspon-dences at the level of image or point patches and then refines them into fine-grained matching of pixels or points. This method has shown strong superiority over detection-based ones due to the exploitation of global contextual informa-tion at patch level. Such success, however, has not been attained for 2D-3D matching. This is because designing a coarse-level 2D-3D matching is non-trivial due to the scale ambiguity between image and point patches caused by per-spective projection (see Fig. 1). On the one hand, the re-ceptive fields for extracting 2D and 3D features could be misaligned, resulting in inconsistency between 2D and 3D features. On the other hand, there could be many pixels or points finding no counterpart on other side due to occlusion, leading to considerable ambiguity for fine-level matching.
We propose 2D3D-MATR, the first, to our knowledge, detection-free method for accurate and robust 2D-3D regis-tration via addressing the challenges above. Adapting the coarse-to-fine pipeline, our method first computes coarse correspondences between downsampled patches of the in-put image and the point cloud and then extends them to form dense correspondences between pixels and points within the patch regions. To achieve accurate feature alignment be-tweem image and point patches, we design a coarse-level matching module based on transformer [50] which jointly learns global contextual constraints with self-attention and cross-modality correlations with cross-attention.
Our key insight is that the feature misalignment between 2D and 3D due to projection can be resolved by image-space multi-scale sampling and matching, assuming that the area of local patches is small and the projection distor-tions is negligible. We construct a multi-scale pyramid for each image patch. During training, we find for each point patch the best matching image patch at a proper resolution level through computing the bilateral overlap between them in the image space. During test, our model can automati-cally infer 2D-3D patch correspondences at a proper scale and produces dense correspondence in a high inlier ratio.
Extensive experiments on the RGB-D Scenes V2 [22] and 7-Scenes [17] benchmarks demonstrate the efficacy of our method. In particular, 2D3D-MATR outperforms the pre-vious state-of-the-art P2-Net [51] by at least 20 percentage points on inlier ratio and over 10 points on registration re-call on the two benchmarks. Our contributions include:
• The first detection-free coarse-to-fine matching net-work for 2D-3D registration which first establishes coarse correspondences of patch level and then refines them into dense correspondences of pixel/point level.
• A transformer-based coarse matching module learning well-aligned 2D and 3D features with both global con-textual constraints and cross-modality correlations.
• A multi-scale 2D-3D matching scheme that resolves 2D-3D feature misalignment through learning image-space multi-scale features and feature-scale selection. 2.