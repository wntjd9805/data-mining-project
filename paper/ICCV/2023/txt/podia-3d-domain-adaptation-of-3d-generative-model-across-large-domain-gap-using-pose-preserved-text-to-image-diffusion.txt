Abstract
Recently, significant advancements have been made in 3D generative models, however training these models across diverse domains is challenging and requires an huge amount of training data and knowledge of pose distribution. Text-†Corresponding author. guided domain adaptation methods have allowed the gener-ator to be adapted to the target domains using text prompts, thereby obviating the need for assembling numerous data.
Recently, DATID-3D presents impressive quality of sam-ples in text-guided domain, preserving diversity in text by leveraging text-to-image diffusion. However, adapting 3D generators to domains with significant domain gaps from
the source domain still remains challenging due to issues in current text-to-image diffusion models as following: 1) shape-pose trade-off in diffusion-based translation, 2) pose bias, and 3) instance bias in the target domain, resulting in inferior 3D shapes, low text-image correspondence, and low intra-domain diversity in the generated samples. To address these issues, we propose a novel pipeline called PODIA-3D, which uses pose-preserved text-to-image diffusion-based do-main adaptation for 3D generative models. We construct a pose-preserved text-to-image diffusion model that allows the use of extremely high-level noise for significant domain changes. We also propose specialized-to-general sampling strategies to improve the details of the generated samples.
Moreover, to overcome the instance bias, we introduce a text-guided debiasing method that improves intra-domain diversity. Consequently, our method successfully adapts 3D generators across significant domain gaps. Our qualitative results and user study demonstrate that our approach outper-forms existing 3D text-guided domain adaptation methods in terms of text-image correspondence, realism, diversity of rendered images, and sense of depth of 3D shapes in the generated samples. 1.

Introduction
Recently, 3D generative models [20, 37, 8, 11, 23, 24, 40, 38, 5, 23, 25, 33, 10, 39, 4, 1] have been advanced to enable multi-view consistent and explicitly pose-controlled image synthesis. However, training state-of-the-art 3D gen-erative models is challenging due to the requirement of a large number of images and knowledge about their camera pose distribution. This prerequisite has resulted in limited applications of these models to only a few domains.
Text-guided domain adaptation methods such as
StyleGAN-NADA [9], HyperDomainNet [2], DATID-3D [18], and StyleGANFusion [35] have emerged as a promising solution to overcome the challenge of need for additional data of the target domain. These methods leverage
CLIP [27] or text-to-image diffusion models [31, 28, 32] that are pretrained on a large number of image-text pairs.
Although non-adversarial fine-tuning methods like
StyleGAN-NADA [9], HyperDomainNet [2], and Style-GANFusion [35] have demonstrated impressive results, they suffer from the inherent loss of diversity in a text prompt and suboptimal text-image correspondence, as il-lustrated in Fig. 1 (See results of StyleGAN-NADA*). Re-cently, a diversity-preserved domain adaptation method called DATID-3D [18] has been developed for 3D generators, which achieves compelling quality of multi-view consistent image synthesis in text-guided domains. This method gener-ates pose-aware target dataset using text-to-image diffusion models and fine-tunes the 3D generator on the target dataset.
Despite the use of this method, the adaptation of 3D gen-erators to domains that have significant domain gaps from 0 generated from xsrc the source domain remains challenging due to the problems encountered in the current text-to-image diffusion models. 1) shape-pose trade-off in diffusion-based translation: For text-guided pose-aware target generation, we first perturb the source image or latent xsrc 0 until tr ∈ [1, T ] such that xtrg tr should represent the features cor-responding the target domains without altering pose of xsrc 0 .
However, our investigations show that when the target do-main requires selecting a high tr to achieve a significant structural change, preserving the pose is not guaranteed, as depicted in Fig. 2(a). Consequently, shifting the generator to a target domain that requires significant shape changes can lead to poor 3D shapes or low text correspondence, as illustrated in Fig. 1 (See SpongeBob by DATID-3D [18]). 2)
We found that a publicly available text-to-image diffusion model, has pose bias issues for certain target domain text prompts, as illustrated in Fig. 2(b). Pose bias represents that the position and orientation of certain objects in images from T2I diffusion models are biased (mainly toward the front or side). Accordingly, the shifted generators guided by these text prompts result in either poor 3D structure as represented in Fig. 1 (See Horse by DATID-3D [18]). 3)
We also found that the text-to-image diffusion models often generate images with one or a few instances among many in-stances representing the text prompts as represented in Fig. 2.
In consequence, the shifted generators guided by these text prompts result in low intra-domain diversity as represented in Fig. 1. (See Dog by DATID-3D [18]).
To address these issues, we propose a novel pipeline called PODIA-3D, a method of POse-preserved text-to-image DIffusion-based domain Adaptation for 3D generative model. We construct pose-preserved text-to-image diffusion models. We first collect target images that have the same pose but different shapes with source images through 3 strate-gies: identity mixing, text-guided image translation with pose-guaranteed prompts, utilizing the different domain gen-erator. Then, we fine-tune the depth-guided diffusion model to make it ignore the shape information from the depth map and focus only on pose information. Furthermore, we pro-pose a specialized-to-general sampling strategy to improve details of generated images and resolve the detail bias issue.
Using pose-preserved diffusion models and specialized-to-general sampling, we are able to synthesize pose-consistent target images with excellent text-image correspondence by using extremely high-level noise for large shape change. We then fine-tune the state-of-the-art 3D generator adversari-ally on the generated target images. Moreover, to improve intra-domain diversity, we propose a text-guided debiasing method, which enables the fine-tuned generator to reach the diverse modes. As a result, our method effectively adapts 3D generators across significant domain gaps, generating excel-lent text-image correspondence and 3D shapes, as shown in
Fig. 1. Our approach has been demonstrated to outperform
Figure 2. Issues in pose-aware target generation for domain adaption of 3D generative models using current text-to-image diffusion models: (a) a shape-pose trade-off in diffusion-based translation, (b) pose bias, and (c) instance bias in the target domain. existing 3D text-guided domain adaptation methods in terms of text-image correspondence, realism, diversity of rendered images, and sense of depth of 3D shapes in the generated samples via the qualitative results and user study. 2.