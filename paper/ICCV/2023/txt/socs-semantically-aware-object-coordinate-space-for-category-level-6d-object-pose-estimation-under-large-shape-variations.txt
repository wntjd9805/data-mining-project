Abstract
Most learning-based approaches to category-level 6D pose estimation are design around normalized object coor-dinate space (NOCS). While being successful, NOCS-based methods become inaccurate and less robust when handling objects of a category containing significant intra-category shape variations. This is because the object coordinates induced by global and rigid alignment of objects are se-mantically incoherent, making the coordinate regression hard to learn and generalize. We propose Semantically-aware Object Coordinate Space (SOCS) built by warping-and-aligning the objects guided by a sparse set of keypoints with semantically meaningful correspondence. SOCS is se-mantically coherent: Any point on the surface of a object can be mapped to a semantically meaningful location in
SOCS, allowing for accurate pose and size estimation under large shape variations. To learn effective coordinate regres-sion to SOCS, we propose a novel multi-scale coordinate-based attention network. Evaluations demonstrate that our method is easy to train, well-generalizing for large intra-category shape variations and robust to inter-object occlu-sions. Code is provided at: https://github.com/ wanboyan/SOCS. 1.

Introduction 6D object pose estimation, i.e., determining the 3D rota-tion and translation of a object in the camera coordinate sys-tem, is an important computer vision task with a large body of literature [2, 27, 13, 17]. Category-level object pose esti-mation attempts to solve the problem without relying on the exact CAD model of the target object [28], which is hence more challenging than instance-level one. Since the semi-nal work of Wang et al. [28], most existing category-level works are based on a canonical representation of Normal-ized Object Coordinate Space (NOCS). Given an unseen object instance, they learn a neural network to map the per-*Joint first authors
†Corresponding author
Figure 1. NOCS [28], constructed with globally aligned objects, finds difficulty in handling large intra-category shape variations.
In this example, the coordinates regressed against NOCS for the long-lens camera contain much error (w.r.t. the CAD model un-der ground-truth pose and size) and the resulting pose is incorrect
In contrast, the coordinates regressed against SOCS, (middle). built by semantically-guided non-rigid object alignment, are se-mantically coherent, leading to better pose estimation (right). spective projection of the object to the NOCS of the corre-sponding category from which object pose can be estimated.
Given an object category, NOCS is defined by globally aligning a set of 3D object instances with normalized size and poses. It works well for objects with moderate intra-category shape variations. When handling object categories containing significant shape variations, however, NOCS-based methods become inaccurate and less robust. This is because the object coordinates induced by global and rigid alignment are not semantically coherent. For instance, a point on the lens of a long-lens camera would be mapped to a semantically incorrect point in NOCS if the NOCS was constructed with camera models of significantly varying part proportions. Such misalignment makes the mapping network hard to learn and generalize, thus causing inferior pose accuracy under large shape variations (Figure 1).
To tackle this issue, we propose Semantically-aware Ob-ject Coordinate Space (SOCS) to achieve accurate and ro-bust category-level 6D object pose and size estimation un-der large shape variations. Unlike NOCS which is con-structed by directly aligning pose and size normalized ob-jects of a specific category, SOCS is built by warping-and-aligning the objects guided by a sparse set of keypoints with semantically meaningful correspondence, leveraging the state-of-the-art category-specific keypoint selection and matching for a shape set [25].
In particular, we align all objects of a specific category in the training set to the av-erage shape [26] of the set. We utilize 3D thin-plate spline warping [9] to ensure a smooth non-rigid deformation and hence coordinate interpolation. SOCS is therefore seman-tically coherent: Any point on the surface of a object can be mapped to a semantically meaningful location in SOCS, allowing for accurate pose and size estimation.
To learn the mapping from image space to SOCS effec-tively, we propose a novel multi-scale coordinate-based at-tention network. To capture the shape variation of the target object in image space, we devise a multi-scale feature ex-traction network with cross-attention feature aggregation.
In the cross-attention module, we encode global point po-sitions to help better extract coordinate-sensitive features.
Thanks to such global positional encoding, our network is able to model 3D points in the full space, which further en-ables a dense point sampling in SOCS training. The latter facilitates dense coordinate estimation even for unobserved locations, which is critical to handling inter-object occlu-sions. To attain pose invariance, the network is trained in a contrastive fashion with a pose consistency loss.
We conducted extensive evaluations demonstrating that our method is 1) easy to train, 2) well-generalizing for large intra-category shape variations, and 3) robust to inter-object occlusions. Even with the vanilla mapping net-work of [28], our method is still comparable to state of the arts, clearly showing the effectiveness of SOCS. Our full method achieves state-of-the-art on the NOCS-REAL275 improving the 5◦5cm and ModelNet40-partial datasets, score by 5.6 pts on NOCS-REAL275 and 5◦0.05 score by 16 pts on ModelNet40-partial. In particular, ModelNet40-partial contains categories containing objects with large shape variations.
In summary, our work makes two contributions. First, we propose semantically-aligned object coordinate space (SOCS) to accommodate large intra-category shape varia-tions for semantically coherent coordinate regression. Sec-ond, we propose a multi-scale attention network for learning the mapping from image space to SOCS effectively allow-ing for dense coordinate regression. 2.