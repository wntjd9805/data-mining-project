Abstract more results: https://research.nvidia.com/labs/dir/space/.
Animating portraits using speech has received growing attention in recent years, with various creative and practical use cases. An ideal generated video should have good lip sync with the audio, natural facial expressions and head motions, and high frame quality. In this work, we present
SPACE, which uses speech and a single image to generate high-resolution, and expressive videos with realistic head pose, without requiring a driving video. It uses a multi-stage approach, combining the controllability of facial landmarks with the high-quality synthesis power of a pretrained face generator. SPACE also allows for the control of emotions and their intensities. Our method outperforms prior methods in objective metrics for image quality and facial motions and is strongly preferred by users in pair-wise comparisons.
Please visit the project page to view the videos and to see 1.

Introduction
Speech-driven portrait animation, which concerns animat-ing a still image of a face using an arbitrary input speech signal, has a wide range of applications. For example, it can be used for driving characters in computer games, dub-bing in movies, and animating avatars for virtual assistance, virtual reality, and telecommunications. It has to use the provided speech to predict all the nuances in human facial expressions while also guaranteeing that the animation looks natural, matches what is being said in the speech sample, and preserves the per-frame and video quality.
These requirements make the task of speech-driven por-trait animation challenging. To make things harder, there
Table 1: Comparison of functionality with prior works. For fairness, we only compare with one-shot person-agnostic animation models.
SPACE Wav2Lip MakeItTalk PC-AVS Audio2Head MEAD EAMM GC-AVT (ours)
[36]
[23]
[16]
[42]
[37]
[19]
[43]
Controllability
• Emotion
• Facial landmarks
• Pose transfer
• Pose generation
Output resolution
✓
✓
✓
✓ 512
✗
✗
✓
✗ 96
✗
✓
✗
✓ 256
✗
✗
✓
✗ 224
✗
✗
✗
✓ 256
✓
✗
✗
✗ 384
✓
✗
✓
✗ 256
✓
✗
✓
✗ 256 exist a large number of languages and facial structures that can be provided as input, each with its own unique character-istics. Furthermore, the mapping from a given input speech to corresponding facial motions is inherently one-to-many due to variations in head poses, emotions, and expressions.
Despite these challenges, the ability to control each of these aspects is vital. For example, a video game character anima-tion application might prefer the generation of exaggerated expressions and head poses, while a newscaster animation application would prefer a neutral expression. In addition, natural motions and high-resolution outputs are desirable to provide the best end-user experience. Unfortunately, no prior framework supports the ability to control emotions, facial landmarks, and poses in a single framework while producing a high-resolution output video, as summarized in Table 1.
In this work, we present SPACE—a method for Speech-driven Portrait Animation with Controllable Expression.
SPACE decomposes the task into several subtasks that al-low for better interpretability and fine-grained controllability.
Given an input speech and a facial image, we first predict facial landmark motions in a normalized space. Operating in the facial landmark space gives us the ability to modify facial features and add actions such as blinking when desired.
Next, we apply the desired head pose to the facial landmarks and transform them into a latent keypoint space used by our pretrained face image generator [39]. This unsupervisedly-learned latent keypoint space has been shown to produce better synthesis quality than using conventional facial land-marks [25, 39, 26]. Finally, we feed these per-frame latent keypoints to our generator and produce an output video at 512×512 resolution. SPACE also introduces emotion conditioning, enabling control over the emotion types and intensities in the generated video.
Even though previous approaches have also followed sim-ilar strategies wherein the audio is mapped to an intermediate representation such as facial landmarks [43] or latent key-points [37, 16], no previous work has utilized both facial landmarks and latent keypoints simultaneously as interme-diate face representations. By using both explicit and latent keypoints, we are able to leverage the interpretability and direct controllability of explicit landmarks while also taking advantage of better motion transfer and image quality ob-tained with latent keypoints and a pretrained generator. The main contributions of our work are as follows:
• We achieve state-of-the-art quality for speech-driven portrait image animation. SPACE provides better quality in terms of FID and landmark distances com-pared to previous methods while also generating higher-resolution output videos.
• Our method can produce realistic head poses while also being able to transfer poses from another video. It also offers increased controllability by utilizing facial land-marks as an intermediate stage, allowing manipulations such as blinking, eye gaze control, etc.
• For the same set of inputs, our method allows for the ma-nipulation of the emotion labels and their corresponding intensities in the output video. 2.