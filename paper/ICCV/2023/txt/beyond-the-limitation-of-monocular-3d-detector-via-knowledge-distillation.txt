Abstract
Knowledge distillation (KD) is a promising approach that facilitates the compact student model to learn dark knowledge from the huge teacher model for better results.
Although KD methods are well explored in the 2D detection task, existing approaches are not suitable for 3D monocular detection without considering spatial cues. Motivated by the potential of depth information, we propose a novel dis-tillation framework that validly improves the performance of the student model without extra depth labels. Specif-ically, we ﬁrst put forward a perspective-induced feature imitation, which utilizes the perspective principle (the far-ther the smaller) to facilitate the student to imitate more features of farther objects from the teacher model. More-over, we construct a depth-guided matrix by the predicted depth gap of teacher and student to facilitate the model to learn more knowledge of farther objects in prediction level distillation. The proposed method is available for ad-vanced monocular detectors with various backbones, which also brings no extra inference time. Extensive experiments on the KITTI and nuScenes benchmarks with diverse set-tings demonstrate that the proposed method outperforms the state-of-the-art KD methods. 1.

Introduction
With the development of autonomous driving, 3D ob-ject detection has become a popular ﬁeld. In general, multi-view-based[39, 20, 15, 14, 26, 27, 21, 42] and LiDAR-based approaches[17, 45] show impressive performance. But con-sidering the massive expensive equipment demand of the above methods, monocular vision methods have attracted more attention in recent years. Moreover, lightweight mod-∗Equal contribution.
†Corresponding author.
Figure 1: The statistics for object depth and area in KITTI.
We set a 3m length in a bin, and the number and area repre-sent the object amount and average pixel area, respectively.
The logarithmic scale is used for area coordinates. els [47, 33] are more favored for practical deployments due to the constraints of hardware resources and the need for real-time. But a lightweight 3D monocular detector usu-ally has a weak feature extraction ability and an inaccuracy depth estimation, which results in a not satisfying perfor-mance.
To tackle the issue, knowledge distillation (KD) [12] is proposed to facilitate the compact student model to learn implicit knowledge from the huge teacher model. KD can indeed improve the performance of the student model without extra inference costs, which motivates researchers to propose better distillation techniques in classiﬁcation
[1, 16, 5, 49, 22, 44] and detection [4, 19, 37, 35, 10, 8] task. Moreover, some 3D distillation methods [7, 13] also adopt the LiDAR data as the input for the teacher model to guide the student. But there is a cost associated with the acquisition of LiDAR data. Therefore, an interesting idea naturally arises: can we only use a vision-based strategy to optimize the student model like 2D distillation approaches? 2D distillation methods perform poorly on 3D tasks ow-posed approach surpasses the previous SOTA methods on different teacher-student pairs with various backbones. Our main contributions can be summarized as follows: 1) We design a perspective matrix that follows the im-plicit depth distribution map to achieve better feature imitation of the distant object. 2) We devise a depth-guided prediction distillation method, which facilitates the student to learn the dis-tant instance prediction from the teacher model. 3) We propose a uniﬁed 3D monocular distillation frame-work purely based on a visual scheme, which intro-duces implicit depth information to the knowledge-transferring process. Extensive experiments on KITTI and nuScenes demonstrate that the proposed method outperforms the previous SOTA distillation methods. 2.