Abstract
Composed Image Retrieval (CIR) aims to retrieve a tar-get image based on a query composed of a reference image and a relative caption that describes the difference between the two images. The high effort and cost required for la-beling datasets for CIR hamper the widespread usage of existing methods, as they rely on supervised learning. In this work, we propose a new task, Zero-Shot CIR (ZS-CIR), that aims to address CIR without requiring a labeled train-ing dataset. Our approach, named zero-Shot composEd imAge Retrieval with textuaL invErsion (SEARLE), maps the visual features of the reference image into a pseudo-word token in CLIP token embedding space and integrates it with the relative caption. To support research on ZS-CIR, we introduce an open-domain benchmarking dataset named Composed Image Retrieval on Common Objects in context (CIRCO), which is the first dataset for CIR con-taining multiple ground truths for each query. The ex-periments show that SEARLE exhibits better performance than the baselines on the two main datasets for CIR tasks,
FashionIQ and CIRR, and on the proposed CIRCO. The dataset, the code and the model are publicly available at https://github.com/miccunifi/SEARLE. 1.

Introduction
Given a query composed of a reference image and a rela-tive caption, Composed Image Retrieval (CIR) [24,35] aims to retrieve target images that are visually similar to the ref-erence one but incorporate the changes specified in the rela-tive caption. The bi-modality of the query provides users with more precise control over the characteristics of the desired image, as some features are more easily described with language, while others can be better expressed visu-ally. Figure 3 shows some query examples.
∗ Equal contribution. Author ordering was determined by coin flip.
Figure 1. Workflow of our method. Top: in the pre-training phase, we generate pseudo-word tokens of unlabeled images with an optimization-based textual inversion and then distill their knowl-edge to a textual inversion network. Bottom: at inference time on
ZS-CIR, we map the reference image to a pseudo-word S∗ and concatenate it with the relative caption. Then, we use CLIP text encoder to perform text-to-image retrieval.
CIR datasets consist of triplets (Ir, Tr, It) composed of a reference image, a relative caption, and a target image, re-spectively. Creating a dataset for CIR is expensive as this type of data is not easily available on the internet, and gener-ating it in an automated way is still very challenging. Thus, researchers must resort to manual labeling efforts. The manual process involves identifying pairs of reference and target images and writing a descriptive caption that captures the differences between them. This is a time-consuming and resource-intensive task, especially when creating large training sets. Current works tackling CIR [2, 3, 11, 22, 24] rely on supervision to learn how to combine the reference image and the relative caption. For instance, [2] proposes a fully-supervised two-stage approach that involves fine-tuning CLIP text encoder and training a combiner network.
While current approaches for CIR have shown promis-ing results, their reliance on expensive manually-annotated datasets limits their scalability and broader use in domains different from that of the datasets used for their training.
To remove the necessity of expensive labeled training data we introduce a new task, Zero-Shot Composed Image
Retrieval (ZS-CIR). In ZS-CIR, the aim is to design an ap-proach that manages to combine the reference image and the relative caption without the need for supervised learning.
To tackle ZS-CIR, we propose an approach named zero-Shot composEd imAge Retrieval with textuaL invErsion (SEARLE) 1 that exploits the frozen pre-trained CLIP [25] vision-language model. Our method reduces CIR to stan-dard text-to-image retrieval by mapping the reference im-age into a learned pseudo-word which is then concatenated with the relative caption. The pseudo-word corresponds to a pseudo-word token residing in CLIP token embed-ding space. We refer to this mapping process with textual inversion, following the terminology introduced in [13].
SEARLE involves pre-training a textual inversion network
ϕ on an unlabeled image-only dataset. The training com-prises two stages: an Optimization-based Textual Inversion (OTI) with a GPT-powered regularization loss to generate a set of pseudo-word tokens, and the distillation of their knowledge to ϕ. After the training, we obtain a network
ϕ that is able to perform textual inversion with a single for-ward pass. At inference time, given a query (Ir, Tr), we use ϕ to predict the pseudo-word associated with Ir and concatenate it to Tr. Then, we leverage the CLIP common embedding space to carry out text-to-image retrieval. Fig-ure 1 illustrates the workflow of the proposed approach.
Most available datasets for Composed Image Retrieval (CIR) focus on specific domains such as fashion [4, 14, 15, 36], birds [12], or synthetic objects [35]. To the best of our knowledge, the CIRR dataset [24] is the only one that con-siders natural images in an open domain. However, CIRR suffers from two main issues. First, the dataset contains several false negatives, which could lead to an inaccurate performance evaluation. Second, the queries often do not consider the visual content of the reference image, mak-ing the task addressable with standard text-to-image tech-niques. Furthermore, existing CIR datasets have only one annotated ground truth image for each query. To address these issues and support research on ZS-CIR, we introduce an open-domain benchmarking dataset named Composed
Image Retrieval on Common Objects in context (CIRCO)2, consisting of validation and test sets based on images from
COCO [23]. Being a benchmarking dataset for ZS-CIR, the need for a large training set is removed, resulting in a 1John Searle is an American philosopher who has studied the philoso-phy of language and how words are used to refer to specific objects. 2CIRCO is pronounced as /Ùirko/. significant reduction in labeling effort. To overcome the single ground truth limitation of existing CIR datasets, we propose a novel strategy that leverages SEARLE to ease the annotation process of multiple ground truths. As a re-sult, CIRCO is the first CIR dataset with multiple annotated ground truths, enabling a more comprehensive evaluation of CIR models. We release only the validation set ground truths of CIRCO and host an evaluation server to allow re-searchers to obtain performance metrics on the test set3.
The experiments show that our approach obtains sub-stantial improvements (up to 7%) compared to the baselines on three different datasets: FashionIQ [36], CIRR [24] and the proposed CIRCO.
Recently, a concurrent work [31] has independently pro-posed the same task as ours. In Sec. 2 and Sec. 3 we provide a detailed comparison illustrating the numerous differences from our approach, while in Sec. 5 we show that our method outperforms this work on all the test datasets.
Our contributions can be summarized as follows:
• We propose a new task, Zero-Shot Composed Image
Retrieval (ZS-CIR), to remove the need for high-effort labeled data for CIR;
• We propose a novel approach, named SEARLE, which employs a textual inversion network to tackle ZS-CIR by mapping images into pseudo-words. It involves two stages: an optimization-based textual inversion using a GPT-powered regularization loss and the training of the textual inversion network with a distillation loss;
• We introduce CIRCO, an open-domain benchmarking dataset for ZS-CIR with multiple annotated ground truths and reduced false negatives. To ease the annota-tion process we propose to leverage SEARLE;
• SEARLE obtains significant improvements over base-lines and competing methods achieving SotA on three different datasets: FashionIQ, CIRR, and the proposed
CIRCO. 2.