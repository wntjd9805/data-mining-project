Abstract
The goal of continual learning (CL) is to continuously learn new tasks without forgetting previously learned old tasks. To alleviate catastrophic forgetting, gradient projec-tion based CL methods require that the gradient updates of new tasks are orthogonal to the subspace spanned by old tasks. This limits the learning process and leads to poor performance on the new task due to the projection constraint being too strong. In this paper, we ﬁrst revisit the gradient projection method from the perspective of ﬂatness of loss surface, and ﬁnd that unﬂatness of the loss surface leads to catastrophic forgetting of the old tasks when the projec-tion constraint is reduced to improve the performance of new tasks. Based on our ﬁndings, we propose a Data Aug-mented Flatness-aware Gradient Projection (DFGP) method to solve the problem, which consists of three modules: data and weight perturbation, ﬂatness-aware optimization, and gradient projection. Speciﬁcally, we ﬁrst perform a ﬂatness-aware perturbation on the task data and current weights to
ﬁnd the case that makes the task loss worst. Next, ﬂatness-aware optimization optimizes both the loss and the ﬂatness of the loss surface on raw and worst-case perturbed data to obtain a ﬂatness-aware gradient. Finally, gradient projec-tion updates the network with the ﬂatness-aware gradient along directions orthogonal to the subspace of the old tasks.
Extensive experiments on four datasets show that our method improves the ﬂatness of loss surface and the performance of new tasks, and achieves state-of-the-art (SOTA) performance in the average accuracy of all tasks. 1.

Introduction
Humans can learn a series of continuously encountered tasks without forgetting previously learned knowledge. In recent years, many researches target for making the neu-ral network achieve continual learning (CL) ability like hu-*Corresponding author.
Figure 1: New tasks’ accuracy (Higher Better) of GPM,
SGD, and MTL on CIFAR-100 dataset, where a signiﬁcant performance gap exists between GPM and SGD/MTL. mans [38, 47]. One of the main challenge of CL is to mitigate the catastrophic forgetting [40, 17, 53] of the knowledge of previous tasks when learning new tasks [51].
To alleviate catastrophic forgetting of old tasks, several works [57, 15, 8, 27, 43] propose to constrain the gradient update direction of the new tasks. The new tasks only update the network along the orthogonal direction to the gradient subspaces deemed for the old tasks. Compared to other methods, the recently proposed Gradient Projection Memory (GPM) [43] shows better performance in CL. However, we observe poor performance for the new tasks in GPM. As shown in Fig. 1, by comparing vanilla SGD (which learns new tasks without any gradient constraints) or MTL (which learns all tasks simultaneously and can be seen as an upper bound for CL) with GPM, we ﬁnd that the performance of new tasks with GPM has a large gap compared with SGD and
MTL. When learning the 10-th task T 10, MTL and SGD can achieve 85.40% and 83.00% accuracy, respectively, while
GPM can only achieve 74.50%. This vast gap has prompted us to explore the reasons behind it and devise effective ways to address this issue.
In this paper, we ﬁrst revisit the ‘stability-plasticity’ dilemma [33] in GPM from the perspective of the ﬂatness of loss surface (we provide the formal deﬁnition of ﬂatness in the appendix). We ﬁnd that the projection threshold in
GPM is a key factor in improving the performance of new
(cid:11)(cid:68)(cid:12)(cid:3) (cid:11)(cid:69)(cid:12)(cid:3) (cid:41)(cid:20)(cid:15)(cid:21) (cid:58) (cid:58) (cid:58)(cid:58) (cid:58)(cid:20) (cid:58)(cid:21) (cid:41)(cid:22)(cid:15)(cid:23) (cid:58) (cid:58) (cid:58) (cid:58)(cid:22) (cid:58)(cid:23)
Figure 2: An illustration of (a) a sharp loss surface and (b) a
ﬂat loss surface. The blue curve represents the loss surface of task T , and the length of the blue arrow (ΔW ) represents the amount of parameter updates for the new task T + 1. tasks, as shown in Fig. 3. Speciﬁcally, reducing the projec-tion threshold can improve the performance of new tasks.
However, it leads to catastrophic forgetting of old tasks and a drop in average performance across all tasks. We suspect that catastrophic forgetting is because the network’s loss surface is not ﬂat enough, which is deﬁned as regions on loss surfaces where the loss changes slowly with the model parameters [20]. Fig. 2 gives an example, under the same amount of update ΔW , when the minimum W 3 of a ﬂat loss surface and the minimum W 1 of a sharp loss surface is moved to W 4 and W 2, respectively, the loss change F3,4 of the former is much smaller than the loss change F1,2 of the latter. In other words, a ﬂat loss surface effectively reduces the degree of catastrophic forgetting. We verify in Fig. 4 that catastrophic forgetting of old tasks at a low projection threshold in GPM is indeed due to unﬂatness loss surfaces.
This inspires us to realize that when the loss surface is ﬂat, we can reduce the projection threshold in GPM to improve the performance of new tasks, while simultaneously ensuring that old tasks are not catastrophically forgotten.
To achieve this goal, we propose a novel Data Augmented
Flatness-aware Gradient Projection (DFGP) method whose critical insight is to encourage a ﬂat loss surface for the CL model. DFGP consists of three modules: data and weight perturbation, ﬂatness-aware optimization, and gradient pro-jection. Speciﬁcally, DFGP ﬁrst performs a ﬂatness-aware mixup with raw training data interpolation to cover potential unseen regions. Data augmentation could be considered a worst-case perturbation w.r.t. training data, which makes the model more robust against distribution shifts (which is the greatest characteristic of CL tasks) and provides a wider exploration (data) space for the subsequent ﬂatness-aware op-timization. At the same time, we perturb the current weight in a ﬁxed-radius neighborhood to explore where the loss function is worst-case w.r.t. the weight. Then, under the original training data and the worst-case perturbed data, the loss and the ﬂatness of loss surface are optimized to obtain a
ﬂatness-aware gradient. Finally, DFGP uses the orthogonal projection to update the network parameters. Since the loss surface of old tasks in DFGP is ﬂat, DFGP allows new tasks to update the network under relatively smaller projection threshold constraints while keeping the forgetting degree of old tasks close to GPM, greatly improving the performance of new tasks and the overall performance of all tasks.
Empirically, on the four widely used CL datasets, PM-NIST, CIFAR-100, 5-Datasets and MiniImageNet, DFGP improves the accuracy by 1.46%, 2.28%, 0.97%, and 8.93%, respectively, compared with GPM. This is signiﬁcant for these benchmark datasets.
It is worth mentioning that a byproduct of our DFGP is that it is more robust to adversarial examples than GPM, since the perturbation of training data in DFGP increases the chance of covering noisy samples.
In addition, our proposed data-augmented ﬂatness-aware optimization can be effortlessly combined with other CL methods (such as regularization-based and memory-based methods) to further boost their performance.
The main contributions can be summarized as four-fold:
• We revisit the GPM approach from the perspective of loss surface ﬂatness, and ﬁnd that when we improve the performance of new tasks, catastrophic forgetting of old tasks occurs due to the loss surface unﬂatness.
• We propose a data augmented ﬂatness-aware gradient projection (DFGP) method for CL, which simultane-ously optimizes the loss and the ﬂatness of the loss surface from both the data and weight levels.
• We compare the proposed DFGP with multiple CL methods on four widely used benchmark datasets and verify that DFGP improves the performance of new tasks and the average performance of all tasks.
• We demonstrate that DFGP is more robust to adversarial attacks relative to GPM, and our optimization strategy can be effortlessly combined with other CL methods to further improve their performance. 2. Rethinking the GPM Method
In this section, we ﬁrst introduce the problem setup of CL and the Gradient Projection Memory (GPM) method, and then revisit the poor performance of the GPM method on the new arriving task, i.e., the plasticity of GPM. 2.1. Problem Setup
CL [38, 47] is trained with sequential arriving tasks
{D(1), . . . , D(T )}, where D(t) = {(X (t), Y (t))} is the train-ing data of task t. When learning the t-th task, we only have access to the data D(t) for the t-th task. CL expects that a neural network with L layers f (W , ·) to be capable of learning these sequentially encountered tasks and not for-getting previously learned tasks. The weight of the network l=1} and W l is the weight of is denoted as W = {(W l)L l-th layer. Given an input data x(t) i ∈ X (t), denote the in-put and output of the l-th layer of the network as x(t),l and x(t),l+1
= f (W l, x(t),l is denoted i as the representations of x(t) i of task t at layer l. CL expects to minimize the loss of all tasks. According to the Empirical
Risk Minimization (ERM) principle [49], we can obtain the
), respectively, where x(t),l i i i
objective of CL as follows: min
W 1
T
T(cid:2) 1
|D(t)| (cid:2) (cid:3)
Lt f (W , x(t) i ), y(t) i (cid:4)
, t=1 (x(t) i
,y(t) i
)∈D(t) where Lt(·) represents the loss function of the t-th task, e.g., mean squared or cross-entropy loss. 2.2. GPM Method (cid:5) (x(t) i 1
|D(t)| i ), y(t)
)∈D(t) Lt(f (W , x(t) the optimization objective
Since CL can only use the data of task t when learning t-th task, is: i ). To avoid min
,y(t)
W i catastrophic forgetting of old tasks, the weight {(W l)L l=1} on task t is updated along the direction orthogonal to the subspace Sl spanned by the previous t − 1 tasks in gradient projection based CL methods [15, 57, 27, 43]. In particular,
GPM [43] has achieved remarkable results compared to other approaches. For task t, the update rule of l-th layer (l ∈ {1, 2, . . . , L}) weight W l in GPM is as follows: (t) (t)
W l = W l − η · (g
W l = ∇Lt(f (W l, x(t)
W l − ProjSl (g
W l )), i ) represents the gradient (1) i (t) i ), y(t) where g
, y(t) of the training sample (x(t)
) with respect to the weight i
W l, η is the step size, and ProjSl (·) is a projection operator (t) that projects g
W l into Core Gradient Space (CGS), i.e., sub-space Sl spanned by old tasks. The core operation of GPM is to ﬁnd and store the basis of the old task’s CGS. Speciﬁcally, when task t is learned, it ﬁrst randomly samples n samples to obtain the representation matrix R(t),l = [x(t),l
, . . . , x(t),l n ] of l-th layer of the CL network. Then, it performs SVD [11] on R(t),l = U (t),lΣ(t),lV (t),l and approximates its k-rank according to the following projection criteria for
R a given threshold (cid:3)l th (we call it projection threshold):
F ≥ (cid:3)l (cid:4)2
F . Next, GPM deﬁnes the feature (cid:4)R vectors corresponding to the top-k maximum singular values as the basis of the l-th layer, and constitutes bases M l of the space Sl by them: M l = span{u(t),l
}. Finally, the calculation rule of the projection operation is as follows:
ProjSl (g(t),l) =M l(M l)(cid:3)g(t),l. th(cid:4)R(t),l(cid:4)2
, . . . , u(t),l (t),l k (t),l k k 1 1 2.3. Rethinking the Plasticity in GPM
Regretfully, GPM still performs poorly on new tasks (see
Fig. 1), limiting its overall performance. Below, we try to explore what limits the performance of GPM.
Which factor inﬂuences the performance of new tasks in
GPM? There exist distribution drifts between the tasks en-countered continuously in CL. When the constraints imposed by new tasks in CL are too strong, it causes the network to fail to ﬁt the distribution of the new tasks, resulting in poor performance. In particular, the factor that determines the strength of constraints in GPM is the projection threshold (cid:3)l th. We show in Fig. 3 (red line) that a relatively lower projection threshold can achieve better performance on new tasks. Speciﬁcally, when we reduce the projection threshold
Figure 3: The accuracy of an old task (T 1, blue line), a new task (T 10, red line) and the average accuracy of all tasks (purple line) under different projection threshold (cid:3)l th of GPM method on CIFAR-100. At,i represents the accuracy of the model tested on task i after the trained task t. from 0.99 to 0.80, the performance A10,10 of the new task
T 10 improves from 73.30% to 82.16%.
What are the problems with increasing the performance of new tasks in GPM? Although a relatively small thresh-old leads to better performance on new tasks, it also leads to catastrophic forgetting of old tasks, resulting in overall performance degradation. Speciﬁcally, as shown in Fig. 3 (brown line), task T 1 can achieve 76.82% accuracy when it is learned. However, after learning task T 10, the accuracy
A10,1 (blue line) of task T 1 drops to 76.28%, 57.00% at two projection thresholds of 0.99 to 0.80.
Is it possible to improve the performance of new tasks without catastrophic forgetting of old tasks? According to previous analysis for GPM, when we want to achieve high performance for new tasks by setting a relatively small projection threshold, we need to ensure that old tasks are not catastrophically forgotten. A ﬂat loss surface [20, 16, 22] seems to offer a chance that old tasks are not catastrophically forgotten. An example is given in Fig. 2, after task T is learned, the weight is W 1 (a minimum of sharp loss surface), and then the updated amount of new task T +1 is ΔW , and the weight of the network becomes W 2. When we use
W 2 to predict the old task T , the loss for task T increases signiﬁcantly (i.e., a large F1,2). However, if the loss surface is ﬂat enough, that is, task T converges to W 3 (a minimum of ﬂat loss surface), and task T + 1 updated to W 4, the loss of old task T does not change signiﬁcantly (that is,
F3,4 << F1,2), i.e., old tasks are forgotten less.
Is the loss surface not ﬂat in GPM? We are curious about whether the catastrophic forgetting of GPM at a small pro-jection threshold is due to the loss surface is not ﬂat. To answer this question, we quantitatively analyze the ﬂatness of the loss surface in GPM in Fig. 4. Speciﬁcally, we use the maximum eigenvalue of the Hessian matrix w.r.t the weight W to measure the ﬂatness of a loss surface, and a large eigenvalue means a sharp loss surface (The deﬁnition and measurement of ﬂatness are explained in the appendix).
We can observe that when the projection threshold (cid:3)l th de-perturbation at the weight level in Sec. 3.2(2). The ﬁrst row in Eq. 2 represents the loss minimization on the raw data (X, Y ) and the worst-case perturbed data ( ˜X, ˜Y ), the sec-ond and third rows represent the ﬂatness of loss surface on the original data and perturbed data. The goal of both data perturbation and weight perturbation is to ﬁnd the worst-case of the network for optimization to obtain a ﬂatness loss surface in Sec. 3.2(3). Compared with SAM [16], our method further performs data-level perturbation in addition to weight-level perturbation, which can better adapt to distri-bution drift between tasks and ﬂatness-aware optimization on a wider exploration (data) space.
The algorithm for solving DFGP is summarized in Alg. 1.
Below, we separately describe the solution process of data perturbation and weight perturbation to compute the ﬂatness-aware gradient, respectively. 3.2. Data Augmented Flatness-aware Optimization j and x(t)
Our DFGP in Eq. 2 collaboratively improves the ﬂat-ness of the loss surface by simultaneously disturbing the data and weight. Speciﬁcally, the goal of data perturba-tion is to ﬁnd the worst-case perturbation ˆγ that maximizes the loss Lt(W , ˜X(ˆγ), ˜Y (ˆγ)) for task t. The perturbed data ( ˜X(ˆγ), ˜Y (ˆγ)) are constructed by interpolating the raw data (X, Y ) according to ˆγ. The goal of weight perturbation is to ﬁnd a worst-case δ in the neighborhood of the weight W , which maximizes the loss of task t. Afterward, DFGP opti-mizes the task loss and loss surface on the worst-case data and weight perturbations to obtain a ﬂatness-aware gradient. (1) Data perturbation. Inspired by Mixup [59, 60], we perturbed the data and extended the raw training data dis-tribution. Speciﬁcally, we ﬁrst interpolate any two samples from X (and the corresponding labels y(t) x(t) i and y(t) form Y ) in a minibatch from task t to generate a perturbed training example (˜x(t)(γ), ˜y(t)(γ)): ˜x(t)(γ) =
γx(t)
, where
γ ∈ [0, 1] represents the weight when the two samples are mixed. However, unlike the vanilla Mixup, which
γ ∼ Beta(α, α) is directly sampled from a Beta distribu-tion, we perform a further optimization on γ whose goal is to maximize the loss corresponding to the samples after in-terpolation to promote the ﬂatness-aware optimization of the network. In other words, we will add a perturbation (cid:3) to the sampled γ in the neighborhood of radius ρ, so that the loss is worst-case on the new image mixed with the perturbed
ˆγ(ˆγ := γ + (cid:3)). In addition, to ensure that the interpolated samples still belong to the class corresponding to y(t) and y(t)
, we clip ˆγ to be between [0, 1]. The formal expression j of our gamma’s (i.e., ˆγ) goal is as follows: j , ˜y(t)(γ) = γy(t) i + (1 − γ)x(t) i + (1 − γ)y(t) j j i i (cid:4) (cid:3) max (ˆγ:=γ+(cid:4))∈[0,1],(cid:4)(cid:3)(cid:4)2≤ρ
Lt
W , ˜X(γ + (cid:3)), ˜Y (γ + (cid:3))
. (3)
However, it is very difﬁcult to exactly ﬁnd the solution of
Figure 4: The maximum eigenvalue of task T 1 in the GPM method under different projection thresholds on CIFAR-100. creases, the loss surface of Task T 1 becomes sharper, that is, the maximum eigenvalue increases. For example, when the projection threshold is 0.80 and 0.99 respectively, the maximum eigenvalue of the former is much higher than that of the latter, that is, 1878.11 >> 346.52. Therefore, catas-trophic forgetting is more likely to occur at a relatively small projection threshold due to a sharp loss surface in GPM.
Inspired by the above observations, we propose a novel gradient projection method for CL, namely data augmented
ﬂatness-aware gradient projection (DFGP). DFGP avoids catastrophic forgetting of old tasks by improving the ﬂatness of the loss surface, so it allows a relatively small projection threshold to improve the performance of new tasks. 3. Methodology
In this section, we will ﬁrst introduce the proposed DFGP method in Sec. 3.1 and then describe its components in detail in Sec. 3.2 and Sec. 3.3. 3.1. DFGP Formulation
W
Since GPM only minimizes empirical risk when learning
Lt(W , X, Y ), where (X, Y ) is training task t, i.e., min data of task t (we omit the superscript for brevity), and does not consider any ﬂatness-aware optimization objective, the loss surface in GPM is generally sharp. In this paper, inspired by sharpness-aware minimization (SAM) [16, 28, 48, 34], we optimize both the loss and the ﬂatness of loss surface. The key of our method is to ﬁnd the sharpest case of the loss surface from two levels of data and parameters to optimize to obtain a ﬂat loss surface. Speciﬁcally, our proposed data augmented ﬂatness-aware gradient projection (DFGP) overall optimization objective is as follows: (cid:6) min
W
Lt (W , X, Y ) + λLt (cid:7) (cid:8)
+ max (cid:4)δ(cid:4)2≤ρ (cid:3)
W , ˜X, ˜Y (cid:4)
Lt (W + δ, X, Y ) − Lt (W , X, Y ) (cid:9) (2) (cid:8)
+ λ
Lt(W + δ, ˜X, ˜Y ) − Lt(W , ˜X, ˜Y ) (cid:9)(cid:10)(cid:11)
, where ( ˜X, ˜Y ) represents the worst-case perturbation at the data level in Sec. 3.2(1), and W +δ represents the worst-case
Algorithm 1 Algorithm of our DFGP 1: Function Train (W , Dtrain η, (cid:3)th, ε, ρ, λ, α) 2: Initialize: M ← {(M l)L l=1}, M l ← [ ], for all l = 1, 2, . . . , L train, (cid:3)th, t) n , W , ρ, ε, λ, α) 3: Initialize: W ← W 0 4: for task t = 1, 2, . . . , T do 5: 6: (t),l (t),l train (t),l
W l end for
W l , M l) until convergence
W l − Proj(g repeat
B(t) n ← (X (t), Y (t)) ∼ Dt
W ← FlatnessAwareGradient (B(t) (t) g for layer l = 1, 2, . . . , L do if t > 1 then (t),l
W l ← g g end if
W l ← W l − η · g 7: 8: 9: 10: 11: 12: 13: 14: 15: M ← UpdateGradientMemory (W , M , Dt 16: end for 17: return W 18: end Function 19: 20: Procedure FlatnessAwareGradient (Bn, W , ρ, ε, λ, α) n ← ( ˜X, ˜Y ) ← Mixup(Bn, γ), γ(cid:6) ← Beta(α, α) 21: B(cid:6) 22: ˆ(cid:3) ← ρ ·
ˆδ ← ρ ·
∇W Lt(Bn)+λ∇W Lt(B(cid:2) (cid:4)∇W Lt(Bn)+λ∇W Lt(B(cid:2)
∇γ Lt(B(cid:2) (cid:4)∇γ Lt(B(cid:2) n) n)(cid:4)+ε n) n)(cid:4)+ε (t)
W = {(g (t)
W , where g n))| ˆW (t)
W l )L l=1} 23: 24: ˆγ ← Clamp(γ + ˆ(cid:3), 0, 1), ˆW ← W + ˆδ n ← ( ˜X, ˜Y ) ← Mixup(Bn, ˆγ) 25: B(cid:6)(cid:6) (t)
W ← ∇W (Lt(Bn) + λLt(B(cid:6)(cid:6) 26: g 27: return g 28: end Procedure 29: 30: Procedure UpdateGradientMemory (W , M , Dt 31: B(t) 32: R(t) ← forward(B(t) 33: for layer l = 1, 2, . . . , L do 34: 35: n ← (X (t), Y (t)) ∼ Dt
ˆR if t > 1 then (t),l
= R(t),l train (t),l
= R(t),l − ProjSl (R(t),l) n , W ), where R(t) = {(R(t),l)L l=1} train, (cid:3)th, t) 36: 37: 38:
ˆR end if (t),l
ˆU k ← criteria( ˆR 39: 40: M l ← [M l, ˆU 41: end for 42: return M 43: end Procedure
← SVD( ˆR (t),l (t),l
) (t),l
, R(t),l, (cid:3)l
[0 : k]] th)
Eq. 3, so the optimization goal of (cid:3) is the following ﬁrst-order Taylor approximate problem, and the solution ˆ(cid:3) is: (cid:4) (cid:3) (cid:3) (cid:4)
ˆ(cid:3) ≈arg max (cid:4)(cid:3)(cid:4)2≤ρ
=ρ · ∇γLt
Lt (cid:3)
W , ˜X, ˜Y (cid:4)
+ (cid:4) (cid:3)(∇γLt (cid:3)
W , ˜X, ˜Y
/(cid:4)∇γLt
W , ˜X, ˜Y
W , ˜X, ˜Y (cid:4) (cid:4)2 (4) (2) Weight perturbation. At the same time, we perform weight perturbation on raw data (X, Y ) and Mixup data ( ˜X(γ), ˜Y (γ)) for task t. Note that the reason the weight perturbation is also performed on the augmented data is to allow the weight perturbation to explore in a wider data space. Speciﬁcally, the objective of the weight perturbation is to ﬁnd the δ that causes the worst-case loss of task t in the neighborhood with W as the center and ρ as the radius, i.e., (cid:3) (cid:4) max (cid:4)δ(cid:4)2≤ρ
Lt (W + δ, X, Y ) + λLt
W + δ, ˜X, ˜Y
. (5)
We obtain the solution of ˆδ by solving the ﬁrst-order Taylor approximation problem of problem Eq. 5 as follows:
ˆδ ≈ arg max (cid:4)δ(cid:4)2≤ρ (cid:3)
Lt (W , X, Y ) + λLt(W , ˜X, ˜Y ) (cid:3)
+ δ
= ρ ·
∇W Lt (W , X, Y ) + λ∇W Lt(W , ˜X, ˜Y )
∇W Lt (W , X, Y ) + λ∇W Lt(W , ˜X, ˜Y ) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)∇W Lt (W , X, Y ) + λ∇W Lt(W , ˜X, ˜Y ) (cid:12) (cid:4) (6)
. 2
It should be mentioned that, observing the forms of the two optimization problems of Eq. 6 and Eq. 4, we can ﬁnd that the optimal weight perturbation ˆδ and data perturbation ˆ(cid:3) can be computed together through a single backpropagation, as opposed to requiring separate backpropagations. (3) Flatness-aware gradient optimization. The ﬂatness-aware optimization problem is to optimize the loss in Eq. 2 on the worst-case data and weight perturbation. When Eq. 2 is minimized, the model converges to a minimum W ∗ of a ﬂat loss surface. Bringing the worst-case perturbed data ( ˜X(ˆγ), ˜Y (ˆγ)) and weight ˆW into Eq. 2 and removing re-dundant terms, our ﬂatness-aware minimization objective function is: (cid:3) (cid:4) (cid:3) (cid:4)
ˆW , X, Y
+ λLt
ˆW , ˜X, ˜Y
, (7)
Lt min
W where ˆγ = γ+ˆ(cid:3), ˆW = W +ˆδ; ˆ(cid:3) and ˆδ are from Eq. 4 and Eq. 6, respectively. Therefore, we compute the ﬁnal ﬂatness-aware gradient approximation to update parameters W as: (cid:13) (cid:13) (cid:13)
W ≈ ∇W (Lt (W , X, Y )+λLt(W , ˜X, ˜Y )) (8) (t) g
.
ˆW (t)
So far, we have obtained a ﬂatness-aware gradient (i.e., g
W in Eq. 8) with respect to W that optimizes both loss and
ﬂatness of loss surface for task t. 3.3. Flatness-aware Gradient Projection
In this subsection, we describe how to combine the gradi-ent of data augumented ﬂatness-aware optimization with the orthogonal gradient projection method in CL.
Learning task 1. The learning process of task 1 includes gradient update and projected memory update. 1) Weight update: The ﬁrst task in CL is learned without any con-straints. Therefore, after obtaining the gradient in Eq. 8 (see line 7 in Alg. 1), we directly perform the following gradient descent for parameter update of layer l (see line 12 in Alg. 1): W l = W l − η · g
W l , l ∈ {1, . . . , L}, where (t),l
1 1 k n (t)
, . . . , x(1),l
, . . . , u(1),l
η is the step size. 2) Memory update: After the task 1 has converged, we need to calculate the core gradient space (CGS) of task 1 to construct the projection matrix M . We
ﬁrst select randomly n samples to obtain the representation
R(1),l = [x(1),l
] of each layer l ∈ {1, . . . , L} (see line 32 in Alg. 1), and then perform SVD decomposition on this representation R(1),l = U (1),lΣ(1),lV (1),l to select the top-k most important basis for task 1 (see lines 38-39 in
Alg. 1). Finally, we take the eigenvectors corresponding to the top-k maximum eigenvalues as the core gradient space of task 1, that is, M l = span{u(1),l
} (see line 40 in
Alg. 1). When task 2 updates the parameters, it needs to be orthogonal to this core gradient space, which can effectively reduce the interference to task 1.
Learning task 2 to T. The learning process of task 2 ∼
T includes gradient projection, gradient update, and pro-jected memory update. 1) Gradient Projection: Similar to task 1, we use Eq. 8 to compute the ﬂatness-aware
W for task t (see line 7 in Alg. 1). But in-gradient g stead of using the gradient directly for updating, the gra-dient components parallel to task t − 1 are eliminated (t),l
W l ), and only the gradi-(i.e., ProjSl (g ent components orthogonal to task t − 1 are retained (i.e., (t),l
W l −ProjSl (g
W l )). 2) Weight update: We update the param-g eters with the remaining gradient (see lines 9-12 in Alg. 1), (t),l (t),l that is: W l = W l − η · (g
W l )). 3) Mem-ory update: Since the space of previous t − 1 tasks may contain signiﬁcant gradient directions of the task t, when building the core gradient space of task t, only the impor-tant basis additionally included in task t is added to the gradient memory M l. Speﬁcically, we ﬁrst project the rep-resentation R(t),l = [x(t),l
, . . . , x(t),l n ] of task t for each layer l into the gradient space formed by previous tasks to remove duplicate representations (see line 36 in Alg. 1): ˆR
=
R(t),l − ProjSl (R(t),l) = R(t),l − M l(M l)(cid:3)R(t),l. Then we perform SVD decomposition ( ˆR
)) on this part of the representation and require that the fol-lowing formula be satisﬁed (see lines 38-39 in Alg. 1): (cid:4)Proj(R(t),l)(cid:4)2
F ≥ (cid:3)l
F . Finally, we add the eigenvectors corresponding to the maximum top-k eigenvalues of task t into the gradient memory, i.e.,
M l ←
W l ) = M l(M l)(cid:3)g (see line 40 in Alg. 1).
W l − ProjSl (g th(cid:4)R(t),l(cid:4)2
F + (cid:4)( ˆR
M l, ˆU (t),l ˆΣ
[0 : k]
= ˆU (t),l k
)(cid:4)2 ( ˆV (t),l (t),l (t),l (t),l (t),l (t),l (t),l (cid:14) (cid:15) 1 4. Experiments
In this section, we conduct extensive experiments to demonstrate the effectiveness of DGFP. The implementation details and additional experiments are provided Appendix. 4.1. Experimental Setup
Datasets We evaluate our method on four benchmark datasets for CL [43, 27]: Permuted MNIST, 10-Split CIFAR-100, 5-Datasets and 20-Spilt miniImageNet.
Figure 5: New tasks’ accuracy of GPM, SGD, MTL, and
DFGP on CIFAR-100.
Baselines. We compare DFGP with four types of CL meth-ods. Architecture-based method: HAT [44]. Regularization-based methods: EWC [23], MAS [1]. Memory-based meth-ods: ER [10], A-GEM [9]. Orthogonal-Projection-based methods: OWM [57], GPM [43], FS-DGPM [12]. We also combine GPM with Classiﬁer-Projection Regulariza-tion(CPR) [6] as a strong baseline (GPM+CPR).
Evaluation metrics. We evaluate the performance by aver-age accuracy (ACC) and backward transfer (BWT) [43].
Speciﬁcally, ACC represents the average test accuracy of the model trained on all tasks. BWT measures the forgetting of old tasks. ACC and BWT are deﬁned as:
ACC = 1 i=1 AT,i − Ai,i, where
T
At,i is the accuracy of the model tested on task i after the training of task t is completed. T is the number of tasks. i=1AT,i, BWT = 1
T −1 (cid:2)T −1 (cid:2)T 4.2. Experimental Results
Performance. As shown in Tab. 1, the accuracy of DFGP is signiﬁcantly improved over previous work on all datasets.
For example, DFGP achieves the accuracy gains of 1.07%, 1.01%, 0.77%, and 7.78% on the four datasets PMNIST,
CIFAR-100, 5-Datasets, and MiniImageNet, respectively, compared to the best baseline methods GPM+CPR, FS-DGPM, HAT, and GPM+CPR. It should be mentioned that these datasets are widely studied benchmark datasets in CL.
Therefore, the improvement of our method on these datasets is already signiﬁcant. In addition, there is a clear trend that
DFGP can achieve more signiﬁcant beneﬁts compared with other methods with a larger number of tasks. For exam-ple, the number of tasks contained in PMNIST, CIFAR-100, 5-Datasets, and MiniImageNet are 10, 10, 5, and 20, respec-tively, so the accuracy gain shows a trend of MiniImageNet
> PMNIST, CIFAR-100 > 5-Datasets. This is because old tasks are more likely to be forgotten when the num-ber of tasks increases. DFGP method obviously alleviates the forgetting problem, and thus achieves a more signiﬁ-cant improvement on the dataset with a larger number of tasks. Speciﬁcally, DFGP shows the lowest forgetting degree (BWT in Tab. 1) compared to other competitive methods.
Stability-Plasticity analysis. As shown in Fig. 6(a-b), when we reduce (cid:3)l th (i.e., our DFGP is (cid:3)l th = 0.95, GPM is (cid:3)l th = 0.97), DFGP achieves comparable or even better
Table 1: The averaged accuracy (ACC) and backward transfer (BWT) over all tasks on different datasets. Note that, MTL learns all tasks simultaneously in a single network by using the entire dataset, which does not belong to the setting of CL, it can be used as an upper bound for CL learning. SGD means that new tasks are learned without any constraints.
Method
PMNIST (10 Tasks)
CIFAR-100 (10 Tasks) 5-Datasets (5 Tasks)
MiniImageNet (20 Tasks)
SGD
MTL
ACC(%) 53.56±2.89 96.70±0.02 89.97±0.57
EWC 86.98±0.84
MAS
-HAT 83.56±0.16
A-GEM 87.24±0.53
ER 90.71±0.11
OWM 93.18±0.14
GPM
FS-DGPM 92.89±0.18
GPM+CPR 93.57±0.15 94.64±0.17
DFGP
BWT
-0.48±0.03
-0.00±0.00
-0.04±0.01
-0.04±0.01
--0.14±0.00
-0.11±0.01
-0.01±0.00
-0.03±0.00
-0.36±0.01
-0.03±0.00
-0.01±0.00
ACC(%) 56.39±1.14 80.67±0.42 68.80±0.88 67.96±0.44 72.06±0.50 63.98±1.22 71.73±0.63 50.94±0.60 72.31±0.20 73.58±0.14 72.21±0.43 74.59±0.33
BWT
-0.22±0.01
-0.00±0.00
-0.02±0.01
-0.05±0.00
-0.00±0.00
-0.15±0.02
-0.06±0.01
-0.30±0.01
-0.00±0.00
-0.30±0.00
-0.00±0.00
-0.00±0.00
ACC(%) 80.23±1.16 93.15±0.16 88.64±0.26 88.73±0.79 91.32±0.18 84.04±0.33 88.31±0.22
-91.12±0.00
-89.72±0.48 92.09±0.18
BWT
-0.16±0.01
-0.00±0.00
-0.04±0.01
-0.04±0.01
-0.01±0.00
-0.12±0.01
-0.04±0.00
--0.01±0.00
--0.01±0.00
-0.01±0.00
ACC(%) 49.71±1.80 84.16±1.26 52.01±2.53 60.80±2.96 59.78±0.57 57.24±0.72 58.94±0.85
-60.99±2.01
-62.14±1.89 69.92±0.90
BWT
-0.21±0.01
-0.00±0.00
-0.12±0.03
-0.09±0.02
-0.03±0.00
-0.12±0.01
-0.07±0.01
--0.05±0.01
--0.04±0.01
-0.01±0.00
Table 2: Effectiveness of each component. th = 0.97; (b) Acc of our DFGP: best (cid:3)l
Figure 6: The accuracy (Higher Better) and the maximum eigenvalue (Lower Better) on the CIFAR-100 dataset. (a) Acc of
GPM: best (cid:3)l th = 0.95; (c) Maximum eigenvalue of GPM; (d) Maximum eigenvalue of our DFGP. t-th row represents the accuracy of the network tested on tasks 1−t after task t is learned. than GPM on the old tasks. For example, when task T 10 is learned, the accuracy of GPM on tasks T 2, T 4, and T 6 is 67.9%, 69.5%, and 72.4%, respectively. DFGP is 68.9%, 71.8%, and 75.2%, respectively. These evidences suggest that our method ﬂattening the loss surface really helps alle-viate catastrophic forgetting. In addition, we highlight the improved performance that our method brings to new tasks.
As shown in Fig. 5, GPM can only achieve the accuracy of 69.66% and 74.50% when learning new task T 8 and new task T 10, while DFGP achieves the accuracy of 74.90% and 79.30% respectively. In conclusion, our method greatly im-proves the performance of new tasks while maintaining that old tasks are not catastrophically forgotten.
CIFAR-100
PMNIST
ACC(%)
ACC(%)
Method
BWT
BWT
-0.00
-0.01
-0.02
-0.02
-0.00 72.31 73.46 73.19 72.94 74.59 93.18 94.49 94.04 93.81 94.64
GPM w/ W(ˆδ) w/ D(ˆγ) w/ D(γ)
DFGP
-0.03
-0.02
-0.03
-0.03
-0.01 with vanilla Mixup (w/ D(γ)), the accuracy will be further reduced. For example, on PMNIST(CIFAR-100), it has dropped from 94.04%(73.19%) to 93.81%(72.94%).
Flatness visualization. We verify that DFGP shows a ﬂatter loss surface compared to GPM. We measure the ﬂatness of loss surface via the maximum eigenvalue of the network’s
Hessian matrix in Fig. 6(c-d). The maximum eigenvalue corresponding to DFGP is much smaller than that of GPM, e.g., the maximum eigenvalues of the former and the latter in task T 10 are 61.8 and 308.3, respectively.
Combined with other CL methods. We further combine
Data augmented Flatness-aware optimization (abbreviated as DF) with other three kinds of CL algorithms, including orthogonal projection based (TRGP [27]), memory-based (ER [10]), and regularization-based (MAS [1]) methods. 4.3. Ablation Study
Effectiveness of each component. Compared with GPM,
DFGP perturbs both data and weight to perform ﬂatness-aware optimization. As shown in Tab. 2, when we perform perturbation on only one level, it suffers from a certain accu-racy drop, proving the effectiveness of both components in
DFGP. For example, when using only weight perturbation (w/ W(ˆδ)) or data perturbation (w/ D(ˆγ)) on CIFAR-100, the ACC drops from 74.59% to 73.46% and 73.19%. After we replace the ﬂatness-aware data perturbation (w/ D(ˆγ))
Table 3: Data augmented ﬂatness-aware optimization on more CL methods for accuracy and BWT testing.
Method
PMNIST
CIFAR-100
ACC(%) BWT
ACC(%) BWT
TRGP
DF-TRGP (ours)
ER
DF-ER (ours)
MAS
DF-MAS (ours) 96.34 96.90 87.24 89.99 86.98 88.90
-0.01
-0.01
-0.11
-0.08
-0.04
-0.03 72.67 73.63 71.73 73.62 67.96 70.44
-0.19
-0.01
-0.06
-0.07
-0.05
-0.02
Table 4: The averaged accuracy (ACC) of GPM and DFGP when tested on adversarial examples (X adv, Y ).
PMNIST
CIFAR-100
GPM
DFGP
GPM
DFGP
μ=0.0 93.56(-0.00) 94.64(-0.00) 72.31(-0.00) 74.59(-0.00)
μ=0.0001 93.51(-0.05) 94.60(-0.04) 71.91(-0.40) 74.50(-0.09)
μ=0.001 93.07(-0.49) 94.31(-0.34) 71.43(-0.88) 73.92(-0.67)
μ=0.01 87.81(-5.75) 90.80(-4.05) 66.33(-5.97) 68.91(-5.68)
Speciﬁcally, we use the ﬂatness-aware gradient proposed in this paper instead of the vanilla gradient for parameter updates of three approaches. As shown in Tab. 3, on the PM-NIST, DF-ER and DF-MAS achieve absolute accuracy gains of 2.75% and 1.92% over ER and MAS, respectively. On the CIFAR-100, DF-TRGP, DF-ER and DF-MAS achieve absolute accuracy gains of 0.96%, 1.89% and 2.48% over
TRGP, ER and MAS, respectively.
Robustness analysis. We verify that DFGP is more ad-versarial robust than GPM. To test the robustness, we refer to the popular FGSM [18] method for adversarial pertur-bation of the input image, and the detailed attack rule is:
, where μ is a hy-X adv = X + μ · sign perparameter of adversarial strength, and X represents the samples of the test set. As shown in Tab. 4, we ﬁnd that the larger μ is, the greater the performance degrades of both methods. In addition, the performance drop of DFGP on both datasets is smaller than that of GPM. For example, when
μ is 0.01, on CIFAR100 dataset, the former and the latter decrease by −5.97 and −5.68; on PMNIST dataset, GPM and DFGP decrease by −5.75 and −4.05, respectively.
∇X Lt(W , X, Y ) (cid:9) (cid:8) 5.