Abstract composite image.
The goal of image harmonization is adjusting the fore-ground appearance in a composite image to make the whole image harmonious. To construct paired training images, existing datasets adopt different ways to adjust the illumination statistics of foregrounds of real images to produce synthetic composite images. However, differ-ent datasets have considerable domain gap and the per-formances on small-scale datasets are limited by insuffi-cient training data. In this work, we explore learnable aug-mentation to enrich the illumination diversity of small-scale datasets for better harmonization performance. In particu-lar, our designed SYthetic COmposite Network (SycoNet) takes in a real image with foreground mask and a ran-dom vector to learn suitable color transformation, which is applied to the foreground of this real image to pro-duce a synthetic composite image. Comprehensive exper-iments demonstrate the effectiveness of our proposed learn-able augmentation for image harmonization. The code of SycoNet is released at https://github.com/bcmi/SycoNet-Adaptive-Image-Harmonization. 1.

Introduction
As a prevalent image editing operation, image compo-sition [28] aims to cut the foreground from one image and paste it on another background image, making a realistic-looking composite image. Image composition plays a crit-ical role in artistic creation, augmented reality, automatic advertising, and so on [37, 44]. However, the illumina-tion statistics of foreground and background could be in-consistent due to distinct capture conditions or capture de-vices (e.g., season, weather, time of the day, camera setting), which makes the resultant composite image unrealistic. To address this issue, image harmonization [5] targets at ad-justing the illumination statistics of foreground to make it compatible with the background, leading to a harmonious
*Corresponding author.
Training data-hungry deep learning models for image harmonization relies on abundant pairs of composite im-ages and harmonious images. Nevertheless, it is extremely difficult and expensive to manually adjust the foreground of composite image to produce harmonious image. There-fore, recent works turn to an inverse manner, that is, ad-justing the foreground of real image to produce a synthetic composite image, resulting in pairs of synthetic composite images and ground-truth harmonious real images. In this inverse manner, the work in [5] constructed four datasets (HCOCO, HFlickr, HAdobe5k, and Hday2night), which are collectively called iHarmony4. Despite similar construc-tion pipeline, four datasets adopt different foreground ad-justment approaches.
In particular, HCOCO and HFlickr adopt traditional color transfer methods [31, 38, 11, 30] to adjust the foreground, while HAdobe5k (resp., Hday2night) replaces the foreground with the counterpart retouched by different experts (resp., captured at different times). Previ-ous works usually train a deep image harmonization model based on the union of training sets from four datasets. How-ever, the data distributions of different datasets are consider-ably different, which is caused by many factors (e.g., image source, capture device, scene type, foreground adjustment approach). Following the terminology of domain adapta-tion [35, 29], we treat each dataset as one domain and four datasets have large domain gap. We observe that finetuning on different datasets can bring notable perfor-mance gain (see Section 4.2), but the performance is still limited by insufficient training data on small-scale datasets (e.g., HFlickr, Hday2night).
In this work, we attempt to augment small-scale datasets with more synthetic composite images to enrich the illumi-nation diversity, which may not be easily achieved by us-ing the original foreground adjustment approach. Specif-ically, for HFlickr, we need to manually filter unqualified synthetic composite images [5], otherwise the performance would be significantly compromised (see Section 4.2). For
Hday2night, it is almost impossible to recapture the same scene at different times. Therefore, we design an aug-Figure 1: In the left two columns, we show a pair of real image Ir and original synthetic composite image Ic from HFlickr (resp., Hday2night) dataset in the top (resp., bottom) row, with the real foreground outlined in red. In the other columns, we show K augmented synthetic composite images {Ig k=1} generated by our SycoNet. k|K mentation network named SYthetic COmposite Network (SycoNet) to produce synthetic composite images automat-ically, which simulates the original foreground adjustment approach.
Our SycoNet takes in a real image with foreground mask and a random vector, generating a synthetic composite im-age with adjusted foreground. By sampling multiple ran-dom vectors, we can generate multiple synthetic compos-ite images. As shown in Figure 2(a), we adopt a CNN encoder to extract feature from real image and foreground mask, which is concatenated with a random vector to pro-duce suitable color transformation for the foreground. The color transformation is realized by a linear combination of basis LUTs [4]. The combined LUT is applied to the fore-ground to produce a synthetic composite image. Moreover, we establish a bijection between random vectors and origi-nal synthetic composite images in the dataset to ensure the quality and diversity of generated synthetic composite im-ages. Concretely, we employ another CNN encoder to pro-duce a latent code, which is expected to encode the neces-sary information required to transfer from real foreground to original composite foreground. Thus, when using this latent code to predict color transformation, we hope that the generated synthetic composite image can reconstruct the original synthetic composite image in the dataset.
After training SycoNet, we freeze the model parameters and integrate it with an existing image harmonization net-work, as shown in Figure 2(b). When training the image harmonization network, besides the original training pairs of synthetic composite images and real images, SycoNet produces extra synthetic composite images as augmented data. Both original and augmented synthetic composite images (see Figure 1) should be harmonized to approach ground-truth real images.
Our proposed learnable augmentation is helpful for adapting a pretrained image harmonization model to a new domain with limited data. Given a test image which we do not know which domain it belongs to, we can apply our learnable augmentation in the following two ways. 1) We use learnable augmentation to enhance the harmonization model of each domain. Given a test image, we can first predict its domain label using a domain classi-fier and apply the model of the corresponding domain (see
Section 4.6). 2) We use learnable augmentation to enhance one unified harmonization model for all domains (see Sec-tion 4.7). The second option is more compact, at the cost of performance degradation.
We conduct experiments on iHarmony4, which demon-strates that our proposed learnable augmentation can sig-nificantly boost the harmonization performance. Our major contributions are summarized as follows: 1) We propose learnable augmentation to enrich the illumination diversity for image harmonization; 2) We design a novel augmen-tation network named SycoNet, which can automatically generate synthetic composite images by simulating the fore-ground adjustment in the original dataset; 3) Extensive ex-periments prove the effectiveness of our proposed learnable augmentation. 2.