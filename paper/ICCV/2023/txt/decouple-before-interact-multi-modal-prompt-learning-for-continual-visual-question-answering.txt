Abstract
In the real world, a desirable Visual Question An-swering model is expected to provide correct answers to new questions and images in a continual setting (recog-nized as CL-VQA). However, existing works formulate CL-VQA from a vision-only or language-only perspective, and straightforwardly apply the uni-modal continual learning (CL) strategies to this multi-modal task, which is improper and suboptimal. On the one hand, such a partial for-mulation may result in limited evaluations. On the other hand, neglecting the interactions between modalities will lead to poor performance. To tackle these challenging is-sues, we propose a comprehensive formulation for CL-VQA from the perspective of multi-modal vision-language fusion.
Based on our formulation, we further propose MulTi-Modal
PRompt LearnIng with DecouPLing bEfore InTeraction (TRIPLET), a novel approach that builds on a pre-trained vision-language model and consists of decoupled prompts and prompt interaction strategies to capture the complex interactions between modalities. In particular, decoupled prompts contain learnable parameters that are decoupled w.r.t different aspects, and the prompt interaction strategies are in charge of modeling interactions between inputs and prompts. Additionally, we build two CL-VQA benchmarks for a more comprehensive evaluation. Extensive experi-ments demonstrate that our TRIPLET outperforms state-of-the-art methods in both uni-modal and multi-modal contin-ual settings for CL-VQA. 1.

Introduction
Visual Question Answering (VQA) [2, 11, 35, 25] aims to train a machine learning model capable of answering questions given visual images as accurately as possible.
*Corresponding authors
Figure 1: Comparison between (a) existing CL-VQA meth-ods [43, 28] and (b) our proposed TRIPLET model. Ex-isting methods train all the parameters similar to typical uni-modal CL-methods, while our TRIPLET model trains parameters in prompts and classifiers, as well as explicitly model the rich and complex modality-wise interactions.
In real-world dynamic environments [22], an ideal VQA model is expected to generate answers for new questions, new images, as well as new question-image simultaneously, which is recognized as CL-VQA [19], i.e., learn a sequence of VQA tasks with a single model without suffering from catastrophic forgetting [27] on previously observed data.
Existing works [19, 28] formulate CL-VQA as a vision-only or language-only continual learning setting, and straightforwardly apply the uni-model continual learning (CL) methods to this multi-modal task. However, model-ing CL-VQA from such a uni-model view is suboptimal, posing two challenging issues. First, the existing partial for-mulation does not take the multi-modal nature of CL-VQA into account, which leads to a limited view and improper evaluations. Second, by straightforwardly employing the uni-model CL methods, existing CL-VQA methods may ne-glect the rich and complex interactions between modalities, which leads to deteriorating performance.
To tackle the two challenging issues, we first propose a comprehensive formulation for CL-VQA explicitly cover-ing both multi-modal and uni-modal perspectives, so that more extensive evaluations can be conducted in terms of
input distributions. Specifically, we carefully design three i.e., scenarios according to different input distributions,
Continual Vision Scenario, Continual Language Scenario, and Continual Vision-Language Scenario, depending on in-cremental visual images, textual questions, and both.
Secondly, based on our CL-VQA formulation with three scenarios, we propose MulTi-Modal PRompt LearnIng with DecouPLing bEfore InTeraction (TRIPLET), a multi-learning-based continual model for CL-modal prompt
VQA. TRIPLET employs the widely adopted pre-trained vision-language models with state-of-the-art VQA perfor-mance as initialization, and consists of decoupled prompts and prompt interaction strategies. To be specific, decou-pled prompts contain a set of learnable parameters decou-pled in three aspects, i.e., modality aspect, layer aspect, and complementary aspect, which are attached to transformer layers. Then the prompt interaction strategies are designed to model the interactions between the input and prompts, modality-wise prompts, as well as task-wise prompts. Fig. 1 illustrate a comparison between existing CL-VQA methods and our proposed TRIPLET model.
In addition, we build two CL-VQA benchmarks on two datasets (i.e., TDIUC [14] and VQA2.0 [9]), carrying out extensive experiments on three scenarios. Our TRIPLET model is able to consistently outperform baselines and SO-TAs1 significantly across various settings. Besides, we con-duct ablation studies to validate the effectiveness of dif-ferent components in TRIPLET, demonstrating TRIPLET’s superiority. In summary, our contributions are as follows:
• We propose a comprehensive formulation for CL-VQA with multi-modal continual setting, enabling the contin-ual evaluations of various approaches in three scenarios based on different input distributions.
• We propose TRIPLET, a novel CL-VQA model contain-ing decoupled prompts and prompt interaction strategies, which is able to accurately generate answers in three con-tinual scenarios without rehearsal buffer. To the best of our knowledge, TRIPLET is the first multi-modal prompt learning-based continual model for CL-VQA.
• We build up two CL-VQA benchmarks (i.e., CL-VQA2.0 and CL-TDIUC) for empirical evaluations of CL-VQA including multi-modal continual setting. Our proposed
TRIPLET model achieves significant improvement over state-of-the-art approaches in all three scenarios for both two benchmarks.
Extensive ablation studies further demonstrate the effectiveness of different components in
TRIPLET. 2.