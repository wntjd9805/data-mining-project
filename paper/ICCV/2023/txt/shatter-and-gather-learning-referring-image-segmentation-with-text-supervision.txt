Abstract
Referring image segmentation, the task of segmenting any arbitrary entities described in free-form texts, opens up a variety of vision applications. However, manual labeling of training data for this task is prohibitively costly, leading to lack of labeled data for training. We address this issue by a weakly supervised learning approach using text descrip-tions of training images as the only source of supervision.
To this end, we first present a new model that discovers se-mantic entities in input image and then combines such enti-ties relevant to text query to predict the mask of the referent.
We also present a new loss function that allows the model to be trained without any further supervision. Our method was evaluated on four public benchmarks for referring im-age segmentation, where it clearly outperformed the exist-ing method for the same task and recent open-vocabulary segmentation models on all the benchmarks. 1.

Introduction
Referring image segmentation is the task of segmenting the referent corresponding to a natural language expression given as a query. Unlike the conventional semantic segmen-tation that aims at segmenting a pre-defined set of classes, referring image segmentation enables segmentation of any arbitrary entities described in free-form texts and thus opens up a wide variety of applications such as human-computer interaction [8, 57] and text-based image editing [7, 17].
Thanks to the development of deep neural networks, re-cent studies have demonstrated remarkable results on re-ferring image segmentation in the supervised learning set-ting [14, 22, 25, 27, 67, 68, 69]. However, obtaining manual annotation of training data for referring image segmenta-tion is prohibitively costly since the data requires two types of labels for each image, i.e., natural language expressions describing the entities that appear in the image and segmen-tation labels corresponding to them. This typically demands tremendously more manpower than annotation for semantic
∗Equal contribution.
Figure 1: Our model consists of two different attention modules: bottom-up attention that identifies individual en-tities in an image, and top-down attention that combines the entities based on a referring expression. It is trained in a contrastive manner to ensure consistency between match-ing image-text pairs. In inference, a segmentation mask is predicted by combining the entities found by bottom-up at-tention with weights derived by top-down attention. segmentation, leading to lack of labeled data for training.
A solution to this issue is weakly supervised learning, which trains a model on a dataset providing weaker forms of supervision than the conventional ones. In this paper, we particularly focus on learning referring image segmentation only with natural language expressions describing the as-sociated images. Since these labels are readily available in many vision-language datasets [28, 47], the issue on lack of labeled training data can be alleviated by our approach.
However, learning referring image segmentation in this set-ting introduces another challenge: Since natural language expressions often contain relations between visual entities such as instances and object parts (e.g., “The cat next to the
table leg”), a model to be trained should be aware of the inter-object relations to exploit the expressions as supervi-sion. Thereby we need a method to discover individual en-tities and infer their relations without any supervision but the referring expression.
To fulfill this requirement, we introduce a new bottom-up and top-down attention framework, which is illustrated in
Fig. 1. Individual entities existing in an image are first dis-covered by bottom-up attention, which solely exploits visual information. For the bottom-up attention, an entity discov-ery module progressively refines a set of embedding vectors named slots to capture distinct visual entities, following slot attention [42]. We moreover propose a novel slot formu-lation named entity slot, which enables fine-grained entity discovery in real-world images. As a result, entity slots af-ter the refinement capture individual visual entities without any segmentation supervision. These entities are consid-ered primitive units for composing the segmentation mask of a referent during inference.
Afterward, based on the relational and structural infor-mation of referring expression, top-down attention selec-tively attends to the referred entities and combines them into the predicted mask. Top-down attention is implemented by a modality fusion module consists of cross-attention transformer [15]. This attention scheme enables the mod-ule to infer the relevance between discovered entities and the referring expression instead of relying on the cosine similarity or heuristic similarity functions used in previous work [18, 53, 64, 73].
For training, we propose the contrastive cycle-consistency (C3) loss that enforces cycle-consistency be-tween image-text pairs under the contrastive learning frame-work [47]. To establish such consistency, the model is trained to preserve the textual information after the fusion of entities and textual features. By doing so, we observe that latent relevance between the discovered entities and re-ferring expression automatically emerges in the top-down attention without any explicit mask supervision.
One may wonder the difference between our task and open-vocabulary segmentation [18, 64, 73], i.e., semantic segmentation of arbitrary categories without explicit mask supervision. The key difference is that a referent to segment in referring image segmentation is given by a complex free-form text, which introduces additional challenges.
The proposed method was evaluated and compared on four public benchmarks for referring image segmentation, where it substantially outperformed the previous weakly su-pervised learning method [53]. Moreover, we reproduced recent open-vocabulary segmentation models [64, 73] and evaluated them on referring image segmentation bench-marks; the results show that the proposed model surpassed them even without pre-training on large-scale image-text data [5, 47, 54].
In summary, our contribution is three-fold as follows:
• We present a new weakly supervised learning model for referring image segmentation. Our model employs bottom-up and top-down attention to discover individ-ual entities and infer their relations, which enables to exploit natural language expressions as supervision ef-fectively during training.
• We propose a new loss function called contrastive cycle-consistency loss, which allows latent relevance between discovered entities and referring expressions to emerge without requiring further supervision.
• Our model clearly outperformed the existing method for the same task and recent open-vocabulary segmen-tation models on all the four benchmarks. 2.