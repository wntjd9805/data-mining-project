Abstract
Viewpoint is a fundamental modality that carries the in-teraction between observers and their environment. This paper proposes the ﬁrst deep-learning framework for the viewpoint modality. The challenge in formulating learning frameworks for viewpoints resides in a suitable multimodal representation that links across the camera viewing space and 3D environment. Traditional approaches reduce the problem to image analysis instances, making them compu-tationally expensive and not adequately modelling the in-trinsic geometry and environmental context of 6DoF view-points. We improve these issues in two ways. 1) We pro-pose a generalized viewpoint representation forgoing the analysis of photometric pixels in favor of encoded viewing ray embeddings attained from point cloud learning frame-works. 2) We propose a novel SE(3)-bijective 6D viewing ray, hyper-ray, that addresses the DoF deﬁciency problem of using 5DoF viewing rays representing 6DoF viewpoints.
We demonstrate our approach has both efﬁciency and accu-racy superiority over existing methods in novel real-world environments. 1.

Introduction
Viewpoints play a critical role in a broad range of tasks in computer vision [15, 12, 33, 35], graphics [28, 10], robotics
[29, 23, 34, 32] and HCI [20, 2]. Whether given as input or required as output, viewpoints embody complex dependen-cies among capture dynamics/constraints, observer prefer-ences and task-speciﬁc goals. To enable AI systems to un-derstand and characterize these complex dependencies in a data-driven manner, we propose a novel encoding scheme and learning framework for the viewpoint modality, similar to how images being encoded with CNNs. This is a ﬁrst step towards research and application focusing on, for example cross-modal learning of text-and-view (i.e. ﬁnd/describe views with texts), text-based robot navigation or behavior recognition from camera movements.
Viewpoint instances play the role of a geometric link be-tween the environment and captured contents, with viewing
Figure 1: Viewpoint Learning. Examples showing our method learns to replicate 6DoF viewpoint capture patterns in novel environments. rays conveying info on both capture-time camera pose and scene content observability. However, existing approaches
[12, 15] focused on (rendered) viewpoint image analysis are handicapped in two important ways: First, the inherently local scope of individual photometric pixels obfuscates the geometric and environmental contexts that are crucial cues to characterize the viewpoint. Second, sampling the view-ing space through image rendering is computationally de-manding. Addressing both of these shortcomings requires novel data representations and compute frameworks.
Photometric pixels, which convey the signal carried by viewing rays, are widely used in existing viewpoint repre-sentations. However, the geometry of viewing rays (i.e. ori-gin, length and direction) is usually ignored, even though such parameters govern the mapping from scene content to the observer. Importantly, viewing ray geometry uniquely describes the capture-time camera pose and observed scene geometry of a given viewpoint. Hence, we represent view-points as a collection of viewing rays, which are in turn encoded in terms of both their geometry and their carried signal. To encode viewing rays, we tightly couple the ray encoding process with point cloud learning frameworks
through our proposed Harmonics Ray Encoders (HREs).
The HREs are spherical feature ﬁelds that can differentiably encode a generalized range-bearing feature of the viewing rays emanating from the observer to the scene elements. In this way, the ray geometry and the corresponding environ-mental content can be jointly encoded into latent embed-dings with rich geometric and semantic context. Given that
HREs are parameterized by a compact learnable set of co-efﬁcients, they are suitable as an extension for off-the-shelf point cloud learning frameworks.
Next, we address the heavy computational burden of viewpoint learning frameworks induce by the DoF deﬁ-ciency problem of using viewing rays (5DoF) for represent-ing viewpoints (6DoF). Due to the lower DoF of viewing rays, the methods need to spend large computation budgets on rendering an over-speciﬁed set of pixels/rays to uniquely characterize a viewpoint. This also results in a same view-ing ray belonging to many different viewpoints, which lays heavy burden on the subsequent analyzer to inspect the in-terplay of the viewpoints’ ray bundles for characterizing dif-ferent viewpoints. Therefore, we propose a novel SE(3)-bijective 6D viewing rays, namely hyper-rays, where each hyper-ray can uniquely represent a 6DoF viewpoint. The in-troduction of hyper-rays removes the correlation among the viewing rays within a viewpoint, which greatly improves the efﬁciency by relieving the burden on the post-analysis stage as well as allowing sparser ray samplings.
Finally, we integrate our proposed ray representation and encoding mechanism into a hierarchical learning framework that ﬁrst examines the panoramic environment to determine a location sanity score, followed by a 6DoF viewpoint san-ity evaluation. By decoupling the location and viewpoint, we can quickly ﬁlter the search space by discarding the viewpoints at unlikely locations, as well as enhancing the viewpoint analysis with its panoramic environment. Since our method only needs a sparse point cloud as input, it can efﬁciently sample and analyze dense 6DoF viewpoint hy-potheses of indoor environments within ∼10 seconds on a commodity GPU. We summarize our main technical contri-bution/insights as:
• We propose a new viewpoint representation using en-coded viewing rays to endow rich geometric context for viewpoint learning.
• We propose harmonics ray encoders to bridge ray encod-ing with existing point cloud networks, endowing the em-beddings with rich environmental context.
• We extend the representation dimension of viewing rays to enable unambiguous encoding of a SE(3) camera pose in each ray’s geometry.
• We propose an efﬁcient inference workﬂow decouples location and orientation, enabling efﬁcient dense 6DoF viewpoint sampling and analysis. 2.