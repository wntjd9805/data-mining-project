Abstract
In many video restoration/translation tasks, image pro-cessing operations are na¨ıvely extended to the video do-main by processing each frame independently, disregard-ing the temporal connection of the video frames. This dis-regard for the temporal connection often leads to severe temporal inconsistencies. State-Of-The-Art (SOTA) tech-niques that address these inconsistencies rely on the avail-ability of unprocessed videos to implicitly siphon and utilize consistent video dynamics to restore the temporal consis-tency of frame-wise processed videos which often jeopar-dizes the translation effect. We propose a general frame-work for this task that learns to infer and utilize consis-tent motion dynamics from inconsistent videos to mitigate the temporal flicker while preserving the perceptual qual-ity for both the temporally neighboring and relatively dis-tant frames without requiring the raw videos at test time.
The proposed framework produces SOTA results on two benchmark datasets, DAVIS and videvo.net, processed by numerous image processing applications. The code and the trained models are available at https://github. com/MKashifAli/TARONVD. 1.

Introduction
Video sharing social media platforms like Snapchat and
TikTok have introduced the common populace to a plethora of computer vision applications such as Style Transfer [9],
Colorization [43], Denoising [42], and Dehazing [38]. With this wide-scale integration of these classical computer vi-sion applications in such platforms, various image process-ing operations are na¨ıvely extended to videos due to scarcity of annotated video datasets and their computational com-plexity of video processing methodologies. This na¨ıve ex-tension of static image processing methodologies to videos disregards the temporal connection of the video progres-sion and introduces severe temporal flickering in the videos.
This temporal flicker can appear for various reasons; for in-∗: Corresponding author. stance, these image processing methods can produce dras-tically different results for temporally neighboring frames due to slight changes in their global or local content statis-tics, or it could also happen due to the multi-modality of the application as there could exist a number of valid solutions for images with similar content as highlighted in [3, 41].
Therefore, the extension of these image-to-image transla-tion tasks to the video domain is an active area of research in computer vision, and methods that can help extend various image processing applications with little to no knowledge of the operations used to create videos are quite useful.
Currently, task-dependent there are several temporal consistency correction approaches available such as [4, 23, 34], but due to the complex nature of these applications, only a handful of approaches have been proposed to tackle the problem of blind temporal consistency correction. Bon-neel et al. [3] initiated the investigation of blind temporal consistency correction with gradient-domain minimization of per-frame processed video with the unprocessed video to minimize the warping error between the frames. Lai et al. [16] extended the formulation of [3] with the help of recurrent Convolutional Neural Networks (CNN) and intro-duced a perceptual penalty in their formulation to restrict the deviation of perceptual content of the restored video from the frame-wise processed video. Deep Video Prior (DVP) [19] extended Deep Image Prior (DIP) [35] to the temporal dimension and proposed to formulate enforcing temporal consistency by training a CNN to generate pro-cessed video from the unprocessed video without utilizing optical flow. All of these approaches rely on the availability of unprocessed videos for implicit extraction of consistent motion dynamics to use as a restoration guide.
Please note that all the previously proposed approaches for this task define it with the help of unprocessed videos (both task-specific and task-agnostic). Having access to un-processed videos at test time helps the models in siphon-ing and transferring consistent motion dynamics to the in-consistent videos to decrease temporal flicker. Despite its efficacy, this implicit definition limits the applicability of the previously proposed approaches to only the videos for which their unprocessed counterparts are available and also
Key features
Does not require raw videos
Explicit video dynamics design
Faithful perceptual restoration
Sequential processing
Long term consistency
Sharpness in processed videos
High-frequency flicker removal
Test time training
Runtime (per frame avg.)
Bonneel et al. [3] Lai et al. [16] DVP [19] Ours
×
×
×
✓
×
✓
×
× 0.2146
×
✓
×
✓
×
×
✓
× 2.4635*
×
×
×
×
✓
×
✓
✓ 3.6365
✓
✓
✓
✓
✓
✓
✓
× 0.2236
Table 1: Key differences in available approaches. ”*” in front of Bonnel et al. [3]’s runtime highlights environment difference and is only presented for reference only. Please note that the runtime is averaged over the same video se-quence for all the methods. introduce an inherent bias towards the unprocessed videos which can compromise the translation effect and quality of the processed videos (as presented in Fig. 8 ∼ 10).
In order to overcome these limitation, we aim to learn and infer consistent motion representation solely from tem-porally inconsistent videos. Doing so does not only alle-viate the need for raw videos at test time, but also miti-gates the inherent bias towards the raw videos that is com-monly present in the currently available approaches for this task. For the task of learning these consistent motion rep-resentation, we fine-tune conventional optical flow estima-tions networks like [11, 31, 33] along with the restorative part of the network in an end-to-end manner. The proposed model pipeline is illustrated in Fig. 1. The proposed model achieves state-of-the-art qualitative and quantitative results.
The proposed formulation can also handle the resolution mismatch problem in processed and raw videos and makes it possible to extend the Single Image Super-Resolution [17] methods to Video Super-Resolution methods without any modification. The detailed description of our formulation is presented in 3. We summarize our contributions as follows:
• Identify and propose tailored solutions for various chal-lenges of na¨ıve extensions of image processing applications to videos.
• Learn to infer consistent motion representations from tem-porally inconsistent videos.
• Utilize the learned consistent motion representations to propose a general framework for task agnostic temporal consistency correction that produce SOTA results. 2.