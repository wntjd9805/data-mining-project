Abstract
Image super-resolution (SR) has witnessed extensive neural network designs from CNN to transformer ar-chitectures. However, prevailing SR models suffer from prohibitive memory footprint and intensive computations, which limits further deployment on edge devices. This work investigates the potential of network pruning for super-resolution to take advantage of off-the-shelf network designs and reduce the underlying computational overhead.
Two main challenges remain in applying pruning methods for SR. First, the widely-used filter pruning technique reflects limited granularity and restricted adaptability to diverse network structures. Second, existing pruning methods generally operate upon a pre-trained network for the sparse structure determination, hard to get rid of dense model training in the traditional SR paradigm. To address these challenges, we adopt unstructured pruning with sparse models directly trained from scratch. Specifically, we propose a novel Iterative Soft Shrinkage-Percentage (ISS-P) method by optimizing the sparse structure of a randomly initialized network at each iteration and tweaking unimportant weights with a small amount proportional to the magnitude scale on-the-fly. We observe that the proposed ISS-P can dynamically learn sparse structures adapting to the optimization process and preserve the sparse model’s trainability by yielding a more regular-Experiments on benchmark throughput. ized gradient datasets demonstrate the effectiveness of the proposed
ISS-P over diverse network architectures. Code is avail-able at https://github.com/Jiamian-Wang/
Iterative-Soft-Shrinkage-SR 1.

Introduction
Single image super-resolution [18, 20] aims to recon-struct the high-resolution (HR) image from a low-resolution (LR) input. Towards a high-fidelity reconstruction, research efforts have been made by relying on the strong modeling capacity of convolutional neural networks [7, 19, 26, 47].
*Corresponding authors: Yulun Zhang (yulun100@gmail.com) and
Zhiqiang Tao (zxtics@rit.edu)
More recently, advanced Transformer architectures [25, 45, 50] are elaborated, enabling photorealistic restoration. De-spite the impressive performance, the excessive memory footprint of existing models has been de facto in the field, which inevitably prohibits the deployment of advanced SR models on computational-constrained devices.
To alleviate the computational complexity, we study net-work pruning, which takes advantage of off-the-shelf ad-vanced network architectures to realize efficient yet accu-rate SR models. Network pruning has been long devel-oped in two mainstream directions. On the one hand, filter pruning (structured pruning) [23] cuts off the specific filter of convolutional layers, among which representative prac-tices in SR is to prune cross-layer filters governed by resid-ual connections [48, 49]. However, these methods need to consider the layer-wise topology by posing structural con-straints, requiring a heuristic design and thus making them inflexible. Plus, filter pruning inhibits a more fine-grained manipulation to the network. On the other hand, weight pruning (unstructured pruning) directly removes weight scalars across the network, endowed with more flexibility by accommodating weight discrepancy. Also, the weight pruning method allows a very high pruning ratio, e.g., a ra-tio of 99% with competitive performance [10, 11]. To this end, this work focuses on delivering a highly-adaptive un-structured pruning solution for diverse SR architectures.
Generally, pruning algorithms are widely recognized to have three steps: (1) pre-training, (2) sparse structure acqui-sition, and (3) fine-tuning the sparse network. Among these steps, the dense network pre-training usually introduces heavy costs beyond the sparse network optimization. For example, before obtaining a sparse network, the CAT [50] network architecture takes 2 days to train its dense coun-terpart on 4 A100 GPUs. Thus, a natural question arises to save training time further – can we directly explore the sparsity of network structures from random initialization?
We start from the baseline method by performing ran-dom pruning on weights at initialization, whose limitation is the irrelevance between the sparse structure and the weight distribution varying to the optimization. Following this line, we apply the widely-used L1 norm [23, 11] pruning on ran-domly initialized weights. However, the immutable sparsity
cannot be well aligned with the optimization, leading to lim-ited performance. To tackle this problem, we introduce an iterative hard thresholding [4, 5] method (IHT) stemming from compressive sensing [6, 8], where the iterative gradi-ent descent step is regularized by a hard thresholding func-tion. Unlike previous works, we tailor IHT to iteratively set unimportant weights as zeros and preserve the impor-tant weight magnitudes. By this means, the sparse structure adapts to the weight distribution throughout the training, which potentially better selects essential weights. However, the sparse structure alignment in IHT is heavily susceptible to the magnitude-gradient relationship. The zeroed weight can be continually trapped as “unimportant” once the scales of magnitude and gradient are incomparable. Moreover, by directly zeroing out unimportant weights, IHT blocks the error back-propagation at each iteration, especially hinder-ing the optimization in shallow layers.
To address the aforementioned negative effects, we in-troduce a more flexible thresholding function for an expres-sive treatment of unimportant weights. A natural tuning approach is to softly shrink the weights rather than hard threshold. We first explore the soft shrinkage by a growing regularization method [49], namely ISS-R. Per each itera-tion, the proposed ISS-R constrains weight magnitudes with a gradually increasing weighted L2 regularization, to avoid the conflict between network pruning and smooth sparse network optimization. However, the growing regularization schedule involves a number of hyperparameter tuning, re-quiring cumbersome manual efforts. Notably, the L2 reg-ularization shrinkage inside ISS-R is, in essence, propor-tional to the weight magnitude. Based on this insight, we propose a new iterative soft shrinkage function to simplify the regularization by equivalently shrinking the weight with a percentage (ISS-P). It turns out that ISS-P not only en-courages dynamic network sparsity, but also preserves the sparse network trainability, resulting in better convergence.
We summarize the contributions of this work as follows:
• We introduce a novel unstructured pruning method, namely iterative soft shrinkage-percentage (ISS-P), which is compatible with diverse SR network designs.
Unlike existing pruning strategies for SR, the proposed method trains the sparse network from scratch, provid-ing a practical solution for sparse network acquisition under computational budget constraints.
• We explore pruning behaviors by interpreting the train-ability of sparse networks. The proposed ISS-P enjoys a more promising gradient convergence and enables dynamic sparse structures in the training process, of-fering new insights to design pruning methods for SR.
• Extensive experimental results on benchmark testing datasets at different pruning ratios and scales demon-strate the effectiveness of the proposed method com-pared with state-of-the-art pruning solutions. 2.