Abstract
Large-scale vision-language models (VLMs) like CLIP successfully find correspondences between images and text.
Through the standard deterministic mapping process, an image or a text sample is mapped to a single vector in the embedding space. This is problematic: as multiple samples (images or text) can abstract the same concept in the physical world, deterministic embeddings do not re-flect the inherent ambiguity in the embedding space. We propose ProbVLM, a probabilistic adapter that estimates probability distributions for the embeddings of pre-trained
VLMs via inter/intra-modal alignment in a post-hoc man-ner without needing large-scale datasets or computing. On four challenging datasets, i.e., COCO, Flickr, CUB, and
Oxford-flowers, we estimate the multi-modal embedding un-certainties for two VLMs, i.e., CLIP and BLIP, quantify the calibration of embedding uncertainties in retrieval tasks and show that ProbVLM outperforms other methods. Fur-thermore, we propose active learning and model selection as two real-world downstream tasks for VLMs and show that the estimated uncertainty aids both tasks. Lastly, we present a novel technique for visualizing the embedding dis-tributions using a large-scale pre-trained latent diffusion model. Code is available at https://github.com/
ExplainableML/ProbVLM 1.

Introduction
Recently, large vision-language models (VLMs) [62, 51, 45, 74, 1, 35] have become exceedingly popular due to their ability to align images and text. These models such as CLIP [62] and BLIP [45] are trained on large-scale datasets such as LAION-400M [70] and YFCC-100M [79] and have shown strong performance when evaluated in a zero-shot fashion (i.e without requiring fine-tuning on spe-cific datasets) for a variety of downstream tasks. One of the most popular applications of VLMs is cross-modal re-trieval [86, 88] i.e retrieving images (text) for a queried text (images). However, image-to-text matching (and vice-*Authors contributed equally.
Figure 1: We provide probabilistic embeddings for deter-ministic pre-trained vision-language models that are frozen.
By capturing the ambiguity inherently present in the inputs, we obtain well-calibrated uncertainty estimates. versa) is fundamentally ill-posed due to the inherent ambi-guity in either modality [97], i.e. the same caption (or im-age) can be valid for multiple images (or captions). There-fore, it becomes essential to model the ambiguity inherently present in the various modalities, and combinations thereof.
Instead of mapping inputs to embeddings, probabilistic embedding methods [57, 10] learn to map input samples to distributions. This is achieved by parameterizing the distri-butions of the embeddings and training a deep neural net-work to maximize its likelihood. Although they model am-biguities in the embedding space, such probabilistic models require training deep networks from scratch. This requires access to the large-scale datasets and the computational re-sources of the recent VLMs [62, 35, 51, 74, 45].
We propose ProbVLM, a post-hoc probabilistic adapter, the first method to convert the deterministic embeddings provided by a frozen large-scale vision-language models into probabilistic ones, as shown in Figure 1. This enables us to efficiently retain the benefits of large-scale pre-training while learning distributions that model the inherent ambigu-ities in the different modalities. Our ProbVLM models the embedding distribution as a heteroscedastic probability dis-tribution and is trained using a combination of intra-modal and cross-modal alignment objectives and provides well-calibrated uncertainty estimates, useful for several tasks.
We demonstrate on two large vision-language datasets, i.e., COCO [46] and Flickr [60], and on two fine-grained image datasets, i.e., CUB [85] and Oxford-Flowers [55] with sentences from [66], that ProbVLM learns calibrated
uncertainties without requiring large-scale models to be trained from scratch. This sharply contrasts previous works on probabilistic embeddings [57, 10] that train new models from scratch. We perform a series of analyses to understand the impact of the training objective and to study the proper-ties of the resulting uncertainties. Furthermore, we demon-strate that our uncertainty estimates can be used to select the optimal model from a set of finetuned vision-language mod-els on an unlabeled target dataset. They can also be used to choose the most suitable samples for fine-tuning the model in an active learning setup. Finally, with the help of a pre-trained latent diffusion model [67], i.e., Stable Diffusion, we decode sampled embeddings from predicted distribution to visualize the predicted embedding distributions. We show that the predicted embedding distributions indeed capture meaningful modes of variation, that may be interpretable. 2.