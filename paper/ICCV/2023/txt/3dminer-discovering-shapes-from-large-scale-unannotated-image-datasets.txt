Abstract 1.

Introduction
We present 3DMiner – a pipeline for mining 3D shapes from challenging large-scale unannotated image datasets.
Unlike other unsupervised 3D reconstruction methods, we assume that, within a large-enough dataset, there must ex-ist images of objects with similar shapes but varying back-grounds, textures, and viewpoints. Our approach lever-ages the recent advances in learning self-supervised image representations to cluster images with geometrically sim-ilar shapes and find common image correspondences be-tween them. We then exploit these correspondences to ob-tain rough camera estimates as initialization for bundle-adjustment. Finally, for every image cluster, we apply a pro-gressive bundle-adjusting reconstruction method to learn a neural occupancy field representing the underlying shape.
We show that this procedure is robust to several types of er-rors introduced in previous steps (e.g., wrong camera poses, images containing dissimilar shapes, etc.), allowing us to obtain shape and pose annotations for images in-the-wild.
When using images from Pix3D chairs, our method is ca-pable of producing significantly better results than state-of-the-art unsupervised 3D reconstruction techniques, both quantitatively and qualitatively. Furthermore, we show how 3DMiner can be applied to in-the-wild data by reconstruct-ing shapes present in images from the LAION-5B dataset.
Project Page: https://ttchengab.github.io/3dminerOfficial.
Learning-based systems that try to reason about 3D ge-ometry from images suffer from a fundamental limitation: the amount of available 3D data. Despite recent advances in capturing the world tridimensionally, the biggest image datasets still contain many orders of magnitude more data points than their 3D counterparts.
In practice, the sheer quantity of images ends up capturing a much richer visual vocabulary – different textures, illuminations, shapes, envi-ronments, types of objects and relationships between them.
Therefore, developing techniques that can leverage this in-formation is the key to general, high-performing 3D recon-struction algorithms. Unfortunately, the abundance and va-riety of visual data in image datasets leads to great com-plexity when trying to extract 3D information. Consider a very simple dataset consisting of multiple images of the same object in the same environment taken from different camera viewpoints. Extracting 3D information is not en-tirely trivial, but potentially doable through stucture-from-motion approaches. However, if we increase the complex-ity of this dataset by adding images of the object in differ-ent environments, different materials and with slight shape variations, structure-from-motion techniques will fail. The complexity only increases when we consider all the possi-ble image permutations that one can find online: a myriad
*Work was partially done during internship at Adobe Research. 1
of object types, occluded objects, partial observations, non-photorealistic imagery and so on. How can we extract 3D information from such complicated image datasets?
Various image-to-3D approaches have tried to tame the visual complexity in big image datasets. They usually em-ploy different amounts of manual image annotations; i.e., object poses, masks, keypoints, part segmentations, and so on. These techniques use models that are trained to disen-tangle 3D geometry from various other factors while trying to reconstruct the original image and its annotations. Due to the ill-posed nature of the single-view reconstruction prob-lem, training these models is very hard and multiple regu-larizations are necessary. When presented with more chal-lenging datasets, with real images, even if they only depict a single object type (e.g., Pix3D chairs), the best models fail to produce reasonable results.
In this work we aim to extract 3D shapes from image datasets in a completely different way. We call our approach 3DMiner. Given a very large set of images, our initial goal is to separate them in groups containing similar shapes.
Within these groups, we estimate robust pairwise image cor-respondences that will give us a good idea about the rela-tive pose of the objects in the images. Using this informa-tion, we can estimate the underlying 3D geometry in ev-ery image group, effectively treating the single-view recon-struction problem as a noisy multi-view one. Unfortunately, this is not a straightforward structure-from-motion setup – within the same group, the objects are similar but not ex-actly the same; they have different colors, backgrounds, and even slightly different geometry. To circumvent this issue, we adopt modern reconstruction techniques based on neural fields. These representations give us the ability to train parametric occupancy fields through gradient descent while refining camera poses, intrinsics, and more impor-tantly, giving us a proxy for the quality of the recovered 3D shape – the image reconstruction loss. Ultimately, the entire pipeline provides association between the shape and poses across images in-the-wild for arbritrary categories – a task for which many datasets rely on manual annotations.
Instead of having a single hard-to-train model extracting shapes from images, we opt for dissecting the problem and breaking it in pieces that can be tackled with well-studied tools. At the heart of our approach are recent advances in learning representations from images in an unsupervised manner. Those techniques allow us to identify image as-sociations and to find common features more conveniently, establishing robust correspondences and ignoring nuisance factors like different backgrounds, illumination and small shape variations. As a notable example, the recent DINO-ViT, trained through self-supervision on ImageNet data, has shown remarkable ability to distinguish foregrounds, per-form part segmentation, and generate common keypoints.
More importantly: further improvements in image repre-sentation learning can be immediately incorporated into our method – no model needs to be retrained or fine-tuned. The same can also be applied to advances in neural fields. Once better methods for optimizing occupancy fields are devel-oped, they can be directly plugged into our pipeline.
We refer to our method as unsupervised, meaning that it does not require 3D data to perform 3D reconstruction.
Thus, using other features/outputs from models trained without 3D data do not alter the unsupervised 3D re-construction setup. We do, however, restrain from using keypoints/pose estimators as they limit our approach to category-specific conventions.
We demonstrate the capabilities of our approach in two empirical studies using Pix3D and LAION-5B. For Pix3D, we use our 3D mining pipeline as a way of generating 3D for each image in the dataset and directly compare it against state-of-the-art single-view reconstruction approaches that do not use 3D information during training. Our experiments show that, differently from the other approaches, 3DMiner is capable of generating reasonable 3D shapes and displays a relative F-score improvement of 80% over the state-of-the-art. In order to showcase the versatility of our approach we also ran 3DMiner on subsets of images from LAION-5B gathered using various short text prompts. Despite the chal-lenges presented by the diversity in this dataset, 3DMiner is still capable of finding reasonable 3D representations across multiple categories.
In summary, our contributions are: (i) a new problem formulation on mining 3D shapes from large-scale web-retrieved images without any priors or annotations; (ii) an end-to-end pipeline to cluster, estimate pose, and generate neural 3D representations from unannotated image datasets without any 3D ground truths; (iii) a detailed empirical study to showcase the superiority of our method on chal-lenging datasets and robustness under categories where no previous ground-truth reconstructions exist. 2.