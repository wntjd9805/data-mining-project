Abstract
Existing text-to-image generation approaches have set high standards for photorealism and text-image corre-spondence, largely beneﬁting from web-scale text-image datasets, which can include up to 5 billion pairs. How-ever, text-to-image generation models trained on domain-speciﬁc datasets, such as urban scenes, medical images, and faces, still suffer from low text-image correspon-dence due to the lack of text-image pairs. Additionally, collecting billions of text-image pairs for a speciﬁc do-main can be time-consuming and costly. Thus, ensur-ing high text-image correspondence without relying on web-scale text-image datasets remains a challenging task.
In this paper, we present a novel approach for enhanc-ing text-image correspondence by leveraging available se-mantic layouts.
Speciﬁcally, we propose a Gaussian-categorical diffusion process that simultaneously gener-ates both images and corresponding layout pairs. Our experiments reveal that we can guide text-to-image gen-eration models to be aware of the semantics of different image regions, by training the model to generate seman-tic labels for each pixel. We demonstrate that our ap-proach achieves higher text-image correspondence com-pared to existing text-to-image generation approaches in the Multi-Modal CelebA-HQ and the Cityscapes dataset, where text-image pairs are scarce. Codes are available at https://pmh9960.github.io/research/GCDP. 1.

Introduction
Text-to-image generation aims to materialize text de-scriptions into images, where the main challenge comes from ensuring high image quality and correspondence be-tween input text and output images. While texts convey intuitive semantic depictions of images, they often lack de-tailed spatial descriptions. For example, text descriptions such as “A woman is wearing earrings.” do not describe where the earrings are located within the image. Thus, when a small number of text-image pairs are given, it is challeng-1 * indicates equal contribution.
Figure 1. Recall of facial attributes speciﬁed in the text descrip-tions. Text-to-image generation approaches trained on a subset of the Multi-Modal CelebA-HQ [17, 22] often fail to reﬂect text con-ditions. Facial attributes are classiﬁed with a pretrained attribute classiﬁer [30]. ing for a generative model to learn what part of the image corresponds to which words in the text.
Overcome this hurdle, recent text-to-image generation approaches [28, 29, 31, 32] leverage web-scale text-image datasets [29, 33] containing up to 5 billion text-image pairs.
With access to such data, generative models can fully learn the correspondence between input texts and output images and synthesize photorealistic images while properly reﬂect-ing text descriptions.
However, the cost of such large-scale training remains a major obstacle, often requiring weeks of training even with hundreds of GPUs, which limits participation in the subject to only a few researchers. Moreover, when generating im-ages in a speciﬁc domain, such as faces or urban scenes, col-lecting billions of text-image pairs can be challenging due to the difﬁculties in collecting images. Even with a general-purpose pretrained model, ﬁnetuning on datasets with large domain gaps (e.g., urban scenes or medical images) leads to poor image quality and low text-image correspondence. Re-cent text-to-image models trained on speciﬁc domains often fail to reﬂect text conditions in the absence of web-scale text-image pairs. To examine the issue in data-scarce sce-narios, we evaluate text-to-image generation models trained on a subset of the Multi-Modal CelebA-HQ [17,22] dataset.
As shown in Figure 1, existing models struggle to gener-ate certain attributes speciﬁed in the given text conditions.
Thus, ensuring high text-image correspondence remains a
challenge for domain-speciﬁc generation.
In this paper, we present a novel approach to achieve high text-image correspondence for domain-speciﬁc text-to-image generation by leveraging semantic layouts. Rather than solely generating images based on text descriptions, we propose to concurrently generate both images and their corresponding semantic layouts. To this end, we design a
Gaussian-categorical diffusion process that models the joint distribution of image-layout pairs. To the best of our knowl-edge, this is the ﬁrst approach to combine Gaussian and cat-egorical diffusion processes into a uniﬁed diffusion process.
By generating semantic labels for each pixel in the image, our generative model can learn the semantics of different parts of the image, allowing it to effectively learn which text descriptions correspond to which locations in the im-age, even with limited text-image pairs.
We experiment our approach on subsets of the Multi-Modal CelebA-HQ [19, 22] to simulate cases where text-image pairs are limited and semantic layouts are available.
We also add text descriptions to the Cityscapes dataset [6] to evaluate text-to-image generation in complex scenes with multiple objects, where learning text-image correspondence can be challenging. Our experiments and analyses reveal that modeling the joint image-layout distribution can effec-tively facilitate text-to-image generation models to achieve high text-image correspondence when web-scale text-image pairs are unavailable. We also demonstrate potential appli-cations of the Gaussian-categorical diffusion models in se-mantic image synthesis and semantic segmentation, through cross-modal outpainting.
Our contributions are threefold:
• We deﬁne a Gaussian-categorical diffusion process for modeling joint image-layout distributions, which is the ﬁrst approach to unify two diffusion processes for image-layout generation.
• Our experiments reveal that generating image-layout pairs can be a practical alternative to increase text-image correspondence in circumstances where collect-ing web-scale text-image pairs is infeasible.
• We present cross-modal outpainting, which demon-strates that Gaussian-categorical diffusion models are also capable of modeling conditional distributions for semantic image synthesis and semantic segmentation. 2.