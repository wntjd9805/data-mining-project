Abstract
Recent inversion methods have shown that real images can be inverted into StyleGAN’s latent space and numer-ous edits can be achieved on those images thanks to the se-mantically rich feature representations of well-trained GAN models. However, extensive research has also shown that image inversion is challenging due to the trade-off between
In this paper, high-fidelity reconstruction and editability. we tackle an even more difficult task, inverting erased im-ages into GAN’s latent space for realistic inpaintings and editings. Furthermore, by augmenting inverted latent codes with different latent samples, we achieve diverse inpaint-ings. Specifically, we propose to learn an encoder and mix-ing network to combine encoded features from erased im-ages with StyleGAN’s mapped features from random sam-ples. To encourage the mixing network to utilize both in-puts, we train the networks with generated data via a novel set-up. We also utilize higher-rate features to prevent color inconsistencies between the inpainted and unerased parts.
We run extensive experiments and compare our method with state-of-the-art inversion and inpainting methods. Qualita-tive metrics and visual comparisons show significant im-provements. 1.

Introduction
Generative Adversarial Networks (GANs) achieve im-pressive results on unconditional [13, 40, 14, 37] and condi-tional image generation [31, 9, 18], inpainting [36, 16, 42], and image editing tasks [1, 4, 8]. Traditionally, each of these tasks has been explored with a dedicated network and training pipeline. However, recently it has been shown that high-quality image editing can be achieved with well-trained GAN models and especially by StyleGAN networks
[13, 14] that are trained to generate images without a con-dition [28, 30]. This approach achieves numerous edits via semantically rich feature representations of well-trained
*Joint first authors, contributed equally.
Figure 1. Our framework achieves diverse inpainting and editing with GAN inversion. Compared to other works that are proposed to invert and edit images with StyleGAN (HyperStyle [3], HFGI
[30], StyleRes [23]), our framework has advantages, as in the pre-sented example.
GANs [27, 11, 21, 4]. In this work, we are interested in tak-ing this direction one step further and learning an encoder for a pretrained GAN that can achieve high-quality image inversion, diverse inpainting, and editing under one frame-work.
To benefit from GAN’s image editing capabilities, exten-sive research is conducted on image inversion algorithms to find the corresponding latent codes that will generate a par-ticular real image [7, 25, 43, 28]. It is observed that there exists a trade-off between image reconstruction fidelity and image editing quality [28]. Studies show that low-rate la-tent spaces (z, W , or W + for StyleGANs) are limited in their expressiveness power, and not every image can be in-1
verted with high-fidelity reconstruction to GAN’s natural low-rate latent space [28]. When images are not projected to GAN’s natural latent space, even when an image is re-constructed with high fidelity, it will not be editable. Meth-ods are proposed to skip spatially higher resolution features (higher-rate latent codes) from an encoder to a GAN gener-ator for better reconstruction and editing properties [30, 3].
In this work, we are interested in a more challenging sce-nario; learning an image encoder that can project an erased image to a well-trained GAN’s natural latent space. This framework provides new capabilities for image editing, as shown in Fig. 1.
Recently, GAN inversion-based image inpainting meth-ods have been proposed [38, 32]. These methods optimize an image encoder and learn skip connections to the pre-trained StyleGAN model. Even though promising results are achieved, these methods model image inpainting in a deterministic way. They are trained to reconstruct the orig-inal image from the erased ones without any stochasticity.
They are not regularized to project the image into GAN’s natural latent space and do not enjoy the editing capabilities of them.
In this work, we propose a framework that achieves high-quality image inversion, diverse inpainting, and editing si-multaneously. We design an encoder architecture that takes an erased input image and encodes latent codes for the vis-ible parts. We also design a mixing network that combines randomly sampled latent codes with encoded ones. This setup allows the model to output diverse results. However, we find that the framework learns to ignore the randomly sampled latent codes and has the tendency to output deter-ministic results. To achieve diversity, we propose to train the framework with a novel design. Our design includes generating images with W + and erasing a part of it. Dur-ing training, sometimes the same W + that generated the input image is fed to the mixing network for the whole in-put reconstruction, and sometimes a newly sampled W + is used to reconstruct only the visible part. This way, the net-work is regularized to use both inputs; erased image and sampled W +. Secondly, to achieve high fidelity inversion of unerased pixels and to prevent the color discrepancy be-tween the unerased and erased parts, we learn higher dimen-sion latent codes. In summary, our main contributions are as follows:
• We propose a novel framework for image inpainting with
GAN inversion. Our framework includes an encoder to embed images and a mixing network to combine them with randomly sampled latent codes to achieve diversity.
The mixing network has a gating mechanism that im-proves the results.
• To achieve diversity, we propose a novel set-up to train the networks. We use GAN-generated images and train the network with full image reconstruction and valid pixel image reconstruction depending on if we feed the same latent code to the mixing network that is used to generate the input image or not.
• We conduct extensive experiments to show the effective-ness of our framework and achieve significant improve-ments over state-of-the-art models for image inpainting.
Additionally, we show our framework can achieve diverse inpainting and editing under one framework, as shown in
Fig. 1. 2.