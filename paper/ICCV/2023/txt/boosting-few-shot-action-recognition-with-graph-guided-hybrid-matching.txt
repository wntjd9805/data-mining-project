Abstract
Class prototype construction and matching are core as-pects of few-shot action recognition. Previous methods mainly focus on designing spatiotemporal relation model-ing modules or complex temporal alignment algorithms.
Despite the promising results, they ignored the value of class prototype construction and matching, leading to un-satisfactory performance in recognizing similar categories in every task.
In this paper, we propose GgHM, a new framework with Graph-guided Hybrid Matching. Con-cretely, we learn task-oriented features by the guidance of a graph neural network during class prototype construc-tion, optimizing the intra- and inter-class feature correla-tion explicitly. Next, we design a hybrid matching strategy, combining frame-level and tuple-level matching to classify videos with multivariate styles. We additionally propose a learnable dense temporal modeling module to enhance the video feature temporal representation to build a more solid foundation for the matching process. GgHM shows consistent improvements over other challenging baselines on several few-shot datasets, demonstrating the effective-ness of our method. The code will be publicly available at https://github.com/jiazheng-xing/GgHM. 1.

Introduction
Compared with general action recognition, few-shot ac-tion recognition requires limited labeled samples to learn new categories quickly.
It can avoid the massive, time-consuming, and labor-consuming data annotation com-monly associated with supervised tasks, making it more adaptable for industrial applications. According to this ad-vantage, increasing attention has been directed toward the field of few-shot action recognition [4, 27, 30, 35, 44, 14, 23, 38]. However, since few-shot action recognition has
*Equal Contribution.
†Corresponding author.
Figure 1. (a): Similarity visualization between query and sup-port videos with different methods on the 5-way 1-shot task of
UCF101 [29]. A higher score indicates a greater degree of simi-larity. TRX [27] misclassifies the drumming as the jumping jack, and OTAM [4] misidentifies the high jump as the long jump.
Our method identifies all categories of videos accurately. (b):
Different types of class prototype construction. Previous works did not do any information interaction among different videos.
HyRSM [35] operates an inter-relation function without leverag-ing label-informed supervision. Our method utilizes the graph network with label-informed supervision to learn the correlation between different videos. (c): Different types of class prototype matching. Frame-level matching [46, 4, 35] uses single individual frames for matching, while tuple-level [30, 38, 27] matching com-bines several frames into a tuple as the matching unit. Our method combines both to complement each other’s shortcomings.
limited learning material, learning well-generalized models are challenging.
Current attempts to address the above problems [4, 46, 41, 27, 30, 38, 35] mainly adopt the metric-based frame-work and episode training to solve the difficulty of model migration on new categories. Empirically, we observed that previous approaches failed to effectively address the prob-lem of misclassification of videos from similar categories.
Taking the action of the high jump and long jump as an instance, some methods (e.g., OTAM [4]) is easy to con-fuse the two classes by assigning close prediction scores due to their similarity in scenes and sub-actions, as shown in Fig. 1(a). We have analyzed the main reasons from three folds. (i) Class prototype construction: task-oriented class features can optimize videos’ intra- and inter-class corre-lation. As shown in Fig. 1(b), most previous work has yet to use the whole task video features to extract rele-vant discriminative patterns. Although HyRSM [35] ma-nipulates interrelationship functions on different videos to get task-specific embeddings, it does not explicitly optimize intra- and inter-class correlations. (ii) Matching mecha-nisms: proper matching mechanisms need to be established to solve the confusion problem of similar videos. As shown in Fig. 1(c), current work almost all use a simple class pro-totype matching mechanism. Some methods use the frame-level matching mechanism [46, 4, 35], which is suitable for spatial-related datasets [16, 29, 5], and the others use the tuple-level(multiple frames combined into a tuple) match-ing mechanism [30, 38, 27] that is appropriate for temporal-related datasets [13]. None of these previous methods can cope with video tasks of variable types well. (iii) Feature modeling: a powerful and highly discriminative feature is first needed to distinguish similar classes. Most previous works model the temporal feature through hand-designed temporal alignment algorithms [46, 4] or simple temporal attention operations [35, 38], leading to a simplistic explo-ration of the temporal relationship without dissecting it into more detailed patch and channel temporal relations to ana-lyze.
Based on the above observations, we propose a novel method for few-shot action recognition, dubbed GgHM, a short for Graph-guided Hybrid Matching. Specifically, we apply a graph neural network (GNN) for constructing task-oriented features, as shown in Fig.1(b).
It could interac-tively transfer information between video features in a task to enhance the prior knowledge of the unknown video. We utilize the ground truth of the constructed graph edges to explicitly learn the correlation of these video features to su-pervise the similarity score learning between the query and support videos. Second, as shown in Fig.1(c), we propose a hybrid prototype matching strategy that combines frame-level and tuple-level matching based on the bidirectional
Hausdorff Distance. Although the Hausdorff metric frame-level matching can alleviate the strictly ordered constraints of acquiring better query-support correspondences, it fails to capture temporal order. As a result, it can be confused for actions with similar action scenes strongly dependent on temporal order, e.g., putting something in the box and tak-ing something out of it. However, the construction of tuples strictly follows a chronological order, which can compen-sate for the frame-level matching problem. Fig.1(a) visu-alizes the predicted similarities between query and support videos with different methods on the 5-way 1-shot task of
UCF101 [29]. Our method achieves more discriminative re-sults for similar videos in each task compared to OTAM [4] and TRX [27]. Additionally, we design a learnable dense temporal modeling module to consolidate the representation foundation. It includes a temporal patch and temporal chan-nel relation modeling block, and their combination allows for dense temporal modeling in both spatial and channel do-mains. Finally, extensive experiments on four widely-used datasets demonstrate the effectiveness of our method.
In summary, we make the following contributions:
• We apply a graph neural network to guide the task-oriented features learning during the class prototype construction, explicitly optimizing the intra- and inter-class correlation within video features.
• We propose a hybrid class prototype matching strategy based on the frame- and tuple-level prototype match-ing, giving rise to effectively coping with video tasks of multivariate styles.
• We design a learnable dense temporal modeling mod-ule consisting of a temporal patch and temporal chan-nel relation modeling block for dense temporal model-ing in both spatial and channel domains. 2.