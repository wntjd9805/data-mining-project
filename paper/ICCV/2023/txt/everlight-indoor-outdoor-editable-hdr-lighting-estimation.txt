Abstract
Because of the diversity in lighting environments, ex-isting illumination estimation techniques have been de-signed explicitly on indoor or outdoor environments. Meth-ods have focused specifically on capturing accurate energy (e.g., through parametric lighting models), which empha-sizes shading and strong cast shadows; or producing plau-sible texture (e.g., with GANs), which prioritizes plausible reflections. Approaches which provide editable lighting ca-pabilities have been proposed, but these tend to be with simplified lighting models, offering limited realism. In this work, we propose to bridge the gap between these recent trends in the literature, and propose a method which com-bines a parametric light model with 360◦ panoramas, ready
*Research partly done when Mohammad Reza was an intern at Adobe. to use as HDRI in rendering engines. We leverage recent advances in GAN-based LDR panorama extrapolation from a regular image, which we extend to HDR using parametric spherical gaussians. To achieve this, we introduce a novel lighting co-modulation method that injects lighting-related features throughout the generator, tightly coupling the orig-inal or edited scene illumination within the panorama gen-eration process.
In our representation, users can easily edit light direction, intensity, number, etc. to impact shad-ing while providing rich, complex reflections while seam-lessly blending with the edits. Furthermore, our method en-compasses indoor and outdoor environments, demonstrat-ing state-of-the-art results even when compared to domain-specific methods.
1.

Introduction
The realistic blending of virtual assets in real imagery is required in many scenarios, ranging from special effects to augmented reality (AR) and advanced image editing. In this context, “getting the lighting right” is one of the key chal-lenges. Image-based lighting [7] can be employed to solve this problem, but it requires physical access to the scene and specialized equipment. In an attempt to automate this process, techniques that learn to predict lighting directly from captured imagery have been proposed. While earlier approaches relied on engineered features [26], they have since been replaced by learning-based techniques [21, 13].
Driven by the popularity of on-device AR applications, this line of research has recently attracted much attention, and several trends have emerged in the literature.
Perhaps the most popular trend, naturally, has been to de-velop richer, more expressive lighting representations to im-prove the accuracy of lighting estimations. The seemingly most popular representation is environment maps (equirect-angular images representing the entire field of view in 360◦) [13, 43, 29, 20, 42, 47], but others have been ex-plored as well, including spatially-varying spherical har-monics [14], parametric light sources [21, 60, 12], dense spherical Gaussians in 2D [30] or 3D [50], sparse spherical gaussians [57, 55], sparse needlets [56], multi-scale volume of implicit features [44], and full neural light fields [49].
Recently, hybrid approaches combining environment maps and a single parametric light have also been proposed [51].
Another identifiable trend has been to create domain-specific approaches to design the representation and/or ap-proach specifically for a target domain. The most com-mon way of defining a domain has been to explicitly consider indoor (e.g., [13, 14, 30, 50]) vs outdoor (e.g.,
[26, 21, 60, 20, 64]) domains. Of note, Legendre et al. [29] proposed what is perhaps the only method in the literature that works for both indoor and outdoor scenes.
A third, more recent trend are user-editable methods, where the goal is to employ or design a lighting represen-tation that can easily be understood and modified by a user.
For example, parametric models [21, 60, 12] represent the dominant light sources using intuitive parameters (e.g., po-sition, intensity, etc.) that can be interacted with easily, but fail to generate realistic reflections. Methods based on hy-brid models [51] or GAN inversion [47] have demonstrated promising results, but are either limited to a single light source [51] or employ a slow optimization process [47].
In this paper, we present a single, coherent framework that unifies these three main trends. Our approach, dubbed
EverLight, predicts a rich light representation in the form of a highly detailed 360◦ environment map; is domain-generic as it works on both indoor and outdoor scenes seamlessly; and is editable, as it estimates individual HDR light sources from the image which can synthesize both realistic shad-ing and reflections (see fig. 1). EverLight is, to the best of our knowledge, the first editable HDR lighting estima-tion technique that works on both indoor and outdoor scenes seamlessly. Our work bridges the gap between HDR para-metric lighting estimation and high-resolution field of view extrapolation by introducing a novel editable lighting co-modulation technique, which combines the flexibility and intuitiveness of parametric lighting models with the genera-tive power of GANs. Extensive experiments demonstrate that EverLight either compares favorably or outperforms indoor- and outdoor-specific approaches, both qualitatively and quantitatively. 2.