Abstract
Recent advances in neural reconstruction using posed image sequences have made remarkable progress. How-ever, due to the lack of depth information, existing volumetric-based techniques simply duplicate 2D image features of the object surface along the entire camera ray.
We contend this duplication introduces noise in empty and occluded spaces, posing challenges for producing high-quality 3D geometry. Drawing inspiration from tradi-tional multi-view stereo methods, we propose an end-to-end 3D neural reconstruction framework CVRecon, designed to exploit the rich geometric embedding in the cost vol-umes to facilitate 3D geometric feature learning. Further-more, we present Ray-contextual Compensated Cost Vol-ume (RCCV ), a novel 3D geometric feature representation that encodes view-dependent information with improved in-tegrity and robustness. Through comprehensive experi-ments, we demonstrate that our approach significantly im-proves the reconstruction quality in various metrics and re-covers clear fine details of the 3D geometries. Our exten-sive ablation studies provide insights into the development of effective 3D geometric feature learning schemes. Project page: https://cvrecon.ziyue.cool 1.

Introduction
Monocular 3D reconstruction is a fundamental task in computer vision with wide-ranging applications, including augmented reality [31, 24], virtual reality, robotics [25, 7], and autonomous driving[38, 10]. In recent years, learning-based methods [1, 21, 24, 29, 31] have shown promising results for this task. These methods use a sequence of posed images to predict a Truncated Signed Distance Field (TSDF) volume as the 3D reconstruction. They can be broadly categorized into two groups: volumetric-based and depth-based.
Existing volumetric-based methods [1, 21, 29, 31] suf-Figure 1. Novel 3D geometric feature learning paradigm. Exist-ing volumetric-based neural reconstruction methods simply back-project and duplicate the 2D image features along the entire cam-era ray, including empty spaces in front of the object and occluded areas behind the object, which will introduce noise and degrade the performance. We propose Ray-contextual Compensated Cost
Volume (RCCV ) as a novel 3D geometric feature representation.
Geometries could be inferred from a fusion of the RCCV s. fer from a critical limitation where the image feature is in the 2D while the reconstruction target is in 3D. As shown in Fig. 1, previous works simply duplicate the 2D features along the entire camera ray to the 3D space. When 2D image features representing object surfaces are placed in empty or occluded spaces without differentiation, it can complicate feature fusion and TSDF prediction in later stages, introduce noise and limit the model’s ability to pre-dict fine geometries. Therefore, it is more logical to directly build 3D feature representations that encode the geometry clue instead of simply filling it with 2D feature copies.
The cost volume, widely used in the depth prediction task, is a great choice for 3D geometric feature represen-tation. It is composed of multi-view feature matching con-fidences across a set of pre-defined depth planes. A higher feature matching confidence at a given position indicates a greater likelihood of that position being an object surface.
Compared to the simply duplicated 2D image features, cost volume explicitly encodes the depth probability distribution along the ray. However, existing depth-based reconstruction methods [3, 16, 18, 24, 33, 40] predict 2D depth maps from the 3D cost volume and then reconstruct the 3D structure
through the TSDF Fusion [22]. The 3D-2D-3D pipeline is not optimal because it doesn’t take into account global con-text and frame consistency, which can result in the loss of structural details or the introduction of floating artifacts dur-ing the 3D-2D transformation. In contrast, if the 3D feature representation of each keyframe is retained and fused for the final reconstruction, the model will be able to estimate the structure holistically and unleash a greater potential.
In this paper, we propose to directly construct a novel
Ray-contextual Compensated Cost Volume (RCCV ) as a 3D geometric feature representation, which is a multi-view cost volume with 2 contributions: (1) For a keyframe im-age, we argue that the feature matching cost at a single lo-cation is not sufficient for inferring the geometry. As shown in Fig 4, the confidence distribution along the entire cam-era ray is needed for reference: the highest matching con-fidence position within a camera ray is more likely to be the surface. Therefore, we devised a distribution feature for individual camera rays, which is subsequently integrated into each voxel along the said ray. (2) We observe the cost volume fails to encode any useful information in the non-overlapping regions and is excessively noisy in areas with low-contrasting textures as shown in Fig 3. Therefore, be-sides the feature matching confidence, the original 2D im-age feature is also fused to the cost volume and we name it
”Contextual Compensation”. With these two contributions, our proposed RCCV encodes comprehensive 3D geomet-ric information. The reconstruction result could be inferred from a fusion of our RCCV s from multiple keyframes. Ex-tensive experiments have demonstrated, as a generic repre-sentation, our RCCV is agnostic to the downstream fusion and prediction models, and provides a more effective 3D geometric feature learning scheme.
To summarize, Our contributions are as follows:
• We identify fundamental limitations of the existing feature learning scheme in the neural reconstruction field and accordingly propose to leverage the multi-view cost volume as a direct 3D geometric feature rep-resentation.
• We observe the widely-used standard cost volume lacks the distribution reference information along the camera ray and propose the Ray Compensation mech-anism to solve this problem.
• To improve the robustness of the cost volume in the non-overlapping and low-texture areas, we propose a novel Contextual Compensation module.
• Our extensive experiments show the effectiveness of our proposed RCCV , and its agnostic nature with downstream fusion and prediction models. 2.