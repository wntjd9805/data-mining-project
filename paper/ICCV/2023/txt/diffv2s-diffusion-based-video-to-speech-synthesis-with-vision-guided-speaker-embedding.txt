Abstract
Recent research has demonstrated impressive results in video-to-speech synthesis which involves reconstructing speech solely from visual input. However, previous works have struggled to accurately synthesize speech due to a lack of sufficient guidance for the model to infer the correct con-tent with the appropriate sound. To resolve the issue, they have adopted an extra speaker embedding as a speaking style guidance from a reference auditory information. Neverthe-less, it is not always possible to obtain the audio information from the corresponding video input, especially during the in-ference time. In this paper, we present a novel vision-guided speaker embedding extractor using a self-supervised pre-trained model and prompt tuning technique. In doing so, the rich speaker embedding information can be produced solely from input visual information, and the extra audio informa-tion is not necessary during the inference time. Using the extracted vision-guided speaker embedding representations, we further develop a diffusion-based video-to-speech synthe-sis model, so called DiffV2S, conditioned on those speaker embeddings and the visual representation extracted from the input video. The proposed DiffV2S not only maintains phoneme details contained in the input video frames, but also creates a highly intelligible mel-spectrogram in which the speaker identities of the multiple speakers are all pre-served. Our experimental results show that DiffV2S achieves the state-of-the-art performance compared to the previous video-to-speech synthesis technique. 1.

Introduction
Video-to-speech synthesis techniques [7, 15, 21, 22, 23, 33, 34, 39] have been broadly studied in lip-reading research areas. It reconstructs speech from a silent talking face video, which has an advantage of not requiring extra text informa-tion of the given video input during training. However, it is still regarded as a challenging task, especially in multi-*Both authors have contributed equally to this work. speaker and noisy environment settings, since the video-to-speech synthesis technique needs to capture the complex relationship between various lip movements and speech.
The relationship between lip movements and speech is not always straightforward; there is considerable variation in how different people articulate sounds, as well as how their lip movements are affected by factors such as facial expressions, accents, and noise. To resolve the complicated factors that speakers themselves contain, several recent stud-ies [7, 23, 33, 39] have utilized extra speaker embedding representations obtained from the original audio information of the video input with the same speaker. The speaker embed-dings are helpful for obtaining the speaker’s characteristics, where those characteristics cannot be directly derived from silent talking face video. However, directly manipulating the reference audio information during the inference is not always possible as the audio information is sometimes unob-tainable because of noisy environments, absence of speech, and unseen speakers during the inference time.
To alleviate the aforementioned issue, we present a novel vision-guided speaker embedding extractor using a self-supervised pre-trained model. With the largely trained audio-visual speech representation model [40], we adopt a prompt tuning technique [28] to train the certain part of the model in order to extract the appropriate speaker embedding features from the input video sequences. We set only a small amount of downstream task-specific parameters as the learnable pa-rameters for extracting speaker embedding into the input space while freezing the other parts of the pre-trained model.
By doing so, the rich speaker embedding information can be produced from solely on input visual information, and the extra audio information is not necessary during the inference time period.
Furthermore, we propose a conditional diffusion-based video-to-speech synthesis model, named DiffV2S, using the vision-guided speaker embedding representations. As the denoising diffusion model has been proven to be effec-tive in generating semantically meaningful representation in both image and audio processing [18, 32, 43], we also newly adopt the diffusion model to achieve high-quality mel-spectrogram containing semantically detailed information.
The proposed DiffV2S is comprised of conditional diffusion modeling and sampling with speaker embedding guidance.
During training, the proposed DiffV2S reconstructs a mel-spectrogram from a standard Gaussian distribution with the condition of speaker embedding representations concate-nated with the visual features extracted from the input silent talking face video. During sampling, the speaker character-istics are driven to enable the model to properly adopt the speaker’s style, such as voice and accent, while maintaining the articulate phoeneme details faithfully. Therefore, our model not only maintains phoneme details contained in the input video frames, but also creates a noise-free and highly intelligible mel-spectrogram in which the speaker identity characteristics are entirely preserved.
To validate the effectiveness of the proposed method, we utilize LRS2 [9] and LRS3 [1], the largest sentence-level audio-visual datasets obtained in the wild. Through comprehensive experiments, we show that the generated speech from the proposed DiffV2S contains much detailed contents, thus producing noise-free audio waveform with high performances.
Our key contributions are as follows:
• We propose a vision-guided speaker embedding extrac-tor, so the rich speaker information can be produced solely from the input video frames. In doing so, the audio information is not necessary during the inference time.
• We present the novel diffusion-based video-to-speech synthesis model, DiffV2S, conditioned on the speaker embedding representations and the visual representa-tion extracted from the input talking face video. The
DiffV2S not only maintains phoneme details contained in the input video frames, but also creates a highly intel-ligible mel-spectrogram in which the speaker identities are all preserved.
• To best of our knowledge, this is first time to utilize the diffusion model in video-to-speech synthesis. The generated speech from the proposed DiffV2S contains much detailed information, thus producing noise-free audio waveform with high performances. 2.