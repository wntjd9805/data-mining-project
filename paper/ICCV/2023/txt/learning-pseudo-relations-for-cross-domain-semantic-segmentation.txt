Abstract
Domain adaptive semantic segmentation aims to adapt a model trained on labeled source domain to unlabeled tar-get domain. Self-training shows competitive potential in this ﬁeld. Existing methods along this stream mainly focus on selecting reliable predictions on target data as pseudo-labels for category learning, while ignoring the useful re-lations between pixels for relation learning. In this paper, we propose a pseudo-relation learning framework, Relation
Teacher (RTea), which can exploitable pixel relations to ef-ﬁciently use unreliable pixels and learn generalized repre-sentations. In this framework, we build reasonable pseudo-relations on local grids and fuse them with low-level rela-tions in the image space, which are motivated by the reli-able local relations prior and available low-level relations prior. Then, we design a pseudo-relation learning strat-egy and optimize the class probability to meet the relation consistency by ﬁnding the optimal sub-graph division. In this way, the model’s certainty and consistency of predic-tion are enhanced on the target domain, and the cross-domain inadaptation is further eliminated. Extensive ex-periments on three datasets demonstrate the effectiveness of the proposed method. The code will be available at https://github.com/DZhaoXd/RTea. 1.

Introduction
Semantic segmentation is a challenging problem of as-signing each pixel a class label in an image. Driven by deep neural networks, signiﬁcant progress has been made in this
ﬁeld. Despite these efforts, a segmentation model trained with a speciﬁc domain does not generalize well to other do-mains. It is known to be caused by the domain gap between the training (source) and testing (target) domains [13]. To
This work is supported by the National Key R&D Program of China under Grant No. 2021ZD0110400, the National Natural Science Founda-tion of China(No.62271377, No.62201407), the Key Research and Devel-opment Program of Shannxi (No.2021ZDLGY01-06, No.2022ZDLGY01-12), the China Postdoctoral Science Foundation (No. 2022M722496), the Foreign Scholars in University Research and Teaching Program’s 111
Project (B07048).
Figure 1: Overview of our motivation.
In A, previous self-training methods select reliable pseudo-labels for cate-gory learning. In B, our method reasonably utilizes reliable pseudo-labels and unreliable ones for relation learning. solve this problem, unsupervised domain adaptation (UDA) is proposed to improve the segmentation model’s adaptabil-ity to the target domain.
Domain alignment is one of the mainstream UDA se-mantic segmentation methods, aiming to align the distribu-tion of source and target domains in input [30, 60, 25, 4], feature [53, 32, 19, 22, 35, 65, 50, 56], or output spaces
[48, 33, 49]. Works along this line achieve positive adapta-tion beneﬁts but the lack of speciﬁc target domain knowl-edge leads to slight improvement [39, 68, 66].
To this end, self-training methods [27, 10, 1, 69, 64, 67] are proposed to mine target-speciﬁc knowledge. These methods use the pseudo-labels generated by the pre-adapted model to further train the model on the target domain.
Consequently, the quality of pseudo-labels for training di-rectly determines the performance of self-training. Fol-lowing this key point, reliability measure-based and uncer-tainty estimation-based self-training methods are proposed
[71, 27, 69, 10, 1, 64, 52]. These methods reduce the noise interference of pseudo-labels for category learning, bring-ing considerable performance improvement.
In this paper, we explore the potential of self-training from another perspective, as shown in Fig. 1. In Fig. 1
A, previous self-training methods perform category learn-ing on reliable pseudo-labels and discard unreliable ones.
However, we ﬁnd that not only category learning but re-lation learning can be performed in pseudo-labels to further improve the adaptability of the model. In Fig. 1 B, we argue that relation learning in pseudo-labels can be performed in two ways: 1(cid:13) The relations between reliable pseudo-labels can be additionally used for representation learning to build a more generalized representation space. 2(cid:13) By establish-ing the relations between reliable and unreliable pseudo-labels, discarded pixels can also be effectively used for self-training to increase the certainty of the model. In this way, the available knowledge contained in pseudo-labels can be fully exploited, both reliable and unreliable pixels.
To achieve the above goals, building reasonable relations between pixels is the core. Dense pixel relations can be rep-resented by a relation matrix (or afﬁnity matrix) [8, 20, 29], modeling the similarity between pixels on an image. Due to massive noise contained in the pseudo-labels, the relation matrix constructed by them also contain noisy relations. We explore two observational priors to guide the building of re-lations, as shown in Fig. 2. We use the predictions of the unadapted model for the target images to observe noise dis-tribution of relations. Comparing Fig. 2 (e) and (f), we observe that high-level relations built by pseudo-labels are noisy in long-distance association but are reliable within lo-cal areas, which are termed as reliable local relations prior.
We analyze this because the insufﬁciently adapted model cannot transfer global semantics and can only give reason-able relations in local regions. Besides, in Fig. 2(d), we further explore the low-level relations built on each local grid in image space using Gaussian kernel. We ﬁnd that the low-level relations in local grids can capture the bound-aries of objects and contain exploitable relations, which are termed as available low-level relations prior. We argue that such relations, although lacking in semantics, provides class boundary clues can be reasonably exploited.
With these aspects in mind, we propose a pseudo-relation learning-based self-training framework, Relation
Teacher (RTea), forcing the student model to learn the pseudo-relations between pixels from the teacher model and achieve co-evolution for both models. In this framework, with the guidance of the above two priors, we ﬁrst use pseudo-labels to build high-level relations in each divided grid, which avoids being misled by long-distance relation-ships. Then, we fuse low-level relations in image space into high-level relations to attenuate noisy relations and assist semantic relations in identifying category boundaries. Next, we explore the way of learning pseudo-relations and de-vise a novel pseudo-relations loss, which optimizes the class
Figure 2: Overview of two observational priors. (b) is the source-only model’s prediction, (d) is the local relation map in image space built by Gaussian kernel in each grid, (e) is the pseudo-relation matrix. (f) is the relation matrix built by ground truth. probability to meet the relation consistency by ﬁnding the optimal sub-graph division from the global pseudo-relation of each class. It has two advantages over the naive relation-learning loss: one is global relational modeling, which can be easily implemented by matrix multiplication; the other is threshold-free learning by dynamically weighting class probabilities and pseudo-relations.
With RTea, the model’s certainty and consistency of pre-diction can be enhanced on the target domain, and the cross-domain inadaptation is further eliminated. Sufﬁcient ex-periments show our method can further mine the available knowledge in pseudo-labels, and it can be easily incorpo-rated into existing self-training method to further boost their performance. 2.