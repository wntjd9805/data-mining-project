Abstract
Video matting has broad applications, from adding in-teresting effects to casually captured movies to assisting video production professionals. Matting with associated effects such as shadows and reflections has also attracted increasing research activity, and methods like Omnimatte have been proposed to separate dynamic foreground ob-jects of interest into their own layers. However, prior works represent video backgrounds as 2D image layers, limiting their capacity to express more complicated scenes, thus hin-dering application to real-world videos. In this paper, we propose a novel video matting method, OmnimatteRF, that combines dynamic 2D foreground layers and a 3D back-ground model. The 2D layers preserve the details of the sub-jects, while the 3D background robustly reconstructs scenes in real-world videos. Extensive experiments demonstrate that our method reconstructs scenes with better quality on various videos. 1.

Introduction
Video matting is the problem of separating a video into multiple layers with associated alpha mattes such that the layers are composited back to the original video. It has a wide variety of applications in video editing as it allows for substituting layers or processing them individually be-fore compositing back, and thus has been studied well over decades. In typical applications like rotoscoping in video production and background blurring in online meetings, the goal is to obtain the masks containing only the object of interest.
In many cases, however, it is often preferred to be able to create video mattes that include not only the ob-ject of interest but also its associated effects, like shadow and reflections. This could reduce the often-required, ad-ditional manual segmentation of secondary effects and help increase realism in the resulting edited video. Being able to factor out the related effects of foreground objects also helps reconstruct a clean background, which is preferred in (a) Omnimatte BG (b) Omnimatte FG (c) Our BG (d) Our FG
Figure 1. Video with parallax effects. Limited by their 2D im-age representation (a), previous works such as Omnimatte fail to handle videos with parallax effects in the background. Their fore-ground layer (b) has to capture (dis)occlusion effects to minimize the reconstruction loss.
In contrast, our method employs a 3D background (c), enabling us to obtain clean foreground layers (d). applications like object removal. Despite these benefits, this problem is much more ill-posed and has been much less ex-plored than the conventional matting problem.
The most promising attempt to tackle this problem is
Omnimatte [21]. Omnimattes are RGBA layers that cap-ture dynamic foreground objects and their associated ef-fects. Given a video and one or more coarse mask videos, each corresponding to a foreground object of interest, the method reconstructs an omnimatte for each object, in addi-tion to a static background that is free from all of the ob-jects of interest and their associated effects. While Om-nimatte [21] works well for many videos, it is limited by its use of homography to model backgrounds, which re-quires the background be planar or the video contains only rotational motion. This is not the case as long as there ex-ists parallax caused by camera motions and objects occlude each other. This limitation hinders its application in many real-world videos, as shown in Fig. 1.
D2NeRF [36] attempts to address this issue using two
radiance fields, which model the dynamic and static part of the scene. The method works entirely in 3D and can han-dle complicated scenes with significant camera motion. It is also self-supervised in the sense that no mask input is necessary. However, it separates all moving objects from a static background and it is not clear how to incorporate 2D guidance defined on video such as rough masks. Further, it cannot independently model multiple foreground objects. A simple solution of modeling each foreground object with a separate radiance field could lead to excessive training time, yet it is not clear how motions could be separated meaning-fully in each radiance field.
We propose a method that has the benefit of both by combining 2D foreground layers with a 3D background model. The lightweight 2D foreground layers can repre-sent multiple object layers, including complicated objects, motions, and effects that may be challenging to be mod-eled in 3D. At the same time, modeling background in 3D enables handling background of complex geometry and non-rotational camera motions, allowing for processing a broader set of videos than 2D methods. We call this method
OmnimatteRF and show in experiments that it works ro-bustly on various videos without per-video parameter tun-ing. To quantitatively evaluate the background separation of a 3D scene, D2NeRF released a dataset of 5 videos ren-dered with Kubrics, which are simple indoor scenes with few pieces of furniture and some moving objects that cast solid shadows. We also render five videos from open-source
Blender movies [6] with sophisticated motions and lighting conditions for more realistic and challenging settings. Our method outperforms prior works in both datasets, and we release the videos to facilitate future research.
In summary, our contributions include the following: 1. We propose a novel method to make Omnimatte [21] more robust by better modeling the static background in 3D using radiance fields [22]. 2. Utilizing the omnimatte masks, we propose a simple yet effective re-training step to obtain a clean static 3D reconstruction from videos with moving subjects. 3. We release a new dataset of 5 challenging video se-quences rendered from open-source blender movies
[6] with ground truths to better facilitate the develop-ment and evaluation of the video matting with associ-ated effects (aka omnimatting [21]) problem. 2.