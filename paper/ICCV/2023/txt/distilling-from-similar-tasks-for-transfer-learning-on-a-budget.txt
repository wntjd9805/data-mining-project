Abstract
We address the challenge of getting efficient yet accu-rate recognition systems with limited labels. While recog-nition models improve with model size and amount of data, many specialized applications of computer vision have se-vere resource constraints both during training and inference.
Transfer learning is an effective solution for training with few labels, however often at the expense of a computation-ally costly fine-tuning of large base models. We propose to mitigate this unpleasant trade-off between compute and accuracy via semi-supervised cross-domain distillation from a set of diverse source models. Initially, we show how to use task similarity metrics to select a single suitable source model to distill from, and that a good selection process is imperative for good downstream performance of a target model. We dub this approach DISTILLNEAREST. Though effective, DISTILLNEAREST assumes a single source model matches the target task, which is not always the case. To al-leviate this, we propose a weighted multi-source distillation method to distill multiple source models trained on different domains weighted by their relevance for the target task into a single efficient model (named DISTILLWEIGHTED). Our methods need no access to source data and merely need features and pseudo-labels of the source models. When the goal is accurate recognition under computational con-straints, both DISTILLNEAREST and DISTILLWEIGHTED approaches outperform both transfer learning from strong
ImageNet initializations as well as state-of-the-art semi-supervised techniques such as FixMatch. Averaged over 8 diverse target tasks our multi-source method outperforms the baselines by 5.6%-points and 4.5%-points, respectively.
Code: github.com/Kennethborup/DistillWeighted 1.

Introduction
Recognition models get more accurate the larger they are and the more data they are trained on [21, 36, 45]. This is a problem for many applications of interest in medicine (e.g.
X-ray analysis) or science (e.g. satellite-image analysis) where both labeled training data, as well as computational
Figure 1: Average test accuracy over five target tasks with different methods for weighting source models for distilla-tion. Our methods outperform the baselines and transfer learning from ImageNet. See Section 5.3 for details. resources needed to train such large models, are lacking.
The challenge of limited labeled data can potentially be alleviated by fine-tuning large-scale “foundation models”
[13, 21, 45]. However, fine-tuning is computationally ex-pensive, especially when one looks at foundation models with billions of parameters [13]. Unfortunately, all evidence suggests that larger foundation models perform better at fine-tuning [21, 45]. This leaves downstream applications the unpleasant trade-off of expensive computational hard-ware for fine-tuning large models, or inaccurate results from smaller models. Motivated by this challenge, we ask can we train accurate models on tight data and compute budgets without fine-tuning large foundation models?
To set the scene, we assume the existence of a diverse set (both in architecture and task) of pre-trained source models (or foundation models). We do not have the resources to fine-tune these models, but we assume we can perform inference on these models and extract features, e.g. through APIs on cloud services [8, 34]. For the target task, we assume that labeled data is very limited, but unlabeled data is available.
We then propose a simple and effective strategy for building an accurate model for the target task: DISTILLNEAREST.
Concretely, we first compute a measure of “task similarity” between our target task and each source model and rank the source models accordingly. Then we pseudo-label the unlabeled data using the most similar source model. These pseudo-labels may not even be in the same label space as the target task, but we conjecture that due to the similarity between the source and target tasks, the pseudo-labels will still group the target data points in a task-relevant manner.
Finally, we train the target model using the pseudo-labels and the available ground truth labeled data. This allows us to bypass the large computations required to fine-tune source models and directly work on the target model. At the same time, we get to effectively use the knowledge of the large source model even if it is trained on a different task.
DISTILLNEAREST assumes that a single best source model exists. But for some target tasks, we might need to combine multiple source models to achieve a sufficiently diverse representation to distill. We, therefore, propose an extension of our approach that distills multiple (diverse) source models trained on different domains, weighted by their relevance for the target task. This extension obtains even further improvements on our target performance (see
Figure 1). We dub this method DISTILLWEIGHTED.
We summarize our contributions as follows:
• We train more than 200 models across a diverse set of source and target tasks using single-source distillation, and extensively show that the choice of source model is imperative for the predictive performance of the target model. To the best of our knowledge, no previous work has addressed how to efficiently select a teacher model for (cross-domain) distillation.
• We find that task similarity metrics correlate well with predictive performance and can be used to efficiently select and weight source models for single- and multi-source distillation without access to any source data.
• We show that our approaches yield the best accuracy on multiple target tasks under compute and data con-straints. We compare our DISTILLNEAREST and DIS-TILLWEIGHTED methods to two baselines (transfer learn-ing and FixMatch), as well as the na¨ıve case of DIS-TILLWEIGHTED with equal weighting (called DISTILLE-QUAL), among others. Averaged over 8 diverse datasets, our DISTILLWEIGHTED outperforms the baselines with at least 4.5% and in particular 17.5% on CUB200. 2.