Abstract
High-resolution dense prediction enables many appeal-ing real-world applications, such as computational pho-tography, autonomous driving, etc. However, the vast computational cost makes deploying state-of-the-art high-resolution dense prediction models on hardware devices difficult. This work presents EfficientViT, a new family of high-resolution vision models with novel lightweight multi-scale attention. Unlike prior high-resolution dense pre-diction models that rely on heavy self-attention, hardware-inefficient large-kernel convolution, or complicated topol-ogy structure to obtain good performances, our lightweight multi-scale attention achieves a global receptive field and multi-scale learning (two critical features for high-resolution dense prediction) with only lightweight and hardware-efficient operations. As such, EfficientViT deliv-ers remarkable performance gains over previous state-of-the-art high-resolution dense prediction models with signifi-cant speedup on diverse hardware platforms, including mo-bile CPU, edge GPU, and cloud GPU. Without performance loss on Cityscapes, our EfficientViT provides up to 8.8× and 3.8× GPU latency reduction over SegFormer and SegNeXt, respectively. For super-resolution, EfficientViT provides up to 6.4× speedup over Restormer while providing 0.11dB gain in PSNR. 1.

Introduction
High-resolution dense prediction is a fundamental task in computer vision and has broad applications in real-world scenarios, including autonomous driving, medical image processing, computational photography, etc. Therefore, de-ploying state-of-the-art (SOTA) high-resolution dense pre-diction models on hardware devices can benefit many use cases.
However, there is a large gap between the computational cost required by SOTA high-resolution dense prediction models and the limited resources of hardware devices. It makes using these models in real-world applications im-practical.
In particular, high-resolution dense prediction models require high-resolution images and strong context information extraction ability to work well [2, 41, 53, 58, 54, 47]. Therefore, directly porting efficient model archi-tecture from image classification is unsuitable for high-resolution dense prediction.
This work introduces EfficientViT, a new family of models for efficient high-resolution dense prediction. The core of EfficientViT is a novel lightweight multi-scale atten-tion module that enables a global receptive field and multi-scale learning with hardware-efficient operations. Our mod-ule is motivated by prior SOTA high-resolution dense pre-diction models. They demonstrate that the multi-scale learning [53, 58], and global receptive field [51] play a crit-ical role in improving the performances. However, they do not consider hardware efficiency when designing their models, which is essential for real-world applications. For example, SegFormer [51] introduces self-attention into the backbone to have a global receptive field. But its computa-tional complexity is quadratic to the input resolution, mak-ing it unable to handle high-resolution images efficiently.
SegNeXt [18] proposes a multi-branch module with large-kernel convolutions (kernel size up to 21) to enable a large receptive field and multi-scale learning. However, large-kernel convolution requires exceptional support on hard-ware to achieve good efficiency [16, 50], which is usually not available on hardware devices.
Hence, the design principle of our module is to en-able these two critical features while avoiding hardware-inefficient operations. Specifically, to have a global re-ceptive field, we propose substituting the inefficient self-attention with lightweight ReLU-based global attention
[28]. By leveraging the associative property of matrix multiplication, ReLU-based global attention can reduce the computational complexity from quadratic to linear while preserving functionality.
In addition, it avoids hardware-inefficient operations like softmax, making it more suitable for hardware deployment (Figure 3).
Furthermore, we propose a novel lightweight multi-scale
Figure 1: Latency vs. Performance. All performance results are obtained with the single model and single-scale inference.
The mobile latency results are obtained on the Qualcomm Snapdragon 8Gen1 (S-8Gen1) CPU using Tensorflow-Lite, batch size 1, and fp32. The GPU latency results are obtained on two edge GPUs (Jetson Nano, Jetson AGX Orin) and one cloud
GPU (A100) using TensorRT, batch size 1, and fp16. EfficientViT consistently achieves a remarkable boost in speed on diverse hardware platforms while providing the same/higher performances on Cityscapes and ADE20K than state-of-the-art (SOTA) segmentation models.
Table 1: Desirable Features for Efficient High-Resolution Dense Prediction. ‘Linear computational complexity’ means the computational cost grows linearly as the input resolution increases.
Features
SegFormer [51] HRFormer [55] SegNeXt [18] EfficientViT
Global receptive field
Multi-scale learning
Linear computational complexity
Hardware efficiency
✓
✓
✓
✓
✓
✓
✓
✓
✓ attention module based on the ReLU-based global attention.
Specifically, we aggregate nearby tokens with small-kernel convolutions to generate multi-scale tokens and perform
ReLU-based global attention on multi-scale tokens (Fig-ure 2) to combine the global receptive field with multi-scale learning. We summarize the comparison between our work and prior SOTA high-resolution dense prediction models in
Table 1.
We extensively evaluate EfficientViT on two popular high-resolution dense prediction tasks: semantic segmenta-tion and super-resolution. EfficientViT provides significant performance boosts over prior SOTA high-resolution dense prediction models. More importantly, EfficientViT does not involve hardware-inefficient operations, so our FLOPs re-duction can easily translate to latency reduction on hard-ware devices (Figure 1). We summarize our contributions as follows:
• We introduce a novel lightweight multi-scale attention for efficient high-resolution dense prediction. It achieves a global receptive field and multi-scale learning while maintaining good efficiency on hardware.
• We design EfficientViT, a new family of high-resolution vision models, based on the proposed lightweight multi-scale attention module.
• On semantic segmentation, super-resolution and Ima-geNet classification, our model demonstrates remarkable speedup on diverse hardware platforms (mobile CPU, edge GPU, and cloud GPU) over prior SOTA models.
Figure 2: Illustration of EfficientViT’s Building Block (left) and the Lightweight Multi-Scale Attention (right). Left:
A building block of EfficientViT consists of a lightweight MSA module and an MBConv. The lightweight MSA module is responsible for capturing context information, while the MBConv is for capturing local information. Right: After getting
Q/K/V tokens via the linear projection layer, we generate multi-scale tokens by aggregating nearby tokens via lightweight small-kernel convolutions. ReLU-based global attention is applied to multi-scale tokens, and the outputs are concatenated and fed to the final linear projection layer for feature fusing. 2.1. Lightweight Multi-Scale Attention
Our lightweight MSA module balances two crucial as-pects of efficient high-resolution dense prediction, i.e., per-formance and efficiency. Specifically, a global receptive field and multi-scale learning are essential from the perfor-mance perspective. Previous SOTA high-resolution dense prediction models provide strong performances by enabling these features but fail to provide good efficiency. Our mod-ule tackles this issue by trading slight capacity loss for sig-nificant efficiency improvements.
An illustration of the proposed lightweight MSA mod-ule is provided in Figure 2 (right). In particular, we pro-pose to use lightweight ReLU-based attention [28] to enable the global receptive field instead of the heavy self-attention
[46]. While ReLU-based attention [28] and other linear at-tention modules [3, 12, 43, 48] has been explored in other domains, it has never been successfully applied to high-resolution dense prediction. To the best of our knowledge,
EfficientViT is the first work demonstrating ReLU-based at-tention’s effectiveness in high-resolution dense prediction.
In addition, our work introduces novel designs (lightweight
MSA module) to enhance the capacity, making it more pow-erful in high-resolution dense prediction.
Enabling Global Receptive Field with Lightweight
ReLU-based Attention. Given input x ∈ RN ×f , the gen-eralized form of self-attention can be written as:
Oi =
N (cid:88) j=1
Sim(Qi, Kj) j=1 Sim(Qi, Kj) (cid:80)N
Vj, (1)
Figure 3: Latency Comparison Between Softmax Atten-tion and ReLU-Based Linear Attention. ReLU-based linear attention is 3.3-4.5× faster than softmax attention with similar computation, thanks to removing hardware-unfriendly operations (e.g., softmax). Latency is measured on Qualcomm Snapdragon 855 CPU with TensorFlow-Lite, batch size 1, and fp32. 2. Method
This section first introduces lightweight Multi-Scale At-tention (MSA). Unlike prior works, our lightweight MSA module simultaneously achieves a global receptive field and multi-scale learning with only hardware-efficient op-erations. Then, based on the proposed MSA module, we present a new family of vision models named EfficientViT for high-resolution dense prediction.
Figure 4: Macro Architecture of EfficientViT. We adopt the standard backbone-head/encoder-decoder design. In the back-bone, we insert our EfficientViT modules in Stages 3 & 4. Following the common practice, we feed the features from the last three stages (P2, P3, and P4) to the head. We use addition to fuse these features for simplicity and efficiency. We adopt a simple head design that consists of several MBConv blocks and output layers. memory footprint from quadratic to linear without changing the functionality:
Figure 5: Illustration of the Aggregation Process for
Generating Multi-Scale Tokens. The information aggre-gation is done independently for each Q, K, and V in each head. ‘d’ denotes the dimension of each token. The typical value of d is 32. where Q = xWQ, K = xWK, V = xWV and
WQ/WK/WV ∈ Rf ×d is the learnable linear projection matrix. Oi represents the i-th row of matrix O. Sim(·, ·) is the similarity function. When using the similarity function
Sim(Q, K) = exp( QKT
), Eq. (1) becomes the original d self-attention [46].
Apart from exp( QKT d
), we can use other similarity func-In this work, we use ReLU-based global attention tions.
[28] to achieve both the global receptive field and linear computational complexity. In ReLU-based global attention, the similarity function is defined as
√
√
Sim(Q, K) = ReLU(Q)ReLU(K)T . (2)
With Sim(Q, K) = ReLU(Q)ReLU(K)T , Eq. (1) can be rewritten as:
Oi =
=
N (cid:88) j=1 (cid:80)N (cid:80)N
ReLU(Qi)ReLU(Kj)T j=1 ReLU(Qi)ReLU(Kj)T j=1(ReLU(Qi)ReLU(Kj)T )Vj
ReLU(Qi) (cid:80)N j=1 ReLU(Kj)T
Vj
.
Then, we can leverage the associative property of matrix multiplication to reduce the computational complexity and
Oi =
=
= (cid:80)N j=1 [ReLU(Qi)ReLU(Kj)T ]Vj
ReLU(Qi) (cid:80)N j=1 ReLU(Qi)[(ReLU(Kj)T Vj)]
ReLU(Qi) (cid:80)N j=1 ReLU(Kj)T (cid:80)N
ReLU(Qi)((cid:80)N
ReLU(Qi)((cid:80)N j=1 ReLU(Kj)T j=1 ReLU(Kj)T Vj) j=1 ReLU(Kj)T )
. (3) shown in Eq. j=1 ReLU(Kj)T Vj) ∈ Rd×d and ((cid:80)N (3), we only need to compute
As ((cid:80)N j=1 ReLU(Kj)T )
∈ Rd×1 once, then can reuse them for each query, thereby only requires O(N ) computational cost and O(N ) memory.
Another key merit of ReLU-based global attention is that it does not involve hardware-unfriendly operations like soft-max, making it more efficient on hardware. For example,
Figure 3 shows the latency comparison between softmax at-tention and ReLU-based linear attention. With similar com-putation, ReLU-based linear attention is significantly faster than softmax attention on mobile.
Generate Multi-Scale Tokens. ReLU-based attention alone has limited model capacity. To mitigate this limi-tation, we first enhance it with the depthwise convolution to improve its local information extraction ability (Figure 2 left). In addition, to enhance its multi-scale learning abil-ity, we propose to aggregate the information from nearby
Q/K/V tokens to get multi-scale tokens. The aggregation process is illustrated in Figure 5. This information aggre-gation process is independent for each Q, K, and V in each head. We only use small-kernel convolutions for informa-tion aggregation to avoid hurting hardware efficiency.
In the practical implementation, independently executing these aggregation operations is inefficient on GPU. There-fore, we take advantage of the infrastructure of group con-volution in modern deep learning frameworks to reduce the
Table 2: Detailed Architecture Configurations of Different EfficientViT Variants. We build a series of models to fit different efficiency constraints. ‘C’ denotes the number of channels. ‘L’ denotes the number of blocks. ‘H’ is the height of the feature map, and ‘W’ is the width of the feature map.
Variants
Feature Map Shape
EfficientViT-B0
EfficientViT-B1
EfficientViT-B2
EfficientViT-B3
Input Stem
Stage1
Stage2
Stage3
Stage4
Head
C × H
C × H
C × H
C × H
C × H
C × H 8 4 2 2 × W 4 × W 8 × W 16 × W 32 × W 8 × W 8 16 32
C = 8, L = 1
C = 16, L = 1
C = 24, L = 1
C = 32, L = 1
C = 16, L = 2
C = 32, L = 2
C = 48, L = 3
C = 64, L = 4
C = 32, L = 2
C = 64, L = 3
C = 96, L = 4
C = 128, L = 6
C = 64, L = 2
C = 128, L = 3
C = 192, L = 4
C = 256, L = 6
C = 128, L = 2
C = 256, L = 4
C = 384, L = 6
C = 512, L = 9
C = 32, L = 1
C = 64, L = 3
C = 96, L = 3
C = 128, L = 3 number of total operations. Specifically, all DWConvs are fused into a single DWConv while all 1x1 Convs are com-bined into a single 1x1 group convolution (Figure 2 right) where the number of groups is 3 × #heads and the number of channels in each group is d.
After getting multi-scale tokens, we perform global at-tention upon them to extract multi-scale global features.
Finally, we concatenate the features from different scales along the head dimension and feed them to the final linear projection layer to fuse the features. 2.2. EfficientViT Architecture
We build a new family of vision models based on the pro-posed lightweight MSA module. The core building block (denoted as ‘EfficientViT Module’) is illustrated in Fig-ure 2 (left). Specifically, an EfficientViT module comprises a lightweight MSA module and an MBConv [42]. The lightweight MSA module is for context information extrac-tion, while the MBConv is for local information extraction.
The macro architecture of EfficientViT is demonstrated in Figure 4. We use the standard backbone-head/encoder-decoder architecture design.
• Backbone. The backbone of EfficientViT also follows the standard design, which consists of the input stem and four stages with gradually decreased feature map size and gradually increased channel number. We insert the Effi-cientViT module in Stages 3 and 4. For downsampling, we use an MBConv with stride 2.
• Head. P2, P3, and P4 denote the outputs of Stages 2, 3, and 4, forming a pyramid of feature maps. For simplic-ity and efficiency, we use 1x1 convolution and standard upsampling operation (e.g., bilinear/bicubic upsampling) to match their spatial and channel size and fuse them via addition. Since our backbone already has a strong con-text information extraction capacity, we adopt a simple head design that comprises several MBConv blocks and the output layers (i.e., prediction and upsample). In the
Table 3: Ablation Study on Two Key Components of Our
EfficientViT Module. The mIoU and MACs are measured on Cityscapes with 1024x2048 input resolution. We rescale the width of the models so that they have the same MACs.
Multi-scale learning and the global receptive field are essen-tial for obtaining good semantic segmentation performance.
Components
Multi-scale Global att. mIoU ↑
Params ↓ MACs ↓
✓
✓
✓
✓ 68.1 72.3 72.2 74.5 0.7M 0.7M 0.7M 0.7M 4.4G 4.4G 4.4G 4.4G experiments, we empirically find this simple head design is sufficient for achieving SOTA performances thanks to our lightweight MSA module.
In addition to dense prediction, our model can be ap-plied to other vision tasks, such as image classification, by combining the backbone with task-specific heads.
Following the same macro architecture, we design a se-ries of models with different sizes to satisfy various effi-ciency constraints. The detailed configurations are demon-strated in Table 2. We name these models as EfficientViT-B0, EfficientViT-B1, EfficientViT-B2, and EfficientViT-B3, respectively. 3. Experiments 3.1. Setups
Datasets. We evaluate the effectiveness of EfficientViT on two representative high-resolution dense predic-tion tasks, including semantic segmentation and super-resolution.
For semantic segmentation, we use two popular bench-mark datasets: Cityscapes [13] and ADE20K [59].
Figure 6: MACs vs. Performance. EfficientViT provides a better trade-off between MACs and performance than SOTA semantic segmentation and image classification models.
Table 4: Backbone Performance of EfficientViT on ImageNet Classification.
‘r224’ means the input resolution is 224x224. With 6.5G MACs, EfficientViT-B3 achieves 84.2 ImageNet top1 accuracy, surpassing EfficientNet-B6 while re-ducing the MACs by 2.9x and being 3.4x faster on Jetson Nano GPU and 3.7x faster on A100 GPU. ‘bs1’ represents that the latency is measured with batch size 1.
Models
Top1 Acc ↑
Top5 Acc ↑
Params ↓ MACs ↓
Latency ↓
Nano(bs1) Orin(bs1) A100(bs16)
EfficientNet-B1 [44]
EfficientViT-B1 (r224)
EfficientNet-B2 [44]
EfficientViT-B1 (r288)
CoAtNet-0 [14]
ConvNeXt-T [34]
EfficientNet-B3 [44]
EfficientViT-B2 (r256)
Swin-B [33]
CoAtNet-1 [14]
ConvNeXt-S [34]
EfficientNet-B4 [44]
EfficientViT-B3 (r224)
EfficientNet-B6 [44]
CoAtNet-2 [14]
ConvNeXt-B [34]
EfficientViT-B3 (r288)
CoAtNet-3 [14]
ConvNeXt-L [34]
EfficientNetV2-S [45]
EfficientViT-L1 (r224) 79.1 79.4 80.1 80.4 81.6 82.1 81.6 82.7 83.5 83.3 83.1 82.9 83.5 84.0 84.1 83.8 84.2 84.5 84.3 83.9 84.5 94.4 94.3 94.9 95.0
--95.7 96.1
---96.4 96.4 96.8
--96.7
---96.9 7.8M 9.1M 9.2M 9.1M 25M 29M 12M 24M 88M 42M 50M 19M 49M 43M 75M 89M 49M 168M 198M 22M 53M 0.70G 0.52G 1.0G 0.86G 4.2G 4.5G 1.8G 2.1G 15G 8.4G 8.7G 4.2G 4.0G 19G 16G 15G 6.5G 35G 34G 8.8G 5.3G 36.2ms 24.8ms 45.1ms 34.5ms 95.8ms 87.9ms 67.8ms 58.5ms 240ms 171ms 146ms 143ms 101ms 475ms 254ms 211ms 141ms
----2.0ms 1.5ms 2.3ms 1.8ms 4.5ms 3.8ms 3.3ms 2.8ms 6.0ms 8.3ms 6.5ms 5.7ms 4.4ms 16.2ms 10.3ms 7.8ms 5.6ms
----2.9ms 1.9ms 3.5ms 2.9ms 6.9ms 6.1ms 5.1ms 4.0ms 6.9ms 13.0ms 10.0ms 9.7ms 5.7ms 30.2ms 16.8ms 12.3ms 8.2ms 24.9ms 18.5ms 7.0ms 3.2ms
Cityscapes is an autonomous driving dataset that mainly focuses on urban scenes. It contains 5,000 fine-annotated high-resolution (1024x2048) images with 19 classes di-vided into three subsets of size 2,975/500/1,525 for train-ing/validation/testing. ADE20K is a scene-parsing dataset with 150 classes. It contains 20,210/2,000/3,352 images for training, validation, and testing, respectively.
For super-resolution, we evaluate EfficientViT under two settings: lightweight super-resolution (SR) and high-resolution SR. For lightweight SR, we train models on
DIV2K [1] and test on BSD100 [37]. For high-resolution
SR, we train models on the first 3000 training images of
Table 5: Comparison with SOTA Semantic Segmentation Models on Cityscapes. The input resolution is 1024x2048 for all models. Models with similar mIoU are grouped for efficiency comparison. Compared with SegNeXt-T, EfficientViT-B1 achieves 2.0x MACs reduction, 3.8x latency reduction on Jetson AGX Orin GPU, and 0.7 higher mIoU. Compared with
SegFormer-B1, EfficientViT-B1 obtains 9.8x MACs saving and 2.0 higher mIoU.
Models mIoU ↑
Params ↓ MACs ↓
Latency ↓
Nano(bs1) Orin(bs1) A100(bs1)
PSPNet-Mbv2 [58]
DeepLabV3plus-Mbv2 [8]
FCN-Mbv2 [35]
EfficientViT-B0
HRFormer-S [55]
SegFormer-B1 [51]
SegNeXt-T [18]
EfficientViT-B1
HRFormer-B [55]
SegFormer-B3 [51]
SegNeXt-S [18]
EfficientViT-B2
SegFormer-B5 [51]
SegNeXt-B [18]
EfficientViT-B3 70.2 75.2 61.5 75.7 80.0 78.5 79.8 80.5 81.9 81.7 81.3 82.1 82.4 82.6 83.0 14M 15M 9.8M 0.7M 14M 14M 4.3M 4.8M 56M 47M 14M 15M 85M 28M 40M 423G 555G 317G 4.4G 836G 244G 51G 25G 2224G 963G 125G 74G 1460G 276G 179G
--2.1s 0.28s
-5.6s 2.2s 0.82s
--3.4s 1.7s
--3.2s
-83.5ms 52.0ms 9.9ms
-146ms 93.2ms 24.3ms
-407ms 127ms 46.5ms 638ms 228ms 81.8ms
-9.8ms 6.4ms 3.8ms
-20.4ms 10.5ms 5.7ms
-54.3ms 14.2ms 8.9ms 82.1ms 24.2ms 14.2ms transfer time is included in the reported latency.
Implementation Details. We implement our models us-ing Pytorch [39] and train them on GPUs. We use the
AdamW optimizer with cosine learning rate decay for train-ing our models. For lightweight multi-scale attention, we use a two-branch design for the best trade-off between per-formance and efficiency, where 5x5 nearby tokens are ag-gregated to generate multi-scale tokens.
For semantic segmentation experiments, we use the mean Intersection over Union (mIoU) as our evaluation metric. The backbone is initialized with weights pretrained on ImageNet and the head is initialized randomly, following the common practice.
For super-resolution, we use PSNR and SSIM on the Y channel as the evaluation metrics, same as previous work
[31]. The models are trained with random initialization. 3.2. Ablation Study
Effectiveness of Our Lightweight MSA Module. We conduct ablation study experiments on Cityscapes to study the effectiveness of two key design components of our Ef-ficientViT module, i.e., multi-scale learning and global at-tention. To eliminate the impact of pre-training, we train all models from random initialization. In addition, we rescale the width of the models so that they have the same #MACs.
The results are summarized in Table 3. We can see that re-moving either global attention or multi-scale learning will significantly hurt the performances. It shows that all of them
Figure 7: Qualitative results on Cityscapes.
FFHQ [27] and test on the first 500 validation images of
FFHQ.
Apart from dense prediction, we also study the effective-ness of EfficientViT for image classification using the Ima-geNet dataset [15].
Latency Measurement. We measure the mobile latency on Qualcomm Snapdragon 8Gen1 CPU with Tensorflow-Lite1, batch size 1 and fp32. We use TensorRT2 and fp16 to measure the latency on edge GPU and cloud GPU. The data 1https://www.tensorflow.org/lite 2https://docs.nvidia.com/deeplearning/tensorrt/
Table 6: Comparison with SOTA Semantic Segmentation Models on ADE20K. Compared with SegNeXt-S, EfficientViT-B2 provides a 2.0x speedup on Jetson Nano GPU and 1.6 mIoU gain. Compared with SegFormer-B1, EfficientViT-B1 achieves 0.6 higher mIoU with a 3.1x speedup on Jetson AGX Orin GPU.
Models mIoU ↑
Params ↓ MACs ↓
Latency ↓
Nano(bs1) Orin(bs1) A100(bs1)
SegFormer-B1 [51]
SegNeXt-T [18]
EfficientViT-B1
HRFormer-S [55]
SegNeXt-S [18]
EfficientViT-B2
HRFormer-B [55]
Mask2Former [10]
MaskFormer [11]
SegFormer-B2 [51]
SegNeXt-B [18]
EfficientViT-B3 42.2 41.1 42.8 44.0 44.3 45.9 48.7 47.7 46.7 46.5 48.5 49.0 14M 4.3M 4.8M 14M 14M 15M 56M 47M 42M 28M 28M 39M 16G 6.6G 3.1G 110G 16G 9.1G 280G 74G 55G 62G 35G 22G 389ms 281ms 110ms
-428ms 212ms
---920ms 806ms 411ms 12.3ms 12.4ms 4.0ms
-17.2ms 7.3ms
---24.3ms 32.9ms 12.5ms 2.7ms 3.0ms 1.6ms
-3.3ms 2.2ms
---4.6ms 6.2ms 3.3ms are essential for achieving a better trade-off between perfor-mance and efficiency.
Backbone Performance on ImageNet. To understand the effectiveness of EfficientViT’s backbone in image clas-sification, we train our models on ImageNet following the standard training strategy. We summarize the results and compare our models with SOTA image classification mod-els in Table 4.
Though EfficientViT is designed for high-resolution it achieves highly competitive per-dense prediction, formances on ImageNet classification.
In particular,
EfficientViT-B3 obtains 84.2 top1 accuracy on ImageNet, providing +0.2 accuracy gain over EfficientNet-B6 and 3.7x speedup on A100 GPU. 3.3. Main Results
Cityscapes. Table 5 reports the comparison between Ef-ficientViT and SOTA semantic segmentation models on
Cityscapes. EfficientViT achieves remarkable efficiency improvements over prior SOTA semantic segmentation
Specifically, models without sacrificing performances. compared with SegFormer, EfficientViT obtains up to 13x
MACs saving and up to 8.8x latency reduction with higher mIoU. Compared with SegNeXt, EfficientViT provides up to 2.0x MACs reduction and 3.8x speedup on GPU while maintaining higher mIoU.
Having similar computational cost, EfficientViT yields significant performance gains over previous SOTA models.
For example, EfficientViT-B3 yields +4.5 mIoU gain over
SegFormer-B1 with lower MACs.
In addition to the quantitative results, we visualize
EfficientViT and the baseline models qualitatively on
Cityscapes. The results are shown in Figure 7. We can find that EfficientViT can better recognize boundaries and small objects than the baseline models while achieving lower la-tency on GPU.
ADE20K. Table 6 summarizes the comparison between
EfficientViT and SOTA semantic segmentation models on
ADE20K. Similar to Cityscapes, we can see that Ef-ficientViT also achieves significant efficiency improve-ments on ADE20K. For example, with +0.6 mIoU gain,
EfficientViT-B1 provides 5.2x MACs reduction and up to 3.5x GPU latency reduction than SegFormer-B1. With +1.6 mIoU gain, EfficientViT-B2 requires 1.8x fewer computa-tional cost and runs 2.4x faster on Jetson AGX Orin GPU than SegNeXt-S.
Super-Resolution. Table 7 presents the comparison of
EfficientViT with SOTA ViT-based SR methods (SwinIR
[31] and Restormer [56]) and SOTA CNN-based SR meth-ods (VapSR [60] and BSRN [30]). EfficientViT provides a better latency-performance trade-off than all compared methods.
On lightweight SR, EfficientViT provides up to 0.09dB gain in PSNR on BSD100 while maintaining the same or lower GPU latency compared with SOTA CNN-based SR methods. Compared with SOTA ViT-based SR methods, Ef-ficientViT provides up to 5.4× speedup on GPU and main-tains the same PSNR on BSD100.
On high-resolution SR, the advantage of EfficientViT over previous ViT-based SR methods becomes more sig-nificant. Compared with Restormer, EfficientViT achieves up to 6.4× speedup on GPU and provides 0.11dB gain in
PSNR on FFHQ.
Table 7: Comparison with SOTA super-resolution models. Regarding the latency-performance trade-off, EfficientViT outperforms previous SOTA models by a significant margin, providing up to 6.4× A100 latency reduction while providing higher PSNR and SSIM on FFHQ compared with Restormer.
Model
FFHQ (512x512 → 1024x1024)
BSD100 (160x240 → 320x480)
PSNR ↑
SSIM ↑ A100(bs1) ↓
Speedup ↑
PSNR ↑
SSIM ↑ A100(bs1) ↓
Speedup ↑
Restormer [56]
SwinIR [31]
VapSR [60]
BSRN [30]
EfficientViT w0.75
EfficientViT 43.43 43.49
--43.54 43.58 0.9806 0.9807
--0.9809 0.9810 92.0ms 61.2ms
--14.3ms 17.8ms 1x 1.5x
--6.4x 5.2x 32.31 32.31 32.27 32.24 32.31 32.33 0.9021 0.9012 0.9011 0.9006 0.9016 0.9019 15.1ms 9.7ms 4.8ms 4.5ms 2.8ms 3.2ms 1x 1.6x 3.1x 3.4x 5.4x 4.7x 4.