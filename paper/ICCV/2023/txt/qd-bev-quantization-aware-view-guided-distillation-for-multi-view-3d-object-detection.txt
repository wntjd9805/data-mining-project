Abstract
Multi-view 3D detection based on BEV (bird-eye-view) has recently achieved significant improvements. However, the huge memory consumption of state-of-the-art models makes it hard to deploy them on vehicles, and the non-trivial latency will affect the real-time perception of stream-ing applications. Despite the wide application of quanti-zation to lighten models, we show in our paper that di-rectly applying quantization in BEV tasks will 1) make the training unstable, and 2) lead to intolerable performance degradation. To solve these issues, our method QD-BEV enables a novel view-guided distillation (VGD) objective, which can stabilize the quantization-aware training (QAT) while enhancing the model performance by leveraging both image features and BEV features. Our experiments show that QD-BEV achieves similar or even better accuracy than previous methods with significant efficiency gains. On the nuScenes datasets, the 4-bit weight and 6-bit activation quantized QD-BEV-Tiny model achieves 37.2% NDS with only 15.8 MB model size, outperforming BevFormer-Tiny by 1.8% with an 8× model compression. On the Small and
Base variants, QD-BEV models also perform superbly and achieve 47.9% NDS (28.2 MB) and 50.9% NDS (32.9 MB), respectively. 1.

Introduction
Given its potential in enabling autopilot, multi-view 3D detection based on BEV (bird-eye-view) has become
* Equal contribution : zhang yifan@smail.nju.edu.cn, zhendong@berkeley.edu
† Corresponding authors : ldu@nju.edu.cn, shanghang@pku.edu.cn (a) mAP curve. (b) NDS curve.
Figure 1: Training curves of QD-BEV (with VGD) versus progressive QAT on W4A6 quantization of BEVFormer-Tiny. Note that standard QAT works even worse compared to progressive QAT and it falls out of the targeted accuracy range in the figures. an important research direction for autonomous driving.
Based on input sensors, previous work can be divided into
LiDAR-based methods [18, 47] and camera-only methods
[23, 38, 14, 13, 24, 25]. Compared to the LiDAR-based methods, camera-only methods have the merits of lower deployment cost, closer to human eyes, and easier access to visual information in the driving environment. However, even if using the camera-only methods, the computational and memory costs of running state-of-the-art BEV models are still formidable, making it difficult to deploy them onto vehicles. For example, BEVFormer-Base has a 540 ms in-ference latency (corresponds to 1.85 fps) on one NVIDIA
V100 GPU, which is infeasible for real-time applications that generally require 30 fps. Since a non-trivial latency will harm the streaming perception, it is particularly crucial to explore and devise lightweight models for camera-only 3D object detection based on BEV.
Quantization [16, 11, 42, 10] can reduce the bitwidth used to represent weights and activations in deep neural net-Figure 2: Illustration of QD-BEV. (a) In our pipeline, multi-camera images are input into the floating-point teacher network and the quantized student network in order to compute the KL divergence in an element-wise manner. The KL divergence is used as distillation loss in the image feature and the BEV feature, respectively. Then we conduct view-guided distillation using the BEV mask obtained from the external parameters of the camera. Please refer to Sec.3.1 for more details. (b) The lower flow chart shows the computation process of the view-guided distillation loss. Specific details are in Sec. 3.3. works, which can greatly save the model size and compu-tational costs while improving the speed of model reason-ing. However, directly applying quantization would lead to significant performance degradation. Compared to im-age classification and 2D object detection tasks where the standard quantization methods shine, multi-camera 3D de-tection tasks are much more complicated and difficult due to the existence of multiple views and information from multiple dimensions (for example, the temporal information and spatial information used in BEVFormer [23]). Con-sequently, the structure of BEV networks tends to become more complex, with a deeper convolutional neural network backbone to extract image information from multiple views, and with transformers to encode and decode the features of the BEV domain. The presence of different neural archi-tectures, multiple objectives, and knowledge from differ-ent modalities greatly challenges the standard quantization methods, decreasing their stability and accuracy, and even making the whole training process diverge. In Figure 1, we show the training curves when applying W4A6 quantization on a BEVFormer-Tiny model. As can be seen, the perfor-mance of quantization-aware training (QAT) fluctuates sig-nificantly in different epochs, while the performance of our proposed method QD-BEV shows a stable rising trend. We perform more experiments to validate the effectiveness of
QD-BEV in Sec 5.
To solve the problems of standard QAT, in this work, we first conduct systematic experiments and analyses on quan-tizing BEV networks. Then we devise a quantization-aware view-guided distillation method (referred to as QD-BEV) that can decently solve the stability issue while improv-ing the final performance of compact BEV models. Our proposed view-guided distillation (VGD) can better lever-age information from both the image and the BEV do-mains for multi-view 3D object detection. This can signif-icantly outperform previous distillation methods that can-not jointly handle the different types of losses in BEV net-works. Specifically, as shown in Figure 2, we first take
the FP (floating-point) model as the teacher model and the quantized model as the student model, then we calculate the
KL divergence on the image feature and the BEV feature, respectively. Finally, we leverage the mapping relationship and realize VGD by organically combining the image fea-ture and the BEV feature through the camera’s external pa-rameters. Note that in QD-BEV, neither additional training data nor larger powerful teacher networks are used to tune the accuracy, but QD-BEV models are still able to outper-form previous baselines while having significantly smaller model sizes and computational requirements. Our contribu-tions are as follows:
• We conduct systematic experiments on quantizing
BEV models, unveiling major issues hampering stan-dard quantization-aware training methods on BEV.
• We specially design view-guided distillation (VGD) for BEV models, which jointly leverages both image domain and BEV domain information. VGD boosts the final performance while solving the stability issue of standard QAT.
• Our W4A6 quantized QD-BEV-Tiny has 37.2% NDS with only 15.8 MB model size, which outperforms the 8× larger BevFormer-Tiny model by 1.8%. 2.