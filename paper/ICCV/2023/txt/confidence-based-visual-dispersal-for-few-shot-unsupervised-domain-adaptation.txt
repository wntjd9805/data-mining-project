Abstract
Unsupervised domain adaptation aims to transfer knowledge from a fully-labeled source domain to an un-labeled target domain. However, in real-world scenarios, providing abundant labeled data even in the source domain can be infeasible due to the difficulty and high expense of annotation. To address this issue, recent works consider the
Few-shot Unsupervised Domain Adaptation (FUDA) where only a few source samples are labeled, and conduct knowl-edge transfer via self-supervised learning methods. Yet ex-isting methods generally overlook that the sparse label set-ting hinders learning reliable source knowledge for trans-fer. Additionally, the learning difficulty difference in tar-get samples is different but ignored, leaving hard target samples poorly classified. To tackle both deficiencies, in this paper, we propose a novel Confidence-based Visual
Dispersal Transfer learning method (C-VisDiT) for FUDA.
Specifically, C-VisDiT consists of a cross-domain visual dis-persal strategy that transfers only high-confidence source knowledge for model adaptation and an intra-domain vi-sual dispersal strategy that guides the learning of hard tar-get samples with easy ones. We conduct extensive exper-iments on Office-31, Office-Home, VisDA-C, and Domain-Net benchmark datasets and the results demonstrate that the proposed C-VisDiT significantly outperforms state-of-the-art FUDA methods. Our code is available at https:
//github.com/Bostoncake/C-VisDiT. 1.

Introduction
Deep learning has achieved considerably high perfor-mance in various computer vision tasks, such as instance recognition [13, 51], semantic segmentation [23, 21, 2], and
*Corresponding author. object detection [32, 6, 22]. Despite the remarkable suc-cess, well-tuned deep models usually suffer from dramatic performance drops when being applied in real-world sce-narios because of the domain gap issue [38, 39, 54, 44], i.e. the shift between the learnt distribution and the testing distribution. To cope with such performance degradation, researchers have been dedicated to unsupervised domain adaptation (UDA) [23, 9, 39, 25] aiming at transferring the learned knowledge from a labeled source domain to another unlabeled target domain.
Though being promising, UDA assumes that a fully-annotated source domain can be conveniently accessed for training. However, in some real-world scenarios, such an assumption is violated, ending up with sparsely labeled source data. For example, in the field of chip defect de-tection, it is unrealistic to perfectly annotate massive defect data in the source domain, due to the strict quality regula-tion and high expense of annotation [7, 15]. In the med-ical industry, the high cost and difficulty of labeling also prohibit providing sufficient labeled source data [42, 53].
Therefore, how to realize UDA with sparsely labeled source data becomes appealing and has attracted increasing atten-tion [17, 50, 1, 31].
Recently, researchers address this issue by considering the Few-shot Unsupervised Domain Adaptation (FUDA), where only a few source samples are labeled. Compared with UDA, FUDA is more challenging due to the insuffi-ciency of the source knowledge. With only a few labeled source samples, deep models are difficult to learn a dis-criminative feature space to represent the complicated se-mantic information, thus largely affecting the knowledge transfer. And thus to compensate for the shortage of la-beled source data, existing methods [17, 50] generally em-ploy self-supervised learning strategies to mine the latent correspondence between the source distribution and the tar-get one. Specifically, Kim et al. [17] proposed a cross-domain visual dispersal between samples of different learn-ing difficulties for the target domain, aiming to guide the learning of hard target samples by the easy ones. In con-trast to existing methods, our intra-domain visual dispersal can diminish the learning disparity between target samples of different learning difficulties, thus boosting the model adaptation.
We emphasize that the core contribution of our C-VisDiT is the confidence-based strategies, motivated by the idea that samples with low prediction confidence greatly harm the training in the label-scarse scenario of FUDA. Although we conduct visual dispersal via MixUp [52], we show that our C-VisDiT and its variants can greatly outperform their vanilla MixUp-based alternatives which completely dis-card the confidence of samples, well confirming the impor-tance of the proposed confidence-based transfer learning for
FUDA.
To summarize, our contributions are three-fold:
• We propose a confidence-based visual dispersal trans-fer learning method (C-VisDiT) for FUDA, which si-multaneously takes the reliability of the source sam-ples and the learning difficulty of the target samples into consideration.
• We introduce a cross-domain visual dispersal to avoid the negative impact of source knowledge with low con-fidence. An intra-domain visual dispersal is also em-ployed to boost the learning of hard unlabeled target samples with the guidance of easy ones.
• We conduct extensive experiments on four pop-i.e., Office-ular benchmark datasets for FUDA, 31 [33], Office-home [41], VisDA-C [28], and Do-Experiment results show that our mainNet [27].
C-VisDiT consistently outperforms existing methods with a significant margin, establishing new state-of-the-art results for all datasets. That well demonstrate the effectiveness and superiority of our method. 2.