Abstract
Query image
Rendered pose
Localization prior
Estimated pose 1st iteration
Estimated pose 2nd iteration
Beyond novel view synthesis, Neural Radiance Fields (NeRF) are useful for applications that interact with the real world. In this paper, we use them as an implicit map of a given scene and propose a camera relocalization algorithm tailored for this representation. The proposed method en-ables to compute in real-time the precise position of a de-vice using a single RGB camera, during its navigation. In contrast with previous work, we do not rely on pose regres-sion or photometric alignment but rather use dense local features obtained through volumetric rendering which are specialized on the scene with a self-supervised objective.
As a result, our algorithm is more accurate than competi-tors, able to operate in dynamic outdoor environments with changing lightning conditions and can be readily integrated in any volumetric neural renderer. 1.

Introduction
Visual localization, i.e. the problem of camera pose estimation in a known environment [36], enables to build camera-based positioning systems for various applications such as autonomous driving [28], robotics [2] or augmented reality [24]. Map-based navigation systems for such ap-plications operate with a reference map of the environ-ment, built from previously collected data. These maps are commonly defined with explicit 3D scenes representa-tions (point cloud, voxels, meshes, etc.), which only store discrete information while the underlying environment they represent is continuous.
Recently, Neural Radiance Fields (NeRF) [26] and re-lated volumetric-based approaches [31, 53] have emerged as a new way to implicitly represent a scene. 3D coordinates are mapped to volume density and radiance in a neural net-work. NeRF is trained with a sparse set of posed images of a scene and learns its 3D geometry via differentiable render-ing. The resulting model is continuous, i.e. the radiance of
Figure 1: Visual localization in a neural renderer. Start-ing from a coarse localization prior, our algorithm estimates the pose of a query image by comparing image features to descriptors rendered from a neural scene representation. all 3D points in the scene can be computed, which enables the rendering of photorealistic views from any viewpoint.
Beyond their rendering ability, implicit scene represen-tations are actively investigated to be used as the map rep-resentation for navigation systems [1, 35, 20, 17]. This work focuses on one aspect of the navigation pipeline, un-derstudied in the specific case of implicit scene representa-tion, the image localization problem. Our motivation is to provide a camera relocalization algorithm (i.e. 6-DoF pose estimation) from one RGB image based only on a learned volumetric-based implicit map. We aim to design a method for robotics applications: it must be fast to compute, robust to outdoor conditions and could be deployed in dynamic en-vironments. Existing localization methods that use implicit maps either have limited accuracy by lack of geometric rea-soning [29, 7], or do not meet the aforementioned require-ments because photometric alignment [55, 19] can be slow and assumes constant lightning conditions.
Contribution.
In this paper, we introduce local descrip-tors in NeRFâ€™s implicit formulation and we use the result-ing model, named CROSSFIRE, as the scene representation of a 2D-3D features matching method. We train simul-taneously a CNN feature extractor and a neural renderer to provide consistent scene-specific descriptors in a self-supervised way. During training, we leverage the 3D in-formation learned by the radiance field in a metric learning optimization objective which does not require supervised pixel correspondences on image pairs nor a pre-computed 3D model. The proposed descriptors represent not only the local 2D image content but also the 3D position of the observed point, which enables to solve ambiguities in ar-eas with repetitive patterns. Our method can use any dif-ferentiable neural renderer and, hence, can directly ben-efit from recent NeRF improvements. For instance, we make the model computationally tractable thanks to the multi-resolution hash encoding from Instant-NGP [31] and adapted to dynamic outdoor scenes thanks to appearance embeddings from Nerf-W [25].
Finally, we show that these features can be used to solve the visual relocalization task with an iterative algorithm composed of a dense features matching step followed by standard Perspective-n-Points (PnP) camera pose computa-tion. We take inspiration from structure-based visual local-ization pipelines [41, 39] but replace the commonly used sparse 3D model obtained from Structure-from-Motion by our neural field from which dense features are extracted.
For a given camera pose candidate, we render dense de-scriptors and depth maps. Descriptors are used to estab-lish 2D-2D matches which are upgraded to 2D-3D matches by the rendered depth. We can iteratively refine the esti-mated pose by repeating the aforementioned procedure, as presented in Figure 1. 2.