Abstract
Video relation grounding (VRG) is a significant and challenging problem in the domains of cross-modal learn-ing and video understanding. In this study, we introduce a novel approach called inverse compositional learning (ICL) for weakly-supervised video relation grounding. Our ap-proach represents relations at both the holistic and partial levels, formulating VRG as a joint optimization problem that encompasses reasoning at both levels. For holistic-level reasoning, we propose an inverse attention mechanism and a compositional encoder to generate compositional rel-evance features. Additionally, we introduce an inverse loss to evaluate and learn the relevance between visual features and relation features. At the partial-level reasoning, we introduce a grounding by classification scheme. By lever-aging the learned holistic-level features and partial-level features, we train the entire model in an end-to-end man-ner. We conduct evaluations on two challenging datasets and demonstrate the substantial superiority of our proposed method over state-of-the-art methods. Extensive ablation studies confirm the effectiveness of our approach. 1.

Introduction
The objective of the Video Relation Grounding (VRG) task is to determine the spatial and temporal extents of a given query relation within an untrimmed video.
A relation is represented as a three-tuple linguistic phrase ⟨subject, predicate, object⟩, where the subject and object are interconnected by the predicate, such as
⟨person, ride, bicycle⟩, as illustrated in Fig. 1 (a). VRG is approached as a weakly supervised problem [40], where only the relation phrase is provided during training, while the spatial bounding boxes of the subject and object, and the temporal duration of the relation in the video, are not available. VRG is a crucial task for various multimodal ap-plications, including video captioning [34] and visual ques-*Ping Wei is the corresponding author.
Figure 1. grounding. (d) Holistic-partial structure of a relation. (b) An ambiguous result. (a) VRG task. (c) Video tion answering [14]. adopt
Existing visual grounding approaches the paradigm of linguistic reconstruction to ground targets
[40, 25, 46, 18, 12]. They learn a linguistic description representation to match visual features of the targets and then reconstruct the linguistic description with the matched visual features. This paradigm has demonstrated effective-ness in previous visual grounding tasks involving lengthy and intricate linguistic descriptions [25, 18], as depicted in Fig. 1 (b). The effectiveness of this paradigm can be attributed to the fact that sophisticated linguistic descrip-tions contain abundant semantic and intricate information, imposing stringent constraints on feature matching and enabling precise target grounding. However, this paradigm proves to be less productive in the VRG, primarily due to the simplicity and sparsity of the information present in a 3-tuple relation phrase. These relation phrases impose weaker constraints on feature matching. As a result, certain visual features that are weakly related or ambiguous, as illustrated in Fig. 1 (c), can still effectively reconstruct the relation phrase. This phenomenon can potentially cause the
model to become ‘lazy’ in terms of accurately localizing the precise targets, thereby impeding the performance.
Another issue of the current grounding methods is that they treat a relation as a whole and reason about it at a holis-tic level, without accounting for partial aspects [40, 25, 2, 18]. Indeed, a relation encompasses both holistic attributes and partial attributes, as exemplified in Fig. 1 (d). The re-lation person-ride-bicycle can be perceived as a holistic concept that represents the overall relation. It can also be disassembled into three constituent parts: person, ride, and bicycle, allowing for a description at the partial level. Ne-glecting the partial-level relation may result in a failure to identify the crucial cues and information embedded within these constituent parts.
In this paper, we rethink video relations from a new per-spective and propose a novel inverse compositional learning (ICL) approach for video relation grounding. A relation is represented both at the holistic level and the partial level, acknowledging their distinct characteristics. We formulate
VRG as a joint optimization problem that incorporates both holistic-level reasoning and partial-level reasoning. For the holistic-level reasoning, we propose an inverse composition learning method, where both the attention and inverse atten-tion are computed for the subject and object, respectively.
The attention encodes the distribution of relevant visual fea-tures, while the inverse attention captures the distribution of irrelevant visual features. By paring different attention and inverse attention features of the subject and object, the compositional visual features are generated. We devise an inverse loss function to learn the compositional relevance between the relation and visual features, which encourages the model to extract and emphasize the visual features that are most pertinent to the given relation. To facilitate partial-level reasoning, we decompose the relation phrase into three parts: subject, predicate and object. A grounding by clas-sification scheme is proposed to learn the partial-level fea-ture. By employing this partial-level reasoning approach, we aim to enhance the accuracy of the localization process, enabling a more precise identification and tracking of the subject and object throughout the video.
With the holistic-level and partial-level features, the model is trained in an end to end way.
In inference, the grounding results are computed by jointly optimizing the holistic-level reasoning and partial-level reasoning. The proposed method is tested on two challenging datasets:
ImageNet-VidVRD [27] and HICO-Det [1]. It outperforms the state-of-the-art methods by a large margin. 2.