Abstract
Existing video captioning approaches typically require to first sample video frames from a decoded video and then conduct a subsequent process (e.g., feature extraction and/or captioning model learning). In this pipeline, manual frame sampling may ignore key information in videos and thus degrade performance. Additionally, redundant infor-mation in the sampled frames may result in low efficiency in the inference of video captioning. Addressing this, we study video captioning from a different perspective in compressed domain, which brings multi-fold advantages over the exist-ing pipeline: 1) Compared to raw images from the decoded video, the compressed video, consisting of I-frames, mo-tion vectors and residuals, is highly distinguishable, which allows us to leverage the entire video for learning with-out manual sampling through a specialized model design; 2) The captioning model is more efficient in inference as smaller and less redundant information is processed. We propose a simple yet effective end-to-end transformer in the compressed domain for video captioning that enables learn-ing from the compressed video for captioning. We show that even with a simple design, our method can achieve state-of-the-art performance on different benchmarks while running almost 2× faster than existing approaches. Code is avail-able at https://github.com/acherstyx/CoCap. 1.

Introduction
Video captioning is a representative example of applying deep learning to the fields of computer vision and natural language processing with a long list of applications, such as blind navigation, video event commentary, and human-computer interaction. To generate captions for a video, the model needs to not only identify objects and actions in the video, but also be able to express them accurately in natural
*The two authors make equal contributions and are co-first authors.
†Corresponding author: Libo Zhang (libo@iscas.ac.cn).
Figure 1. Comparing our method with prior methods for video cap-tioning. Prior works are all based on decoding video frames. The difference between them is that some methods use offline extracted multiple features as input and generate captions, while others di-rectly take dense video frames as input. By avoiding heavy re-dundant information and offline multiple feature extraction, our method speedup the caption generation process while maintaining high quality results. language. Despite significant progress, accurate and fast video captioning remains a challenge.
Video captioning requires both 2D appearance informa-tion, which reflects the objects in the video, and 3D action information, which reflects the actions. The interaction be-tween these two types of information is crucial for accu-rately captioning the actions of objects in the video. Most of the existing methods [36, 38, 22] are shown in Fig. 1 (the upper branch), mainly including the three-steps: (1)
Decoding the video and densely sampling frames. (2) Ex-tracting the 2D/3D features of the video frames offline. (3)
Training the model based on these 2D/3D features. In these methods, densely sampled video frames result in signifi-cant redundancy, which in turn increases the computation and inference time of the model. This is because the model needs to extract features from each video frame and use all
r
E
D
C
I 58 56 54 52 50 48 0.1
Ours(I+MV+Res, L-14)
Ours(I+MV+Res)
Ours(I+MV)
Ours(I)
SwinBERT [18]
HMN [36] 10
SGN [27] 1
Second
Figure 2. Comparison of model inference speed and CIDEr score on MSRVTT dataset. I, MV and Res refer to I-frame, motion vec-tor and residual respectively. The test is run on 1 Card V100 ma-chine with batch size set to 1. of these features as input. Furthermore, extracting 2D ap-pearance features, 3D action features, and region features for each video frame requires additional time. To address the speed issue and improve inference speed, some recent works [18, 29] have adopted an end-to-end approach that avoids extracting multiple visual features offline. As shown in Fig. 1 (The middle branch), the flow of their method is as follows: (1) Decoding the video and densely sample frames. (2) Take video frames directly as input and then end-to-end training model. These approaches involve a trainable vi-sual feature extractor, rather than relying on multiple offline 2D/3D feature extractors. For example, SwinBERT [18] uses VidSwin [19] as the trainable feature extractor, while
MV-GPT [29] uses ViViT [1]. While these two-steps meth-ods address the time consumption associated with offline feature extraction, they do not alleviate the computational burden and time required to handle the redundancy of infor-mation.
To address the above problems, we propose an end-to-end video captioning method based on compressed video.
Our work significantly simplifies the video caption pipeline by eliminating time-consuming video decoding and feature extraction steps. As in Fig. 1 (the lower branch), unlike previous methods, we take compressed video information as input and directly output a natural language description of the video. Compressed video is mainly composed of I-frame, motion vector and residual, and there is no redun-dant information between them, and they are all refined in-formation. Therefore, the model needs less computation to process compressed domain information, and model infer-ence is faster. At the same time, the end-to-end network structure in our proposed method can also avoid the time consumption caused by extracting multiple features. Be-sides, Our model is better at understanding the content of videos by utilizing the refined information in compressed domain, including the 2D feature from I-frame and the 3D action feature extracted from motion vector and residual.
As shown in Fig. 2, compared with other two-steps and three-steps methods, such as SwinBERT [18], HMN [36] and SGN [27], our method is not only faster, but also has competitive performance. Our model comprises two parts, as depicted in Fig. 4. One part consists of three encoders that extract features and an action encoder that fuses them, while the other part comprises a multimodal decoder that generates video captions. Specifically, we first extract the context feature, motion vector feature and residual feature of the compressed video through I-frame Encoder, Motion
Encoder, and Residual Encoder, respectively. The context feature contains information about objects in the video, but action information is missing.
In order to extract the ac-tion feature of the video, we fuse the motion vector feature, residual feature, and context feature through the action en-coder. Then use the context feature and action feature as visual input of the multimodal decoder to generate video captions.
The contributions of this paper are summarized below: 1. We propose a simple and effective transformer that can take compressed video as input and directly generate a video description. 2. Our experimental results demonstrate that our method is nearly 2× further than the fastest existing state-of-the-art method in inference time, while maintaining competitive results on three challenging video captioning datasets, e.g., MSVD, MSRVTT and VATEX. 2.