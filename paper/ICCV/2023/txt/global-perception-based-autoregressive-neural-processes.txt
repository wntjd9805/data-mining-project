Abstract
Increasingly, autoregressive approaches are being used to serialize observed variables based on speciﬁc criteria.
The Neural Processes (NPs) model variable distribution as a continuous function and provide quick solutions for differ-ent tasks using a meta-learning framework. This paper pro-poses an autoregressive-based framework for NPs, based on their autoregressive properties. This framework lever-ages the autoregressive stacking effects of various variables to enhance the representation of the latent distribution, con-currently reﬁning local and global relationships within the positional representation through the use of a sliding win-dow mechanism. Autoregression improves function approx-imations in a stacked fashion, thereby raising the upper bound of the optimization. We have designated this frame-work as Autoregressive Neural Processes (AENPs) or Con-ditional Autoregressive Neural Processes (CAENPs). Tradi-tional NP models and their variants aim to capture relation-ships between the context sample points, without addressing either local or global considerations. Speciﬁcally, we cap-ture contextual relationships in the deterministic path and introduce sliding window attention and global attention to reconcile local and global relationships in the context sam-ple points. Autoregressive constraints exist between multi-ple latent variables in the latent paths, thus building a com-plex global structure that allows our model to learn com-plex distributions. Finally, we demonstrate the effectiveness of the NPs or CFANPs models for 1D data, Bayesian opti-mization, and 2D data. 1.

Introduction
Neural processes (NPs) are different from traditional stochastic processes such as Gaussian processes (GPs)
[15, 20] in that GPs are difﬁcult to use due to the neces-sity of selecting an appropriate kernel function. In practice, the selection of the right kernel function for different distri-butions is often a matter of specialized knowledge and ex-*Corresponding author r1 r2 (cid:258)(cid:258) rn r
Z r1 r2 (cid:258)(cid:258) rn z1 z2 (cid:258)(cid:258) zn
Z*
Figure 1. The NPs model [5] (left) represents the context sample points denoted as r using an average value aggregation. In this model, the latent distribution is represented by the mean μz and variance σz normal distribution of a single variable. On the other model, the BANPs model [14] (right) represents the latent distri-bution of each context sample point as a single variable with a mean μn and variance σn normal distribution. perience. In addition, different kernel functions can result in different computational complexity and model accuracy.
In contrast, NPs provide a more ﬂexible approach to data modeling by eliminating the need to select a speciﬁc ker-nel function. This approach can lead to more efﬁcient and accurate predictions without requiring extensive knowledge of the data distribution. NPs [5] is a new model for the combination of parametric [19, 24] Neural Networks and
Stochastic Processes. In the given data, NPs are modeled as a new class of Stochastic Processes with a ﬂexible ap-proach to the original data distribution. In particular, NPs deal with non-trivial function distributions for which it is difﬁcult to ﬁnd a suitable prior representation of the GPs function. In this sense, GPs perform data modeling by driv-ing a prior distribution, while NPs perform the same task by driving the data. While maintaining the ﬂexible properties of the model during the training process, the NPs are imple-mented with a meta-learning framework so that they can be quickly adapted to new functional tasks.
Despite the increasing attention in Stochastic Processes methods, the NPs model still has several limitations that hinder its development [13]. One of the main problems
is that the NPs encoder maps context sample points to a
ﬁxed-length latent representation, while the decoder maps the above latent representation and the target sample point input to the target sample point output. However, the en-coder achieves a ﬁxed-length representation of the context sample points through an average aggregation module that assigns equal weight to each context sample point. As a result, it is difﬁcult for the decoder to determine which con-text sample points provide relevant information for predict-ing the target sample points, leading to underﬁtting [11].
Another limitation of NPs is their susceptibility to noise in real data, which causes the sampled context sample points to contain more interference information, resulting in devi-ations in the predicted location of the target sample points.
This is because NPs fail to capture the embedding relation-ships between the context sample points [12]. Furthermore, the distribution in the real world is complex and constantly changing, making it difﬁcult for the NPs model to express its latent distribution by a single latent variable [6]. Exist-ing NPs and their variants represent the latent distribution by stacking single or multiple latent variables [14, 13, 5].
However, this approach has limitations that need to be ad-dressed to further improve the effectiveness and accuracy of the NPs model.
In this paper, we introduce two frameworks, Autoregres-sive Neural Processes (AENPs) and Conditional Autore-gressive Neural Processes (CAENPs), which jointly model the global structure by using multiple latent Gaussian vari-ables in an autoregressive manner. Our approach provides better modeling performance, especially for complex distri-bution modeling processes. While existing NPs and their variants typically use self-attention implementations as the deterministic path encoding process, this can be computa-tionally expensive and may not effectively capture the re-lationships between the local context sample points. To overcome this, we propose a combination of sliding win-dow attention and global attention that is implemented au-toregressively, allowing us to capture relationships between the global and local context sample points while avoiding high computational cost. We demonstrate the advantages of our approach on 1D and 2D datasets, as well as in Bayesian optimization. Our approach improves the latent distribution performance of multiple latent variables, leading to better modeling performance for complex distribution modeling processes. 2.