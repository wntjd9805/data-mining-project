Abstract
Formula-driven supervised learning (FDSL) is a pre-training method that relies on synthetic images generated from mathematical formulae such as fractals. Prior work on FDSL has shown that pre-training vision transformers on such synthetic datasets can yield competitive accuracy on a wide range of downstream tasks. These synthetic im-ages are categorized according to the parameters in the mathematical formula that generate them.
In the present work, we hypothesize that the process for generating dif-ferent instances for the same category in FDSL, can be viewed as a form of data augmentation. We validate this hypothesis by replacing the instances with data augmenta-tion, which means we only need a single image per cate-gory. Our experiments shows that this one-instance fractal database (OFDB) performs better than the original dataset where instances were explicitly generated. We further scale up OFDB to 21,000 categories and show that it matches, or even surpasses, the model pre-trained on ImageNet-21k in
ImageNet-1k fine-tuning. The number of images in OFDB is 21k, whereas ImageNet-21k has 14M. This opens new possibilities for pre-training vision transformers with much smaller datasets. 1.

Introduction
Pre-training has become a standard procedure when training deep neural networks in computer vision [5, 45].
Pre-trained models are known to exhibit superior conver-gence and generalization for downstream tasks as compared to models that are trained from scratch. However, pre-training large vision models requires enormous data, which makes pre-training state-of-the-art vision models very ex-pensive regarding the required amount of data and compu-tation.
During the past decade, ImageNet has served as a com-mon dataset for pre-training vision models [13]. Models pre-trained on the classification task of 1.28M images in
∗equal contribution
GitHub code : https://github.com/ryoo-nakamura/OFDB/ (a) Fine-tuning accuracy on CIFAR-100 for
Figure 1: the number of images for pre-training. The line plot for
ImageNet-1k indicates the results when using random sam-pling to reduce the data for pre-training. (b) One-instance fractal database (OFDB) consists of only 1,000 images in total. The figure shows the category representation. OFDB contains a single instance per category.
ImageNet were transferred to Object Detection [15, 38, 45],
Semantic Segmentation [40, 34], and Video Recognition [8, 2].
In the 2020s, the pre-training of Vision Transform-ers (ViT) [14] has become increasingly popular. However large vision transformers are said to require datasets that are much larger than ImageNet, such as JFT-300M/3B [36], in order to achieve their true potential.
Pre-training increasingly larger models on increasingly larger datasets has shown a monotonic improvement in the accuracy of downstream tasks. However, creating datasets with billions of labeled images is not the ultimate solution to all our problems. First, the cost of manually labeling such
huge datasets is prohibitive. Self-supervised learning (SSL) has received great attention due to its competitive perfor-mance when used to pre-train large models without requir-ing labeled images. DINO [7], MoCoV3 [11], and BEiT [4] have shown promising results in this regard. However, SSL still relies on a vast amount of unlabeled data.
Therefore, we should pause and ask the question, “Are we using all this data efficiently?” There have been efforts in this direction to reduce the amount of pre-training data while retaining accuracy on downstream tasks. For ViTs,
DeiT [37] has demonstrated that distillation and data aug-mentation can enhance the pre-training effect of ImageNet-1k to match that of larger datasets. MAE [16] also show a high performance, even when trained on ImageNet-1k.
More recent studies, such as “Training Vision Transform-ers with Only 2,040 Images” [44], show how ViTs can be pre-trained on relatively small datasets.
Another approach to those above is pre-training a ViT using formula-driven supervised learning (FDSL) [28, 21, 19, 32, 14], in which not even a real image is required. In a follow-up study, Kataoka et al. [19] created an alternative synthetic dataset with emphasis on the contours in the im-age and showed that it is possible to surpass the accuracy of a ViT pre-trained on ImageNet-21k by using a synthetic dataset of the same size. FDSL method can generate the labels automatically from the parameters used to generate the images, so there is no labeling cost. Furthermore, un-like SSL, FDSL does not even require real images. The fact that a ViT pre-trained on synthetic datasets can outperform a ViT pre-trained on a fairly large human-labeled dataset
ImageNet-21k is significant. However, synthesizing more than a million images is costly.
It is possible that many of the images in these synthetic datasets are redundant or are simply not contributing to the pre-training since these synthetic images are expanded with basic procedures from a single image. In case of FractalDB [21], a single image inside of category was augmented to 1,000 instances with image rotation, parameter fluctuation, and patch patterns.
It is natural to rely on the combination of data augmenta-tion methods in pre-training phase. In this context, it may be possible to reach the same fine-tuning accuracy for both one instance with the same data augmentations and prepro-cessed 1,000 instances.
In this paper, we present an FDSL approach to pre-train a ViT with a single instance per category. Therefore, we require only 1,000 images when the dataset contains 1,000 categories. The proposed dataset, i.e., the one-instance frac-tal database (OFDB), significantly improves data efficiency under the assumption that data augmentation is used dur-ing pre-training. In previous FDSL datasets, the different instances for each category are created through some form of manipulation of the original image that defines that cat-egory. Therefore, it is quite natural to wonder whether these instances can be created through data augmentation techniques. Although a detailed description is provided in
Section 4, we disclose that the fractal instances can be re-placed by data augmentation, e.g., image rotation. Based on the above considerations, we use a novel FDSL dataset that only requires a single image per category and training with data augmentation including random pattern augmen-tation and random texture augmentation as proposed data augmentation methods for FDSL pre-training. We validate this hypothesis by creating small, yet effective, pre-training datasets, 2D-OFDB and 3D-OFDB, respectively.
Our main contributions can be summarized as follows:
Conceptual contribution. We propose two datasets, 2D-OFDB and 3D-OFDB, which consist of only one represen-tative fractal per category. For example, 2D-OFDB-1k con-sists of only 1,000 images but enables ViT to effectively learn visual representations for image classification. Along this line, we also implement random pattern augmentation and random texture augmentation for fractal pre-training.
Experimental contribution. We show that OFDBs achieve comparable performance to well-defined million-scale datasets (Figure 1 and Table 1). Furthermore, we show that the computational time of pre-training is reduced by 78.7%. In ImageNet-1k fine-tuning, 2D/3D-OFDB-21k performed at equal or better rates than baseline pre-training datasets with only 21k images. (see Table 2). We also show that OFDBs perform better than state-of-the-art methods for training ViTs on small datasets [44] (Table 3). 2.