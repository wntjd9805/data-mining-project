Abstract
Sparsely activated Mixture-of-Experts (MoE) is becom-ing a promising paradigm for multi-task learning (MTL).
Instead of compressing multiple tasks’ knowledge into a sin-gle model, MoE separates the parameter space and only utilizes the relevant model pieces given task type and its input, which provides stabilized MTL training and ultra-efficient inference. However, current MoE approaches adopt a fixed network capacity (e.g., two experts in usual) for all tasks.
It potentially results in the over-fitting of simple tasks or the under-fitting of challenging scenar-ios, especially when tasks are significantly distinctive in
In this paper, we propose an adaptive their complexity.
MoE framework for multi-task vision recognition, dubbed
AdaMV-MoE. Based on the training dynamics, it auto-matically determines the number of activated experts for each task, avoiding the laborious manual tuning of opti-mal model size. To validate our proposal, we benchmark it on ImageNet classification and COCO object detection
& instance segmentation which are notoriously difficult to learn in concert, due to their discrepancy. Extensive ex-periments across a variety of vision transformers demon-strate a superior performance of AdaMV-MoE, compared to MTL with a shared backbone and the recent state-of-the-art (SoTA) MTL MoE approach. Codes are available on-line: https://github.com/google-research/ google-research/tree/master/moe_mtl. 1.

Introduction
Multi-task vision recognition aims to simultaneously solve multiple objectives, which is commonly required in real-world applications. For instance, robotics [64] need to learn how to pick, place, cover, rearrange, and align objects simultaneously; autonomous vehicles [42] are expected to concurrently perform drivable area estimation, lane detec-tion, pedestrian detection, and more. Classic multi-task learning (MTL) methods [52, 60, 28, 48, 67, 85, 51] learn a shared representation among different tasks and attach task-*Equal Contribution. †Work done while at Google.
Figure 1. The multi-task vision recognition performance of vari-ous ViT architectures on ImageNet classification (y-axis), COCO object detection and instance segmentation (x-axis) benchmarks.
Averaged results of detection’s AP (%) and segmentation APmask (%) are reported. Markers ✫ and ❍ denote ours and baseline ap-proaches, respectively. A larger marker indicates more floating point operations (FLOPs) are used for inference. ViT-Small∗ is a reduced backbone variant with half transformer layers. specific heads. Following the generic trend in visual recog-nition, recent MTL works leveraged Vision Transformers (ViTs) [25, 68, 49, 10] as the new unified backbone [7, 5].
However, such MTL models with a single backbone suf-fer from unstable training and inefficient inference. As pointed out by [56, 81, 17], the shared parameters might receive conflicted update directions from different objec-tives, and this negative competition usually leads to poor training convergence, biased representations, and inferior performance. Meantime, existing MTL regimes usually activate the whole network backbone, regardless of what tasks come. It causes a waste of computations in potential since various real-world MTL systems [64] perform one or a few tasks at each moment, which may only require the relevant model pieces. The sparsely activated Mixture-of-Experts (SMoE) serves as an encouraging remedy for tack-ling these two MTL bottlenecks. Specifically, a pioneering study [46] inserts SMoE layers into the MTL ViT by replac-ing its dense feedforward network with a series of sparsely
activated MoE experts (e.g., multilayer perception (MLP)).
Then, task-dependent routing policies are enforced to se-lect a subset of task-relevant experts. Impressive results are demonstrated with this MTL MoE [46].
Despite these preliminary investigations, key challenges still persist in building an effective MTL system: How to determine an appropriate network capacity for each task in
MTL? By treating it as a hyperparameter, performing the manual tuning for each task is laborious and infeasible due to the entanglement between tasks. Thus, a fixed model size across all tasks is a conventional setup of existing MTL ap-proaches (e.g., always using 4 experts in [46]). However, this rigid and sub-optimal design potentially sacrifices the learning of certain tasks, since excessive or insufficient net-work capacity leads to either over-fitting or under-fitting in simple or complex scenarios, respectively [72]. The dis-advantages will be further amplified when optimizing mul-tiple tasks with a substantial variation in task complexity.
Take image classification and object detection tasks as ex-amples. First, the common benchmarks for classification have a lower input resolution like 32 × 32 for CIFAR [40] and 224 × 224 for ImageNet [24], while object detection is normally evaluated on the COCO [47] dataset with a higher resolution of 640×640 or 892×892. Second, to obtain a sat-isfying performance, the routine network for detection [9] is usually larger than the ones for classification [69], such as
ResNet-101 [34] versus ResNet-50. Third, as for the task objectives, object detection contains both object localiza-tion and recognition, and thus is more complicated than classification which can be essentially regarded as a sub-task. As discovered in [18, 33], their mismatched learning goals emphasize different feature proprieties (i.e., location invariant [8] versus sensitive). Given such heterogeneity of task complexity, these two tasks are notoriously difficult to learn together with a shared feature extractor and unified model size. An adaptive mechanism is therefore demanded.
In this paper, we propose AdaMV-MoE, to address the aforementioned key barriers, by seamlessly customizing the current state-of-the-art (SOTA) MTL MoE [46]. To be spe-cific, an adaptive expert selection mechanism is proposed to automatically determine the number of experts (or model capacity) in use for different vision tasks. We monitor the validation loss to adaptively determine activating more/less experts to prevent under-fitting/over-fitting. Our contribu-tions are summarized below:
⋆ We target the problem of multi-task vision recognition, and tackle the key challenge of choosing a suitable net-work capacity for distinctive tasks. According to train-ing dynamics, our algorithm controls the task-specific model size in an adaptive and automatic manner.
⋆ We introduce a customized MoE to resolve image clas-sification, object detection, and instance segmentation simultaneously, which used to be a troublesome com-bination for MTL. Visualization of our learned task-specific routing decisions is provided and exposes spe-cialization patterns, particularly for image contents.
⋆ Extensive experiments are conducted to reveal the ef-fectiveness of AdaMV-MoE in MTL, as shown in Fig-ure 1. For example, our approaches surpass the vanilla
MTL ViT with a shared feature extractor, by a signif-icant performance margin of {6.66% ∼ 7.39% accu-racy, 0.87% ∼ 1.13% AP, 0.84% ∼ 0.89% APmask} for {image classification, object detection, instance segmentation} on ImageNet and COCO datasets with
UViT-Base backbones [16]. 2.