Abstract
Conventional multi-label classification (MLC) methods assume that all samples are fully labeled and identically distributed. Unfortunately, this assumption is unrealistic in large-scale MLC data that has long-tailed (LT) distribution and partial labels (PL). To address the problem, we introduce a novel task, Partial labeling and Long-Tailed Multi-Label
Classification (PLT-MLC), to jointly consider the above two imperfect learning environments. Not surprisingly, we find that most LT-MLC and PL-MLC approaches fail to solve the
PLT-MLC, resulting in significant performance degradation on the two proposed PLT-MLC benchmarks. Therefore, we propose an end-to-end learning framework: COrrection →
ModificatIon → balanCe, abbreviated as COMIC. Our boot-strapping philosophy is to simultaneously correct the miss-ing labels (Correction) with convinced prediction confidence over a class-aware threshold and to learn from these recall labels during training. We next propose a novel multi-focal modifier loss that simultaneously addresses head-tail imbal-ance and positive-negative imbalance to adaptively modify the attention to different samples (Modification) under the LT class distribution. In addition, we develop a balanced train-ing strategy by distilling the model’s learning effect from head and tail samples, and thus design a balanced classifier (Balance) conditioned on the head and tail learning effect to maintain stable performance for all samples. Our experi-mental study shows that the proposed COMIC significantly outperforms general MLC, LT-MLC and PL-MLC methods in terms of effectiveness and robustness on our newly created
PLT-MLC datasets. Codes and benchmarks are available on the link https://https://github.com/wannature/COMIC 1.

Introduction
In recent years, the development of deep learning prosper-ity to the field of computer vision [20, 17, 19, 16, 23, 22, 25, 24, 43, 42, 18]. Images typically contain multiple objects and concepts, highlighting the importance of multi-label clas-sification (MLC) [34] for real-world tasks. Along with the wide adoption of deep learning, recent MLC approaches have made remarkable progress in visual recognition [36, 38], but the performance is limited by two common assumptions: all categories have comparable numbers of instances and each training instance has been fully annotated with all the relevant labels. While this conventional setting provides a perfect training environment for various studies, it conceals a number of complexities that typically arise in real-world applications: i) Long-Tailed (LT) Class Distribution. With the growth of digital data, the crux of making a large-scale dataset is no longer about where to collect, but how to bal-ance it [33]. However, the cost of expanding the dataset to a larger class vocabulary with balanced data is not linear — but exponential — since the data is inevitably long-tailed following Zipf’s distribution [30]. ii) Partial Labels (PL) of Instances. In the case of a large number of categories, it is difficult and even impractical to fully annotate all relevant labels for each image [39, 46]. Intuitively, humans tend to focus on different aspects of image contents due to human bias, i.e.,, their preference, personality and sentiment [41], which indirectly affects how and what we annotate. In fact,
LT and PL are often co-occurring, and therefore, the MLC model must be sufficiently robust to handle different data distributions and imperfect datasets.
In this paper, we present a new challenge for MLC at scale, Partial labeling and Long-Tailed Multi-Label Classifi-cation (PLT-MLC), with concomitant existence of both PL setting [39] and LT distribution [33] problems. As captured in the overview of PLT-MLC in Figure 1 (a), it has the fol-lowing three challenges: i) False Negative Training. Under the PL setting, the MLC model treats the un-annotated la-), which may produce sub-optimal bels ( decision boundary as it adds noise of false negative labels (Figure 1 (b)). The situation is further exacerbated in the
) as negatives (
Figure 1: (a) illustrates an overview of the proposed PLT-MLC task. (b) depicts three key challenges of a PLT-MLC task. (c)
: false negative, depicts a concise version of our proposed model for facilitating the PLT-MLC. (
: negative,
: positive,
: corrected positive)
) “person” (
LT class distribution as some tail categories are prone to missing annotations in practice. For instance, in Figure 1 (a), “person” is the head class in the PLT-MLC dataset and is often notable in an image to labeling for annotators.
In contrast, the “tie” often occupies a tiny region in the scene compared with the “person”. The annotator may miss the “tie” object, which will aggravate the LT distri-bution and further increase the difficulty of learning from tail classes. ii) Head-Tail and Positive-Negative Imbal-ance. There are two imbalance issues in a PLT-MLC task: inter-instance head-tail imbalance and intra-instance posi-tive-negative imbalance. As shown in Figure 1 (b), the inter-) : tail instance ratio of head positive (
) “tie” ( positive (
) ≈ 32 under the LT data distribution, and the intra-instance ratio of tail negative (
) categories
)“tie” (
: tail positive (
) = 78 as an image only con-tains a small fraction of the positive labels. Consequently, a robust PLT-MLC model should address the co-occurring imbalances simultaneously. iii) Head Overfitting and Tail
Underfitting. Different from the general LT distribution, the classification model downplays the minor tail and overplays the major head. The PLT-MLC has an extreme LT distribu-tion and Figure 1(c) illustrates an interesting phenomenon of MLC model learning: the general classification model is prone to overfitting to head class with extensive samples and underfitting to tail classes with a few samples. This figure also indicates that only the medium class shows a steady growth in performance, which means that existing LT meth-ods focusing on lifting up the tail performance may not solve the PLT-MLC problem satisfactorily.
Suppose a trained model is used to correct the missing labels and then an LT classifier is trained using the updated labels, we might not be able to obtain a satisfying PLT-MLC performance, either. While machine learning methods can easily detect the head samples, they may have difficulty in identifying the tail samples. As a result, the corrected labels may still exhibit an LT distribution that inevitably hurts bal-anced learning. Moreover, even when a general LT classifier affords the trade-off to improve the tail performance condi-tioned on the head performance drop, it is still incapable of simultaneously addressing the issue of head overfitting and tail underfitting problem. Further, the decoupled learning paradigm is impractical since it needs the “stop” training and human “re-start” training, i.e., an end-to-end learning scheme is more desirable. Thus, these limitations motivate us to reconsider the solution for the PLT-MLC task.
To this end, we propose an end-to-end PLT-MLC frame-work: COrrection → ModificatIon → balanCe (Figure 1), called COMIC, which progressively addresses the three key
PLT-MLC challenges. Step 1: The Correction module aims to gradually correct the missing labels according to the pre-dicted confidence and dynamically adjusts the classified loss of the corrected samples under the real-time estimated class distribution. Step 2: After the label correction, the Mod-ification module introduces a novel Multi-Focal Modifier (MFM) Loss, which contains two focal factors to address the two imbalance issues in PLT-MLC independently. Moti-vated by [3], the first is an intra-instance positive-negative factor that determines the concentration of learning on hard negatives and positives with different exponential decay fac-tors. The second is an inter-instance head-tail factor that increases the impact of rare categories, ensuring that the loss contribution of rare samples will not be overwhelmed by fre-quent ones. Step 3: Finally, the Balance module measures the model’s optimization direction with a calculated moving average vector of the gradient over all past samples. And thus, we devise a head model and a tail model by subtracting or adding this moving vector, which can respectively im-prove head and tail performance. Subsequently, a balanced classifier deduces a balanced learning effect under the super-vision of the head classifier and tail classifier. It protects the model training from being too medium biased, and hence
the balanced classifier is able to achieve the balanced learn-ing schema. Notably, our solution is an end-to-end learning framework, which is re-training-free and effectively enables balanced prediction.
Our contributions are three-fold: (1) We present a new challenging task: Partial labeling and Long-Tailed Multi-Label Classification (PLT-MLC), together with two newly designed benchmarks: PLT-COCO and PLT-VOC. (2) We propose an end-to-end PLT-MLC learning framework, called
COMIC, to effectively perform the PLT-MLC task as a pro-gressive learning paradigm, i.e., Correction → Modification
→ Balance. (3) Through an extensive experimental study, we show that our method improves all the prevalent LT and
ML line-ups on PLT-MLC benchmarks by a large margin. 2.