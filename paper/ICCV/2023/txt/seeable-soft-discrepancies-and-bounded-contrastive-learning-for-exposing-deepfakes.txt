Abstract
Modern deepfake detectors have achieved encouraging results, when training and test images are drawn from the same data collection. However, when these detectors are applied to images produced with unknown deepfake-generation techniques, considerable performance degrada-tions are commonly observed.
In this paper, we propose a novel deepfake detector, called SeeABLE, that formalizes the detection problem as a (one-class) out-of-distribution detection task and generalizes better to unseen deepfakes.
Specifically, SeeABLE first generates local image perturba-tions (referred to as soft-discrepancies) and then pushes the perturbed faces towards predefined prototypes using a novel regression-based bounded contrastive loss. To strengthen the generalization performance of SeeABLE to unknown deepfake types, we generate a rich set of soft discrepan-cies and train the detector: (i) to localize, which part of the face was modified, and (ii) to identify the alteration type. To demonstrate the capabilities of SeeABLE, we per-form rigorous experiments on several widely-used deep-fake datasets and show that our model convincingly outper-forms competing state-of-the-art detectors, while exhibiting highly encouraging generalization capabilities. The source code for SeeABLE is available from: https://github. com/anonymous-author-sub/seeable. 1.

Introduction
Recent advances in (deep) generative models, such as generative adversarial networks (GAN) [23], diffusion models [32] and generative normalizing flows [14], have made it possible to generate fake images and videos with unprecedented levels of realism. Human faces have been a particularly popular target for such models, enabling the creation of so-called deepfakes [12,19,65,66], i.e., manipu-lated facial images commonly used for malicious purposes.
These deepfakes have been shown to constitute a serious psychological and financial treat to individuals, but also so-ciety as a whole [8, 52]. As a result, the deep learning com-munity is actively working on countermeasures and detec-tion techniques that can help to mitigate this threat.
By having access to datasets with both, real and forged (manipulated) faces [15,16,36,41,47,57,70], existing deep-Figure 1: Examples of faces with soft-discrepancies. Can you identify the discrepancy in each image? SeeABLE can.
SeeABLE’s answer: the perturbed area of the four facial images is within the circle shown on the right. Note that a different soft discrepancy is used in each image. fake detectors [1,7,9,10,33,50,57] essentially learn a binary decision boundary that leads to reasonable detection per-formance with deepfake-generation techniques seen during training. However, as recent empirical studies [18, 39, 67] report, the performance of such (discriminatively-trained) detectors degrades significantly when used with unseen face-manipulation methods, which severely limits their use in real-life deployment scenarios [75].
A powerful solution to improve the generalization capa-bilities of deepfake detectors is to use synthetic data (i.e., pseudo deepfakes) during training and encourage the mod-els to learn generalizable decision boundaries. Such strate-gies are at the core of many of the state-of-the-art (SoTA) detection models [5, 45, 48, 60, 71] that either enrich the di-versity of available deepfakes by synthesizing novel fake images for training, or rely completely on synthetic deep-fakes when learning the detection models. These methods differ in the type of augmentations considered: blending-or adversarial-based techniques, adoption of global or local transformations, and use of single or multiple source/target images, as summarized in Table 1. Once the pseudo-fakes are generated, a classifier is learned to distinguish between real and fake faces. While these methods were observed to lead to highly competitive detection performance, espe-cially in cross-dataset settings, they are still limited by the discriminative nature of the training procedure that tries to differentiate between real faces and the specific artifacts in-duced by the pseudo-deepfake generation procedure.
In this paper, we propose a novel deepfake detector, called SeeABLE (Soft discrepancies and bounded con-trastive larning for exposing deepfakes), that formulates the detection problem as a (one-class) out-of-distribution detec-Deefake detection model
Face Xray [45], PCL [71], OST [48]
SLADD [5]
SBI [60]
SeeABLE (proposed)
Augmentation (blended, adversarial) Alteration artifact
Problem formulation global local multiple
✓ single multiple single
✓
✓
✓ global local classification regression
✓
✓
✓
✓
✓
✓
✓
✓
✓
Table 1: Comparison of SeeABLE and SoTA detectors that use pseudo-deepfake synthesis during model learning.
The existing techniques differ in terms of augmentation techniques used, the level at which alterations are applied (local vs. global), and the problem formulation. As can be seen, SeeABLE differs significantly from existing techniques. tion task and generalizes better to unseen deepfakes than discriminatively-learned models. SeeABLE is trained with images of real faces only and differs significantly from ex-isting (pseudo-fake based) detectors, as seen in Table 1.
Specifically, the model first generates (subtle) local image perturbations, referred to as soft discrepancies, using a rich set of image transformations, as illustrated in Figure 1.
Next, the generated soft discrepancies are pushed towards a set of target representations (i.e., hard prototypes) using a single multi-task regressor learned with a novel bounded contrastive regression loss. Here, the objective of the re-gressor is two-fold: (1) to map the different soft discrepan-cies into well-separated (and tightly clustered) prototypes that facilitate efficient similarity scoring (akin to prototype matching), and (2) to localize the spatial area of the lo-cal image perturbation and, thus, to exploit an auxiliary source of information for the regression task. The subtle im-age changes introduced by the soft discrepancies force See-ABLE to learn to detect minute image inconsistencies (and in turn a highly robust detector), whereas the local nature of the perturbation allows the model to exploit an additional localization (pretext) task that infuses complementary cues into the learning procedure. Unlike competing one-class detectors that typically rely on (low-level) per-pixel recon-structions to identify deepfakes, e.g. [37], SeeABLE, learns rich and semantically meaningful features for the detec-tion process that, as we show in the experimental section, lead to highly competitive detection results.
To demonstrate the capabilities of SeeABLE, we eval-uate the model in comprehensive cross-dataset and cross-manipulation experiments on multiple datasets, i.e., FF++
[57], CDF-v2 [47], DFDC-p [16] and DFDC [15], and in comparison to twelve SoTa competitors. The results of the experiments show that the proposed model achieves highly competitive results on all considered datasets, while exhibit-ing encouraging generalization capabilities.
In summary, the main contributions of this paper are:
• We propose SeeABLE, a new state-of-the-art deepfake detector trained in one-class self-supervised anomaly de-tection setting that captures high-level semantic infor-mation for the detection task by localizing artificially-generated (spatial and frequency-domain) image pertur-bations. Unlike (most) competing solutions, SeeABLE learns to provide an anomaly score that allows it to effi-ciently discriminate between real and fake imagery.
• We introduce a novel Bounded Contrastive Regression (BCR) loss that enables SeeABLE to efficiently push/map the local soft discrepancies to a predefined set of (evenly-distributed) prototypes, and, in turn, to facilitate distance-based prototype matching for deepfake detection.
• Through rigorous (cross-dataset and cross-manipulation) experiments on multiple dataset, we demonstrate the su-perior generalization capabilities of SeeABLE compared to existing (SoTA) deepfake detectors. 2.