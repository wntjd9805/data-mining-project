Abstract
Deepfake has recently raised a plethora of societal con-cerns over its possible security threats and dissemination of fake information. Much research on deepfake detection has been undertaken. However, detecting low quality as well as simultaneously detecting different qualities of deepfakes still remains a grave challenge. Most SOTA approaches are limited by using a single specific model for detecting certain deepfake video quality type. When constructing multiple models with prior information about video quality, this kind of strategy incurs significant computational cost, as well as model and training data overhead. Further, it cannot be scalable and practical to deploy in real-world settings.
In this work, we propose a universal intra-model collab-orative learning framework to enable the effective and si-multaneous detection of different quality of deepfakes. That is, our approach is the quality-agnostic deepfake detection method, dubbed QAD. In particular, by observing the upper bound of general error expectation, we maximize the de-pendency between intermediate representations of images from different quality levels via Hilbert-Schmidt Indepen-dence Criterion. In addition, an Adversarial Weight Pertur-bation module is carefully devised to enable the model to be more robust against image corruption while boosting the overall model’s performance. Extensive experiments over seven popular deepfake datasets demonstrate the superior-ity of our QAD model over prior SOTA benchmarks. 1.

Introduction
Deep learning approaches for facial manipulation, such as deepfakes, have recently received considerable attention
[54, 31, 61, 25, 20, 23], because they can be abused for the malicious purposes such as fake news, pornography, etc.
Due to the advancements made in Generative Adversarial
Networks and other deep learning-based computer vision algorithms, deepfakes have also become more realistic and
*Corresponding author.
Figure 1. A summary of our goal. Our approach stands out from previous works that detect deepfakes using separate models for different qualities (e.g. Baseline 2 [28]) or a single model with-out considering the interaction between qualities (e.g. Baseline 1 [43]). Instead, our method employs all quality levels and im-proves the performance of the model on each quality level, leading to overall enhanced performance. natural, making it harder not only for humans, but also for classifiers to tell them apart. Moreover, it has been simpler than ever before to create convincing deepfakes using sim-ple programs and apps without requiring advanced machine learning knowledge. Such easy-to-create and realistic fake images and videos can be maliciously exploited, raising sig-nificant security, privacy, and societal concerns such as fake news propagation [39], and stealing personal information via phishing and scams [10].
To mitigate such problems caused by deepfakes, there has been a tremendous research effort put into constructing reliable detectors [32, 25, 8, 38, 61, 44]. Although they have achieved outstanding performance with high-quality deep-fakes, most of them have failed to detect low-quality deep-fakes effectively [8, 43]. While video compression steps do not significantly impact on visualization, it drastically drop deepfake detectors’ performance on low-quality deepfakes (c40). A handful of research has been focused on detecting low-quality deepfakes such as ADD [28] and BZNet [29].
However, their methods can only detect low-quality com-pressed deepfakes. And, those prior approaches expose a critical problem, when deployed in practice since the prior video quality information of the input is unknown. More-over, developing different models for each input quality induces significant computational overhead. Other works, such as LipForensics [16], also attempt to make their de-tectors robust against various corruptions and compression.
Nevertheless, it is unable to detect image-based deepfakes with random lossy compression like JPEG.
In this research, we propose the novel deepfake detec-tion method, QAD, which can simultaneously detect both high and low-quality (quality-agnostic) deepfakes in a sin-gle model, as illustrated in Fig. 1. Especially, we propose a universal intra-model collaborative learning framework to provide the effective detection of different quality deep-fakes. We modulate the conventional model-based collab-orative learning [47] to an instance-based intra-model col-laborative learning framework in our training. During the training phase, our single model simultaneously learns the representations of one image, but with different qualities.
By utilizing the collaborative learning framework, our QAD can align the distributions of high and low-quality image representations to be geometrically similar. Hence, it can avoid the overfitting caused by compressed images and the overconfidence caused by raw images, while boosting its overall performance.
In particular, we perform a rigorous theoretical analysis, and show that the low-quality deepfake classification error can be bounded by two terms: classification loss and the dis-tance between the representations of high and low-quality images. Instead of using a direct pairwise regularization to minimize the gaps between the high and low-quality image representations, we propose to apply Hilbert-Schmidt In-dependence Criterion (HSIC) to maximize the dependence between a mini-batch of high and low-quality images, thus maximizing the mutual information between them, and sup-porting the high-level representations and effective output predictions. Meanwhile, to enhance the model’s robust-ness under heavy input compression, we propose Adversar-ial Weight Perturbation (AWP) [56, 3], which can further flatten the weight loss landscape of the model, bridging the gap in multiple quality learning for deepfake detection.
Finally, we conduct extensive experiments to show the effectiveness of our QAD with seven different popular benchmark datasets. We first show that our method can out-perform previous baselines when training with data from various video and image compression qualities. Further-more, we show that our QAD exceeds the performance of the SOTA quality-aware models such as BZNet [29] by a significant margin, while requiring remarkably fewer com-putational parameters and no prior knowledge of the inputs.
Our contributions are summarized as follows: 1) We theoretically analyze and prove that the classifica-tion error of low-quality deepfakes can be bounded by its classification loss and the representation distance with its corresponding high-quality images. 2) We propose a unified quality-agnostic deepfake de-tection framework (QAD), utilizing instance-based intra-model collaborative learning. We use the Hilbert-Schmidt
Independence Criterion (HSIC) to maximize the geometri-cal similarity between intermediate representations of high and low-quality deepfakes, and Adversarial Weight Pertur-bation (AWP) to make our model robust under varying input compression. 3) We demonstrate that our approach outperforms well-including the total of eight quality-known baselines, agnostic and quality-aware SOTA methods with seven pop-ular benchmark datasets. 2.