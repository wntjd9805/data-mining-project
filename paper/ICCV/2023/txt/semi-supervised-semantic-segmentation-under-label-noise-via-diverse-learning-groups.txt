Abstract
Semi-supervised semantic segmentation methods use a small amount of clean pixel-level annotations to guide the interpretation of a larger quantity of unlabelled image data.
The challenges of providing pixel-accurate annotations at scale mean that the labels are typically noisy, and this con-taminates the final results. In this work, we propose an ap-proach that is robust to label noise in the annotated data.
The method uses two diverse learning groups with different network architectures to effectively handle both label noise and unlabelled images. Each learning group consists of a teacher network, a student network and a novel filter mod-ule. The filter module of each learning group utilizes pixel-level features from the teacher network to detect incorrectly labelled pixels. To reduce confirmation bias, we employ the labels cleaned by the filter module from one learning group to train the other learning group. Experimental results on two different benchmarks and settings demonstrate the su-periority of our method over state-of-the-art approaches. 1.

Introduction
Semantic segmentation is a critical step in many com-puter vision tasks, and one that, like so many, is now pre-dominantly achieved through deep learning [5, 35]. A key factor in the success of these methods has been access to high-quality annotated datasets. Pixel-accurate annotation of segmentation data is challenging, and becomes imprac-tical as datasets grow. Unlabelled data is abundant and thus semi-supervised semantic segmentation has received increasing attention [15, 20, 8]. The predominant semi-supervised approach employs a teacher-student framework to allocate pseudo labels to unlabelled data [11, 23, 36, 40].
These methods rely on the availability of a small but per-fectly annotated dataset, however. This is a challenge in many practical situations where annotations are collected via crowd-sourcing (e.g., Sagemaker MTurk), or automated labelling techniques [31, 3], and are thus inevitably noisy.
Figure 1: Existing semi-supervised semantic segmentation models are vulnerable to the inevitable label noise in the pixel-level annotations. The proposed method performs sig-nificantly better than the current state-of-the-art [20] in the presence label noise. We apply three types of noise (labelled
PL, RDE and SCP in Sec. 5.2) to achieve a 9% error rate in the labelled data.
We consider here the practically important case whereby semi-supervised semantic segmentation is to be applied de-spite the fact that the provided pixel labels contain noise.
We propose an approach that is robust to this noise to the ex-tent that it outperforms the baselines by a significant margin (see Fig. 1), despite being relatively simple to implement and incurring no extra computation cost at test time.
It has been shown recently [19] that supervised segmen-tation models trained on noisy pixel-level labels first fit the clean labels during an “early-learning” phase before even-tually memorizing the noisy labels. The above learning dy-namics are exploited in [19] for weakly supervised segmen-tation by adding a multi-scale consistency loss. However, no noise detection module is used in [19] and the effect of unsupervised consistency loss on the label noise is weak.
Recently there has been a surge of interest in data-centric methods [38, 39] that utilize the k-NN of the feature rep-resentation of the data-points to detect annotation noise in classification datasets. A straightforward extension of this approach to semantic segmentation would lead to comput-ing the k-NN of the feature representation at pixel-level over all images, which would be infeasible. In this work, we 1
adapt [39] to semi-supervised semantic segmentation by in-troducing an explicit label noise detection module that pre-dicts label noise at the pixel level without significant addi-tional memory or computation requirements.
Pseudo labelling approaches [11, 23, 36, 2] for semi-supervised segmentation often use a single model to gen-erate pseudo labels on the unlabelled data as well as train on them. This leads to confirmation bias as the model can overfit to its prediction errors. The same bias is apparent even when a single model is employed to filter label noise and train on those cleaned labels [19]. To mitigate this effect, we introduce two diverse learning groups with dif-ferent network architectures that encourage mutual knowl-edge distillation and effective noise filtering. These learn-ing groups also exploit complementary learning paths that boost the competency of each group. Our contributions are thus summarized as follows:
• We propose an approach to semi-supervised semantic segmentation that is robust to label noise.
• We present a general architecture that maintains two diverse learning groups to overcome confirmation bias in label assignment problems.
• We introduce a filter module that extends [39] for semantic segmentation while demonstrating a good trade-off between accuracy and computation cost.
• We conduct extensive experiments showing that our method significantly outperforms semi/weakly-supervised semantic segmentation baselines regardless of the presence or the type of label noise. 2.