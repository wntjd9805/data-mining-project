Abstract
In contrastive self-supervised learning, the common way to learn discriminative representation is to pull different augmented “views” of the same image closer while pushing all other images further apart, which has been proven to be effective. However, it is unavoidable to construct unde-sirable views containing different semantic concepts during the augmentation procedure. It would damage the semantic consistency of representation to pull these augmentations closer in the feature space indiscriminately. In this study, we introduce feature-level augmentation and propose a novel semantics-consistent feature search (SCFS) method to miti-gate this negative effect. The main idea of SCFS is to adap-tively search semantics-consistent features to enhance the contrast between semantics-consistent regions in different augmentations. Thus, the trained model can learn to fo-cus on meaningful object regions, improving the semantic representation ability. Extensive experiments conducted on different datasets and tasks demonstrate that SCFS effec-tively improves the performance of self-supervised learning and achieves state-of-the-art performance on different down-stream tasks.1 1.

Introduction
Due to the tremendous potential in learning discriminative feature representation without using data annotations, self-supervised learning, including contrastive learning [16, 5] and masked image modeling (MIM) [1, 15, 38] has received much attention in the representation learning field. Con-trastive learning, as a type of discriminative self-supervised learning method, is heavily studied and has shown remark-able progress in the computer vision field in recent years.
It aims at pulling different augmented “views” of the same image (positive pairs) closer while pushing diverse images (negative pairs) far from each other. To this end, a contrastive 1Code: https://github.com/skyoux/scfs
Figure 1. Semantic inconsistency of over-augmentation. (a) shows three augmentations of two images, in which the third augmentation is over-augmented and contains only background. (b) shows cate-gory probability distributions of the corresponding images in (a), which are obtained from a supervised pre-trained ResNet50 [18] model. (c)(d)(e) show three different samples of an image (the data-augmented image, the original image, and the semantics-consistent feature-augmented sample generated by Eq. (7) in this study) and their corresponding probability distributions, which point out that the over-augmented image generates different category with the original image, while the feature-augmented sample gets a balanced category probability. loss between the features of different views extracted from an encoder network is employed to train the encoder network end-to-end. According to whether the negative pairs are used, current contrastive learning can be generally divided into two categories.
The first category [16, 5] utilizes both positive pairs and negative pairs for contrast. MoCo [16, 7] uses a momentum update mechanism to maintain a memory bank of negative examples. SimCLR [5, 6] directly trains a single encoder network with a large batch size to ensure sufficient positive and negative samples for learning. Based on MoCo and Sim-CLR, some methods [24, 20, 36, 10, 49, 39, 19, 9, 48, 14] are proposed to improve the performance. For example, MSF
[20], ISD [36] and NNCLR [10] aim to search semantics-consistent samples for contrast, solving the false negative problem. While some studies, such as Momentum2Teacher
[24] and DCL [49], aim to solve the limitation that large batch size is necessary for satisfactory performance.
The second category of contrastive learning methods
[13, 8, 3, 50, 2, 12, 4, 31, 35] only constructs positive pairs for contrast. Based on MoCo [16] and SimCLR [5], respec-tively, BYOL [13] and SimSiam [8] abandon the negative samples and use an asymmetric architecture to avoid model collapse. SwAV [3] uses online clustering to cluster samples and forces the consistency among cluster assignments of dif-ferent augmentations. After that, some studies [4, 39, 10, 19] point out that enriching the augmented samples can improve the performance of contrastive learning. In addition, the study in [31] shows that improving the quality of positive augmented samples is important for self-supervised learning.
However, it is unavoidable to construct data augmen-tations containing different semantic concepts. Fig. 1(a) shows three augmentations of two images, in which the third augmentation is over-augmented and contains only the background. Fig. 1(b) shows category probability distribu-tions of the corresponding images in (a), which are obtained from a supervised pre-trained ResNet50 [18]. We observed that the probability distribution of over-augmented images changes greatly compared with the first two augmentations, which indicates that the semantic information of the over-augmented images deviates from the normally-augmented images. Similar observation can be found in Fig. 1(c)(d).
The original image in (d) shows a max probability for “am-bulance”, while the over-augmented image in (c) represents the different category “telescope”. Due to such semantic inconsistency, conducting contrastive learning on these over-augmentations is harmful to representation learning. In this study, we found that semantics-consistent feature augmenta-tion (Fig. 1(e), generated by Eq. (7)) can balance the original semantics “ambulance” and the over-augmented semantics
“telescope”, which can alleviate the influence of semantic inconsistency.
Motivated by this observation, we propose a novel semantics-consistent feature search (SCFS) method to al-leviate the negative influence of semantic inconsistency in contrastive learning. SCFS utilizes the global feature of a view to adaptively search the semantics-consistent features of another view for contrast according to their similarity. It constructs informative feature augmentations and conducts contrast learning between feature augmentations and data augmentations. Thus, the pre-trained model can learn to focus on meaningful object regions to alleviate the nega-tive influence of unmatched semantic alignment in current contrastive learning for better representation learning. In addition, the feature search is conducted on multiple layers of the backbone network, further enhancing the semantic alignment at different scales of features. Extensive exper-iments conducted on different datasets and tasks demon-strate that SCFS effectively improves the performance of self-supervised learning and achieves state-of-the-art per-formance on different downstream tasks. For example, it achieves state-of-the-art 75.7% ImageNet top-1 accuracy under the pre-training setting of 1024 batch size and 800 epochs for ResNet50.
The main contributions of this study are threefold:
• A novel contrastive learning method, i.e., SCFS, is proposed, and it can enhance semantic alignment in contrastive learning. To our knowledge, this is the first work that defines a feature search task in contrastive learning.
• We expand contrastive learning from a data-to-data manner to a feature-to-data manner, which enriches the diversity of augmentations.
• The proposed SCFS achieves state-of-the-art perfor-mance on different downstream tasks. 2.