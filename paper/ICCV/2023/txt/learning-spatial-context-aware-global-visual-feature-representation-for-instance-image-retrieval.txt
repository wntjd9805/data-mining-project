Abstract
In instance image retrieval, considering local spatial in-formation within an image has proven effective to boost re-trieval performance, as demonstrated by local visual de-scriptor based geometric verification. Nevertheless, it will be highly valuable to make ordinary global image repre-sentations spatial-context-aware because global represen-tation based image retrieval is appealing thanks to its al-gorithmic simplicity, low memory cost, and being friendly to sophisticated data structures. To this end, we propose a novel feature learning framework for instance image re-trieval, which embeds local spatial context information into the learned global feature representations. Specifically, in parallel to the visual feature branch in a CNN backbone, we design a spatial context branch that consists of two modules called online token learning and distance encod-ing. For each local descriptor learned in CNN, the for-mer module is used to indicate the types of its surrounding descriptors, while their spatial distribution information is captured by the latter module. After that, the visual fea-ture branch and the spatial context branch are fused to produce a single global feature representation per image.
As experimentally demonstrated, with the spatial-context-aware characteristic, we can well improve the performance of global representation based image retrieval while main-taining all of its appealing properties. Our code is available at https://github.com/Zy-Zhang/SpCa. 1.

Introduction
Given a query image, the purpose of instance-level im-age retrieval is to search and retrieve the images containing the identical object described by the query from a large-scale image dataset. In this task, visual feature representa-tions of images play a crucial role in measuring the sim-ilarities between a query and candidate images. A vari-ety of handcrafted feature-based methods [14, 38, 2] have been proposed to significantly improve the performance in the past two decades. Recently, due to the development of deep learning technologies, deep feature representations have been overtaking the position of conventional hand-crafted ones and bringing great progress in the task of in-stance image retrieval [1, 3, 17, 28, 15].
Generally, deep feature representations used in instance image retrieval can be categorized into two types. One type is global feature representation, which describes the visual content of an image as a whole. As a multi-dimensional vector, it can be efficiently used to measure the similarity of two images, say, via Euclidean distance or cosine similarity.
For a retrieval task, the total number of global feature repre-sentations is just the size of image database, and they can be pre-extracted and economically stored for use. In addition, one global feature representation per image works well with the classic data structures designed for searching. The other type is local descriptors, which describe the local informa-tion within an image and collectively reflect the spatial in-formation of the visual cues in an image. In image retrieval, they are important for conducting geometric verification to confirm if two images truly match or not. However, the total number of local descriptors per image could be large (e.g., 1, 000) and the verification involves non-trivial com-putation, making this process expensive in computational cost and memory footprint.
This situation leads to a wide use of “two-stage” paradigm in instance image retrieval [23, 3, 17]. An ini-tial retrieval result is firstly obtained via the global feature representation. After that, a re-ranking step utilizes the lo-cal descriptors to refine a small number of top-retrieved im-ages. Nevertheless, this two-stage paradigm not only re-sults in two separate procedures [26, 10] but also consider-ably increases retrieval time and memory expense for prac-tical retrieval tasks [18], due to the presence of the local descriptor-based spatial verification step. Therefore, it will be highly valuable to make ordinary global image represen-tations spatial-context-aware by considering their appealing properties of algorithmic simplicity, low memory cost, and being friendly to data structures.
To achieve this, we propose a novel feature learning
framework to effectively embed spatial context information into global feature representation of images. Doing so will help to boost the retrieval performance of global feature rep-resentation, improving its efficacy when the local descriptor based re-ranking becomes costly or infeasible.
Specifically, two types of information are extracted in our framework. One is conventional visual information ob-tained by a visual feature branch in a CNN backbone. The other one is spatial context information that describes for each local descriptor learned in CNN, what kind of sur-rounding local descriptors are and how they spatially dis-tribute on a feature map. To obtain this information, we develop a spatial context branch that operates in parallel to the visual feature branch in CNN. This branch consists of two modules called online token learning and distance encoding. The module of online token learning addresses the “what kind” issue. A set of semantic tokens that could be regarded as anchors in the space of local descriptors is learned in an online manner. Comparing the tokens to the visual words in a visual dictionary, each local descriptor can be uniquely labelled via soft coding as a token-based iden-tification. The module of distance encoding cares about the
“spatial distribution” issue. A probability transition based encoding is devised to reflect the relative spatial distance between each pair of local descriptors, so as to capture their spatial distribution information in a feature map. After that, the token-based identification and the spatial distribution in-formation are integrated to produce a spatial context clue for each local descriptor. With both visual and spatial informa-tion, a feature fusion operation is conducted to fuse them together to generate spatial-context-aware local descriptors.
Finally, a global pooling followed by a whitening layer is added to embed the context-aware local descriptors into a global feature representation for each image.
Our contributions are summarised as follows: i. We propose an end-to-end feature learning framework to characterise and embed spatial context information into the process of information processing and extraction in
CNN. It makes global feature representations become more capable for instance image retrieval. ii. To implement the framework, we design a spatial con-text branch. With its online token learning module, the types of local descriptors are identified and easily com-pared. With the distance encoding module, the informa-tion about the spatial distribution of different types of local descriptors around a given descriptor is obtained. iii. To verify the efficacy of the proposed framework, we conduct extensive experiments on instance image re-trieval benchmark datasets, ROxford and RParis, with one million distractors. As demonstrated, our global feature representations effectively improve the perfor-mance of instance image retrieval, making global repre-sentation a more competitive option for practical tasks. 2.