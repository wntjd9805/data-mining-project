Abstract
Physical adversarial attacks against deep neural net-works (DNNs) have recently gained increasing attention.
The current mainstream physical attacks use printed adver-sarial patches or camouflage to alter the appearance of the target object. However, these approaches generate conspic-uous adversarial patterns that show poor stealthiness. An-other physical deployable attack is the optical attack, fea-turing stealthiness while exhibiting weakly in the daytime with sunlight. In this paper, we propose a novel Reflected
Light Attack (RFLA), featuring effective and stealthy in both the digital and physical world, which is implemented by placing the color transparent plastic sheet and a paper cut of a specific shape in front of the mirror to create different colored geometries on the target object. To achieve these goals, we devise a general framework based on the circle to model the reflected light on the target object. Specifi-cally, we optimize a circle (composed of a coordinate and radius) to carry various geometrical shapes determined by the optimized angle. The fill color of the geometry shape and its corresponding transparency are also optimized. We extensively evaluate the effectiveness of RFLA on different datasets and models. Experiment results suggest that the proposed method achieves over 99% success rate on differ-ent datasets and models in the digital world. Additionally, we verify the effectiveness of the proposed method in differ-ent physical environments by using sunlight or a flashlight. 1.

Introduction
Deep neural networks (DNNs) have increasingly been applied to daily life as their dramatic capabilities, such as
*Corresponding Author. 1
Figure 1. The reflected light is modulated by the color trans-parency plastic sheet and paper cut of the specific shape for better attack performance. The Reflected light source can be sunlight or a flashlight (when the sunlight is unreachable). automatic driving, facial payment, and computer-aided di-agnosis. However, DNN-based systems have exposed se-curity risks caused by adversarial examples [39]. Adver-sarial examples are crafted by carefully designed noise that is invisible to humans but can deceive the DNNs. Further-more, recent researches [34, 21] reported that physically de-ployed DNN-based systems are also exposed to such secu-rity risks. Therefore, exploring various potential risks in security-sensitive systems to avoid possible loss is urgent.
Existing adversarial attack methods can be categorized into digital attacks and physical attacks. The former focus on pursuing higher attack performance on limitation condi-tions, such as breaking the model equipped with adversar-ial defense [20, 3, 7, 24], preventing the attacker from ac-cessing the target model’s information (e.g., architecture or dataset), i.e., black-box attack [1, 22, 23]. Although some researchers suggested that adversarial examples generated by the digital attack can be applied to physical attacks [21],
the attack performance is not satisfying. The possible rea-son is that the adversarial perturbation is too small to resist the environmental noise in the physical world. In contrast, physical attacks are designed to be physically deployable, where one crucial change is eliminating the perturbation’s magnitude constraint.
A line of physical adversarial attack methods [10, 9, 43, 44] has been proposed, which can be grouped into contact attacks and contactless attacks. The former requires the at-tacker to approach the target object and then modify the ap-pearance of the target object by pasting the adversarial patch or camouflage. However, the adversarial pattern generated by these methods is conspicuous, which easily alerts hu-mans and results in attack failure. By contrast, contactless physical attacks do not require the attacker to approach the target object while modifying the appearance of the target object by projecting or emitting light or laser on the tar-get object to perform attacks, making it stealthy and dan-gerous. Optical attacks are representative contactless at-tacks. Although several optical attacks have been proposed
[28, 32, 8, 17], they merely work in dark environments as the strong light (e.g., sunlight) environment would disturb the emitted light, limiting their usability.
In this paper, we get inspiration from the fact that the driver is easily affected by the strong reflected light, result-ing in a potential car accident, and such potential risks to the automatic driving system remain unexplored. We explore the vulnerability of the DNNs toward the reflected light at-tack by elaborately designing the position, geometry, and color of the reflected light. Specifically, we propose a Re-flected Light Attack (RFLA), which can solve the issue of the poor attack performance of existing optical attacks in strong-light environments, as our light source is sunlight.
To perform physical attacks, we use a mirror to reflect the sunlight toward the target object to modify its appearance.
However, the monotonous sunlight (usually white) may not obtain the desired performance. Therefore, we first use different colored transparent plastic sheets to modulate the color of the reflected light, then apply a paper cut of a spe-cific shape to control the shape of reflected light on the tar-get object (see Figure 1). Finally, we can create different colors and shapes of the reflected light on the target object’s specific region to achieve desired attack performance.
To achieve the above goals, we present a general frame-work based on the circle to model the above problem.
Specifically, we first initialize a circle with a random co-ordinate and radius. On this circle, we create a point on the circle using sine and cosine with a randomly selected an-gle. Then, we customize a shape by adding a new angle, which is used to create a new point in the circumference.
The other points required to create a geometry can be ob-tained by applying the center symmetry of the circle. More-over, the fill color and its transparency are also considered in the optimization. Finally, we adopt the particle swarm op-timization (PSO) algorithm to find the optimal result. Our contributions are listed as follows.
• We propose a novel reflect-light-based physical adver-sarial attack under the black-box scenario. It reflects the natural sunlight toward the target object using a mirror, making it controllable and stealthy.
• We devise a general framework based on a circle to search for the best position, geometry, and color of the reflected light to achieve better attack performance.
• We comprehensively investigate the influence of the geometry, position, and color of the reflected light on attack performance in the digital world. We conduct the physical adversarial attack by using sunlight for daytime and a flashlight for sunlight unavailable, and the experiment results verify the effectiveness of the proposed method. 2.