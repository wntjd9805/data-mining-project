Abstract
Event-based cameras offer reliable measurements for preforming computer vision tasks in high-dynamic range environments and during fast motion maneuvers. However, adopting deep learning in event-based vision faces the chal-lenge of annotated data scarcity due to recency of event cameras. Transferring the knowledge that can be obtained from conventional camera annotated data offers a practi-cal solution to this challenge. We develop an unsupervised domain adaptation algorithm for training a deep network for event-based data image classification using contrastive learning and uncorrelated conditioning of data. Our solu-tion outperforms the existing algorithms for this purpose. 1.

Introduction
Inspired by biological visual systems, event-based cam-eras are designed to measure image pixels independently and asynchronously through recording changes in bright-ness, compared to a reference brightness level [25]. A stream of events is measured which encodes the location, time, and polarity of the intensity changes. Compared to conventional frame-based cameras, they offer high dynamic range, high temporal resolution, and low latency which make them ideal imaging devices for preforming computer vision (CV) tasks that involve high dynamic range and fast movements. Despite this promising prospect, adopting arti-ficial intelligence and particularly deep learning to automate
CV tasks in this domain is challenging. The primary reason is that training deep neural networks relies on manually an-notated large datasets [49]. However, annotated event-data is scarce due to the recency of these cameras: event-based data represent only 3.14% of existing vision data [18].
Unsupervised domain adaptation (UDA) [10] is a learn-ing framework to train a model for a target domain with unannotated data through transferring knowledge from a secondary source domain with annotated data. The core idea in UDA is to reduce the distributional domain gap by mapping data from both domains into a shared domain-agnostic embedding space [1, 2, 55, 36, 45, 46, 47]. Al-though event-based and frame-based cameras are signifi-cantly different, the intrinsic relationship between their out-puts through the real-world relates the information content of the outputs considerably. As a result, a frame-based an-notated dataset can be served as a source domain for an event-based domain as the target domain in the context of a UDA problem. Even if the related frame-based data is unannotated, annotating frame-based data is much simpler.
The above possibility has led to a line of UDA algorithms for event-based CV tasks. Initial works in this area consider datasets with paired events and frames, i.e., the pixel-level recordings describe the same input [18, 63]. Despite be-ing effective, these methods are highly limited because they consider that both measurement types are preformed simul-taneously. Recently, a few UDA algorithms have been de-veloped for unpaired event-based data [43, 32]. An effec-tive idea is to use the idea of translation in UDA [34] and generate events for corresponding video data [12, 19]. An alternative idea is to generate event-based annotated data synthetically [41] and use it as the source domain [39].
We develop a new UDA algorithm for event-based data using unpaired frame-based data. Our contribution is two-fold. First, we use contrastive learning in combination with data augmentation to improve the model generalizability.
The idea is to project different augmentation types of one object to a latent representation and train an encoder to maintain identities for all augmentation types. Second, we introduce an uncorrelated conditioning loss term. The new loss term is used to regularize the model with additional in-formation: the latent vector representation of an object un-der event cameras should be uncorrelated to how the object looks like under event cameras. Experiments show that our method leads to learning a better representation of events in an unsupervised fashion. Our results indicate 2.0% on Cal-tech101 → N-Caltech101 and 3.7% on CIFA10 → CIFA10-DVS performance improvements over state-of-the-art.
2.