Abstract
Temporal Action Detection (TAD) is challenging but fundamental for real-world video applications. Recently,
DETR-based models have been devised for TAD but have not performed well yet. In this paper, we point out the prob-lem in the self-attention of DETR for TAD; the attention modules focus on a few key elements, called temporal col-lapse problem.
It degrades the capability of the encoder and decoder since their self-attention modules play no role.
To solve the problem, we propose a novel framework, Self-DETR, which utilizes cross-attention maps of the decoder to reactivate self-attention modules. We recover the rela-tionship between encoder features by simple matrix multi-plication of the cross-attention map and its transpose. Like-wise, we also get the information within decoder queries.
By guiding collapsed self-attention maps with the guidance map calculated, we settle down the temporal collapse of self-attention modules in the encoder and decoder. Our ex-tensive experiments demonstrate that Self-DETR resolves the temporal collapse problem by keeping high diversity of attention over all layers. Moreover, it is validated that our simple framework achieves a new state-of-the-art per-formance on THUMOS14 and outperforms all the DETR-based approaches on ActivityNet-v1.3. 1.

Introduction
Understanding videos has become fundamental as un-countable videos are produced all over devices every mo-ment. In the first place, action recognition using trimmed video clips led the field with tremendous advances during past decades. However, unacceptable costs to snip real-world videos fostered the literature towards temporal ac-tion detection (TAD). Temporal action detection not just classifies an action but also predicts time boundaries of untrimmed video.
Pioneering methods [2, 3, 11] adopted the concept of fixed-length windows called action proposals inspired by
†Corresponding author
Figure 1: Temporal collapse problem of self-attention.
The figure shows self-attention maps by DETR-based meth-ods in object detection (OD) and temporal action detection (TAD). Each map at the top and bottom is from the last layer of the encoder and decoder, respectively. We can see that the self-attention maps of the encoder and decoder in
TAD are collapsed to a small number of keys (b, e). On the other hand, those from OD and ours show high correlation for neighboring features (a, c) or query themselves (d, f). object detection [15, 16, 36]. Meanwhile, the following approaches [20, 25, 53, 58] developed point-wise learning where they predict probabilities of start and end boundaries to solve the low-recall issue. They reached the high-recall score from more flexible action proposals by grouping each pair of start and end boundaries, but unfortunately, a bunch of generated proposals with various lengths made the rank-ing process more challenging. Accordingly, the previous methods heavily rely on ranking with post-processing such as non-maximum suppression (NMS) to cope with low-precision action proposals.
As DETR [4] has had a great impact on object detec-tion, DETR-based methods for videos [22,30,32,39,44] are also introduced recently. In TAD, queries of DETR are de-fined as action instances of a video with their time intervals, called action queries. Here, the model learns to map these query vectors to relevant temporal features of the video to
classify and localize the actions of interest. Since there is no pre-set mapping between queries and ground-truth in-stances, bipartite matching associates them with minimal cost for the objectives to assign the labels. This approach can tackle the task in an end-to-end manner without any heuristics like NMS via the set-based objective.
However, it is discovered that the original DETR ar-chitecture suffers from several problems with videos and thereby does not perform well in TAD. It has been esti-mated that the main problem is the failure of dense atten-tion mechanism [30, 44]. Dense attention here indicates standard attention mechanism which relates all elements without any inductive bias such as locality in convolution.
To address this issue, previous DETR-based approaches in
TAD revised the standard attention to boundary-sensitive module [44], deformable attention [30], or query relational attention [39].
Nevertheless, the problem of the standard attention still remains setting back DETR for TAD far behind in perfor-mance. In this paper, we confront the problem of the stan-dard attention. Fig. 1 shows self-attention maps of DETR methods. From the figure, we find that the self-attention for
TAD severely suffers from collapse to a few key elements, which we define as temporal collapse problem. This phe-nomenon implies that the model selects the shortcut to skip
In the self-attention to elude degeneration of the output. contrast, self-attention in object detection and ours is highly correlated without any collapse. Hence, we point out that the temporal collapse problem in self-attention is the core to degrade DETR-based methods for TAD.
To solve the problem, we propose a new framework,
Self-DETR, which provides feedback to self-attention from the encoder-decoder cross-attention. The cross-attention map contains the entire relation between the encoder and decoder features. We view the similarity between decoder queries as how much they focus on similar encoder fea-tures. Likewise, we consider the similarity between encoder features as how much they are attended by analogous de-coder queries. We can obtain these two kinds of guidance by simple matrix multiplication of the cross-attention map and its transpose. From these guidance maps, the temporal collapse is relaxed by minimizing the gap of the guidance and self-attention maps. Through our extensive experiments on the public benchmarks, we validate that Self-DETR set-tle downs the collapse by retaining high diversity of atten-tion. As a result, Self-DETR has achieved a new state-of-the-art performance on THUMOS14, and outperforms all the DETR-based methods in ActivityNet-v1.3 without de-formable attention.
To sum up, our main contributions are as follows: encoder and decoder.
• We propose a novel framework, Self-DETR, which uti-lizes cross-attention maps to provide feedback to self-attention of the encoder and decoder to prevent the temporal collapse.
• Our extensive experiments demonstrate that Self-DETR blocks the temporal collapse efficiently by keeping high diversity of attention. Also we vali-date that our model reaches a new state-of-the-art per-formance on THUMOS14, and outperforms all the
DETR-based methods on AcitvityNet-v1.3. 2.