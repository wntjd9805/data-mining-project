Abstract
We introduce Iterated Integrated Attributions (IIA) - a generic method for explaining the predictions of vision mod-els. IIA employs iterative integration across the input im-age, the internal representations generated by the model, and their gradients, yielding precise and focused explana-tion maps. We demonstrate the effectiveness of IIA through comprehensive evaluations across various tasks, datasets, and network architectures. Our results showcase that IIA produces accurate explanation maps, outperforming other state-of-the-art explanation techniques. 1.

Introduction
The emergence of deep learning has ushered in signiﬁcant breakthroughs within the realm of artiﬁcial intelligence, par-ticularly in computer vision. Advanced deep Convolutional
Neural Networks (CNNs) architectures [50, 30, 32, 41], and recent Vision Transformer (ViT) models [20, 28] have demonstrated state-of-the-art performance in image classiﬁ-cation [37, 50], object detection [29, 17, 10], and semantic segmentation [17, 4] tasks. Yet, many deep learning models lack interpretability, making it difﬁcult to explain the rea-soning behind their predictions. As a result, Explainable
AI (XAI) has become a prominent research area in com-puter vision, and numerous methods have been proposed for explaining and interpreting the internal workings of dif-ferent neural network architectures in various application domains [60, 49, 46, 15, 7, 44, 6, 8, 26].
Explanation methods attempt to produce an explanation map in the form of a heatmap (also known as relevance or saliency map) that attributes the prediction to the input by highlighting speciﬁc regions in the input image. Early gradient-based methods produced explanation maps based the input image on the gradient of the prediction w.r.t.
[49, 50, 52]. Then, Grad-CAM [46] and the follow-up works
*Denotes equal contribution. by [12, 33, 5] proposed to compute the explanation maps based on the internal activation maps (also known as Class
Activation Maps (CAM)) and their corresponding gradients.
In parallel, path integration methods such as Integrated Gra-dients (IG) [54] proposed to produce an explanation map by accumulating the gradients of the linear interpolations be-tween the input and reference images. The aforementioned techniques were formulated and evaluated on CNNs. Fol-lowing the advent of Transformer-based architectures [55], a variety of approaches has also been proposed for interpreting
Vision Transformer (ViT) models [15, 56, 14].
This paper presents Iterated Integrated Attributions (IIA)
- a universal technique for explaining vision models, applica-ble to both CNN and ViT architectures. IIA employs iterative integration across the input image, the internal representa-tions generated by the model, and their gradients. Thereby,
IIA leverages information from the activation (or attention) maps created by all network layers, including those from the input image. We present comprehensive objective and sub-jective evaluations that demonstrate the effectiveness of IIA in generating faithful explanations for both CNN and ViT models. Our results show that IIA outperforms current state-of-the-art methods on various explanation and segmentation tests across all datasets, model architectures, and metrics. 2.