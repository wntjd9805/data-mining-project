Abstract
Factored feature volumes offer a simple way to build more compact, efficient, and intepretable neural fields, but also introduce biases that are not necessarily beneficial for real-world data. In this work, we (1) characterize the undesir-able biases that these architectures have for axis-aligned signals—they can lead to radiance field reconstruction dif-ferences of as high as 2 PSNR—and (2) explore how learn-ing a set of canonicalizing transformations can improve rep-resentations by removing these biases. We prove in a simple two-dimensional model problem that a hybrid architecture that simultaneously learns these transformations together with scene appearance succeeds with drastically improved efficiency. We validate the resulting architectures, which we call TILTED, using 2D image, signed distance field, and radiance field reconstruction tasks, where we observe im-provements across quality, robustness, compactness, and runtime. Results demonstrate that TILTED can enable ca-pabilities comparable to baselines that are 2x larger, while highlighting weaknesses of standard procedures for evalu-ating neural field representations. 1.

Introduction
Our physical world layers complexity on top of regular-ity. Tucked below the details that imbue our environments with character—the intricate fibers of a fine-grained veneer, or the light-catching specularities of everyday metal, plastic, and glass—one finds the simple geometric primitives and symmetries associated with built and natural structures. The challenge of representations for the world, such as those used for 3D reconstruction, anchors itself in the interaction be-tween the two ends of this dichotomy: point clouds and voxel grids offer versatility, but their inability to capture structure results in resource usage that can grow too intractably to be useful for complex details; meshes harness the uniformity of surfaces for compactness, but still fail on entities with de-tails that step outside of an acceptable regime—consider fog or deviations on curves.
In this work, we build on the idea that scalably capturing the details of a complex signal is only possible when a repre-sentation enables capture of its structure. We use this theme to study and improve state-of-the-art hybrid neural fields, which typically pair neural decoders with factored feature volumes [32, 51, 63, 66–68]. Aided by an ability to exploit sparse and low-rank structure, factorization is simple to im-plement and offers a host of advantages, such as compact-ness, efficiency, and interpretability. However, naive factor-ization also introduces the disadvantage of an implicit frame of representation, which is not guaranteed to be aligned with the structure of scenes or signals one aims to represent.
Drawing on insights from both low-rank texture extraction
[9] and implicit regularization in optimization methods for factorization [21, 42], we theoretically characterize the im-portance of this alignment and then show how it can moti-vate practical improvements to neural field architectures that rely on factored feature volumes. Our contributions are as follows: (1) We theoretically characterize the fragility of fac-tored grids in a two-dimensional model problem, where re-source efficiency on simple-to-capture structures can be un-dermined even by small planar rotations (Section 3). We prove that this fragility can be overcome by jointly optimiz-ing over the parameters of the representation and a trans-formation of domain capturing pose, when the underlying structure is well-aligned in some frame of representation. (2) We study how this same weakness affects practical neural field architectures, where it can lead to radiance field accuracy differences of as high as 2 PSNR. We propose opti-mization of more robust, transform-invariant latent decom-positions (TILTED) via a modification of existing factored representations (Section 4). TILTED models recover canon-ical factors by jointly recovering factors of a decomposed feature volume with a set of canonicalizing transformations, which are simple to add to existing factorization techniques. (3) We evaluate the TILTED family of models on three tasks: 2D image, signed distance field, and neural radi-ance field reconstruction (Section 5). Our experiments high-light biases in existing neural field architecture and eval-uation procedures, while demonstrating TILTED’s advan-tages across quality, robustness, compactness, and runtime.
For real-world scenes, TILTED can simultaneously improve reconstruction, halve memory consumption, and accelerate training times by 25%. 2.