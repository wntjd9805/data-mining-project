Abstract
Although deep learning has significantly facilitated the progress of single image super-resolution (SISR) in recent years, it still hits bottlenecks to further improve SR perfor-mance with the continuous growth of model scale. There-fore, one of the hotspots in the field is to construct efficient
SISR models by elevating the effectiveness of feature repre-sentation. In this work, we present a straightforward and generic approach for feature enhancement that can effec-tively promote the performance of SR models, dubbed par-tial channel shifting (PCS). Specifically, it is inspired by the temporal shifting in video understanding and displaces part of the channels along the spatial dimensions, thus allowing the effective receptive field to be amplified and the feature diversity to be augmented at almost zero cost. Also, it can be assembled into off-the-shelf models as a plug-and-play component for performance boosting without extra network parameters and computational overhead. However, regu-lating the features with PCS encounters some issues, like shifting directions and amplitudes, proportions, patterns of shifted channels, etc. We impose some technical constraints on the issues to simplify the general channel shifting. Ex-tensive and throughout experiments illustrate that the PCS indeed enlarges the effective receptive field, augments the feature diversity for efficiently enhancing SR recovery, and can endow obvious performance gains to existing models. 1.

Introduction
Single image super-resolution (SISR) is a long-standing and challenging hotspot in the computer vision community, to reconstruct a high-resolution (HR) image from one low-resolution (LR) image. Since an LR image can be degraded from an infinite number of HR images, it is substantially a mathematically ill-posed inverse problem. Convolutional neural networks (CNNs) [25, 24] have demonstrated sig-nificant breakthroughs in recent years, by leveraging large-scale datasets, a highly nonlinear mapping from LR and HR image pairs is learned. As a pioneering contribution, Dong
*Corresponding author, email: zxlation@foxmail.com
Code is available at https://github.com/OwXiaoM/PCS
Figure 1: Representative shifting patterns of applying the PCS to intermediate features, which show different spatial misalignments that enhance the Effective Receptive Field (ERF) [34] and feature diversity of the models. The pixels removed from the feature grids are discarded outright, and the emptied grids are simply padded with zeros to maintain the grid structure. These misalignments are conducive to expanding the local association among intermediate features while maintaining spatial integrity. et al. [7] draw lessons from sparse representation [51, 52] and propose a super-resolution convolutional neural net-work (SRCNN) to learn an end-to-end nonlinear mapping from LR to HR images. This model is further extended from the perspectives of sparse coding [48], increasing network depth [20] and sharing parameters [21, 42], etc. Although these models are capable of reconstructing high-fidelity SR results, their achievements rely immensely on the upscaling of the model scale, including network parameters, depth, and subtle topologies. For instance, EDSR [30] maintains more than 43M network parameters and about 70 layers of network depth. While RCAN [58] facilitates the effective-ness of feature representation via attention mechanism [14] and reduces the parameters to âˆ¼16M, it reaches over 400 layers of depth and significantly lengthens the time of in-ference. However, simply increasing the model scale also hits the bottleneck of performance improvement, and it is difficult to elevate the utilization efficiency of network as-sets. Moreover, model training also suffers from the in-tractable holdback of information weakening as the model scale continues to grow. Therefore, a promising compro-mise can probably be to promote the effectiveness of inter-mediate features, just like RCAN [58], which is intuitively helpful for further improving the upper bound of SR perfor-mance and developing lightweight SR models suitable for real-world scenarios.
At present, the measures to enhance the effectiveness of features mainly include feature fusion [16, 2, 60], wide ac-tivation [40, 53], gating mechanism [14, 58, 61] and non-locality [46, 59, 6]. The most commonly-used trick may be the attention mechanism [14] that adjusts the feature re-sponses towards the most informative and important com-ponents of the inputs. The SR community has fully em-braced this approach and extended it in various innovative ways [15, 59, 39, 38, 3]. The ideas of these methods mainly focus on adapting the channel-wise attention to other di-mensions, such as spatial-wise or layer-wise attention, or exploring the interaction of the attention mechanism [14] in different dimensions. To accelerate model inference, it can also be improved from the perspective of sparse representa-tion [51, 52]. Transformer [44] is a more advanced archi-tecture based on the self-attention mechanism, and has re-cently been adopted for the tasks of low-level computer vi-sion [29] and made remarkable success. Although these ap-proaches have made great progress in improving the effec-tiveness of features, they always introduce extra cost (e.g., network depth or parameters) compared with the bench-mark and compromise with the loss of efficiency.
Both enlarging the scale of SR models and enhancing the effectiveness of intermediate features aim to achieve more accurate nonlinear inference for SR tasks, neverthe-less, they are usually accompanied by additional overheads.
Different from these two manners, we expect to improve the nonlinear inference of SR models with zero cost by en-larging the receptive field of local mapping and increasing the diversity of hierarchical features, which is simply imple-mented by a generic operation termed partial channel shift-ing (PCS). Specifically, it displaces a part of the feature channels along two spatial dimensions (H and W ), so that the originally aligned features present certain dislocation in spatial dimensions, as shown in Fig. 1. The elements re-moved from the feature grids are directly discarded, while the emptied grids are simply padded with zeros. However, naive shifting suffers from the problem of infinite enumera-tion, such as shifting directions and magnitudes, the propor-tions of the shifted channels, and the combinational patterns of unshifted and shifted channels, etc. Therefore, we further investigate some technical constraints of these issues to sim-plify the application of naive shifting, which results in four
PCS patterns as illustrated in Fig. 1. Our benchmark ex-periments demonstrate the synergistic effect of the PCS on existing SR models. We also use analytical tools [34, 9] to
Note we refer to channel shifting as shifting feature maps on different channels along their spatial dimensions, which is different from [13]. illustrate the role of PCS in enhancing feature effectiveness.
In summary, the main contributions of this work include: (1) We propose a simple and effective method of fea-ture enhancement for SISR models, termed as PCS, which avails amplifying the Effective Receptive Fields (ERFs) [34] and augmenting the feature diversity with-out extra parameters and computational overhead. (2) We discuss several technical constraints to mitigate the problem of infinite enumeration, which streamlines the naive shifting and finally distills the state space down to four representative patterns for channel shifting. (3) The benefits of the PCS for model inference and fea-ture enhancement is experimentally analyzed and ver-ified with the assistance of Effective Receptive Fields (ERFs) and Local Attribution Maps (LAM) [9]. (4) The benchmark experiments that apply the PCS to off-the-shelf SISR models, including typical lightweight and large-scale ones, provide strong evidence of its ef-fectiveness in promoting SR performance. 2.