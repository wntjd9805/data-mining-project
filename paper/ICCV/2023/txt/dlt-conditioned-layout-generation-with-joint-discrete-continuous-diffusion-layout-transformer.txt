Abstract
Generating visual layouts is an essential ingredient of graphic design. The ability to condition layout genera-tion on a partial subset of component attributes is criti-cal to real-world applications that involve user interaction.
Recently, diffusion models have demonstrated high-quality generative performances in various domains. However, it is unclear how to apply diffusion models to the natural rep-resentation of layouts which consists of a mix of discrete (class) and continuous (location, size) attributes. To ad-dress the conditioning layout generation problem, we in-troduce DLT, a joint discrete-continuous diffusion model.
DLT is a transformer-based model which has a flexible con-ditioning mechanism that allows for conditioning on any given subset of all the layout component classes, locations, and sizes. Our method outperforms state-of-the-art gen-erative models on various layout generation datasets with respect to different metrics and conditioning settings. Ad-ditionally, we validate the effectiveness of our proposed conditioning mechanism and the joint continuous-diffusion process. This joint process can be incorporated into a wide range of mixed discrete-continuous generative tasks.
More information can be found on our project webpage: https://wix-incubator.github.io/DLT
Figure 1: Overview of our Joint Discrete-Continuous diffusion process for layout generation. The diffusion process is applied jointly on both the continuous attributes of the components (size and location) by adding a small Gaussian noise, and on the discrete attributes of the components (class) by adding a noise that adds a mass to the Mask class. The reverse diffusion model can be conditioned on any given subset of the attributes. 1.

Introduction
An essential aspect of graphic design is layout creation: the arrangement and sizing of visual components on a can-vas or document. A well-designed layout enables users to easily comprehend and efficiently interact with the informa-tion presented. The ability to generate high-quality layouts is crucial for a range of applications, including user inter-faces for mobile apps and websites [4] and graphic design for information slides [6], magazines [38], scientific papers
[40], infographics [28], and indoor scenes [5].
To facilitate graphic design tasks, modelling approaches such as Generative Adversarial Networks (GANs) [18],
Variational Autoencoders (VAEs) [14, 1], and masking completion [36, 16, 7] were explored for generating novel layouts. These approaches employ various architectures such as Recurrent Neural Networks (RNNs) [14], and
Graph Neural Networks (GNNs) [17]. Recently proposed,
Transformer-based approaches [1, 36, 16, 7] have been shown to produce diverse and plausible layouts for a variety of applications and user requirements.
Layouts are often represented as a set of components, each consisting of several attributes, such as class, position, and size [18]. Inspired by recent advancements in NLP, a common practice is to model a component as a sequence of discrete attribute tokens, and a layout as the concatenation of all attribute tokens [36, 7, 16]. The model is trained using a self-supervised objective (either next-word completion or
unmasking) to generate an output sequence. The discretiza-tion process is done by binning geometry attributes, such as position and size. However, using discrete tokens fails to take into account the natural representation of (geometry) data, which is continuous, and either result in poor resolu-tion of layout composition, or in a large sparse vocabulary that is not covered well by smaller datasets.
Providing controllable layout generation is crucial for many real-world graphic design applications that require user interaction. We would like to allow a user to fix cer-tain component attributes, such as component class, or a set of components, and then generate the remaining attributes (position and size) or components. In recent iterative-based masking completion approaches [16], such conditioning is performed only during inference, in a way that does not pro-vide the model with the ability to separate between the con-ditioned and generated parts of the layout during the itera-tive refinement. This lack of separation might lead to ambi-guity in the generation process which can result in subopti-mal performance.
Diffusion models are a generative approach that has gained significant attention recently. Continuous diffu-sion models have demonstrated remarkable generative ca-pabilities in various tasks and domains, such as text-to-image [29], video generation [11], and audio [39]. While diffusion models were extended to discrete spaces [41, 2], a challenge still remains on how to apply these models to generative tasks whose representation consists of both con-tinuous and discrete features. In this paper, we introduce a Diffusion Layout Transformer (DLT) for layout genera-tion and flexible editing. Different from traditional diffu-sion models, we propose a novel framework based on a joint continuous-discrete diffusion process and provide theoreti-cal justification for the derived optimization objective. Us-ing a transformer-encoder architecture, we apply a diffusion process jointly both on the continuous attributes of the lay-out components (size and location) and on the discrete ones (class), as depicted in Figure 1. Our diffusion model ad-dresses an important limitation of transformer-based mod-els that rely on a discrete layout representation, which can result in reduced resolution and limited diversity of layout compositions.
Our framework offers flexible layout editing which al-lows for conditioning on any subset of the attributes of the layout components provided (class, size and location). Un-like masking completion approaches [36, 16, 7], which per-form conditioning only during inference, and inspired by diffusion image inpainting techniques [24], we explicitly train the model to perform layout conditioning. We use con-dition embedding to control on which layout attributes to apply the diffusion process. In this way, the model is able to separate between the condition and the generation parts during the iterative refinement. Our experiments reveal that these design choices significantly improve the model’s per-formance compared to inference-based conditioning.
We study the effectiveness of our DLT model by evalu-ating it on three popular layout datasets of diverse graphic design tasks over several common metrics. Using extensive experiments, we demonstrate that our proposed model out-performs state-of-the-art layout generation models on lay-out synthesis and editing tasks while maintaining runtime complexity on par with these models. By comparing dif-ferent alternatives, we show that both the joint diffusion process and the conditioning mechanism contribute to the model’s strong generative capabilities. Our joint discrete-continuous diffusion framework has a generic design and thus is applicable to other domains such as multi-modal generation tasks (text+image, text + audio, etc’). 2.