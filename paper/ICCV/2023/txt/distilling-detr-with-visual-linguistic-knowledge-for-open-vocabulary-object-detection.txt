Abstract
Current methods for open-vocabulary object detec-tion (OVOD) rely on a pre-trained vision-language model (VLM) to acquire the recognition ability.
In this paper, we propose a simple yet effective framework to Distill the
Knowledge from the VLM to a DETR-like detector, termed
DK-DETR. Specifically, we present two ingenious distilla-tion schemes named semantic knowledge distillation (SKD) and relational knowledge distillation (RKD). To utilize the rich knowledge from the VLM systematically, SKD trans-fers the semantic knowledge explicitly, while RKD exploits implicit relationship information between objects. Further-more, a distillation branch including a group of auxiliary queries is added to the detector to mitigate the negative ef-fect on base categories. Equipped with SKD and RKD on the distillation branch, DK-DETR improves the detection performance of novel categories significantly and avoids disturbing the detection of base categories. Extensive ex-periments on LVIS and COCO datasets show that DK-DETR surpasses existing OVOD methods under the setting that the base-category supervision is solely available. The code and models are available at https://github. com/hikvision-research/opera. 1.

Introduction
Object detection has witnessed rapid progress [10, 9, 28, 13, 20, 2, 45, 33, 32, 35] for years, which aims to localize and categorize objects in an image. However, the object de-tection model can only perform well on a closed and small set of categories, while cannot detect novel ones which are not trained. Recently, visual-language models [24, 15] (VLM), consisting of an image encoder and a text encoder, have shown impressive zero-shot classification ability af-ter being trained on large-scale loosely aligned image-text
*Corresponding author.
Figure 1. Comparison of different distillation implementations.
VKD in (a) denotes vanilla knowledge distillation that forces source features to align with target features in a one-to-one man-ner. SKD in (b) and RKD in (c) are newly proposed for OVOD in this paper. (d) indicates that we implement distillation on auxiliary queries to avoid original detection branch being disturbed. pairs. This motivates the community to implement an open-vocabulary object detector [11, 5, 7, 39, 43, 42, 44, 23] which is expected to recognize arbitrary categories.
Distilling the knowledge of novel-category objects from the VLM to the detector is a typical practice [11, 36, 22] to solve the open-vocabulary object detection (OVOD) prob-lem. ViLD [11] is a representative distillation-based ap-proach. Category text embeddings, extracted by the VLM text encoder, serve as the classifier to perform OVOD (known as ViLD-text). A knowledge distillation (KD) mod-ule is further introduced to align the object features to vi-sual embeddings extracted by the VLM image encoder. By adopting the vanilla KD as depicted in Figure 1, ViLD ev-idently improves the performance of novel categories. Be-sides, ZSD-YOLO[36] and HierKD [22] also adopt knowl-Method
ViLD-text [11]
ViLD [11]
APr 10.1 16.6
APc 23.9 24.6
APf 32.5 30.3 and auxiliary queries are only used for training, and do not introduce any budget at inference. The main contributions of this work are summarized as follows.
Table 1. OVOD performance of ViLD on LVIS benchmark.
APr, APc and APf denote the performance of rare, common and frequent categories in LVIS dataset, respectively. Compared with
ViLD-text which does not adopt distillation, there is an evident
APf drop for ViLD. edge distillation techniques to improve the novel-category performance based on one-stage detectors [27, 41], instead of the two-stage detector [13] used in ViLD. Recently, end-to-end detectors [2, 45] boost the development of object de-tection due to their high efficiency and effectiveness. How-ever, distilling the knowledge to an end-to-end detector is less studied in the OVOD field.
In this paper, we propose a framework to distill the knowledge from the VLM to a DETR-like detector, termed
DK-DETR. Nevertheless, the vanilla knowledge distillation adopted by aforementioned methods leads to limited im-provement on novel categories. To this end, we propose two ingenious knowledge distillation schemes, namely se-mantic knowledge distillation (SKD) and relational knowl-edge distillation (RKD) , as shown in Figure 1. In SKD, the feature alignment between the detector and the VLM im-age encoder is treated as a pseudo-classification problem instead of a regression problem as in vanilla knowledge dis-tillation. It not only pulls together features belonging to the same object but also pushes away features from different objects.
In RKD, considering that the VLM can construct a well-structured feature space among abundant visual en-tities, we propose to model relationships between objects hidden in the VLM image encoder and distill the relational knowledge to our detector.
Although knowledge distillation can effectively improve the novel-category performance, it negatively affects base categories which are well trained with sufficient ground-truth labels (e.g., base-category performance APf in ViLD drops from 32.5 to 30.3 in Table 1). Such a phenomenon can be attributed to training objective inconsistence and do-main shift between the VLM and the detector. Under the supervision of ground-truth labels, object features in the de-tector are trained to localize and recognize base-category objects, but distillation forces object features to align with
VLM visual embeddings, which results in feature distur-bance. Consequently, we add a group of auxiliary queries in our approach for distillation exclusively, which avoids the performance degradation of base categories.
Equipped with SKD and RKD on auxiliary queries, DK-DETR achieves satisfactory performance on both base and novel categories. Note that both distilling implementations
• We propose a simple yet effective distilling framework for the end-to-end open-vocabulary object detector and effectively improve novel-category performance.
• To distill the knowledge from the VLM to the detector, the proposed SKD transfers the semantic knowledge ex-plicitly, while RKD exploits implicit relationship infor-mation between objects.
• By introducing a group of auxiliary queries, DK-DETR disentangles the training of the detection and distillation, which avoids the performance degradation of base categories.
• DK-DETR surpasses existing OVOD methods on both
LVIS and COCO datasets under the setting that only the base-category supervision is available, and also achieves competitive performance when it generalizes to other datasets. 2.