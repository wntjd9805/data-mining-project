Abstract
The favorable performance of Vision Transformers (ViTs) is often attributed to the multi-head self-attention (MSA), which enables global interactions at each layer of a ViT model. Previous works acknowledge the property of long-range dependency for the effectiveness in MSA. In this work, we study the role of MSA in terms of the different axis, density. Our preliminary analyses suggest that the spatial interactions of learned attention maps are close to dense interactions rather than sparse ones. This is a curious phe-nomenon because dense attention maps are harder for the model to learn due to softmax. We interpret this oppo-site behavior against softmax as a strong preference for the ViT models to include dense interaction. We thus man-ually insert the dense uniform attention to each layer of the
ViT models to supply the much-needed dense interactions.
We call this method Context Broadcasting, CB. Our study demonstrates the inclusion of CB takes the role of dense at-tention and thereby reduces the degree of density in the orig-inal attention maps by complying softmax in MSA. We also show that, with negligible costs of CB (1 line in your model code and no additional parameters), both the capacity and generalizability of the ViT models are increased. 1.

Introduction
After the success of Transformers [58] in language do-mains, Dosovitskiy et al. [12] have extended to Vision
Transformers (ViTs) that operate almost identically to the
Transformers but for computer vision tasks. Recent stud-ies [12, 56] have shown that ViTs achieve superior perfor-mance on image classification tasks. Further, the univer-sal nature of ViTs’ input has demonstrated its potential to multi-modal input extensions [2, 14, 26, 31].
The favorable performance is often attributed to the multi-head self-attention (MSA) in ViTs [12, 56, 59, 7, 50,
*This work was done during N. Hyeon-Woo’s intern at NAVER AI Lab.
Tae-Hyun Oh is in Department of Electrical Engineering and Grad. School of Artificial Intelligence, POSTECH, and joint affiliated with Institute for
Convergence Research and Education in Advanced Technology, Yonsei
University, Korea.
Figure 1. Motivation of our work. Top: dense attention is hard to learn with softmax, but self-attention tends to learn it more than sparse one. Bottom: we infuse dense attention explicitly, named
CB, to split the responsibility of interactions; the burden of interac-tions of self-attention is reduced. Self-attention is now more likely to learn sparse interaction that is in favor of softmax. 44], which facilitates long-range dependency1. Specifically,
MSA is designed for long-range interactions of spatial infor-mation in all layers. This is a structurally contrasting fea-ture with a large body of successful predecessors, convo-lutional neural networks (CNNs), which gradually increase the range of interactions by stacking many fixed and hard-coded local operations, i.e., convolutional layers. Raghu et al. [44] and Naseer et al. [39] have shown the effectiveness of the self-attention in ViTs for the global interactions of spatial information compared to CNNs.
Unlike previous works [44, 39] that focused on the ef-fectiveness of long-range dependency, we study the role of density in spatial attention. “Long-range” can be either
“sparse” or “dense”. We examine whether the learned at-tention of ViTs is dense or sparse. Our preliminary analy-sis based on the entropy measure suggests that the learned 1Long-range dependency is described in the literature with various termi-nologies: non-local, global, large receptive fields, etc.
attention maps tend to be dense across all spatial loca-tions. This is a curious phenomenon because denser atten-tion maps are harder to learn by the softmax operation. Its gradients become larger (less stable) around denser atten-In other words, ViTs are trying hard to learn tion maps. dense attention maps despite the difficulty of learning them through gradient descent.
While dense attention is unlikely to be learned via gradi-ent descent, it is easy to implement it manually. We insert uniform attention explicitly, the densest form of attention, to confirm our observation of the effort of learning dense attention. We call our module Context Broadcasting (CB).
The module adds the averaged token to every individual to-ken at intermediate layers. We find that when CB is added to
ViT, CB reduces the degree of density in attention maps in all layers preserving the long-range dependency. CB takes over the role of the dense global aggregation from self-attention, as illustrated in Fig. 1. CB also makes the overall optimiza-tion for a ViT model easier and improves its generalization.
CB brings consistent gains in the image classification task on ImageNet [47, 46, 4] and the semantic segmenta-tion task on ADE20K [67, 68]. Overall, CB seems to help a
ViT model divert its resources from learning dense attention maps to learning other informative signals. We also demon-strate that our module improves the Vision-Language Trans-former, ViLT [31], on a multi-modal task, VQAv2 [16].
Such benefits come with only negligible costs. Only 1 line of code needs to be inserted in your model.py. No addi-tional parameters are introduced; only a negligible number of operations are. Our contributions are as follows:
• Our observations of the dense interaction preference of
ViTs but the learning difficulty from softmax (Sec. 3.1);
• A simple and effective modules, CB and CBS, for infusing dense interactions (Sec. 3.2);
• Phenomena for CB to divert the capacity of MSA for sparse interactions (Sec. 3.3); 2.