Abstract
State-of-the-art deep neural networks are trained with large amounts (millions or even billions) of data. The ex-pensive computation and memory costs make it difficult to train them on limited hardware resources, especially for recent popular large language models (LLM) and com-puter vision models (CV). Recent popular dataset distil-lation methods are thus developed, aiming to reduce the number of training samples via synthesizing small-scale datasets via gradient matching. However, as the gradient calculation is coupled with the specific network architec-ture, the synthesized dataset is biased and performs poorly when used for training unseen architectures. To address these limitations, we present dataset quantization (DQ), a new framework to compress large-scale datasets into small subsets which can be used for training any neural network architectures. Extensive experiments demonstrate that DQ is able to generate condensed small datasets for training unseen network architectures with state-of-the-art compres-sion ratios for lossless model training. To the best of our knowledge, DQ is the first method that can successfully dis-till large-scale datasets such as ImageNet-1k with a state-of-the-art compression ratio. Notably, with 60% data from
ImageNet and 20% data from Alpaca’s instruction tuning data, the models can be trained with negligible or no per-formance drop for both vision tasks (including classifica-tion, semantic segmentation, and object detection) as well as language tasks (including instruction tuning tasks such as BBH and DROP). 1.

Introduction
Deep neural networks have shown superior performance in a wide range of fields such as computer vision [22, 21,
*Equal first author.
†Corresponding author.
Figure 1: Lossless dataset compression with Dataset
Quantization (DQ) framework. On both vision and lan-guage tasks. In the plot, we use ResNet18 as backbone for all tasks and LLaMA-7B for all language tasks with instruc-tion fine-tuning. 15] and natural language processing [14, 3]. Their per-formance depends heavily on the amount of training data.
For example, recent state-of-the-art models [32, 55, 12, 59] on ImageNet-1K takes three billion data for pre-training.
This is hardly affordable for researchers with limited com-putational resources. However, are all the data in the large dataset beneficial or necessary to the training? Is it pos-sible to remove some redundant samples without degrad-ing the training performance? What is the performance of the pretrained models with less data on downstream tasks?
In this paper, we conduct extensive experiments and con-duct detailed explorations on those questions. To address the first question, several Dataset Distillation (DD) algo-rithms [62, 60, 30, 61, 53, 4, 16, 52, 35] are proposed re-cently to reduce the training dataset size by synthesizing a new set of data that is significantly smaller than the origi-nal one. With the new synthesized dataset, the training cost is reduced significantly, while yielding comparable results with the models trained on the original datasets.
Method Hours
IN-1K COCO
DM-60
DQ-60
Full 28K 72
--89.0 89.4
-39.0 39.2 (a) (b) (c)
Figure 2: Our proposed dataset quantization outperforms existing dataset distillation and coreset selection methods significantly. (a) Model training accuracy from DD (DC [62] and DM [61]), coreset selection (Craig [39], GradMatch [29], and GC [26]), and our proposed DQ across different data keep ratios. ‘Hours’ denotes the time for compressing ImageNet dataset with 60% data keep ratio. (b) Visualization of the samples diversity of GraphCut and DQ, where ρ is the data keep ratio (better in color). (c) Cross-architecture visualization of the feature distributions among the dataset generated by a dataset distillation methods ‘distribution matching’ (DM) and DQ on ResNet-18 on CIFAR-10 bird class. Compared with DM, our proposed DQ effectively captures the whole dataset distribution for all the architectures, thus generalizing better.
Although having made significant progress, two limita-tions make those algorithms hard to be deployed in an in-i) Poor generalization capability. dustrial environment:
They all rely on specific metrics to match the synthetic and real samples [63, 61]. Thus the synthetic datasets are in-evitably biased by the model architecture involved in the metric computation, resulting in poor performance when used for training unseen model architectures. For exam-ple, as shown in Fig. 2c, the dataset synthesized based on ResNet-18 [22] suffers a 59.4% accuracy drop when used for training Swin-Tiny [36] (81.2% vs 21.6%). ii)
Low scalability to larger datasets. Different from other deep learning tasks that optimize the parameters of a given architecture, dataset distillation aims to optimize the syn-thetic set, the computational cost is quadratically propor-tional to the size of the synthetic set. When the size is large, the computational cost becomes unaffordable. For example, as in Fig. 2a, previous SOTA method DM [61] needs 28, 000 GPU hours to distill ImageNet-1K with 60% data processing.
To address these limitations, we explore a different di-rection from synthesizing samples based on our empiri-cal observations that the samples selected by coreset meth-ods [27, 19, 8, 1, 42] could be used to train unseen network architectures (i.e. good cross-architecture generalization).
However, as the data keep ratio is small, the selected sam-ples tend to lose the diversity, leading to a low performance for model training. As in the first row in Figure 2b, coreset methods tend to sample data points in a biased region. This led to a significant accuracy drop when used for model train-ing. As shown in Fig. 6 in the following section, our pro-posed DQ is able to achieve 10% (75.7% vs 85.2%) higher accuracy over the previous SOTA coreset method.
In this paper, we aim to develop a method that com-bines the advantages of Dataset Distillation methods and the
Coreset methods: a unified dataset compression method that generates compact datasets useful for training various net-work architectures while maintaining state-of-the-art train-ing performance under all data keep ratios. We start with investigating the reason behind the poor performance of the coreset selection method [45] under low data keep ratio, and we find it lies in the one-time selection strategy, re-sulting in a low diversity of the selected data. This will lead to a significant performance drop as shown in Fig. 2b.
More detailed analysis on previous coreset selection meth-ods [26, 29] can be found in Sec. 3.1 and in the Appendix.
We thus propose a new pipeline to overcome the afore-mentioned issues of the coreset algorithm and term it
Dataset Quantization (DQ). Specifically, DQ first divides the entire dataset into a set of non-overlapping bins recur-sively based on the submodular gains [26] that aims to max-imize the diversity gains as defined in Eqn. 1. Then, a small portion of data samples is uniformly sampled from all bins. In this manner, the selected samples are optimized to cover as much as possible the entire dataset with the inter-data diversity maximized. We prove mathematically that the dataset selected by DQ indeed has larger diversity than the coreset selection based methods. Motivated by recent patch-based image representation [15, 20, 66], we measure the importance scores of patches and save the most impor-tant ones to reduce the storage cost. At the training stage, we reconstruct training images via important patches and a pre-trained MAE [20] model.
Different from dataset distillation methods, as shown in the second row in Fig. 2c, the quantized dataset main-tains a high coverage over the entire data in the latent fea-ture space across different model architectures. The valida-tion accuracy is also significantly higher than those models trained with DD algorithms (e.g., 34.4% higher for ViT-Tiny). Compared with DD methods, DQ only takes 72
GPU hours to quantize ImageNet data with 60% keep ratio, which is 388× (28, 000 vs 72 GPU hours) faster, while achieving much higher performance on large data keep ratios. On the other hand, when comparing to core-set selection methods, as shown in Fig. 2a and 2b, DQ se-lects samples with larger diversity and achieves better per-formance when the data keep ratio is low (10% data kept).
We conduct extensive experiments and show that the pro-posed dataset quantization method is able to generate com-pact datasets that can be used to train unseen models such as model families from ViT, ResNet and MobileNetV2,
LlaMA, etc.
Specifically, for vision tasks, on CIFAR-10 and
ImageNet-1K, only 60% of the data are used to train the models to achieve a comparable model performance as for language tasks, on those trained with full datasets.
BBH and DROP benchmark, only 2% instruction data are needed to achieve comparable model performance as those trained with full datasets. We further verify that the model weights pre-trained on the quantized dataset can be general-ized into downstream tasks such as object detection and seg-mentation. As shown in Fig. 6, the ResNet-50 [22] model pre-trained on 60% ImageNet also achieves negligible per-formance drop when finetuned on COCO [34] (39.0% vs 39.2%) and ADE20K [64] (42.3% vs 42.5%).
Our main contributions are summarized as:
• We propose a new framework, Dataset Quantization (DQ), to compress datasets into a small compact one that can be used for training unseen network architec-tures with state-of-the-art compression performance.
• We propose a scalable and efficient dataset compres-sion algorithm that can be used for large dataset such as ImageNet-1K. With Dataset Quantization, we are able to remove 40% data from ImageNet-1K dataset and 80% data from the Alpaca instruction dataset with no training performance loss.
• We verify that the models trained with a compressed dataset can be used for downstream tasks. The models pre-trained with 60% of data on ImageNet-1K achieve no performance on COCO for object detection and
ADE20K for segmentation. 2.