Abstract
Gaze target detection aims to predict the image location where the person is looking and the probability that a gaze is out of the scene. Several works have tackled this task by regressing a gaze heatmap centered on the gaze location, however, they overlooked decoding the relationship between the people and the gazed objects. This paper proposes a
Transformer-based architecture that automatically detects objects (including heads) in the scene to build associations between every head and the gazed-head/object, resulting in a comprehensive, explainable gaze analysis composed of: gaze target area, gaze pixel point, the class and the image location of the gazed-object. Upon evaluation of the in-the-wild benchmarks, our method achieves state-of-the-art results on all metrics (up to 2.91% gain in AUC, 50% reduc-tion in gaze distance, and 9% gain in out-of-frame average precision) for gaze target detection and 11-13% improve-ment in average precision for the classification and the lo-calization of the gazed-objects. The code of the proposed method is publicly available1. 1.

Introduction
Gazing is a powerful nonverbal signal, which indicates the visual attention of a person and allows one to understand the interest, intention, or (future) action of people [12]. For this reason, gaze analysis has widely been used in several disciplines such as human-computer interaction [26, 36], neuroscience [8, 28], social and organizational psychology
[3, 11], and social robotics [1] to name a few.
Even though human beings have a remarkable capability to decode the gaze behavior of others, realizing this task au-tomatically remains a challenging problem [2, 33, 34]. The computer vision community has tackled the automated gaze behavior analysis in terms of two tasks: (a) gaze estimation and (b) gaze target detection. Gaze estimation stands for predicting the person’s gaze direction (usually in 3D) when typically a cropped human head image is given as the in-1https://github.com/francescotonini/ object-aware-gaze-target-detection l a n o i t i d a r
T
] 4 3
[
. l a t e u
T s r u
O
Figure 1: The overall methodology of the existing ap-proaches and ours. put [4, 14, 15, 20]. Instead, gaze target detection (also re-ferred to as gaze-following) is to determine the specific (2D or 3D) location that a human is looking at in an in-the-wild scene [7, 13, 18].
Several works utilize head pose features and the saliency maps of possible gaze targets to perform gaze target detec-tion. For instance, [6, 22, 29, 30] followed a two-pathway learning scheme, where one path learns feature embeddings from the scene image, and the other path models the head crops belonging to the person whose gaze target is aimed to be predicted. Chong et al. [7] extend the aforementioned two-pathway approach to perform spatio-temporal model-ing to determine the gaze targets in videos.
In the same vein, a few other methods exist: [2, 13, 19, 25, 33]. Among them, some consider a third path to model the depth map of the scene image, which is determined by a monocular depth estimator [13, 19, 25, 33]. Differently, others [2] in-ject depth maps and 2D-human poses to improve the 3D un-derstanding of the scenes, resulting in better gaze target de-tection. The results achieved by these approaches (referred to as traditional methods throughout the manuscript, see
Fig. 1-top) are highly remarkable since they demonstrated
that gaze target estimation could be directly performed on images or videos in contrast to using low-intrusive wear-able eye trackers, which notoriously have several issues in terms of cost, battery life, and calibration. On the other hand, traditional methods also have some major drawbacks.
First, both training and inference require carefully human-annotated head crops. Therefore, to ensure that traditional methods work well in real-life practical applications, there is a need for additional and highly accurate head detectors.
Indeed, Tu et al. [34] showed notable performance drops of traditional methods when head detectors were involved instead of using manually annotated head locations. A sec-ond limitation concerns the fact that traditional methods can perform a single gaze target detection at a time; thus, for scenes containing multiple people, the models should be run repeatedly for each person. Besides the computational complexity such implementation brings in, post-processing is also needed to combine the detected gaze targets of differ-ent subjects in the same scene. Tu et al. [34] to some extent overcome the shortcomings mentioned above by introduc-ing a Transformer-based architecture that explicitly learns how to detect and localize the head during gaze target de-tection (see Fig. 1-middle). However, the contribution of objects to decipher the human-human/object gazing is com-pletely omitted in [34].
Several studies show that people typically gaze at liv-ing or non-living objects in the scene during social and physical interactions [5, 21, 24, 32, 35, 37, 38]. Moti-vated by this, we pursue an object-aware gaze target de-tection, instead of using features extracted from holistic scene images and head crops as in traditional methods:
[2, 6, 7, 13, 19, 22, 25, 29, 33] or learning how to detect and localize the head of the person-in-interest (the one whose gaze target to be detected) as in [34]. Our proposal is not only able to predict the gaze area (in terms of heatmaps) that people looking at and determine if the gaze target is in-side or outside of the scene but also localize the objects and predicts the objects’ classes (including head) on which the gaze point is (see Fig. 1-bottom). The further has signifi-cant practical usage since it brings in an explainable gaze analysis (see Table 1 for details).
The proposed method is an end-to-end Transformer-based architecture. Given a scene image, we first extract all objects, including the ones classified as heads, with an
Object Detector Transformer. Then, for each head, a gaze vector is predicted. Using this gaze vector, we build a gaze cone for each person individually, allowing the model to fil-ter out the objects that are not in a person’s Field of View (FoV). Subsequently, a masked transformer (called Gaze
Object Transformer) learns the interactions between the de-tected heads and objects, boosting the gaze target detection performance in terms of both heatmaps and gaze points (i.e. a single pixel in the scene). Furthermore, this architecture
Method
Traditional
Tu et al. [34]
Ours
Wout/ Head Multiple
People
Loc. Given
✗
✗
✓
✓
✓
✓
Head
Object
Object
Detection Localization Classification
✗
✓
✓
✗
✗
✓
✗
✗
✓
Table 1: Existing gaze target detection methods compared to ours. Ours is more explainable since, for every person in the scene, it can detect the object class and bounding box location on which the gaze is. It learns the scene objects (including the head) without using the head locations sup-plied by datasets. has a remarkable capability to predict whether a gaze tar-get point is out of the frame. The extensive evaluations on two large-scale benchmark datasets show the superior per-formance of our method w.r.t. state-of-the-art (SOTA) gaze target detectors. At the same time, our model has addi-tional competence to accurately predict the gazed objects’ locations and the associated classes as empirically demon-strated. The ablation study highlights the importance of all components and specifically the needs for our main techni-cal contributions, i.e. the Gaze Cone Predictor and the Gaze
Object Transformer.
To summarize: (1) We introduce a novel object-oriented gaze target detection method. (2) This end-to-end
Transformer-based model automatically detects the heads and other objects in the scene to build associations be-tween every head and the gazed-head/object, resulting in a comprehensive, explainable gaze analysis composed of: gaze target area, gaze pixel point, the class of gazed-object, the bounding box of the gazed-object as well as predicting whether the gazed point is out of the frame. (3) We demon-strate SOTA results on standard datasets regarding all evalu-ation metrics for gaze-target detection (up to 2.91% gain in
AUC, 50% reduction in gaze distance, and 9% gain in out-of-frame AP), gazed-object classification and localization (11-13% gain in AP) and in case of low/high variance across gaze annotations. (4) The code of the proposed method is publicly available. We also release our implementation2 for
[34] since during our private communications with the au-thors, we are informed that their code at the moment cannot be shared by them due to their ongoing collaborations with a company. 2.