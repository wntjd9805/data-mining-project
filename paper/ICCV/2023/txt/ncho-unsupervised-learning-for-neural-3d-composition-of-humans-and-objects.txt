Abstract
Deep generative models have been recently extended to synthesizing 3D digital humans. However, previous ap-proaches treat clothed humans as a single chunk of geometry without considering the compositionality of clothing and accessories. As a result, individual items cannot be naturally composed into novel identities, leading to limited expres-siveness and controllability of generative 3D avatars. While several methods attempt to address this by leveraging syn-thetic data, the interaction between humans and objects is not authentic due to the domain gap, and manual asset creation is difficult to scale for a wide variety of objects.
In this work, we present a novel framework for learning a compositional generative model of humans and objects (backpacks, coats, scarves, and more) from real-world 3D scans. Our compositional model is interaction-aware, mean-ing the spatial relationship between humans and objects, and the mutual shape change by physical contact is fully incor-porated. The key challenge is that, since humans and objects are in contact, their 3D scans are merged into a single piece.
To decompose them without manual annotations, we propose to leverage two sets of 3D scans of a single person with and without objects. Our approach learns to decompose objects and naturally compose them back into a generative human model in an unsupervised manner. Despite our simple setup requiring only the capture of a single subject with objects, our experiments demonstrate the strong generalization of our model by enabling the natural composition of objects to diverse identities in various poses and the composition of multiple objects, which is unseen in training data. The project page is available at https://taeksuu.github.io/ncho. 1.

Introduction
Generative modeling of 3D humans from real-world data has shown promise to represent and synthesize diverse hu-man shapes, poses, and motions. Especially, the ability to create realistic humans with diverse clothing and accessories
(e.g., backpacks, scarves, and hats) is indispensable for a myriad of applications including AR/VR, entertainment, and virtual try-on. The early work [4, 28, 36, 50, 71] has demon-strated success in modeling “undressed” human bodies from real-world scans. More recently, the research community has been increasingly focused on the generative modeling of clothed humans [13, 16, 38], to better represent humans in everyday life.
Recent advancements in shape representations such as
Neural Fields [69] mitigate the need for pre-defining topol-ogy or template of clothing, enabling to build animatable clothed humans from raw 3D scans [14, 56]. Along with its advantage in strong expressive power for avatar modeling, this approach also allows the models to learn faithful physi-cal interactions between objects and humans. However, since raw 3D scans do not provide a clear separation of different components, existing approaches typically treat human bod-ies, clothing, and accessories as an entangled block of geom-etry [13]. In this paper, we argue that this leads to suboptimal expressiveness and composability of the generative avatars.
Many applications require more intuitive control to add, re-place, or modify objects while maintaining human identity.
To make avatars explicitly compositable with objects, several works propose to leverage synthetic data [6, 16, 27]. How-ever, the manual creation of 3D assets remains a challenge and is extremely difficult to scale. Moreover, the physical interaction of bodies, clothing, and accessories in synthetic data tends to be less faithful due to the domain gap.
In contrast to prior methods, our goal is to build a com-positional generative model of objects and humans from real-world observations. The core challenge lies in the dif-ficulty of learning the composition and decomposition of objects in contact from raw 3D scans. Capturing objects in isolation does not lead to faithful composition due to the lack of realistic deformations induced by physical contact.
Thus, while it is essential to collect 3D scan data on objects and humans in contact, the joint scanning of humans with objects only provides an entangled block of 3D geometry as mentioned, and accurately segmenting different components requires non-trivial 3D annotation efforts.
Upon these challenges, our contributions are: scalable data capture protocol, unsupervised decomposition of objects and humans, and generalizable neural object composition.
Scalable Data Capture. Capturing multiple identities with various poses and objects requires prohibitively large time and storage. To overcome this issue, we propose to collect human-object interactions with diverse poses only from a single subject, referred to as the ”source human”. To enable decomposition of objects, we also capture the same person without any objects, where the deviation between two sets defines “objects” in our setup. Examples are shown in Fig. 2.
This capture protocol offers sufficient diversity in poses and object types within a reasonable capture time.
Unsupervised Decomposition of Objects. To separate ob-jects from the source human, we leverage the expressiveness of the generative human model based on implicit surface rep-resentation [13]. We train a human module without objects, and then jointly optimize the latent codes of the avatar and a generative model for objects to best explain the 3D scans of the person with objects. While the human module accounts for state differences in pose and clothing, the object-only module learns to synthesize the residual geometry as an ob-ject layer in an unsupervised manner. Notably, objects in our work are defined as residual geometry that cannot be explained by the trained human-only module.
Neural Object Composition. While the unsupervised de-composition successfully separates objects from the source human, we observe that naively composing it to novel identi-ties from other datasets [52, 75] leads to undesired artifacts and misalignment in the contact regions. To address this, we propose a neural composition method by introducing another composition MLP that takes latent features from both human and object modules to make a final shape prediction. Due to the local nature of MLPs, our approach plausibly composes objects to novel identities without retraining as in Fig. 1.
Our experiments show that our compositional generative model is superior to existing approaches without explicit disentanglement of objects and humans [13]. In addition, we show that our model can be used for fine-grained controls including object removal from 3D scans and multiple object compositions on a human, demonstrating the utility and expressiveness of our approach beyond our training data. 2.