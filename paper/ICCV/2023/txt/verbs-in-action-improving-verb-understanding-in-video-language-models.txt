Abstract 1.

Introduction
Understanding verbs is crucial to modelling how peo-ple and objects interact with each other and the environ-ment through space and time. Recently, state-of-the-art video-language models based on CLIP have been shown to have limited verb understanding and to rely extensively on nouns, restricting their performance in real-world video applications that require action and temporal understand-ing. In this work, we improve verb understanding for CLIP-based video-language models by proposing a new Verb-Focused Contrastive (VFC) framework. This consists of two main components: (1) leveraging pretrained large language models (LLMs) to create hard negatives for cross-modal contrastive learning, together with a calibration strategy to balance the occurrence of concepts in positive and negative pairs; and (2) enforcing a ﬁne-grained, verb phrase align-ment loss. Our method achieves state-of-the-art results for zero-shot performance on three downstream tasks that fo-cus on verb understanding, including video-text matching, video question-answering and video classiﬁcation; while maintaining performance on noun-focused settings. To the best of our knowledge, this is the ﬁrst work which proposes a method to alleviate the verb understanding problem, and does not simply highlight it. Our code is publicly available at [16] : scenic/projects/verbs in action.
Large-scale visual-language models (VLMs) such as
CLIP [58] have shown strong performance on multiple video-language tasks such as text-to-video retrieval [44], video question-answering, and open-set action recogni-tion [42]. These models perform surprisingly well on these tasks in a zero-shot setting, despite being trained only on image-language pairs (with no access to temporal data), even outperforming strong video-speciﬁc models [5, 87].
A recently highlighted and well-documented problem with such models, however, is their strong noun or ob-ject bias, as evidenced by their lower performance in dis-tinguishing between verbs in natural language descrip-tions [31, 53, 93]. This was ﬁrst studied in images alone by the SVO-Probes benchmark [31], which shows that im-age-language models struggle to distinguish between dif-ferent verbs, and often rely on the nouns instead. This problem persists with video-language models that inherit these VLMs, even after they are ﬁne-tuned on video-text datasets [62, 85]. For example, Park et al. [53] similarly propose evaluation sets with hard verb negatives, and show that CLIP-based models, even when ﬁne-tuned on video datasets, have difﬁculties discriminating verbs in a multi-choice setting where the context remains unchanged. Yuk-sekgonul et al. [93] further highlight limitations of vision-language models at understanding attribute, relationship,
and order information. This deﬁciency in verb understand-ing limits the model’s applicability for real-world tasks.
Verbs encapsulate how people and objects interact with each other, and the environment, via actions in space and time.
We believe that there are two probable causes for this deﬁciency, even after ﬁne-tuning on video-text data: (i) ex-isting visual-text datasets have a strong bias towards single-frame concepts such as objects and backgrounds as well as static actions [9, 37, 67]. Models are hence less incentivized to understand dynamics and temporal actions [67], biasing them towards noun understanding; and (ii) the limitations of the cross-modal contrastive pretraining objective used by
In contrastive most current vision-language models [93]. learning, the model is trained to distinguish correct video-caption pairs from incorrect ones. Since it is unlikely that existing datasets contain many examples with captions of similar context but different verbs, the task can be solved by taking little verb information into account. This relates to shortcut learning in deep neural networks [27].
In an attempt to mitigate this problem, we propose a novel training framework for tackling the task of verb un-derstanding in vision-language models. Our framework, called Verb-Focused Contrastive pretraining (VFC), con-sists of two novel technical modiﬁcations to the contrastive learning framework. We ﬁrst introduce a method to au-tomatically generate negative sentences for training where only the verb has changed, keeping the context the same.
This is done using LLMs [23, 59], in an automatic and scal-able manner. Note that we generate hard negative captions, unlike works that simply mine hard negatives from an exist-ing paired dataset [57], or change the order of words [93].
For example, given the caption ‘two brown horses eating grass’, we generate the negative caption ‘two brown horses running on the grass’ (see Fig. 1). While this improves per-formance on some downstream tasks, we ﬁnd that introduc-ing concepts simply in negative examples can also lead to an imbalance in the contrastive objective, favouring certain concepts in the feature space. To solve this, we propose a simple but effective calibration strategy to balance the oc-currence of verbs in both positive and negative captions.
Secondly, inspired by recent works on grounding con-cepts in vision-language learning [10, 35], we also intro-duce a verb phrase loss that explicitly isolates the verb from a caption for more focused training. For example, we ex-tract the verb phrase ‘eating grass’ from the caption ‘two brown horses eating grass’ (see Fig. 1). We ﬁnd that this helps particularly for zero-shot performance on downstream tasks that do not use long sentences in their evaluation [28].
Verb phrases are also extracted from sentences using LLMs.
We then train a CLIP-based model [44] on a video-language dataset with this novel training framework. We show that a single model trained in this way transfers well to diverse downstream tasks that focus particularly on verb understanding, including three video benchmarks (multiple choice video-text matching on MSR-VTT [85], video ques-tion answering on Next-QA [82], action recognition on Ki-netics [11]) and one image benchmark (SVO-probes [31]), achieving state-of-the-art performance compared to previ-ous works in zero-shot settings (and often with ﬁne-tuning as well); while maintaining performance on noun-focused settings. On Kinetics, we also introduce a verb split of the data which speciﬁcally highlights classes that are challeng-ing to distinguish without ﬁne-grained verb understanding (‘brushing hair’ vs ‘curling hair’) and show that our model particularly improves performance on this split. 2.