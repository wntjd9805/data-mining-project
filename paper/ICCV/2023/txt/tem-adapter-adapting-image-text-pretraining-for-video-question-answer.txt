Abstract
Video-language pre-trained models have shown remark-able success in guiding video question-answering (VideoQA) tasks. However, due to the length of video sequences, train-ing large-scale video-based models incurs considerably higher costs than training image-based ones. This moti-vates us to leverage the knowledge from image-based pre-training, despite the obvious gaps between image and video domains. To bridge these gaps, in this paper, we propose
Tem-Adapter, which enables the learning of temporal dy-namics and complex semantics by a visual Temporal Aligner and a textual Semantic Aligner. Unlike conventional pre-trained knowledge adaptation methods that only concentrate on the downstream task objective, the Temporal Aligner in-troduces an extra language-guided autoregressive task aimed at facilitating the learning of temporal dependencies, with the objective of predicting future states based on historical clues and language guidance that describes event progres-sion. Besides, to reduce the semantic gap and adapt the textual representation for better event description, we intro-duce a Semantic Aligner that first designs a template to fuse question and answer pairs as event descriptions and then learns a Transformer decoder with the whole video sequence as guidance for refinement. We evaluate Tem-Adapter and different pre-train transferring methods on two VideoQA benchmarks, and the significant performance improvement demonstrates the effectiveness of our method. 1 1.

Introduction
Video Question Answering (VideoQA) is a task that aims to answer natural language questions based on the infor-mation available in observed videos. It has attracted con-siderable attention recently due to its promise to develop interactive AI systems capable of communicating with the dynamic visual environment using natural language. Despite significant advancements in recent years, VideoQA remains a challenging problem that requires models to comprehen-sively understand and dynamically align the semantics of both the visual world and natural language. 1Our code can be found at: https://github.com/XLiu443/Tem-adapter
*Equal contribution.
â€ Corresponding author.
Recently, a variety of methods [79, 78, 82, 34, 74, 63] have demonstrated impressive results in utilizing the large-scale vision-language pre-trained (VLP) model to enhance downstream VideoQA tasks. These methods pre-train mod-els by aligning the video and language domains and then apply them to VideoQA tasks via fine-tuning or even zero-shot learning. Nevertheless, pre-training large-scale models necessitate a large number of video-text pairs, e.g., more than 10 million videos, and entails expensive computational costs.
This motivates us to explore cheaper and lighter alternative pre-trained models.
Using image-VLP models is one potential option as they also align the semantics of vision and language domains.
Compared to video-based models, image-based models offer two significant advantages. Firstly, training image-based models is less expensive when considering an equal number of data pairs. Secondly, image-VLP models have made sig-nificant progress in recent years, with some of them widely available and used, such as CLIP [56]. However, as depicted in Figure 1, domain gaps persist between image-based pre-training and VideoQA tasks since image-based models are unable to learn the temporal dynamics and corresponding complex event semantics of the visual world.
To address the challenge posed by the domain gaps, this paper proposes Tem-adapter , an adapter network that leverages the interaction between visual and textual modal-ities to learn temporal dynamics and complex semantics.
Unlike existing methods that directly adapt visual and tex-tual features with the objective of the downstream tasks, such as cross-modal matching in VideoQA, Tem-Adapter introduces an additional language-guided autoregressive task to facilitate learning of temporal dynamics, which enables visual representations to predict future states based on his-torical information and language guidance. Besides, we introduce cross-modal interactions to further reduce the se-mantic gap and refine textual representation.
Our approach consists of a visual Temporal Aligner and a textual Semantic Aligner. The Temporal Aligner uses a Transformer encoder to learn the temporal relations and refine visual features. To optimize it, we build an auto-regression model by a Transformer decoder to generate the future state with historical information. We also incorporate the language information for visual refinement by adding textual embeddings as the condition memory of the Trans-former decoder. This auto-regression model is supervised by a reconstruction loss with ground truth future frames to en-courage the learning of temporal dependencies. In addition to the visual branch, we introduce a Semantic Aligner for bet-ter event description by reducing the semantic gap between the original texts crawled from the net and question-answer pairs in VideoQA. First, we design templates to fuse textual questions and answers to generate declarative sentences to reduce the domain gap between training and downstream languages. Then, a Transformer decoder is employed to refine textual embeddings by incorporating the entire video sequence as a memory condition for video-text interactions.
We conduct experiments on two public VideoQA bench-marks including SUTD-TrafficQA [76] and MSR-VTT-MC [75], and evaluate different categories of methods that transfer the pre-trained knowledge to downstream tasks, such as other adapter models [12], prompt learning [89, 21], and fully or partial finetuning. The significant improvement demonstrates the effectiveness of Tem-adapter to adapt the image-language pre-trained model for VideoQA tasks. 2.