Abstract
Coordinate networks are widely used in computer vi-sion due to their ability to represent signals as compressed, continuous entities. However, training these networks with first-order optimizers can be slow, hindering their use in real-time applications. Recent works have opted for shal-low voxel-based representations to achieve faster training, but this sacrifices memory efficiency. This work proposes a solution that leverages second-order optimization meth-ods to significantly reduce training times for coordinate net-works while maintaining their compressibility. Experiments demonstrate the effectiveness of this approach on various signal modalities, such as audio, images, videos, shape and neural radiance fields (NeRF). 1.

Introduction
Coordinate networks [39], or implicit neural functions
[35], achieve state-of-the-art results in multidimensional signal reconstruction tasks, such as image synthesis [37, 6], geometry [36, 21], and robotics [16, 5]. However, co-ordinate networks admitting traditional activation func-tions (e.g., ReLU, sigmoid, and tanh) fail to capture high-frequency details due to spectral bias [29]. To overcome this limitation, positional embedding layers [40] are often added, but they can produce noisy first-order gradients that hinder architectures requiring backpropagation [17, 8]. A recent alternative approach is to use non-traditional activa-tion functions, such as sine [35] or Gaussian [30] activa-tions, which enable high-frequency encoding without posi-tional embedding layers. The major benefit of these activa-tions over positional embedding layers is their well-behaved gradients [35, 30].
Although coordinate networks have shown remarkable performance in signal reconstruction tasks, they are typi-1∗Equal contribution.
Correspondence to: Hemanth Saratchan-dran <hemanth.saratchandran@adelaide.edu.au>,
Shin-Fang Chng
<shinfang.chng@adelaide.edu.au>. Source code will be available at https://github.com/sfchng/curvature-aware-INRs. git
Figure 1: Sine- and ReLU-coordinate networks were com-pared on an image reconstruction task using L-BFGS and
Adam optimizers. The L-BFGS optimizer showed faster convergence for the sine-network, while the ReLU-network converged faster with Adam. cally trained using first-order optimizers like Adam, lead-ing to slow training times. Consequently, some in the vision community have resorted to using shallow voxel-based rep-resentations [10, 44, 3, 38], despite their drawbacks such as high memory usage and lack of implicit architectural bias.
In this paper, we present an intriguing revelation that a new breed of coordinate networks [35, 30], activated by sine and Gaussian functions, can be trained efficiently us-ing second-order optimizers such as L-BFGS [24]. This is because their loss landscapes exhibit favorable gradient and curvature conditioning, which leads to superlinear con-vergence, in contrast to the linear convergence seen with
Adam. Fig. 1 showcases this point by comparing sine-and ReLU-coordinate networks trained with an L-BFGS optimizer [24], a curvature-aware second-order optimizer, and an Adam optimizer. The convergence rate of the sine-network trained with L-BFGS is significantly faster than the one trained with Adam – highlighting the good curvature
In contrast, the ReLU-properties of the loss landscape. network trained with Adam has faster convergence rate than the one trained with L-BFGS, a manifestation of the poor curvature properties of its loss landscape.
However, one of the downsides of second-order optimiz-ers is their computational complexity when dealing with a large number of parameters. We explore this issue in the context of coordinate networks and demonstrate that, as the network size grows, Adam may outperform L-BFGS in terms of training time. To address this challenge, we pro-pose a novel strategy of breaking down large-scale datasets into smaller patches and training a single coordinate net-work with a second-order optimizer on each patch. Our experiments reveal that this approach can lead to training time accelerations of up to 6 − 14 times faster than Adam, and serves as an effective remedy for modelling larger size signals.
A summary of our contributions are:-1. Our paper is the first to examine the training of coor-dinate networks using L-BFGS and theoretically show that while superlinear convergence is guaranteed for networks activated by sine or Gaussian functions, it is not generally guaranteed for ReLU (with or without positional embedding). 2. We validate this theory empirically by showing that sine-/Gaussian-activated coordinate networks are up to 5 times faster when trained with L-BFGS over Adam.
We present results on image, audio, video, shape and neural radiance field reconstruction tasks. 3. We explore a patch-based decomposition strategy to limit the considerable computational cost of L-BFGS as the size of the signal or network grows. Specifi-cally, we demonstrate that a sine-activated patch-based
NeRF (i.e. KiloNeRF [32]) trained with L-BFGS is 6 times more efficient than the same network trained with Adam. 2.