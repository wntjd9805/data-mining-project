Abstract
Vision transformers have achieved leading performance on various visual tasks yet still suffer from high computa-tional complexity. The situation deteriorates in dense pre-diction tasks like semantic segmentation, as high-resolution inputs and outputs usually imply more tokens involved in computations. Directly removing the less attentive tokens has been discussed for the image classiﬁcation task but can not be extended to semantic segmentation since a dense pre-diction is required for every patch. To this end, this work introduces a Dynamic Token Pruning (DToP) method based on the early exit of tokens for semantic segmentation. Mo-tivated by the coarse-to-ﬁne segmentation process by hu-mans, we naturally split the widely adopted auxiliary-loss-based network architecture into several stages, where each auxiliary block grades every token’s difﬁculty level. We can
ﬁnalize the prediction of easy tokens in advance without completing the entire forward pass. Moreover, we keep k highest conﬁdence tokens for each semantic category to up-hold the representative context information. Thus, compu-tational complexity will change with the difﬁculty of the in-put, akin to the way humans do segmentation. Experiments suggest that the proposed DToP architecture reduces on av-erage 20% ∼ 35% of computational cost for current seman-tic segmentation methods based on plain vision transform-ers without accuracy degradation. The code is available through the following link: https://github.com/ zbwxp/Dynamic-Token-Pruning. 1.

Introduction
The Transformer [21] is a remarkable invention because of its exceptional capability to model long-range depen-dencies in natural language processing.
It has been ex-tended to computer vision applications and is known as the Vision Transformer (ViT) [7], by treating every image
* Equal contribution. The work was done during Quan’s visit to the
† Corresponding
Australian Institution of Machine Learning (AIML). author: fgliu@scut.edu.cn, yifan.liu04@adelaide.edu.au. (cid:22)(cid:150)(cid:131)(cid:137)(cid:135)(cid:3)(cid:851)(cid:883) (cid:22)(cid:150)(cid:131)(cid:137)(cid:135)(cid:3)(cid:851)(cid:884) (cid:22)(cid:150)(cid:131)(cid:137)(cid:135)(cid:3)(cid:851)(cid:885)
Figure 1. Illustration of token difﬁculty levels by three stages using ADE20K dataset. The network is naturally split into stages using inherent auxiliary blocks. Each sextuplet presents the early-exited/pruned tokens and their corresponding predictions succes-sively for an image, where bright areas represent early-exited easy tokens at the current stage, while the dark ones are the kept hard tokens for the following computing. patch as a token. Beneﬁting from the global multi-head self-attention, competitive results have been achieved on various vision tasks, e.g. image classiﬁcation [7, 27], ob-ject detection [3, 32] and semantic segmentation [5, 6, 28].
However, heavy computational overhead still impedes its broad application, especially in resource-constrained envi-ronments. In semantic segmentation, the situation deterio-rates since high-resolution images generate numerous input tokens. Therefore, reducing computational costs for ViT has attracted much research attention.
Since the computational complexity of vision transform-ers is quadratic to the token number, decreasing its magni-tude is a direct path to lessen the burden of computation.
There has been a line of works studying persuasive tech-niques of token pruning regarding the image classiﬁcation task. For example, DynamicViT [19] determines kept to-kens using predicted probability by extra subnetworks, and
EViT [13] reorganizes inattentive tokens by computing their relevance with the [cls] token. Nevertheless, removing to-kens, even if they are inattentive, can not be directly ex-tended to semantic segmentation since a dense prediction is required for every image patch. Most recently, Liang et al. [12] proposed a token reconstruction layer that rebuilds clustered tokens to address the issue.
In this work, a fresh angle is taken and breaks out of the cycle of token clustering or reconstruction. Motivated by humans’ coarse-to-ﬁne and easy-to-hard segmentation pro-cess, we progressive grade tokens by their difﬁculty levels at each stage. Hence for easy tokens, their predictions can be
ﬁnalized in very early layers and their forward propagation can be halted early on. Consequently, only hard tokens are processed in the following layers. We refer to the process as the early exit of tokens. Figure 1 gives an illustration.
The main body of the relatively larger objects in the image is ﬁrst recognized and their process is ceased, while deeper layers progressively handle those challenging and confusing boundary regions and smaller objects. These predictions from the staged, early-exiting process can be used together with those from the completed inference. Since both out-puts then form the ﬁnal results jointly, it requires no token reconstruction operation and results in a simple yet effective form of efﬁcient ViT for segmentation.
This work introduces a novel Dynamic Token Pruning (DToP) paradigm in plain vision transformers for seman-tic segmentation. Given that auxiliary losses [29, 30] are widely adopted, DToP divides a transformer into stages us-ing the inherent auxiliary blocks without introducing extra modules and calculations. While previous works discard auxiliary predictions irrespectively, we make good use of them to grade all tokens’ difﬁculty levels. The intuition of such a design lies in the dissimilar recognition difﬁculties of image patches represented by individual tokens. Easy tokens are halted and pruned early on in the ViT, while hard ones are kept to be computed in the following lay-ers. We note that having this observation and shifting from auxiliary-loss-based architecture to DToP for token reduc-tion is a non-trivial contribution. A possible situation ex-ists where objects consisting of only extremely easy tokens, e.g. sky. As a result, DToP completely discards tokens from easy-to-recognize categories in early layers, and this causes a severe loss of contextual information for the few remain-ing tokens in their computations. To fully utilize the inter-class feature dependencies and uphold representative con-text information, we keep k highest conﬁdence tokens for each semantic category during each pruning process. Con-tributions are summarized as follows:
• We introduce a dynamic token pruning paradigm based on the early exit of easy-to-recognize tokens for se-mantic segmentation transformers. The ﬁnalized easy tokens at intermediate layers are pruned from the rest of the computation, and others are kept for continued processing.
• We uphold the context information by retaining k high-est conﬁdence tokens for each semantic category for the following computation, which improves segmenta-tion accuracy by guaranteeing that enough contextual information is available even in extremely easy cases.
• We apply DToP to mainstream semantic segmenta-tion transformers and conduct extensive experiments on three challenging benchmarks. Results suggest that
DToP can reduce up to 35% computation costs without a notable accuracy drop. 2.