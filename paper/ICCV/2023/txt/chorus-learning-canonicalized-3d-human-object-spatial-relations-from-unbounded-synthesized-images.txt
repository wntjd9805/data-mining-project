Abstract
We present a method for teaching machines to understand and model the underlying spatial common sense of diverse human-object interactions in 3D in a self-supervised way.
This is a challenging task, as there exist specific manifolds of the interactions that can be considered human-like and natural, but the human pose and the geometry of objects can vary even for similar interactions. Such diversity makes the annotating task of 3D interactions difficult and hard to scale, which limits the potential to reason about that in a super-vised way. One way of learning the 3D spatial relationship between humans and objects during interaction is by show-ing multiple 2D images captured from different viewpoints when humans interact with the same type of objects. The core idea of our method is to leverage a generative model that produces high-quality 2D images from an arbitrary text prompt input as an “unbounded” data generator with effec-tive controllability and view diversity. Despite its imperfec-tion of the image quality over real images, we demonstrate that the synthesized images are sufficient to learn the 3D human-object spatial relations. We present multiple strate-gies to leverage the synthesized images, including (1) the first method to leverage a generative image model for 3D human-object spatial relation learning; (2) a framework to reason about the 3D spatial relations from inconsistent 2D cues in a self-supervised manner via 3D occupancy reason-ing with pose canonicalization; (3) semantic clustering to disambiguate different types of interactions with the same object types; and (4) a novel metric to assess the quality of 3D spatial learning of interaction. 1.

Introduction
Humans interact with objects in specific ways. We wear shoes on our feet, a hat on our heads, and ride a bike by holding handles and putting two feet on the pedals. While this common sense regarding the 3D arrangements of the way we interact with objects is known to us, teaching such things to robots and machines is challenging, requiring numerous variations in diverse human-object interactions in 3D.
As providing 3D supervision by manually annotating various cases is hard to scale, an alternative way of teach-ing such things is by showing 2D photos from the Internet containing the interaction with the same object from many viewpoints. A text-based image retrieval (e.g., Google Im-age Search) can be an option to crawl many images from a text description similar to NEIL [8]. However, this approach fundamentally suffers from several obstacles such as (other than the challenges in 3D spatial relation reasoning): (1) viewpoint variations are insufficient and hard to control; (2) the number of related images decreases drastically as the compositionality of the prompt increases; and (3) the images are often biased due to commercial websites.
Project Page: https://jellyheadandrew.github.io/projects/chorus
Figure 2. Internet-crawled images vs. synthesized images. Under the prompt “A person is riding a bicycle, top view”, synthesized images exhibit superior fidelity to the intended “top view” perspec-tive compared to internet-crawled images. The correct viewpoint image is indicated by the green bounding box.
In this paper, we present a novel idea of leveraging a text-conditional generative model [52] as a controllable data producer in synthesizing “unbounded”, “multi-view”, “di-verse” images to learn the 3D human-object spatial relations in a self-supervised manner. Despite its imperfectness in quality, we observe that the synthesized images from genera-tive models are more suitable for our objective, as the gen-erative model effectively links desired semantics of human-object interaction (HOI) described in natural language. Our synthesis-based approach allows better controllability in ob-taining images for spatial relation learning, providing more relevant images from diverse viewpoints. See the examples in Fig. 2.
Nonetheless, inferring 3D spatial knowledge in human-object interaction from in-the-wild 2D image collections is still a non-trivial problem due to the inconsistency and “wild-ness” among synthesized images. In particular, a method should handle the following challenges: (1) Semantic varia-tion: given a category, there can be various semantic situa-tions of human-object interaction; thus, the spatial distribu-tion of object may vary significantly as semantics vary; (2)
Human pose variation: even assuming the same semantic sit-uation, human postures can vary as diverse actions and pose are available in the same situation; (3) Intra-class variation: object can exist in various forms (even if same category); and (4) Visual variance: when learning from 2D cues, the visual properties (e.g., illumination, camera) may vary for even the same 3D arrangement, making it difficult to localize and extract 2D cues.
To this end, we present a self-supervised method to learn the spatial common sense of diverse human-object interac-tions in 3D for arbitrary object categories without any human annotations. We present several novel components to achieve the goal, including (1) automatic prompt generation for di-verse view and semantic variations via chatGPT [45], (2) outlier filtering strategies, (3) automatic camera view calibra-tions by using a human as an anchor, (4) accumulating spatial interaction cues from inconsistent multi-view 2D knowledge with varying human pose, object geometry, and (5) clus-tering for various semantic human-object interaction types.
The output of our method can be considered as possible oc-cupancy distributions of the category-specified 3D object relative to the human body in a canonical person-centric space regarding the intra-class object variation. We demon-strate the efficacy of our method on various object categories and human-object interaction types, as shown in Fig. 1 and
Fig. 7. As the first in this direction, we also introduce a new metric, namely Projective Average Precision, to quantify the quality of 3D spatial inference outputs. Our contributions are summarized as follows: (1) the first method to leverage a generative text/image model for 3D human-object spatial relation learning, including automatic prompt generation, outlier filtering, and 3D viewpoint estimation via estimated 3D humans; (2) a framework to reason about the 3D spatial relations from inconsistent 2D cues in a self-supervised man-ner via 3D occupancy reasoning with pose canonicalization; (3) semantic clustering to disambiguate different types of interactions with the same object types; (4) a novel metric to assess the quality of 3D spatial learning of interaction. 2.