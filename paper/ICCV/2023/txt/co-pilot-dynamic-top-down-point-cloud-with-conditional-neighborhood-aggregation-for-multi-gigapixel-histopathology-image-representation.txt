Abstract
Predicting survival rates based on multi-gigapixel histopathology images is one of the most challenging tasks in digital pathology. Due to the computational complexities,
Multiple Instance Learning (MIL) has become the conven-tional approach for this process as it breaks the image into smaller patches. However, this technique fails to account for the individual cells present in each patch, while they are the fundamental part of the tissue. In this work, we devel-oped a novel dynamic and hierarchical point-cloud-based method (CO-PILOT) for the processing of cellular graphs extracted from routine histopathology images. By using bottom-up information propagation and top-down condi-tional attention, our model gains access to an adaptive focus across different levels of tissue hierarchy. Through comprehensive experiments, we demonstrate that our model can outperform all the state-of-the-art methods in survival prediction, including the hierarchical Vision Transformer (ViT), across three datasets and four metrics with only half of the parameters of the closest baseline. Importantly, our model is able to stratify the patients into different risk co-horts with statistically different outcomes across three large datasets, a task that was previously achievable only us-ing genomic information. Furthermore, we publish a large dataset containing 873 cellular graphs from 188 patients, along with their survival information, making it one of the largest publicly available datasets in this context. 1.

Introduction
The utilization of deep learning models in the digital pro-cessing of medical images has garnered substantial inter-est in the computer vision community, where these mod-els have been used for a wide range of image types (e.g., histopathology images and CT scans) and tasks (e.g., clas-sification, segmentation, and survival prediction) [41, 6, 49, 28, 47, 30, 37, 40, 29]. The ability of these models to learn meaningful features from raw images with little to no su-Figure 1: Cellular graph constructed by connecting the ad-jacent nodes within a 4, 000 × 4, 000 pixels image. The two enlarged windows demonstrated two different tissue types with distinct spatial positioning and composition of the cells. The window on the right demonstrates a high-density area while the one on the left is associated with a low-density tissue region. pervision has created exciting opportunities, especially in digitized histopathology where unique challenges are posed due to the large scale and high granularity of input images, also known as Whole-Slide Images or WSIs (Fig. 1).
The high resolution and intricate details of WSIs (each image reaching up to 150,000×150,000 pixels in size) can pose intriguing challenges in computer vision such as mem-ory limit issues during end-to-end training. To address the computational difficulties, Multiple Instance Learning (MIL) techniques are often used as the main training strat-egy. These techniques divide the WSI into smaller patches, pass them through a pre-trained feature extractor, and ag-gregate the patch embeddings to provide a representation for the whole slide. Although MIL has shown promising results in several tasks, including cancer subtype classi-fication and survival prediction, it has several significant shortcomings [20, 2, 6]. Firstly, due to the large number of patches generated from the high-resolution WSIs, most studies utilize either a simple pooling [20] or hierarchical aggregation [6]. However, the former limits the representa-tional capacity of the model, and the latter requires substan-tial computational power. Secondly, the bottom-up infor-mation flow of these methods prevents them from attending to high-granular details lying at higher resolutions. On the other hand, acquiring a top-down aggregation strategy along with the bottom-up information flow can potentially address this issue. Thirdly, the training process is heavily dependent on the number of available images, resulting in lower gen-eralizability when only a limited number of data points are available. Lastly, focusing on patches rather than individ-ual cells leads to missing the mutual interactions of cells, thereby reducing the representation power of the model to-ward the biological basis.
Various studies have shown that the spatial positioning of the cells and their mutual interactions can have a promi-nent impact on the progression of the tumor [35, 50, 34].
For instance, Sirinukunwattana et al. [34] have shown that quantitative statistics extracted from cell-cell connections can provide meaningful insights into cancer metastasis, and
Son et al. [35] explained different roles of tumor-tumor, tumor-stromal, and tumor-extracellular matrix connections in the development of therapeutic resistance. Hence, di-recting attention toward the processing of cellular structures may boost models’ performance by offering a comprehen-sive, multi-scaled perspective of the tissue.
In this study, with a point-cloud perspective, we in-vestigate the utilization of graph neural networks (GNNs) for the representation learning of the histopathology im-ages through the dynamic and hierarchical processing of the cellular graphs extracted from these images. Cellu-lar graphs grant our model the ability to examine cell-level information and the interconnections between cells (Fig.1). The versatility in focus at different scales (rang-ing from cell to tissue level) permits the model to have a multi-faceted perspective of the tissue. This is in con-trast to MIL models, which only examine patches with a pre-determined resolution and magnification. Furthermore, compared to the costly hierarchical pooling procedure in
Visual-Transformer-based methods [6], GNNs offer a more efficient approach for the processing of WSIs due to the weight sharing across the graph nodes. Consequently, this could help with the mitigation of over-parameterization is-sues in low-data regimes.
We present a dynamic top-down point-cloud-based GNN with conditional neighborhood aggregation (CO-PILOT) for learning the representation of histopathology images.
Our model begins by processing information at the cel-lular level and gradually expands to larger neighborhoods of cells, capturing the hierarchical structure of the tissue.
Through a bottom-up hierarchical process, our model com-bines the representation of each cell with its surrounding neighbors using conditional and position-aware information propagation. However, it utilizes a top-down procedure to aggregate the representations from higher to lower levels.
This enables our model to attend to finer details in the tis-sue, which is critical for challenging tasks like survival pre-diction. Our work advances the frontiers of MIL, Vision
Transformer (ViT), and GNNs in multiple directions:
• We introduce the first dynamic top-down GNN on cel-lular graphs with conditional neighborhood aggrega-tion that achieves state-of-the-art survival prediction results across three large datasets comprising 872 pa-tients.
• CO-PILOT eliminates the critical barriers of MIL models, enabling efficient training of multi-gigapixel images on a single GPU and outperforming all the baselines including Vision Transformer (ViT). It also implements the hierarchical structure of ViT while keeping the number of parameters significantly lower during end-to-end training.
• For the first time, we demonstrate that it is possible to stratify high-grade serous patients (the most aggres-sive and common subtype of ovarian cancer) into dif-ferent risk groups soley based on routine hematoxylin and eosin (H&E)-stained tissue slides.
• We will also publish a large cellular graph dataset, con-taining 873 graphs from 188 high-grade serous ovarian patients along with their survival information. To be best of our knowledge, this dataset is one of the largest datasets in this context. 2.