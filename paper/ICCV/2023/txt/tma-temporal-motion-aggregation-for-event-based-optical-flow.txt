Abstract
Event cameras have the ability to record continuous and detailed trajectories of objects with high temporal resolu-tion, thereby providing intuitive motion cues for optical flow estimation. Nevertheless, most existing learning-based ap-proaches for event optical flow estimation directly remould the paradigm of conventional images by representing the consecutive event stream as static frames, ignoring the in-herent temporal continuity of event data. In this paper, we argue that temporal continuity is a vital element of event-based optical flow and propose a novel Temporal Motion
Aggregation (TMA) approach to unlock its potential. Tech-nically, TMA comprises three components: an event split-ting strategy to incorporate intermediate motion informa-tion underlying the temporal context, a linear lookup strat-egy to align temporally fine-grained motion features and a novel motion pattern aggregation module to emphasize consistent patterns for motion feature enhancement. By incorporating temporally fine-grained motion information,
TMA can derive better flow estimates than existing meth-ods at early stages, which not only enables TMA to obtain more accurate final predictions, but also greatly reduces the demand for a number of refinements. Extensive exper-iments on DSEC-Flow and MVSEC datasets verify the ef-fectiveness and superiority of our TMA. Remarkably, com-pared to E-RAFT, TMA achieves a 6% improvement in ac-curacy and a 40% reduction in inference time on DSEC-Flow. Code will be available at https://github. com/ispc-lab/TMA. 1.

Introduction
Optical flow aims to compute the velocity of objects on the image plane without geometry prior and is a fundamen-tal topic in event-based vision [8, 42]. It plays an impor-tant role in many applications, such as ego-motion estima-tion [38, 45], image reconstruction [24], and video frame learning-based meth-interpolation [32, 31]. Recently, ods [29, 18, 30] have dominated frame-based optical flow
*corresponding author: guangchen@tongii.edu.cn
Figure 1: Accuracy illustrations. (a) Reference image from DSEC-Flow. (b) Corresponding event frame. (c) The final flow prediction and visualization of the motion feature from E-RAFT [11]. (d) The final flow prediction and visu-alization of the motion feature from our proposed method.
We visualize the motion feature at the initial stage by taking the average across channels. E-RAFT fails to generate in-formative motion features and results in blurred boundaries.
In contrast, our method utilizes temporally fine-grained mo-tion information to address the information scarcity issue in motion features, generating high-quality predictions. by employing correlation volumes to derive motion features for flow regression.
Inspired by this, several event-based approaches [11, 34] adopt a similar paradigm by converting the consecutive event stream into grid-like representations.
Despite encouraging progress, these methods still suffer from non-negligible limitations, due to the great differences between event data and conventional images. Distinct from dense and colorful conventional (RGB) images, event data features spatial sparsity and a lack of intensity information.
It is prone to result in close matching scores and invalid regions in the correlation volume. Consequently, less infor-mative motion features are derived and regress inaccurate
predictions. Figure 1 illustrates this issue in E-RAFT [11].
As the saying goes, each coin has two sides. Though event cameras encode visual information in a sparse man-ner, they are capable of capturing continuous and detailed object trajectories with high temporal resolution, thereby providing rich motion cues for optical flow estimation. Sev-eral model-based methods [9, 27] benefit from the tempo-ral continuity by warping events along point trajectories, achieving decent performance. Drawing on this observa-tion, we contend that the temporal continuity is a vital ele-ment of event-based optical flow and propose to unleash its potential within a learning-based framework.
To materialize our idea, we propose a novel Temporal
Motion Aggregation (TMA) approach to explore the inher-ent temporal continuity of event data. Technically, we re-vamp the dominant learning-based architectures with three distinct components. We first introduce an event splitting strategy for temporally-dense correlation volumes compu-tation. By splitting the event stream into multiple seg-ments and extracting their features, feature similarities are compared between the first feature and all others, which record rich intermediate motion information. Aware of that correlation volumes record motions between different time spans, we then design a linear lookup strategy to sample each correlation volume based on the corresponding flow estimate to encode motion features. As a result, fine-grained motions are warped to the same pixels across motion fea-tures. Furthermore, we consider the incorrect motion pat-terns in intermediate motion features due to the manual lookup. Therefore, we propose a novel pattern aggrega-tion module for motion feature enhancement by aggregating consistent patterns between each intermediate motion fea-ture and the last one (without manual lookup). Thanks to the incorporation of temporally fine-grained motion informa-tion, TMA can effectively address the information scarcity in motion features and thus generate good flow estimates at early stages, which not only enable TMA to obtain accurate flow predictions, but also greatly reduces the demand for a number of time-consuming refinements.
We evaluate our TMA on DSEC-Flow [10] and
MVSEC [44] datasets. Extensive experiments demonstrate the superior advantages of TMA.
In summary, our main contributions are as follows:
• We argue that the temporal continuity of event data is a vital element of event-based optical flow and propose to unlock its potential in a learning-based framework.
• We propose a novel Temporal Motion Aggregation (TMA) approach, which comprises three components: an event splitting strategy, a linear lookup strategy and a motion pattern aggregation module.
• The incorporation of temporally fine-grained motion information enables TMA to achieve high accuracy while maintaining high efficiency. Compared with E-RAFT, TMA achieves a 6% improvement in accuracy and a 40% reduction in inference time on DSEC-Flow. 2.