Abstract
Text-driven 3D stylization is a complex and crucial task in the fields of computer vision (CV) and computer graph-ics (CG), aimed at transforming a bare mesh to fit a tar-get text. Prior methods adopt text-independent multilayer perceptrons (MLPs) to predict the attributes of the target mesh with the supervision of CLIP loss. However, such text-independent architecture lacks textual guidance during predicting attributes, thus leading to unsatisfactory styliza-tion and slow convergence. To address these limitations, we present X-Mesh, an innovative text-driven 3D styliza-tion framework that incorporates a novel Text-guided Dy-namic Attention Module (TDAM). The TDAM dynamically integrates the guidance of the target text by utilizing text-relevant spatial and channel-wise attentions during vertex feature extraction, resulting in more accurate attribute pre-diction and faster convergence speed. Furthermore, exist-ing works lack standard benchmarks and automated met-rics for evaluation, often relying on subjective and non-reproducible user studies to assess the quality of stylized 3D assets. To overcome this limitation, we introduce a new standard text-mesh benchmark, namely MIT-30, and two automated metrics, which will enable future research to achieve fair and objective comparisons. Our exten-sive qualitative and quantitative experiments demonstrate that X-Mesh outperforms previous state-of-the-art meth-ods. Our codes and results are available at our project webpage: https://xmu-xiaoma666.github.io/
Projects/X-Mesh/ 1.

Introduction
In recent years, 3D asset creation through stylization, i.e., transforming bare meshes to match text prompts [39, 6,
*Corresponding author; ‡Equal contributions.
Figure 1. (a) A typical text-driven 3D stylization framework. (b)
Our proposed X-Mesh framework. X-Mesh achieves better styl-ization and faster convergence. 68], images [66, 80], and 3D shapes [77], has received sig-nificant attention in the fields of computer vision and graph-ics [14, 15, 21]. The resulting stylized 3D assets are applied to a range of practical applications, such as gaming, virtual reality, and film. Among the stylization techniques avail-able, text-driven 3D stylization is particularly user-friendly, as text prompts are more readily available than images or 3D shapes. However, creating stylized 3D assets through text input presents a significant challenge due to the signifi-cant gap between visual and linguistic information.
The emergence of Contrastive Language-Image Pre-training (CLIP) [47] has made it possible to achieve text-driven 3D stylization. Recently, Text2Mesh [39] and
TANGO [6] have made significant contributions in this field by predicting the attributes of each vertex on the mesh with the supervision of CLIP loss. Specifically, Text2Mesh pre-dicts the color and displacement of each mesh vertex to gen-erate a stylized mesh that aligns with the target text prompt.
Similarly, TANGO employs neural networks to forecast dif-fuse, roughness, specular, and normal maps to create pho-torealistic 3D meshes following a comparable approach.
Despite achieving impressive results, existing text-driven 3D stylization methods have limitations that hinder their effectiveness and efficiency. One major drawback is their failure to fully consider the semantics of the input text during the prediction of mesh vertex attributes. Current methods only rely on CLIP loss to align the rendered images from the stylized mesh with the text prompt, without any ad-ditional textual semantic guidance during predicting vertex attributes. Such approaches lead to several issues, includ-ing unsatisfactory stylization and slow convergence. For in-stance, as shown in Fig. 1(a), conventional neural style net-works do not utilize textual guidance during attribute pre-diction. As a result, the predicted vertex attributes may not align with the semantic context of the target text prompt, leading to an inconsistent stylized mesh. Moreover, the lack of additional text guidance makes it difficult to rapidly converge to an acceptable result. Typically, previous meth-ods require over 500 iterations (equivalent to over 8 minutes of training) to attain stable stylized outcomes, which is im-practical for users.
To address the issues of inconsistency and slow conver-gence in conventional neural style networks, we propose
X-Mesh, a framework that leverages textual semantic guid-ance to predict vertex attributes. As shown in Fig. 1(b),
X-Mesh produces high-quality stylized results that are con-sistent with the input text. Besides, with textual guidance during vertex attribute prediction, X-Mesh usually achieves stable results in just 200 iterations (approximately 3 min-utes of training). Our approach relies on a novel Text-guided Dynamic Attention Module (TDAM) for text-aware attribute prediction. Fig. 2(b) illustrates how spatial and channel-wise attentions are employed in TDAM to extract text-relevant vertex features. Notably, the parameters of the attention modules are dynamically generated by textual fea-tures, which makes the vertex features prompt-aware.
Additionally, the quality evaluation of the stylized results from existing text-driven 3D stylization methods [6, 39] poses a significant challenge. This challenge is mainly re-flected in two aspects. Firstly, the lack of a standard bench-mark for the text-driven 3D stylization problem presents a challenge in evaluating the effectiveness of existing meth-ods. Without fixed text prompts and meshes, the results ob-tained from previous methods are incomparable. This in turn hinders progress and the development of more effec-tive solutions. Secondly, the current evaluation of stylized 3D assets relies heavily on user studies, which is a time-consuming and expensive process. Furthermore, this eval-uation method is also subject to individual interpretation, which further hinders the reproducibility of results.
To address the aforementioned challenges, we propose a standardized text-mesh benchmark and two automatic eval-uation metrics for the fair, objective, and reproducible com-parison of text-driven 3D stylization methods. The pro-posed benchmark, called Mesh wIth Text (MIT-30), contains 30 categories of bare meshes, each of which is annotated with 5 different text prompts for diverse stylization. The proposed two evaluation metrics aims to overcome the limi-tations of subjective and non-reproducible user studies used in prior work. Specifically, we render 24 images of the styl-ized 3D mesh from fixed elevation and azimuth angles, and propose two metrics, Multi-view Expert Score (MES) and
Iteration for Target Score (ITS), to evaluate the stylization quality and convergence speed.
This paper presents two main contributions:
• We propose X-Mesh that incorporates a novel text-guided dynamic attention module (TDAM) to improve the accuracy and convergence speed of 3D stylization.
• We construct a standard benchmark and propose two automatic evaluation metrics, which facilitate objec-tive and reproducible assessments of text-driven 3D stylization techniques, and may aid in advancing this field of research. 2.