Abstract
Semi-supervised learning is attracting blooming atten-tion, due to its success in combining unlabeled data. To miti-gate potentially incorrect pseudo labels, recent frameworks mostly set a fixed confidence threshold to discard uncertain samples. This practice ensures high-quality pseudo labels, but incurs a relatively low utilization of the whole unlabeled set. In this work, our key insight is that these uncertain sam-ples can be turned into certain ones, as long as the confusion classes for the top-1 class are detected and removed. Invoked by this, we propose a novel method dubbed ShrinkMatch to learn uncertain samples. For each uncertain sample, it adap-tively seeks a shrunk class space, which merely contains the original top-1 class, as well as remaining less likely classes. Since the confusion ones are removed in this space, the re-calculated top-1 confidence can satisfy the pre-defined threshold. We then impose a consistency regularization be-tween a pair of strongly and weakly augmented samples in the shrunk space to strive for discriminative representations.
Furthermore, considering the varied reliability among un-certain samples and the gradually improved model during training, we correspondingly design two reweighting princi-ples for our uncertain loss. Our method exhibits impressive performance on widely adopted benchmarks. 1.

Introduction
In the last decade, our computer vision community has witnessed inspiring progress, thanks to large-scale datasets
[21, 10]. Nevertheless, it is laborious and costly to anno-tate massive images, hindering the progress to benefit a broader range of real-world scenarios. Inspired by this, semi-supervised learning (SSL) was proposed to utilize the unla-beled data under the assistance of limited labeled data.
The frameworks in SSL are typically based on the strategy of pseudo labeling. Briefly, the model acquires knowledge from the labeled data, and then assigns predictions on the
*Corresponding authors
Figure 1: Illustration of our motivation. Due to confusion classes for the top-1 class, the certainty fails to reach the pre-defined threshold (gray dotted line). FixMatch discards such uncertain samples. Our method, however, detects and removes confusion classes to enhance certainty, then enjoy-ing full and safe utilization of all unlabeled images. unlabeled data. The two sources of data are finally combined to train a better model. During this process, it is obvious that predictions on unlabeled data are not reliable. If the model is iteratively trained with incorrect pseudo labels, it will suffer the confirmation bias issue [1]. To address this dilemma, recent works [28] simply set a fixed confidence threshold to discard potentially unreliable samples. This simple strategy effectively retains high-quality pseudo labels, however, it also incurs a low utilization of the whole unlabeled set. As evidenced by our pilot study on CIFAR-100 [17], nearly 20% unlabeled images are filtered out for not satisfying the threshold of 0.95. Instead of blindly throwing them away, we believe there should exist a more promising approach.
This work is just aimed to fully leverage previously uncertain samples in an informative but also safe manner.
So first, why are these samples uncertain? According to our observations on CIFAR-100 and ImageNet, although the top-1 accuracy could be low, the top-5 accuracy is much higher. This indicates in most cases, the model struggles to discriminate among a small portion of classes. As illustrated
that samples with larger confidence should be attached more importance. To this end, we propose to balance different uncertain samples by their confidence in the original space. (2) Moreover, the regularization term also overlooks the gradually improved model state during training. At the start of training, there are abundant uncertain samples, but their predictions are extremely noisy, or even random. So even the highest scored class may share no relationship with the true class. Then as the training proceeds, the top classes become reliable. Considering this, we further propose to adaptively reweight the uncertain loss according to the model state. The model state is tracked and approximately estimated via per-forming exponential moving average on the proportion of certain samples in each mini-batch. With the two reweight-ing principles, the model turns out more stable, and avoids accumulating much noise from uncertain samples, especially at early training iterations.
To summarize, our contributions lie in three aspects:
• We first point out that low certainty is typically caused by a small portion of confusion classes. To enhance the certainty, we propose to shrink the original class space by adaptively detecting and removing confusion ones for the top-1 class to turn it certain in the new space.
• We manage to reweight the uncertain loss from two perspectives: the image-based varied reliability among different uncertain samples, and the model-based grad-ually improved state as the training proceeds.
• Our proposed ShrinkMatch establishes new state-of-the-art results on widely acknowledged benchmarks. 2.