Abstract
Deep learning has made significant strides in video un-derstanding tasks, but the computation required to classify lengthy and massive videos using clip-level video classifiers remains impractical and prohibitively expensive. To address this issue, we propose Audio-Visual Glance Network (AVGN), which leverages the commonly available audio and visual modalities to efficiently process the spatio-temporally im-portant parts of a video. AVGN firstly divides the video into snippets of image-audio clip pair and employs lightweight unimodal encoders to extract global visual features and au-dio features. To identify the important temporal segments, we use an Audio-Visual Temporal Saliency Transformer (AV-TeST) that estimates the saliency scores of each frame. To further increase efficiency in the spatial dimension, AVGN processes only the important patches instead of the whole images. We use an Audio-Enhanced Spatial Patch Attention (AESPA) module to produce a set of enhanced coarse visual features, which are fed to a policy network that produces the coordinates of the important patches. This approach enables us to focus only on the most important spatio-temporally parts of the video, leading to more efficient video recognition.
Moreover, we incorporate various training techniques and multi-modal feature fusion to enhance the robustness and effectiveness of our AVGN. By combining these strategies, our AVGN sets new state-of-the-art performance in multi-ple video recognition benchmarks while achieving faster processing speed. 1.

Introduction
The exponential growth of diverse video contents, partic-ularly those involving human-related actions, has prompted the development of deep learning algorithms [2, 7, 32, 45] that can effectively process and understand such data. Video recognition can bring benefits to multiple fields, such as sports for performance analysis [4], military for situational awareness [9], transportation for traffic monitoring [1], se-curity for threat detection [29] and surveillance for public safety [5]. However, the current state-of-the-art methods of-Figure 1. Audio-Visual Glance Network (AVGN) performs ef-ficient video recognition by processing only a few highly salient frames based on the audio and visual information. Then, it deter-mines and extracts the important spatial area in those frames to construct even more compact video representations. ten require high computational costs, especially when deal-ing with lengthy and heavy videos, hindering their practical application in real-world scenarios. This has led to an increas-ing demand for efficient video understanding methods [31].
To address this issue, various approaches have been pro-posed, such as developing efficient and lightweight architec-ture [17], or adaptively selecting only the most informative subset of given videos [10, 43, 47], or training a policy net-work using policy gradient [24, 30]. On the other hand, some approaches manipulate the spatial resolution of input video frames to achieve efficiency, such as extracting only im-portant spatial patches [36, 37] or adaptively changing the resolution frames based on the importance [24].
From all those approaches, we found that for efficient action recognition, we need to selectively locate and process only the important temporal and spatial location of the video.
Intuitively, we need to know when and where to look at. For example in Fig. 1, to determine the action class “skiing”, we only need the frames that contain a person riding the ski, not the frames that only show the snowfield or the person
doing no action. This inspires us to create a network that can do efficient action recognition by glancing through the video to selectively find the important frames in a low-cost man-ner. Further, we aim to utilize the additional audio modality which is naturally available together with the visual modality in video recording, mimicking the human intuition of skim-ming through long sequences using both visual and audio cues to find important keyframes. Compared to the visual modality, the action-discriminative features of audio modal-ity are easier to compute [31], and it also helps to distinguish actions that are visually similar [15].
We propose Audio-Visual Glance Network (AVGN), a comprehensive framework designed to enhance efficiency in both spatial and temporal dimensions. For temporal ef-ficiency, AVGN aims to make the correct recognition of a video sequence with only a few important frames that actu-ally contain distinctive cues. To achieve this, we construct an Audio-Visual Temporal Saliency Transformer (AV-TeST) that estimates the temporal saliency of a frame using coarse features generated by lightweight audio and visual back-bones. Furthermore, to ensure spatial efficiency, we con-struct an Audio-Enhanced Spatial Patch Attention (AESPA) module that learns the relationship between audio feature sequences and visual features. This module generates audio-enhanced visual features that can be used by a patch extrac-tion network to extract important spatial patches. For each frame, the patch contains only the important area of the im-age frame and has a lower pixel size compared to the original image. In addition to these modules, we also devise an ap-propriate feature fusion for classifier input and the training techniques to optimize these modules.
Our experimental results demonstrate that AVGN effec-tively incorporates audio and visual modalities for efficient action recognition. Our comparison with other state-of-the-art methods in Fig. 2 shows that AVGN achieves a higher mAP and lower FLOP cost on the ActivityNet dataset, in-dicating that it achieves pareto optimality. To conclude, our contributions are as follows:
• We show that incorporating audio modality into the video recognition process can lead to a pareto optimal solution, i.e., improved accuracy without sacrificing efficiency.
• Our approach combines audio and visual information to improve temporal and spatial efficiency in a unified manner, and incorporates tailored training strategies that further optimize the performance of our AVGN.
• AVGN achieves state-of-the-art performance on mul-tiple video recognition benchmarks as a result of the model building blocks and training techniques. 2.