Abstract
Deep neural networks have achieved remarkable results across various tasks. However, they are susceptible to ad-versarial examples, which are generated by adding adver-sarial perturbations to original data. Adversarial training (AT) is the most effective defense mechanism against adver-sarial examples and has received significant attention. Re-cent studies highlight the importance of example exploita-tion, where the model’s learning intensity is altered for specific examples to extend classic AT approaches. How-ever, the analysis methodologies employed by these stud-ies are varied and contradictory, which may lead to con-fusion in future research. To address this issue, we pro-vide a comprehensive summary of representative strategies focusing on exploiting examples within a unified frame-work. Furthermore, we investigate the role of examples in AT and find that examples which contribute primarily to accuracy or robustness are distinct. Based on this find-ing, we propose a novel example-exploitation idea that can further improve the performance of advanced AT meth-ods. This new idea suggests that critical challenges in AT, such as the accuracy-robustness trade-off, robust overfit-ting, and catastrophic overfitting, can be alleviated simulta-neously from an example-exploitation perspective. The code can be found in https://github.com/geyao1995/advancing-example-exploitation-in-adversarial-training. 1.

Introduction
Adversarial examples, generated by adding adversarial perturbations to original data, can easily deceive today’s deep learning models [1]. Among numerous methods to mitigate this vulnerability, adversarial training (AT) is the most effective which takes adversarial examples into the training process [2, 3, 4, 5]. However, AT is not without
*Corresponding author (liyun@njupt.edu.cn).
†This work was partially supported by National Natural Science Foun-dation of China (No.61772284) and Graduate Research and Innovation
Projects of Jiangsu Province (No.KYCX21 0795). limitations and confronts critical challenges, including: (1) the trade-off between accuracy (classification success rate for original samples) and robustness (classification success rate for examples after adding adversarial perturbations), where improving one metric comes at the expense of the other [3]; (2) the phenomenon of robust overfitting (RO), which is characterized by a gradual decline in robustness during the later stage of training [6]; and (3) the occur-rence of catastrophic overfitting (CO), which leads to a sud-den drop in robustness after a particular epoch of training
[7]. To alleviate these challenges, numerous research ef-forts have explored diverse perspectives, such as modifying model components [8, 9], refining weight optimization poli-cies [10, 11], and generating supplementary training data
[12, 13]. In addition, example exploitation has emerged as a promising perspective that has gained sustained attention.
It emphasizes the unequal contribution of examples to the model during AT and is less computationally demanding compared to other perspectives [14, 15, 16, 17, 18]. More-over, it holds great potential to uncover the essence of ad-versarial examples.
To enhance AT, example-exploitation methods typically aim to promote or discourage the learning of specific ex-ample features by the model. To prevent the model from overfitting erroneous features, SAT dynamically adjusts the one-hot label of each example in each training epoch [14].
MART prioritizes the impact of misclassified examples on model robustness by incorporating a misclassification-aware term into its objective function [15]. FAT assigns each example a different attack iteration to search for friendly adversarial examples that can improve model ac-curacy [16]. GAIRAT reweights the loss function for each example based on its geometry value, which approximates the distance from the example to the class boundary [17].
The recent work TEAT integrates the temporal ensembling approach to prevent excessive memorization of noisy ad-versarial examples [18]. Although these works offer vari-ous strategies for exploiting examples, their underlying in-sights are different and sometimes conflicting. For instance,
MART focuses on examples that FAT aims to avoid. To
eliminate confusion caused by these discrepancies in fu-ture studies, a systematic summary of example-exploitation methods is necessary to advance this field.
In this paper, we propose an unified framework to sum-marize the exploiting strategies used in representative works by dividing examples into two crucial parts: accuracy-crucial (A-C) and robustness-crucial (R-C). Our investiga-tion shows that A-C and R-C examples significantly con-tribute to accuracy and robustness, respectively, and the in-sights of existing example-exploitation research can be in-terpreted as treating A-C and R-C examples differently. We also demonstrate that there is further potential for advance-ment in the topic of example-exploitation AT by investigat-ing the roles of A-C and R-C examples. To improve the efficacy of A-C and R-C examples, we propose a novel ex-ample treatment that emphasizes the importance of both A-C examples for accuracy and R-C examples for robustness.
By applying this treatment in AT, we achieve simultaneous alleviation of the previously mentioned critical challenges from the perspective of example exploitation, which has not been achieved by any prior work. Specifically, our contri-butions are summarized as follows:
• We perform a systematic analysis of example-exploitation methods in adversarial training and identify the examples that have a greater impact on improving either accuracy or robustness.
• We propose a novel treatment idea for exploiting exam-ples in adversarial training, which fully leverages the po-tential of accuracy-crucial examples to improve accuracy and robustness-crucial examples to enhance robustness.
• Through simply applying our treatment to adversarial training, we demonstrate, for the first time, the possi-bility of simultaneously alleviating three critical chal-the accuracy-robustness trade-off, robust over-lenges: fitting, and catastrophic overfitting, solely from the example-exploitation perspective. 2.