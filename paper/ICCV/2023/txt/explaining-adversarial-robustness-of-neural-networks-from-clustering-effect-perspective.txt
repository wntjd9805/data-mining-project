Abstract
Adversarial training (AT) is the most commonly used mechanism to improve the robustness of deep neural net-works. Recently, a novel adversarial attack against inter-mediate layers exploits the extra fragility of adversarially trained networks to output incorrect predictions. The result implies the insufficiency in the searching space of the adver-sarial perturbation in adversarial training. To straighten out the reason for the effectiveness of the intermediate-layer attack, we interpret the forward propagation as the Cluster-ing Effect, characterizing that the intermediate-layer rep-resentations of neural networks for samples i.i.d. to the training set with the same label are similar, and we theo-retically prove the existence of Clustering Effect by corre-sponding Information Bottleneck Theory. We afterward ob-serve that the intermediate-layer attack disobeys the clus-tering effect of the AT-trained model. Inspired by these sig-nificant observations, we propose a regularization method to extend the perturbation searching space during train-ing, named sufficient adversarial training (SAT). We give a proven robustness bound of neural networks through rig-orous mathematical proof. The experimental evaluations manifest the superiority of SAT over other state-of-the-art
AT mechanisms in defending against adversarial attacks against both output and intermediate layers. Our code and Appendix can be found at https://github.com/ clustering-effect/SAT. 1.

Introduction
While the striking success of neural networks has been deployed into diverse real-world application scenarios [9, 28, 7, 15, 31, 32], recent studies have demonstrated that deep models are brittle to a series of crafted human-*Corresponding Author imperceptible perturbations which cause the target model to produce an incorrect output [8, 13, 3, 17, 30, 21]. This phe-nomenon leads to a significant controversy in the applica-tion of neural networks in safety-critical scenarios, e.g., au-tomatic driving systems [4], brain-computer interface sys-tems, etc. Thus, resistance to adversarial perturbations on the inputs [17], i.e., adversarial samples, is becoming a cru-cial design goal that pushes researchers to dive into propos-ing a sizable number of defense mechanisms for the adver-sarial robustness settings.
The most prosperous methodology among those defense mechanisms, i.e., Adversarial Training (AT) [8, 17, 29, 22], attempts to solve a min-max optimization problem of the loss function. AT firstly searches the constraint perturbation added to the sample as the maximum of the loss function in the input space. Thereafter, AT updates the parameters of the model utilizing the stochastic gradient descent (SGD) algorithm to approach the minimum of loss function in the parameter space. [8, 17] choose the ordinary Cross-Entropy (CE) loss where the inner maximization is equivalent to an adversarial attack to acquire incorrect outputs. [29, 22, 25] defined rectified CE loss to characterize the distance from the sample to the decision boundary. These mechanisms ob-tain pleasurable achievements against output-layer attacks (OLA), i.e., FGSM [8], PGD [17], AutoAttack [5].
However, a recent research[27] manifests the extra vul-nerability of the above defense methods to the intermediate-layer attack (ILA) [27] which disturbs features extracted by intermediate layers. This novel fragility implies AT
[8, 17, 29, 22, 25] can merely regularize the model to de-fend against OLA [8, 17, 5] rather than ILA [27]. AT merely adopts Cross-Entropy loss which entirely relies on the output of the model, so perturbations involved in train-ing are located in the maximum of Cross-Entropy loss in input space. These perturbations are constrained in a nar-row searching space during AT and can not represent the
maximum of other loss functions. Besides, ILA constructs a novel loss function different from Cross-Entropy loss by exploiting the information of both output and intermedi-ate layers, resulting in perturbations far from the search-ing space of OLA. Therefore, AT may be powerless against
ILA since the searching space of ILA and OLA are dis-similar. In a nutshell, AT is insufficient in defense against
ILA. A host of other defense methods [1, 26, 12, 18] have been on the scene, which exploit the abnormal behaviors of the extracted features induced by adversarial perturbations.
Unfortunately, this research yet hardly states a sufficient AT defending both output-layer and intermediate-layer attacks.
In this paper, we go deeply into the insufficiency of AT to derive a unified AT framework protecting the entire net-work. We set things moving by delving into the diverse im-pact of ILA and OLA at intermediate layers on “robust” AT-trained models. We highlight the effect of the adversarial perturbation through forward propagation which is univer-sally acknowledged as the process of feature extraction. To fundamentally elucidate the distortion at intermediate lay-ers induced by adversarial attacks, we raise and answer the following question as the preliminary: “How to interpret and materialize the feature extraction process of the neural network?”
As the answer to the question, it turns out that model ex-tracts features progressively in a clustering-type manner as the features are passed forward as we called Clustering Ef-fect of the model. To put it simply, given examples from the same label, which are i.i.d. to the training set, the outputs of the intermediate layer will close to a fixed vector in the sense of Lp norm. Specifically, the vector is the centroid of all outputs of the intermediate layer. The result is un-derstandable which fits the intuition that the trained model similarly encodes the samples with the same label. We de-fine the logical definition of the metric of the performance at the intermediate layer as the clustering accuracy (Clu.Acc).
We find Clu.Acc converges to the classification accuracy in deep layers, indicating the model indeed extracts essential features from samples. Finally, we theoretically explain the existence of clustering accuracy by corresponding with In-formation Bottleneck Theory[19, 20].
Then, we observe the explicit distinction between ILA and OLA where the Clu.Acc does not converge to the classi-fication accuracy under ILA on AT-trained ‘robust’ models.
Therefore, we pinpoint that the AT-trained model does not ever train sufficiently on perturbations generated by ILA.
To thoroughly eliminate the vulnerability of AT, we pro-pose the sufficient adversarial training (SAT) with a regu-larization item characterizing the deviation from extracted features to the corresponding clustering centroid, which is then incorporated into the cross-entropy loss as our devised loss function. We adversarially train models utilizing the proposed loss function as a novel AT framework Sufficient
Adversarial Training (SAT). SAT is a generalized form of previous AT since we additionally train robust intermedi-ate layers. Mathematically, we strictly prove that minimiz-ing the proposed regularization item is equivalent to min-imizing the Information Bottleneck loss function, and give a proved robustness lower bound relevant to the proposed regularization item. To summarize, we make the following contributions.
• We demonstrate the Clustering Effect of the intermedi-ate layer in extensive experiments, which characterizes the extracted features for samples i.i.d. to the training set with the same label are similar. Thereafter, we the-oretically prove the existence of the Clustering Effect by corresponding Information Bottleneck Theory.
• We observe perturbations generated by ILA deviat-ing from the Clustering Effect, demonstrating the AT-trained model has not ever been trained sufficiently on
ILA perturbations. We further visualize the distinction between OLA and ILA at intermediate presentations clearly, which is set as our motivation.
• We propose a sufficient adversarial training framework to defend against both ILA and OLA by incorporat-ing a regularization loss characterized by the interme-diate layers’ clustering effect into Cross-Entropy loss.
Mathematically, we rigidly prove proved robustness lower bound relevant to the proposed regularization item.
• We demonstrate the capacity and efficiency of SAT to enhance the robustness faced against ILA. We evalu-ate SAT on CIFAR10, SVHN, and CIFAR100 against six state-of-the-art adversarial attacks (including 5 of
OLA and 1 of ILA), manifesting its remarkable per-formance in improving the adversarial robustness of the neural network. 2.