Abstract
Existing text-video retrieval solutions are, in essence, dis-criminant models focused on maximizing the conditional likelihood, i.e., p(candidates|query). While straightforward, this de facto paradigm overlooks the underlying data distri-bution p(query), which makes it challenging to identify out-of-distribution data. To address this limitation, we creatively tackle this task from a generative viewpoint and model the correlation between the text and the video as their joint prob-ability p(candidates, query). This is accomplished through a diffusion-based text-video retrieval framework (Diffusion-Ret), which models the retrieval task as a process of gradu-ally generating joint distribution from noise. During training,
DiffusionRet is optimized from both the generation and dis-crimination perspectives, with the generator being optimized by generation loss and the feature extractor trained with contrastive loss. In this way, DiffusionRet cleverly lever-ages the strengths of both generative and discriminative methods. Extensive experiments on five commonly used text-video retrieval benchmarks, including MSRVTT, LSMDC,
MSVD, ActivityNet Captions, and DiDeMo, with superior performances, justify the efficacy of our method. More en-couragingly, without any modification, DiffusionRet even performs well in out-domain retrieval settings. We believe this work brings fundamental insights into the related fields.
Code is available at https://github.com/jpthu17/DiffusionRet. 1.

Introduction
In recent years, text-video retrieval has made significant progress, allowing humans to associate textual concepts with video entities and vice versa [67, 66]. Existing methods for video-text retrieval typically model the cross-modal interac-*Equal contribution.
†Corresponding author: Li Yuan, Jie Chen.
Figure 1: Diffusion model for text-video retrieval. (a) We propose to model the correlation between the query and the candidates as their joint probability. (b) The diffusion model has demonstrated remarkable generative power in various fields, and due to its coarse-to-fine nature, we utilize the diffusion model for joint probability generation. tion as discriminant models [24, 51]. Under the discriminant paradigm based on contrastive learning [53], the primary focus of mainstream methods is to improve the dense fea-ture extractor to learn better representation. This has led to the emergence of a large number of discriminative so-lutions [31, 4, 50, 22], and recent advances in large-scale vision-language pre-training models [56, 41] have pushed their state-of-the-art performance even further.
However, from a probabilistic perspective, discriminant models only learn the conditional probability distribution, i.e., p(candidates|query). This leads to a limitation of dis-criminant models that they fail to model the underlying data distribution [6, 48], and their latent space contains fewer intrinsic data characteristics p(query), making it difficult to achieve good generalization on unseen data [48]. In con-trast to discriminant models, generative models capture the joint probability distribution of the query and candidates, i.e., p(candidates, query), which allows them to project data into the correct latent space based on the semantic infor-mation of the data. As a typical consequence, generative models are often more generalizable and transferrable than discriminant models. Recently, generative models have made significant progress in various fields, such as generating high-quality synthetic images [19, 9, 59], natural language [10], speech [52], and music [18]. One of the most popular gener-ative paradigms is the diffusion model [61, 29, 19], which is a type of likelihood-based model that gradually remove noise via a learned denoising model to generate the signal.
In particular, the coarse-to-fine nature of the diffusion model enables it to progressively uncover the correlation between text and video, making it a promising solution for cross-modal retrieval. Therefore, we argue that it is time to rethink the current discriminant retrieval regime from a generative perspective, utilizing the diffusion model.
To this end, we propose a novel diffusion-based text-video retrieval framework, called DiffusionRet, which ad-dresses the limitations of current discriminative solutions from a generative perspective. As shown in Fig. 1, we model the retrieval task as a process of gradually generating joint distribution from noise. Given a query and a gallery of can-didates, we adopt the diffusion model to generate the joint probability distribution p(candidates, query). To improve the performance of the generative model, we optimize the proposed method from both generation and discrimination perspectives. During training, the generator is optimized by common generation loss, i.e., Kullback-Leibler diver-gence [37]. Simultaneously, the feature extractor is trained with the contrastive loss, i.e., InfoNCE loss [53], to enable discriminative representation learning. In this way, Diffu-sionRet has both the high performance of discriminant meth-ods and the generalization ability of generative methods.
The proposed DiffusionRet has two compelling advan-tages: First, the generative paradigm of our method makes it inherently generalizable and transferrable, enabling Diffu-sionRet to adapt to out-of-domain samples without requiring additional design. Second, the iterative refinement property of the diffusion model allows DiffusionRet to progressively enhance the retrieval results from coarse to fine. Experimen-tal results on five benchmark datasets for text-video retrieval, including MSRVTT [70], LSMDC [60], MSVD [11], Ac-tivityNet Captions [36], and DiDeMo [2], demonstrate the advantages of DiffusionRet. To further evaluate the general-ization of our method to unseen data, we propose a new out-domain retrieval task. In the out-domain retrieval task [13], labeled visual data with paired text descriptions are available in one domain (the “source”), but no data are available in the domain of interest (the “target”). Our method not only represents a novel effort to promote generative methods for in-domain retrieval, but also provides evidence of the merits of generative approaches in challenging out-domain retrieval settings. The main contributions are as follows:
• To the best of our knowledge, we are the first to tackle the text-video retrieval from a generative viewpoint.
Moreover, we are the first to adapt the diffusion model for cross-modal retrieval.
• Our method achieves new state-of-the-art perfor-mance on text-video retrieval benchmarks of MSRVTT,
LSMDC, MSVD, ActivityNet Captions and DiDeMo.
• More impressively, our method performs well on out-domain retrieval without any modification, which may have a significant impact on the community. 2.