Abstract
Recent vision-only perception models for autonomous driving achieved promising results by encoding multi-view image features into Bird’s-Eye-View (BEV) space. A criti-cal step and the main bottleneck of these methods is trans-forming image features into the BEV coordinate frame. This paper focuses on leveraging geometry information, such as depth, to model such feature transformation. Existing works rely on non-parametric depth distribution modeling leading to significant memory consumption, or ignore the geome-try information to address this problem.
In contrast, we propose to use parametric depth distribution modeling for feature transformation. We first lift the 2D image features to the 3D space defined for the ego vehicle via a predicted parametric depth distribution for each pixel in each view.
Then, we aggregate the 3D feature volume based on the 3D space occupancy derived from depth to the BEV frame. Fi-nally, we use the transformed features for downstream tasks such as object detection and semantic segmentation. Exist-ing semantic segmentation methods do also suffer from an hallucination problem as they do not take visibility infor-mation into account. This hallucination can be particularly problematic for subsequent modules such as control and planning. To mitigate the issue, our method provides depth uncertainty and reliable visibility-aware estimations. We further leverage our parametric depth modeling to present a novel visibility-aware evaluation metric that, when taken
∗The work is done during an internship at NVIDIA into account, can mitigate the hallucination problem. Ex-tensive experiments on object detection and semantic seg-mentation on the nuScenes datasets demonstrate that our method outperforms existing methods on both tasks. 1.

Introduction
In autonomous driving, multiple input sensors are of-ten available, each of which has its coordinate frame, such as the coordinate image frame used by RGB cameras or the egocentric coordinate frame used by the Lidar scanner.
Downstream tasks, such as motion planning, usually require inputs in a unified egocentric coordinate system, like the widely used Bird’s Eye View (BEV) space. Thus, trans-forming features from multiple sensors into the BEV space has become a critical step for autonomous driving. Here, we focus on this transformation for the vision-only setup where we take as input multi-view RGB images captured in a single time stamp by cameras mounted on the ego vehi-cle and output estimation results, such as object detection and segmentation, in a unified BEV space, see Fig. 1. In general, accurate depth information is crucial to achieve ef-fective transformations.
Early methods[16, 22] forgo explicit depth estimation and learn implicit feature transformations using neural net-works, which suffers from the generalization problem since the neural network does not have an explicit prior of the un-derlying geometric relations. More recent methods [18, 33]
adopt explicit but simplified depth representations for the transformation, which either requires large memory con-sumption, limiting the resolution [18]; or over-simplifies the representation leading to noise in the BEV space[33]. these simplified depth representation do not
Moreover, have the ability to efficiently provide visibility information.
As downstream tasks such as semantic segmentation are trained using aerial map ground truth, the lack of visibil-ity estimation usually results in hallucination effects where the network segments areas that are not visible to the sen-sor [18, 33], see Figure 2. As a consequence, those esti-mations can mislead downstream planning tasks as it is ex-tremely dangerous to drive towards hallucinated road but actually non-driveable, especially in high speed.
To address these limitations, we propose to adopt explicit parametric depth representation and geometric derivations as guidance to build a novel feature transformation pipeline.
We estimate a parametric depth distribution and use it to derive both a depth likelihood map and an occupancy distri-bution to guide the transformation from image features into the BEV space. Our approach consists of two sequential modules: a geometry-aware feature lifting module and an occupancy-aware feature aggregation module. Moreover, our parametric depth-based representation enables us to ef-ficiently derive a visibility map in BEV space, which pro-vides valuable information to decouple visible and occluded areas in the estimations and thus, mitigate the hallucina-tion problem. We also derive ground-truth visibility in BEV space, which enables us to design a novel evaluation met-ric for BEV segmentation that takes visibility into account and reveals insight of selected recent methods [18, 33] in terms of estimation on visible region and hallucination on occluded region.
Our contributions can be summarized as follows:
• We propose a geometry-aware feature transformation based on parametric depth distribution modeling to map multi-view image features into the BEV space.
Our depth distribution modeling enables the estimation of visibility maps to decouple visible and occluded ar-eas for downstream tasks.
• The proposed feature transformation framework con-sists of a novel feature lifting module that leverages the computed depth likelihood to lift 2D image features to the 3D space; and a feature aggregation module to project feature to the BEV frame through the derived 3D occupancy.
• We further propose a novel visibility-aware evaluation metric for segmentation in BEV space that reveals the insight of estimation on visible space and hallucination on occluded space.
Extensive experiments on the nuScenes dataset on object
Figure 2: Hallucination in semantic segmentation. Current methods use ground truth obtained from maps (a) and there-fore, predicted outputs (b) might represent parts that are not visible to the camera. As information is actually not avail-able, it is not possible to determine if the road areas in the occluded areas is actually free for driving. Our approach enables creating a Visibility map (c) to decouple areas that are totally occluded to the camera from those that are actu-ally visible. detection and semantic segmentation demonstrate the effec-tiveness of our method yielding state of the art results for these two tasks with a negligible compute overhead. 2.