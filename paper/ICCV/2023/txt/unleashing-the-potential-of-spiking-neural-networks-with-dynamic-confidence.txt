Abstract
This paper presents a new methodology to alleviate the fundamental trade-off between accuracy and latency in spiking neural networks (SNNs). The approach involves de-coding confidence information over time from the SNN out-puts and using it to develop a decision-making agent that can dynamically determine when to terminate each infer-ence.
The proposed method, Dynamic Confidence, provides several significant benefits to SNNs. 1. It can effectively op-timize latency dynamically at runtime, setting it apart from many existing low-latency SNN algorithms. Our experi-ments on CIFAR-10 and ImageNet datasets have demon-strated an average 40% speedup across eight different set-tings after applying Dynamic Confidence. 2. The decision-making agent in Dynamic Confidence is straightforward to construct and highly robust in parameter space, making it extremely easy to implement. 3. The proposed method en-ables visualizing the potential of any given SNN, which sets a target for current SNNs to approach. For instance, if an
SNN can terminate at the most appropriate time point for each input sample, a ResNet-50 SNN can achieve an accu-racy as high as 82.47% on ImageNet within just 4.71 time steps on average. Unlocking the potential of SNNs needs a highly-reliable decision-making agent to be constructed and fed with a high-quality estimation of ground truth. In this regard, Dynamic Confidence represents a meaning-ful step toward realizing the potential of SNNs. Code is available1.
*These authors contributed equally to this work. 1https://github.com/chenlicodebank/
Dynamic-Confidence-in-Spiking-Neural-Networks
Figure 1. The upper-bound performance of SNNs when fully uti-lizing dynamic strategies at runtime, as shown by the red curves in a. ResNet18 on CIFAR-10 and b. ResNet-50 on ImageNet.
The black curves represent baseline SNN performance without dynamic strategies. Additional figures with other settings can be found in the Supplementary Material. c. The diagram of the pro-posed Dynamic Confidence, which can be implemented on-the-fly. 1.

Introduction
Deep artificial neural networks (ANNs) have achieved remarkable success in computer vision tasks [24, 37, 30].
These improvements have been accompanied by ever-growing model complexity and neural network depth.
Though the classification accuracy has improved dramati-cally, the latency and energy cost have also increased, which poses a challenge for real-world AI applications on edge de-vices, such as mobile phones, smartwatches, and IoT hard-ware.
Deep spiking neural networks (SNNs) are promising to offer power and latency advantages over counterpart deep artificial neural networks during inference. However, when
their power (e.g. averaged spike counts per inference) or latency (e.g. averaged inference time steps per inference) are strictly constrained, SNNs suffer huge accuracy degra-dation. In recent years, much scholarly attention has been paid to the conflict between accuracy and latency in SNNs, growing the field of low-latency SNNs (also known as fast
SNNs). The primary goal of fast SNN research is achiev-ing equivalent or slightly-lower accuracy than the baseline
ANN accuracy with as few inference time steps as possi-ble. With the reduction in the number of inference time steps, the spike counts required for each inference can po-tentially be reduced as well, if the firing rates can be kept the same when latency decreases. In hardware that can take scale power with spike processing operations, this research on low-latency SNNs can contribute to reducing the power requirements of SNNs.
In this paper, we detail a runtime optimization method called Dynamic Confidence and show that it can effec-tively reduce the inference latency in low-latency SNNs on
CIFAR-10 and ImageNet, without the need to sacrifice ac-curacy or to increase firing rates. Our paper has several new contributions:
• We propose a method (Dynamic Confidence) to de-code a temporal series of confidence values from the inference results of an SNN, and utilize this confidence information to dynamically guide inference. It is the first study to formulate confidence in SNNs and in-troduce the formal concept of dynamic strategies to
SNNs.
• Dynamic Confidence can further reduce the inference latency by 40% on average on the state-of-the-art low-latency SNNs (QFFS[26] and QCFS[3]) on CIFAR-10 and ImageNet. By reducing the inference latency and maintaining the same firing rates, the power consump-tion of SNN inference is reduced consequently.
• We provide a method to calculate the upper-bound per-formance that an SNN can achieve when applying dy-namic strategies at runtime. Our findings shed light on the vast untapped potential within current SNNs, providing valuable insights to answer the fundamental question of why to use SNNs instead of ANNs.
• Dynamic Confidence is a lightweight, parameter-insensitive, on-the-fly method. These features make
Dynamic Confidence extremely easy to be applied on a variety of low-latency SNN algorithms whenever fur-ther latency and power reductions are desirable. 2. Motivation
• A main research method adopted in current SNN studies is improving SNNs by leveraging knowledge from ANNs. Some successful examples of this in-clude, but are not limited to, straight-through estima-tors [32], quantization [26], Backpropagation through time (BPTT) [2], network architectures [28], neural ar-chitecture search [22], and neural network simulation tools [39]. The primary concern during this process is how to make ANN knowledge applicable to SNNs, which usually requires additional optimization of spike dynamics in SNNs to ensure smooth knowledge trans-fer. The study presented in this paper also follows this research method. Specifically, this study is intended to explore whether confidence, a key concept in Bayesian neural networks and interpretable AI can be applied to
SNNs to help reduce inference latency as well as in-ference power. We discovered that the temporal di-mension of SNNs provides richer information in con-fidence compared to ANNs. This paper exemplifies a simple way to leverage the information in SNN confi-dence to further boost the performance of SNNs.
• Dynamic strategies have received considerable critical attention in ANNs due to their effectiveness in bring-ing considerable speedup and computational savings without degrading prediction accuracy [40]. In con-trast, there is not any research studying the applica-tion of dynamic strategies to SNNs and the evalua-tion of possible speed and power gains on nontrivial datasets such as CIFAR-10 and ImageNet. This paper seeks to address an existing gap by sharing the prelim-inary findings of implementing a dynamic strategy in
SNNs. Different from other dynamic strategy research in ANNs, we found in this paper that spikes make it convenient to apply a dynamic strategy explicitly in the dimension of precision. This task has been notably challenging in existing ANN dynamic strategy litera-ture.
• Information is in the central position in SNN research.
For example, a primary concern in directly-trained
SNNs is whether better spatial-temporal information representation and computing can be achieved by sur-rogate gradients. This paper also emphasizes the sig-nificance of the information, but from a different per-spective. Instead of seeking more spatial-temporal in-formation by replacing ANN-to-SNN conversion with spatial-temporal training by surrogate gradients, we stick to using ANN-to-SNN conversion but try to seek if any information in the SNN output is missed by other research and has not been fully exploited. Our results show that confidence in SNN outputs contains valuable information and can be leveraged to improve
SNN performance on latency and power.
3.