Abstract
Extracting useful visual cues for the downstream tasks is especially challenging under low-light vision. Prior works create enhanced representations by either correlating visual quality with machine perception or designing illumination-degrading transformation methods that require pre-training on synthetic datasets. We argue that optimizing enhanced im-age representation pertaining to the loss of the downstream task can result in more expressive representations. Therefore, in this work, we propose a novel module, FeatEnHancer, that hierarchically combines multiscale features using multi-headed attention guided by task-related loss function to cre-ate suitable representations. Furthermore, our intra-scale enhancement improves the quality of features extracted at each scale or level, as well as combines features from differ-ent scales in a way that reflects their relative importance for the task at hand. FeatEnHancer is a general-purpose plug-and-play module and can be incorporated into any low-light vision pipeline. We show with extensive experimentation that the enhanced representation produced with FeatEnHancer significantly and consistently improves results in several low-light vision tasks, including dark object detection (+5.7 mAP on ExDark), face detection (+1.5 mAP on DARK FACE), nighttime semantic segmentation (+5.1 mIoU on ACDC), and video object detection (+1.8 mAP on DarkVision), high-lighting the effectiveness of enhancing hierarchical features under low-light vision. 1.

Introduction
Recent remarkable advancements in high-level vision tasks have shown that given a high-quality image, current vision backbone networks [20, 15, 12, 32, 31], object de-tectors [42, 28, 43, 19, 2, 3, 49, 4, 71, 64, 65] and se-mantic segmentation models [34, 48, 57, 7, 58] can effec-tively learn desired features to perform vision tasks. Simi-larly, modern low-light image enhancement (LLIE) meth-ods [44, 67, 14, 21, 17, 25] are capable of transforming a low-light image into a visual-friendly representation. How-ever, a naive combination of the two brings sub-optimal gains when it comes to high-level vision tasks under low-light vision.
This work explores the underlying reasons for the low performance of the combination of LLIE with high-level vision methods and observes the following limitations: 1)
Although existing LLIE methods push the envelope of visual perception for human eyes, they do not align with vision backbone networks [20, 12, 15, 32, 31] due to lack of multi-scale features. For instance, it is likely that the enhancement method increases brightness in some regions. However, it simultaneously corrupts the edges and texture information of objects. 2) The pixel distribution among different low-light images may have huge variance owing to the disparity in less illuminated environments [17, 25, 68]. This increases intra-class variance in some cases (see Fig. 3, where only one bicycle is recognized by [17] instead of two bicycles in the ground-truth). 3) Current LLIE approaches [14, 17, 25, 21, 44, 56, 67] employ enhancement loss functions to optimize the enhancement networks. These loss functions compel the network to attend to all pixels equally, lacking the learning of informative details necessary for high-level downstream vision tasks such as object pose and shape for object detection. Furthermore, to train these enhancement networks, most of them [44, 14, 67, 56] require a set of high-quality images, which is hardly available in a real-world setting.
Motivated by these observations and inspired by recent developments in LLIE [17, 25] and vision-based backbone networks [15, 32, 31], this paper aims to bridge the gap by exploring an end-to-end trainable recipe that jointly opti-mizes the enhancement and downstream task objectives in a single network. To this end, we present FeatEnHancer, a general-purpose feature enhancer that learns to enrich multi-scale hierarchical features favourable for downstream vision tasks in a low-light setting. An example of learned hierarchi-cal representation and the enhanced image is illustrated in
Fig. 1.
In particular, our FeatEnHancer first downsamples a low-light RGB input image to construct multi-scale hierarchical representations. Subsequently, these representations are fed to our Feature Enhancement Network (FEN), which is a deep convolutional network, employed to enrich intra-scale se-mantic representations. Note that the parameters of FEN can be adjusted through task-related loss functions, which pushes the FEN to only enhance the task-related features. This multi-scale learning allows the network to enhance both global and local information from higher and lower-resolution features, respectively. Once the enhanced representations on differ-ent scales are obtained, the remaining obstacle is to fuse them effectively. To achieve, this, we select two different strategies to capture both global and local information from higher and lower-resolution features. First, to merge high-resolution features, inspired by multi-head attention in [50], we design a Scale-aware Attentional Feature Aggregation (SAFA) method that jointly attends information from differ-ent scales. Second, for lower-resolution features, the skip connection [20] scheme is adopted to merge the enhanced representation from SAFA to lower-resolution features. With these jointly learned hierarchical features, our FeatEnHancer provides semantically powerful representations which can be exploited by advanced methods such as feature pyramid networks [27] for object detection [43] and instance segmen-tation [19], or UNet [45] for semantic segmentation [34].
The main contributions of this work can be summarized as follows: 1. We propose FeatEnHancer, a novel module that en-hances hierarchical features to boost downstream vi-sion tasks under low-light vision. Our intra-scale fea-ture enhancement and scale-aware attentional feature aggregation schemes are aligned with vision backbone networks and produce powerful semantic representa-tions. FeatEnHancer is a general-purpose plug-and-play module that can be trained end-to-end with any high-level vision task. 2. To the best of our knowledge, this is the first work that fully exploits multi-scale hierarchical features in low-light scenarios and generalizes to several down-stream vision tasks such as object detection, semantic segmentation, and video object detection. 3. Extensive experiments on four different downstream visions tasks covering both images and videos demon-strate that our method brings consistent and significant improvements over baselines, LLIE methods and task-specific state-of-the-art approaches. 2.