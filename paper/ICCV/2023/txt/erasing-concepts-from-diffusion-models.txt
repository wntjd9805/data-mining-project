Abstract 1.

Introduction
Motivated by concerns that large-scale diffusion models can produce undesirable output such as sexually explicit content or copyrighted artistic styles, we study erasure of speciﬁc concepts from diffusion model weights. We propose a ﬁne-tuning method that can erase a visual concept from a pre-trained diffusion model, given only the name of the style and using negative guidance as a teacher. We benchmark our method against previous approaches that remove sexually ex-plicit content and demonstrate its effectiveness, performing on par with Safe Latent Diffusion and censored training. To evaluate artistic style removal, we conduct experiments eras-ing ﬁve modern artists from the network and conduct a user study to assess the human perception of the removed styles.
Unlike previous methods, our approach can remove concepts from a diffusion model permanently rather than modifying the output at the inference time, so it cannot be circumvented even if a user has access to model weights. Our code, data, and results are available at erasing.baulab.info.
*Equal contribution
Recent text-to-image generative models have attracted at-tention due to their remarkable image quality and seemingly inﬁnite generation capabilities. These models are trained on vast internet datasets, which enables them to imitate a wide range of concepts. However, some concepts learned by the model are undesirable, including copyrighted content and pornography, which we aim to avoid in the model’s out-put [27, 16, 29]. In this paper, we propose an approach for selectively removing a single concept from a text-conditional model’s weights after pretraining. Prior approaches have focused on dataset ﬁltering [30], post-generation ﬁltering
[29], or inference guiding [38]. Unlike data ﬁltering meth-ods, our method does not require retraining, which is pro-hibitive for large models. Inference-based methods can cen-sor [29] or steer the output away from undesired concepts effectively [38], but they can be easily circumvented. In contrast, our approach directly removes the concept from the model’s parameters, making it safe to distribute its weights.
The open-source release of the Stable Diffusion text-to-image diffusion model has made image generation technol-ogy accessible to a broad audience. To limit the generation of unsafe images, the ﬁrst version was bundled with a simple
NSFW ﬁlter to censor images if the ﬁlter is triggered [29], yet since both the code and model weights are publicly avail-able, it is easy to disable the ﬁlter [43]. In an effort to prevent the generation of sensitive content, the subsequent SD 2.0 model is trained on data ﬁltered to remove explicit images, an experiment consuming 150,000 GPU-hours of computa-tion [32] over the 5-billion-image LAION dataset [39]. The high cost of the process makes it challenging to establish a causal connection between speciﬁc changes in the data and the capabilities that emerge, but users report that removing explicit images and other subjects from the training data may have had a negative impact on the output quality [30].
And despite the effort, explicit content remains prevalent in the model’s output: when we evaluate generation of images using prompts from the 4,703 prompts of the Inappropriate
Image Prompts (I2P) benchmark [38], we ﬁnd that the popu-lar SD 1.4 model produces 796 images with exposed body parts identiﬁed by a nudity detector, while the new training-set-restricted SD 2.0 model produces 417 (Figure 7).
Another major concern regarding the text-to-image mod-els is their ability to imitate potentially copyrighted content.
Not only is the quality of the AI-generated art on par with the human-generated art [34], it can also faithfully replicate an artistic style of real artists. Users of Stable Diffusion [31] and other large-scale text-to-image synthesis systems have discovered that prompts such as “art in the style of [artist]” can mimic styles of speciﬁc artists, potentially devaluing original work. Copyright concerns of several artists has led to a lawsuit against the makers of Stable Diffusion [1], raising new legal issues [41]; the courts have yet to rule on these cases. Recent work [42] aims to protect the artist by applying an adversarial perturbation to artwork before posting it online to prevent the model from imitating it. That approach, however, cannot remove a learned artistic style from a pretrained model.
In response to safety and copyright infringement con-cerns, we propose a method for erasing a concept from a text-to-image model. Our method, Erased Stable Diffusion (ESD), ﬁne-tunes the model’s parameters using only unde-sired concept descriptions and no additional training data.
Unlike training-set censorship approaches, our method is fast and does not require training the whole system from scratch. Furthermore, our method can be applied to existing models without the need to modify input images [42]. Un-like the post-ﬁltering [29] or simple blacklisting methods, erasure cannot be easily circumvented, even by users who have access to the parameters. We benchmark our method on removing offensive content and ﬁnd that it is as effective as Safe Latent Diffusion [38] for removing offensive images.
We also test the ability of our method to remove an artistic style from the model. We conduct a user study to test the impact of erasure on user perception of the remove artist’s style in output images, as well as the interference with other artistic styles and their impact on image quality. Finally, we also test our method on erasure of complete object classes. 2.