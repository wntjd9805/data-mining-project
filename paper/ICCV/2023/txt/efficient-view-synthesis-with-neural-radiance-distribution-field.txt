Abstract
Recent work on Neural Radiance Fields (NeRF) has demonstrated significant advances in high-quality view syn-thesis. A major limitation of NeRF is its low rendering effi-ciency due to the need for multiple network forwardings to render a single pixel. Existing methods to improve NeRF ei-ther reduce the number of required samples or optimize the implementation to accelerate the network forwarding. De-spite these efforts, the problem of multiple sampling persists due to the intrinsic representation of radiance fields. In con-trast, Neural Light Fields (NeLF) reduce the computation cost of NeRF by querying only one single network forward-ing per pixel. To achieve a close visual quality to NeRF, existing NeLF methods require significantly larger network capacities which limits their rendering efficiency in prac-tice. In this work, we propose a new representation called
Neural Radiance Distribution Field (NeRDF) that targets efficient view synthesis in real-time. Specifically, we use a small network similar to NeRF while preserving the ren-dering speed with a single network forwarding per pixel as in NeLF. The key is to model the radiance distribution along each ray with frequency basis and predict frequency weights using the network. Pixel values are then computed via vol-ume rendering on radiance distributions. Experiments show that our proposed method offers a better trade-off among speed, quality, and network size than existing methods: we achieve a ∼254× speed-up over NeRF with similar network size, with only a marginal performance decline. Our project page is at yushuang-wu.github.io/NeRDF. 1.

Introduction
The problem of digitally representing a 3D scene for novel view synthesis from arbitrary directions is an im-portant research topic with many applications, ranging from immersive conferencing to augmented reality. The
∗ This work was done when Yushuang Wu was an intern at MSRA.
† Corresponding author. breakthroughs made by the pioneering work of NeRF [23] demonstrate considerable advancements in the view synthe-sis field, as it represents 3D scenes using implicit radiance fields modeled via neural networks. Despite these advances, a significant drawback of NeRF is its computationally in-tensive nature, requiring hundreds of network evaluations per pixel, resulting in slow rendering speed (e.g. ∼0.2 FPS on a high-end GPU). Subsequent research has aimed to en-hance the rendering speed by improving importance sam-pling strategies, reducing the number of samples, or opti-mizing the code implementation. However, these do not address the fundamental problem of multiple sampling in-herent in radiance fields, which limits the extent to which rendering speed can be improved (usually 5-30 FPS con-tingent on implementation) at the cost of increased mem-ory requirements and additional implementation efforts. To achieve efficient view synthesis, the research community is exploring alternative representations such as neural light fields (NeLF) [38, 22, 41, 35, 1]. A NeLF maps rays di-rectly into the RGB space, predicting pixel color based on the ray parameters (e.g. the origin and direction), thereby reducing the intrinsic computational complexity to one sin-gle network forwarding per pixel. Recent advances have in-dicated that the synthesis quality of NeLF can be compara-ble to that of NeRF. However, this usually comes at the cost of much larger networks - for instance, R2L [41] utilizes an 88-layer MLP network that is 11× larger than NeRF. The increased size of networks used in NeLF methods results in significantly higher computational and memory costs, as well as limited rendering efficiency in practice.
In this paper, we examine the challenge of achieving ef-ficient view synthesis in practical settings, which requires a careful balance among multiple considerations including perceptual quality (measured by PSNR), computational ef-ficiency (measured by FPS), and memory requirements (e.g. model size). To achieve this goal, we start from a key obser-vation: previous NeLF methods have no explicit perception to the 3D geometry information. As known, NeRF attends to the radiance information at spatial locations along each camera ray as illustrated in Fig. 1, which enables NeRF
Figure 1. The overview of our Neural Radiance Distribution Field (NeRDF) and the comparison of (a) NeRF, (b) NeLF, and (c) NeRDF. NeRF requires hundreds of network forwarding per ray to predict the volume density and color, and output the pixel RGB via volume rendering. NeLF takes only one single forwarding per ray to predict the pixel RGB but strongly depends on a much larger network. Our NeRDF absorbs both advantages that takes only one single forwarding per ray as NeLF with only a small network as NeRF. The key idea is to directly predict the radiance distribution from the ray input. to have a perception of the 3D geometry information of a scene. However, NeLF learns the direct mapping from the huge ray space to the pixel RGB space without any 3D prior.
This paradigm hinders NeLF from learning the actual in-trinsic 3D layout/structure of a scene, which results in an over-dependence on a large network capacity, as previous
NeLF methods [1, 41] suffer from.
Based on this observation, we propose a novel implicit representation, termed as Neural Radiance Distribution
Field (NeRDF). NeRDF is based on a simple yet effective key idea: from the input ray space as in NeLF, NeRDF yet learns the radiance distribution of a given ray, so that the spatial 3D geometry information can be perceived. Specifi-cally, we train the network to produce the radiance distribu-tion along a ray, parameterized using a set of trigonometric functions. The final pixel color is re-synthesized via volume rendering from the output radiance distribution as in NeRF.
Thus, the proposed NeRDF combines the strengths of both
NeRF-based and NeLF-based methods. NeRDF models the parametric ray radiance distribution in order for a signifi-cantly more compact target space than direct modeling the ray-to-pixel mapping. This enables NeRDF to represent scenes with much smaller neural networks that are compa-rable to those in NeRF. Additionally, the prediction of the radiance distribution of a ray only takes one single network forwarding, so only one evaluation is required to render a pixel as in NeLF.
The learning of NeRDF is based on a knowledge distil-lation framework inspired by [41], where a teacher NeRF synthesizes dense and diverse views that then serve as the pseudo-training data. We further contribute three novel de-signs to enhance the framework: (i) an input ray encod-ing method that captures rich ray information, (ii) an on-line view sampling strategy that expands the diversity of the pseudo-training data, and (iii) a volume density constraint loss that promotes the learning of a strong 3D prior.
We have evaluated the efficiency of the NeRDF method on the Real Forward-Facing (LLFF) dataset [23]. On aver-age, without any specific optimization in network inference, our proposed NeRDF method achieves a comparable visual quality to existing methods while rendering at a speed of
∼21 FPS and using only an 8-layer MLP network. This is
∼5× faster than the R2L method [41] and ∼100× faster than the original NeRF. Additionally, our method can ben-efit from any off-the-shelf inference optimization methods.
By employing tiny-cuda-nn [25] as our inference backend, our NeRDF achieves a rendering speed of ∼369 FPS, which is a ∼1400× speed-up over an unoptimized NeRF and a 10-15× speed-up compared with previous methods using the same backend. We also demonstrate that the NeRDF method provides a superior trade-off between speed, mem-ory, and visual quality compared with previous NeRF-based and NeLF-based methods. Our contributions are as follows:
• A new neural representation for 3D scenes, Neural Ra-diance Distribution Field (NeRDF), that outputs radi-ance distribution along rays.
• A method for NeRDF learning with a compact neural network that has high rendering speed, low memory cost, and plausible quality.
• An efficient view synthesis solution that has a good trade-off among visual quality, speed, and memory. 2.