Abstract
Training deep generative models usually requires a large amount of data. To alleviate the data collection cost, the task of zero-shot GAN adaptation aims to reuse well-trained generators to synthesize images of an unseen target domain without any further training samples. Due to the data ab-sence, the textual description of the target domain and the vision-language models, e.g., CLIP, are utilized to effec-tively guide the generator. However, with only a single rep-resentative text feature instead of real images, the synthe-sized images gradually lose diversity as the model is opti-mized, which is also known as mode collapse. To tackle the problem, we propose a novel method to find semantic varia-tions of the target text in the CLIP space. Specifically, we ex-plore diverse semantic variations based on the informative text feature of the target domain while regularizing the un-controlled deviation of the semantic information. With the obtained variations, we design a novel directional moment loss that matches the first and second moments of image and text direction distributions. Moreover, we introduce elas-tic weight consolidation and a relation consistency loss to effectively preserve valuable content information from the source domain, e.g., appearances. Through extensive exper-iments, we demonstrate the efficacy of the proposed methods in ensuring sample diversity in various scenarios of zero-shot GAN adaptation. We also conduct ablation studies to validate the effect of each proposed component. Notably, our model achieves a new state-of-the-art on zero-shot GAN adaptation in terms of both diversity and quality. 1.

Introduction
In recent years, deep generative models, especially gen-erative adversarial networks (GANs) [9], have shown dra-matic advancements by successfully mimicking the real dis-tribution of images [3, 13, 6]. However, as diagnosed in the literature [47, 25], building powerful generative models re-*Work done during his internship at Microsoft Research Asia
†Corresponding author quires a huge number of visual samples as well as expensive training costs. This essentially restricts the applicability of the models to domains where it is prohibitively expensive or even infeasible to collect sufficient data, such as medical images or artworks of specific artists. To alleviate the lim-itation, researchers give their attention to the task of GAN adaptation, which aims to reuse the representation power of well-trained generators for synthesizing images of a target domain. To this end, existing works manage to transfer the generation capability of pre-trained GANs to unseen target domains by exploiting a tiny dataset [40] with only a few vi-sual samples (i.e., few-shot) [39, 22, 23, 28, 34, 29], or even no data at all (i.e., zero-shot) [8]. This paper focuses on the zero-shot setting, where a generative model pre-trained on a source dataset is supposed to be adapted to an unseen target domain that contains no visual samples for training.
In order to perform adaptation with no accessibility to data of the target domain, the previous work hinges on the powerful vision-language model, i.e., CLIP [32], which learns the shared latent space between vision and text modalities. Specifically, StyleGAN-NADA [8] embeds two textual prompts respectively describing the source and tar-get domains into the CLIP space and derives the difference vector between them. Considering the difference vector to be the guiding direction, the generated images gradually step toward the target domain. Eventually, the generator is able to synthesize visually plausible images of the target domain even without seeing any samples of the domain.
However, the adapted model with only a single guiding direction suffers from mode collapse. That is, the generated target samples share the same characteristics without dis-tinction. For instance, the generated faces under the “Photo-to-Pixar” scenario have exactly the same attributes, e.g., emotional expression, slightly opened mouth, dark hair (fig-In another example, the results under ure 1 (a) center). the “Dog-to-Cat” scenario exhibit nearly identical cat faces with few differences (figure 1 (b) center). These problems come from the one-to-one mechanism of the CLIP text en-coder; given a single guiding direction, the adapted genera-tor is unable to handle the diversity of the target domain. In-tuitively, the target textual prompt provides the most general
Figure 1. Illustration of our motivation. For two adaptation scenarios, i.e., “Photo-to-Pixar” and “Dog-to-Cat”, we present source domain images and the corresponding generated images of the target domain by StyleGAN-NADA [8] and ours. information about the target domain while there are an in-finitely large number of semantic variations underneath. For instance, the target domain “Cat” implicitly covers a vari-ety of species with diverse characteristics such as a smiling
Sphynx and a brown Scottish Fold. Hence, relying solely on a single target description fails to exploit the inherent se-mantic variations, and it is crucial to model a one-to-many relation for enhancing sample diversity after adaptation.
In this paper, we explore semantic variations of the given text prompt of the target domain with a novel two-stage framework in order to alleviate the mode collapse problem.
In the first stage, to discover the variations, we impose a set of learnable perturbations on the target text embedding in the CLIP space. They are encouraged to be orthogonal to each other for redundancy reduction yet not to disturb the original semantics of the target domain. The obtained vari-ations are then used to compute guiding directions. In the second stage, to improve sample diversity using multiple guidances, we introduce a novel directional moment loss. It effectively aligns image-updating directions with the guid-ances by matching their first and second moments.
In addition, we propose a relation consistency loss to bet-ter sustain the knowledge of the generator learned from the source domain. Ideally, the relation between two generated images should remain the same during adaptation to ensure consistency of semantic information. From this motivation, the relation consistency loss is designed to minimize the dis-tribution gap between the inter-image relations of the source and target domains. Further, we employ the elastic weight consolidation [19] to suppress excessive changes of impor-tant parameters of the generator during adaptation. This prevents our model from losing the strong content repre-sentability of the generator during the adaptation, thereby preserving the original content information.
Equipped with the proposed components, our model is able to generate images with diverse semantic variations of the target domain, while successfully preserving the orig-inal semantic information of the source domain. The su-periority of our method over the previous work is clearly showcased in Figure 1. Through extensive experiments on various adaptation scenarios, we demonstrate the effective-ness of each component of our model. Moreover, our model achieves a new state-of-the-art on zero-shot GAN adapta-tion in terms of quality as well as diversity. 2.