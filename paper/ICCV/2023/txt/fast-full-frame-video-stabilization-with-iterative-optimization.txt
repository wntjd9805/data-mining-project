Abstract
Video stabilization refers to the problem of transform-ing a shaky video into a visually pleasing one. The ques-tion of how to strike a good trade-off between visual qual-ity and computational speed has remained one of the open challenges in video stabilization. Inspired by the analogy between wobbly frames and jigsaw puzzles, we propose an iterative optimization-based learning approach using syn-thetic datasets for video stabilization, which consists of two interacting submodules: motion trajectory smoothing and full-frame outpainting. First, we develop a two-level (coarse-to-ﬁne) stabilizing algorithm based on the proba-bilistic ﬂow ﬁeld. The conﬁdence map associated with the estimated optical ﬂow is exploited to guide the search for shared regions through backpropagation. Second, we take a divide-and-conquer approach and propose a novel multi-frame fusion strategy to render full-frame stabilized views.
An important new insight brought about by our iterative optimization approach is that the target video can be in-terpreted as the ﬁxed point of nonlinear mapping for video stabilization. We formulate video stabilization as a problem of minimizing the amount of jerkiness in motion trajecto-ries, which guarantees convergence with the help of ﬁxed-point theory. Extensive experimental results are reported to demonstrate the superiority of the proposed approach in terms of computational speed and visual quality. The code will be available on GitHub. 1.

Introduction
With the growing popularity of short videos on social media platforms (e.g., TikTok, Instagram), video has played an increasingly important role in our daily life. However, casually captured videos are often shaky and wobbly due to
*Corresponding author https://github.com/zwyking/Fast-Stab amateur shooting. Although it is possible to alleviate those problems by resorting to professional equipment (e.g., dol-lies and steadicams), the cost of hardware-based solutions is often expensive, making it impractical for real-world ap-plications. By contrast, software-based or computational solutions such as video stabilization algorithms [12] have become an attractive alternative to improve the visual qual-ity of shaky video by eliminating undesirable jitter.
Existing video stabilization techniques can be classiﬁed into two categories: optimization-based and learning-based.
Traditional optimization-based algorithms [11,18,21,27,39] have been widely studied due to their speed and robust-ness. The challenges of them are the occlusion caused by changes in depth of ﬁeld and the interference caused by foreground objects on camera pose regression. Further-more, their results often contain large missing regions at frame borders, particularly when videos with a large cam-era motion. In recent years, learning-based video stabiliza-tion algorithms [6, 23, 45, 50] have shown their superiority by achieving higher visual quality compared to traditional methods. However, their stabilization model is too complex for rapid computation, and its generalization property is un-known due to the scarcity of training datasets.
To overcome those limitations, we present an iterative optimization-based learning approach that is efﬁcient and robust, capable of achieving high-quality stabilization re-sults with full-frame rendering, as shown in Fig. 1. The probabilistic stabilized network addresses the issues of oc-clusion and interfering objects, and achieves fast pose esti-mation. Then the full-frame outpainting module retains the original ﬁeld of view (FoV) without aggressive cropping.
An important new insight brought by our approach is that the objective of video stabilization is to suppress the implic-itly embedded noise in the video frames rather than the ex-plicit noise in the pixel intensity values. This inspired us to adopt an expectation-maximization (EM)-like approach for video stabilization. Importantly, considering the strong re-dundancy of video in the temporal domain, we ingeniously
Figure 1. Overview of our video stabilization framework. It consists of motion trajectory smoothing (in Sec. 3 and Sec. 4) and full-frame outpainting modules (in Sec. 5). The former adopts the two-level (coarse-to-ﬁne) stabilizing algorithm to obtain a stabilized video. The latter further render a full-frame video with strategies of ﬂow outpainting and multiframe fusion. consider stable video (the target of video stabilization) as the ﬁxed point of nonlinear mapping. Such a ﬁxed-point perspective allows us to formulate an optimization problem of the optical ﬂow ﬁeld in commonly shared regions. Unlike most methods that resort to the ad hoc video dataset [56] or the deblurred dataset [35] as stabilized videos, we propose to construct a synthetic training dataset to facilitate joint op-timization of model parameters in different network mod-ules.
To solve the formulated iterative optimization problem, we take a divide-and-conquer approach by designing two modules: probabilistic stabilization network (for motion trajectory smoothing) and video outpainting network (for full-frame video rendering). For the former, we propose to build on the previous work of PDCNet [37, 38] and ex-tend it using a coarse-to-ﬁne strategy to improve robustness.
For a more robust estimate of the uncertainty of the op-tical ﬂow, we infer masks from the optical ﬂow by bidirec-tional propagation with a low computational cost (around 1/5 of the time Yu et al. [50]). Accordingly, we have devel-oped a two-level (coarse-to-ﬁne) ﬂow smoothing strategy that ﬁrst aligns adjacent frames by global afﬁne transfor-mation and then reﬁnes the result by warping the ﬁelds of intermediate frames. For the latter, we propose a two-stage approach (ﬂow and image outpainting) to render full-frame video. Our experimental results have shown highly compe-tent performance against others on three public benchmark datasets. The main contribution of this work is threefold:
• We propose a formulation of video stabilization as a
ﬁxed-point problem of the optical ﬂow ﬁeld and pro-pose a novel procedure to generate a model-based syn-thetic dataset.
• We construct a probabilistic stabilization network based on PDCNet and propose an effective coarse-to-ﬁne strategy for robust and efﬁcient smoothing of op-tical ﬂow ﬁelds.
• We propose a novel video outpainting network to ren-der stabilized full-frame video by exploiting the spatial coherence in the ﬂow ﬁeld. 2.