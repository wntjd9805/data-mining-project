Abstract
When handling complicated text images (e.g., irregular structures, low resolution, heavy occlusion, and uneven illu-mination), existing supervised text recognition methods are data-hungry. Although these methods employ large-scale synthetic text images to reduce the dependence on anno-tated real images, the domain gap still limits the recog-nition performance. Therefore, exploring the robust text feature representations on unlabeled real images by self-supervised learning is a good solution. However, existing self-supervised text recognition methods conduct sequence-to-sequence representation learning by roughly splitting the visual features along the horizontal axis, which lim-its the flexibility of the augmentations, as large geometric-based augmentations may lead to sequence-to-sequence feature inconsistency. Motivated by this, we propose a novel self-supervised Character-to-Character Distillation method, CCD, which enables versatile augmentations to fa-cilitate general text representation learning. Specifically, we delineate the character structures of unlabeled real im-ages by designing a self-supervised character segmentation module. Following this, CCD easily enriches the diver-sity of local characters while keeping their pairwise align-ment under flexible augmentations, using the transforma-tion matrix between two augmented views from images. Ex-periments demonstrate that CCD achieves state-of-the-art results, with average performance gains of 1.38% in text recognition, 1.7% in text segmentation, 0.24 dB (PSNR) and 0.0321 (SSIM) in text super-resolution. Code is available at https://github.com/TongkunGuan/CCD. 1.

Introduction
Recognizing text from images is a fundamental task in computer vision with applications in various scenarios, such as recognizing latex formulas [41], work-piece serial numbers [18], text logos [38], etc., and contributes sig-nificantly to the multi-modal analysis [55] and text under-standing [62, 46]. However, existing text recognition meth-Figure 1. Conceptual illustration of two different self-supervised paradigms. (a) is a sequence-level method that takes feature blocks horizontally-split from a sequence as the basic items for represen-tation learning. (b) is our character-level method that incorporates a self-supervised segmentation head to generate individual charac-ter structures (Sreg and Sirr), and utilizes the resulting character-level features as the basic items for representation learning. ods [48, 16, 51, 66, 50, 30, 24] are data-hungry, i.e., they require sufficient data with at least text transcriptions for producing accurate character predictions through implicit attention learning [19]. Even more, some supervised at-tention methods [24, 48, 32] require extra character-level bounding boxes. These annotations on text images are ex-pensive and laborious. Although they employ text synthe-sis techniques [20, 26] to substitute for labour-intensive text annotation tasks, the domain gap between real and synthetic images still limits the performance of text recognition [61].
Therefore, exploring the potential of unlabeled real text im-ages is of great importance as they are readily available.
Recently, self-supervised learning methods [1, 61, 34] have attracted considerable attention, which attempt to leverage the intrinsic qualities of unlabeled real text images to learn proper visual representations, followed by fine-tuning on text-related downstream tasks with less annotated data. Specifically, SeqCLR [1] adopts the SimCLR [11] framework to ensure sequence-to-sequence consistency be-tween the two augmented views, in which the sequence is composed of several non-overlapping feature blocks hori-zontally splitting from the visual features of the text im-age. DiG [61] employs both a sequence-level contrastive learning task [1] and a masked image modeling task [22] to learn feature representations. These methods formulate the learning via sequence-level pretext tasks illustrated in
Fig.1 (a). We argue that roughly splitting visual features of text images into a feature sequence along the horizontal axis has two weaknesses: 1) Inflexible data augmentation strategy, as large geometric transformations may cause in-consistency among the corresponding items in the feature sequence generated from different views. However, versa-tile data augmentations are demanded for self-supervised learning in many previous works [11, 2, 25]; 2) Neglect-ing character structures, which confuses networks to cause inter-character mixture, further downgrades the perception of semantic clue information in character-centric text im-ages. Thus a suitable self-supervised learning paradigm that is tailored for text images with a diversity of word length is in demand.
To address this issue, we propose a new self-supervised learning paradigm in character-level, named Character-to-Character Distillation (CCD), as shown in Fig.2, which en-ables feature representation consistency across various aug-mentations by organizing text images into entities, i.e., each character and background regions. Specifically, two views are first generated from each input image: a regular view with color jitter and an irregular view with additional geo-metric transformations. Each view is fed into the encoder of the student-teacher branches for extracting features that rep-resent the whole view. Then, character regions from regular view are delineated by a joint self-supervised text segmen-tation and density-based spatial clustering task, and those from irregular view are generated using the known trans-formation matrix between two views.
In this way, CCD naturally ensures the consistency of corresponding charac-ters across views and branches. Consequently, by enjoying the pairwise diversity of local characters under flexible aug-mentations, CCD effectively enhances the robustness and generalization of the learned features, making it more suit-In summary, the able for text-related downstream tasks. main contributions are as follows:
• We propose a novel self-supervised method cus-tomized for character-centric text images with a di-versity of word length, termed CCD. Different from prior works with sequence-to-sequence pretext tasks,
CCD delineates the character structures to establish character-to-character feature representation consis-tency, enjoying significant augmentation flexibility for extracting general text feature representations. its prominent superiority in self-supervised representation learning, and consistently the state-of-the-art and significantly outperforms
DiG [61] by an average of 1.38% in text recogni-tion, 1.7% in text segmentation, 0.24 dB (PSNR) and
• CCD shows 0.0321 (SSIM) in text image super-resolution task, with the same parameters and latency. 2.