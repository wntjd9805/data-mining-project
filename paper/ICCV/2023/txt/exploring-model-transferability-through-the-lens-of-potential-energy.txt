Abstract
Transfer learning has become crucial in computer vision tasks due to the vast availability of pre-trained deep learn-ing models. However, selecting the optimal pre-trained model from a diverse pool for a specific downstream task remains a challenge. Existing methods for measuring the transferability of pre-trained models rely on statistical cor-relations between encoded static features and task labels, but they overlook the impact of underlying representa-tion dynamics during fine-tuning, leading to unreliable re-sults, especially for self-supervised models. In this paper, we present an insightful physics-inspired approach named
PED to address these challenges. We reframe the chal-lenge of model selection through the lens of potential en-ergy and directly model the interaction forces that influ-ence fine-tuning dynamics. By capturing the motion of dy-namic representations to decline the potential energy within a force-driven physical model, we can acquire an enhanced and more stable observation for estimating transferability.
The experimental results on 10 downstream tasks and 12 self-supervised models demonstrate that our approach can seamlessly integrate into existing ranking techniques and enhance their performances, revealing its effectiveness for the model selection task and its potential for understanding the mechanism in transfer learning. Code is available at https://github.com/lixiaotong97/PED. 1.

Introduction
Transfer learning has achieved remarkable success in computer vision by fine-tuning models pre-trained on large-scale datasets (e.g., ImageNet [16]) for downstream tasks.
However, the proliferation of various network designs and training strategies presents a challenge in selecting an opti-mal model from the extensive range of options for a par-ticular downstream task. While fine-tuning each poten-*Corresponding Author.
Figure 1. We analogy the physical concept and consider the trans-fer learning dynamics in the perspective of potential energy. The objective to push apart different classes can be viewed as an in-teraction “force” to decline the system “potential energy” and the dynamics can be seen as a process from unstable to stable point of the energy plane. tial model in a brute-force manner is a direct approach for model selection, it is computationally infeasible due to the growing number of model candidates.
To address this challenge, prior studies [31, 46, 40, 36] have endeavored to efficiently measure the transferability of pre-trained models related to the separability of encoded representations. The principle underlying these approaches is to select a pre-trained model that can effectively segregate its initial features using the ground-truth labels (i.e., image classes) in the downstream task.
While the aforementioned methodology is effective for ranking supervised pre-trained models, which are originally optimized toward class separability, it is not always reli-able for ranking un/self-supervised pre-trained models [20].
These models have emerged as dominant in transfer learn-ing and have exhibited superior performance compared to supervised learning models. Nevertheless, self-supervised models exhibit different properties due to the discrepancy between pre-training target and downstream classification objective [20]. We argue that the limitations of the exist-ing separability-based methodology stem from its inability to consider the underlying representation dynamics during
the fine-tuning process of transfer learning and encounter challenges for ranking self-supervised models.
Modeling the representation dynamics for model ranking is a crucial yet challenging task. The present study focuses on image classification tasks without loss of generality. To understand the nature of model evolution in transfer learn-ing, we examine the process of backward-propagating gra-dients measured by classification cross-entropy loss. The process aims to cluster features out of the same classes, which can be viewed as creating a force that separates the clusters and system potential energy gets decreased driven by the force from a physical perspective [18, 48]. Refram-ing model evolution through the lens of potential energy reveals that the pre-trained model attains a state of equi-librium after pre-training, with low interaction forces and stable sample relationships. However, this stable state is disrupted when the model is transferred to a downstream task, leading to changes in the potential energy plane. In-tuitively, predicting model transferability based on an un-stable observation will hinder its predicting performance.
Drawing from the principles of physics [10], the present unstable state is inclined to move towards a reduction in potential energy and results in a more stable state. To prop-erly predict a model’s transferability, it is essential to model the force that determines the system’s tendency.
We therefore formulate the representation dynamics in terms of potential energy1 and propose the approach to tackle these challenges named Potential Energy Decline (PED), as demonstrated in Fig.1. To quantify the inter-action force acting on each class cluster and measure its corresponding movement on the potential energy landscape implicitly defined by the optimization objective, we con-sider each class’s representations in the downstream task as a ball in the latent space, with the class center indicat-ing the coordinate and the variation representing the radius.
The interaction force between different classes is formu-lated by the overlap radius of the two balls. We can simulate the positions of dynamic representations without backward-propagation by unfreezing the system and observing the moving tendency that leads to a new state with lower po-tential energy. Our force-directed dynamic representations provide a better observation and can be readily integrated into existing ranking algorithms, such as LogME [46], to achieve better model transferability measurement.
To the best of our knowledge, we are the first to explore model transferabilty through the lens of potential energy and simulate the underlying representation dynamics during transfer learning in a physics-driven approach. To evaluate our proposed method, we conduct extensive experiments on a variety of self-supervised pre-trained models. Our method can be easily integrated into existing approaches with neg-1Throughout this paper, the “energy” is a quantitative property held by features because of relative class positions in its latent space. ligible time consumption. The experimental results on 10 downstream tasks and 12 self-supervised pre-trained mod-els demonstrate our method can boost various metrics for more accurate prediction. Our findings might have impli-cations beyond the realm of image classification, as our ap-proach is generic and can be extended to more pre-trained models and other downstream tasks. We hope that our work will inspire future studies and have a broader impact in the field of transfer learning. 2.