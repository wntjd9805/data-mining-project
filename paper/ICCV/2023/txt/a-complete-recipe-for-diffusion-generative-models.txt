Abstract
Score-based Generative Models (SGMs) have demon-strated exceptional synthesis outcomes across various tasks.
However, the current design landscape of the forward dif-fusion process remains largely untapped and often relies on physical heuristics or simplifying assumptions. Utilizing insights from the development of scalable Bayesian poste-rior samplers, we present a complete recipe for formulating forward processes in SGMs, ensuring convergence to the desired target distribution. Our approach reveals that sev-eral existing SGMs can be seen as specific manifestations of our framework. Building upon this method, we introduce
Phase Space Langevin Diffusion (PSLD), which relies on score-based modeling within an augmented space enriched by auxiliary variables akin to physical phase space. Empiri-cal results exhibit the superior sample quality and improved speed-quality trade-off of PSLD compared to various compet-ing approaches on established image synthesis benchmarks.
Remarkably, PSLD achieves sample quality akin to state-of-the-art SGMs (FID: 2.10 for unconditional CIFAR-10 gener-ation). Lastly, we demonstrate the applicability of PSLD in conditional synthesis using pre-trained score networks, offer-ing an appealing alternative as an SGM backbone for future advancements. Code and model checkpoints can be accessed at https://github.com/mandt-lab/PSLD. 1.

Introduction
Score-based Generative Models [15, 50, 53, 56] are a class of explicit-likelihood based generative models that have recently demonstrated impressive performance on various synthesis benchmarks, such as image generation
[7, 16, 39, 42, 43], video synthesis [17, 62] and 3D shape generation [32, 68]. SGMs employ a forward stochastic process to add noise to data incrementally, transforming the data-generating distribution to a tractable prior distribution that enables sampling. Subsequently, a learnable reverse process transforms the prior distribution back to the data distribution using a parametric estimator of the gradient field
Figure 1: Unconditional PSLD generated samples. AFHQv2 128 x 128 (Top), CelebA-64 (Bottom Left, FID=2.01) and
CIFAR-10 (Bottom Right, FID=2.10) of the log probability density of the data (a.k.a score).
However, a principled framework for extending the cur-rent design space of diffusion processes is still missing. Al-though some studies have proposed augmenting the forward diffusion process with auxiliary variables [9] to improve sample quality, their design is primarily motivated by physi-cal intuition and non-obvious how to generalize. Therefore, a principled framework is required to explore the space of possible diffusion processes better.
In this work, we propose a complete recipe for the design of diffusion processes, motivated by the design of stochas-tic gradient MCMC samplers [5, 33, 61]. Our recipe leads to a flexible parameterization of the forward diffusion pro-cess without requiring physical intuition. Moreover, under the proposed parameterization, the forward process is guar-anteed to converge to a prior distribution of interest. We show that several existing SGMs can be cast under our dif-fusion process parameterization. Furthermore, using our proposed recipe, we introduce PSLD, a novel SGM which performs diffusion in the joint space of data and auxiliary variables. We demonstrate that PSLD generalizes Critically
Damped Langevin Diffusion (CLD) [9] and outperforms existing baselines on several empirical settings on standard image synthesis benchmarks such as CIFAR-10 [25] and
CelebA-64 [30]. More specifically, we make the following
theoretical and empirical contributions: 1. A Complete Recipe for SGM Design: We propose a specific parameterization of the forward process, guar-anteed to converge asymptotically to a desired station-ary “prior” distribution. The proposed recipe is com-plete in the sense that it subsumes all possible Marko-vian stochastic processes which converge to this distri-bution. We show that several existing SGMs [9, 56] can be cast as specific instantiations of our recipe. 2. Phase Space Langevin Diffusion(PSLD): To exem-plify the proposed diffusion parameterization con-cretely, we propose PSLD: a novel SGM which per-forms diffusion in the phase space by adding noise in both data and the momentum space. 3. Superior Sample Quality and Speed-Quality Trade-offs: Using ablation experiments on standard image synthesis benchmarks like CIFAR-10 and CelebA-64, we demonstrate the benefits of adding stochastic noise in both the data and the momentum space on overall sample quality and the speed quality trade-offs associ-ated with PSLD. Furthermore, using similar score net-work architectures, our proposed method outperforms existing diffusion baselines on both criteria across dif-ferent sampler settings. 4. State-of-the-Art Sample Quality: We show that PSLD outperforms competing baselines and achieves compet-itive perceptual sample quality to other state-of-the-art methods. Our model achieves an FID [13] score of 2.10, an IS score [45] of 9.93 on unconditional CIFAR-10 and an FID score of 2.01 on CelebA-64. 5. Conditional synthesis: We show that pre-trained un-conditional PSLD models can be used for conditional synthesis tasks like class-conditional generation and image inpainting during inference.
Overall, given the superior performance of PSLD on several tasks, we present an attractive alternative to existing SGM backbones for further development. We organize the rest of our work as follows: Section 2 presents some background on
SGMs and our proposed recipe for SGM design. Section 3 presents the construction of our novel PSLD model. Section 4 presents our empirical findings. Lastly, Section 5 compares our proposed contributions to several existing works while we present some directions for future work in Section 6. 2. A Complete Recipe for SGM Design 2.1.