Abstract 1.

Introduction
We present Semantify: a self-supervised method that uti-lizes the semantic power of CLIP language-vision founda-tion model [32] to simplify the control of 3D morphable models. Given a parametric model, training data is cre-ated by randomly sampling the model’s parameters, creat-ing various shapes and rendering them. The similarity be-tween the output images and a set of word descriptors is calculated in CLIP’s latent space. Our key idea is first to choose a small set of semantically meaningful and disen-tangled descriptors that characterize the 3DMM, and then learn a non-linear mapping from scores across this set to the parametric coefficients of the given 3DMM. The non-linear mapping is defined by training a neural network with-out a human-in-the-loop. We present results on numerous 3DMMs: body shape models, face shape and expression models, as well as animal shapes. We demonstrate how our method defines a simple slider interface for intuitive model-ing, and show how the mapping can be used to instantly fit a 3D parametric body shape to in-the-wild images. See our project page at https://omergral.github.io/Semantify/ 3D modeling techniques have evolved tremendously over the last few years. Such techniques are used in count-less industries including the burgeoning AR/VR industry, fashion design, game development, film, and many others.
However, designing a high-quality 3D model is not a sim-ple task for most people and might require a well-trained 3D artist. Even when using more recent parametric mor-phable models (3DMM) [24, 31, 45, 3, 42] it is still hard for humans to understand how to choose the correct set of parameters, e.g. to achieve a specific human body shape or
It is also difficult to find what are the facial expression. limits of the given parametric model in terms of coverage and expressiveness. The reason is that in most cases, the provided set of parameters is not interpretable, as they are commonly calculated using automatic optimization mech-anisms followed by dimensionality reduction using PCA.
Thus, they carry no clear semantic meaning.
To simplify the use of 3DMMs and allow for natural interactive human modeling, a key research question is – how to insert semantics into 3DMM control? Previous ap-proaches [34] relied mostly on human intelligence and la-beling, which is often time consuming and expensive. In addition, previous methods create a large number of seman-tic control descriptors that are often correlated and entan-gled. This means it is difficult to anticipate the effect of each descriptor as changing a value in one may modify oth-ers, resulting in difficulties to control the model.
In this paper, instead of using human labeling, we rely on the remarkable abilities of huge foundation models that combine natural language and visual understanding. We present a self-supervised method that utilizes the semantic power of CLIP [32] to define a method to control 3DMMs that carries two main advantages. First, it is more natural for humans to use as it allows modeling using a small num-ber of semantically meaningful descriptors, that cover the space of deformations but are disentangled. Second, it cov-ers even extreme examples in the shape/pose space of para-metric models as it utilizes CLIP’s pre-training on a large number of images. The main idea is first to use CLIP to select a small subset of semantically meaningful and disen-tangled descriptors, and then learn a non-linear mapping of this set to the coefficients of the given 3DMM.
Given a parametric 3DMM, we sample its parameter space to create a dataset containing a variety of 3D mesh shapes. We then render each mesh from different camera views to create a diverse set of images corresponding to the parameter samples (see Figure 2). Next, we gather a set of semantically descriptive textual terms related to the parametric 3D model, which we call descriptors. We en-code both the images (using CLIP’s image encoder) and the descriptors (using CLIP’s text encoder) into CLIP’s latent space and compare them. This defines a vector of similar-ity scores between the input vector of 3DMM coefficients (of each image sample) and each corresponding semantic descriptor. Next, we define a selection scheme to choose a small number of descriptors that are de-correlated to control the model. Lastly, we train a neural network to learn a map-ping between the vector of similarity scores to the vector of 3DMM parameters.
We demonstrate how the learned mapping can be used to define an interface to control a 3DMM in a way that is simple and effective using a small set of semantically mean-ingful sliders (see Figure 1 and 4). Such sliders are easy to employ for designing high-quality 3D models and cover the shape space well. We demonstrate this for four parametric models: human face’s shape and expression (FLAME [24]), human body shapes (SMPL [25] and SMPL-X [31]), and even animals (SMAL [45]). We also show how the map-ping can be used to instantly fit a 3D parametric body shape to an input image that works well “in the wild” even in ex-treme poses and body shapes.
Our main contribution is a novel, self-supervised method for defining a small set of semantic descriptors to control a parametric model more naturally, by learning a mapping from a semantic space to a parametric representation with-out human-in-the-loop. We demonstrate the effectiveness of our approach on several parametric models and utilize it to define a simple interface for modeling and to instantly fit a 3D model to images in the wild. We will release our code for future research. 2.