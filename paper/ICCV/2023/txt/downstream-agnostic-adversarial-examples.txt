Abstract
Self-supervised learning usually uses a large amount of unlabeled data to pre-train an encoder which can be used as a general-purpose feature extractor, such that downstream users only need to perform fine-tuning operations to en-joy the benefit of “large model”. Despite this promising prospect, the security of pre-trained encoder has not been thoroughly investigated yet, especially when the pre-trained encoder is publicly available for commercial use.
In this paper, we propose AdvEncoder, the first frame-work for generating downstream-agnostic universal adver-sarial examples based on the pre-trained encoder. AdvEn-coder aims to construct a universal adversarial perturba-tion or patch for a set of natural images that can fool all the downstream tasks inheriting the victim pre-trained en-coder. Unlike traditional adversarial example works, the pre-trained encoder only outputs feature vectors rather than classification labels. Therefore, we first exploit the high fre-quency component information of the image to guide the generation of adversarial examples. Then we design a gen-erative attack framework to construct adversarial pertur-bations/patches by learning the distribution of the attack surrogate dataset to improve their attack success rates and transferability. Our results show that an attacker can suc-cessfully attack downstream tasks without knowing either the pre-training dataset or the downstream dataset. We also tailor four defenses for pre-trained encoders, the re-1National Engineering Research Center for Big Data Technology and
System 2Services Computing Technology and System Lab 3Hubei Key
Laboratory of Distributed System Security 4Hubei Engineering Research
Center on Big Data Security 5Cluster and Grid Computing Lab
Figure 1: An overview of adversarial examples against dif-ferent downstream tasks based on a pre-trained encoder sults of which further prove the attack ability of AdvEn-coder. Our codes are available at: https://github. com/CGCL-codes/AdvEncoder. 1.

Introduction
Self-supervised learning [8, 10] (SSL) is an emerging machine learning paradigm that seeks to overcome the re-strictions of labeled data.
It usually uses a large volume of unlabeled data to pre-train a general-purpose encoder, which can be used as a feature extractor for various down-stream tasks like image classification, image retrieval, ob-ject detection, etc. As a result, any resource-constrained user can enjoy the advantages of “large model” without per-forming the expensive training from scratch, where only light-weight fine-tuning operations are needed at its request.
Driven by this promising prospect, pre-training encoders become popular in industry and many service providers
publicly release their pre-trained encoders (e.g., SimCLR by Google [8, 9], MoCo by Meta [12, 26]) or deploy them as a commercial service (e.g., OpenAI [52], Clarifai [13]).
Meanwhile, it is well known that deep neural networks (DNNs) are vulnerable to various adversarial attacks [23, 44, 61, 65], which will make pre-trained encoder fragile as well. However, the security of pre-trained encoder has re-ceived much less consideration in the literature. Although some recent works studied security threats on pre-trained encoders including backdoor attack [31, 32], poisoning at-tack [40], and privacy risks [15, 41], none of them paid at-tention to adversarial examples, another kind of prevalent and destructive attack on DNNs. Constructing adversar-ial examples against pre-trained encoders is quite different from its traditional attack route due to the fact that the at-tacker has no knowledge of the downstream tasks. In other words, the attacker needs to attack a DNN without knowing its task type, the pre-training dataset, and the downstream dataset, even when the whole model will get fine-tuned. To the best of our knowledge, how to realize adversarial ex-ample attack in the practical scenario of pre-training still remains challenging and unresolved.
In this work, we take a big step towards bridging the gap between adversarial examples and pre-trained encoders. We consider both adversarial perturbation [5, 23, 37, 45] and patch [3, 30, 39, 61]. The former one has a high imper-ceptibility, while the latter one is visible but confined to a small area of the image and more readily applicable in the physical world. Furthermore, without the knowledge of downstream data, we aim to realize universal adversar-ial attacks [25, 44, 63] where one adversarial perturbation or patch applies to a set of natural images and can cause model misclassification.
Specifically, we propose AdvEncoder, a novel attack framework for generating downstream-agnostic universal adversarial examples. The most challenging job lies in ad-dressing the limitations and lacking supervised signals and the information about the downstream tasks.
Inspired by the fact that deep neural networks are biased towards tex-ture features of images [34, 60], the change of texture infor-mation, i.e., the high frequency components (HFC) of the image, is very likely to cause the model decision change.
We first exploit a high frequency component filter to get the HFC of benign and adversarial samples, and pull away their Euclidean distance as much as possible to influence the model’s decision. We then design a generative attack framework to construct adversarial perturbations or patches with high attack success rates and transferability by learn-ing the distribution of the data, with a fixed random noise as input. Our main contributions are summarized as follows:
• We propose AdvEncoder, the first attack framework to construct downstream-agnostic universal adversar-ial examples in self-supervised learning. We reveal that the pre-trained encoder incurs severe security risks for the downstream tasks.
• We design a frequency-based generative network to generate universal adversarial examples by directly alearting the texture features of the image itself. It is a flexible framework that can generate both adversarial perturbations and patches.
• Our extensive experiments on fourteen self-supervised training methods and four image datasets show that our AdvEncoder achieves high attack success rates and transferability against different downstream tasks.
• We tailor four popular defenses to mitigate AdvEn-coder. The results further prove the attack ability of
AdvEncoder and highlight the needs of new defense mechanism to defend pre-trained encoders. 2.