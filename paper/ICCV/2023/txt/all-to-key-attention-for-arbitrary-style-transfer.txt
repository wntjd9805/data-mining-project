Abstract
Attention-based arbitrary style transfer studies have shown promising performance in synthesizing vivid lo-cal style details. They typically use the all-to-all atten-tion mechanism—each position of content features is fully matched to all positions of style features. However, all-to-all attention tends to generate distorted style patterns and has quadratic complexity, limiting the effectiveness and ef-ficiency of arbitrary style transfer. In this paper, we pro-pose a novel all-to-key attention mechanism—each posi-tion of content features is matched to stable key positions of style features—that is more in line with the characteris-tics of style transfer. Specifically, it integrates two newly proposed attention forms: distributed and progressive at-tention. Distributed attention assigns attention to key style representations that depict the style distribution of local regions; Progressive attention pays attention from coarse-grained regions to fine-grained key positions. The resultant module, dubbed StyA2K, shows extraordinary performance in preserving the semantic structure and rendering consis-tent style patterns. Qualitative and quantitative compar-isons with state-of-the-art methods demonstrate the supe-rior performance of our approach. Codes and models are available on https://github.com/LearningHx/StyA2K. 1.

Introduction
Arbitrary style transfer (AST) is an important computer vision task. It aims to render a natural image (i.e., content image) with the artistic style of an arbitrary painting (i.e.,
*Both authors contributed equally to this work.
†Corresponding author: Nannan Wang.
Figure 1. Image/Video style transfer results of AdaAttN [27] (sec-ond column) and our StyA2K (third column). The videos in the first row can be found in the supplementary material. style image), enabling the generated image to imitate any artistic style. There have been notable improvements in fea-ture transformation modules [13, 33, 12, 30, 49, 7, 27, 17, 50], novel architectures [24, 32, 1, 42, 6], and practical ob-jectives [19, 5, 51]. The core of AST is the matching of con-tent features and style features in the feed-forward proce-dure. Holistic feature distribution matching [13, 24, 17, 50] and locality-aware feature matching [33, 12, 30, 49, 27] are two categories of existing approaches.
The attention-based method is the research focus of the locality-aware feature matching category for its capability to capture long-range dependencies. Typically, it estab-lishes a dense correspondence between point-wise tokens of the content and style features via an all-to-all attention mechanism [52]. The transferred feature of each local posi-tion is computed as the weighted sum of the local style fea-tures of all positions, where the weights are computed by applying a sof tmax function to the normalized dot prod-ucts’ results. Despite its encouraging results, the attention-based AST method suffers from two main predicaments.
The first one is the introduction of distorted style patterns and unstable matching effects. As shown in Fig. 1, the im-age stylization result of AdaAttN is seriously affected by eye patterns, which significantly affects visual perception.
Besides, the video stylization result of AdaAttN has an ap-parent flickering phenomenon, which also affects the visual quality. The second one is the high computational complex-ity. Handling image features with high spatial resolution and multiple layers with all-to-all attention requires signifi-cant computational consumption.
We argue that the inherent limitations of the all-to-all at-tention mechanism cause the above problems. (1) All-to-all attention has no error tolerance and is sensitive to posi-tion variation. It tends to concentrate on the most similar value since sof tmax shows strong exclusiveness in atten-tion score due to exponential computation. A distorted style pattern appears when the most similar key is semantically distinct from the query. For example, the reason for the ap-pearance of irrational eye patterns in AdaAttN’s results is that the all-to-all attention almost exclusively concentrates on the eye patterns in the style image for the positions with edge-like patterns in the content image. Its sensitivity is em-bodied in that slight changes in query at different positions will lead to complete semantic changes in matched keys.
For instance, the object motion and the light change in the video cause severe flickering phenomena between consecu-tive stylized frames that are frame-by-frame independently generated by AdaAttN. (2) All-to-all attention has quadratic computational complexity since it establishes a fully con-nected correspondence between queries and keys. Its com-putational complexity scales quadratically to image size.
How to maintain the advantage of the attention mech-anism (enhancing local semantics) and mitigate the disad-vantages of all-to-all attention (no error tolerance, unstable, time-consuming)? Our solution is a novel all-to-key atten-tion (A2K) mechanism that matches each query with stable (1) It learns
“key” keys. A2k comprises two inventions. distributed keys that depict the style distribution of all local regions of the style features; thus, each query of the con-tent features is matched with these stable and representa-tive keys. (2) It gradually concentrates its attention from coarse-grained regions to fine-grained keys; thus, queries within a local region are matched to the same stable keys within a local region. The former, dubbed as distributed at-tention (DA), improves matching error tolerance since the most similar key matched is a regional style representation rather than an isolated position. The latter, dubbed as pro-gressive attention (PA), mitigates the problem of “cannot
Figure 2. Illustration of all-to-key attention (A2K). A2K matches stable “key” keys for each query, integrating two novel attention forms: distributed attention and progressive attention.
Figure 3. Comparison between all-to-all attention and our pro-posed all-to-key attention (A2K). All-to-all attention matches all keys for each query, whereas A2K comprises distributed attention (DA) and progressive attention (PA). DA matches the learned dis-tributed keys depicting the style distribution of all local regions.
PA gradually concentrates the attention from coarse-grained re-gions of the style features to fine-grained keys. see the woods for the trees” existing in all-to-all attention and ensures semantic correctness from a more macro per-spective. In addition, the keys of DA and PA are stable, so
A2K can reduce the sensitivity of position variation and ren-der consistent style patterns. Furthermore, we implement
DA and PA in a blocked, sparse fashion, saving consider-able computational costs. Finally, an effective and efficient arbitrary style transfer model based on all-to-key attention (StyA2K) arrives. Fig. 2 and Fig. 3 illustrate the A2K con-cept and its distinction from all-to-all attention. We sum-marize the main contributions of this paper as the following points:
• We point out the inherent limitations of all-to-all atten-tion and present a novel all-to-key attention mechanism for effective and efficient arbitrary style transfer.
• We propose distributed attention to improve matching er-ror tolerance and progressive attention to ensure seman-tic correctness, both of which enjoy the stable matching property and save significant computational consumption.
• We conduct extensive experiments to demonstrate the su-periority of our approach over state-of-the-art methods in preserving semantic structures and rendering consistent style patterns. 2.