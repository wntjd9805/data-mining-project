Abstract
Out-of-distribution (OOD) detection aims to identify test examples that do not belong to the training distribution and are thus unlikely to be predicted reliably. Despite a plethora of existing works, most of them focused only on the scenario where OOD examples come from semantic shift (e.g., unseen categories), ignoring other possible causes (e.g., covariate shift). In this paper, we present a novel, unifying framework to study OOD detection in a broader scope. Instead of de-tecting OOD examples from a particular cause, we propose to detect examples that a deployed machine learning model (e.g., an image classifier) is unable to predict correctly. That is, whether a test example should be detected and rejected or not is “model-specific”. We show that this framework unifies the detection of OOD examples caused by semantic shift and covariate shift, and closely addresses the concern of applying a machine learning model to uncontrolled envi-ronments. We provide an extensive analysis that involves a variety of models (e.g., different architectures and training strategies), sources of OOD examples, and OOD detection approaches, and reveal several insights into improving and understanding OOD detection in uncontrolled environments. 1.

Introduction
Equipping a model with the capability to identify “what it does not know” is critical for reliable machine learning. Take image classification as an example. While state-of-the-art neural network models [11, 6, 17, 24] could perform fairly well on “in-distribution (ID)” data that belong to the training distribution, their accuracy often degrades drastically when facing data with covariate shift (e.g., different image domains or styles) [46] or semantic shift (e.g., novel categories) [22].
It is thus crucial to detect data on which the models cannot perform well and reject them from being classified.
Out-of-distribution (OOD) detection [13], which aims to identify test examples drawn from a distribution different from the training distribution, is a promising paradigm to-ward such a goal. OOD detection has attracted significant attention lately, with a plethora of methods being developed
Figure 1: Model-Specific Out-of-Distribution (MS-OOD) De-tection, using ImageNet [34, 4] as an example. The blue, purple, and yellow regions denote in-distribution (ID), covariate shift (C-OOD), and semantic shift (S-OOD) data, respectively, each with its datasets and representative images. Given an ImageNet classifier, the shaded red region denotes the correctly classified images (called the acceptance region). A robust classifier would have its accep-tance region cover the ID and C-OOD data as much as possible.
Using this framework, we can separate test data into model-specific acceptance (MS-A) and rejection (MS-R) cases, corresponding to the shaded red region and its complement. The goal of MS-OOD
DETECTION is to detect the MS-R cases (i.e., examples misclassi-fied by the classifier).
[50, 35]. However, most of them focus solely on detecting examples with semantic shift, which arguably limits their applicability in uncontrolled environments where other kinds of OOD examples (e.g., with covariate shift) may appear.
In this paper, we thus attempt to expand the scope of OOD detection to further include covariate shift. At first glance, one may treat OOD examples with covariate shift (denoted as C-OOD) the same as OOD examples with semantic shift (denoted as S-OOD) — i.e., to detect and reject as many of them as possible. However, unlike S-OOD examples, which the deployed classifier can never classify correctly, C-OOD examples have a chance to be correctly classified, as their label space is covered by the classifier. For instance, if the co-variate shift is small (e.g., ImageNet [34] as ID; ImageNetV2 as C-OOD [32]), many of the C-OOD examples will likely be correctly classified. Even if the covariate shift is more pronounced (e.g., ImageNet-Sketch [42]), a robust classifier (e.g., with the CLIP-pre-trained backbone [30]) is likely to
still classify a decent amount of C-OOD examples correctly.
Blindly rejecting these correctly classified C-OOD examples would adversely reduce the efficacy of the classifier.
Taking this insight into account, we propose a novel, uni-fying framework — MS-OOD DETECTION — to study
OOD detection from a “Model-Specific” perspective. In MS-OOD DETECTION, whether an example should be detected and rejected from being classified (denoted by a ground-truth label −1) depends on whether the deployed classifier would misclassify it. With this definition, every test example can be deterministically assigned a ground-truth label based on the deployed classifier: +1 for correctly classified examples, which should not be rejected; −1 for misclassified examples, which should be rejected. This enables us to study different causes of OOD examples in a unifying way. It is worth not-ing that while C-OOD examples could be assigned different ground-truth labels, all the S-OOD examples are assigned ground-truth labels −1. In other words, similar to conven-tional OOD detection, all the S-OOD examples should be detected and rejected in MS-OOD DETECTION.
Nevertheless, unlike conventional OOD detection, which aims to accept all the ID examples drawn from the training distribution, MS-OOD DETECTION, according to how it assigns ground-truth labels, aims to detect and reject mis-classified ID examples as well. That is, what MS-OOD
DETECTION aims to accept are correctly classified ID and
C-OOD examples; what MS-OOD DETECTION aims to reject are misclassified ID and C-OOD examples, and all the
S-OOD examples (which are always misclassified). Such a definition seamlessly unifies and generalizes the two related problems studied in the seminal work by Hendrycks and
Gimpel [13] — detecting misclassified and OOD examples
— and could better reflect real-world application scenarios.
For instance, for end-users who seek to reliably apply the machine learning model in uncontrolled environments, mis-classification reveals the limitation of the model or the in-herent difficulty of the examples (e.g., hard or ambiguous examples), implying the need for end-user intervention.
We conduct an extensive empirical study of MS-OOD
DETECTION. We consider three dimensions: 1) sources of
OOD examples, which include both semantic and covariate shift; 2) deployed classifiers, which include different neural network architectures and training strategies; 3) OOD de-tection methods, which include representative approaches such as Maximum Softmax Probabilities (MSP) [13], Energy
Score [23], Maximum Logit Score (MLS) [41], Virtual-logit
Matching (ViM) [43], and GradNorm [18]. New datasets, classifiers, and OOD methods can easily be incorporated to broaden the scope. This experimental framework not only offers a platform to unify the community but also provides a “manual” to end-users for selecting the appropriate OOD methods in their respective use cases.
Along with this study are 1) a list of novel insights into
OOD detection and 2) a unifying re-validation of several ex-isting but seemingly isolated insights found in different con-texts. For instance, we find that the best detection methods for S-OOD, misclassified C-OOD, and misclassified ID data are not consistent; their effectiveness could be influenced by the paired model, hence “model-specific”. Specifically for
C-OOD examples, we find that the more robust the classifier is (i.e., having a higher accuracy in classifying C-OOD ex-amples), the easier the misclassified C-OOD examples can be detected. For S-OOD examples, while in general, we see the same trend as in [41] — the stronger the classifier is (i.e., having a higher accuracy in classifying ID examples), the easier the S-OOD examples can be detected — there are exceptions when we apply particular detection methods and classifiers. This suggests the need for a more thorough study.
For misclassified ID examples, we find that they normally have lower scores (e.g., softmax probabilities) than correctly
In other words, one could set a classified ID examples. higher threshold to reject more S-OOD examples without sacrificing the true positive rate of accepting ID examples that are correctly classified. Last but not least, we find that the baseline OOD detection method MSP [13] performs fa-vorably in detecting misclassified ID and C-OOD examples, often outperforming other more advanced methods.
Contributions. Our contributions are two-folded:
• We propose a novel framework, MS-OOD DETECTION, which enables us to study different OOD examples (e.g., covariate shift and semantic shift) in a unifying way.
• We conduct an extensive study of MS-OOD DETECTION, which reveals novel insights into OOD detection and uni-fies existing insights gained from different contexts. 2.