Abstract
While large-scale image-text pretrained models such as CLIP have been used for multiple video-level tasks on trimmed videos, their use for temporal localization in untrimmed videos is still a relatively unexplored task. We design a new approach for this called UnLoc, which uses pretrained image and text towers, and feeds tokens to a video-text fusion model. The output of the fusion module are then used to construct a feature pyramid in which each level connects to a head to predict a per-frame relevancy score and start/end time displacements. Unlike previous works, our architecture enables Moment Retrieval, Tem-poral Localization, and Action Segmentation with a single stage model, without the need for action proposals, motion based pretrained features or representation masking. Un-like specialized models, we achieve state of the art results on all three different localization tasks with a unified ap-proach. Code is available at: https://github.com/ google-research/scenic. 1.

Introduction
Contrastive vision-language pretraining has been shown to learn powerful feature representations, and moreover en-ables open-set inference on a wide range of tasks [55, 27].
As a result, pretrained models such as CLIP [55] have been adapted to multiple diverse tasks including video classifica-tion [52, 37], object detection [46] and segmentation [18].
In this paper, we study how to adapt large-scale, con-trastively trained image-text models to untrimmed video understanding tasks that involve localization. While CLIP has been used widely for trimmed video tasks (classifica-tion [52, 37] or retrieval [4]), its use on long, untrimmed video is still in a nascent stage. Long videos come with mul-tiple challenges – CLIP is pretrained on images only, and localization in untrimmed videos requires exploiting fine-*Equal contribution.
†Work done while an intern at Google
Figure 1. Applying two-tower CLIP to video localization tasks:
We propose UnLoc, a single stage, unified model that achieves state of the art results on 3 different video localization tasks - mo-ment retrieval, temporal action localization and action segmenta-tion. UnLoc leverages a two-tower model (with a vision and text encoder) in conjunction with a video-text fusion module and fea-ture pyramid to perform mid-level feature fusion without the need for any temporal proposals. grained temporal structured information in videos. In par-ticular, it is challenging for image and language models to learn properties of temporal backgrounds (with respective to foreground actions) during training. In contrast, natural videos often come with a large, variable proportion of back-ground and detecting specific actions is critical for local-ization tasks [49]. Finally, localization in long untrimmed videos also typically involves detecting events at multiple temporal scales. Consequently, existing approaches that use CLIP typically focus on a two-stage approach involving off-the-shelf proposal generators [28], or use temporal fea-tures such as I3D [48] or C3D [60]. In contrast, we propose an end-to-end trainable one-stage approach starting from a
CLIP two tower model only.
We focus specifically on three different video localiza-tion tasks - Moment Retrieval (MR) [29, 16], Temporal Ac-tion Localization (TAL) [21, 26] and Action Segmentation (AS) [62]. These tasks have typically been studied sepa-rately, with different techniques proposed for each task. We show how we can use a single, unified approach, to address
all of these tasks, without using any external proposals. We do this by leveraging a two-tower model (with a vision and text encoder), in conjunction with a single video-text fu-sion module, which performs mid-level fusion of text and visual tokens (Figure 1). Our two tower model can natu-rally handle tasks such as moment retrieval which contain both video and text as input modalities, and can be used for open-set inference in other tasks such as temporal action lo-calization and action segmentation. While many works use the visual encoder only [58, 8, 20, 79], we believe that the language priors learnt with the pretrained text encoder can contain useful information and should be leveraged together with the image encoder early in the model design (particu-larly for open-set evaluation), and not right at the end for similarity computation. Inspired by existing object detec-tion works [31], we also use the output frame tokens from our fusion module to construct a feature pyramid, to enable understanding at multiple temporal scales.
Our approach achieves state-of-the-art results across all three video localization tasks - MR [29, 16], TAL [21, 26] and AS [62]. We also perform thorough ablation studies, studying the effect of modelling choices across a range of tasks. 2.