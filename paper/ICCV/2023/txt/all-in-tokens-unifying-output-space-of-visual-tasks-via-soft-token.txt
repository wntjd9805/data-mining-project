Abstract
We introduce AiT, a unified output representation for var-ious vision tasks, which is a crucial step towards general-purpose vision task solvers. Despite the challenges posed by the high-dimensional and task-specific outputs, we show-case the potential of using discrete representation (VQ-VAE) to model the dense outputs of many computer vi-sion tasks as a sequence of discrete tokens. This is in-spired by the established ability of VQ-VAE to conserve the structures spanning multiple pixels using few discrete codes. To that end, we present a modified shallower ar-chitecture for VQ-VAE that improves efficiency while keep-ing prediction accuracy. Our approach also incorporates uncertainty into the decoding process by using a soft fu-sion of the codebook entries, providing a more stable training process, which notably improved prediction accu-racy. Our evaluation of AiT on depth estimation and in-stance segmentation tasks, with both continuous and dis-crete labels, demonstrates its superiority compared to other unified models. The code and models are available at https://github.com/SwinTransformer/AiT. 1.

Introduction
A central goal of AI is to develop a unified model capable of handling many tasks. Recent developments in large-scale language models such as GPT-3 [5] have shown remarkable success as general-purpose solvers for language tasks. It in-spires to examine the feasibility of creating universal mod-els for various computer vision tasks.
Current research is approaching the goal from a diverse
*Equal contribution. range of perspectives. Perceiver [18] and Perceiver-IO [17] propose to use exactly the same Transformer architecture to handle different modalities such as natural language, com-puter vision and StarCraft II. However, it allocates a query for each output and ignores their dependency, making it un-able to model interdependent outputs such as the coordi-nates of a box. Some works attempt to address multiple visual tasks but they are still limited to only a few. For ex-ample, Flamingo [1] handles only tasks with language as output; CLIP [31] and its follow-ups [48, 49, 53] tackle only retrieval and image classification tasks; Chen et al. [8] deal with tasks that have describable and sequential outputs.
Pix2SeqV2 [8] tries to unify different vision tasks using to-kens. But the tokens need to be designed manually for each task. For example, they use polygon to represent the in-stance segmentation, which can not be applied to other tasks such as depth estimation.
In this paper, we aim to develop a comprehensive solu-tion to various vision tasks. To achieve this, we first iden-tify a key challenge in the field - while the NLP tasks typ-ically have similar inputs and outputs represented by lan-guage tokens, the outputs of vision tasks are highly diverse.
For example, object detection produces labels and coordi-nates, semantic segmentation generates discrete label maps and depth estimation results in value-rich images. We tackle this hindrance by unifying the output spaces of various vi-sual tasks through a general tokenizer which is implemented using VQ-VAE [36]. It transforms the task output into a set of tokens by the encoder, which are then reconstituted into the original output by the decoder. The task solver for each vision task is realized using an auto-regressive encoder-decoder model. The model takes in images as inputs and outputs a sequence of tokens in a causal manner, which is then converted back into the original task-specific output
using the decoder. we comprehensively assess the impact of various architectural designs in the VQ-VAE model. Our findings reveal that a shallower encoder/decoder architec-ture, with a maximum of 5 convolution layers, 2 residual blocks, and 128 codebook entries improves inference effi-ciency without losing prediction accuracy. As a result, the parameters and computations of VQVAE required remain minimal, amounting to only 2 million parameters and 0.06G
FLOPs.
To enhance its effectiveness, we propose several inno-vative techniques that specifically address the unique chal-lenges of visual tasks.
Firstly, we incorporate uncertainty into the decoding pro-cess by using a soft fusion of the codebook entries. We rep-resent a soft token by a probability vector where each value representing the probability of membership in the code-book. When a soft token is fed to the detokenizer or the next token prediction network, its input embedding is com-puted as the weighted average of the corresponding code-book embeddings. This demonstrates that the soft token embedding spans a continuous, interpolable space, which may more accurately reflect visual outputs, particularly in cases where they are continuous in nature. Additionally, the continuous nature of the soft token enables the implemen-tation of an auxiliary loss function, which learns the task output end-to-end.
Secondly, to handle visual tasks that have corrupted, un-defined, or invalid values in their annotations, we propose mask augmentation in training. For example, depth estima-tion is a typical task that faces this challenge, with occluded areas not being defined [35], as shown in Figure 2. These undefined regions can make it difficult for the tokenizer and detokenizer to be trained, as it is not clear what should be reconstructed in these areas. To overcome this issue, we randomly mask segments of the input depth map dur-ing VQ-VAE training. Unlike the undefined regions, these manually masked sections have known ground-truth anno-tations, which help train the VQ-VAE network to be able to recover that ground truth for the undefined regions. Our ex-periments demonstrate that this technique significantly im-proves the accuracy.
Thirdly, we propose a Parallel Vision Modeling method on dense vision tokens. Parallel Vision Modeling uses a fixed embedding as the input of the dense token prediction instead of the last predicted token. Obviously, Parallel Vi-sion Modeling can accelerate the auto-regressive prediction by predicting a bunch of visual tokens at a time, we also show they can improve the performance effectively. This method is similar to Perceiver-IO [17] and DETR [6], but unlike the Perceiver-IO which only uses the cross-attention and DETR which uses the bidirectional self-attention, Par-allel Vision Modeling uses the unidirectional attention with causal mask and only applied to a portion of tokens, which is more general and can be inserted into any auto-regressive model.
We mainly study our method on two classical vi-sual tasks with diverse outputs: depth estimation and in-stance segmentation, utilizing floating-point maps and bi-nary masks as output formats. These tasks differ in the size of their output, with depth estimation having a fixed size and instance segmentation having a variable size. Our ap-proach achieved competing results. In particular, it achieves the state-of-the-art results on the NYUv2 depth estimation benchmark [35]. The proposed framework and techniques are versatile, and we also show more results on other tasks in experiment. 2.