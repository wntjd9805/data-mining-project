Abstract
Most existing neural network pruning methods hand-crafted their importance criteria and structures to prune.
This constructs heavy and unintended dependencies on heuristics and expert experience for both the objective and the parameters of the pruning approach. In this paper, we try to solve this problem by introducing a principled and unified framework based on Information Bottleneck (IB) theory, which further guides us to an automatic pruning ap-proach. Specifically, we first formulate the channel pruning problem from an IB perspective, and then implement the IB principle by solving a Hilbert-Schmidt Independence Crite-rion (HSIC) Lasso problem under certain conditions. Based on the theoretical guidance, we then provide an automatic pruning scheme by searching for global penalty coefficients.
Verified by extensive experiments, our method yields state-of-the-art performance on various benchmark networks and datasets. For example, with VGG-16, we achieve a 60%-FLOPs reduction by removing 76% of the parameters, with an improvement of 0.40% in top-1 accuracy on CIFAR-10.
With ResNet-50, we achieve a 56%-FLOPs reduction by re-moving 50% of the parameters, with a small loss of 0.08% in the top-1 accuracy on ImageNet. The code is available at https://github.com/sunggo/APIB. 1.

Introduction
Convolutional Neural Networks (CNNs)
[27] have gained great success in computer vision applications such
*Corresponding author.
†Equal contribution. as image classification [17, 51], objective detection [13, 44] and segmentation [37, 4]. However, the high and yet still increasing demands on computing power and memory foot-print limit their deployment on edge devices, such as mobile phones or wearable devices. Therefore, many model com-pression technologies are proposed to compress and accel-erate networks including network pruning [16], parameter quantization [5] and knowledge distillation [22]. Among these methods, network pruning has been recognized as an effective tool to support model deployment by reduc-ing the redundancy of neural networks. Prevalent pruning methods can be roughly categorized into weight pruning, channel pruning, and N:M sparsity. Weight pruning, also called unstructured pruning, removes individual weight in weight matrix [10, 7, 28, 16], which requires specific hard-ware or software and has limited application. Channel prun-ing [20, 23, 36, 63] breaks this limit by pruning the entire channels and filters, which is more versatile on hardware.
N:M sparsity optimizes the sparsity of DNNs so that only
N weights are non-zero for every continuous M weights.
[67, 65], which also requires specific hardware capabilities to accelerate the networks. In this paper, we focus on chan-nel pruning because of its high adaptability on devices.
The core of channel pruning, which distinguishes dif-ferent approaches, consists of two aspects: (1) method of channel selection; (2) design of pruning ratio.
Channel Selection. Typical channel pruning methods propose their criteria to measure the importance of channels or filters, such as Norm-based [39, 30, 18], Gradient-based
[35, 55], Rank-based [34], BN-based [36], Activation-based
[38, 23] and so on. Some other methods [15, 3, 9] assign de-signed masks or gates for channels as their importance indi-cators. However, most of these methods have two problems.
First, they are based on heuristics and lack theoretical guid-ance. Second, They only focus on the inherent properties of individual channels or feature maps, which cannot reason-ably interpret network pruning from a global perspective.
In this paper, we introduce a novel view from an Informa-tion Bottleneck (IB) perspective to channel selection, which supplies the theoretic support for channel pruning and pro-vides a global view across channels.
IB [53] aims to extract the relevant information that an input random variable X contains about an output random variable Y . Let ˜X denote the compressed representation of X. Then the goal of filter pruning is to find the ˜X that preserves the most “relevant” part of X with respect to Y .
And the IB is a metric designed to measure this “relevance,” even in a non-linear case. In other words, we propose to find the optimal representation ˜X via minimizing the following
IB objective
I(X; ˜X) − βI( ˜X; Y ) where β is a positive Lagrange multiplier that trades off the complexity of the compression process I(X; ˜X), and the amount of the preserved relevant information, I( ˜X; Y ).
While it is ultimately desired to perform a joint optimization on all the layers, as the first trial in this direction, in this pa-per we perform this IB optimization on each unpruned layer to make it tractable. Then in our context, for a single layer in our method, X denotes the original input feature maps and Y denotes the output feature maps, ˜X represents the pruned input feature maps.
However, because the computation of IB involves den-sity estimation in high dimensional space, in most practical cases, it is computationally infeasible to compute. In this paper, we use HSIC Bottleneck (HB) to approximate esti-mate IB. HSIC Bottleneck [40] is an implementation of IB principle by replacing the mutual information terms in the
IB objective with HSIC [14]. In contrast to mutual infor-mation, HSIC can provide a robust computation and does not require density estimation. Thus HB is usually used as an alternative to IB in previous works [40, 43, 59]. And we then prove the equivalence between the HSIC Bottle-neck and HSIC Lasso for the first time, which inspires us to utilize the HSIC Lasso to solve channel pruning in a princi-pled way. HSIC Lasso [61] is a kernel-based nonlinear fea-ture selection algorithm with the main concept of minimum redundancy and maximum relevancy (MRMR). It only in-volves sampled-based Gram matrices, so the prohibitively expensive density estimation could be avoided. As will be shown below, we could use HSIC Lasso to substitute the IB objective under certain circumstances. With further exper-iments on the application of HSIC Lasso on filter pruning, we found more unique properties of HSIC Lasso: (1) as the batch size increases, the channels selected by HSIC Lasso are almost unchanged; (2) many existing channel pruning methods have highly similar channel selection results, e.g.
L1 norm, L2 norm [30], FPGM [18], BN-based [36] and
Taylor-FO, Taylor-SO [41]. However, the HSIC Lasso can find better channels than the consensus of existing methods.
Pruning Ratio Design. Most pruning methods assume the pruning ratio of each layer a given parameter and thus depend on expert experience to achieve good performance.
Our approach solves this problem by treating the pruning ratio for each layer as a prediction output. Specifically, we design a global penalty coefficient λ∗ to control the sparsity level, which is shared by all the convolution layers and acts as the coefficient of the HSIC Lasso regularization item.
Given a target number of parameters or FLOPs, the target can be achieved by an automatic search for the value of λ∗.
The details are illustrated in Sec. 3.3.
We summarize our contributions as follows:
• We interpret channel pruning based on Information
Bottleneck theory, which provides theoretic guidance for network pruning, and hints at the limitation of
Norm-based approaches. To our best knowledge, this is the first paper to build the relation between input feature maps and output feature maps based on the In-formation Bottleneck principle.
• For the first time, we prove the equivalence between
HB optimization objective and HSIC Lasso, which paved the path for utilizing HSIC Lasso to prune net-works based on IB principle.
• We demonstrate that HSIC Lasso-based pruning yields an excellent performance on various benchmarks, with stability and a surprising difference in the chosen chan-nels from other pruning methods.
We conduct extensive experiments on three benchmarks:
CIFAR-10, CIFAR-100 [26] and ImageNet [46]. We test various representative networks such as VGG [49],
MobileNet-V2 [47], GoogleNet [51] and ResNet [17].
Many Experiments demonstrate that our method has supe-rior performance than other SOTA pruning methods in both model acceleration and compression. 2.