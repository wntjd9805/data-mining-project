Abstract
Most self-supervised methods for representation learn-ing leverage a cross-view consistency objective i.e. they maximize the representation similarity of a given image’s augmented views. Recent work NNCLR goes beyond the cross-view paradigm and uses positive pairs from different images obtained via nearest neighbor bootstrapping in a contrastive setting. We empirically show that as opposed to the contrastive learning setting which relies on negative samples, incorporating nearest neighbor bootstrapping in a self-distillation scheme can lead to a performance drop or even collapse. We scrutinize the reason for this unexpected behavior and provide a solution. We propose to adaptively bootstrap neighbors based on the estimated quality of the latent space. We report consistent improvements compared to the naive bootstrapping approach and the original base-lines. Our approach leads to performance improvements for various self-distillation method/backbone combinations and standard downstream tasks. Our code is publicly avail-able at https://github.com/tileb1/AdaSim. 1.

Introduction
Self-supervised learning (SSL) methods have seen a lot of breakthroughs over the past few years. Most recent self-supervised methods train features invariant to data aug-mentation by maximizing the similarity between two aug-mentations of a single input image. However, this task is ill-posed as this optimization procedure admits trivial solutions (resulting in a “collapsed” scenario). Similarity maximization (or cross-view consistency) SSL methods can be categorized based on how they avoid trivial solutions.
The most famous subset are contrastive learning methods
[10, 9, 25, 11, 13] in which the collapse is avoided by using negative pairs. On the one hand, the learning procedure is
* denotes equal contribution.
Figure 1: The selection of positive image pairs used for cross-view consistency in self-supervised representation learning is key for good performance. With our method, given the query (or anchor) image on the left, similar im-ages are successfully ranked according to pwin(xj| xi) (il-lustrated as a green bar on the bottom left of each image).
Our algorithm enforces similarity between the query xi and an image xj sampled from pwin(xj| xi). These results are non-cherry-picked and obtained at the ﬁnal epoch (800) of the pretraining. Best viewed in color and zoomed-in. robust since collapse avoidance is explicitly modeled in the training objective, but on the other hand, it requires large batches to have a sufﬁcient pool of negative samples. This makes them GPU memory inefﬁcient and limits research to those who dispose of large distributed computing infrastruc-ture.
More recently, self-distillation methods have been gain-ing traction [12, 8, 24, 33]. These similarity maximiza-tion algorithms avoid trivial solutions by using asymmetry.
This asymmetry can take the form of an additional predictor
[12, 24] on one branch, using stop-gradients [12, 8, 33, 24], a momentum encoder [8, 24], etc. These methods are of particular interest as 1) they do not require large batch sizes, and 2) they currently show state-of-the-art perfor-mance [8, 33] on standard downstream tasks.
Orthogonal to the choice of the framework (contrastive vs self-distillation), one can wonder what is the best way to obtain positive pairs.
Intuitively, similarity maximiza-tion SSL methods could be improved by using positive pairs from different images. Indeed if an oracle indicating valid positive pairs [28, 29] was available, instead of taking two augmentations from the same image, we could simply take pairs from the oracle. The features would, therefore, not be trained to be invariant to handcrafted data augmentations but invariant to intra-class variation, which would make them more aligned with most common downstream tasks, e.g. classiﬁcation.
In the absence of labels, we can leverage the structure of the latent space to obtain a proxy for the oracle. Semanti-cally related images are expected to lie in the vicinity of one another in the latent space. However, this is a chicken and egg problem, as this assumption only holds when the qual-ity of the learned latent space is good enough. If the learned latent space is not of good quality, bootstrapping the proxy leads to unwanted gradient ﬂows, e.g., an image of a cat is pulled closer to the image of a building.
Nevertheless, recent work NNCLR [19] has successfully incorporated nearest neighbor (NN) bootstrapping in a con-trastive setting. Considering that self-distillation typically outperforms contrastive methods, in this work, we explore how the same can be achieved without explicit use of nega-tives.
Unfortunately, this combination does not work out of the box. We empirically observe that it can be hurtful and even lead to collapse. We scrutinize the reason for this unexpected behavior and provide a solution. We propose to estimate the quality of the latent space and adaptively use positive pairs sampled from a ranked set of neighbors (Fig. 1) if the estimated quality of the latent space is high enough. This leads to an Adaptive learning algorithm based on Similarity bootstrapping dubbed AdaSim. The overall framework is shown in Figure 2. We summarize our contri-butions as follows: 1. We provide empirical evidence that when combined with self-distillation, straightforward bootstrapping as in [19] can lead to a performance drop or even col-lapse. This is validated for multiple self-distillation methods and backbone combinations; 2. We propose an adaptive similarity bootstrapping learn-ing method (AdaSim) in which the amount of boot-strapping is modulated via a single temperature param-eter. Using a temperature parameter of 0, AdaSim de-faults to self-distillation with standard positive image pairs generated from augmented views of the same im-age. We show that AdaSim performs best with a non-zero temperature parameter and outperforms the base-lines on standard downstream tasks. 2.