Abstract
Test-time task adaptation in few-shot learning aims to adapt a pre-trained task-agnostic model for capturing task-speciﬁc knowledge of the test task, rely only on few-labeled support samples. Previous approaches generally focus on developing advanced algorithms to achieve the goal, while neglecting the inherent problems of the given support sam-ples. In fact, with only a handful of samples available, the adverse effect of either the image noise (a.k.a. X-noise) or the label noise (a.k.a. Y-noise) from support samples can be severely ampliﬁed. To address this challenge, in this work we propose DEnoised Task Adaptation (DETA), a
ﬁrst, uniﬁed image- and label-denoising framework orthog-onal to existing task adaptation approaches. Without ex-tra supervision, DETA ﬁlters out task-irrelevant, noisy rep-resentations by taking advantage of both global visual in-formation and local region details of support samples. On the challenging Meta-Dataset, DETA consistently improves the performance of a broad spectrum of baseline methods applied on various pre-trained models. Notably, by tack-ling the overlooked image noise in Meta-Dataset, DETA es-tablishes new state-of-the-art results. Code is released at https://github.com/JimZAI/DETA. 1.

Introduction
Few-Shot Learning (FSL) refers to rapidly deriving new knowledge from a limited number of samples, a central ca-pability that humans naturally possess, but “data-hungry” machines still lack. Over the past years, a community-wide enthusiasm has been ignited to narrow this gap, especially in ﬁelds such as computer vision [15,26,47], machine trans-lation [4, 30, 54] and reinforcement learning [9, 17, 39].
The general formulation of FSL involves two stages: 1) training-time task-agnostic knowledge accumulation, and 2) test-time task-speciﬁc knowledge acquisition, a.k.a. task adaptation. In particular, the former stage seeks to pre-train
*Corresponding author.
Figure 1. Dual noises in the support samples of a few-shot task.
Image noise (a.k.a. X-noise): the target object regions are often obscured by interfering factors such as cluttered backgrounds, im-age corruption, etc. Label noise (a.k.a. Y-noise): mislabeled sam-ples. The goal of this work is to develop a ﬁrst, uniﬁed image- and label-denoising framework for reliable task adaptation. a task-agnostic model on large amounts of training sam-ples collected from a set of base classes. While the latter targets adapting the pre-trained model for capturing task-speciﬁc knowledge of the few-shot (or test) task with novel classes, given a tiny set of labeled support samples. Early progress in FSL has been predominantly achieved using the idea of meta-learning, which aligns the learning objec-tives of the two stages to better generalize the accumulated knowledge towards few-shot tasks [39, 47, 54]. Neverthe-less, recent studies [13, 25, 35, 42] revealed that a good test-time task adaptation approach with any pre-trained models – no matter what training paradigms they were learned by, can be more effective than sophisticated meta-learning algo-rithms. Furthermore, with the recent success in model pre-training techniques [14, 16, 33], designing efﬁcient adapter-based [24, 25, 55] or ﬁnetuning-based [6, 19, 46] task adap-tation algorithms that can ﬂexibly borrow “free” knowledge from a wide range of pre-trained models is therefore of great practical value, and has made remarkable progress in FSL.
free contrastive relevance aggregation (CoRA) module is
ﬁrst designed to determine the weights of regions and im-ages in support samples, based on which two losses are pro-posed for noise-robust (or reliable) task adaptation: a local compactness loss Ll that promotes the intra-class compact-ness of clean regions, along with a global dispersion loss Lg that encourages the inter-class dispersion of clean, image-level class prototypes. The two losses complement each other to take advantage of both global visual information and local region details of support samples to softly ignore the dual noises during the optimization. An overview of our
DETA framework is shown in Figure 3.
Flexibility and Strong Performance. The proposed DETA is orthogonal to existing adapter-based task adaptation (A-TA) and ﬁnetuning-based task adaptation (F-TA) paradigms, therefore can be plugged into any types of these approaches to improve model robustness under the joint (image, label)-noise. On average, by performing image-denoising on the vanilla Meta-Dataset (MD) [51], DETA improves the clas-siﬁcation accuracy of A-TA, F-TA baselines by 1.8%∼1.9%, 2.2%∼4.1%, respectively (Table 1). In particular, by tack-ling the overlooked image noise in the vanilla MD, DETA further boosts the state-of-the-art TSA [25] by 1.8%∼2.1% (Table 5). Also, by conducting label-denoising on the label-corrupted MD, DETA outperforms A-TA, F-TA baselines by 1.8%∼4.2%, 2.8%∼6.1%, respectively (Table 2).
Contributions. To summarize, our contributions are three-fold. 1) We propose DETA, a ﬁrst, uniﬁed image- and label-denoising framework for FSL. 2) Our DETA can be ﬂexibly plugged into both adapter-based and ﬁnetuning-based task adaptation paradigms. 3) Extensive experiments on Meta-Dataset show the effectiveness and ﬂexibility of DETA. 2.