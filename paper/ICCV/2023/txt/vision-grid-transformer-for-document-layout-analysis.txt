Abstract
Document pre-trained models and grid-based models have proven to be very effective on various tasks in Docu-ment AI. However, for the document layout analysis (DLA) task, existing document pre-trained models, even those pre-trained in a multi-modal fashion, usually rely on either textual features or visual features. Grid-based models for
DLA are multi-modality but largely neglect the effect of pre-training. To fully leverage multi-modal information and exploit pre-training techniques to learn better repre-sentation for DLA, in this paper, we present VGT, a two-stream Vision Grid Transformer, in which Grid Transformer (GiT) is proposed and pre-trained for 2D token-level and segment-level semantic understanding. Furthermore, a new dataset named D4LA, which is so far the most diverse and detailed manually-annotated benchmark for document layout analysis, is curated and released. Experiment re-sults have illustrated that the proposed VGT model achieves new state-of-the-art results on DLA tasks, e.g. PubLayNet (95.7%→96.2%), DocBank (79.6%→84.1%), and D4LA (67.7%→68.8%). The code and models as well as the D4LA dataset will be made publicly available 1. 1.

Introduction
Documents are important carriers of human knowledge.
With the advancement of digitization, the techniques for au-tomatically reading [38, 50, 39, 40, 9], parsing [51, 31], and understanding documents [32, 46, 20, 25] have become a crucial part of the success of digital transformation [8].
Document Layout Analysis (DLA) [4], which transforms documents into structured representations, is an essential stage for downstream tasks, such as document retrieval, ta-ble extraction, and document information extraction. Tech-nically, the goal of DLA is to detect and identify homoge-neous document regions based on visual cues and textual content within the document. However, performing DLA in real-world scenarios is faced with numerous difficulties: va-*Equal contribution. † Corresponding author. 1https://github.com/AlibabaResearch/AdvancedLiterateMachinery
Table 1. Comparisons of the use of different modalities and pre-training techniques with existing SOTA methods in DLA task.
Models
Vision Text Pre-trained
CNN-based [37, 34]
ViT-based [25]
Multi-modal PTM [15, 20]
Grid-based [45, 47]
VGT (Ours)
✓
✓
✓
✓
✓
✗
✗
✗
✓
✓
✗
✓
✓
✗
✓ riety of document types, complex layouts, low-quality im-ages, semantic understanding, etc. In this sense, DLA is a very challenging task in practical applications.
Basically, DLA can be regarded as an object detection or semantic segmentation task for document images in com-puter vision. Early works [37, 34] directly use visual fea-tures encoded by convolutional neural networks (CNN) [19] for layout units detection [36, 30, 35, 17], and have been proven to be effective. Recent years have witnessed the success of document pre-training. Document Image Trans-former (DiT) [25] uses images for pre-training, obtaining good performance on DLA. Due to the multi-modal nature of documents, previous methods [15, 20] propose to pre-train multi-modal Transformers for document understand-ing. However, these methods still employ only visual infor-mation for DLA fine-tuning. This might lead to degraded performances and generalization ability for DLA models.
To better exploit both visual and textual information for the DLA task, grid-based methods [45, 22, 47] cast text with layout information into 2D semantic representations (char-grid [23, 22] or sentence-grid [45, 47]) and combine them with visual features, achieving good results in the DLA task.
Although grid-based methods equip with an additional tex-tual input of grid, only visual supervision is used for the model training in the DLA task. Since there are no explicit textual objectives to guide the linguistic modeling, we con-sider that the capability of semantic understanding is lim-ited in grid-based models, compared with the existing docu-ment pre-trained models [20]. Therefore, how to effectively model semantic features based on grid representations is a
Figure 1. Document examples in the public dataset PubLayNet (a) and DocBank (b) and document examples in real-world applications.
Table 2. Comparisons with previous DLA datasets.
Dataset Type Category Labeling Training Validation
PubLayNet
DocBank
D4LA 1 1 12 5 13 27
XML
LATEX
Manual 335,703 400,000 8,868 11,245 50,000 2,224 model 2D language information. Specifically, we represent a document as a 2D token-level grid as in the grid-based methods [45, 22, 47] and feed the grid into GiT. For bet-ter token-level and segment-level semantic awareness, we propose two new pre-training objectives for GiT. First, in-spired by BERT [11], the Masked Grid Language Modeling (MGLM) task is proposed to learn better token-level seman-tics for grid features, which randomly masks some tokens in the 2D grid input, and recovers the original text tokens on the document through its 2D spacial context. Second, the Segment Language Modeling (SLM) task is proposed to enforce the understanding of segment-level semantics in the grid features, which aims to align the segment-level se-mantic representations from GiT with pseudo-features gen-erated by existing language model (e.g. , BERT [3] or Lay-outLM [43]) via contrastive learning. Both token-level and segment-level features are obtained from the 2D grid features encoded by GiT via RoIAlign [17], according to the coordinates. Combining image features from Vision
Transformer (ViT) further, VGT can make full use of tex-tual and visual features from GiT and ViT respectively and leverage multi-modal information for better document lay-out analysis, especially in text-related classes.
In addition, to facilitate the further advancement of DLA research for real-world applications, we propose the D4LA dataset, which is the most Diverse and Detailed Dataset ever for Document Layout Analysis. The differences from the existing datasets for DLA are listed in Table 2. Specifically,
Figure 2. Some special layout categories of D4LA dataset. vital step to improve the performance of DLA. The differ-ences between existing DLA methods are shown in Table 1.
As a classic Document AI task, there are many datasets for document layout analysis. However, the variety of ex-isting DLA datasets is very limited. The majority of docu-ments are scientific papers, even in the two large-scale DLA datasets PubLayNet [48] and DocBank [26] (in Figure 1 (a)
& (b)), which have significantly accelerated the develop-ment of document layout analysis recently. As shown in
Figure 1, in real-world scenarios, there are diverse types of documents, not limited to scientific papers, but also in-cluding letters, forms, invoices, and so on. Furthermore, the document layout categories in existing DLA datasets are tai-lored to scientific-paper-type documents such as titles, para-graphs, and abstracts. These document layout categories are not diverse enough and thus are not suitable for all types of documents, such as the commonly encountered Key-Value areas in invoices and the line-less list areas in budget sheets.
It is evident that a significant gap exists between the exist-ing DLA datasets and the actual document data in the real world, which hinders the further development of document layout analysis and real-world applications.
In this paper, we present VGT, a two-stream multi-modal Vision Grid Transformer for document layout analy-sis, of which Grid Transformer (GiT) is proposed to directly
Figure 3. The model architecture of Vision Grid Transformer (VGT) with pre-training objectives for the GiT branch.
D4LA dataset contains 12 types of documents as shown in
Figure 1. We define 27 document layout categories and manually annotate them. Some special layout classes are illustrated in Figure 2, which are more challenging and text-related. Experiment results on DocBank, PubLayNet and
D4LA show the SOTA performance of VGT.
The contributions of this work are as follows: 1) We introduce VGT, a two-stream Vision Grid Trans-former for document layout analysis, which can lever-age token-level and segment-level semantics in the text grid by two new proposed pre-training tasks: MGLM and SLM. To the best of our knowledge, VGT is the first to explore grid pre-training for 2D semantic un-derstanding in documents. 2) A new benchmark named D4LA, which is the most di-verse and detailed manually-labeled dataset for docu-ment layout analysis, is released. It contains 12 types of documents and 27 document layout categories. 3) Experimental results on the existing DLA benchmarks (PubLayNet and DocBank) and the proposed D4LA dataset demonstrate that the proposed VGT model out-performs previous state-of-the-arts. 2.