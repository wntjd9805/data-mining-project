Abstract
This work dedicates to continuous sign language recog-nition (CSLR), which is a weakly supervised task dealing with the recognition of continuous signs from videos, with-out any prior knowledge about the temporal boundaries be-tween consecutive signs. Data scarcity heavily impedes the progress of CSLR. Existing approaches typically train CSLR models on a monolingual corpus, which is orders of magni-tude smaller than that of speech recognition. In this work, we explore the feasibility of utilizing multilingual sign lan-guage corpora to facilitate monolingual CSLR. Our work is built upon the observation of cross-lingual signs, which originate from different sign languages but have similar vi-sual signals (e.g., hand shape and motion). The underlying idea of our approach is to identify the cross-lingual signs in one sign language and properly leverage them as auxiliary training data to improve the recognition capability of an-other. To achieve the goal, we first build two sign language dictionaries containing isolated signs that appear in two datasets. Then we identify the sign-to-sign mappings be-tween two sign languages via a well-optimized isolated sign language recognition model. At last, we train a CSLR model on the combination of the target data with original labels and the auxiliary data with mapped labels. Experimentally, our approach achieves state-of-the-art performance on two widely-used CSLR datasets: Phoenix-2014 and Phoenix-2014T. 1.

Introduction
Sign languages are visual-spatial signals for communi-cation among deaf communities. These languages are pri-marily expressed through hand shape but are also greatly aided by the movement of the body, head, mouth, and eyes.
Sign language recognition, which aims to establish com-munication between hearing people and deaf people, can be roughly categorized into two sorts: isolated sign language recognition (ISLR) [49, 26, 19, 27] and continuous sign lan-guage recognition (CSLR) [24, 8, 22, 47, 46, 10]. ISLR is a supervised classification task—it requires models to rec-ognize and classify isolated signs from videos. In contrast, (a) Cross-lingual signs usually have distinct meanings. (b) Cross-lingual signs occasionally convey the same meaning.
Figure 1: Cross-lingual signs are those that originate from different sign languages but have similar visual signals (e.g. hand shape and motion). We show two examples identi-fied by our approach from a German sign language (DGS) dataset and a Chinese sign language (CSL) dataset.
CSLR is a weakly supervised task dedicated to the recog-nition of continuous signs from videos, without any prior knowledge about the temporal boundaries between consec-utive signs. The objective of this work is to develop a CSLR framework, with the assistance of an ISLR model and mul-tilingual corpus.
In contrast to the promising achievements in automatic speech recognition [28, 2, 35, 41], the lack of large-scale training data heavily impedes the progress of CSLR. In general, training a satisfying speech recognition model re-quires thousands of hours of training data [20, 34]. How-ever, existing sign language datasets [23, 6, 44] are or-ders of magnitude smaller, containing only fewer than 20
Figure 2: Illustration of our approach. The objective is to train a continuous sign language recognition (CSLR) model in
DGS with the assistance of a CSL dataset. We first build two sign dictionaries containing isolated signs in DGS and CSL respectively. Then we train an isolated sign language recognition (ISLR) model to identify the CSL-to-DGS mapping for each isolated sign in the CSL dictionary. Finally, the CSLR-DGS model is trained on a combination of the DGS dataset with original labels and the CSL dataset with mapped labels through the CTC loss [16]. hours of paralleled samples. Previous approaches [24, 8, 22, 47, 46, 10] typically train CSLR models on mono-lingual corpora such as German sign language (Deutsche
Geb¨ardensprache, DGS) datasets (e.g. Phoenix-2014 [23] and Phoenix-2014T [6]), and Chinese sign language (CSL) datasets (e.g. CSL-Daily [44]). Unfortunately, the limited training data immensely restricts the recognition capacity.
One possible technique to alleviate the data scarcity issue is semi-supervised learning, which requires a large volume of unlabeled data in addition to a collection of labeled data.
However, existing CSLR datasets are collected in specific domains, e.g., Phoenix-2014 [23] and Phoenix-2014T [6] are concentrated in the domain of weather forecast. Col-lecting domain-relevant data for CSLR becomes challeng-ing and unpractical, impeding semi-supervised learning.
Nevertheless, since sign languages are visual languages, it is possible for them to employ the same sign to express either the same or different meanings. If the hypothesis is true, it will be feasible to utilize multilingual sign language corpora to enrich training data. Fortunately, we conduct ex-periments to confirm the existence of these signs, which are referred to as cross-lingual signs in our work. Cross-lingual signs are those that originate from different sign languages but have similar visual signals (e.g., hand shape and mo-tion). Since most sign languages are mutually unintelli-gible, cross-lingual signs typically have distinct meanings in different sign languages (Figure 1a).
Interestingly, we find that they might convey the same meanings occasionally (Figure 1b). The discovery of cross-lingual signs inspires us to identify these signs in one sign language and prop-erly leverage them as auxiliary training data to improve the recognition performance of the other.
Consider a scenario where we are going to train a CSLR model in DGS given a primary DGS dataset and an aux-iliary dataset in another sign language, e.g., CSL. In gen-eral, the size of the auxiliary dataset should exceed that of the primary one. The underlying idea behind our ap-proach is to find the CSL-to-DGS mapping for each iso-lated sign in the CSL dataset. Figure 2 illustrates our so-lution, which contains three steps: (1) build two sign lan-guage dictionaries containing isolated signs that appear in
CSL and DGS datasets, respectively; (2) identify the CSL-to-DGS mapping for each isolated sign in CSL dictionary according to the cross-lingual sign similarity calculated by a well-optimized isolated sign language recognition model; (3) train a CSLR model in DGS on the combination of DGS data with original labels and CSL data with mapped labels through the well-established CTC loss [16].
It is worth mentioning that it is non-trivial to directly build a sign lan-guage dictionary from a CSLR dataset due to the absence of sign boundary annotations. To tackle this problem, we adopt a pre-trained CSLR model to split the continuous signs into the isolated ones for dictionary construction.
The contributions of this work can be summarized as:
• We are the first to utilize a multilingual sign language corpus to facilitate monolingual CSLR based on the finding of cross-lingual signs.
• We present a comprehensive solution for seamlessly incorporating an auxiliary dataset—though in another sign language—into the training of CSLR.
• Our approach achieves state-of-the-art performance on two widely-used CSLR datasets: Phoenix-2014 [23] and Phoenix-2014T [6]. 2.