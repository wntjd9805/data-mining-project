Abstract
Guided depth map super-resolution (GDSR), as a hot topic in multi-modal image processing, aims to upsample low-resolution (LR) depth maps with additional informa-tion involved in high-resolution (HR) RGB images from the same scene. The critical step of this task is to effec-tively extract domain-shared and domain-private RGB/depth features. In addition, three detailed issues, namely blurry edges, noisy surfaces, and over-transferred RGB texture, need to be addressed. In this paper, we propose the Spheri-cal Space feature Decomposition Network (SSDNet) to solve the above issues. To better model cross-modality features,
Restormer block-based RGB/depth encoders are employed for extracting local-global features. Then, the extracted features are mapped to the spherical space to complete the separation of private features and the alignment of shared features. Shared features of RGB are fused with the depth features to complete the GDSR task. Subsequently, a spherical contrast reﬁnement (SCR) module is proposed to further address the detail issues. Patches that are clas-siﬁed according to imperfect categories are input into the
SCR module, where the patch features are pulled closer to the ground truth and pushed away from the correspond-ing imperfect samples in the spherical feature space via contrastive learning. Extensive experiments demonstrate that our method can achieve state-of-the-art results on four test datasets, as well as successfully generalize to real-world scenes. The code is available at https://github. com/Zhaozixiang1228/GDSR-SSDNet. 1.

Introduction
Depth maps, as images that measure the distance of scene points from the sensor, are widely used in autonomous driv-ing [38, 50, 87–89], pose estimation [75, 83], virtual real-ity [41, 51, 79, 80], and scene understanding [24, 53, 65, 96].
∗Corresponding author.
Figure 1: Our SSDNet achieves outstanding performance on the
RGBDD dataset for ×4, ×8, and ×16 and Middlebury for ×4 while being computationally efﬁcient.
However, the depth maps produced by current consumer-level depth sensors, e.g., Time-of-Flight (ToF) and Kinect cameras, often have the disadvantages of low resolution and noise. These disadvantages are insufﬁcient to meet the re-quirements of advanced computing vision tasks [52, 76, 93].
To obtain high-resolution depth maps, we naturally hope to accomplish depth map super-resolution (SR) by transferring mature SR models in the RGB image do-main [13, 39, 86] to the depth SR task. However, a potential risk is that RGB SR models tend to focus on reconstructing high-frequency image information, such as details and tex-ture. Conversely, for depth images, the objects’ depth infor-mation is often textureless and piecewise, and more sensitive to unclear edges and noise. Therefore, it is unreasonable to directly apply RGB SR models to the depth SR task. On the other hand, while acquiring the depth map, it is relatively easy to obtain HR and noise-free RGB images in the same scene. Furthermore, there are statistical co-occurrences be-tween the edges in RGB images and the discontinuities in
depth maps [55]. Therefore, we hope to use the HR RGB images to provide edges and contour information missing in the depth map, and to fuse the multi-modal information to accomplish the LR depth image upsampling.
In the era of deep learning (DL), numerous methods have been utilized to learn the mapping between LR → HR depth maps. These methods succeeded to some extent in modeling the cross-modality features and reconstructing the contour and edge information. However, DL models which rely on natural priors will limit the model’s ﬂexibility. Models learning the LR → HR mapping via data-driven methods are difﬁcult to interpret due to the unclear working mech-anism [90]. Thus, effectively extracting and distinguish-ing domain-shared and domain-private RGB/depth features is still a challenge. While microscopically, the obtained depth images are often plagued by three detail issues: blurry edges, noisy surfaces, and over-transferred RGB texture, all of which affect the display quality of depth maps.
In response to addressing the above challenges, we ex-pect to limit the solution space by constraining the extracted features and further improve the modeling of the dependen-cies between different modalities. Based on our observation,
RGB images and depth maps contain shared features, such as depth map discontinuities and edge features of RGB ob-jects, which can be aligned in the feature space. In addition, unique private features, such as the distance information of the depth map and the texture of RGB object surface, should be separated conversely. Thus, in the feature space, domain-shared and domain-private features are expected to separate and align respectively, while the distance between HR fea-tures and imperfect features with the above-mentioned detail issues should also be pushed away.
The above goal can be divided into two steps. First, extracting features through an effective encoder, and then selecting an appropriate distance measure to complete the alignment and separation of features. Considering that CNN-based architectures limit feature extraction capabilities due to content-independent convolution kernels and the lack of global information, we employ the Restormer blocks [84], which has been proved to effectively extract features in the low-level vision domain, as the basic unit of our encoder.
For the choice of distance measure, we ﬁrst think of the
Euclidean distance, such as (cid:96)1 or (cid:96)2 distance. However, cross-modality features often contain different scales and orders of magnitude, and the Euclidean distance is easily affected by the scale, which makes it difﬁcult to achieve our goal. Recently, with the development of spherical DL models [4, 7, 21, 22], the spherical feature learning is well known by virtue of its advantages over the Euclidean feature learning in many applications, e.g., domain adaptation [21].
Due to the distances between spherical features being reg-ularized, the alignment and separation of features can be done more easily without losing the model’s representational capacity.
According to the above analysis, we propose our Spheri-cal Space feature Decomposition Network (SSDNet). The speciﬁc workﬂow is displayed in Fig. 2. Our contribution can be summarized in three-fold:
• We propose a spherical space feature decomposition framework to model the cross-modality features. The features extracted by the Restormer block are mapped to the spherical space for separation and alignment. This is the ﬁrst time that the Transformer and spherical-space distance measure are applied to the GDSR task.
• The spherical contrast reﬁnement module, cooperating with the imperfect patch classiﬁcation and the corre-sponding contrastive learning branch, is proposed to address the possible detail issues in depth maps. This is also the ﬁrst time that the contrastive learning technique has been used for the GDSR task.
• Experiments on four GDSR benchmarks and a real scene dataset demonstrate that our method can generate satisfactory HR depth maps in different scenarios and exhibits good generalization ability. 2.