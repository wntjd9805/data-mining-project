Abstract
Human-Object Interaction (HOI) detection is essential for understanding and modeling real-world events. Ex-isting works on HOI detection mainly focus on static im-ages and a closed setting, where all HOI classes are pro-vided in the training set.
In comparison, detecting HOIs in videos in open set scenarios is more challenging. First, under open set circumstances, HOI detectors are expected to hold strong generalizability to recognize unseen HOIs not included in the training data. Second, accurately cap-turing temporal contextual information from videos is diffi-cult, but it is crucial for detecting temporal-related actions such as open, close, pull, push. To this end, we propose ACoLP, a model of Action-centric Chain-of-Look
Prompting for open set video HOI detection. ACoLP re-gards actions as the carrier of semantics in videos, which captures the essential semantic information across frames.
To make the model generalizable on unseen classes, in-spired by the chain-of-thought prompting in natural lan-guage processing, we introduce the chain-of-look prompt-ing scheme that decomposes prompt generation from large-scale vision-language model into a series of intermediate visual reasoning steps. Consequently, our model captures complex visual reasoning processes underlying the HOI events in videos, providing essential guidance for detecting unseen classes. Extensive experiments on two video HOI datasets, VidHOI and CAD120, demonstrate that ACoLP achieves competitive performance compared with the state-of-the-art methods in the conventional closed setting, and outperforms existing methods by a large margin in the open set setting. Our code is avaliable at https://github. com/southnx/ACoLP. 1.

Introduction
Human-object interaction (HOI) detection generates a set of meaningful <person, interaction, object> triplets
It has attracted much attention in re-for a given scene.
Figure 1. Illustration on chain-of-thought prompting and chain-of-look prompting. (a) Chain-of-thought prompting di-vides a problem into a series of intermediate reasoning steps, en-abling large language models on symbolic reasoning tasks. (b) We propose chain-of-look prompting scheme to captures complex vi-sual reasoning processes with two reasoning networks from large-scale vision-language models. Each reasoning network consists of two visual reasoning chains. The learned visual reasoning pro-cesses can be expanded to unseen classes with strong generaliza-tion ability. cent years, due to its importance for scene understand-ing, and applications in healthcare, autonomous driving, etc.
[47, 42, 15, 6, 12, 16, 24]. Most current works investigate HOI detection in static images in a closed setting[45, 15, 6, 29, 32], where all HOI classes are pre-defined with examples provided in the training set. How-ever, real-world scenarios are often open-set, where HOIs with novel actions are not present in the training set. De-tecting open set HOIs in videos is challenging due to the following reasons. First, static images barely contain tem-poral information of human and objects, thus directly trans-ferring HOI detection methods for static images to videos fails to model the temporal dynamics. Second, the possi-ble HOIs occurred in videos can hardly be exhausted since the compositional space resulting from the <person, inter-action, object> triplets is tremendous. Moreover, semantic ambiguity of HOIs widely exists in videos, especially for temporal-related interactions. For example, <person, hold, cup> and <person, lift, cup> contain similar and overlap-ping semantic information, but it is challenging to distin-guish these interdependent HOIs. To this end, we need to build a robust HOI detector for videos with strong general-izability that is capable of handling unseen and ambiguous
HOI classes in the open set setting.
Several existing works have investigated HOI detection in videos. A number of methods are based on spatio-temporal graph, including structured RNN [13], LIGHTEN
[41], STIGPN [46] and weakly-supervised video HOI de-tection [21]. Another line of works model the inherent prop-erties of video HOI to help HOI detection. For instance,
ASSIGN [28] models asynchronous and sparse properties of HOIs in videos to detect the structure of interaction events in a video scene. 2G-GCN [31] extends ASSIGN by considering geometric features while modeling human and object dependencies. Recently, Transformer-based methods
[43] have been proposed for video HOI detection by struc-turizing a video into a few tubelet tokens. Although these works have made significant progresses in detecting HOIs in videos, they largely focus on the nouns, i.e., human and objects, in the video, and infer interactions based on human and objects features. This strategy may result in the loss of valuable information on the verbs, i.e., actions, inherent in videos. In addition, previous efforts assume that all testing
HOI classes are known in training, leading to unsatisfactory results and poor generalizability when applied to open set circumstances.
To address the challenges mentioned above, we propose
ACoLP in this work, which is an open set HOI detection model for videos. The key idea of ACoLP model is to ab-stract each frame into action prompts and model the prompt generating processes as a series of intermediate visual rea-soning steps. The motivation underlying this idea is that the verbs (actions) convey central information of the events happening in a video. In light of this, videos can be regarded as a sequence of actions. Modeling the temporal dynamics of those action sequences captures the core semantic infor-mation of events in videos. Meanwhile, to empower the model with strong generalization ability for video HOI de-tection, ACoLP adopts the chain-of-thought prompting [48] strategy in natural language processing (NLP) to “prompt” the model with input-output visual reasoning steps, which we call the chain-of-look. Those visual reasoning steps, equipped with the few/zero-shot learning ability from large-scale visual-language (VL) models, confer the capability of reasoning visual events on unseen classes. As shown in
Figure 1, we introduce visual-semantic reasoning network (VSR) and spatio-temporal reasoning network (STR) for
HOI prompting and action prompting, respectively. Each network contains two steps of chains of reasoning for prompt generation, which are akin to the chain-of-thought prompting in NLP. Specifically, VSR includes CaptionHOI
Prompting (CHP) and VisualHOI Prompting (VHP). CHP is designed to incorporate global semantic information into in-dividual HOI prompts, serving as the first reasoning chain.
VHP follows CHP as the other reasoning chain with vi-sual information for HOI prompting. Similarly, STR also contains two reasoning chains: Action Prompting (AP) and
Dynamic GNN (D-GNN). AP is introduced to abstract vi-sual information of each frame into a fixed number of action prompt representations. D-GNN is then employed to model the temporal dynamics across frames, which also benefits open set HOI detection by propagating semantic informa-tion to neighboring frames, thus enabling action prompt representations to be more semantic-aware and discrimi-native in open set settings. Finally, HOI classification and bounding box regression are conducted under the guidance of action prompt representations, which is less noisy than only utilizing HOI prompts. With this open set HOI detec-tion model, we can better infer unseen HOIs in videos.
We validate the effectiveness of ACoLP model via com-prehensive experiments on two video HOI detection bench-mark datasets: VidHOI [4] and CAD-120 [18]. Comparing with current state-of-the-art (SOTA) methods for both video
HOI detection and image HOI detection, our model out-performs these SOTA methods in the open set setting and achieves comparable results in the closed setting.
Our main contributions on the open set HOI detection in videos are summarized as follows:
• We present an action-centric video HOI detection model, which focuses on modeling the HOIs via the verbs (actions) instead of nouns (humans and objects).
It helps more reliably capture the most central seman-tic information in understanding video HOIs.
• We introduce the chain-of-look prompting scheme to capture the underlying visual reasoning processes in videos, and generate visual-semantic aware and spatio-temporal aware prompts from VL models. Visual reasoning processes are further expanded to unseen classes for better generalization ability.
• Our model achieves substantial improvements in terms of open set video HOI detection and is on par with or better than SOTA methods in the closed setting. 2.