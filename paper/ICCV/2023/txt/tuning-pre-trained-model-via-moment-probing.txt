Abstract
Recently, efficient fine-tuning of large-scale pre-trained models has attracted increasing research interests, where linear probing (LP) as a fundamental module is involved in exploiting the final representations for task-dependent classification. However, most of the existing methods fo-cus on how to effectively introduce a few of learnable pa-rameters, and little work pays attention to the commonly used LP module.
In this paper, we propose a novel Mo-ment Probing (MP) method to further explore the potential of LP. Distinguished from LP which builds a linear classifi-cation head based on the mean of final features (e.g., word tokens for ViT) or classification tokens, our MP performs a linear classifier on feature distribution, which provides the stronger representation ability by exploiting richer statisti-cal information inherent in features. Specifically, we repre-sent feature distribution by its characteristic function, which is efficiently approximated by using first- and second-order moments of features. Furthermore, we propose a multi-head convolutional cross-covariance (MHC3) to compute second-order moments in an efficient and effective manner.
By considering that MP could affect feature learning, we introduce a partially shared module to learn two recalibrat-ing parameters (PSRP) for backbones based on MP, namely
MP+. Extensive experiments on ten benchmarks using var-ious models show that our MP significantly outperforms LP and is competitive with counterparts at lower training cost, while our MP+ achieves state-of-the-art performance. 1.

Introduction
Benefiting from the emergence of huge-scale datasets [8, 42, 41], the rapid development of neural network archi-tectures [10, 17, 43, 35] and self-supervised learning [15, 39, 5], large-scale pre-trained models dependent on suffi-cient computational resources show the great potential of
† This work was done when Mingze Gao was an intern at Baidu Re-search. ∗ Corresponding author
Method
Linear probing
MP (Ours)
VPT-Shallow [23]
VPT-Deep [23]
AdaptFormer [6]
SSF [32]
Full fine-tuning
MP+ (Ours)
IN-1K (%) 82.04 83.15 82.08 82.45 83.01 83.10 83.58 83.62
NABirds (%) 75.9 84.9 78.8 84.2 84.7 85.7 82.7 86.1
Params. (M) 0.77 3.65 0.92 1.23 1.07 0.97 86.57 4.10
Time (ms) 60 72 115 120 125 187 157 140
Mem. (G) 3.23 3.34 11.39 11.39 10.49 13.78 11.92 11.22
Table 1: Comparison of various tuning methods for pre-trained models in terms of recognition accuracy (%), learn-able parameters (Params.), training time (Time) per mini-batch, and GPU memory (Mem.) usage of training on
ImageNet-1K (IN-1K) and NABirds, where ViT-B/16 pre-trained on IN-21K is used as basic backbone. the transferability on downstream tasks [9, 7, 40, 4], where full fine-tuning as a basic method has achieved promising performance. However, full fine-tuning suffers from a high computational cost and is easy overfitting on small-scale datasets [32, 23]. In contrast, a simpler and more efficient method is only to tune a linear classifier (i.e., linear prob-ing [16]). Compared to full fine-tuning, linear probing (LP) usually suffers from inferior performance. To address this, existing works make a lot of efforts on parameter-efficient strategies [21, 2, 22, 30, 24, 23, 32, 6]. Going beyond LP, they focus on introducing a few learnable parameters to re-calibrate features from frozen pre-trained models for down-stream tasks. These methods avoid tuning massive parame-ters and show better efficiency and effectiveness trade-off.
Although many advanced efforts are made on parameter-efficient tuning, little work pays attention to the most funda-mental LP, which is involved in all existing tuning methods to learn a task-dependent classification head, and is closely related to the performance of downstream tasks. It can be observed that LP learns a linear classifier on input features, which are generally presented by classification token [10], average pooling (mean) of word tokens [43, 35] or convo-lution features [36]. From the statistical perspective, LP
(a) (b)
Figure 1: (a) Overview of proposed MP+ method for tuning pre-trained models, whose core is Moment Probing (MP) indi-cated by blue dashed line instead of the original linear probing. Specifically, our MP performs a linear classifier on powerful representations characterized by feature distribution, which is approximated by using first- and second-order moments of features. To efficiently explore second-order moments, we present a (b) multi-head convolutional cross-covariance (MHC3) method, whose details can refer to Sec. 3.1. Besides, a partially shared module to learn two recalibrating parameters (PSRP) is introduced for exploring the potential of MP on feature learning. mainly exploits first-order statistics of features, taking no full merit of rich statistical information inherent in features.
In this paper, we further explore the potential of LP by generating more powerful representations for linear clas-sifier. To this end, we propose a Moment Probing (MP) method, whose core is to perform a linear classifier on fea-ture distribution, which portrays the full picture of features and provides more powerful representations. Specifically, we model probability density of features by their character-istic function, and approximate it by using first- and second-order moments of features for computational efficiency.
Furthermore, we present a multi-head convolutional cross-covariance (MHC3) method to efficiently compute second-order moments, avoiding the issues of computation and gen-eralization brought by high-dimensional second-order rep-resentations. For computing MHC3, we first split features into several groups and compute cross-covariance between each two adjacent groups to extract second-order statis-tics. Then, a parameter-efficient module based on convolu-tions is designed to strengthen the interactions among cross-covariances and reduce the size of second-order representa-tions. Compared to the original second-order moments, our
MHC3 performs better in terms of both efficiency and effec-tiveness. Table 1 shows our MP shares similar training cost with LP, but obtains much higher accuracy. Meanwhile, MP is comparable to or better than existing parameter-efficient methods at lower training cost.
Since deep models are trained in an end-to-end learn-ing manner, the classifier will bring effect on feature learn-ing. To explore the potential of our MP on feature learning, we introduce a partially shared module to learn two recali-brating parameters (PSRP) for pre-trained models, inspired by parameter-efficient methods [6, 32]. By combining MP with PSRP, our MP+ surpasses full fine-tuning while learn-ing much fewer parameters, whose overview is illustrated in Figure 1. The contributions of our work are summarized as follows: (1) To our best knowledge, we make the first attempt to explore the potential of LP for tuning pre-trained models. To this end, we propose a Moment Probing (MP) method, which performs a linear classifier on powerful rep-resentations characterized by feature distribution. (2) For the efficiency of MP, we approximate feature distribution by using first- and second-order moments of features, and then present a multi-head convolutional cross-covariance (MHC3) method to explore second-order moments in an ef-ficient and effective manner. (3) By considering the effect of our MP on feature learning, we introduce a partially shared module to learn two recalibrating parameters (PSRP), re-sulting in a MP+ method. It further exploits the potential of our MP in tuning pre-trained models. (4) We conduct exten-sive experiments on ten benchmarks using various models, and results show our MP is superior to LP while general-izing well to pre-training strategies, few-shot and out-of-distribution settings. Besides, our MP+ outperforms exist-ing parameter-efficient methods, while achieving state-of-the-art performance. 2.