Abstract
In this paper, we propose P3D, the human part-wise mo-tion context learning framework for sign language recogni-tion. Our main contributions lie in two dimensions: learn-ing the part-wise motion context and employing the pose ensemble to utilize 2D and 3D pose jointly. First, our empirical observation implies that part-wise context en-coding benefits the performance of sign language recogni-tion. While previous methods of sign language recognition learned motion context from the sequence of the entire pose, we argue that such methods cannot exploit part-specific mo-tion context. In order to utilize part-wise motion context, we propose the alternating combination of a part-wise encod-ing Transformer (PET) and a whole-body encoding Trans-former (WET). PET encodes the motion contexts from a part sequence, while WET merges them into a unified context. By learning part-wise motion context, our P3D achieves supe-rior performance on WLASL compared to previous state-of-the-art methods. Second, our framework is the first to ensemble 2D and 3D poses for sign language recognition.
Since the 3D pose holds rich motion context and depth in-formation to distinguish the words, our P3D outperformed the previous state-of-the-art methods employing a pose en-semble.
Figure 1. Overall pipeline of proposed system. We first extract expressive 3D human pose from RGB video using off-the-shelf pose estimation methods [5, 36]. The expressive 3D human pose consists of 2D pose, 3D pose and facial expression. Then our pro-posed P3D predicts the word from the expressive 3D human pose. 1.

Introduction
Understanding and translating sign language is essential due to the huge potential benefit to society. Sign language recognition (SLR) can be considered as a specific kind of action recognition (AR) since both tasks aim to classify hu-man motion. However, SLR has a characteristic that is dis-tinguished from AR; Actions mainly focus on the motion of the human body, while the SLR system should be aware of more detailed information such as hand gestures and facial expressions. Thus, a specified system for SLR will provide further gain over using the AR models on SLR.
Previous approaches on SLR problem are mainly cat-egorized into RGB-based [1, 4, 31] and pose-based meth-ods [25, 29, 45, 47, 50]. Pose-based methods are known to have the advantage over RGB-based approaches in terms of the robustness on domain gap [14, 37]. For that, we aimed to enhance the performance of SLR, while preserving the benefits inherent in the pose-based approach.
Although recent pose-based SLR methods have shown noticeable progress, there are two main limitations of ex-isting methods. First, they utilize only whole-body rep-resentation (e.g., skeleton) to understand the motion con-text of human [20, 30, 47]. Since such approaches over-look the detailed motion context within a specific part
Figure 2. Effectiveness of 3D pose for sign language recognition. The 2D poses of the words ”Avoid” and ”Behind” are almost identical.
Due to the absence of depth information, solely relying on 2D pose makes it hard to distinguish the words. On the other hand, as shown in the red boxes, additionally employing the 3D pose with rich depth information can effectively resolve the ambiguity in sign language recognition. of human [18, 32], it leads to a decrease in SLR accu-racy. Second, existing pose-based methods only employ 2D pose [3, 20, 30, 47], which induces ambiguities in dis-tinguishing the different words. As shown in Figure 2, the 2D pose of different words can be almost identical due to the absence of depth information.
To overcome the above limitations, we propose P3D, the human part-wise motion context learning framework for accurate sign language recognition. The proposed P3D employs the part-wise encoding Transformer (PET) and whole-body encoding Transformer (WET) as main com-ponents. Given the pose sequence, we first divide it into four parts: body, right hand, left hand, and face. In con-trast to previous methods where entire human poses are jointly processed, we utilize part-specific PET to encode the motion context of each part. As each PET takes a pose sequence of a single part, it can learn more detailed mo-tion context, which is pivotal for sign language recognition.
Then, the encoded features from each part are merged by
WET. While PET only considers the intra-part motion con-text, WET covers the entire part. For the effective fusion of the inter-part and intra-part motion context, we alter-nately combined PET and WET inspired by spatio-temporal
Transformers [2,51]. As a result, our proposed P3D has out-performed the previous state-of-the-art [3, 50] on WLASL.
Our ablation study on layer design supports part-wise mo-tion context learning benefits SLR performance. Espe-cially, we observed that solely forming the model with WET (i.e., without part-wise motion context) degrades the per-formance, thus directly induces that our PET contributed to improving the accuracy.
We alleviate the second limitation of previous methods through the ensemble of 2D and 3D poses.
In terms of 3D pose, we utilize both 3D positional pose and 3D rota-tional pose (e.g., SMPL-X [41] pose parameters). The 3D positional pose complements depth information that the 2D pose does not contain, and the 3D rotational pose provides rich motion context on the kinematic chain. Our experiment shows that our P3D outperforms the previous methods by a large margin with using identical inputs; our model shows the superior result from only 2D pose, and also from en-semble of 2D and 3D pose. Moreover, our framework is the first pose-based method that recorded a comparable score or even outperformed the RGB-based methods. We claim our main contributions as follows:
• We proposed P3D, the human part-wise motion con-text learning framework for sign language recognition.
Our experiment supports that part-wise learning highly
benefits recognition performance. As a result, our pro-posed P3D outperformed the previous SOTA on the
WLASL benchmark.
• We employ the pose ensemble by joint-wise pose con-catenation in order to exploit both 2D and 3D poses.
We observed significant performance gain via pose en-semble, and P3D outperformed the previous methods by a large margin by jointly using 2D and 3D poses. 2.