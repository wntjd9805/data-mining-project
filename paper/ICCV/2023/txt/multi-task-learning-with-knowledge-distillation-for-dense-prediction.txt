Abstract
While multi-task learning (MTL) has become an attrac-tive topic, its training usually poses more difﬁculties than the single-task case. How to successfully apply knowl-edge distillation into MTL to improve training efﬁciency and model performance is still a challenging problem. In this paper, we introduce a new knowledge distillation pro-cedure with an alternative match for MTL of dense predic-tion based on two simple design principles. First, for mem-ory and training efﬁciency, we use a single strong multi-task model as a teacher during training instead of multiple teachers, as widely adopted in existing studies. Second, we employ a less sensitive Cauchy-Schwarz (CS) divergence instead of the Kullback–Leibler (KL) divergence and pro-pose a CS distillation loss accordingly. With the less sen-sitive divergence, our knowledge distillation with an alter-native match is applied for capturing inter-task and intra-task information between the teacher model and the student model of each task, thereby learning more ”dark knowl-edge” for effective distillation. We conducted extensive ex-periments on dense prediction datasets, including NYUD-v2 and PASCAL-Context, for multiple vision tasks, such as se-mantic segmentation, human parts segmentation, depth es-timation, surface normal estimation, and boundary detec-tion. The results show that our proposed method decidedly improves model performance and the practical inference ef-ﬁciency. 1.

Introduction
Multi-task learning (MTL) has become an increasingly popular approach in the ﬁeld of computer vision, where the objective is to train a single model to perform multiple tasks simultaneously. MTL can provide several advantages over traditional single-task learning, including improved ef-ﬁciency and generalization. First, the shared feature repre-*Corresponding author. sentations can be learned more efﬁciently than task-speciﬁc representations, reducing the overall training time com-pared to training multiple models independently. Second, the shared feature representations learned across tasks can capture more generalizable information in the data, leading to improved performance on all tasks. For those reasons, the MTL approach has been used extensively in various ma-chine learning problems, such as natural language process-ing [11], computer vision [3] and speech recognition [4].
Speciﬁcally, in this paper, we focus on dense (pixel-wise) prediction vision tasks [49, 15, 23, 41, 20, 21, 29, 48], such as semantic segmentation, instance segmentation, depth es-timation, surface normal estimation, saliency estimation, object detection and boundary detection from images.
A march of works [32, 46, 9, 3, 52, 2, 56, 53] aims to develop novel MTL architectures and construct efﬁ-cient shared representation in the multi-taskdense predic-tion ﬁeld, which leverages the encoder-decoder architec-ture. In these frameworks, an encoder is considered to gen-erate a shared feature and then use a decoder to perform multi-task of dense predictions. In the pursuit of better per-formance, current MTL models are often designed to be deeper and wider, resulting in increasingly larger models.
[3, 56] and [53, 24] demonstrate that better performance can be obtained by utilizing a larger backbone network. How-ever, such a heavy model can be more demanding for com-putation and storage. It has been challenging to design an effective MTL framework that can learn these tasks efﬁ-ciently. In this paper, we pose and study the question, how can the knowledge from a large size MTL model be trans-ferred to a small MTL model without increasing its size?
Recently, knowledge distillation (KD) [13] has been ex-plored as a method to improve the MTL of dense prediction tasks, such as semantic segmentation, depth estimation, and surface normal prediction. Some studies [30, 12, 16, 1] leverage the unique logit of each task to provide task-speciﬁc information to the student model during knowl-edge transfer. The essence of knowledge distillation lies
in translating knowledge from the teacher model (large model) to the student model (small model) by mimicking the teacher model’s outputs. Typically, knowledge distilla-tion methods use the logits matching by Kullback–Leibler (KL) divergence [13] between the probability distributions of the teacher and the probability distributions of the stu-dent. In this way, during training, the student model can be guided by more valuable information signals from the teacher model, therefore, is expected to have a more promis-ing performance than training alone. This approach has shown promising results in improving model performance and generalization and speeding up convergence in MTL.
For instance, We found that some works [22, 35, 12, 30, 17] attempt to use knowledge distillation to transfer the knowl-edge from teacher to student for multiple vision tasks in
MTL. We, in particular, identify two technical challenges. 1) Most previous approaches [22, 35, 17] must train a task-speciﬁc model for each task, then load the trained task-speciﬁc models. 2) Exact logit matching of teacher and student predictions with KL divergence can interfere with the training of the student model and is sensitive to outliers, leading to less effective knowledge distillation.
To address these challenges, we explore knowledge dis-tillation in multi-task learning of dense prediction tasks. We present a novel framework that leverages task-speciﬁc guid-ance to enable effective knowledge transfer. In Figure 1, we show the difference between the existing and our methods.
Since using multiple single-task models as teacher models would require an inordinate amount of memory, we only load a single multi-task model as a teacher model instead of loading multiple teacher models. We opt for one strong teacher model to reduce computational and memory costs instead of using multiple teacher models that perform a load of each task separately.
We take inspiration from mathematical statistics meth-ods from other domains to reduce the uncertainty in stu-dents’ prediction when using KL divergence (see Figure 1).
We propose a novel knowledge distillation with Cauchy-Schwarz (CS) divergence to replace the KL divergence. In addition, during the logit matching, inter-task and intra-task information are transferred from teacher to student. Speciﬁ-cally, we gather the corresponding predicted probabilities in a batch for all tasks, then transfer the inter-task information from teacher to student. For each task, we gather the cor-responding predicted probabilities of all classes in a batch, then transfer the intra-task information from teacher to stu-dent. Our proposed knowledge distillation with the alterna-tive match is less sensitive to small probabilities and can account for uncertainty in the student model predictions.
To further close the computational and memory cost gap, we only use one multi-task teacher model during training.
This MTL knowledge distillation procedure provides mul-tiple training settings.
Multi-task
Student (cid:2320)S mt
Multi-task
Student (cid:2320)S mt
Transmitted 
Knowledge
KL Divergence
Transmitted 
Knowledge
CS Divergence (cid:2320)T st (cid:2320)T st
··· (cid:2320)T st (cid:2320)T mt
Task 1 
Teacher 
Task 2 
Teacher
···
Task n 
Teacher
Multi-task 
Teacher (a) Existing method (b) Our method
Figure 1: Difference between existing KD methods and our KD method. F T st means the single-task (st) teacher (T ) model output.
F T mt means the multi-task (mt) student (S) model output. n denotes the num-ber of tasks. mt means the multi-task (mt) teacher model output. F S
Through extensive experiments on several publicly available dense prediction datasets (i.e., NYUD-v2 and
PASCAL-Context), we compare our results with state-of-the-art methods to demonstrate the effectiveness of our framework.
In summary, our work makes the following contribu-tions:
• We introduce a new procedure based on knowledge distillation with an alternative match named KDAM, which leverages inter-task and intra-task information.
It is less sensitive to small probabilities and can ac-count for uncertainty in the student model predictions.
• Our new distillation procedure aims at reproducing computational and memory costs by loading a strong multi-task model as a teacher to guide student learning.
• We conduct experiments using our KDAM on two
MTL of dense prediction datasets, showing the superi-ority under different experiment settings. 2.