Abstract
In recent years, prompt tuning has proven effective in adapting pre-trained vision-language models to down-stream tasks. These methods aim to adapt the pre-trained models by introducing learnable prompts while keeping pre-trained weights frozen. However, learnable prompts can affect the internal representation within the self-attention module, which may negatively impact performance vari-ance and generalization, especially in data-deficient set-tings. To address these issues, we propose a novel ap-proach, Read-only Prompt Optimization (RPO). RPO lever-ages masked attention to prevent the internal representa-tion shift in the pre-trained model. Further, to facilitate the optimization of RPO, the read-only prompts are ini-tialized based on special tokens of the pre-trained model.
Our extensive experiments demonstrate that RPO outper-forms CLIP and CoCoOp in base-to-new generalization and domain generalization while displaying better robust-ness. Also, the proposed method achieves better generaliza-tion on extremely data-deficient settings, while improving parameter efficiency and computational overhead. Code is available at https://github.com/mlvlab/RPO. 1.

Introduction
Vision-language models like CLIP [6], ALIGN [24], and
FILIP [50] have achieved excellent performance in vari-ous vision-language tasks. Since vision-language models are supervised by natural language based on the contrastive learning objective, by placing the class name in a textual template (e.g.,“A photo of a [CLASS]”), vision-language models can effectively classify images in open-vocabulary settings [6].
Recent works have explored the adaptation of these vision-language models on downstream tasks [19]. How-*Equal contribution.
†Corresponding author.
Figure 1: Variance of CoCoOp, CoOp, and linear prob-ing. Linear probing, which does not shift the pre-trained representation, shows lower variance in performance com-pared with prompt learning methods such as CoOp and Co-CoOp. ever, unlike small pre-trained models, large-scale archi-tectures (e.g., CLIP) are difficult to fine-tune, since it is inefficient, resource-intensive, and possibly damaging to the good representations learned during pre-training.
In
CLIP, prompt engineering is conducted to provide domain-specific context to downstream tasks (e.g., “A photo of a
[CLASS], a type of car”) [6]. However, this means that the prompt has to be chosen manually, based on trial and error.
To mitigate this issue, Context Optimization (CoOp) [33] suggests automating prompt engineering on CLIP, replac-ing the context words in natural language-based prompts with learnable vectors. Conditional Context Optimization (CoCoOp) [31] extended CoOp with an image-conditional prompt, generated by an additional neural network, to im-prove generalization.
Although these existing methods are proposed to avoid adversely affecting the learned parameters of the pre-trained model during prompt learning, they still affect the model’s hidden representation through the attention mechanism, which we call the internal representation shift. We visu-alize this process of representation shift in Figure 2a. As
(a) Conventional Prompt Tuning (b) Linear Probing (c) Read-only Prompt Optimization
Figure 2: Illustration of methods for model adaptation and RPO. (a) As denoted by ⇔, token features and prompt features can see each other in conventional prompt tuning methods. Although the weight of the model has been frozen, the internal representations of pre-trained CLIP are increasingly shifted by the newly introduced learnable prompts through the self-attention mechanism. (b) In linear probing, internal representations as well as pre-trained parameters are frozen. The linear layer on top of the model is trained for model adaptation. (c) As denoted by ⇒, only the prompts can read token features and not the other way around in our method, RPO. This keeps token features frozen and unaffected by introduced prompts while our read-only prompts only read useful information from token features. tokens are processed through transformer [55] layers, the internal representations of the pre-trained model are largely changed by the learnable prompts. This can be beneficial, as it allows the model to better adapt to the downstream task.
However, as shown in Figure 1, this shift has the potential to negatively impact the robustness and generalization of the model in data-deficient settings. On the other hand, lin-ear probing has no internal representation shift, as shown in
Figure 2b, but the linear layer introduces parameter ineffi-ciency.
To inspect how representation shift influences model variance in data-deficient settings, we conduct a prelimi-nary experiment with linear probing CLIP, which does not change the internal representation of pre-trained CLIP. We train the model with 10 random few-shot training data split on the FGVCAircraft dataset with the 16-shot learning set-ting and visualize the variance of performance.
Interest-ingly, as shown in Figure 1, we observed that linear prob-ing significantly lowers variance compared to CoOp and
CoCoOp, even though it requires more training parameters (262K) compared to CoOp (2K) and CoCoOp (35K). This result shows that internal representation shifts induced by training with deficient data may result in high variance. At the same time, as CoOp empirically showed, linear probing sometimes shows a lack of generalizability in domain-shift tasks, and the amount of its additional parameters is unde-sirable.
Motivated by this observation, we propose Read-only
Prompt Optimization (RPO) that learns read-only prompts as shown in Figure 2c. RPO prevents representation shift during adaptation while being parameter-efficient, leading to a more robust and generalizable adaptation.
Our contributions can be summarized as follows:
• We propose Read-only Prompt Optimization (RPO), which allows prompts only to read information from the attention-based interactions of a pre-trained vision-language model, thereby preventing the internal repre-sentation shift.
• We develop a simple yet effective initialization method for our read-only prompts, leveraging the special token embeddings of the pre-trained CLIP vision-language model.
• Our extensive experiments and analyses demonstrate the generalization of RPO on domain and label shift in few-shot adaptation settings, achieving the best per-formance in 9 benchmarks on base to new generaliza-tion and in 4 benchmarks on domain generalization, at the same time reducing variance depending on the few-shot sample.
2.