Abstract
Recent advances in diffusion models have set an im-pressive milestone in many generation tasks, and trend-ing works such as DALL-E2, Imagen, and Stable Diffu-sion have attracted great interest. Despite the rapid land-scape changes, recent new approaches focus on extensions and performance rather than capacity, thus requiring sep-arate models for separate tasks. In this work, we expand the existing single-flow diffusion pipeline into a multi-task multimodal network, dubbed Versatile Diffusion (VD), that handles multiple flows of text-to-image, image-to-text, and variations in one unified model. The pipeline design of
VD instantiates a unified multi-flow diffusion framework, consisting of sharable and swappable layer modules that enable the crossmodal generality beyond images and text.
Through extensive experiments, we demonstrate that VD successfully achieves the following: a) VD outperforms the baseline approaches and handles all its base tasks with competitive quality; b) VD enables novel extensions such as disentanglement of style and semantics, dual- and multi-context blending, etc.; c) The success of our multi-flow multimodal framework over images and text may inspire further diffusion-based universal AI research. Our code and models are open-sourced at https://github.com/SHI-Labs/Versatile-Diffusion. 1.

Introduction
Multi-modality is the “crown jewel” for achieving uni-versal AI. With the attributes of deep learning, methods designed for traditional tasks such as classification, detec-tion, segmentation, etc., have reached near-human level
accuracy. On top of them, multimodal research such as
[19, 37, 3, 31] primarily focused on discriminative tasks of jointly recognizing, matching, or understanding multi-modal data. Nevertheless, research on multimodal genera-tive models remains scarce. Previously, the best-performing generative vision models, generative adversarial networks (GAN) [34, 7, 35] merely focus on specific domains (i.e. faces [35, 10, 94], fonts [99, 45], natural scenes [75, 51], etc.); and on specific tasks (inpainting [84, 103, 98], super-resolution [48], image-to-image translation [30, 105], etc.).
The recent success of diffusion models [28, 78, 63, 70, 67] has brought new horizons. Diffusion models are likelihood-based models that gradually restore image con-tents from Gaussian corruptions.
It has proved to be ef-fective in bridging modalities and tasks, for instance, un-conditional generation [28, 78, 16], density estimation [39], super-resolution [71], and text-to-image generation [56, 63, 70, 67]. The success of diffusion models can be at-tributed to several aspects. Firstly, their training objec-tives lead to a more robust training procedure than other approaches like GANs. The iterative refinement inference procedure also expands the model capability at the cost of more running time. Besides, the competitive performance of recent diffusion models such as DALL-E2 [63], Ima-gen [70], and Stable Diffusion [67] benefits from the re-markable data collection such as LAION [74], CC12M [11],
COYO [9], etc. The disadvantages of earlier diffusion mod-els, such as the data hunger and high inference costs, are gradually alleviated by more efficient structures and sched-ulers [78, 43, 73, 29, 67]. Diffusion-based text-to-image methods [63, 70, 67] arguably set new state-of-the-art for multi-modal generative AI. However, those works by far al-most exclusively hinge on single-flow diffusion pipelines (illustrated in Section 3); and meanwhile, most of them are trained and evaluated on a single specialized generation task (e.g., text to image) despite being cross-modality.
What is the next move forward, then? We believe in the central role of multimodal, multi-task models in uni-versal AI, and we consider diffusion models to be a promis-ing workhorse to enable so. To fulfill our goal, we pro-posed Versatile Diffusion (VD) that comprehensively solves text, images, and variations within one unified generative model. The key underlying technique is a novel multi-flow diffusion framework, that generalizes existing single-flow diffusion pipelines to handle multiple modalities and tasks simultaneously while effectively sharing information across them. Thanks to the larger capacity as well as cap-turing crossmodal semantics, VD not only performs well on the aforementioned supported tasks but notably derives many new capabilities including semantic-style disentan-glement, cross-modal dual context or multi-context gener-ation (blending), leading to remarkable advances of empir-ical performance for multi-modal generative AI. Our main contributions are summarized in the following:
• We introduce Versatile Diffusion (VD), a multimodal, multi-task diffusion network that adopts a novel gener-alized multi-flow pipeline, unlike existing single-flow diffusion models.
• VD solves multiple modalities and tasks in one uni-fied model, including image generation (text-to-image, image-variation), and text generation (image-to-text, text-variation). Through comprehensive experiments, we show that VD outperforms the baselines via scores and quality. For example, VD’s high-quality text-to-image and image-variation results demonstrate that it indeed better captures the context semantics.
• The unique multi-flow multimodal property of VD en-ables more novel derivative tasks, that may further fa-cilitate downstream users engaged in this technology, including the semantic-style disentanglement, dual-context and multi-context blending, etc. 2.