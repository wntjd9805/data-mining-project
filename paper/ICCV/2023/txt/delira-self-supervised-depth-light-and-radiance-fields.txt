Abstract
Differentiable volumetric rendering is a powerful paradigm for 3D reconstruction and novel view synthesis.
However, standard volume rendering approaches struggle with degenerate geometries in the case of limited viewpoint diversity, a common scenario in robotics applications. In this work, we propose to use the multi-view photometric ob-jective from the self-supervised depth estimation literature as a geometric regularizer for volumetric rendering, sig-nificantly improving novel view synthesis without requiring additional information. Building upon this insight, we ex-plore the explicit modeling of scene geometry using a gen-eralist Transformer, jointly learning a radiance field as well as depth and light fields with a set of shared latent codes.
We demonstrate that sharing geometric information across tasks is mutually beneficial, leading to improvements over single-task learning without an increase in network com-plexity. Our DeLiRa architecture achieves state-of-the-art results on the ScanNet benchmark, enabling high quality volumetric rendering as well as real-time novel view and depth synthesis in the limited viewpoint diversity setting.
Our project page is https://sites.google.com/view/tri-delira. 1.

Introduction
Inferring 3D geometry from 2D images is a cornerstone capability in computer vision and computer graphics.
In recent years, the state of the art has significantly advanced due to the development of neural fields [47], which param-eterize continuous functions in 3D space using neural net-works, and differentiable rendering [40, 23, 36], which en-ables learning these functions directly from images. How-ever, recovering 3D geometry from 2D information is an ill-posed problem: there is an inherent ambiguity of shape and the shape-radiance ambiguity [54]). These radiance (i.e. representations thus require a large number of diverse cam-era viewpoints in order to converge to the correct geome-try. Alternatively, methods that explicitly leverage geomet-ric priors at training time, via the self-supervised multi-view photometric objective, have achieved great success for tasks such as depth [6, 9, 45, 8], ego-motion [38, 37], camera ge-ometry [41, 4, 7], optical flow [10], and scene flow [10, 14].
In this work, we combine these two paradigms and in-troduce the multi-view photometric loss as a complement to the view synthesis objective. Specifically, we use depth inferred via volumetric rendering to warp images, with the photometric consistency between synthesized and original images serving as a self-supervisory regularizer to scene structure. We show through experiments that this explicit regularization facilitates the recovery of accurate geometry in the case of low viewpoint diversity, without requiring ad-ditional data. Because the multi-view photometric objective is unable to model view-dependent effects (since it assumes a Lambertian scene), we propose an attenuation schedule that gradually removes it from the optimization, and show that our learned scene geometry is stable, leading to further improvements in view and depth synthesis.
We take advantage of this accurate learned geometry and propose DeLiRa, an auto-decoder architecture inspired by [16] that jointly estimates Depth [12], Light [35], and
Radiance [23] fields. We maintain a shared latent repre-sentation across task-specific decoders, and show that this increases the expressiveness of learned features and is ben-eficial for all considered tasks, improving performance over single-task networks without additional complexity. Fur-thermore, we explore other synergies between these repre-sentations: volumetric predictions are used as pseudo-labels for the depth and light fields, improving viewpoint gener-alization; and depth field predictions are used as guidance for volumetric sampling, significantly improving efficiency without sacrificing performance.
To summarize, our contributions are as follows.
In our first contribution, we show that the multi-view pho-tometric objective is an effective regularization tool for volumetric rendering, as a way to mitigate the shape-radiance ambiguity. To further leverage this geometrically-consistent implicit representation, in our second contribu-tion we propose a novel architecture for the joint learn-ing of depth, light, and radiance fields, decoded from a set of shared latent codes. We show that jointly model-ing these three fields leads to improvements over single-task networks, without requiring additional complexity in the form of regularization or image-space priors. As a re-sult, our proposed method achieves state-of-the-art view synthesis and depth estimation results on the ScanNet benchmark, outperforming methods that require explicit supervision from ground truth or pre-trained networks. 2.