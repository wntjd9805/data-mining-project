Abstract
Neural implicit representations have recently demon-strated compelling results on dense Simultaneous Localiza-tion And Mapping (SLAM) but suffer from the accumula-tion of errors in camera tracking and distortion in the re-construction. Purposely, we present GO-SLAM, a deep-learning-based dense visual SLAM framework globally op-timizing poses and 3D reconstruction in real-time. Ro-bust pose estimation is at its core, supported by efficient loop closing and online full bundle adjustment, which op-timize per frame by utilizing the learned global geometry of the complete history of input frames. Simultaneously, we update the implicit and continuous surface represen-tation on-the-fly to ensure global consistency of 3D re-construction. Results on various synthetic and real-world datasets demonstrate that GO-SLAM outperforms state-of-the-art approaches at tracking robustness and reconstruc-tion accuracy. Furthermore, GO-SLAM is versatile and can run with monocular, stereo, and RGB-D input. 1.

Introduction
The demand for high-fidelity 3D object and scene re-constructions grows continuously in various fields, includ-ing robotics and augmented/virtual reality applications.
Thus, faithfully representing objects and scenes in three-dimensional space is paramount to modelling them as con-tinuous surfaces rather than discrete points. However, de-spite the significant advancements in 3D reconstruction techniques, obtaining high-quality representations in real-time without compromising accuracy and spatial resolution remains challenging. This fact is further demanding in on-line reconstruction scenarios, where handling camera mo-tions and achieving real-time performance are critical.
Dense visual Simultaneous Localization and Mapping (SLAM) systems [28, 45, 32, 13, 46] have been intro-duced recently, enabling real-time, dense indoor scene re-constructions using RGB-D sensors. In particular, Bundle-Fusion [13] is the first volumetric approach that focuses on globally consistent 3D reconstruction on large-scale scenes at a real-time rate. However, consumer depth sensors have a limited working range [32, 12] and could yield extremely noisy [27] measurements. This issues make the represen-tation mapped by RGB-D SLAM suffer from blurring or over-smoothed geometric details, degrading the accuracy of pose estimation and reconstruction.
In parallel, scene reconstruction from monocular imagery is emerging as a more convenient solution compared to RGB-D or LiDAR sensors. Camera sensors are lightweight, inexpensive, and represent the most straightforward configuration. Several deep-learning approaches [38, 4, 11, 51, 39, 32, 41] have advanced monocular 3D reconstruction. However, their sur-face representations – point cloud, surfel-based and volu-metric representations – lack flexibility at shape extraction and thus inhibit high-fidelity reconstruction.
More recently, the advent of Neural Radiance Fields (NeRFs) also impacted dense visual SLAM, offering pho-tometrically accurate 3D representations of the world.
The implicit representation yielded by continuous radiance fields allows for high-quality rendering of both visible and occluded regions, enabling extraction of the underlying shapes at arbitrary resolution. Recent [35, 19, 53] and con-current [20, 31, 9, 52] works demonstrate that NeRF-based visual SLAM can yield precise 3D reconstructions and cam-era pose estimation in small-scale scenes. However, due to the lack of global online optimization, such as loop closure (LC) and global bundle adjustment (BA), camera drift error accumulates as the number of processed frames grows, and the 3D reconstruction quickly collapses, as shown in Fig. 1.
Purposely, this paper introduces GO-SLAM, a deep-learning-based SLAM system featuring on-the-fly, globally consistent 3D reconstruction, facilitated by our robust cam-era tracking and real-time implicit surface updates. As real-world 3D scenes may be very complex, drifting cannot be completely avoided by only locally tracking camera motion, especially in the monocular camera setting, due to the lack of explicit depth measurements. In addition to the local reg-istration commonly performed by current SLAM systems, we present an efficient loop closing to correct trajectory in real-time, accompanied by an online full BA module to ac-tively optimize the 3D geometry of the complete keyframes history. In contrast to previous works performing BA or LC with sparse visual features [26, 6, 13, 11], our end-to-end global optimization procedure is naturally robust to chal-lenging regions, thanks to richer features and geometry cues (e.g., pixel-wise flow) learned by neural networks. Further-more, GO-SLAM implements instant mapping based on a neural implicit network with multi-resolution hash encod-ing [24]. Its compact, multiscale representation enables up-dating the 3D reconstruction at high-frequency according to newly-optimized camera poses and depths from our global optimization system, thus ensuring global consistency in the dense map and capturing local details. Our contributions can be resumed as follows:
• A novel deep-learning-based, real-time global pose op-timization system that considers the complete history of input frames and continuously aligns all poses.
• An efficient alignment strategy that enables instanta-neous loop closures and correction of global structure, being both memory and time efficient.
• An instant 3D implicit reconstruction approach, en-abling on-the-fly and continuous 3D model update with the latest global pose estimates. This strategy fa-cilitates real-time 3D reconstructions.
• The first deep-learning architecture for joint robust pose estimation and dense 3D reconstruction suited for any setup: monocular, stereo, or RGB-D cameras. 2.