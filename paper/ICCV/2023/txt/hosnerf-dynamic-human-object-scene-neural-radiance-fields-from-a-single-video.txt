Abstract
We introduce HOSNeRF, a novel 360° free-viewpoint rendering method that reconstructs neural radiance fields for dynamic human-object-scene from a single monocular in-the-wild video. Our method enables pausing the video at any frame and rendering all scene details (dynamic hu-mans, objects, and backgrounds) from arbitrary viewpoints.
The first challenge in this task is the complex object mo-tions in human-object interactions, which we tackle by in-troducing the new object bones into the conventional human skeleton hierarchy to effectively estimate large object defor-mations in our dynamic human-object model. The second challenge is that humans interact with different objects at different times, for which we introduce two new learnable object state embeddings that can be used as conditions for learning our human-object representation and scene rep-resentation, respectively. Extensive experiments show that
HOSNeRF significantly outperforms SOTA approaches on two challenging datasets by a large margin of 40% ∼ 50% in terms of LPIPS. The code, data, and compelling exam-ples of 360° free-viewpoint renderings from single videos: https://showlab.github.io/HOSNeRF. 1.

Introduction
Video reconstruction and free-viewpoint rendering of-fer innovative opportunities for creating immersive expe-riences, encompassing virtual reality, telepresence, meta-verse, and 3D animation production. While reconstructing videos has the potential to enhance user engagement and provide more realistic environments, it also poses signifi-cant challenges in terms of monocular viewpoints [55, 48] and complicated human-environment interactions [50, 14].
In recent years, remarkable progress has been made in
*Work is partially done during internship at ARC Lab, Tencent PCG.
†Corresponding Authors. novel view synthesis, particularly since the introduction of
Neural Radiance Fields (NeRF) [27]. While initially limited to reconstructing static 3D scenes based on multi-view im-ages, subsequent studies have proposed various approaches to address the challenge of dynamic view synthesis [34, 29, 30, 47, 20, 8]. NeRF-based techniques have evolved to ei-ther incorporate deformation fields that map dynamic fields to canonical NeRF spaces [34, 29, 30, 47], or model dy-namic scenes as 4D spatio-temporal radiance fields [20, 8].
While these approaches have shown promising results in dynamic view synthesis, they are limited to simple defor-mations. Another line of research is specifically designed for dynamic neural human modeling that relies on esti-mated human poses as a priori information [32, 50]. Re-cently, Neuman [14] combines pose-driven dynamic human models with static scene models for representing dynamic human-centric scenes.
However, none of the aforementioned techniques can accurately reconstruct challenging monocular videos with fast and complex human-object-scene motions and inter-actions, as shown in Fig. 1(d). This is due to two par-ticular challenges listed below. To tackle them, we intro-duce a novel method called Human-Object-Scene Neural
Radiance Fields (HOSNeRF). i) Complex object motions in human-object interac-tions. In contrast to the simple motions that can be mod-eled by general deformation modules [29, 30], the object motion during human-object interaction is often drastic and composed of various atomic motions (e.g., play tennis). To tackle this challenge, we propose new object bones that are attached to the human skeleton hierarchy to estimate human-object deformations in a coarse-to-fine manner for our dynamic human-object model. The object bones and underlying object linear blend skinning (object LBS) al-low for the accurate estimation of objects’ deformations through the relative transformations in the kinematic tree of the skeleton hierarchy.
Figure 1: Our HOSNeRF (b) takes a single monocular in-the-wild video (a) as input, and creates high-fidelity 360° free-viewpoint rendering of all scene details (dynamic human body, objects, and background) at any time (d). Our method enables rendering views with novel object poses and novel human poses as shown in (c), and produces high-fidelity dynamic novel view synthesis results at novel timesteps, with significant improvements over SOTA approaches as shown in (d). ii) Humans interact with different objects at different times. The above human-object model is designed for hu-mans interacting with the same object over time. But when the person puts down the current object or picks a new ob-ject, it is not clear how to dynamically remove/add such ob-jects in the static background model and the human-object model whose canonical space is static. To handle this chal-lenge, we introduce two new learnable object state embed-dings that can be used as conditions for learning our human-object representation and scene representation, respectively.
Finally, we systematically explore and identify effec-tive training objectives and strategies for our proposed
HOSNeRF, including deformation cycle consistency, opti-cal flow supervisions, and foreground-background render-ing. On two challenging datasets collected by ourselves and NeuMan [14], our HOSNeRF achieves high-fidelity dy-namic novel view synthesis results and enables pausing the monocular video at any time and rendering all scene details (dynamic humans, objects, and backgrounds) from arbitrary viewpoints, as shown in Fig. 1(d).
In summary, our main contributions are:
• We present a novel framework of HOSNeRF, the first work to achieve 360° free-viewpoint high-fidelity novel view synthesis for dynamic scenes with human-environment interactions from a single video.
• We propose the object bones and state-conditional rep-resentations to handle the non-rigid motions and inter-actions of humans, objects, and the environment.
• Extensive experiments show that HOSNeRF signifi-cantly outperforms SOTA approaches on two challeng-ing datasets by 40% ∼ 50% in terms of LPIPS. 2.