Abstract
This paper presents WALDO (WArping Layer-Decomposed Objects), a novel approach to the prediction of future video frames from past ones. Individual images are decomposed into multiple layers combining object masks and a small set of control points. The layer structure is shared across all frames in each video to build dense inter-frame connections. Complex scene motions are mod-eled by combining parametric geometric transformations associated with individual layers, and video synthesis is broken down into discovering the layers associated with past frames, predicting the corresponding transformations for upcoming ones and warping the associated object regions accordingly, and ﬁlling in the remaining image parts.
Extensive experiments on multiple benchmarks including urban videos (Cityscapes and KITTI) and videos featuring nonrigid motions (UCF-Sports and H3.6M), show that our method consistently outperforms the state of the art by a signiﬁcant margin in every case. Code, pretrained models, and video samples synthesized by our approach can be found in the project webpage.1 1.

Introduction
Predicting the future from a video stream is an important tool to make autonomous agents more robust and safe. In this paper, we are interested in the case where future frames are synthesized from a ﬁxed number of past ones. One possibility is to build on advanced image synthesis mod-els [20, 23, 44, 59] and adapt them to predict new frames conditioned on past ones [79]. Extending these already memory- and compute-intensive methods to our task may, however, lead to prohibitive costs due to the extra temporal dimension. Hence, the resolution of videos predicted by this approach is often limited [36, 95, 96]. Other works resort to compression [64, 82] to reduce computations [63, 97, 103],
*corresponding author: guillaume.le-moing@inria.fr 1url: https://16lemoing.github.io/waldo
T
T +10
T
T +10
Figure 1. WALDO synthesizes future frames by deforming grids of control points, automatically associated with different objects. at the potential cost of poorer temporal consistency [61].
Our work relies instead on semantic and motion cues ex-tracted from the past to model complex dynamics in high resolution and predict the future. Wu et al. [99] decom-pose scenes into objects/background, predict afﬁne trans-formations for the objects and non-afﬁne ones for the back-ground, and warp the last input frame to produce new ones.
Bei et al. [7] predict dense ﬂow maps for individual seman-tic regions. Geng et al. [29] augment classical frame recon-struction losses with ﬂow-based correspondences between pairs of frames. Wu et al. [100] build on a pretrained video frame interpolation model which they adapt to future video prediction. Contrary to [99], our model automatically dis-covers the object decomposition without explicit supervi-sion. Moreover, rather than directly predicting optical ﬂow at each pixel [7,99,100], we use thin-plate splines (TPS) [9] as a parametric model of per layer ﬂow for any pair of frames. This improves robustness since we predict future frames using all past ones as opposed to [7, 29, 99, 100]. In addition, TPS provide optical ﬂow at any resolution, and allow using a lower resolution for fast training while retain-ing good performance with high-resolution inputs at infer-ence. Lastly, a few time-dependent control points associ-ated with each object and the background are sufﬁcient to parameterize TPS. This compact yet expressive representa-tion of motion allows us to break down video synthesis into: (a) discovering the layers associated with past frames, (b) predicting the corresponding transformations for upcoming ones while handling complex motions [13] and modeling future uncertainty if needed [1,2], and (c) warping the asso-ciated object regions accordingly and ﬁlling in the remain-f l e h s
-e h t
-f f g n i s s e c o r p e r p g n i s s e c o r p e r
P
O
Segmentation maps (a) 
Layered video decomposition 
Flow maps
Input frames
Layer features (b) 
Future layer prediction
Control points
Layer masks
Future control points (c) 
Warping, inpainting and fusion
Future frames
Figure 2. Overview of WALDO. Given an input video sequence and associated semantic segmentation and optical ﬂow maps preprocessed by off-the-shelf models [14, 77], our approach breaks down video synthesis into (a) layered video decomposition: using semantic and motion cues to decompose the sequence into layers represented by both object masks and features, with spatial information in the form of a small set of control points, (b) future layer prediction: predicting the new position of these points in the target output frames, and (c) warping, inpainting and fusion: using the corresponding offset and a thin-plate spline deformation model to warp the input frames and object masks, merge the corresponding regions, and ﬁll in the empty image parts. Our model is trained, without explicit annotations, on a set of videos of T +K frames by using the ﬁrst T to predict the next K. At inference, a (potentially greater) number K of frames is predicted from T input ones by repeating (a), (b) and (c) in an autoregressive fashion if needed. ing image parts. By combining these three components, our approach (Figure 2) to predict future frames by WArping
Layer-Decomposed Objects (WALDO) from past ones sets a new state of the art on diverse benchmarks including urban scenes (Cityscapes [17] and KITTI [28]), and scenes featur-ing nonrigid motions (UCF-Sports [67] and H3.6M [41]).
Our main contributions are twofold:
• Much broader operating assumptions: Previous ap-proaches to video prediction that decompose individual frames into layers assume prior foreground/background knowledge [99] or reliable keypoint detection [88,106], and they do not allow the recovery of dense scene ﬂow at arbi-trary resolutions for arbitrary pairs of frames [1, 2, 7, 13, 29, 99, 100]. Our approach overcomes these limitations. Un-like [7, 13, 29, 99, 100], it also allows multiple predictions.
• Novelty: The main novelties of our approach to video pre-diction are (a) a layer decomposition algorithm that lever-ages a transformer architecture to exploit long-range de-pendencies between semantic and motion cues; (b) a low-parameter deformation model that allows the long-term pre-diction of sharp frames from a small set of adjustable con-trol points; and (c) the effective fusion of multiple predic-tions from past frames using state-of-the-art inpainting to handle (dis)occlusion. Some of these elements have been used separately in the past (e.g., [61, 104, 106]), but never, to the best of our knowledge, in an integrated setting.
These contributions are validated through extensive ex-periments on urban datasets (Cityscapes and KITTI), as well as scenes featuring nonrigid motions (UCF-Sports,
H3.6M), where our method outperforms the state of the art by a signiﬁcant margin in every case 2.