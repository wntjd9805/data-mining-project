Abstract 1.

Introduction
Virtual try-on tasks aim at synthesizing realistic try-on results by trying target clothes on humans. Most previ-ous works relied on the Thin Plate Spline or appearance flows to warp clothes to fit human body shapes. However, both approaches cannot handle complex warping, leading to over distortion or misalignment. Furthermore, there is a critical unaddressed challenge of adjusting clothing sizes for try-on. To tackle these issues, we propose a
Clothing-Oriented Transformation Try-On Network (COT-TON). COTTON leverages clothing structure with land-marks and segmentation to design a novel landmark-guided transformation for precisely deforming clothes, allowing for size adjustment during try-on. Additionally, to prop-erly remove the clothing region from the human image with-out losing significant human characteristics, we propose a clothing elimination policy based on both transformed clothes and human segmentation. This method enables users to try on clothes tucked-in or untucked while retaining more human characteristics. Both qualitative and quantita-tive results show that COTTON outperforms the state-of-the-art high-resolution virtual try-on approaches. All the code is available at https://github.com/cotton6/COTTON-size-does-matter.
Image-based virtual try-on replaces clothing items on a person with the desired ones, creating realistic try-on re-sults and lowering costs associated with on-model photos for the e-commerce industry1. Additionally, it enables cus-tomers to use virtual dressing rooms when shopping online, potentially enhancing the e-commerce experience and in-creasing conversion rates2. As online shopping becomes popular, the virtual try-on tasks have received more and more attention [2,4,5,9,13,17,23,28,29,33,35]. For exam-ple, to extend the virtual try-on resolution from low (256 × 192) to high (1024 × 768), the most obvious difficulty is the misalignment issue. [5] proposed the alignment-aware seg-ment normalization to remove the misleading information in the misaligned area. Additionally, [23] further performs appearance flow-based warping and segmentation map gen-eration simultaneously to tackle misalignment.
Despite the considerable progress made in previous works, there are still some challenges that have not been i) Handling adequately addressed, as outlined below. complex warping without misalignment: General image-*These authors contributed equally to this work. 1https://www.zmo.ai/aimodels/ 2https://www.revery.ai
based virtual try-on frameworks [5, 16, 21, 28, 31] typically employ a clothing deformation module, such as the Thin
Plate Spline (TPS) method, to align clothing images with the target human pose. However, research [6, 15, 23] has found that TPS may not effectively handle complex warp-ing when different garment regions require different de-formations.
In attempts to improve warping results, they have replaced TPS with dense appearance flows. However, when the transformation between the garment and corre-sponding body parts is significant (as illustrated in case II in
Fig. 4), the performance of both TPS (VITON-HD [5]) and flow-based (HR-VITON [23]) methods deteriorates drasti-cally. Hence, existing methods still can’t properly address the challenge of clothing deformation. ii) Adjusting cloth-ing sizes: Previous works predicted the try-on segmentation conditioned on the clothing image by only considering the shape of the clothes without the scale information. Thus, given a clothing image, it is impossible for previous works to change the clothing size. This limitation severely restricts the practical application of virtual try-on because people of-ten try on different sizes in the fitting room. iii) Appropri-ate clothing elimination policy: Previous works [5, 23, 28] rely on segmentation maps to eliminate the original cloth-ing region and utilize the remaining area as a guide for try-ing on target clothes. However, this method can lead to either excessive or insufficient removal of the original im-age. For instance, when prior works try on upper clothes, the lower part of the upper clothes remains intact, which limits the length of the upper clothes, as shown in Fig. 6.
The lower part of the upper clothing is forced to be tucked in, which reduces the model’s ability to generate complete upper clothes, thereby reducing the practicality of virtual try-on. Additionally, previous works remove the entire arm of input human images, which eliminates essential human characteristics such as tattoos and arm width that should be preserved in the final output, as illustrated in case I of Fig. 5.
Therefore, developing an appropriate clothing elimination strategy that preserves crucial information while removing the clothing region is a significant challenge.
To address these challenges, we propose a simple yet powerful approach called Clothing-Oriented Transforma-tion Try-On Network (COTTON). Specifically, COTTON first exploits geometric information with Clothing Land-mark Predictor and Clothing Segmentation Network to pre-dict clothing landmarks and segmentation masks respec-tively. To overcome the first challenge posed by complex warping, we propose the Landmark-guided Transformation that first separates clothes into sub-parts via the segmen-tation mask and then uses clothing landmarks to estimate homography matrices that fit these sub-parts to the target human pose. To solve the second challenge, we introduce a clothing landmarks adjustment approach that allows users to change clothing sizes. Adjustment of landmarks will al-ter the homography matrices and then cause clothing size changes, as demonstrated in Fig. 1, which remarkably en-hances the practicality of virtual try-on. To address the third challenge, we employ the transformed clothing images to identify and remove the areas that would be covered by the clothes. This enables our proposed Clothing Elimination
Policy to effectively eliminate the clothing region while pre-serving important details and offer flexibility in tucking the clothes or not, as shown in Fig. 6. Extensive experiments show that COTTON achieves superior results than state-of-the-arts both quantitatively and qualitatively. We summa-rize our contributions as follows:
• We propose a Clothing-Oriented Transformation Try-On Network (COTTON), which improves clothing transformation quality and addresses complex warping misalignments by leveraging clothing geometry.
• We introduce adjustable clothing landmarks to provide clothing size information based only on images. To the best of our knowledge, this is the first work en-abling 2D-based virtual try-on with different clothing sizes for approaching real-world try-on.
• To preserve critical information, we propose a Cloth-ing Elimination Policy to properly remove clothing in-formation while retaining valuable human characteris-tics, and offer flexibility in tucking the clothes or not.
• Extensive experiments on the Dress Code dataset [28] show that our model significantly outperforms SOTAs, e.g., at least 41.1% improvement in terms of FID. 2.