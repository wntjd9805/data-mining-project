Abstract
Training data for video segmentation are expensive to annotate. This impedes extensions of end-to-end al-gorithms to new video segmentation tasks, especially in
To ‘track anything’ without large-vocabulary settings. training on video data for every individual task, we de-velop a decoupled video segmentation approach (DEVA), composed of task-specific image-level segmentation and class/task-agnostic bi-directional temporal propagation.
Due to this design, we only need an image-level model for the target task (which is cheaper to train) and a universal temporal propagation model which is trained once and generalizes across tasks. To effectively com-bine these two modules, we use bi-directional propaga-tion for (semi-)online fusion of segmentation hypotheses from different frames to generate a coherent segmenta-tion. We show that this decoupled formulation compares favorably to end-to-end approaches in several data-scarce tasks including large-vocabulary video panoptic segmen-tation, open-world video segmentation, referring video segmentation, and unsupervised video object segmenta-tion. Code is available at: hkchengrex.github.io/
Tracking-Anything-with-DEVA. 1.

Introduction
Video segmentation aims to segment and associate ob-jects in a video. It is a fundamental task in computer vision and is crucial for many video understanding applications.
Most existing video segmentation approaches train end-to-end video-level networks on annotated video datasets.
They have made significant strides on common benchmarks like YouTube-VIS [61] and Cityscape-VPS [24]. However,
)
% ( e n i l e s a b r e v o t n e m e v o r p m i e v i t a l e
R 70 60 50 40 30 20
All classes
Common classes
Rare classes 10 25 50
Percentage of training data in the target domain used (%) 100
Figure 2. We plot relative VPQ increase of our decoupled ap-proach over the end-to-end baseline when we vary the training data in the target domain (VIPSeg [39]). Common/rare classes are the top/bottom 50% most annotated object category in the training set. Our improvement is most significant (>60%) in rare classes when there is a small amount of training data. This is because our decoupling allows the use of external class-agnostic temporal propagation data – data that cannot be used by existing end-to-end baselines. Details in Section 4.5.1. these datasets have small vocabularies: YouTube-VIS con-tains 40 object categories, and Cityscape-VPS only has 19.
It is questionable whether recent end-to-end paradigms are scalable to large-vocabulary, or even open-world video data.
A recent larger vocabulary (124 classes) video segmentation dataset, VIPSeg [39], has been shown to be more difficult – using the same backbone, a recent method [30] achieves only 26.1 VPQ compared with 57.8 VPQ on Cityscape-VPS. To the best of our knowledge, recent video segmen-tation methods [2, 35] developed for the open-world set-ting (e.g., BURST [2]) are not end-to-end and are based on tracking of per-frame segmentation – further highlight-ing the difficulty of end-to-end training on large-vocabulary datasets. As the number of classes and scenarios in the dataset increases, it becomes more challenging to train and develop end-to-end video models to jointly solve segmenta-tion and association, especially if annotations are scarce.
In this work, we aim to reduce reliance on the amount of target training data by leveraging external data outside of the target domain. For this, we propose to study de-coupled video segmentation, which combines task-specific image-level segmentation and task-agnostic temporal prop-agation. Due to this design, we only need an image-level model for the target task (which is cheaper) and a univer-sal temporal propagation model which is trained once and generalizes across tasks. Universal promptable image seg-mentation models like ‘segment anything’ (SAM) [27] have recently become available and serve as excellent candidates for the image-level model in a ‘track anything’ pipeline –
Figure 1 shows some promising results of our integration with these methods.
Researchers have studied decoupled formulations be-fore, as ‘tracking-by-detection’ [23, 51, 3]. However, these approaches often consider image-level detections im-mutable, while the temporal model only associates detected objects. This formulation depends heavily on the quality of per-image detections and is sensitive to image-level errors.
In contrast, we develop a (semi-)online bi-directional propagation algorithm to 1) denoise image-level segmen-tation with in-clip consensus (Section 3.2.1), and 2) com-bine results from temporal propagation and in-clip consen-sus gracefully (Section 3.2.2). This bi-directional propaga-tion allows temporally more coherent and potentially better results than those of an image-level model (see Figure 2).
We do not aim to replace end-to-end video approaches.
Indeed, we emphasize that specialized frameworks on video tasks with sufficient video-level training data (e.g.,
YouTubeVIS [61]) outperform the developed method. In-stead, we show that our decoupled approach acts as a strong baseline when an image model is available but video data is scarce. This is in spirit similar to pretraining of large lan-guage models [46]: a task-agnostic understanding of natu-ral language is available before being finetuned on specific tasks – in our case, we learn propagation of segmentations of class-agnostic objects in videos via a temporal propa-gation module and make technical strides in applying this knowledge to specific tasks. The proposed decoupled ap-proach transfers well to large-scale or open-world datasets, and achieves state-of-the-art results in large-scale video panoptic segmentation (VIPSeg [39]) and open-world video segmentation (BURST [2]). It also performs competitively on referring video segmentation (Ref-YouTubeVOS [49],
Ref-DAVIS [22]) and unsupervised video object segmen-tation (DAVIS-16/17[5]) without end-to-end training.
To summarize:
• We propose using decoupled video segmentation that leverages external data, which allows it to general-ize better to target tasks with limited annotations than end-to-end video approaches and allows us to seam-lessly incorporate existing universal image segmenta-tion models like SAM [27].
• We develop bi-directional propagation that denoises image segmentations and merges image segmentations with temporally propagated segmentations gracefully.
• We empirically show that our approach achieves favor-able results in several important tasks including large-scale video panoptic segmentation, open-world video segmentation, referring video segmentation, and unsu-pervised video object segmentation. 2.