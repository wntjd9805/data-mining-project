Abstract
With the popularity of implicit neural representations, or neural radiance fields (NeRF), there is a pressing need for editing methods to interact with the implicit 3D mod-els for tasks like post-processing reconstructed scenes and 3D content creation. While previous works have explored
NeRF editing from various perspectives, they are restricted in editing flexibility, quality, and speed, failing to offer di-rect editing response and instant preview. The key chal-lenge is to conceive a locally editable neural representa-tion that can directly reflect the editing instructions and up-date instantly. To bridge the gap, we propose a new interac-tive editing method and system for implicit representations, called Seal-3D 1 , which allows users to edit NeRF models in a pixel-level and free manner with a wide range of NeRF-∗Equal contribution.
†Corresponding author.
Project page: https://windingwind.github.io/seal-3d/ 1“Seal” derived from the name of rubber stamp in Adobe Photoshop. like backbone and preview the editing effects instantly. To achieve the effects, the challenges are addressed by our pro-posed proxy function mapping the editing instructions to the original space of NeRF models in the teacher model and a two-stage training strategy for the student model with local pretraining and global finetuning. A NeRF editing system is built to showcase various editing types. Our system can achieve compelling editing effects with an interactive speed of about 1 second. 1.

Introduction
Implicit neural representations, e.g. neural radiance fields (NeRF) [24], have gained increasing attention as novel 3D representations with neural networks to model a 3D scene. Benefiting from the high reconstruction accu-racy and rendering quality with relatively low memory con-sumption, NeRF and its variations [50, 3, 33, 26, 4, 45, 41] have demonstrated great potential in many 3D applica-tions like 3D reconstruction, novel view synthesis, and Vir-1
tual/Augmented Reality.
With the popularity of the new implicit representations and an increasing number of implicit 3D models, there is a pressing demand for human-friendly editing tools to in-teract with these 3D models. Editing with implicit neural representations is a fundamental technique required to fully empower the representation. Objects reconstructed from the real world are likely to contain artifacts due to the noise of captured data and the limitations of the reconstruction algo-rithms. In a typical 3D scanning pipeline, manual correc-tion and refinement to remove artifacts are common stages.
On the other hand, in 3D content creation applications like 3D games, animations, and filming, artists usually need to create new content based on existing 3D models.
Prior works have made attempts to edit 3D scenes rep-resented by NeRF, including object segmentation [20, 44] , object removal [19] , appearance editing [14, 27, 22] , and object blending [7], etc. These existing NeRF editing meth-ods mainly focus on coarse-grained object-level editing and the convergence speed can not meet the demands of inter-active editing. Some recent methods [48, 5] transform the editing of NeRF into mesh editing by introducing a mesh as an edit proxy. This requires the user to operate on an additional meshing tool, which limits interactivity and user-friendliness. To the best of our knowledge, there are no existing methods that are able to support interactive pixel-level editing of neural radiance fields with fast converging speed, which is mainly due to the challenges discussed be-low.
Unlike existing explicit 3D representations e.g. point cloud, textured mesh, and occupancy volume, which store the explicit geometry structure of objects and scenes, im-plicit representations use neural networks to query features of a 3D scene including geometry and color. Existing 3D editing methods, taking the mesh-based representation as an example, can change object geometry by displacing ver-tices corresponding to target object surface areas and object textures. Without explicit explainable correspondence be-tween the visual effects and the underlying representations, editing the implicit 3D models is indirect and challenging.
Further, it is difficult to locate implicit network parameters in local areas of the scene, meaning that adaptations of the network parameters may lead to undesired global changes.
This results in more challenges for fine-grained editing.
To bridge the gap, in this paper, we propose an inter-active pixel-level editing method and system for implicit neural representations for 3D scenes, dubbed Seal-3D. The name is borrowed from the popular 2D image editing soft-ware Adobe PhotoShop [1], as its seal tool provides similar editing operations. As shown in Fig. 1, the editing system consists of five types of editing as examples: 1) Bounding box tool. It transforms and scales things inside a bounding box, like a copy-paste operation. 2) Brushing tool. It paints specified color on the selected zone and can increase or de-crease the surface height, like an oil paint brush or graver. 3) Anchor tool. It allows the user to freely move a control point and affect its neighbor space according to the user in-put. 4) Color tool. It edits the color of the object surfaces.
To achieve the interactive NeRF editing effects, we ad-dress the challenges of implicit representations discussed above. First, to establish the correspondence between the explicit editing instructions to the update of implicit net-work parameters, we propose a proxy function that maps the target 3D space (determined by the user edit instructions from an interactive GUI) to the original 3D scene space, and a teacher-student distillation strategy to update the parame-ters with the corresponding content supervision acquired by the proxy function from the original scenes. Second, to en-able local editing, i.e. mitigating the influence of the local editing effect on the global 3D scenes under the non-local implicit representations, we propose a two-stage training process: a pretraining stage of updating only the positional embedding grids with local losses for editing areas while freezing the subsequent MLP decoder to prevent global de-generation, and a finetuning stage of updating both the em-bedding grids and the MLP decoder with global photomet-ric losses. With this design, the pretraining stage updates lo-cal editing features and the finetuning stage blends the local editing areas with global structures and colors of unedited space to achieve view consistency. This design has the ben-efit of an instant preview of the editing: the pretraining can converge very fast and presents local editing effects within approximately 1 second only.
In summary, our contributions are as follows:
• We propose the first interactive pixel-level editing method and system for neural radiance fields, which exemplifies fine-grained multiple types of editing tools, including geometry (bounding box tool, brush tool, and anchor tool) and color edits;
• A proxy function is proposed to establish the cor-respondence between the explicit editing instructions and the update of implicit network parameters and a teacher-student distillation strategy is proposed to up-date the parameters;
• A two-stage training strategy is proposed to enable in-stant preview of local fine-grained editing without con-taminating the global 3D scenes. 2.