Abstract
Depth-aware panoptic segmentation is an emerging topic in computer vision which combines semantic and ge-ometric understanding for more robust scene interpreta-tion. Recent works pursue unified frameworks to tackle this challenge but mostly still treat it as two individual learn-ing tasks, which limits their potential for exploring cross-domain information. We propose a deeply unified frame-work for depth-aware panoptic segmentation, which per-forms joint segmentation and depth estimation both in a per-segment manner with identical object queries. To narrow the gap between the two tasks, we further design a geomet-ric query enhancement method, which is able to integrate scene geometry into object queries using latent represen-tations. In addition, we propose a bi-directional guidance learning approach to facilitate cross-task feature learning by taking advantage of their mutual relations. Our method sets the new state of the art for depth-aware panoptic seg-mentation on both Cityscapes-DVPS and SemKITTI-DVPS datasets. Moreover, our guidance learning approach is shown to deliver performance improvement even under in-complete supervision labels. Code and models are avail-able at https://github.com/jwh97nn/DeepDPS. 1.

Introduction
Scene understanding plays a crucial role in autonomous driving perception systems, but relying solely on 2D repre-sentations falls short for advanced systems. To address this limitation, Depth-aware Panoptic Segmentation (DPS) [44] has been proposed as a novel approach for geometric scene understanding, which enables the creation of 3D instance-*Corresponding author
Figure 1: Pipeline comparison of prior work [55] (left) and ours (right). We integrate unified queries with geometric enhancement and mutual learning from cross-modality su-pervision, towards a deeper unified manner. level semantic labels from a single image by means of in-verse projection. More precisely, the simplified problem can be decomposed into two sub-tasks: panoptic segmenta-tion and monocular depth estimation.
Early methods [44, 46] tackle this task by simply attach-ing a dense depth prediction head on top of the off-the-shelf panoptic segmentation model [7]. However, these meth-ods are intuitively sub-optimal, because the separate task-oriented head design treats these two sub-tasks indepen-dently and ignores their mutual relation. Recent methods
[40, 55] propose unified architectures that output both pre-dictions in the same instance-wise manner, and utilize cor-responding task-specific kernels (or queries) to jointly pro-duce masks and depth maps for individual instances, which leverages the mutual benefits between semantic and depth information (shown in Fig. 1 left).
Despite recent efforts to unify the two sub-tasks, their learning processes remain largely separate. Specifically, they employ task-specific loss functions to guide individual predictions, which overlook the potential benefits of cross-domain knowledge learning. While some attempts [16, 23] have been done to learn depth representations from seman-tic segmentation implicitly, the reciprocal relationship be-tween the two tasks remains largely unexplored.
In this study, we introduce a new deeply unified frame-work for depth-aware panoptic segmentation, which lever-ages cross-modality knowledge not only at the architectural level but also during the learning phase. Rather than using separate queries for each task, we employ unified queries followed by geometry enhancement with latent represen-tations. Furthermore, we design a bi-directional guidance learning approach to optimize multi-task feature learning, which can better leverage their interdependence by using the supervision of one to guide the other.
We propose a deeply unified encoder-decoder architec-ture, which performs joint panoptic segmentation and depth estimation in a per-segment manner with identical queries.
We first generate instance-specific masks using unified per-segment queries, and enhance the queries with intermedi-ate depth features as well as learned latent representations to integrate scene geometry, via the proposed Geometric
Query Enhancement (Fig. 3). Subsequently, we predict depth maps from each enhanced query via a dot product with the depth embedding, and apply corresponding mask predictions to produce instance-wise depth predictions. To account for low-confidence filtering, which causes imper-fect masks or blank segments, we introduce an extra backup query to cover up these regions.
Moreover, we present a novel approach to leverage cross-modality knowledge by refining intermediate feature representations through Bi-directional Guidance Learning.
Our approach is based on the intuition that pixels crossing the semantic boundary are more likely to have a significant difference in depth and vice versa. To this end, we pro-pose Semantic-to-Depth guidance to optimize relative depth feature distances using contrastive learning, and Depth-to-Semantic guidance to synchronize semantic feature conti-nuity with depth annotations. The combination of both guidance mechanisms enables us to exploit their deeply-coupled relations and promote a more mutually-beneficial learning process.
Our method makes the following contributions: 1. We propose a new deeply unified architecture for depth-aware panoptic segmentation, which tackles both sub-tasks in a per-segment manner, by integrat-ing scene geometry into unified queries with geometry enhancement. 2. We propose a new training method that refines both intermediate features simultaneously through bi-directional guidance learning, leveraging their mutual relations and boosting performance under incomplete supervision. 3. Extensive experiments on Cityscapes-DVPS [44] and
SemKITTI-DVPS [44] demonstrate the effectiveness of our proposed method, leading to state-of-the-art per-formance on depth-aware panoptic segmentation and individual sub-tasks. 2.