Abstract
Video-language pre-training is crucial for learning pow-erful multi-modal representation. However, it typically re-quires a massive amount of computation.
In this paper, we develop SMAUG, an efficient pre-training framework for video-language models. The foundation component in SMAUG is masked autoencoders. Different from prior works which only mask textual inputs, our masking strat-egy considers both visual and textual modalities, provid-ing a better cross-modal alignment and saving more pre-training costs. On top of that, we introduce a space-time token sparsification module, which leverages context infor-mation to further select only “important” spatial regions and temporal frames for pre-training. Coupling all these designs allows our method to enjoy both competitive per-formances on text-to-video retrieval and video question an-swering tasks, and much less pre-training costs by 1.9× or more. For example, our SMAUG only needs ∼50 NVIDIA
A6000 GPU hours for pre-training to attain competitive performances on these two video-language tasks across six popular benchmarks. 1.

Introduction
Recently, video-language pre-training [27, 29, 2, 67, 12, 38, 54] stands as the common practice to learn cross-modal representations on large-scale video-text datasets [47, 6, 2].
Such pre-trained models show strong transfer performances on a range of vision and language tasks, including visual question answering [62, 34], text-to-video retrieval [62, 1], visual reasoning [48] and video understanding [33, 4, 58].
Nonetheless, the corresponding training cost of these ad-vanced video-language models is enormous. For exam-ple, the training of CLIP4Clip [38] needs ∼2 weeks with 8
GPUs, which therefore largely limits their explorations in a wider aspect. This invites us to ponder a thought-provoking but rarely explored question in this paper: How can we still pre-train powerful video-language models while signif-icantly reducing their pre-training cost?
Interestingly, we note that the recent work Masked Au-Figure 1: An overview of SMAUG (Sparse Masked
Autoencoder for video-langUaGe pre-training). (a) During pre-training, we randomly mask out a large subset of indi-vidual frames’ patches, and then utilize the visible patches and language sentences for video-text pre-training, which includes masked visual/language modeling (MVM/MLM) and etc. (b) The pre-trained models can then be fine-tuned on several down-stream video-language tasks, e.g., text-to-video retrieval and video question answering (video QA). toencoders (MAE) [17], which establishes an efficient self-supervised paradigm for training models at scale, poten-tially offering a solution to the aforementioned question.
In MAE, a large amount of image patches (e.g., 75%) are masked. The heavy encoder only executes on a small por-tion of the visible patches, and the lightweight decoder reconstructs the other large portion of masked patches.
This mask reconstruction process is a computationally effi-cient instantiation of masked visual modeling (i.e., MVM), which has already been shown effective for helping video-language pre-training [13, 24]. Therefore, we conjecture that resorting to such MAE fashion can substantially miti-gate the computational burden and still achieve satisfactory performances for video-language pre-training models.
Another interesting observation is that, even by masking out a significant portion of image patches as in MAE, the information could still be redundant [58, 11]. As argued in [44, 31], not all patches are equally important: an im-age could contain a significant amount of less informative visual patches (e.g., background patches), which scarcely or even negatively contribute to vision-language represen-tation learning. What is worse, this issue could be sev-erer in the video-language setting, as additionally, not all frames are equally important [26, 27]. For example, a video clip may contain a non-negligible portion of frames with just trivial noises (e.g., camera shake). Further eliminating these redundancies is expected to provide an extra speedup to video-language pre-training.
Based on the observations above, we present SMAUG, an efficient pre-training framework for video-language models. Our work is built upon MAE. We mask out a signif-icant amount of space-time patches and let the autoencoder learn to reconstruct them. Next, we introduce a space-time token sparsification module to remove spatial and temporal redundancies: 1) we leverage the attention weights in the visual encoder to predict attentive or inattentive tokens to reduce spatial patches among individual frames. Attentive tokens are preserved while inattentive tokens are fused; 2) we propose a learnable Transformer-based network to pick up important video frames among the given video clip.
We evaluate SMAUG on two video-language tasks, in-cluding text-to-video retrieval and video question answer-ing across six datasets. For text-to-video retrieval, the ex-periments are performed on MSRVTT [62], DiDeMo [1] and ActivityNet Captions [22].
For video question answering, MSRVTT-MC [64], MSRVTT-QA [60] and
ActivityNet-QA [65] are used. SMAUG can achieve state-of-the-art or comparable performances over all six datasets.
Meanwhile, the proposed method can achieve ∼1.9× video-language pre-training speedup. For example, SMAUG can finish video-language pre-training only with ∼50 NVIDIA
A6000 GPU hours. 2.