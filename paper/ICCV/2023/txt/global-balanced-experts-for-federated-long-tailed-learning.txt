Abstract
Federated learning (FL) is a prevalent distributed ma-chine learning approach that enables collaborative train-ing of a global model across multiple devices without sharing local data. However, the presence of long-tailed data can negatively deteriorate the model’s performance in real-world FL applications. Moreover, existing re-balance strategies are less effective for the federated long-tailed is-sue when directly utilizing local label distribution as the class prior at the clients’ side. To this end, we propose a novel Global Balanced Multi-Expert (GBME) frame-work to optimize a balanced global objective, which does not require additional information beyond the standard FL pipeline. In particular, a proxy is derived from the accumu-lated gradients uploaded by the clients after local training, and is shared by all clients as the class prior for re-balance training. Such a proxy can also guide the client grouping to train a multi-expert model, where the knowledge from different clients can be aggregated via the ensemble of dif-ferent experts corresponding to different client groups. To further strengthen the privacy-preserving ability, we present a GBME-p algorithm with a theoretical guarantee to pre-vent privacy leakage from the proxy. Extensive experiments on long-tailed decentralized datasets demonstrate the effec-tiveness of GBME and GBME-p, both of which show supe-rior performance to state-of-the-art methods. The code is available at here. 1.

Introduction
Federated Learning (FL) is a collaborative training method to develop a global model by utilizing decentralized data from multiple clients [21]. It enables knowledge aggre-gation over disparate data sources while mitigating privacy
∗ indicates equal contribution.
† corresponding denotes wubaoyuan@cuhk.edu.cn). author (avrillliu@hkust-gz.edu.cn,
Figure 1: Global re-balance VS. Local re-balance1. X-axis denotes several classical re-balance strategies. Global re-balance significantly outperforms local re-balance in the FL setting with long-tailed data, motivating us to optimize a global balanced objective. risks for individual clients. However, the model’s perfor-mance of a FL system can be severely deteriorated in the presence of long-tailed data distribution, which is an ubiq-uitous problem in various realistic scenarios [13, 36], such as medical applications [17], personal information protec-tion [44] and autonomous vehicles [26].
Importantly, it is extremely difficult to learn a balanced global model in the FL setting with long-tailed data [34], especially for the minority classes [30].
In the concrete, due to data heterogeneity, there may exist a large divergence among the imbalanced distributions of different clients, e.g., different local datasets have different imbalance ratios or minority classes (visualized in Section 9 in Supplementary
Material). Additionally, during the standard FL training, partial client selection may randomly drop some minority samples at each communication round, further decreasing the model performance on minority classes. Therefore, the long-tailed issue is more challenging in the FL scenarios.
Several techniques have been proposed to tackle the federated long-tailed problem, such as loss re-weighting
[30, 34], client clustering [6], and client selection [39]. 1Global re-balance means that the re-balance strategies adopt global label distribution as the class prior, while local re-balance utilizes local label distribution as the class prior.
However, it is generally assumed that some sensitive in-formation is accessible to the server, e.g., a balanced mini-dataset [6, 34] or learnable hyper-parameters of clients [30], which may not be available in realistic applications. Be-sides, most of them focus on datasets with few classes (e.g., ten or twenty), and their effectiveness diminishes on large-scale imbalanced datasets with a higher number of classes
[20, 43]. Furthermore, we report the performance of sev-eral classical re-balance methods [2, 25, 12, 35, 4] under the federated long-tailed setting in Figure 1. It is observed that the performance improvement is limited compared with Fe-dAvg [21] when taking local label distribution as the class prior of re-balancing strategies (i.e., local re-balance).
Aiming to tackle the above problem, this work further explores the effectiveness of existing class-prior based re-balance algorithms for federated long-tailed learning. Ex-perimentally, as indicated by Figure 1, deploying these al-gorithms with global re-balance yields higher accuracy than that with local re-balance1. The main reason arises from the optimization objective gap between these two re-balance strategies, where the global one provides a consistent ob-jective with the centralized balancing training (introduced in Section 3.1). However, global re-balance requires clients to upload local label distributions to obtain global label dis-tribution, thus increasing the risk of privacy leakage [34].
To get rid of those constraints, we propose a Global
Balanced Multi-Expert (GBME) framework to deal with the federated long-tailed issue without requiring additional information beyond the standard FL. Specifically, we derive a local proxy from the accumulated gradients of the clients after local training rather than from the local label distri-bution, and then a global proxy is formulated as the class prior for re-balance algorithms by integrating local proxies of each client. Based on the cosine similarity between the local and global proxy, clients can be divided into different groups corresponding to different experts in a multi-expert model. During the local training of a client, the correspond-ing expert is trainable to learn balanced knowledge using the global proxy as the prior, while other experts are frozen to maintain the knowledge learned from other groups. Us-ing a multiple selection strategy, a client can implicitly in-teract with other groups in an ensemble manner to aggre-gate balanced knowledge learned from different groups. To further improve the privacy-preserving ability, we present a
GBME-p algorithm based on the differential privacy (DP)
[7] to prevent the privacy leakage of local label distribu-tions. Concretely, the random Gaussian noises are added to the weights of the final fully connected (FC) layer for lo-cal proxy computation at the clients’ side before uploading.
The overall GBME framework is illustrated in Figure 2. In summary, the key contributions of this work are as follows. (i) We experimentally and theoretically explore the effec-tiveness of existing class-prior based re-balance algo-rithms in federated long-tailed learning. It is demon-strated that there is a mismatch between the optimiza-tion objectives of local and global re-balance strate-gies, where global re-balance performs better than the local one on the imbalanced decentralized data. (ii) We propose a GBME framework to achieve global balanced training, where a proxy is designed as the class prior for re-balancing algorithms without requir-ing additional private information. The clients are di-vided into multiple groups to collaboratively train a multi-expert model, where the knowledge from differ-ent groups can be aggregated in an ensemble manner. (iii) To enhance the privacy-preserving ability, we present a GBME-p algorithm with a theoretical guarantee to prevent the privacy leakage of local label distributions, where the Gaussian noises are added to the weights of the last FC layer at the clients’ side before uploading. (iv) The experiments on multiple benchmark datasets demonstrate that GBME without requiring additional private information can significantly outperform pre-vious state-of-the-art (SOTA) methods.
Besides,
GBME-p can still achieve superior performance under the protection of the differential privacy. 2.