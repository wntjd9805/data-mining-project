Abstract
We introduce a learning-based depth map fusion frame-work that accepts a set of depth and confidence maps gen-erated by a Multi-View Stereo (MVS) algorithm as input and improves them. This is accomplished by integrat-ing volumetric visibility constraints that encode long-range surface relationships across different views into an end-to-end trainable architecture. We also introduce a depth search window estimation sub-network trained jointly with the larger fusion sub-network to reduce the depth hypoth-esis search space along each ray. Our method learns to model depth consensus and violations of visibility con-straints directly from the data; effectively removing the ne-cessity of fine-tuning fusion parameters. Extensive exper-iments on MVS datasets show substantial improvements in the accuracy of the output fused depth and confidence maps. Our code is available at https://github.com/ nburgdorfer/V-FUSE 1.

Introduction
Much like other areas of computer vision, Multi-View
Stereo (MVS) has benefited from the advent of deep learn-ing. Progress has been driven by the creation of end-to-end systems, unifying all aspects of the MVS pipeline, and by replacing heuristics in the components of the pipeline with optimized network modules. An aspect of MVS that re-quires further investigation is depth map fusion, which is still implemented as a sequence of heuristic operations.
Considering that the top performing MVS systems in terms of geometric accuracy1 use depth map collections as the representation, depth map fusion can be a crucial step for obtaining the final 3D reconstruction of the scene.
As has been shown by conventional fusion research [23], fusing depth maps, guided by geometric constraints, im-proves the precision of correct depth estimates by blend-ing them with supporting estimates for the same part of the surface, detects and removes outliers, and reduces redun-1NeRF [26] has inspired a vastly expanding class of algorithms that produce superior results in view synthesis, but not in 3D reconstruction.
We consider NeRF a separate line of work from MVS.
Figure 1. Point cloud reconstructions from DTU [1] and Tanks &
Temples [19] datasets using depth maps from NP-CVP-MVSNet
[40] and UCSNet [3] as input to V-FUSE. dancy in the final 3D model. Current deep MVS approaches
[3, 11, 21, 25, 42, 40, 41], however, bypass depth map fu-sion and proceed directly to filtering fusion, which includes various heuristic post-processing steps to obtain a global point cloud by filtering the point cloud reconstructed from the set of depth maps. This approach has been successful; however, without depth map fusion, not all geometric infor-mation from the scene is utilized. Our motivation in this work is to build an end-to-end fusion network that can gen-erate much more accurate depth and confidence maps.
Filtering fusion that operates on local 3D neighborhoods is unable to leverage relationships among distant surface primitives, such as a surface being occluded from a faraway object. Similarly, convolution networks have a limited re-ceptive fields and can only reason about local interactions.
We present V-FUSE, an approach that allows a 3D convolu-tional network to benefit from such geometric information, in a differentiable manner, controlled by learnable hyper-parameters.
V-FUSE considers three types of constraints, inspired by the work of Merrell et al. [23]: support among consistent depth estimates across multiple views, occlusions and free-space violations that provide evidence against depth esti-mates contradicting surfaces estimated in different depth maps. Free-space violations provide the added benefit of
encoding conflicts with respect to surfaces that may be in-visible in the frame of the reference camera. There are three substantial differences between our approach and that of
Merrell et al.: (i) theirs operates in 2 1 2 D while ours oper-ates in a 3D volume, (ii) their algorithms make decisions per pixel without considering context, and (iii) all parameters in our approach are learned end-to-end. Specifying visibility constraints in the fusion volume allows V-FUSE to reason based on interactions among depth estimates along the rays, as well as spatially among neighboring voxels. In the ab-sence of these constraints, only the latter would have been possible via 3D convolutions, which cannot reason about long-range conflicts.
Reducing the storage and computational requirements of deep MVS networks is a necessity for increasing the reso-lution and quality of 3D reconstruction. 3D convolutional networks operating on cost volumes are forced to downsam-ple high resolution inputs. Since our framework is also vol-umetric, we propose a technique for achieving high resolu-tion near the surfaces while keeping memory requirements manageable. Specifically, we learn to generate a per-pixel, narrow depth search window by examining the input depth and confidence estimates. Unlike previous networks that it-eratively refine the depth search space, our framework lever-ages the availability of input depth and confidence estimates to determine a reduced search space in a single pass.
Our main contributions are:
• An end-to-end learning-based method for the fusion of depth and confidence maps, leveraging long-range, volumetric visibility constraints encoded into a visibil-ity constraint volume (VCV).
• A pixel-wise search window estimation sub-network to refine the depth search space.
We provide extensive evaluation of V-FUSE on MVS benchmarks [1, 19, 43], using 2D and 3D error metrics. 2.