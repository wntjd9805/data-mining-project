Abstract
Can we leverage the audiovisual information already present in video to improve self-supervised representation learning? To answer this question, we study various pre-training architectures and objectives within the masked au-toencoding framework, motivated by the success of sim-ilar methods in natural language and image understand-ing. We show that we can achieve significant improvements on audiovisual downstream classification tasks, surpassing the state-of-the-art on VGGSound and AudioSet. Further-more, we can leverage our audiovisual pretraining scheme for multiple unimodal downstream tasks using a single au-diovisual pretrained model. We additionally demonstrate the transferability of our representations, achieving state-of-the-art audiovisual results on Epic Kitchens without pre-training specifically for this dataset. 1.

Introduction
The computer vision community has witnessed rapid progress across a wide range of tasks and domains, driven by progressively larger models and datasets [11, 24, 58, 63, 78]. However, pretraining models on large labelled datasets [21, 50, 67] and then finetuning on smaller tar-get datasets is not scalable: Annotating large pretraining datasets is expensive and time-consuming, and larger, more performant models require more pretraining data [24]. This has led to growing research in self-supervised pretrain-ing methods which learn feature representations from un-labelled data, and has been extremely successful in natural language processing (NLP) for developing large language models [13, 22, 65]. More recently, similar methods have been adopted in the vision community as well [10, 37, 76].
In this paper, we propose to leverage the audiovisual information present in video to improve self-supervised representation learning. Despite recent advances in self-supervised image- [10, 37] and video-representation learn-ing [26, 70, 76], these works still ignore the additional audi-tory information that is already present in their pretraining
*Equal contribution. Correspondence to aarnab@google.com.
†Work done during an internship at Google.
Figure 1: Overview of our Audiovisual Masked Autoencoder. We jointly encode and reconstruct audiovisual inputs, to leverage the correlations between the two modalities to learn stronger repre-sentations of the data. Our pretrained encoder can then be used for audiovisual, audio-only and video-only downstream tasks. sets. Intuitively, we aim to exploit the correlations between the modalities already present in video to learn stronger rep-resentations of the data for unimodal and multimodal down-stream tasks. Our approach is further motivated by the fact that the world, and human perception of it, is inherently multimodal [61, 64].
Our approach is inspired by the masked autoencoding framework [10, 37], which itself is based on similar masked data modelling approaches in NLP [22] and earlier works
on denoising autoencoders [55, 72]. We develop multiple pretraining architectures to jointly encode and reconstruct audiovisual inputs, and conduct thorough ablation studies to verify our design choices. To encourage further cross-modal information modelling, we also propose a novel “in-painting” objective which tasks our transformer model with predicting audio from video tokens and vice versa.
Our audiovisual pretraining enables us to achieve state-of-the-art results in downstream, audiovisual datasets such as VGGSound and AudioSet. Moreover, we show how we can reuse our audiovisual pretraining for unimodal, i.e. audio-only or video-only downstream classification tasks.
Furthermore, we show how the representations learned by our model transfer between different pretraining and down-stream finetuning datasets, enabling us to achieve state-of-the-art art results on the Epic Kitchens dataset. 2.