Abstract
In this paper, we propose a novel channel pruning method to reduce the computational and storage costs of
Convolutional Neural Networks (CNNs). Many existing one-shot pruning methods directly remove redundant struc-tures, which brings a huge gap between the model before and after network pruning. This gap will no doubt result in performance loss for network pruning. To mitigate this gap, we first learn a target sub-network during the model training process, and then we use this sub-network to guide the learning of model weights through partial regulariza-tion. The target sub-network is learned and produced by using an architecture generator, and it can be optimized ef-ficiently.
In addition, we also derive the proximal gradi-ent for our proposed partial regularization to facilitate the structural alignment process. With these designs, the gap between the pruned model and the sub-network is reduced, thus improving the pruning performance. Empirical re-sults also suggest that the sub-network found by our method has a much higher performance than the one-shot pruning setting. Extensive experiments show that our method can achieve state-of-the-art performances on CIFAR-10 and Im-ageNet with ResNets and MobileNet-V2. 1.

Introduction
Convolutional Neural Networks (CNNs) have achieved many successes in different computer vision tasks [27, 46, 47, 50, 2, 6, 11]. To tackle real-world challenges, re-cent CNNs [27, 51, 14] become larger and larger regard-ing width, depth, etc. With such capacities, CNNs can ob-tain better performance on different benchmarks at the cost
*Corresponding author. This work was partially supported by NSF
IIS 1838627, 1837956, 1956002, 2211492, CNS 2213701, CCF 2217003,
DBI 2225775. of computational and storage burdens. At the same time, with the recent developments of mobile and embedded de-vices, the demand for deploying CNNs on these devices has increased dramatically. However, there is a natural con-flict between the size of CNNs and the hardware capabil-ity of these devices. To overcome these challenges, many works [13, 12] try to reduce the size of CNNs, and make them possible to be deployed on edge devices.
There are many directions to reduce the size of CNNs.
Among them, weight pruning and structural pruning are two popular topics. Structural pruning, especially channel pruning, is more friendly to hardware than weight pruning since no post-processing steps are required to acquire sav-ings in computational and storage costs. Thus, our paper focuses on channel pruning for CNNs. Many existing one-shot pruning works [44, 56, 9, 42, 17, 8] prune trained mod-els directly. No matter what method is used, there will be a significant gap between the selected sub-network and the pruned model. Such a gap creates difficulties in regaining performance during the fine-tuning process. On the other hand, soft pruning methods [16, 23] softly remove struc-tures during the training process, which can produce good results with a shorter fine-tuning process. However, soft pruning methods generally perform worse than typical one-shot pruning methods, probably because the weight space is restricted during the training process because of soft prun-ing.
To tackle the above problems, we introduce a novel par-tial regularization technique to align model weights and the discovered sub-network during the training process, which can produce a high performance sub-network and reduce the gap between the sub-network and the original model.
In addition, unlike soft-pruning methods, all model struc-tures are used for training. The partial regularization term contains a partial group lasso regularization on selected weights, and other weights remain intact without modifica-tions. An architecture generator is trained to select which weights should be aligned, and it is also updated during the training process. Our partial regularization formulation is related to partial regularization in lasso [40].
Inspired by the nonmonotone proximal gradient (NPG) method used in [40], we also use a proximal gradient method to solve the partial regularization problem in our setting. Note that our method dynamically changes which channels should be put in the partial regularization. As a result, we add a scalar to balance the regularization strength for different layers be-cause the number of pruned channels is different for them.
To maximally keep the capacity of the original model, we insert the partial regularization in the middle of the training process. This is because weights are vulnerable to prun-ing at the early training stage, and the FLOPs regularization will dominate updates of the architecture generator, which can create bad sub-networks and mislead the training pro-cess. Finally, we update model weights and the architecture generator periodically, and they are connected by the par-tial regularization term during training. To maintain simi-lar training efficiency as the original model, we only use a small portion of samples to train the architecture generator.
Thus, there is only a small overhead compared to the orig-inal training process. Our method successfully finds per-formant sub-networks from the original model with these techniques. In summary, the contributions of this paper can be summarized as follows: 1) We propose to align the sub-network in the original model with the final pruned model through partial regularization. By structural alignment, the gap be-tween the selected sub-network and the pruned model is largely reduced, which naturally improves the per-formance of the pruned model. 2) We use an architecture generator parameterized by neural networks to select the proper sub-network struc-ture and guide the partial regularization. Inspired by partial regularization in lasso [40], we propose to solve the partial regularization problem via proximal gradi-ents, which facilitate the alignment process. 3) Empirical results show that the sub-network discov-ered by our method performs much better than the one-shot pruning setting. Extensive experiments on
CIFAR-10 and ImageNet show that our method outper-forms existing channel pruning methods on ResNets and MobileNet-V2. 2.