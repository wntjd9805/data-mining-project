Abstract
There has been a longstanding belief that generation can facilitate a true understanding of visual data. In line with this, we revisit generatively pre-training visual repre-sentations in light of recent interest in denoising diffusion models. While directly pre-training with diffusion models does not produce strong representations, we condition diffu-sion models on masked input and formulate diffusion mod-els as masked autoencoders (DiffMAE). Our approach is capable of (i) serving as a strong initialization for down-stream recognition tasks, (ii) conducting high-quality im-age inpainting, and (iii) being effortlessly extended to video where it produces state-of-the-art classification accuracy.
We further perform a comprehensive study on the pros and cons of design choices and build connections between dif-fusion models and masked autoencoders. Project page. 1.

Introduction
“What I cannot create, I do not understand.”
—— Richard P. Feynman, 1988
For many years, there has been a desire to achieve a deeper understanding of visual data through the process of gener-ation. Early approaches, such as deep belief networks [33] and denoising autoencoders [72], employed generative pre-training to initialize deep networks for downstream recogni-tion tasks. As generative models can create new samples by approximating the data distribution, it stands to reason that such modeling should simultaneously arrive at a semantic understanding of the raw visual data, as required for recog-nition tasks, following Feynman.
In line with this philosophy, generative language mod-els, e.g., Generative Pre-trained Transformers or GPTs [3], obtain a broad understanding of language and an immense knowledge base, excelling as both a few-shot learner and a pre-trained base model. Nevertheless, recent explorations in vision generative pre-training fall out of favor. For ex-ample, GAN-based BiGAN [15, 16] and auto-regressive iGPT [7] noticeably fall short of their concurrent con-trastive algorithms despite of using 10× more parame-Figure 1. Inference process of DiffMAE, which iteratively un-folds from random Gaussian noise to the sampled output. During training, the model learns to denoise the input at different noise levels (from top row to the bottom) and simultaneously performs self-supervised pre-training for downstream recognition. ters. The challenge stems, in part, from the divergent fo-cus: While recognition models primarily focus on the high-level low-frequency structure of images, generation mod-els must also allocate capacity for low-level high-frequency details [60]. Given this discrepancy, it remains an open question whether and how generative pre-training can ef-fectively compete with other self-supervised algorithms on downstream recognition tasks despite its intuitive appeal.
In recent years, the field of image generation has been dominated by denoising diffusion models [67, 35]. These models employ a straightforward process of iteratively re-fining noisy samples. As a result, the generated images are of impressively high quality, and even better, can generate an extensive range of diverse samples [50, 59, 64]. In light of this progress, we revisit the potential of generative pre-training in the context of diffusion models.
To begin with, we directly fine-tune a pre-trained diffu-sion model [13] on ImageNet classification [11]. Despite its strong performance for unconditional image generation, the pre-trained diffusion model only yields a marginal improve-ment in classification when compared to training the same architecture from scratch (Sec. 3), and is outperformed by concurrent self-supervised pre-training algorithms such as
Masked Autoencoders (MAE) [30].
MAE demonstrates strong recognition performance by learning to regress pixels of masked patches given the other visible patches. Inspired by MAE, we incorporate masking into diffusion models and cast Diffusion Models as Masked
Autoencoders (DiffMAE). We formulate the masked pre-diction task as a conditional generative objective, i.e., to ap-proximate the pixel distribution of the masked region con-ditioned on the visible region. We learn models with our diffusion approach, within the MAE framework, introduc-ing no extra training cost. During pre-training, our model is trained to denoise the input at different noise levels and learns a strong representation for recognition and genera-tion. We evaluate the pre-trained model by fine-tuning on downstream recognition tasks, as well as image inpainting for which the model generates samples by iteratively un-folding from random Gaussian noise, illustrated in Fig. 1.
The diffusion nature of DiffMAE allows it to generate intricate visual details, e.g., of objects (Fig. 2). In contrast,
MAE is known to produce blurry reconstructions that lack high-frequency components. Further, DiffMAE maintains strong performance on image and video recognition tasks.
We make the following observations in this work: (i) DiffMAE is a strong pre-training approach for fine-tuning on downstream recognition tasks, obtaining compa-rable performance to leading self-supervised learning algo-rithms that focus solely on recognition. When combined with CLIP [57] features, our DiffMAE is even able to out-perform recent work that combines MAE and CLIP. (ii) DiffMAE is able to generate high quality images conditioning on masked input. Specifically, DiffMAE gen-erations outperform leading inpainting methods quantita-tively and also appear more semantically meaningful. (iii) DiffMAE can be extended to the video domain ef-fortlessly, providing high-quality inpainting and state-of-the-art recognition accuracy, outperforming recent works. (iv) We reveal a connection between MAE and diffusion models, as MAE effectively performs the first inference step of diffusion. In other words, we believe the success of MAE aligns with the philosophy of generation for recognition.
We further perform a comprehensive empirical study to elucidate the pros and cons of the design choices on down-stream recogntion and inpainting generation tasks. 2.