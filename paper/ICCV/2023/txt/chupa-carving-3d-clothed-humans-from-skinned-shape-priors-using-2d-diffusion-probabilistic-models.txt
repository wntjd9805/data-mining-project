Abstract
We propose a 3D generation pipeline that uses diffusion models to generate realistic human digital avatars. Due to the wide variety of human identities, poses, and stochastic details, the generation of 3D human meshes has been a chal-lenging problem. To address this, we decompose the problem into 2D normal map generation and normal map-based 3D reconstruction. Specifically, we first simultaneously generate realistic normal maps for the front and backside of a clothed human, dubbed dual normal maps, using a pose-conditional diffusion model. For 3D reconstruction, we “carve” the prior
SMPL-X mesh to a detailed 3D mesh according to the nor-mal maps through mesh optimization. To further enhance the
*Equal contribution high-frequency details, we present a diffusion resampling scheme on both body and facial regions, thus encouraging the generation of realistic digital avatars. We also seamlessly incorporate a recent text-to-image diffusion model to sup-port text-based human identity control. Our method, namely,
Chupa, is capable of generating realistic 3D clothed humans with better perceptual quality and identity variety. 1.

Introduction
The creation of clothed 3D human characters, which we refer to as “digital avatars”, has become an essential part of many fields including gaming, animation, virtual/mixed reality, and the 3D industry in general. These digital avatars allow users to use their virtual representation for a range
of purposes, thus enhancing user immersion within such services. However, creating high-quality digital avatars often requires specialized 3D artists using a sophisticated creation pipeline [23, 42], making it a laborious process.
The recent advances in deep generative models [21, 25, 37] have enabled the creation of high-quality images that accurately reflects the textual input semantics [49, 61]. How-ever, the usage of such generative models in creating 3D has mainly focused on object generation [56, 72, 89, 92, 94] and shown rather limited performance in generating full-body, realistic 3D human avatars due to the difficulty of collect-ing a large-scale ground truth dataset. Many previous 3D generative models [2, 6, 7, 22, 27, 52, 93] focus on training generative models on large-scale image datasets along with implicit 3D shape representations and differentiable volume rendering [51, 82]. However, those approaches are rather lim-ited in generating full-body humans with realistic details and rely on computationally expensive volume rendering. Other approach [9] directly uses high-quality 3D datasets [60, 91] to train generative models based on auto-decoding frame-works [53], but the resulting stochastic details tend to be unrealistic, due to the usage of an adversarial loss [21].
In this paper, we decompose the problem of 3D gener-ation into 2D normal map generation and 3D reconstruc-tion, bridging the power of generative models in the image domain toward 3D generation. Following the intuition of
“sandwich-like” approaches for single image-based 3D hu-man reconstruction [19, 73, 87], we generate normal maps for frontal and backside regions of human mesh to get rich details mitigating the computational cost of 3D representa-tions. We adopt a diffusion model [25, 61] to simultaneously create consistent normal maps for both frontal and backside regions, which we call dual normal maps, conditioned on a posed SMPL-X [43, 54]. Since diffusion models are well known for their mode coverage [85], we find it suitable to generate diverse 3D digital avatars. The dual normal maps are then used as input for our 3D reconstruction pipeline, in which we carve the initial posed SMPL-X mesh to a clothed, realistic human mesh with normal map-based mesh optimization inspired by NDS [84]. During optimization, the initial mesh is gradually deformed to match the gen-erated normal maps through a differentiable rasterization pipeline [39] and geometric regularization including a loss function for plausible side-view. Our dual normal map-based 3D generation pipeline alleviates the difficulty of generat-ing consistent multi-views, which is the fundamental reason that diffusion-based 3D generative models [56, 79, 89] suf-fer from slow convergence or fail to generate multi-view consistent results. We show that the diffusion model can generate consistent dual normal maps and they are sufficient to generate plausible 3D humans along with SMPL-X prior.
Then, we can further improve the generated mesh by using a resampling scheme motivated by SDEdit [47], in which we use separate diffusion models for the body and facial regions to refine the perceptual quality of the rendered normals in different viewpoints while preserving the view and identity consistency. The refined normal maps are subsequently used as inputs for the mesh optimization, thus creating a realistic 3D digital avatar with high-frequency details.
As shown in Fig. 1, our pipeline, which we dub it Chupa, can be extended to text-based generation for further control-lability on the human identity (e.g., gender, clothing, hair, etc.), by leveraging the power of a pre-trained text-to-image diffusion model, e.g., Stable Diffusion [61]. Specifically, we modify and fine-tune the text-to-image model [3, 90] to en-able conditioning on posed SMPL-X, such that the model creates detailed normal maps according to both the pose information and textual descriptions. Afterward, we pass the generated frontal normal map as guidance to the dual normal map generator to complete dual normal maps, seamlessly connecting text-based generation to our original pipeline.
Trained from posed 3D scans only, Chupa is capable of generating various digital avatars from pose and textual infor-mation, with realistic, high-fidelity features such as wrinkles and large varieties in human identity and clothing. We evalu-ate our method through established benchmarks along with a perceptual study and show that our method outperforms the previous baseline. In summary, our contributions are:
• A 3D generation pipeline that directly leverages the 2D image generation capability of diffusion models towards 3D reconstruction.
• A diffusion-based normal map generation and refine-ment strategy for view-consistent normal maps, targeted for 3D generation.
• A method to effectively allow text-based 3D full-body digital avatar creation, providing an intuitive scenario for digital avatar creation. 2.