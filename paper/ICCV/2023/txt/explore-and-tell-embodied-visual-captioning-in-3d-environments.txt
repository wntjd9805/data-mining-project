Abstract
While current visual captioning models have achieved impressive performance, they often assume that the image is well-captured and provides a complete view of the scene.
In real-world scenarios, however, a single image may not offer a good viewpoint, hindering fine-grained scene under-standing. To overcome this limitation, we propose a novel task called Embodied Captioning, which equips visual cap-tioning models with navigation capabilities, enabling them to actively explore the scene and reduce visual ambigu-ity from suboptimal viewpoints. Specifically, starting at a random viewpoint, an agent must navigate the environ-ment to gather information from different viewpoints and generate a comprehensive paragraph describing all objects in the scene. To support this task, we build the ET-Cap dataset with Kubric simulator, consisting of 10K 3D scenes with cluttered objects and three annotated paragraphs per scene. We propose a Cascade Embodied Captioning model (CaBOT), which comprises of a navigator and a captioner, to tackle this task. The navigator predicts which actions to take in the environment, while the captioner gener-ates a paragraph description based on the whole naviga-tion trajectory. Extensive experiments demonstrate that our model outperforms other carefully designed baselines.
Our dataset, codes and models are available at https:
//aim3-ruc.github.io/ExploreAndTell. 1.

Introduction
Visual captioning [14, 22, 32, 18, 9] is an essential vision-and-language task which aims to generate natural language descriptions of visual contents. In recent years, many captioning models have been developed and achieved significant improvements in describing major objects and relationships in an image [41, 4, 28, 25, 46, 23, 39]. How-ever, the current models typically rely on well-captured im-*Corresponding Author.
Figure 1. Existing visual captioning models generate a sentence to describe a single image, which often fail when the image is not well-captured at good viewpoints (the captions next to the im-ages are generated by the state-of-the-art BLIP model [23]). To generate more accurate and comprehensive visual descriptions, we propose a new Embodied Captioning task which allows agents to navigate the environment to reduce visual ambiguity of the scene. ages that provide a good viewpoint of the scene. Unfortu-nately, in real-world scenarios, capturing such images may not always be feasible. As illustrated in Fig. 1, the initial position to capture an image may not provide a complete view of the scene, and a single image may not be sufficient to capture all objects within it, potentially leading to incom-plete or inaccurate visual captions. To address this limita-tion, it is essential for visual caption models to navigate the environment actively and gather information from multiple viewpoints in order to generate more comprehensive and accurate visual captions.
In this paper, we propose a novel task called Embodied
Captioning, which integrates such navigation ability into vi-sual captioning. The task requires an agent, which starts at a random viewpoint in a 3D environment, to navigate the environment to reduce visual ambiguity of the scene, and finally generate a comprehensive paragraph that describes all objects in the scene. Different from existing navigation tasks [5, 15, 44, 30, 24], our Embodied Captioning task does not explicitly define a target location to navigate. Instead, we have an implicit target which is to accurately recognize all objects along with their attributes and relationships in
the scene as soon as possible.
To support the Embodied Captioning task, we build a high-quality 3D dataset ET-Cap with manually annotated paragraph descriptions. Leveraging high-quality 3D ob-ject assets from ShapeNet [11] and GSO [16], we use
Kubric [17] to construct 10,000 scenes. For each scene, three annotators are provided with 20 images from different viewpoints of the scene and asked to write a detailed para-graph to describe all visible instances. We also require the annotators to select images with good viewpoints among all the image candidates. Although our dataset is based on synthetic scenes with limited obstacles, it still presents significant challenges for Embodied Captioning. The agent only receives an RGB image of a restricted field of view at each step without any localization information, such as the location of the agent and objects. Therefore, a model must be equipped with long-term memories of previous vi-sual observations and actions in order to efficiently explore the environment, accurately recognize objects, and generate fine-grained scene descriptions.
To address these challenges, we propose a Cascade Em-bodied Captioning model (CaBOT), which consists of a
History-aware Navigator and a Trajectory-aware Captioner.
The navigator leverages histories of both observed images and performed actions to predict the next action via a trans-former model. The captioner is fed with all images in the predicted trajectory and utilizes a bi-level cross attention over spatial and temporal dimensions of the image sequence to generate a paragraph. The proposed CaBOT model out-performs carefully designed baselines for the challenging
Embodied Captioning task. Nevertheless, there is still much room to improve compared to human performance, such as joint modeling the navigation and captioning process.
In summary, our contributions are three-fold:
• We propose a novel and challenging Embodied Cap-tioning task which requires agents to explore in 3D en-vironments to generate better visual descriptions.
• A high-quality dataset is constructed to benchmark the Embodied Captioning task, with 10K synthetic 3D scenes and 24K manually annotated good viewpoints and 30K paragraph descriptions.
• We present a Cascade Embodied Captioning model which incorporates navigation histories for captioning, providing a strong starting point for future work. 2.