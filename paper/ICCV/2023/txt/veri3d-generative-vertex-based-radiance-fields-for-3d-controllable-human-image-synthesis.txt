Abstract
Unsupervised learning of 3D-aware generative adver-sarial networks has lately made much progress. Some re-cent work demonstrates promising results of learning hu-man generative models using neural articulated radiance fields, yet their generalization ability and controllability lag behind parametric human models, i.e., they do not perform well when generalizing to novel pose/shape and are not part controllable. To solve these problems, we propose VeRi3D, a generative human vertex-based radiance field parameter-ized by vertices of the parametric human template, SMPL.
*Corresponding authors.
We map each 3D point to the local coordinate system de-fined on its neighboring vertices, and use the corresponding vertex feature and local coordinates for mapping it to color and density values. We demonstrate that our simple ap-proach allows for generating photorealistic human images with free control over camera pose, human pose, shape, as well as enabling part-level editing. 1.

Introduction
Generating diverse photorealistic renderings of clothed humans has a wide range of applications including visual effects, virtual try-on, VR/AR, and creative image editing.
Compared to designing 3D human avatars manually which is expensive and time-consuming, learning generative hu-man models from data is a promising alternative to reduce the design effort. To meet the requirements of the aforemen-tioned applications, the generative model should be capable of rendering photorealistic clothed humans with free con-trol over camera pose, human pose, shape, and appearance, ideally at the part-level, such as changing a hoody from a sweater for the same individual.
Existing generative human models do not fulfill these re-quirements yet. Parametric 3D body models [1, 32, 47], e.g., SMPL, faithfully capture the human pose and shape statistics with explicit control over these attributes but do not model clothing. Based on SMPL, generative models for clothed human avatars have been proposed [6, 9, 35]. How-ever, these methods do not model appearance and require 3D supervision for modeling geometry.
Recently, 3D-aware generative adversarial networks (GANs) have shown great success in learning 3D repre-sentations, e.g., neural radiance fields, from single-view 2D supervisions. While most 3D-aware GANs focus on non-articulated objects, e.g., faces and cars [53, 5, 4, 16], there are some recent attempts to learn generative human radi-ance fields from 2D images [42, 2, 66]. All these meth-ods map a 3D point in the observation space to a canon-ical space, allowing for learning shape and appearance in a pose-agnostic space. One line of work models the map-ping function via a learned blending field [42], yet learn-ing the distribution of the blending field from single-view 2D images is highly challenging and struggles to generalize to out-of-distribution poses. Another line of works model articulation using a fixed surface-based mapping without learning parameters [2, 66]. This indeed simplifies the task of the generator and leads to better generalization, but the surface-based mapping can be inaccurate, e.g., an unoccu-pied background point in the observation space might be mapped to an occupied foreground point in the canonical space and thus yielding ghosting artifacts. Further, despite providing control over the human pose, the controllability over the shape and fine-grained parts is yet to be explored.
In this work, we propose generative Vertex-based human
Radiance fields, VeRi3D , aiming for bridging the full con-trollability of parametric models and the image fidelity of 3D-aware GANs. Our key idea is to parametrize the gen-erative human radiance fields using a set of vertices prede-fined by a human model, SMPL, and learning distributions over the vertex features to generative diverse human images.
Specifically, we transform a 3D point to the individual lo-cal coordinate systems of its nearest vertices, mapping the point to a color and a density value based on the feature vectors attached to the nearest vertices and its locations in the local coordinates. Our representation has the following advantages: i) This formulation allows us to enjoy the ben-efit of using a fixed mapping guided by SMPL pose, leav-ing the generator to learn a pose-agnostic human represen-tation only. ii) Further, our formulation does not suffer from ghosting artifacts as each point is mapped to an individual local coordinate system instead of a shared global canonical space as in previous methods [2, 66]. iii) Our formulation is naturally suited for generative models as we can learn a set of pose-agnostic vertex features based on a fixed UV mapping of the vertices using a 2D convolution neural net-work. iv) We demonstrate that our method naturally enables control over pose and shape by editing the vertex locations.
Further, it can easily achieve part-level control by manipu-lating vertex features of the same part.
In conclusion, our contributions are as follows: 1) We present a 3D-aware GAN method for generating control-lable radiance fields of human bodies. 2) Our method in-troduces a human radiance field representation that enables generalization to novel poses and body shapes, as well as editing of local cloth shape and appearance. 3) We demon-strate high-quality results for unconditional generation and animation of human bodies using several datasets, including
Surreal, AIST++ and DeepFashion. 2.