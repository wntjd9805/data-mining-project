Abstract 1.

Introduction
We present DreamPose, a diffusion-based method for generating animated fashion videos from still images.
Given an image and a sequence of human body poses, our method synthesizes a video containing both human and fabric motion. To achieve this, we transform a pre-trained text-to-image model (Stable Diffusion [16]) into a pose-and-image guided video synthesis model, using a novel ﬁnetuning strategy, a set of architectural changes to support the added conditioning signals, and techniques to encourage temporal consistency. We ﬁne-tune on a col-lection of fashion videos from the UBC Fashion dataset
[50]. We evaluate our method on a variety of cloth-ing styles and poses, and demonstrate that our method produces state-of-the-art results on fashion video anima-tion. Video results are available on our project page: https://grail.cs.washington.edu/projects/dreampose
Fashion photography is incredibly prevalent online, from social media platforms to online retail sites. Unfortu-nately, these still photographs are limited in the informa-tion they convey, and fail to capture many of the crucial nu-ances of a garment, such as how it drapes and ﬂows when worn. Fashion videos, on the other hand, do showcase all these details, and for this reason are highly informative for consumer decision-making. Despite this beneﬁt, however, these videos are a relatively rare commodity.
In this paper, we introduce DreamPose, a method that turns fashion photographs into realistic, animated videos, using a driving pose sequence. Our method is a diffusion video synthesis model based upon Stable Diffusion [16].
Given one or more images of a human and a pose sequence,
DreamPose generates a high-quality video of the input sub-Figure 2: Architecture Overview. We modify the original Stable Diffusion architecture in order to enable image and pose conditioning. First, we replace the CLIP text encoder with a dual CLIP-VAE image encoder and adapter module (shown in the blue box). The adapter module jointly models and reshapes the pretrained CLIP and VAE input image embeddings.
Then, we concatenate the target pose representation, consisting of 5 consecutive poses surrounding the target pose, to the input noise. During training, we ﬁne-tune the denoising UNet and our Adapter module on the full dataset and further perform subject-speciﬁc ﬁnetuning of the UNet, Adapter, and VAE decoder on a single input image. ject following the pose sequence (Figure 1).
This is a challenging task in several ways. While im-age diffusion models have shown impressive, high-quality results [16, 31, 35], video diffusion models have yet to achieve the same quality of results and are often limited to
“textural” motion or cartoon-like appearance [13, 17, 19, 40, 49]. Moreover, existing video diffusion models suffer from poor temporal consistency, motion jitter, lack of re-alism, and the inability to control the motion or detailed object appearance in the target video. This is partly be-cause existing models are primarily conditioned on text, as opposed to other conditioning signals (e.g., motion) which may offer more ﬁne-grained control. In contrast, our image-and-pose conditioning scheme allows for greater appear-ance ﬁdelity and frame-to-frame consistency.
Our model is ﬁne-tuned from an existing pretrained im-age diffusion model, which already effectively models the distribution of natural images. When using such a model, the task of image animation can effectively be simpliﬁed to
ﬁnding the subspace of natural images consistent with the conditioning signals. To accomplish this, we redesign the encoder and conditioning mechanisms of the Stable Dif-fusion [16] architecture, in order to enable aligned-image and unaligned-pose conditioning. Further, we propose a two-stage ﬁnetuning scheme that consists of ﬁnetuning both
UNet and VAE from one or more input images. results on a diverse range of garment patterns and shapes, (2) a simple, yet effective, pose conditioning approach that greatly improves temporal consistency across frames, (3) a split CLIP-VAE encoder that increases the output ﬁdelity to the conditioning image, (4) a ﬁnetuning strategy that ef-fectively balances image ﬁdelity and generalization to new poses. 2.