Abstract
We introduce a novel method to automatically generate an artistic typography by stylizing one or more letter fonts to visually convey the semantics of an input word, while ensuring that the output remains readable. To address an assortment of challenges with our task at hand including conflicting goals (artistic stylization vs. legibility), lack of ground truth, and immense search space, our approach uti-lizes large language models to bridge texts and visual im-ages for stylization and build an unsupervised generative model with a diffusion model backbone. Specifically, we employ the denoising generator in Latent Diffusion Model (LDM), with the key addition of a CNN-based discriminator to adapt the input style onto the input text. The discrimina-tor uses rasterized images of a given letter/word font as real samples and the output of the denoising generator as fake samples. Our model is coined DS-Fusion for discriminated and stylized diffusion. We showcase the quality and versa-tility of our method through numerous examples, qualita-tive and quantitative evaluation, and ablation studies. User studies comparing to strong baselines including CLIPDraw,
DALL-E 2, Stable Diffusion, as well as artist-crafted ty-pographies, demonstrate strong performance of DS-Fusion.
Code is available at https://ds-fusion.github.io/. 1.

Introduction
We explore the artistic-creative potential of automated generative processes modeled by modern neural networks.
Specifically, we are interested in the generation of artistic typography. According to Wikipedia, typography is the art and technique of arranging type to make written language legible, readable, and appealing when displayed. Artistic typography represents a style of typography that goes be-yond the basic function of conveying information through text and seeks to create a visual impact on the reader. It in-volves using typography as a form of artistic expression and allows designers to create eye-catching typographic designs that express a message visually and creatively.
In this paper, we aim to automatically generate an artistic typography by stylizing one or more letter fonts, to visually convey the semantics of an input word, while ensuring that the output typography is readable; see Figure 1 for such ex-amples referred to as “word-as-image” [3, 27, 34, 9]. It is an arduous task to combine semantics and text in a legible and artistic manner for several reasons. First, the goal of incorporating the aesthetics of a style in an abstract and cre-ative way into a letter or word can conflict with the desire to maintain readability of the original word/letter. Second, what a good artistic typography is can be a subjective mat-ter. Without a universally accepted “ground truth”, a viable learning approach will have to be unsupervised. Last but not least, semantics can be depicted in numerous ways. For instance, to indicate the presence of a lion, one can use the entire face, the tail, or the whole animal. There is a vast range of lion images, icons, and shapes that are accessible, making it nearly impossible to manually search, deform, and substitute them. While experienced artists and design-ers are capable of producing beautiful semantic typography,
Figure 2. The pipeline of DS-Fusion, which takes as input a style prompt and a glyph image. The style images are generated according to the style word and attribute. DS-Fusion first utilizes a latent diffusion process [21] to construct the latent space of the given style and then introduces a discriminator to blend the style into the glyph shape. The parameters of a module are pre-trained and frozen if there is an icon of a lock on the bottom right. The “+” module denotes the iterative noise injection process of diffusion models. obtaining reasonable results for ordinary users and hobby-ists without proper assistive tools are out of reach.
To address all the above challenges, we resort to recently popularized large language models [21, 19] to bridge texts and visual images for stylization and build our unsupervised generative model for artistic typography on Latent Diffu-sion [21]. Specifically, we employ the denoising generator in Latent Diffusion Model (LDM), with the key addition of a CNN-based discriminator to adapt the input style onto the input text (Figure 2). The discriminator uses rasterized images of a given letter/word font as real samples and out-put of the denoising generator as fake samples. To obtain images for the denoising generator to guide the letter styl-ization, we generate 25 style images from the input word, again, with Latent Diffusion Model. The selection of this number aims to ensure an adequate number of instances for the diffusion model to extract underlying features and attain diversity in the outputs. We fine-tune the denoising gener-ator on these images using the diffusion loss based on the style images and the discriminator loss.
Our model is coined DS-Fusion for discriminated and stylized diffusion. While the core idea is quite simple, it is among the first to integrate adversarial learning and diffu-sion in a single framework. By utilizing the powerful gener-ation capabilities of diffusion models and employing a dis-criminator as a critic, we ensure that the produced artistic typography remains true to the input font. Figure 1 shows some results generated by DS-Fusion fully automatically.
We showcase the effectiveness of DS-Fusion for generat-ing artistic typography through numerous experiments. Our approach produces visual results that demonstrate its gener-ality and versatility in accommodating different semantics, letters, and artistic styles. We report quantitative evaluations and ablation studies to assess the contribution of individual components. Additionally, we have conducted user studies to assess the quality of our automatically generated typog-raphy results. These studies reveal that in about 42% of the cases, our results were favored over or considered equally good as results produced by professional artists, and in close to 50% of cases, our method outperforms the state-of-the-art alternatives, including DALL-E 2 [20], a strong base-line that had been trained on significantly more images than
LDM. It is worth noting that we did not choose specific in-puts catering to DS-Fusion for these comparisons. Instead, we performed a Google image search on “artistic typogra-phy” and extracted a suitable subset of artist-generated re-sults to come up with inputs for both user studies. 2.