Abstract
The current interacting hand (IH) datasets are relatively simplistic in terms of background and texture, with hand joints being annotated by a machine annotator, which may result in inaccuracies, and the diversity of pose distribution is limited. However, the variability of background, pose distribution, and texture can greatly influence the gener-alization ability. Therefore, we present a large-scale syn-thetic dataset –RenderIH– for interacting hands with accu-rate and diverse pose annotations. The dataset contains 1M photo-realistic images with varied backgrounds, per-spectives, and hand textures. To generate natural and di-verse interacting poses, we propose a new pose optimiza-tion algorithm. Additionally, for better pose estimation accuracy, we introduce a transformer-based pose estima-tion network, TransHand, to leverage the correlation be-tween interacting hands and verify the effectiveness of Ren-derIH in improving results. Our dataset is model-agnostic and can improve more accuracy of any hand pose esti-mation method in comparison to other real or synthetic datasets. Experiments have shown that pretraining on our synthetic data can significantly decrease the error from 6.76mm to 5.79mm, and our Transhand surpasses contem-porary methods. Our dataset and code are available at https://github.com/adwardlee/RenderIH. 1.

Introduction 3D interacting hand (IH) pose estimation from a single
RGB image is a key task for human action understanding and has many applications, such as human-computer in-teraction, augmented and virtual reality, and sign language
*Corresponding author: 4065156@qq.com
Figure 1. Randomly selected samples from RenderIH dataset.
The rendered hands are realistic and varied, capturing a variety of poses, textures, backgrounds, and illuminations. recognition. However, obtaining 3D interacting hand pose annotations from real images is very challenging and time-consuming due to the severe self-occlusion problem. Some previous works [12, 18] have collected some real hand inter-action data using a sophisticated multi-view camera system and made manual annotations, but the amount of data is lim-ited. Synthetic 3D annotation data has become increasingly popular among researchers because of its easy acquisition and accurate annotation [27, 22, 3, 7, 15, 24, 40]. How-ever, there remain two main challenges: the validity of the generated 3D hand poses and the diversity and realism of the generated images. Therefore, in this paper, we present a high-fidelity synthetic dataset of 3D hand interaction poses for precise monocular hand pose estimation.
Firstly, ensuring the validity of the generated 3D inter-acting hand poses is a crucial challenge for a synthetic hand system. For example, the pose of Ego3d [22] is randomized which means a significant portion of the data is not valid. To ensure effective hand interactions, the generated two-hand poses must be proximal to each other, while increasing the risk of hand interpenetration. Therefore, we design an op-Dataset
Type
Data size MT AP background illumination Hand type
IH Size
NYU [31]
STB [39]
H2O-3D [12]
H2O [37]
MVHM [3]
ObMan [15]
DARTset [7] real real real real synthetic synthetic synthetic
----lab lab lab (cid:37) 243K (cid:37) 36K (cid:37) 76K (cid:37) indoor scenes 571K 320K (cid:37) (cid:37) static scenes 147K (cid:33) (cid:37) static scenes 800K (cid:33) (cid:37) static scenes uniform uniform uniform uniform uniform uniform manual
SH
SH
HO
HO
SH
HO
SH
-------real synthetic synthetic
IH2.6M [26]
Ego3d [22]
RenderIH (Ours) (cid:37) 2.6M 50K (cid:37) (cid:37) static scenes 1M (cid:33) (cid:33) HDR scenes
Table 1. Comparison of the related hand datasets. “MT” is short for multi-textures and means whether the hand models in the dataset are assigned with diverse textures, AP is short for anti-penetration, “Hand type” means which interaction type the dataset focus on (SH-single hand, HO-hand to object, IH-hand to hand), and “IH Size” means the proportion of IH poses. “HDR” is short for High Dynamic Range.
Static scenes refer to the use of randomly selected images as the background. uniform random dynamic 628K 40K 1M
IH
IH
IH lab
-timization process that considers the constraints of hand at-traction and anti-penetration in the meantime, to ensure the proximity of two interacting hands and prevent the occur-rence of hand penetration (Section 3.1).
In addition, the plausibility of hand poses must also be considered. Hence, we introduce anatomic pose constraints and apply adversar-ial learning to ensure that the generated hand poses adhere to anatomical constraints and realism. Benefiting from pose optimization, our generated dataset contains a rich set of validated two-hand interaction poses as shown in Figure 1.
Secondly, most existing 3D synthetic hand images lack diversity in terms of backgrounds, lighting, and texture con-ditions, which prevents them from capturing the complex distribution of real hand data [22, 3, 15]. Most existing datasets for hand gesture recognition, such as Ego3d [22],
Obman [15], and MVHM [3], do not consider the quality and diversity of the images. For instance, Ego3d [22] uses the same texture as the MANO model [29], which is un-realistic and monotonous. In contrast, our rendering sys-tem introduces various textures, backgrounds, and lighting effects that can produce vivid and realistic synthetic hand images (see Section 3.2). By combining HDR background, dynamic lighting, and ray-tracing renderer, we obtain 1M high-quality gesture images (see Figure 1).
To assess the performance of our proposed dataset, we carried out comprehensive experiments on it. We demon-strate how much we can reduce the dependency on real data by using our synthetic dataset. Then we contrast our pro-posed RenderIH with other 3D hand datasets, such as H2O-3D [12] and Ego3d [22], by training a probing model for each of them and testing on a third-party dataset. Finally, we train a transformer-based network on a mixed dataset of
RenderIH and InterHand2.6M (IH2.6M) and achieve state-of-the-art (SOTA) results on 3D interacting hand pose esti-mation. Our main contributions are as follows:
• We propose an optimization method to generate valid and natural hand-interacting poses that are tightly coupled and avoid interpenetration. For image generation, we design a high-quality image synthesis system that combines rich textures, backgrounds, and lighting, which ensures the di-versity and realism of the generated images.
• Based on our data generation system, we construct a large-scale high-fidelity synthetic interacting hand dataset called RenderIH, which contains 1 million synthetic im-ages and 100K interacting hand poses. To the best of our knowledge, this is the largest and most high-quality syn-thetic interacting dataset so far.
• We conduct extensive experiments to verify the effec-tiveness of our proposed dataset-RenderIH. The results show that with the help of our synthetic dataset, using only 10% of real data can achieve comparable accuracy as the models trained on real hand data. We also propose a transformer-based network that leverages our dataset and achieves SOTA results. 2.