Abstract
The co-occurrence signals (e.g., hand shape, facial ex-pression, and lip pattern) play a critical role in Continu-ous Sign Language Recognition (CSLR). Compared to RGB data, skeleton data provide a more efficient and concise option, and lay a good foundation for the co-occurrence exploration in CSLR. However, skeleton data are often used as a tool to assist visual grounding and have not at-tracted sufficient attention.
In this paper, we propose a simple yet effective GCN-based approach, named CoSign, to incorporate Co-occurrence Signals and explore the po-tential of skeleton data in CSLR. Specifically, we pro-pose a group-specific GCN to better exploit the knowl-edge of each signal and a complementary regularization to prevent complex co-adaptation across signals. Fur-thermore, we propose a two-stream framework that grad-ually fuses both static and dynamic information in skeleton data. Experimental results on three public CSLR datasets (PHOENIX14, PHOENIX14-T and CSL-Daily) show that the proposed CoSign achieves competitive performance with recent video-based approaches while reducing the computation cost during training. 1.

Introduction
Sign languages, as the primary means of communication within the Deaf community, are naturally evolved and di-versely structured systems in a rule-governed way [2]. Due to their unique physical transmission system, the grammar and vocabulary of sign languages differ greatly from that of spoken languages. To provide a convenient channel be-tween the Deaf and hearing people, vision-based Sign Lan-guage Recognition (SLR) has attracted much attention [23] and recent works can be roughly divided into Isolated Sign
Figure 1. Two examples of signs, EUROPA (Europe, the upper) and UNWETTER (storm, the lower), from PHOENIX14 dataset.
SignWriting [43] entries (lexical notations) are positioned at the bottom left-hand corner of images and the occurred signals (B,
RH, LH, M and F represent body, right hand, left hand, mouth and face, respectively) are marked on the left side of the images.
EUROPA is mainly signed with mouth, right hand and body, and nearly all signals occurred in UNWETTER.
Language Recognition (ISLR) [28, 18] and Continuous
Sign Language Recognition (CSLR) [1, 30].
Different to ISLR which predicts the corresponding gloss from a segmented sign video, CSLR aims to recognize a sequence of glosses from a continuous image sequence and is more common in real-life applications. Video-based
CSLR develops rapidly in the last few years [13, 35, 9, 32, 5, 17, 30, 6]. However, video-based approaches are sensitive to background and illumination changes [21] and may in-troduce privacy concerns [3]. Meanwhile, the computation cost is a considerable problem because sign videos contain much visual redundancy and recent CSLR approaches usu-ally extract visual features in a frame-by-frame way. Com-pared with videos, skeleton data provide a more concise and efficient representation for human body.
Recent single person pose estimation solutions [42, 16] have achieved high performance in complex scenes and are widely used in action recognition [48, 15]. Different to ac-tion recognition, CSLR focuses more on fine-grained infor-mation. However, recent skeleton-based methods [8, 33] achieve inferior performance compared with video-based approaches in CSLR, and some works [50, 8] leverage skeleton data to assist the learning of video data. These findings raise an interesting question: what is the obstacle that prevents the utilization of skeleton data in CSLR?
As shown in Fig. 1, sign language conveys information through both manual signals (hand shape, orientation, place of articulation and movement) and non-manual signals (lip pattern, facial expression, head and upper body orienta-tion) [14, 24]. For skeleton data, these synchronous signals can be easily modeled by the interactions among different groups of keypoints, but directly treating all keypoints as a whole may prevent the model from learning these co-occurrence signals. Therefore, we argue that efficiently modeling these co-occurrence signals is the key to boost the performance of skeleton-based CSLR method.
To incorporate co-occurrence signals, we propose a simple yet effective Graph Convolution Network (GCN) based approach, named CoSign, which consists of a group-specific GCN and a complementary regularization. The group-specific GCN contains several customized modules to independently process different signals. Specifically, the estimated skeleton data are first divided into five groups: body, left hand, right hand, mouth, and face, and then the keypoints of each group are sent to the corresponding group-specific module to process each signal. Compared to directly treating the human skeleton as a whole, this design can better exploit the knowledge of each signal.
Due to the fast movement and heavy self-occlusion of hands, off-the-shelf estimators often miss or predict inac-curate keypoints, which may affect the accuracy of CSLR models. The proposed complementary regularization en-courages the consistency between predictions based on two complementary subsets of signals, which can make bet-ter use of signals with different intensities and relieve the effects of inaccurate estimations. Moreover, we design a two-stream framework to explicitly capture static and dy-namic information from skeleton data and gradually fuse them through an extra fusion branch.
In conclusion, this paper focuses on the utilization of skeleton data in CSLR. We propose the CoSign to exploit the co-occurrence signals in sign language. Experimental results on several popular CSLR datasets show that the pro-posed CoSign can achieve comparable results with video-based approaches while reducing the training cost.
The main contributions are summarized as follows:
• Exploring the potential of skeleton data in CSLR, and attributing the key to utilizing co-occurrence signals.
• Proposing a group-specific GCN to exploit the knowl-edge of each signal in sign language independently.
• Proposing a complementary regularization to handle noisy skeleton input and co-adaptation across signals.
• Designing a two-stream framework to gradually fuse static and dynamic information in skeleton data. 2.