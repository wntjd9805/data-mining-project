Abstract
Referring Expression Segmentation (RES) is a widely ex-plored multi-modal task, which endeavors to segment the pre-existing object within a single image with a given lin-guistic expression. However, in broader real-world scenar-ios, it is not always possible to determine if the described object exists in a specific image. Generally, a collection of images is available, some of which potentially contain the target objects. To this end, we propose a more realistic setting, named Group-wise Referring Expression Segmen-tation (GRES), which expands RES to a group of related images, allowing the described objects to exist in a subset of the input image group. To support this new setting, we introduce an elaborately compiled dataset named Grouped
Referring Dataset (GRD), containing complete group-wise annotations of the target objects described by given expres-sions. Moreover, we also present a baseline method named
Grouped Referring Segmenter (GRSer), which explicitly captures the language-vision and intra-group vision-vision interactions to achieve state-of-the-art results on the pro-posed GRES setting and related tasks, such as Co-Salient
Object Detection and traditional RES. Our dataset and codes are publicly released in https://github.com/shikras/d-cube. 1.

Introduction
Segmenting target objects described by users in a col-lection of images is a fundamental but overlooked capabil-ity that facilitates various real-world applications (as illus-trated in Fig. 1), such as Internet images automatic label-ing, multi-monitors joint event discovery, and mobile al-bum retrieval. In recent years, Referring Expression Seg-mentation (RES) has become a research hotspot with great potentials to solve this demand. Various promising ap-proaches [17, 38, 44, 6, 14] and datasets [21, 46, 36, 42] have contributed to significant advancements in this field.
* Equal Contribution
Figure 1: Real-world applications of our proposed Group-wise Referring Expression Segmentation (GRES) setting, which facilitates Internet images automatic labeling (up-per), multi-monitors joint event discovery (lower), etc.
However, the setting of RES is overly idealistic, which aims to segment what has been known to exist in a single image.
This has restricted the practicality of RES in real-world sit-uations, given that it is not always possible to determine if the described object exists in a specific image. Generally, a group of images is available, only some of which may po-tentially contain the target objects.
To address this limitation, in this paper, we introduce a new realistic setting, namely Group-wise Referring Ex-pression Segmentation (GRES), and define it as segmenting objects described in language expression from a group of related images. Compared with traditional RES task, our
GRES setting get extended in two points: (a) tolerance for negative images where no target object exists, and (b) si-multaneous processing of a group of images. Note that the application scope of GRES is the superset of RES, which means (a) or (b) can be omitted based on the actual sce-narios. Moreover, we establish the foundation of GRES in two aspects: firstly, a baseline method named Grouped
Referring Segmenter (GRSer) that explicitly leverages lan-guage and intra-group vision connections to obtain promis-g
O
C
O
C f e
R
D
R
G g
O
C
O
C f e
R
D
R
G (a) Annotation completeness (b) Annotation fineness
Figure 2: Proposed GRD vs. RefCOCOg on the annotation completeness and annotation fineness. ing results, and secondly, a meticulously annotated dataset,
Group Referring Dataset (GRD), that ensures complete an-notations of described objects across all images in a group.
Our proposed baseline method GRSer, illustrated in Fig. 3, facilitates a simultaneous processing of multiple input images with an expression, and generates segmentation masks for all described objects. We devise a Triphasic
Query Module (TQM), where the target objects not only queried by linguistic features, but also by intra-group vi-sual features. In contrast to segmenting based solely on lin-guistic expression, querying target objects with intra-group homo-modal visual features bridges the modal gap and as-sembles a more precise target concept.
In the proposed
Heatmap Hierarchizer, heatmaps generated by intra-group visual querying are ranked based on their confidences, and then jointly used to predict segmentation masks in condition of the ranking priorities. Furthermore, we propose a mir-ror training strategy and triplet loss to learn anti-expression features, which are crucial for the TQM and Heatmap Hier-archizer, and enable GRSer to comprehend the image back-ground and negative samples. The promising performance of GRSer makes it a strong research baseline for our pro-posed GRES setting.
To facilitate the research in novel GRES setting, the
GRD dataset is introduced, which effectively overcomes the incomplete annotation problem in current RES datasets [21, 46, 36]. For example, in Fig. 2 (a), RefCOCOg’s expression of the 1st image also corresponds to objects in images 2, 3, and 4, but the target objects are not annotated. This leads to false positive samples during evaluation even though the target objects are correctly segmented by the model. In con-trast, in our GRD dataset, the described objects are com-pletely annotated in all images across the dataset, including images without the targets or with multiple targets. Addi-tionally, GRD collects images from Internet search engines by a group of keywords, where negative samples inherently exist in each group, making them hard negatives and effec-tively increasing the dataset’s difficulty. Finally, as shown in
Fig. 2 (b), compared with current RES datasets, GRD care-fully labels details in segmentation masks, such as block-ing and hollowing-out regions, which contributes to a more objective and reliable evaluation efficacy for model perfor-mances.
Our contributions can be summarized as:
• We formalize a Group-wise Referring Expression Seg-mentation (GRES) setting over the RES task, which advances user-specified object segmentation towards more practical applications.
• To support GRES research, we present a meticulously compiled dataset named GRD, possessing complete group-wise annotations of target objects. The dataset will also benefit various other vision-language tasks.
• Extensive experiments show the effectiveness and gen-eralizability of our proposed baseline method GRSer, which achieves SOTA results on the GRES and related tasks, such as Co-Salient Object Detection and RES. 2.