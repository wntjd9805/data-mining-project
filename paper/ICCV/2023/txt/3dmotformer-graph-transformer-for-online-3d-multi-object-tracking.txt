Abstract
Tracking 3D objects accurately and consistently is cru-cial for autonomous vehicles, enabling more reliable down-stream tasks such as trajectory prediction and motion plan-ning. Based on the substantial progress in object detec-tion in recent years, the tracking-by-detection paradigm has become a popular choice due to its simplicity and ef-ficiency. State-of-the-art 3D multi-object tracking (MOT) approaches typically rely on non-learned model-based al-gorithms such as Kalman Filter but require many manu-ally tuned parameters. On the other hand, learning-based approaches face the problem of adapting the training to the online setting, leading to inevitable distribution mis-match between training and inference as well as suboptimal performance. In this work, we propose 3DMOTFormer, a learned geometry-based 3D MOT framework building upon the transformer architecture. We use an Edge-Augmented
Graph Transformer to reason on the track-detection bi-partite graph frame-by-frame and conduct data associa-tion via edge classification. To reduce the distribution mis-match between training and inference, we propose a novel online training strategy with an autoregressive and recur-rent forward pass as well as sequential batch optimiza-tion. Using CenterPoint detections, our approach achieves 71.2% and 68.2% AMOTA on the nuScenes validation and test split, respectively.
In addition, a trained 3DMOT-Former model generalizes well across different object de-tectors. Code is available at: https://github.com/ dsx0511/3DMOTFormer. 1.

Introduction 3D multi-object tracking (MOT) is a fundamental task in many applications such as autonomous driving and mobile robots, aiming at localization, classification and persistent
Figure 1. We propose 3DMOTFormer that reasons on the track-detection bipartite graph and estimates the data association using a Graph Transformer. Tracks for the next frame are generated by the matching and track update module autoregressively. identification of surrounding objects over time. Especially accurate and consistent online tracking is of great impor-tance for downstream tasks such as trajectory prediction, motion planning and robot navigation.
Due to recent advances in object detection performance, the tracking-by-detection paradigm has become a popular choice to accomplish MOT [5, 41, 45, 38, 42, 17]. Most tracking-by-detection approaches utilize detections from every frame generated with an off-the-shelf object detector and focus on associating the detection results across frames.
State-of-the-art tracking-by-detection methods typically use non-learned algorithms, e.g. Kalman Filters [38, 17, 3, 15], with a pre-defined motion assumption, e.g. con-stant turn rate and velocity (CTRV) model, followed by a geometric association metric, e.g. center distance or 3D
IoU, which requires a lot of handcrafting and heuristics.
Learning-based approaches on the other hand aim at reduc-ing heuristics but face the challenge of lifting the training to the online inference setting. Some approaches [8, 39, 14] adopt a teacher-forcing [40] training using ground truth trajectories with annotated instance IDs and/or annotated bounding boxes. However, during online inference, the net-work has to associate noisy detections to the tracked trajec-tories containing false associations caused by the network itself. This results in a strong distribution mismatch or over-fitting despite applying plenty of data augmentations. An-other line of works [19, 43, 14] regards detections as nodes in a spatiotemporal graph and applies Neural Message Pass-ing (NMP) [11]. However, for online MOT, these methods require a graph with a fixed time window to evolve frame-by-frame, while the training is done on the static graph with the same time window but without dynamic evolving.
OGR3MOT [43] worked on adapting this graph representa-tion to the online setting but still uses a semi-online training and uses an additional heuristic track update for inference.
In this work, we present a novel transformer-based 3D
MOT framework, which we call 3DMOTFormer, that is learnable and relies only on geometric cues, as shown in Figure 1. Our model iteratively reasons on the relation-ship between existing tracks as well as detections in a new frame and conducts association using edge classification. A greedy matching and a simple track update module generate tracks as input for the next frame, yielding an autoregres-sive loop. This results in a bipartite graph representation between tracks and detections. In contrast to existing ap-proaches that process a spatiotemporal graph with a fixed time window [43, 14], we directly feed the processed track features into the new frame as initial track features to ac-cess temporal information, similar to the hidden states in
RNNs. To tackle the different operation modes between training and test time, we propose a novel fully online train-ing strategy which consists of an autoregressive forward pass and a sequential batch backward pass. Concretely, identical to the inference phase, our model evolves frame-by-frame autoregressively on sampled sequence clips dur-ing training, instead of modelling the sequence in a graph as a whole. We accumulate the loss at each frame and op-timize the network after the whole training sequence was processed. The forward pass fully simulates the operation mode and the data distribution during the online inference phase, while the optimization method learns to recover from errors and considers the whole sequence. Considering the remarkable achievements of autoregressive models based on transformers in natural language processing [35, 28, 6], we use Edge-Augmented Graph Transformers [12], a vari-ant of transformers that generalizes to sparse graphs and takes edge features into account for attention calculation.
Also, structural information in the bipartite graphs between tracks and detections can be effectively captured using cross-attention, which justifies transformer-based models as a suitable choice for our MOT framework.
We evaluate our method on the nuScenes [7] tracking benchmark using CenterPoint detections [42] as input. Our method achieves 71.2% and 68.2% AMOTA on the vali-dation and test split, respectively, yielding state-of-the-art performance among all geometry-based approaches. We show the generalization of 3DMOTFormer where a frozen 3DMOTFormer model still achieves competitive perfor-mance when inferring on detections from another detector.
Our main contributions can be summarized as follows:
• We propose 3DMOTFormer, a novel online 3D MOT framework based on Edge Augmented Graph Trans-formers [12] for learning data association, which re-duces the need for handcrafted components compared to previous state-of-the-art.
• 3DMOTFormer is tailored towards an online training strategy for MOT, which fully mimics the setup and hence the data distribution during online inference.
• Our method achieves state-of-the-art performance, in particular 71.2% and 68.2% AMOTA on the nuScenes [7] validation and test split, respectively, us-ing CenterPoint detections [42] as input.
• 3DMOTFormer achieves competitive performance when inferring on detections from another detector.
This allows flexible deployment of the same 3DMOT-Former model independent of the object detector. 2.