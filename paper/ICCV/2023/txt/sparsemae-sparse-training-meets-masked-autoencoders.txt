Abstract
Masked Autoencoders (MAE) and its variants have proven to be effective for pretraining large-scale Vision
Transformers (ViTs). However, small-scale models do not benefit from the pretraining mechanisms due to limited ca-pacity. Sparse training is a method of transferring represen-tations from large models to small ones by pruning unim-portant parameters. However, naively combining MAE fine-tuning with sparse training make the network task-specific, resulting in the loss of task-agnostic knowledge, which is crucial for model generalization.
In this paper, we aim to reduce model complexity from large vision transformers pretrained by MAE with assistant of sparse training. We summarize various sparse training methods to prune large vision transformers during MAE pretraining and finetun-ing stages, and discuss their shortcomings. To improve learning both task-agnostic and task-specific knowledge, we propose SparseMAE, a novel two-stage sparse train-ing method that includes sparse pretraining and sparse finetuning. In sparse pretraining, we dynamically prune a small-scale sub-network from a ViT-Base. During finetun-ing, the sparse sub-network adaptively changes its topology connections under the task-agnostic knowledge of the full model. Extensive experimental results demonstrate the ef-fectiveness of our method and its superiority on small-scale vision transformers. Code will be available at https:
//github.com/aojunzz/SparseMAE. 1.

Introduction
Recently, several pretrained Masked Image Model-ing (MIM) methods, such as MAE [15] and data2vec [1], have achieved great success in various computer vision tasks.
They usually involve a two-stage training scheme where the model learns task-agnostic knowledge through a pretrain-ing pretext task, and is subsequently finetuned on down-stream tasks to acquire task-specific knowledge. Among
*The first two authors equally contribute to this paper. these models, Masked Autoencoders (MAE) demonstrate superior learning capability on large-scale vision transform-ers (Fig. 1 (a)), thanks to large models’ strong capacity to learn powerful general representations in the pretraining phase and versatile transferability to specific vision tasks.
Yet, the large deep models are burdensome and difficult to be deployed to computationally restricted real-world scenarios.
On the other hand, smaller and more efficient models, such as ViT-Tiny and ViT-Small, fail to perform well after pre-trained by MAE or data2vec frameworks. For example, ViT-Tiny’s finetuning results are even inferior to fully-supervised training (see Fig. 1 (b)). These empirical findings suggest that, under MIM frameworks, when the model is scaled down, it becomes more difficult to learn well via masked pretraining due to its limited capacity during pretraining and consequently hinders its performance when transferred to downstream tasks. In the realm of MIM pretraining, model capacity becomes a key ingredient to learning task-agnostic knowledge via the pretext tasks.
In our studies, we try to tackle the issues of MIM pre-training for small-scale vision transformers. Particularly, we apply pruning techniques [28, 20, 4, 42, 23] in the MIM frameworks in order to obtain performant small-scale models from larger ones. As a first step in our exploration, we use existing sparse training methods [42, 23] for MAE pretrain-ing and finetuning on unstructured and hardware-friendly
N:M sparsity to obtain a model with a similar scale to ViT-Tiny, which is pruned from ViT-Base (see Tab. 5 row 3). It achieves notably improvements compared to directly training
ViT-Tiny using MAE on downstream ImageNet finetuning accuracy (77.6% vs. 71.6%) and ADE20K [43] semantic segmentation (39.8 mIoU vs. 37.6 mIoU). The empirical results indicate that on small-scale ViTs sparse training via pruning has promising applications under the MIM pretrain-ing paradigm. However, there is still a large performance gap between the sparse model and the vanilla ViT-Base trained densely under MAE in their ImageNet finetuning accura-cies (77.6% vs. 83.2%). The sparsity constraints adversely limit the model capacity and prevent it from learning good task-agnostic knowledge, similar to the predicament faced
Specifically, we design three different strategies to train sparse networks under MAE and analyze their limita-tions.
• Based on the findings from above, we propose a novel sparse training framework, SparseMAE for Masked
Autoencoders to improve the acquisition of both the task-agnostic and task-specific knowledge during the pretraining and finetuning stages.
• We present extensive experiments to validate the effec-tiveness of our proposed method and achieve state-of-the-art performance with small-scale vision transform-ers on ImageNet classification and various downstream tasks. 2.