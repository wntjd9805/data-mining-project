Abstract
Exploiting spatial-angular correlation is crucial to light field (LF) image super-resolution (SR), but is highly chal-lenging due to its non-local property caused by the dis-parities among LF images. Although many deep neural networks (DNNs) have been developed for LF image SR and achieved continuously improved performance, exist-ing methods cannot well leverage the long-range spatial-angular correlation and thus suffer a significant perfor-mance drop when handling scenes with large disparity vari-ations.
In this paper, we propose a simple yet effective method to learn the non-local spatial-angular correlation for LF image SR. In our method, we adopt the epipolar plane image (EPI) representation to project the 4D spatial-angular correlation onto multiple 2D EPI planes, and then develop a Transformer network with repetitive self-attention operations to learn the spatial-angular correlation by mod-eling the dependencies between each pair of EPI pixels.
Our method can fully incorporate the information from all angular views while achieving a global receptive field along the epipolar line. We conduct extensive experiments with insightful visualizations to validate the effectiveness of our method. Comparative results on five public datasets show that our method not only achieves state-of-the-art SR per-formance but also performs robust to disparity variations.
Code is publicly available at https://github.com/
ZhengyuLiang24/EPIT. 1.

Introduction
Light field (LF) cameras record both intensity and direc-tions of light rays, and enable various applications such as depth perception [25, 29, 32], view rendering [3, 52, 66], virtual reality [11, 74], and 3D reconstruction [6, 77]. How-ever, due to the inherent spatial-angular trade-off [82], an
LF camera can either provide dense angular samplings with low-resolution (LR) sub-aperture images (SAIs), or capture high-resolution (HR) SAIs with sparse angular sampling.
Figure 1. Visualization of 4Ã— SR results and the corresponding at-tribution maps [18] of our method and four state-of-the-art meth-ods [26, 36, 60, 78] under different manually sheared disparity values. Here, the patch marked by the green box in the HR image is selected as the target region, and the regions that contribute to the final SR results are highlighted in red. Our method can well exploit the non-local spatial-angular correlation and achieve supe-rior SR performance. More examples are provided in Fig. 8.
To handle this problem, many methods have been proposed to enhance the angular resolution via novel view synthesis
[28, 43, 67], or enhance the spatial resolution by performing
LF image super-resolution (SR) [10, 20]. In this paper, we focus on the latter task, i.e., generating HR LF images from their LR counterparts.
Recently, convolutional neural networks (CNNs) have been widely applied to LF image SR and demonstrated su-perior performance over traditional paradigms [1, 34, 44, 49, 64]. To incorporate the complementary information (i.e., angular information) from different views, existing
CNNs adopted various mechanisms such as adjacent-view combination [73], view-stack integration [26, 78, 79], bidi-rectional recurrent fusion [59], spatial-angular disentangle-ment [36, 60, 61, 72, 9], and 4D convolutions [42, 43].
However, as illustrated in both Fig. 1 and Sec. 4.3, existing methods achieve promising results on LFs with small base-lines, but suffer a notable performance drop when handling scenes with large disparity variations.
We attribute this performance drop to the contradictions between the local receptive field of CNNs and the require-ment of incorporating non-local spatial-angular correlation in LF image SR. That is, LF images provide multiple obser-vations of a scene from a number of regularly distributed viewpoints, and a scene point is projected onto different but correlated spatial locations on different angular views, which is termed as the spatial-angular correlation. Note that, the spatial-angular correlation has the non-local prop-erty since the difference between the spatial locations of two views (i.e., disparity value) depends on several factors (e.g., angular coordinates of the selected views, the depth value of the scene point, the baseline length of the LF camera, and the resolution of LF images), and can be very large in some situations. Consequently, it is appealing for LF im-age SR methods to incorporate complementary information from different views by exploiting the spatial-angular cor-relation under large disparity variations.
In this paper, we propose a simple yet effective method to learn the non-local spatial-angular correlation for LF im-age SR. In our method, we re-organize 4D LFs as multi-ple 2D epipolar plane images (EPIs) to manifest the spatial-angular correlation to the line patterns with different slopes.
Then, we develop a Transformer-based network called EPIT to learn the spatial-angular correlation by modeling the de-pendencies between each pair of pixels on EPIs. Specif-ically, we design a basic Transformer block to alternately process horizontal and vertical EPIs, and thus progressively incorporate the complementary information from all angu-lar views. Compared to existing LF image SR methods, our method can achieve a global receptive field along the epipo-lar line, and thus performs robust to disparity variations.
In summary, the contributions of this work are as fol-(1) We address the importance of exploiting non-lows: local spatial-angular correlation in LF image SR, and pro-pose a simple yet effective method to handle this problem. (2) We develop a Transformer-based network to learn the non-local spatial-angular correlation from horizontal and vertical EPIs, and validate the effectiveness of our method through extensive experiments and visualizations. (3) Com-pared to existing state-of-the-art LF image SR methods, our method achieves superior performance on public LF datasets, and is much more robust to disparity variations. 2.