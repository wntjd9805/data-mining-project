Abstract
Despite significant progress in single image-based 3D human mesh recovery, accurately and smoothly recovering 3D human motion from a video remains challenging. Ex-isting video-based methods generally recover human mesh by estimating the complex pose and shape parameters from coupled image features, whose high complexity and low rep-resentation ability often result in inconsistent pose motion and limited shape patterns. To alleviate this issue, we in-troduce 3D pose as the intermediary and propose a Pose and Mesh Co-Evolution network (PMCE) that decouples this task into two parts: 1) video-based 3D human pose esti-mation and 2) mesh vertices regression from the estimated 3D pose and temporal image feature. Specifically, we pro-pose a two-stream encoder that estimates mid-frame 3D pose and extracts a temporal image feature from the input image sequence. In addition, we design a co-evolution decoder that performs pose and mesh interactions with the image-guided Adaptive Layer Normalization (AdaLN) to make pose and mesh fit the human body shape. Extensive experiments demonstrate that the proposed PMCE outperforms previous state-of-the-art methods in terms of both per-frame accu-racy and temporal consistency on three benchmark datasets: 3DPW, Human3.6M, and MPI-INF-3DHP. Our code is avail-able at https://github.com/kasvii/PMCE. 1.

Introduction
Recovering 3D human mesh from an image or a video is an essential yet challenging task for many applications, such as human-robot interaction, virtual reality, and motion analysis. The challenges of this task arise from the 2D-to-3D ambiguity, cluttered background, and occlusions. Recently, many studies [8, 13, 16, 19, 22, 33] have been proposed to recover the 3D human mesh from a single image, which can generally be categorized into RGB-based methods and pose-based methods. RGB-based methods predict human mesh
B Corresponding author.
Figure 1: Comparison with video-based 3D human mesh recovery methods upon accuracy (MPJPE ↓) and temporal consistency (acceleration error ↓) on 3DPW [40] (left) and
Human3.6M [12] (right) datasets. end-to-end from image pixels, typically predicting the pose and shape parameters of the parametric human model (e.g.,
SMPL [27]) to generate the 3D human mesh. However, the representation ability of the parametric model is con-strained by the limited pose and shape space [18, 19]. To overcome this limitation, non-parametric approaches have been proposed to predict the 3D coordinates of mesh ver-tices directly, which generally use Graph Convolutional Net-works (GCNs) [8, 42] or Transformers [5, 24, 51] to capture the relations among vertices. In contrast, pose-based meth-ods leverage 2D pose detectors [4, 36] as the front-end to recover human mesh from the detected 2D poses. With the significant advancements in 2D pose detection, pose-based methods have become increasingly robust and lightweight, making them popular for real-world applications [51].
Despite the significant progress in single-image 3D hu-man mesh recovery, these methods still struggle to capture temporally consistent human motion from videos. To solve this problem, several works [7, 15, 28, 43] extend single image-based methods to video cases. They use a pre-trained
Convolutional Neural Network (CNN) [17] to extract static features for each frame, then train a temporal network to pre-dict SMPL parameters. Although these video-based methods significantly improve the temporal consistency of 3D human motion, there exists a trade-off between per-frame accuracy
and motion smoothness for the following two main reasons: 1) The highly coupled image feature. The extracted image features in deep CNN layers are low-resolution and tightly coupled [37], which inevitably discard the spatial informa-tion in the image [41]. 2) The limited representation ability of the parametric human model. The SMPL model repre-sents pose using 3D rotation, which might face periodicity and discontinuity issues [18], making pose prediction in videos more challenging. Besides, the local and swift mesh deformations described by the shape parameters are difficult to learn.
In recent years, video-based 3D human pose estimation from the detected 2D poses has achieved high pose accuracy and motion smoothness [21, 50], which inspires us that the skeleton sequence contains sufficient spatial and temporal pose information of human motion. But for the mesh recov-ery task, detailed shape information is needed, which can be acquired from the image features. Based on the above observations, we propose the Pose and Mesh Co-Evolution network (PMCE) to recover 3D human mesh from videos in a non-parametric way. We decouple the 3D human mesh recovery task into two consecutive parts: 1) video-based 3D pose estimation and 2) mesh vertices regression from 3D pose and image feature, where the latter is the focus. Specif-ically, in the first part, we propose a two-stream encoder.
One stream takes a 2D pose sequence detected from input images to estimate the mid-frame 3D pose, and the other stream takes static image features extracted from images and aggregates them for a temporal image feature. In the second part, we design a co-evolution decoder that performs pose and mesh interactions with an image-guided Adaptive
Layer Normalization (AdaLN). AdaLN adjusts the statisti-cal characteristics of joint and vertex features to make the pose and mesh better fit the human body shape. As shown in Figure 1, compared to previous video-based methods,
PMCE achieves better performance in terms of per-frame accuracy and temporal consistency on 3DPW [40] and Hu-man3.6M [12] datasets.
In summary, our contributions are as follows:
• We propose a Pose and Mesh Co-Evolution net-work (PMCE) for recovering 3D human mesh from video. It decouples the task into two parts: video-based 3D pose estimation, and mesh vertices regression by image-guided pose and mesh co-evolution, achieving accurate and temporally consistent results.
• We design the co-evolution decoder that performs pose and mesh interactions guided by our proposed AdaLN.
AdaLN adjusts the statistical characteristics of joint and vertex features based on the image feature to make them conform to the human body shape.
• Our method achieves state-of-the-art performance on challenging datasets like 3DPW, reducing MPJPE by 12.1%, PVE by 8.4% and acceleration error by 8.5%. 2.