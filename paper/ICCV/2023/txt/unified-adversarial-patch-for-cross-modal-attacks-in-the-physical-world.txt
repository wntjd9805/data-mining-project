Abstract
Recently, physical adversarial attacks have been pre-sented to evade DNNs-based object detectors. To ensure the security, many scenarios are simultaneously deployed with visible sensors and infrared sensors, leading to the failures of these single-modal physical attacks. To show the poten-tial risks under such scenes, we propose a unified adversar-ial patch to perform cross-modal physical attacks, i.e., fool-ing visible and infrared object detectors at the same time via a single patch. Considering different imaging mecha-nisms of visible and infrared sensors, our work focuses on modeling the shapes of adversarial patches, which can be captured in different modalities when they change. To this end, we design a novel boundary-limited shape optimiza-tion to achieve the compact and smooth shapes, and thus they can be easily implemented in the physical world. In ad-dition, to balance the fooling degree between visible detec-tor and infrared detector during the optimization process, we propose a score-aware iterative evaluation, which can guide the adversarial patch to iteratively reduce the pre-dicted scores of the multi-modal sensors. We finally test our method against the one-stage detector: YOLOv3 and the two-stage detector: Faster RCNN. Results show that our unified patch achieves an Attack Success Rate (ASR) of 73.33% and 69.17%, respectively. More importantly, we verify the effective attacks in the physical world when visi-ble and infrared sensors shoot the objects under various set-tings like different angles, distances, postures, and scenes. 1.

Introduction
Deep Neural Networks (DNNs) are vulnerable to adver-sarial examples [18], which pose a serious security threat to DNNs-based object detectors in the physical world[31, 34, 33]. These physical attacks can help to evaluate the
*Corresponding author
Table 1. Various adversarial attacks in different settings.
Physical world
[20], [17] [10],[6]
[26],[29],[33] etc.
Ours
Digital world
[28],[4],[30],[8],
[11],[13] etc.
[1],[21]etc.
Cross-modal
Single-modal robustness of DNNs deployed in real-life systems, which have important practical values. Recently, RGB sensors and thermal infrared sensors have been simultaneously used in many safety-critical tasks such as security monitoring.
When performing object detection tasks, visible images could provide abundant information on the target’s texture in the daytime, while infrared images could display the tar-get’s thermal distribution at night. Thus, combining visible images and infrared images together will result in a round-the-clock application. To evade object detectors under such multi-modal imaging scenarios, a necessary way is to de-velop a cross-modal physical attack to fool visible object detectors and infrared object detectors at the same time.
However, current physical attacks are limited in the single-modal domain. For example, some studies [20, 6, 27] hide from object detectors in the visible modality, and some studies [34, 33, 25] hide from object detectors in the infrared modality. Because visible sensors and infrared sensors have different imaging mechanisms, these single-modal physical attacks cannot simultaneously attack multi-modal object detectors. Specifically, the generated pertur-bations in the visible modality can not be captured by in-frared sensors, and in turn, changing the object’s thermal radiation would not be reflected in the visible light domain.
There also exist some cross-modal attacks in the digital world [1, 21, 24], but they usually aim to modify the im-age pixels or point clouds after sensors’ imaging, ignoring the different imaging mechanisms across sensors, and thus cannot transfer to physical world well.
Based on these discussions, this paper aims to design a unified cross-modal attack in the physical world to fill in the gap of this area, as illustrated in Table 1. Generally speaking, the lack of features that can operate in different
the first work to simultaneously evade visible detectors and infrared detectors in the physical world.
• We design two novel techniques: boundary-limited shape optimization and score-aware iterative evalua-tion, to achieve the feasible patches in the digital world while balancing the multi-modal detectors.
• We verify our cross-modal patches in the pedestrian detection task not only in the digital world but also in the physical world. Experimental results show that cross-modal patches can work well in various angles, distances, postures, and scenes. 2.