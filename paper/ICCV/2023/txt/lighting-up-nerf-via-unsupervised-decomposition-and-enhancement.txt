Abstract
Neural Radiance Field (NeRF) is a promising approach for synthesizing novel views, given a set of images and the corresponding camera poses of a scene. However, images photographed from a low-light scene can hardly be used to train a NeRF model to produce high-quality results, due to their low pixel intensities, heavy noise, and color dis-tortion. Combining existing low-light image enhancement methods with NeRF methods also does not work well due to the view inconsistency caused by the individual 2D en-hancement process. In this paper, we propose a novel ap-proach, called Low-Light NeRF (or LLNeRF), to enhance the scene representation and synthesize normal-light novel views directly from sRGB low-light images in an unsuper-vised manner. The core of our approach is a decomposition of radiance ﬁeld learning, which allows us to enhance the illumination, reduce noise and correct the distorted colors jointly with the NeRF optimization process. Our method is able to produce novel view images with proper lighting and vivid colors and details, given a collection of camera-ﬁnished low dynamic range (8-bits/channel) images from a low-light scene. Experiments demonstrate that our method outperforms existing low-light enhancement methods and
NeRF methods. 1.

Introduction
Neural Radiance Field (NeRF) [22] is a powerful ap-proach to render novel view images through learning scene representations as implicit functions. These implicit func-tions are parameterized by multi-layer perceptrons (MLPs) and optimized by measuring the colorimetric errors of the input views. Consequently, high-quality input images are the precondition for the high-quality results of NeRF. In other words, training NeRF models typically requires the input images to have high visibility, and almost all the pixels to faithfully represent the scene illumination and object col-ors. However, when taking photos under low-light condi-* Joint corresponding authors.
Input
LLE+NeRF
LLFlow [29]
SNR [34]
URetinexNet [31]
Figure 1. A comparison of the baseline model (LLE+NeRF),
SOTA low light enhancement models, and our model.
LLNeRF (Ours) tions, the quality of the images is not guaranteed. Low-light images typically have low visibility. Noise from the camera is also relatively ampliﬁed due to the low photons, which further buries the scene details and distorts object colors.
Such characteristics of low-light images fail existing NeRF models in producing high-quality novel view images.
We note that recently there are some methods proposed to train NeRF models from degraded inputs [21, 32, 18].
Ma et al. [18] present a method to synthesize novel view images from blurry inputs taken in normal-light scenes.
Mildenhall et al. [21] show that when training with high dy-namic range RAW data, NeRF can be robust to zero-mean noise of low-light input images. Huang et al. [32] propose
HDR-NeRF, which produces high dynamic range (HDR) novel views from a set of low dynamic range (LDR) input images taken at different known exposure levels. The lat-ter two methods take advantages of HDR information and metadata (i.e., exposure levels) recorded in the RAW im-ages to enhance the scene representations. However, these methods do not work on camera-ﬁnished sRGB images (8-bits/channel) taken in low-light scenes. Unlike RAW data, sRGB images are produced by the camera ISP process.
They are of low dynamic range and low signal-to-noise ra-tio.
A straightforward solution to this problem is to ﬁrst en-hance the low-light input images and then use the enhanced
results to train a NeRF model. However, while this may be able to improve the brightness, existing low-light enhance-ment models do not consider how to maintain consistency across multi-view images. Besides, these learning-based enhancement methods tend to learn speciﬁc mappings of brightness from their own training data, which may not gen-eralize well to in-the-wild scenes. These two reasons cause
NeRF to learn biased information across different views due to the view-dependent optimization of NeRF, resulting in unrealistic novel images. See examples in Fig. 1.
In this paper, we propose a new approach for render-ing novel normal-light images from a set of 8-bit low-light sRGB images without the supervision of ground truth.
Our key solution to this problem is that: the colors of 3D points can be decoupled into view-dependent and view-independent components within the NeRF optimization, and the view-dependent component is dominated by the effect of lighting. So the manipulations of the lighting-related view-independent components are able to enhance the brightness, correct the colors, and reduce the noise while keeping the texture and structure of the scene. Experi-ments demonstrate that the proposed method outperforms the state-of-the-art NeRF models and the baselines (i.e., combining NeRF with state-of-the-art enhancement meth-ods).
In summary, we propose the ﬁrst method to reconstruct a NeRF model of proper lighting from a collection of LDR low-light images. Our main contributions includes: 1. We propose to decompose NeRF into view-dependent and -independent color components for enhancement.
The decomposition does not require ground truth. 2. We formulate an unsupervised method to enhance the lighting and correct the colors while rendering noise-free novel view images. 3. We collect a real-world dataset, and conduct extensive experiments to analyze our method and demonstrate its effectiveness in real-world scenes. 2.