Abstract
Pre-trained point cloud models have found extensive ap-plications in 3D understanding tasks like object classifica-tion and part segmentation. However, the prevailing strat-egy of full fine-tuning in downstream tasks leads to large per-task storage overhead for model parameters, which limits the efficiency when applying large-scale pre-trained models. Inspired by the recent success of visual prompt tun-ing (VPT), this paper attempts to explore prompt tuning on pre-trained point cloud models, to pursue an elegant bal-ance between performance and parameter efficiency. We find while instance-agnostic static prompting, e.g. VPT, shows some efficacy in downstream transfer, it is vulnera-ble to the distribution diversity caused by various types of noises in real-world point cloud data. To conquer this limi-tation, we propose a novel Instance-aware Dynamic Prompt
Tuning (IDPT) strategy for pre-trained point cloud mod-els. The essence of IDPT is to develop a dynamic prompt generation module to perceive semantic prior features of each point cloud instance and generate adaptive prompt to-kens to enhance the model’s robustness. Notably, extensive experiments demonstrate that IDPT outperforms full fine-tuning in most tasks with a mere 7% of the trainable param-eters, providing a promising solution to parameter-efficient learning for pre-trained point cloud models. Code is avail-able at https://github.com/zyh16143998882/
ICCV23-IDPT. 1.

Introduction
With the rapid development of 3D scanning technol-ogy, point clouds, as irregular point sets that represent
*These authors contributed equally to this work.
†Corresponding author. ((cid:0) daitao.edu@gmail.com)
Figure 1. The pipeline of (a) the previous static prompt tuning in
VPT [22] and (b) our dynamic prompt tuning. Unlike the static prompt tuning that is instance-agnostic, ours is adaptive to input by concatenating the instance-aware prompt generated by a prompt module into the last Transformer layer input. 3D geometry, have been widely used in various fields and tasks. Deep learning-based point cloud processing tech-niques [16, 24, 31, 33, 34, 45, 35] have drawn consider-able attention as they can directly process raw point cloud data while preserving its rich information. As a classic deep learning paradigm, fine-tuning the foundation model pre-trained [11, 26, 32, 51, 55] on massive raw point clouds in specific downstream tasks has achieved state-of-the-art per-formance. However, this approach is storage-intensive, as it requires storing and deploying a separate copy of the back-bone parameters for each task.
Recently, prompt tuning has surpassed fine-tuning in multiple downstream tasks in the language and image do-mains, significantly reducing storage requirements by fix-ing the parameters of a pre-trained model and introducing a small amount of task-specific learnable parameters into the input space. Although some work [21, 20, 46, 54] have attempted to introduce prompt into point cloud processing, they all relied on pre-trained image models [38, 12]. To date, less research has been denoted to prompt tuning in
point cloud pre-trained models.
Inspired by the success of visual prompt tuning (VPT)
[22]), it is natural to adopt this idea to point clouds. As shown in Figure 1(a), we call VPT a static prompting strat-egy because it introduces prompt tokens as a few learnable parameters concatenated to the input of a pre-trained point cloud model. The prompt tokens are shared by any input and therefore are instance-agnostic. Although such a strat-egy performs well on synthetic datasets (e.g. ModelNet40
[47]), it causes significant performance degradation on real scanned point cloud datasets (e.g. ScanObjectNN [41]).
Thus, static prompt tuning is not suitable for real point clouds, where point clouds with different types of missing or noisy points belong to different distributions. These ob-servations motivate us to design a universal prompt-tuning strategy for both synthetic and real point clouds.
To address this issue, we proposed an Instance-aware
Dynamic Prompt Tuning (IDPT) for point cloud pre-trained models. As shown in Figure 1(b), IDPT develops a prompt generation module to perceive the semantic prior features of each point cloud instance and produces adaptive prompts for different inputs. The proposed IDPT enables an adjust-ing effect to mitigate the adverse noises in point cloud in-stances and thus can enhance the robustness of pre-trained models. We insert IDPT into the last Transformer layer for a more accurate representation of point clouds. Extensive experiments demonstrate the effectiveness of IDPT. Typi-cally, IDPT yields competitive performance compared with full fine-tuning but just requires about 7% of trainable pa-rameters in downstream transfer.
The main contributions can be summarized as follows:
• To our best knowledge, this is the first exploration of prompt tuning on pre-trained point cloud models. We reveal that VPT, the static prompting strategy, suffers from the distributional diversity issue caused by vari-ous types of noises in real-world point cloud data.
• To address the shortcoming, we propose an Instance-aware Dynamic Prompt Tuning (IDPT) as an effective solution. We develop a dynamic prompt generation module to capture semantic prior features of each point cloud instance and generate adaptive prompt tokens to mitigate the noises.
• Extensive experiments on a variety of downstream tasks show the competitive performance of IDPT with full fine-tuning in most tasks while requiring much less tunable parameters, e.g. about 7%. 2.