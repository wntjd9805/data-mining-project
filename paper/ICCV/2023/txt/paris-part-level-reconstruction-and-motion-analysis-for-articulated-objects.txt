Abstract
We address the task of simultaneous part-level recon-struction and motion parameter estimation for articulated objects. Given two sets of multi-view images of an object in two static articulation states, we decouple the movable part from the static part and reconstruct shape and appear-ance while predicting the motion parameters. To tackle this problem, we present PARIS: a self-supervised, end-to-end architecture that learns part-level implicit shape and ap-pearance models and optimizes motion parameters jointly without any 3D supervision, motion, or semantic annota-tion. Our experiments show that our method generalizes better across object categories, and outperforms baselines and prior work that are given 3D point clouds as input.
Our approach improves reconstruction relative to state-of-the-art baselines with a Chamfer-L1 distance reduction of 3.94 (45.2%) for objects and 26.79 (84.5%) for parts, and achieves 5% error rate for motion estimation across 10 ob-ject categories. 1.

Introduction
Articulated objects consist of interconnected static and movable parts that exhibit motion. Such objects are ubiq-uitous in real life (e.g., drawers, ovens, chairs, laptops, staplers). Thus, perception and understanding of articu-lated object structure is important in many areas including robotics [49, 25, 9, 23], animation [39, 7], and industrial de-sign [19]. Articulated object motion analysis enables robots to manipulate objects more effectively [11]. Acquiring dig-ital replicas of articulated objects [20, 16] also enables sim-ulating object articulation in applications involving robotic agents and embodied AI [37, 41].
Prior work on articulated object understanding uses su-pervised learning which requires 3D supervision and artic-ulation annotation [45, 50]. Unfortunately, such supervi-sory data is expensive and unavailable at scale. Another line of prior work assumes a known object category and learns separate models for each category [21, 29, 47, 43].
This makes generalization to arbitrary unseen objects diffi-Figure 1. We present PARIS: a method that takes only multi-view images of an object in two articulation states (top left) and recov-ers part-level shape and appearance while jointly estimating artic-ulation motion parameters (top right). Our method can generate unseen states and render the object from arbitrary views (bottom). cult. Recently, Jiang et al. [20] proposed Ditto: a category-agnostic approach for motion and part geometry prediction from a pair of 3D point clouds. However, this approach is limited in generalization to unseen object categories and it does not address detailed appearance reconstruction.
We make the observation that perception of articulated objects involves two subproblems: reconstruction and mo-tion analysis. These subproblems are tightly intertwined since knowing the complete geometry of an object makes motion analysis easier, while knowing the motion param-eters of an object provides a signal for better reconstruc-tion of the object given observations in different articulation states. Our insight is that by leveraging the intertwined na-ture of articulated object perception we can avoid reliance on explicit 3D data and motion parameter supervision.
In this paper, we propose PARIS: a self-supervised ap-proach for joint reconstruction and motion analysis of ar-ticulated objects. By observing an articulated object in two states (see Figure 1), our method reconstructs the shape and appearance of the static and movable parts in two implicit neural fields, while predicting the articulation motion pa-rameters. The separated neural fields are composited using the estimated motion parameters to set up self-supervisory losses relying only on the input RGB images.
Thus, our approach is category-agnostic and does not re-quire any 3D data or supervisory signals for part segmenta-tion, motion parameters, or object category semantics.
In summary:
• We address joint reconstruction and motion analysis for articulated objects including part-level shape and appearance, and motion estimation given RGB images of the object in only two static states.
• We develop PARIS: self-a supervised and end-to-end approach that jointly per-forms reconstruction and motion analysis without 3D supervision, motion parameter or semantic annotation. category-agnostic,
• We systematically evaluate our approach on synthetic and real data, and show we significantly outperform prior work and baselines in shape and appearance qual-ity, and motion parameter estimation accuracy. 2.