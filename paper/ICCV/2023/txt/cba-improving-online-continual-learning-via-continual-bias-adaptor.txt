Abstract
Online continual learning (CL) aims to learn new knowl-edge and consolidate previously learned knowledge from non-stationary data streams. Due to the time-varying train-ing setting, the model learned from a changing distribu-tion easily forgets the previously learned knowledge and bi-ases toward the newly received task. To address this prob-lem, we propose a Continual Bias Adaptor (CBA) module to augment the classifier network to adapt to catastrophic distribution change during training, such that the classi-fier network is able to learn a stable consolidation of pre-viously learned tasks.
In the testing stage, CBA can be removed which introduces no additional computation cost and memory overhead. We theoretically reveal the reason why the proposed method can effectively alleviate catas-trophic distribution shifts, and empirically demonstrate its effectiveness through extensive experiments based on four rehearsal-based baselines and three public continual learn-ing benchmarks1. 1.

Introduction
Continual learning (CL) [40, 48] focuses on designing a model that can learn continuously from streaming data and accumulate new knowledge while consolidating previ-ously learned knowledge.
In the context of CL, the data distribution of streaming tasks is in general non-stationary and changes over time, which violates the independent and identically distributed (i.i.d) assumption that is commonly adopted in machine learning. Therefore, continual learn-ing suffers from the notorious catastrophic forgetting prob-lem [16], where the model severely forgets the previously learned knowledge after being trained on a new task.
Traditional offline CL stores all training batches of the current task and the model is trained on these samples for
*Corresponding author 1Code is available at https://github.com/wqza/CBA-online-CL multiple epochs with repeat shuffle. However, the availabil-ity of previously learned batches might be restricted due to privacy concerns [14] or memory limitations.
In this pa-per, we mainly focus on a more challenging and realistic setting, online CL [28], where samples from each task can be trained only single-pass (i.e., one epoch) and the past batches are not accessible.
In online CL, the distribution of the training data changes over time and it differs not only from the joint distribution of all tasks (as in offline CL) but also from the distribution of the task they belong to. Therefore, online CL commonly causes an even more severe distribution shift, which further intensifies catastrophic forgetting. To alleviate this prob-lem, rehearsal-based algorithms [35, 48] employed a small memory buffer to store examples of previous tasks so as to approximate the joint distribution of all seen training data, then collectively trained the model on the memory buffer with the current task. Along this line, DER++ [7] utilized an additional knowledge distillation to further reply logits of old task samples, and RAR [46] used random augmen-tation to address the overfitting problem of the small mem-ory buffer. In another vein, a wide range of works attribute catastrophic forgetting to task-recency bias [30], i.e., the classifiers tend to classify samples into currently received classes. They in turn proposed to improve the original linear classifier [1, 20, 44] or directly replace the classifier with the nearest classifier [30, 35] to mitigate the adverse effects of class imbalance between currently received classes and re-played classes. Despite the promising performance, almost all of these methods implicitly view task-recency bias as a label distribution shift and tackle it from the perspective of class imbalance problem, which makes these methods sub-optimal in practice [8].
In fact, the target of online CL is to accomplish a stable consolidation of knowledge for all learned tasks, by training a classifier to fit the posterior probability P(Y |X), where X and Y represent stochastic variables of the input data and the corresponding label, respectively. According to Bayes’s rule P(Y |X) ∝ P(X|Y )P(Y ), the posterior probability de-the posterior distribution shift in an online manner.
We theoretically investigate the proposed method from gradient alignment and reveal the reason why it can ef-fectively alleviate catastrophic distribution shifts.
• We propose a CBA module that can plug in most of the rehearsal-based methods during training, and be removed in the test stage so that it involves no com-putation cost and memory overhead in inference.
• We evaluate the performance based on four rehearsal-based baselines with extensive experiments over vari-ous benchmarks. We show that the proposed method can effectively consolidate previously learned knowl-edge. This is also demonstrated by task-blurry online
CL and offline CL settings. 2.