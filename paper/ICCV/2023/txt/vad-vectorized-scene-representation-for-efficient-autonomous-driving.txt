Abstract
Autonomous driving requires a comprehensive under-standing of the surrounding environment for reliable tra-jectory planning. Previous works rely on dense rasterized scene representation (e.g., agent occupancy and semantic map) to perform planning, which is computationally in-tensive and misses the instance-level structure information.
In this paper, we propose VAD, an end-to-end vectorized paradigm for autonomous driving, which models the driv-ing scene as a fully vectorized representation. The proposed vectorized paradigm has two significant advantages. On one hand, VAD exploits the vectorized agent motion and map elements as explicit instance-level planning constraints which effectively improves planning safety. On the other hand, VAD runs much faster than previous end-to-end plan-ning methods by getting rid of computation-intensive ras-terized representation and hand-designed post-processing steps. VAD achieves state-of-the-art end-to-end planning performance on the nuScenes dataset, outperforming the previous best method by a large margin. Our base model,
VAD-Base, greatly reduces the average collision rate by 29.0% and runs 2.5× faster. Besides, a lightweight vari-ant, VAD-Tiny, greatly improves the inference speed (up to 9.3×) while achieving comparable planning performance.
We believe the excellent performance and the high efficiency of VAD are critical for the real-world deployment of an au-tonomous driving system. Code and models are available at https://github.com/hustvl/VAD for facilitat-ing future research. 1.

Introduction
Autonomous driving requires both comprehensive scene understanding for ensuring safety and high efficiency for
⋆ Equal contribution; ⋄ This work was done during the internship of
Bo Jiang, Shaoyu Chen, and Bencheng Liao at Horizon Robotics; ⊠ Cor-responding author: xgwang@hust.edu.cn
Figure 1. Previous paradigms mainly rely on rasterized represen-tation (a) for planning (e.g., semantic map, occupancy map, flow map, and cost map), which is computationally intensive. The pro-posed VAD is fully based on the vectorized scene representation (b) for end-to-end planning. VAD leverages instance-level struc-ture information as planning constraints and guidance, achieving promising performance and efficiency. real-world deployment. An autonomous vehicle needs to efficiently perceive the driving scene and perform reason-able planning based on the scene information.
Traditional autonomous driving methods [23, 14, 7, 48] adopt a modular paradigm, where perception and planning are decoupled into standalone modules. The disadvantage is, the planning module cannot access the original sensor data, which contains rich semantic information. And since planning is fully based on preceding perception results, the error in perception may severely influence planning and can not be recognized and cured in the planning stage, which leads to the safety problem.
Recently, end-to-end autonomous driving methods [19, 21, 2, 11] take sensor data as input for perception and output planning results with one holistic model. Some works [40, 9, 41] directly output planning results based on the sensor data without learning scene representation, which lacks interpretability and is difficult to optimize. Most works [19, 21, 2] transform the sensor data into rasterized
scene representation (e.g., semantic map, occupancy map, flow map, and cost map) for planning. Though straightfor-ward, rasterized representation is computationally intensive and misses critical instance-level structure information.
In this work, we propose VAD (Vectorized Autonomous
Driving), an end-to-end vectorized paradigm for au-tonomous driving. VAD models the scene in a fully vec-torized way (i.e., vectorized agent motion and map), getting rid of computationally intensive rasterized representation.
We argue that vectorized scene representation is superior to rasterized one. Vectorized map (represented as boundary vectors and lane vectors) provides road structure informa-tion (e.g., traffic flow, drivable boundary, and lane direc-tion), and helps the autonomous vehicle narrow down the trajectory search space and plan a reasonable future trajec-tory. The motion of traffic participants (represented as agent motion vectors) provides instance-level restriction for colli-sion avoidance. What’s more, vectorized scene representa-tion is efficient in terms of computation, which is important for real-world applications.
VAD takes full advantage of the vectorized information to guide planning both implicitly and explicitly. On one hand, VAD adopts map queries and agent queries to implic-itly learn instance-level map features and agent motion fea-tures from sensor data, and extracts guidance information for planning via query interaction. On the other hand, VAD proposes three instance-level planning constraints based on the explicit vectorized scene representation: the ego-agent collision constraint for maintaining a safe distance between the ego vehicle and other dynamic agents both laterally and longitudinally; the ego-boundary overstepping constraint for pushing the planning trajectory away from the road boundary; and the ego-lane direction constraint for regular-izing the future motion direction of the autonomous vehicle with vectorized lane direction. Our proposed framework and the vectorized planning constraints effectively improve the planning performance, without incurring large compu-tational overhead.
Without fancy tricks or hand-designed post-processing steps, VAD achieves state-of-the-art (SOTA) end-to-end planning performance and the best efficiency on the chal-lenging nuScenes [1] dataset. Compared with the previous
SOTA method UniAD [21], our base model, VAD-Base, greatly reduces the average planning displacement error by 30.1% (1.03m v.s. 0.72m) and the average collision rate by 29.0% (0.31% v.s. 0.22%), while running 2.5× faster (1.8
FPS v.s. 4.5 FPS). The lightweight variant, VAD-Tiny, runs 9.3× faster (1.8 FPS v.s. 16.8 FPS) while achieving compa-rable planning performance, the average planning displace-ment error is 0.78m and the average collision rate is 0.38%.
We also demonstrate the effectiveness of our design choices through thorough ablations.
Our key contributions are summarized as follows:
• We propose VAD, an end-to-end vectorized paradigm for autonomous driving. VAD models the driving scene as a fully vectorized representation, getting rid of computationally intensive dense rasterized representa-tion and hand-designed post-processing steps.
• VAD implicitly and explicitly utilizes the vectorized scene information to improve planning safety, via query interaction and vectorized planning constraints.
• VAD achieves SOTA end-to-end planning perfor-mance, outperforming previous methods by a large margin. Not only that, because of the vectorized scene representation and our concise model design, VAD greatly improves the inference speed, which is criti-cal for the real-world deployment of an autonomous driving system.
It’s our belief that autonomous driving can be performed in a fully vectorized manner with high efficiency. We hope the impressive performance of VAD can reveal the potential of vectorized paradigm to the community. 2.