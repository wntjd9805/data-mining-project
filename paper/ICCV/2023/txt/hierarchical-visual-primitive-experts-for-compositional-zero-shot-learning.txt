Abstract
Compositional zero-shot learning (CZSL) aims to recog-nize unseen compositions with prior knowledge of known primitives (attribute and object). Previous works for CZSL often suffer from grasping the contextuality between at-tribute and object, as well as the discriminability of vi-sual features, and the long-tailed distribution of real-world compositional data. We propose a simple and scalable framework called Composition Transformer (CoT) to ad-dress these issues. CoT employs object and attribute experts in distinctive manners to generate representative embed-dings, using the visual network hierarchically. The object expert extracts representative object embeddings from the
ﬁnal layer in a bottom-up manner, while the attribute expert makes attribute embeddings in a top-down manner with a proposed object-guided attention module that models con-textuality explicitly. To remedy biased prediction caused by imbalanced data distribution, we develop a simple minor-ity attribute augmentation (MAA) that synthesizes virtual samples by mixing two images and oversampling minority attribute classes. Our method achieves SoTA performance on several benchmarks, including MIT-States, C-GQA, and
VAW-CZSL. We also demonstrate the effectiveness of CoT in improving visual discrimination and addressing the model bias from the imbalanced data distribution. The code is available at https://github.com/HanjaeKim98/
CoT. 1.

Introduction
Humans perceive entities as hierarchies of parts; for ex-ample, we recognize ‘Cute Cat’ by composing the meaning of ‘Cute’ and ‘Cat’. People can even perceive new con-cepts by composing the primitive meanings they already
*Corresponding author
This work was supported by the Institute of Information Communica-tions Technology Planning Evaluation (IITP) grant funded by the Korean government (MSIT) (No. 2021-0-02068, Artiﬁcial Intelligence Innovation
Hub) lee.j@navercorp.com
Jacket
Water
Jeans
Sky
Shirt
Blank
Concrete Brick
Tiled
Stone (a) OADis (b) CoT (Ours)
Figure 1: Visual feature distribution of previous state-of-the-art approach (OADis) [53] and CoT (Ours). The features are gath-ered with respect to attribute ‘Blue -’ (Top) and object ‘- Wall’ (Bottom). Notably, the previous method suffers from degraded vi-sual discrimination such as (‘Blue, Jeans’ vs. ‘Blue, Shirt’), or all attributes composed into object ‘Wall’. knew. Such compositionality is a fundamental ability of hu-mans for all cognition. For this reason, compositional zero-shot learning (CZSL), recognizing a novel composition of known components (i.e., object and attribute), has been re-garded as a crucial problem in the research community.
A naive approach for CZSL is to combine attribute and object scores estimated from individually trained classi-ﬁers [39, 21]. However, it is hard for these independent pre-dictions to consider the interactions that result from a com-bination of different primitives, which is contextuality (e.g. different meaning of ‘Old’ in ‘Old Car’ and ‘Old Cat’) in the composition [39]. Previous approaches [42, 76, 36, 41, 53, 80] have solved the problem by modeling the compositional label (text) embedding for each class. They leverage exter-nal knowledge bases like Glove [37, 47] to extract attribute and object semantic vectors, and concatenate them with a few layers into the composition label embedding. The em-beddings are then aligned with visual features in the shared embedding space, where the recognition of unseen image
query becomes the nearest neighbor search problem [36].
Nevertheless, the extent to which contextuality is taken into account in the visual domain has still been restricted. tion [79, 59], the COT is well generalized with minimal computational costs, resolving the bias problem induced by the majority classes.
Prior works [52, 53] have also pointed out the impor-tance of discriminability in visual features, which is related to generalization performance towards unseen composition recognition [62, 38]. A popular solution is disentangle-ment [52, 53, 27, 80], in which independent layers are al-located to extract intermediate visual representations of at-tribute and object. However, as shown in Fig. 1, it may be challenging to extract their unique characteristics to un-derstand the heterogeneous interaction. We carefully argue that this problem arises from how the visual backbone (e.g.
ResNet18 [16]) is used. This is because attribute and ob-ject features, which require completely different character-istics [30, 22], are based on the same visual representation from the deep layer of the backbone.
Another challenge in CZSL is the long-tailed distribution of real-world compositional data [2, 63, 3]. Few attribute classes are dominantly associated with objects, which may cause a hubness problem [10, 12] among visual features.
The visual features from the head (frequent) composition become a hub, which aggravates the visual features to be indistinguishable in the embedding space [12], and induces a biased prediction towards dominant composition [61, 81].
In this paper, based on the discussions above, we propose
Composition Transformer (CoT) to enlarge the visual dis-crimination for robust CZSL. Motivated by bottom-up and top-down attention [31, 32, 28], the COT presents object and attribute experts, each forming its feature in different layers of the visual network. Speciﬁcally, the object ex-pert generates a representative object embedding from the last layer (i.e., bottom-up pathway) that is most robust to identify object category with high-level semantics [72, 44].
Then we explicitly model contextuality through an object-guided attention module that explores intermediate layers of the backbone network and builds attribute-related fea-tures associated with the object (i.e., top-down pathway).
Based on this module, the attribute expert generates a dis-tinctive attribute embedding in conjunction with the object embedding. By utilizing all the features of each layer ex-hibiting different characteristics, our method comprehen-sively leverages a visual network to diversify the compo-nents’ features in the shared embedding space.
Finally, we further develop a simple minority attribute augmentation (MAA) methodology tailored for CZSL to address the biased prediction caused by imbalanced data distribution [61]. Unlike GAN-based augmentations [56, 27] that lead to overwhelming computation during training, our method simpliﬁes the synthesis process of the virtual sample by blending two images while oversampling minor-ity attribute classes [7, 45]. Thanks to the label smooth-ing effect of the balanced data distribution by augmenta-Our contributions are summarized in three-folds:
• To enhance visual discrimination and contextuality, we propose a Composition Transformer (COT). In this framework, object and attribute experts hierarchically estimate primitive embeddings by fully utilizing inter-mediate outputs of the visual network.
• We introduce a simple yet robust MAA that alleviates dominant prediction on head compositions with major-ity attributes.
• The remarkable experimental results demonstrate that our COT and MAA are harmonized to improve the performance on several CZSL benchmarks, showing state-of-the-art performance. 2.