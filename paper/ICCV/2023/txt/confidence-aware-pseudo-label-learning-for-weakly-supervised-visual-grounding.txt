Abstract
Visual grounding aims at localizing the target object in image which is most related to the given free-form nat-ural language query. As labeling the position of target object is labor-intensive, the weakly supervised methods, where only image-sentence annotations are required during model training have recently received increasing attention.
Most of the existing weakly-supervised methods first gen-erate region proposals via pre-trained object detectors and then employ either cross-modal similarity score or recon-struction loss as the criteria to select proposal from them.
However, due to the cross-modal heterogeneous gap, these method often suffer from high confidence spurious associa-tion and model prone to error propagation. In this paper, we propose Confidence-aware Pseudo-label Learning (CPL) to overcome the above limitations. Specifically, we first adopt both the uni-modal and cross-modal pre-trained models and propose conditional prompt engineering to automatically generate multiple ‘descriptive, realistic and diverse’ pseudo language queries for each region proposal, and then es-tablish reliable cross-modal association for model training based on the uni-modal similarity score (between pseudo and real text queries). Secondly, we propose a confidence-aware pseudo label verification module which reduces the amount of noise encountered in the training process and the risk of error propagation. Experiments on five widely used datasets validate the efficacy of our proposed components and demonstrate state-of-the-art performance. Code can be found at https://github.com/zjh31/CPL.git 1.

Introduction
Visual grounding is an important task with vast poten-tial applications in visual question answering [1], robot ma-nipulation [38, 51], etc. The goal is to find the target ob-*Corresponding author
Figure 1. Our method compare with other weakly supervised vi-sual grounding methods. (a) Existing weakly supervised methods. (b) Our approach. ject (region) in an image associated with a given free-form natural language query. Fully supervised visual ground-ing [6, 42, 43, 46, 48, 15, 21] has witnessed remarkable progress recently. However, accurate box annotations for each target object are unfortunately expensive to obtain and thus difficult to scale. Therefore the weakly supervised set-ting, where only image-level descriptions are available dur-ing training, is more practical and draws increasing atten-tion from the community.
Most existing weakly supervised solutions generate re-gion proposals via pre-trained object detectors and then em-ploy either the contrastive learning-based or reconstruction-based paradigms to select from them. As shown in Fig-ure 1(a), the proposal selection is conducted based on the cross-modal (region-textual) (directly compute the match-ing score between the proposal and query). Specifically, contrastive learning-based methods learn the cross-modal alignment in the image level by maximizing the matching scores of the image and the paired descriptions while sup-pressing that of the unpaired ones. Reconstruction-based
methods perform the proposal selection with the cross-modal reconstruction loss, assuming that the proposals that match the text should best reconstruct the entire query.
However, both paradigms have the following two limi-tations. Firstly, due to the heterogeneous gap between the high-level concepts of text descriptions and the pixel-level contents of the image region, using cross-modal matching score or reconstruction query directly for proposal selection is not reliable. Such matching ambiguity often misleads the grounding model to learn spurious association, which greatly hinders the grounding performance. Secondly, ex-isting approaches are trapped by the error propagation and accumulation as they neglect the confidence of the learned cross-modal association and unavoidably keep overfitting to some incorrect ones encountered during the model training.
A recent work [13] proposes generating pseudo queries for proposals in an unsupervised method and using them for training a grounding model directly. However, it can only generate short and unreliable descriptions with limited style and structure based on hand-crafted templates.
To address the above limitations, we introduce a novel weakly supervised method for visual grounding by us-ing more reliable uni-modal matching for proposal selec-tion and perform association verification before leverag-ing them in model training. We call it Confidence-aware
Pseudo-label Learning (CPL). Firstly, to establish more re-liable region-text association for model training, we pro-pose to use three complementary pipeline to automati-cally generate multiple ‘descriptive, realistic and diverse’ pseudo language queries for each region proposal and form
<Region − P seudoQuery> pairs. As shown in Figure 1(b), our method then perform proposal selection based on the uni-modal similarity score (between real query and pseudo queries) and form <Region − RealQuery> pairs.
All region-query pairs are used to train a fully-supervised grounding model. To reduce the contribution of error region-query pairs, we propose an confidence-aware cross-modal verification module that estimates the confidence score of the region-query associations. We propose a se-lective grounding loss based on the confidence score to re-balance the weight of each sample in the training process.
To sum up, the main contributions of our work are:
• In contrast to performing proposal selection based on cross-modal matching scores, we propose to generate multiple ‘descriptive, realistic and diverse’ pseudo lan-guage queries for each region proposal, and then estab-lish more reliable cross-modal association for model training based on the uni-modal similarity (between pseudo and real text queries).
• We propose a confidence-aware cross-modal verifica-tion module and selective grounding loss to suppress the contribution of spurious association, which reduces the risk of error propagation in the training process.
• Experiments on the RefCOCO [47], RefCOCO+ [47],
RefCOCOg [25], ReferItGame [14] and Fliker30K
Entities[28] datasets demonstrate the effectiveness of our method in weakly supervised visual grounding. 2.