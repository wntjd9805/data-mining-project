Abstract
Nearest neighbour-based methods have proved to be one of the most successful self-supervised learning (SSL) approaches due to their high generalization capabilities.
However, their computational efficiency decreases when more than one neighbour is used. In this paper, we propose a novel contrastive SSL approach, which we call All4One, that reduces the distance between neighbour representa-tions using ”centroids” created through a self-attention mechanism. We use a Centroid Contrasting objective along with single Neighbour Contrasting and Feature Contrast-ing objectives. Centroids help in learning contextual in-formation from multiple neighbours whereas the neighbour contrast enables learning representations directly from the neighbours and the feature contrast allows learning repre-sentations unique to the features. This combination enables
All4One to outperform popular instance discrimination ap-proaches by more than 1% on linear classification evalua-tion for popular benchmark datasets and obtains state-of-the-art (SoTA) results. Finally, we show that All4One is robust towards embedding dimensionalities and augmenta-tions, surpassing NNCLR and Barlow Twins by more than 5% on low dimensionality and weak augmentation settings.
Source code is available in https://github.com/
ImaGonEs/all4one.
Figure 1: Simplified architecture of All4One. All4One uses three different objective functions that contrast differ-ent representations: Centroid objective contrasts the contex-tual information extracted from multiple neighbours while the Neighbour objective assures diversity [14]. Addition-ally, the Feature contrast objective measures the correlation of the generated features and increases their independence. 1.

Introduction
Deep learning (DL) models strongly depend on the avail-ability of large and high-quality training datasets whose construction is very expensive [25]. Self-supervised learn-ing (SSL) claims to allow the training of DL models with-out the need for large annotated data, which serves as a milestone in speeding up the DL progression [25]. The
most popular SSL approaches rely on instance discrimina-tion learning, a strategy that trains the model to be invariant to the distortions applied to a single image defined as pos-itive samples [6, 18, 31]. As all the views belong to the same image, consequently, they belong to the same seman-tic class. Bringing them together in the same feature space encourages the model to create similar representations for similar images. The encouraging results of initial works such as SimCLR [6] and BYOL [15] boosted multiple im-provements that address common problems of instance dis-crimination such as lack of diversity between samples and model collapse.
Neighbour contrastive learning hinges on the fact that data augmentations do not provide enough diversity in se-lecting the positive samples, as all of them are extracted from the same initial image [14]. To solve it, Nearest neigh-bour Contrastive Learning (NNCLR) [14] proposes the use of nearest neighbours (NN) to increase the diversity among the positive samples which in turn boosts the generalization of the model. Instead of bringing together two distortions created from the same image, they increase the proximity between a distorted sample and the NN of another distorted sample. However, relying entirely on the first neighbour holds back the real potential of the approach. MSF [21] proposes the use of k neighbours to increase the generaliza-tion capability of the model. However, MSF suffers from high computation as the objective function needs to be com-puted for each neighbour (k times). Apart from the low diversity of positive samples, instance discrimination ap-proaches suffer from model collapse, a scenario where the model learns a constant trivial solution [15]. Barlow Twins
[38] proposes a redundancy reduction-based approach that naturally avoids the collapse by measuring the correlation among the features on the generated image representations.
However, this collapse avoidance suffers from the require-ment of projecting embeddings in high dimensions.
In our work, we contrast information from multiple neighbours in a more efficient way by avoiding multiple computations of the objective function. This way, we are able to increase the generalization from neighbour con-trastive approaches while avoiding their flaws. For that, we propose the use of a new embedding constructed by a self-attention mechanism, such as a transformer encoder, that combines the extracted neighbour representations in a sin-gle representation containing contextual information about all of them. Hence, we are able to contrast all the neigh-bours’ information on a single objective computation. We make use of a Support Set that actively stores the repre-sentations computed during the training [14] so that we can extract the required neighbours. In addition, we integrate our approach with a redundancy reduction approach [38].
Making the computed cross-correlation matrix close to the identity reduces the features redundancy of the same image representation while also making them invariant to their dis-tortions. This idea contrasts the representations in a com-pletely different way than the rest of instance discrimina-tion approaches [28, 30, 14]. For this reason, we increase the richness of the representations learnt by the model by combining the neighbour contrast approach with the redun-dancy reduction objective that directly contrasts the features generated by the encoder and aims to increase their indepen-dence. In addition, the need for high-dimensional embed-dings of redundancy reduction feature contrast approaches
[38] is alleviated thanks to our SSL objective combination.
As a summary, in this paper, we introduce a new sym-biotic SSL approach, which we call All4One, that lever-ages the idea of neighbour contrastive learning while com-bining it with a feature contrast approach (see Figure 1).
All4One integrates three different objectives that prove to benefit each other and provide better representation learn-ing. Our contributions are as follows: (i) We define a novel objective function, centroid contrast, that is based on a pro-jection of sample neighbours in a new latent space through self-attention mechanisms. (ii) Our proposal, All4One, is based on a combination of centroid contrast, neighbour con-trast and feature contrast objectives, going beyond the sin-gle neighbour contrast while avoiding multiple computa-tions of the objective function; (iii) We demonstrate how contrasting different representations (neighbours and dis-torted samples) using InfoNCE [28] based and feature con-trast objectives benefit the overall performance and alleviate individual flaws such as the reliance of high-dimensional embeddings on feature contrast approaches; (iv) We show that All4One, by contrasting the contextual information of the neighbours and multiple representations, outperforms single nearest neighbour SSL on low-augmentation settings and low-data regimes, proving much less reliance on aug-mentations and increased generalization capability; and (v)
We prove that All4One outperforms single-neighbour con-trastive approaches (among others) by more than 1% in dif-ferent public datasets and using different backbones. 2.