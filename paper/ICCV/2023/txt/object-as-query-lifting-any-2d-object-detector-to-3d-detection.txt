Abstract 3D object detection from multi-view images has drawn much attention over the past few years. Existing methods mainly establish 3D representations from multi-view im-ages and adopt a dense detection head for object detec-tion, or employ object queries distributed in 3D space to localize objects.
In this paper, we design Multi-View 2D
Objects guided 3D Object Detector (MV2D), which can lift any 2D object detector to multi-view 3D object detection.
Since 2D detections can provide valuable priors for object existence, MV2D exploits 2D detectors to generate object queries conditioned on the rich image semantics. These dynamically generated queries help MV2D to recall ob-jects in the field of view and show a strong capability of localizing 3D objects. For the generated queries, we de-sign a sparse cross attention module to force them to fo-cus on the features of specific objects, which suppresses interference from noises. The evaluation results on the nuScenes dataset demonstrate the dynamic object queries and sparse feature aggregation can promote 3D detection capability. MV2D also exhibits a state-of-the-art perfor-mance among existing methods. We hope MV2D can serve as a new baseline for future research. Code is available at https://github.com/tusen-ai/MV2D. 1.

Introduction
Camera-based 3D object detection in unconstrained real-world scenes has drawn much attention over the past few years. Early monocular 3D object detection methods [30, 45, 1, 36, 18, 39, 40] typically build their framework fol-lowing the 2D object detection pipeline. The 3D location and attributes of objects are directly predicted from a sin-gle view image. Though these methods have achieved great progress, they are incapable of utilizing the geometric con-figuration of surrounding cameras and multi-view image correspondences, which are pivotal for the 3D position of objects in the real world. Moreover, adapting these methods to multi-view setting relies on sophisticated cross-camera
Figure 1. Motivation of MV2D. The 3D detector with fixed object queries (fixed queries means the queries are invariant for differ-ent inputs) might mislocate or ignore some objects (b), which are however successfully detected by a 2D detector (c). If generating object queries based on 2D detector, a 3D detector can produce more precise locations (d). post-processing, which further causes degraded efficiency and efficacy. To handle these problems, recent researchers
[41, 16, 23, 22, 26] propose to directly localize objects in 3D world space based on multi-view images, providing a new paradigm for vision-based 3D object detection.
According to the representation of fused features, current multi-view 3D object detection methods can be mainly di-vided into two streams: dense 3D methods and sparse query methods. Concretely, dense 3D methods render multi-view features into 3D space, such as Bird’s-Eye-View (BEV) fea-ture space [16, 22, 23, 5] or voxel feature space [35, 21].
However, since the computational costs are squarely pro-portional to the range of 3D space, they inevitably cannot scale up to large-scale scenarios [9]. Alternatively, query-based methods [41, 26] adopt learnable object queries to ag-gregate features from multi-view images and predict object bounding boxes based on query features. Although fixed number of object queries avoid computational cost explod-ing with 3D space, the query number and position relied on empirical prior may cause false positive or undetected ob-jects in dynamic scenarios.
In this paper, we seek a more reliable way to localize ob-jects. The motivation comes from the rapid developments of 2D object detection methods [12, 34, 24, 37] which can generate high-quality 2D bounding boxes for object local-ization in image space. One natural idea is to turn each 2D detection into one reference for the following 3D detection task. In this way, we design Multi-View 2D Objects guided 3D Object Detector (MV2D), which could lift any 2D de-tector to 3D detection, and the 3D detector could harvest all the advancements from 2D detection field.
Given the input multi-view images, we first obtain 2D detection results from a 2D detector and then generate a dynamic object query for each 2D bounding box. Instead of aggregating features from all regions in the multi-view inputs, one object query is forced to focus on one specific object. To this end, we propose an efficient relevant fea-ture selection method based on the 2D detection results and camera configurations. Then the dynamically generated ob-ject queries, together with their 3D position embedded rele-vant features, are input to a transformer decoder with sparse cross-attention layers. Lastly, the updated object queries predict the final 3D bounding boxes.
Thanks to the powerful 2D detection performances, the dynamic queries generated from 2D detection results can cover most objects that appeared in the images, leading to higher precision and recall, especially for small and distant objects compared with fixed queries. As shown in Figure 1, the objects that a fixed query-based 3D detector might miss can be recalled in MV2D with the help of a 2D object detec-tor. Theoretically, since 3D object queries stem from 2D de-tection results, our method can benefit from all off-the-shelf excellent 2D detector improvements. Our contributions can be summarized as:
• We propose a framework MV2D that can lift any 2D object detector to multi-view 3D object detection.
• We demonstrate that dynamic object queries and ag-gregation from certain relevant regions in multi-view images based on 2D detection can boost 3D detection performance.
• We evaluate MV2D on the standard nuScenes dataset, and it achieves state-of-the-art performance. 2.