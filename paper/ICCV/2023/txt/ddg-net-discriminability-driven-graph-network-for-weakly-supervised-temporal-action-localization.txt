Abstract
Weakly-supervised temporal action localization (WTAL) is a practical yet challenging task. Due to large-scale datasets, most existing methods use a network pretrained in other datasets to extract features, which are not suitable enough for WTAL. To address this problem, researchers de-sign several modules for feature enhancement, which im-prove the performance of the localization module, espe-cially modeling the temporal relationship between snip-pets. However, all of them omit that ambiguous snippets deliver contradictory information, which would reduce the discriminability of linked snippets. Considering this phe-nomenon, we propose Discriminability-Driven Graph Net-work (DDG-Net), which explicitly models ambiguous snip-pets and discriminative snippets with well-designed connec-tions, preventing the transmission of ambiguous informa-tion and enhancing the discriminability of snippet-level rep-resentations. Additionally, we propose feature consistency loss to prevent the assimilation of features and drive the graph convolution network to generate more discriminative representations. Extensive experiments on THUMOS14 and
ActivityNet1.2 benchmarks demonstrate the effectiveness of
DDG-Net, establishing new state-of-the-art results on both datasets. Source code is available at https://github. com/XiaojunTang22/ICCV2023-DDGNet. 1.

Introduction
Temporal action localization (TAL) is a significant yet challenging task in video understanding.
It aims to lo-calize the start and end timestamps of the action propos-als of interest and recognize their categories in untrimmed
∗Corresponding author.
Figure 1. An example of ambiguous snippets. The orange and green curves denote the attention weights generated by RGB and optical flow separately. The black horizontal line represents the threshold. videos [2, 49, 54].
It has attracted great attention in academia and industry due to potential applications for video retrieval, surveillance, and anomaly detection. Full-supervised temporal action localization [6, 31, 32, 46, 50, 56, 66, 68] has made significant progress in recent years.
However, these methods require frame-level annotations, which are time-consuming and labor-intensive for large-scale datasets. Therefore, a number of researchers pay es-sential attention to weakly-supervised temporal action lo-calization (WTAL).
WTAL learns to localize action instances with only video-level labels (i.e. action categories in the video) dur-ing the training process. Most existing methods adopt the paradigm of multi-instance learning (MIL), regarding the video as a bag and action regions as instances. However, due to the gap between classification and localization tasks, it is difficult to localize precise action proposals without frame-level annotations. Besides, except for the discrimina-tive snippets (action or background), there are massive am-biguous snippets (e.g. action context) in untrimmed videos.
For example, as shown in Figure1, during the “Cricket-Bowling” action, action snippets with obvious appearance and quick movement, and background snippets with ir-relevant content are easy to be recognized.
Instead, the ambiguous snippets (action actually) contain action infor-mation (similar appearance) and background information (slow movement) simultaneously, which increases the dif-ficulty of discriminating.
Therefore, it is crucial to localize and recognize ambigu-ous snippets exactly. Several works [14, 33, 34, 43] are devoted to relieving the interference of action context by separating action and context. Meanwhile, we note that a few works [11, 44, 47, 59] pursue to model the temporal relationship between snippets for receiving more complete information. These methods have achieved certain perfor-mances, but they overlook the adverse effects of ambiguous information. When ambiguous snippets spread out infor-mation to similar action or background snippets, ambiguity is delivered together. It leads to the result that ambiguous snippets would reduce the discriminability of others when transmitting contradictory information to them.
Inspired by the above research and analysis, we pro-pose a novel graph network, named Discriminablity-Driven
Graph Network (DDG-Net), which explicitly separates am-biguous snippets and discriminative snippets to perform more effective graph inference with well-designed con-nections. Specifically, as shown in Figure 1, we divide firstly the whole video into three types of snippets (i.e. pseudo-action, pseudo-background, and ambiguous snip-pets). Then, we design different connections among them to ensure only the transmission of discriminative informa-tion. In this way, the snippets receive complementary infor-mation from linked snippets through graph inference. What is more, the discriminability of ambiguous snippets is en-hanced while the snippets never do harm to other snippets due to distinctive connections. Besides, we propose feature consistency loss to maintain characteristics of snippet-level representations for localization. Our main contributions can be summarized as follows.
• We propose a novel graph network DDG-Net, which explicitly models ambiguous and discriminative snip-pets with different types of connections, aiming to spread complementary information and enhance the discriminability of snippet-level features while elimi-nating the adverse effects of ambiguous information.
• We propose feature consistency loss with DDG-Net, which prevents the assimilation of snippet-level fea-tures and drives the graph convolution network to gen-erate more discriminative representations.
• Extensive experiments demonstrate that our method is effective, and establishes new state-of-the-art results on THUMOS14 and ActivityNet1.2 datasets. 2.