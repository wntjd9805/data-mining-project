Abstract
Federated learning enables multiple decentralized clients to learn collaboratively without sharing local data.
However, the expensive annotation cost on local clients remains an obstacle in utilizing local data.
In this pa-per, we propose a federated active learning paradigm to efficiently learn a global model with a limited annota-tion budget while protecting data privacy in a decentral-ized learning manner. The main challenge faced by fed-erated active learning is the mismatch between the ac-tive sampling goal of the global model on the server and that of the asynchronous local clients. This becomes even more significant when data is distributed non-IID across local clients. To address the aforementioned challenge, we propose Knowledge-Aware Federated Active Learning (KAFAL), which consists of Knowledge-Specialized Active
Sampling (KSAS) and Knowledge-Compensatory Federated
Update (KCFU). Specifically, KSAS is a novel active sam-pling method tailored for the federated active learning problem, aiming to deal with the mismatch challenge by sampling actively based on the discrepancies between local and global models. KSAS intensifies specialized knowledge in local clients, ensuring the sampled data is informative for both the local clients and the global model. Meanwhile,
KCFU deals with the client heterogeneity caused by lim-ited data and non-IID data distributions by compensating for each client’s ability in weak classes with the assistance of the global model. Extensive experiments and analyses are conducted to show the superiority of KAFAL over recent state-of-the-art active learning methods. Code is available at https://github.com/ycao5602/KAFAL. 1.

Introduction
Federated learning is a decentralized paradigm that al-lows collaborative learning of local devices to attain a pow-erful global model in a central server through aggregation without accessing local data [25, 35]. Most federated learn-Figure 1. The primary federated active learning framework with non-IID data. Each client maintains an active learning loop to select informative data for annotation with a limited annotation budget. We show each model’s existing labelled data in different classes with pink bars and the newly acquired labels with green bars. Clients specialize in different classes due to non-IID data distributions. ing methods consider supervised learning scenarios with fully annotated training data on each local client. However, the high annotation cost has been a challenge for real-world federated learning scenarios, e.g., large-scale medical data located in different medical institutions while medical spe-cialists for data annotation are very limited in each institu-tion. In this paper, we consider a new federated active learn-ing paradigm, which aims to not only protect data privacy but also make the most of the very limited annotation bud-get on each local client for decentralized model training. An illustration of the proposed federated active learning with non-IID data framework is shown in Fig. 1.
In federated active learning, we aim to attain a powerful global model on the server by sampling only local data and training the model on each local client. A straightforward solution for federated learning is to directly apply off-the-shelf active learning methods to each client. Specifically, existing methods can mainly be categorized into diversity-based [42, 2], uncertainty-based [49, 44, 51, 45, 23, 8, 3], and discrepancy-based. [43, 6]. Therefore, we actively sam-ple based on either the statistics from each client model or the downloaded global model. However, the former ap-proach may yield benefits primarily for local clients, while the latter might result in the loss of valuable information during aggregation, even if the selected data is advanta-geous for the global model on the server. Through exper-iments in subsequent sections, we demonstrate that active sampling with the global model on the server struggles to derive benefits due to this indirect process.
A major challenge in federated active learning is there-fore, the mismatch between the active sampling goal of the clients and that of the model on server caused by asyn-chronous models. What makes it even more challenging is the statistical heterogeneity resulting from the non-IID data distributions on clients in a typical federated learning setting [35, 52, 17, 29]. Ideally, the models can synchro-nize with sufficiently many aggregations from local clients to the model on the server. However, the communication costs usually make the above-mentioned solution impracti-cal [35]. Therefore, the model parameters of each client and the global model vary due to non-IID distributions, leading to a higher degree of mismatch between the sampling goals.
To address the aforementioned challenge, we propose a federated active learning scheme, namely Knowledge-Aware Federated Active Learning (KAFAL). It com-prises two key components, Knowledge-Specialized Active
Sampling and Knowledge-Compensatory Federated Up-date. Knowledge-Specialized Active Sampling (KSAS) is a new active sampling strategy, where each client model learns to intensify its specialized knowledge in order to annotate universally informative data that benefit both the clients and the global model. Specifically, we compute the intensified discrepancy between the client and global model outputs based on the specialized knowledge of each client.
In addition, the insufficiency of labelled training data together with the statistical heterogeneity caused by non-IID data can degrade the federated update quality, e.g., clients may perform weakly for certain classes. Aggre-gating these clients, extra communications are required to achieve convergence. Therefore, we further devise a new update rule, Knowledge-Compensatory Federated Up-date (KCFU), by compensating for weak classes (or low-frequency classes) on each client through knowledge distil-lation from the global model. The main contributions of this paper are as follows:
• We explore a rarely studied problem, federated active learning with non-IID data, which aims at efficiently learning a global model with a limited annotation bud-get on each client under a heterogeneous federated learning framework. Notably, we reveal the main chal-lenge in federated active learning is the mismatch be-tween the active sampling goal of the clients and that of the server caused by asynchronous models.
• We introduce a federated active learning paradigm, known as KAFAL, with a novel active sampling method KSAS and a novel federated update method
KCFU to handle the aforementioned challenge. KSAS is designed to sample universally informative data by computing the intensified discrepancies between the clients’ and the global model’s outputs based on the specialized knowledge of each client. KCFU is de-vised to deal with data heterogeneity by compensating for weak classes using knowledge distillation from the global model.
• We conduct extensive experiments on different bench-marks to demonstrate the superiority of the proposed method, where comprehensive ablation studies are also provided to validate the design of the proposed
KAFAL. 2.