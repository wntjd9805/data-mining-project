Abstract
We study the problem of 3D-aware full-body human gen-eration, aiming at creating animatable human avatars with high-quality textures and geometries. Generally, two chal-i) existing methods struggle lenges remain in this field: to generate geometries with rich realistic details such as the wrinkles of garments; ii) they typically utilize volu-metric radiance fields and neural renderers in the synthe-sis process, making high-resolution rendering non-trivial.
To overcome these problems, we propose GETAvatar, a
Generative model that directly generates Explicit Textured 3D meshes for animatable human Avatar, with photo-realistic appearance and fine geometric details. Specifi-cally, we first design an articulated 3D human represen-tation with explicit surface modeling, and enrich the gener-ated humans with realistic surface details by learning from the 2D normal maps of 3D scan data. Second, with the explicit mesh representation, we can use a rasterization-based renderer to perform surface rendering, allowing us to achieve high-resolution image generation efficiently. Ex-tensive experiments demonstrate that GETAvatar achieves state-of-the-art performance on 3D-aware human genera-*Equal contribution. tion both in appearance and geometry quality. Notably,
GETAvatar can generate images at 5122 resolution with 17FPS and 10242 resolution with 14FPS, improving upon previous methods by 2×. Our code and models will be at https://getavatar.github.io/. 1.

Introduction
Generating high-quality 3D human avatars with explicit control over camera poses, body poses and shapes has been a long-standing challenge in computer vision and graph-ics. It has wide applications in video games, AR/VR, and movie production. Recently, 3D-aware generative models have demonstrated impressive results in producing multi-view-consistent images of 3D shapes [29, 4, 22, 3, 24, 6].
However, despite their success in modeling relatively sim-ple and rigid objects, it remains challenging for modeling dynamic human bodies with large articulated motions. The main reason is that these 3D GANs are not designed to han-dle body deformations, such as variations in human shapes and poses. Thus, they struggle to manipulate or animate the generated avatars given the control signals.
Some recent works [23, 2, 9, 35] have incorporated hu-man priors [17] into 3D-aware generative models [4, 3] to
generate animatable 3D human avatars. However, these methods face two challenges. First, the generated human avatars lack fine geometric details, such as cloth wrin-kles and hair, which are highly desirable for the photo-realistic 3D human generation. Second, existing meth-ods [23, 2, 9, 35] adopt volumetric neural renderers in the synthesis process, which suffers from high computational costs, making high-resolution rendering non-trivial.
In this work, we propose GETAvatar, a generative model that produces explicit textured 3D meshes with rich surface details (see Fig. 1) for animatable human avatars. Previous methods [23, 2, 9, 35] model the human body with implicit geometry representations, i.e., density fields and signed dis-tance fields, which produce either noisy or over-smoothed geometries due to the lack of explicit surface modeling and insufficient geometric supervision. To improve the geome-try quality of the generated humans, different from previous methods, we propose to model fine geometric details with a normal field [7], which associates a normal vector with each point on the surface. The direction and magnitude of the normal vector provide crucial geometric information to represent the detailed human body surface.
Specifically, we design a body-controllable articulated 3D human representation with body deformation model-ing and explicit surface modeling. The former allows us to deform the generated humans to target pose and shape, and the latter enables us to extract the underlying human body surface as an explicit mesh in a differentiable man-ner. Based on the extracted meshes, we further construct a normal field to depict the detailed surface of the generated humans. The normal field enables the model to capture re-alistic geometric details from 2D normal maps that are ren-dered from available 3D human scans, improving geometry quality and resulting in higher fidelity appearance genera-tion significantly.
Besides, existing methods [23, 2, 9] struggle to ren-der high-resolution images as the volume rendering pro-cess require intensive memory and computational costs.
The main issue is that volumetric radiance field and neu-ral renders perform volume sampling on both occupied and free regions [19] that do not contribute to the rendered im-ages, resulting in computational inefficiency.
In contrast,
GETAvatar benefits from the proposed explicit representa-tion and thus can generate textured meshes in a differen-tiable manner. With the extracted mesh surface, we can render high-resolution images up to 10242 with a highly ef-ficient rasterization-based surface renderer [12].
To validate the effectiveness of GETAvatar, we conduct extensive experiments on two 3D human datasets [1, 34].
The quantitative and qualitative results demonstrate that
GETAvatar consistently outperforms previous methods in terms of both visual and geometry quality (see Fig. 3).
Overall, our work makes the following contributions: 1. We propose a generative model, GETAvatar, that en-ables high-quality 3D-aware human generation, with full control over camera poses, body shapes, and hu-man poses. 2. We propose to model the complex body surface using a 3D normal field, which significantly improves the ge-ometric details of the generated clothed humans. 3. We design an articulated 3D human representation with differentiable surface modeling. The explicit mesh representation supports 360◦ free-view, high-resolution image synthesis (10242) for the generated avatars, and supports normal map rendering. 4. Our GETAvatar can be applied to a wide range of tasks, such as re-texturing, single-view 3D reconstruction, and re-animation. 2.