Abstract
Speech-driven 3D facial animation has been widely ex-plored, with applications in gaming, character animation, virtual reality, and telepresence systems. State-of-the-art methods deform the face topology of the target actor to sync the input audio without considering the identity-specific speaking style and facial idiosyncrasies, thus, resulting in unrealistic and inaccurate lip movements. To address this, we present Imitator, a speech-driven facial expres-sion synthesis method, which learns identity-specific de-tails from a short input video and produces novel facial expressions matching the identity-specific speaking style and facial idiosyncrasies of the target actor. Specifically, we train a style-agnostic transformer on a large facial ex-pression dataset which we use as a prior for audio-driven facial expressions. We utilize this prior to optimize for identity-specific speaking style based on a short reference video. To train the prior, we introduce a novel loss function based on detected bilabial consonants to ensure plausible lip closures and consequently improve the realism of the generated expressions. Through detailed experiments and user studies, we show that our approach improves Lip-Sync by 49% and produces expressive facial animations from input audio while preserving the actor’s speaking style.
Project page: https://balamuruganthambiraja. github.io/Imitator 1.

Introduction 3D digital humans raised a lot of attention in the past few years as they aim to replicate the appearance and mo-tion of real humans for immersive applications, like telep-resence in AR or VR, character animation and creation for entertainment (movies and games), and virtual mirrors for e-commerce. Especially, with the introduction of neu-1
ral rendering [25, 27], we see immense progress in the photo-realistic synthesis of such digital doubles [36, 11, 19].
These avatars can be controlled via visual tracking to mir-ror the facial expressions of a real human. However, for a series of applications, we need to control the facial avatars with text or audio inputs. For example, AI-driven digital as-sistants rely on motion synthesis instead of motion cloning.
Even telepresence applications might need to work with au-dio inputs only, when the face of the person is occluded or cannot be tracked, since a face capture device is not available. To this end, we analyze motion synthesis for fa-cial animations from audio inputs; note that text-to-speech approaches can be used to generate such audio. Humans are generally sensitive towards faces, especially facial mo-tions, as they are crucial for communication (e.g., micro-expressions). Without full expressiveness and proper lip closures, the generated animation will be perceived as un-natural and implausible. Especially if the person is known, the animations must match the subject’s idiosyncrasies.
Recent methods for speech-driven 3D facial anima-tion [16, 5, 20, 10] are data-driven. They are trained on high-quality motion capture data and leverage pretrained speech models [13, 21] to extract an intermediate audio rep-resentation. We can classify these data-driven methods into two categories, generalized [5, 20, 10] and personalized an-imation generation methods [16]. In contrast to those ap-proaches, we aim at a personalized 3D facial animation syn-thesis that can adapt to a new user while only relying on in-put RGB videos captured with commodity cameras. Specif-ically, we propose a transformer-based auto-regressive mo-tion synthesis method that predicts a generalized motion representation. This intermediate representation is decoded by a motion decoder which is adaptable to new users. A speaker embedding is adjusted for a new user, and a new motion basis for the motion decoder is computed.
Our method is trained on the VOCA dataset [5] and can be applied to new subjects captured in a short monocular
RGB video. As lip closures are of paramount importance for bilabial consonants (’m’,’b’,’p’), we introduce a novel loss based on the detection of bilabials to ensure that the lips are closed properly. We take inspiration from the lo-comotion synthesis field [18, 14], where similar losses are used to enforce foot contact with the ground and transfer it to our scenario of physically plausible lip motions.
In a series of experiments and ablation studies, we demonstrate that our method is able to synthesize facial ex-pressions that match the target subject’s motions in terms of style and expressiveness. Our method outperforms state-of-the-art methods in the metrical evaluation and user study.
Please refer to our suppl. video for a detailed qualitative comparison. In the user study, we confirm that personalized facial expressions are important for the perceived realism.
The contributions of our work Imitator are as follows:
• We explore the speaking style-adaption problem and show that personalization of speaking-style is critical for improving realism and expressiveness in 3D facial animation synthesis,
• we, therefore, propose a novel light-weight speaking style-adaption approach that allows for efficient style-adaption to new users from a short reference video by disentangling generalized viseme generation and identity-specific motion decoding,
• and introduce a novel lip contact loss formulation for improved lip closures based on physiological cues of bilabial consonants (’m’,’b’,’p’) which also improves other state-of-the-art motion synthesis methods. 2.