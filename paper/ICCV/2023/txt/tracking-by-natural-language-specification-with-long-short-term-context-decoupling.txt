Abstract
The main challenge of Tracking by Natural Language
Speciﬁcation (TNL) is to predict the movement of the tar-get object by giving two heterogeneous information, e.g., one is the static description of the main characteristics of a video contained in the textual query, i.e., long-term context; the other one is an image patch containing the object and its surroundings cropped from the current frame, i.e., the search area. Currently, most methods still struggle with the rationality of using those two information and simply fusing the two. However, the linguistic information contained in the textual query and the visual representation stored in the search area may sometimes be inconsistent, in which case the direct fusion of the two may lead to conﬂicts. To ad-dress this problem, we propose DecoupleTNL, introducing a video clip containing short-term context information into the framework of TNL and exploring a proper way to reduce the impact when visual representation is inconsistent with linguistic information. Concretely, we design two jointly optimized tasks, i.e., short-term context-matching and long-term context-perceiving. The context-matching task aims to gather the dynamic short-term context information in a period, while the context-perceiving task tends to extract the static long-term context information. After that, we de-sign a long short-term modulation module to integrate both context information for accurate tracking. Extensive exper-iments have been conducted on three tracking benchmark datasets to demonstrate the superiority of DecoupleTNL. 1.

Introduction
Tracking by natural language speciﬁcation (TNL), which aims to localize the speciﬁc target referred to by the textual query in a given frame, is a new topic to bridge the two het-erogeneous representations of natural language expression and visual content. Therefore, TNL [20, 8, 7, 9, 22] has re-ceived more and more attention thanks to the fact that it does not require the manually-speciﬁed bounding box to initial-(a) (b) (c)
Figure 1: (a) Most TNL methods directly fuse the visual and linguistic contents to localize the speciﬁc target object. (b)
The textual query describes long-term context information, which may be inconsistent with the visual contents. (c) Our proposed DecoupleTNL framework. ize the tracker. The way natural language understanding is incorporated into visual object tracking has many bene-ﬁts, such as breaking the limitation by using a manually-speciﬁed bounding box and providing extra scene informa-tion from the textual query.
In general, most of the TNL methods share a similar pro-cedure (Figure 1a): encoding visual and linguistic inputs through visual and linguistic components and estimating the location of objects by merging the frame representation and sentence embedding with multi-modal fusion. However, we have found that visual and linguistic content is sometimes inconsistent, in which case a direct fusion of the two may conﬂict (Figure 1b). To be speciﬁc, for the visual part, the model wishes to strengthen the discriminative ability to dis-tinguish the speciﬁc target from distractors in the current scene. On the contrary, the linguistic part desires to max-imize the representation similarity of objects belonging to the same category. To this end, the inconsistent optimiza-tion in these two components limits the development of the current TNL framework in a more accurate way.
Apart from that, most of the previous methods [20, 8, 7, 9, 22] localizing the target may differ in an indirectly manner. In practice, they usually aim to design language-guided candidate matching or selection modules, ignoring the dynamic surroundings through the video ﬂow. To solve this problem, some works have to design an extra module to generate a set of candidates, e.g., region proposals [8] and anchor boxes [9, 32]. Therefore, the performance of these methods is fragile since the predictions are derived from well-designed candidates.
Depending on the above analysis, we propose a self-motivated feature decoupling strategy and a context mod-ulation approach in fusion module design, termed Decou-pleTNL. DecoupleTNL is implemented with Transformers because the attention module is qualiﬁed to establish intra-and inter-modality correspondence for vision and language.
The pipeline of the proposed framework is shown in Figure 1c. We decouple the context information into the form of long-term context (i.e., textual query) and short-term con-text (i.e., video clip). The Short-term Context-Matching (SCM) branch aims to guide the video network in captur-ing the dynamic scene information in a certain period. A contrastive estimation loss is used between the video clip and frame representations, prompting the learning of short-term information. Because the textual query contains in-formation about the entire video, the Long-term Context-Perceiving (LCP) branch requires the model to perceive future scenes based on the information of the given video clip. In such a way, the learned representation contains se-mantic and long-term scene information.
In this branch, contrastive learning compares the predicted and “ground truth” at each point through spatial and temporal spans.
Then, the long short-term context tokens and frame to-kens are fused, and a Long Short-term Modulation (LSM) module is utilized to perform feature modulation.
In summary, we draw the contribution of this paper in the following three aspects:
• We analyze the limitations of TNL trackers and propose a novel long short-term context decoupling framework for tracking by NL description. It simultaneously models the long-term and short-term context information for a more robust representation of learning.
• We propose a long short-term modulation module for in-jecting the long short-term context information into the current frame representation, making our model adap-tively perceive dynamic and static surroundings.
• Extensive experiments are conducted on three popular tracking benchmark datasets. The experimental results fully verify the effectiveness of our proposed method for tracking by the natural language speciﬁcation. 2.