Abstract
Although considerable progress has been made in image deraining under synthetic data, real rain removal is still a tough problem due to the huge domain gap between syn-thetic and real data. Besides, difficulties in collecting and labeling diverse real rain images hinder the progress of this field. Consequently, we attempt to promote real rain re-moval from rain image generation (RIG) perspective. Ex-isting RIG methods mainly focus on diversity but miss real-istic, or the realistic but neglect diversity of the generation.
To solve this dilemma, we propose a physical alignment and controllable generation network (PCGNet) for diverse and realistic rain generation. Our key idea is to simultaneously utilize the controllability of attributes from synthetic and the realism of appearance from real data. Specifically, we de-vise a unified framework to disentangle background, rain attributes, and appearance style from synthetic and real data. Then we collaboratively align the factors with a novel semi-supervised weight moving strategy for attribute, an ex-plicit distribution modeling method for real rain style. Fur-thermore, we pack these aligned factors into the generation model, achieving physical controllable mapping from the attributes to real rain with image-level and attribute-level consistency loss. Extensive experiments show that PCGNet can effectively generate appealing rainy results, which sig-nificantly improve the performance under synthetic and real scenes for all existing deraining methods. 1.

Introduction
The objective of single image deraining (SID) is to re-claim a clean image from the rainy image, a task often deemed an indispensable preliminary stage within outdoor computer vision tasks such as flow estimation [15, 47], de-tection [33], and segmentation [9]. Top performing SID methods [13, 49, 39, 43, 44, 19, 28, 48, 40, 29, 41, 50, 10] are suffering from a common issue: the scarcity of paired real rainy-clean images, thereby resulting in an obvious performance drop in the real rain removal. To overcome this issue, tremendous efforts have been made to advance the real image deraining era. However, most of them
[44, 52, 7, 23, 45, 54, 55, 58] focus on the network archi-tecture for real rain removal, while fewer works [43, 53, 42] have paid attention to datasets via paired clean-rain image generation. In this work, we attempt to promote the real rain
removal of existing SID methods from a data perspective.
Currently, most of the existing rain datasets are sim-ulated by a physical synthetic model. These simulators
[49, 32, 19, 28, 16] utilize hand-crafted prior such as mo-tion blur to simulate the rain streaks and then add them to the clean images. The diverse rainy images are generated by empirically modulated synthetic parameters. However, empirically setting through human subjective assumptions would restrict the generated rain types [42]. To get rid of empirical restrictions, some supervised-learning-based methods are proposed [42, 37] to further promote diver-sity of generation results. The intrinsic of these methods is utilizing the powerful representation of deep CNN to over-fit the physical synthetic rain generation model. Although these diverse synthetic datasets can be used to train deep derainers to some extent, the physical and learning-based rain simulators simplified the degradation of the real rain, resulting in an obvious domain gap between these synthetic datasets and complicated real rain and inevitably bringing performance drop in the real rain removal [55].
To reduce the gap between the synthetic simulator and real degradation, some works [53] treat the rain image gen-eration as a style transfer task through adversarial learning with real rain. However, the GAN-based methods suffer from model collapse [1] and uncontrollable inference, re-sulting in undesired artifacts and unitary appearance in the generation results. Meanwhile, some works [43, 2] utilize the temporal information in the video of real rain to obtain the real paired rainy/clean image. Though the rain images are realistic, there are many limitations to generating the corresponding ‘clean’ image. On the one hand, the clean image generation is very rough, which may be left residual streaks in the generated clean one [42]. On the other hand, the procedure of clean image generation requires that the video scene is strictly static, imposing restrictions on the diversity of the background [43].
In general, existing rain generation methods either learn from the synthetic rainy dataset with controllable diversity but neglect the photorealism, or learn from the real rainy dataset with realism but miss the diversity. However, the degradation process of real rain is complex and morphol-ogy varies. As shown in Fig. 1, we argue that diversity and photorealism are equally important for rain generation methods. For this goal, we propose physical alignment and controllable generation network (PCGNet) to generate di-verse and realistic rainy-clean image pairs. Specifically, we first learn to physically disentangle the rain images into the background, attributes of rain steaks, and style space. Since the attribute of real rain is unlabeled, we propose a novel semi-supervised weight-moving strategy to enable mutual interaction between real and synthetic data. For style space, we explicitly model the appearance style of real rain in the latent space and align the style of real and synthetic images through distribution resampling. Furthermore, we pack these aligned factors into the generation model, achieving physical controllable mapping from the attributes to real rain with image-level and attribute-level consistency loss.
In summary, our contributions are mainly three folds:
• Different from existing real rain removal methods which focus on refining network architectures, we at-tempt to promote real rain removal from a data per-spective. We establish a connection between synthetic and authentic rain images within a semi-supervised framework, facilitating the concurrent generation of diverse and realistic outputs, which offers a new in-sight into the rain image generation community.
• We find the physical relationship of the rain attribute and style between synthetic and real data. Thus, we propose a novel physical disentangle module to decou-ple the synthetic and real rain into attributes and style, and align them in a semi-supervised strategy. Fur-thermore, we propose a controllable generation model, achieving physical mapping from attributes to real rain and enabling realism and diverse generations.
• Extensive experiments show that our method achieves more appealing generation, and significantly improves all the deraining methods both in the synthetic and real scenes. Moreover, PCGNet enables the attribute fine-grained manipulation of the real rain. 2.