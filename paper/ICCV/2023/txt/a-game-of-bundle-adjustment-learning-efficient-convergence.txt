Abstract
Bundle adjustment is the common way to solve localiza-tion and mapping. It is an iterative process in which a sys-tem of non-linear equations is solved using two optimization methods, weighted by a damping factor. In the classic ap-proach, the latter is chosen heuristically by the Levenberg-Marquardt algorithm on each iteration. This might take many iterations, making the process computationally ex-pensive, which might be harmful to real-time applications.
We propose to replace this heuristic by viewing the prob-lem in a holistic manner, as a game, and formulating it as a reinforcement-learning task. We set an environment which solves the non-linear equations and train an agent to choose the damping factor in a learned manner. We demonstrate that our approach considerably reduces the number of it-erations required to reach the bundle adjustment’s conver-gence, on both synthetic and real-life scenarios. We show that this reduction benefits the classic approach and can be integrated with other bundle adjustment acceleration meth-ods. 1.

Introduction
Simultaneous Localization And Mapping (SLAM) is suc-cessfully used in numerous fields, including computer vi-sion [27], augmented reality [1, 22, 32] and autonomous driving [22, 32, 23]. Its input is a series of 2D images of a scene taken by a single camera from different viewpoints, from which a set of 2D matches are extracted. The goal is to estimate the objects’ 3D locations, and the camera’s poses (locations and angles) throughout the capturing according to the 2D matches. See Fig.1 where the 3D locations appear in black and the camera’s poses form the trajectory in red.
Structure From Motion (SFM) is a similar process where the images are taken by several cameras [34, 33, 22, 8].
SLAM is commonly solved using the iterative Bundle
Adjustment (BA) process [9, 21].
In fact, BA occupies roughly 60%–80% of the execution time needed for the
*Equal contribution and corresponding author
†Equal contribution
Figure 1. Given a series of 2D images taken by a camera from different positions, the iterative Bundle Adjustment (BA) process evaluates the 3D locations of the objects in the images (in black) and the camera’s poses, as seen in the red trajectory. We propose a method to accelerate the process by reducing the number iterations required for the solving. mapping [25]. On each iteration the 3D locations and cam-era poses are first evaluated by a combination of two op-timization methods: Gradient descend (GD) and Gauss-Newton (GN), which are weighted according to a damp-ing factor, termed λ. Then, the evaluated locations are projected into 2D according to the evaluated poses. The stopping criterion (convergence) of this iterative process is usually met when the difference between the evaluated 2D projections and the initially extracted 2D matches (termed projection error) is lower than a certain threshold. Due to computational constraints, if convergence is not achieved within a fixed number of iterations, the process is stopped.
Two main factors influence the execution time: (1) the duration of a single iteration, which is mainly affected by the Hessian’s calculation that GN entails; (2) the required number of iterations to reach convergence, caused by ineffi-cient choosing of λ. Some previous BA acceleration meth-ods focus on the first factor and reduce the duration of each iteration, by suggesting efficient ways to calculate and in-vert the sparse Hessian [32, 13]. The focus of this paper is on the second factor—decreasing the number of iterations. the value of λ is deter-mined heuristically by the Levenberg-Marquardt (LM) al-gorithm [21] on each iteration. It may change only by one of two specific constant factors between consecutive itera-In the classic approach,
tions. This limits the ability to efficiently change the opti-mization scheme between GD and GN, even when it can be beneficial. We propose to address this problem differently.
Our key idea is to learn a dynamic value of λ. As the choice of λ’s value on each iteration may influence the solv-ing for several iterations, we propose to view the process in a new light. Differently from previous approaches, we view the BA process in a holistic manner as a game. We show how a simple Reinforcement Learning (RL) framework suf-fices to achieve a solution that upholds a dynamic and ef-ficient weighting of GD and GN, which is determined by
λ. Briefly, RL tasks are defined by an environment and an agent. The agent learns to preform actions according to the environment’s response to these actions (at the form of re-wards). The agent aims to maximize the sum of the ex-pected rewards, which is the key to handling delayed and sparse rewards like the BA’s single and delayed conver-gence. In our case, the environment solves the BA problem and its step performs a single BA iteration. As we aim at a learned λ, we chose to represent the value of λ as the agent’s action. The reward is positive only on the iteration conver-gence is achieved and is negative otherwise. Therefore, in every iteration convergence is not achieved the agent gets a negative reward as a ”fine”. Since the agent aims at max-imizing the sum of the expected rewards, it is encouraged to find a valid solution (reach convergence) within as few iterations as possible.
Our method is shown to reduce the number of iterations required to achieve the BA convergence by a factor of 3-5 on both KITTI [10] and BAL [1] benchmarks. Further-more, our approach is likely to impact common real-life
BA problems, whose solving may require much time due to their large size.
In addition, we demonstrate that our agent could be trained in a time-efficient manner on small synthetic scenes of randomly chosen locations and camera’s poses, and still accelerate the solving of real-life scenarios.
Finally, our approach may be integrated and added to pre-vious works that focus on reducing the time of each itera-tion [32, 13], but might be less beneficial to other methods such as [23] that suggest hardware-based BA optimizations.
Hence our work makes the following contributions: 1. We propose a general and unified approach that learns the ideal value of λ. It can be integrated within other
BA acceleration methods. 2. We propose a network that utilizes this approach using
Reinforcement Learning. We show that it achieves a significant reduction in the number of iterations and running time. On the KITTI benchmark for instance, a 1/5 of the iterations were required, which led to an overall speedup of 3. 2.