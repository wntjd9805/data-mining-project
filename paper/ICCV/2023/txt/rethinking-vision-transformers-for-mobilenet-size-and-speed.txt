Abstract
With the success of Vision Transformers (ViTs) in com-puter vision tasks, recent arts try to optimize the performance and complexity of ViTs to enable efficient deployment on mo-bile devices. Multiple approaches are proposed to accelerate attention mechanism, improve inefficient designs, or incorpo-rate mobile-friendly lightweight convolutions to form hybrid architectures. However, ViT and its variants still have higher latency or considerably more parameters than lightweight
CNNs, even true for the years-old MobileNet. In practice, latency and size are both crucial for efficient deployment on resource-constraint hardware. In this work, we investigate a central question, can transformer models run as fast as
MobileNet and maintain a similar size? We revisit the de-sign choices of ViTs and propose a novel supernet with low latency and high parameter efficiency. We further introduce a novel fine-grained joint search strategy for transformer models that can find efficient architectures by optimizing latency and number of parameters simultaneously. The pro-posed models, EfficientFormerV2, achieve 3.5% higher top-1 accuracy than MobileNetV2 on ImageNet-1K with similar latency and parameters. This work demonstrate that prop-erly designed and optimized vision transformers can achieve high performance even with MobileNet-level size and speed1. 1.

Introduction
The promising performance of Vision Transformers (ViTs) [20] has inspired many follow-up works to further 1Code: https://github.com/snap-research/EfficientFormer.
Figure 1. Comparison of model size, speed, and performance (top-1 accuracy on ImageNet-1K). Latency is profiled by iPhone 12 (iOS 16). The area of each circle is proportional to the number of parameters (model size). EfficientFormerV2 achieves high per-formance with small model sizes and fast inference speed. refine the model architecture and improve training strate-gies, leading to superior results on most computer vision benchmarks, such as classification [49, 51, 8, 54], segmen-tation [80, 14, 6], detection [7, 44, 66], and image synthe-sis [21, 26]. As the essence of ViT, Multi Head Self At-tention (MHSA) mechanism is proved to be effective in modeling spatial dependencies in 2D images, enabling a global receptive field. In addition, MHSA learns second-order information with the attention heatmap as dynamic weights, which is a missing property in Convolution Neural
Networks (CNNs) [27]. However, the cost of MSHA is also obvious–quadratic computation complexity with respect to
the number of tokens (resolution). Consequently, ViTs tend to be more computation intensive and have higher latency compared to widely adopted lightweight CNNs [33, 32], es-pecially on resource-constrained mobile devices, limiting their wide deployment in real-world applications.
Many research efforts [56, 57, 58, 45] are taken to allevi-ate this limitation. Among them, one direction is to reduce the quadratic computation complexity of the attention mech-anism. Swin [50] and following works [19, 49] propose window-based attention such that the receptive field is con-strained to a pre-defined window size, which also inspires subsequent work to refine attention patterns [11, 75, 78, 59].
With the pre-defined span of attention, the computation com-plexity becomes linear to resolution. However, sophisticated attention patterns are generally difficult to support or ac-celerate on mobile devices because of intensive shape and index operations. Another track is to combine lightweight
CNN and attention mechanism to form a hybrid architecture
[56, 13, 55]. The benefit comes two-fold. First, convolu-tions are shift invariant and are good at capturing local and detailed information, which can be considered as a good complement to ViTs [27]. Second, by placing convolutions in the early stages while placing MHSA in the last several stages to model global dependency, we can naturally avoid performing MHSA on high resolution and save computa-tions [48]. Albeit achieving satisfactory performance, the latency and model size are still less competitive compared to lightweight CNNs. For instance, MobileViT [56] achieves better performance than MobileNetV2 while being at least 5× slower on iPhone 12. As applicable to CNNs, architec-ture search, pruning, and quantization techniques are also thoroughly investigated [35, 36, 52, 37, 9, 45, 48]. Never-theless, these models still emerge obvious weaknesses, e.g.,
EfficientFormer-L1 [45] achieves comparable speed and bet-ter performance than MobileNetV2×1.4, while being 2× larger. Thus, a practical yet challenging question arises:
Can we design a transformer-based model that is both light and fast, and preserves high performance?
In this work, we address the above question and pro-pose a new family of mobile vision backbones. We con-sider three vital factors: number of parameters, latency, and model performance, as they reflect disk storage and mo-bile applications. First, we introduce novel architectural improvements to form a strong design paradigm. Second, we propose a fine-grained architecture search algorithm that jointly optimizes model size and speed for transformer mod-els. With our network design and search method, we obtain a series of models under various constraints of model size and speed while maintaining high performance, named Efficient-FormerV2. Under the exact same size and latency (on iPhone 12), EfficientFormerV2-S0 outperforms MobileNetV2 by 3.5% higher top-1 accuracy on ImageNet-1K [18]. Com-pared to EfficientFormer-L1 [45], EfficientFormerV2-S1 has similar performance while being 2× smaller and 1.3× faster (Tab. 2). We further demonstrate promising results in down-stream tasks such as detection and segmentation (Tab. 3).
Our contributions can be concluded as follows.
• We comprehensively study mobile-friendly design choices and introduce novel changes, which is a practical guide to obtaining ultra-efficient vision transformer backbones.
• We propose a novel fine-grained joint search algorithm that simultaneously optimizes model size and speed for transformer models, achieving superior Pareto optimality.
• For the first time, we show that vision transformer models can be as small and fast as MobileNetV2 while obtaining much better performance. EfficientFormerV2 can serve as a strong backbone in various downstream tasks. 2.