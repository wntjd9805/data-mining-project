Abstract
Gaze behaviors such as eye-contact or shared attention are important markers for diagnosing developmental dis-orders in children. While previous studies have looked at some of these elements, the analysis is usually performed on private datasets and is restricted to lab settings. Further-more, all publicly available gaze target prediction bench-marks mostly contain instances of adults, which makes mod-els trained on them less applicable to scenarios with young children. In this paper, we propose the first study for pre-dicting the gaze target of children and interacting adults. To this end, we introduce the ChildPlay dataset: a curated col-lection of short video clips featuring children playing and interacting with adults in uncontrolled environments (e.g. kindergarten, therapy centers, preschools etc.), which we annotate with rich gaze information. We further propose a new model for gaze target prediction that is geometrically grounded by explicitly identifying the scene parts in the 3D field of view (3DFoV) of the person, leveraging recent ge-ometry preserving depth inference methods. Our model achieves state of the art results on benchmark datasets and ChildPlay. Furthermore, results show that looking at faces prediction performance on children is much worse than on adults, and can be significantly improved by fine-tuning models using child gaze annotations. Our dataset is available at https://www.idiap.ch/en/dataset/ childplay-gaze. Code will be made available soon. 1.

Introduction
Gaze is a non-verbal cue that provides rich information about people. In particular, it plays fundamental roles in so-cial interactions and human communication, like initiating interaction, showing attention, monitoring the floor, or reg-ulating intimacy, and as such finds many applications in hu-man interaction analysis. Hence, gaze extraction and anal-ysis finds applications in human-human or human-robot in-teraction [54, 40], including psychological studies [38] and medical diagnoses.
* indicates equal contribution
Figure 1. Sample images from the ChildPlay dataset with head bounding box and gaze point annotations. Such scenes strongly depart from existing gaze benchmarks (e.g. standing adults).
Figure 2. Qualitative results of our geometrically grounded model on ChildPlay. Our 3D Field of View (3DFoV) highlights potential gaze targets, excluding objects where the depth does not match.
Gaze target predictions are given in green and GT ones in red.
In this regard, acquiring appropriate social gaze behavior skills is important in the cognitive development of children.
For instance, gaze following has been shown to help chil-dren with language acquisition [5, 6], while gaze aversion can help children perform better in cognitively demanding tasks [42]. Furthermore, abnormal gaze patterns are known to correlate with several neuro-developmental disorders like
Autism Spectrum Disorder (ASD) [53, 37]. This has led to the development of specific gold standard markers for the screening of these impairments, e.g. by measuring deficits in the initiation of joint attention or shared attention [12].
ChildPlay dataset. Due to this importance, several meth-ods have been proposed to analyze children’s gaze, espe-cially in the context of autism [48, 7, 22, 50, 1, 55, 56, 8, 31]. However, they are all tested on private datasets [11] due to the sensitive nature of the data, hindering proper comparison across algorithms. Some of them may be ac-cessible in anonymized formats (e.g. pose skeletons), but this makes them difficult to use for the study of gaze behav-ior. While other datasets have gaze related labels involving
autistic children [4, 47], they are usually recorded in a fixed lab setting and with coarse annotations. Alternatively, we could use standard public benchmarks like [10] for learn-ing gaze prediction models, but children and the physical situations and task performed (seating on the floor, playing with objects) would be severely underrepresented.
It has been shown that training models on datasets with mainly adults can lead to a significant drop in performance when tested on children, e.g. for body landmark prediction [52].
Given the importance of pose information in gaze prediction
[21, 3] and the difference in gaze behavior between adults and children [16], there is a need for gaze annotated datasets featuring lower age groups in general settings.
To address this problem, we introduce the ChildPlay dataset: a set of videos featuring children in free-play en-vironments interacting with their surrounding. The dataset is rich in unprompted social behaviors, communicative ges-tures and interactions and features high quality dense gaze annotations, including a gaze class to account for special scenarios that arise in 2D gaze following. Further, the 2D gaze information can be used to model other attention-related behaviors like shared attention, gaze shifts, eye con-tact and fixation points with minimal processing. To the best of our knowledge, we are the first to establish a more representative gaze dataset aiming to cover children. 3D Field of View (3DFoV) for gaze target prediction.
Recasens et al. [46] introduced this task, also called gaze following. In contrast with previous gaze objectives which were mainly attempting at inferring the 3D gaze direction from head and eye images [17, 34], it aims to predict the im-age 2D gaze location of a person in the image for arbitrary and general scenes. Since then, many works have embraced this paradigm [32, 9, 63, 19, 39, 26], proposing new models exploiting temporal information [10], or exploiting further cues like depth [15, 2, 21].
Indeed, inferring and under-standing depth is crucial, as it provides information about the scene structure enabling geometric reasoning and ruling out salient objects or people which fall along the 2D line of sight of a person, but are actually not visible to the per-son in the 3D space. In this context, as most datasets do not have depth information, some methods opted for pre-trained monocular depth (or disparity) estimators to extract scene depth cues [15, 2, 21]. However, such algorithms [44, 61] typically estimate the depth up to an unknown shift and scale factors which often result in stretched and distorted scenes unsuitable for proper 3D analysis.
In this paper we provide a more geometrically grounded approach leveraging a new algorithm [41] addressing these points, correcting shifts, and yielding geometry-preserving depth maps that can be used to derive a proper scene point cloud, and explicitly match the predicted 3D gaze vector with this point cloud to derive the 3DFoV of the person.
In experiments, we show that this method generalizes well, providing better cross-dataset performance.
New gaze metric: looking at heads precision (P.Head).
Standard performance metric for gaze following either have no physical interpretation (the Area Under Curve, AUC), or may not provide rich enough information about perfor-mance, as is the case of 2D distance metrics (how far is a 2D gaze prediction from the GT). In practice, one is more interested at semantics, e.g. how accurate is a model at predicting the category (person, body part, object) of im-age regions being looked at. As objects might be hard to annotate at scale, in this paper, leveraging highly accurate head detectors and since heads are one of the most impor-tant gaze category in many applications (ex. child looking at clinician for ASD diagnosis [12]), we propose to exploit looking at head precision (P.Head) metric for performance evaluation. We show that this measure greatly varies across datasets and can have rather low performance, that the dis-tance and P.Head metrics may disagree and performance on children is rather different than on adults. On ChildPlay, while children exhibit a better distance performance, their
P.Head metric is much worse.
Contributions. Our main contributions are:
• We introduce the ChildPlay dataset, a curated collec-tion of clips recording children playing and interacting with adults in uncontrolled environments, annotated with rich gaze information;
• We propose a new model for gaze target prediction that relies on the explicit modeling of the 3DFoV by ex-ploiting geometrically consistent inferred depth maps.
• We propose to use the Looking At Head Precision met-ric to characterize performance.
Extensive experiments on the GazeFollow, VideoAttention-Target and ChildPlay datasets demonstrate that our ap-proach produces the best or state-of-the-art results, moti-vating further studies on the topic. The dataset and models are publicly available. 2.