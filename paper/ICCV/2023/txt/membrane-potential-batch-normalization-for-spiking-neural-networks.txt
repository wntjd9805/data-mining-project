Abstract
As one of the energy-efficient alternatives of conven-tional neural networks (CNNs), spiking neural networks (SNNs) have gained more and more interest recently. To train the deep models, some effective batch normalization (BN) techniques are proposed in SNNs. All these BNs are suggested to be used after the convolution layer as usually doing in CNNs. However, the spiking neuron is much more complex with the spatio-temporal dynamics. The regulated data flow after the BN layer will be disturbed again by the membrane potential updating operation before the firing function, i.e., the nonlinear activation. Therefore, we advo-cate adding another BN layer before the firing function to normalize the membrane potential again, called MPBN. To eliminate the induced time cost of MPBN, we also propose a training-inference-decoupled re-parameterization tech-nique to fold the trained MPBN into the firing threshold.
With the re-parameterization technique, the MPBN will not introduce any extra time burden in the inference. Further-more, the MPBN can also adopt the element-wised form, while these BNs after the convolution layer can only use the channel-wised form. Experimental results show that the proposed MPBN performs well on both popular non-spiking static and neuromorphic datasets. 1.

Introduction
Emerged as a biology-inspired method, spiking neural networks (SNNs) have received much attention in artificial intelligence and neuroscience recently [17, 13, 57, 56, 47, 58, 59]. SNNs deal with binary event-driven spikes as their activations and therefore the multiplications of activations and weights can be substituted for additions or only keep-ing silents. Benefitting from such a computation paradigm,
SNNs derive extreme energy efficiency and run efficiently when implemented on neuromorphic hardware [1, 42, 5].
*Equal contribution.
â€ Corresponding author, mazhe thu@163.com.
Despite the SNN has achieved great success in diverse fields including pattern recognition [12, 21, 19, 14], ob-ject detection [30], language processing [55], robotics [9], and so on, its development is deeply inspired by the expe-rience of convolutional neural networks (CNNs) in many aspects. However, the spiking neuron model along with the rich spatio-temporal dynamic makes SNNs much differ-ent from CNNs, and directly transferring some experience of CNNs to SNNs without any modifications may be not a good idea. As one of the famous techniques in CNNs, the batch normalization(BN) technique shows great advan-tages. It can reduce the gradient exploding/vanishing prob-lem, flatten the loss landscape, and reduce the internal co-variate shift, thus being widely used in CNNs. There are also some works trying to apply normalization approaches in the SNN field to help model convergence. For example, inspired by BN in CNNs, NeuNorm [51] was proposed to normalize the data along the channel dimension. Consider-ing that the temporal dimension is also important in SNNs, threshold-dependent batch normalization (tdBN) [62] then extended the scope of BN to the additional temporal dimen-sion. Subsequently, to better depict the differences of data flow distributions in different time dimensions, the tempo-ral batch normalization through time (BNTT) [31], postsy-naptic potential normalization (PSP-BN) [28], and temporal effective batch normalization (TEBN) [10] that regulate the data flows with multiple BNs on different time steps were proposed.
However, all these BNs proposed in SNNs are advised to be used after convolution layers as usually doing in CNNs.
This ignores the fact that the nonlinear transformation in the SNN spiking neuron is much more complex than that of the ReLU neuron. In the spiking neuron, the data flow after the convolution layer will be first injected into the residual membrane potential (MP) coming from the pre-vious time step to generate a new MP at the current time step. And then the neuron will fire a spike or keep silent still based on whether or not the new MP is up to the fir-ing threshold. Obviously, though the data flow has been normalized by the BN after the convolution layer, it will be
with other state-of-the-art SNN models on both static and dynamic datasets, e.g., 96.47% top-1 accuracy and 79.51% top-1 accuracy are achieved on the CIFAR-10 and CIFAR-100 with only 2 time steps. 2.