Abstract
Recent research on one-shot face re-enactment has pro-gressively overcome the low-resolution constraint with the help of StyleGAN’s high-fidelity portrait generation. How-ever, such approaches rely on explicit 2D/3D structural pri-ors for guidance and/or use flow-based warping which con-strain their performance. Moreover, existing methods are sensitive (not robust) to the source frame’s facial expres-sions and head pose, even though ideally only the iden-tity of the source frame should have an effect. Address-ing these limitations, we propose a novel framework ex-ploiting the implicit 3D prior and inherent latent proper-ties of StyleGAN2 to facilitate one-shot face re-enactment at 10242 (1) with zero dependencies on explicit structural priors, (2) accommodating attribute edits, and (3) robust to diverse facial expressions and head poses of the source frame. We train an encoder using a self-supervised ap-proach to decompose the identity and facial deformation of a portrait image within the pre-trained StyleGAN2’s pre-defined latent spaces itself (automatically facilitating (1) and (2)). The decomposed identity latent of the source and the facial deformation latents of the driving sequence are used to generate re-enacted frames using the Style-GAN2 generator. Additionally, to improve the identity reconstruction and to enable seamless transfer of driv-ing motion, we propose a novel approach, Cyclic Man-ifold Adjustment. We perform extensive qualitative and quantitative analyses which demonstrate the superiority of the proposed approach against state-of-the-art methods.
Project page: https://trevineoorloff.github.io/
FaceVideoReenactment_HybridLatents.io/.
Figure 2: The pipeline of the proposed framework. The high-level re-enactment process (Top), the expanded architectures of the encoding (Bottom-Left) and re-enactment (Bottom-Right) processes are depicted. In encoding, given a frame, the En-coder, E, outputs a pair of latents: Identity latent, WID, and Facial-deformation latent, SF , that reside within the predefined
W + and SS spaces of StyleGAN2. In re-enactment, W S
F (driving frame) are combined to form the animated SS latent, which is used to obtain the re-enacted frame using the StyleGAN2 Generator, G.
ID (source) transformed using A(·) and S D 1.

Introduction
One-shot face video re-enactment refers to the process of generating a video by animating the identity of a por-trait image (source frame) mimicking the facial deforma-tions and head pose of a driving video. The increasing in-terest in virtual reality has stimulated video re-enactment due to its wide range of applications (e.g., digital avatars, animated movies, telepresence).
The task of one-shot face re-enactment is challenging since it requires extraction of (1) identity and 3D facial structure of the given 2D source frame and (2) motion in-formation from the driving frames, to facilitate realistic an-imations, despite the unavailability of paired data. To this end, most research employ facial landmarks [18, 24, 32, 43] or 3D parametrized models [12, 19, 26, 42] to capture the underlying facial structure and/or motion. Even though explicit facial structure priors may support rigorous con-trol, they suffer from lack of generalizability (for differ-ent face geometries), inability to capture fine/complex facial deformations (e.g., wrinkles, internal mouth details such as tongue and teeth), inability to handle accessories such as eyeglasses, and inconsistencies in predictions which hinder their performance. While latent-based models [9, 37, 39, 44] and predictive keypoint models [30, 36] alleviate the dependency on explicit structural priors they are limited to producing low resolution videos.
Moreover, since the source frame is only responsible for determining the identity of the animated frame in one-shot face re-enactment, ideally, the expression and head pose of the source frame should not have an effect on the animated frame. We define this property as one-shot robustness. Ex-isting one-shot re-enactment methods do not address this issue and thus have poor robustness to diverse expressions and head poses of the source frame (see Fig. 1: Right).
StyleGAN2’s [22] ability to generate high-resolution (10242) photo-realistic faces and semantic interpretabil-ity of its latent spaces [13, 29, 40] lead to improved re-enactment generations [19, 25, 42]. Considering Style-GAN2 latent space manipulations [1, 13, 25, 40], which facilitate edits on head pose, smile, gaze, blink, etc., it is ev-ident that the predefined latent space of a pre-trained Style-GAN2 has implicit 3D information embedded within it. We conjecture that the StyleGAN2’s latent spaces are not yet fully exploited for re-enactment and use of explicit struc-tural representations as in [19, 42] (3DMM) is redundant and limits the performance of StyleGAN2 to the capacity-limits of such structural priors as discussed previously.
In this work, we address the following question: Can we learn a general model to facilitate face identity, attributes, and motion edits leveraging the predefined latent spaces of
StyleGAN2 without reliance on explicit 2D/3D facial struc-ture priors while improving the performance of generating realistic, high-quality, and temporally consistent one-shot face re-enactment videos that are also robust to diverse ex-pressions and head poses of the source frame?
We train an encoder through a self-supervised approach to encode a given portrait image as an Identity latent, WID, and a Facial deformation latent, SF , that reside in the prede-fined W + and StyleSpace (SS) latent spaces [40] of Style-GAN2 respectively, thus forming a hybrid latent space.
This novel approach of decomposing identity and facial de-formation within the predefined latent spaces of StyleGAN2 itself obviates the need for explicit structural priors and ac-commodate latent-based attribute manipulation (e.g., smile, pose, age, etc.) proposed by previous research [13, 29]. We exploit the inherent latent properties of the W + and SS la-tent spaces: best distortion-editability trade-off and best dis-entanglement [40] respectively, to design the decomposition framework as explained in Sec. 3. The decomposed identity latent of the source and the deformation latents of the diving sequence of frames are then used to generate high-fidelity one-shot face re-enactment video (with or without attribute edits) at 10242 using a pre-trained StyleGAN2 generator (Fig. 2). Further, the model was carefully designed such that the re-enacted video is robust to diverse head poses and expressions of the source frame.
Additionally, we propose a novel algorithm, Cyclic Man-ifold Adjustment (CMA) inspired by PTI [28], to address (1)
StyleGAN2’s poor identity reconstruction of out-of-domain source frames and (2) the non-homogeneity of the local manifolds around the driving identity and the source iden-tity latents (i.e., a facial deformation edit applied to two identity latents would have slight differences in the rendered animation due to the non-homogeneity of local manifolds around different identities). Thus improving the source identity reconstruction and enabling seamless transfer of fa-cial deformations of the driving video. While research such as [5, 28, 34] addresses the former, to the best of our knowl-edge no research has been conducted to address the latter.
In summary, our key contributions include:
• A novel StyleGAN2-based hybrid latent space frame-work that enables high-fidelity one-shot face video re-enactment at 10242, robust to diverse head poses and expressions of the source yielding state-of-the-art re-sults (quantitative improvement of upto 12% in cross-identity re-enactment and 50% in one-shot robustness),
• A novel hybrid latent space approach of decomposing the identity and facial deformation of a portrait im-age within the predefined latent spaces of StyleGAN2 itself obviating the need for explicit structural priors for guidance and facilitating re-enactments with latent-based attribute edits,
• A novel algorithm, Cyclic Manifold Adjustment, that locally adjusts the StyleGAN2’s manifold to improve the reconstruction of an out-of-domain source and en-able seamless transfer of facial deformations of the driving video to the source image.
To the best of our knowledge, we are the first to decom-pose identity and facial deformation within the pre-trained
StyleGAN2’s predefined latent spaces itself, the first to han-dle robustness to diverse head pose and expressions of the source frames, and the first to propose a manifold adjust-ment technique handling both source identity reconstruc-tion and non-homogeneity of the latent space in the task of latent-based re-enactment. 2.