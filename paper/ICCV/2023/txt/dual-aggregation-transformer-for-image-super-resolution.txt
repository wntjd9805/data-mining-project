Abstract
Transformer has recently gained considerable popu-larity in low-level vision tasks, including image super-resolution (SR). These networks utilize self-attention along different dimensions, spatial or channel, and achieve im-pressive performance. This inspires us to combine the two dimensions in Transformer for a more powerful rep-resentation capability. Based on the above idea, we pro-pose a novel Transformer model, Dual Aggregation Trans-former (DAT), for image SR. Our DAT aggregates fea-tures across spatial and channel dimensions, in the inter-block and intra-block dual manner. Specifically, we alter-nately apply spatial and channel self-attention in consec-utive Transformer blocks. The alternate strategy enables
DAT to capture the global context and realize inter-block feature aggregation. Furthermore, we propose the adaptive interaction module (AIM) and the spatial-gate feed-forward network (SGFN) to achieve intra-block feature aggregation.
AIM complements two self-attention mechanisms from cor-responding dimensions. Meanwhile, SGFN introduces ad-ditional non-linear spatial information in the feed-forward network. Extensive experiments show that our DAT sur-passes current methods. Code and models are obtainable at https://github.com/zhengchen1999/DAT. 1.

Introduction
Single image super-resolution (SR) is a traditional low-level vision task that focuses on recovering a high-resolution (HR) image from a low-resolution (LR) coun-terpart. As an ill-posed problem with multiple potential solutions for a given LR input, various approaches have emerged to tackle this challenge in recent years. Many of these methods utilize convolutional neural networks (CNNs) [12, 47, 10, 29]. However, the convolution adopts a local mechanism, which hinders the establishment of global dependencies and restricts the performance of the model.
Recently, Transformer proposed in natural language pro-cessing (NLP) has performed notably in multiple high-level vision tasks [13, 39, 24, 11, 7]. The core of Transformer
*Corresponding authors: Yulun Zhang, yulun100@gmail.com; Linghe
Kong, linghe.kong@sjtu.edu.cn
HR
Bicubic
CSNLN [29]
Urban100: img_059
SwinIR [20]
CAT-A [9]
DAT (Ours)
Figure 1: Visual comparison (×4) on Urban100. CSNLN,
SwinIR, and CAT-A suffer from blurring artifacts. is the self-attention (SA) mechanism, which is capable of establishing global dependencies. This property alleviates the limitations of CNN-based algorithms. Considering the potential of Transformer, some researchers attempt to ap-ply Transformer to low-level tasks [20, 44, 42, 9], including image SR. They explore efficient usages of Transformer on high-resolution images from different perspectives to miti-gate the high complexity of global self-attention [13]. For the spatial aspect, some methods [20, 46, 9] apply local spa-tial windows to limit the scope of self-attention. For the channel aspect, the “transposed” attention [44] is proposed, which calculates self-attention along the channel dimension rather than the spatial dimension. These methods all ex-hibit remarkable results due to the strong modeling ability in their respective dimensions. Spatial window self-attention (SW-SA) is able to model fine-grained spatial relationships between pixels. Channel-wise self-attention (CW-SA) can model relationships among feature maps, thus exploiting global image information. Generally, both extracting spa-tial information and capturing channel context are crucial to the performance of Transformer in image SR.
Motivated by the aforementioned findings, we propose the Dual Aggregation Transformer (DAT) for image SR.
Our DAT aggregates spatial and channel features via the inter-block and intra-block dual way to obtain power-ful representation capability. Specifically, we alternately apply spatial window and channel-wise self-attention in successive dual aggregation Transformer blocks (DATBs).
Through this alternate strategy, our DAT can capture both spatial and channel context and realize inter-block fea-ture aggregation between different dimensions. Moreover, the two self-attention mechanisms complement each other.
Spatial window self-attention enriches the spatial expres-sion of each feature map, helping to model channel depen-dencies. Channel-wise self-attention provides the global information between features for spatial self-attention, ex-panding the receptive field of window attention.
Meanwhile, since self-attention mechanisms focus on modeling global information, we incorporate convolution to self-attention in parallel, to complement Transformer with the locality. To enhance the fusion of the two branches and aggregate both spatial and channel information within a single self-attention module, we propose the adaptive inter-action module (AIM). It consists of two interaction opera-tions, spatial-interaction (S-I) and channel-interaction (C-I), which act between two branches to exchange information.
Through S-I and C-I, the AIM adaptively re-weight the fea-ture maps of two branches from the spatial or channel di-mension, according to different self-attention mechanisms.
Besides, with AIM, we design two new self-attention mech-anisms, adaptive spatial self-attention (AS-SA) and adap-tive channel self-attention (AC-SA), based on the spatial window and channel-wise self-attention, respectively.
Furthermore, another component of the Transformer block, the feed-forward network (FFN) [38], extracts fea-tures through fully-connected layers. It ignores modeling spatial information. In addition, the redundant information between channels obstructs further advances in feature rep-resentation learning. To cope with these issues, we design the spatial-gate feed-forward network (SGFN), which in-troduces the spatial-gate (SG) module between two fully-connected layers of FFN. The SG module is a simple gat-ing mechanism (depth-wise convolution and element-wise multiplication). The input feature of SG is partitioned into two segments along the channel dimension for convolution and multiplicative bypass. Our SG module can complement
FFN with additional non-linear spatial information and re-lieve channel redundancy. In general, based on AIM and
SGFN, DAT can realize intra-block feature aggregation.
Overall, with the above three designs, our DAT can ag-gregate spatial and channel information through the inter-block and intra-block dual way to achieve strong feature expressions. Consequently, as displayed in Fig. 1, our DAT achieves superior visual results against recent state-of-the-art SR methods. Our contributions are three-fold:
• We design a new image SR model, dual aggregation
Transformer (DAT). Our DAT aggregates spatial and channel features in the inter-block and intra-block dual manner to obtain powerful representation ability.
• We alternately adopt spatial and channel self-attention, realizing inter-block spatial and channel feature ag-gregation. Moreover, we propose AIM and SGFN to achieve intra-block feature aggregation.
• We conduct extensive experiments to demonstrate that our DAT outperforms state-of-the-art methods, while retaining lower complexity and model size. 2.