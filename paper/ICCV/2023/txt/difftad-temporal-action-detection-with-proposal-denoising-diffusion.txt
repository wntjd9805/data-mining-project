Abstract
We propose a new formulation of temporal action de-tection (TAD) with denoising diffusion, DiffTAD in short.
Taking as input random temporal proposals, it can yield ac-tion proposals accurately given an untrimmed long video.
This presents a generative modeling perspective, against previous discriminative learning manners. This capabil-ity is achieved by first diffusing the ground-truth propos-als to random ones (i.e., the forward/noising process) and then learning to reverse the noising process (i.e., the back-ward/denoising process). Concretely, we establish the de-noising process in the Transformer decoder (e.g., DETR) by introducing a temporal location query design with faster convergence in training. We further propose a cross-step selective conditioning algorithm for inference accelera-tion. Extensive evaluations on ActivityNet and THUMOS show that our DiffTAD achieves top performance com-pared to previous art alternatives. The code is available at https://github.com/sauradip/DiffusionTAD. 1.

Introduction
Temporal action detection (TAD) aims to predict the temporal duration (i.e., start and end time) and the class la-bel of each action instance in an untrimmed video [32, 9].
Existing methods rely on proposal prediction by regressing anchor proposals [86, 13, 21, 47] or predicting the start/end times of proposals [41, 8, 42, 90, 52, 87, 88, 50, 49, 51].
These models are all discriminative learning based.
In the generative learning perspective, diffusion model
[66] has been recently exploited in image based object de-tection [15]. This represents a new direction for designing detection models in general. Although conceptually sim-ilar to object detection, the TAD problem presents more complexity due to the presence of temporal dynamics. Be-sides, there are several limitations with the detection diffu-sion formulation in [15]. First, a two-stage pipeline (e.g.,
RCNN [13]) is adopted, suffering localization-error prop-*This work was done during internship with Jiankang Deng.
Figure 1. Diffusion for temporal action detection (TAD). (a) A diffusion model for text-to-image generation where text embed-dings are passed as the condition in the denoising (reverse) pro-cess. We draw an analogy by exploiting (b) a diffusion model for
TAD: To generate action temporal boundaries from noisy propos-als with the condition of video embedding. agation from proposal generation to proposal classification
[48]. Second, as each proposal is processed individually, their relationship modeling is overlooked, potentially hurt-ing the learning efficacy. To avoid these issues, we adopt the one-stage detection pipelines [75, 80] that have already shown excellent performance with a relatively simpler de-sign, in particular, DETR [11].
Nonetheless, it is non-trivial to integrate denoising diffu-sion with existing detection models, due to several reasons. (1) A unique challenge with TAD is big boundary ambigu-ity. This is because activities are continuous in time without clear start/end points (i.e., non-zero momentum), and the transition between consecutive activities is often stochastic.
Further, human perception of action boundaries is also in-stinctive and subjective. (2) Denoising diffusion and action detection both suffer low efficiency, and their combination would make it even worse. Both of the problems have not been investigated systematically thus far.
To address the aforementioned challenges, a novel Pro-posal Denoising Diffusion method is proposed for effi-ciently tackling the TAD task in a diffusion formulation, abbreviated as DiffTAD. It takes a set of random temporal proposals (i.e., the start and end time pairs) following Gaus-sian distribution, and outputs the action proposals of a given untrimmed video. At training time, Gaussian noises are first added to the ground truth proposals to make noisy propos-als. These discrete noisy proposals are then projected into a continuous vector space using sinusoidal projection [44] to form noisy queries in which the decoder (e.g., DETR) will conduct the denoising diffusion process. Our denoising space choice facilitates the adoption of existing diffusion models, as discussed above. As a byproduct, the denois-ing queries strategy itself can accelerate the training conver-gence of DETR type models [38]. At inference time, con-ditioned on a test video, DiffTAD can generate action tem-poral boundaries by reversing the learned diffusion process from Gaussian random proposals. For improving inference efficiency, we further introduce a cross-timestep selective conditioning mechanism with two key functions: (1) mini-mizing the redundancy of intermediate predictions at each sampling step by filtering out the proposals far away from the distribution of corrupted proposals generated in training, and (2) conditioning the next sampling step by selected pro-posals to regulate the diffusion direction for more accurate inference.
Our contributions are summarized as follows. (1) For the first time we formulate the temporal action detection problem through denoising diffusion in the elegant trans-former decoder framework. Additionally, integrating de-noising diffusion with this decoder design solves the typi-cal slow-convergence limitation. (2) We further enhance the diffusion sampling efficiency and accuracy by introducing a novel selective conditioning mechanism during inference. (3) Extensive experiments on ActivityNet and THUMOS benchmarks show that our DiffTAD achieves favorable per-formance against prior art alternatives. 2.