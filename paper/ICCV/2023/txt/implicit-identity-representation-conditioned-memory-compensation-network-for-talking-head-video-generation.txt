Abstract
Talking head video generation aims to animate a human face in a still image with dynamic poses and expressions us-ing motion information derived from a target-driving video, while maintaining the person’s identity in the source im-age. However, dramatic and complex motions in the driv-ing video cause ambiguous generation, because the still source image cannot provide sufficient appearance infor-mation for occluded regions or delicate expression vari-ations, which produces severe artifacts and significantly degrades the generation quality. To tackle this problem, we propose to learn a global facial representation space, and design a novel implicit identity representation condi-tioned memory compensation network, coined as MCNet, for high-fidelity talking head generation. Specifically, we devise a network module to learn a unified spatial facial meta-memory bank from all training samples, which can provide rich facial structure and appearance priors to com-pensate warped source facial features for the generation.
Furthermore, we propose an effective query mechanism based on implicit identity representations learned from the discrete keypoints of the source image. It can greatly facil-itate the retrieval of more correlated information from the memory bank for the compensation. Extensive experiments demonstrate that MCNet can learn representative and com-plementary facial memory, and can clearly outperform pre-vious state-of-the-art talking head generation methods on
VoxCeleb1 and CelebV datasets. Please check our Project. 1.

Introduction
In this paper, we aim at addressing the problem of gener-ating a realistic talking head video given one still source im-age and one dynamic driving video, which is widely known as talking head video generation. A high-quality talking head generation model needs to imitate vivid facial expres-sions and complex head movements, and should be appli-cable for different facial identities presented in the source
image and the target video. It has been attracting rapidly increasing attention from the community, and a wide range of realistic applications remarkably benefits from this task, such as digital human broadcast, AI-based human conver-sation, and virtual anchors in films.
Significant progress has been achieved on this task in terms of both quality and robustness in recent years. Exist-ing works mainly focus on learning more accurate motion estimation and representation in 2D or 3D to improve the generation. More specifically, 2D facial keypoints or land-marks are learned to model the motion flow (see Fig. 1c) between the source image and any target image in the driv-ing video [38, 36, 7]. Some works also consider utilizing 3D facial prior model (e.g. 3DMM[1]) with decoupled expres-sion codes [38, 36] or learning dense facial geometries in a self-supervised manner [7] to model complex facial expres-sion movements to produce more fine-grained facial gener-ation. However, no matter how accurately the motion can be estimated and represented, highly dynamic and complex motions in the driving video cause ambiguous generation from the source image (see Fig. 1d), because the still source image cannot provide sufficient appearance information for occluded regions or delicate expression variations, which severely produces artifacts and significantly degrades the generation quality.
Intuitively, we understand that human faces are highly symmetrical and structured, and many regions of the hu-man faces are essentially not discriminative. For instance, only blocking a very small eye region of a face image makes a well-trained facial recognition model largely drop the recognition performance [16], which indicates to a cer-tain extent that the structure and appearance representations of human faces crossing different face identities are generic and transferable. Therefore, learning global facial priors on spatial structure and appearance from all available training face images, and utilizing the learned facial priors for com-pensating the dynamic facial synthesis is highly potential for high-fidelity talking head generation, while it has been barely explored in existing works.
In this paper, to effectively deal with the ambiguities in dramatic appearance changes from the still source im-age, we propose an implicit identity representation condi-tioned Memory Compensation Network, coined as MCNet, to learn and transfer global facial representations to com-pensate ambiguous facial details for a high-fidelity genera-tion. Specifically, we design and learn a global and spatial facial meta-memory bank. The optimization gradients from all the training images during training contribute together to the updating of the meta memory, and thus it can capture the most representative facial patterns globally. Since the different source face images contain distinct structures and appearances, to more effectively query the learned global meta memory bank, we propose an implicit identity repre-sentation conditioned memory module (IICM) (see Fig. 3).
The implicit identity representation is learned from both the discrete keypoint coordinates of the source face image that contains the facial structure information, and the warped source feature map that represents facial appearance distri-bution. Then, we further use it to condition the query on the global facial meta-memory bank to learn a more correlated memory bank for the source, which can effectively compen-sate the source facial feature maps for the generation. The compensation is then performed through a proposed mem-ory compensation module (MCM) (see Fig. 4).
We conduct extensive experiments to evaluate the pro-posed MCNet on two competitive talking head generation datasets (i.e. VoxCeleb [15] and CelebV [29]. Experimen-tal results demonstrate the effectiveness of learning global facial memory to tackle the appearance ambiguities in the talking head generation, and also show clearly improved generation results over state-of-the-art methods from both qualitative and quantitative perspectives.
In summary, our main contribution is three-fold:
• We propose to learn a global facial meta-memory bank to transfer representative facial patterns to handle the appearance and structure ambiguities caused by highly dynamic generation from a still source image. To the best of our knowledge, it is the first exploration in the literature to model global facial representations to ad-dress the ambiguities in talking head generation.
• We propose a novel implicit identity representation conditioned memory compensation network (MCNet) for talking head video generation, in which an implicit identity representation conditioned memory module (IICM) and a facial memory compensation module (MCM) are designed to respectively perform the meta-memory query and feature compensation.
• Qualitative and quantitative experiments extensively show the effectiveness of the learned meta memory bank for addressing the ambiguities in generation, and our framework establishes a clear state-of-the-art per-formance on the talking head generation. The gener-alization experiment also shows that the proposed ap-proach can effectively boost the performance of differ-ent talking head generation frameworks. 2.