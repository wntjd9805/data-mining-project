Abstract
Casually captured Neural Radiance Fields (NeRFs) suf-fer from artifacts such as floaters or flawed geometry when rendered outside the input camera trajectory. Existing eval-uation protocols often do not capture these effects, since they usually only assess image quality at every 8th frame of the training capture. To aid in the development and eval-uation of new methods in novel-view synthesis, we propose a new dataset and evaluation procedure, where two camera trajectories are recorded of the scene: one used for train-ing, and the other for evaluation. In this more challenging in-the-wild setting, we find that existing hand-crafted regu-larizers do not remove floaters nor improve scene geometry.
Thus, we propose a 3D diffusion-based method that lever-ages local 3D priors and a novel density-based score dis-tillation sampling loss to discourage artifacts during NeRF optimization. We show that this data-driven prior removes floaters and improves scene geometry for casual captures. 1.

Introduction
Casual captures of Neural Radiance Fields (NeRFs) [20] are usually of lower quality than most captures shown in
NeRF papers. When a typical user (e.g., a hobbyist) cap-tures a NeRFs, the ultimate objective is often to render a fly-through path from a considerably different set of view-points than the originally captured images. This large view-point change between training and rendering views usu-ally reveals floater artifacts and bad geometry, as shown in Fig. 1a. One way to resolve these artifacts is to teach or otherwise encourage users to more extensively capture a scene, as is commonly done in apps such as Polycam1 and Luma2, which will direct users to make three circles at three different elevations looking inward at the object of in-terest. However, these capture processes can be tedious, and
*Denotes equal contribution 1https://poly.cam/ 2https://lumalabs.ai/
furthermore, users may not always follow complex capture instructions well enough to get an artifact-free capture.
Another way to clean NeRF artifacts is to develop algo-rithms that allow for better out-of-distribution NeRF ren-derings. Prior work has explored ways of mitigating arti-facts by using camera pose optimization [38, 13] to handle noisy camera poses, per-image appearance embeddings to handle changes in exposure [15], or robust loss functions to handle transient occluders [32]. However, while these tech-niques and others show improvements on standard bench-marks, most benchmarks focus on evaluating image quality at held-out frames from the training sequence, which is not usually representative of visual quality at novel viewpoints.
Fig. 1c shows how the Nerfacto method starts to degrade as the novel-view becomes more extreme.
In this paper, we propose both (1) a novel method for cleaning up casually captured NeRFs and (2) a new eval-uation procedure for measuring the quality of a NeRF that better reflects rendered image quality at novel viewpoints.
Our proposed evaluation protocol is to capture two videos: one for training a NeRF, and a second for novel-view evalu-ation (Fig. 1b). Using the images from the second capture as ground-truth (as well as depth and normals extracted from a reconstruction on all frames), we can compute a set of met-rics on visible regions where we expect the scene to have been reasonably captured in the training sequence. Follow-ing this evaluation protocol, we capture a new dataset with 12 scenes, each with two camera sequences for training and evaluation.
We also propose Nerfbusters, a method aimed at im-proving geometry for everyday NeRF captures by improv-ing surface coherence, cleaning up floaters, and removing cloudy artifacts. Our method learns a local 3D geometric prior with a diffusion network trained on synthetic 3D data and uses this prior to encourage plausible geometry during
NeRF optimization. Compared to global 3D priors, local geometry is simpler, category-agnostic, and more repeat-able, making it suitable for arbitrary scenes and smaller-scale networks (a 28 Mb U-Net effectively models the dis-tribution of all plausible surface patches). Given this data-driven, local 3D prior, we use a novel unconditional Den-sity Score Distillation Sampling (DSDS) loss to regularize the NeRF. We find that this technique removes floaters and makes the scene geometry crisper. To the best of our knowl-edge, we are the first to demonstrate that a learned local 3D prior can improve NeRFs. Empirically, we demonstrate that
Nerfbusters achieves state-of-the-art performance for casual captures compared to other geometry regularizers.
We implement our evaluation procedure and Nerfbusters method in the open-source Nerfstudio repository [35]. The code and data can be found at https://ethanweber. me/nerfbusters.
Figure 2: Evaluation protocols. Current evaluation of NeRFs (e.g., MipNeRF 360) measures render quality at every 8th frame of the captured (training) trajectory, thus only testing the modelâ€™s ability to render views with small viewpoint changes. In contrast, we propose a new evaluation protocol, where two sequences are captured of the same scene: one for training and one for evalua-tion. Please see the supplementary material for plots showing the training and evaluation sequences for various NeRF datasets, in-cluding our proposed Nerfbuster Dataset. 2.