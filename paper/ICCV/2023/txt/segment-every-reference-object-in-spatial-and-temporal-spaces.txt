Abstract
The reference-based object segmentation tasks, namely referring image segmentation (RIS), referring video ob-ject segmentation (RVOS), and video object segmentation (VOS), aim to segment a specific object by utilizing either language or annotated masks as references. Despite sig-nificant progress in each respective field, current methods are task-specifically designed and developed in different di-rections, which hinders the activation of multi-task capa-bilities for these tasks.
In this work, we end the current fragmented situation and propose UniRef to unify the three reference-based object segmentation tasks with a single ar-chitecture. At the heart of our approach is the multiway-fusion for handling different task with respect to their spec-ified references. And a unified Transformer architecture is then adopted for performing instance-level segmentation.
With the unified designs, UniRef can be jointly trained on a broad range of benchmarks and can flexibly perform mul-tiple tasks at runtime by specifying the corresponding ref-erences. We evaluate the jointly trained network on various benchmarks. Extensive experimental results indicate that our proposed UniRef achieves state-of-the-art performance on RIS and RVOS, and performs competitively on VOS with a single network. 1.

Introduction
The reference-guided object segmentation aims at seg-menting the specified object with the given references (e.g.,
The language or first-frame mask annotation). three representative tasks include referring image seg-mentation (RIS) [94], referring video object segmentation (RVOS) [34] and semi-supervised video object segmen-tation (VOS) [61]. They are the fundamental tasks for vision understanding and have wide applications in im-age/video editing, video surveillance, etc. Over time, many advanced methods have ballooned in their respective fields and rapidly improves the state-of-the-art performance.
Despite witnessing the significant progress, these tasks are separately tackled with specialized designed models.
Figure 1: A single, jointly trained UniRef can perform three different reference-based tasks by specifying the cor-responding references. ‘L’ and ‘M’ represent language and mask reference, respectively.
In that regard, the individual methods need extra training time and produce different sets of model weights on each task. This would cause expensive computational cost and yield redundant parameters. More importantly, the inde-pendent designs prevent the synergy and facilitation of dif-ferent tasks. We argue that the current fragmented situa-tion is unnecessary as the three tasks have essentially the same definition in a high-level aspect: they all use the refer-ences (language or first-frame annotated mask) as guidance to perform the per-pixel segmentation of the target object.
This motivates us to build a unified model within the same parameters which can perform different tasks at runtime by specifying the corresponding references.
Towards the unification of reference-based object seg-mentation tasks, it poses great challenges in connecting the isolated landscapes as a whole: (i) The mainstream methods in different fields vary greatly. RIS and RVOS methods [91, 29, 86, 66] mostly focus on the deep cross-modal fusion of vision and language information. While the space-time memory network for pixel-level matching has long dominated the VOS domain [59, 16, 88, 15]. (ii)
Previous RVOS and VOS tasks are solved in two dif-ferent paradigms. The previous state-of-the-art RVOS methods [6, 78] take the whole video as input and gener-ate the prediction results for all frames in one single step, which termed as offline methods. In contrast, VOS meth-ods [59, 16] operate in an online fashion where they readout the historical features to propagate the target masks frame by frame. (iii) The image-level RIS methods cannot be simply extended to the video domain for RVOS. RIS only requires to segment the referred target in a single image.
For the video tasks, however, the objects may encounter oc-clusion, fast motion or disapperance-reappearance in many complex scenes, which requires the networks to leverage the spatio-temporal information to track the objects throughout the whole video. Hence, simply adopting the image-level methods for each frame independently cannot ensure the temporal consistency for target object in videos.
In this work, we conquer the challenges above and pro-pose a unified model, UniRef, for the reference-based ob-ject segmentation tasks. The key idea behind our approach is to formulate all three tasks as instance-level segmenta-tion problems, and the information of references can be in-jected into the network through an attention-based fusion process regardless of their modalities. As illustrated in Fig-ure 1, for different tasks, UniRef receives the current frame and utilizes the corresponding references to perform the fu-sion process, termed as multiway-fusion. Specifically, the language description and annotated mask in the first frame are leveraged as references for RIS and VOS tasks, respec-tively. And we emphasize that, for RVOS, both the language and mask references are used. This design not only tackles
RVOS in an online fashion, but also can utilize the histori-cal information for mask propagation to ensure the tempo-ral consistency for target object, and thus establishing a new paradigm for RVOS. Practically, we introduce a UniFusion module to fuse the visual features and the specified refer-ences. Afterwards, the visual features of current frame are fed into a unified Transformer architecture, where queries are employed for instance-level segmentation of the target object. Thanks to the unified architecture, our model can be jointly trained on the broad range of benchmarks to learn the general knowledge, and can flexibly perform multi-tasks at runtime by specifying the corresponding references.
To summarize, our contributions are as follows:
• We propose UniRef, a unified model to perform three reference-based object segmentation tasks (RIS, RVOS,
VOS) with the same model weights.
• We introduce a UniFusion module to inject the reference information into the network regardless of their modali-ties. And we establish a new online paradigm for RVOS by leveraging both language and mask as references.
• Extensive experiments demonstrate that our models achieve state-of-the-art performance for RIS and RVOS, and perform competitively for VOS. 2.