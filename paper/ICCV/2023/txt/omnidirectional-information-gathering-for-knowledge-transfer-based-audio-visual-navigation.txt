Abstract
Audio-visual navigation is an audio-targeted wayﬁnd-ing task where a robot agent is entailed to travel a never-before-seen 3D environment towards the sounding source.
In this article, we present ORAN, an omnidirectional audio-visual navigator based on cross-task navigation skill trans-In particular, ORAN sharpens its two basic abilities fer. for a such challenging task, namely wayﬁnding and audio-visual information gathering. First, ORAN is trained with a conﬁdence-aware cross-task policy distillation (CCPD) strategy. CCPD transfers the fundamental, point-to-point wayﬁnding skill that is well trained on the large-scale Point-Goal task to ORAN, so as to help ORAN to better master audio-visual navigation with far fewer training samples.
To improve the efﬁciency of knowledge transfer and ad-dress the domain gap, CCPD is made to be adaptive to the decision conﬁdence of the teacher policy. Second, ORAN is equipped with an omnidirectional information gathering (OIG) mechanism, i.e., gleaning visual-acoustic observa-tions from different directions before decision-making. As a result, ORAN yields more robust navigation behaviour. Tak-ing CCPD and OIG together, ORAN signiﬁcantly outper-forms previous competitors. After the model ensemble, we got 1st in Soundspaces Challenge 2022, improving SPL and
SR by 53% and 35% relatively. 1.

Introduction
Developing intelligent autonomous wayﬁnding agents that can robustly navigate in unexplored environments to reach target locations is one of the most classic and funda-mental tasks in robotics and is widely viewed as a critical building block of embodied AI. To simulate different real-world application scenarios of wayﬁnding agents, various navigation tasks are proposed, where the target goal is ap-pointed by, for example, GPS coordinate [42, 27, 26], se-mantic tag [57, 8, 9, 40], visual language instruction [23,
*Corresponding author: Wenguan Wang, Si Liu.
Figure 1: Top: Based on CCPD, our ORAN transfers wayﬁnding knowledge well-trained on PointGoal task to audio-visual navigation. Bottom: Based on OIG, ORAN collects visual and acoustic information from different di-rections for robust decision-making. 36, 4, 25, 3, 38, 30, 56, 37, 2, 55, 47, 49, 48, 15], image photo [10, 1, 34, 29]. Among these navigation tasks, in this article, we are particularly interested in audio-visual nav-igation (also named as AudioGoal) [12, 19], in which the navigation agent is entailed to ﬁnd sound-emitting objects in visually-and-acoustically rich 3D environments. Audio plays an irreplaceable role here – it reveals not only the
properties of the target object but also the layout of the un-explored areas. For example, when we hear a sound, we can know what makes the such sound and locate the sound source even before we observe it. Along this direction, existing AudioGoal solutions are typically reinforcement learning (RL) algorithms for direct audio-visual perception to low-level navigation action mapping [12, 11, 53]. Some methods further predict the next intermediate goal [13] on a top-down topology map to improve the agent’s long-term stability. Methods like [53] instead consider the sound at-tacks to promote navigation robustness in complicated au-dio environments.
In this article, we develop ORAN, an omnidirectional audio-visual navigation agent based on cross-task wayﬁnd-ing skill transfer. As shown in Figure 1, ORAN advances state-of-the-art technologies for AudioGoal in two aspects, namely wayﬁnding and visual-audio information gather-ing. Our technique innovations are born from two crucial insights. First, besides comprehending the received vi-sual and audio signals, a successful AudioGoal agent needs to master some very basic wayﬁnding skills, such as pre-cisely moving towards a short-term target, safely travelling without collision, and entering/leaving a room through the door. It is clear that these basic wayﬁnding skills are shared among different wayﬁnding tasks. Hence it is reasonable to assume that an AudioGoal agent can beneﬁt from the knowledge of a high-performance navigator that is already well-trained on other navigation tasks, especially consider-ing that the training samples of AudioGoal are relatively limited, while AudioGoal itself is a very challenging task.
Second, it is natural for our human to turn around upon hearing a sound behind us, or turn our head to ﬁnd out what we have heard [5]. In sharp contrast, existing AudioGoal agents only make use of visual-audio information perceived forward during navigation decision-making. It is clear that there exists a huge gap between such a simple, forward-only perception regime between the omnidirectional decision-making mode (particularly for those top-down map-based
AudioGoal agents [13, 24]).
Our ORAN is elaborately designed to be fully aware of the aforementioned issues, so as to sharpen its wayﬁnd-ing and visual-audio information-gathering abilities. First,
ORAN is trained with a Conﬁdence-aware Cross-task Pol-icy Distillation (CCPD) strategy. CCPD allows ORAN to transfer the navigation knowledge learned on PointGoal to
AudioGoal. Recently, PointGoal [41, 45] has seen signif-icant advance – with millions of frames of experience and assistance of a GPS+Compass sensor, PointGoal agents can achieve nearly perfect navigation performance, given the coordinates of starting and target locations [50, 35]. During the training of AudioGoal, we consider the behaviours of a well-trained PointGoal agent, queried with the same starting and target waypoint locations, as informative demonstra-tions for ORAN. The reuse of the navigation knowledge re-lieves ORAN’s burden of learning to both masters basic nav-igation skills and how to understand and plan with visual-audio perception. Moreover, to further better overcome the domain gap between the two tasks and improve the efﬁ-ciency of such cross-task navigation knowledge transfer, we render larger weights to those more conﬁdent steps of the PointGoal policy during policy distillation. Second,
ORAN is empowered with an omnidirectional information gathering (OIG) ability. OIG enables ORAN to make use of acoustic-visual information collected from different direc-tions, instead of the only direction it is facing, to support its omnidirectional navigation decision-making.
We experimentally demonstrate that combining CCPD and OIG together makes our ORAN a powerful Audio-Goal navigator, which sets state-of-the-art performance on
Soundspaces Challenge dataset [12], e.g., >10% absolute lifting in SPL on unhead sets. Moreover, after model as-sembling, ORAN yields further 12% absolute promotion in
SPL. The fused model won 1st on the Soundspaces chal-lenge 2022 [12] and improves SPL by 53% and SR by 35% relatively. 2.