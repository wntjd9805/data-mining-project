Abstract
Multi-label image classiﬁcation is a prediction task that aims to identify more than one label from a given image. This paper considers the semantic consistency of the latent space between the visual patch and linguistic label domains and introduces the conditional transport (CT) theory to bridge the acknowledged gap. While recent cross-modal attention-based studies have attempted to align such two representa-tions and achieved impressive performance, they required carefully-designed alignment modules and extra complex operations in the attention computation. We ﬁnd that by formulating the multi-label classiﬁcation as a CT problem, we can exploit the interactions between the image and label efﬁciently by minimizing the bidirectional CT cost. Specif-ically, after feeding the images and textual labels into the modality-speciﬁc encoders, we view each image as a mix-ture of patch embeddings and a mixture of label embeddings, which capture the local region features and the class proto-types, respectively. CT is then employed to learn and align those two semantic sets by deﬁning the forward and back-ward navigators. Importantly, the deﬁned navigators in CT distance model the similarities between patches and labels, which provides an interpretable tool to visualize the learned prototypes. Extensive experiments on three public image benchmarks show that the proposed model consistently out-performs the previous methods. 1.

Introduction
Multi-label image classiﬁcation is a fundamental yet chal-lenging task in computer vision, where a set of objects needs
*Authors contributed equally. l
M l 6 6 l 5
… cat l 2 g dog boat b l l 3 7
Text Domain  i
C ,  l1 e3 l 1 frisbee a aero e 1 1
C ,  l3 e1 e 3 e 4 e
N e 7
… eee e 2 e 6
Image Domain 
C ,  l3 e6
Embedding Space label  embedding patch embedding
Figure 1: Motivation of the proposed PatchCT. We represent each image as a set of patch embeddings and a set of label embeddings, and then the conditional transport is employed as a semantic regu-larization to align such two domains. to be predicted within one image. It has practical appli-cations in wide ﬁelds such as image retrieval [45], scene understanding [37], recommendation [48, 24], and biology analysis [21, 1]. In addition to identifying the regions of in-terest, multi-label image classiﬁcation also requires special attention on 1) semantic information of labels, e.g., label correlations, and 2) interactions between the visual image and textual label domains. Thus, it is often more complex and challenging compared to single-label case.
To address the aforementioned challenges, several signiﬁ-cant attempts have been developed from various directions.
An increasing research attention is to learn semantic label representation. The core idea is intuitive: labels should be more correlated if they co-occurrence often. For instance, trafﬁc lights usually co-appeared with crosswalk, and chair has a high conﬁdence score to appear if table exists in the image. Existing methods adopt pair-wise ranking loss, co-variance matrices, recurrent structures, contrastive learning, graph neural networks (GCNs), and pre-trained language model to this end [55, 2, 43, 50, 25, 14, 1, 9, 8, 51, 29, 62].
Those methods either regular the learning with a pre-deﬁned label graph which is often obtained from the training set or the pre-trained word embedding, e.g. Glove [33], or explor-ing label dependency implicitly (e.g., contrastive learning and BERT embeddings [15]). Meanwhile, some works aim to solve the second issue by adopting cross-modal attention-based framework [23, 29, 26, 56, 11], where a vision trans-former (ViT [17]) is often employed to obtain local patch fea-tures and the cross-attention between the labels and patches is then applied to align such two modalities. The alignment module acts as the core role in those models and needs to be designed carefully. Moreover, conventional attention mecha-nisms are guided by task-speciﬁc losses, without explicitly training signals to encourage alignment. The learned atten-tion weights are therefore often dense and uninterpretable, leading to less effective relational prediction [5].
We address whether there is a more simple and princi-pled approach to efﬁcient alignments of vision-text domains.
To explore this, in this paper, we introduce the conditional transport (CT) theory [59, 39] and reformulate the multi-label classiﬁcation task as a CT problem, where an image is represented as two discrete distributions over different supports, e.g., the visual patch domain and the textual label domain (as shown in Fig. 1). Our idea is intuitive: those two distributions (or sets) are different modality representations of the same image, and therefore they share semantic consis-tency. As a result, the learning of multi-label classiﬁcation can be viewed as the process of aligning the textual label set to be as close to the visual set as possible. Accordingly, it is indeed key to ﬁnd out how to measure the distance be-tween two empirical distributions with different supports.
Fortunately, conditional transport is well-studied in recent researches [59, 38, 40, 42] and provides a powerful tool for measuring the distance from one discrete distribution to an-other given the cost matrix. It is natural for us to develop a new framework for multi-label classiﬁcation based on the minimization of CT distance.
Speciﬁcally, the visual patch embeddings and textual la-bel embeddings are ﬁrst extracted by feeding the image and label descriptions into the corresponding encoders. We spec-ify the image encoder as a ViT to capture spatial patch depen-dencies and the text encoder as a BERT to explore the label semantics. Besides, inspired by the current prompt learning success [31, 60], we here design a simple and efﬁcient tem-plate to reduce the domain shift between the language model and the multi-label classiﬁcation and quickly distill the pre-trained knowledge to the downstream tasks simultaneously.
After that, we collect patch features as a discrete probability distribution where each patch has a label-aware probability value that reﬂects the important score for multi-label predic-tion, e.g. object patches have high probability values while background patches with lower attention. Similarly, we con-struct the textual label sets as a discrete distribution, whose probability values are obtained by normalizing the ground-truth label of the image. Given such two sets, the cost matrix in CT is then deﬁned according to the similarity between the patches and labels, e.g. the cosine distance of the patch and label embeddings. The CT divergence is deﬁned with a bidi-rectional set-to-set transport, where a forward CT measures the transport cost from the patch set to the label set, and a backward CT that reverses the transport direction. There-fore, by minimizing the bidirectional CT cost, we explicitly minimize the embedding distance between the domains, i.e., optimizing towards better patch-label alignments. Moreover, the learned transport plan models the semantic relations be-tween patches and labels, which provides an interpretable tool to visualize the label concepts.
Our main contributions can be summarized as follows:
• We propose a novel vision-text alignment framework based on conditional transport theory, where the inter-actions between the patches and labels are explored by minimizing the bidirectional CT distance between those two modalities to produce high semantic consistency for multi-label classiﬁcation.
• We design sparse and layer-wise CT formulations to reduce the computational cost and enhance the deep interactions across modalities, contributing to robust and accurate alignments between patches and labels.
• Extensive experiments on three widely used visual benchmarks demonstrate the effectiveness of the pro-posed model by establishing consistent improvements on all data sets. 2.