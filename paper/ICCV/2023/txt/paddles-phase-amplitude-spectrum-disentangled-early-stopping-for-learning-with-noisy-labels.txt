Abstract
Convolutional Neural Networks (CNNs) are powerful in learning patterns of different vision tasks, but they are sen-sitive to label noise and may overfit to noisy labels during training. The early stopping strategy averts updating CNNs during the early training phase and is widely employed in the presence of noisy labels. Motivated by biological find-ings that the amplitude spectrum (AS) and phase spectrum (PS) in the frequency domain play different roles in the an-imal’s vision system, we observe that PS, which captures more semantic information, can increase the robustness of
CNNs to label noise, more so than AS can. We thus pro-pose early stops at different times for AS and PS by disen-tangling the features of some layer(s) into AS and PS us-ing Discrete Fourier Transform (DFT) during training. Our proposed Phase-AmplituDe DisentangLed Early Stopping (PADDLES) method is shown to be effective on both syn-thetic and real-world label-noise datasets. PADDLES out-performs other early stopping methods and obtains state-of-the-art performance. 1.

Introduction
Learning from noisy labels (LNL) [1, 34] is an active area of research within the deep learning community [45, 13, 38, 15, 67, 64, 69, 21]. Noisy labels are common in real-world applications [61, 56, 66, 49], and trustworthy AI should be robust to mislabelling.
It has been argued that CNNs learn first the actual pattern before overfitting the noise [3], which inspired many works in LNL [15, 58, 27, 28, 63, 32, 31]. A training strategy is early stopping (ES), which stops the gradient-based opti-mization at a specific early training step. Due to its effec-tiveness, ES is widely applied in current LNL models and has achieved promising performance [53, 27, 39, 4, 31].
*Project lead. †Co-first authors. ‡Corresponding Author: Tongliang
Liu (tongliang.liu@sydney.edu.au). Our codes are available at https://github.com/CoderHHX/PADDLES.
The frequency and spatial domains are alternative codes for depicting signal data such as images and text [42, 50].
Different frequency components contain different informa-tion [7]. The amplitude spectrum (AS) quantifies how much of each sinusoidal component is present, while the phase spectrum (PS) reveals the location of each sinusoidal com-ponent within an image. Biological justification and psy-chological patterns testing [6, 47, 14] demonstrate that the response of cells in the primary visual cortex (V1) is closely related to the local AS for specific image patterns (fre-quency and orientation). That is, the AS component usu-ally represents the intensity of the patterns in the image. On the other hand, previous qualitative and quantitative stud-ies [7, 14] indicate that the PS is the key to locating salient object areas and holds visible structured information for vi-sion recognition [41, 12, 26], thus contains more semantic information than the AS.
As a robust vision system, human vision focuses on se-mantic parts during object recognition, and relies more on the image components related to the PS than the AS [41, 14, 26, 8]. This system builds a strong connection between se-mantic feature space and label space, helping humans ‘un-derstand’ the actual correlation between objects and their corresponding identifiers (labels). The human visual system is very robust to label noise. However, CNNs profit from human unperceivable high-frequency information in im-ages [22, 57]. Without adequate regulations, CNNs model the correlation of objects and their labels mainly based on the connection between AS and the given annotations. Such over-dependence is demonstrated as the leading cause of their sensitivity to image perturbation and overconfidence in out-of-distribution (OOD) detection [8, 20]. We argue that
CNNs’ over-dependence of connection between the less se-mantic AS and labels may spoil their recognition robust-ness, resulting in their vulnerability to label noise.
To investigate the impact of label noise on deep mod-els trained with different image components, we used DFT to transform raw images from CIFAR-10 [25] into AS and
PS. Then, three ResNet-18 models [17] were trained using standard cross-entropy loss with raw images, AS, and PS
(a) Training loss with clean labels (b) Training loss with noisy labels (c) Testing after clean-label training (d) Testing after noisy-label training
Figure 1: Results of training ResNet-18 models on CIFAR-10 using original images, AS, and PS components (“Train IM”,
“Train AS”, and “Train PS” in the Figure) on cleanly and noisily labeled subsets. The curves are averaged across five random runs. The dotted vertical lines indicate the best performance steps of different image components. The converging speed of the deep model trained on AS and PS differs, especially on noisy labeled examples. Approaching the end of the training, when the wrong labels begin to be memorized, the model accelerates fitting to AS, resulting in an intersection on the training curves of AS and PS, shown in Figure 1b. Hence, PS can help the deep model become more resistant to label noises than AS. as inputs. Figures 1a and 1c illustrate the results of these models trained on clean labeled samples. Figure 1a shows the training losses, while Figure 1c displays the testing ac-curacy. The models were also trained with 50% symmetric label noise [55, 15], and the results are shown in Figures 1b and 1d, where Figure 1b shows the training loss, and
Figure 1d presents the classification accuracy on the clean testing set. Under noisy labels, the convergence speeds of
CNNs on AS and PS differ more than with clean labels, as shown in Figures 1a and 1b. When CNNs begin to overfit the noisy labels, they converge much faster on AS than on
PS (Figure 1b). Meanwhile, the convergence speed on PS is slower than on AS and raw images, suggesting that PS can help make CNNs more robust to mislabeled data compared to AS or raw inputs. Note that the model trained with only
AS or PS performs worse than the one trained with the raw images (Figure 1d). This is not surprising as either AS or PS could miss some information from the original image data.
Therefore, an intuitive solution to improve the robustness of the CNNs to the noisy labels is choosing different early stop points for AS and PS, during the training of the CNNs. In this way, we can suppress the over-dependence of CNNs on
AS while shifting to utilize more PS components.
Controlling the optimization of the model on the raw AS or PS components is difficult since current CNNs are trained based on gradient updates via backward propagation, and the raw images are fixed and do not require gradient com-putations during optimization. To tackle this challenge, we propose to use deep features to represent the ‘image’, as each ‘pixel’ of the feature map corresponds to an original image patch. Moreover, a similar study to that shown in
Figure 1 for the deep features of ResNet blocks supports our solution. We observe that different frequency compo-nents from the deep features hold a similar property to those from the raw image (Please refer to the supplemental mate-rials for this study). Specifically, we propose to disentangle the deep image features into AS and PS at different training steps by Discrete Fourier Transform (DFT). We first detach the AS component from the gradient computational graph to stop its involvement in the model update, which can alle-viate the potential negative effects of AS in the later training stage. With AS being detached, we continue train the deep model with PS components. The optimization on the PS components will be stopped after a few training epochs. No-tice that the detached components will regenerate the deep features in the spatial domain through inverse DFT (iDFT).
This is efficient as there is no modification to the original architecture. Moreover, complete information is used for training. We call the proposed method as Phase-AmplituDe
DisentangLed Early Stopping (PADDLES). To the best of our knowledge, PADDLES is the first method to consider features learned with noisy labels in the frequency domain and thus is orthogonal to existing methods that mainly focus on the spatial domain. Our contributions are as follows:
• We study learning with noisy labels (LNL) from the frequency domain and find that PS can help CNNs be-come more resistant to label noise than AS.
• We propose to early stop training at different stages for
AS and PS in LNL. Our method can benefit from the robustness of the PS without losing information on AS during the training of CNN. Extensive experiments on several benchmark datasets validate the effectiveness of the proposed method. 2.