Abstract
Vision in adverse weather conditions, whether it be snow, rain, or fog is challenging.
In these scenarios, scatter-ing and attenuation severly degrades image quality. Han-dling such inclement weather conditions, however, is essen-tial to operate autonomous vehicles, drones and robotic ap-plications where human performance is impeded the most.
A large body of work explores removing weather-induced image degradations with dehazing methods. Most meth-ods rely on single images as input and struggle to gener-alize from synthetic fully-supervised training approaches or to generate high fidelity results from unpaired real-world datasets. With data as bottleneck and most of today’s training data relying on good weather conditions with in-clement weather as outlier, we rely on an inverse render-ing approach to reconstruct the scene content. We intro-duce ScatterNeRF, a neural rendering method which ade-quately renders foggy scenes and decomposes the fog-free background from the participating media – exploiting the multiple views from a short automotive sequence without the need for a large training data corpus. Instead, the ren-dering approach is optimized on the multi-view scene itself, which can be typically captured by an autonomous vehicle, robot or drone during operation. Specifically, we propose a disentangled representation for the scattering volume and the scene objects, and learn the scene reconstruction with physics-inspired losses. We validate our method by captur-ing multi-view In-the-Wild data and controlled captures in a large-scale fog chamber. Our code and datasets are avail-able at https://light.princeton.edu/scatternerf. 1.

Introduction
Imaging and scene understanding in the presence of scat-tering media, such as fog, smog, light rain and snow, is an open challenge for computer vision and photography. As rare out-of-distribution events that occur based on geogra-phy and region [8], these weather phenomena can drasti-cally reduce the image quality of the captured intensity im-ages, reducing local contrast, color reproduction, and image resolution [8]. A large body of existing work has investi-Figure 1: ScatterNeRF produces accurate renderings for scenes with volumetric scattering (b). By learning a dis-entangled representation of participating media and clear scene, the proposed method is able to recover dehazed scene content (c) with accurate depth (d). gated methods for dehazing [57, 5, 49, 29, 73, 77] with the most successful methods employing learned feed-forward models [57, 5, 49, 29, 73]. Some methods [49, 5, 35] use synthetic data and full supervision, but struggle to over-come the domain gap between simulation and real world.
Acquiring paired data in real world conditions is challeng-ing and existing methods either learn natural image priors from large unpaired datasets [74, 73], or they rely on cross-modal semi-supervision to learn to separate atmospheric ef-fects from clear RGB intensity [57]. Unfortunately, as the semi-supervised training cues are weak compared to paired supervised data, these methods often fail to completely sep-arate atmospheric scatter from clear image content, espe-cially at long distances. The problem of predicting clear images in the presence of haze is an open challenge, and notably harsh weather also results in severely impaired hu-man vision – a major driver behind fatal automotive acci-dents [4].
As the distribution of natural scenes with participating media is long-tailed in typical training datasets [14, 60, 19, 18, 8], this also makes training and evaluation of computer vision tasks that operate on RGB streams in bad weather challenging. For supervised approaches to scene under-standing tasks, these ”edge” scenarios often directly re-1
sult in failure cases, including detection [8], depth estima-tion [20], and segmentation [52]. To tackle weather-based dataset bias, existing methods have proposed augmentation approaches that either simulate atmospheric weather effects on clear images [52, 65] or they employ fully synthetic sim-ulation to generate physically-based adverse weather sce-narios [13, 23, 52]. Unfortunately, both directions cannot compete with supervised training data, either due to the do-main gap between real and synthetic data, or, as a result of an approximate physical forward model [65].
As such, the capability of both physically accurate model-ing and separating scattering in participating media is es-sential for imaging and scene understanding tasks.
In this work, we depart from both feed-forward dehaz-ing methods and fully synthetic training data, and we ad-dress this challenge as an inverse rendering problem. In-stead of predicting clean images from RGB frames, we propose to learn a neural scene representation that ex-plains foggy images with a physically accurate forward rendering process. Once this representation is fitted to a scene, this allows us to render novel views with real-world physics-based scattering, disentangle appearance and ge-ometry without scattering (i.e., reconstruct the dehazed scene), and estimate physical scattering parameters accu-rately. To be able to optimize a scene representation effi-ciently, we build on the large body of neural radiance field methods [45, 75, 69, 70, 72, 47, 32, 9, 51, 58, 10, 76, 7, 66].
While existing NeRF methods assume peaky ray termina-tion distributions and free-space propagation, we propose a forward model that can accurately model participating me-dia. As an inductive bias, the scene representation, by de-sign, separates learning the clear 3D scene and the partic-ipating media. We validate that the proposed method ac-curately models foggy scenes in real-world and controlled scenes, and we demonstrate that the disentangled scene in-tensity and depth outperform existing dehazing and depth estimation methods in diverse driving scenes.
Specifically, we make the following contributions:
• We propose a method to learn a disentangled repre-sentation of the participating media by introducing the
Koschmieder scattering model into the volume render-ing process.
• Our approach adds a single MLP used to model the scattering media proprieties and does not require any additional sampling or other procedures, making it a lightweight framework in terms of both computation and memory consumption. 2.