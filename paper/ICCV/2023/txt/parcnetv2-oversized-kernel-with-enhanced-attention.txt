Abstract
Transformers have shown great potential in various computer vision tasks. By borrowing design concepts from transformers, many studies revolutionized CNNs and showed remarkable results. This paper falls in this line of studies. Specifically, we propose a new convolutional neu-ral network, ParCNetV2, that extends the research line of
ParCNetV1 by bridging the gap between CNN and ViT. It introduces two key designs: 1) Oversized Convolution (OC) with twice the size of the input, and 2) Bifurcate Gate Unit (BGU) to ensure that the model is input adaptive. Fusing
OC and BGU in a unified CNN, ParCNetV2 is capable of flexibly extracting global features like ViT, while maintain-ing lower latency and better accuracy. Extensive experi-ments demonstrate the superiority of our method over other convolutional neural networks and hybrid models that com-bine CNNs and transformers. The code are publicly avail-able at https://github.com/XuRuihan/ParCNetV2. 1.

Introduction
Transformers have shown great potential in computer vi-sion recently. ViT [14] and its variants [52, 62, 55, 37] have been adopted to various vision tasks such as object detection [3, 15], semantic segmentation [67], and multi-modal tasks such as visual question answering [29] and text-to-image synthesis [42]. Despite the great performance of vision transformers, they do not win CNNs in all as-pects. For example, the computational complexity of self-attention modules, one of the critical designs in transform-ers, is quadratic (O(N 2C)) to the resolution of inputs [53].
This property restricts its adoption in real applications such as defect inspection, which finds small defects in high-resolution images [65]. Moreover, transformers are ar-guably more data-hungry than CNNs [14, 52, 21, 17], mak-ing them difficult to deploy to long-tail applications with-out large-scale data. Lastly, CNNs have been intensively
*Work was done when R. Xu was an intern at Intellifusion. ‡ denotes corresponding author.
Figure 1. Comparison between ParCNetV2 with the prevailing transformer (Swin), CNN (ConvNeXt), and large kernel CNNs (RepLKNet & SLaK) when trained from scratch on ImageNet-1K.
Left: performance curve of model size vs. top-1 accuracy. Right: top-1 accuracy. IG performance curve of inference latency vs. represents using the implicit gemm acceleration algorithm. studied in the past several decades [30]. There are lots of off-the-shelf dedicated features already developed in exist-ing deployment hardware (CPU, GPU, FPGA, ASIC, etc.).
Some acceleration and deployment techniques are designed mainly around convolution operations, such as operator fu-sion [45] and multi-level tiling [66, 6].
Thus pushing the envelope of CNNs is still important and valuable. Recent works have improved CNNs from multi-ple perspectives. A straightforward approach is to take the benefits from both CNNs and transformers by mixing their building blocks [18, 49, 39, 7, 34]. While bringing together merits from the two parties, those approaches still keep the
ViT blocks and has the quadratic complexity problem. An-other line of research is to design purely convolutional ar-chitectures. For example, with larger convolution kernels,
ConvNeXt [38], RepLKNet [12], and ParCNetV1 [64] suc-cessfully improved the performance of CNNs by encoding broader spatial contexts.
Specifically, ParCNetV1 introduced position-aware circular convolutions (ParC) to CNNs. It uses depth-wise circular 1D convolutions of input feature map size (C ×
H × 1 and C × 1 × W ) to achieve global receptive fields. To avoid spatial over-smoothing caused by global kernels, Par-Figure 2. Comparison between circular convolution and over-sized convolution. We only show horizontal convolution for il-lustration purposes. a) Circular convolution in ParCNetV1 in-evitably distorts context information at the boundary of images. b) Oversized convolution resolves the distortion while maintain-ing the global receptive field over the whole image.
CNetV1 augmented the feature input with absolute position encoding to ensure the feature output is still location sensi-tive. It also brought attention mechanisms into the frame-work by adopting squeeze-and-excitation block [27]. These modifications lead to the superior performance of ParC-NetV1, especially on mobile devices.
Despite improved model efficiency and accuracy, ParC-NetV1 still suffers from some design drawbacks. Firstly, as mentioned in [64] and shown in Fig 2, the circular padding introduces spatial distortion by performing convo-lutions crossing image borders. Secondly, the attention de-sign is relatively weak compared with transformers which may limit the framework performance. Thirdly, it is not feasible to apply global convolution to all blocks in CNNs, especially those shallow blocks due to expensive computa-tional costs and over-smoothing effects.
To address these issues, we propose a pure convolutional neural network architecture called ParCNetV2. It is com-posed of three essential improvements over ParCNetV1.
First, we push the kernel size to the extreme by doubling the circular convolution kernel and removing the absolute positional encoding. As shown in Fig. 2, through large size (equal to the size of the input) padding, the convolution op-eration avoids feature distortion around image borders. By using constant paddings, the oversized kernel implicitly en-codes spatial locations when it convolves with the feature maps [28]. It enables us to discard the positional encoding module without hurting network performance. We explain why 2× is the extreme in Sec.3.1.
Second, the original ParC block uses a limited attention mechanism inserted at the end of the channel mixing phase.
We propose a more flexible bifurcate gate unit (BGU) at both the token mixing phase (spatial BGU) and channel mixing phase (channel BGU) in our newly designed block.
Compared to the squeeze-and-excitation block, the BGU is stronger while more compact and general to combine with various structures, leading to spatial attention and channel attention. The enhanced attention mechanism also simpli-fies our ParC V2 block, as both phases adopt the consistent
BGU structure.
Last, in contrast to ParCNetV1 which applies large ker-nel convolutions only on later-stage CNN blocks, we unify the block design by mixing large kernel convolutions with local depth-wise convolutions in all the blocks. Both types of convolutions are operated on the input feature map chan-nels. This progressive design combines local features and global features in one convolution step, unlike many other works that stack the two sequentially [18, 60, 64] or as two separate branches [7, 39, 9]. To this end, the resulting re-designed ParC V2 structure is capable of performing local convolutions, global convolutions, token channel mixing, and BGU-based attention all in one block.
To summarize, the main contributions of this paper are as follows:
• We propose oversized convolutions for the effective modeling of long-range feature interactions in CNNs.
Compared to ParCNetV1, it enables homogeneous convolution across all spatial locations, while removes the need for extra position encoding.
• We propose two bifurcate gate units (spatial BGU and channel BGU), which are compact and powerful atten-tion modules. They boost the performance of ParC-NetV2 and could be easily integrated into other net-work structures.
• We bring oversized convolution to shallow layers of
CNNs and unify the local-global convolution design across blocks.
Extensive experiments are conducted to demonstrate that
ParCNetV2 outperforms all other CNNs given a similar amount of parameters and computation budgets as shown in Fig. 1. It also beats state-of-the-art ViTs and CNN-ViT hybrids, which indicates that convolution networks are as strong as transformers in extracting features. 2.