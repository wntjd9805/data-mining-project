Abstract
A number of computer vision deep regression ap-proaches report improved results when adding a classifica-tion loss to the regression loss. Here, we explore why this is useful in practice and when it is beneficial. To do so, we start from precisely controlled dataset variations and data samplings and find that the effect of adding a classification loss is the most pronounced for regression with imbalanced data. We explain these empirical findings by formalizing the relation between the balanced and imbalanced regression losses. Finally, we show that our findings hold on two real imbalanced image datasets for depth estimation (NYUD2-DIR), and age estimation (IMDB-WIKI-DIR), and on the problem of imbalanced video progress prediction (Break-fast). Our main takeaway is: for a regression task, if the data sampling is imbalanced, then add a classification loss. 1.

Introduction
Regression models predict continuous outputs. In con-trast, classification models make discrete, binned, predic-tions. For a continuous task, regression targets are a super-set of the classification labels: they are more precise, taking values in-between the discrete classification bins. For re-gression, the error is only bounded by the precision of the measurements, for classification this also depends on the bin sizes: e.g. an age estimation classifier that can pre-dict only young/old classes, cannot discriminate between middle-aged people. Additionally, when training a regres-sion model, losses are proportional to the error magnitude, while for classification all errors receive an equal penalty: predicting bin 10 instead of 20, is just as incorrect as pre-dicting bin 10 instead of bin 100. So classification cannot add anything new to regression; or can it?
Surprisingly, adding a classification loss to the regres-sion loss [30, 44, 46, 51], or even replacing the regression loss with classification [10, 11, 38] is extensively used in practice when training deep models for predicting continu-Figure 1. To probe “Why does classification help regression?” we design a fully controlled dataset including the following scenar-ios: Clean data – 1D non-linear functions defined by the sum of two sine waves with different frequencies and amplitudes; Noisy data – uniform noise added to the outputs; Out of distribution – sampling different regions of the input space during training and during testing. (We show a single function here. The gray shading groups the function targets into 4 classes, as an example.) ous outputs. The classification is typically defined by bin-ning the regression targets into a fixed number of classes.
This is shown to be beneficial for tasks such as: depth esti-mation [11], horizon line detection [44], object orientation estimation [30, 51], age estimations [34]. And the reported motivation for discretizing the regression loss is that: it im-proves performance [44, 46], or that it helps in dealing with noisy data [44], or that it helps overcome the overly-smooth regression predictions [30, 40], or that it helps better regu-larize the model [22, 44]. However, none of these assump-tions has been thoroughly investigated.
In this work, we aim to explore in the context of deep learning: Why does classification help regression? Intu-itively, the regression targets contain more information than the classification labels. And adding a classification loss does not contribute any novel information. What is it really that a classification loss can add to a standard MSE (mean squared error) regression loss? And why does it seem ben-eficial in practice?
To take a step towards understanding why classification helps regression, we start the analysis in a fully controlled setup, using a set of 1D synthetic functions. We consider several prior hypothesis of when classification can help re-gression: noisy data, out-of-distribution data, and the nor-mal clean data case, as in Fig. 1. Additionally, we vary the sampling of the data from uniform to highly imbalanced as in Fig. 2. We empirically find out in which of these cases adding a classification loss improves the regression quality on the test set. Moreover, we explain these empirical obser-vations by formulating them into a probabilistic analysis.
We urge the reader to note that our goal is not propos-ing a novel regression loss, nor do we aim to improve re-sults with “superior performance” over state-of-the-art re-gression models. But rather, we aim to investigate a com-mon computer vision practice: i.e. adding a classification loss to the regression loss, and we analyze for what kind of dataset properties and dataset samplings this practice is useful. Finally, we show experimentally that our find-ings hold in practice on two imbalanced real-world com-puter vision datasets: NYUD2-DIR (depth estimation) and
IMDB-WIKI-DIR (age estimation) [47], and on the Break-fast dataset [23] when predicting video progression. 2. Why does classification help regression?
For an input dataset, D, containing N samples of the form (x, y)∈D, we analyze what happens when we train a deep network with parameters ω to predict a target y∗=f (x, ω) for a sample x, by minimizing NLL (negative log-likelihood) or equivalently, minimizing the MSE (mean squared error) regression loss:
ω∗ = arg min
ω
= arg min
ω (cid:88)
L(y, x, ω) (x,y)∈D
 (cid:88)


− log p(y|x, ω)

≡ arg min
λ
ω (x,y)∈D 1
N (cid:88) (x,y)∈D (y − y∗)2 (1) (2) (3) where ω∗ are the optimal parameters, and we can reinterpret the imbalanced likelihood as the mean of a Gaussian distri-bution with σ noise: p(y|x, ω) = N (y; y∗, σ2I), in which case minimizing the NLL is equivalent to minimizing the
MSE loss [3], and λ is a function of the noise σ.
We contrast Eq. (3) to the case when we discretize the targets y into a set of C classes and use a classification loss next to the regression loss:
L(y, x, ω) =λ (y − y∗)2 − log p(y∗ c |x, ω), (4) where y∗ k, k∈{1, .., C} denotes the model predictions binned into classes, and specifically y∗ c is the prediction at the true class indexed by c, for the sample x. For the clas-sification term, we make the standard softmax distribution assumption.
Figure 2. Data sampling. On the columns we increase the data imbalance from uniform (balanced) to severely imbalanced. On the first row we show the function f (x) where darker datapoint colors visualize higher density. The gray shading on the first row groups the targets into 4 classes. On the second row we show the log-counts per function value, sampled for the training data. We sample the test data uniformly. 2.1. Controlled 1D analysis
To probe the question ”Why does classification help re-gression?” we first want to know in which cases does classi-fication help regression. We measure test-time MSE scores for each case in Fig. 1, and compare training with a re-gression loss as in Eq. (3), with training using an extra classification loss as in Eq. (4). We randomly sample 10 functions of the form: f (x)=a sin(cx) + b sin(dx), where f (x)∈[−1.5, 1.5] and x∈[−1, 1]. For every function we vary the dataset scenario as in Fig. 1, and we also vary the sampling of the data from uniform to severely imbalanced sampling, as in Fig. 2. Each dataset sampling is repeatedly performed with 5 different random seeds, where we always sample ≈30, 000 samples in total, and then randomly pick 1/3 for training, for validation, and for testing, respectively.
In the out-of-distribution case, the training set misses cer-tain function regions that are present in the validation/test set, and vice-versa; and there is an overlap of 1/4 between the function regions in the training set and the regions in the validation/test set. For the imbalanced sampling, we aim to sample a range of the targets y more frequently than other ranges. For this, we randomly select a location along the y-axis in each repetition: this defines the center of the peak in Fig. 2, second row. And depending on the sampling sce-nario, we use a fixed variance ratio around the peak (0.3, 0.1 and 0.03 for mild, moderate and severe sampling) to define the region from which we draw the frequent samples. We sample 75% of the samples from the peak region, and the rest uniformly from the other function areas.
We train a simple MLP (multi layer perceptron) with 3 linear layers ([1×6], [6×16], [16×1]) and ReLU non-2.2. Anchoring 1D experimental observations
In Section 2.1 we observe that classification has a more pronounced effect when the sampling of the data is im-balanced. Therefore, from here on we focus the anal-ysis on imbalanced data sampling. We start from the derivations of Ren et al. [31] who define the relation be-tween the NLL (negative log-likelihood) of imbalanced samples − log (cid:101)p(y|x, ω) and the NLL of the balanced sam-ples − log p(y|x, ω):
− log (cid:101)p(y|x, ω) = − log (cid:82) p(y|x, ω)(cid:101)p(y) y′ p(y′|x, ω)(cid:101)p(y′)dy′ (5) where (cid:101)p(y) denotes the prior over imbalanced targets.
Eq. (5) holds under the assumption that the data function remains unchanged, (cid:101)p(x|y)=p(x|y), which is the case for the clean and noisy data scenarios above. We decompose the log and rewrite the relation between the NLL of the bal-anced and the NLL of the imbalanced data:
− log (cid:101)p(y|x, ω) + Lextra(y, x, ω) = − log p(y|x, ω),
Lextra contains all the information about the imbalanced re-gression targets: (6)
Lextra = log (cid:101)p(y)− log
= log (cid:101)p(y)− log (cid:90) y′ (cid:90) y′ p(y′|x, ω)(cid:101)p(y′)dy′,
N (y′; y∗, σ2I)(cid:101)p(y′)dy′, (7) (8) where again y∗ are the predicted targets.
To derive the link between optimizing a model on imbal-anced data and using both a regression MSE loss and a clas-sification loss, we assume the imbalanced regression targets y can be discretized into a set of classes, k∈{1, .., C} such that (cid:80)C k=1 p(yk)=1. By going from continuous regression targets to discrete classes, we change the form of the log-likelihood from Gaussian to softmax:
Lextra ≈ log (cid:101)p(yc) − log
C (cid:88) k=1 p(y∗ k|x, ω)(cid:101)p(yk), (9) where we denote the true class label by yc. Note that the regression targets y are imbalanced, but the classes yk do not necessarily need to be imbalanced. We analyze in the experimental section the effect of defining balanced classes.
We make the observation that if we could optimize the class assignment, the Lextra term would disappear.
If the classes are optimized, then the class likelihoods are close to 0 for all classes except the true class: p(y∗ k|x, ω)≈0, ∀k̸=c, where c indexes the true class. Using this in the expression of Lextra, we obtain:
Lextra ≈ log (cid:101)p(yc) − log p(y∗
≈ − log p(y∗ c |x, ω), c |x, ω)(cid:101)p(yc), (10) (11)
Figure 3. MSE per class, where we vary the number of classes from 4 to 1024. We evaluate on uniformly sampled test sets, across 5 repetitions. We plot means and standard deviations for each dataset (rows) and sampling variation (columns), where the shad-ing represents the standard deviation. The red line, reg, should be constant across classes but it varies due to the sampling/training randomness. We also print the gap between the reg and reg+cls measured as absolute difference of MSE scores. The effect of the classification loss is present when the sampling of the data is im-balanced for the clean and noisy data. linearities. For setting the hyperparameter λ, we perform a hyper-parameter search on the validation set. We find the best λ to be 1e+2, 1e+3 and 1e+4 for the clean, noisy and out-of-distribution. We train for 80 epochs using an Adam optimizer with a learning rate of 1e−3, 1e−2 and 1e−4 for clean, noisy and out-of-distribution respectively. We use a weight decay of 1e−3. For classification we add at training-time a linear layer ([16×C]) predicting C classes. More details are in the supplementary material.
In Fig. 3 we show the MSE across all dataset and sam-pling variations, for {22, 24, 26, 28, 210} classes. We define the class ranges uniformly. The test sets are uniformly sam-pled and we perform 5 repetitions. We plot the means and standard deviations. We print on every plot the gap between the reg and reg+cls measured as the average absolute dif-ference of MSE scores. From this 1D analysis, we observe that the effect of the classification loss is visible when the training data is imbalanced for clean and noisy data.
where the (cid:101)p(yc) terms cancel out when decomposing the second log. Therefore, we can see that optimizing the class cross-entropy loss reduces the gap between between the
NLL of imbalanced data and NLL of balanced data. (Note: we observe in practice that if the classifier fails to converge, adding a classification loss is detrimental to regression.) 2.3. Defining balanced classes in practice
Existing works show that optimizing imbalanced classes is problematic [26, 49]. In practice, researchers opt for us-ing balanced classes in combination with regression [30, 44, 46, 51]. Here, the data is imbalanced, however we are free to define the class ranges such that we obtain balanced classes over imbalanced data sampling. To empirically test the added value of using balanced classes, we need a way to define balanced classes over imbalanced data sampling.
Given an imbalanced data sampling, we bin samples into classes, using uniform class ranges. This generates imbal-anced classes, which we then re-balance by redefining the class ranges such that the class histogram is approximately uniform. To this end, we apply histogram equalization over the original classes: q(k) =




C
N k (cid:88) j=1



 ,
HC(j) (12) where ⌊x⌋ rounds x down to the nearest integer, and HC(·) computes the histogram of the samples per class, and q(·) is a mapping function that maps the old classes indexed by k∈{1, .., C} to a new set of classes {1, .., C}. Eq. (12) merges class ranges such that their counts are as close as possible. Thus, the number of equalized classes is lower or equal to the original number of classes, C ≤ C.
After class equalization, the new classes are not per-fectly uniform. We further define a class-keeping proba-bility ρ(k), as the ratio between the minimum class count and the current equalized class count HC(k):
ρ(k) = minC j=1 HC(j)
HC(k)
, (13) where HC(·) computes the histogram of equalized classes.
Selecting training samples using only Eq. (13), without first equalizing the classes, will lead to never seeing samples from the most frequent classes. During training, for the re-gression loss we use all samples, while for the classification loss we pick samples (x, yk) with a probability defined by
ρ(k). More details are in the supplementary material. 3. Empirical analysis 3.1. Hypothesis analysis on 1D data
Figure 4. Effect of the λ hyperparameter on the validation:
We evaluate a range of values λ∈{1e−3, 1e−2, 1e−1, 1, 1e+1, 1e+2, 1e+3, 1e+4} on the validation set. The shading represents the standard deviation of the MSE error across 3 repetitions. Set-ting λ correctly is essential when using a classification loss next to the regression loss. regression with classification — reg+cls from Eq. (4). We start with the 1D data because it is easily interpretable and it offers a controlled environment to test the hypothesis that classification helps regression and to analyze the properties of the classes. 1
Effect of the λ hyperparameter. We perform hyperpa-rameter search on the validation set for setting the λ in
Eq. (4). We vary λ∈{1e−3, 1e−2, 1e−1, 1, 1e+1, 1e+2, 1e+3, 1e+4}. Fig. 4 shows the MSE across 3 repetitions, when considering different number of classes, for clean and noisy data scenarios, across sampling variations. Higher values of λ typically perform better in this case. When the sampling of the data is imbalanced, there exists a value of
λ such that the reg baseline is outperformed by the reg+cls.
We use the best λ values found on the validation, when eval-uating on the test set.
Effect of balancing the classes on imbalanced data. We numerically evaluate in Tab. 1 if using balanced classes (as defined in Eq. (12)-Eq. (13)) is less sensitive to the choice of λ. We perform 3 repetitions over all dataset scenarios and sampling cases, and vary λ∈{1e−3, 1e−2, 1e−1, 1, 1e+1, 1e+2, 1e+3, 1e+4}. For this we consider the percentage of runs (across different number of classes, random seeds, and values of λ) where classification helps regression — where reg+cls MSE is lower than reg MSE.
Ideally this number should be close to 100%. Balancing the classes is more robust to the choice of λ, as on average there
We use the 10 randomly sampled 1D functions to fur-ther analyze the regression loss — reg from Eq. (3), and 1Our source code will be made available online, at the address: https://github.com/SilviaLauraPintea/reg-cls.
Imbalanced classes (↑)
Balanced classes (↑)
Dataset case
Clean
Noisy data
Clean
Noisy data
Uniform
Mild
Moderate
Severe 55.42% 54.17% 47.50% 36.83% 59.83% 55.75% 56.58% 49.08% 59.00% 57.67% 47.50% 34.25% 65.00% 62.42% 56.00% 46.25%
Avg 57.42% 49.60% 48.48% 55.31%
Table 1. Effect of balancing the classes: On the validation sets we test how sensitive reg+cls is to the choice of λ when balancing classes versus when using imbalanced classes. For this we vary
λ∈{1e−3, 1e−2, 1e−1, 1, 1e+1, 1e+2, 1e+3, 1e+4}. And we measure the percentage of runs where adding a classification loss improves the regression predictions. Using balanced classes is less sensitive to the choice of λ.
Figure 5. Effect of noisy targets: We vary the amount of noise in the targets, along the y-axis. We plot both using imbalanced classes (green), and using balanced classes (lime) compared to the reg baseline (red). We report MSE on the test sets across 5 repeti-tions. Despite the the noise level increasing drastically, the benefits of adding a classification loss to the regression loss remain visible. are more runs where classification helps regression.
Effect of the noisy targets. In Fig. 3 we considered a sin-gle noise level for the Noisy data scenario. Here, we fur-ther analyze the effect on the MSE scores when varying the noise level in the targets for the baseline reg (in red) and the reg+cls with imbalanced classes (in green) as well as reg+cls bal where the classes are balanced (in lime color).
We vary the level of noise σ of the targets on the y-axis
σ∈{0.05, 0.1, 0.5}, and plot mean MSE and standard devi-ation on uniform test sets across 5 repetitions. For the bal-anced case, we indicate the original number of classes on the x-axis, while in practice the number of balanced classes is typically 2× lower than the initial one. Fig. 5 shows that despite severely increasing the noise level, adding a classifi-cation loss remains beneficial when the data is imbalanced. 3.2. Realistic image datasets
Imbalanced realistic image datasets. We run experiments on two realistic imbalanced datasets from Yang et al. [47]: depth estimation on the NYUD2-DIR dataset, and age esti-mation on the IMDB-WIKI-DIR. The supplementary ma-terial plots dataset statistics. For both datasets we use the Adam optimizer and set the learning rate to 1e−4 for
NYUD2-DIR with 5 epochs1 and batch size 16 accumu-lated over 2 batches (to mimic batch size 32 on 2 GPUs), while for IMDB-WIKI we use a learning rate of 1e−3 for 90 epochs and batch size 128. When comparing with Ren et al. [31] we use their best results (their GAI method). We also use the evaluation code provided in Yang et al. [47] and we report RMSE (root mean squared error) on NYUD2-DIR and MAE (mean absolute error)/MSE (mean squared error) for IMDB-WIKI-DIR as also done in [31, 47]. When re-running the baseline reg results we observed a large vari-ability across different runs by varying the random seed, especially on the NYUD2-DIR dataset, therefore we re-port results averaged over 3 random seeds. We use the ar-chitecture of Yang et al. [47]: ResNet-50 [17] for IMDB-WIKI-DIR, and the ResNet-50-based model from [19] for
NYUD2-DIR.
For adding the classification loss on IMDB-WIKI-DIR we append, only during training, a linear layer of size [F, C] followed by a softmax activation and a cross-entropy loss, where F is the number of channels in the one-to-last layer.
For the NYUD2-DIR the predictions are per-pixel, thus we use the segmentation head from Mask R-CNN [16] com-posed of a transposed convolution of size 2×2, ReLU, and a 1×1 convolution predicting the number of classes, C. At test time the classification branch is not used. We estimate the λ hyperparameter using the validation set provided in
[47] on IMDB-WIKI-DIR. For NYUD2-DIR we define a validation set by randomly selecting 1/5 of the training di-rectories with a seed of 0, and we use the same training/ validation/test split for all our results. For NYUD2-DIR we use λ=1.0 for 100 classes and λ=0.1 for 10 and 2 classes.
For IMDB-WIKI-DIR we set λ=0.1 for 100 classes and
λ=1.0 for 10 and 2 classes. We compare the reg results with reg+cls adding the classification loss, and reg+cls bal. with balanced classes, where we consider 2, 10 and 100 classes.
Tab. 2 shows the RMSE results on NYUD2-DIR when training with the standard MSE loss compared to adding a classification loss. We report RMSE on the test, where the best model is selected on the validation set across epochs
— RMSE-val. To compare with previous work who selects the best model on the test, we also report this as RMSE-test 1Using more epochs on NYUD2-DIR seems to lead to overfitting.
NYUD2-DIR RMSE-val (↓)
NYUD2-DIR RMSE-test (↓)
Samples All
Many
Med.
Few
All
Many
Med.
Few
Kernel [47] —
Balanced [31] — reg (MSE) 1.614 (±0.051)
—
— 0.554 (±0.002)
—
— 0.934 (±0.042)
—
— 2.360 (±0.081) 1.338 1.251 1.499 (±0.083) 0.670 0.692 0.578 (±0.010) 0.851 0.959 0.896 (±0.043) 1.880 1.703 2.171 (±0.136) reg+cls (2 cls) reg+cls (10 cls) reg+cls (100 cls) 1.587 (±0.026) 1.576 (±0.063) 1.536 (±0.090) 0.618 (±0.003) 0.585 (±0.008) 0.569 (±0.018) 1.062 (±0.025) 0.982 (±0.058) 0.966 (±0.041) 2.278 (±0.041) 2.282 (±0.095) 2.222 (±0.146) 1.532 (±0.082) 1.509 (±0.022) 1.488 (±0.028) 0.624 (±0.036) 0.582 (±0.013) 0.578 (±0.015) 0.946 (±0.032) 0.947 (±0.046) 0.971 (±0.049) 2.204 (±0.141) 2.178 (±0.047) 2.141 (±0.045) 1.599 (±0.020) 1.454 (±0.044) 1.553 (±0.117) reg+cls bal. (2 cls) reg+cls bal. (10 cls) reg+cls bal. (100 cls) 2.166 (±0.118) 2.077 (±0.087) 2.156 (±0.084)
Table 2. Imbalanced realistic image data: NYUD2-DIR depth estimation. We evaluate the baseline reg trained with MSE, and the reg+cls variants. We report RMSE when the best model is selected on the validation (RMSE-val ), or as in [47] on the test set (RMSE-test). We average over 3 different random seeds. Adding a classification loss helps regression, which validates our hypothesis. 0.616 (±0.026) 0.607 (±0.041) 0.563 (±0.033) 1.003 (±0.059) 0.965 (±0.023) 0.869 (±0.019) 2.304 (±0.044) 2.077 (±0.087) 2.263 (±0.193) 0.665 (±0.033) 0.607 (±0.041) 0.574 (±0.014) 1.522 (±0.060) 1.454 (±0.044) 1.487 (±0.051) 1.033 (±0.058) 0.965 (±0.023) 0.897 (±0.043)
Samples All
Many
Med.
Few
All
Many
Med.
Few
IMDB-WIKI-DIR MAE(↓)
IMDB-WIKI-DIR MSE(↓)
Focal [27]
Kernel [47]
Balanced [31] reg (MAE) reg+cls (2 cls) reg+cls (10 cls) reg+cls (100 cls) 7.97 7.78 8.12 8.09 (±0.01) 7.95 (±0.05) 7.93 (±0.06) 7.61 (±0.02) 7.12 7.20 7.58 7.23 (±0.02) 7.11 (±0.03) 7.12 (±0.06) 6.90 (±0.03) 15.14 12.61 12.27 15.48 (±0.15) 15.08 (±0.27) 14.93 (±0.17) 13.46 (±0.44) 26.96 22.19 23.05 26.81 (±0.48) 136.98 129.35
— 138.53 (±1.17) 106.87 106.52
— 107.82 (±0.96) 368.60 311.49
— 375.27 (±6.97) 1002.90 811.82
— 1017.59 (±27.51) 26.15 (±0.06) 25.91 (±0.27) 25.04 (± 0.82) 135.15 (±0.40) 135.69 (±1.65) 129.73 (±1.33) 105.86 (±0.27) 106.58 (±1.42) 103.25 (±1.12) 361.02 (±7.00) 359.27 (±5.70) 328.17 (± 10.40) 973.53 (±11.52) 975.37 (±34.09) 933.33 (± 9.16) reg+cls bal. (2 cls) reg+cls bal. (10 cls) reg+cls bal. (100 cls) 7.94 (±0.06) 7.92 (±0.06) 7.59 (±0.09) 7.14 (±0.09) 7.10 (±0.05) 6.86 (±0.08) 14.71 (±0.51) 14.99 (±0.21) 13.61 (±0.35) 26.20 (±0.73) 25.58 (±0.39) 25.30 (±0.19) 134.91 (±1.20) 134.53 (±0.75) 127.87 (±1.99) 106.36 (±1.68) 105.40 (±0.43) 101.34 (±1.74) 351.48 (±14.82) 362.48 (±4.41) 327.55 (±12.98) 980.61 (±39.44) 940.91 (±18.70) 925.65 (±21.96)
Table 3. Imbalanced realistic image data: IMDB-WIKI-DIR age estimation. We evaluate the baseline reg when trained with MAE, and the reg+cls variants. We report test MSE and MAE on the test set, averaged over 3 repetitions with different random seeds. Adding a classification loss next to the regression loss is specifically beneficial on this dataset, where this simple yet popular strategy performs on par with state-of-the-art. (We use “—” where the authors’ results are missing).
.
Balanced NYUD2-DIR
All RMSE-val (↓) All RMSE-test (↓) reg (MSE) reg+cls 1.442 (±0.077) 1.456 (±0.033) 1.492 (±0.042) 1.593 (±0.025)
Balanced IMDB-WIKI-DIR
All MAE (↓)
All MSE (↓) reg (MSE) reg+cls 7.74 (±0.04) 7.71 (±0.06) 131.03 (±1.44) 131.27 (±1.00)
Table 4. Balanced realistic image data: NYUD2-DIR depth es-timation and IMDB-WIKI-DIR age estimation. We compare the reg and reg+cls (using 100 classes) when the data is balanced.
We report RMSE and MAE/MSE, respectively, across 3 repeti-tions. There are no clear improvements when adding a classifica-tion loss to the regression on balanced data. (despite this being a bad practice). Additionally, note that our training set is slightly smaller because of using a valida-tion set, so the reg results are worse than in [47] (i.e. 1.477
RMSE-test). We observe that there is an inconsistency be-tween the training and test set, as the best model on the test set does not correspond to the best model on the validation set (which is a subset of the training). Despite all these, adding a classification loss still improves across all class-options, when selecting the best model on the validation set, and for 100 classes and 10 balanced classes, when selecting the best model on the test set. This may be due to the clas-sifier overfitting on the training data for fewer classes.
Tab. 3 gives the MAE and MSE results on IMDB-WIKI-DIR when using the standard MSE loss during training compared to when adding the classification at training time.
The best results are obtained using 100 balanced classes.
Here, adding a classification loss not only improves over the regression baseline, but it is also on-par with state-of-the-art methods specifically designed for imbalanced regression, such as [32, 47]. Adding a classification loss is similar to
[32, 47], who define smooth classes over the data.
Balanced realistic image datasets. On the same two datasets: NYUD2-DIR and IMDB-WIKI, we test the effect of adding a classification loss when we re-balance the data by binning the targets into 100 bins and selecting samples per batch during training as defined as in Eq. (12)-Eq. (13) for both reg and reg+cls. Because we mask samples per batch to balance the training data, for IMDB-WIKI here we use a batch size of 128 and learning rate 1e−4 for 90 epochs. While for NYUD2-DIR we use a learning rate of 1e−4 and batch size of 8, accumulated over 4 batches (to mimic a batch of 32 on 1 GPU), for 5 epochs. Tab. 4 shows
Breakfast RMSE % (↓)
Breakfast RMSE frames (↓) – unnormalized
Dataset split
S1
S2
S3
S4
S1
S2
S3
S4
Random baseline
[24] reg (RMSE) 58.50 (±0.29) 31.24 (±0.80) 32.57 (±1.45) 58.37 (±0.07) 31.49 (±0.98) 33.30 (±1.69) 58.39 (±0.14) 30.85 (±0.66) 32.52 (±1.17) 58.38 (±0.12) 30.78 (±0.53) 33.08 (±1.49) 1394.48 (±992.50) 1079.38 (±775.17) 860.17 (±583.44) 1511.04 (±1132.60) 1235.02 (±932.89) 891.39 (±641.58) 1450.26 (±1085.56) 1172.67 (±880.69) 862.65 (±609.01) 1420.21 (±1063.16) 1170.07 (±886.92) 845.48 (±605.46) reg+cls (100 cls) reg+cls bal. (100 cls) — 28.71 (±0.38) 28.84 (±0.55)
— 28.46 (±0.48)
— 28.44 (±0.50)
— 837.59 (±573.15) 837.61 (±573.17) (a) Percentage prediction (normalized). 870.55 (±630.89) 870.54 (±630.88) 845.65 (±601.73) 845.65 (±601.72) (b) Frame prediction (unnormalized). 809.76 (±595.37) 809.76 (±595.37)
Table 5. Imbalanced video progress prediction on Breakfast. We report mean RMSE and standard deviations averaged over all 10 cooking tasks of the Breakfast dataset. (a) Progress prediction in video percentages. (b) Progress prediction in frame numbers. The overall progress prediction results leave space for improvements for all methods, because of the dataset challenges. However, also for this regression problem adding a classification loss has benefits. the results across 3 repetitions. When the data is already balanced, adding a classification loss has limited effect. 3.3. Imbalanced video progress prediction
As an additional investigation on imbalanced data, we explore videos which are naturally imbalanced in the num-ber of frames. We perform video progress prediction on the Breakfast video dataset [23] containing 52 participants performing 10 cooking activities. We follow the standard dataset split (S1, S2, S3, S4) [23] and use the correspond-ing train/test splits. We adopt the method in [24] and train a simple MLP on top of IDT (improved dense trajectory) fea-tures [41] of dimension 64 over trajectories of 15 frames.
We evaluate RMSE when predicting either video progress percentages, or absolute frame numbers. Kukleva et al.
[24] use an MLP with 3 linear layers and sigmoid activa-tions. We change the sigmoid activations into ReLU acti-vations for the reg and reg+cls models, since it works bet-ter when predicting absolute frame numbers. For all meth-ods we keep the training hyperparameters from [24]: learn-ing rate 1e−3, Adam optimizer, and training for 40 epochs with the learning rate decreased by 0.1 at 30 epochs. We also report the random baseline results when using the un-trained model. The data is imbalanced, in the sense that video lengths vary widely (see supplementary material for data distribution). We test again if adding a classification loss can benefit the regression predictions when using 100 classes in the reg+cls. We search for λ on a validation set created by randomly selecting 1/3 of the training videos with a seed of 0. We use λ=100 when predicting percent-ages, and λ=10 when predicting frame numbers. At train-ing time, we add the classification loss via a linear layer on top of the one-to-last layer, followed by softmax. We train one model per task and report mean and standard deviations over all 10 tasks.
Tab. 5 depicts the results across all 4 splits. We re-port RMSE scores when predicting video progress in terms of percentages in Tab. 5(a), and when predicting video progress in terms of frames in Tab. 5(b). Even if we predict video progress percentages between [0,100]% in Tab. 5(a), because the video length varies widely, for some videos we will have a lot more frame than for others, causing the data sampling to still be imbalanced. In both cases there is gain from adding a classification loss to the regression loss. 4. Discussion and limitations
Relation to nonlinear ICA. Hyv¨arinen et al. [21] show that there is a relation between learning to bin a continu-ous function and performing nonlinear ICA. Hyv¨arinen et al. [21] start from a continuous signal x generated by a non-linear combination of source-signals: x=f (s), where f (·) is a non-linear function and s are the independent and non-stationary sources. They split the signal into C temporal segments x={x1, x2, .., xC} and train an MLP to predict for every sample xki the segment it belongs to: g(xki, θ)=k, k∈{1, .., C} and xki ⊂xk, and θ are the MLP parameters. Hyv¨arinen et al. [21] prove that the last hid-den layer of the MLP hg(·, θ) recovers the original sources s within a linear transformation: hg(xki, θ) = wski + b, where w, b define a linear transformation.
Intuitively,
Hyv¨arinen et al. [21] discretize the signal into segments and classify the segments, thus performing classification on a continuous function. However, they do not focus on com-bining the binning with optimizing a regression problem.
Similar to Hyv¨arinen et al. [21], predicting discrete sig-nal bins has been successfully used for unsupervised video representation learning by discriminating between video segments [9] or classifying video speed [42]. Up to the point which the underlying continuous regression function (e.g. speed, time, age, depth) can be assumed to be gener-ated by a non-linear combination of non-stationary source (whose statistics change with the function), adding a clas-sification loss decorrelates the independent sources. Addi-tionally, we hypothesize that there may be a relation be-tween nonlinear ICA and imbalanced sampling: the inde-pendent sources s are also continuous and shared across samples. And having the hidden representation of the MLP constrained to be independent across dimensions may lead to a better use of sparse samples in certain areas of the target-space. However, we leave this for future research.
Limitations of analysis. The analysis performed here is still elementary and only aims to scratch the surface on the
usefulness of adding a classification loss when performing regression. A number of things have been disregarded here such as: the effect of the model depth, while keeping the model size fixed. Additionally, the choice of the optimizer and the loss function during training may also play an im-portant role. Finally, delving more into the relation between nonlinear ICA and adding a classification loss to the regres-sion, may be an interesting future research avenue. 5.