Abstract
This work aims to transfer a Generative Adversarial
Network (GAN) pre-trained on one image domain to an-other domain referred to as few as just one reference image. The challenge is that, under limited supervision, it is extremely difficult to synthesize photo-realistic and highly diverse images while retaining the representative characters of the target domain. Different from existing approaches that adopt the vanilla fine-tuning strategy, we design two lightweight modules in the generator and the discriminator respectively. We first introduce an attribute adaptor in the generator and freeze the generator’s original parameters, which can reuse the prior knowledge to the most extent and maintain the synthesis quality and diversity.
We then equip the well-learned discriminator with an attribute classifier to ensure that the generator with the attribute adaptor captures the appropriate characters of the reference image. Furthermore, considering the very limited diversity of the training data (i.e., as few as only one image), we propose to constrain the diversity of the latent space through truncation in the training process, alleviating the optimization difficulty. Our approach brings appealing results under various settings, substantially sur-passing state-of-the-art alternatives, especially in terms of synthesis diversity. Noticeably, our method works well even with large domain gaps and robustly converges within a few minutes for each experiment. Code and models are available at https://genforce.github.io/genda/. 1.

Introduction
Generative Adversarial Network (GAN) [4], consisting of a generator and a discriminator, has significantly ad-vanced image synthesis yet relies on training with a large number of images [5, 7, 8, 1]. Many attempts have been
† Equal contribution. 1
made to train GANs from scratch with limited data [27, 32, 30, 6, 25], but it still requires hundreds or thousands of images to get a satisfying synthesis result. Sometimes, however, we may have only a few images, or in extreme like the cases only one single image as the reference, masterpiece Mona Lisa by Leonardo da Vinci. Under such a case, learning a generative model with both good quality and high diversity seems impossible.
Domain adaptation is a commonly used technique that applies a model trained on one data domain to another [2].
Prior works [23, 13, 22, 12, 29, 10, 16] have introduced this technique to GAN training to alleviate the requirement on the data scale. Typically, they first train a large-scale model in the source domain with adequate data, and then transfer it to the target domain with only a few samples.
A common practice is to fine-tune both the generator and the discriminator on the target dataset until the generator produces samples conforming to the target domain. To stabilize the fine-tuning process and improve the genera-tion quality and diversity, existing approaches propose to tune partial parameters [13, 12, 16] and introduce some regularizers [10, 14], but the overall adaptation strategy stays the same. When there is only one image from the target domain, these methods would fall short of synthesis diversity, producing very similar images.
Remember that the pre-trained model can produce highly diverse images in the source domain. Then what does cause the diversity drop in the adaptation process? We find that directly tuning the model weights results in the loss of the prior knowledge gained from the large-scale data due to the model parameter’s collapse into one mode. However, when adapting the model to the target domain, most variation factors (e.g., gender, and pose of human faces) should be reused as much as possible. These observations lead to a question: is it possible to simply focus on the most repre-sentative characters of the reference image while inheriting all the other knowledge from the source domain?
To answer the question above, we develop a novel method, called GenDA, for one-shot Generative Domain
Adaptation. In particular, we design a lightweight module connecting the latent space and the synthesis network. We call this module an attribute adaptor since it helps adapt the generator with the attributes of the target image. Unlike the conventional fine-tuning strategy, we freeze the parameters of the original generator and merely optimize the attribute adaptor during training. Thereby, we manage to reuse the prior knowledge learned by the source model and hence inherit the synthesis quality and, more importantly, the diversity. Meanwhile, we employ the discriminator to compete with the generator via a domain-specific attribute classification. In this way, the generator is forced to capture the most representative attributes from the reference; other-wise, the discriminator would spot the discrepancy. How-ever, instead of directly tuning the original discriminator, we freeze its entire backbone’s parameter and introduce a lightweight attribute classifier on top of that. Similar to the generator, the discriminator has also learned rich knowledge in its pre-training. Since the synthesized images before and after adaptation share most visual concepts (e.g., a face model would still produce faces after domain transfer), the discriminator can be reused as a well-learned feature extractor. Therefore, we simply train the attribute classifier to help guide the generator. Furthermore, since there is only one training sample (which means no diversity in the target domain), we propose to also constrain the diversity of the generative domain by truncating the latent distribution during training. Intuitively, learning a one-to-one mapping would be easier than learning a many-to-one mapping. Such a design mitigates the optimization difficulty and further improves the synthesis quality.
We evaluate our approach through extensive experiments on synthesizing faces and outdoor scenes. Given only one training image, GenDA can adapt the source model to the target domain with sufficiently high quality and diversity. Such an adaptation is successful at both the attribute level and the style level, shown in Fig. 1. Our method outperforms the state-of-the-art competitors by a substantial margin both qualitatively and quantitatively. We also show that when the number of samples available in the target domain increases, GenDA can filter out the individual attributes and capture their common characters (see Fig. 4).
Noticeably, GenDA can work on some extreme cases where there is a large domain gap, like transferring the characters of Mona Lisa to churches (see Fig. 5), creating interesting visual special effect. 2.