Abstract
We present a novel model for Tracking Any Point (TAP) that effectively tracks any queried point on any physical sur-face throughout a video sequence. Our approach employs two stages: (1) a matching stage, which independently lo-cates a suitable candidate point match for the query point on every other frame, and (2) a refinement stage, which up-dates both the trajectory and query features based on lo-cal correlations. The resulting model surpasses all baseline methods by a significant margin on the TAP-Vid benchmark, as demonstrated by an approximate 20% absolute average
Jaccard (AJ) improvement on DAVIS. Our model facilitates fast inference on long and high-resolution video sequences.
On a modern GPU, our implementation has the capacity to track points faster than real-time. Given the high-quality trajectories extracted from a large dataset, we demonstrate a proof-of-concept diffusion model which generates trajec-tories from static images, enabling plausible animations.
Visualizations, source code, and pretrained models can be found at https://deepmind-tapir.github.io. 1.

Introduction
The problem of point level correspondence —i.e., deter-mining whether two pixels in two different images are pro-jections of the same point on the same physical surface— has long been a fundamental challenge in computer vision, with enormous potential for providing insights about physi-cal properties and 3D shape. We consider its formulation as
“Tracking Any Point” (TAP) [12]: given a video and (po-tentially dense) query points on solid surfaces, an algorithm should reliably output the locations those points correspond to in every other frame where they are visible, and indicate frames where they are not – see Fig. 4 for illustration.
Our main contribution is a new model: TAP with per-frame Initialization and temporal Refinement (TAPIR),
Figure 1. Retrospective evolution of point tracking performance over time on the recent TAP-Vid-Kinetics and TAP-Vid-DAVIS benchmarks, as measured by Average Jaccard (higher is better).
In this paper we introduce TAPIR, which significantly improves performance over the state-of-the-art. This unlocks new capabili-ties, which we demonstrate on motion-based future prediction. which greatly improves over the state-of-the-art on the recently-proposed TAP-Vid benchmark [12]. There are many challenges to TAP: we must robustly estimate oc-clusions and recover when points reappear (unlike optical flow and structure-from-motion keypoints), meaning that search must be incorporated; yet when points remain visi-ble for many consecutive frames, it is important to integrate information about appearance and motion across many of those frames in order to optimally predict positions. Fur-thermore, little real-world ground truth is available to learn from, so supervised-learning algorithms need to learn from synthetic data without overfitting to the data distribution (i.e., sim2real).
There are three core design decisions that define TAPIR.
The first is to use a coarse-to-fine approach. This approach has been used across many high-precision estimation prob-lems in computer vision [7, 28, 32, 38, 41, 58, 69, 73, 78].
For our application, the initial ‘coarse’ tracking consists of an occlusion-robust matching performed separately on every frame, where tracks are hypothesized using low-resolution features, without enforcing temporal continuity.
The ‘fine’ refinement iteratively uses local, spatio-temporal information at a higher resolution, wherein a neural net-work can trade-off smoothness of motion with local appear-ance cues to produce the most likely track. The second de-sign decision is to be fully-convolutional in time: the layers of our neural network consist principally of feature com-parisons, spatial convolutions, and temporal convolutions, resulting in a model which efficiently maps onto modern
GPU and TPU hardware. The third design decision is that the model should estimate its own uncertainty with regard to its position estimate, in a self-supervised manner. This ensures that low-confidence predictions can be suppressed, which improves benchmark scores. We hypothesize that this may help downstream algorithms (e.g. structure-from-motion) that rely on precision, and can benefit when low-quality matches are removed.
We find that two existing architectures already have some of the pieces we need: TAP-Net [12] and Persis-tent Independent Particles (PIPs) [19]. Therefore, a key contribution of our work is to effectively combine them while achieving the benefits from both. TAP-Net performs a global search on every frame independently, providing a coarse track that is robust to occlusions. However, it does not make use of the continuity of videos, resulting in jit-tery, unrealistic tracks. PIPs, meanwhile, gives a recipe for refinement: given an initialization, it searches over a local neighborhood and smooths the track over time. However,
PIPs processes videos sequentially in chunks, initializing each chunk with the output from the last. The procedure struggles with occlusion and is difficult to parallelize, re-sulting in slow processing (i.e., 1 month to evaluate TAP-Vid-Kinetics on 1 GPU). A key contribution of this work is observing the complementarity of these two methods.
As shown in Fig. 1, we find that TAPIR improves over prior works by a large margin, as measured by performance on the TAP-Vid benchmark [12]. On TAP-Vid-DAVIS,
TAPIR outperforms TAP-Net by ∼20% while on the more challenging TAP-Vid-Kinetics, TAPIR outperforms PIPs by
∼20%, and substantially reduces its inference runtime. To demonstrate the quality of TAPIR trajectories, we show-case a proof-of-concept model trained to generate trajec-tories given individual images, and find that this model can generate plausible animations from single photographs.
In summary, our contributions are as follows: 1) We propose a new model for long term point tracking, bridg-ing the gap between TAP-Net and PIPs. 2) We show that the model achieves state-of-the-art results on the challeng-ing TAP-Vid benchmark, with a significant boost on per-formance. 3) We provide an extensive analysis of the ar-chitectural decisions that matter for high-performance point tracking. 4) We provide a proof-of-concept of video predic-tion enabled by TAPIR’s high-quality trajectories. Finally, 5) after analyzing components, we separately perform care-ful tuning of hyperparameters across entire method, in or-der to develop the best-performing model, which we release at https://www.github.com/deepmind/tapnet for the benefit of the community. 2.