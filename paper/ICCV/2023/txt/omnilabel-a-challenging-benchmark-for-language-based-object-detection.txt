Abstract
Language-based object detection is a promising direc-tion towards building a natural interface to describe objects in images that goes far beyond plain category names. While recent methods show great progress in that direction, proper evaluation is lacking. With OmniLabel, we propose a novel task definition, dataset, and evaluation metric. The task subsumes standard- and open-vocabulary detection as well as referring expressions. With more than 28K unique ob-ject descriptions on over 25K images, OmniLabel provides a challenging benchmark with diverse and complex object descriptions in a naturally open-vocabulary setting. More-over, a key differentiation to existing benchmarks is that our object descriptions can refer to one, multiple or even no ob-ject, hence, providing negative examples in free-form text.
The proposed evaluation handles the large label space and judges performance via a modified average precision met-ric, which we validate by evaluating strong language-based baselines. OmniLabel indeed provides a challenging test bed for future research on language-based detection. Visit the project website at https://www.omnilabel.org 1.

Introduction
A nuanced understanding of the rich semantics of the world around us is a key ability in the visual perception system of humans. Identifying objects from a description like “person wearing blue-and-white striped T-shirt stand-ing next to the traffic sign” feels easy, because humans understand the composition of object category names, at-tributes, actions, and spatial or semantic relations between objects. When automated, this same ability can improve and enable a plethora of applications in robotics, autonomous vehicles, navigation, retail, etc.
With the recent advances in vision & language mod-els [17, 19, 37], along with extensions towards object lo-Figure 1: (Left) OmniLabel extends upon standard detec-tion, open-vocabulary detection, as well as referring expres-sions, subsuming these tasks as special cases. (Right) The
OmniLabel dataset is semantically rich with diverse free-form text descriptions of objects. Moreover, with object descriptions referring to multiple instances, dedicated neg-ative descriptions, and a novel evaluation metric, our bench-mark poses a challenging task for language-based detectors. calization [15, 18, 24, 27, 49], a comprehensive evaluation benchmark is needed. However, existing ones fall short in various aspects. While object detection datasets sig-nificantly increased the label space over time (from 20 in
Pascal [12] to 1200 in LVIS [16]), a fixed label space is assumed. The zero-shot [4] and open-vocabulary detec-tion [15, 46] settings drop the fixed-labelspace assumption, but corresponding benchmarks only evaluate simple cate-gory names, neglecting more complex descriptions. Refer-ring expression datasets [33, 45] probe models with free-form text descriptions of objects. However, the correspond-ing dataset annotations and metrics do not allow for a com-prehensive evaluation of models.
We introduce a novel benchmark called OmniLabel with the goal to comprehensively probe models for their abil-ity to understand complex, free-form textual descriptions of objects and to locate the corresponding instances. This re-quires a novel task definition and evaluation metric, which we propose in Sec. 3. Our evaluation benchmark does not assume a fixed label space (unlike standard detection), uses
h t g n e l
. r c s e
D m r o f
-e e r
F s e g a m i
# y r a l u b a c o v
-n e p
O e c n a t s n
I
-i t l u
M e v i t a g e
N s n u o n e u q i n u
# n o i t a u l a v
E
Dataset
LVIS [16]
ODinW [25]
RefCOCO [33, 45] 4.3K ✓ 4.5 3.5K ✓ ✗ ✗
Flickr30k [36] 5K ✗ – 1.2K ✗ ✓ ✓ AP 27.3k ✗ – 0.3K ✓ ✓ ✓ AP
P 1.0K ✓ 2.4 1.9K ✓ ✗ ✗ R 2.9K ✓ 2.0 1.5K ✓ ✓ ✗ IoU 12.2K ✓ 5.6 4.6K ✓ ✓ ✓ AP
PhraseCut [42]
OmniLabel
Table 1: Comparing OmniLabel to existing benchmarks:
On 12.2K images, OmniLabel provides free-form text de-scriptions of objects with an average description length of 5.6 words, covering 4.6K unique nouns. Each description can refer to multiple objects, or no object, i.e., a negative example, an important factor in our evaluation. (Numbers are computed on validation sets. P: Precision, R: Recall,
AP: Average Precision, IoU: Intersection over Union) complex object descriptions beyond plain category names (unlike open-vocabulary detection), and evaluates true de-tection ability with descriptions referring to zero, one or more instances in a given image (unlike referring expres-sions). A unique aspect of our benchmark are the descrip-tions that refer to zero instances, which pose a challenge to existing methods as hard negative examples. Fig. 1 posi-tions our OmniLabel benchmark.
To build this evaluation benchmark, we collected a set of novel annotations upon existing object detection datasets.
We augment the existing plain category names with novel free-form text descriptions of objects. Our specific annota-tion process (Sec. 4) increases the difficulty of the task by ensuring that at least one of the following conditions is true: (a) Multiple instances of the same underlying object cate-gory are present in the same image (b) One object description can refer to multiple objects (c) An image contains a negative object description, which refers to no object but is related to the image’s semantics (d) Descriptions do not use the original category name
Tab. 1 highlights the key differences of OmniLabel to exist-ing benchmarks: The diversity in the free-form text descrip-tions and the evaluation as an object detection task, includ-ing multiple instances per description as well as negative object descriptions. The numbers in the table reflect our public validation set, which is roughly the same size as our private test set. Fig. 2 provides examples of the dataset.
Figure 2: Examples of the OmniLabel ground truth anno-tations. Positive descriptions (above each image) can refer to one or multiple instances. Negative descriptions (below each image) are semantically related to the image but refer to no object. our benchmark, including RegionCLIP [51], Detic [53],
MDETR [18], GLIP [27] and FIBER [11]. Summarized in Sec. 6.2, our key observation is that the proposed bench-mark is difficult for all methods, and the evaluation metric is more stringent than in prior benchmarks. Negative object descriptions pose the biggest challenge to current methods.
We summarize our contributions as follows: (a) A novel benchmark to unify standard detection, open-vocabulary detection and referring expressions (b) A data annotation process to collect diverse and com-plex free-form text descriptions of objects, including negative examples (c) A comprehensive evaluation metric that handles the vir-We also evaluate recent language-based detectors on tually infinite label space
2.