Abstract
Partially annotated images are easy to obtain in multi-label classification. However, unknown labels in partially annotated images exacerbate the positive-negative imbal-ance inherent in multi-label classification, which affects supervised learning of known labels. Most current meth-ods require sufficient image annotations, and do not focus on the imbalance of the labels in the supervised training phase.
In this paper, we propose saliency regularization (SR) for a novel self-training framework. In particular, we model saliency on the class-specific maps, and strengthen the saliency of object regions corresponding to the present labels. Besides, we introduce consistency regularization to mine unlabeled information to complement unknown labels with the help of SR. It is verified to alleviate the negative dominance caused by the imbalance, and achieve state-of-the-art performance on Pascal VOC 2007, MS-COCO, VG-200, and OpenImages V3. 1.

Introduction
The multi-label classification task is a practical vision task that has received much attention. It is widely used in applications such as image retrieval and recommendation systems. In recent years, significant progress has been made in multi-label classification with the development of deep neural networks [20, 30, 15, 11]. These efforts benefit from network structure construction [35, 7, 8] and optimization function design [38, 28, 14]. Meanwhile, the excellent per-formance of a model is inseparable from a large-scale and high-quality dataset. However, as the number of samples and semantic concepts increases significantly, annotating all possible labels for each image is very difficult. Thus, anno-tating a large-scale multi-label benchmark with full labels is very laborious and time-consuming, which poses chal-lenges for multi-label classification. Collecting a partially
*Corresponding author (email to xex@hust.edu.cn). (a) (b)
Figure 1. Probability distribution of prediction on the MS-COCO validation set from ResNet101 trained by BCE with (a) 10%, and (b) 50% known labels on its training set. The y-axis represents the number of all class ground-truths in each probability interval.
The positive-negative ratio of samples is same in the training sets with different known proportions. It is obviously observed that the number of misclassified positive labels in the interval [0, 0.5] is significantly higher when 10% known labels. annotated multi-label dataset is an alternative and feasible strategy to solve the problem. For a partially annotated im-age, a small subset of its labels is annotated by positive and negative labels, and the rest labels are unknown. Partial labeling saves time and labor costs considerably. This pa-per focuses on how to learn deep neural network models on multi-label datasets with partial labels.
To simplify the classification problem of partial label-ing, unknown labels are ignored or treated as negative la-bels [32, 17]. However, the mode of ignoring unknown la-bels leads to higher entropy of unlabeled data and weakens the generalization of the trained model. Treating unknown labels as negative may reduce the entropy of unlabeled data, which also introduces label noise into model training. The self-training (ST) framework is proposed to deal with the problems of generalization and label noise. Known labels are utilized for training a model first, then the trained model is used to generate pseudo-labels for unknown labels, and at last, the model is trained by all labels again. The works
[12, 16, 6] exploit known labels to supervise training and mine label correlations or instance similarities to comple-ment the unknown labels. The method [1] is based on class distribution estimated by the trained model and label like-lihood to select negative pseudo-labels for unknown labels.
These methods achieve remarkable performance in settings with large proportions (e.g., >50%) of known labels. How-ever, it is hard to capture label correlation and instance sim-ilarity when the proportion of known labels is small (e.g.,
<30%) for the works [12, 16, 6]. The number of positive labels is small in a low known proportion (e.g., 10%), and the increase of negative pseudo-labels may exacerbate the positive-negative imbalance of samples for the work [1].
Supervised learning with limited known labels, espe-cially low known proportions, is important for self-training.
It determines the direction of model optimization and af-fects the reliability of the generated pseudo-labels. The known labels of each image in the training set contain fewer positive labels and more negative labels, thus the classifier of each class excels at classifying negative samples rather than positive samples. As shown in both Fig. 1a and Fig. 1b, negative samples are misclassified much less than positive samples. Meanwhile, the error rate of positive samples is higher in a low known proportion under the same positive-negative ratio, namely, more unknown labels exacerbate the imbalance in Fig. 1. The imbalance of label level makes the spatial object regions of the present labels get less at-tention, namely, the activation outputs of the object regions are suppressed. Few works [1, 41] focus on the positive-negative imbalance of the label level to improve supervised learning. Through increasing the saliency of object regions corresponding to the present labels at the spatial level, we alleviate the negative dominance caused by the imbalance.
In this work, we design a novel ST framework based on saliency regularization (SR), including supervised and un-supervised learning modules. We model the saliency on the class-specific maps (CSM). To alleviate the negative dominance, we boil down an optimization problem about strengthening the saliency of object regions corresponding to the present labels. We transform the optimization prob-lem into a regularization of logit space and prove that such an operation can address the imbalance of easy and hard samples. We introduce consistency regularization (CR) to mine unlabeled information. SR will enlarge the probabili-ties of possible positive labels from the weak augmentation of an image, which helps to complement the unknown la-bels for its strong augmentation. Our contributions can be summarized as 1) We build a novel end-to-end ST frame-work, which introduces CR to eliminate the restrictions of pre-trained models and improves supervised learning to adapt the model to different known label proportions. 2)
We propose SR to mitigate the negative dominance dur-ing training, which is also verified to address the imbalance of easy and hard samples. 3) Extensive experiments con-ducted in simulated and real multi-label datasets show that our method alleviates the negative dominance and achieves state-of-the-art (SOTA) performance. 2.