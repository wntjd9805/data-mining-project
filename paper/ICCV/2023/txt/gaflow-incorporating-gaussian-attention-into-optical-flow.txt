Abstract
Optical flow, or the estimation of motion fields from image sequences, is one of the fundamental problems in computer vision. Unlike most pixel-wise tasks that aim at achieving consistent representations of the same category, optical flow raises extra demands for obtaining local dis-crimination and smoothness, which yet is not fully explored by existing approaches.
In this paper, we push Gaussian
Attention (GA) into the optical flow models to accentuate local properties during representation learning and enforce the motion affinity during matching. Specifically, we in-troduce a novel Gaussian-Constrained Layer (GCL) which can be easily plugged into existing Transformer blocks to highlight the local neighborhood that contains fine-grained structural information. Moreover, for reliable motion anal-ysis, we provide a new Gaussian-Guided Attention Mod-ule (GGAM) which not only inherits properties from Gaus-sian distribution to instinctively revolve around the neigh-bor fields of each point but also is empowered to put the emphasis on contextually related regions during match-ing. Our fully-equipped model, namely Gaussian Attention
Flow network (GAFlow), naturally incorporates a series of novel Gaussian-based modules into the conventional opti-cal flow framework for reliable motion analysis. Extensive experiments on standard optical flow datasets consistently demonstrate the exceptional performance of the proposed approach in terms of both generalization ability evalua-tion and online benchmark testing. Code is available at https://github.com/LA30/GAFlow. 1.

Introduction
Optical flow aims to establish pixel-wise correspon-dences across images, playing a crucial role in video un-derstanding.
It unifies representation learning and fea-ture matching as a problem of pixel-wise motion inference.
Modern optical flow models typically focus on either im-proving representation learning techniques (e.g., alternative
*Corresponding author
Figure 1: Visualization of feature discrimination (left) and smoothness constraint (right). For a random point P in frame t, our GCL enhances feature discrimination by em-phasizing fine-grained structural details. Concurrently, our
GGAM effectively captures smoothness constraints, center-ing on pertinent local regions. learning [40, 20, 54] and reinforcement learning [1]) or re-fining feature similarity measurement methodologies (e.g., 4D correlation volumes [42] or 4D Transformer [16]). De-spite these significant advancements, a glaring limitation persists: these models largely neglect the exploration of local structural information. This oversight hinders their performance, particularly in challenging scenarios involv-ing large motions, occlusions, blurring effects, and shifts in appearance. Such situations demand an elevated focus on local discrimination and flow consistency. This brings us to an intriguing inquiry: Is it feasible to architect optical flow models that intrinsically focus on local structural in-formation during both representation learning and feature matching?
In response to the aforementioned challenge, we present the Gaussian Attention Flow network (GAFlow), a pioneer-ing framework that leverages Gaussian Attention (GA) to inform both the feature encoder and the matching mod-ule. Central to our approach is a novel attention mech-anism equipped with a learnable Gaussian kernel. This mechanism naturally prioritizes the local surroundings of each point, thereby accentuating the importance of local structural information. For the representation learning com-ponent, we introduce a novel Gaussian-Constrained Layer (GCL) embedded within the canonical feature encoder.
This layer, fortified with a learnable Gaussian kernel, nat-urally prioritizes the local surroundings of each point, thus
It’s accentuating the significance of local structural data. worth noting that while the GCL builds upon the pixel-wise modeling typical of the Vision Transformer block, it pos-sesses a sharper focus on local connections, which effec-tively enhances the feature discrimination for cross-frame matching, as depicted in Fig. 1. Moreover, our GCL is dy-namic, comprising learnable parameters that can be fine-tuned in concert with the encoder.
In optical flow prediction, traditional techniques prior-itize the matching function, viewing it as indispensable when paired with distinctive features. These methods in-corporate feature-similarity measures and smoothness con-straints to optimize results [15, 5, 6]. With the advent of deep learning, the spotlight has largely shifted to feature-similarity [16, 42], often sidelining the crucial smoothness constraint. When considering it, the implicit smoothness loss is primarily employed [51, 26], presuming uniform flow fields —– a simplification that overlooks intricate ob-ject deformations. While some approaches employ Graph techniques [30] or Transformers [22] to capture global mo-tion, they tend to neglect the inherent locality of motion, po-tentially introducing inaccuracies. The challenge of lever-aging neural networks to model motion relationships re-mains somewhat uncharted.
An ideal neural module for motion modeling should have i) Neighbourhood priority. The three key properties: motion of object(s) appears locally in visual scenes, and thus the module should instinctively focus more on the nearest neighbor fields for each pixel. ii) Matching-prior awareness. Previous work [4] shows that matching prior helps in large displacements and avoids over-smoothing; iii) High-order relation centered. Mining high-order re-lations [30, 22, 29] is essential for dealing with occlusions and lighting changes, as low-level similarities (like color) are often fragile. It would also be advantageous if this mod-ule’s parameters could be trained in a data-driven manner.
Targeting the above goals, we introduce a novel
Gaussian-Guided Attention Module (GGAM) to explicitly model the motion affinities for optical flow. To meet the neighborhood priority requirement, our GGAM is formu-lated as Gaussian-guided attention that naturally empha-sizes the neighboring fields of each point. Second, unlike conventional Non-Local operation [44], our GGAM is built across the context feature map and embedded correlation (cost) volume for more comprehensive relation modeling.
Specifically, for capturing the matching-prior knowledge, the 4D correlation volumes [42] are mapped to be the am-plitudes and offsets. The amplitude for each position is set to the Gaussian attention for scaling its amplitude value and the offsets are encoded by the Gaussian attention via the warping operation. It enables free-form deformation of the
Gaussian attention and adaptive attention for handling the large displacement of objects. For the last requirement, we draw inspiration from self-attention operations, and map the context feature to the query and key features for modeling the appearance self-similarities. All these operations are fully differentiable in our GGAM.
Our fully-equipped model, called Gaussian Attention
Flow network (GAFlow), unites GCL and GGAM to con-duct motion analysis by considering both feature similari-ties and motion affinities. It achieves the top performance on both Sintel and KITTI benchmarks with limited extra computational cost. Overall, the main contributions of this paper are: 1) We introduce a novel approach to en-hance the local properties of underlying representations.
Our proposed Gaussian-Constrained Layer (GCL) can work complementarily with the standard feature encoder to build more discriminative features for optical flow. 2) We ana-lyze three important properties for motion affinity model-ing, leading to a novel Gaussian-Guided Attention Module (GGAM). For the first time, we show that it is feasible to capture the local relations by learning the Gaussian atten-tion and refining the motion fields for a more reliable optical flow estimation. 3) We unite our GCL and GGAM into the contemporary optical flow architecture, making the model stronger at highlighting local structural information. Our
Gaussian Attention Flow networks set new records on a va-riety of benchmarks, e.g., Sintel (clean and final) and KITTI datasets, and outperform existing optical flow models by a relatively large margin. 2.