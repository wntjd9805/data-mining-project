Abstract
Therefore, the recent
Catastrophic forgetting; the loss of old knowledge upon acquiring new knowledge, is a pitfall faced by deep neu-ral networks in real-world applications. Many prevail-ing solutions to this problem rely on storing exemplars (previously encountered data), which may not be feasi-ble in applications with memory limitations or privacy constraints. focus has been on
Non-Exemplar based Class Incremental Learning (NECIL) where a model incrementally learns about new classes with-out using any past exemplars. However, due to the lack of old data, NECIL methods struggle to discriminate be-tween old and new classes causing their feature represen-tations to overlap. We propose NAPA-VQ: Neighborhood
Aware Prototype Augmentation with Vector Quantization, a framework that reduces this class overlap in NECIL. We draw inspiration from Neural Gas to learn the topological relationships in the feature space, identifying the neighbor-ing classes that are most likely to get confused with each other. This neighborhood information is utilized to enforce strong separation between the neighboring classes as well as to generate old class representative prototypes that can better aid in obtaining a discriminative decision boundary between old and new classes. Our comprehensive experi-ments on CIFAR-100, TinyImageNet, and ImageNet-Subset demonstrate that NAPA-VQ outperforms the State-of-the-art NECIL methods by an average improvement of 5%, 2%, and 4% in accuracy and 10%, 3%, and 9% in for-getting respectively. Our code can be found in https:
//github.com/TamashaM/NAPA-VQ.git. 1.

Introduction
The achievements of deep neural networks over the years have grown significantly, and their efficiency and applica-bility have been demonstrated by numerous state-of-the-art works [29, 23, 35, 20]. However, a major requirement for the optimal operation of gradient-based optimization – the ubiquitous learning paradigm – is the data samples being in-dependently and identically distributed (IID) [21]. This as-sumption may not hold in real data owing to various factors such as the addition of new classes of data, the removal of old data due to memory or availability constraints, and the changes in the data-generating phenomena (concept drift).
As a result, neural networks may experience catastrophic forgetting where the network forgets the previously learned knowledge upon acquiring new knowledge [25].
“Continual Learning” is a field of research pursuing mechanisms to mitigate this forgetting [13].
In this manuscript, we focus on one paradigm of continual learn-ing, named Class Incremental Learning (CIL) [38]. In CIL, a neural network is trained over a series of tasks and at each task, the network learns a new set of classes. At any given time, the network should classify between all learned classes thus far. Among the techniques proposed for CIL, rehearsal-based methods have demonstrated promising re-sults in mitigating forgetting by storing exemplars (old sam-ples) and reusing them while learning new tasks [38, 5, 36].
However, such storage is not always possible due to mem-ory limitations and privacy constraints [45]. Therefore, we focus on Non-Exemplar based CIL (NECIL), a more prag-matic yet challenging scenario, which attempts to preserve the old knowledge without storing any exemplars [57, 58].
NECIL methods often struggle with overlapping old and new class representations due to the unavailability of ex-emplars, resulting in catastrophic forgetting. [57]. While prototypes of old classes in the deep feature space are a vi-able alternative to reusing exemplars [57, 56], if not prop-erly generated, the class boundaries refined using such pro-totypes tend to be muddled, causing confusion between the old and new classes, and in turn, leading to catastrophic for-getting. To overcome this limitation, we propose NAPA-VQ: Neighborhood-Aware Prototype Augmentation with
Vector Quantization framework. NAPA-VQ not only pro-poses a novel way to create prototypes of old classes by con-sidering class neighborhoods but also incorporates a novel quantization mechanism to create clearer class boundaries by enforcing a strong separation between the neighboring classes. This reduction in representation overlap effectively mitigates catastrophic forgetting. NAPA-VQ builds on the principles of unsupervised Neural Gas (NG) [49] and super-vised Learning Vector Quantization (LVQ) [26] to facilitate this neighborhood awareness and enforce more discrimina-tive boundaries. (I) two components:
NAPA-VQ contains a
Neighborhood-aware Vector Quantizer (NA-VQ) and (II) a Neighborhood-aware Prototype Augmenter (NA-PA).
NA-VQ learns the topology of the feature space manifold
Z which is the output of a deep feature extractor (e.g.
ResNet), identifying the neighboring classes that share similar features and are hence prone to get confused with each other. This knowledge of the neighboring classes and their class distributions is utilized by NA-VQ to increase their separability and by NA-PA for old class prototype to generate surrogate exemplars to augmentation, facilitate the retention of old information when new classes are being learned.
To summarize, i.e.,
• We propose an improved supervised vector quantiza-tion method to discretize the latent space and improve class separation.
• We propose a prototype augmentation method that uses the topological information of classes in the latent space to avoid confusion between classes and catas-trophic forgetting.
• We demonstrate the utility of the above two contri-butions combined in NECIL, obtaining superior per-formance compared to the existing NECIL methods on CIFAR-100, TinyImageNet, ImageNet-Subset, and
ImageNet-1k datasets. 2.