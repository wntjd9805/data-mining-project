Abstract
Single-Camera
Dense 3D reconstruction and ego-motion estimation are key challenges in autonomous driving and robotics. Com-pared to the complex, multi-modal systems deployed today, multi-camera systems provide a simpler, low-cost alterna-tive. However, camera-based 3D reconstruction of com-plex dynamic scenes has proven extremely difficult, as ex-isting solutions often produce incomplete or incoherent re-sults. We propose R3D3, a multi-camera system for dense 3D reconstruction and ego-motion estimation. Our ap-proach iterates between geometric estimation that exploits spatial-temporal information from multiple cameras, and monocular depth refinement. We integrate multi-camera feature correlation and dense bundle adjustment operators that yield robust geometric depth and pose estimates. To improve reconstruction where geometric depth is unreli-able, e.g. for moving objects or low-textured regions, we introduce learnable scene priors via a depth refinement net-work. We show that this design enables a dense, consistent 3D reconstruction of challenging, dynamic outdoor envi-ronments. Consequently, we achieve state-of-the-art dense depth prediction on the DDAD and NuScenes benchmarks. 1.

Introduction
Translating sensory inputs into a dense 3D reconstruc-tion of the environment and tracking the position of the ob-server is a cornerstone of robotics and fundamental to the development of autonomous vehicles (AVs). Contemporary systems rely on fusing many sensor modalities like cam-era, LiDAR, RADAR, IMU and more, making hardware and software stacks complex and expensive.
In contrast, multi-camera systems provide a simpler, low-cost alterna-tive already widely available in modern consumer vehicles.
However, image-based dense 3D reconstruction and ego-motion estimation of large-scale, dynamic scenes is an open research problem as moving objects, uniform and repetitive textures, and optical degradations pose significant algorith-mic challenges.
*Equal contribution.
Multi-Camera
R3D3 (Ours)
Figure 1. Many methods use temporal context but neglect inter-camera information, leading to incomplete results (top). Other works focus on exploiting inter-camera context but neglect tempo-ral information, yielding incoherent predictions (middle). In con-trast, our method achieves a consistent, dense 3D reconstruction by iteratively integrating geometric depth estimation from multi-ple cameras with monocular depth refinement (bottom).
Existing works approaching the aforementioned task can be divided into two lines of research. Many methods have focused on recovering 3D scene structure via structure-from-motion (SfM). In particular, simultaneous localiza-tion and mapping (SLAM) methods focus on accurate ego-motion estimation and usually only recover sparse 3D struc-ture [9, 29, 34, 12, 11]. They typically treat dynamic objects or uniformly textured regions as outliers yielding an incom-plete 3D reconstruction result, which makes them less suit-able for AVs and robotics. In addition, only a few works have focused on multi-camera setups [24, 8, 33, 30, 26]. In contrast, multi-view stereo (MVS) methods [31, 32, 41, 28,
57, 27, 17] aim to estimate dense 3D geometry but focus on static scenes and highly overlapping sets of images with known poses.
The second line of research focuses on dense depth pre-diction from monocular cues, such as perspective object ap-pearance and scene context [65, 15, 16, 59, 49, 61, 19].
However, due to the injective property of projecting 3D structures onto a 2D plane, reconstructing depth from a sin-gle image is an ill-posed problem, which limits the accu-racy and generalization of these methods. Recent meth-ods [51, 18, 21], inspired by MVS literature, combine monocular cues with temporal context but are focused on front-facing, single-camera setups. A handful of recent works extend monocular depth estimation to multi-camera setups [22, 52, 54]. These methods utilize the spatial con-text to improve accuracy and realize absolute scale depth learning. However, those works neglect the temporal do-main which provides useful cues for depth estimation.
Motivated by this observation, we introduce R3D3, a system for dense 3D reconstruction and ego-motion esti-mation from multiple cameras of dynamic outdoor environ-ments. Our approach combines monocular cues with geo-metric depth estimates from both spatial inter-camera con-text as well as inter- and intra-camera temporal context. We compute accurate geometric depth and pose estimates via iterative dense correspondence on frames in a co-visibility graph. For this, we extend the dense bundle adjustment (DBA) operator in [48] to multi-camera setups, increasing robustness and recovering absolute scene scale. To deter-mine co-visible frames across cameras, we propose a sim-ple yet effective multi-camera algorithm that balances per-formance and efficiency. A depth refinement network takes geometric depth and uncertainty as input and produces a re-fined depth that improves the reconstruction of, e.g., moving objects and uniformly textured areas. We train this network on real-world driving data without requiring any LiDAR ground-truth. Finally, the refined depth estimates serve as the basis for the next iterations of geometric estimation, thus closing the loop between incremental geometric reconstruc-tion and monocular depth estimation.
We summarize our contributions as follows. 1) we pro-pose R3D3, a system for dense 3D reconstruction and ego-motion estimation in dynamic scenes, 2) we estimate geo-metric depth and poses with a novel multi-camera DBA for-mulation and a multi-camera co-visibility graph, 3) we inte-grate prior geometric depth and uncertainty with monocular cues via a depth refinement network.
As a result, we achieve state-of-the-art performance across two widely used multi-camera depth estimation benchmarks, namely DDAD [19] and NuScenes [3]. Fur-ther, we show that our system exhibits superior accu-racy and robustness compared to monocular SLAM meth-ods [48, 5]. 2.