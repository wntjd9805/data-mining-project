Abstract
Conventional semi-supervised learning (SSL) lies in the close-set assumption that the labeled and unlabeled sets contain data with the same seen classes, called in-distribution (ID) data.
In contrast, safe SSL investigates a more challenging open-set problem where unlabeled set may involve some out-of-distribution (OOD) data with un-seen classes, which could harm the performance of SSL.
When we are experimenting with the mainstream safe SSL methods, we have a surprising finding that all OOD data show a clear tendency to gather in the feature space. This inspires us to solve the safe SSL problem from a fresh per-spective. Specifically, for a classification task with K seen classes, we utilize a prototype network not only to generate
K prototypes of all seen classes, but also explicitly model an additional prototype for the OOD data, transferring the K-way classification on the open-set to the (K+1)-way on the close-set. In this way, the typical SSL techniques (e.g., con-sistency regularization and pseudo labeling) can be applied to tackle the safe SSL problem without additional consider-ation of OOD data processing like other safe SSL methods do. Particularly, considering the possible low-confidence pseudo labels, we further propose an iterative negative learning (INL) paradigm to enforce the network learning knowledge from complementary labels on wider classes, im-proving the network’s classification performance. Extensive experiments on four benchmark datasets show that our ap-proach remarkably lifts the performance on safe SSL and outperforms the state-of-the-art methods. 1.

Introduction
In the past decade, the development of deep learning brings prosperity to the field of computer vision, such as image classification, attributed to the growing scale of la-*Equal contribution.
†Corresponding author: Yan Wang (wangyanscu@hotmail.com).
Figure 1. (a) An example of close-set and open-set. In open-set, the unlabeled set contains classes not seen in the labeled set (indi-cated by red boxes); (b) t-SNE [34] visualization of feature distri-butions of ID (colored) data and OOD (black) data from CIFAR10
[17], Images of the same category are shown in the same color. beled data [18, 9, 23]. However, it is time-consuming and expensive to collect large amounts of labeled data. Numer-ous methods resort to semi-supervised learning (SSL) to re-lieve this restriction [33, 19, 32]. By exploring the massive valuable information from the abundant easy-to-acquire un-labeled data, SSL methods can effectively narrow the per-formance gap towards the fully-supervised models.
Although the SSL methods are proven to be effective in improving the classification performance in case of sparse labels, most of them follow a close-set assumption by de-fault, that is, the labeled data and the unlabeled data share the same label space. See Figure 1(a) for example, both la-beled data and unlabeled data are built on the classes of cat and dog, which are also called in-distribution (ID) data. Yet, in many real scenarios, such a simple and crude assumption often breaks down since the unlabeled data could be col-lected in the wild [40]. In this case, there will be many sam-ples with unknown classes (or unseen classes) appearing in the unlabeled dataset, which we call out-of-distribution (OOD) data, resulting in an open-set problem. As shown in
Figure 1, in the open-set, besides the cat and dog classes in the labeled set (or seen classes), there are also some unseen classes, e.g., butterfly, hen, rabbit. Without reliable labels for these unseen classes, the OOD data may lead to a sig-nificant degradation of model performance [8, 4, 25], and even pose a significant safety risk to the accurate predic-tion since the model is forced to predict the unseen class as a seen one. This severely hinders the SSL methods from being deployed in real-world applications.
To handle this problem, a series of enhanced SSL meth-ods have been emerged to improve the classification perfor-mance and guarantee the prediction safety in a more real-istic scenario, namely safe semi-supervised learning (safe
SSL) methods [8, 4, 10, 11, 41]. The safe here means that the model using extra unlabeled data will not be worse than a simple supervised one. Most current safe SSL methods adhere a two-step scheme: 1) identifying the OOD data, and 2) dealing with the OOD data. In stage one, these methods tend to identify the OOD data by classifying the OOD data and the ID data into two separate broad classes [8, 4, 10, 41].
After the OOD data is detected, some methods treat them as hazards and just discard them in the second stage [4, 41]. In contrast, He et al. [10] argued that the valuable information contained in the OOD data can be used to further improve the identification of OOD data. To this end, they endeav-ored to calibrate the seen-class probability distribution into a uniform distribution, thus suppressing the over-confidence problem to unseen class and eliminating the risk of hard
OOD data being recognized as ID data.
Despite the effectiveness of the two-step pipeline, uti-lizing it in reality might still be cumbersome and requires consideration of some additional caveats. For instance, in stage one, clustering all ID data into one class is a coarse-grained binary classification task, which may conflict with the final fine-grained multi-class classification task in terms of feature space learning [14]. Furthermore, in stage two, it is not easy to come up with a feasible way to utilize OOD data to enhance model performance, either. When we are experimenting with these safe SSL methods, an interesting phenomenon strikes us as a surprise. We find that most safe
SSL methods [8, 10, 14] show an intrinsic ability to gather the OOD features in the first stage like Figure 1(b), espe-cially for [14] employing the self-supervised learning man-ner. This involkes us to ask the following question: Why the
OOD data which belong to different classes are gathered in the feature space?
We try to answer this question by inspecting the Class
Activation Map (CAM) [44] for ID and OOD images. As observed in Figure 2, the model is mainly concerned with the high-level information of the objects (e.g. the body of cats and birds) for ID data. For OOD data, on the other hand, due to the lack of corresponding categories of ob-jects, the model can only pay more attention to some com-mon category-agnostic low-level information, such as color, edge, and corner. Therefore, although the OOD data come from different classes, their features still present a gathering tendency in feature space.
Figure 2. CAM visualization of the results. For the ID data (Bird,
Cat, Dog), the features are more class-discriminative.
This inspires us to solve the safe SSL problem from a new viewpoint, that is we can treat all the OOD data as an individual class peer to other seen classes during training. In this way, we can transfer a K-way classification task on the open-set to a simple (K+1)-way classification task on the close-set. Then we can tackle the complex safe SSL task in the same way as the trivial SSL task using all the meth-ods that are applicable on the close-set assumption, greatly streamlining the safe SSL problem.
Following the above inspiration, in this paper, we pro-pose a prototype-based safe SSL framework for image classification from a fresh perspective. Specifically, our framework includes two stages, including the first stage of unseen-class prototype generation and the second stage of semi-supervised image classification. Assuming that there are K seen classes, the first stage starts by training a pro-totype network [31] on the labeled ID data so that it can produce K prototypes of seen classes. Based on the proto-types, we can distinguish the feature distribution of all the
OOD data according to the distance of the features from the prototypes, thereby explicitly modeling an additional proto-type of all unseen classes. These K+1 prototypes allow the framework to make predictions for both the K fine-grained seen classes and the remaining unseen class simultaneously, avoiding the conflict with coarse-grained classification. By casting the K-way classification to the (K+1)-way clas-sification, we can directly use any common and effective
SSL technique to carry out the semi-supervised classifica-tion task in the second stage.
A widely accepted solution for SSL is to filter high con-fidence pseudo labels for unlabeled data. Yet, removing all the unreliable predictions could lead to insufficient and cat-egorically imbalanced training [37]. To make full use of un-labeled data, negative learning (NL) [37, 38, 16] is proposed to provide complementary labels which indicate the class to which the current sample is least likely to belong, namely negative class. However, the conventional NL may become less powerful when solving a classification task with a large number of classes. The reasons can be attributed to the fol-lowing aspects: 1) Current NL methods don’t ensure the complementary label of the same sample keep unchanged during the training process. Wavering labels are unfavor-able for stable training; 2) When there are excessive classes, current NL methods can only determine the complementary labels on one negative class at a time, which can solely sup-plement limited knowledge to the network and leave mas-sive information on other classes unexplored. To alleviate these limitations, we propose an iterative negative learning (INL) paradigm in the second stage. Unlike the previous
NL methods [16, 37], we harness a memory bank to pre-serve the complementary labels of unlabeled data at each training iteration. As training goes, the complementary la-bels will explore new negative classes iteratively based on the saved historical complementary labels. By this, the INL paradigm enables to fully unearth the knowledge of all un-labeled data in the whole feature space, helping the model classify the samples more confidently. We summarize our contributions as follows:
• We rethink the safe SSL problem from a fresh perspec-tive and propose a prototype-based safe SSL frame-work to explicitly model the OOD data as a novel class peer to ID classes. In this manner, we transform the safe SSL from an open-set problem to a close-set one.
• We raise an INL paradigm to enhance the feature learn-ing capability of our framework with regard to the un-reliable predictions in SSL. By employing a memory bank to progressively update the complementary label until it covers most negative classes, our model can excavate the knowledge of the unlabeled data in the whole feature space, producing more confident classi-fication results.
• We evaluate our framework on extensive benchmark datasets and the experimental results show that our method remarkably outperforms the existing state-of-the-art safe SSL methods. 2.