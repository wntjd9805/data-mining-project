Abstract
Transparent objects are commonly seen in indoor scenes but are hard to estimate. Currently, commercial depth cameras face difficulties in estimating the depth of transparent objects due to the light reflection and refraction on their surface. As a result, they tend to make a noisy and incorrect depth value for transparent objects. These incorrect depth data make the traditional RGB-D SLAM method fails in reconstructing the scenes that contain transparent objects. An exact depth value of the transparent object is required to restore in advance and it is essential that the depth value of the transparent object must keep consistent in different views, or the recon-struction result will be distorted. Previous depth prediction methods of transparent objects can restore these missing depth values but none of them can provide a good result in reconstruction due to the inconsistency prediction. In this work, we propose a real-time reconstruction method using a novel stereo-based depth prediction network to keep the con-sistency of depth prediction in a sequence of images. Because there is no video dataset about transparent objects currently to train our model, we construct a synthetic RGB-D video dataset with different transparent objects. Moreover, to test generalization capability, we capture video from real scenes using the RealSense D435i RGB-D camera. We compare the metrics on our dataset and SLAM reconstruction results in both synthetic scenes and real scenes with the previous methods. Experiments show our significant improvement in accuracy on depth prediction and scene reconstruction. 1.

Introduction
Transparent objects frequently appear in our daily life, in-cluding glasses, goblets, plastic objects, vases, etc. Because of the nature of transparent materials, the light will reflect and refract on the surface of transparent objects, which will cause errors for the commonly used depth camera. Fig. 2 show this ambiguity: the depth captured by the camera is the
*Bo Ren is the corresponding author (a) RGB (b) ClearGrasp + ElasticFusion (c) TransCG + ElasticFusion (d) Ours
Figure 1. RGB-D reconstruction result comparison in the real scene. Subfigures (b) and (c) show the reconstruction result using
ElasticFusion [38] with predicted depth data from the state-of-the-art transparent object depth prediction method: ClearGrasp [31] and TransCG [8]. Subfigure (d) shows our reconstruction result. In (b) and (c), the transparent object can not be well reconstructed.
Compared with them, our model performs well in reconstruction. distorted result of the object behind.
SLAM(Simultaneous localization and mapping) is a popular topic in computer vision and robotic fields. There are lots of
SLAM methods [7, 25, 38] that take RGB-D as their input to reconstruct the scene and recover the motion of the camera.
The RealSense [19] and Kinect [12] are often used to capture the RGB-D data and are common input devices for RGB-D
SLAM. However, due to the reason we state above, these cameras can not estimate the depth of transparent objects. So, restoring the correct depth of transparent objects is critical in reconstruction.
Many depth completion methods for indoor scenes train and test in the TUM dataset [34], which do not contain transpar-ent objects. Also, there are many attempts [8, 31, 44] to try to predict the depth of transparent objects using different deep neural network models. However, these monocular-based methods perform poorly when using their predicted depth in
(a) (d) (b) (e) (c) (f)
Figure 2. Effect of the transparent object. The left figure shows that the ray path changes when there is a transparent object, which causes inaccurate depth data. In (a), a red box shows where the transparent object is located. (b) shows the depth data from the depth camera. The region around the transparent object is noisy and incorrect and (c) shows the reconstruction result using these depth data. The transparent object is missing. (d) and (e) are our depth prediction results of the transparent object in different views. (f) is our reconstruction result. the SLAM reconstruction. We attribute it to the depth incon-sistency problem between frames. Estimating the consistent depth that can be used for SLAM is a challenge. Previous work like [3, 9, 23, 35] has mentioned the similar problem.
Since depth between frames is strictly constrained by camera trajectory, monocular depth estimation can not ensure depth predicted under the same absolute scale. So instead of using a single frame to predict the depth of the transparent object, we choose multiple views as our input and build a connec-tion between adjacent RGB images to keep consistency in prediction.
In this paper, we propose a stereo-based transparent depth prediction model based on DPSNet [17] to keep consistency in prediction, which can be directly used for the SLAM reconstruction of transparent objects. Our model uses the plane sweep stereo module to build a spatial consistency restriction between sequential input images. To obtain scale information on the more-accurate background depth, we separate the transparent object using the predicted mask and transform it to point clouds. We use a designed lite PointNet based on [27, 28] to learn this information and combine it into our model. We further design another surface normal prediction branch to assist and refine our depth prediction result. The detail of our model can be found in Sec. 3. To make a real-time reconstruction of the transparent object, we embed our network into ElasticFusion [38] as our pseudo-SLAM method in Sec. 4.
There are many datasets for the transparent object, such as
ClearGrasp dataset [31], TransCG dataset [8], and Omni-verse Object dataset [44]. However, all of them contain only single image segmentation data and do not have video seg-mentation data of transparent objects. In [45], a natural scene video dataset for the transparent object is proposed but needs the ground truth depth value of transparent objects for model training. So we use Blender to create a synthetic dataset for our depth prediction model. Besides, we use the RGB-D camera to capture real scene data to evaluate the generaliza-tion capability of our model and test the 3D reconstruction of these scenes. We also test the reconstruction result using the previous depth prediction method of the transparent object in our pseudo-SLAM pipeline. Fig. 1 shows the result com-parison. Besides this figure, a variety of experiments in Sec. 5 demonstrate that our proposed method makes a significant improvement.
In summary, our contributions are:
• We propose a real-time 3D transparent object recon-struction method without any prior knowledge require-ment of the scene using a stereo-based depth prediction model.
• We construct a new RGB-D dataset of transparent ob-jects with ground truth depth data for model training to predict the depth of transparent objects in continuous
RGB-D data sequences. 2.