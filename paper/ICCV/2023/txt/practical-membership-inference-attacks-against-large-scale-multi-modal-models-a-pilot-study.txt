Abstract 1.

Introduction
Membership inference attacks (MIAs) aim to infer whether a data point has been used to train a machine learning model. These attacks can be employed to iden-tify potential privacy vulnerabilities and detect unautho-rized use of personal data. While MIAs have been tradi-tionally studied for simple classification models, recent ad-vancements in multi-modal pre-training, such as CLIP, have demonstrated remarkable zero-shot performance across a range of computer vision tasks. However, the sheer scale of data and models presents significant computational chal-lenges for performing the attacks.
This paper takes a first step towards developing practi-cal MIAs against large-scale multi-modal models. We intro-duce a simple baseline strategy by thresholding the cosine similarity between text and image features of a target point and propose further enhancing the baseline by aggregating cosine similarity across transformations of the target. We also present a new weakly supervised attack method that leverages ground-truth non-members (e.g., obtained by us-ing the publication date of a target model and the times-tamps of the open data) to further enhance the attack. Our evaluation shows that CLIP models are susceptible to our attack strategies, with our simple baseline achieving over 75% membership identification accuracy. Furthermore, our enhanced attacks outperform the baseline across multiple models and datasets, with the weakly supervised attack demonstrating an average-case performance improvement of 17% and being at least 7X more effective at low false-positive rates. These findings highlight the importance of protecting the privacy of multi-modal foundational mod-els, which were previously assumed to be less susceptible to MIAs due to less overfitting. Our code is available at https://github.com/ruoxi-jia-group/CLIP-MIA.
Membership inference attack (MIA) is a type of privacy attack that attempts to determine if a specific data point was used to train a machine learning model [14]. This type of at-tack can compromise the privacy of individuals whose data was used to train the models [14], but can be also used to identify vulnerabilities, privacy leakage, and unauthorized use of personal data in machine learning models [21, 31].
Moreover, the rise of a foundational model trained on vast amounts of open data has highlighted the potential breach of contextual integrity, a fundamental principle in legal discussions of privacy [22]. MIA can, therefore, be used as an effective tool for individuals to check if compa-nies store their personal information and request its deletion to comply with the European General Data Protection Reg-ulation (GDPR).
Existing MIAs that achieve advanced performance rely on the idea of shadow training [4, 35]. The shadow mod-els are usually at a scale of hundreds [4] and are with the same or similar architecture as the target model. The train-ing algorithm needs to be the same as the one that trains the target model. The difference between shadow models that contain a certain sample and those that do not is then uti-lized to learn proper features to identify the membership of an individual sample. However, reliance on shadow training forecloses its application to large-scale models. As a con-crete example, training Contrastive Language-Image Pre-Training (CLIP) [23], a cutting-edge multi-modal learning paradigm, takes 18 days even with hundreds of advanced
GPUs [10]. This makes shadow training to attack CLIP prohibitively expensive. In addition, obtaining full details of the training algorithm performed by these state-of-the-art models is often difficult, as they are considered intellectual property and are not published.
This paper is a pilot study of practical MIAs against large-scale multi-modal models. The proposed techniques bypass the shadow training and work with only black-box access. We use CLIP as an example for the extensive evaluation, due to the following reasons: First, CLIP is
widely used for zero-shot learning in various computer vi-sion tasks [25, 24, 8], which entails the understanding of its privacy risk. Also, CLIP has been trained on massive data scraped from the Internet with undisclosed algorithms, thus exemplifying the scale and the threat model for other emer-gent foundation models [3]. Our contributions are summa-rized as follows. (1) Benchmarking the susceptibility of CLIP to mem-bership inference. We introduce a simple baseline attack that identifies membership based on cosine similarity (CS) between image and text features, akin to those MIAs de-signed for single-modal models based on loss or confidence scores in prior literature [37, 19, 32]. The rationale for us-ing CS as a signal for membership inference is that CLIP is trained to maximize CS on training samples, which can re-sult in members having higher CS than non-members. This simple method achieves a reasonable membership identifi-cation accuracy ranging from 66.5% to 78.8% but has lim-ited performance in the low false-positive regime. (2) Improving the CS-based attack via target data augmentations. We develop an enhanced MIA technique that involves applying transformations to a specific point and aggregating the changes in CS across various com-mon transformations (e.g., resizing, cropping, rotation, and translation). Our evaluation indicates that this method consistently improves attack performance across multiple datasets and CLIP model architectures, albeit to a small ex-tent. The inspiration for this technique stems from our em-pirical observations that training points experience a larger
CS drop than non-training ones when subject to transforma-tions. (3) Developing a new weakly supervised MIA frame-work given one-sided non-member information. Both the baseline attack and the augmentation-enhanced attack demonstrate high performance without requiring ancillary information, but their accuracy is limited at false positive rates (e.g., less than 10% true positive rate at a false pos-itive rate of 1%). In this paper, we identify a new threat model that is plausible for models trained on Internet data.
Specifically, we consider a scenario where the attacker has one-sided knowledge about non-members. By scraping In-ternet data posted after the target modelâ€™s publication date, the attacker can acquire a set of data guaranteed not to have participated in the training. We propose a weakly su-pervised attack that utilizes this non-member set to con-struct a model that predicts membership. This approach shows remarkable attack performance, improving the base-line accuracy by 17% and being around 7X more effective at low false-positive rates, despite the absence of information about members. (4) Exploring potential defenses. Our findings demon-strate the vulnerability of large-scale multi-modal models to membership inference risks. To address this issue, we investigate potential defenses and their associated privacy-utility tradeoff. This pilot study serves as a starting point for assessing the privacy risks associated with emerging large-scale multi-modal models. 2.