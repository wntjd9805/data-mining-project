Abstract
Despite the success in large-scale text-to-image gener-ation and text-conditioned image editing, existing methods still struggle to produce consistent generation and editing results. For example, generation approaches usually fail to synthesize multiple images of the same objects/characters but with different views or poses. Meanwhile, existing edit-ing methods either fail to achieve effective complex non-rigid editing while maintaining the overall textures and identity, or require time-consuming fine-tuning to capture the image-specific appearance. In this paper, we develop
MasaCtrl, a tuning-free method to achieve consistent im-age generation and complex non-rigid image editing si-multaneously. Specifically, MasaCtrl converts existing self-attention in diffusion models into mutual self-attention, so that it can query correlated local contents and textures from source images for consistency. To further alleviate the query
*Work done during an internship at ARC Lab, Tencent PCG. confusion between foreground and background, we pro-pose a mask-guided mutual self-attention strategy, where the mask can be easily extracted from the cross-attention maps. Extensive experiments show that the proposed Mas-aCtrl can produce impressive results in both consistent im-age generation and complex non-rigid real image editing. 1.

Introduction
Recent advances in text-to-image (T2I) generation [25, 19, 41, 24, 27] have achieved great success. Those large-scale T2I models, such as Stable Diffusion [27], can gen-erate diverse and high-quality images conforming to given text prompts. When leveraging the T2I models, we can also perform promising text-conditioned image editing [19, 10, 35, 21]. However, there is still a large gap between our needs and existing methods in terms of consistent genera-tion and editing. 1
For the text-to-image generation, we usually want to generate several images of the same objects/characters but with different views or complex non-rigid variances (e.g., the changes of posture). Such capabilities are urgently needed for creating comic books and generating short videos using existing powerful T2I image models. How-ever, this requirement is highly challenging. Even if we fix the input random noise and use very similar prompts (e.g.,
‘a sitting cat’ vs. ‘a laying cat’ shown in Fig. 2), the gener-ated images vary in both structures and identity.
For text-conditioned image editing, existing meth-ods [10, 35, 21] achieve impressive editing effects in image translation, stylization, and appearance replacement while keeping the input structure and scene layout unchanged.
However, those methods usually fail to change poses or views while maintaining the overall textures and identity, leading to inconsistent editing results. The latter editing way is a more complicated non-rigid editing for practical use. Imagic [12] is then proposed to address this challenge.
It allows complex non-rigid edits while preserving its orig-inal characteristics. It can make a standing dog sit down, cause a bird to spread its wings, etc. Nevertheless, it re-quires fine-tuning the entire T2I diffusion model and opti-mizing the textual embedding to capture the image-specific appearance for each edit, which is time-consuming and im-practical for real-world applications.
In this paper, we aim to develop a tuning-free method to address the above challenges, enabling a more consis-tent generation of multiple images and complex non-rigid editing without fine-tuning. The core challenge is how to keep consistent. Unlike previous works [10, 21, 3] that usu-ally operate on cross-attention in T2I models, we propose to convert existing self-attention to mutual self-attention, so that it can query correlated local structures and textures from a source image for consistency. Specifically, we first generate an image from a random (or inverted a real im-age) noise, resulting in the denoising process (DP1) for the source image synthesis. In the new denoising process (DP2) of generating a new image or editing an existing one, we can use the Query features in DP2 self-attentions to query the corresponding Key and Value features in DP1 self-attentions. In other words, we transform the existing self-attention into ‘cross-attention’, where the crossing op-eration happens in the self-attentions of two related denois-ing processes, rather than between the U-Net features and text embeddings. We call this ‘crossing self-attention as mutual self-attention. However, directly applying this strat-egy can only generate images almost identical to the source image and cannot comply with the target text prompt (as analyzed in Fig. 9). Thus we further control the denoising timestep and the layer position in U-Net for performing mu-tual self-attention to achieve consistent synthesis and edit-In this ing. More analyses are in Sec. 4.1 and Sec. 5.5.
Figure 2: First row: images synthesized from fixed ran-dom seed (middle, changed identity) and our method (right, maintained identity). Second row: image synthesized with-out mask guidance (middle) and with mask guidance (right). way, we can use contents in the source image as the gen-eration material to better maintain the texture and identity.
Meanwhile, its structure, pose, and non-rigid variances can be controlled by target text prompt, and guided by recent controllable T2I-Adapters [17] or ControlNet [45]. Fig. 1 shows some synthesis and editing examples.
The proposed mutual self-attention control can work well for images with disentangled foreground and back-ground, but may fail in the synthesis and editing pro-cess where the foreground and background have similar patterns and colors.
In these cases, mutual self-attention tends to confuse the foreground and background, leading to a messy result (2nd row in Fig. 2). To address this problem, we further propose a mask-guided mutual self-attention. Specifically, we first utilize the cross attention in T2I diffusion models to extract a mask associated with the main object in the image. This mask can success-fully separate foreground and background, and restrict the target foreground/background features to query only fore-ground/background features of the source image, respec-tively. Such an operation can effectively alleviate the query confusion between the foreground and background.
Our contributions can be summarized as follows. 1)
We propose a tuning-free method, namely, MasaCtrl, to si-multaneously achieve consistent image synthesis and com-plex non-rigid image editing. 2) An effective mutual self-attention mechanism with delicate designs is proposed to change pose, view, structures, and non-rigid variances while maintaining the characteristics, texture, and identity. 3) To alleviate the query confusion between foreground and back-ground, we further propose a masked-guided mutual self-attention, where the mask can be easily computed from the 2
cross-attentions. 4) Experimental results have shown the ef-fectiveness of our proposed MasaCtrl in both consistent im-age generation and complex non-rigid real image editing. 2.