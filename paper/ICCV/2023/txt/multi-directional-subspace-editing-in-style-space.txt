Abstract our model with respect to those measures1.
This paper describes a new technique for finding disen-tangled semantic directions in the latent space of StyleGAN.
Our method identifies meaningful orthogonal subspaces that allow editing of one human face attribute, while min-imizing undesired changes in other attributes. Our model is capable of editing a single attribute in multiple direc-tions, resulting in a range of possible generated images. We compare our scheme with three state-of-the-art models and show that our method outperforms them in terms of face editing and disentanglement capabilities. Additionally, we suggest quantitative measures for evaluating attribute sep-aration and disentanglement, and exhibit the superiority of 1.

Introduction
Recent developments in computer vision have enabled the generation of photorealistic, high-resolution synthetic images. The most notable technique is Generative Adver-sarial Networks (GANs) [11], which have advanced the progress of many applications, including image generation, super-resolution, image inpainting, and more. In particu-lar, the generator of StyleGAN [1], one of the most notable
GAN models, has been extensively explored by researchers.
StyleGAN samples a latent vector z ∈ R512 from a 1Project page and code are available at https://chennaveh. github.io/MDSE/
Gaussian distribution N (0, I), and maps it to an interme-diate vector w ∈ W = R512 using a mapping network.
This vector is then used to generate a 1024x1024 RGB im-age. The vector w is inserted into 18 multi-resolution style blocks that control various characteristics of the synthe-sized image. Vectors at lower resolutions determine high-level features such as pose and hair. Intermediate resolu-tions affect facial expressions, while vectors at higher res-olutions dictate fine details like colors and texture. The original StyleGAN model uses the W latent space and the same w vector for all 18 style blocks. Subsequent stud-ies [1, 2, 26, 32] utilize the W + = R18·512 space, which ex-tends the W space, and applies different w vectors to differ-ent style blocks. This allows for enhanced control over the resulting image. The W + latent space is commonly used for inverse mapping, converting an image into w ∈ W +.
Recent works have explored methods for gaining con-trol over the synthesized images by altering the latent space vectors [3, 7, 12, 25, 28, 29]. For instance, shifting the la-tent vector of a generated image towards the direction cor-responding to “smile” can progressively amplify the smile in the output image. While these methods showcase im-pressive and realistic editing capabilities, pinpointing the attribute directions remains a challenge. One approach is to use vectors in latent space sampled from the GAN model combined with attribute ground truth (either manually la-beled or determined using a pre-trained attribute estima-tor) [10, 16, 28]. A linear classifier in the latent space is then employed. The normal to the classification bound-ary corresponds to the most significant direction of the at-tribute. These models often suffer from issues of entangled data and biases in the training set. Such biases might re-sult in attribute correlations, like glasses with age or beard with gender. Consequently, adding glasses to a face may inadvertently age it. Other methods use unsupervised tech-niques [12, 29], such as PCA, to find meaningful orthogo-nal directions in the latent space. However, these directions typically require subjective human post-annotation to link directions to attributes in the synthesized images. All these models share the notion of a singular direction for each at-tribute, despite the possibility of some attributes defined by multiple dimensions (e.g., age might be affected by factors like hair color and skin wrinkles, see Fig. 1).
In this work, we present MDSE, an acronym for Multi-Directional Subspace Editing. This framework aims to identify orthogonal semantics within the latent space of a pre-trained StyleGAN. Our goal is to discover meaningful subspaces that are mutually orthogonal, with each subspace controlling specific facial attributes.
“Multi-directional editing” means altering a latent vector within a particular subspace in various directions. This approach enables the generation of a wide range of images, each varying in a specific attribute. Furthermore, because the subspaces are mutually orthogonal, modifications in one attribute result in minimal changes to others. We explore the disentangle-ment capabilities of our model and quantitatively assess its performance in comparison to leading image editing tech-niques. 2.