Abstract
Continual semantic segmentation (CSS) aims to extend an existing model to tackle unseen tasks while retaining its old knowledge. Naively fine-tuning the old model on new data leads to catastrophic forgetting. A common solution is knowledge distillation (KD), where the output distribution of the new model is regularized to be similar to that of the old model. However, in CSS, this is challenging because of the background shift issue. Existing KD-based CSS meth-ods continue to suffer from confusion between the back-ground and novel classes since they fail to establish a re-liable class correspondence for distillation. To address this issue, we propose a new label-guided knowledge distilla-tion (LGKD) loss, where the old model output is expanded and transplanted (with the guidance of the ground truth la-bel) to form a semantically appropriate class correspon-dence with the new model output. Consequently, the useful knowledge from the old model can be effectively distilled into the new model without causing confusion. We con-duct extensive experiments on two prevailing CSS bench-marks, Pascal-VOC and ADE20K, where our LGKD sig-nificantly boosts the performance of three competing meth-ods, especially on novel mIoU by up to +76%, setting new state-of-the-art. Finally, to further demonstrate its gen-eralization ability, we introduce the first CSS benchmark for 3D point cloud based on ScanNet, along with several re-implemented baselines for comparison. Experiments show that LGKD is versatile in both 2D and 3D modali-ties without requiring ad hoc design. Codes are available at https://github.com/Ze-Yang/LGKD. 1.

Introduction
Fully supervised semantic segmentation has witnessed tremendous success [33, 63, 27, 8, 16, 22, 47, 65, 9] in re-*Corresponding author: Guosheng Lin
Figure 1. Illustration of knowledge distillation (KD) strategies in continual semantic segmentation. Standard KD and unbiased KD reduce the new class (bicycle) dimension(s) of the probabilities output by the new model via remove and combine respectively, which collapses the class correspondence across incremental steps and raises confusion between background and new classes (see
Sec. 1). In contrast, our label-guided KD uses the ground truth label as guidance to expand the probabilities predicted by the old model. It builds a reliable class correspondence across different learning steps without discrepancy (ILT) or entanglement (MiB) (see Sec. 3.2). A mimic===⇒ B: encourage A to be similar to B. cent years. These algorithms generally assume a fixed num-ber of classes to be learned. However, in real-world appli-cations, it is often expected that a deployed model can be continuously generalized to handle new classes while not forgetting the old ones. A simple solution is to expand the original dataset with newly available samples and retrain a new model from scratch, dubbed as Joint Training. Ob-viously, this is computationally expensive and requires an increasing amount of space to store the old data over time.
Further, it may raise privacy issues in some circumstances, e.g., medical images and face data.
To address this problem, continual semantic segmenta-tion (CSS) has been proposed by [35] as an emerging re-search direction, where the training scheme is separated into several steps with each step tackling a set of unseen classes. Specifically, given the old model and new training data (only new classes are labeled while others are treated as background), the new model is supposed to recognize both old and new classes. Under this scenario, naively fine-tuning the old model on new data tends to suffer from catas-trophic forgetting [17, 26], where the recognition capability of old classes is quickly lost.
Knowledge distillation (KD), first proposed in image classification [19], has recently been introduced to CSS [35, 4] to mitigate the forgetting issue. As Fig. 1 shows, ILT [35] removes new class probabilities (green bar) predicted by the new model, and simply distill old classes and background accordingly. However, they ignore the background shift problem [4], where the new class bicycle at current step t was labeled as background at last step t − 1. As a result, the old model, which regards bicycle as background, will output a high background score for the new class bicycle pixels. Via KD, it will mislead the new model to misclas-sify new classes as background. Obviously, this KD strategy hinders the learning of new classes because the class corre-spondence for distillation is corrupted, i.e., naively mapping the new background to the old background.
This issue was highlighted in MiB [4] where they pro-posed a new class correspondence by combination. Con-cretely, they combined new classes with the new back-ground via probability summation (Fig. 1) to form a pseudo class, which was treated as the counterpart to the old back-ground for distillation. However, this strategy, though al-leviating class mis-correspondence, entangles new classes with the new background and tends to misclassify back-ground as new classes, as detailed in Sec. 3.2. In this paper, we term the error of mistaking background for new (novel) classes or vice-versa as novel-background confusion.
The key insight to overcome the novel-background con-fusion is to build a reliable class correspondence across dif-ferent learning steps without corruption or entanglement.
To this end, we devise a novel Label-Guided Knowledge
Distillation (LGKD) loss, where the class probabilities pre-dicted by the old model are expanded to have the same di-mension as the output of the new model (see the blue block in Fig. 1). The background probability is then transplanted to the corresponding ground truth label (class) of the in-put pixels. In this way, the knowledge from the old back-ground at the last step can be correctly distilled into its cor-responding semantic class at the current step, i.e., either the new background or a novel class. Note that our LGKD is a generic regularization term with negligible computa-tional cost and can be easily incorporated into existing arts.
We validate its effectiveness on two prevailing CSS bench-marks Pascal-VOC and ADE20K, where our LGKD consis-tently yields promising improvements upon three compet-ing methods, especially on novel (new class) mIoU (up to
+76%), setting new state-of-the-art. To further demonstrate the generalization ability of our approach, we establish a challenging CSS benchmark based on ScanNet for 3D point cloud and re-implement multiple baselines for comparison.
To our best knowledge, this is the first work to conduct CSS on 3D point cloud. Extensive experiments showcase that our LGKD loss is capable to handle both 2D and 3D modal-ities with no ad hoc design.
The main contributions of this paper can be summarized as:
• We propose a new label-guided knowledge distillation (LGKD) loss for CSS, which builds a reliable class correspondence across incremental steps and alleviates novel-background confusion.
• LGKD is a generic regularization term with negligi-ble computational cost, which can be readily com-bined with existing methods. Extensive experiments on two prevailing CSS benchmarks, Pascal-VOC and
ADE20K, showcase that LGKD significantly improves three competitive methods, particularly on novel mIoU by up to +76%, setting new state-of-the-art.
• To further demonstrate its generalization capability, we establish the first CSS benchmark for 3D point cloud based on ScanNet and re-implement multiple baselines for comparison. Experiments illustrate that our LGKD is versatile in both 2D and 3D modalities without any ad hoc design. 2.