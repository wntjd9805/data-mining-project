Abstract
Vision-language pre-training (VLP) methods are blos-soming recently, and its crucial goal is to jointly learn vi-sual and textual features via a transformer-based architec-ture, demonstrating promising improvements on a variety of vision-language tasks. Prior arts usually focus on how to align visual and textual features, but strategies for improv-ing the robustness of model and speeding up model conver-gence are left insufficiently explored.
In this paper, we propose a novel method ViLTA, com-prising of two components to further facilitate the model to learn fine-grained representations among image-text pairs.
For Masked Language Modeling (MLM), we propose a cross-distillation method to generate soft labels to enhance the robustness of model, which alleviates the problem of treating synonyms of masked words as negative samples in one-hot labels. For Image-Text Matching (ITM), we lever-age the current language encoder to synthesize hard nega-tives based on the context of language input, encouraging the model to learn high-quality representations by increas-ing the difficulty of the ITM task. By leveraging the above techniques, our ViLTA can achieve better performance on various vision-language tasks. Extensive experiments on benchmark datasets demonstrate that the effectiveness of
ViLTA and its promising potential for vision-language pre-training. 1.

Introduction
Recent advancements in Vision-Language Pre-training (VLP) have achieved significant improvements in a wide range of multimodal tasks, such as visual question answer-ing (VQA) [4], image captioning [3], and image-text re-trieval [34, 43]. The target of VLP generally starts with training a model on massive image-text pairs in a self-supervised way, which empowers a new paradigm for fine-tuning various downstream tasks. Most recent VLP mod-*Equal contribution.
†Corresponding author. els [42, 30, 6, 62, 5] usually utilize a transformer-based architecture [52] with some specific training techniques (e.g. image-text contrastive learning (ITC), masked lan-guage modeling (MLM), and image-text matching (ITM)) to align visual and textual information. These models have achieved outstanding performance on a variety of multi-modal benchmarks, which further advances the field of mul-timodal representation learning. However, the above VLP models also suffer from two vital limitations: (1) one-hot labels in MLM hinder the robustness of the model; (2) neg-ative samples selection in ITM affects the model conver-gency and downstream performance.
In specific, the MLM task is designed to predict masked tokens in a given language input by utilizing both visual and textual features. However, compared to traditional MLM approaches in NLP [13], the MLM task in vision-language pre-training presents a unique limitation. VLP models use a pre-trained language model for secondary pre-training on image-text pairs, which may result in a loss of knowl-edge initially acquired from NLP datasets. Empirical re-sults from previous studies [16] indicate that pre-training on multimodal datasets may lead to degraded performance on unimodal language tasks. Moreover, the presence of multi-ple candidate words to fill a masked position in an image-text pair can hinder the training of MLM. For instance, in the sentence ”Two giraffes pace around their habitat”, sub-stituting ”pace” with ”walk” does not alter the sentence’s meaning. Consequently, treating these words as negative samples in one-hot labels can impede MLM training.
As a popular pre-training task in VLP, the goal of ITM task is to distinguish positive and negative image-text pairs based on the learned representations. The common and straightforward way for negative selection is to randomly sample negatives for any given image-text pair. However, such a simple method can not provide more contributions for model convergence and result in sub-optimal perfor-mance. As a result, the model can easily achieve high accu-racy in the first training epoch, usually above 99%. To fur-ther improve the model’s ability to learn fine-grained rep-resentations, an effective method is to offer hard negative samples to make the pre-training task more challenging.
Figure 1. The overall architecture of ViLTA. The framework of ViLTA contains three components, including vision, language, and multi-modal encoders (Cf. (a)); soft labels obtained by the froze language encoder to enhance the robustness of model with noisy data in MLM (Cf. (b)); synthetic hard negatives generated by the current language encoder for ITM (Cf. (c)).
These hard negative samples are similar to positives and it is difficult for the model to distinguish them from positives.
A growing body of research [47, 41, 20] illustrates that min-ing hard negatives can drastically alter the performance of multimodal models among various tasks, highlighting the significance of hard negative samples for enhancing the rep-resentation capabilities of the model. However, the exist-ing attempts in mining hard negatives for vision-language pre-training only focus on sampling negatives in the dis-crete data space, ignoring the relationship among image-text pairs and the context of language input.
Present Work. To address the abovementioned issues, we propose a novel vision-language pre-training model named
ViLTA, comprising two key components. For MLM, we propose a cross-distillation method to generate soft labels for improving the learning efficiency and boosting the ro-bustness of the model. In specific, such a distillation method leverages the frozen language encoder to generate soft la-bels, which can be integrated into the original MLM task for joint training. In ITM, we propose to synthesize hard neg-atives based on the current language model by leveraging the context of language input, which is significantly differ-ent from previous works that select hard negatives from the raw data [30, 6]. By utilizing these two techniques, our pro-posed ViLTA can achieve better performance on a variety of downstream tasks, including visual question answering, vi-sual entailment, visual reasoning, image-text retrieval, and image captioning. Extensive experimental results demon-strate the effectiveness of ViLTA. We summarize the contri-butions of this work as follows: 1) The proposed knowledge distillation method cross-distillation generates soft labels to allow the model to better capture representations among image-text pairs, enabling the learning of the model more smooth and robust. 2) As opposed to sampling hard negatives from the raw data, we propose a strategy to synthesize hard negative sam-ples based on the current language model, boosting the rep-resentation ability of the model by enhancing the difficulty of the ITM task. 3) By effectively integrating these two techniques, our
ViLTA brings about outstanding performance improve-ments on various downstream tasks, demonstrating the su-periority of ViLTA. 2.