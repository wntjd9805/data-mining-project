Abstract 3D scene understanding plays a vital role in vision-based autonomous driving. While most existing meth-ods focus on 3D object detection, they have difficulty de-scribing real-world objects of arbitrary shapes and infi-nite classes. Towards a more comprehensive perception of a 3D scene, in this paper, we propose a SurroundOcc method to predict the 3D occupancy with multi-camera im-ages. We first extract multi-scale features for each image and adopt spatial 2D-3D attention to lift them to the 3D volume space. Then we apply 3D convolutions to progres-sively upsample the volume features and impose supervision on multiple levels. To obtain dense occupancy prediction, we design a pipeline to generate dense occupancy ground truth without expansive occupancy annotations. Specifi-cally, we fuse multi-frame LiDAR scans of dynamic ob-jects and static scenes separately. Then we adopt Pois-son Reconstruction to fill the holes and voxelize the mesh to get dense occupancy labels. Extensive experiments on nuScenes and SemanticKITTI datasets demonstrate the su-periority of our method. Code and dataset are available at https://github.com/weiyithu/SurroundOcc. 1.

Introduction
Understanding the 3D geometry of the surrounding scene serves as the basic procedure in an autonomous driv-ing system. While LiDAR is a direct solution to capture this geometric information, it suffers from high-cost sensors and sparse scanned points, limiting its further application.
Recently, vision-centric autonomous driving has attracted extensive attention as a promising direction. Taking multi-*Equal contribution.
â€ Corresponding author.
Figure 1. The overview of SurroundOcc. Given multi-camera im-ages, our method can predict volumetric occupancy of surrounding 3D scenes. To train the network, we design a pipeline to generate dense occupancy labels with sparse LiDAR points. Better viewed when zoomed in. camera images as inputs, it has demonstrated competitive performance in various 3D perception tasks including depth estimation [17, 58], 3D object detection [30, 27, 36, 34, 20], and semantic map construction [66, 19, 1].
Although multi-camera 3D object detection plays an im-portant role [30, 27, 34, 20] in vision-based 3D perception, it is easy to suffer from the long-tail problem and difficult to recognize all classes of objects in the real world. Com-plementary to 3D detection, reconstructing surrounding 3D scenes can better help the downstream perception tasks. Re-cent works [17, 58] incorporate information from multiple views and predict surrounding depth maps. However, depth maps only predict the nearest occupied point in each optical ray and are unable to recover the occluded parts of the 3D
scene. Different from depth-based methods, another trend
[7, 21] is to directly predict the 3D occupancy of the scene.
It describes a 3D scene by assigning an occupied proba-bility to each voxel in the 3D space. We advocate 3D oc-cupancy to be a good 3D representation for multi-camera scene reconstruction, which naturally guarantees the multi-camera geometry consistency and is able to recover oc-cluded parts. Also, it is flexible to extend to other 3D down-stream tasks such as 3D semantic segmentation [69, 73, 15].
As one of the pioneering works, MonoScene [7] infers the dense 3D voxelized semantic scene with monocular images.
However, simply fusing multi-camera results with cross-camera post-processing will lead to low performance [30].
TPVFormer [21] uses sparse LiDAR points as supervision, which results in sparse occupancy prediction.
To address this, we propose a SurroundOcc method, which aims to predict dense and accurate 3D occupancy with multi-camera images input. We first use a 2D back-bone network to extract multi-scale feature maps from each image. Then we perform 2D-3D spatial attention to lift multi-camera image information to 3D volume features in-stead of BEV features. A 3D convolution network is then employed to progressively upsample the low-resolution vol-ume features and fuse them with high-resolution ones to ob-tain fine-grained 3D representations. At each level, we use a decayed weighted loss to supervise the network. To get dense predictions, we need dense occupancy labels. How-ever, the mainstream multi-camera dataset nuScenes [6] only provides sparse LiDAR points. To avoid expensive oc-cupancy annotations, we devise a pipeline to generate dense occupancy ground truth only with the existing 3D detection and 3D semantic segmentation labels. Specifically, we first combine multi-frame points of dynamic objects and static scenes respectively. Then we leverage Poisson Reconstruc-tion [23] algorithm to further fill the holes. Finally, NN and voxelization are used to obtain dense 3D occupancy labels.
With the dense occupancy ground truth, we train the model and compare it with other state-of-the-art meth-ods on nuScenes [6] dataset. Both the quantitative re-sults and visualizations demonstrate the effectiveness of our method. Moreover, we further conduct experiments on Se-manticKITTI dataset [2]. Although our method is not de-signed for the monocular setting, it achieves state-of-the-art performance on the monocular 3D semantic scene comple-tion benchmark. 2.