Abstract 1.

Introduction
Text-to-image diffusion models are gradually introduced into computer graphics, recently enabling the development of Text-to-3D pipelines in an open domain. However, for interactive editing purposes, local manipulations of content through a simplistic textual interface can be arduous. Incor-porating user guided sketches with Text-to-image pipelines offers users more intuitive control. Still, as state-of-the-art
Text-to-3D pipelines rely on optimizing Neural Radiance
Fields (NeRF) through gradients from arbitrary rendering views, conditioning on sketches is not straightforward. In this paper, we present SKED, a technique for editing 3D shapes represented by NeRFs. Our technique utilizes as few as two guiding sketches from different views to alter an ex-isting neural field. The edited region respects the prompt semantics through a pre-trained diffusion model. To ensure the generated output adheres to the provided sketches, we propose novel loss functions to generate the desired edits while preserving the density and radiance of the base in-stance. We demonstrate the effectiveness of our proposed method through several qualitative and quantitative experi-ments. https://sked-paper.github.io/
Art is a reflection of the figments of human imagination.
While many are limited in their practical creative capabili-ties, by pushing the boundaries of digital media, new ways can be created for casual artists and experts alike to express their ideas. At the same time, current neural generative art takes away much of the control from humans. In this work, we attempt to take a step towards restoring some of that control, enabling neural networks to complement users and naturally extend their skills rather than taking hold over the generative process.
The field of image synthesis has been significantly pro-pelled by neural generative models, particularly by the lat-est text-to-image models that predominantly rely on large language-image models [3, 54, 55, 56]. These models have revolutionized the field of computer vision, as they can pro-duce astonishing visual outcomes from text prompts alone.
The ability of text-to-image models has sparked a wave of editing methods that utilize these models. Many of these techniques rely on prompt editing [14, 18, 27, 36, 44, 52].
Nevertheless, simplifying the interface to text alone means users lack the necessary level of granularity to produce their exact desired outcomes. Sketch-guided editing, on the other
hand, provides intuitive control that aligns with user’s con-ventional drawing and painting skills. By incorporating user-guided sketches into text-to-image models, powerful editing systems can be created, offering a high degree of flexibility and fine-grained control for manipulating visual content [83, 73].
Although sketch-guided and text-driven methods have proven successful in generating and manipulating 2D im-ages [40, 73, 9], it immediately raises the intriguing ques-tion of whether a similar approach could be developed to edit 3D shapes. Since direct text-to-3D models require an abundance of data to scale, state-of-the-art 3D genera-tive models, such as DreamFusion [52] and Magic3D [36], which build on the capabilities of text-to-image models, may be considered as an alternative. However, maintaining control via conditioning with such models remains a chal-lenging task, as these generative pipelines optimize a Neural
Radiance Field (NeRF) [42] by amortizing gradients from a multitude of 2D views. In particular, providing consis-tent sketches across all possible views presents a hurdle for users.
Instead, a plausible user interface should act with guidance from as few views as possible, e.g. up to two or three.
In this paper, we present SKED, a SKetch-guided 3D
EDiting technique. Our method acts on reconstructed or generated NeRF models. We assume a text prompt and a minimum of two sketches as input and provide output edits over the neural field faithful to the input conditions. Meet-ing all input requirements can be challenging as the text prompt may not match the sketch’s semantics, and sketch views may lack coherence. To undertake this complex task, we conceptually break it down into two subtasks that are easier to handle: one that depends on pure geometric rea-soning and the other that exploits the rich semantic knowl-edge of the generative model. These two subtasks work to-gether, with the former providing a coarse estimate of loca-tion and boundary, and the latter adding and refining geo-metric and texture details through fine-grained operations.
Our experiments highlight the effectiveness of our ap-proach for editing various pretrained NeRF instances. We introduce assorted accessories, objects, and artifacts, which are generated and blended into the original neural field seamlessly. Finally, we validate our method through quan-titative evaluations and ablation studies to assert the contri-bution of individual components in our method. 2.