Abstract
This work investigates the functionality of Semantic in-formation in Contrastive Learning (SemCL). An advanced pretext task is designed: a contrast is performed between each object and its environment, taken from a scene. This allows the SemCL pretrained model to extract objects from their environment in an image, significantly improving the spatial understanding of the pretrained models. Downstream tasks of semantic/instance segmentation, object detection and depth estimation are implemented on PASCAl VOC,
Cityscapes, COCO, KITTI, etc. SemCL pretrained models substantially outperform ImageNet pretrained counterparts and are competitive with well-known works on downstream tasks. The results suggest that a dedicated pretext task lever-aging semantic information can be powerful in benchmarks related to spatial understanding. The code is available at https://github.com/sjiang95/semcl. 1.

Introduction
Within the field of visual representation learning, con-trastive learning attracts special attention for its inspiring performance in transfer learning [57, 49, 31, 22, 8, 9]. The concept of contrastive learning can literally be explained as discovering the difference between positive and negative samples. The definition of positive-negative samples is one of the main subjects of contrastive learning, which directly determines the pretext task and the corresponding loss func-tion.
We humans can recognize an object from its even compli-cated environment because we have learned and bound the typical geometric feature and the corresponding semantic concept of the object class. For example, we can recog-nize a cat in a scene thanks to our knowledge of its ap-pearance (typical geometric feature) and the concept of cat (semantic information). Similarly, the target of prevalent
*Corresponding author.
Figure 1: Perceptual cognition of SemCL. Leveraging se-mantic information, SemCL separates an object from its environment. Each sample is divided into objects and sur-roundings and considered as a contrastive pair. The task is to maximize the distance between the object and its environ-ment in the embedding space. visual tasks, such as semantic/instance segmentation, object detection, etc., can be summarized as distinguishing and localizing subjects from surroundings. Inspired by and to extend the spatial recognition mechanism to CV, we pro-pose our method SemCL which directly teaches models to extract a subject from its environment. Utilizing publicly off-the-shelf datasets providing semantic labels, SemCL can be considered as a supervised contrastive learning approach whose samples consist of subjects and their corresponding surroundings (semantically not the subject). The pretext task of SemCL is to tell the difference between one subject and its surroundings. From a representation learning perspec-tive, the pretext task is to maximize the distance (L2 norm) between a subject and its environment in the embedding space (see Figure 1). The pretext task mimics the spatial recognition mechanism to discriminate objects and surround-ings, which significantly improves the spatial information understanding of pretrained models.
In SemCL, contrast is performed pairwise between positive-negative pairs from one image/scene. Inter-scene contrast is considered inappropriate: only the contrasts
among subject-surrounding pairs from one scene are con-sidered valid. Therefore, paired InfoNCE loss is adopted.
With the mechanism of contrasting only paired samples,
SemCL is characterized by decoupling the number of nega-tive samples from the batch size. Compared to MoCo [22], whose performance is positively correlated with batch size and thus increases hardware cost (e.g. batchsize = 4096 on 128 GPUs for ViT-B in MoCo v3 [9]) for satisfactory re-sults, SemCL can achieve consistent yet competitive results on downstream tasks with relatively small batch size (e.g., batchsize = 64 @ 224 Ã— 224). We adopt the MoCo v3 [9] as the pretraining framework.
The primary motivation of representation learning is to pretrain general representations that can be fine-tuned and transferred to downstream tasks. In this work, the SemCL pretrained models are benchmarked on semantic segmenta-tion, object detection and instance segmentation, and depth estimation tasks. SemCL models substantially outperform their ImageNet pretrained counterparts due to the improved ability of spacial information understanding. In a system-level comparison, SemCL models also show gains over pre-vious well-known works (e.g. Deeplabv3+ [7] in semantic segmentation, Mask2Former [10] in instance segmentation and Binsformer [35] in depth estimation). 2.