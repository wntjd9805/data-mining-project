Abstract
The growth of the Machine-Learning-As-A-Service (MLaaS) market has highlighted clients’ data privacy and security issues. Private inference (PI) techniques using cryptographic primitives offer a solution but often have high computation and communication costs, particularly with non-linear operators like ReLU. Many attempts to re-duce ReLU operations exist, but they may need heuristic threshold selection or cause substantial accuracy loss. This work introduces AutoReP, a gradient-based approach to
It lessen non-linear operators and alleviate these issues. automates the selection of ReLU and polynomial functions to speed up PI applications and introduces distribution-aware polynomial approximation (DaPa) to maintain model expressivity while accurately approximating ReLUs. Our experimental results demonstrate significant accuracy im-provements of 6.12% (94.31%, 12.9K ReLU budget, CIFAR-10), 8.39% (74.92%, 12.9K ReLU budget, CIFAR-100), and 9.45% (63.69%, 55K ReLU budget, Tiny-ImageNet) over current state-of-the-art methods, e.g., SNL. Morever, Au-toReP is applied to EfficientNet-B2 on ImageNet dataset, and achieved 75.55% accuracy with 176.1 × ReLU budget reduction. The codes are shared on Github1. 1.

Introduction
The MLaaS market has seen significant growth in recent years, with many MLaaS platform providers eatablished, e.g, AWS Sagmaker [22], Google AI Platform [3], Azure
ML [42]. However, most MLaaS solutions require clients to share their private input, compromising data privacy and
*Z. Wang is now affiliated with Walmart Global Tech, Sunnyvale, CA. 1https://github.com/HarveyP123/AutoReP
Figure 1: Network bandwidth: 1 GB/s. (a) Latency break-down of Wide-ResNet 22-8 operators under 2PC PI setup. (b) Latency reduction with polynomial replacement. security. Private inference (PI) techniques, have emerged to preserve data and model confidentiality, providing strong security guarantees. The existing highly-secure PI solu-tions usually use cryptographic primitives include multi-party computation (MPC) [16, 4, 15] and homomorphic en-cryption (HE) [23, 5, 11, 26]. Recently, MPC-based PI be-comes popular as it supports large-scale networks by parti-tioning the inference between clients and MLaaS providers.
The main challenge of applying cryptographic primitives in PI comes from the non-linear operators (e.g., ReLU), which introduces ultra-high computation and communica-tion overhead. Fig. 1 (a) shows that ReLU dominates the PI latency, i.e., up to 18.6 × than the combination of convolu-tional (Conv) and batch normalization (BN) operations. Re-ducing these ReLU operators could bring latency reduction, as highlighted in Fig. 1 (b). Centered by this observation, several approaches have been discussed, including replac-ing ReLUs with linear functions (e.g., SNL [8], DeepRe-1
duce [21]) or low degree polynomials (e.g., Delphi [34],
SAFENet [32]), designing neural architectures with fewer
ReLUs (e.g., CryptoNAS [14] and Sphynx [7]), and ultra-low bit representations (TAPAS [39], XONN [38]). How-ever, these techniques (i) require a heuristic threshold selec-tion on ReLU counts, therefore can not effectively perform design space exploration on ReLU reduction, resulting in sub-optimal solutions, and (ii) result in a significant accu-racy drop on large networks and datasets such as ImageNet, hence are not scalable as the number of ReLUs or the num-ber of bits decreases.
We argue that the root cause of the limitations is the dis-jointedness of non-linear operator reduction and model ex-pressivity in this emerging field. We aim to systematically solve the efficient PI problem by answering two gradually advancing questions: 1⃝ Which non-linear operators should be replaced, and 2⃝ What to be replaced with to maintain a high model accuracy and expressivity, especially for large
DNNs and datasets?
In this work, we introduce a gradient-based automatic
ReLU replacement (AutoReP) framework that incorpo-rates joint fine-grained replacement policy (addressing 1⃝) and polynomial approximation (addressing 2⃝). Our frame-work could simultaneously reduce the non-linear operators and maintain high model accuracy and expressivity. In sum-mary, our contributions are as follows: 1. We introduce a parameterized discrete indicator function, co-trained with model weights until conver-gence. Our approach allows for fine-grained selection of ReLU and polynomial functions at the pixel level, resulting in a more optimized and efficient model. 2. We present a hysteresis loop update function to en-hance the stability of the binarized ReLU replacement training process, which enables a recoverable and sta-ble replacement and leads to better convergence and higher accuracy. 3. Our proposed method, distribution-aware polyno-mial approximation (DaPa), offers a novel solution to the problem of accurately approximating ReLUs us-ing polynomial functions under specific feature distri-butions. By minimizing the structural difference be-tween the original and replaced networks and main-taining high model expressivity.
Experimental results show that our AutoReP (ResNet-18) achieves 74.92% accuracy with 12.9K ReLU budget, 8.39% higher than SNL [8], with 1.7x latency reduction, on
CIFAR-100. For 73.79% accuracy, AutoReP requires only 6K ReLUs, an 8.2x reduction in ReLU budget vs. SNL [8].
When applied to the larger EfficientNet-B2 on the ImageNet dataset, AutoReP achieved an accuracy of 75.55% with a significant reduction of 176.1 × in ReLU budget.
Figure 2: Plaintext vs. ciphertext evaluation (4 bits). 2.