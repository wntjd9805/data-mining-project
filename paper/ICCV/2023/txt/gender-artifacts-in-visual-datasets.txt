Abstract
Average pose
Average color
F
M
F
M
Gender biases are known to exist within large-scale vi-sual datasets and can be reﬂected or even ampliﬁed in downstream models. Many prior works have proposed methods for mitigating gender biases, often by attempt-ing to remove gender expression information from images.
To understand the feasibility and practicality of these ap-proaches, we investigate what “gender artifacts” exist in large-scale visual datasets. We deﬁne a “gender artifact” as a visual cue correlated with gender, focusing speciﬁ-cally on cues that are learnable by a modern image clas-siﬁer and have an interpretable human corollary. Through our analyses, we ﬁnd that gender artifacts are ubiquitous in the COCO and OpenImages datasets, occurring every-where from low-level information (e.g., the mean value of the color channels) to higher-level image composition (e.g., pose and location of people). Further, bias mitigation meth-ods that attempt to remove gender actually remove more in-formation from the scene than the person. Given the preva-lence of gender artifacts, we claim that attempts to remove these artifacts from such datasets are largely infeasible as certain removed artifacts may be necessary for the down-stream task of object recognition. Instead, the responsibil-ity lies with researchers and practitioners to be aware that the distribution of images within datasets is highly gendered and hence develop fairness-aware methods which are ro-bust to these distributional shifts across groups. 1.

Introduction
It has been well established that machine learning sys-tems contain gender biases [12, 15, 31, 76, 82, 83]. For instance, image tagging systems have labelled similarly posed female politicians differently than their male coun-terparts [64], and image search engines can return stereo-typical results mirroring harmful gender roles [38, 48, 50]. 0.4 0.2 0.0
F
M
Area 0.2 0.1 0.0
F
M
Distance 2.0 1.0 0.0
F
M
Aspect l e u a
V l e n n a h
C 100 50 l r o o 0C
F
M
F
M
F
M
Red
Green
Blue
Figure 1. We aim to understand gender artifacts in image datasets by training a classiﬁer to distinguish between images containing a person labelled “female” vs. “male”. We ﬁnd that such a classiﬁer frequently performs signiﬁcantly above random chance even when trained and evaluated on modiﬁed versions of images (e.g. where a person’s appearance is obscured). On the left, we trained a CNN on COCO [42] images with only the person segmentation mask visible; on the right, we trained a logistic regression classiﬁer us-ing only 3 features (i.e. average color) per COCO image. Both classiﬁers perform signiﬁcantly above random chance, illustrating how deeply gender artifacts are embedded within the dataset. We visualize the classiﬁers’ top 20 (most “female”) and bottom 20 (most “male”) images (top), and further show the differences be-tween their segmentation masks (left bottom) in normalized area, distance from the center, and aspect ratio, and the differences be-tween the R, G and B features (right bottom). These results sug-gest that gender artifacts are pervasive in computer vision datasets, and current “fairness through blindness” methods that aim to “re-move” gendered information may not be as effective as hoped.
A prevailing assumption in the fairness community is that biases in models originate in biases in the input data.
Thus, there has been an effort to mitigate these dataset biases (and thus also model biases). While many biases stem from the lack of representation of certain demographic groups, including women [13, 82, 83], naively balancing gender distributions has been shown to be insufﬁcient for mitigating model biases [76]. As a result, a large vein of work has attempted to pursue “fairness through blind-ness” [22] which operates under the assumption that remov-   
ing gendered information can remove the potential for gen-der bias in the dataset. These attempts include using seg-mentation masks to occlude the person in images [9, 31, 70] or removing image features correlated with gender [75, 76].
In our work, we focus not on mitigating these dataset biases, but rather on exploring to what extent gendered in-formation can truly be removed from the dataset. To do so, we develop a framework to identify gender artifacts, or visual cues correlated with gender labels. We use the term “artifact” to emphasize how these correlations may not be related to gender presentation, rather simply a result of the dataset collection process. While there can be inﬁnitely many potential correlations in an image, we focus on those that are learnable (i.e., result in a learnable difference in a model) and interpretable (i.e., have an interpretable human corollary, such as color and shape).1 Overall, we do not claim (nor believe) the existence of a speciﬁc gender arti-fact necessarily leads to disparate performance on down-stream tasks for different groups (a common measure of fairness[13, 63, 80]), however, the source of disparate per-formance lies in gender artifacts which are critical to study.
To discover gender artifacts, we use a gender classiﬁer, which we refer to as a “gender artifact model,” since the model is not predicting gender so much as it is predict-ing correlations to gender expression in the training dataset.
Our goal is to understand what the model uses as predic-tive features and how variations of the dataset with partic-ular gender artifacts removed may affect its performance.
Thus, we systematically manipulate the datasets (e.g., oc-clude the person, occlude the background, etc.) and train our model on this data to identify potential gender artifacts.
This framework for discovering gender artifacts is more complex than prior work [74], as it goes beyond analyz-ing annotated attributes to include higher level perceptual components of the image (e.g., resolution, color) and per-son and background artifacts to gain a deeper understanding of where gender artifacts exist in visual datasets.
It is important to note we do not condone the use of au-tomated gender recognition in practice. We perform gender classiﬁcation only to understand differences in data distri-butions, not for the sake of classifying gender itself. We never use the model outputs as the end goal and no causal claims about the observed correlation between an attribute and gender labels can be made.
Instead, the outputs are a means for better understanding the data. Furthermore, our use of the term “gender” when referencing people in image datasets refers to the binarized perceived gender ex-pression. Since most image datasets do not include self-reported gender identity, we rely on external annotators’ perception as gender labels [63]. To be consistent with prior works [13, 31, 76, 82, 83], we use “male” and “female” to refer to gender expression. Finally, we do not make any nor-1See Appendix A for details on artifact constraints. mative claims about the gender artifacts (i.e., if it is good or bad); we focus simply on identifying which artifacts exist.
Using our proposed framework, we perform an in-depth analysis into understanding the wide variety of gender arti-facts present in two popular image datasets: Common Ob-jects in Context (COCO) [42] and OpenImages [40]. Our results show that gender artifacts are everywhere, from the shape of the person segmentation mask (Fig. 1) to randomly selected contextual objects. Even the average color of the image is sufﬁcient to distinguish between the genders: when each image is represented by just three features, the mean pixel value of the red, green, and blue color channels, the classiﬁer is able to achieve an AUC of 58.0% 0.4 in COCO and 59.1% 0.4 in OpenImages.
±
These ﬁndings have the following implications:
±
• Many prior works have proposed mitigation methods that attempt to remove gender expression information from the image [1, 9, 31, 70, 76]. In contrast, we show gender artifacts are so intricately embedded in vi-sual datasets that attempts to remove them via mit-igation techniques may be a futile endeavor. This is evidenced by experiments where even after a per-son is entirely occluded with a rectangular mask and the background is visible, the gender artifact model is nevertheless able to reliably distinguish which of the two genders is present in the image, achieving an
AUC of 70.8% in COCO [42] and 63.0% in OpenIm-ages [40]. Using a popular adversarial approach [76] as a case study, we ﬁnd 76.8% of the removed infor-mation comes from the scene rather than the person.
• We more realistically advise practitioners to adopt fairness-aware models [22, 80] and disaggregated evaluation metrics [5], which explicitly account for po-tential discrepancies between protected groups.
• Since there are so many salient artifacts in image datasets that are (spuriously) correlated with gender, our ﬁndings point to an incoherence of gender pre-diction. Any time a model predicts “gender,” we should wonder what the prediction refers to; it could easily be relying on the background scene color, rather than societally meaningful forms of gender expression. 2.