Abstract
Deep Neural Networks (DNNs)-based semantic segmen-tation models trained on a source domain often struggle to generalize to unseen target domains, i.e., a domain gap problem. Texture often contributes to the domain gap, mak-ing DNNs vulnerable to domain shift because they are prone to be texture-biased. Existing Domain Generalized Seman-tic Segmentation (DGSS) methods have alleviated the do-main gap problem by guiding models to prioritize shape over texture. On the other hand, shape and texture are two prominent and complementary cues in semantic segmenta-tion. This paper argues that leveraging texture is crucial for improving performance in DGSS. Speciﬁcally, we pro-pose a novel framework, coined Texture Learning Domain
Randomization (TLDR). TLDR includes two novel losses to effectively enhance texture learning in DGSS: (1) a texture regularization loss to prevent overﬁtting to source domain textures by using texture features from an ImageNet pre-trained model and (2) a texture generalization loss that uti-lizes random style images to learn diverse texture represen-tations in a self-supervised manner. Extensive experimental results demonstrate the superiority of the proposed TLDR; e.g., TLDR achieves 46.5 mIoU on GTA→Cityscapes us-ing ResNet-50, which improves the prior state-of-the-art method by 1.9 mIoU. The source code is available at https://github.com/ssssshwan/TLDR. 1.

Introduction
Semantic segmentation is an essential task in com-puter vision with many real-world applications, such as au-tonomous vehicles, augmented reality, and medical imag-ing. Deep Neural Networks (DNNs)-based semantic seg-mentation models work well when the data distributions are consistent between a source domain and target domains
[1,5–7,22,34,55]. However, the performance of the models tends to degrade signiﬁcantly in practical settings that in-*Corresponding author.
Unseen Target Image (a) Normalization & Whitening (b) Domain Randomization (c) TLDR (ours)
Figure 1. Segmentation results for an image from an unseen do-main (i.e., Cityscapes [12]), using models trained on GTA [47] with DGSS methods. (a-b) Existing methods [10, 59] have difﬁ-culty in distinguishing between road, sidewalk, and terrain which have similar shapes and contexts, as the texture is not sufﬁciently considered during the training process. (c) Our Texture Learning
Domain Randomization (TLDR) can distinguish the classes effec-tively as we utilize the texture as prediction cues. volve unseen out-of-distribution scenarios, also known as a domain gap problem. Many domain adaptation and gener-alization methods have been proposed to solve the domain gap problem [10, 24, 28, 42–44, 54, 59, 60]. Domain adapta-tion assumes accessibility of target domain images, differ-ing from domain generalization. This paper addresses Do-main Generalized Semantic Segmentation (DGSS), which aims to train models that can generalize to diverse unseen domains by training on a single source domain.
Existing DGSS methods have attempted to address the domain gap problem by guiding models to focus on shape rather than texture. Given that texture often varies across different domains (e.g., synthetic/real and sunny/rainy/foggy), DNNs are susceptible to domain shift because they tend to be texture-biased [19, 39]. Accord-ingly, there are two main approaches for the DGSS meth-ods. The ﬁrst approach is Normalization and Whitening
Original Source (a) Norm. & Whiten. (b) Domain Random.
GTA
Cityscapes
BDD
Mapillary d a o
R
Figure 2. Reconstructed source images from the feature maps of (a) normalization and whitening and (b) domain randomization.
Texture features are often omitted in existing DGSS methods. (a) Shape Features (b) Texture Features
Figure 3. The t-SNE [52] plots for the road, sidewalk, and terrain classes from Cityscapes [12] that have similar shapes. While the shape features (Canny edge [3]) are entangled in (a), the texture features (Gram-matrix) of these classes are clearly separated in (b). The plots are based on an ImageNet pre-trained model. (NW), which involves normalizing and whitening the fea-tures [10, 42–44]. It is possible to remove domain-speciﬁc texture features and learn domain-invariant shape features with NW (see Figure 2a). The second approach is Domain
Randomization (DR), which trains by transforming source images into randomly stylized images [24,28,45,54,59–61].
The model learns domain-invariant shape features because texture cues are mostly replaced by random styles (see Fig-ure 2b) [38, 51].
While the existing methods are effective at making the models focus on shape features, they need to give more con-sideration to texture features. In addition to utilizing shape features like edges and structures, DNNs also use texture features such as patterns, color variations, and histograms as important cues for prediction [18]. Particularly in seman-tic segmentation, texture plays a crucial role in accurately maintaining the boundaries between objects [26, 63].
Figure 1 demonstrates the results of predicting an unseen image from Cityscapes [12] using DGSS methods trained on GTA [47]. One can see that the models trained with NW and DR have difﬁculty distinguishing between the road, sidewalk, and terrain classes, which have similar shapes and contexts. In order to determine these classes accurately, it is necessary to use texture cues. This assertion is further the emphasized through t-SNE [52] plots of the classes: shape features are entangled in Figure 3a, whereas the tex-ture features are clearly separated in Figure 3b. Meanwhile, some textures remain relatively unchanged across domains in DGSS as shown in Figure 4. Based on these observations, we suggest utilizing texture as valuable cues for prediction.
We propose Texture Learning Domain Randomization (TLDR), which enables DGSS models to learn both shape and texture. Since only source domain images are available l k a w e d
S i i n a r r e
T
Figure 4. Visualization of texture for the road, sidewalk, and ter-rain classes from GTA [47], Cityscapes [12], BDD [58], and Map-illary [40] datasets. For each class, there are commonalities in tex-ture across the datasets. in DGSS, texture must be learned from them. To accurately capture the texture, we leverage the source domain images without any modiﬁcation, which we refer to as the original source images. The stylized source images from DR are more focused on learning shape features, and the original source images are more focused on learning texture features (Section 4.1). To further improve texture learning in DGSS, we propose a texture regularization loss and a texture gen-eralization loss. While there are commonalities in texture across different domains, there are also clear texture differ-ences. Thus, if the model overﬁts source domain textures, it will result in a performance drop. To mitigate this problem, we propose the texture regularization loss to prevent over-ﬁtting to source domain textures by using texture features from an ImageNet pre-trained model (Section 4.2). Since only source domain textures alone may not be sufﬁcient for learning general texture representations, we propose the texture generalization loss that utilizes random style images to learn diverse texture representations in a self-supervised manner (Section 4.3).
Our contribution can be summarized into three aspects.
First, to the best of our knowledge, we are approaching
DGSS for the ﬁrst time from both the shape and texture perspectives. We argue that leveraging texture is essential in distinguishing between classes with similar shapes de-spite the domain gap. Second, to enhance texture learning in DGSS, we introduce two novel losses: the texture reg-ularization loss and the texture generalization loss. Third, extensive experiments over multiple DGSS tasks show that our proposed TLDR achieves state-of-the-art performance.
Our method attains 46.5 mIoU on GTA→Cityscapes using
ResNet-50, surpassing the prior state-of-the-art method by 1.9 mIoU.
Original Source Image
Stylized Source Images
STM
TEO (cid:3400)
Random Style Images
Feature Map
Gram-matrix (a) Style Transfer Module (STM) (b) Texture Extraction Operator (TEO)
Figure 5. (a) Visualization of using the Style Transfer Module (STM) to transform an original source image into stylized source images with random style images. (b) Illustration of the process of using Texture Extraction Operator (TEO) to extract only texture features, i.e., a Gram-matrix, from a feature map. 2.