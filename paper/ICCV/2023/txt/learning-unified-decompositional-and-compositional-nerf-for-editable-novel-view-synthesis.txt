Abstract
Implicit neural representations have shown powerful ca-pacity in modeling real-world 3D scenes, offering superior performance in novel view synthesis. In this paper, we tar-get a more challenging scenario, i.e., joint scene novel view synthesis and editing based on implicit neural scene repre-sentations. State-of-the-art methods in this direction typ-ically consider building separate networks for these two tasks (i.e., view synthesis and editing). Thus, the model-ing of interactions and correlations between these two tasks is very limited, which, however, is critical for learning high-quality scene representations. To tackle this problem, in this paper, we propose a unified Neural Radiance Field (NeRF) framework to effectively perform joint scene decomposition and composition for modeling real-world scenes. The de-composition aims at learning disentangled 3D representa-tions of different objects and the background, allowing for scene editing, while scene composition models an entire scene representation for novel view synthesis. Specifically, with a two-stage NeRF framework, we learn a coarse stage for predicting a global radiance field as guidance for point sampling, and in the second fine-grained stage, we perform scene decomposition by a novel one-hot object radiance field regularization module and a pseudo supervision via in-painting to handle ambiguous background regions occluded by objects. The decomposed object-level radiance fields are further composed by using activations from the decomposi-tion module. Extensive quantitative and qualitative results show the effectiveness of our method for scene decomposi-tion and composition, outperforming state-of-the-art meth-ods for both novel-view synthesis and editing tasks1. 1.

Introduction
The reconstruction and rendering of natural scenes are important for computers to understand the 3D physical
*Corresponding author 1Project: https://w-ted.github.io/publications/udc-nerf
Figure 1. Illustration of our decomposition-composition design. It enables scene editing and novel view synthesis in a unified NeRF framework. The proposed decomposition targets learning disen-tangled 3D representations of different foreground objects and the background, allowing for scene editing, while scene composition constructs an entire scene representation for novel view synthesis. world. Fine-grained object-level representations within the scene can bring significant benefits in various applications, such as scene understanding, novel content generation via object editing, and robotic manipulation. The emerging neural rendering techniques [17, 18, 24] allow learning object-level or scene-level representations from multi-view posed images and enable rendering high-quality images from novel views. However, most implicit neural methods model an entire scene and lack fine-grained representations of objects in the scene, which severely limits object-level scene representation and understanding.
Learning effective 3D object-aware implicit neural scene representations is still in its infancy. Existing works that tackle this challenging problem typically utilize additional object-level semantic supervision as a priori knowledge in the model optimization [35, 37, 42]. For instance, Ob-jectSDF [35] considers learning object-level geometries and semantics with an implicit signed distance function (SDF), which achieves effective scene composition from object-level representations. However, it remains challenging for this framework to perform joint object editing and novel view synthesis. On the other hand, ObjectNeRF [37] ex-plores scene modeling with Neural Radiance Fields (NeRF) via introducing learnable object activation codes as switch-ers to condition the radiance field prediction among differ-ent objects. This framework can perform joint scene editing and novel view synthesis benefiting from the radiance field representations. However, it learns two separate networks for these two tasks, a global scene branch for novel view synthesis, and a local object branch for scene editing, with-out modeling any interaction or correlation among them, which, however, is critical for learning more effective scene representations, due to the simple fact that, global scene and local object representations are two important perspec-tives of the same scene. The global entire scene representa-tion can model the scene’s overall structures and appearance consistency, and the local object-specific representation can learn more fine-grained object details. They are highly com-plementary for learning a high-quality scene representation.
To target the above-mentioned problem, this paper pro-poses a unified decompositional and compositional neural radiance field framework (see Fig. 1), to learn 3D scene rep-resentation for joint scene editing and novel view synthesis.
The decomposition can provide the functionality of learning disentangled 3D representations of different objects and the background, allowing for scene editing, while scene compo-sition models an entire scene representation for novel view synthesis. These two can be united to facilitate the con-sistency constraint in the unified optimization framework.
Specifically, we design a new two-stage framework for the scene decomposition and composition, consisting a coarse and a fine stage. In the coarse stage, we learn to predict a global scene radiance field as guidance for point sampling, and in the second fine-grained stage, we perform joint scene decomposition and composition. To perform effective scene decomposition, in the fine-stage, we apply a set of learnable object codes to predict distinct object-level radiance fields, and also propose two novel decomposition strategies: (i) 3D one-hot object radiance activation and regularization. For the object or background branches, only one branch is ac-tivated during training using a designed Gumbel-Softmax activation function directly applied on the point density pre-dicted from different object/background branches with also a corresponding one-hot regularization; (ii) an in-painting pseudo supervision strategy. A challenge in scene render-ing is modeling the appearance and geometry of regions oc-cluded by objects. This causes generation ambiguity espe-cially when the occluded regions are unseen in all the train-ing views. To address this issue, we propose to use a pre-trained inpainting model to provide additional pseudo-color supervision for those ambiguous areas. Note that although the 2D inpainting may bring new ambiguity for the regions seen in other views, the supervision from multi-view consis-tency is strong enough to suppress most ambiguities. The composition is further performed by utilizing the learned one-hot activation weights for different object-level radi-ance fields.
In summary, this paper has the following contributions:
• We propose a novel NeRF framework for joint scene decomposition and composition to effectively learn object-level and scene-level implicit representations for effective scene modeling, allowing object editing and novel view synthesis in a unified pipeline.
• To learn a robust scene decomposition, we design two novel strategies, i.e., 3D one-hot radiance regulariza-tion and 2D in-painting pseudo supervision, to signifi-cantly improve the rendering and editing qualities.
• Extensive experiments demonstrate the effective-ness of our approach for scene decomposition and show clear improvements upon state-of-the-art object-compositional methods on two important downstream tasks, i.e. novel-view synthesis and object editing. 2.