Abstract
This paper explores Deployment Authorization (DPA) as a means of restricting the generalization capabilities of vision models on certain domains to protect intellectual property. Nevertheless, the current advancements in DPA are predominantly confined to fully supervised settings. Such settings require the accessibility of annotated images from any unauthorized domain, rendering the DPA approaches impractical for real-world applications due to its exorbitant costs.
To address this issue, we propose Source-Only Deploy-ment Authorization (SDPA), which assumes that only autho-rized domains are accessible during training phases, and the model’s performance on unauthorized domains must be suppressed in inference stages. Drawing inspiration from dis-tributional robust statistics, we present a lightweight method called Domain-Specified Optimization (DSO) for SDPA that degrades the model’s generalization over a divergence ball.
DSO comes with theoretical guarantees on the convergence property and its authorization performance. As a comple-mentary of SDPA, we also propose Target-Combined De-ployment Authorization (TPDA), where unauthorized do-mains are partially accessible, and simplify the DSO method to a perturbation operation on the pseudo predictions, re-ferred to as Target-Dependent Domain-Specified Optimiza-tion (TDSO). We demonstrate the effectiveness of our pro-posed DSO and TDSO methods through extensive experi-ments on six image benchmarks, achieving dominant perfor-mance on both SDPA and TDPA settings.
*Indicates equal contribution.
Figure 1: Our domain specified optimization is designed for maximizing the risk over unauthorized domains (e.g.,
PU1 and PU2 as framed yellow triangles) close to the au-thorized domain Ps, and minimizing the risk on PS as the centroid (risk quantifies the consistency between the learn-ing model and the domain). Besides, unauthorized domains far away from the authorized domain (e.g., PU3 and PU4 as framed blue squares) are not considered, as the model trained on PS naturally generalizes poorly on them. 1.

Introduction
Deep learning has achieved remarkable progress in a vari-ety of vision applications [4, 7, 47, 48, 51, 38] with tremen-dous commercial values [22, 41, 20, 40]. However, training a deep model from scratch is non-trivial, as it contains abun-dant knowledge including a professional tuning process [7], large-scale image datasets with exhaustive annotations [9], and expensive computational resources [2]. Therefore, the in-tellectual property (IP) protection/authorization of the knowl-edge embodied in a pre-trained deep model has been raised as a matter of increasing concern [46, 43].
Previous studies on IP protection/authorization consist of two branches, including authorizations of the model owner and the model user [43], respectively. In addition to address-ing who can own or use the model, a data-centric authoriza-tion problem known as deployment authorization (DPA) has been proposed [43]. DPA aims to address which data can
Figure 2: Overview of supervised DPA (SUDPA), Source-only DPA (SDPA), and Target-combined DPA (TDPA), where the subscripts S, U , and f refer to the authorized data, the unauthorized domains, and the learning model, respectively. SUDPA allows the model authorizer to access both the images X and labels Y on PU in the training stage. Without labels, TPDA allows the authorizer only to access the images X on PU . Both X and Y are inaccessible for the authorizer in SPDA. be deployed on the model. More specifically, DPA seeks to restrict the model’s generalization capability on certain domains so that its performance is severely degraded when deployed on unauthorized data, while it maintains its perfor-mance when deployed on authorized data.
The practical significance of DPA can be attributed to two main factors: (a) its commercial aspect, which safe-guards the IP of commercial models from competitors, and (b) its ethical aspect, which restricts unauthorized users from exploiting models to target vulnerable populations. Concern-ing the commercial aspect, DPA is essential for applications such as online interactions between users and models via interfaces, where well-trained models on specific data cannot be shared and require payment to deploy, such as Amazon’s
Machine-Learning-as-a-Service. In terms of the ethical as-pect, a typical example would be a company, like Meta, that trains a recommendation system (RS) using adult data and ap-plies DPA to prevent teenagers from abusing the system [43].
Without DPA, if the RS generalizes well to teenagers’ data and is deployed without authorization, it could potentially recommend inappropriate items such as alcohol to teenagers.
Due to the growing importance of data-centric IP protection, our paper focuses on exploring DPA-related issues.
Concerning the current advances in DPA, a promising solution called non-transferable learning (NTL) has been proposed to address the supervised deployment authoriza-tion (SUDPA) problem [43], as illustrated in Figure 2(a). In particular, SUDPA assumes that the model authorizer can fully access both authorized and unauthorized domains [43].
As highlighted in our teenager-adult example, implement-ing SUDPA requires annotated data (e.g., labeled images of items) from both the teenager and adult domains. Unfor-tunately, collecting and labeling images from unauthorized domains is too expensive, which is a significant limitation for practical applications. Furthermore, this cost can escalate infinitely since the authorizer might specify an arbitrarily large range of unauthorized domains. To address this issue, we propose two more practical yet more challenging DPA problems, as illustrated in Figure 2(b) and (c).
• Source-only deployment authorization (SDPA): In the teenager-adult example, the authorizer may only want to deploy the recognition model trained on adult data on the authorized domain. As illustrated in Fig-ure 2 (b), our proposed SDPA approach addresses this requirement by assuming that the authorizer: (a) has ac-cess to only the authorized domain during the training phase, and (b) intends to suppress the model’s perfor-mance on all other domains.
• Target-combined authorization problem (TDPA):
When additional unauthorized domains are specified, the authorizer only has to collect them directly with-out annotations. Figure 2 (c) illustrates this scenario, where the TDPA problem assumes that the authorizer has access to both the entire authorized domain (i.e., la-beled adult data) and the partially labeled unauthorized domain (i.e., unlabeled teenager data).
In this paper, we propose a lightweight method named
“Domain Specified Optimization” (DSO) to address the
SDPA problem. DSO defines a divergence ball centered around the training distribution, covering each neighboring distribution close to the training domain [11]. Fig 1 illus-trates that DSO simultaneously minimizes the model risk on the training domain and maximizes the model risk on each domain in the divergence ball except for the training domain.
Theoretical results ensure that: (a) DSO possesses well-established convergence properties, and (b) DSO achieves reliable authorization on any unauthorized domain. To solve the TDPA problem, we adapt our DSO method into the
Target-specified DSO (TDSO) method, which perturbs the pseudo predictions on the unauthorized domain. In summary, we list our main contributions as follows:
Problem Contribution In addition to the vanilla DPA problem, we introduce two more practical and challenging problems: the Source-only DPA (SDPA) and the Target-combined DPA (TDPA). The SDPA problem aims to achieve uniform deployment authorization of vision models by de-grading the model’s performance on any other domains, while the TDPA problem responds to the requirement that
the model authorizer specifies unauthorized domains without incurring the cost of annotation.
Method Contribution To solve SDPA, we contribute a novel method named “Domain Specified Optimiza-tion” (DSO), which achieves reliable authorization on any unauthorized domain close to the training distribution. Fur-thermore, we solve the TDPA problem by proposing a pseudo-prediction perturbation strategy named “TDAO”.
Experimental Results We conduct extensive exper-iments on six image classification benchmarks, includ-ing digit datasets (MNIST, USPS, SVHN, SYN_D and
MNIST_M), Cifar10 & STL10, Office-31, Visda 2017,
PACS and VLCS, to verify the effectiveness of our DSO and TDSO on SDPA and TDPA problems. 2.