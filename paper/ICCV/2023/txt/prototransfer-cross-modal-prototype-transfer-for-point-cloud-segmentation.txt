Abstract
Knowledge transfer from multi-modal, i.e., LiDAR points and images, to a single LiDAR modal can take advantage of complimentary information from modal-fusion but keep a single modal inference speed, showing a promising direc-tion for point cloud semantic segmentation in autonomous driving. Recent advances in point cloud segmentation distill knowledge from strictly aligned point-pixel fusion features while leaving a large number of unmatched image pixels unexplored and unmatched LiDAR points under-benefited.
In this paper, we propose a novel approach, named Pro-toTransfer, which not only fully exploits image representa-tions but also transfers the learned multi-modal knowledge to all point cloud features. Specifically, based on the ba-sic multi-modal learning framework, we build up a class-wise prototype bank from the strictly-aligned fusion fea-tures and encourage all the point cloud features to learn from the prototypes during model training. Moreover, to exploit the massive unmatched point and pixel features, we use a pseudo-labeling scheme and further accumulate these features into the class-wise prototype bank with a carefully designed fusion strategy. Without bells and whistles, our approach demonstrates superior performance over the pub-lished state-of-the-arts on two large-scale benchmarks, i.e., nuScenes and SemanticKITTI, and ranks 2nd on the com-petitive nuScenes Lidarseg challenge leaderboard. 1.

Introduction
Semantic segmentation on point cloud [6, 15, 37, 40] has attracted increasing attention from the computer vision community for its crucial role in scene understanding of 3D space. Although LiDAR point cloud can provide ac-curate location and depth information of interested scenes, the sparse and textureless shortages inevitably restrict its se-mantic segmentation performance. On the other hand, 2D images consist of dense pixels with rich color and subtle
* Corresponding author. (a) Point-pixel matching and feature fusion: only
Figure 1. (b) features of strictly matched point-pixel pairs can be fused.
Distillation-based methods are only performed between fusion prediction and the corresponding matched point prediction while other unmatched points have no chance to mimic the fusion predic-tion. (c) Our ProtoTransfer performs knowledge transfer from prototype bank to all point features directly without being re-stricted by matching requirements. The prototype bank is updated using fusion, point, and unmatched image features. textures [34]. Therefore, incorporating these two comple-mentary modals altogether can be a more plausible solution.
Recently, there are mainly two stream approaches of uti-lizing multi-modal data, i.e., fusion-based methods [10, 50] and distillation-based methods [42]. The former methods project point clouds to the camera coordinate to obtain a point-to-pixel mapping, based on which point features are fused with corresponding image features to produce the final point-wise segmentation during both the training and evaluation phases. Although robust and accurate seg-ments are achieved by fusing different sensors, the fusion-based methods may suffer from heavy memory and time consumption for processing these two modal data simul-In order to benefit from multi-modal fusion taneously. while bypassing the computation burden, the latter meth-ods [42] propose to utilize the distillation technique to trans-fer knowledge learned from cross-modal fusion at the train-ing stage and discard the fusion module for evaluation. The impressive segmentation performance suggests the feasibil-ity of the methods in this stream. Therefore, this paper fur-ther explores this research direction.
Unlike common distillation approaches in image classi-fication tasks [14, 44], distilling from multi-modal to the single LiDAR modal may meet quite different pitfalls: (1)
Not all LiDAR points can be used in knowledge distillation because LiDAR-to-image is partially matched. Due to the difference in the way sensors collect data, not all the LiDAR points can be aligned to the dense image pixels. For exam-ple, we empirically find that only around 16.07% LiDAR points in SemanticKITTI dataset can be matched to cor-responding image pixels, and Fig. 1(a) also demonstrates this observation. Thus, classic logit-/probability-style dis-tillation strategies [42] can only be performed on the well-aligned point clouds, leading to a sub-optimal knowledge distillation. (2) Massive image pixels are overlooked. Since only partial image pixels1 will contribute to the modal fu-sion based on the above matching constraints, a large num-ber of pixels are not used in knowledge distillation, resulting in a great loss of rich semantic information in dense pixels.
In this paper, we propose a novel approach for point cloud semantic segmentation which successfully avoids the above pitfalls.
Specifically, following the existing work [17, 34, 42], we first construct a modal fusion mod-ule to combine the feature representations of matched Li-DAR points and image pixels. Next, in order to transfer the knowledge from the fusion module to each point feature representation, we create a class-wise prototype bank to ac-cumulate the fusion features learned in the fusion module and encourage the similarity between features of all LiDAR points and corresponding-class prototypes as high as possi-ble. By this means, every LiDAR point has a fair chance to mimic the fusion features as shown in Fig. 1(c), and we dub our method as ProtoTransfer. Moreover, to make full use of semantic information inhered in dense image pix-els, we propose to incorporate the image features into the class-wise prototype bank through a cleverly designed fu-sion strategy. Since images in the point cloud semantic seg-mentation datasets usually lack per-pixel annotations, we further use a pseudo-labeling scheme to generate pseudo-labels for each pixel and thus make the update of pixel-feature into class-wise prototypes become possible. In sum-mary, the main contributions of this paper are three aspects:
• We investigate the pitfalls of point cloud semantic segmentation on distillation-based methods and find that a 1Only 5% image pixels are matched to LiDAR points for a typical 32-beam LiDAR scanner as presented in BEVFusion [22]. large number of image features are not well-utilized and point features are not well distillation-benefited.
• We introduce the prototype bank concept into point cloud semantic segmentation and propose a novel approach
ProtoTransfer to successfully overcome the above pitfalls.
• We conduct experiments on both SemanticKITTI and nuScenes benchmarks to demonstrate the effectiveness of our approach and also achieve a 2nd place on the competi-tive nuScenes Lidarseg leaderboard. 2.