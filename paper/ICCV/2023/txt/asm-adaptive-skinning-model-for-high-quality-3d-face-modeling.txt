Abstract
The research fields of parametric face model and 3D face reconstruction have been extensively studied. How-ever, a critical question remains unanswered: how to tai-lor the face model for specific reconstruction settings. We argue that reconstruction with multi-view uncalibrated im-ages demands a new model with stronger capacity. Our study shifts attention from data-dependent 3D Morphable
Models (3DMM) to an understudied human-designed skin-ning model. We propose Adaptive Skinning Model (ASM), which redefines the skinning model with more compact and fully tunable parameters. With extensive experiments, we demonstrate that ASM achieves significantly improved capacity than 3DMM, with the additional advantage of model size and easy implementation for new topology. We achieve state-of-the-art performance with ASM for multi-view reconstruction on the Florence MICC Coop bench-mark. Our quantitative analysis demonstrates the impor-tance of a high-capacity model for fully exploiting abun-dant information from multi-view input in reconstruction.
Furthermore, our model with physical-semantic parameters can be directly utilized for real-world applications, such as in-game avatar creation. As a result, our work opens up new research direction for parametric face model and facil-itates future research on multi-view reconstruction. 1.

Introduction
A key preliminary decision factor for 3D face mod-eling is a proper choice of face representation, as there is no one representation that fits all.
For reconstruc-tion with abundant constraints from multiple calibrated images (high-end), high capacity in the form of raw 3D points is essential to achieve high-fidelity scans with fine-grained details within the Multi-view Stereo (MVS) frame-*Corresponding author. work [5, 6, 12, 13, 20]. For reconstruction with a sin-gle in-the-wild image (low-end), an intrinsically ill-posed problem, parametric face models with a strong prior are indispensable to ensure robust reconstruction with consis-tent topology [10, 11, 36, 18]. Reconstruction with multi-view uncalibrated images (middle-end) is a previously less explored scenario with performance on par with the low-end setting, and far behind the high-fidelity scans in the high-end setting. This suggests that the additional con-straints from multi-view uncalibrated images are not fully exploited. Previous studies in this category [1, 15, 33, 3, 2] have used parametric face models interchangeably with the low-end setting. We contend that parametric face mod-els with a higher representation capacity should be em-ployed to accommodate extra constraints from multi-view images. Consequently, this study investigates the design of high-capacity parametric face models for reconstruction with multi-view uncalibrated images. This understudied scenario is increasingly relevant in real-world applications due to the widespread use of high-quality camera-equipped mobile phones and the need for precise reconstruction for applications such as avatar creation and facial animation.
The parametric face model is an extensively researched field. The majority of studies are based on the 3D Mor-phable Model (3DMM), originally introduced in the pio-neering work of Blanz and Vetter [7]. Subsequent studies have continued to refine the 3DMM method by either im-proving the amount and diversity of data [19, 34] or propos-ing new methods [26, 30, 8] for dimensional reduction given such data. Simultaneously, a different trend has emerged in the game and film industries, where parametric face models are primarily represented in the form of human-designed skinning models. These models employ a set of control-lable bones and skinning weights, which determine the de-gree to which each vertex on the mesh is influenced by the surrounding bones. This representation has demonstrated sufficient capability for extensive applications such as fa-cial animation and avatar customization [16, 28, 29].
Comparing human-designed skinning models with data-dependent 3DMMs for 3D face modeling presents an in-triguing yet understudied topic. These two models are fundamentally different in terms of constraint mechanism and capacity scaling. While 3DMMs derive their con-straints from data, skinning models acquire proper con-straints through the design process, such as converting em-pirical knowledge of real faces into the placement of bones and the definitions of skinning weights. Regarding capac-ity scaling, 3DMMs heavily rely on the collection of facial scan data, which is prohibitively expensive to scale. In con-trast, the capacity of skinning models can be easily scaled by merely adjusting the number of parameters for bones and skinning weights, making it a more cost-effective and ideal candidate for high-capacity parametric face models.
With a closer look into standard skinning models with the vanilla Linear Blend Skinning (LBS), we find that their capacity can be further improved. Standard skinning mod-els, which typically feature hundreds of bones on tens of thousands of vertices, usually posses tens of parameters for bone position, hundreds of parameters for transformation, and millions of parameters for skinning weights. These ex-tensive skinning weights must be determined beforehand and remain fixed during subsequent 3D face modeling.
They are usually determined either by professional anima-tors or through data-driven learning [21, 22], with certain initial estimations [4]. Since skinning weights depend on bone position, which also needs to be predefined and fixed, transformation remains the sole variable in face modeling.
Within this paradigm, improving model capacity relies on increasing the number of bones or refining predefined skin-ning weights. We refer to these standard skinning models as Static Skinning Models (SSM). We argue that the cur-rent paradigm of SSM fundamentally limits capacity, as the critical skinning weights are fixed.
A neglected fact is that skinning weights, despite being defined in the form of a high-dimensional matrix, invariably result in low-dimensional patterns that are smooth, concen-trated, and sparse. Given the strong structural nature of the human face, the movement space of each vertex is highly correlated and constrained. Consequently, skinning weights do not necessitate high-dimensional definition initially. We introduce the Adaptive Skinning Model (ASM), which de-fines skinning weights in a more compact form using the
Gaussian Mixture Model (GMM). This new design signifi-cantly reduces the dimension of skinning weights to a level comparable with the transformation matrix. As a result, all parameters of skinning weights, transformation, and bone position can be simultaneously solved during reconstruc-tion. This eliminates not only the labor-intensive manual design required in SSM but also the need for training data as in 3DMMs. Compared to SSM, our model can achieve a significantly increased capacity with even fewer total pa-rameters.
The main contributions of this paper are as follows:
• A novel parametric face model is proposed, named ASM, by redefining skinning model with fully tunable param-eters via introducing a more compact skinning weights representation with Gaussian Mixture Model.
• We demonstrate that ASM outperforms existing models in terms of capacity, model size, ease of implementation with arbitrary topology, and manual editing with semantic parameters. Moreover, it eliminates the need for labori-ous manual design and costly training data collection.
• State-of-the-art performance in 3D face reconstruction with multi-view uncalibrated images is achieved using
ASM. 2.