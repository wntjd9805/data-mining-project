Abstract
Transformers have become the primary backbone of the computer vision community due to their impressive perfor-mance. However, the unfriendly computation cost impedes their potential in the video recognition domain. To opti-mize the speed-accuracy trade-off, we propose Semantic-aware Temporal Accumulation score (STA) to prune spatio-temporal tokens integrally. STA score considers two crit-ical factors: temporal redundancy and semantic impor-tance.
The former depicts a specific region based on whether it is a new occurrence or a seen entity by ag-gregating token-to-token similarity in consecutive frames while the latter evaluates each token based on its contri-bution to the overall prediction. As a result, tokens with higher scores of STA carry more temporal redundancy as well as lower semantics thus being pruned. Based on the
STA score, we are able to progressively prune the tokens without introducing any additional parameters or requir-ing further re-training. We directly apply the STA module to off-the-shelf ViT and VideoSwin backbones, and the em-pirical results on Kinetics-400 and Something-Something
V2 achieve over 30% computation reduction with a neg-ligible ∼ 0.2% accuracy drop. The code is released at https://github.com/Mark12Ding/STA. 1.

Introduction
Recently, there has been an unstoppable shift in the gen-eral backbone design from Convolutional Neural Networks (ConvNets) to Transformers, which are originally employed in natural language processing, and has shown promising potential for various vision tasks [8, 48, 47, 22, 1, 32, 5].
The key component of Transformers is the self-attention mechanism, which is apt to capture long-range dependen-cies and empowers ViT to perceive the global reception field. The seminal work, Vision Transformer (ViT) [8]
*Work done during an internship at Huawei Cloud.
†Corresponding author. Email: tian.qi1@huawei.com.
Figure 1: Kinectics-400 result for ViT and VideoSwin. The bubble’s area is proportional to FLOPs of a variant in a model family. ViT/VideoSwin here takes 16/32 × 2242 video. The proposed STA saves over 30% FLOPs for all model variants with a negligible drop in performance. closely follows the original Transformer architecture [38].
Equipped with a large-scale model and dataset, ViT outper-forms ConvNets in image classification by a considerable margin. Inspired by this superior scaling behavior, Trans-formers have gained popularity as a backbone choice and are widely adopted for image recognition [22, 37], action recognition [1, 23], semantic segmentation [49, 6], action detection [43, 12], temporal perception [33, 34], etc.
Despite the promising potential of Transformers in spatio-temporal vision tasks, such as action recognition, the quadratic increase in complexity caused by the temporal dimension makes the video Transformers computationally unfriendly compared to images. For instance, earlier work
TimeSformer [2], which applies Transformer backbone for video, required 7.14 Tera FLOPs to achieve 80.7% accuracy on the Kinetics-400 action recognition benchmark. The ex-cessive computational cost makes it impractical for deploy-ment in real-world scenarios. Therefore, there is an urgent need to explore ways to profit from the performance gains
of Transformers while maintaining an affordable computa-tion burden.
Fortunately, Transformers can handle a flexible number of tokens as input. Recent attempts [31, 30, 42], that dy-namically prune tokens for images, have remarkably re-duced computation overhead. These pruning approaches have inspired us to explore token pruning in the video do-main, so as to balance accuracy and computation costs ac-cordingly. However, performing frame-wise pruning alone seems to be not optimal since it ignores temporal context and disrupts the dynamic structure of the video. To ad-dress this issue, recent work STTS [40] decouples token pruning into temporal and spatial dimensions. Specifically,
STTS first drops meaningless frames and then filters out detail-rich regions from the remained frames. However, this spatio-temporal decoupling strategy lacks contextual mod-eling of continuous temporal information, leading to limited performance.
In this paper, we argue that pruning spatio-temporal to-kens integrally can lead to further computation reduction at an acceptable cost of accuracy degradation. To this end, we propose the Semantic-aware Temporal Accumulation (STA) score to depict the importance of each token. We take two factors into consideration, temporal redundancy and semantic importance. Our motivation is to discard to-kens that have similar counterparts appearing earlier in the sequence while retaining only semantically significant to-kens. As an example, static backgrounds across all times-tamps contain highly repetitive information that is unnec-essary to be included. Therefore, keeping only a few rep-resentative background patches is sufficient for reasoning.
Specifically, we evaluate the temporal redundancy of a re-gion by determining whether it is a novel or previously ob-served entity. In practice, we aggregate repetitive informa-tion on a per-frame basis and assign each token a score of temporal repetition degree. Nevertheless, there are cases where a repetitive region reveals a crucial action and should be retained. For example, if the sequence of tokens de-scribes human-body motion, it is necessary to keep all the tokens for better understanding, even if there are only slight differences over time. Thus, we also take semantic impor-tance into account during the pruning procedure. To depict each token’s semantic contribution to video recognition, we take the summation of the feature activation map and then integrate this summation with the score of temporal aggre-gation to enhance the awareness of semantics. Based on the
STA score, we progressively prune the tokens of the video
Transformers three times. The whole pruning process does not introduce any tuning parameter and directly accelerates the off-the-shelf video Transformers without the need to re-train.
We apply our pruning strategy to two mainstream video
Transformers, ViT [8] and VideoSwin [23], and evalu-ate 10 off-the-shelf backbones on two action recognition benchmarks, Kinetics-400 [14] and Something-Something
V2 [11], to demonstrate the effectiveness of our method.
As shown in Figure 1, we achieve significant computa-tion reduction with a negligible accuracy drop on Kinetics-400. For instance, using ViT-H as the backbone, by hier-archically pruning 57% of the input tokens, STA reduces 49% FLOPs while the accuracy drop is only 0.2%. Be-sides, with STA, FLOPs of VideoSwin-B are decreased by 38% while maintaining 82.5% accuracy with only a mini-mal drop of 0.2%. A similar trend can also be observed in the Something-Something V2 dataset. Notably, we surpass
STTS [40] by a 0.4% accuracy gain with 40% fewer FLOPs on Kinetics-400 and by a 0.5% accuracy increase with 20% fewer FLOPs on Something-Something V2 when using the same backbone. 2.