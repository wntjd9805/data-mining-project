Abstract
Classifier guidance—using the gradients of an image classifier to steer the generations of a diffusion model— has the potential to dramatically expand the creative con-trol over image generation and editing. However, currently classifier guidance requires either training new noise-aware models to obtain accurate gradients or using a one-step denoising approximation of the final generation, which leads to misaligned gradients and sub-optimal control.We highlight this approximation’s shortcomings and propose a novel guidance method: Direct Optimization of Diffu-sion Latents (DOODL), which enables plug-and-play guid-ance by optimizing diffusion latents w.r.t. the gradients of a pre-trained classifier on the true generated pixels, using an invertible diffusion process to achieve memory-efficient backpropagation. Showcasing the potential of more pre-cise guidance, DOODL outperforms one-step classifier guidance on computational and human evaluation metrics across different forms of guidance: using CLIP guidance to improve generations of complex prompts from DrawBench, using fine-grained visual classifiers to expand the vocabu-lary of Stable Diffusion, enabling image-conditioned gen-eration with a CLIP visual encoder, and improving image aesthetics using an aesthetic scoring network. 1.

Introduction
Text-conditioned denoising diffusion models (DDMs), such as Latent/Stable Diffusion, Imagen and numerous oth-ers, have been widely adopted for synthesizing realistic im-ages given an input text prompt [35, 36, 38, 34, 2]. The con-ditioning setup requires DDMs to be trained using paired data of images and the conditioning modality—image-caption pairs in the case of text conditioning. Once trained, the DDM can be steered to generate images using the condi-tioning modality. However, the conditioning paradigm con-stricts the image generation capabilities of a trained DDM model; a text-conditioned diffusion model cannot readily utilize other modalities such as depth maps or image clas-sifiers or audio as conditioning signals. While DDMs con-ditioned on multiple modalities have been trained [30, 36] a “plug-and-play” approach that allows for a pretrained
DDM to be guided by any external function that determines whether some generation criterion is satisfied is desirable.
In principle, classifier guidance [43, 8] enables this ca-pability in DDMs. Classifier guidance, named so since it was first demonstrated using pretrained image classification models, combines the score estimate of the diffusion model with the gradient of the image classifier to steer the genera-tion process to produce images that correspond to a partic-ular class. Any differentiable loss function can be used for classifier guidance. In addition to class-conditional gener-ation, it has been shown to improve compositionality using cross-modal guidance [24]. There are two existing ways of incorporating classifier guidance. In the first approach, we train a noise-aware classifier that can be used to com-pute an accurate gradient w.r.t. an intermediate generation step [43, 8]. This approach requires re-training the classifier model, which can be computationally expensive/infeasible
In the second ap-due to lack of access to training data. proach, at a time step t, we denoise the image with a single application of the DDM and compute the gradient using this approximately denoised image [24]. This one-step approxi-mation is necessitated by the prohibitive memory require-ments of computing a gradient w.r.t. the latents through the entire diffusion process containing many steps. Since this approach only obtains gradients using an one-step de-noising approximation of the final generation, the approxi-mately denoised images are often misaligned with the final generations which classifier guidance is aiming to modify (Figure 2), leading to sub-optimal guidance signal.
To enable flexible and exact model guidance, without noise-aware classifiers or approximations, we propose Di-rect Optimization Of Diffusion Latents (DOODL). DOODL optimizes the initial diffusion noise vectors w.r.t. a model-based loss on images generated from the full-chain diffusion process. We leverage EDICT, a recently developed drop-in discretely invertible diffusion algorithm [46], which admits backpropagation with constant memory cost w.r.t the num-ber of diffusion steps, to compute classifier gradients on the pixels of the final generation w.r.t. the original noise vec-tors. This enables efficient iterative optimization of diffu-sion latents w.r.t. any differentiable loss on the image pixels
Figure 2: Stable Diffusion with prompt “A kangaroo in a field” using 50 DDIM steps. vs. a single step. The one-step approximation is inaccurate for high noise levels, even though it is relied upon by classifier guidance. DOODL cal-culates loss gradients with respect to fully formed realistic images as opposed to approximations. and accurate calculation of gradients for classifier guidance.
We demonstrate the efficacy of DOODL through a di-verse set of guidance signals (Figure 1) using quantitative and human evaluation studies. First, we show that CLIP classifier guidance using DOODL improves generation of images guided by text prompts from the DrawBench [38] dataset, which test compositionality and the ability to guide using unusual captions. Second, we show the ability to ex-pand the vocabulary of a pretrained Stable Diffusion model using fine-grained visual classifiers; a capability that one-step classifier guidance does not have. Third, we demon-strate that DOODL can be used for personalized entity gen-eration (e.g. “A dog in sunglasses”), with zero retraining of any new network—a first to our knowledge. Finally, we utilize DOODL to perform a novel task; increasing the per-ceived aesthetic quality of generated/real images. We hope that DOODL can enable and inspire diverse plug-and-play capabilities for pretrained diffusion models. 2.