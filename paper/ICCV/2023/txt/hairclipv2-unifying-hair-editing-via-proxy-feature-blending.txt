Abstract
Hair editing has made tremendous progress in recent years. Early hair editing methods use well-drawn sketches or masks to specify the editing conditions. Even though they can enable very fine-grained local control, such interaction modes are inefficient for the editing conditions that can be easily specified by language descriptions or reference im-ages. Thanks to the recent breakthrough of cross-modal models (e.g., CLIP), HairCLIP is the first work that en-ables hair editing based on text descriptions or reference images. However, such text-driven and reference-driven interaction modes make HairCLIP unable to support fine-grained controls specified by sketch or mask. In this paper, we propose HairCLIPv2, aiming to support all the afore-mentioned interactions with one unified framework. Simul-taneously, it improves upon HairCLIP with better irrelevant attributes (e.g., identity, background) preservation and un-seen text descriptions support. The key idea is to convert all the hair editing tasks into hair transfer tasks, with editing conditions converted into different proxies accordingly. The editing effects are added upon the input image by blend-† Wenbo Zhou is the corresponding author. ing the corresponding proxy features within the hairstyle or hair color feature spaces. Besides the unprecedented user interaction mode support, quantitative and qualitative experiments demonstrate the superiority of HairCLIPv2 in terms of editing effects, irrelevant attribute preservation and visual naturalness. Our code is available at https:
//github.com/wty-ustc/HairCLIPv2. 1.

Introduction
Hair editing as an interesting and challenging problem has attracted a lot of research attention from both academia tremendous and industry. Over the past few decades, progress [50, 41, 31, 36] has been made in this field, en-abling high-fidelity hair editing based on various types of user interactions or controls. In earlier hair editing meth-ods [50, 41], commonly supported user editing conditions are sketches and masks, which can enable fine-grained local controls. But in real scenarios, many hair editing conditions can be specified by simpler interactions, e.g., text descrip-tions (e.g., “bowl cut hairstyle”) and reference images.
Recently, cross-modal visual and language representa-tion learning [29, 22, 43, 8, 35, 38, 45] has made remark-able breakthrough, which makes text-guided image manipu-Aligned Hair Transfer
Unaligned Hair Transfer
Text
Mask
Sketch
Local Hairstyle Editing
Local Hair Color Editing
HairCLIP [36] (cid:34) (cid:34) (cid:34) (cid:37) (cid:37) (cid:37) (cid:37)
LOHO [24] (cid:34) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37)
Barbershop [47] (cid:34) (cid:37) (cid:37) (cid:34) (cid:37) (cid:37) (cid:37)
HairNet [48] (cid:34) (cid:34) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37)
SYH [20] MichiGAN [31] (cid:34) (cid:34) (cid:37) (cid:34) (cid:37) (cid:37) (cid:37) (cid:34) (cid:37) (cid:37) (cid:34) (cid:34) (cid:34) (cid:37)
SketchSalon [41] (cid:37) (cid:37) (cid:37) (cid:37) (cid:34) (cid:34) (cid:34)
Ours (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34)
Table 1. Comparisons between our approach and mainstream hair editing methods in terms of available interaction modes and functionality.
Only our method supports all interaction modes and enables both global and local hair editing. lation possible. HairCLIP [36] presents the first attempt that supports hair editing via text description and reference im-age within one unified framework. Despite such text-driven and reference-driven interaction being more efficient and user-friendly, HairCLIP cannot support fine-grained con-trols like sketches and masks. Moreover, HairCLIP has two other limitations: 1) Since hair editing in HairCLIP is accomplished by pure latent code manipulation, it will in-evitably alter other irrelevant attributes (e.g., identity, back-ground) because fully decoupling different attributes in la-tent codes is difficult; 2) It struggles in yielding satisfactory results for text descriptions that differ significantly from training texts.
In this paper, we take a step forward and propose Hair-CLIPv2, a unified hair editing system that unprecedentedly supports all the aforementioned interaction modes, includ-ing the natural text/reference-driven interaction and fine-grained local interaction.
In Table 1, we list the interac-tion modes and editing functionality supported by existing hair editing methods. Moreover, with fundamentally differ-ent editing mechanism design, HairCLIPv2 makes great im-provement upon HairCLIP with better irrelevant attributes preservation and unseen text description support.
The key idea of HairCLIPv2 is converting all the hair editing tasks into hair transfer tasks, and the editing con-ditions are converted into different transfer proxies accord-ingly. Conceptually, it can be understood as “find proxy hair images that satisfy the editing conditions and transfer the corresponding attributes to the source image”. Note that we use the StyleGAN latent code or feature corresponding to such proxies rather than use the proxy images explicitly.
More specifically, we first transform the input source im-age into the bald proxy, which inpaints the hair-covered re-gions (e.g., background, ears) with reasonable semantic at-tributes. This can help avoid the editing artifacts caused by occlusion when blending the source image with condition proxies. For different editing proxies, we define their gener-ation as different tasks performed in StyleGAN according to their characteristics. Depending on the users’ editing pref-erences, hair editing effects are then enforced upon the input image by blending the corresponding proxy features within the hairstyle feature space or hair color feature space. This is different from HairCLIP that achieves the editing effect by manipulating the 1-d latent codes. Such feature blending based editing naturally supports global and local hair edit-ing by controlling the blending area to cover the entire hair area or part of it.
To show the superiority of HairCLIPv2, we conduct ex-tensive comparisons. In addition to more complete user in-teraction modes support, HairCLIPv2 also shows obvious advantages in terms of manipulation accuracy, irrelevant at-tribute preservation, and visual naturalness. Some interac-tive editing examples are provided in Figure 1. Our contri-butions can be summarized as below:
• We present a fresh perspective for hair editing tasks and propose a novel hair editing paradigm that uni-fies various types of editing into the form of proxy hair transfers. We achieve all editing effects with the fea-ture blending mechanism, which not only alleviates the editing pressure on each proxy but also enables excel-lent irrelevant attribute preservation.
• We dedicately design the proxy generation for differ-ent conditions based on their own special properties, e.g., for the text proxy, the decoupled proxy design and optimization starting point selection strategy help us achieve better editing effects and arbitrary text sup-port; for the sketch proxy, we achieve local hairstyle editing support for the first time within the StyleGAN-based framework by formalizing its generation as the image translation task and incorporating insights of se-mantic layering in StyleGAN.
• Our system pushes the interactions of hair editing to a new level, supporting arbitrary text, mask, reference image, sketch and their combinations, and enabling both global and local hair editing, which has never been realized before. 2.