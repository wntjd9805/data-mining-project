Abstract
In this paper, we propose a novel cross-modal distillation method, called TinyCLIP, for large-scale language-image pre-trained models. The method introduces two core tech-niques: affinity mimicking and weight inheritance. Affin-ity mimicking explores the interaction between modalities during distillation, enabling student models to mimic teach-ers’ behavior of learning cross-modal feature alignment in a visual-linguistic affinity space. Weight inheritance transmits the pre-trained weights from the teacher mod-els to their student counterparts to improve distillation effi-ciency. Moreover, we extend the method into a multi-stage progressive distillation to mitigate the loss of informative weights during extreme compression. Comprehensive ex-periments demonstrate the efficacy of TinyCLIP, showing that it can reduce the size of the pre-trained CLIP ViT-B/32 by 50%, while maintaining comparable zero-shot perfor-mance. While aiming for comparable performance, distil-lation with weight inheritance can speed up the training by 1.4 - 7.8× compared to training from scratch. Moreover, our TinyCLIP ViT-8M/16, trained on YFCC-15M, achieves an impressive zero-shot top-1 accuracy of 41.1% on Im-ageNet, surpassing the original CLIP ViT-B/16 by 3.5% while utilizing only 8.9% parameters. Finally, we demon-strate the good transferability of TinyCLIP in various down-stream tasks. Code and models will be open-sourced at aka.ms/tinyclip. 1.

Introduction
Large-scale language-image pretraining, e.g., CLIP [46], has recently gained significant attention due to its remark-able zero-shot transfer capability [46] and unprecedented performance in text-to-image generation [47]. Due to com-plex nature of vision and language, current approaches of-ten resort to utilizing huge amounts of parameters to endow models with cross-modal capabilities [46, 21, 71, 72, 43, 39,
∗ Equal contribution. Kan and Zhenghong were interns of Microsoft.
† Corresponding: houwen.peng@microsoft.com
Figure 1. Comparison of the OpenCLIP [20] and TinyCLIP. Tiny-CLIP is pre-trained and distilled on LAION-400M [50] using
OpenCLIP ViT-B/32 [20] as the teacher, whose zero-shot top-1 accuracy is 65.6%. 1]. This in turn leads to high costs in terms of storage, mem-ory, and computation time for these models, which moti-vates the need for model compression to make them smaller and faster for real-world applications [64, 13].
As a core compression technique, knowledge distillation has been extensively studied and applied in single-modal settings [19, 16]. However, its potential for multi-modality remains underexplored. Unlike single-modal models, the distillation of language-image cross-modal models poses distinct challenges. First, CLIP-like language-image mod-els commonly consist of two branches: an image encoder and a text encoder [21, 72, 52, 73]. When distilling such multi-branch models, it is crucial to consider the interac-tion of information across the different modality branches in both the teacher and student models. Second, the original
CLIP [46] models are pre-trained on 400 million image-text pairs for 32 epochs, which requires thousands of GPU days, making distillation a significant challenge when computa-tional resources are limited. Is there any way to reduce the cost of CLIP distillation?
To tackle these challenges, we present a novel cross-modal distillation method dubbed TinyCLIP, which intro-duces two key techniques: affinity mimicking and weight inheritance. In contrast to the methods that rely on either
image or text features for distillation, we empirically show that distilling knowledge in an image-text affinity space is more effective. Specifically, we leverage the cosine similar-ity of the image and text embeddings in the teacher model to facilitate the distillation of the student model, allowing the student to mimic the teacher’s visual-linguistic feature alignment. We refer to this process as affinity mimicking.
To improve distillation efficiency, we introduce weight inheritance, a technique transferring the pre-trained weights from teacher models to their student counterparts. Since in-heriting weights provides a good initialization for the stu-dent models, the distillation progress can be largely accel-erated. The key challenge of weight inheritance lies in de-termining which weights are more advantageous. To ad-dress this issue, we introduce two solutions: manual and automatic inheritance. We surprisingly found that a simple manual selection of k-dimension or k-layer weights from the teacher model can yield satisfactory results for CLIP distillation. On the other hand, we also introduce learnable masks to automatically identify the most important weights from the teacher model. The masks are imposed indepen-dently on the vision and language branches, enabling them to capture the differences across modalities.
Moreover, we extend the proposed weight inheritance to a multi-stage progressive procedure, where each subsequent stage automatically inherits the important weights from the preceding stages. We observed that when the teacher model exhibits higher performance and shares a similar architec-ture with the student, weight inheritance can provide better results. This is because significant architecture differences may undermine the learned weights when transmitting from the teacher to the student. Therefore, we break the inheri-tance into multiple stages, allowing the student model in each stage to share a more similar structure with the prede-cessor teacher and inherit the weights progressively.
Our experiments show that TinyCLIP delivers compet-itive models at all levels of speedups and model sizes in ImageNet zero-shot evaluation and various downstream tasks. As shown in Fig. 1, pre-trained on the same LAION-400M dataset [50], TinyCLIP-ViT using 63M parame-ters achieves 61.4% zero-shot top-1 accuracy on ImageNet
[7], being 2.0× smaller and 1.5× faster than the Open-CLIP [20] model (62.9% accuracy with 126M parameters).
Meanwhile, TinyCLIP-ResNet with 38M parameters ob-tains 56.4% zero-shot top-1 accuracy, being 2.1× smaller and 2.0× faster than CLIP ResNet-50 (59.6% accuracy with 76M parameters). Moreover, our method can speed up the original OpenCLIP training by 1.4× – 7.8× while achiev-ing similar performance. Also, TinyCLIP demonstrates good transfer capacities in downstream scenarios. In sum-mary, the contributions of this work are two-fold:
• We propose a new cross-modal distillation approach to unleash the capacity of small CLIP models, fully lever-aging large-scale models as well as pre-training data. To our best knowledge, this is the first work exploring the pre-training distillation of language-image models.
• We present state-of-the-art language-image pre-trained models at small scale, striking the best trade-off between speed and accuracy. Extensive experiments demonstrate the superiority and good generalization ability of the small models in various downstream tasks. 2.