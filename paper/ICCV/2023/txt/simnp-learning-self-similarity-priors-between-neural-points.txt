Abstract 1.

Introduction
Existing neural field representations for 3D object re-construction either (1) utilize object-level representations, but suffer from low-quality details due to conditioning on a global latent code, or (2) are able to perfectly reconstruct the observations, but fail to utilize object-level prior knowl-edge to infer unobserved regions. We present SimNP, a method to learn category-level self-similarities, which com-bines the advantages of both worlds by connecting neural point radiance fields with a category-level self-similarity representation. Our contribution is two-fold. (1) We de-sign the first neural point representation on a category level by utilizing the concept of coherent point clouds. The re-sulting neural point radiance fields store a high level of de-tail for locally supported object regions. (2) We learn how information is shared between neural points in an uncon-strained and unsupervised fashion, which allows to derive unobserved regions of an object during the reconstruction process from given observations. We show that SimNP is able to outperform previous methods in reconstructing sym-metric unseen object regions, surpassing methods that build upon category-level or pixel-aligned radiance fields, while providing semantic correspondences between instances.
The human visual system succeeds in deriving 3D repre-sentations of objects just from incomplete 2D observations.
Key to this ability is that given observations are successfully complemented by previously learned information about the 3D world. Replicating this ability has been a longstanding goal in computer vision.
Since the task of reconstructing complete objects re-lies on generalization from a set of known examples, deep learning is an intuitive solution. The common approach is to use large amounts of data to train a category-level model [35, 41, 54, 29, 13, 26, 25, 22, 31, 9, 44, 57] and let the reconstruction process combine observations with the prior knowledge learned from data, which we refer to as the data prior. Notably, this introduces an inherent trade-off be-tween contributions of the data prior and the observations.
On one extreme of the spectrum, NeRF-like meth-ods [32, 55, 4] do not use a data prior at all. With a high number of observations and an optimization process, they are able to nearly perfectly reconstruct novel views of scenes and objects. However, this renders them incapable of deriving unseen regions. On the other extreme of the spec-trum are methods that learn the full space of radiance or
signed distance functions belonging to a specific object cat-egory, such as SRN [41] and DeepSDF [35]. While these methods succeed in learning a complete representation of objects on an abstract level, they fail to represent individ-ual details and the reconstruction process often performs retrieval from the learned data prior [42]. A similar be-haviour has also been observed for generative models based on GANs [6] or diffusion [2], which yield visually impres-sive results but still diverge from given observations.
The key challenge lies in combining the strengths of the methods from both ends of the spectrum. While one can perform a highly detailed reconstruction of the visible re-gions, the object model should allow to reuse this informa-tion in unseen regions. To this end, it is important to know that most objects show many structured self-similarities, often arising from different types of symmetries, such as point/plane symmetries or more general variants. None of the current approaches try to explicitly learn such self-similarities to perform better inference.
This is where SimNP comes in. We propose a better data prior vs. observation trade-off by combining the best of both worlds: (1) a category-level data prior encoding self-similarities on top of a (2) local representation with test-time optimization. Instead of learning the full space of ra-diance functions for a given category, we move to learning a data prior one level of abstraction higher, i.e., we learn how information can be shared between local object re-gions. This enables us to learn characteristic self-similarity patterns from training data, which are used to propagate in-formation from visible to invisible parts during inference.
As learning a representation for category-level self-similarities implies modeling relationships between local regions of objects, a key observation in this work is that neural point representations are especially well-suited to de-scribe such relationships. Besides their capacity to capture high-frequency patterns, the underlying sparse point cloud allows for explicit formulations of similarities.
In summary, the contributions of our work are: 1. We present the first generalizable neural point radiance field for representation of objects categories. 2. We propose a simple but effective mechanism that learns general similarities between local object regions in an unconstrained and unsupervised fashion. 3. We show that our model improves upon state of the art in reconstructing unobserved regions from a single im-age and by outperforming existing two-view methods by a large margin. At the same time, it is more efficient in training and rendering. 2.