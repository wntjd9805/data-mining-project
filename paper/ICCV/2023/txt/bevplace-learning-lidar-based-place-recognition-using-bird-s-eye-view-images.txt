Abstract
Place recognition is a key module for long-term SLAM systems. Current LiDAR-based place recognition meth-ods usually use representations of point clouds such as un-ordered points or range images. These methods achieve high recall rates of retrieval, but their performance may de-grade in the case of view variation or scene changes. In this work, we explore the potential of a different representation in place recognition, i.e. bird’s eye view (BEV) images. We validate that, in scenes of slight viewpoint changes, a simple
NetVLAD network trained on BEV images achieves com-parable performance to the state-of-the-art place recogni-tion methods. For robustness to view variations, we pro-pose a rotation-invariant network called BEVPlace. We use group convolution to extract rotation-equivariant local features from the images and NetVLAD for global feature aggregation. In addition, we observe that the distance be-tween BEV features is correlated with the geometry distance of point clouds. Based on the observation, we develop a method to estimate the position of the query cloud, extend-ing the usage of place recognition. The experiments con-ducted on large-scale public datasets show that our method 1) achieves state-of-the-art performance in terms of recall rates, 2) is robust to view changes, 3) shows strong gen-eralization ability, and 4) can estimate the positions of query point clouds. Source codes are publicly available at https://github.com/zjuluolun/BEVPlace . 1.

Introduction
Place recognition plays an important role in both the map construction and localization phases of long-term Simulta-*Corresponding author.
Figure 1. (a) Two range images projected from two point clouds of
KITTI that are 5 meters apart from each other. A small point cloud translation will cause distortions such as scale changes and occlu-sion. (b) The corresponding BEV images. The scale and position distribution of objects on the road almost remains unchanged. (c)
Performance on various datasets. A simple BEV-based NetVALD network achieves comparable Top-1 recall to the SOTA methods.
Our BEVPlace enhances the baseline further. neous Localization and Mapping (SLAM) systems [3]. In the map construction phase, it can provide loop closure con-straints to eliminate the accumulated drift of the odometry.
In the localization phase, it can re-localize the system when the pose tracking is lost and improve the robustness of the system. In recent years, lots of image-based place recogni-tion methods [8, 2, 23] have been developed and achieved satisfactory performance. However, these methods are vul-nerable to illumination changes and view variation due to the imaging mechanism of camera sensors. On the contrary, point clouds of LiDAR sensors are robust to illumination changes due to active sensing. In addition, the availability of precise depth information can help more accurate place recognition [1, 19].
LiDAR-based place recognition can be regarded as a retrieval problem, that is, finding the most similar frame to a query from a pre-built database. The key to solv-ing this problem is to generate a global feature that can model the similarity between point clouds. PointNetVLAD
[1] gives the first deep-learning solution to the problem of large-scale LiDAR-based place recognition. It uses Point-Net [27] to extract local features from unordered points and
NetVLAD [2] to generate global features. There are lots of subsequent methods that follow PointNetVLAD and in-troduce auxiliary modules such as attentions [36, 30], hand-crafted features [19], and sparse convolution [15]. Recently, some methods [4, 22] based on range images have been developed. The range image is the sphere projection of a point cloud. Due to the projection mechanism, the trans-lation of the range image is equivariant to the rotation of the point cloud. Based on this, OverlapTransformer [22] uses a convolution network and a transformer to extract rotation-invariant features from the images. Some methods
[13, 14, 4] use similar projections and also achieve place recognition robust to view changes.
Although the aforementioned methods have made great progress, they still have limitations in terms of general-ization ability. This is because both unordered points and range images used for place recognition are sensitive to the motions of the LiDAR sensor. Specifically, for un-ordered points, the point coordinate and the relative posi-tions between points will change severely along with mo-tions of the LiDAR sensor. For range images, the image contents suffer various distortions with translations of point clouds although they are robust to rotations. Current meth-ods [1, 36, 19] force the network to learn these variations of data with data augmentation. However, as pointed out in
[17], data augmentation needs the network to be as flexible as possible to capture all the variations, which may result in the large risk of overfitting and poor generalization ability.
In this work, we explore the potential of place recogni-tion using bird’s eye view (BEV) images. The BEV image is generated by projecting a point cloud to the ground space.
In road scenes, the transformations of point clouds are ap-proximately equivariant to the rotations and translations of
BEV images [20]. Thus, the contents of BEV images are more robust to sensor motions. As shown in Fig. 1, the translation of a point cloud causes little appearance changes in the BEV image but introduces geometry distortions to the range image. The results shown in Fig. 1 (c) validates that a simple NetVLAD based on the BEV representation achieves comparable performance with the state-of-the art methods. To achieve robustness to viewpoint changes, we design a group convolution [31] network to extract local features from BEV images. Then, we use NetVLAD [2] for global rotation-invariant features extraction. Benefit-ing from the design of rotation invariance for BEV images, our method has the strong ability of place retrieval in the cases of both viewpoint variations and scene changes. In addition, we observe that the distances of the BEV features correlate well with the geometry distances of point clouds.
According to this correlation, we map the feature distance to the geometry distance and then estimate the position of the query cloud, which extends the usage of LiDAR-based place recognition.
We summarize the contributions of this paper as follows:
• We experimentally show that, without any delicate design, a simple NetVLAD network based on the
BEV representation outperforms SOTA methods on the KITTI dataset[9] and the benchmark dataset[1].
• We propose a novel LiDAR-based place recognition method called BEVPlace.
The method is robust to view changes, has strong generalization ability, and achieves SOTA performance on three large-scale datasets.
• We explore the statistical correlation between the fea-ture distance and the geometry distance of point cloud pairs. To the best of our knowledge, this paper is the first to perform position estimation directly from global descriptors. 2.