Abstract
With the rapid advances in high-throughput sequencing technologies, the focus of survival analysis has shifted from examining clinical indicators to incorporating genomic pro-files with pathological images. However, existing methods either directly adopt a straightforward fusion of patholog-ical features and genomic profiles for survival prediction, or take genomic profiles as guidance to integrate the fea-tures of pathological images. The former would overlook intrinsic cross-modal correlations. The latter would discard pathological information irrelevant to gene expression. To address these issues, we present a Cross-Modal Transla-tion and Alignment (CMTA) framework to explore the in-trinsic cross-modal correlations and transfer potential com-plementary information. Specifically, we construct two par-allel encoder-decoder structures for multi-modal data to in-tegrate intra-modal information and generate cross-modal representation. Taking the generated cross-modal represen-tation to enhance and recalibrate intra-modal representa-tion can significantly improve its discrimination for com-prehensive survival analysis. To explore the intrinsic cross-modal correlations, we further design a cross-modal at-tention module as the information bridge between different modalities to perform cross-modal interactions and transfer complementary information. Our extensive experiments on five public TCGA datasets demonstrate that our proposed framework outperforms the state-of-the-art methods. The source code has been released †. 1.

Introduction
Survival analysis is a crucial topic in clinical progno-sis research, which aims to predict the time elapsed from a known origin to an event of interest, such as death, relapse of disease, and development of an adverse reaction. Ac-*Corresponding author
†https://github.com/FT-ZHOU-ZZZ/CMTA curate survival prediction is essential for doctors to assess the clinical outcomes for disease progression and treatment efficiency. Traditionally, survival analysis relies on short-term clinical indicators [15, 44] and long-term follow-up reports [1, 4], which are time-consuming and impractical in clinical applications. In recent years, medical image anal-ysis has made significant progress, driven by the success of deep learning techniques. Consequently, an increasing number of researchers are working to model the connection between imaging features and survival events.
Radiology involves the use of medical imaging technolo-gies such as X-rays, CT (Computerized Tomography) scans,
MRI (Magnetic Resonance Imaging) scans, and ultrasound to visualize internal structures and detect abnormalities. Ra-diological images can provide valuable macroscopic infor-mation such as lesion location, morphological texture, and tumor metastasis, which can help predict the prognosis for the patient [14, 32, 28]. However, due to its lower sensi-tivity, radiology is not widely considered as the gold stan-dard in cancer diagnosis. To improve diagnosis accuracy, the pathological examination will be conducted to sample lesion tissues and acquire pathological images, also known as whole slide images (WSIs). Pathological images can provide information about microscopic changes in tumor cells and their microenvironment. Generally, multi-instance learning (MIL) is the most commonly used paradigm in pathology-based survival analysis [46, 40, 42, 30, 7], which can identify and highlight important regions within the pathological image that contribute to the survival event, revealing insights into the underlying pathological pheno-types of disease. Recently, with the rapid advances in high-throughput sequencing technologies, more and more ac-cessible large-scale genomics datasets provide an unprece-dented opportunity to deeply understand the survival events from the molecular perspective [21, 12, 13].
Although survival analysis using single-modality has achieved promising results, combining multi-modal data from different perspectives can provide complementary in-formation for each other. Intuitively, it can increase the sen-sitivity of survival analysis by detecting subtle changes that may not be visible within the single-modality. The most straightforward method is to integrate all features learned from multi-modal data together [5, 8, 3]. Obviously, these methods neglect the potential correlations and interactions between multi-modalities, which is crucial for informa-tion sharing and feature fusion in medical image analysis.
Thereby, the attention mechanism has been introduced to capture the shared context in multi-modalities. For exam-ple, some researchers utilized clinical reports [22, 33] or genomic profiles [39, 9] as guidance for models to focus on the relevant parts of pathological images. Under the su-pervision of advanced knowledge, models can identify use-ful phenotypes and discover possible biomarkers associated with specific gene expression or clinical outcomes.
The above-mentioned cross-modal interaction methods are plausible when the reference modality is superior to other retention modalities. However, in some cases, the performance of pathology-based survival analysis is bet-ter than genomics-based or report-based methods. In such cases, if we still leverage the worse modality as the ref-erence to supervise the feature learning of better retention modalities, the more discriminative information in retention modalities will be contaminated by mediocre information in the reference modality. Moreover, the original purpose of multi-modal medical image analysis is to integrate the complementary information contained in multi-modal data and make more accurate predictions. These attention-based cross-modal interaction methods will discard pathological information irrelevant to gene expression or clinical reports.
In light of these observations, we propose a novel Cross-Modal Translation and Alignment (CMTA) framework to explore the intrinsic cross-modal correlations and transfer potential complementary information. Concretely, we con-struct two parallel encoder-decoder structures for multi-modal data to extract intra-modal representation within single-modality and generates cross-modal representation from cross-modal information. To explore the potential cross-modal correlations, we leverage a cross-modal atten-tion module as the information bridge between different modalities to perform cross-modal interaction and transfer complementary information. The cross-modal representa-tion is utilized to enhance and recalibrate intra-modal rep-resentation. Finally, all intra-modal representations are in-tegrated to yield the final survival prediction. The main con-tributions of this paper can be summarized as follows:
• We propose a novel Cross-Modal Translation and
Alignment (CMTA) framework for survival analy-sis using pathological images and genomic profiles, where two parallel encoder-decoder structures are con-structed for multi-modal data to integrate intra-modal information and generate cross-modal representation.
• We introduce the attention mechanism to design a cross-modal attention module, which is embedded into the encoder-decoder structure to explore the intrinsic cross-modal correlations, perform the potential inter-actions and transfer cross-modal complementary infor-mation between different modalities.
• We conduct extensive experiments on five public
TCGA datasets to evaluate the effectiveness of our proposed model. The experimental results show that our model consistently achieves superior performance compared to the state-of-the-art methods. 2.