Abstract
The combination of Spiking Neural Networks (SNNs) and Transformers has attracted significant attention due to their potential for high energy efficiency and high-performance nature. However, existing works on this topic typically rely on direct training, which can lead to subop-timal performance. To address this issue, we propose to leverage the benefits of the ANN-to-SNN conversion method to combine SNNs and Transformers, resulting in signifi-cantly improved performance over existing state-of-the-art
SNN models. Furthermore, inspired by the quantal synap-tic failures observed in the nervous system, which reduce the number of spikes transmitted across synapses, we in-troduce a novel Masked Spiking Transformer (MST) frame-work. This incorporates a Random Spike Masking (RSM) method to prune redundant spikes and reduce energy con-sumption without sacrificing performance. Our experi-mental results demonstrate that the proposed MST model achieves a significant reduction of 26.8% in power con-sumption when the masking ratio is 75% while maintaining the same level of performance as the unmasked model. The code is available at: https://github.com/bic-L/
Masked-Spiking-Transformer. 1.

Introduction
Spiking neural networks (SNNs), considered as the next generation neural networks [30], are brain-inspired neural networks based on the dynamic characteristics of biological neurons [31, 17]. SNNs have attracted significant attention due to their unique properties in handling sparse data, which can yield great energy efficiency benefits on neuromorphic hardware. Due to their specialties, they have been widely
*Equal contribution.
†Corresponding author
Figure 1. Performance of MST and other SOTA SNN models re-garding top-1 accuracy and time steps. The markers, represented by circles and star shapes, denote the direct training (DT) and the ANN-to-SNN (A2S) conversion method, respectively, where the marker size corresponds to the model size. Results show that the proposed MST model achieves higher accuracy than the other
SNN models. utilized in various fields, such as classification [32, 18], object detection [3] and tracking [51], etc. Nevertheless,
SNNs currently can hardly realize a comparable perfor-mance to that of artificial neural networks (ANNs), espe-cially for complex tasks such as ImageNet [40].
In order to improve the performance of SNNs, various training methods have been proposed, broadly categorized as the direct training method and the ANN-to-SNN con-version method. Direct training methods leverage a con-tinuous relaxation of the non-smooth spiking mechanism to enable backpropagation with a surrogate gradient function for handling non-differentiability [36], but this can lead to unstable gradient propagation and relatively low accuracy
compared to leading ANNs [38]. Alternatively, ANN-to-SNN conversion methods convert pre-trained ANNs into
SNNs for better performance while requiring more time steps, with increased power consumption to reduce con-version errors [46, 25, 2, 7]. Our focus is on implement-ing the ANN-to-SNN conversion method to narrow the per-formance gap between leading ANNs and SNNs, but the required long time steps pose challenges in reducing en-ergy consumption. Therefore, identifying strategies to de-crease power consumption while maintaining excellent per-formance is crucial.
The biological nervous system offers valuable in-sights for addressing the challenges of implementing high-performance Spiking Transformers using the ANN-to-SNN conversion method. The quantal synaptic failure theory suggests that missing information during neuronal signal transmission may not impact the computational informa-tion transmitted to a postsynaptic neuron under certain con-ditions, but can reduce energy consumption and heat pro-duction [23]. Likewise, in the ANN-to-SNN conversion process, missing spikes can possibly be compensated for by leveraging the correlations between signals in the space and time domains during the information propagation over multiple time steps.
In addition, neural network models possess lots of redundant connections: prior works reveal that the redundancy in the self-attention module of Trans-formers can be pruned without significantly impacting per-formance [34, 49]. Therefore, eliminating redundant in-formation during the transmission of neuronal signals can possibly reduce overall energy consumption in the Spiking
Transformer model while preserving high performance.
In our work, we propose a Masked Spiking Trans-former (MST), which incorporates a Random Spike Mask-ing (RSM) method designed specifically for SNNs. The
RSM method randomly selects only a subset of input spikes, significantly reducing the number of spikes involved in the computation process. We evaluate the MST model on both static and neuromorphic datasets, demonstrating its supe-riority over existing SNN models. Our experiments show that the RSM method can reduce energy consumption on the self-attention module and the MLP module in Transformer, enabling the SNNs to take advantage of energy efficiency and high performance. Furthermore, the proposed RSM method is not limited to Transformer, but can be extended to other backbones such as ResNet and VGG, highlighting its potential as a general technique to improve SNN efficiency.
Our results demonstrate the potential of this approach to provide a new direction for developing high-performance and energy-efficient SNN models.
The main contributions of this paper can be summarized as follows:
• We propose a Masked Spiking Transformer (MST) us-ing the ANN-to-SNN conversion method. To the best
Figure 2. Overview of our MST. (a) Schematic of the model ar-chitecture of the Swin Transformer, which is the backbone of our model. (b) Schematic of the proposed Transformer blocks, where
BN layers replace the original LN layers. (c) Conceptual illustra-tion of the RSM method, which involves randomly masking the input spike. (d-e) The RSM method in self-attention and MLP module. of our knowledge, it is the first exploration of applying the self-attention mechanism fully in SNNs utilizing the ANN-to-SNN conversion method.
• The MST model is evaluated on both static and neuro-morphic datasets, and the results show that it outper-forms state-of-the-art (SOTA) SNNs on all datasets.
In specific, the top-1 accuracy of the MST model is 1.21%, 7.3%, and 3.7% higher than the current SOTA
SNN model on the CIFAR-10, CIFAR-100, and Ima-geNet datasets, respectively.
• We design a Random Spike Masking (RSM) method for SNNs trained with the ANN-to-SNN conversion method to prune the redundant spikes during inference and save energy consumption.
• Extensive experiments show that our proposed RSM is a versatile and general method that can be utilized in other spike-based deep networks, such as ResNet and
VGG SNN model variants. 2.