Abstract
In this study, we address the challenge of 3D scene struc-ture recovery from monocular depth estimation. While tra-ditional depth estimation methods leverage labeled datasets to directly predict absolute depth, recent advancements ad-vocate for mix-dataset training, enhancing generalization across diverse scenes. However, such mixed dataset train-ing yields depth predictions only up to an unknown scale and shift, hindering accurate 3D reconstructions. Exist-ing solutions necessitate extra 3D datasets or geometry-complete depth annotations, constraints that limit their ver-satility. In this paper, we propose a learning framework that
*Equal contributions.
†Corresponding author. trains models to predict geometry-preserving depth without requiring extra data or annotations. To produce realistic 3D structures, we render novel views of the reconstructed scenes and design loss functions to promote depth estima-tion consistency across different views. Comprehensive ex-periments underscore our framework’s superior generaliza-tion capabilities, surpassing existing state-of-the-art meth-ods on several benchmark datasets without leveraging extra training information. Moreover, our innovative loss func-tions empower the model to autonomously recover domain-specific scale-and-shift coefficients using solely unlabeled images.
1.

Introduction
Recovering 3D geometries of scenes from monocular images has become an area of significant interest, driven by recent advances in monocular depth estimation, with wide-ranging applications such as 3D photography [31]. 3D scene recovery [45, 44] of in-the-wild monocular images relies on a powerful depth estimator [24, 48, 45, 25, 49] that can accurately predict the geometry of diverse scenes.
State-of-the-art depth estimation models [25, 45, 24, 49] now advocate mix-dataset training [43, 25], which can gen-erate robust depth predictions across diverse scenes. This opens up the possibility of large-scale pre-training and de-ploying only a single model in various application scenar-ios.
To enable mix-dataset training, scale-and-shift invari-ant (SSI) losses [25, 45] are designed to normalize depth representations explicitly, thereby removing scale-and-shift changes between different data sources. As a result, datasets with various depth representations, such as metric depth, uncalibrated disparity maps, and relative depth up to scale (UTS), can be jointly utilized for training. However, de-spite the strong generalization capabilities across scenes, mix-dataset training comes with its own set of drawbacks.
Unlike previous depth estimation models that produce abso-lute depth or relative depth up to scale, which can be directly unprojected to 3D structures given the intrinsic camera pa-rameters, models trained with SSI losses predict depth up to unknown scale and shift factors (UTSS), which is geo-metrically incomplete [22] for reconstructing 3D models.
Although scaling depth maps typically adheres to the orig-inal 3D scene recovery’s geometric integrity, the unknown shifts may introduce structural distortions. Depth estima-tion models that are optimized to produce absolute depth or relative depth up to scale do not suffer from this problem, but they require geometry-complete depth annotations, such as metric depth or relative depth from multi-view stereo, for learning. Leres [45] offers a potential solution by rec-tifying the distorted point cloud via a separately optimized post-processing module, which, however, necessitates ad-ditional 3D datasets. Unfortunately, the extra 3D data or geometry-complete depth annotations are significantly less diverse than the geometrically incomplete data used in the original mix-data training, and as a result, their generaliza-tion ability on in-the-wild images is limited.
In this research, our primary objective is to develop depth estimation models that can predict geometry-preserving depth up to a scale for 3D scene recovery without requir-ing extra data or annotations through mix-dataset training.
To achieve this goal, we propose a novel framework based on differentiable rendering. Specifically, we reconstruct 3D point clouds based on the predicted depth and use a differentiable renderer to generate novel views of the 3D model. We then predict the depth of the synthesized views with the same model and employ loss functions to ensure that the depth predictions of the rendered views are con-sistent. Fig. 1 illustrates our motivation. In this process, the network is optimized to produce undistorted 3D struc-tures from depth using the informative gradients from the differentiable renderer. This ensures that the rendered im-ages from different views look realistic and their depth esti-mations are consistent. Compared with previous works, our method can produce geometry-preserving predictions with-out relying on extra annotations or 3D datasets, enabling us to make full use of mixed datasets collected from various resources to improve generalization. Our loss functions can also recover domain-specific scale and shift coefficients of a trained UTSS model, such as Midas [25] and HDN [49], in a self-supervised manner using unlabelled images from the same domain. Moreover, we demonstrate that our proposed self-supervised loss can be used to predict intrinsic camera parameters, such as focal length, by selecting the parameter from a few options that minimize the proposed consistency losses. Our extensive experiments on multiple benchmark datasets validate the effectiveness of our design. Our main contributions are summarized as follows:
• We propose a novel depth estimation learning frame-work that can produce geometry-preserving depth without relying on extra datasets or annotations.
• Our proposed consistency loss can recover domain-specific affine coefficients of a trained model in a self-supervised manner using unlabelled images from the same domain.
• We demonstrate that the self-supervised loss can also be used to roughly estimate camera intrinsic parame-ters.
• Experiments on multiple benchmark datasets show that our method can better recover scene structures of diverse images both quantitatively and qualitatively. 2.