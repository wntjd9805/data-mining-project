Abstract
We are concerned with a challenging scenario in un-In this case, the model paired multiview video learning. aims to learn comprehensive multiview representations while the cross-view semantic information exhibits varia-tions. We propose Semantics-based Unpaired Multiview
Learning (SUM-L) to tackle this unpaired multiview learn-ing problem. The key idea is to build cross-view pseudo-pairs and do view-invariant alignment by leveraging the semantic information of videos. To facilitate the data effi-ciency of multiview learning, we further perform video-text alignment for first-person and third-person videos, to fully leverage the semantic knowledge to improve video repre-sentations. Extensive experiments on multiple benchmark datasets verify the effectiveness of our framework. Our method also outperforms multiple existing view-alignment methods, under the more challenging scenario than typ-ical paired or unpaired multimodal or multiview learn-ing. Our code is available at https://github.com/ wqtwjt1996/SUM-L. 1.

Introduction
Recent years have witnessed a tremendous increase in the research regarding egocentric video understanding and learning [22, 26, 20, 41, 29, 30, 14, 42, 7, 31, 46, 9]. In parallel, a recent study [22] verifies that the activity of third-person videos in fact positively inform first-person video learning. Early works [39, 51] of egocentric video learning and understanding pay a lot of attention to paired multiview data representation learning. However, synchronized first-person and third-person video pairs are difficult to collect.
This severely limits the data scale and scope of multiview learning for egocentric data.
Recently, egocentric video datasets [4, 5, 10] with larger scale have become available. However, since head-mounted cameras were not available until recent years, very few first-person videos have paired third-person videos [15].
Figure 1. (a) The typical multiview video learning method learns view-invariant representations from paired first-person and third-person videos. (b) Recent multimodal study [19] is interested in the alignment between unpaired samples which share identical se-mantics. In this paper, we study a more challenging scenario than existing works: unpaired multiview samples alignment with par-tially similar semantics, such as videos with (c) the same verb only, or (d) the same object only. The verb-object phrases above the video clips denote the semantic information of videos.
Fortunately, there exist third-person videos from different datasets [12, 16].
It is worth noting that unpaired third-person videos are relatively easier to capture and obtain compared with paired ones. Therefore, in this paper, we study an interesting yet seldom investigated problem: how to leverage unpaired third-person videos to help egocentric video learning?
Unpaired multiview learning is rarely studied. While re-cent works [28, 27, 38, 53] study the scenario where paired modalities are missing. But they do not leverage the po-tential of unpaired data from diverse datasets. Moreover, to align the unpaired multimodal samples, Kundu et al. [19] propose one relation distillation method to align the un-paired samples. They assume that unpaired data need to share identical semantic information. However, in the un-paired first-person and third-person video learning scenario, for the vast majority of egocentric videos, it is almost im-possible to mine the third-person samples with identical
Figure 2. Statistics of semantic similarities between the first-person samples in [39, 4, 5] and the most similar single-agent third-person samples they can find from the LEMMA [15] dataset. The semantic similarity is the cosine distance between textual phrase vectors of videos encoded by a large language model [34]. Each bar in the figure represents the percentage of action classes that fall within one semantic similarity range (e.g., [0.8, 1.0]) in an egocentric dataset compared to all action classes in this dataset. In all of the three popular egocentric datasets, very few first-person videos are able to find third-person videos with identical semantics. For reference, in the studies for paired multiview data [43, 39, 51, 54], all the semantic similarities of cross-view data pairs are (or very close to) “1.0”. semantics. The only alternatives are partially semantics-similar unpaired third-person videos. For example, for one egocentric video clip with “cut watermelon” semantics, the most similar third-person video is those with “cut lemon” (same verb) or “eat watermelon” (same object) semantics.
Examples and comparisons between our scenario and typi-cal paired or unpaired multiview or multimodal learning are shown in Figure 1, and we display comprehensive seman-tics similarity statistics of multiview data studied in this pa-per (see Figure 2). Therefore, how to employ the unpaired and partially semantics-similar third-person views to help first-person view learning is an open problem. Intuitively, compared with previous research, our setting is more chal-lenging since most of the first-person videos only share par-tially similar semantics with third-person views.
In this paper, we are interested in view-invariant align-ment in the partially semantics-similar pseudo-pair setting.
We propose Semantics-based Unpaired Multiview Learning (SUM-L), where multiview pseudo-pairs with high simi-larities are aligned in a semantics-aware manner.
In our method, we also employ video-text alignment to permit all first-person videos to obtain knowledge from samples with different views or modalities. We highlight that our SUM-L employs the large language model [34] to help align cross-view and cross-modality data. This is different from [22], which proposes to distill beneficial knowledge from un-paired third-person videos [16], mostly with the help of the off-the-shelf image recognition model [6] and hand-object detector [37]. Moreover, our method is better than exist-ing view-invariant alignment methods, such as typical con-trastive learning [44, 43, 50, 54] and triplet loss based learn-ing [35, 51] in the new setting, since they naively reduce multiview feature distance, which leads to learning subop-timal first-person representations.
To summarize, our contribution is three-fold:
• We study a new problem: in the partially semantics-similar multiview setting, how to leverage unpaired third-person videos to help first-person video learning.
• We propose SUM-L for this new problem, which is more effective than existing view-alignment methods in our unpaired multiview learning scenario.
• Experiments in standard benchmark datasets includ-ing Charades-Ego [39], EPIC-Kitchens [4], and EPIC-Kitchens-100 [5] validate the competitive performance of our methods over existing view-alignment methods including typical contrastive learning and triplet loss based learning. 2.