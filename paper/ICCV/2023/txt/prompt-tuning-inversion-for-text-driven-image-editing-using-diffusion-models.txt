Abstract
Recently large-scale language-image models (e.g., text-guided diffusion models) have considerably improved the image generation capabilities to generate photorealistic im-ages in various domains. Based on this success, current image editing methods use texts to achieve intuitive and versatile modification of images. To edit a real image us-ing diffusion models, one must first invert the image to a noisy latent from which an edited image is sampled with a target text prompt. However, most methods lack one of the following: user-friendliness (e.g., additional masks or precise descriptions of the input image are required), gen-eralization to larger domains, or high fidelity to the input image. In this paper, we design an accurate and quick in-version technique, Prompt Tuning Inversion, for text-driven image editing. Specifically, our proposed editing method consists of a reconstruction stage and an editing stage. In the first stage, we encode the information of the input image into a learnable conditional embedding via Prompt Tun-ing Inversion. In the second stage, we apply classifier-free guidance to sample the edited image, where the conditional embedding is calculated by linearly interpolating between the target embedding and the optimized one obtained in the first stage. This technique ensures a superior trade-off be-tween editability and high fidelity to the input image of our method. For example, we can change the color of a specific object while preserving its original shape and background under the guidance of only a target text prompt. Extensive experiments on ImageNet demonstrate the superior editing performance of our method compared to the state-of-the-art baselines. 1.

Introduction
Text-based image editing, a long-standing problem in image processing, aims to modify an input image to align its visual content with the target text prompts.
It has drawn increasing attention in recent years and many meth-*This work was done when Xiaoyue Duan was an intern at Baidu VIS.
†Corresponding author
Illustration of different methods in editing the color
Figure 1. of the car. Methods based on the original DDIM inversion (i.e.,
DDIM-Edit, DiffEdit and DiffEdit w/o src) cannot preserve the shape of the car. In contrast, our method successfully changes the color while preserving the structural information. The target text is “a yellow car”. The source text is “a red car” for DiffEdit. ods built upon text-to-image generation have been devel-In past years, GAN-based image editing meth-oped. ods [30, 31, 52, 53] achieve impressive results due to the powerful generation abilities of GANs [37, 23, 33, 60].
However, these methods only work well in domains where the models are trained. More recently, diffusion models such as DDPM [18] and score-based generative models [48] have demonstrated competitive or even better capability of generating images compared to VAE-, GAN-, flow- and autoregressive-based models [36, 14, 38, 12]. Especially, large-scale language-image models (LLIMs), such as Im-agen [43], DALL-E2 [35] and Stable Diffusion [41], have attracted unprecedented attention from the research com-munity and public society. With the help of large-scale pre-trained language models [34, 10], LLIMs can generate high-fidelity images well aligned with the provided text prompts without further fine-tuning. To fully leverage the generation and generalization capabilities of LLIMs, we aim to develop a text-driven image editing method based on open-sourced
LLIMs, e.g., Stable Diffusion [41].
Editability and fidelity are two essential requirements
of image editing tasks. The former requires that the edited images are supposed to contain visual contents well aligned with the corresponding textual contents provided in the tar-get prompts, while the latter expects that areas other than the edited parts should stay as close to those of the input image as possible. For example, when modifying the color of a specific object, its other attributes (e.g., size and shape) are expected to be preserved. As shown in Fig. 1, given an im-age of a red car and the target text prompt (“a yellow car”), the desired edited image should contain a yellow car while keeping the background as well as the car’s size and shape unchanged. To achieve this editing, the simplest way is to first invert the image to a noisy latent via the reversed deter-ministic DDIM sampling process [46], and then obtain the edited image via the deterministic DDIM sampling process with the guidance of the target prompt embedding. We re-fer to this approach as “DDIM-Edit” in our paper. Although this approach successfully turns the color of the car to yel-low (see “DDIM-Edit” in Fig. 1), the background and the shape of the car change drastically, which obviously fails to meet the requirement of high fidelity. The reason lies in that the deterministic DDIM sampling process cannot be re-versed perfectly in practice. A slight error is amplified by a large classifier-free guidance scale, and is accumulated in each sampling step, which consequently results in a signifi-cantly different image.
To improve fidelity, some methods consider the image editing tasks as inpainting tasks, which require users to explicitly provide masks of the inpainting regions [3, 25].
With the mask prior, the background can remain the same, but masking out image contents also removes important structural information that is helpful in the editing process, leading to unsatisfactory editing results. Moreover, ask-ing users to provide masks is cumbersome and not suit-able for quick and intuitive text-driven image editing. As a solution, DiffEdit [7] presents an algorithm that can au-tomatically generate a mask given a target text prompt to locate the region to be edited. However, the editability of
DiffEdit largely depends on DDIM-Edit, which may fail to preserve the structural information of the edited object, e.g., the shape of the car (see “DiffEdit” in Fig. 1). Moreover, to generate an accurate mask, DiffEdit requires a precise text description of the input image (referred to as “source text”), hampering the editing efficiency. Without the source text (see “DiffEdit w/o src” in Fig. 1), the automatically gen-erated mask cannot locate the body of the car accurately, further decreasing editability.
In this work, we aim to propose an image editing method to mitigate all the above problems, i.e., the method should be user-friendly, generalizable to various domains, and gen-erate edited images with high fidelity. Specifically, for a quick and intuitive text-based method, users only need to provide an input image and the corresponding target text prompts, without the need for a mask or a source text de-scribing the input image. Secondly, the method should be able to operate on real images from various domains.
Thirdly, the objects should be precisely edited with the
In some cases, only certain at-background preserved. tributes of the objects should be modified, while other at-tributes are supposed to be left untouched.
To achieve these merits, we believe that image editing needs a new inversion method based on diffusion models to reconstruct the input image. Inspired by the classifier-free guidance [20] and textual-inversion methods [27], we propose a Prompt Tuning Inversion method to encode the information of the input image into a conditional embed-ding. More specifically, we first apply DDIM inversion to the input image latent to obtain a sequence of noisy ones.
These noisy latents can be taken as a prior trajectory for reconstructing the original image. Then, we introduce a learnable embedding in the sampling process. The diffu-sion model reconstructs the input image step by step along the trajectory conditioned on this embedding while optimiz-ing it at the same time. In this way, the contents of the in-put image are learned in the embedding. Finally, we obtain a new conditional embedding by linearly interpolating be-tween the optimized embedding and the target embedding, resulting in a representation that combines both the struc-tural information of the input image and the visual content of the target text.
Overall, our proposed method consists of two stages. In the first stage, we encode the information of the input image into a learnable conditional embedding via prompt tuning in the reconstruction process. In the second stage, a new conditional embedding is computed by linearly interpolat-ing between the target embedding and the optimized one obtained in the first stage, which boosts a trade-off between editability and fidelity. The classifier-free guidance is then applied to sample the edited image. In sum, our contribu-tions are as follows:
• We propose a user-friendly text-driven image editing method which requires only an input image and a tar-get text for editing, without any need for user-provided masks or source descriptions of the input images.
• We propose a Prompt Tuning Inversion method for dif-fusion models which can quickly and accurately recon-struct the original image, providing a strong basis for sampling edited images with high fidelity to the inputs.
• We compare against the state-of-the-art methods both qualitatively and quantitatively, and show that our method outperforms these works in terms of the trade-off between editability and fidelity.
2.