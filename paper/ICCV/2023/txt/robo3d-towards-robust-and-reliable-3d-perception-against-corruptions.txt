Abstract
The robustness of 3D perception systems under natu-ral corruptions from environments and sensors is pivotal for safety-critical applications. Existing large-scale 3D perception datasets often contain data that are meticu-lously cleaned. Such configurations, however, cannot reflect the reliability of perception models during the deployment stage. In this work, we present Robo3D, the first compre-hensive benchmark heading toward probing the robustness of 3D detectors and segmentors under out-of-distribution scenarios against natural corruptions that occur in real-world environments. Specifically, we consider eight corrup-tion types stemming from severe weather conditions, exter-nal disturbances, and internal sensor failure. We uncover that, although promising results have been progressively achieved on standard benchmarks, state-of-the-art 3D per-ception models are at risk of being vulnerable to corrup-tions. We draw key observations on the use of data represen-tations, augmentation schemes, and training strategies, that could severely affect the model’s performance. To pursue better robustness, we propose a density-insensitive training framework along with a simple flexible voxelization strat-egy to enhance the model resiliency. We hope our bench-mark and approach could inspire future research in design-ing more robust and reliable 3D perception models. Our robustness benchmark suite is publicly available1. 1.

Introduction 3D perception aims to detect and segment accurate po-sition, orientation, semantics, and temporary relation of (∗) The first three authors contributed equally to this work. 1https://github.com/ldkong1205/Robo3D.
the objects and backgrounds around the ego-vehicle in the three-dimensional world [3, 19, 25]. With the emergence of large-scale autonomous driving perception datasets, var-ious approaches in the fields of LiDAR semantic seg-mentation and 3D object detection advent each year, with record-breaking performances on the mainstream bench-marks [18, 4, 8, 17, 61].
Despite the great success achieved on the “clean” evalua-tion sets, the model’s robustness against out-of-distribution (OoD) scenarios remain obscure. Recent attempts mainly focus on probing the OoD robustness from two aspects. The first line focuses on the transfer of 3D perception models to unseen domains, e.g., sim2real [72], day2night [26], and city2city [30] adaptations, to probe the model’s generaliz-ability. The second line aims to design adversarial examples which can cause the model to make incorrect predictions while keeping the attacked input close to its original format, i.e., to test the model’s worst-case scenarios [50, 9, 65].
In this work, different from the above two directions, we aim at understanding the cause of performance deterioration under real-world corruption and sensor failure. Current 3D perception models learn point features from LiDAR sensors or RGB-D cameras, where data corruptions are inevitable due to issues of data collection, processing, weather condi-tions, and scene complexity [49]. While recent works target creating corrupted point clouds from indoor scenes [28] or object-centric CAD models [60, 89, 2], we simulate corrup-tions on large-scale LiDAR point clouds from the complex outdoor driving scenes [18, 4, 8, 61].
As shown in Fig. 1, we consider three distinct corrup-tion sources that are with a high likelihood to occur in real-world deployment: 1) Severe weather conditions (fog, rain, and snow) which cause back-scattering, attenuation, and re-flection of the laser pulses [21, 20, 59]; 2) External distur-bances, e.g., bumpy surfaces, dust, insects, that often lead to nonnegligible motion blur and LiDAR beam missing is-sues [46]; and 3) Internal sensor failure, such as the in-complete echo or miss detection of instances with a dark color (e.g., black car) and crosstalk among multiple sen-sors, which likely deteriorates the 3D perception accuracy
[83, 7]. Besides the environmental factors, it is also im-portant to understand the cross-sensor discrepancy to avoid sudden failure caused by the sensor configuration change.
To properly fulfill such pursues, we simulate physically-principled corruptions on the val sets of KITTI [18], Se-manticKITTI [4], nuScenes [8], and Waymo Oepn [61], as our corruption suite dubbed Robo3D. Analogous to the pop-ular 2D corruption benchmarks [23, 81, 41], we create three severity levels for each corruption and design suitable met-rics as the main indicator for robustness comparisons. Fi-nally, we conduct exhaustive experiments to understand the pros and cons of different designs from existing models. We observe that modern 3D perception models are at risk of be-ing vulnerable even though their performance on standard benchmarks is improving. Through fine-grained analyses on a wide range of 3D perception datasets, we diagnose that:
• Sensor setups have direct impacts on feature learn-ing. 3D perception models trained on data collected with different sensor configurations and protocols of-ten yield inconsistent resilience.
• 3D data representations often coupled with the model’s robustness. The voxel and point-voxel fu-sion approaches exhibit clear superiority over the projection-based methods, e.g., range view.
• 3D detectors and segmentors show distinct sensitivities to different corruption types. A sophisticated combina-tion of both tasks is a viable way to achieve robust and reliable 3D perception.
• Out-of-context augmentation (OCA) and flexible ras-terization strategies can improve model’s robustness.
We thus propose a solution to enhance the robustness of existing 3D perception models, which consists of a density-insensitive training framework and a simple flexible voxelization strategy.
The key contributions of this work are summarized as:
• We introduce Robo3D, systematically-designed robustness evaluation suite for LiDAR-based 3D perception under corruptions and sensor failure. the first
• We benchmark 34 perception models for LiDAR-based semantic segmentation and 3D object detection tasks, on their robustness against corruptions.
• Based on our observations, we draw in-depth discus-sions on the design receipt and propose novel tech-niques for building more robust 3D perception models. 2.