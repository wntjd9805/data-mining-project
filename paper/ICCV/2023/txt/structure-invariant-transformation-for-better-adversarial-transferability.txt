Abstract
Given the severe vulnerability of Deep Neural Networks (DNNs) against adversarial examples, there is an urgent need for an effective adversarial attack to identify the de-ficiencies of DNNs in security-sensitive applications. As one of the prevalent black-box adversarial attacks, the ex-isting transfer-based attacks still cannot achieve compara-ble performance with the white-box attacks. Among these, input transformation based attacks have shown remark-able effectiveness in boosting transferability. In this work, we find that the existing input transformation based at-tacks transform the input image globally, resulting in lim-ited diversity of the transformed images. We postulate that the more diverse transformed images result in better transferability. Thus, we investigate how to locally ap-ply various transformations onto the input image to im-prove such diversity while preserving the structure of im-age. To this end, we propose a novel input transforma-tion based attack, called Structure Invariant Transforma-tion (SIA), which applies a random image transformation onto each image block to craft a set of diverse images for gradient calculation. Extensive experiments on the stan-dard ImageNet dataset demonstrate that SIA exhibits much better transferability than the existing SOTA input transfor-mation based attacks on CNN-based and transformer-based models, showing its generality and superiority in boosting transferability. Code is available at https://github. com/xiaosen-wang/SIT. 1.

Introduction
With the unprecedented progress of Deep Neural Net-works (DNNs) [25, 20, 23, 50, 12], they have been de-ployed in many security-sensitive applications, such as face recognition [41, 51, 43, 44], autonomous driving [14, 31], etc. On the other hand, recent works have found that
DNNs are vulnerable to adversarial examples [47, 15], which mislead the deep models with imperceptible pertur-bations. This brings a huge threat to the real-world applica-tions [42, 13, 45, 57, 71, 78] and makes it imperative for an
DIM
TIM
SIM
Raw Image
Admix
SSA
SIA (Ours)
Figure 1: The raw image and its transformed images by
DIM (resize factor of 0.8) [64], TIM (translated by 15 pix-els) [11], SIM (scale factor of 0.5), Admix (admix strength of 0.2 and scale factor of 0.5) [55], SSA (turning factor of 0.5) [36] and our proposed SIA (3 × 3 blocks). effective attack to identify the deficiencies of DNNs when we robustify the deep models or deploy them for commer-cial applications.
Existing adversarial attacks usually fall into two cate-gories: white-box attacks [15, 38, 37, 26, 54] can fetch any information of the target model, including (hyper-)parameters, gradient, architecture, while black-box at-tacks [24, 7, 4, 27, 33, 77] are only allowed limited access to the target model. One of the significant properties of ad-versarial examples is their transferability [64, 10, 53, 76], in which the adversarial examples generated on one model can still mislead other models, making it possible to attack the real-world applications in the black-box setting. However, existing adversarial attacks [26, 37] often exhibit superior white-box attack performance but poor transferability.
To craft more transferable adversarial examples, various techniques have been proposed, such as momentum-based methods [10, 32, 53, 56], input transformations [64, 11, 32, 68, 72], ensemble attacks [33, 66, 36], advanced objective functions [61, 80] and model-specific approaches [28, 60].
Among which, input transformations (e.g., random resizing and padding [64], translation [11], scale [32] admix [55], etc.) that transform the image before gradient calculation,
have achieved superior transferability and attracted broad attention. Nevertheless, we find that the existing input trans-formation based attacks, even the SOTA attack Admix [55] and SSA [36], transform the image globally without chang-ing the local relationship among objects in the input image.
We argue that the diversity of such transformed images is still not enough, leading to limited transferability.
In this work, we postulate and empirically validate that the more diverse transformed images lead to better trans-ferability. Based on this observation, instead of applying a single transformation on the input image, we apply differ-ent transformations locally on different parts of the image to enhance the diversity of transformed images. As shown in Fig. 1, such transformation can bring much more differ-ence to the generated image compared with the raw image but still preserve the global structure of the object. Based on this transformation, we propose a novel input transfor-mation based attack called structure invariant attack (SIA), which utilizes the gradient of these transformed images to update the adversarial examples for better transferability.
Our contributions are summarized as follows:
• We empirically validate that the high diversity of trans-formed images is beneficial to improve transferability, which sheds new light on how to design new input trans-formations for more transferable adversarial examples.
• We design a new image transformation method, which locally applies different transformations on different parts of the image to generate more diverse images but preserve its global structure.
• Based on the proposed image transformation, we devise a new input transformation based attack, called structure invariant attack (SIA), to generate more transferable ad-versarial examples.
• Extensive experiments on ImageNet dataset demonstrate that SIA outperforms the baselines with a clear margin on
CNN-based as well as transformer-based models, show-ing its superiority and generality. 2.