Abstract 1.

Introduction
While large text-to-image models are able to synthesize
“novel” images, these images are necessarily a reflection of the training data. The problem of data attribution in such models – which of the images in the training set are most responsible for the appearance of a given generated image – is a difficult yet important one. As an initial step toward this problem, we evaluate attribution through “customization” methods, which tune an existing large-scale model toward a given exemplar object or style. Our key insight is that this allow us to efficiently create synthetic images that are computationally influenced by the exemplar by construction.
With our new dataset of such exemplar-influenced images, we are able to evaluate various data attribution algorithms and different possible feature spaces. Furthermore, by training on our dataset, we can tune standard models, such as DINO,
CLIP, and ViT, toward the attribution problem. Even though the procedure is tuned towards small exemplar sets, we show generalization to larger sets. Finally, by taking into account the inherent uncertainty of the problem, we can assign soft attribution scores over a set of training images.
Contemporary generative models create high-quality syn-thetic images that are “novel”, i.e., different from any image seen in the training set. Yet, these synthetic images would not be possible without the vast training sets employed by the models. It is quite remarkable, yet poorly understood, how the generative process is able to compose objects, styles, and attributes from different training images into a coher-ent novel scene. Because of copyright and ownership of the training images, understanding the interplay between training data and generative model outputs has become in-creasingly necessary, both for scientific progress, as well as for practical or legal reasons.
There is a general agreement in the community that not all the billions of training images contribute equally to the appearance of a given synthesized output image. So, given a particular network output, can we identify the subset of train-ing images that are most responsible for it? Even for image classifiers, this has remained an open, difficult machine learn-ing problem. Approaches such as influence functions [34] (inspired by robust statistics), and training and analyzing many models on random subsets [17] (inspired by Shapley
value from economics [60]), are difficult to scale even to modest dataset sizes, let alone the billions of images and parameters that make up modern generative models.
One potential approach that scales well is to run image retrieval in a pre-defined feature space, as, intuitively, syn-thesized images that are close to a particular training image are more likely to have been influenced by it. However, an out-of-the-box feature space, such as CLIP, is trained for a completely different task and is not necessarily suited for the attribution problem. How can we objectively evaluate which feature spaces are suitable for visual data attribution?
A considerable challenge is obtaining “ground truth” attri-bution data. No method exists for obtaining the set of ground truth training images that influenced a synthesized image.
One way of searching influential images is to check if remov-ing particular images during training will affect the model output. This approach for classifiers has been explored by
Feldman and Zhang [17], where they define influence by training on different subsets of the dataset and analyzing the differences between the resulting models. However, this is computationally infeasible for generation, as training even a single model takes considerable resources, let alone the num-ber of models needed to analyze billions of images. Instead, our work takes inspiration from this but establishes ground truth attribution in a tractable way.
To do this, we exploit a simple insight – by taking a pretrained generative model and tuning it toward a new ex-emplar image using “customization” methods [18, 53, 35] we can efficiently create synthesized images that are compu-tationally influenced by the exemplar by construction (see
Figure 1a). While such synthesized images are not only in-fluenced by the exemplar, it serves as a noisy but informative ground truth, and sheds light on how a given training image can be composed into different possible synthesized images.
We create a large dataset of pairs of exemplar and syn-thesized images using Custom Diffusion [35]. We use both exemplar objects (from ImageNet) and styles (from BAM-FG [54] and Artchive [22]). Given a synthesized image, the exemplar image, and other random training images from the training set, a strong attribution algorithm should choose the exemplar image over most of the other distractor images.
We leverage this dataset to evaluate candidate retrieval feature spaces, including self-supervised methods [8, 11, 49], copy detection [47], and style descriptors [54]. Furthermore, our dataset can be used to tune the feature spaces to be bet-ter suited for the attribution problem through a contrastive learning procedure. Finally, we can estimate the likelihood of a candidate image being the exemplar image by taking a thresholded softmax over the retrieval score. Though our ground truth dataset is of a single image, this enables us to rank and obtain a set of soft attribution scores, assessing
“influence” over multiple candidate training images. While we train and evaluate for attribution on exemplar-based cus-tomization (1-10 related images), we demonstrate that the method generalizes even when tuning on larger sets (100-1000 random, unrelated images), suggesting applicability to the general, more challenging data attribution problem.
To summarize our contributions: 1) We propose an ef-ficient method for generating a dataset of synthesized im-ages paired with ground-truth exemplar images that influ-enced them. 2) We leverage this dataset to evaluate candi-date image retrieval feature spaces. Furthermore, we demon-strate our dataset can improve feature spaces through a con-trastive learning objective. 3) We can softly assess influence scores over the training image dataset. Our code, model, and dataset are released at: https://peterwang512. github.io/GenDataAttribution. 2.