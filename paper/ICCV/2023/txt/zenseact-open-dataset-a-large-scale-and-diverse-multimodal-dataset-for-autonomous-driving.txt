Abstract
Existing datasets for autonomous driving (AD) often lack diversity and long-range capabilities, focusing instead on 360° perception and temporal reasoning. To address this gap, we introduce Zenseact Open Dataset (ZOD), a large-scale and diverse multimodal dataset collected over two years in various European countries, covering an area 9× that of existing datasets. ZOD boasts the highest range and resolution sensors among comparable datasets, cou-pled with detailed keyframe annotations for 2D and 3D ob-jects (up to 245m), road instance/semantic segmentation, traffic sign recognition, and road classification. We believe that this unique combination will facilitate breakthroughs in long-range perception and multi-task learning. The dataset is composed of Frames, Sequences, and Drives, designed to encompass both data diversity and support for spatio-temporal learning, sensor fusion, localization, and map-ping. Frames consist of 100k curated camera images with two seconds of other supporting sensor data, while the 1473
Sequences and 29 Drives include the entire sensor suite for 20 seconds and a few minutes, respectively. ZOD is the only large-scale AD dataset released under a permissive license, allowing for both research and commercial use.
More information, and an extensive devkit, can be found at zod.zenseact.com. 1.

Introduction
Road traffic accidents cause more than 1.3 million deaths and many more nonfatal injuries and disabilities globally each year [20]. Automated driving has the potential to im-prove road safety by intervening in accident-prone situa-tions or even controlling the entire ride from start to desti-∗Equal contribution.
†Corresponding author.
Figure 1: Geographical coverage comparison with other
AD datasets using the diversity area metric defined in [25] (top left), and geographical distribution of ZOD Frames overlaid on the map. The numbers in the quantized regions represent the amount of annotated frames in that geograph-ical region. ZOD Frames contain data collected over two years from 14 different European countries, from the north of Sweden down to Italy. nation. Regardless of the level of automation, autonomous vehicles require sensors such as cameras, GNSS (Global
Navigation Satellite System), IMU (Inertial Measurement
Unit), and range sensors such as radar or LiDAR (Light
Detection and Ranging) to perceive their surroundings ac-curately. Moreover, they require advanced perception, fu-sion, and planning algorithms to make use of this data ef-ficiently. Machine learning (ML) algorithms, particularly deep learning (DL), have been increasingly used to develop autonomous driving (AD) software, but they require high-quality and diverse data from real-world traffic scenarios to achieve the necessary performance.
The development of AD systems owes much of its recent success to the availability of large-scale image datasets [3, 8, 9, 16, 19, 34] and dedicated multimodal AD datasets [4, 6, 11, 12, 17, 25, 30, 32]. These AD datasets have em-phasized temporal reasoning and 360° perception, as this corresponds to a nominal self-driving setup. However, as data from the same scene is highly correlated, this naturally limits diversity in terms of weather or lighting conditions, driving situations, and geographical distribution. This can result in overly specialized solutions, which may not gen-eralize to the full operational design domain of real-world
AD systems.
To complement existing datasets, we introduce Zenseact
Open Dataset (ZOD), Europe’s largest and most diverse multimodal AD dataset. ZOD consists of more than 100k traffic scenes that have been carefully curated to cover a wide range of real-world driving scenarios. The dataset is split into three subsets: 100k independent Frames, 1473 twenty-second Sequences, and 29 Drives lasting a few min-utes. Frames are primarily suitable for non-temporal per-ception tasks, Sequences are intended for spatio-temporal learning and prediction, and Drives are aimed at longer-term tasks such as localization, mapping, and planning.
This separation allows ZOD to cover an area 9× larger than any other AD dataset, offering ample opportunities for de-veloping robust algorithms that generalize well across mul-tiple operational domains. We also facilitate research in do-main adaptation and transfer learning by providing compre-hensive metadata for each scene.
Robust performance in various conditions, including high speeds, will be vital for the deployment of AD systems.
In particular, high-speed scenarios puts a hard requirement on long-range perception, which in turn puts challenging requirements on sensor resolution. ZOD stands out from other AD datasets by employing high-resolution sensors, such as an 8MP camera, rooftop LiDARs with 254k points per scan, and a high-precision GNSS/IMU inertial naviga-tion system with 0.01m position accuracy. Additionally, we provide manual keyframe annotations for several perception tasks, such as semantic and instance segmentation masks for roads and lanes, 2D and 3D bounding boxes for static and dynamic objects (up to 245 meters), and road condition la-bels. We further annotate traffic signs with a rich taxonomy of 156 classes. The 446k unique labeled instances consti-tute the largest traffic sign dataset to date. We believe that the combination of high-quality sensors and detailed anno-tations in ZOD will enable breakthroughs in accurate and long-range perception, which is crucial for high-speed driv-ing scenarios.
ZOD’s extensive annotations for multiple perception tasks make it an ideal dataset for multi-task learning, which is a recent trend in computer vision and AD [26, 35, 36].
The core idea of multi-task learning is to learn a shared representation that can benefit all tasks, resulting in im-proved generalization and performance on individual tasks
[5]. This approach also allows models to make better use of data and available resources, a crucial feature for real-world applications such as AD systems as they typically operate on embedded hardware with limited computational power.
To ensure the privacy of individuals and comply with le-gal and regulatory requirements, we employ two approaches to anonymize faces and license plates: blurring and replace-ment with synthetic data. These anonymization techniques were chosen to enable research on the impact of anonymiza-tion techniques on learning methods, with initial results demonstrating that none of the techniques have a negative impact on performance.
Finally, ZOD is the first large-scale AD dataset released under the permissive CC BY-SA 4.0 license [1]. This li-cense allows for research and commercial use (subject to the license terms), as well as sharing and adapting permits, which provides an opportunity for startups and other com-mercial entities to leverage the dataset for their projects. We believe that this open and inclusive approach will foster in-novation and accelerate the development of AD technology beyond the research community. To facilitate a rapid start with ZOD, it comes with an extensive development kit, in-cluding multiple tutorials and examples.
In summary, our main contributions are the following:
• We release ZOD, the most diverse autonomous driving dataset to date. The data is collected from Europe over multiple years and is curated to contain a wide range of traffic scenarios, weather conditions, road types, and lighting conditions.
• The data is collected using high-resolution sensors and coupled with detailed keyframe annotations for 2D/3D objects, lane instances, and road segmentation, en-abling long-range perception with annotated objects farther away than in any other comparable AD dataset.
• The object annotations include a rich traffic sign tax-onomy with more than twice as many unique instances as the largest existing traffic sign dataset.
• ZOD is the first large-scale AD dataset released under a permissive license, allowing both research and com-mercial use.
Dataset
KITTI [11] nuScenes [4]
ONCE [17]
PandaSet [32]
Waymo Open [25]
A2D2 [12]
Argoverse 2 [30]
Locations
Karlsruhe
Boston, Singapore
China
San Francisco 6 U.S. cities 3 German cities 6 U.S cities
ZOD Frames
ZOD Sequences
ZOD Drives 14 European countries 6 European countries 2 European countries
Geo. coverage Ann. frames
-5 km2§
--76 km2§
-17 km2 705 km2 26 km2
-15k⋆ 40k⋆ 16k⋆ 8k⋆ 400k⋆ 12k 150k⋆ 100k 1473
-Sequences 22 1000 581 103 2030
-1000
-1473 29 1.5 5.5 27.8 0.2 11.3
-4.2 55.6‡ 8.2 1.5
Size (hr) Ann. range† Avg. LiDAR points Camera Map
No
Yes
No
No
No
No
Yes 91 m 141 m 81 m 300 m 80 m 103 m 214 m 120k 34k 65k 166k 177k 7k 107k 90◦ 360◦ 360◦ 360◦ 360◦ 360◦ 360◦ 245 m 245 m
-254k 254k 254k 120◦ 120◦ 120◦
No
No
No
Table 1: Dataset comparison. Geographical coverage is computed over annotated frames, § refers to values taken from [25].
⋆Sequential annotation, allowing temporal tasks. ‡Counting each Frame as a 2-second LiDAR sequence, note that only the center image is provided. †Showing the 99.9th percentile computed using the publicly available data. 2.