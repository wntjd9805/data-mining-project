Abstract
Fine-grained capture of 3D Human-Object Interactions (HOIs) enhances human activity comprehension and sup-ports various downstream visual tasks. However, previous models often assume that humans interact with rigid objects using only a few body parts, constraining their applicability.
In this paper, we address the intricate challenge of Full-Body
Articulated Human-Object Interaction (f-AHOI), where com-plete human bodies interact with articulated objects having interconnected movable joints. We introduce CHAIRS, an extensive motion-captured f-AHOI dataset comprising 17.3 hours of diverse interactions involving 46 participants and 81 articulated as well as rigid sittable objects. The CHAIRS provides 3D meshes of both humans and articulated ob-jects throughout the interactive sequences, offering realistic and physically plausible full-body interactions. We demon-strate the utility of CHAIRS through object pose estimation.
Leveraging the geometric relationships inherent in HOI, we propose a pioneering model that employs human pose estima-tion to address articulated object pose and shape estimation within whole-body interactions. Given an image and an esti-mated human pose, our model reconstructs the object’s pose and shape, reﬁning the reconstruction based on a learned in-teraction prior. Across two evaluation scenarios, our model signiﬁcantly outperforms baseline methods. Additionally, we showcase the signiﬁcance of CHAIRS in a downstream task involving human pose generation conditioned on interacting with articulated objects. We anticipate that the availability of CHAIRS will advance the community’s understanding of
ﬁner-grained interactions. 1.

Introduction the
In the realm of computer vision and robotics, fundamental comprehension of Human-Object Interaction (HOI) [30, 31, 64, 62] lies at the core of dissecting intri-cate human activities. This paper embarks on unraveling the complex challenge of Full-Body Articulated Human-Object 1
Table 1: Comparisons between CHAIRS and other HOI datasets.
Dataset
# object # participants # instructions # hours fps # view articulated objects human annotation type
GRAB [46]
D3D-HOI [58]
BEHAVE [2]
ARCTIC [11]
COUCH [67]
CHAIRS (Ours) 51 24 20 10 4 81 10 5 8 9 6 46 4
/ 6 1 6 32 3.8 0.6 4.2 1.2 3 17.3 120 3 30 30 60 30 0 1 4 8+1 4 4
No
Yes
No
Yes
No
Yes mocap manual
Whole-body
Whole-body
Whole-body multi-kinect
Two hands
Whole-body
Whole-body mocap mocap mocap
Interaction (f-AHOI). This endeavor mandates tackling two pivotal dimensions: (i) fashioning kinematic-agnostic repre-sentations for articulated objects and (ii) delving into the intricate spatial-temporal tapestry interweaving objects with human whole-bodies. Our primary focus resides in address-ing the intricate task of object pose estimation within the realm of f-AHOI, considering that reconstructing 3D human poses from frontal viewpoints is comparably uncomplicated.
The crux of object pose estimation within the context of f-AHOI is punctuated by three principal challenges:
The dearth of comprehensive f-AHOI datasets Exist-ing strides in 3D HOI predominantly either assume interac-tions with rigid objects or conﬁne themselves to involving speciﬁc segments of the human anatomy [2, 46, 65, 29, 67, 11, 58, 14]. Regrettably, these assumptions drastically over-simplify genuine human interactions that span diverse body parts engaging with articulated objects embodying moveable elements such as cabinets and ofﬁce chairs. A more intricate level of interaction necessitates a richer dataset.
The multifaceted landscape of object kinematic struc-tures Objects constituting the realm of f-AHOI exhibit notable disparities in their kinematic frameworks, even when categorized under the same umbrella. Prevailing methodolo-gies often lean towards uniform structures [58, 32, 11, 14], thereby disregarding the diverse tapestry that constitutes real-world scenarios. The endeavor of accurately reconstructing objects manifesting divergent geometries and structures is plagued with its own set of challenges.
The intricacies of complex interactions Engaging with articulated objects entails grappling with intricate spa-tial and physical relationships, often entailing occlusions and intricate points of contact. The intricacy of these dynamics thrusts conventional pose estimation mechanisms reliant on point cloud template-matching [65, 49, 19, 39, 28] into the realm of insufﬁciency. The prominence of contacts further compounds the endeavor of precise reconstruction, as slight inaccuracies can swiftly usher implausible interactions into the picture.
The trajectory of this research endeavors to navigate the above three challenges through the prism of three principal solutions, respectively:
To confront the scarcity of f-AHOI datasets, we introduce
CHAIRS, a multi-view RGB-D dataset. Illustrated in Fig. 1,
CHAIRS chronicles a diverse tapestry of interactions, seam-lessly intertwining 46 participants with 81 sittable objects (e.g., chairs, sofas, stools, and benches). 28 of these objects are endowed with moveable parts. Each frame encapsulates 3D meshes of both human whole-bodies and objects, casting a spotlight on interactions with sittable objects that encom-pass a diverse spectrum of structures and distinctive movable elements conducive to multifarious human interactions.
To traverse the labyrinth of kinematic diversity, CHAIRS meticulously selects representative objects characterized by an eclectic array of structures. Unlike traditional datasets and methodologies tethered to uniform kinematics [55, 51, 32], we champion real-world heterogeneity, encompassing the gamut from rigid stools to swivel chairs boasting up to 7 movable components. Each component is linked to its parent through a nexus of revolute, prismatic, or composite joints.
To unravel the enigma of complex interactions, we proffer an innovative approach to articulated object pose estimation, one that harnesses the subtle interplay of ﬁne-grained interac-tion relationships to reconstruct the object in question. This approach diverges from the conventional recourse of man-ually labeling contact maps corresponding to human body parts [65, 16, 2]. Instead, our approach melds the intricacies of these relationships with a reconstruction model and an in-teraction prior, the latter of which is imbued with the essence of a conditional Variational Auto-Encoder (cVAE). This evo-lution sidesteps the need for predeﬁned knowledge grounded in laborious annotation. Moreover, the signiﬁcance of these intricate relationships is showcased through our venture into learning human poses within the ambit of articulated ob-jects. By juxtaposing the generative prowess harnessed from
CHAIRS with that stemming from a dataset centered on rigid objects [67], we underscore the pivotal role played by the nuanced geometrical relationships encapsulated within
CHAIRS in the broader canvas of downstream tasks.
Our contributions are four-fold: (i) CHAIRS, a sprawl-ing multi-view RGB-D repository infused with diverse 3D meshes. (ii) The seamless extension of articulated object pose estimation to the arduous landscape of f-AHOI. (iii)
An object pose estimation approach that transcends the stric-tures of structure. (iv) An overarching interaction prior that captures the subtleties of ﬁne-grained interactions, acting as a catalyst for the journey of pose estimation.
(a) sequences of objects articulating over time (b) frames of diverse interactions
Figure 2: Samples from the CHAIRS dataset. CHAIRS encompasses diverse AHOIs captured through precisely calibrated multi-view
RGB-D cameras, offering detailed 3D meshes of human participants and articulated objects. The ﬁgure showcases (a) RGB frames alongside corresponding ground-truth mesh sequences and (b) an array of varied AHOI instances. 2.