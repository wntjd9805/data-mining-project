Abstract
Transfer learning has been a popular learning paradigm in the deep learning era, especially in annotation-insufficient scenarios. Better ImageNet pre-trained mod-els have been demonstrated, from the perspective of archi-tecture, by previous research to have better transferability to downstream tasks[26]. However, in this paper, we find that during the same pre-training process, models at middle epochs, which are inadequately pre-trained, can outper-form fully trained models when used as feature extractors (FE), while the fine-tuning (FT) performance still grows with the source performance. This reveals that there is not a solid positive correlation between top-1 accuracy on Im-ageNet and the transferring result on target data. Based on the contradictory phenomenon between FE and FT that a better feature extractor fails to be fine-tuned better ac-cordingly, we conduct comprehensive analyses on features before the softmax layer to provide insightful explanations.
Our discoveries suggest that, during pre-training, models tend to first learn spectral components corresponding to large singular values and the residual components con-tribute more when fine-tuning.
Figure 1. Toy experiment of transfer learning from a ResNet18[19] model pre-trained on CIFAR10[28] to a subset of MNIST[29]. FE means viewing the pre-trained model as a feature extractor, and FT means fine-tuning the whole model. It can be seen from the figure that the 5th-epoch model brings the best FE performance, which suggests that further pre-training on the source task would harm the feature quality for the target task. When fine-tuning the whole model, more adequate pre-training tends to deliver higher transfer learning performance. 1.

Introduction
Deep learning has achieved tremendous success in mod-ern computer vision with the aid of the strong supervision of well-labeled datasets, such as ImageNet[10]. However, data annotation is notoriously labor-extensive and time-consuming, especially in some specific domains where ex-pertise is highly required. In such scenarios, transfer learn-ing is of great interest for practitioners to train deep mod-els with a small labeled dataset. Fortunately, existing ef-forts observe that when training on large-scale datasets, middle features of DNNs exhibit remarkable transferabil-ity to various downstream tasks [80, 75]. This facilitates popular deep transfer learning paradigms of fine-tuning a pre-trained model (FT) or simply employing the pre-trained
†Equal contribution. Work is partly done when Andong was an intern at Baidu Research. *Corresponding author. model as a feature extractor (FE). With relatively suffi-cient labeled examples, fine-tuning the whole network usu-ally achieves higher performance. Despite this, FE is still important when training resources are limited, or end-to-end training is not feasible. For example, some applica-tions combine DNN features and other handcrafted fea-tures to obtain both accurate and explainable shallow clas-sifiers [37, 55, 54].
Despite the ubiquitous utilization of pre-trained models, it still remains mysterious how such models benefit transfer learning. Several works pioneer to explore this plausible yet essential problem. [26] systematically investigates whether better-performing models on source tasks, e.g. ImageNet, necessarily yield better performances on downstream tasks.
They confirm this hypothesis for both FE and FT, over deep architectures with different capacities. However, recent works in the domain of adversarial training discover that an
adversarially pre-trained model, though performs worse on
ImageNet due to additional adversarial regularizations, can still transfer better than its natural (following the naming practice in [63], referring to pre-training without adversarial methods) counterpart (with the same architecture)[57, 63].
In fact, these discoveries are to some extent in contradic-tion to the findings in [26], which argues that worse source models may transfer better.
Our work investigates the influence of pre-training on transfer effects from a different perspective. Specifically, we focus on the trajectory of the pre-training process, in-spired by recent studies on the learning order of DNNs.
Several works [2, 23, 35, 41] discover that DNNs tend to firstly learn simple and shallow features, e.g. colors and tex-tures, which are regarded as more general and transferable across different data domains [80]. From the perspective of the frequency domain, such features lie in low-frequency spectrums. On the other hand, several other works re-veal that high-frequency features obtained by a pre-trained model are likely to cause a negative transfer [9].
The aforementioned observations motivate a question that, does a fully pre-trained model definitely outperform its inadequately pre-trained version when transferring to target tasks (according to claims in [26] ), or is there an intermedi-ate pre-trained checkpoint that yields a better transfer effect than that of the fully pre-trained version? To our best knowl-edge, very little work manages to explore how the transfer-ability of a model is impacted by the different stages in a pre-training process.
To investigate this question, we run a toy experiment us-ing CIFAR10 as the source dataset and a subset of MNIST (we randomly choose 100 data points for each digit from the official training split, resulting in a 1000-sample train-ing set) as the target. Briefly, we train a ResNet-18[19] on
CIFAR10 for 200 epochs and choose a set of checkpoints to run transfer learning in two different settings.
In one setting, we treat the pre-trained model as a feature extrac-tor (FE) and only retrain a softmax classifier, while in the other we fine-tune the whole model (FT). The retraining or fine-tuning continues for 100 epochs on the target dataset.
As shown in Figure 1, the best performance of FE comes from the early 5th-epoch model, while the FT performance is higher for later checkpoints.
Two counter-intuitive facts can be observed from our re-sults. One is that, a pre-trained model with higher accuracy on the source task is not necessarily better on the target task, especially when used as a feature extractor (FE). Among the checkpoints on the pre-training trajectory, there is no posi-tive correlation between the source and target accuracy. The other observation shows inconsistent behaviors between FE and FT, indicating that a good starting point (FE) does not guarantee a good final result (FT). In order to explain the observed phenomenons, we investigate the spectral compo-nents of deep features before the FC layer (in Section 4.4), and observe that different parts of components contribute diversely for different pre-trained checkpoints within the same pre-training process.
In this paper, we conduct extensive transfer learning experiments, including ImageNet and the other 9 bench-mark datasets. The results suggest that, when retraining a new classifier on top of the features extracted from pre-trained models, inadequately pre-trained ImageNet models yield significantly better performance than that of the stan-dard 90-epoch pre-trained version, but the performance still highly correlates with the source performance when fine-tuning. Further, we present insightful analyses to explain such a difference from the perspective of spectral compo-nents of the extracted features and find that there are specific components corresponding to pre-trained models at differ-ent pre-training stages. In summary, our main contributions are as follows:
• Our work is the first to investigate how different checkpoints in the same pre-training process perform on transfer learning tasks. This contributes to a broader and deeper understanding of the transferability of neu-ral networks.
• We discover that in the same pre-training process, an inadquately pre-trained model tends to transfer bet-ter than its fully pre-trained counterpart, especially when the pre-trained model is used as a frozen feature extractor. We also further experimentally consolidate this claim beyond image classification.
• We observe that FT prefers later pre-training check-points, compared with FE. Our analyses based on spec-trum decomposition indicate that the learning order of different feature components leads to different prefer-ences of pre-trained checkpoints between FE and FT.
• We also point out the risk of utilizing transferability assessment approaches as a general tool to select pre-trained models. We evaluate LogME [76], LEEP[44] and NCE[62], which are dependent on frozen pre-trained models. Aiming to select the best pre-trained model among different checkpoints, scores obtained by these algorithms often show poor correlations with the actual fine-tuning performance. 2.