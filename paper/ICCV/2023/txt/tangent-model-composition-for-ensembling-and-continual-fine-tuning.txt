Abstract
Tangent Model Composition (TMC) is a method to com-bine component models independently fine-tuned around a pre-trained point. Component models are tangent vec-tors to the pre-trained model that can be added, scaled, or subtracted to support incremental learning, ensembling, or unlearning. Component models are composed at infer-ence time via scalar combination, reducing the cost of en-sembling to that of a single model. TMC improves accu-racy by 4.2% compared to ensembling non-linearly fine-tuned models at a 2.5× to 10× reduction of inference cost, growing linearly with the number of component models.
Each component model can be forgotten at zero cost, with no residual effect on the resulting inference. When used for continual fine-tuning, TMC is not constrained by se-quential bias and can be executed in parallel on federated data. TMC outperforms recently published continual fine-tuning methods almost uniformly on each setting – task-incremental, class-incremental, and data-incremental – on a total of 13 experiments across 3 benchmark datasets, de-spite not using any replay buffer. TMC is designed for com-posing models that are local to a pre-trained embedding, but could be extended to more general settings. The code is available at: https://github.com/tianyu139/ tangent-model-composition 1.

Introduction
Compositionality has long been considered central to cognition [5], possibly a reflection of compositionality in neural activity [50]. After decades of unsuccessful attempts to design compositional representations, deep neural net-works trained on language data are beginning to exhibit emergent compositionality at scale, by capturing the com-positional structure of the data. These models are now be-ing used for visual inference, which has rekindled interest in the study of compositionality of visual representations.
But beyond the data, any compositional structure that may be latent in the model is not directly accessible: One cannot simply “compose” weights or inner activations and expect meaningful outcomes. The computational architecture of
Transformers [52] has been leveraged extensively to co-opt the compositional structure of data through prompts or to-kens [29, 55], but still the activations of trained models do not appear to be meaningfully composable. Composition-ality of neural activity would allow one to combine activa-tions from different models to capture novel concepts, or incorporate knowledge from different data without having to re-train or fine-tune the core models. This would enable open-universe classification and, more generally, combina-torial expansion of the hypothesis space. Continual learning could be performed simply by composing models trained on different data.
In this paper, we explore the simplest form of compo-sitionality, that is linear combination. We leverage recent results on the linearization of deep neural networks around a pre-trained point, that can be trained by solving a con-vex optimization problem and yet perform on-par with non-linear fine-tuning [1]. This suggests that the tangent space at pre-trained models may be used to linearly compose neu-ral activations, or equivalently compose different models trained or fine-tuned on different datasets and/or for differ-ent tasks.
We show that different models obtained from the lin-earization around a pre-trained point can be composed, combined, rescaled, and forgotten (“unlearning”), simply by scalar combinations. This fact can be used to perform ensembling at the inference cost of a single model (Sec. 3).
It can also be used for continual fine-tuning, with each com-ponent model trained independently and in parallel, if nec-essary on federated data that can therefore be easily forgot-ten if needed.
Linear combination is not viable for general concept composition nor arbitrary multi-task learning. In Sec. 5, we discuss limitations of our approach, which can only mean-ingfully compose models that are “local” to a pre-trained embedding. If there are models trained on tasks that are far in representation space, or even antagonistic, they are likely to live on different tangent planes, making linear combi-nation inappropriate. One such example, described in the appendix, concerns models trained on real images (Ima-Figure 1. Composition of Fine-tuned Models: Purple indicates final models used for inference, yellow indicates specialist models fine-tuned on individual tasks. The paragon (a) is a model trained jointly on all tasks. However, in continual learning, different tasks are instantiated at different times, an re-training on their union is impractical. Ensembling (b) combines the output of different models trained separately on each task. Weight averaging (c) yields a “model soup” of fine-tuned non-linear models, which improves inference time;
Tanget Model Composition (d) linearly composes models fine-tuned on the tangent space of a pre-trained model. geNet) and clip art or sketches (DomainNet). Nonetheless, in more common settings, TMC is competitive with general forms of ensembling such as averaging activations or log-its of non-linearly fine-tuned models, and with more gen-eral forms of continual learning, including methods that use a replay buffer. For broader task coverage, one could use
TMC to construct a tangent bundle of models around differ-ent pre-trained points, akin to a “tangent model zoo.” This concept is not too dissimilar from the architecture of some
Foundation Models with a shared backbone and multiple distinct “heavy heads” [54], but far more compact, modu-lar, efficient, interpretable, and easy to work with.
In the next section we place our contributions in the con-text of prior art, and in the following one we describe our method in more detail. Sec. 4 summarizes empirical evi-dence in support of our approach, and finally Sec. 5 dis-cusses its main limitations. 2.