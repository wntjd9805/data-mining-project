Abstract
Diffusion model (DM) has achieved SOTA performance by modeling the image synthesis process into a sequential application of a denoising network. However, different from image synthesis, image restoration (IR) has a strong con-straint to generate results in accordance with ground-truth.
Thus, for IR, traditional DMs running massive iterations on a large model to estimate whole images or feature maps is inefficient. To address this issue, we propose an effi-cient DM for IR (DiffIR), which consists of a compact IR prior extraction network (CPEN), dynamic IR transformer (DIRformer), and denoising network. Specifically, DiffIR has two training stages: pretraining and training DM. In pretraining, we input ground-truth images into CPENS1 to capture a compact IR prior representation (IPR) to guide
DIRformer. In the second stage, we train the DM to directly estimate the same IRP as pretrained CPENS1 only using LQ images. We observe that since the IPR is only a compact vector, DiffIR can use fewer iterations than traditional DM to obtain accurate estimations and generate more stable and realistic results. Since the iterations are few, our Dif-fIR can adopt a joint optimization of CPENS2, DIRformer, and denoising network, which can further reduce the esti-mation error influence. We conduct extensive experiments on several IR tasks and achieve SOTA performance while consuming less computational costs. Code is available at https://github.com/Zj-BinXia/DiffIR. 1.

Introduction
Image Restoration (IR) is a long-standing problem due to its extensive application value and ill-posed nature. IR aims to restore a high-quality (HQ) image from its low-quality (LQ) counterpart corrupted by various degradation factors (e.g., blur, mask, downsampling). Presently, deep-learning based IR methods have achieved impressive success, as they can learn strong priors from large-scale datasets.
Recently, Diffusion Models (DMs) [50], which is built
*Corresponding Author from a hierarchy of denoising autoencoders, have achieved impressive results in image synthesis [21, 51, 11, 22] and IR tasks (such as inpainting [37, 46] and super-resolution [48]).
Specifically, DMs are trained to iteratively denoise the im-age by reversing a diffusion process. DMs have shown that the principled probabilistic diffusion modeling can real-ize high-quality mapping from randomly sampled Gaussian noise to the complex target distribution, such as a realistic image or latent [46] distribution, without suffering mode-collapse and training instabilities as GANs.
As a class of likelihood-based models, DMs require a large number of iteration steps (about 50 − 1000 steps) on large denoising models to model precise details of the data, which consumes massive computational resources. Un-like the image synthesis tasks generating each pixel from scratch, IR tasks only require adding accurate details on the given LQ images. Therefore, if DMs adopt the paradigm of image synthesis for IR, it would not only waste a large num-ber of computational resources but also be easy to generate some details that do not match given LQ images.
In this paper, we aim to design a DM-based IR network that can fully and efficiently use the powerful distribution mapping abilities of DM to restore images. To this end, we propose DiffIR. Since the transformer can model long-range pixel dependencies, we adopt the transformer blocks as our basic unit of DiffIR. We stack transformer blocks in
Unet shape to form Dynamic IRformer (DIRformer) to ex-tract and aggregate multi-level features. We train our DiffIR in two stages: (1) In the first stage (Fig. 2 (a)), we develop a compact IR prior extraction network (CPEN) to extract a compact IR prior representation (IPR) from ground-truth images to guide the DIRformer. Besides, we develop Dy-namic Gated Feed-Forward Network (DGFN) and Dynamic
Multi-Head Transposed Attention (DMTA) for DIRformer to fully use the IPR. It is notable that CPEN and DIRformer are optimized together. (2) In the second stage (Fig. 2 (b)), we train the DM to directly estimate the accurate IPR from
LQ images. Since the IPR is light and only adds details for restoration, our DM can estimate quite an accurate IPR and obtain stable visual results after several iterations.
Apart from the above scheme and architectural novel-(a) Inpainting (Tab. 1) (b) Super-Resolution (Tab. 2)
Figure 1. The Mult-Adds are measured on 256×256 inputs. Our DiffIR achieves SOTA performance on IR tasks. Notably, LDM [46] and
RePaint [37] are DM-based methods, and DiffIR is 1000× more efficient than RePaint while achieving better performance. (c) Motion deblurring (Tab. 3) ties, we show the effectiveness of joint optimization. In the second stage, we observe that the estimated IPR may still have minor errors, which will affect the performance of the
DIRformer. However, the previous DMs need many itera-tions, which is unavailable to optimize DM with the decoder together. Since our DiffIR requires few iterations, we can run all iterations and obtain the estimated IPR to optimize with DIRformer jointly. As shown in Fig. 1, our DiffIR achieves SOTA performance consuming much less compu-tation than other DM-based methods (e.g., RePaint [37] and
LDM [46]). In particular, DiffIR is 1000× more efficient than RePaint. Our main contributions are threefold:
• We propose DiffIR, a strong, simple, and efficient DM-based baseline for IR. Unlike image synthesis, most pixels of input images in IR are given. Thus, we use the strong mapping abilities of DM to estimate a com-pact IPR to guide IR, which can improve the restora-tion efficiency and stability for DM in IR.
• We propose DGTA and DGFN for Dynamic IRformer to fully exploit the IPR. Different from the previous latent DMs optimizing the denoising network individ-ually, we propose joint optimization of the denoising network and decoder (i.e., DIRformer) to further im-prove the robustness of estimation errors.
• Extensive experiments show that the proposed DiffIR can achieve SOTA performance in IR tasks while con-suming much less computational resources compared with other DM-based methods. 2.