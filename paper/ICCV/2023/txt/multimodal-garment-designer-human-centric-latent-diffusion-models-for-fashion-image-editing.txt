Abstract
Fashion illustration is used by designers to communi-cate their vision and to bring the design idea from con-ceptualization to realization, showing how clothes inter-act with the human body.
In this context, computer vi-sion can thus be used to improve the fashion design pro-cess. Differently from previous works that mainly focused on the virtual try-on of garments, we propose the task of multimodal-conditioned fashion image editing, guiding the generation of human-centric fashion images by following multimodal prompts, such as text, human body poses, and garment sketches. We tackle this problem by proposing a new architecture based on latent diffusion models, an ap-proach that has not been used before in the fashion do-main. Given the lack of existing datasets suitable for the task, we also extend two existing fashion datasets, namely
Dress Code and VITON-HD, with multimodal annotations collected in a semi-automatic manner. Experimental re-sults on these new datasets demonstrate the effectiveness of our proposal, both in terms of realism and coherence with the given multimodal inputs. Source code and col-lected multimodal annotations are publicly available at: https://github.com/aimagelab/multimodal-garment-designer. 1.

Introduction
Computer Vision research has always paid much atten-tion both to the human person and to fashion-related prob-lems, especially working on the recognition and retrieval of clothing items [11, 22], the recommendation of similar garments [8, 18, 39], and the virtual try-on of clothes and accessories [7, 13, 27, 28, 47, 52]. In the last years, some re-search efforts have been dedicated to the text-conditioned image editing task where, given a model image and a tex-tual description of a garment, the goal is to generate the
∗Equal contribution.
input model wearing a new clothing item corresponding to the given textual description.
In this context, only a few works [19, 33, 56] have been proposed, exclusively employ-ing GAN-based approaches for the generative step.
Recently, diffusion models [10, 17, 30, 41] have attracted more and more attention due to their outstanding genera-tion capabilities, allowing the improvement of a variety of downstream tasks in several domains, while their applica-bility to the fashion domain is still unexplored. Many dif-ferent solutions have been introduced and can roughly be identified based on the denoising conditions used to guide the diffusion process, which can enable greater control of the synthesized output. A particular type of diffusion model has been proposed in [37] that, instead of applying the dif-fusion process in the pixel space, defines the forward and the reverse processes in the latent space of a pre-trained au-toencoder, becoming one of the leading choices thanks to its reduced computational cost. Although this solution can generate highly realistic images, it does not perform well in human-centric generation tasks and can not deal with mul-tiple conditioning signals to guide the generation phase.
In this work, we address an extended and more gen-eral framework and define the new task of multimodal-conditioned fashion image editing, which allows guiding the generative process via multimodal prompts while pre-serving the identity and body shape of a given person (Fig. 1). To tackle this task, we introduce a new archi-tecture, called Multimodal Garment Designer (MGD), that emulates the process of a designer conceiving a new gar-ment on a model shape, based on preliminary indications provided through a textual sentence or a garment sketch.
In particular, starting from Stable Diffusion [37], we pro-pose a denoising network that can be conditioned by multi-ple modalities and also takes into account the pose consis-tency between input and generated images, thus improving the effectiveness of human-centric diffusion models.
To address the newly proposed task, we present a semi-automatic framework to extend existing datasets with mul-timodal data. Specifically, we start from two famous virtual try-on datasets (i.e. Dress Code [28] and VITON-HD [7]) and extend them with textual descriptions and garment sketches. Experimental results on the two proposed mul-timodal fashion benchmarks show both quantitatively and qualitatively that our proposed architecture generates high-quality images based on the given multimodal inputs and outperforms all considered competitors and baselines, also according to human evaluations.
To sum up, our contributions are as follows: (1) We pro-pose a novel task of multimodal-conditioned fashion image editing, which entails the use of multimodal data to guide the generation. (2) We introduce a new human-centric gen-erative architecture based on latent diffusion models, capa-ble of following multimodal prompts while preserving the model’s characteristics. (3) To tackle the new task, we ex-tend two existing fashion datasets with textual sentences and garment sketches devising a semi-automatic annotation framework. (4) Extensive experiments demonstrate that the proposed approach outperforms other competitors in terms of realism and coherence with multimodal inputs. 2.