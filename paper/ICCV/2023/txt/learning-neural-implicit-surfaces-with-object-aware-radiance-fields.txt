Abstract 1.

Introduction
Recent progress on multi-view 3D object reconstruction has featured neural implicit surfaces via learning high-fidelity radiance fields. However, most approaches hinge on the visual hull derived from cost-expensive silhouette masks to obtain object surfaces.
In this paper, we pro-pose a novel Object-aware Radiance Fields (ORF) to au-tomatically learn an object-aware geometry reconstruction.
The geometric correspondences between multi-view 2D ob-ject regions and 3D implicit/explicit object surfaces are additionally exploited to boost the learning of object sur-faces. Technically, a critical transparency discriminator is designed to distinguish the object-intersected and object-bypassed rays based on the estimated 2D object regions, leading to 3D implicit object surfaces. Such implicit sur-faces can be directly converted into explicit object surfaces (e.g., meshes) via marching cubes. Then, we build the geo-metric correspondence between 2D planes and 3D meshes by rasterization, and project the estimated object regions into 3D explicit object surfaces by aggregating the object information across multiple views. The aggregated object information in 3D explicit object surfaces is further repro-jected back to 2D planes, aiming to update 2D object re-gions and enforce them to be multi-view consistent. Exten-sive experiments on DTU and BlendedMVS verify the capa-bility of ORF to produce comparable surfaces against the state-of-the-art models that demand silhouette masks.
Multi-view 3D reconstruction, i.e., the task of recon-structing 3D geometry/surface of objects from multi-view 2D images, has played a fundamental challenge to computer vision and computer graphics communities for decades. In the early stage, the mainstream solution is the classical
Multi-View Stereo (MVS) [3, 5, 12, 20, 34, 35] that exploits photometric consistency across different camera views to learn explicit geometry representations (e.g., meshes or voxel grids). The ultimate reconstruction relies heavily on the quality of cross-view matching. In practice, such match-ing often fails to associate objects with sparse textures, re-sulting in severe artifact or missing parts on surfaces. To address this issue, recent studies turn their focus on inves-tigating how to represent 3D surfaces as implicit geometry representations. Many consider learning a continuous im-plicit function that formulates neural implicit surfaces in occupancy field [25, 31] or signed distance field [29]. In between, surface rendering techniques [27, 47, 49] are de-signed to optimize these fields, leading to impressive recon-struction quality via differentiable rendering from images.
Nevertheless, learning such implicit geometry representa-tions requires additional object masks of scenes, since the color of each ray is assumed to only correspond to a single point where a surface intersects with this ray. Moreover, the gradient of differentiable rendering is only backpropa-gated to the local surface near intersection, resulting in a sub-optimal solution for neural implicit surfaces.
To mitigate these limitations of local gradient propaga-tion and the demand for the input object masks during fields optimization, a series of volume rendering based neural ra-diance fields techniques [9, 28, 41, 46] start to emerge.
Recently, the seminal work of NeRF [26] builds up the foundation of volumetric scene in neural radiance fields for view synthesis, and performs classical volume rendering via alpha-compositing colors of the sampled points along rays. The subsequent methods further remould classical volume rendering by imposing implicit surface representa-tions, e.g., occupancy network [28] or signed distance func-tion [9, 41, 46]. These approaches produce more accurate surfaces by reconstructing the holistic scenery. Neverthe-less, the learnt implicit surfaces are inevitably composed of rich geometry representations in both foreground and back-ground (Figure 1 (a)), which are not distinguished during radiance fields optimization. Accordingly, existing tech-niques utilize additional cost-expensive silhouette masks to trim the learnt 3D meshes derived from implicit surfaces.
The removal of vertices and/or surfaces outside the visual hull directly highlights the geometry of the key objects in the foreground, yielding object surfaces.
In this work, we devise a new Object-aware Radiance
Field (ORF), which goes one step further to eliminate the need of the visual hull derived from human-annotated object masks in scenes for learning object surfaces. Our launch-ing point is to introduce an object-aware volumetric scene representation by inferring the foreground/object and back-ground radiance fields on-the-fly. Both the geometric corre-spondences between multi-view 2D object regions and 3D implicit or explicit object surfaces are exploited to boost the learning of object-aware volumetric scene representa-tion. ORF is henceforth able to encourage the reconstruc-tion of foreground/object geometry without additional 3D supervision of visual hull. Specifically, we first estimate 2D object regions directly based on the multi-view images.
Next, on the basis of the estimated 2D object regions, ORF capitalizes on a transparency discriminator to automatically recognize the transparency of each ray in the radiance field.
A low transparency indicates that the ray intersects with the object, and rays with high transparency are considered to be object-bypassed. Such predicted transparency of rays is further regarded as prior information to regularize the radi-ance field in an object-aware manner. Especially, as shown in Figure 1 (b), all the sampled rays are divided into object-intersected rays and object-bypassed rays according to pre-dictions given by the transparency discriminator, leading to 3D implicit object surfaces. After that, we leverage march-ing cubes to directly convert the 3D implicit object surfaces into 3D explicit object surfaces. The inherent geometric correspondence between 2D planes and 3D explicit object surfaces are thus constructed via rasterization. This geo-metric correspondence enables the projection from the es-timated 2D object regions into 3D explicit object surfaces, and meanwhile the object information across multiple views are aggregated. Furthermore, ORF projects the aggregated object information in 3D explicit object surfaces back to 2D planes, thereby updating the 2D object regions and enforc-ing them multi-view consistent. The whole process refines the estimated 2D object regions and 3D implicit/explicit ob-ject surfaces, pursuing an object reconstruction without triv-ial background surfaces.
In sum, we have made the following contributions: (I)
ORF designs a transparency discriminator to automatically capture useful object-aware inductive bias, which further supervises radiance fields to learn object-aware geometry (II) ORF additionally mines the inherent reconstruction. geometric correspondence between multi-view 2D object regions and 3D object surfaces to refine them along with volume rendering. (III) We evaluate ORF on two widely-used benchmarks (DTU and BlendedMVS), demonstrating the effectiveness of our proposal. 2.