Abstract 1.

Introduction
We explore the task of embodied view synthesis from monocular videos of deformable scenes. Given a minute-long RGBD video of people interacting with their pets, we render the scene from novel camera trajectories derived from the in-scene motion of actors: (1) egocentric cam-eras that simulate the point of view of a target actor and (2) 3rd-person cameras that follow the actor. Building such a system requires reconstructing the root-body and articu-lated motion of every actor, as well as a scene representa-tion that supports free-viewpoint synthesis. Longer videos are more likely to capture the scene from diverse viewpoints (which helps reconstruction) but are also more likely to con-tain larger motions (which complicates reconstruction). To address these challenges, we present Total-Recon, the first method to photorealistically reconstruct deformable scenes from long monocular RGBD videos. Crucially, to scale to long videos, our method hierarchically decomposes the scene into the background and objects, whose motion is de-composed into carefully initialized root-body motion and local articulations. To quantify such “in-the-wild” recon-struction and view synthesis, we collect ground-truth data from a specialized stereo RGBD capture rig for 11 chal-lenging videos, significantly outperforming prior methods.
We explore embodied view synthesis, a new class of novel-view synthesis tasks that renders deformable scenes from novel 6-DOF trajectories reconstructed from the in-scene motion of actors: egocentric cameras [45, 7] that sim-ulate the point-of-view of moving actors and 3rd-person-follow cameras [54, 7] that track a moving actor from be-hind (Figure 1). We focus on everyday scenes of people interacting with their pets, producing renderings from the point-of-view of the person and pet (Figure 1). While such camera trajectories could be manually constructed (e.g., by artists via keyframing), building an automated system is an interesting problem of its own: spatial cognition the-ory [57] suggests that the ability to visualize behavior from another actor’s perspective is necessary for action learning and imitation; in the context of gaming and virtual real-ity [7, 45], egocentric cameras offer high levels of user im-mersion, while 3rd-person-follow cameras provide a large field of view that is useful for exploring a user’s environ-ment.
Challenges. Building a system for embodied view syn-thesis is challenging for many reasons. First, to reconstruct everyday-but-interesting content, it needs to process long, monocular captures of multiple interacting actors. How-Figure 2: Method Overview. Total-Recon represents the entire scene as a composition of M object-centric neural fields, one for the rigid background and each of the M − 1 deformable objects. To render a scene, (1) each object field j is transformed into the camera space with a rigid transformation (cid:0)Gt (cid:1)−1 that encodes root-body motion and, for each deformable object, an additional deformation field Jt,→ that encodes articulated motion. Next, all (2) posed object fields are combined into a (3) composite field, which is then volume-rendered into (4) color, depth, optical flow, and object silhouettes. Each rendered output defines a reconstruction loss that derives supervision from a monocular RGBD video captured by a moving iPad Pro. j j ever, such videos are likely to contain large scene motions, which we demonstrate are difficult to reconstruct with cur-rent approaches. Second, it needs to produce a deformable 3D scene representation that supports free-viewpoint synthesis, which also would benefit from long videos likely to capture the scene from diverse viewpoints. Recent approaches have extended Neural Radiance Fields (NeRFs)
[28] to deformable scenes, but such work is often limited to rigid-only object motion [18, 33], short videos with limited scene motion [41, 35, 21, 55, 36, 60, 10, 61, 58], or reconstructing single objects as opposed the entire scene [65, 66, 67, 4]. Third, it needs to compute global 6-DOF trajectories of root-bodies and articulated body parts (e.g., head) of multiple actors.
Key Ideas. To address these challenges, we introduce
Total-Recon, the first monocular NeRF that enables em-bodied view synthesis for deformable scenes with large motions. Given a monocular RGBD video, Total-Recon re-constructs the scene as a composition of object-centric rep-resentations, which encode the 3D appearance, geometry, and motion of each deformable object and the background.
Crucially, Total-Recon hierarchically decomposes scene motion into the motion of individual objects, which itself is decomposed into global root-body movement and the local deformation of articulated body parts. We demonstrate that such decomposition of object motion, along with appropri-ate initialization of root-body pose, allows reconstruction to scale to longer videos, enabling free-viewpoint synthesis.
By reconstructing such motions in a globally-consistent coordinate frame, Total-Recon can generate renderings from egocentric and 3rd-person-follow cameras, as well as static but extreme viewpoints like bird’s-eye-views.
Evaluation. Due to the difficulty of collecting ground-truth data for embodied view synthesis on in-the-wild videos, we evaluate our method on the proxy task of stereo-view synthesis [35], which compares rendered views to those captured from a stereo pair. To this end, we build a stereo RGBD sensor capture rig for ground-truthing and collect a dataset of 11 long video sequences in various indoor environments, including people interacting with their pets. Total-Recon outperforms the state-of-the-art monocular deformable NeRF methods [36, 60], even when modified to use depth sensor measurements.
In summary, our contributions are:
Contributions. (1)
Total-Recon, a hierarchical 3D representation that mod-els deformable scenes as a composition of object-centric representations, each of which decomposes object motion into its global root-body motion and its local articulations; (2) a system based on Total-Recon for automated embod-ied view synthesis from casual, minute-long RGBD videos of highly dynamic scenes; (3) a dataset of stereo RGBD videos containing various deformable objects, such as hu-mans and pets, in a host of different background environ-ments. Our code, models, and data can be found at https:
//andrewsonga.github.io/totalrecon.
Method
Entire
Scenes
Deform.
Objects
Beyond
Humans
BANMo [67]
PNF [18]
NeuMan [16]
SLAHMR [69]
HyperNeRF [36]
D2NeRF [60]
DynIBaR [22]
SUDS [56]
Ours
✗
✓
✓
✗
✓
✓
✓
✓
✓
✓
✗
✓
✓
✓
✓
✓
✓
✓
✓
✓
✗
✗
✓
✓
✓
✓
✓
Global 6-DOF
Traj.
✗
✓
✓
✓
✗
✗
✗
✗
✓
Long
Videos
Extreme
Views
✓
✗
✗
✗
✗
✗
✓
✓
✓
✓
✗
✗
✗
✗
✗
✗
✗
✓
Table 1: Comparison to