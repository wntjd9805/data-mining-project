Abstract
In recent years, Transformer networks are beginning to replace pure convolutional neural networks (CNNs) in the
ﬁeld of computer vision due to their global receptive ﬁeld and adaptability to input. However, the quadratic com-putational complexity of softmax-attention limits the wide application in image dehazing task, especially for high-resolution images. To address this issue, we propose a new
Transformer variant, which applies the Taylor expansion to approximate the softmax-attention and achieves linear computational complexity. A multi-scale attention reﬁne-ment module is proposed as a complement to correct the error of the Taylor expansion. Furthermore, we introduce a multi-branch architecture with multi-scale patch embed-ding to the proposed Transformer, which embeds features by overlapping deformable convolution of different scales.
The design of multi-scale patch embedding is based on three key ideas: 1) various sizes of the receptive ﬁeld; 2) multi-level semantic information; 3) ﬂexible shapes of the recep-tive ﬁeld. Our model, named Multi-branch Transformer expanded by Taylor formula (MB-TaylorFormer), can em-bed coarse to ﬁne features more ﬂexibly at the patch em-bedding stage and capture long-distance pixel interactions with limited computational cost. Experimental results on several dehazing benchmarks show that MB-TaylorFormer achieves state-of-the-art (SOTA) performance with a light computational burden. The source code and pre-trained models are available at https://github.com/FVL2020/ICCV-2023-MB-TaylorFormer. 1.

Introduction
Single image dehazing is an image restoration task which aims to estimate latent haze-free images from hazy images. Starting with early CNN-based approaches [49, 5]
*Corresponding author: jinzh26@mail2.sysu.edu.cn
Figure 1: Improvement of MB-TaylorFormer over the
SOTA approaches. The circle size is proportional to the number of model parameters. All models are trained on
SOTS-Indoor [32]. and their revolutionary performance in dehazing, haze re-moval gradually shifts from prior based strategies [89, 20] to deep learning-based methods. In the past decade, deep dehazing networks achieve signiﬁcant performance im-provement due to advanced architectures like multi-scale in-formation fusion [49, 47, 37], sophisticated variants of con-volution [74, 36], and attention mechanisms [45, 86, 21].
Recently, Transformer has been popularly employed in various computer vision tasks and subsequently contributes greatly to the progress of high-level vision tasks [39, 71,
In the ﬁeld of image dehazing, there are several 61, 6]. challenges with the direct application of Transformer: 1) the computational complexity of Transformer is quadratic with the resolution of feature map, which makes it poorly suited to the pixel-to-pixel task of dehazing. Although some works apply self-attention in small spatial windows [35, 73] to relieve this problem, the receptive ﬁeld of Transformer is restricted; 2) the basic elements of visual Transformer usually have more ﬂexible scales [39]. However, existing
visual Transformer networks [78, 73] generally generate
ﬁxed-scale tokens by ﬁxed convolution kernels. Thus, there is still room for improvement via introducing ﬂexible patch embedding Transformer to the dehazing task.
To address the ﬁrst challenge, we propose a Transformer variant expanded by Taylor formula (TaylorFormer), which applies self-attention on the entire feature map across spa-tial dimension and maintains linear computational complex-ity. Speciﬁcally, we calculate the weights of self-attention by performing a Taylor expansion on softmax, and then re-duce the computational complexity of self-attention from
O(n2) to O(n) by applying the associative law of matrix multiplication. This strategy brings three advantages: 1) it retains the ability of Transformer to model long-distance dependencies among data, and avoids reducing the recep-tive ﬁeld caused by window splitting [58]; 2) it provides a stronger value approximation than methods using a kernel-based formulation of self-attention [28], and is similar to the vanilla Transformer [64]; 3) it makes the Transformer concerned with pixel-level interactions rather than channel-level interactions [78], which allows for more ﬁne-grained processing of features.
Considering the error caused by ignoring Peano’s form of remainder [43] in the Taylor formula, we introduce a multi-scale attention reﬁnement (MSAR) module to reﬁne
TaylorFormer. We exploit the local correlation within im-age by convolving the local information of queries and keys to output a feature map with scaling factors. The number of feature map channels is equal to the number of heads in multi-head self-attention (MSA), so each head has a cor-responding scaling factor. Our experiments show that the proposed MSAR module effectively improves model per-formance with tiny computational burden (see Table 3).
To tackle the second challenge, inspired by the suc-cess of inception modules [60, 59] and deformable convo-lutions [13] in CNN-based dehazing networks [18, 65, 44, 74], we propose a multi-branch encoder-decoder backbone for TaylorFormer, termed ad MB-TaylorForemr, based on multi-scale patch embedding. The multi-scale patch em-bedding has various sizes of receptive ﬁeld, multi-level se-mantic information, and ﬂexible shape of receptive ﬁeld.
Considering that the generation of each token should fol-low the local relevance prior, we truncate the offsets of the deformable convolutions. We reduce computational com-plexity and the number of parameters by the depthwise sep-arable method. The tokens from different scales are then fed independently into TaylorFormer and ﬁnally fused. The multi-scale patch embedding module is capable of gener-ating tokens with different scales and dimensions, and the multi-branch structure is capable of processing them simul-taneously to capture more powerful features.
To summarize, our main contributions are as follows: (1)
We propose a new variant of linearized Transformer based on Taylor expansion to model long-distance interactions be-tween pixels without window splitting. An MSAR module is introduced to further correct errors in the self-attention of TaylorFormer; (2) We design a multi-branch architec-ture with multi-scale patch embedding. Among it, multiple
ﬁeld sizes, ﬂexible shape of receptive ﬁeld, and multi-level semantic information can help simultaneously generate to-kens with multi-scales and capture more powerful features; (3) Experimental results on public synthetic and real de-hazing datasets show that the proposed MB-TaylorForme achieves state-of-the-art (SOTA) performance with few pa-rameters and MACs. 2.