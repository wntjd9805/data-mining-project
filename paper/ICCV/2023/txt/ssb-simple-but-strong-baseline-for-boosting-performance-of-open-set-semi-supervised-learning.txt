Abstract
Semi-supervised learning (SSL) methods effectively leverage unlabeled data to improve model generalization.
However, SSL models often underperform in open-set sce-narios, where unlabeled data contain outliers from novel
In this categories that do not appear in the labeled set. paper, we study the challenging and realistic open-set SSL setting, where the goal is to both correctly classify in-liers and to detect outliers.
Intuitively, the inlier classi-ﬁer should be trained on inlier data only. However, we
ﬁnd that inlier classiﬁcation performance can be largely improved by incorporating high-conﬁdence pseudo-labeled data, regardless of whether they are inliers or outliers. Also, we propose to utilize non-linear transformations to sepa-rate the features used for inlier classiﬁcation and outlier detection in the multi-task learning framework, preventing adverse effects between them. Additionally, we introduce pseudo-negative mining, which further boosts outlier detec-tion performance. The three ingredients lead to what we call Simple but Strong Baseline (SSB) for open-set SSL. In experiments, SSB greatly improves both inlier classiﬁcation and outlier detection performance, outperforming existing methods by a large margin. Our code will be released at https://github.com/YUE-FAN/SSB. 1.

Introduction
Semi-supervised learning (SSL) has achieved great suc-cess in improving model performance by leveraging un-labeled data [26, 25, 42, 29, 4, 3, 41, 45, 11, 50, 44].
However, standard SSL assumes that the unlabeled samples come from the same set of categories as the labeled samples, which makes them struggle in open-set settings [33], where unlabeled data contain out-of-distribution (OOD) samples from novel classes that do not appear in the labeled set (see
Fig. 1). In this paper, we study this more realistic setting called open-set semi-supervised learning, where the goal is
Inliers (cid:71)(cid:82)(cid:74) (cid:69)(cid:76)(cid:78)(cid:72) (cid:70)(cid:68)(cid:87)
Seen outliers
Unseen outliers
TRAIN
Labeled       Data
Unlabeled         Data
TEST
Figure 1: Open-set semi-supervised learning considers a re-alistic and challenging setting, where unlabeled data con-tains samples from novel classes (seen outliers) that do not appear in the labeled data. At test time, the model should correctly classify inliers, while identifying outliers seen during the training and, most importantly, unseen outliers that do not appear in the training set. We measure test accu-racy for the inlier classiﬁcation performance and AUROC for the outlier detection performance. Our method (SSB) achieves superior performance in both tasks. to learn both a good closed-set classiﬁer to classify inliers and to detect outliers as shown in Fig. 1.
Recent works on open-set SSL [20, 7, 38, 48, 14, 16, 17, 21] have achieved strong performance [43, 28, 19, 1] through a multi-task learning framework, which consists of an inlier classiﬁer, an outlier detector, and a shared fea-ture encoder, as shown in Figure 2. The outlier detector is trained to ﬁlter out OOD data from the unlabeled data so that the classiﬁer is only trained on inliers. However, this framework has two major drawbacks. First, detector-based
ﬁltering often removes many inliers along with OOD data, leading to suboptimal classiﬁcation performance due to the low utilization ratio of unlabeled data. Second, the inlier
classiﬁer which shares the same feature encoder with the outlier detector can have an adverse effect on the detection performance as shown in Table 1.
To this end, we contribute a Simple but Strong Baseline,
SSB, for open-set SSL with three ingredients to address the above issues. (1) In contrast to detector-based ﬁltering aim-ing to remove OOD data, we propose to incorporate pseudo-labels with high inlier classiﬁer conﬁdence into the training, irrespective of whether a sample is an inlier or OOD. This not only effectively improves the unlabeled data utilization ratio but also includes many useful OOD data that can be seen as natural data augmentations of inliers (see Fig. 5). (2) Instead of directly sharing features between the classi-ﬁer and detector, we add non-linear transformations for the task-speciﬁc heads and ﬁnd that this effectively reduces mu-tual interference between them, resulting in more special-ized features and improved performance for both tasks. (3)
In addition, we propose pseudo-negative mining to further improve outlier detector training by enhancing the data di-versity of OOD data with pseudo-outliers. Despite its sim-plicity, SSB achieves signiﬁcant improvements in both in-lier classiﬁcation and OOD detection. As shown in Fig. 1, existing methods either struggle in detecting outliers or have difﬁculties with inlier classiﬁcation while SSB obtains good performance for both tasks. 2.