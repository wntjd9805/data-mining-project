Abstract
Although deep learning-based solutions have achieved impressive reconstruction performance in image super-resolution (SR), these models are generally large, with com-plex architectures, making them incompatible with low-power devices with many computational and memory con-straints.
To overcome these challenges, we propose a spatially-adaptive feature modulation (SAFM) mechanism for efficient SR design. In detail, the SAFM layer uses inde-pendent computations to learn multi-scale feature represen-tations and aggregates these features for dynamic spatial modulation. As the SAFM prioritizes exploiting non-local feature dependencies, we further introduce a convolutional channel mixer (CCM) to encode local contextual informa-tion and mix channels simultaneously. Extensive experi-mental results show that the proposed method is 3× smaller than state-of-the-art efficient SR methods, e.g., IMDN, and yields comparable performance with much less memory us-age. Our source codes and pre-trained models are available at: https://github.com/sunny2109/SAFMN . 1.

Introduction
Single image super-resolution (SISR) aims to restore a high-resolution (HR) image from its low-resolution (LR) counterpart by recovering lost details. This longstanding and challenging task has recently attracted much attention due to the rapid development of streaming media or high-definition devices. As these scenarios are usually resource-constrained, it is of great interest to develop an efficient and effective SR method to estimate HR images for better visual display on these platforms or products.
Deep learning-based SR methods have achieved signifi-cant performance improvements with the great evolution of hardware technologies, as we can use large amounts of data to train much larger or deeper neural networks for image
SR [32, 53, 31, 5]. For example, RCAN [53] is a repre-sentative CNN-based image SR network with 15.59M pa-rameters and reaching a depth of over 400 layers. One of the most significant drawbacks of these large models is that they require high computational costs, which makes them
*Corresponding author
Figure 1. Model complexity and performance comparison be-tween our proposed SAFMN model and other lightweight methods on Set5 [4] for ×2 SR. Circle sizes indicate the number of param-eters. The proposed method achieves a better trade-off between model complexity and reconstruction performance. challenging to deploy. Moreover, recent visual transform-ers (ViTs) [12, 31, 5] outperform convolutional neural net-works (CNNs) in low-level vision tasks, and their results demonstrate that exploring non-local feature interactions is essential for high-quality reconstruction. But existing self-attention mechanisms are computationally expensive and unfriendly to efficient SR design. This, therefore, moti-vates us to develop a lightweight yet effective model for real-world applications of image super-resolution by inte-grating the principles of convolution and self-attention.
To reduce the heavy computational burden, vari-ous methods, including efficient module design [11, 41, 1, 20, 26, 33, 44, 45, 54, 30], knowledge distilla-tion [16], neural architecture search [7], and structural re-parameterization [52], are trying to improve the effi-ciency of SR algorithms. Among these efficient SR mod-els, one direction is to reduce model parameters or com-plexity (FLOPs). Lightweight strategies like recursive man-ner [23, 46], parameter sharing [1], and spare convolu-tions [1, 45, 30] are adopted. Although these approaches certainly reduce the model size, they usually compensate for the performance drop caused by shared recursive mod-ules or sparse convolutions by increasing the depth or width
of the model, which affects the inference efficiency when performing SR reconstruction.
Another direction is to accelerate the inference time. The post-upsampling [11, 44] is an important replacement for the pre-defined input [10, 24], which significantly speeds up the runtime. Model quantization [21] effectively ac-celerates latency and reduces energy consumption, particu-larly when deploying algorithms in edge-devices. Structural re-parameterization [52, 8] improves the speed of a well-trained model in the inference stage. These methods en-joy fast running time but poor reconstruction performance.
Consequently, there is still room for a better trade-off be-tween model efficiency and reconstruction performance.
To address the above-mentioned issues, we design a sim-ple yet effective model by developing a spatially-adaptive feature modulation, namely SAFMN, to realize a favorable trade-off between performance and efficiency. Specifically, we first exploit non-local feature relations to dynamically select representative features (see Figure 2 and 5) by imple-menting a feature modulation mechanism based on multi-scale representations. As the modulation mechanism pro-cesses input features from a non-local perspective, there is a requirement to complement local contextual information.
To this end, we present a convolutional channel mixer based on the FMBConv [47] to encode local features and mix channels. Taken together, we find that the SAFMN network is able to achieve a better trade-off between SR performance and model complexity, as shown in Figure 1.
The main contributions of this paper are summarized as follows:
• We develop an efficient feature modulation mechanism to learn feature dependencies, which absorbs CNN-like efficiency and transformer-like adaptability.
• We present a compact convolutional channel mixer that simultaneously encodes local contextual informa-tion and performs channel mixing.
• We evaluate the proposed method quantitatively and qualitatively on benchmark datasets, and the results show that our SAFMN achieves a favorable trade-off between accuracy and model complexity. 2.