Abstract
In this paper we present an approach for localizing steps of procedural activities in narrated how-to videos. To deal with the scarcity of labeled data at scale, we source the step descriptions from a language knowledge base (wiki-How) containing instructional articles for a large variety of procedural tasks. Without any form of manual supervision, our model learns to temporally ground the steps of proce-dural articles in how-to videos by matching three modali-ties: frames, narrations, and step descriptions. Specifically, our method aligns steps to video by fusing information from two distinct pathways: i) direct alignment of step descrip-tions to frames, ii) indirect alignment obtained by compos-ing steps-to-narrations with narrations-to-video correspon-dences. Notably, our approach performs global temporal grounding of all steps in an article at once by exploiting order information, and is trained with step pseudo-labels
In which are iteratively refined and aggressively filtered. order to validate our model we introduce a new bench-mark – HT-Step – obtained by manually annotating a 124-hour subset of HowTo100M1 with steps sourced from wik-iHow articles. Experiments on this benchmark as well as zero-shot evaluations on CrossTask demonstrate that our multi-modality alignment yields dramatic gains over sev-eral baselines and prior works. Finally, we show that our inner module for matching narration-to-video outperforms by a large margin the state of the art on the HTM-Align narration-video alignment benchmark. 1.

Introduction
Instructional videos have emerged as a popular means for people to learn new skills and improve their abilities in executing complex procedural activities, such as cooking a recipe, performing home improvements, or fixing things.
In addition to being useful teaching materials for humans, how-to videos are a promising medium for learning by ma-*Equal contribution 1A test server is challenges/challenge-page/2082. accessible at https://eval.ai/web/
Figure 1: Our proposed Video, Instructions, and Narrations
Aligner (VINA) learns to simultaneously ground narrations and instruction steps in how-to videos from an uncurated set of narrated videos and a separate knowledge base of in-structional articles, without any manual annotations. This is contrary to prior work that learns how to align a video with a single sequence of sentences by leveraging ground-truth pairs of video-text sequences, e.g., a video and its narra-tions [22], or a video and an annotated, ordered list of steps demonstrated in it [16]. chines, as they provide revealing visual demonstrations of complex activities and show elaborate human-object inter-actions in a variety of domains. Motivated by this observa-tion, in this work we look at the task of temporally localiz-ing the steps of procedural activities in instructional videos.
This problem is foundational to the broader goal of human-procedure understanding and advances on this task promise to enable breakthrough applications, such as AI-powered skill coaching and human-to-robot imitation learning.
Prior work has tackled procedural step localization by leveraging either (a) fully-annotated datasets where the task
shown in the video is given (video-level labeling) and manu-ally annotated temporal segments are provided for each step (segment-level labeling) [59] or (b) weakly-annotated train-ing sets where the task and the order in which the steps ap-pear in the video is given [79]. However, due to the inherent manual cost involved in collecting step annotations, these works have relied on datasets that are small-scale both in the number of tasks (e.g., at most few hundreds [79]) and in the number of video samples (e.g., 12k videos [59]). These limitations affect both the generality and the complexity of the models that can be trained on these benchmarks.
In this paper, we therefore pose the following question: can we leverage large-scale, unlabeled video datasets to train a model that can ground procedural steps in how-to videos?
To answer this question, we propose a novel training framework for weakly-supervised step grounding that uti-lizes two freely available sources of information: (a) in-structional articles which define ordered lists of steps for a wide variety of tasks (e.g., from wikiHow) and (b) nar-rations which provide instance-specific rich commentaries of the execution of the task in the video, e.g., from ASR transcriptions. Our work treats the former as an abstraction of the latter and uses the video-specific narrations to support the grounding of the article steps. Specifically, during train-ing, our method leverages narrations as an auxiliary signal to (i) identify the task shown in the video, (ii) temporally ground the article steps that are visually-demonstrated and (iii) filter out steps that are not executed in the given in-stance. To further motivate this mechanism, let us look at the example in Figure 1. The narrations help disambiguate the task (make a pumpkin puree), enabling the automatic retrieval of relevant instructional articles for the video. Fur-thermore, the narrations can be matched to steps described in the articles to roughly localize the steps that are repre-sented in the video. In this example, the timestamp of “First thing you’ll need to do is cut off the stem” provides a loose temporal prior for the matching step “Cut the pumpkins.”
On the other hand, steps that do not have any matching nar-rations (e.g., “Wash the pumpkins”) are unlikely to be rep-resented in the video and thus can be rejected. Based on this intuition, we propose a procedure that learns to align steps to video by fusing information from two pathways. The first is an indirect pathway inferring step-frame alignments by composing step-to-narration assignments with narration-to-frame correspondences. The second is a direct pathway that learns associations between step descriptions and frames by leveraging information from all videos having steps in com-mon.
In our experiments we demonstrate that our multi-modality alignment leads to significant performance gains over several baselines, including single-pathway temporal grounding, as well adaptations of prior works to our prob-lem. During inference, the direct pathway can be used by itself to temporally ground steps in absence of transcribed narrations. When narrations are available at test time, our method improves further the accuracy of temporal ground-ing by fusing the inference outputs of the two pathways.
To summarize, our work makes the following contribu-tions: 1) we learn to align steps to frames in how-to videos, using only weak supervision in the form of noisy ASR nar-rations and instructional articles; 2) we propose a novel ap-proach for joint dense temporal grounding of instructional steps and video narrations; 3) we introduce a new bench-mark for evaluating instructional step grounding which we will make available to the community; 4) we demonstrate state-of the art results on multiple benchmarks for both step as well as narration grounding. 2.