Abstract
Deep Image Prior (DIP) shows that some network ar-chitectures inherently tend towards generating smooth im-ages while resisting noise, a phenomenon known as spec-tral bias. Image denoising is a natural application of this property. Although denoising with DIP mitigates the need for large training sets, two often intertwined practical chal-lenges need to be overcome: architectural design and noise fitting. Existing methods either handcraft or search for suit-able architectures from a vast design space, due to the lim-ited understanding of how architectural choices affect the denoising outcome. In this study, we demonstrate from a frequency perspective that unlearnt upsampling is the main driving force behind the denoising phenomenon with DIP.
This finding leads to straightforward strategies for identify-ing a suitable architecture for every image without labori-ous search. Extensive experiments show that the estimated architectures achieve superior denoising results than exist-ing methods with up to 95% fewer parameters. Thanks to this under-parameterization, the resulting architectures are less prone to noise-fitting1. 1.

Introduction
Image denoising is useful on its own and can be a plug-in module for many other image restoration tasks [37, 38, 4].
Deep neural networks have become the tool of choice for image denoising owing to their ability to learn natural im-age priors from large-scale datasets. Yet, Deep Image Prior (DIP) [33] requires only a single degraded image for im-age resotration. Remarkably, DIP shows that a randomly initialized convolutional neural network (CNN) can regu-larize image restoration through its architecture and early-stopping optimization. This is inspired by the phenomenon that some network architectures act inherently as image pri-ors, favoring generating smooth, natural images and re-* These authors contribute equally. 1https://github.com/YilinLiu97/FasterDIP-devil-in-upsampling.git
Figure 1: Top: Performance (SSIM↑) under different levels of
Gaussian noises. Bottom: Denoising of a fine-grained (1strow) and a coarse-grained image (2ndrow). Most existing methods, including the recent image-specific ISNAS-DIP[1], struggle to perform well simultaneously in both cases. Our simple strate-gies are flexible in image-specific architectural adaptation with-out requiring a search. Moreover, the results of the lightweight
ConvDecoder[10] and Deep Decoder[15] suggest that without proper model setups, under-parameterization itself can neither en-sure good denoising performance nor remove the need for early-stopping. sisting noises or degradations. However, image denois-ing with DIP is greatly influenced by architectural design
[3, 1, 33, 15, 17], and the associated tendency to fit the orig-inal noisy image, i.e., over-fitting [15, 16].
Architectural design for DIP remains an open problem.
One prevailing view is that model under-parameterization limits noise over-fitting and thus mitigates the need for early stopping [15]. However, our experiments reveal that multi-ple model architectures can exist under a similar parameter budget, where inappropriate model setups can still lead to
noise fitting and over-smoothing (Fig.1, Fig.3). Another line of work automates the architecture identification us-ing Neural Architecture Search (NAS)[6, 17, 1]. Without prior knowledge on suitable architectures, extensive search incurs substantial computational costs, prohibiting image-wise NAS for optimal restoration [1]. Arican et al.
[1] narrow the search space using training-free metrics, but the need for candidate comparison dramatically prolongs the restoration time (∼ 7 hours/image). Moreover, NAS-based models, along with many DIP models, are typically heavily parameterized and prone to over-fitting, as corroborated in our experiments. Thus, their performance largely depends on the timing of early stopping, which is typically image-specific and hard to pinpoint without access to ground truth.
Directly identifying an effective under-parameterized ar-chitecture for each image is challenging in light of the vast number of different architectures and the absence of ground truth for explicit supervision. To simplify the architectural decisions for DIP, we rethink the architectural influences on its performance in the context of image denoising.
We start by noting that denoising performance is at-tributable to only a few componenets, primarily the unlearnt upsampling operations. Our frequency analysis reveals that the fixed upsampling operations tend to bias the architecture towards low-frequency contents more strongly than linear or convolutional layers, critically influencing both the peak
PSNR and the point of early stopping to avoid noise-fitting.
Importantly, this finding leads to empirical discovery on the roles of typical architectural components in DIP: as-suming a standard hourglass network, i) simply scaling the depth and width can balance smoothing and preservation of details, due to the low-pass filtering effects of the up-sampling operations inserted in-between the layers. As we observed, a wider and shallower network is better at pre-serving details and therefore suitable for fine-grained im-ages. This suggests that the "optimal" architecture can vary across images, and image texture should be considered for more effective denoising. ii) Skip connections make a deep network perform similarly as a shallower one likely by re-ducing the "effective upsampling rate". This implies the possibility of discarding skip connections to simplify DIP architectural design.
Based on these insights, we find it sufficient to restrict the design choices to only the network depth and width, reducing the problem to searching through only a handful of sub-networks and adapting them according to the level of image details. This can be done as a pre-processing step without costly searching or evaluation. We show that this simple strategy works with both the hourglass and de-coder structures, and with proper setups, the estimated net-works can denoise while preserving the details better than the larger networks with only 5%∼40% number of param-eters. The resulting under-parameterization alleviates the need for early stopping. Our contributions are as follows:
• We pinpoint that unlearnt upsampling is the main driv-ing force behind the spectral bias of DIP.
• Leveraging this insight, we empirically identify the in-fluences of depth, width and skip connections, along with their correlations with image texture, allowing for quick, effective and more interpretable architectural design for every image without the laborious search.
• We are the first to associate DIP architectural design with image texture. To promote future research, we build a Texture-DIP Dataset consisting of images from three popular datasets, reclassified into several prede-fined width choices – validated through experiments, according to textural complexity.
• We show that with proper setups, a highly under-parameterized subnetwork could match and even out-perform larger counterparts, especially at a higher noise level. We conducted extensive synthetic and real-world noise removal experiments to validate our findings and approach. 2.