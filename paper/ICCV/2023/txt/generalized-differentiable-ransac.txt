Abstract
We propose ∇-RANSAC, a generalized differentiable
RANSAC that allows learning the entire randomized ro-bust estimation pipeline. The proposed approach enables the use of relaxation techniques for estimating the gradi-ents in the sampling distribution, which are then propa-gated through a differentiable solver. The trainable quality function marginalizes over the scores from all the models estimated within ∇-RANSAC to guide the network learn-ing accurate and useful inlier probabilities or to train fea-ture detection and matching networks. Our method directly maximizes the probability of drawing a good hypothesis, al-lowing us to learn better sampling distributions. We test
∇-RANSAC on various real-world scenarios on fundamen-tal and essential matrix estimation, and 3D point cloud registration, outdoors and indoors, with handcrafted and learning-based features. It is superior to the state-of-the-art in terms of accuracy while running at a similar speed to its less accurate alternatives. The code and trained models are available at https://github.com/weitong8591/ differentiable_ransac. 1.

Introduction
Robust estimation is a fundamental component in vi-sion pipelines, including relative pose estimation [18], wide baseline matching [54, 47, 48], multi-model fitting [34, 53], image-based localization [9], motion segmentation [70], and pose graph initialization of Structure-from-Motion (SfM) algorithms [61, 63]. While several robust estima-tors have been proposed throughout the years [30, 32, 41, 75], randomized hypothesize-and-verify approaches, like
RANSAC [24] and its recent variants [4, 6, 5, 35], have become the most widely used methods due to their robust-ness, simplicity, and efficiency. RANSAC repeatedly se-lects random minimal subsets of the input data sufficient to fit a model hypothesis, e.g., a 3D plane to three points or a fundamental matrix to seven point correspondences. The model score is then computed as the cardinality of the inlier set, formed by the points consistent with the model hypoth-esis, i.e., having residuals smaller than a threshold. The so-far-the-best model is updated if a new model is found with higher quality. RANSAC terminates when the probability of finding a better hypothesis falls below a threshold. Fi-nally, the model with the highest quality, polished, e.g., by least-squares fitting on all inliers, is returned.
Numerous improvements have been made to the original
RANSAC algorithm, including refinement of hypotheses through local optimization [17, 4], better scoring [71, 6, 5], detection of degenerate cases [19], and speed-ups through techniques such as weighted random sampling of hypothe-ses [15] or preemptive hypothesis verification strategies [14, 16]. See [72] for a recent survey and benchmark. To
[72], the most accurate method for relative pose estima-tion is MAGSAC++ [5] with PROSAC sampling [15] and
DEGENSAC-based degeneracy [19] testing.
In recent years, neural networks (NNs) have been em-ployed to estimate tentative matches, including their coor-dinates and confidences [76, 67, 78, 79, 21, 66]. These pre-dicted confidences could be used for pre-filtering matches or weighted random sampling in RANSAC. However, learning these NNs endowed with RANSAC, particularly for optimizing the desired evaluation metric, such as pose error, remains a challenging problem. One of the main chal-lenges is that minimal solvers are often complex and not readily differentiable. Additionally, learning the sampling distribution for optimal RANSAC performance is challeng-ing, both in terms of formalizing the problem and estimating gradients for the sampling probabilities. Prior work in this direction [9, 10] will be discussed in detail (Section 2).
In this paper, we make contributions to the learnable robust estimation family and propose a new differentiable
RANSAC, ∇-RANSAC, with all the components differen-tiable. The main contributions are as follows:
• We investigate all the components of the RANSAC pipeline and propose a new differentiable alternative that allows learning inlier probabilities while directly optimizing test-time evaluation metrics, e.g., pose er-ror, as a new learning objective.
Figure 1. Pipeline of training ∇-RANSAC (forward and backward). Given an image pair, we can either use a hand-crafted feature matcher and feed the tentative correspondences into the consensus learning network from [79], or use learning-based matching method that outputs matches and their confidence. The predictions input to the ∇-RANSAC module for robust estimation. In each iteration, the differentiable and randomized Gumbel sampler (Section 3.1) selects a minimal sample of m correspondences. Model ˆθ is estimated by differentiable solvers (Section 3.2) and its loss is calculated based on trainable quality functions (Section 3.3) with the ground truth.
• We propose a new random sampling approach based on a re-parametrization strategy, i.e. Gumbel Softmax sampler, that allows gradient propagation through the entire randomized procedure.
• To demonstrate its potential to unlock the end-to-end training of geometric pipelines, ∇-RANSAC is incor-porated into an end-to-end feature matcher, LoFTR
[66], to improve the predicted matches and confidence.
• Technically, we implement and include a differentiable version of the widely used minimal solvers, e.g., five and seven-point algorithms [50], and standard Kabsch algorithm [38] for rigid transformation during training.
∇-RANSAC has significant implications for learning-based vision systems, enabling training such pipelines that were previously difficult or impossible to train. 2.