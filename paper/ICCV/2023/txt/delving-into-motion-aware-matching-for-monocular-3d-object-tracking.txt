Abstract
Recent advances of monocular 3D object detection fa-cilitate the 3D multi-object tracking task based on low-cost camera sensors.
In this paper, we find that the mo-tion cue of objects along different time frames is critical in 3D multi-object tracking, which is less explored in ex-isting monocular-based approaches. To this end, we pro-pose MoMA-M3T, a framework that mainly consists of three motion-aware components. First, we represent the possi-ble movement of an object related to all object tracklets in the feature space as its motion features. Then, we further model the historical object tracklet along the time frame in a spatial-temporal perspective via a motion transformer. Fi-nally, we propose a motion-aware matching module to as-sociate historical object tracklets and current observations as final tracking results. We conduct extensive experiments on the nuScenes and KITTI datasets to demonstrate that our MoMA-M3T achieves competitive performance against state-of-the-art methods. Moreover, the proposed tracker is flexible and can be easily plugged into existing image-based 3D object detectors without re-training. Code and models are available at https:// github.com/ kuanchihhuang/
MoMA-M3T. 1.

Introduction 3D Multi-Object Tracking (3D MOT) is a crucial prob-lem for various applications like autonomous driving. Nu-merous LiDAR-based methods [49, 50] have achieved re-markable results thanks to powerful 3D object detectors
[13, 23, 38, 39]. Due to the lower cost of camera sen-sors, some image-based 3D object detection approaches
[4, 16, 20, 26, 32, 35] receive much attention and achieve promising performance, and thus enable 3D object tracking based on merely the camera.
One straightforward approach to deal with monocular 3D object tracking is to match object features in adjacent frames [24, 61] (see Figure 1(a)). Although significant progress has been made, these methods may still fail to cap-ture multi-frame motion information of objects. To tackle the long-range dependency, another line of work [6, 15]
Figure 1. Comparisons of different association methods in monocular 3D object tracking. (a) Time3D [24] learns to match 3D object features in adjacent frames. (b) QD-3DT [15] and DEFT [6] utilize the object’s previous features to predict their current states, and match with the observations in the output space. (c) Our approach directly aggregates the object’s previous features and matches them with current observations in the feature space. predicts the object states from the historical observations, in which the predicted and observed states in the current frame are in the output space that explicitly contains the ob-ject information, e.g., location and pose of the object (see
Figure 1(b)). However, these approaches may suffer from noisy observations of object states predicted by the inaccu-rate monocular 3D object detector.
For the above-mentioned methods, one critical step is data association, in which the goal is to match observations across historical time frames and produce final tracking re-sults. Therefore, in monocular 3D MOT, two main chal-lenges are 1) how to obtain the long-range observations that can provide richer information for data association? 2) what
are the better representations the algorithm utilizes as obser-vations, in order to mitigate the problem of matching under noisy observations from the inaccurate monocular 3D de-tector? In this paper, we propose MoMA-M3T, a motion-aware matching approach for monocular 3D MOT, to han-dle these two challenges (see Figure 1(c)). Our main idea is to encode the multi-frame motion information of the ob-ject tracklets, i.e., their historically relative positions, into a feature space for data association, instead of encoding their absolute locations in the output space. To this end, the ob-ject movements encoded in the learned representations can be used for matching between tracklets and current object observations.
Specifically, MoMA-M3T consists of three main com-ponents: 1) we first use a motion encoder to encode the 3D object information, e.g., relative position and object size/heading angle, into a motion-aware feature space; 2)
Then, these encoded features also form a motion feature bank to record historical features, followed by a motion transformer module to generate spatial-temporal motion features as representations of object tracklets; 3) Finally, a motion-aware matching module to generate tracking results is introduced for data association between object observa-tions and tracklets based on motion features. Moreover, our method that considers motion features enables the feasibil-ity of applying learning strategies. We adopt a contrastive learning objective that samples several subsets of different object trajectories and learns better feature representations, e.g., data points from the same trajectory but in augmented views are positive samples.
Extensive experiments on nuScenes [5] and KITTI [12] datasets demonstrate that our method achieves state-of-the-art performance based on monocular camera sensors. In ad-dition, we show the benefit of our proposed components, in-cluding the usage of motion features, motion transformers, and motion-aware matching. More interestingly, we present the robustness of MoMA-M3T by plugging our learned modules with frozen weights into the same framework, but based on detection outputs from different 3D object detec-tors. Results show that our motion-aware approach gener-alizes well to various pre-trained detectors.
The main contributions of this work are as follows:
• We present MoMA-M3T, a framework that introduces motion features with a motion-aware matching mech-anism for monocular 3D MOT.
• We propose a motion transformer module that captures the movement of object tracklets in a spatial-temporal perspective, enabling robust motion feature learning.
• Extensive experiments on nuScenes and KITTI datasets show that our method achieves competitive performance based on monocular sensors, with the flexibility to apply various pre-trained 3D detectors. 2.