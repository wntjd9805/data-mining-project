Abstract
Existing text-to-image diffusion models struggle to syn-thesize realistic images given dense captions, where each text prompt provides a detailed description for a specific image region. To address this, we propose DenseDiffusion, a training-free method that adapts a pre-trained text-to-image model to handle such dense captions while offering con-trol over the scene layout. We first analyze the relation-ship between generated images’ layouts and the pre-trained model’s intermediate attention maps. Next, we develop an attention modulation method that guides objects to appear in specific regions according to layout guidance. Without requiring additional fine-tuning or datasets, we improve image generation performance given dense captions regard-ing both automatic and human evaluation scores. In addi-tion, we achieve similar-quality visual results with models specifically trained with layout conditions. Code and data are available at https://github.com/naver-ai/
DenseDiffusion. 1
1.

Introduction
Recently developed text-to-image models can synthe-size diverse, high-quality images from short scene descrip-tions [24, 15, 38, 44, 46, 45, 29, 58, 7]. However, as shown in Figure 1, they often encounter challenges when handling dense captions and tend to omit or blend the visual features of different objects. Here the term “dense” is inspired by the work of dense captioning [27] in which each phrase is used to describe a specific image region. Moreover, it is difficult for users to precisely control the scene layout of a generated image with text prompts alone.
Several recent works propose providing users with spatial control by training or fine-tuning layout-conditioned text-to-image models [2, 54, 59, 33, 45, 19, 56, 37]. While Make-a-Scene [19] and Latent Diffusion Models [45] train models from scratch given both text and layout conditions, other recent and concurrent works, such as SpaText [2] and Con-trolNet [59], introduce additional spatial controls to existing text-to-image models with model fine-tuning. Unfortunately, training or fine-tuning a model can be computationally-expensive. Moreover, the model needs to be retrained for every new user condition, every new domain, and every new text-to-image base model.
To address the above issue, we propose a training-free method that supports dense captions and offers layout con-trol. We first analyze the intermediate features of a pre-trained text-to-image diffusion model to show that the layout of a generated image is significantly related to self-attention and cross-attention maps. Based on this observation, we modulate intermediate attention maps according to the lay-out condition on the fly. We further propose considering the value range of original attention scores and adjusting the degree of modulation according to the area of each segment.
Our experiments show that our method improves the per-formance of Stable Diffusion [45] and outperforms several compositional diffusion models [35, 6, 17] on dense cap-tions regarding both text conditions, layout conditions, and image quality. We verify them using both automatic met-In addition, our method is on par rics and user studies. with existing layout-conditioned models regarding qualita-tive results. Finally, we include a detailed ablation study regarding our design choices and discuss several failure cases. Our code, models, and data are available at https:
//github.com/naver-ai/DenseDiffusion. 2.