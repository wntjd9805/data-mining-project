Abstract
Given its wide applications, there is increasing focus on generating 3D human motions from textual descriptions.
Differing from the majority of previous works, which regard actions as single entities and can only generate short se-quences for simple motions, we propose EMS, an elabora-tive motion synthesis model conditioned on detailed natu-ral language descriptions. It generates natural and smooth motion sequences for long and complicated actions by fac-torizing them into groups of atomic actions. Meanwhile, it understands atomic-action level attributes (e.g., motion direction, speed, and body parts) and enables users to gen-erate sequences of unseen complex actions from unique se-quences of known atomic actions with independent attribute settings and timings applied. We evaluate our method on the KIT Motion-Language and BABEL benchmarks, where it outperforms all previous state-of-the-art with noticeable margins. 1.

Introduction
Generating 3D human motions has been widely used in many areas, such as in film and game industries to animate characters in the scene or in sports and medical applica-tions to analyze and explain the movement of players and patients, respectively. Many of these tasks have resorted to motion data, which require laborious manual work by animators or high recording costs. Furthermore, existing tools often are not user-friendly and require extensive train-ing in advance of using the tools. Recently, attention to generating 3D human motions conditioned on natural lan-guage descriptions (i.e., text-to-motion) has grown in the vision and graphics community as an alternative method that can provide high-quality and flexible results from one of the most intuitive and comfortable interfaces for people, which is a textual description. The task, however, is chal-lenging, given the diversity of language descriptions and
*Corresponding Author
Figure 1. The majority of previous works are conditioned on the ambiguous action-level description, which prevents them from generating motion sequences with detailed controls (e.g., they can’t explicitly generate motions of lifting things up in place or at another location). On the other hand, our model conditioned on the atomic action-level description can easily generate motion se-quences with detailed controls. complexity of 3D human motion synthesis. Many recent works have attempted to make progress in this space. The earliest works [13, 28] focus on generating motions con-ditioned on a single action label (i.e., a word); however, action labels contain limited semantic representations and can’t guide detailed motion generation as a result. The fol-lowing works [23, 10, 2] naturally go further by encoding longer natural language descriptions as text input. The most recent works [17, 29, 39, 6] either focus on improving the generalization of extracted semantic representations or im-proving the generation quality of the motion synthesis mod-ule. To the best of our knowledge, all of them try to gener-ate a single latent vector to describe the whole textual input then forward it to a decoder for motion synthesis.
If re-garding generated motions as a ”visualized” language, pre-vious works are similar to seq-to-seq methods designed for
It works well when the sentence-to-sentence translation. natural language input contains limited semantic informa-tion which can be fully covered by a single sentence. How-ever, such methods would have difficulties to model all the semantic representations and inherited dependencies when the input information increases to a paragraph or article level. Furthermore, the GPU memory size will limit the maximum length of the input sequence, making the meth-ods unable to tackle the full paragraph or article as a single entity. A natural idea may come up as directly factorizing the input paragraph into sentences and then applying a seq-to-seq method autoregressively to translate the factorized sentences one by one. However, given the successful ex-perience in [8, 46], such simple concatenation won’t work well because it doesn’t capture the contextual information thoroughly where the same sentence may have different se-mantic meanings depending on its contexts. This also ap-plies to text-conditioned motion synthesis. For example, the same ”stand up” action could be visually different when its previous action is either ”sit down” or ”squat down”.
Inspired by this, we propose EMS, an Elaborative
Motion Synthesis model conditioned on detailed natural language descriptions. EMS aims to generate natural and smooth 3D motion sequences of complicated actions by fac-torizing them into components of atomic actions. As is shown in Figure 2, EMS is a two-stage generation model where a sequence of atomic actions (i.e., unique behaviors) are first generated by the atomic action generation subnet
Navae then they are refined and connected by the connec-tion sub-net Ncvae.
In our implementation, both of the sub-nets are transformer-based variational auto-encoders (VAE). However, our framework does not rely on spe-cific ”sentence-level” translation backbones, so we can also use the recently released diffusion module [39] or graph
GAN[1] as our backbone. Although our model is espe-cially designed for generating complicated long motion se-quences, when compared with previous state-of-the-art on
KIT-ML benchmark, our model still outperforms in all ex-isting metrics with noticeable margins. When it comes to the more challenging BABEL benchmark, our method is on average 36 percent better than the previous SOTA.
In a nutshell, our contributions are four-fold:
• We propose EMS, a two-stage text-conditioned elabo-rative motion synthesis model which generates smooth and natural 3D motion sequences for complicated actions given detailed natural language descriptions.
Users can generate sequences of unseen complex ac-tions from unique sequences of known atomic actions with independent attribute settings and timings ap-plied.
• We propose Natural Loss for the text-to-motion task, an objective function especially designed to judge if the generated motions look natural by calculating the difference between prior and posterior distributions.
• We built a synthetic (i.e., augmented) dataset contain-ing contrastive atomic action samples, which enable the model to understand atomic-action level attributes.
• We demonstrate that our model outperforms all previ-ous SOTA in various metrics with noticeable margins on the two existing benchmarks. 2.