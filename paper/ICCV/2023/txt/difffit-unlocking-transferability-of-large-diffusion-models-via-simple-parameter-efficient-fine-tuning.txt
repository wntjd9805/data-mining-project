Abstract
Diffusion models have proven to be highly effective in generating high-quality images. However, adapting large pre-trained diffusion models to new domains remains an open challenge, which is critical for real-world applica-tions. This paper proposes DiffFit, a parameter-efﬁcient strategy to ﬁne-tune large pre-trained diffusion models that enable fast adaptation to new domains. DiffFit is em-barrassingly simple that only ﬁne-tunes the bias term and newly-added scaling factors in speciﬁc layers, yet result-ing in signiﬁcant training speed-up and reduced model stor-age costs. Compared with full ﬁne-tuning, DiffFit achieves
Correspondence to {xie.enze, li.zhenguo}@huawei.com 2× training speed-up and only needs to store approximately 0.12% of the total model parameters. Intuitive theoretical analysis has been provided to justify the efﬁcacy of scal-ing factors on fast adaptation. On 8 downstream datasets,
DiffFit achieves superior or competitive performances com-pared to the full ﬁne-tuning while being more efﬁcient. Re-markably, we show that DiffFit can adapt a pre-trained low-resolution generative model to a high-resolution one by adding minimal cost. Among diffusion-based methods,
DiffFit sets a new state-of-the-art FID of 3.02 on ImageNet 512×512 benchmark by ﬁne-tuning only 25 epochs from a public pre-trained ImageNet 256×256 checkpoint while be-ing 30× more training efﬁcient than the closest competitor. 1
1.

Introduction
Denoising diffusion probabilistic models (DDPMs) [20, 51, 49] have recently emerged as a formidable technique for generative modeling and have demonstrated impres-sive results in image synthesis [43, 12, 45], video genera-tion [21, 19, 65] and 3D editing [40]. However, the cur-rent state-of-the-art DDPMs suffer from signiﬁcant com-putational expenses due to their large parameter sizes and numerous inference steps per image. For example, the re-cent DALL· E 2 [45] comprises 4 separate diffusion models and requires 5.5B parameters. In practice, not all users are able to afford the necessary computational and storage re-sources. As such, there is a pressing need to explore meth-ods for adapting publicly available, large, pre-trained dif-fusion models to suit speciﬁc tasks effectively. In light of this, a central challenge arises: Can we devise an inexpen-sive method to ﬁne-tune large pre-trained diffusion models efﬁciently?
Take the recent popular Diffusion Transformer (DiT) as an example, the DiT-XL/2 model, which is the largest model in the DiT family and yields state-of-the-art gener-ative performance on the ImageNet class-conditional gen-eration benchmark. In detail, DiT-XL/2 comprises 640M parameters and involves computationally demanding train-ing procedures. Our estimation indicates that the train-ing process for DiT-XL/2 on 256×256 images necessitates 950 V100 GPU days (7M iterations), whereas the training on 512×512 images requires 1733 V100 GPU days (3M iterations). The high computational cost makes training
DiT from scratch unaffordable for most users. Further-more, extensive ﬁne-tuning of the DiT on diverse down-stream datasets requires storing multiple copies of the whole model, which results in linear storage expenditures.
In this paper, we propose DiffFit, a simple and parameter-efﬁcient ﬁne-tuning strategy for large diffusion models, building on the DiT as the base model. The mo-tivation can be found in Figure 2. Recent work in natu-ral language processing (BitFit [61]) has demonstrated that
ﬁne-tuning only the bias term in a pre-trained model per-forms sufﬁciently well on downstream tasks. We, there-fore, seek to extend these efﬁcient ﬁne-tuning techniques to image generative tasks. We start with directly applying
BitFit [61] and empirically observe that simply using the
BitFit technique is a good baseline for adaptation. We then introduce learnable scaling factors γ to speciﬁc layers of the model, initialized to 1.0, and made dataset-speciﬁc to accommodate enhancement of feature scaling and results in better adaptation to new domains. Interestingly, the empir-ical ﬁndings show that incorporating γ at speciﬁc locations of the model is important to reaching a better FID score.
Source Domain Denoising Process
Source 
Domain 
Target 
Domain 
Target Domain Denoising Process
Figure 2: The denoising process of a diffusion model typ-ically involves iteratively generating images from random noise. In DiffFit, the pre-trained large diffusion model in the source domain can be ﬁne-tuned to adapt to a target do-main with only a few speciﬁc parameter adjustments.
In other words, the FID score does not improve linearly with the number of γ included in the model. In addition, we conducted a theoretical analysis of the mechanism un-derlying the proposed DiffFit for ﬁne-tuning large diffusion models. We provided intuitive theoretical analysis to help understand the effect of the newly-added scaling factors in the shift of distributions. including BitFit
[61], AdaptFormer
We employed several parameter-efﬁcient ﬁne-tuning techniques,
[7],
LoRA [23], and VPT [24], and evaluated their performance on 8 downstream datasets. Our results demonstrate that
DiffFit outperforms these methods regarding Frechet Incep-tion Distance (FID) [38] trade-off and the number of train-able parameters. Furthermore, we surprisingly discovered that by treating high-resolution images as a special domain from low-resolution ones, our DiffFit approach could be seamlessly applied to ﬁne-tune a low-resolution diffusion model, enabling it to adapt to high-resolution image gener-ation at a minimal cost. For example, starting from a pre-trained ImageNet 256×256 checkpoint, by ﬁne-tuning DIT for only 25 epochs (≈0.1M iterations), DiffFit surpassed the previous state-of-the-art diffusion models on the Ima-geNet 512×512 setting. Even though DiffFit has only about 0.9 million trainable parameters, it outperforms the original
DiT-XL/2-512 model (which has 640M trainable parame-ters and 3M iterations) in terms of FID (3.02 vs. 3.04), while reducing 30× training time. In conclusion, DiffFit aims to establish a simple and strong baseline for parameter-efﬁcient ﬁne-tuning in image generation and shed light on the efﬁcient ﬁne-tuning of larger diffusion models.
Our contributions can be summarized as follows: 1. We propose a simple parameter-efﬁcient ﬁne-tuning approach for diffusion image generation named Diff-Fit. It achieves superior results compared to full ﬁne-tuning while leveraging only 0.12% trainable param-eters. Quantitative evaluations across 8 downstream datasets demonstrate that DiffFit outperforms existing well-designed ﬁne-tuning strategies (as shown in Fig-ure 3 and Table 1). 2. We conduct an intuitive theoretical analysis and design detailed ablation studies to provide a deeper under-standing of why this simple parameter-efﬁcient ﬁne-tuning strategy can fast adapt to new distributions. 3. We show that by treating high-resolution image gener-ation as a downstream task of the low-resolution pre-trained generative model, DiffFit can be seamlessly ex-tended to achieve superior generation results with FID 3.02 on ImageNet and reducing training time by 30 times, thereby demonstrating its scalability. 2.