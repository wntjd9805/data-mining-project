Abstract
Image manipulation techniques have drawn growing concerns as manipulated images might cause morality and security problems. Various methods have been proposed to detect manipulations and achieved promising performance.
However, these methods might be vulnerable to adversarial attacks. In this work, we design an Adversarial Manipula-tion Generation (AMG) task to explore the vulnerability of image manipulation detectors. We first propose an optimal loss function and extend existing attacks to generate adver-sarial examples. We observe that existing spatial attacks cause large degradation in image quality and find the loss of high-frequency detailed components might be its major reason. Inspired by this observation, we propose a novel adversarial attack that incorporates both spatial and fre-quency features into the GAN architecture to generate ad-versarial examples. We further design an encoder-decoder architecture with skip connections of high-frequency com-ponents to preserve fine details. We evaluated our method on three image manipulation detectors (FCN, ManTra-Net and MVSS-Net) with three benchmark datasets (DEFACTO,
CASIAv2 and COVER). Experiments show that our method generates adversarial examples significantly fast (0.01s per image), preserves better image quality (PSNR 30% higher than spatial attacks), and achieves a high attack success rate. We also observe that the examples generated by AMG can fool both classification and segmentation models, which indicates better transferability among different tasks. 1.

Introduction
With the rapid development of advanced editing soft-ware, manipulated images are becoming more common on social media. Despite the positive aspects, there are pos-sibilities that manipulated images are used to spread fake news and misleading information. Therefore, it is impor-tant to develop methods that can automatically detect ma-nipulated images.
Various image manipulation detectors [5, 20, 32, 4] have been proposed and achieved high performance on manip-Figure 1. Adversarial examples generated under the proposed
AMG task. For the manipulated image, the manipulations of the original image can be detected by a detector named MVSS-Net
[5]. After applying attacks, the detector fails to correctly detect manipulations. For the authentic image, attacking the original im-age results in false-positive detection. Compared to previous at-tack methods, the examples generated by our method have much less noise and can successfully fool the detector at pixel-level. ulated image datasets [26, 7, 36, 16]. However, existing methods can be vulnerable to adversarial examples. For a manipulated image, adding imperceptible adversarial per-turbations might cause the detector to detect a completely wrong result. Several works have already studied adversar-ial face forgery that fools the classification result of face forgery detector by the adversarial examples [15, 19, 14].
Since the detectors developed recently not only focus on classifying manipulated or authentic but also try to pin-point the manipulated regions, we also go one step further to generate adversarial examples for the detectors that ap-ply semantic segmentation. We name this task Adversarial
Manipulation Generation (AMG). This topic has not been studied before and it is more difficult because instead of only fooling a class label, our target is to fool every pixel in both manipulated and authentic regions. We further show
the examples generated by AMG have better transferability among different tasks (details in Section 5.5).
As shown in Figure 1, images added with imperceptible perturbations generated by our task can successfully fool the detector. This task is also different from dense adver-sary generation [39, 3, 30] which is designed for segmen-tation and detection models under an untargeted attack set-ting. Our work considers applying targeted adversarial at-tacks on the manipulated and authentic images to explore the vulnerability of the image manipulation detectors. This is a specific scenario that the adversarial examples actually bring serious social problems. We first design a loss func-tion that optimizes perturbation at pixel-level to generate such adversarial examples.
Algorithms such as FGSM [11] and PGD [25, 18] can be extended using our loss function to generate adversar-ial examples. However, these examples can be easily rec-ognized by human eyes since the images are perturbated with visible noise. In addition, generating examples using iterative methods such as PGD can be time-consuming. To address these problems, we proposed a novel adversarial at-tack that generates perturbations using both frequency and spatial features to avoid image quality degradation. In addi-tion, instead of optimizing each example against the target model, our method only needs to train the generator once, and then it can generate perturbation for any images ex-tremely fast. The contributions of this work are three-fold:
• We explore the vulnerability of the image manipula-tion detectors by proposing an AMG task. We design a loss function and extend existing attacks to generate adversarial examples. Although these attacks can fool the detectors, the perturbations added to the image are visible to human eyes. We perform an analysis and find the loss of high-frequency components can be the major reason to cause the degradation of image quality.
• Inspired by the above observation, we propose an ad-versarial attack that incorporates both spatial and fre-quency features into the GAN architecture to gener-ate adversarial examples. To preserve fine details, an encoder-decoder with skip connections of high-frequency components is also combined. Compared with previous attacks, our method generates more im-perceptible perturbations for human observers.
• We evaluate our method on three manipulation detec-tors with three benchmark datasets, under both white-box and black-box settings. Experiments show that our method generates adversarial examples significantly fast, preserves better image quality, and achieves a high attack success rate. We further observe the ex-amples generated by AMG have better transferability and can fool both classification and detection models. 2.