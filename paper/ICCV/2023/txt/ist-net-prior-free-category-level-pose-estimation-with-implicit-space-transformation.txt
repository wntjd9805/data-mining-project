Abstract
Category-level 6D pose estimation aims to predict the poses and sizes of unseen objects from a specific cate-gory. Thanks to prior deformation, which explicitly adapts a category-specific 3D prior (i.e., a 3D template) to a given object instance, prior-based methods attained great success and have become a major research stream. However, ob-taining category-specific priors requires collecting a large amount of 3D models, which is labor-consuming and of-ten not accessible in practice. This motivates us to inves-tigate whether priors are necessary to make prior-based methods effective. Our empirical study shows that the 3D prior itself is not the credit to the high performance. The keypoint actually is the explicit deformation process, which aligns camera and world coordinates supervised by world-space 3D models (also called canonical space). Inspired by these observations, we introduce a simple prior-free im-plicit space transformation network, namely IST-Net, to transform camera-space features to world-space counter-parts and build correspondences between them in an im-plicit manner without relying on 3D priors. Besides, we de-sign camera- and world-space enhancers to enrich the fea-tures with pose-sensitive information and geometrical con-straints, respectively. Albeit simple, IST-Net achieves state-of-the-art performance based-on prior-free design, with top inference speed on the REAL275 benchmark. Our code and models are available at https://github.com/
CVMI-Lab/IST-Net. 1.

Introduction
Category-level pose estimation [39] draws great atten-tion and plays an important role in practical applications, including robotic manipulation [6, 28, 22], augmented re-ality [34], and scene understanding [29, 43, 19, 20, 18, 14, 46]. Unlike instance-level pose estimation [15, 17, 23, 30, 13, 42], which requires a 3D CAD model for each object instance, this task aims at exploiting category-specific in-Figure 1. Comparison with competitive methods on REAL275 dateset. DPN refers to the DualPoseNet [26]. All speeds are mea-sured on a single RTX3090Ti GPU. We use blue/red to distinguish prior-based/free methods. IST-Net achieves top performance on 3D75 with the best inference speed. formation and thus can further generalize to unseen objects within given categories.
Recently, many methods [25, 3, 35, 40, 4, 2, 5] have been proposed for category-level pose estimation, which can be categorized into two groups: prior-free methods and prior-based methods. Prior-free methods [4, 26, 39, 38] mainly focus on designing network structures to fit the training data better. These methods are relatively simple but struggle to generalize to novel objects and suffer from poor perfor-mance.
To address this issue, prior-based methods [25, 3, 35, 7, 21, 10] leverage category-specific 3D priors (templates) to guide pose estimation. They adopt a prior-driven deforma-tion module [35] to deform the prior for synthesizing the target object in world-space. And then, they formulate the pose estimation problem as a camera- and world-space cor-respondence learning problem which explicitly aligns the coordinates [35]. Although considerable progress has been attained with prior-based methods [3, 45, 40], the require-ments of collecting a large amount of ground-truth 3D mod-els of target objects for obtaining the 3D prior and super-vising training the prior deformation module hinders their practical applicability.
This motivates us to investigate the mechanism that makes prior-based methods effective. We experiment with the shape deformation module which is used to deform the given shape prior to the desired instance (Fig. 3) by replac-ing the shape priors with random noise and fixed shape prior from another category (see Fig. 2). We observe that the de-formation module can adapt any inputs (noise or fixed prior) into a target world-space object (see Fig. 2 (b)). Besides, the model performance remains high regardless of the 3D priors (see Fig. 2 (a)). The above suggests: the shape prior itself is not necessary for the high performance of prior-based methods, but the deformation module that learns to synthe-size world-space target objects and explicitly builds the cor-respondence between camera and world-space is the key as the performance degrades dramatically without prior defor-mation. This promotes us to investigate new ways to build camera-to-world correspondence without requiring 3D pri-ors and models.
In this paper, we propose a simple yet effective prior-free model, named Implicit Space Transformation Network (IST-Net), which implicitly sets up feature correspondence between camera-space and world-space without requiring 3D priors or ground-truth 3D models of target objects.
Specifically, given the camera-space features, the network transforms them into world-space features which together with the camera-space features are further used for estimat-ing camera poses. For learning the transformation, we pro-pose a world-space enhancer that distills standard world-space features to supervise the transformed features. Note that the standard world-space features are obtained by trans-forming the input target object into its world-space with the ground-truth pose and feeding them into a feature ex-tractor. Besides, given camera-space inputs, the backbone network’s feature extraction capabilities are boosted by in-troducing an auxiliary pose estimation loss, namely the camera-space enhancer. Notably, both enhancers are train-ing only, which brings considerable performance improve-ments without introducing computational overhead.
Our main contributions are summarized as follows:
• We investigate prior-based methods and find that shape priors are not necessary for obtaining high perfor-mance while building the camera and world-space cor-respondence with prior deformation is a key factor.
• We propose a simple yet effective Implicit Space
Transformation Network (IST-Net) implicitly builds the correspondence between camera- and world-space on feature-level without requiring 3D models or priors. that
• We introduce two different space enhancers to facili-tate learning the transformation and enhance their rep-resentation capability for pose estimation.
• We conduct a series of experiments on REAL275 [39] and Wild6D [10] datasets to demonstrate the effective-ness of the proposed method. Notably, IST-Net is cur-rently the only prior-free method that achieves state-of-the-art performance on the REAL275 benchmark and attains notable gains over the prior-based method in terms of efficiency and accuracy (see Fig. 1). 2.