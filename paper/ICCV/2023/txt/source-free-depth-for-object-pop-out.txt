Abstract
Depth cues are known to be useful for visual percep-tion. However, direct measurement of depth is often im-practicable. Fortunately, though, modern learning-based methods offer promising depth maps by inference in the wild. In this work, we adapt such depth inference models for object segmentation using the objects’ “pop-out” prior in 3D. The “pop-out” is a simple composition prior that assumes objects reside on the background surface. Such compositional prior allows us to reason about objects in the 3D space. More specifically, we adapt the inferred depth maps such that objects can be localized using only 3D information. Such separation, however, requires knowl-edge about contact surface which we learn using the weak supervision of the segmentation mask. Our intermediate representation of contact surface, and thereby reasoning about objects purely in 3D, allows us to better transfer the depth knowledge into semantics. The proposed adapta-tion method uses only the depth model without needing the source data used for training, making the learning process efficient and practical. Our experiments on eight datasets of two challenging tasks, namely salient object detection and camouflaged object detection, consistently demonstrate the benefit of our method in terms of both performance and generalizability. The source code is publicly available at https://github.com/Zongwei97/PopNet. 1.

Introduction
The 3D knowledge of the scene is long known to be complementary to the task of visual perception [12, 16, 56, 67, 91]. Often in practice, though, visual perception needs to be carried out using only 2D images. Given mul-tiple images, 3D geometry may be recovered using the structure-from-motion techniques [26, 41, 55, 86]. Such in-version, however, is not compatible when only a single im-age is available. Under such circumstances, image inversion
*Corresponding Author: Deng-Ping Fan (dengpfan@gmail.com)
Figure 1. Depth to semantics conversion using the object pop-out prior. For input RGB and source-free depth pair, we learn contact surface. The obtained contact surface is then used to separate ob-jects and backgrounds to derive pseudo semantics for supervision. to depth map is usually done using learning-based meth-ods [13, 45, 50, 51, 65], which have shown unparalleled suc-cess in the recent years. Unfortunately, the learning-based methods may not offer high-quality depth maps due to the generalization deficiency across domains [68, 73].
Despite poor generalization, the knowledge gained in one domain is shown to be useful in other close-by domains.
This utility is harnessed by performing the so-called domain adaptation (DA) [2,3,6,53,61,89]. In fact, it has been shown recently that DA methods can efficiently transfer knowledge using only the prediction models, i.e., without requiring ac-cess to the data where the model is trained – also known as the source-free domain adaptation (SDA) [25,28,39,71,77].
The SDA methods are of gripping interest due to their effi-ciency and privacy promises.
The most existing SDA methods make one or both of these implicit assumptions: (a) a similar (as that of the source) supervising task at the target [28, 75, 77]; (b) task of discrete (and known) label space [27, 29, 36, 71]. The former assumption not only makes the source and target domains easier to compare but also potentially keeps the two domains closer. The latter assumption allows perform-ing SDA by self-training where the discrete labels facilitate reasoning about the model’s confidence. The self-training is then performed by boosting the confidence at the target for some picked reliable examples. 1
Figure 2. The performance gained in F-measure using our method over the established baselines. Our method (▲,■) significantly improves baselines on 8 datasets and 2 tasks (each task on 4 datasets– SOD: left two; COD: right two). We also compare 24 methods (•), where our method offers state-of-the-art results despite their task specialization. Note that all methods are connected using lines to illustrate their performance fluctuations across datasets. Please, refer Tables 1 & 2 and Section 4.3 for more details and discussions.
In this work, we aim for source-free transfer of depth knowledge for object detection. Such transfer is desired to assist in locating the object by depth cues and to ex-ploit the depth knowledge despite the domain gap. The ad-dressed problem setting differs from the standard SDA in terms of (a) the source and target tasks’ difference; (b) and the continuous label space of the depth. These differences (with standard SDA) make our task at hand very challeng-ing, which is addressed for the first time in this paper, up to our knowledge. To address such a challenging problem, we rely on the “pop-out” prior, which allows us to reason about the object’s location directly in the 3D space. The “pop-out” is a simple composition prior that assumes objects reside on the background surface. A graphical illustration of the used
“pop-out” prior, using the results obtained by our method, is given in Figure 1.
The pop-out prior for image composition was success-fully used by Kang et al. in [24]. An early work of Treis-man has provided an in-depth study of such prior in [62]. In this work, we rely on the same compositional foundation of these works and exploit the pop-out before transferring the depth of knowledge across domains. Although our motiva-tion comes from these early works, our experimental setup largely differs from theirs. We differ in terms of not only depth knowledge transfer across domains (without source data) but also in target supervision using only semantics.
The proposed method exploits the source-free depth to map it into a space where objects in depth stand out bet-ter against the background. This mapping is used for ob-ject and background separation using a learned contact sur-face between them. Such separation allows us to derive the semantic masks which can be directly compared against the ground truth for supervision. Using this supervision at the target, we can minimize the domain and task gap be-tween the source and target. The overall framework of our method that performs cross-task cross-domain knowledge transfer by using the intermediate representation of the pop-out space is shown in Figure 3. As can be seen, we first leverage an object popping network to encourage the object to jump out from the source-free depth. Then, we introduce another network i.e., the segmentation with contact sur-face, to localize the object and predict the contact surface.
These learning modules are jointly trained in an end-to-end manner, transferring the source-free depths into interme-diate representations which are adapted to the target task, i.e., object detection. To evaluate the proposed method, we conducted exhaustive experiments on eight datasets of two challenging tasks, namely salient object detection and cam-ouflaged object detection. In both tasks, our method signif-icantly improves the established baselines and offers state-of-the-art results at the same time, whose overview can be seen in Figure 2. The major contributions are as follows:
• Our problem of transferring source-free depth knowl-edge across domains and tasks is practical and novel.
• Our method relies on our object pop-out prior for vi-sual understanding, which is simple and effective.
• Results of our method in two different tasks are signif-icantly better than the baselines and existing models. 2.