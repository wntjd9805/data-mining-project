Abstract
Driver distraction has become a significant cause of se-vere traffic accidents over the past decade. Despite the growing development of vision-driven driver monitoring systems, the lack of comprehensive perception datasets re-In this paper, we stricts road safety and traffic security.
†These authors are second contributions. Project lead.
§Corresponding author. present an AssIstive Driving pErception dataset (AIDE) that considers context information both inside and outside the vehicle in naturalistic scenarios. AIDE facilitates holis-tic driver monitoring through three distinctive character-istics, including multi-view settings of driver and scene, multi-modal annotations of face, body, posture, and ges-ture, and four pragmatic task designs for driving under-standing. To thoroughly explore AIDE, we provide exper-imental benchmarks on three kinds of baseline frameworks
via extensive methods. Moreover, two fusion strategies are introduced to give new insights into learning effective multi-stream/modal representations. We also systematically in-vestigate the importance and rationality of the key com-ponents in AIDE and benchmarks. The project link is https://github.com/ydk122024/AIDE. 1.

Introduction
Driving safety has been a significant concern over the past decade [12, 34], especially during the transition of au-tomated driving technology from level 2 to 3 [26]. Accord-ing to the World Health Organization [58], there are ap-proximately 1.35 million road traffic deaths worldwide each year. More alarmingly, nearly one-fifth of road accidents are caused by driver distraction that manifests in behav-ior [53] or emotion [42]. As a result, active monitoring of the driver’s state and intention has become an indispensable component in significantly improving road safety via Driver
Monitoring Systems (DMS). Currently, vision is the most cost-effective and richest source [69] of perception infor-mation, facilitating the rapid development of DMS [15, 35].
Most commercial DMS rely on vehicle measures such as steering or lateral control to assess drivers [15]. In contrast, the scientific communities [20, 33, 37, 54, 59, 98] focus on developing the next-generation vision-driven DMS to de-tect potential distractions and alert drivers to improve driv-ing attention. Although DMS-related datasets [1, 16, 28, 29, 31, 42, 44, 53, 59, 64, 73, 94] offer promising prospects for enhancing driving comfort and eliminating safety haz-ards [54], two serious shortcomings among them restrict the progress and application in practical driving scenarios.
We first illustrate a comprehensive comparison of main-stream vision-driven assistive driving perception datasets in
Table 1. Specifically, previous datasets [1, 20, 37, 53, 59, 73, 94, 97, 98] mainly concern the in-vehicle view to ob-serve driver-centered endogenous representations, such as anomaly detection [37], drowsiness prediction [20, 98], and distraction recognition [1, 73, 94]. However, the equally im-portant exogenous scene factors that cause driver distraction are usually ignored. The driver’s state inside the vehicle is frequently closely correlated with the traffic scene outside the vehicle [61, 93]. For instance, the reason for an an-gry driver to look around is most likely due to a traffic jam or malicious overtaking [38]. Meanwhile, most smoking or talking behaviors occur in smooth traffic conditions. A holistic understanding of driver performance, vehicle con-dition, and scene context is imperative and promising for achieving more effective assistive driving perception.
Another shortcoming is that most existing datasets [16, 29, 37, 53, 59, 64] focus on identifying driver behavior characteristics while neglecting to evaluate their emotional states. Driver emotion plays an essential role in complex driving dynamics as it inevitably affects driver behavior and road safety [41]. Many researchers [3, 63] have indicated that drivers with peaceful emotions tend to maintain the best driving performance (i.e., normal driving). Conversely, negative emotional states (e.g., weariness) are more likely to induce distractions and secondary behaviors (e.g., doz-ing off ) [30]. Despite initial progress in driving emotion understanding works [13, 31, 42, 44], these inadequate ef-forts only consider facial expressions and ignore the valu-able clues provided by the body posture and scene con-text [86, 87, 88, 89, 90, 91]. Most importantly, there are no comprehensive datasets that simultaneously consider the complementary perception information among driver be-havior, emotion, and traffic context, which potentially limits the improvement of the next-generation DMS.
Motivated by the above observations, we propose an As-sIstive Driving pErception dataset (AIDE) to facilitate fur-ther research on the vision-driven DMS. AIDE captures rich information inside and outside the vehicle from sev-eral drivers in realistic driving conditions. As shown in Fig-ure 1, we assign AIDE three significant characteristics. (i)
Multi-view: four distinct camera views provide an expan-sive perception perspective, including three out-of-vehicle views to observe the traffic scene context and an in-vehicle (ii) Multi-modal: di-view to record the driver’s state. verse data annotations from the driver support comprehen-sive perception features, including face, body, posture, and gesture information. (iii) Multi-task: four pragmatic driv-ing understanding tasks guarantee holistic assistive percep-tion, including driver-centered behavior and emotion recog-nition, traffic context, and vehicle condition recognition.
To systematically evaluate the challenges brought by
AIDE, we implement three types of baseline frame-works using representative and impressive methods, which resource-efficient, and state-of-the-art involve classical, (SOTA) backbone models. Diverse benchmarking frame-works provide sufficient insights to specify suitable net-work architectures for real-world driving perception. For multi-stream/modal inputs, we design adaptive and cross-attention fusion modules to learn effectively shared repre-sentations. Additionally, numerous ablation studies are per-formed to thoroughly demonstrate the effectiveness of key components and the importance of AIDE. 2.