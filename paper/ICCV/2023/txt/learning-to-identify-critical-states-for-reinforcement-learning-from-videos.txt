Abstract
Recent work on deep reinforcement learning (DRL) has pointed out that algorithmic information about good poli-cies can be extracted from offline data which lack explicit information about executed actions [45, 46, 30]. For exam-ple, videos of humans or robots may convey a lot of im-plicit information about rewarding action sequences, but a DRL machine that wants to profit from watching such videos must first learn by itself to identify and recognize relevant states/actions/rewards. Without relying on ground-truth annotations, our new method called Deep State Iden-tifier learns to predict returns from episodes encoded as videos. Then it uses a kind of mask-based sensitivity analy-sis to extract/identify important critical states. Extensive ex-periments showcase our method’s potential for understand-ing and improving agent behavior. The source code and the generated datasets are available at https://github.com/AI-Initiative-KAUST/VideoRLCS. 1.

Introduction
In deep reinforcement learning (DRL), the cumulative reward—also known as the return—of an episode is ob-tained through a long sequence of dynamic interactions be-tween an agent (i.e., a decision-maker) and its environment.
In such a setting, the rewards may be sparse and delayed, and it is often unclear which decision points were critical to achieve a specific return.
Several existing methods use the notion of localizing critical states, such as EDGE [19] and RUDDER [1]. These methods typically require explicit action information or pol-icy parameters to localize critical states. This limits their potential applicability in settings like video-based offline
† Equal Contribution. (cid:66) Corresponding Author.
Accepted to ICCV23.
Figure 1. Motivation of the proposed method. In the illustrated race between a turtle and a rabbit, the sleep state is critical in deter-mining the winner of the race. Our method is proposed to identify such critical states.
RL, where an agent’s actions are often hard to measure, an-notate, or estimate [72, 32]. To avoid this pitfall, in this work, we explicitly study the relationship between sequen-tial visual observations and episodic returns without access-ing explicit action information.
Inspired by the existing evidence that frequently only a few decision points are important in determining the return of an episode [1, 12], and as shown in Fig. 1, we focus on identifying the state underlying these critical decision points. However, the problem of directly inferring critical visual input based on the return is nontrivial [12], and com-pounded by our lack of explicit access to actions or policies during inference. To overcome these problems—inspired by the success of data-driven approaches [65, 39, 24]—our method learns to infer critical states from historical visual trajectories of agents.
We propose a novel framework, namely the Deep State
Identifier, to identify critical states in video-based environ-ments. A principal challenge of working in such settings lies in acquiring ground-truth annotations of critical states;
it is laborious to manually label in videos critical states cor-responding to complex spatio-temporal patterns. The Deep
State Identifier is designed to directly overcome this chal-lenge by identifying the critical states based solely on visual inputs and rewards. Our proposed architecture comprises a return predictor and a critical state detector. The former pre-dicts the return of an agent given a visual trajectory, while the latter learns a soft mask over the visual trajectory where the non-masked frames are sufficient for accurately predict-ing the return. Our training technique explicitly minimizes the number of critical states to avoid redundant information through a novel loss function. If the predictor can achieve the same performance using a small set of frames, we con-sider those frames critical. Using a soft mask, we obtain a rank that indicates the importance of states in a trajec-tory, allowing for the selection of critical states with high scores. During inference, critical states can be directly de-tected without relying on the existence of a return predictor.
Our contributions can be summarized as follows:
• We propose a novel framework that effectively iden-tifies critical states for reinforcement learning from videos, despite the lack of explicit action information.
• We propose new loss functions that effectively enforce compact sets of identified critical states.
• We demonstrate the utility of the learned critical states for policy improvement and comparing policies. 2.