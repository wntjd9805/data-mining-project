Abstract 1.

Introduction
Event cameras have recently gained significant traction since they open up new avenues for low-latency and low-power solutions to complex computer vision problems. To unlock these solutions, it is necessary to develop algorithms that can leverage the unique nature of event data. However, the current state-of-the-art is still highly influenced by the frame-based literature, and usually fails to deliver on these promises. In this work, we take this into consideration and propose a novel self-supervised learning pipeline for the se-quential estimation of event-based optical flow that allows for the scaling of the models to high inference frequencies.
At its core, we have a continuously-running stateful neu-ral model that is trained using a novel formulation of con-trast maximization that makes it robust to nonlinearities and varying statistics in the input events. Results across multi-ple datasets confirm the effectiveness of our method, which establishes a new state of the art in terms of accuracy for approaches trained or optimized without ground truth.
Event cameras capture per-pixel log-brightness changes at microsecond resolution [13]. This operating principle re-sults in a sparse and asynchronous visual signal that, under constant illumination, directly encodes information about the apparent motion (i.e., optical flow) of contrast in the im-age space. These cameras offer several advantages, such as low latency and robustness to motion blur [13], and hence hold the potential of a high-bandwidth estimation of this optical flow information. However, the event-based na-ture of the generated visual signal poses a paradigm shift in the processing pipeline, and traditional, frame-based al-gorithms become suboptimal and often incompatible. De-spite this, the majority of learning-based methods that have been proposed so far for event-based optical flow estimation are still highly influenced by frame-based approaches. This influence is normally reflected in two key aspects of their
The project’s code and additional material can be found at https:// mavlab.tudelft.nl/taming event flow/.
pipelines: (i) the design of the network architecture, and (ii) the formulation of the loss function.
Regarding architecture design, most literature methods format subsets of the input events as dense volumetric rep-resentations [45] that are processed at once by stateless (i.e., non-recurrent) models [17, 29, 37, 45]. Similarly to their frame-based counterparts [12, 38, 39], these models esti-mate the per-pixel displacement over the time-length of the event volume using only the information contained within it. Consequently, these volumes need to encode enough spa-tiotemporal information for motion to be discernible. How-ever, if done over relatively long time periods, the subse-quent models suffer from limitations such as high latency or having to deal with large pixel displacements [17, 18, 42].
With respect to the loss function, multiple options have been explored due to the lack of real-world datasets with per-event ground truth. Pure supervised learning can be used with datasets such as MVSEC [43] or DSEC-Flow
[17], but their ground truth only contains per-pixel displace-ment at low frequencies, which makes models difficult to train. On the other hand, a self-supervised learning (SSL) framework can be formulated using either the accompany-ing frames with the photometric error as a loss [11, 40, 44], or through an events-only contrast maximization [14, 15] proxy loss for motion compensation (i.e., event deblurring)
[19, 26, 29, 35, 45]. However, despite not relying on ground truth, all the literature on SSL for optical flow assumes that the events move linearly within the time window of the loss, which ignores much of the potential of event cameras and their high temporal resolution (see Fig. 1, bottom left).
In this work, we focus on the estimation of high fre-quency event-based optical flow and how this can be learned in an SSL fashion using contrast maximization with a re-laxed linear motion assumption. To achieve this, we build upon the continuous-operation pipeline from Hagenaars et al. [19], which retrieves optical flow by sequentially pro-cessing small partitions of the event stream with a stateful (i.e., recurrent) model over time, instead of dealing with large volumes of input events. We augment (and train) this pipeline with a novel contrast maximization formu-lation that performs event motion compensation in an it-erative manner at multiple temporal scales, as shown in
Fig. 1. Using this framework, we achieve the best accuracy of all contrast-maximization-based approaches on multiple datasets, only being outperformed by pure supervised learn-ing methods trained with ground-truth data.
In summary, the extensions that we propose to the SSL framework in [19], i.e., our main contributions, are:
• The first iterative event warping module in the context of contrast maximization (see Section 3.2). This mod-ule unlocks a novel multi-reference loss function that better captures the trajectory of scene points over time, thus improving the accuracy of the predictions.
• The first multi-timescale approach to contrast max-imization, which adds robustness, improves conver-gence, and reduces tuning requirements of loss-related hyperparameters (see Section 3.3).
As a result, we present the first self-supervised optical flow method for event cameras that relaxes the linear motion as-sumption, and hence that has the potential of exploiting the high temporal resolution of the sensor by producing esti-mates in a close-to continuous manner. We validate the pro-posed framework through extensive evaluations on multiple datasets. Additionally, we conduct ablation studies to show the effectiveness of each individual component. 2.