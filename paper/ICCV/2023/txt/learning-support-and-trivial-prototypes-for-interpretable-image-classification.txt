Abstract
Prototypical part network (ProtoPNet) methods have been designed to achieve interpretable classiﬁcation by associating predictions with a set of training prototypes, which we refer to as trivial prototypes because they are trained to lie far from the classiﬁcation boundary in the feature space. Note that it is possible to make an anal-ogy between ProtoPNet and support vector machine (SVM) given that the classiﬁcation from both methods relies on computing similarity with a set of training points (i.e., triv-ial prototypes in ProtoPNet, and support vectors in SVM).
However, while trivial prototypes are located far from the classiﬁcation boundary, support vectors are located close to this boundary, and we argue that this discrepancy with the well-established SVM theory can result in ProtoPNet mod-els with inferior classiﬁcation accuracy. In this paper, we aim to improve the classiﬁcation of ProtoPNet with a new method to learn support prototypes that lie near the classi-ﬁcation boundary in the feature space, as suggested by the
SVM theory. In addition, we target the improvement of clas-siﬁcation results with a new model, named ST-ProtoPNet, which exploits our support prototypes and the trivial pro-totypes to provide more effective classiﬁcation. Experi-mental results on CUB-200-2011, Stanford Cars, and Stan-ford Dogs datasets demonstrate that ST-ProtoPNet achieves state-of-the-art classiﬁcation accuracy and interpretability results. We also show that the proposed support prototypes tend to be better localised in the object of interest rather than in the background region. 1.

Introduction
Deep convolutional neural networks (CNN) [28, 30, 16] have had remarkable achievements in various visual tasks, e.g., image recognition [16] and object detection [39]. De-spite the excellent feature extraction and discrimination ability, CNNs are generally treated as black-box models due to their complex architectures, high-dimensional fea-Figure 1. The difference between the learning of trivial and sup-port prototypes. (a) Trivial prototypes: the separation loss pushes the prototypes of different classes as far as possible from the clas-siﬁcation boundary. (b) Support prototypes: our new closeness loss enforces the prototypes of different classes to be as close as possible to the classiﬁcation boundary. ture spaces, and the enormous number of learnable param-eters. Such lack of interpretability hinders their successful application in ﬁelds that require understandable and trans-parent decisions [40], e.g., disease diagnosis [47, 13], ﬁnan-cial risk assessment [33], and autonomous driving [23].
Recently, increasing attention has been dedicated to the development of interpretable deep-learning models [25, 1, 4, 3]. A particularly interesting strategy is the prototype-based gray-box models, e.g., prototypical part network (ProtoPNet) [4, 11]. These methods are inherently in-terpretable since they can explain the model’s decisions by showing image classiﬁcation activation maps associated with a set of class-speciﬁc image prototypes. These proto-types are automatically learned from training samples, with classiﬁcation score being computed by comparing testing image parts to the learned training prototypes.
ProtoPNet [4] is trained to learn a classiﬁer from a set of class-speciﬁc prototypes by minimising the cross-entropy classiﬁcation loss and two additional regularisation losses, namely: 1) a clustering loss that pulls together training image patches to at least one prototype of its own class; and 2) a separation loss that pushes apart training image
Figure 2. Two-moon results. (a) Trivial prototypes and training samples in the feature (top) and data (bottom) spaces from the original ProtoPNet [4]. (b) Support prototypes and training sam-ples in the feature (top) and data (bottom) spaces from our method. (c) Support vectors and training samples from a Radial Basis Func-tion (RBF) kernel based SVM [7]. In (a) and (b), each prototype is projected onto the nearest training sample in the feature space. patches from all prototypes of other classes. The combi-nation of these two losses pushes the prototypes as far as possible from the classiﬁcation boundary, but still within the class distribution, so we call them trivial prototypes, as shown in Fig. 1(a). We also display the trivial prototypes, learned with a feed-forward neural network1, for the two-moon problem in Fig. 2(a). Notice that these trivial proto-types are located far from the classiﬁcation boundary. In
ﬁne-grained visual classiﬁcation, the trivial prototypes can mistakenly focus on background regions instead of on the object of interest [42, 41], particularly for those classes with subtle foreground (object) differences but large background variations, as shown in Fig. 3. Different from ProtoPNet’s trivial prototypes, the support vector machine (SVM) [7] classiﬁer relies on a set of support vectors that are close to the classiﬁcation boundary, as in Fig. 2(c). These sup-port vectors are often treated as hard samples. Motivated by
SVM, we propose the derivation of support (i.e., hard-to-learn) prototypes for ProtoPNet methods.
In this paper, we propose an alternative learning strat-egy for ProtoPNet, which forces the learned prototypes to resemble SVM’s support vectors and to be located as close as possible to the classiﬁcation boundary. The strategy is formulated by a new closeness loss that minimises the dis-tance between prototypes of different classes. As shown in Fig. 1(b), our new loss enforces the prototypes to move closer to the classiﬁcation boundary, as also demonstrated by Fig. 2(b) revealing that the support prototypes produced by the introduction of our new closeness loss are indeed more similar to the support vectors of SVM in Fig. 2(c).
Furthermore, to improve the classiﬁcation accuracy, we pro-pose a new ST-ProtoPNet method that integrates both the support and trivial prototypes. The ST-ProtoPNet leverages the two distinct and complementary sets of prototypes to capture both hard (i.e., close to the boundary) and easy (i.e., far from the boundary) visual features for classiﬁcation. 1The network has an input layer of 2 nodes, a hidden layer of 256 nodes (activated by tanh), and an output layer of 2 nodes (activated by sigmoid).
Figure 3. Example prototypes sampled from a VGG19-based Pro-toPNet [4]. In each class, the left prototype focuses on object fea-tures while the right one captures background.
The major contributions of this work are: 1. We provide the ﬁrst study that makes an analogy be-tween the prototype learning from ProtoPNet and sup-port vector learning from SVM, where we propose support (i.e., hard-to-learn) prototypes that can im-prove classiﬁcation accuracy and interpretability. 2. We present a new ST-ProtoPNet method to exploit both support and trivial prototypes for interpretable image classiﬁcation, where the two sets of prototypes can provide complementary information to improve clas-siﬁcation accuracy. 3. We conduct extensive experiments on three bench-marks, showing that our ST-ProtoPNet outperforms current state-of-the-art (SOTA) methods in terms of classiﬁcation accuracy and interpretability.
In our experiments, we also demonstrate that the trivial and support prototypes have different characteristics, where the trivial prototypes tend to focus on both local parts of the visual object of interest and the background, while the sup-port prototypes mainly focus on object parts belonging to the visual class of interest. 2.