Abstract
We present Contrastive Feature Masking Vision Trans-former (CFM-ViT) - an image-text pretraining methodology that achieves simultaneous learning of image- and region-level representation for open-vocabulary object detection (OVD). Our approach combines the masked autoencoder (MAE) objective into the contrastive learning objective to improve the representation for localization tasks. Un-like standard MAE, we perform reconstruction in the joint image-text embedding space, rather than the pixel space as is customary with the classical MAE method, which causes the model to better learn region-level semantics. More-over, we introduce Positional Embedding Dropout (PED) to address scale variation between image-text pretraining and detection ﬁnetuning by randomly dropping out the po-sitional embeddings during pretraining. PED improves de-tection performance and enables the use of a frozen ViT backbone as a region classiﬁer, preventing the forgetting of open-vocabulary knowledge during detection ﬁnetuning.
On LVIS open-vocabulary detection benchmark, CFM-ViT achieves a state-of-the-art 33.9 APr, surpassing the best ap-proach by 7.6 points and achieves better zero-shot detection transfer. Finally, CFM-ViT acquires strong image-level rep-resentation, outperforming the state of the art on 8 out of 12 metrics on zero-shot image-text retrieval benchmarks. 1.

Introduction
The ability to detect a vast array of objects in the real world is fundamental to computer vision and machine learn-ing. This powers a wide range of applications from au-tonomous agents to search engines. Unfortunately, to date most modern object detectors rely on manually annotated regions and class labels, which is labor-intensive and im-practical to scale beyond the order of 103 categories.
A new task called open-vocabulary detection (OVD) has been introduced to address the vocabulary limitation in ob-ject detection by using image-text pairs for training and text queries from users at test time [62]. Open-vocabulary detectors represent categories as text embeddings rather than discrete class labels, allowing them to predict objects
Figure 1: We propose CFM-ViT to pretrain vision transformers to capture more pixel and region information for open-vocabulary de-tection. CFM-ViT predicts masked contrastive features on top of the contrastive image-text pretraining. (Top) We visualize (c) the similarity map between (d) the reconstructed image features (see top left) and (e) the query text embedding. CFM-ViT correctly pre-dicts the (c) whole-image semantics from (b) heavily truncated im-ages. (Bottom) Our open-vocabulary detector exploits the frozen
ViT backbone to retain pretrained knowledge and is able to detect base and novel object classes (only novel classes are shown). unavailable during training. Various techniques, such as knowledge distillation [18, 13], weak supervision [71], self-training [68, 46, 65], and frozen backbone [32], have been suggested. Typically, CNN backbones are utilized in these approaches. As vision transformers have gained signiﬁcant traction in image understanding [12, 63, 21, 3], it is crucial to explore open-vocabulary detectors based on vision trans-formers [39]. Moreover, to our knowledge, most current
OVD research assumes the availability of pretrained Vision-Language Models (VLMs) (e.g. CLIP [44]), and proposes adaptation or ﬁnetuning techniques to overcome the dispar-ity between image-level pretraining and object-level ﬁne-tuning [18, 13, 68, 65, 46]. However, as these VLMs are typically optimized for image-level tasks such as classiﬁca-tion and retrieval, they do not adequately utilize the pixel-and region-level information during pretraining, which is crucial for downstream open-vocabulary detection.
We present CFM-ViT (Contrastive Feature Masking Vi-sion Transformer), a simple framework to pretrain vision transformers to capture more detailed pixel/region infor-mation for open-vocabulary object detection (Fig. 1). In-spired by MAE [21], we adopt the concept of masked auto-encoding to enhance object representation during pre-training. However unlike MAE, we perform prediction in the joint image-text embedding space rather than the pixel space as an auxiliary objective to the contrastive image-text learning. This additional objective provides orthogo-nal signal from the contrastive learning, and beneﬁts down-stream detection task without compromising the image-level tasks.
In addition, we propose Positional Embed-ding Dropout (PED) to address overﬁtting to the typically lower-resolution and object-centric pretraining data. By randomly dropping out positional embeddings during pre-training, PED aids the model to learn more robust repre-sentations that better generalize to high-res detection data.
Moreover, PED enables the use of a frozen ViT encoder as an open-vocabulary region-classiﬁer, which prevents the forgetting of open-vocabulary knowledge at detection.
We evaluate CFM-ViT on the widely used LVIS and
COCO open-vocabulary detection benchmarks. Our top-performing model obtains 33.9 APr on LVIS, surpassing the previous best approach by 7.6 APr at system level. On the COCO benchmark, CFM-ViT represents the ﬁrst ViT-based model and achieves a very competitive novel AP without using pseudo labels or weak supervision. Although not optimized for retrieval, CFM-ViT outperforms the state-of-the-art methods of similar or larger capacity on 8 out of 12 image-text retrieval benchmark metrics. In summary:
• We present an image-text pretraining methodology to learn localization cues for open-(CFM-ViT) vocabulary detection by contrastive feature masking.
• We propose Positional Embedding Dropout (PED) to bridge the gap between image-text pretraining and detection ﬁnetuning, which enables the use of a frozen ViT encoder to prevent the forgetting of open-vocabulary knowledge during detection ﬁnetuning.
• CFM-ViT achieves state-of-the-art APr on LVIS open-vocabulary detection benchmark, shows very compet-itive performance on COCO and zero-shot transfer to
Objects365, and outperforms the SOTA on 8 out of 12 metrics of zero-shot image-text retrieval benchmarks.
We hope these discoveries would encourage the community to explore open-vocabulary detection from the perspective of image-text pretraining. 2.