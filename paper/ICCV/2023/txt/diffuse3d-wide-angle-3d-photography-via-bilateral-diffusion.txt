Abstract
This paper aims to resolve the challenging problem of wide- angle novel view synthesis from a single image, a.k.a. wide- angle 3D photography. Existing approaches rely on local context and treat them equally to inpaint occluded
RGB and depth regions, which fail to deal with large- region occlusion (i.e., observing from an extreme angle) and fore-ground layers might blend into background inpainting. To address the above issues, we propose Diffuse3D which em-ploys a pre- trained diffusion model for global synthesis, while amending the model to activate depth- aware infer-ence. Our key insight is to alter the convolution mechanism
*Both authors contributed equally to this research.
â€ Corresponding author: shengfenghe@smu.edu.sg. in the denoising process. We inject depth information into the denoising convolution operation with bilateral kernels, i.e., a depth kernel and a spatial kernel, to consider layered correlations among pixels. In this way, foreground regions are overlooked in background inpainting and only pixels close in depth are leveraged. On the other hand, we propose a global- local balancing approach to maximize both con-textual understandings. Extensive experiments demonstrate that our approach outperforms state- of- the- art methods in novel view synthesis, especially in wide- angle scenarios.
More importantly, our method does not require any training and is a plug- and- play module that can be integrated with any diffusion model. Our code can be found at https:
//github.com/yutaojiang1/Diffuse3D.
1.

Introduction
In our life, images play an important role in carrying and sharing visual memories. Animating a still image can further enhance the immersive experience of an impressive moment. 3D photography [26, 13, 10] is proposed for this purpose to generate 3D viewing experiences from a single image, by interactively changing the camera angles. These methods are all based on modular systems and leverage state-of-the-art depth estimation, inpainting, and segmenta-tion models to understand the layer structure and fill in the holes of occlusions. This component-wise strategy shows robust performance when dealing with in-the-wild scenes.
This line of research focuses on the inpainting quality of disocclusion caused by camera movements, through tak-ing neighboring contextual information into account. How-ever, due to the limited and narrow boundary context for in-painting, prior works [26, 10, 6] are confined to novel view synthesis with small camera view angle changes (generally 5-10 consecutive frames for one scene), which can hardly create a sense of immersion to meet the realistic demand.
Extending the camera angle reveals larger holes that can-not be simply filled by repeating neighboring textures (see
Figure 1b). Indeed, human could conjecture the occluded areas not only from the nearby visible areas but also by connecting to our visual memories (i.e., image synthesis).
On the other hand, the way they inpaint disocclusions ne-glects [26, 6] or implicitly considers [10] the depth prior, which may leak foreground semantics into background re-covery (see Figure 1c).
In this paper, we present a new method, called Dif-fuse3D, to address the above problems. We employ an off-the-shelf pre-trained diffusion model as the generative prior, and we further empower it to be depth-aware by introducing bilateral convolution into the denoising inference process.
Specifically, diffusion models can be conditioned by the masks and generate coherent image contents through con-secutive denoising steps, but all pixels are weighted equally
Inspired by bilateral filter [1, 29, 7] during the process. and depth-aware learning [32], we decompose the denois-ing convolution kernel into two parts, a spatial kernel that averages local regions, and more importantly a depth ker-nel to assign different weights to the pixels according to the depth similarities. In this way, depth prior is injected in the diffusion model and only the surrounding pixels located in the nearby layers are considered in the denoising steps. On the other hand, we introduce two levels of inpainting in the framework, the global and local ones, to obtain diverse con-textual knowledge from two essentially different aspects.
Our proposed method is a plug-and-play design that can easily work with arbitrary diffusion models, and the idea of bilateral diffusion can be easily extended to other diffusion applications to include information other than depth. Exten-sive experiments demonstrate the superior 3D photography performance of our method compared to the state-of-the-art methods. Especially in the wide-angle setting, our ap-proach can effectively differentiate the contextual informa-tion of foreground/background and produce more semanti-cally meaningful inpainted regions (see Figure 1d). 2.