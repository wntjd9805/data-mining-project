Abstract
Although Magnetic Resonance Imaging (MRI) is very helpful for brain tumor segmentation and discovery, it of-ten lacks some modalities in clinical practice. As a re-sult, degradation of prediction performance is inevitable.
According to current implementations, different modalities are considered to be independent and non-interfering with each other during the training process of modal feature ex-traction, however they are complementary. In this paper, considering the sensitivity of different modalities to diverse tumor regions, we propose a Category Aware Group Self-Support Learning framework, called GSS, to make up for the information deﬁcit among the modalities in the indi-vidual modal feature extraction phase. Precisely, within each prediction category, predictions of all modalities form a group, where the prediction with the most extraordi-nary sensitivity is selected as the group leader. Collab-orative efforts between group leaders and members iden-tify the communal learning target with high consistency and certainty. As our minor contribution, we introduce a random mask to reduce the possible biases. GSS adopts the standard training strategy without speciﬁc architectural choices and thus can be easily plugged into existing in-complete multi-modal brain tumor segmentation. Remark-ably, extensive experiments on BraTS2020, BraTS2018, and BraTS2015 datasets demonstrate that GSS can im-prove the performance of existing SOTA algorithms by 1.27-3.20% in Dice on average. The code is released at https://github.com/qysgithubopen/GSS. 1.

Introduction
Magnetic resonance image (MRI) segmentation of brain tumors is becoming increasingly important in clinical eval-uation and diagnosis. MRI is designed for different tissues of brain structures and brain tumors with multiple imag-†Equal Contribution
‡Corresponding Author (cid:28432)
% ( t n e i c i f f e o c y t i r a l i m i s e c i
D 80 78 76 74 72 70 68 66 64
Ours
Ours
+2.17 mmFormer
[40]
+2.36
RFNet
[7]
RobustSeg
[2]
Ours
Ours
Ours
+1.27 mmFormer
+2.16
RFNet
Ours
+3.20
+1.28
RFNet mmFormer
RobustSeg
RobustSeg
U-HVED
U-HVED[8]
U-HVED
BraTs2020
BraTs2018
BraTs2015
Figure 1. Average accuracy on BraTS2020, BraTS2018, and
BraTS2015 datasets. Our GSS enables consistent performance im-provements over state-of-the-arts, i.e. mmFormer [40], RFNet [7], without bringing any change to base networks during inference. ing modalities, such as Fluid Attenuation Inversion Recov-ery (FLAIR), contrast enhanced T1-weighted (T1c), T1-weighted (T1) and T2-weighted (T2). Combining multi-modal images for brain tumor segmentation can signiﬁ-cantly improve segmentation accuracy. Most existing meth-ods stitch multi-modal images on channels and input them into the network [18, 46, 27, 30, 10]. However, in clinical practice, the problem of lost modalities is pervasive due to data corruption, various scanning protocols, and unsuitable conditions of the patient [34, 22, 31, 47]. Therefore, there is a great need for a robust multi-modal approach for ﬂexible and practical clinical applications to address the problem of missing one or more modalities.
The current main direction for incomplete medical im-age segmentation tasks involves multiple stages network to deal with all incomplete modalities cases [11, 8, 7, 2, 40].
This approach considers improving the network’s ability to extract features of interest for individual modalities, which plays a pivotal role in the subsequent fusion phase. How-ever, these efforts only focus on learning invariant fea-tures and lack inter-modal interactions. It is worth noting    
that Ding et al. [7] ﬁnd out that different modalities con-tain distinct appearances and thus have different sensitivi-ties to diverse tumor regions. In particular, Flair is more sensitive to the background (BG), and T1c is more sensi-tive to necrotic, non-enhancing tumor cores (NCR/NET) and GD-enhancing tumors (ET), while Flair and T2 are more sensitive to peritumoral edema (ED), and unfortu-nately, the above approach does not consider it as a pri-ori information. Consequently, multi-modal feature interac-tion could be a knowledge transfer process between sensi-tive and non-sensitive modalities. Fortunately, knowledge distillation is an effective method for addressing this is-sue. However, existing knowledge distillation-based meth-ods [13, 32, 33, 1, 3, 17], often employ another, more com-plex model in order to convey complete modalities feature information, resulting in a tremendous computational effort during training. Meanwhile, there is a risk of conveying in-accurate information through direct mutual knowledge dis-tillation between modalities. In addition, if the result after fusion is used for distillation, it can not match the modali-ties’ input states, and model collapse is inevitable.
To alleviate these problems, we propose Category Aware
Group Self-Support Learning framework for incomplete multi-modal brain tumor segmentation, which is called
GSS. As shown in Fig. 2, during the model training phase, we establish self-support groups among students (modal-ities) instead of the teacher of previous knowledge distil-lation methods, while no new models and parameters are introduced. Speciﬁcally, we establish the groups for each label and, based on how sensitive each modal is to each la-bel, choose one or more top of them to serve as the group leaders. Group leaders can decide on this category with just one afﬁrmative vote. When the group leaders are irreso-lute, the other group members can make another decision by their voting results to assist in making the right deci-sion. If both group leaders and members are in doubt about this category, then take the category for which estimated minimum probability. Ultimately, based on the votes of the self-support group and after normalization, the soft labels of each category for knowledge distillation are determined.
Considering that the decision of the self-help group is posi-tively correlated with the quality of the initial prediction of the model at the early stage of training, which is still ﬂawed at some locations, we introduce a random masking strat-egy to reduce the possible biases. Overall, our contributions are threefold: 1) We propose Category Aware Group Self-Support Learning framework for incomplete multi-modal brain tumor segmentation. The dominating characteristics of several modalities are utilized to direct the distillation of mutual knowledge between modalities without expanding the complexity of the initial network. 2) In the optimal soft label selection, we set up a novel self-support group, aban-doning the direct mutual constraint of modalities through the pseudo-labels generated between each modal, but reﬁn-ing to categories, maximizing the use of information from each modal. 3) Taking advantage of the proposed ran-dom mask strategy, GSS could improve the performance of the state-of-the-art segmentation framework on the widely used BraTs2020, BraTs2018, and BraTs2015 benchmarks (Fig. 1). 2.