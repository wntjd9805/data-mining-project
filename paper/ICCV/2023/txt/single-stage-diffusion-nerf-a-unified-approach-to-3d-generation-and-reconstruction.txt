Abstract 1.

Introduction 3D-aware image synthesis encompasses a variety of tasks, such as scene generation and novel view synthesis from images. Despite numerous task-specific methods, de-veloping a comprehensive model remains challenging. In this paper, we present SSDNeRF, a unified approach that employs an expressive diffusion model to learn a generaliz-able prior of neural radiance fields (NeRF) from multi-view images of diverse objects. Previous studies have used two-stage approaches that rely on pretrained NeRFs as real data to train diffusion models.
In contrast, we propose a new single-stage training paradigm with an end-to-end objective that jointly optimizes a NeRF auto-decoder and a latent dif-fusion model, enabling simultaneous 3D reconstruction and prior learning, even from sparsely available views. At test time, we can directly sample the diffusion prior for uncon-ditional generation, or combine it with arbitrary observa-tions of unseen objects for NeRF reconstruction. SSDNeRF demonstrates robust results comparable to or better than leading task-specific methods in unconditional generation and single/sparse-view 3D reconstruction.6
*Work done during a remote internship with UCSD. 6Project page: https://lakonik.github.io/ssdnerf
Synthesizing 3D visual contents has gained significant attention in computer vision and graphics, thanks to ad-vances in neural rendering and generative models. Al-though numerous methods have emerged to handle individ-ual tasks, such as single-/multi-view 3D reconstruction and 3D content generation, it remains a major challenge to de-velop a comprehensive framework that bridges the state of the art of multiple tasks. For instance, neural radiance fields (NeRF) [28] have shown impressive results in novel view synthesis by solving the inverse rendering problem via per-scene fitting, which is suitable for dense-view inputs but dif-ficult to generalize to sparse observations. In contrast, many sparse-view 3D reconstruction methods [55, 8, 25] rely on feed-forward image-to-3D encoders, but they are unable to handle ambiguity in the occluded region and generate crisp images. Regarding unconditional generation, 3D-aware generative adversarial networks (GAN) [31, 5, 16, 13] are partially limited in their usage of single-image discrimina-tors, which cannot reason cross-view relationships to effec-tively learn from multi-view data.
In this paper, we propose a unified approach to various 3D tasks (Fig. 1) by developing a holistic model that learns generalizable 3D priors from multi-view images. Inspired
by the success of 2D diffusion models [19, 47, 27, 38, 26], we present the Single-Stage Diffusion NeRF (SSDNeRF), which models the generative prior of scene latent codes with a 3D latent diffusion model (LDM).
While similar LDMs have been applied in 2D and 3D generation in previous work [50, 38, 12, 2, 44, 29], they typically require two-stage training, where the first stage pretrains the variational auto-encoders (VAE) [23] or auto-decoders [32] without diffusion models. In the case of dif-fusion NeRFs, however, we argue that two-stage training induces noisy patterns and artifacts in the latent code due to the uncertain nature of inverse rendering, particularly when training from sparse-view data, which prevents the diffusion model from learning a clean latent manifold effectively. To address this issue, we introduce a novel single-stage training paradigm that enables end-to-end learning of diffusion and
NeRF weights (§ 4.1). This approach blends the generative and the rendering biases coherently for improved perfor-mance overall and allows for training on sparse-view data.
Additionally, we show that the learned 3D priors of uncon-ditional diffusion models can be exploited for flexible test-time scene sampling from arbitrary observations (§ 4.2).
We evaluate SSDNeRF on multiple datasets of categori-cal single-object scenes, demonstrating strong performance overall. Our approach represents a significant step towards a unified framework for various 3D tasks.
To summarize, our main contributions are as follows:
• We introduce SSDNeRF, a unified approach to all-round performance in unconditional 3D generation and image-based reconstruction;
• We propose a novel single-stage training paradigm that jointly learns NeRF reconstruction and diffusion model from multi-view images of a large number of objects. Notably, this enables training on as sparse as three views per scene, which is previously infeasible;
• A guidance-finetuning sampling scheme is developed to exploit the learned diffusion priors for 3D recon-struction from arbitrary number of views at test time. 2.