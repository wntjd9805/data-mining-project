Abstract
In text-video retrieval, recent works have beneﬁted from the powerful learning capabilities of pre-trained text-image foundation models (e.g., CLIP) by adapting them to the video domain. A critical problem for them is how to ef-fectively capture the rich semantics inside the video us-ing the image encoder of CLIP. To tackle this, state-of-the-art methods adopt complex cross-modal modeling tech-niques to fuse the text information into video frame rep-resentations, which, however, incurs severe efﬁciency is-sues in large-scale retrieval systems as the video repre-sentations must be recomputed online for every text query.
In this paper, we discard this problematic cross-modal fu-sion process and aim to learn semantically-enhanced rep-resentations purely from the video, so that the video rep-resentations can be computed ofﬂine and reused for differ-ent texts. Concretely, we ﬁrst introduce a spatial-temporal
“Prompt Cube” into the CLIP image encoder and itera-tively switch it within the encoder layers to efﬁciently in-corporate the global video semantics into frame represen-tations. We then propose to apply an auxiliary video cap-tioning objective to train the frame representations, which facilitates the learning of detailed video semantics by pro-viding ﬁne-grained guidance in the semantic space. With a naive temporal fusion strategy (i.e., mean-pooling) on the enhanced frame representations, we obtain state-of-the-art performances on three benchmark datasets, i.e., MSR-VTT,
MSVD, and LSMDC. 1.

Introduction
Text-video retrieval [1, 6, 30, 42] is a fundamental task in the area of video-language understanding that seeks to
ﬁnd the most relevant video from a large set of candidates to match a text query. With the rapid growth of video data, text-video retrieval has become increasingly impor-tant for various applications, including video recommen-dation [36, 43], video search [20, 45], and video summa-∗Co-ﬁrst author. †Corresponds to qi.wu01@adelaide.edu.au.
Code: https://github.com/bladewaltz1/PromptSwitch. (cid:1008)(cid:1012) (cid:1008)(cid:1011) (cid:1008)(cid:1010) (cid:1008)(cid:1009) (cid:1008)(cid:1008) (cid:1008)(cid:1007) (cid:1008)(cid:1006) (cid:68)(cid:94)(cid:90)(cid:115)(cid:100)(cid:100)(cid:3)(cid:90)(cid:923)(cid:1005) (cid:75)(cid:437)(cid:396)(cid:400) (cid:120) (cid:100)(cid:94)(cid:1006)(cid:882)(cid:69)(cid:286)(cid:410) (cid:18)(cid:286)(cid:374)(cid:410)(cid:286)(cid:396)(cid:18)(cid:62)(cid:47)(cid:87) (cid:18)(cid:62)(cid:47)(cid:87)(cid:1008)(cid:18)(cid:367)(cid:349)(cid:393) (cid:120) (cid:121)(cid:882)(cid:87)(cid:381)(cid:381)(cid:367) (cid:120) (cid:121)(cid:882)(cid:18)(cid:62)(cid:47)(cid:87) (cid:24)(cid:349)(cid:258)(cid:373)(cid:286)(cid:410)(cid:286)(cid:396) (cid:1005) (cid:1006) (cid:1008) (cid:1010)(cid:1008) (cid:39)(cid:17) (cid:1005) (cid:1006) (cid:1008) (cid:1012) (cid:1005)(cid:1010) (cid:400)
Figure 1: The performance (i.e., R@1), retrieval time, and memory usage during retrieval for baseline models and ours on the MSRVTT dataset. The center of the bubble indicates the value of R@1. The diameter of the bubble or star is proportional to the memory usage (GB) while the horizontal axis indicates the inference time (s). rization [7, 29, 31, 33, 37, 47]. Due to the high cost of constructing text-video datasets, one promising approach for this task is to leverage pre-trained text-image founda-tion models and transfer their powerful representation ca-pabilities to the video domain. Speciﬁcally, the CLIP [34] model, which is trained using a text-image alignment ob-jective, is particularly suitable for text-video retrieval and has been frequently studied recently [27, 46, 16]. It has two transformer encoders [38, 9] to process images and texts, re-spectively. A vector representation is extracted from the in-put of each modality with the corresponding encoder and is optimized to be close to its paired representation and away from the unpaired ones.
Adapting CLIP to the video domain is non-trivial and re-quires careful consideration of both efﬁciency and effective-ness. In CLIP4Clip [27], the authors directly mean-pool the frame representations extracted by the CLIP image encoder to get the video representation and use it to calculate the co-sine similarity with the text representations during retrieval.
However, the mean-pooling of frame representations may lose some essential semantic details of the video and ham-per the retrieval performance. Thus, more advanced meth-ods, such as [14, 16, 18, 24, 28], generate the video repre-sentation by applying various cross-modal temporal fusion approaches on the frame representations, using text queries as the condition. While achieving state-of-the-art results, these methods encounter severe efﬁciency issues in prac-tice, as the text-conditioned fusion of each video has to be performed on-the-ﬂy for every incoming text query. Even with a lightweight fusion module (compared to the CLIP backbone), its computation cost grows geometrically as the number of videos and texts increases.
Formally, given a query set of Nt texts with an average length of Nw words and a candidate set of Nv videos where each video contains Nf frames. Then, the space and time complexities are O(NvNtNf ) for the text-conditioned fu-sion in X-Pool [16] and TS2-Net [24], and O(NvNtNf Nw) for that of X-CLIP [28]. While for CLIP4Clip, the complex-ity is O(NvNt) as it only requires a simple dot-product be-tween the text and mean-pooled frame representations, al-though its performance is inferior to X-Pool and X-CLIP. To better reveal this gap, we show an example in Figure 1 about the real-world efﬁciency of several methods while omitting the backbone computation. Here, we set Nv = 16384,
Nt = 512, Nf = 12, and Nw = 10. From the ﬁgure, with large Nt and Nv, the latency and memory consumption for text-conditioned temporal fusion methods [16, 24, 28] are orders of magnitude higher than text-agnostic temporal fu-sion (i.e., mean-pooling) [27, 57], and can rapidly become enormous in large-scale scenarios.
On the other hand, the backbone computation of CLIP is much less of a burden in real-world retrieval systems, as the frame representations of the video can be pre-computed of-ﬂine and reused for different text queries. Therefore, a more practical CLIP-based text-video retrieval method should fo-cus on improving the backbone representation ability while keeping the cross-modal interaction as simple as possible.
Motivated by this, we propose a simple and efﬁcient adapta-tion method for CLIP to facilitate its ability to capture both the global and detailed semantics of videos.
Concretely, we ﬁrst feed a tiny (∼0.1M) “Prompt
Cube” into the image encoder of CLIP, which is a 3D ten-sor spanning over the spatial, temporal, and channel axis, as shown in the right of Figure 2.1 It is designed to have the same temporal and spatial sizes and is concatenated with the patch tokens alongside the spatial axis. To propagate temporal semantics among different frames, we switch the spatial and temporal dimensions of the prompt cube before each self-attention layer, so that the prompt cube builds up a peer-to-peer connection between every two-frame pair.
In this way, our modiﬁed CLIP model enjoys an improved global semantic modeling ability thanks to the comprehen-sive spatial-temporal modeling between the prompt cube and the patch tokens of all frames, while only bringing neg-1The channel axis is omitted for simplicity. ligible extra parameters and computations. This also allows the prompt cube to serve as a compact summarization of the whole video, and further enables us to design a CLIP-guided Prompt Aggregation approach and obtain the frame representations from the prompt cube. Then, we use naive mean-pooling instead of cross-modal fusion on these frame representations to get the ﬁnal video representation.
Moreover, since we will not use any ﬁne-grained cross-modal interaction modules in our model, we adopt an Auxil-iary Video Captioning objective as an alternative to provide
ﬁne-grained guidance in the semantic space when learn-ing video representations. Speciﬁcally, we introduce a light captioning head on top of our modiﬁed CLIP image encoder during training, which takes the frame representations ag-gregated from the prompt cube as input and generates the paired text of the input video. This auxiliary objective plays a critical role because CLIP’s original contrastive learning objective is relatively easy to ﬁt due to the lack of in-batch negatives during training (which is generally the case in text-video retrieval). During inference, the light caption-ing branch is removed, thus it incurs no extra computation and memory consumption.
We verify the effectiveness of the proposed method on three text-video retrieval benchmarks, i.e. MSR-VTT [50],
MSVD [4], and LSMDC [35], where our method con-sistently achieves state-of-the-art performance while being signiﬁcantly more efﬁcient than the previous state of the arts. We also provide extensive performance analyses to show the superiority of our proposed method. 2.