Abstract
In this paper, we address the problem of generalized cat-egory discovery (GCD), i.e., given a set of images where part of them are labelled and the rest are not, the task is to automatically cluster the images in the unlabelled data, leveraging the information from the labelled data, while the unlabelled data contain images from the labelled classes and also new ones. GCD is similar to semi-supervised learning (SSL) but is more realistic and challenging, as SSL assumes all the unlabelled images are from the same classes as the labelled ones. We also do not assume the class number in the unlabelled data is known a-priori, making the GCD problem even harder. To tackle the problem of GCD without know-ing the class number, we propose an EM-like framework that alternates between representation learning and class number estimation. We propose a semi-supervised variant of the Gaussian Mixture Model (GMM) with a stochastic splitting and merging mechanism to dynamically determine the prototypes by examining the cluster compactness and separability. With these prototypes, we leverage prototypi-cal contrastive learning for representation learning on the partially labelled data subject to the constraints imposed by the labelled data. Our framework alternates between these two steps until convergence. The cluster assignment for an unlabelled instance can then be retrieved by identify-ing its nearest prototype. We comprehensively evaluate our framework on both generic image classiﬁcation datasets and challenging ﬁne-grained object recognition datasets, achiev-ing state-of-the-art performance. Our code is available at https://github.com/DTennant/GPC. 1.

Introduction
The success of deep learning is driven by the availability of large-scale data with human annotations. Given enough annotated data, deep learning models are able to surpass human-level performance on many important computer vi-*Corresponding author.
Image embedding
E-step 128D
PCA 768D
ViT
Image patches
-----------------------------------------------------------------------------------M-step
+
+
+
-+
+
+
Figure 1: Overview of our proposed EM-like framework.
The input images are fed into a ViT-B model to obtain a 768-dimensional feature vector, then the feature vector will be projected to a lower dimensional space using the projection calculated from PCA. We perform class number estimation and representation learning in this projected space. In the
E-step, we use a semi-supervised GMM that can split sep-arable clusters and merge cluttered clusters to estimate the class number and prototypes, which will be used in the M-step of representation learning with prototypical contrastive learning. sion tasks such as image classiﬁcation [21]. But the cost of collecting a large annotated dataset is not always afford-able, and it is also not possible to annotate all new classes emerging from the real world. Thus, designing models that can learn to deal with large-scale unlabelled data in the open world is of great value and importance. Semi-supervised learning (SSL) [38] is proposed as a solution to learn a model on both labelled data and unlabelled data, with many works achieving promising performance [1, 46, 44]. How-ever, SSL assumes that labelled instances are provided for all object classes in the unlabelled data. The novel category discovery (NCD) task is introduced [17, 16] to automati-cally discover novel classes by transferring the knowledge learned from the labelled instances of known classes, as-suming the unlabelled data only contain instances from new classes. Generalized category discovery (GCD) [48] fur-ther relaxes the assumption in NCD, and tackles a more
generalized setting where the unlabelled data contains in-stances from both known and novel categories. Exist-ing methods for NCD [17, 16, 58, 60, 61, 12, 26] and
GCD [48, 11, 51, 45, 56, 59, 34] learn the representation and cluster assignment assuming the class number is known a priori [58, 26, 60, 12, 61] or precomputed [17, 48]. In practice, the number of categories in the unlabelled data is often unknown, while precomputing the class number with-out taking the representation learning into consideration is likely to lead to a sub-optimal solution.
In this paper, we argue that representation learning and the estimation of class numbers should be considered together and could reinforce each other, i.e., a strong representation could help a more accurate estimation of the class numbers, and an accurate class number could help learn a better feature representation. To this end, we propose a uniﬁed EM-like framework that alternates between feature representation learning and class number estimation where the E-step is aimed at automatically estimating a proper class number and a set of class prototypes in the unlabelled data and the M-step is aimed at learning better representation with the class number and class prototypes estimated. In particular, we propose using a prototype contrastive representation learn-ing [33] method for GCD, which requires a set of prototypes to serve as anchors for representation learning. Prototypical contrastive learning [33] is developed for unsupervised rep-resentation learning to generalize to different tasks, where the prototypes are obtained by over-clustering the dataset with one or multiple given prototype numbers, using non-parametric clustering algorithms like k-means. Instead, to handle the problem of GCD, we propose to estimate the prototype number and prototypes automatically and simulta-neously. To do so, we introduce a semi-supervised variant of the Gaussian Mixture Model (GMM) with a stochastic splitting and merging mechanism to determine the most suit-able clusters based on current representation. These clusters can then be used to form prototypes to facilitate contrastive representation learning. Our framework alternates between the E- and M-step until converging to achieve robust rep-resentation and reliable category estimation. After learn-ing, the cluster assignment for an unlabelled instance, either from known or novel classes, can be retrieved by ﬁnding the nearest prototypes. Thus we name our framework as GPC:
Gaussian mixture model for generalized category discovery with Protypical Contrastive learning.
Our contributions in this paper are as follows: (1) We demonstrate that in generalized category discovery, the class number estimation and representation learning can reinforce each other in the learning process. Strong representations can give a better estimation of the class number, and vice (2) We propose an EM-like framework that alter-versa. nates between prototype estimation with a variant of GMM (E-step) and representation learning based on prototypical (3) We introduce a semi-contrastive learning (M-step). supervised variant of GMM with a stochastic splitting and merging mechanism to allow dynamic change of the proto-types by examining the cluster compactness and separability based on the Metropolis-Hastings ratio [19]. (4) We com-prehensively evaluated our framework on both the generic image classiﬁcation benchmark, including CIFAR10, CI-FAR100, ImageNet-100, and the challenging ﬁne-grained
Semantic Shifts Benchmark suite, which includes CUB-200,
Stanford-Cars, and FGVC-aircrafts, achieving the state-of-the-art results. 2.