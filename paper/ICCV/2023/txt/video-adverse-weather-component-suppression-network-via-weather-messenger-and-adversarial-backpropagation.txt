Abstract
Although convolutional neural networks (CNNs) have been proposed to remove adverse weather conditions in sin-gle images using a single set of pre-trained weights, they fail to restore weather videos due to the absence of tempo-ral information. Furthermore, existing methods for remov-ing adverse weather conditions (e.g., rain, fog, and snow) from videos can only handle one type of adverse weather.
In this work, we propose the first framework for restoring videos from all adverse weather conditions by developing a video adverse-weather-component suppression network (ViWS-Net). To achieve this, we first devise a weather-agnostic video transformer encoder with multiple trans-former stages. Moreover, we design a long short-term tem-poral modeling mechanism for weather messenger to early fuse input adjacent video frames and learn weather-specific information. We further introduce a weather discrimi-nator with gradient reversion, to maintain the weather-invariant common information and suppress the weather-specific information in pixel features, by adversarially pre-dicting weather types. Finally, we develop a messenger-driven video transformer decoder to retrieve the residual weather-specific feature, which is spatiotemporally aggre-gated with hierarchical pixel features and refined to pre-dict the clean target frame of input videos. Experimen-tal results, on benchmark datasets and real-world weather videos, demonstrate that our ViWS-Net outperforms cur-rent state-of-the-art methods in terms of restoring videos degraded by any weather condition. 1.

Introduction
Adverse weather conditions (including rain, fog and snow) often degrade the performance of outdoor vision
†Lei Zhu (leizhu@ust.hk) is the corresponding author. systems, such as autonomous driving and traffic surveil-lance, by reducing environment visibility and corrupting image/video content. Removing these adverse weather ef-fects is challenging yet a promising task. While many video dehazing/deraining/desnowing methods have been proposed, they mainly address one type of weather degra-dation. As they require multiple models and sets of weights for all adverse weather conditions, resulting in expensive memory and computational costs, they are unsuitable for real-time systems. Additionally, the system would have to switch between a series of weather removal algorithms, making the pipeline more complicated and less practical for real-time systems.
Recently, Li et al. [18] proposed an All-in-One bad weather removal network that can remove any weather con-dition from an image, making it the first algorithm to pro-vide a generic solution for adverse weather removal. Fol-lowing this problem setting, several single-image multi-adverse-weather removal methods [8, 38] have been devel-oped to remove the degradation effects by one model in-stance of a single encoder and single decoder. While sig-nificant progress has been witnessed for the single-image multi-adverse-weather removal task, we believe that video-level algorithms can achieve better results by utilizing the temporal redundancy from neighboring frames to reduce the inherent ill-posedness in restoration tasks.
Therefore, a generic framework that can transform an image-level algorithm into its video-level counterpart is highly valuable. However, two bottlenecks need to be ad-dressed: 1) how to effectively maintain the temporal co-herence of background details across video frames, and 2) how to prevent the perturbation of multiple kinds of weather across video frames.
To tackle the aforementioned bottlenecks, we present the
Video Adverse-Weather-Component Suppression Network (ViWS-Net), the first video-level algorithm that can re-move all adverse weather conditions with only one set of 1
pre-trained weights. Specifically, we introduce Temporally-active Weather Messenger tokens to learn weather-specific information across video frames and retrieve them in our messenger-driven video transformer decoder. We also de-sign a Long Short-term Temporal Modeling mechanism for weather messenger tokens to provide early fusion among frames, and support recovery with temporal dependences of different time spans. To impede the negative effects of mul-tiple adverse weather conditions on background recovery, we develop a Weather-Suppression Adversarial Learning by introducing a weather discriminator. Adversarial backprop-agation is adopted, between the video transformer encoder and the discriminator, by gradient reversion to maintain the common background information and simultaneously sup-press the weather-specific information in hierarchical pixel features. Since there has been no public dataset for video desnowing, we synthesize the first video-level snow dataset, named KITTI-snow, which is based on KITTI [22]. We conduct extensive experiments on video deraining, dehaz-ing, and desnowing benchmark datasets, including RainMo-tion [39], REVIDE [49], and KITTI-snow, as well as several real-world weather videos, to validate the effectiveness and generalization of our framework for video multiple adverse weather removal. Our contributions can be summarized as follows:
• We propose a novel unified framework, ViWS-Net, that addresses the problem of recovering video frames from multiple types of adverse weather degradation with a single set of pre-trained weights.
• We introduce temporally-active weather messenger to-kens that provide early temporal fusion and help re-trieving the residual weather-specific information for consistent removal of weather corruptions.
• We design a weather-suppression adversarial learning approach that maintains weather-invariant background information and suppresses weather-specific informa-tion, thereby preventing recovery from the perturba-tion of various weather types.
• To evaluate our framework under multiple adverse weather conditions, we synthesize a video-level snow
Our extensive experiments dataset KITTI-snow. on three benchmark datasets and real-world videos demonstrate the effectiveness and generalization abil-ity of ViWS-Net. Our code is publicly available at https://github.com/scott-yjyang/ViWS-Net. 2.