Abstract
In this paper, we address the problem of conditional scene decoration for 360◦images. Our method takes a 360◦background photograph of an indoor scene and gener-ates decorated images of the same scene in the panorama view. To do this, we develop a 360-aware object layout generator that learns latent object vectors in the 360◦view to enable a variety of furniture arrangements for an input 360◦background image. We use this object layout to condi-tion a generative adversarial network to synthesize images of an input scene. To further reinforce the generation capability of our model, we develop a simple yet effective scene emp-tier that removes the generated furniture and produces an emptied scene for our model to learn a cyclic constraint. We train the model on the Structure3D dataset and show that our model can generate diverse decorations with controllable object layout. Our method achieves state-of-the-art perfor-mance on the Structure3D dataset and generalizes well to the Zillow indoor scene dataset. Our user study confirms the immersive experiences provided by the realistic image quality and furniture layout in our generation results. Our implementation is available at https://github.com/ kcshum/neural_360_decoration.git. 1.

Introduction
Panoramas (360◦images) enable immersive user experi-ences and have been applied intensively to various virtual reality (VR) applications [1, 4, 44]. However, automated generation of indoor scenes in the 360◦view for architec-tural and interior design remains understudied due to many challenges. First, the generation process must conform the common distortions in the 360◦view. Second, generated content must be controllable.
Common generative models, e.g., StyleGAN [22, 23] can generate photorealistic images. However, these methods are unconditional generation techniques, i.e., an output image is generated from a random code sampled in a latent space without interpreted meaning, thus limiting content controlla-bility. Existing conditional image synthesis techniques, e.g., image-to-image translation [18, 65, 54, 50], on the other hand, do not have explicit support for scene representations and thus have limited capability for scene manipulation.
In this work, we focus on conditional image synthesis of 360◦indoor scenes. We are inspired by the neural scene dec-oration (NSD) in [38], aiming to generate a decorated scene image from a given background image and user-defined fur-niture arrangement. However, the NSD method in [38] has several limitations. First, it requires an object layout model-ing furniture arrangement from users, making the generation process not fully automatic. Second, its object layout, repre-sented by rectangles, is not applicable in the 360◦view using equirectangular projection [47]. Third, there is no mecha-nism to control different attributes of the generated furniture, limiting the diversity of the generated content.
We instead take a different approach for scene represen-tation and propose a conditional image synthesis method for automatic scene decoration in the 360◦setting. We first develop a 360-aware object layout generator that learns a set of object vectors representing the furniture arrangement of a 360◦scene. We use this layout as the latent represen-tation in a generative adversarial network to condition the generated content. To support the training of the layout and generative adversarial network, we devise a scene emptier that performs a dual task, i.e., making a decorated scene empty. In summary, we make the following contributions in our work.
• A 360-aware object layout generator that automatically learns an object arrangement from a 360◦background im-age. Generated layouts condition the scene decoration in the 360◦viewer;
• A novel framework that synthesizes diverse and control-lable scene decorations in the 360◦setting;
• A scene emptier for reinforcement of the conditioning ability and generation ability in the training;
• Extensive experiments and user studies on benchmark datasets including the Structured3D [63] and Zillow In-door dataset [9] to validate our method and to provide immersive experiences to users. 2.