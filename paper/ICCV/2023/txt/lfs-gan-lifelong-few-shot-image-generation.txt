Abstract
We address a challenging lifelong few-shot image gen-eration task for the first time.
In this situation, a gener-ative model learns a sequence of tasks using only a few samples per task. Consequently, the learned model encoun-ters both catastrophic forgetting and overfitting problems at a time. Existing studies on lifelong GANs have pro-posed modulation-based methods to prevent catastrophic forgetting. However, they require considerable additional parameters and cannot generate high-fidelity and diverse images from limited data. On the other hand, the existing few-shot GANs suffer from severe catastrophic forgetting when learning multiple tasks. To alleviate these issues, we propose a framework called Lifelong Few-Shot GAN (LFS-GAN) that can generate high-quality and diverse images in lifelong few-shot image generation task. Our proposed framework learns each task using an efficient task-specific modulator - Learnable Factorized Tensor (LeFT). LeFT is rank-constrained and has a rich representation ability due to its unique reconstruction technique. Furthermore, we propose a novel mode seeking loss to improve the diversity of our model in low-data circumstances. Extensive experi-ments demonstrate that the proposed LFS-GAN can gener-ate high-fidelity and diverse images without any forgetting and mode collapse in various domains, achieving state-of-the-art in lifelong few-shot image generation task. Surpris-ingly, we find that our LFS-GAN even outperforms the ex-isting few-shot GANs in the few-shot image generation task.
The code is available at Github. 1.

Introduction
Deep learning has achieved remarkable success in recent years, particularly in a single task learning on a large dataset such as ImageNet [9] or FFHQ [24]. However, obtaining a large amount of refined data for real-world applications is
*Equal contribution
†Corresponding author prohibitively expensive, and there are many domains where only limited data can be collected, such as the artistic do-main. Additionally, when faced with long sequences of tasks over time, it is inevitable to train a new model for each target task.
In this context, many studies have recently highlighted the importance of lifelong few-shot learning. Lifelong few-shot learning combines two challenging settings: lifelong learning and few-shot learning. It seeks to overcome catas-trophic forgetting [32, 12, 26] when learning a sequence of tasks over time, while learning from limited data without any overfitting problem. By integrating the concepts of life-long and few-shot learning, lifelong few-shot learning holds great potential for real-world applications where data is lim-ited or costly to obtain, and where learning a new model for each task is not practical.
Previous studies on lifelong few-shot learning have pri-marily focused on discriminative tasks [1, 31, 21, 38, 37, 39]. However, lifelong few-shot learning on generative tasks is unexplored before. Lifelong few-shot image gen-eration task involves training a model to generate realis-tic and diverse images from handful training images, while continually learning new tasks and preserving the ability to generate images from the previous domains (see Figure 1).
There are two key challenges in this setting. First, since the model learns tasks sequentially, it easily forgets the ability to generate samples of the previous tasks. Second, since the model learns from a biased and sparse distribution, it suffers from the mode collapse problem [2], i.e., it is prone to re-generate the same training samples or produce similar images regardless of the noises provided. Addressing these challenges is critical to enable the successful application of lifelong few-shot image generation in the real-world, where data is limited or costly to obtain.
To alleviate catastrophic forgetting, recent studies in the field of lifelong image generation have proposed a weight modulator inspired by the affine transformation [20], which enables generative models to learn task-specific informa-Figure 1: Illustration of our proposed lifelong few-shot image generation task. We construct a sequence of few-shot tasks. In the training phase, the model learns each task from the pretrained model. By updating only task-specific parameters applied to the model, the model can learn the current task without forgetting. In the inference phase, we can generate high-fidelity and diverse images of not only the current task but also the previous tasks by adopting task-specific parameters to the model. tion in lifelong setting [47, 8, 42, 40]. However, conven-tional generative models [22, 24, 25, 23] consist of con-volutional layers with high-dimensional weights, resulting in a significant increase of the number of parameters re-quired to modulate them. As the number of tasks increases, the memory required to store these parameters becomes a significant burden. Furthermore, since lifelong generative models are designed to synthesize decent images from suf-ficient data, they suffer from mode collapse with handful training images. In the realm of few-shot image generation task, many works propose regularization-based methods to maintain the rich diversity of source models [28, 35, 45, 52].
However, they are prone to forget how to generate the previ-ous tasks when learning a new task, since they fine-tune the models to learn the new task. A recent study [51] has intro-duced a modulation-based approach. This method divides the weights into be modulated and fine-tuned parts, based on their significance. Although this work adopts a modula-tion technique, it suffers from catastrophic forgetting due to its fine-tuning part.
To address these challenges, we propose a novel frame-work Lifelong Few-Shot Generative Adversarial Network (LFS-GAN). Our LFS-GAN learns a new task via a power-ful weight modulation technique called the Learnable Fac-torized Tensor (LeFT) that captures task-specific knowledge with low memory costs while freezing learned weights from the source task. Our proposed LeFT reduces the mem-ory burden by decomposing the weight tensor and restor-ing it during the forward operation to modulate the weight.
This method enables the efficient and effective generation of high-quality images. Furthermore, we propose a cluster-wise mode seeking loss to improve the diversity of gener-ated images. The mode seeking loss [30] has shown its ef-fect to diversify the generated images of GANs. However, in a low-data circumstance, a simple application of mode seeking loss shows less effect because the generated images tend to be similar to the training images. Thus, we alter the mode seeking loss to be effective in our task and achieve greater diversity. Lastly, we find that the intra-cluster LPIPS cannot capture the imbalanced generation with respect to training images. To resolve this issue, we propose a novel diversity measure called Balanced Inter- and Intra-cluster
LPIPS (B-LPIPS) to accurately evaluate generation diver-sity in our task.
Our main contributions can be summarized as follows:
• To the best of our knowledge, we formulate and tackle a challenging lifelong few-shot image generation task for the first time.
• We introduce a novel weight modulation technique, called Learnable Factorized Tensor (LeFT), which en-ables the generative model to learn new tasks without forgetting and significant parameter growth.
• To enhance diversity in the generated images, we pro-pose a cluster-wise mode seeking loss that maximizes the relative distances of intermediate latent codes, fea-ture maps, and images.
• Extensive experiments, including our novel metric B-LPIPS, demonstrate that LFS-GAN outperforms the current state-of-the-art methods on generating high-quality and diverse images not only in lifelong few-shot image generation task but also in few-shot image generation task. 2.