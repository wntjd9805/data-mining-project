Abstract
A key challenge in neural 3D scene reconstruction from monocular images is to fuse features back projected from various views without any depth or occlusion informa-tion. We address this by leveraging monocular depth pri-ors, which effectively guide the fusion to improve sur-face prediction and skip over irrelevant, ambiguous, or occluded features. Furthermore, we revisit the average-based fusion used by most neural 3D reconstruction meth-ods and propose two alternatives, a variance-based and a cross-attention-based fusion module, that are more efficient and effective than the average-based and self-attention-based counterparts. Compared to the NeuralRecon base-line, the proposed DG-Recon models significantly improve the reconstruction quality and completeness while remain-ing in real-time. Our method achieves state-of-the-art on-line reconstruction results on the ScanNet dataset and is on par with the current best offline method, which repeat-edly accesses keyframes from the entire video sequence.
Our ScanNet-trained model also generalizes robustly to the challenging 7-Scenes dataset and a subset of SUN3D con-taining scenes as big as an entire floor.
Figure 1. Depth-guided back projection and fusion. Each grid on the left figure represents a digitized 2D world. The orange rounded rectangle represents a table. The blue circle sketches a chair and the grey rectangle indicates the wall. Non-white cells picture back-projected features from K different camera views
O(1), . . . , O(K). A cell with a red cross in it indicates an erro-neous back projection. The depth priors, denoted as the white sur-face curve in the middle row, introduce geometry awareness to the back projection and cross-view fusion. Objects, e.g. chairs and the monitor, become sharper, more complete, and better separated. 1.

Introduction
Reconstruction of 3D scenes is a fundamental problem in 3D perception of environments, constituting a crucial component of various application domains ranging from robotics and autonomous vehicles to augmented and vir-tual reality. For instance, in the augmented/virtual reality use case, not only the accuracy of the reconstructed meshes but also the runtime efficiency is important in enabling real-time safe user navigation, successful occlusion rendering, and plausible physical simulations on edge devices.
Most traditional 3D scene reconstruction pipelines con-sist of dense depth prediction and a multi-view depth in-tegration process [31, 6] to create truncated signed dis-tance function (TSDF) as a geometrical representation that enables mesh extraction using the marching cubes algo-rithm [26]. While such processes are simple and intuitive, the non-learnable fusion part is unable to effectively in-corporate higher-level inductive biases to handle inconsis-tent and/or noisy depth estimations from different views.
On the other hand, a newly emerging category of meth-ods [30, 42, 1, 41] aims at learning to directly predict the
TSDF by back projecting representations from posed im-ages and then fusing them into volumetric representations of the underlying scenes. These neural methods are in prac-tice either found to be suffering from lower accuracy and incomplete geometries [42], or too costly for real-world and real-time use cases on edge-devices [30, 1, 41].
Such undesirable properties can be attributed to bottle-necks in important components, i.e. feature back projection, feature fusion, and occupancy prediction. More specifi-cally, existing neural reconstruction methods back project image features all along the rays into the volumetric repre-sentations resulting in 1) non-sparse representations and 2)
potential erroneous feature association on the occluded ob-jects. Besides, the feature fusion is either based on simple averaging [42, 30] that is ineffective in properly modeling the multi-view consensus or is based on the self-attention mechanism [1] that is inefficient as it scales quadratically with the number of views.
In this work, we build our method upon the most scal-able algorithm of the 3D volumetric reconstruction cate-gory, NeuralRecon [42], and revisit feature back projection and fusion with the help of depth priors. The depth-guided back projection reduces erroneous feature associations with occluded objects and introduces sparse representations even before fusion, as illustrated in Figure 1. Early availability of sparsity enables the choice of more expressive representa-tion aggregation schemes without significant computational costs. The proposed variance- and cross-attention-based fu-sions are both more effective than the average-based fusion and more efficient than the self-attention-based fusion. Fi-nally, the depth prior also helps improve the reconstruction completeness over the baseline [42] by replacing the overfit-ted occupancy prediction with the depth-derived occupancy mapping. To summarize, our major contributions are:
• We propose to integrate depth priors to the feature back projection and occupancy prediction component in 3D volumetric scene reconstruction methods, which improves cross-view feature association and creates sparse volumetric representation before fusion.
• We formulate and propose two simple and scalable sur-rogate feature fusion schemes, the variance and cross-attention, that are shown to be effective and efficient as compared to the formulations commonly used.
• Our comprehensive empirical evaluations on Scan-Net [5], 7-Scenes [17] and SUN3D [50] show that our proposed method is the new state-of-the-art in 3D scene reconstruction considering the accuracy-efficiency tradeoff. 2.