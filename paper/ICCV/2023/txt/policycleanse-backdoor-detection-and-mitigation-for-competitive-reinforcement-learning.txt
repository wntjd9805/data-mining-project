Abstract
While real-world applications of reinforcement learning (RL) are becoming popular, the security and robustness of
RL systems are worthy of more attention and exploration. In particular, recent works have revealed that, in a multi-agent
RL environment, backdoor trigger actions can be injected into a victim agent (a.k.a. Trojan agent), which can result in a catastrophic failure as soon as it sees the backdoor trigger action. To ensure the security of RL agents against malicious backdoors, in this work, we propose the problem of Back-door Detection in a multi-agent competitive reinforcement learning system, with the objective of detecting Trojan agents as well as the corresponding potential trigger actions, and further trying to mitigate their Trojan behavior. In order to solve this problem, we propose PolicyCleanse that is based on the property that the activated Trojan agent’s accu-mulated rewards degrade noticeably after several timesteps.
Along with PolicyCleanse, we also design a machine unlearning-based approach that can effectively mitigate the detected backdoor. Extensive experiments demonstrate that the proposed methods can accurately detect Trojan agents, and outperform existing backdoor mitigation baseline ap-proaches by at least 3% in winning rate across various types of agents and environments. 1.

Introduction
Reinforcement Learning (RL) is proposed to train smart agents to take actions that can help them to acquire maxi-mum accumulative rewards in a given environment. Such incentive-driven properties make people believe RL can learn general human-level intelligent agents [34], and in recent years, RL has demonstrated its effectiveness in various appli-cations and fields such as computer vision [2, 18, 36], game playing [37], robotics technology [28], and traffic control
[30]. Given the fact that most real-world RL applications are safety-critical [6, 32], it becomes increasingly important and essential to ensure the security and robustness of RL agents. Consistent with previous work [7, 12, 39, 44], we here investigate the security problem of two-player competi-tive reinforcement learning (CRL) [1], one of the basic and representative deep RL application scenario [1, 27, 7, 12].
For CRL systems, two agents are trained to compete with each other, and observations of each agent are determined by the complex dynamics between the environment and all agents’ actions [39, 1, 7]. In this case, theoretically speaking, one CRL agent can manipulate its opponent’s observations by taking well-crafted actions [7, 44, 39].
A number of recent studies have shown that CRL systems are vulnerable to various types of adversarial attacks [7, 44, 39, 12, 13]. One of the most representative attacks is the backdoor attack (e.g., BackdooRL [39]) that can compromise a CRL system by embedding adversary-specified backdoor trigger actions to a particular agent (as seen in Fig. 1). More specifically, in the training phase, BackdooRL embeds a sequence of trigger actions into a victim agent, which we call the Trojan agent. Then during inference, if an agent is compromised to take inconspicuous trigger actions, the
Trojan agent fails as soon as it observes such trigger actions.
Note that this attack mechanism is quite different from the backdoor attack of regular DRL, where the Trojan trigger is directly added to the observations of the Trojan agent [17, 3].
To better ensure the security of CRL, in this work, we con-sider a problem named Backdoor Detection in CRL, which aims at detecting and mitigating the potential backdoor risk associated with a pre-trained RL agent. The problem is much more challenging than the case of conventional RL as the complexity of dynamics between the agents and the envi-ronment in multi-agent scenarios is too high to model and analyze. What’s more, unlike the backdoor detection prob-lem in supervised learning [10, 38, 11, 23, 5], the backdoor
Figure 1. An illustration of backdoor attacks in a competitive reinforcement learning game. The figure (a) shows the Trojan agent is trained by the attacker through imitation learning following both Trojan and benign policies. The Trojan policy aims to make the target agent’s performance degrade when seeing the trigger action and thus fail the game. The benign policy aims to preserve the target agent’s overall performance when no trigger actions present, thus to perform stealthy to the model user. The red humanoid is the trigger agent and the blue humanoid is the victim (Trojan agent with an injected backdoor). In the inference phase (figure (b)), when no trigger action is performed by the red humanoid, the blue humanoid wins the game (first row). However, when the red humanoid performs the trigger actions, the blue humanoid would fall immediately (second row). More details can be found in our open-sourced videos. trigger in CRL is shaped as a sequence of continuous actions with an unknown length, which heavily increases the search space of trigger localization and reconstruction.
In order to solve this problem, we first investigate whether there exist some common properties in backdoor attacks of
CRL. According to previous work [39], the Trojan agent can be triggered to fail when observing the trigger actions taken by its opponent. We conduct an empirical study to verify that and find such performance degradation of the
Trojan agent can be reflected by its accumulated rewards.
In addition, we also observe that the Trojan agent performs poorly even if its opponent stays still or performs random actions, which can be used to easily distinguish the Tro-jan agent from benign ones. Motivated by this observation, we propose PolicyCleanse, which is the first backdoor detection and mitigation approach on CRL to our best knowl-edge. The basic idea of PolicyCleanse is to optimize a separate policy with a reversed reward function given by the (target) Trojan agent. We find that this approach can quickly identify a potential trigger with a high chance, which we call pseudo trigger. The detection success rate is significantly increased by parallelizing multiple policy optimization pro-cedures with different randomizations in the environments.
Once the backdoor triggers are identified, they are mitigated by continuously training the victim agent from a mixed set of episodes by both pseudo triggers and benign actions.
Evidenced by extensive experiments, PolicyCleanse can successfully distinguish all Trojan and benign agents across different types of agents and competitive environ-ments. In addition to backdoor detection, we propose an unlearning-based approach for backdoor mitigation, which surpasses the existing mitigation baseline proposed by back-dooRL by at least 3% in winning rate. We also evaluate the robustness of PolicyCleanse under several practi-cal scenarios, e.g., dynamic trigger lengths, environment randomization, etc.
Contributions. We summarize our contributions as below 1. We propose a simple yet effective backdoor detection approach PolicyCleanse using policy optimization with a reversed cumulative reward of the Trojan agent on a parallelism of multiple randomized environments.
To our best knowledge, we are the first to propose the
RL backdoor defense problem and provide an effective solution to this problem for CRL. 2. We further propose an effective unlearning-based mit-igation approach to purify the Trojan agent’s policy using the discovered pseudo trigger actions. 3. Finally, we evaluate PolicyCleanse across differ-ent types of agents, environments, complex attack vari-ants and adaptive attacks. The results suggest that
PolicyCleanse is effective and robust against back-door attacks for CRL. 2.