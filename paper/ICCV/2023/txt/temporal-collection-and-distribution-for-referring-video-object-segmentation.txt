Abstract
Referring video object segmentation aims to segment a referent throughout a video sequence according to a nat-ural language expression. It requires aligning the natural language expression with the objects’ motions and their dy-namic associations at the global video level but segmenting objects at the frame level. To achieve this goal, we propose to simultaneously maintain a global referent token and a sequence of object queries, where the former is responsi-ble for capturing video-level referent according to the lan-guage expression, while the latter serves to better locate and segment objects with each frame. Furthermore, to ex-plicitly capture object motions and spatial-temporal cross-modal reasoning over objects, we propose a novel temporal collection-distribution mechanism for interacting between the global referent token and object queries. Specifically, the temporal collection mechanism collects global informa-tion for the referent token from object queries to the tempo-ral motions to the language expression. In turn, the tempo-ral distribution first distributes the referent token to the ref-erent sequence across all frames and then performs efficient cross-frame reasoning between the referent sequence and object queries in every frame. Experimental results show that our method outperforms state-of-the-art methods on all benchmarks consistently and significantly. 1.

Introduction
Referring video object segmentation task (RVOS) aims to segment the target referent throughout a video sequence given a natural language expression [22]. It has attracted increasing attention from the academic community, as it is a basis for assessing a comprehensive understanding of visual, temporal and linguistic information. Mean-while, ROVS benefits various downstream interactive ap-plications such as language-driven human-robot interac-tion [39], video editing [25] and video surveillance [44].
†Sibei Yang is the corresponding author.
Figure 1. The comparison of (a) previous best-performing Refer-Former, (b) video-level decoding, and (c) ours referent-guided dy-namic temporal decoding.
Compared to referring image segmentation [12,62] that seg-ments the target object in a single image mainly according to appearances and spatial and semantic relations, ROVS re-quires locating referents with temporal consistency accord-ing to object motions and dynamic associations among ob-jects across frames.
Previous works mainly follow a single-stage bottom-up manner [9, 15, 38, 47, 48] or a two-stage top-down fash-ion [28]. The bottom-up methods early align the vision and language at the local patch level, failing to explicitly model objects and their relations [28, 53]. Although top-down approaches explicitly extract object tracklets and se-lect object tracklets matched with the language expression, their two-stage pipeline is complex and less efficient [53].
Recently, transformer-based frameworks [1, 53] have been proposed to use object queries to capture objects and their associations in an end-to-end manner. They generate ob-ject queries and utilize the transformer decoder to search objects for these object queries in every frame, as shown in
Figure 2. Comparison of temporal modeling for previous works, naive attempt, and ours. Previous works (MTTR and ReferFormer) lack temporal modeling when decoding. Besides, object motion and interaction with other objects are crucial for referring video object segmentation. However, simply modeling temporal interaction with globally visible video-level decoding cannot achieve satisfactory results. Note that our alternating interaction between the global referent and object queries can dynamically capture spatial-temporal cross-modal reasoning over objects guided by the global referent information.
Figure 1a and b.
Although these end-to-end transformer-based frame-works have achieved impressive segmentation results, they still have limitations needed to be improved. First, their object queries are frame-level and independently responsi-ble for the search for objects in each frame, which fails to capture temporal object motions and effective object track-ing, as shown in Figure 2a and b. Therefore, we observe that they often fail to obtain temporally consistent predic-tions for target referents across an entire video, as shown in the left example in Figure 2. Second, their interactions between object queries are also solely performed per frame, leading to failure to model object-level spatial-temporal as-sociations between objects. Therefore, they cannot cor-rectly ground expressions that require cross-modal spatial-temporal reasoning over multiple objects. For example (see the right example in Figure 2), they mislocate the left camel as the target object because they lack to model the relation between the left camel in the current frame and the camel walking in the subsequent frame.
In this paper, we aim to address these limitations by em-powering the decoder with the capability of temporal mod-eling. A straightforward approach to decode objects with temporal information is to convert the frame-level decoding into video-level by inputting a sequence of object queries to search for objects in a sequence of frames like VisTR [49] for video instance segmentation, as shown in Figure 1b and 2e. However, this simple attempt fails to achieve satisfac-tory results while significantly increasing the computational cost and even underperforming existing query-based trans-former frameworks. We further analyze the reasons: (1)
The alignment between the natural language expression and the referent relies on overall objects’ motions and tempo-ral associations in the entire video. For example, the two camels can be distinguished only if the walking motion of the target camel is identified based on the entire video and aligned with the language expression. In contrast, the above naive attempt prematurely aligns language with fine-grained frame-level objects so that its attention is distracted from focusing accurately on the overall motions and relations of objects at the video level. (2) However, the precise local-ization and segmentation of target objects should go back to and rely more on every single frame because objects with different motions may cause them to have very different spatial locations in different frames. A similar observation is discussed by SeqFormer [54] for video instance segmen-tation, which suggests processing attention with each frame independently.
Therefore, to address the challenge of aligning the lan-guage expression with objects’ motions and temporal asso-ciations at the global video level but segmenting objects at the local frame level, we propose to maintain both local ob-ject queries and a global referent token. Global referent to-ken captures the video-level referent information according to the language expression, while local object queries locate and segment objects with each frame. Furthermore, the ob-ject queries and the referent token are interacted to achieve spatial-temporal object information exchange through our well-designed temporal collection and distribution mech-anisms, as shown in Figure 2f. Specifically, the tempo-ral collection collects the referent information from object queries with spatial and visual attributes to temporal ob-ject motions to language semantics. In turn, the temporal distribution first dynamically distributes the referent token to every frame to extract the referent sequence. Then, the referent sequence interacts with object queries within each frame to achieve efficient spatial-temporal reasoning over objects. Note that the temporal collection and distribution alternately propagate information between object queries and the referent token to update each other.
Finally, we propose a Temporal Collection and Distribu-tion network (TempCD) which integrates our novel tempo-ral collection and distribution mechanisms into the query-based transformer framework. Without using sequence matching or post-processing during inference like the pre-vious methods [1,53], Our TempCD can directly predict the segmentation result of every frame based on object queries in the frame and the referent token.
In summary, our main contributions are as follows,
• We introduce to maintain a global referent token and a sequence of local object queries parallelly to bridge the gap between video-level object alignment with lan-guage and frame-level object segmentation.
• We propose a novel collection-distribution mechanism for interacting between the referent token and object queries to capture spatial-temporal cross-modal rea-soning over objects.
• We present an end-to-end Temporal Query Collection and Distribution (TempCD) for RVOS, which can di-rectly predict the segmentation referent of every frame without any post-processing and sequence matching.
• Experiments on Ref-Youtube-VOS, Ref-DAVIS17,
A2D-Sentences and JHMDB-Sentences datasets show that TempCD outperforms state-of-the-art methods on all benchmarks consistently and significantly. 2.