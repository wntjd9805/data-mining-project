Abstract
Single image defocus deblurring (SIDD) is a challeng-ing task due to the spatially-varying nature of defocus blur, characterized by per-pixel point spread functions (PSFs).
Existing deep-learning-based methods for SIDD are limited by either over-ﬁtting due to the lack of model constraints or under-parametrization that restricts their applicability to real-world images. To address the limitations, this pa-per proposes an interpretable approach that explicitly pre-dicts inverse kernels with structural regularization. Moti-vated by the observation that defocus PSFs within an im-age often have similar shapes but different sizes, we repre-sent the inverse kernels linearly over a multi-scale dictio-nary parameterized by implicit neural representations. We predict the corresponding representation coefﬁcients via a duplex scale-recurrent neural network that jointly performs
ﬁne-to-coarse and coarse-to-ﬁne estimations. Extensive ex-periments demonstrate that our approach achieves excellent performance using a lightweight model. 1.

Introduction
When a camera captures an image, objects outside of the focal plane appear blurry. This effect is known as defocus blur and usually occurs when using a large cam-era aperture or capturing scenes with signiﬁcantly varying depths. Removing defocus blur from a single image, known as SIDD, is desired for many practical applications; see e.g. [27, 14, 33, 60]. In general, defocus blur varies spa-tially. Each defocused pixel is a weighted average of its neighboring pixels in the latent all-in-focus image, with the weights determined by a spatially-varying unknown PSF.
SIDD is closely related to defocus map estimation (DME) [3, 44, 7, 5, 51, 32, 17]. Existing DME methods typically model defocus PSFs using a Gaussian function
*This work is supported by the Natural Science Foundation of Guang-dong Province (Grant No. 2022A1515011755 and 2023A1515012841), and Singapore MOE AcRF Tier 1 Grant (WBS No. A-8000981-00-00). parameterized by its variance or a disc function parameter-ized by its radius. The resulting defocus map indicates the per-pixel blur amount in terms of the values of such model parameters, which can then be utilized for SIDD by apply-ing a non-blind image deblurring (NID) algorithm using the corresponding PSFs; see e.g. [18]. However, DME remains a challenging task, and the estimated parameters of PSFs can be erroneous for many pixels. Moreover, Gaussian or disc-indicator functions are overly simplistic for real-world defocus PSFs. All these errors will be magniﬁed in the NID process, resulting in noticeable artifacts.
Recently, many deep-learning-based methods have been proposed for SIDD; see e.g. [41, 25, 21, 47, 36, 40, 1, 56, 58, 23, 31]. Most train an end-to-end neural network (NN) that directly maps blurry images to their sharp correspon-dences. Although these methods provide better results than the DME-based two-step methods, their performance still has much room for improvement due to signiﬁcant vari-ations in the spatial distributions of defocus PSFs among different images. Many existing works (e.g. [21, 40, 56]) explicitly introduce spatially-varying processing blocks to better handle the spatial variance of defocus PSFs. How-ever, those blocks lack constraints and may cause overﬁt-ting. A few studies (e.g. [36, 37]) represent defocus PSFs by some speciﬁc basis for geometric regularization and employ deep unrolling for an NN-based inversion process. Never-theless, there is still room for improvement.
This paper presents an end-to-end deep NN with high in-terpretability for SIDD, which explicitly predicts spatially-varying inverse kernels of defocus PSFs and performs de-blurring using the predicted inverse kernels. The possible overﬁtting issue is tackled by providing a more accurate model with structural constraints/regularization for the in-verse kernels. Speciﬁcally, the inverse kernels are modeled by a linear representation under a dictionary. To achieve a compact yet sufﬁciently general representation, we con-struct the dictionary with a multi-scale structure. This is motivated by the empirical observation (as seen in [47]) that defocus PSFs within an image tend to have the same shape
but vary mainly in size. We show that if two defocus PSFs of different sizes have the same shape under an upsampling operation, so do the dictionary atoms of their inverse ker-nels. Thus, the dictionary atoms of a large size are deﬁned as an upsampled version of the atoms of a small size.
Although plain upsampling offers an economical method for generating larger dictionary atoms from smaller ones, the resulting atoms may have a limited frequency range.
This can lead to poor approximations of certain inverse ker-nels. To address the limitation, we leverage implicit neural representation (INR) [46], a technique that uses coordinate-input NNs to represent geometric objects like 2D shapes, so as to efﬁciently parameterize multi-scale atoms with both sufﬁcient coverage of high-frequency content and implicit regularization to alleviate overﬁtting.
For the multi-scale representation coefﬁcients, we sepa-rate them into scale-related and shape-related components, which are predicted by two scale-recurrent sub-NNs respec-tively. To better utilize information from different scales for coefﬁcient prediction, we introduce a duplex scale-recurrent framework that performs both ﬁne-to-coarse and coarse-to-ﬁne estimations. Once the coefﬁcients are predicted, the corresponding inverse kernels are applied to the input image for deblurring. The resulting pipeline is highly efﬁcient and interpretable, leading to a lightweight yet effective model.
To conclude, this paper proposes an end-to-end NN for
SIDD, which exhibits a noticeable performance boost com-pared to existing ones, while maintaining low complexity.
See below for our main contributions:
• A parametric inverse kernel prediction framework for
SIDD, providing efﬁcient and interpretable deep NNs.
• A multi-scale linear representation model for the in-verse kernels of spatially-varying defocus blur, using an INR-based multi-scale dictionary.
• A duplex scale-recurrent framework for predicting the representation coefﬁcients of inverse kernels. 2.