Abstract
CLIP models have demonstrated impressively high zero-shot recognition accuracy, however, their fine-tuning per-formance on downstream vision tasks is sub-optimal. Con-trarily, masked image modeling (MIM) performs exception-ally for fine-tuning on downstream tasks, despite the ab-sence of semantic labels during training. We note that the two tasks have different ingredients: image-level tar-gets versus token-level targets, a cross-entropy loss ver-sus a regression loss, and full-image inputs versus partial-image inputs. To mitigate the differences, we introduce a classical feature map distillation framework, which can si-multaneously inherit the semantic capability of CLIP mod-els while constructing a task incorporated key ingredi-ents of MIM. Experiments suggest that the feature map distillation approach significantly boosts the fine-tuning performance of CLIP models on several typical down-stream vision tasks. We also observe that the approach yields new CLIP representations which share some diag-nostic properties with those of MIM. Furthermore, the fea-ture map distillation approach generalizes to other pre-training models, such as DINO, DeiT and SwinV2-G, reaching a new record of 64.2 mAP on COCO object detection with +1.1 improvement. The code and mod-els are publicly available at https://github.com/
SwinTransformer/Feature-Distillation. 1.

Introduction
The pre-training and fine-tuning paradigm is instrumen-tal in the success of deep learning methods in computer vision, as evidenced by numerous influential works such as [20, 30, 13, 39]. One common practice is to use model weights pre-trained on ImageNet-1k classification task [10] as the initialization for various downstream vision tasks, such as object detection [13] and semantic segmenta-tion [39]. However, this approach faces two key challenges:
*Corresponding Author. The work is done when Yixuan Wei, Zhenda
Xie, and Ze Liu are interns at Microsoft Research Asia.
Improving the fine-tuning performance of the
Table 1:
ViT-B/16 CLIP model [42] via a classical feature distilla-tion framework. The model is distilled on ImageNet-1K dataset [10] with images only for 300 epochs. Clear gains are observed on four evaluation benchmarks. MAE [17] re-sults are also listed in gray for reference.
Method f.t.
MAE [17] 68.0 83.6
IN-1K (%) ADE20K linear
NYUv2
COCO mIoU APbox APmask RMSE (↓) 48.1 0.383 46.5 40.9
CLIP [42]
FD-CLIP 79.5 82.9 80.1 85.0
∆ ↑0.6 ↑2.1 49.5 51.7
↑2.2 45.0 48.2
↑3.2 39.8 42.5
↑2.7 0.416 0.352
↓0.064 the difficulty in scaling up high-quality image classification data, and the limited semantic information contained in cat-egory labels, both of which constrain the ability to further improve model performance.
The recent CLIP [42] alleviates these challenges. It uti-lizes contrastive learning to learn representations from web-scale noisy vision-language pairs. The learned represen-tations exhibit impressive semantic modeling capabilities, as evidenced by performance on zero-shot image classifi-cation and image-text retrieval tasks. Meanwhile, a new self-supervised pre-training method based on masked im-age modeling (MIM) [2, 58, 17] has also attracted great attention for its excellent fine-tuning performance on var-ious downstream tasks. Without losing generalizability, we mainly discuss MAE [17] in this paper.
When comparing the two pre-training methods, the CLIP model learns richer semantic information reflected by its su-perior linear probing performance on ImageNet-1K. How-ever, its fine-tuning performance on most other tasks are worse than MAE, as shown in Tab. 1. This observation ap-pears counter-intuitive since models with better semantics are usually considered to have better transferability. This raises a further question: can CLIP be made as successful as, or even surpass, MIM in fine-tuning? To answer this question, we firstly decompose the ingredients of these pre-input ratios, training training methods into three aspects:
Table 2: Improving the fine-tuning performance of the ViT-L/14 CLIP model [42] on ImageNet-1K classification.
Method
WiSE-FT [54]
DeiT III [52]
ViT [11]
Scaling [63]
BeiT [2]
CLIP [42]
FD-CLIP
Pre-train datasets
WIT-400M [42]
IN-22K [10]
JFT-300M [50]
JFT-3B[63]
Res. 3362 3842 5122 3842 5122 DALLE [45] & IN-22K 2242 2242 2242 WIT-400M & IN-22K 3362 WIT-400M & IN-22K
WIT-400M
WIT-400M
IN-1K(%) 87.1 87.7 87.8 88.5 88.6 86.1 87.7 (+1.6) 88.3 89.0 target granularity and training losses, as listed in Tab. 3. By comparing the differences between CLIP and two typical
MIM approaches, we exclude the training losses to be re-sponsible for the inferior fine-tuning performance of CLIP, and speculate that the input ratios (i.e. full image vs. partial image) and training target granularity (i.e. image-level vs. token-level) might be key factors. Although narrowing the differences in input ratios is straightforward, changing the granularity of the CLIP training targets from image-level to token-level poses a significant challenge, since existing vision-language training data is more suitable for image-level supervision and lacks fine-grained information.
Knowledge distillation [21] is a widely used technique for transferring information from one model to another, typ-ically for the purpose of model compression. In this paper, we demonstrate that distillation can also perform as a bridge for converting the training target granularity of CLIP mod-els from image-level to token-level, while preserving the semantic information. Specifically, we take the pre-trained
CLIP model as the teacher model, use its output feature map as the distillation target, and distill this information into a randomly initialized student model that shares the same ar-chitecture and size as the teacher model. This process is illustrated in Fig. 1, which we refer to as “feature distilla-tion” to differentiate it from logits distillation [21, 51, 12].
Notably, although the student mimics the teacher model’s output, their different optimization paths can lead to differ-ent diagnostic properties on the inter-mediate layers, which is thought to be important for fine-tuning.
The flexibility of the distillation framework allows us to introduce proper inductive bias and regularization to shape the optimization path of the student model and enhance the student model performance on downstream tasks. Specifi-cally, we propose several crucial adjustments: 1) Standard-ization of the teacher feature map. This adjustment am-plifies the subtle information contained within the teacher model and stabilizes the output value range; 2) Asym-metric drop path rates for the teacher and student models.
This asymmetric regularization enhances the robustness of
Figure 1: Illustration of the feature map distillation that in-troducing token-level targets to distill the pre-trained CLIP models. The orange block stands for [CLS] token, and the orange box means a random crop of the original image. student representations and results in accurate and consis-tent teacher signals. 3) Shared relative position bias. The introduction of this inductive bias further strengthens the translation-invariant property of the student model.
With the feature distillation framework and the above ad-justments, we derive a model that preserves the strong se-mantic information of CLIP while being friendly to down-stream task fine-tuning, as shown in Tab. 1. We observe consistent improvements compared to the original CLIP on various tasks: +2.1 accuracy gains on ImageNet-1K im-age classification [10], +2.2 mIoU gains on ADE20K se-mantic segmentation [66], +3.2 box AP and +2.7 mask AP on COCO object detection and instance segmentation [36], and reducing RMSE(↓) by 0.064 on NYUv2 depth estima-tion [49]. The improvement remains when scaling up to the largest CLIP-L/14 model with a +1.6 accuracy gain on
ImageNet-1K, as shown in Tab. 2. Moreover, when general-izing our method to other models, like DINO [3], DeiT [51] and the advanced SwinV2-G [37], we still earn clear gains on various downstream tasks, especially reaching a new record of 64.2 mAP on COCO object detection with +1.1 mAP improvement on SwinV2-G.
In addition to these improvements in experimental re-sults, we further introduce several diagnostic tools to an-alyze the properties of learned visual representations from different models. These analyses provide deeper insights into understanding how feature distillation improves the
CLIP model: 1) diversifying different attention heads of the
CLIP model in deeper layers; 2) improving the translational invariance of learned representations; and 3) flattening the loss landscapes and reflecting optimization friendliness.
Our contributions are summarized as follows:
• We examine the ingredient differences between CLIP and MIM methods and demonstrate target granularity is vital in the success of MIM in fine-tuning.
• We leverage the classical feature map distillation to convert the training target granularity of CLIP to token-level ones, which enhances its fine-tuning per-formance and preserves its semantic information.
• We propose several crucial techniques during feature distillation that further enlarge the improvements, in-cluding distilling standardized feature maps, asymmet-ric drop path rates, and shared relative position bias.
• With several diagnostic tools, we find that compared to
CLIP, both MIM and FD-CLIP possess several prop-erties that are intuitively good, which may provide in-sights on their superior fine-tuning performance.
• We generalize our method to various pre-training mod-els and observe consistent gains. We also set a new record on COCO object detection, by improving the advanced 3B SwinV2-G model with our framework. 2.