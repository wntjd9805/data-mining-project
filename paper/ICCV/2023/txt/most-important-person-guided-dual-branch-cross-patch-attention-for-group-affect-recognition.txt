Abstract
Group affect refers to the subjective emotion that is evoked by an external stimulus in a group, which is an im-portant factor that shapes group behavior and outcomes.
Recognizing group affect involves identifying important in-dividuals and salient objects among a crowd that can evoke emotions. However, most existing methods lack attention to affective meaning in group dynamics and fail to ac-count for the contextual relevance of faces and objects in group-level images.
In this work, we propose a solution by incorporating the psychological concept of the Most Im-portant Person (MIP), which represents the most notewor-thy face in a crowd and has affective semantic meaning.
We present the Dual-branch Cross-Patch Attention Trans-former (DCAT) which uses global image and MIP together as inputs. Specifically, we first learn the informative fa-cial regions produced by the MIP and the global context separately. Then, the Cross-Patch Attention module is pro-posed to fuse the features of MIP and global context to-gether to complement each other. Our proposed method outperforms state-of-the-art methods on GAF 3.0, GroupE-moW, and HECO datasets. Moreover, we demonstrate the potential for broader applications by showing that our pro-posed model can be transferred to another group affect task, group cohesion, and achieve comparable results.
Global + All 3Random 3MIPs 1Random MIP
Acc (%) 85.01 85.47 87.46 82.30 88.86
Table 1. Different face selection strategies. The “3Random” refers to selecting 3 faces randomly from the detected faces in the im-age, while the “3MIPs” strategy selects the top three detected faces based on their importance ranking. The dataset evaluated here is the GroupEmoW [22]. or group-level emotion [2] 1.
Figure 1. Feature map comparison between using MIP and all faces. The yellow box in (a) represents the MIP region. 1.

Introduction
Humans are active and social creatures, using multi-modal interactions to convey their intentions, attitudes, and feelings. Such physical and emotional interactions between individuals within a group can generate group-level affect
By providing the group-level information, group affect prediction has various real-world applications, e.g., work team outcome prediction [21], social relationship recogni-tion [33], human-machine interaction systems [24]. As a result, the research topics surrounding group affect are di-1In this work, groups range in size from two or three individuals up to hundreds.
verse, including categorical analysis, e.g., group valence (positive, negative, neutral) prediction [16, 22], continuous intensity estimation, e.g., group cohesion prediction [15].
Group affect is influenced by a combination of a group’s affective context (e.g., salient objects in the funeral, party) and affective composition (i.e., the combination of a group members’ state and trait affect) [2]. Previous methods for group affect recognition mainly adopted a bottom-up ap-proach [9, 44, 16, 22], where features were extracted from individuals separately and then fused to output group-level emotion. This was done by utilizing multi-cues from sepa-rate pre-trained detectors, such as facial expressions of in-dividuals, object proposals, and scene features. While these cues can provide valuable information, they may not fully capture the complex interplay of emotions within a group.
As indicated in [12], current group-level recognition ar-chitectures tend to focus on detecting salient regions, but they may lack an understanding of the affective meaning in the image. Therefore, there is a need for a more holis-tic approach that considers the most important region in the crowd and how they interact with the scene.
Hence, in this work, we introduce a crucial psycholog-ical concept, namely, Most Important Person (MIP) [2], which goes beyond saliency and considers the importance of individuals for group affect recognition. The MIP of an image is often the group leaders who can influence the emo-tion of a group [11, 37, 34]. MIP has been explored in many related works in psychology and affective computing, such as the Affective Transfer Process phenomenon in psychol-ogy [2], which indicates that the MIP of an image can in-fluence the group’s emotion. We conducted a preliminary experiment based on the CrossVit [5] model to demonstrate the effectiveness of the MIP in group affect. Our architec-ture uses both the global image and faces as two inputs.
As depicted in Table 1, the results demonstrate that using all faces is considerably less effective than using the MIP, and even less effective than randomly selecting three facial images. Additionally, as observed in the feature maps illus-trated in Figure 1, the model that employs all faces fails to predict accurately due to its focus on irrelevant background regions or faces. In contrast, the network that utilizes MIP can efficiently focus on relevant facial regions, thereby en-hancing prediction accuracy. This is mainly due to the fact that in crowded scenes with many blurry faces, the model trained on all faces exhibits dispersed attention, while the model trained on the MIP region concentrates specifically on the relevant area.
Based on the above observations, we present a dual-pathway vision transformer model, the Dual-branch Cross-Patch Attention Transformer (DCAT), including a global branch and a MIP branch. With the dual-pathway, spatial-wise discriminative clues can be discovered both globally and locally in conjunction with self-attention learning. For the dual-pathway interaction, since there are many interfer-ences in the group, we propose the Cross-Patch Attention (CPA) module. CPA first utilizes the Token Ranking Mod-ule to select the important tokens in each path, and then the cross-attention is calculated between the selected query vec-tors and the entire key-value vectors from the other path. In this way, global and MIP contexts are able to complement and compensate for each other.
This paper makes the following main contributions:
• In this work, we validate that MIP plays a crucial role in group affect recognition. To the best of our knowl-edge, this is the first work introducing MIP into the group affect task.
• The MIP and global affective context information are integrated into the proposed dual-pathway vision transformer architecture for recognizing group emo-tions in a context-aware manner. We also propose a novel CPA module that focuses on the interactions among diverse scene contexts and facial informative regions.
• Experimental results show that the proposed DCAT outperforms both state-of-the-art group affect works and vision transformer models in terms of accuracy and model parameters. This is also the first work to explore group affect recognition that goes beyond the
CNN-based approach. Moreover, our proposed model can be utilized in other group-level affect tasks, i.e., group cohesion analysis. 2.