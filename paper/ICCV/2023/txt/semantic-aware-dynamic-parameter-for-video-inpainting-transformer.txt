Abstract
Recent learning-based video inpainting approaches have achieved considerable progress. However, they still cannot fully utilize semantic information within the video frames and predict improper scene layout, failing to restore clear object boundaries for mixed scenes. To mitigate this prob-lem, we introduce a new transformer-based video inpaint-ing technique that can exploit semantic information within the input and considerably improve reconstruction qual-ity.
In this study, we use the mixture-of-experts scheme and train multiple experts to handle mixed scenes, includ-ing various semantics. We leverage these multiple experts and produce locally (token-wise) different network param-eters to achieve semantic-aware inpainting results. Exten-sive experiments on YouTube-VOS and DAVIS benchmark datasets demonstrate that, compared with existing conven-tional video inpainting approaches, the proposed method has superior performance in synthesizing visually pleasing videos with much clearer semantic structures and textures. 1.

Introduction
A constant increase in demand for video content in our daily lives (e.g., YouTube or TikTok) has led to the develop-ment of video inpainting methods, which aim to complete a missing region or erase unwanted areas such as watermarks and captions from a given input video. In particular, recent convolutional neural networks (CNNs) for video inpainting tasks have shown promising results [34, 15, 2, 43, 37, 9].
In addition, transformer-based inpainting networks [39, 26, 23, 29] significantly escalate the quality of inpainting re-sults through their large network capacity and superior lo-cal/global connectivity based on attention mechanisms.
However, conventional approaches are unable to fully utilize the semantic information within the input video or distinguish the class-specific characteristics of objects with different semantics. As a result, they frequently fail to recover proper object structure, texture, and scene layout.
Several studies [32, 24, 19, 25] have suggested that utiliz-∗Equal contribution.
†Corresponding author. ing semantic information can lead to better results and pro-duce visually more plausible images when filling missing regions. However, research on video inpainting has yet to explore the potential of incorporating semantic maps.
Video inpainting is a challenging task that requires to render temporally consistent video frames. To address this issue, optical flow has been extensively explored, although it is computationally intensive [15, 37, 9, 23, 41, 40, 13].
In this study, we demonstrate that semantic information can also be used to enforce temporal consistency, and we in-troduce a new technique that further facilitates the use of semantic cues in a video by dynamically mixing multiple class-specific experts to handle objects with distinct seman-tics adaptively.
Although previous semantic-guided inpainting networks used the predicted segmentation map as additional in-put [32] or developed a dedicated semantic-aware infer-ence module [24] to learn semantic-aware parameters, the restoration performance is limited, given the use of shared network parameters for the different semantics. This prob-lem has been addressed in a recent study [25] by using a semantic-aware attention module that enables the features to attend solely to regions with identical semantic labels.
However, the computational cost increases proportionally to the number of classes because the attention mechanism is separately carried out for each category.
To mitigate these problems, we propose a novel semantic-aware dynamic parameter selection approach that effectively utilizes the semantic information within input video frames while retaining the number of operations dur-ing the inference phase. Specifically, inspired by Cond-Conv [38], we introduce conditionally parameterized lin-ear operations to learn semantic-aware experts, and pro-duce locally varying (dynamic) parameters by leveraging the given semantic cues based on the notion of mixture-of-experts [31, 30, 38]. Our linear operations with dynamically determined parameters replace the standard feed-forward operation within a conventional transformer block. No-tably, we can keep the number of parameters for inference by mixing the expert parameters before performing linear operations rather than aggregating the output features cal-culated by each expert. Moreover, in contrast to the original
CondConv [38] that produces conditional, but locally uni-form parameters, ours can generate conditional and locally (token-wise) varying parameters, thus improving each to-ken’s representation power.
Our extensive experiments witness the outstanding the proposed Semantic-Aware Video performance of
Inpainting Transformer (SAVIT) in improving video in-painting results. Specifically, SAVIT elevates quantitative performance on conventional video inpainting benchmark datasets (YouTube-VOS [36] and DAVIS [28]) and pro-duces visually more superior results against state-of-the-art methods. We summarize our contribution as follows:
• We tackle leveraging semantic information within the given input video frames for the video inpainting task.
• We introduce a novel semantic-aware video inpainting transformer based on a mixture-of-experts scheme.
• We propose a semantic-aware dynamic linear opera-tion to exploit local semantic cues effectively.
• Extensive experiments demonstrate the superiority and efficacy of our method, especially in recovering se-mantic structures and textures. 2.