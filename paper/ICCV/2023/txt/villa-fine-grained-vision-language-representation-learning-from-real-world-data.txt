Abstract
Vision-language models (VLMs), such as CLIP and
ALIGN, are generally trained on datasets consisting of image-caption pairs obtained from the web. However, real-world multimodal datasets, such as healthcare data, are significantly more complex: each image (e.g. X-ray) is often paired with text (e.g. physician report) that de-scribes many distinct attributes occurring in fine-grained regions of the image. We refer to these samples as ex-hibiting high pairwise complexity, since each image-text pair can be decomposed into a large number of region-attribute pairings. The extent to which VLMs can capture fine-grained relationships between image regions and tex-tual attributes when trained on such data has not been pre-viously evaluated. The first key contribution of this work is to demonstrate through systematic evaluations that as the pairwise complexity of the training dataset increases, standard VLMs struggle to learn region-attribute relation-ships, exhibiting performance degradations of up to 37% on retrieval tasks. In order to address this issue, we intro-duce ViLLA as our second key contribution. ViLLA, which is trained to capture fine-grained region-attribute relation-ships from complex datasets, involves two components: (a) a lightweight, self-supervised mapping model to decompose image-text samples into region-attribute pairs, and (b) a contrastive VLM to learn representations from generated region-attribute pairs. We demonstrate with experiments across four domains (synthetic, product, medical, and nat-ural images) that ViLLA outperforms comparable VLMs on fine-grained reasoning tasks, such as zero-shot object detec-tion (up to 3.6 AP50 points on COCO and 0.6 mAP points on LVIS) and retrieval (up to 14.2 R-Precision points)1. 1.

Introduction recent years to be highly effective on a variety of classifica-tion, retrieval, and robustness tasks [28, 17, 27, 8, 42, 9].
VLMs are trained on large-scale datasets consisting of image-text pairs, where the text takes the form of a concise caption (e.g. alt-text) describing salient attributes in the im-age [28, 33, 34]. During training, VLMs generally model the relationship between a paired image-text sample as a one-to-one mapping: a single embedding of the entire im-age is contrastively aligned with a single embedding of the entire caption [28, 42, 17].
However, real-world multimodal datasets, such as those obtained from healthcare settings or product databases, con-sist of samples that are significantly more complex than standard image-caption pairs [19, 3, 1, 25]. In particular, real-world image-text samples include text that describes many distinct attributes occurring in fine-grained regions of the paired image. For example, medical images (e.g. X-rays) are accompanied by detailed text reports describing a variety of attributes (e.g. characteristics of organs, signs of disease) that map to specific regions of the image [19, 3, 4].
We refer to these samples as exhibiting high pairwise com-plexity, since each image-text pair can be decomposed into a large number of fine-grained region-attribute pairings. Fig-ure 1 provides a visual depiction of pairwise complexity.
Models with knowledge of region-attribute relationships have been shown to exhibit numerous advantages, rang-ing from improved performance on fine-grained tasks (such as object detection) to improved subgroup robustness [43, 32, 29]. However, the extent to which standard one-to-one VLMs can learn these fine-grained relationships when trained on complex, real-world datasets is not currently well understood2. Prior work [43] has observed that standard one-to-one VLMs often struggle to learn region-attribute re-lationships3, yet the specific effects of training dataset com-plexity on the ability of a VLM to capture these relation-ships have not been previously evaluated.
Vision-language models (VLMs), which jointly learn re-lationships between images and text, have been shown in 1Code: https://github.com/StanfordMIMI/villa 2As a motivating example, a VLM trained on X-rays should learn to link the region of the heart to the attribute “enlarged heart” in the report. 3When CLIP is applied to a region-level object detection task, perfor-mance is 40% lower than simply applying CLIP at the image level [43].
Figure 1. (Left) We provide examples of image-text samples with varying complexities. Examples with low pairwise complexity are from the CC3M dataset [34]. Textual attributes are in bold type, and corresponding image regions are marked with bounding boxes. (Right) We introduce ViLLA, a representation learning approach that captures fine-grained relationships between image regions and textual attributes.
As our first key contribution in this work, we conduct a systematic evaluation of the effects of training dataset complexity on the fine-grained reasoning ability of standard one-to-one VLMs. To this end, we introduce the pairwise complexity score, which measures the number of region-attribute pairings that can be composed from an image-text sample. Then, we create a synthetic dataset DOCMNIST, where the average pairwise complexity can be directly con-trolled by altering the number of region-attribute pairs per sample. We use DocMNIST to demonstrate that as the com-plexity of the training data increases, a standard one-to-one
VLM demonstrates a performance drop of 36.9% on a text
→ region retrieval task and 20.5% on a region → text re-trieval task, as measured by R-Precision scores. Our evalu-ation suggests that standard one-to-one VLMs are not effec-tive at capturing region-attribute relationships when trained on datasets exhibiting high pairwise complexity.
Our findings demonstrate the need for a VLM that can learn accurate relationships between image regions and textual attributes when trained on complex multimodal datasets. We establish the following two desiderata for such a model. First, fine-grained relationships should be learned without the need for ground-truth region-attribute pairings, which are generally not provided in datasets and are expensive to manually label (particularly in specialized domains like healthcare). Second, representation learning should be performed using standard one-to-one VLMs. De-spite their current inability to capture region-attribute re-lationships from complex datasets, one-to-one VLMs are highly effective [28], widely-used, and less computationally expensive than alternative fine-grained learning approaches
[37].
As our second key contribution, we present Vision-Language Learning with Attributes (ViLLA), a self-supervised multimodal representation learning approach that satisfies the above desiderata. Our key insight is that providing region-attribute pairs as training data to a stan-dard one-to-one VLM helps improve the fine-grained rea-soning ability of the model. ViLLA employs a two stage pipeline, as shown in Figure 1. The first stage involves training a lightweight mapping model to decompose image-text samples into region-attribute pairs. Here, we model the relationship between a paired image-text sample as a many-to-many mapping; given a set of many candidate im-age regions and a set of many textual attributes, we lever-age self-supervision to learn a mapping between these sets.
Then, the second stage involves training a standard one-to-one VLM on the generated region-attribute mappings.
We demonstrate with four multimodal training datasets across various domains (synthetic images, product data, medical images, and natural images) that ViLLA outper-forms comparable one-to-one VLMs on fine-grained rea-soning tasks, such as zero-shot object detection (up to 3.6
AP50 points on COCO and 0.6 mAP points on LVIS), text→region retrieval (up to 14.2 R-Precision points), and region→text retrieval (up to 7.8 R-Precision points). We show that these improvements are a result of our region-attribute mappings, which are up to 25.8 points more accu-rate than prior approaches.
Our contributions are summarized below:
• We demonstrate through a series of systematic evalua-tions that standard one-to-one VLMs struggle to learn relationships between image regions and textual at-tributes as training dataset complexity increases (lead-ing to performance degradations of up to 37% on re-trieval tasks). We also introduce DOCMNIST, a syn-thetic, customizable training dataset that we hope will be useful for future research on VLMs.
• We present ViLLA, a self-supervised multimodal rep-resentation learning approach that can effectively learn
fine-grained region-attribute relationships, particularly when training datasets exhibit high pairwise complex-ity. We demonstrate that our approach works effec-tively across a variety of real-world domains, outper-forming comparable methods across tasks including zero-shot object detection (COCO and LVIS) and re-trieval (CheXpert 5x200).
The rest of this paper is organized as follows. In Sec-tion 2, we discuss related work. We formally introduce the problem setting in Section 3, followed by our analysis with DOCMNIST in Section 4. In Section 5, we introduce
ViLLA, and in Section 6, we present experimental results.
Finally, we conclude in Section 7. aims to address these issues by introducing a specific train-ing phase to learn region-attribute mappings, rather than di-rectly using an off-the-shelf pretrained VLM model. Our work is also inspired by open vocabulary object detection
[38, 23, 11, 29] and self-supervised patch-token alignment
[37, 21, 15] methods.
Learning from Real-World Multimodal Data: Our work relates closely to prior studies that have developed VLMs for medical [42, 40, 2, 15] and product datasets [1, 5, 41].
We extend these lines of research by developing an ap-proach that can effectively learn fine-grained signal from datasets with high pairwise complexity. We show that
ViLLA works effectively across multiple real-world do-mains. 2.