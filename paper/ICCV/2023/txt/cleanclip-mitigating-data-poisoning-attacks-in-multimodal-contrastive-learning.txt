Abstract
Multimodal contrastive pretraining has been used to train multimodal representation models, such as CLIP, on large amounts of paired image-text data. However, previ-ous studies have revealed that such models are vulnerable to backdoor attacks. Specifically, when trained on back-doored examples, CLIP learns spurious correlations be-tween the embedded backdoor trigger and the target label, aligning their representations in the joint embedding space.
Injecting even a small number of poisoned examples, such as 75 examples in 3 million pretraining data, can signifi-cantly manipulate the model’s behavior, making it difficult to detect or unlearn such correlations. To address this is-sue, we propose CleanCLIP, a finetuning framework that weakens the learned spurious associations introduced by backdoor attacks by independently re-aligning the repre-sentations for individual modalities. We demonstrate that unsupervised finetuning using a combination of multimodal contrastive and unimodal self-supervised objectives for in-dividual modalities can significantly reduce the impact of the backdoor attack. Additionally, we show that supervised finetuning on task-specific labeled image data removes the backdoor trigger from the CLIP vision encoder. We show empirically that CleanCLIP maintains model performance on benign examples while erasing a range of backdoor at-tacks on multimodal contrastive learning. Code and pre-trained checkpoints are available at https://github. com/nishadsinghi/CleanCLIP. 1.

Introduction
In the development of AI, a long-standing goal has been to learn general-purpose representations from diverse
*Equal Contribution
†Equal Contribution
‡Equal Advising modalities [3]. In this regard, multimodal contrastive meth-ods such as CLIP [45], ALIGN [26], and BASIC [42] have enabled joint representations of images and text by training on large-scale, noisy, and uncurated image-text pairs from the web. During training, the model brings the representa-tions of matched image-text pairs closer in the embedding space while pushing the representations of unmatched pairs further apart. Remarkably, these models achieve impressive zero-shot classification performance on ImageNet [14] and demonstrate robustness to natural distribution shift datasets like ImageNet-V2 [46], ImageNet-R [23] and ImageNet-Sketch [54], all without any access to labeled data during representation learning, also known as pretraining.
Despite the successes of multimodal contrastive learn-ing, recent studies by [6, 5] have shown that these mod-els are vulnerable to adversarial attacks. Poisoning even a small fraction of the pretraining data (e.g., 75 out of 3 mil-lion training samples) with specialized triggers injected into randomly selected images and replacing their matched cap-tions with proxy captions for the target label, e.g., “a photo of a banana”, where ‘banana’ is the target label, can result in a backdoor attack (Figure 4a). During pretraining on poi-soned data, the model minimizes the multimodal contrastive loss by bringing the representations of the poisoned images with the backdoor trigger close to the text representation of the matched captions containing the target label. As a re-sult, CLIP learns the multimodal spurious co-occurrence between the presence of the backdoor trigger in the image and the target label in the caption (Figure 4b).
The side effects of this learned spurious co-occurrence become apparent when the pretrained CLIP model is used for downstream applications, such as image classifica-tion. To illustrate, we sample a subset of 500 clean im-ages C = {I1, . . . , I500}, belonging to different classes in the ImageNet-1K validation set, and create a dirty subset
D = { ˆI1, . . . , ˆI500} by embedding a backdoor trigger tg
i ) where I e (Blended [9]) into each image, ˆIi = Ii ◦ tg. Since the im-ages Ii and ˆIi belong to the same class and share most of the information in the pixel space, we expect their visual representations to align with each other in the embedding space. However, our analysis of the visual representations learned by the poisoned CLIP shows that the model clusters all the poisoned images together in the embedding space (Figure 1a). We find that the average distance between the representations of the clean image and its poisoned coun-terpart from the poisoned model, which is calculated as i , ˆI e 2 − 2 × cosine similarity(I e i is the represen-tation of Ii, is 1.62. In comparison, the distance between the visual representations from a CLIP model that is pre-trained on clean data is 0.4. Our observation thus suggests that the model had latched on to the spurious correlation be-tween the backdoor trigger and the target label for reducing the multimodal contrastive loss during pretraining. Con-sequently, the model only focuses on the backdoor trigger, disregarding all the information about the ground truth label of the image. As a result, the poisoned CLIP model predicts the target label for approximately 99% of the images from the ImageNet-1K validation dataset when the backdoor trig-ger is embedded into them. At the same time, the model still predicts the correct class for benign (clean) images. Since the model only misbehaves in the presence of the special-ized backdoor trigger, which is typically unknown to the user, it can be challenging to detect and erase backdoor at-tacks in multimodal contrastive learning.
To mitigate the impact of data poisoning attacks in mul-timodal contrastive learning, we introduce CleanCLIP, a framework designed to remove backdoors from a pretrained
CLIP model by fine-tuning it with clean image-caption data.
Our approach is motivated by the observation that backdoor attacks on multimodal contrastive learning rely on the spu-rious co-occurrence of the backdoor trigger and the target label. Encouraging the model to learn independent repre-sentations of each modality, i.e., image and text, can help break this spurious mapping. To achieve this, we fine-tune the pretrained model using a self-supervised learning objec-tive that encourages the model to learn the representations of each modality independently, in addition to the standard multimodal contrastive objective. Self-supervised learning is a powerful way to learn general features of a dataset in an unsupervised fashion, allowing semantically similar sam-ples to be mapped close to each other in the embedding space [8, 39, 22].
In our experiments (§5.1), we discovered that Clean-CLIP effectively mitigates the impact of various backdoor attacks on CLIP without negatively affecting its perfor-mance on benign images. Moreover, in Figure 1c, we ob-served that CleanCLIP eliminates the spurious connections between the backdoor trigger and the target label, result-ing in the absence of a distinct cluster for the target label in the images containing the embedded backdoor triggers.
Quantitatively, the average distance between the visual rep-resentations of clean images and their corresponding poi-soned images decreased from 1.62 for the poisoned CLIP to 0.57 with CleanCLIP. Additionally, in §5.2, we demon-strated that poisoning a CLIP model pretrained on 400M image-text data is feasible by fine-tuning it with poisoned data. We also discovered that CleanCLIP is effective in re-ducing the impact of backdoor attacks in such scenarios.
Furthermore, we demonstrate that when downstream task-specific, clean, and labeled data are present, simple su-pervised fine-tuning of the CLIP vision encoder with clean data can eliminate the backdoor attack (§5.3). As the CLIP vision backbone adapts to the target distribution, the false backdoor associations are forgotten during the process. This is evidenced by the fact that images containing the back-door trigger do not form a separate cluster in the embedding space (Fig. 1d). Additionally, the average distance between the embeddings of clean images and their backdoored coun-terparts decreased from 1.62 for the poisoned model to 0.71 after supervised fine-tuning on clean data.
While one could devise backdoor defense methods that aim to neutralize the backdoor during the pretraining phase, we concentrate on reducing the impact of backdoor at-tacks via finetuning as it is more practical and sample ef-ficient. Moreover, unlike pretraining from scratch, finetun-ing does not necessitate extensive computation and access to the original pretraining data. Finally, we examine various factors that affect the results, including the strength of the self-supervision signal (§6.1), the number of the backdoor examples and the size of the pretraining data (§6.5), and the choice of the finetuning dataset (§6.2). To our knowl-edge, no prior study has defended multimodal contrastive models against backdoor attacks. Our findings suggest that
CleanCLIP provides a robust defense against a variety of backdoor attacks in multimodal contrastive learning. 2.