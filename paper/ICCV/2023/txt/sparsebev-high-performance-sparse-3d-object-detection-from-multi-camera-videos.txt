Abstract
Camera-based 3D object detection in BEV (Bird’s Eye
View) space has drawn great attention over the past few years. Dense detectors typically follow a two-stage pipeline by first constructing a dense BEV feature and then per-forming object detection in BEV space, which suffers from complex view transformations and high computation cost.
On the other side, sparse detectors follow a query-based paradigm without explicit dense BEV feature construction, but achieve worse performance than the dense counter-parts.
In this paper, we find that the key to mitigate this performance gap is the adaptability of the detector in both
BEV and image space. To achieve this goal, we pro-pose SparseBEV, a fully sparse 3D object detector that outperforms the dense counterparts. SparseBEV contains three key designs, which are (1) scale-adaptive self atten-tion to aggregate features with adaptive receptive field in
BEV space, (2) adaptive spatio-temporal sampling to gen-erate sampling locations under the guidance of queries, and (3) adaptive mixing to decode the sampled features with dynamic weights from the queries. On the test split of nuScenes, SparseBEV achieves the state-of-the-art perfor-mance of 67.5 NDS. On the val split, SparseBEV achieves 55.8 NDS while maintaining a real-time inference speed of 23.5 FPS. Code is available at https://github.com/
MCG-NJU/SparseBEV . 1.

Introduction
Camera-based 3D Object Detection [13, 54, 31, 11, 24, 25, 40] has witnessed great progress over the past few years.
Compared with the LiDAR-based counterparts [19, 56, 4, 36], camera-based approaches have lower deployment cost and can detect long-range objects.
Previous methods can be divided into two paradigms.
BEV (Bird’s Eye View)-based methods [13, 11, 25, 24, 40] follow a two-stage pipeline by first constructing an explicit (cid:0): Corresponding author.
Figure 1: Performance comparison on the val split of nuScenes [1]. All methods use ResNet50 [10] as the im-age backbone and the input size is set to 704 × 256. FPS is measured on a single RTX 3090 with the PyTorch fp32 backend. We balance accuracy and speed by reducing the number of decoder layers without re-training. dense BEV feature from multi-view features and then per-forming object detection in BEV space. Although achiev-ing remarkable progress, they suffer from high computa-tion cost and rely on complex view transformation opera-tors. Another line of work [54, 31, 32] explores the sparse query-based paradigm by initializing a set of sparse refer-ence points in 3D space. Specifically, DETR3D [54] links the queries to image features using 3D-to-2D projection.
It has simpler structure and faster speed, but its perfor-mance still lags far behind the dense ones. PETR series
[31, 32] uses dense global attention for the interaction be-tween query and image feature, which is computationally expensive and buries the advantage of the sparse paradigm.
Thus, a natural question arises whether fully sparse detec-tors achieve similar accuracy to the dense ones?
In this paper, we find that the key to obtain high perfor-mance in sparse 3D object detection is the adaptability of the detector in both BEV and image space. In BEV space,
the detector should be able to aggregate multi-scale features adaptively. Dense BEV-based detectors typically use a BEV encoder to encode multi-scale BEV features.
It can be a stack of residual blocks with FPN [26] (e.g. BEVDet [26]), or a transformer encoder with multi-scale deformable at-tention [60] (e.g. BEVFormer [25]). For sparse detectors such as DETR3D, we argue that the multi-head self atten-tion (MHSA) [49] among queries can play the role of the
BEV encoder, as queries are defined in BEV space. How-ever, the vanilla MHSA has a global receptive field, lacking an explicit multi-scale design. In image space, the detector should be adaptive to different objects with different sizes and categories. This is because although the objects have similar sizes in 3D space, they might vary greatly in images.
However, the single-point sampling in DETR3D has a fixed local receptive field and the sampled feature is processed by static linear layers, hindering its performance.
To this end, we present SparseBEV, a fully sparse 3D object detector that matches or even outperforms the dense counterparts. Our SparseBEV detector contains three key designs, which are (1) scale-adaptive self attention to ag-gregate features with adaptive receptive field in BEV space, (2) adaptive spatio-temporal sampling to generate sam-pling locations under the guidance of queries, and (3) adap-tive mixing to decode the sampled features with dynamic weights from the queries. We also propose to use pillars in-stead of reference points as the formulation of query, since pillars introduce better spatial priors.
We conduct comprehensive experiments on the nuScenes dataset. As shown in Fig. 1, our SparseBEV achieves the performance of 55.8 NDS and the speed of 23.5 FPS on the val split, surpassing all previous methods in both speed and accuracy. Besides, we can flexibly adjust of the infer-ence speed by reducing the number of decoder layers with-out re-training. On test split, SparseBEV with V2-99 [20] backbone achieves 63.6 NDS without using future frames or test-time augmentation. By further utilizing future frames,
SparseBEV achieves 67.5 NDS, outperforming the previous state-of-the-art BEVFormerV2 [55] by 2.7 NDS. 2.