Abstract
Transfer learning is widely used for training deep neu-ral networks (DNN) for building a powerful representation.
Even after the pre-trained model is adapted for the target task, the representation performance of the feature extrac-tor is retained to some extent. As the performance of the pre-trained model can be considered the private property of the owner, it is natural to seek the exclusive right of the gen-eralized performance of the pre-trained weight. To address this issue, we suggest a new paradigm of transfer learning called disposable transfer learning (DTL), which disposes of only the source task without degrading the performance of the target task. To achieve knowledge disposal, we pro-pose a novel loss named Gradient Collision loss (GC loss).
GC loss selectively unlearns the source knowledge by lead-ing the gradient vectors of mini-batches in different direc-tions. Whether the model successfully unlearns the source task is measured by piggyback learning accuracy (PL ac-curacy). PL accuracy estimates the vulnerability of knowl-edge leakage by retraining the scrubbed model on a subset of source data or new downstream data. We demonstrate that GC loss is an effective approach to the DTL problem by showing that the model trained with GC loss retains the performance on the target task with a significantly reduced
PL accuracy. 1.

Introduction
Transfer learning [26] (TL) is one of the bedrocks in the success of deep neural networks (DNNs). The core idea of
TL is to build a strong generic model that can adapt to a broad range of downstream tasks with much less amount of data. The scale of data collection for pre-training be-comes the key objective to build a model with competitive performance on their target tasks [14]. Nowadays, a lot of organizations are interested in collecting an internal dataset to build their own generic model.
For example, JFT-300M [6, 24, 25], a large-scale pri-Figure 1: Illustration of the proposed Disposable Trans-fer Learning (DTL) framework. DTL extends the existing
Transfer Learning (TL) paradigm with an additional knowl-edge disposal stage that scrubs off the prior knowledge irrel-evant to the target task. The goal of DTL is to prevent Pig-gyback Learning (PL) which maliciously exploits the rep-resentation performance of a pre-trained model for a piggy-back task by simply performing an extra fine-tuning step on top of the published transfer-learned model. vate dataset exclusively available to Google, is used for pre-training to reach the state-of-the-art performance of down-stream target tasks [12, 27] with relatively small target datasets. IG-3.5B-17k [18], an internal Facebook AI Re-search dataset, is also used to train a weakly supervised model. Such datasets and trained models have a high eco-nomic value in that collecting data and training a model is time-consuming and costly, and a proficient model is read-ily adaptable to various commercial services.
However, those private properties are exposed to unau-thorized customizing when released. Specifically, we de-note piggyback learning (PL) (Figure 1) as a kind of ex-tra fine-tuning on other downstream tasks for leveraging the benefits of the transfer-learned model with much less effort.
As shown in Figure 2, the performance of TL (blue) and
PL (green) on the downstream tasks is comparable, and is
considerably improved over the model trained from scratch (red). In other words, it enables anyone to exploit the pre-text knowledge even when one does not have access to the pre-trained model by accessing the released transfer-learned model. PL is profitable to those free riders, so they may launch a new service or product by just exploiting the pro-ficient model. It conflicts with the model owner’s interest.
To alleviate this potential risk, we propose a novel TL paradigm that temporarily utilizes and then disposes of the source task knowledge after transfer learning, coined Dis-posable Transfer Learning (DTL). DTL aims to protect the exclusive license of generic performance on the internal pre-training data while achieving a powerful downstream performance.
To address the DTL problem, we propose a novel loss function that scrubs the source task knowledge, named Gra-dient Collision loss (GC loss). GC loss guides the model towards abnormal convergence on the source task by min-imizing inner-products between sample gradients. GC loss deals with a non-typical unlearning problem where the scale of data to be unlearned is much larger and the dataset to be unlearned and the dataset to be retained are heterogeneous, whereas existing unlearning literature [2, 3, 7, 8] mainly fo-cus on forgetting only a small portion of a single kind of training data.
After DTL, we measure the model’s susceptibility to un-wanted PL using Piggyback Learning accuracy (PL accu-racy). We define the PL accuracy of a model as the test ac-curacy measured by learning an additional piggyback task.
A low PL accuracy indicates that the model successfully un-learned the source knowledge so that it is resistant to a small portion of source re-training or fine-tuning on other down-stream tasks. We will show the importance of PL accuracy as a measurement of unlearning for validating knowledge disposal.
We demonstrate that the model scrubbed with GC loss retains the target performance while effectively prevent-ing the exploitation of the performance of the pre-trained model. To the best of our knowledge, DTL with GC loss is the first work in making transfer learning and unlearning compatible.
Our main contributions are summarized as follows:
• We propose a novel forgetting problem, DTL, in which we try to dispose of the generalization power of a pre-trained model while adapting the model only to a spe-cific target downstream task.
• We propose GC loss, which is a novel loss that achieves knowledge disposal of the source task. We also provide an extremely efficient implementation of
GC loss that also allows for distributed training.
• We propose PL accuracy as an evaluation metric that can estimate the performance of a DTL model.
Figure 2: Performance comparison between models trained from scratch, models trained by transfer learning (TL), and models trained by piggyback learning (PL). The horizontal axis indicates the datasets used for performance evaluations.
PL model (green) achieves performance comparable to TL model (blue) and both models perform much better than the models trained from scratch (red). For both TL and PL,
CIFAR-100 is used as the source task. For PL, CIFAR-10-1% is additionally used as the target task before the model is piggybacked. 2.