Abstract
Visual similarity discovery (VSD) is an important task with broad e-commerce applications. Given an image of a certain object, the goal of VSD is to retrieve images of different objects with high perceptual visual similarity. Al-though being a highly addressed problem, the evaluation of proposed methods for VSD is often based on a proxy of an identification-retrieval task, evaluating the ability of a model to retrieve different images of the same object. We posit that evaluating VSD methods based on identification tasks is limited, and faithful evaluation must rely on expert annotations. In this paper, we introduce the first large-scale fashion visual similarity benchmark dataset, consisting of more than 110K expert-annotated image pairs. Besides this major contribution, we share insight from the challenges we faced while curating this dataset. Based on these insights, we propose a novel and efficient labeling procedure that can be applied to any dataset. Our analysis examines its limita-tions and inductive biases, and based on these findings, we propose metrics to mitigate those limitations. Though our primary focus lies on visual similarity, the methodologies we present have broader applications for discovering and evaluating perceptual similarity across various domains. 1.

Introduction
Visual similarity measures the perceptual agreement be-tween two objects based on their visual appearance [50].
Two objects can be similar or dissimilar based on their color, shape, size, pattern, utility, and more. In fact, all of these factors and many others take part in determining the degree of visual similarity between two objects with varying importance. Therefore, defining the perceived visual simi-larity based on these factors is challenging. Nonetheless,
* Equal contribution.
Figure 1. Disagreements between visual similarity and identifica-tion. Top: Two images associated with the same object, exhibiting low visual perceptual similarity. Bottom: Two images of two dif-ferent objects, exhibiting high visual perceptual similarity. learning visual similarities is a key building block for many practical utilities such as search, recommendations, etc.
Most existing methods for VSD are based on an identifi-cation retrieval task - given a query image of an object, the identification task deals with retrieving images of an iden-tical object taken under various conditions, such as differ-ent imaging distances, viewing angles, illuminations, back-grounds, and weather conditions e.g., [69, 66]. In fact, per-formance evaluation of image retrieval tasks is commonly gauged on identification-based classification metrics.
Identification and verification tasks [61, 14, 23, 1, 5, 60] are in fact highly related to VSD, as any object is most similar to itself. Nevertheless, these two tasks are not the same (see Fig. 1). Moreover, theoretically, a model that was trained for identification can obtain perfect results on the identification metrics, by retrieving other images of the same product followed by images of completely dissimilar products.
An additional difficulty with learning identification as a proxy to similarity is simply the fact that multiple images of the same product are often unavailable. Furthermore, even when multiple images of the same product do exist, the images often do not conform with visual similarity, as illustrated in the upper row in Fig. 1.
In order to mitigate these difficulties, auxiliary informa-tion can be utilized. For example, tags [49, 11, 6, 7, 13], metadata [8, 9, 45, 46, 30], collaborative-filtering infor-mation [36, 10, 37], or explicit compatibility data from users [32], were all employed in order to learn better visual similarities. However, such auxiliary information is often unavailable, and even when it is, it may not be a faithful proxy for perceived similarities.
Ultimately we acknowledge that any proxy approach has its limitations and that the only faithful evaluation of vi-sual perceptual similarity must rely on the annotations of human domain experts. However, employing such experts is time-consuming and expensive, and there was no publicly available dataset prior to this study. Thus, visual similarity models still rely on proxy evaluations, mostly the identifi-cation task, despite the aforementioned limitations.
In this work, we address the challenge of discovering, curating, and evaluating visual similarity. We developed the
Efficient Discovery of Similarities (EDS) method - a novel scheme for efficiently collecting feedback from human do-main experts. By employing EDS we were able to label more than 110K image pairs which we release, as part of this paper, to serve as the first large-scale benchmark for evaluation of VSD models.
Our contributions: (1) We put a spotlight on the chal-lenge of evaluating methods for VSD. We differentiate be-tween the task of VSD and the identification task and stress (2) We the need for a true dataset of visual similarities. analyze the difficulty of naively labeling a dataset, and pro-pose an efficient procedure for VSD, with proper evalua-tion metrics. The proposed method and metrics can be uti-lized for discovery and evaluation of perceptual similarity in other application domains. (3) Equipped with the pro-posed procedure, we curate the first large-scale visual simi-Figure 2. Image-pair similarity defined using five tiers of granu-larity levels. The innermost tiers are the most restrictive and well-defined, and the outermost tiers are less restrictive and subject to the definition of “instance” and “category”. The orange (green) area corresponds to the scope of VSD (ISC21). larity benchmark for the fashion domain, consisting of more than 110K labeled image pairs. This dataset enables true evaluations of perceived similarity and would help expedite further research in visual similarity and discovery. (4) We provide an extensive evaluation comparing pretrained and finetuned models for both closed-catalog and wild queries. (5) Finally, we discuss and demonstrate the disadvantages of supervised methods for VSD. 2.