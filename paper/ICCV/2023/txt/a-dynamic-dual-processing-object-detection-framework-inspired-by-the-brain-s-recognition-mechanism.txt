Abstract
There are two main approaches to object detection:
CNN-based and Transformer-based. The former views ob-ject detection as a dense local matching problem, while the latter sees it as a sparse global retrieval problem. Research in neuroscience has shown that the recognition decision in the brain is based on two processes, namely familiarity and recollection. Based on this biological support, we propose an efficient and effective dual-processing object detection framework. It integrates CNN- and Transformer-based de-tectors into a comprehensive object detection system con-sisting of a shared backbone, an efficient dual-stream en-coder, and a dynamic dual-decoder. To better integrate local and global features, we design a search space for the CNN-Transformer dual-stream encoder to find the opti-mal fusion solution. To enable better coordination between the CNN- and Transformer-based decoders, we provide the dual-decoder with a selective mask. This mask dynamically chooses the more advantageous decoder for each position in the image based on high-level representation. As demon-strated by extensive experiments, our approach shows flexi-bility and effectiveness in prompting the mAP of the various source detectors by 3.0∼3.7 without increasing FLOPs. 1.

Introduction
Object detection is a fundamental and challenging re-search problem in the field of computer vision. The task is to predict a bounding box and a category label for each object in an image.
In early times, CNN-based method has made significant progresses in this field, which utilizes convolution to attain both low- and high-level local pattern information from the input image, and performs classifi-cation on all candidate grids paved on the image [33, 22, 39, 5, 9, 17]. Recently, Transformer-based detector is pro-*Equal contribution.
†Minying Zhang is the corresponding author. posed as an alternative solution to this problem.
It em-ploys a Transformer-based encoder and decoder to build attention-based global representation and reason about re-lationships between objects and the global image context via a set of queries [3, 46, 24, 43, 41, 28, 19, 45]. Overall,
CNN- and Transformer-based detectors can be structured into a backbone-encoder-decoder architecture. This archi-tecture includes a backbone that extracts rich and general shallow features, an encoder that generates task-relevant high-level representations, and a decoder that predicts the results. Studies in [35, 44] show that visual perception pro-cess in human brain is also hierarchical and conclude that the ventral visual stream (VVS) of the human brain can be abstracted by a backbone-encoder-decoder structure, which provides biologically-plausible explanation to the structure of current deep learning based object detectors, as shown in
Figure 2(c).
There is increasing evidence that brain’s recognition is, in fact, on the basis of the dual-process detection theory which has already a far-reaching influence in the field of psychology and cognitive neuroscience [44]. It claims that brain’s recognition reflects the joint contribution of two sep-arable retrieval processes, namely familiarity and recollec-tion. Neuroscientists find that familiarity is associated with distinct visual cortex area whose biological mechanism in-spires the CNN architecture [34] and recollection is typi-cally ascribed to the hippocampus which has close relation-ship to the Transformer [42]. These findings indicate that in the field of deep learning based object detectors, the func-tion and working mechanism of CNN- and Transformer-based detectors are both bio-inspired and have close rela-tion to familiarity and recollection processes. But each sin-gle process can not fully reflect how human brain deliver-ing object detection tasks and may easily reach its limit as shown in Figure 1. Some recent works attempt to improve the performances of detectors with CNN-Transformer hy-brid methods [7, 43, 6, 8, 37], by introducing the key prop-erties of one to the other. However they only consider either familiarity-like CNN-based encoding-decoding pipeline or
trary to simply assembling the predictions of two indepen-dent decoders, we provide a dynamic dual-decoder(DDD) equipped with a binary selective mask. This mask dynam-ically chooses the more advantageous decoder for each po-sition in the image based on high-level representation, as shown in Figure 1(c). The learning of this mask can be seen as the competition of CNN- and Transformer-based decoders in predicting the target at the corresponding po-sition. Thus, it avoids the redundant computation and en-able each decoder concentrate on its own powerful side and avoid weakness, as shown in Figure 1(d). We show that the proposed framework achieves promising performance in terms of accuracy and model complexity on the COCO datasets [23]. Extensive experiments validate the effective-ness of the cooperation of CNN and Transformer in both encoder and decoder. 2.