Abstract
Modeling long-term context in videos is crucial for many fine-grained tasks including temporal action segmentation.
An interesting question that is still open is how much long-term temporal context is needed for optimal performance.
While transformers can model the long-term context of a video, this becomes computationally prohibitive for long videos. Recent works on temporal action segmentation thus combine temporal convolutional networks with self-attentions that are computed only for a local temporal win-dow. While these approaches show good results, their per-formance is limited by their inability to capture the full context of a video. In this work, we try to answer how much long-term temporal context is required for temporal action segmentation by introducing a transformer-based model that leverages sparse attention to capture the full context of a video. We compare our model with the current state of the art on three datasets for temporal action segmentation, namely 50Salads, Breakfast, and Assembly101. Our experiments show that modeling the full context of a video is necessary to obtain the best performance for temporal action segmen-tation. 1.

Introduction
Temporal action segmentation can be used in many real-world applications such as monitoring production lines or studying animal behavior. In these settings, the videos can be very long and it is required to recognize the start and end of all actions that occur in a video as illustrated in Fig. 1.
Recently, combinations of temporal convolutional net-works [1, 27] with self- and cross-attention from transform-ers [61, 4] have shown impressive results for temporal action segmentation. These works are in line with other hybrid models [15, 9, 61, 58] that combine the attention modules with convolutions to compensate for the lack of strong in-ductive bias of pure transformers. However, the emergence of datasets like Assembly101 [41], where subjects perform assembly tasks, poses a new challenge in the area of tempo-ral action segmentation due to the existence of long videos
Figure 1: Datasets like Assembly101 contain long videos of assembly tasks and an action label needs to be predicted for each frame. The first row shows some frames of a video.
The second row shows the ground-truth labels for all frames of the video where different colors correspond to different action labels. Rows 3-6 show the predictions of the proposed model for different amounts of long-term context where 100% means the temporal context of the full video. that can last up to 25 minutes. Since modeling the long-term context of such a video is very expensive, Yi et al. [61] pro-posed to compute the attention for a local temporal window.
In order to understand the impact of the window size on the temporal action segmentation accuracy, we analyze the impact of the window size on two datasets with long videos in Section 4.1. Fig. 1 shows some qualitative results of this study on Assembly101. Indeed, the results show that model-ing the long-term context of an entire video is very important for temporal action segmentation.
Based on this finding, we revisit how temporal attention is modeled in transformer architectures for temporal action segmentation. [61, 4] use a hierarchy of temporal windows, which makes the training on long video sequences as they occur in Assembly101 [41] very expensive.
Inspired by works that decompose attention over the spatial and temporal domain for short video clips [6, 62], we propose to iterate between computing windowed local attention and sparse long-term context attention such that both short and long-term context are modeled. This approach is particularly suitable for temporal action segmentation since the local attentions focus on the similarity or dissimilarity of features within an action segment or between neighboring action
segments whereas the long-term context attention focuses on the relations between actions within the entire video.
The source code is available at https://github.com/
LTContext/LTContext. 2.