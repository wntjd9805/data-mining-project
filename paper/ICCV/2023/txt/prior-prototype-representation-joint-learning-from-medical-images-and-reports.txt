Abstract
Contrastive learning based vision-language joint pre-training has emerged as a successful representation learn-ing strategy. In this paper, we present a prototype represen-tation learning framework incorporating both global and
In local alignment between medical images and reports. contrast to standard global multi-modality alignment meth-ods, we employ a local alignment module for fine-grained representation. Furthermore, a cross-modality conditional reconstruction module is designed to interchange informa-tion across modalities in the training phase by reconstruct-ing masked images and reports. For reconstructing long reports, a sentence-wise prototype memory bank is con-structed, enabling the network to focus on low-level local-ized visual and high-level clinical linguistic features. Ad-ditionally, a non-auto-regressive generation paradigm is proposed for reconstructing non-sequential reports. Ex-perimental results on five downstream tasks, including su-pervised classification, zero-shot classification, image-to-text retrieval, semantic segmentation, and object detection, show the proposed method outperforms other state-of-the-art methods across multiple datasets and under different dataset size settings. The code is available at https:
//github.com/QtacierP/PRIOR. 1.

Introduction
Powered by large-scale labeled natural image datasets, deep learning has achieved great success in computer vi-sion [17, 12, 41, 42, 51]. However, annotating medical im-ages is extremely expensive and labor-intensive [4, 45]. A (cid:0) Corresponding author: <tangxy@sustech.edu.cn>. practical approach is to first pre-train a model on a large-scale labeled natural image dataset like ImageNet [11], and then fine-tune it on the downstream medical image dataset with limited annotation [39, 37, 58]. This approach may nevertheless fail to achieve generalized performance due to the domain gap between natural images and medical images
[44, 33]. To effectively inherit representation from images of the same domain, self-supervised learning (SSL) meth-ods [5, 16, 6, 8] have been proposed through pre-training on unlabeled datasets. However, it has been suggested that the performance gain of pre-training on unlabeled medical im-ages is relatively limited compared to ImageNet initializa-tion [57]. There are potentially two reasons: 1) The sample size of medical images, even unlabeled, is still quite lim-ited compared to that of the ImageNet dataset; 2) Medical images often exhibit high inter-class similarity.
Recently, vision-language pre-training (VLP) has shown natural language supervision can effectively transfer lin-representation via well-guistic information to visual designed proxy tasks such as contrastive learning [38, 26, 54, 31, 25] and generative reconstruction [30, 55]. VLP may particularly work for medical image analysis since medi-cal reports are highly likely to be accessible in most situ-ations. However, jointly pre-training medical images and reports is still challenging. First, there are typically mul-tiple sentences in a medical report, and the textual infor-mation is highly complex [53, 27, 24]. Second, most de-scriptions in a medical report are exclusively related to spe-cific sub-regions in the corresponding medical image [32].
Most existing VLP methods tend to ignore fine-grained rep-resentation and may fail to transfer to locality-aware down-stream tasks such as semantic segmentation and object de-tection. Several methods have employed local alignment losses through contrastive tasks [18, 32, 50]. For exam-1
Figure 1. Illustration of the proposed sentence-wise prototype memory bank. The prototype embedding can group sentences sharing similar information. Each sentence representation is updated to the nearest prototype after querying. ple, Huang et al. design a localized feature representation framework, but it still falls into the global alignment cat-egory [18]. Wang et al. propose a local contrastive loss to align locality-aware information [50], but a vanilla con-trastive loss may easily ignore the similarity among nearby sub-regions due to their spatial locations and the overlapped sliding windows in convolution. To address this issue,
Muller et al. employ positiveness probability sampling to avoid selecting a nearby sub-region as the negative sample
[32]. However, this strategy is computationally heavy and time-consuming. Moreover, since contrastive learning is a discriminative SSL paradigm mainly focusing on high-level features, all those aforementioned methods tend to over-look low-level features, such as lesion boundaries in images and symptom descriptions in reports, which are neverthe-less highly crucial for downstream medical image analysis tasks.
In this paper, we present a Prototype Representation framework via joint global and local alignment between medical Images and repORts (PRIOR), wherein we effec-tively combine contrastive learning and cross-modality con-ditional reconstruction. We consider sentence in reports and sub-region in images as the elementary local representation units, and the global representation is obtained via atten-tion pooling over localized features. We propose a cross-modality alignment module to align representation between images and reports from both global and local views. To further learn locality-aware and fine-grained information, we utilize an encoder-decoder architecture to maximize the conditional mutual information between paired images and reports. The reconstruction decoder aims to reconstruct the masked image given the report and generate the report’s representation given the image. We make use of the prior information that descriptions of most medical reports es-sentially can be summarized by multiple structured labels
[19]. That is, sentence-level feature representation can be approximated by prototype categorization without any need to accurately retain redundant information such as syntax.
As shown in Figure 1, each sentence in a report is dis-cretely embedded as prototype representation. In this way, the sentence-level representation learning process can be treated as a classification-like task. Different from the im-age caption task which predicts each word auto-regressively
[52, 29], sentences in a medical report are usually non-sequential.
Inspired by [3], we use a parallel decoder to reconstruct sentence prototype embedding via bipartite matching. We successfully demonstrate the effectiveness of
PRIOR on three challenging datasets for five downstream tasks, including supervised classification, zero-shot classifi-cation, image-to-text retrieval, semantic segmentation, and object detection.
Our main contributions are four-fold:
• We propose a novel cross-modality alignment module utilizing both global and local information to capture fine-grained features. Compared with previous works, our proposed alignment is more effectively operated on local representation.
• Considering the writing paradigm of a medical report, we present a prototype memory bank for report’s sen-tence embeddings. Such discrete representation guides the linguistic features to further focus on high-level representation that links tightly to medical images.
• We leverage a conditional reconstruction task for both vision and language representation, which further fa-cilitates cross-modality feature interaction and ex-plores more structural and causal representation.
• PRIOR outperforms existing state-of-the-art (SOTA) methods on five tasks, including supervised classifi-cation, zero-shot classification, image-to-text retrieval, semantic segmentation, and object detection.
Figure 2. The overall framework of the proposed PRIOR. Given a pair of medical image and report, two independent encoders first encode each modality into a common embedding space. Then, the cross-modality alignment module aligns both global and local information between the two modalities. Finally, the cross-modality conditional reconstruction module reconstructs the masked image given the report and generates the sentence prototypes given the image. 2.