Abstract
Omnidirectional images (ODIs) have become increas-ingly popular, as their large field-of-view (FoV) can offer viewers the chance to freely choose the view directions in immersive environments such as virtual reality. The M¨obius transformation is typically employed to further provide the opportunity for movement and zoom on ODIs, but apply-ing it to the image level often results in blurry effect and aliasing problem. In this paper, we propose a novel deep learning-based approach, called OmniZoomer, to incorpo-rate the M¨obius transformation into the network for move-ment and zoom on ODIs. By learning various transformed feature maps under different conditions, the network is en-hanced to handle the increasing edge curvatures, which al-leviates the blurry effect. Moreover, to address the alias-∗ Intern at ARC Lab, Tencent PCG.
† Corresponding author ing problem, we propose two key components. Firstly, to compensate for the lack of pixels for describing curves, we enhance the feature maps in the high-resolution (HR) space and calculate the transformed index map with a spa-tial index generation module. Secondly, considering that
ODIs are inherently represented in the spherical space, we propose a spherical resampling module that combines the index map and HR feature maps to transform the feature maps for better spherical correlation. The transformed fea-ture maps are decoded to output a zoomed ODI. Experi-ments show that our method can produce HR and high-quality ODIs with the flexibility to move and zoom in to the object of interest. Project page is available at http:
//vlislab22.github.io/OmniZoomer/. 1.

Introduction
Omnidirectional images (ODIs) have garnered signif-icant attention as a means to maximize the amount of content and context captured within a single image, and
transformation into the network for freely moving and zooming in on ODIs, as shown in Fig. 1. By learning trans-formed feature maps in various conditions, the network is enhanced to handle the increasing curves caused by move-ment and zoom, as well as the inherent spherical distortion in ODIs.
In this case, the blurry effect can be solved to some extent, but the aliasing problem still exists, such as edge discontinuity and shape distortion (See Fig. 9(c)).
To further address the aliasing problem, we propose two key components. Firstly, to compensate for the lack of pix-els for describing curves, we propose to enhance the ex-tracted feature maps to high-resolution (HR) space before the transformation. The HR feature maps contain more fine-grained textural details, and are sufficient to represent the increasing curvatures and maintain the object shapes pre-cisely. We then propose a spatial index generation mod-ule (Sec. 3.2) to calculate the transformed index map based on the HR feature maps and M¨obius transformation matrix, which can be conducted on the HR feature space. Although applying M¨obius transformation on HR images with exist-ing super-resolution (SR) methods [24, 40, 44] can serve the same purpose, this solution is sub-optimal because the mod-els might not handle the increasing curves (See Tab. 1 and
Fig. 6). In addition, some image warping methods [36, 22] can learn the warping process in the network but are con-strained to estimate spatial-varying grids on the 2D plane, rather than the sphere. There are also some SR models de-signed for ODIs [8, 43]. However, they are limited to verti-cally captured ODIs or predetermined data structures.
Subsequently, we propose a spherical resampling mod-ule that combines the HR feature maps and transformed in-dex maps for feature map transformation. The spherical re-sampling is inspired by the inherent spherical representation of the ODIs and the spherical conformality of M¨obius trans-formation. It resamples based on the spherical geodesic of two points on the sphere, which better relates the original
HR feature maps and transformed ones. With HR feature representation and the spherical resampling module, Om-niZoomer alleviates the blurry effect and aliasing problem substantially, enabling moving and zooming in to an object of interest on ODIs with preserved shapes and continuous curves. Finally, these feature maps are processed with a de-coder to output a zoomed ODI. After movement and zoom,
OmniZoomer can generate more precise visual results with clear textural and structural details (See Fig. 2).
As collecting real-world ODI pairs under M¨obius trans-formation is difficult, we propose a dataset based on ODI-SR dataset [8], dubbed ODIM dataset, containing synthe-sized ODIs with various M¨obius transformations. We eval-uate the effectiveness of OmniZoomer on the ODIM dataset under various M¨obius transformations and up-sampling fac-tors. The experimental results show that OmniZoomer out-performs existing methods quantitatively and qualitatively.
Figure 2: Visual comparisons of different methods for movement and zoom. Our OmniZoomer predicts more continuous lines. there is a growing demand for utilizing such visual con-tent within devices, e.g., mobile apps and head-mounted displays (HMDs) for virtual reality (VR) [38]. To provide an interactive experience, these devices enable users to con-trol the view direction. However, most 360◦ cameras have a fixed focal length and do not support optical zoom, which causes the apparent size of objects in ODIs fixed. This lim-its the immersive experience when users expect to move and zoom in to an object of interest to see more details.
Generally, there exist three solutions to zoom in on the equirectangular projection (ERP) format ODIs or their per-spective patches. The first is to zoom in on ERP images uniformly. However, as ERP images have non-uniform pixel density in different latitudes [8], uniform zoom can severely distort the object shapes. The second is to zoom in on perspective patches projected from ODIs. As the per-spective patches of ODI have uniform pixel density [10], distortion problem can be solved. However, due to the lim-ited FoV, these patches only concentrate on local regions and ignore the relationship between each other during trans-formations. Thirdly, M¨obius transformation has recently been employed to provide movement and zoom freedom on
ODIs [34, 15, 27]. It is the only conformal bijective trans-formation on the sphere that preserves angles. However, applying M¨obius transformation on the image level often leads to blurry and aliasing problems due to two reasons.
Firstly, zoom-in makes a portion of the ODIs enlarged, mak-ing the enlarged region blurry and pixelated. Moreover, if 360◦ cameras are placed vertically, the ODIs suffer from distortion mainly in high-latitude regions and remain lots of straight lines in equator regions. After transformations, the appearance of the vertically captured ODIs varies greatly, resulting in more curves in both high-latitude and equator regions (See Fig. 2). Describing these curves with the same amount of pixels that originally represent straight lines be-comes challenging.
To obtain high-quality ODIs after movement and zoom, in this paper, we propose a novel deep learning-based ap-proach, dubbed OmniZoomer, to incorporate the M¨obius
Figure 3: The overall pipeline of the proposed OmniZoomer. With the spatial index generation module and spherical resampling module,
OmniZoomer can provide users with a flexible way to zoom in and out to objects of interest, such as the enlarged center building.
The main contributions of this paper can be summarized (I) We propose a novel deep learning-based as follows: approach, called OmniZoomer, to incorporate the M¨obius (II) We enhance transformation into the deep network. the feature maps to HR space and calculate the HR in-dex map with a spatial index generation module. We also propose a spherical resampling module for better spheri-(III) We establish ODIM dataset for su-cal correlation. pervised training. Compared with existing methods, Om-niZoomer achieves the state-of-the-art performance under various M¨obius transformations and up-sampling factors. 2.