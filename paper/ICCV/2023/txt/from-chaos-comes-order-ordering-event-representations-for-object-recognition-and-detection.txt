Abstract
Today, state-of-the-art deep neural networks that process events first convert them into dense, grid-like input repre-sentations before using an off-the-shelf network. However, selecting the appropriate representation for the task tradi-tionally requires training a neural network for each repre-sentation and selecting the best one based on the valida-tion score, which is very time-consuming. This work elimi-nates this bottleneck by selecting representations based on the Gromov-Wasserstein Discrepancy (GWD) between raw events and their representation. It is about 200 times faster to compute than training a neural network and preserves the task performance ranking of event representations across multiple representations, network backbones, datasets, and tasks. Thus finding representations with high task scores is equivalent to finding representations with a low GWD. We use this insight to, for the first time, perform a hyperpa-rameter search on a large family of event representations, revealing new and powerful representations that exceed the
*Equal contribution state-of-the-art. Our optimized representations outperform existing representations by 1.7 mAP on the 1 Mpx dataset and 0.3 mAP on the Gen1 dataset, two established object detection benchmarks, and reach a 3.8% higher classifica-tion score on the mini N-ImageNet benchmark. Moreover, we outperform state-of-the-art by 2.1 mAP on Gen1 and state-of-the-art feed-forward methods by 6.0 mAP on the 1
Mpx datasets. This work opens a new unexplored field of ex-plicit representation optimization for event-based learning.
Multimedia Material
For open-source code please visit https://github. com/uzh-rpg/event_representation_study. 1.

Introduction
Event cameras are biologically inspired vision sensors that function in a fundamentally distinct way [12]. Un-like traditional cameras that capture images at a fixed rate,
these cameras measure brightness changes independently for each pixel, and these changes are referred to as events.
The events encode the time, location, and polarity (sign) of the brightness changes. Event cameras offer several ad-vantages over frame-based cameras, including exception-ally high temporal resolution (in the order of µs), a high dynamic range, and low power consumption. Their numer-ous benefits make them attractive for a wide range of ap-plications like robotics, autonomous vehicles, and virtual reality. However, due to their sparse and asynchronous na-ture, applying classical computer vision algorithms remains challenging.
Many state-of-the-art deep learning models address this challenge by converting sparse and asynchronous events into dense grid-like representations before processing them with off-the-shelf deep neural networks. By using these networks, methods like this enjoy the advantages of ma-ture learning algorithms and network architectures, and op-timized hardware, but we need to make a non-trivial choice of event representation.
In fact, the computer vision and robotics fields are witnessing a surge in the number of re-search papers utilizing event-based vision, resulting in a plethora of new event representations being proposed. De-spite this, extensive comparisons of these representations remain rare, making it unclear whether these newer repre-sentations should be adopted.
No efficient methodology exists for comparing these rep-resentations. Conventionally, comparing event representa-tions involves training a fixed deep-learning model for each event representation separately and subsequently selecting the optimal one based on a validation score. This process is very time-intensive since it requires network training in the loop which often takes hours or days (Fig. 1 a top).
In this work, we propose a fast method to compare event representations which circumvents the need to train a neu-ral network and instead computes the Gromov-Wasserstein
Discrepancy (GWD) between the raw events and event rep-resentation (Fig. 1 a bottom). This metric effectively mea-sures the distortion that is introduced through converting raw events to representations and thus puts an upper bound on the amount of information that can be accessed by down-stream neural networks. We show extensive experimental evidence, that this metric preserves the task-performance ranking across a wide range of input representations for sev-eral datasets, neural network backbones and tasks (Fig. 1 b). Due to its low computational cost, we apply the
GWD to, for the first time, explicitly optimize over a large family of event representations, which reveals a new and powerful representation, which we term 12-channel Event
Representation through Gromov-Wasserstein Optimization (ERGO-12). For the task of object detection, networks trained with these representations outperform other repre-sentations by 1.9 mAP on the 1 Mpx dataset and 0.3 mAP on Gen1, even outperforming state-of-the-art methods by 2.1 mAP on Gen1 and state-of-the-art feed-forward meth-ods by 6.0 mAP on the 1 Mpx dataset. On object recog-nition, we instead find that our representation outperforms state-of-the-art representations by 3.8%. We believe that the GWD is a powerful tool that opens up a new research field that searches for optimized event representations. Our contributions are summarized as follows:
• We introduce a novel, efficient approach for compar-ing dense event representations using the Gromov-Wasserstein Discrepancy (GWD).
• We show extensive experimental evidence that it preserves the task performance ranking of neural networks trained with these representations across datasets, neural network backbones and tasks.
• We use it to, for the first time, conduct a hyperparame-ter search on a vast family of event representations, un-veiling novel and powerful event representations that outperform the current state-of-the-art representations on the object detection and object classification task. 2.