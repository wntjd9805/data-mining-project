Abstract
Conditional diffusion models have demonstrated impres-sive performance in image manipulation tasks. The gen-eral pipeline involves adding noise to the image and then denoising it. However, this method faces a trade-off prob-lem: adding too much noise affects the fidelity of the image while adding too little affects its editability. This largely limits their practical applicability. In this paper, we pro-pose a novel framework, Selective Diffusion Distillation (SDD), that ensures both the fidelity and editability of im-ages. Instead of directly editing images with a diffusion model, we train a feedforward image manipulation network under the guidance of the diffusion model. Besides, we pro-pose an effective indicator to select the semantic-related timestep to obtain the correct semantic guidance from the dif-fusion model. This approach successfully avoids the dilemma caused by the diffusion process. Our extensive experiments demonstrate the advantages of our framework. Code is re-leased at https://github.com/AndysonYs/Selective-Diffusion-Distillation. 1.

Introduction
In recent years, diffusion model [10, 14, 28, 34, 36, 38, 40– 43] has attracted great attention in both academic and in-dustrial communities. It models the Markov transition from a Gaussian distribution to a data distribution to generate high-quality images sequentially. The elegant formulation achieves state-of-the-art performance in various image gener-ation benchmarks. Meanwhile, text-to-image diffusion mod-els [28, 34, 36, 38] also demonstrate their impressive capacity in controllable image synthesis, enabling a wide range of practical applications. Among them, one of the most inter-esting applications is image manipulation.
A typical text-guidance manipulation pipeline is to invert the input image to a noisy latent and then denoise this latent
*Equal contribution
†Corresponding author
Figure 1. Editability and fidelity trade-off of diffusion-based image manipulation. The leftmost is the input image. For each manipulation, we add increasing noise levels from left to right and then denoise the image. Different semantics require different levels of noise to manipulate. with a given text prompt. The inversion process can be the simple noise adding [25] or the DDIM inversion [41, 44].
There is an editability-fidelity tradeoff in such pipelines, as shown in Fig. 1. Adding a lot of noise to the original image gives the diffusion model more freedom to manipulate the image, but it also makes it harder to retain original semantics when denoising back, and vice versa.
More importantly, this trade-off can lead to unsuccessful manipulations, such as “white hair” in Fig. 1. This is because the diffusion model processes different semantics at different stages [6]. If our manipulation corresponds to the semantics in the early stages, we have to add more noise, hence losing much information from the original image. To overcome the failure caused by the trade-off, existing methods attempt to incorporate more guidance information, such as using masks to limit the manipulation region [1, 2, 24]. This is useful for local image manipulation, but for some global structures, such as changing the pose of a human face, it still fails to solve the problem.
In this work, we take a different viewpoint to leverage a diffusion model for image manipulation. Unlike existing methods that directly manipulate images progressively with diffusion models, we train an efficient image manipulator
supervised by a pretrained diffusion model. Specifically, our model takes a feedforward model (e.g., latent space image manipulation model [29]) as the manipulator, and a text-guided diffusion model (e.g., latent diffusion [36]) as the supervisor. During training, the manipulated image is dif-fused and fed to the diffusion model, and the diffusion model produces gradient supervision based on the text condition.
In this sense, it is expected that the manipulator could mimic the promising generation capacity of the diffusion model.
To learn such a manipulator, using correct semantic guid-ance is also crucial, as shown in Fig. 1. Intuitively, the diffu-sion model at the timestep in the red box has a better ability to guide the image manipulator in changing the hair color.
In contrast, other timesteps do not provide useful semantic guidance. Therefore, we propose the hybrid quality score (HQS), an effective indicator that helps to select appropri-ate timesteps. This indicator is built upon the entropy and
L1 norm of the gradient from the diffusion model and is shown to be highly correlated with the manipulation qual-ity. As such, our model could learn with the most effective guidance. In addition to solving the trade-off problem of diffusion-based image manipulation, we have another bonus: our image manipulator requires only one forward for ma-nipulation, and by learning in a specific domain, our image manipulator can manipulate images over the entire domain, which offsets the cost of training. Extensive experiments demonstrate the effectiveness of our methods.
Our contributions are summarized as follows:
• We propose a novel image manipulation approach with a well-trained diffusion model to supervise another im-age manipulator. This avoids the trade-off problem of manipulating images with a diffusion process.
• We propose the hybrid quality score to detect semantic-related timesteps. Only during these timesteps can the diffusion model guide the image manipulator to per-form accurate manipulations.
• Our experiments demonstrate our method’s effective-ness and efficiency in both the qualitative and the quan-titative aspects. 2.