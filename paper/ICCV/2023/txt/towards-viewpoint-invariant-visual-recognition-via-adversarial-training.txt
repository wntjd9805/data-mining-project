Abstract
Visual recognition models are not invariant to viewpoint changes in the 3D world, as different viewing directions can dramatically affect the predictions given the same object.
Compared to 2D transformations, the exploration of 3D viewpoint invariance deserves more attention for its greater practical significance. Motivated by the success of ad-versarial training in promoting model robustness, we pro-pose Viewpoint-Invariant Adversarial Training (VIAT) to improve viewpoint robustness of common image classifiers.
By regarding viewpoint transformation as an attack, VIAT is formulated as a minimax optimization problem, where the inner maximization characterizes diverse adversarial view-points by learning a Gaussian mixture distribution based on a new attack GMVFool, while the outer minimization trains a viewpoint-invariant classifier by minimizing the ex-pected loss over the worst-case adversarial viewpoint dis-tributions. To further improve the generalization perfor-mance, a distribution sharing strategy is introduced lever-aging the transferability of adversarial viewpoints across objects. Experiments validate the effectiveness of VIAT in improving the viewpoint robustness of various image clas-sifiers based on the diversity of adversarial viewpoints gen-erated by GMVFool. 1.

Introduction
The ability of learning invariant representations is highly desirable in numerous computer vision tasks [6, 17] and is conducive to model robustness under semantic-preserving image transformations. Previous works [15, 60, 9, 45] have striven to make visual recognition models invariant to im-age translation, rotation, reflection, and scaling. However, they mainly consider invariances to 2D image transforma-tions, leaving the viewpoint transformation [58] in the 3D world less explored. It has been shown that visual recogni-tion models are susceptible to viewpoint changes [2, 5, 13],
*Corresponding author.
Figure 1. An illustration of viewpoint changes on model perfor-mance. We show the loss landscape w.r.t. yaw and pitch of the camera, which demonstrates multiple regions of adversarial view-points (We use ResNet-50 as the target model [21]). exhibiting a significant gap from the human vision that can robustly recognize objects under different viewpoints [7].
Due to the naturalness and prevalence of viewpoint varia-tions in safety-critical applications (e.g., autonomous driv-ing, robotics, surveillance, etc.), it is thus imperative to en-dow visual recognition models with viewpoint invariance.
Despite the importance, it is extremely challenging to build viewpoint-invariant visual recognition models since typical networks take 2D images as inputs without inferring the structure of 3D objects. As an effective data-driven ap-proach, adversarial training augments training data with ad-versarially generated samples under a specific threat model and shows promise to improve model invariance/robustness to additive adversarial perturbations [63, 59, 54, 57], im-age translation and rotation [15], geometric transformations
[28], etc. However, it is non-trivial to directly apply ad-versarial training to improving viewpoint robustness due to the difficulty of generating the worst-case adversarial view-points. A pioneering work [13] proposes ViewFool, which encodes real-world 3D objects as Neural Radiance Fields (NeRF) [36] given multi-view images and performs black-Figure 2. An overview of our VIAT framework. We first train the NeRF representation of each object given multi-view images. The inner maximization learns a Gaussian mixture distribution of adversarial viewpoints by maximizing the expectation of classification loss and entropy regularization. The outer minimization samples adversarial viewpoints from the optimized distributions and renders 2D images from adversarial viewpoints, which are fed into the network along with clean samples to train viewpoint-invariant classifiers. box optimization for generating a distribution of adversarial viewpoints. Though effective, ViewFool only adopts a uni-modal Gaussian distribution, which is inadequate to char-acterize multiple local maxima of the loss landscape w.r.t. viewpoint changes, as shown in Fig. 1. We verify that this can lead to overfitting of adversarial training to the specific attack. Besides, ViewFool is time-consuming to optimize, making adversarial training intractable.
To address these problems, in this paper, we propose
Viewpoint-Invariant Adversarial Training (VIAT), the first framework to improve the viewpoint robustness of vi-sual recognition models via adversarial training. As shown in Fig. 2, VIAT is formulated as a distribution-based min-imax problem, in which the inner maximization aims to optimize the distribution of diverse adversarial viewpoints while the outer minimization aims to train a viewpoint-invariant classifier by minimizing the expected loss over the worst-case adversarial viewpoint distributions. To address the limitations of ViewFool, we propose GMVFool as a practical solution to the inner problem, which generates a
Gaussian mixture distribution of adversarial viewpoints for each object, with increased diversity to mitigate overfitting of adversarial training. To accelerate training, we adopt a stochastic optimization strategy to reduce the time cost of training and adopt Instant-NGP [38], a fast variant of NeRF, to improve the efficiency of the optimizing process.
In outer maximization, to further improve generalization, we propose a distribution-sharing strategy given the observa-tion that adversarial viewpoint distributions are transferable across objects within the same class. We fine-tune classi-fiers on a mixture of natural and sampled adversarial view-point images to improve their viewpoint invariance.
To verify VIAT’s ability of training a viewpoint-invariant model, a multi-view dataset is required. However, previous datasets [16, 29, 41, 10] usually have limited realism and viewpoint range, posing challenges when applying them to this topic. To address this, we devoted significant effort to creating a new multi-view dataset—IM3D, which contains 1k typical synthetic 3D objects from 100 classes, tailored specifically for ImageNet categories. IM3D has several no-table advantages compared to previous datasets, as shown in Table 1: (1) It covers more categories. (2) It utilizes physics-based rendering (PBR) technology1 to produce re-alistic images. (3) It has accurate camera pose annotations and is sampled from a spherical space, leading to better re-construction quality and exploration of the entire 3D space.
Thus, we mainly use it for training and further evaluating our method on other multi-view datasets. We will release our IM3D dataset, which includes multi-view images, 3D source files, and corresponding Instant-NGP weights.
We conduct extensive experiments to validate the effec-tiveness of both GMVFool and VIAT for generating adver-sarial viewpoints and improving the viewpoint robustness of image classifiers. Experimental results show that GMVFool characterizes more diverse adversarial viewpoints while maintaining high attack success rates. Based on it, VIAT significantly improves the viewpoint robustness of image classifiers ranging from ResNet [21] to Vision Transformer (ViT) [14] and shows superior performance compared with alternative baselines. Moreover, we construct a new out-of-distribution (OOD) benchmark—ImageNet-V+, containing nearly 100k images from the adversarial viewpoints found by GMVFool. It is 10× larger than the previous ImageNet-V benchmark [13]. We hope to serve it as a standard bench-mark for evaluating viewpoint robustness in the future. 1A 3D modeling and rendering technique that enables physically realistic effects.
Dataset
#Objects
#Classes
ALOI [16]
MIRO [29]
OOWL [25]
CO3D [41]
ABO [10]
Dong et al. [13]
IM3D (Ours) 1K 120 500 18.6K 8K 100 1K
-12 25 50 63 85 100
PBR (cid:37) (cid:37) (cid:37) (cid:37) (cid:33) (cid:33) (cid:33)
Full 3D (cid:37) (cid:37) (cid:37) (cid:37) (cid:33) (cid:33) (cid:33)
Spherical Pose (cid:33) (cid:33) (cid:37) (cid:37) (cid:37) (cid:33) (cid:33)
Table 1. Comparison of our multi-view dataset with others. 2.