Abstract
Token compression aims to speed up large-scale vision transformers (e.g. ViTs) by pruning (dropping) or merg-ing tokens.
It is an important but challenging task. Al-though recent advanced approaches achieved great suc-cess, they need to carefully handcraft a compression rate (i.e. number of tokens to remove), which is tedious and leads to sub-optimal performance. To tackle this problem, we propose Differentiable Compression Rate (DiffRate), a novel token compression method that has several appeal-ing properties prior arts do not have. First, DiffRate en-ables propagating the loss function’s gradient onto the com-pression ratio, which is considered as a non-differentiable hyperparameter in previous work.
In this case, different layers can automatically learn different compression rates layer-wisely without extra overhead. Second, token pruning and merging can be naturally performed simultaneously in
DiffRate, while they were isolated in previous works. Third, extensive experiments demonstrate that DiffRate achieves state-of-the-art performance. For example, by applying the learned layer-wise compression rates to an off-the-shelf
ViT-H (MAE) model, we achieve a 40% FLOPs reduction and a 1.5× throughput improvement, with a minor accu-racy drop of 0.16% on ImageNet without ﬁne-tuning, even outperforming previous methods with ﬁne-tuning. Codes and models are available at https://github.com/
OpenGVLab/DiffRate. 1.

Introduction
Vision Transformer (ViT) [7] has rapidly developed and achieved state-of-the-art performance in various vision tasks such as image classiﬁcation[26], object detection [45], and semantic segmentation [36, 12, 15, 16]. Due to the
ﬂexibility in handling various input formats, ViT has also
∗Corresponding authors: Rongrong Ji (rrji@xmu.edu.cn)
† This work was done during his internship at Shanghai AI Laboratory.
‡ Equal Contribution
Figure 1: Comparison of different token compression methods including token pruning EViT [23], token merging
ToMe [1] and our method. Pruned tokens are represented by black and non-border, while merged tokens are repre-sented by patches with the same inner and border color. (a) shows that our method achieves better top-1 accuracy on ImageNet with FLOPs of 2.3G when compressing pre-trained Deit-S [32] without ﬁne-tuning. (b) and (c) show that previous methods typically focus on either pruning or merging tokens using hand-picked compression rate with the guidance of performance. But our method leverages both approaches simultaneously to achieve more effective compression using the differentiable compression rate with gradient optimization. been widely applied to self-supervised learning [13] and other modalities [9, 37]. Despite the remarkable success,
ViTs suffer from the intensive computational complexity that increases quadratically with the token length in the self-attention layer, presenting a challenge for practical applica-tions. Therefore, it is crucial to improve the efﬁciency of
ViTs in order to make them more widespread.
In pursuit of efﬁcient ViTs, various network compres-sion techniques such as weight pruning [39, 34], quanti-zation [27, 42], distillation [17, 35] and so on have been investigated. Among them, token compression[29, 1, 23]
has emerged as a promising approach to reduce redundancy in ViT for several appealing properties. First, token com-pression can be applied without any modiﬁcations to the network structure by exploiting the input-agnostic nature of transformers. Second, token compression is orthogonal to previous compression methods, making it a complementary approach to existing techniques [34, 4].
Existing token compression approaches typically include token pruning [29, 23, 20] and token merging [1]. As shown in Fig. 1(a), token pruning preserves informative tokens by measuring the importance of tokens with a deﬁned met-ric. It can easily identify irrelevant background tokens with low importance. On the other hand, token merging com-presses tokens by merging them with a large semantic sim-ilarity, which can not only discard some background tokens but also merge less informative foreground tokens. How-ever, both pruning and merging handcraft a compression rate (i.e. the ratio of removed tokens and total tokens) for each transformer layer as shown in Fig. 1(b), which has two drawbacks. First, since model complexity metrics such as
FLOPs are related to compression rates in each layer, it is te-dious for practitioners to set layer-wise compression rates in order to meet the complexity constraints while retaining the performance of ViTs as much as possible. Second, infor-mative foreground tokens are prone to being discarded with a hand-picked compression rate, which results in perfor-mance degradation. As shown in Fig. 1(a), when compress-ing tokens at ﬁxed rates, token pruning such as EViT [23] removes most of the informative foreground tokens while token merging [1] also merges many important foreground tokens into a single token, leading to a sudden drop of top-1 accuracy on ImageNet.
To tackle the above issues, this work proposes a uniﬁed token compression framework called Differentiable Com-pression Rate (DiffRate), where both pruning and merging compression rates are determined in a differentiable man-ner. To achieve this goal, we propose a novel method,
In namely Differentiable Discrete Proxy (DDP) module.
DDP, a token sorting procedure is ﬁrst performed to identify important tokens with a token importance metric. Then, a re-parameterization trick enables us to optimally select top-K important tokens with gradient back-propagation. In this way, all input images would have the top-K important to-kens preserved, making it possible for parallel batch com-putation. Notably, the optimization process of DiffRate is highly efﬁcient and can converge within 3 epochs (i.e. 2.7
GPU hours for ViT-B).
Thanks to the inclusion of differentiable compression rates, DiffRate can leverage the beneﬁts of token pruning and merging by seamlessly integrating both techniques into a forward pass. This is possible because both token prun-ing and merging are capable of determining the optimal set of tokens to preserve. As shown in Fig. 1(a), DiffRate can prune most irrelevant background tokens and preserve detailed foreground information, leading to a good trade-off between efﬁciency and performance. With the learned compression rate, DiffRate achieves state-of-the-art perfor-mance in compressing various ViTs. For example, DiffRate can compress an off-the-shelf ViT-H model pre-trained by
MAE [13] with 40% FLOPs reduction and 50% through-put improvement with only 0.16% accuracy drop, outper-forming previous methods that require tuning the network parameter.
Our contributions are summarized as follows:
• We develop a uniﬁed token compression framework,
Differentiable Compression Rate (DiffRate), that in-cludes both token pruning and merging, and formulate token compression as an optimization problem.
• DiffRate employs a Differentiable Discrete Proxy which consists of a token sorting procedure and a re-parameterization trick to determine the optimal com-pression rate under different computation cost con-straints. To our knowledge, it is the ﬁrst study to ex-plore differentiable compression rate optimization in token compression.
• Through extensive experiments, we demonstrate that
DiffRate outperforms previous methods and achieves state-of-the-art performance on the off-the-shelf mod-els. We hope that DiffRate can advance the ﬁeld of token compression and improve the practical applica-tion of Vision Transformers (ViTs). 2.