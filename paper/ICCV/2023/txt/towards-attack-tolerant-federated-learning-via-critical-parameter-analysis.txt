Abstract
Federated learning is used to train a shared model in a decentralized way without clients sharing private data with each other. Federated learning systems are susceptible to poisoning attacks when malicious clients send false updates to the central server. Existing defense strategies are ineffec-tive under non-IID data settings. This paper proposes a new defense strategy, FedCPA (Federated learning with Criti-cal Parameter Analysis). Our attack-tolerant aggregation method is based on the observation that benign local models have similar sets of top-k and bottom-k critical parameters, whereas poisoned local models do not. Experiments with different attack scenarios on multiple datasets demonstrate that our model outperforms existing defense strategies in defending against poisoning attacks. 1.

Introduction
The proliferation of computing devices like mobile phones has led to an increase in proprietary user data. The abun-dance of user data offers the opportunity to create numer-ous applications but also raises concerns about data pri-vacy. Federated learning (FL) is a cutting-edge collabora-tive technique that addresses the privacy challenge by en-abling machine learning on decentralized devices without exchanging locally stored data [26]. For example, a promi-nent FL model, FedAvg [17], works as follows: Given a central server and multiple clients, the central server selects a random subset of clients and sends the global model to them. Then, each selected client uses its own data to op-timize the local model and sends back the model update to the central server. The central server takes the average of these received updates to construct a new global model.
This FL framework enables a decentralized system to train a globally shared model via aggregating updates from local models while preserving data privacy.
However, the averaging operation used in the central server leaves room for poisoning attacks [2, 16] when mali-*Equal contribution to this work. cious clients pose as ordinary clients and submit fraudulent model updates. Attackers can not only impede the con-vergence of model training and degrade performance [23] (which is called untargeted attacks) but they can also manipulate model updates by injecting a backdoor into the resulting global model without substantially degrading its performance [25] (which is called targeted attacks).
Several defense strategies have been proposed to elim-inate false updates from potentially malicious clients and maintain benign updates on FL systems. For instance, one idea is to use outlier-resistant statistics such as the median or trimmed mean [31, 32] rather than the average in model aggregation. Blanchard et al. [3] proposed Krum, which removes atypical model updates with low local density compared to their k-nearest neighbors. Fung et al. [8] and Fu et al. [7] proposed weighted averaging of local updates in proportion to each update’s normality level.
Nevertheless, these defense strategies cannot detect adver-saries in so-called non-IID (non-independent, identically distributed) situations, where data distributions vary sub-stantially among clients. Existing defense strategies project model updates as individual Euclidean vectors and evaluate their abnormality based on their distances from other model updates. Meanwhile, the non-IID property leads to diverse benign updates, which makes malicious and benign updates indistinguishable in Euclidean space. As a result, existing defense strategies become ineffective [2, 19].
This paper presents FedCPA (Federated learning with
Critical Parameter Analysis), an attack-tolerant aggregation method for FL under non-IID data settings. Inspired by a recent observation that not all model parameters contribute equally to optimization [6, 28], we assess the importance of the model parameters in every client’s update. Our analysis shows that benign model updates share similar sets of top-k and bottom-k important parameters, even under non-IID data. However, this pattern is not observed for malicious model updates. Based on this observation, we propose a new defense strategy tailored for FL systems to measure model similarity, which extends beyond the extant
Euclidean-based similarity and provides an efﬁcient way to discard updates from clients that are likely malicious.
FedCPA consists of two steps: (1) computing the normality score of each client’s model concerning param-eter importance and (2) aggregating local updates via a weighted average to remove the effect of likely-malicious updates. In the ﬁrst step, the importance of each parameter is computed by multiplying its value by its change after local training. The resulting parameters are then ranked in order of importance. Top-k and bottom-k most important parameters are extracted for each client’s model and used to compute the similarity among clients’ models. Then, we deﬁne the normality of the model update to measure its similarity with other model updates. Model updates that differ from other updates are considered malicious.
In the second step, outlier local updates are ﬁltered out by adjusting their weights regarding their normality scores.
Our evaluation demonstrates that FedCPA protects against both untargeted and targeted attacks better than ex-isting methods such as Multi-Krum [3], FoolsGold [8], and
ResidualBase [7]. We make the following contributions:
• We empirically show that benign local models in fed-erated learning exhibit similar patterns in how param-eter importance changes during training. The top and bottom parameters have smaller rank order disruptions than the medium-ranked parameters.
• Based on the data observation that holds over non-IID cases, we present a new metric for measuring model similarity (Eq. 5). With this measure, FedCPA can efﬁciently assess the normality of each local update, enabling attack-tolerant aggregation.
• Extensive experiments demonstrate the superiority of FedCPA in terms of defense performance. For example, FedCPA reduces the success rate of targeted attacks by a factor of 3 (from 51.4% to 21.9%) on
CIFAR-10 and by a factor of 2 (from 74.6% to 43.2%) on TinyImageNet.
The proposed model can be used in various federated learning contexts as a more robust and attack-tolerant decentralized computing framework. Codes are available at https://github.com/Sungwon-Han/FEDCPA. 2.