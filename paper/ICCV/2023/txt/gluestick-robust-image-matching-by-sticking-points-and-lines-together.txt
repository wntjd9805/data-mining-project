Abstract
Line segments are powerful features complementary to points. They offer structural cues, robust to drastic view-point and illumination changes, and can be present even in texture-less areas. However, describing and matching them is more challenging compared to points due to partial oc-clusions, lack of texture, or repetitiveness. This paper intro-duces a new matching paradigm, where points, lines, and their descriptors are uniﬁed into a single wireframe struc-ture. We propose GlueStick, a deep matching Graph Neu-ral Network (GNN) that takes two wireframes from differ-ent images and leverages the connectivity information be-In addition to tween nodes to better glue them together. the increased efﬁciency brought by the joint matching, we also demonstrate a large boost of performance when lever-aging the complementary nature of these two features in a single architecture. We show that our matching strat-egy outperforms the state-of-the-art approaches indepen-dently matching line segments and points for a wide vari-ety of datasets and tasks. The code is available at https:
//github.com/cvg/GlueStick. 1.

Introduction
Line segments are high-level geometric structures use-ful in a wide range of computer vision tasks such as
SLAM [18, 81, 41], pose estimation [68], construction mon-itoring [25, 4], and 3D reconstruction [21, 74, 80]. Lines are ubiquitous in structured scenes and offer stronger con-straints than feature points. In particular, lines shine in low-textured scenes where point-based approaches struggle.
However, compared to keypoints, line segments are often poorly localized in the image and suffer from lower repeata-bility. Line segments are also more challenging to describe since they can cover a large spatial extent in the image and suffer from occlusions and perspective effects due to view-point changes. Furthermore, lines often appear as part of repetitive structures in human-made environments, making
* Authors contributed equally.
] 7 4
[ e u l
G r e p u
S
) a ( s r u
O
) b (
] 3 7
[
R
T e n i
L
) c ( s r u
O
) d (
Figure 1: Joint matching of points and lines. Match-ing feature points often fails in textureless areas (a), while current line matching methods struggle with large view-point changes (c). We propose GlueStick, a network jointly matching points and lines. While none of the methods were trained on the rotations of (a)(b), line matches can guide
GlueStick while SuperGlue [47] fails, and vice-versa in (d) where points can complement the line matching. For clarity reasons, we show here only the correct matches. classical descriptor-based matching fail. For this reason, typical matching heuristics such as mutual nearest neighbor and Lowe’s ratio test [34] are often less effective for lines.
Recently, deep learning has ushered in a new paradigm for feature point matching using Graph Neural Networks (GNNs) [47, 57]. This new approach bypasses the need
for matching heuristics or even outlier removal techniques, thanks to the high precision of the predicted matches [47].
A key component to achieve this is to leverage the posi-tional encoding of keypoints directly in the network and to let it combine visual features with geometric informa-tion [47, 57, 59, 24]. Letting the GNN reason with all fea-tures simultaneously brings in additional context and can disambiguate repetitive structures (Fig. 1).
Even though recent advances leveraged similar ideas to enrich line descriptors [73], directly transferring this GNN approach to line matching is not trivial. The large ex-tent of lines and their lack of repeatability make it hard to ﬁnd a good feature representation for them. In this pa-per, we take inspiration from SuperGlue [47] and introduce
GlueStick, to jointly match keypoints and line segments.
Our goal is to leverage their complementary nature in the matching process. By processing them together in a sin-gle GNN, the network can learn to resolve ambiguous line matches by considering nearby distinctive keypoints, and vice versa. We propose to leverage the connectivity be-tween points and lines via a uniﬁed wireframe structure, ef-fectively replacing previous handcrafted heuristics for line matching [75, 49, 32] by a data-driven approach.
Our network takes as input sparse keypoints, lines, and their descriptors extracted from an image pair, and outputs a set of locally discriminative descriptors enriched with the context from all features in both images, before establish-ing the ﬁnal matches.
Inside the network, keypoints and line endpoints are represented as nodes of a wireframe.
The network is composed of self-attention layers between nodes, cross-attention layers exchanging information across the two images, and a new line message passing module enabling communication between neighboring nodes of the wireframe. After the GNN, points and lines are split into two separate matching matrices and a dual-softmax is used to ﬁnd the ﬁnal assignment of the features. Overall, our contributions are as follows: 1. We replace heuristic geometric strategies for line matching with a data-driven approach, by jointly matching points and lines within a single network. 2. We offer a novel architecture exploiting the local con-nectivity of the features within an image. 3. We experimentally show large improvements of our method over previous state-of-the-art point and line matchers on a wide range of datasets and tasks. 2.