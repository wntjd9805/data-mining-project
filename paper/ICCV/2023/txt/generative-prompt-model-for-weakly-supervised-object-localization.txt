Abstract
Weakly supervised object localization (WSOL) remains challenging when learning object localization models from image category labels. Conventional methods that discrim-inatively train activation models ignore representative yet less discriminative object parts. In this study, we propose a generative prompt model (GenPromp), defining the first generative pipeline to localize less discriminative object parts by formulating WSOL as a conditional image denois-ing procedure. During training, GenPromp converts image category labels to learnable prompt embeddings which are fed to a generative model to conditionally recover the in-put image with noise and learn representative embeddings.
During inference, GenPromp combines the representative embeddings with discriminative embeddings (queried from an off-the-shelf vision-language model) for both represen-tative and discriminative capacity. The combined embed-dings are finally used to generate multi-scale high-quality attention maps, which facilitate localizing full object extent.
Experiments on CUB-200-2011 and ILSVRC show that
GenPromp respectively outperforms the best discriminative models by 5.2% and 5.6% (Top-1 Loc), setting a solid base-line for WSOL with the generative model. Code is available at https://github.com/callsys/GenPromp. 1.

Introduction
Weakly supervised object localization (WSOL) is a chal-lenging task when provided with image category super-vision but required to learn object localization models.
As a pioneered WSOL method, Class Activation Map (CAM) [67] defines global average pooling (GAP) to gen-erate semantic-aware localization maps based on a dis-criminatively trained activation model. Such a fundamen-tal method, however, suffers from partial object activation while often missing full object extent, Fig. 1(upper). The nature behind the phenomenon is that discriminative mod-els are born to pursue compact yet discriminative features while ignoring representative ones [3, 20].
*Corresponding author
Figure 1: Comparison of our generative prompt model (GenPromp) with the discriminatively trained activa-tion model. GenPromp aims to localize less discriminative object parts by formulating WSOL as a conditional image denoising procedure. Red, green, and black arrows respec-tively denote information propagation during training, in-ference, and training & inference. fd, fr, and fc denote the discriminative, representative, and combined embeddings.
Many efforts have been proposed to alleviate the par-tial activation issue by introducing spatial regularization terms [32, 33, 34, 52, 59, 60, 65, 66], auxiliary localiza-tion modules [33, 36, 54, 55], or adversarial erasing strate-gies [13, 14, 34, 65]. Nevertheless, the fundamental chal-lenge about how to use a discriminatively trained classifica-tion model to generate precise object locations remains.
In this study, we propose a generative prompt model (GenPromp), Fig. 1(lower), which formulates WSOL as a conditional image denoising procedure, solving the fundamental partial object activation problem in a new and systematic way. During training, for each category (e.g.goldfish in Fig. 1) in the predetermined category set, GenPromp converts each category label to a learn-able prompt embedding (fr) through a pre-trained vision-language model (CLIP) [40]. The learnable prompt embed-ding is then fed to a transformer encoder-decoder to condi-Figure 2: Activation maps and localization results using discriminative and representative embeddings. A proper combination (w=0.6) of discriminative embeddings fd with representative embedding fr as the prompt produces precise activation maps and good WSOL results (Red and green boxes respectively denote GT boxes and localization results). tionally recover the noisy input image. Through multi-level denoising, the representative features of input images are back-propagated from the transformer decoder to the learn-able prompt embedding, which is updated to the represen-tative embedding fr.
During inference, GenPromp linearly combines learned representative embeddings (fr) with discriminative embed-dings (fd) to obtain both object generative and discrimi-nation capability, Fig. 2. fd is queried from a pre-trained vision-language model (CLIP), which incorporates the cor-respondence between text (e.g.category labels) with vi-sion feature embeddings. The combined embedding (fc) is used to generate attention maps at multiple levels and timestamps, which are aggregated to object activation maps through a post-processing strategy. On CUB-200-2011 [49] and ImageNet-1K [45], GenPromp respectively outper-forms the best discriminative models by 5.2% (87.0% vs. 81.8% [55]) and 5.6% (65.2% vs. 59.6% [55]) in Top-1
Loc.
The contributions of this study include:
• We propose a generative prompt model (GenPromp), providing a systematic way to solve the inconsistency between the discriminative models with the generative localization targets by formulating a conditional image denoising procedure.
• We propose to query discriminative embeddings from an off-the-shelf vision-language model using image la-bels as input. Combining the discriminative embed-dings with the generative prompt model facilitates lo-calizing objects while depressing backgrounds.
• GenPromp significantly outperforms its discriminative counterparts on commonly used benchmarks, setting a solid baseline for WSOL with generative models. 2.