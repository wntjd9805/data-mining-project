Abstract
Class-incremental learning (CIL) has achieved remark-able successes in learning new classes consecutively while overcoming catastrophic forgetting on old categories. How-ever, most existing CIL methods unreasonably assume that all old categories have the same forgetting pace, and neglect negative influence of forgetting heterogeneity among differ-ent old classes on forgetting compensation. To surmount the above challenges, we develop a novel Heterogeneous
Forgetting Compensation (HFC) model, which can resolve heterogeneous forgetting of easy-to-forget and hard-to-forget old categories from both representation and gradient as-pects. Specifically, we design a task-semantic aggrega-tion block to alleviate heterogeneous forgetting from rep-resentation aspect. It aggregates local category informa-tion within each task to learn task-shared global represen-tations. Moreover, we develop two novel plug-and-play losses: a gradient-balanced forgetting compensation loss and a gradient-balanced relation distillation loss to alleviate forgetting from gradient aspect. They consider gradient-balanced compensation to rectify forgetting heterogeneity of old categories and heterogeneous relation consistency.
Experiments on several representative datasets illustrate ef-fectiveness of our HFC model. The code is available at https://github.com/JiahuaDong/HFC. 1.

Introduction
Class-incremental learning (CIL) [40, 12, 5] has attracted appealing attentions recently by accumulating previous learned experience to learn new classes incrementally. It
*Equal contributions. †The corresponding author is Prof. Yang Cong.
‡This work was supported in part by the National Nature Science Foun-dation of China under Grant 62127807, 62273333 and 62133005.
Figure 1. Forgetting heterogeneity versus accuracy on CIFAR-100
[33] when the backbone is ViT-Base and number of tasks is 5. plays an indispensable role in developing a large number of intelligent learning systems, such as autonomous driving
[38] and automated surveillance [26]. When learning new classes continuously under the settings of limited memory to replay all previous data of old classes [45], these CIL systems heavily suffer from forgetting on old classes caused by severe class imbalance between old and new categories
[40, 31, 14]. To surmount catastrophic forgetting, a growing amount of CIL methods mainly perform knowledge distilla-tion [24] to preserve past experience [34, 12, 35]; introduce a rehearsal strategy to replay part of old data [40, 17]; or dynamically expand network architectures [28, 41, 54].
However, most existing CIL methods [40, 45, 54, 18] ne-glect heterogeneous forgetting speeds of different old classes.
They unrealistically assume that all old classes suffer from the same degree of catastrophic forgetting, and compensate forgetting for each old classes equally and independently.
Such impracticable assumption enforces existing CIL mod-els [40, 56, 61] to suffer from imbalanced gradient optimiza-tion among different old classes, thus favoring more forget-ting compensation for hard-to-forget old classes while ne-glecting those easy-to-forget old classes (i.e., heterogeneous forgetting). More importantly, such forgetting heterogeneity can significantly worsen the forgetting on hard-to-forget old categories, when new streaming classes becomes part of old categories continually. For example, some old classes with various modalities and appearances (e.g., person and car) in autonomous driving [38] are more difficult to explore task-shared representations across different incremental tasks, when compared with other hard-to-forget old classes (e.g., road and traffic sign) with easily-distinguished vi-sual properties. This phenomenon causes imbalanced gradi-ent propagation between easy-to-forget and hard-to-forget old categories, thus exacerbating heterogeneous forgetting speeds among different old classes. When autonomous ve-hicles [38] learn new classes continually, such forgetting heterogeneity aggravates forgetting on those hard-to-forget old classes (e.g., traffic sign) to some extent.
Inspired by the above practical scenario, we investigate that different old classes have significant forgetting hetero-geneity in this paper, as shown in Fig. 1. This heterogeneous forgetting might heavily weaken forgetting compensation on hard-to-forget old classes, as new classes become a subset of old classes consecutively. In summary, the challenges to tackle heterogeneous forgetting lie in two major aspects:
• Representation Aspect: Some easy-to-forget old classes (e.g., car and bus) with diverse appearances are more difficult for existing CIL methods [45, 54, 12, 3] to learn intrinsic task-shared representations, and thus are sig-nificantly easier to be forgotten than hard-to-forget old classes (e.g., road) with distinctive attributes. Thus, ex-ploring task-shared representations is essential to address heterogeneous forgetting among different old classes.
• Gradient Aspect: To learn complex visual characteriza-tions for easy-to-forget old classes with various modalities and appearances, existing CIL methods [40, 4, 49] are re-quired to allocate more network architectures for gradient updating. It can result in imbalanced gradient propagation between easy-to-forget and hard-to-forget old categories, thus aggravating forgetting heterogeneity of old classes.
To overcome the above challenges, we propose a novel
Heterogeneous Forgetting Compensation (HFC) model, which is an earlier exploration to address heterogeneous forgetting from both representation and gradient perspec-tives in the CIL field [40, 4]. Specifically, we propose a task-semantic aggregation (TSA) block to alleviate heterogeneous forgetting from representation perspective. It can explore task-shared global representations for each class via aggre-gating long-range local category information within each task. Meanwhile, to tackle heterogeneous forgetting from gradient perspective, we develop two novel plug-and-play losses: a gradient-balanced forgetting compensation (GFC) loss and a gradient-balanced relation distillation (GRD) loss.
The GFC loss can rectify heterogeneous forgetting speeds of easy-to-forget and hard-to-forget old classes, while nor-malize different learning paces of new categories to achieve gradient-balanced propagation. Besides, the GRD loss can distill heterogeneous relation consistency brought by forget-ting heterogeneity among different old classes. Experiments show our model has large improvements on several repre-sentative datasets, compared with baseline methods [40, 36].
More importantly, we apply two plug-and-play losses into existing distillation-based CIL methods [40, 17, 4] to signifi-cantly improve their performance from gradient aspect. The novel contributions of this paper are presented as follows:
• We develop a novel Heterogeneous Forgetting Compen-sation (HFC) model to address different forgetting speeds of easy-to-forget and hard-to-forget old classes. To our best knowledge, this paper is the first exploration to tackle forgetting heterogeneity among old categories from rep-resentation and gradient aspects in the CIL field.
• We design a task-semantic aggregation (TSA) block to alleviate heterogeneous forgetting from representation aspect. It can explore robust task-shared representations for each class via aggregating local category information.
• We propose two novel plug-and-play losses: a gradient-balanced forgetting compensation (GFC) loss and a gradient-balanced relation distillation (GRD) loss to sur-mount forgetting heterogeneity from gradient aspect.
They can balance different forgetting of old classes and heterogeneous category-relation consistency to improve performance when applying them into existing methods. 2.