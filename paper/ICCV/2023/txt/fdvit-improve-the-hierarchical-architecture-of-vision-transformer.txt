Abstract
Despite the fact that transformer-based models have yielded great success in computer vision tasks, they suffer from the challenge of high computational costs that limits their use on resource-constrained devices. One major rea-son is that vision transformers have redundant calculations since the self-attention operation generates patches with high similarity at a later stage in the network. Hierarchical architectures have been proposed for vision transformers to alleviate this challenge. However, by shrinking the spatial dimensions to half of the originals with downsampling lay-ers, the challenge is actually overcompensated, as too much information is lost. In this paper, we propose FDViT to im-prove the hierarchical architecture of the vision transformer by using a flexible downsampling layer that is not limited to integer stride to smoothly reduce the sizes of the middle feature maps. Furthermore, a masked auto-encoder archi-tecture is used to facilitate the training of the proposed flex-ible downsampling layer and produces informative outputs.
Experimental results on benchmark datasets demonstrate that the proposed method can reduce computational costs while increasing classification performance and achieving state-of-the-art results. For example, the proposed FDViT-S model achieves a top-1 accuracy of 81.5%, which is 1.7 percent points higher than the ViT-S model and reduces 39%
FLOPs. 1.

Introduction
Convolutional neural networks (CNNs) have been the first choice on computer vision (CV) tasks [25, 54, 56, 55, 19] in the past decade. Transformers with self-attention mechanisms are another kind of neural networks that are widely used in neural language processing (NLP) tasks and have a great success (e.g., BERT [13], GPT-3 [2] and Chat-GPT [1]). In order to utilize the power of transformers for computer vision tasks, many researchers attempt to use the self-attention mechanism. For example, DETR [3] applies the transformer encoder-decoder architecture to object de-tection task, and iGPT [7] trains a sequence transformer for
Figure 1. Cosine similarities between different patches in each layer of ViT models and FDViT models. Patch similarities are reduced in FDViTs. image recognition task with a pre-training stage and fine-tuning stage.
Recently, ViT [15] mitigates the performance gap be-tween transformer models and CNN models and achieves a remarkable performance on the ImageNet dataset with-out using convolutional operation. Different from CNNs,
ViT divides the input image into 16 Ã— 16 patches and treats the patches as sequence input to the consequent transformer blocks. After that DeiT [44] proposes a data-efficient im-age transformer with a distillation training method and fur-ther improves the performance of ViT by a large margin.
Since then, research into vision transformers has exploded and several CV tasks such as image recognition [15, 44], ob-ject detection [16], image segmentation [42] and low-level vision [6] have been studied.
Despite the success of the aforementioned vision trans-formers, they mostly share the network architecture of ViT and have the same disadvantage that the models are cumber-some and have high computational costs. This is mainly be-cause there are many redundant calculations in the trans-former network and the similarity between patches becomes higher as the network goes deeper [43, 39]. Because of this,
vision transformer models are hard to apply to resource-constrained devices such as smartphones, digital cameras, smart wristbands, etc. Therefore, many works attempt to deal with this challenge by reducing the number of patches using hierarchical architectures as CNN models do [53].
With several downsampling layers added to the vision trans-former, the redundant information in different patches can be effectively reduced and a portable model is produced without too much loss of performance.
However, the aforementioned challenge is overcompen-sated by introducing the traditional downsampling lay-ers such as max-pooling and convolutional operation with stride of two into vision transformers. The former shrinks
W the original spatial dimensions in half (from HW to H 2 ) 2 and discards 75% of the data, and the latter discards 50% since the number of channels is always doubled at the same time (see the data loss ratio Eq. 3). Nevertheless, the sim-ilarity between patches is not that high at the early stage of the ViT networks (dashed lines in Fig. 1), and ignoring too much information hurts the final performance of the model.
In this paper, we propose a novel architecture called
FDViT to improve the hierarchical architecture of vision transformer by introducing a flexible downsampling layer (FD layer) that is not limited to integer stride and can pro-duce output feature map with any preset dimension. By doing this, the spatial dimension of the feature maps can be smoothly reduced to avoid too much information loss at the early stage of the network. We also introduce a masked auto-encoder architecture to facilitate the training of the proposed FD layer and generate informative outputs by treating the FD layer as the encoder, and use a decoder to recover the original input. As shown in Fig. 1, we can effec-tively reduce the similarity between patches and express the same amount of information with fewer FLOPs and param-eters. We conduct a series of experiments on the ImageNet dataset and show that the proposed method can reduce the computational cost while at the same time increase the clas-sification performance, which shows the superiority of our method. For example, the proposed model reaches 81.5% top-1 accuracy which is 1.7% higher than ViT-S model and reduces 39% FLOPs. We also verify the effectiveness of
FDViT as a backbone for object detection on MSCOCO 2017 dataset and semantic segmentation task on ADE20K dataset, and the result shows a better performance compared to existing architectures. 2.