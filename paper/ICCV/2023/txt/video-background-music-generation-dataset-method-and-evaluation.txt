Abstract
Music is essential when editing videos, but selecting mu-sic manually is difficult and time-consuming. Thus, we seek to automatically generate background music tracks given video input. This is a challenging task since it requires music-video datasets, efficient architectures for video-to-music generation, and reasonable metrics, none of which currently exist. To close this gap, we introduce a complete recipe including dataset, benchmark model, and evaluation metric for video background music generation. We present
SymMV, a video and symbolic music dataset with various musical annotations. To the best of our knowledge, it is the first video-music dataset with rich musical annotations. We also propose a benchmark video background music gener-ation framework named V-MusProd, which utilizes music priors of chords, melody, and accompaniment along with video-music relations of semantic, color, and motion fea-tures. To address the lack of objective metrics for video-music correspondence, we design a retrieval-based metric
VMCP built upon a powerful video-music representation learning model. Experiments show that with our dataset, V-MusProd outperforms the state-of-the-art method in both music quality and correspondence with videos. We be-lieve our dataset, benchmark model, and evaluation met-ric will boost the development of video background mu-sic generation. Our dataset and code are available at https://github.com/zhuole1025/SymMV . 1.

Introduction
Music plays a crucial role when creating videos, improv-ing the overall quality of videos and enhancing the immer-sion for viewers. With the rapid growth of social platforms, the needs to find suitable music for videos extend from pro-fessional fields, e.g., soundtrack production in the film in-dustry, to amateur usages like video blogs and TikTok short
*Equal contribution.
â€ Corresponding author. videos. However, finding proper music for videos and mak-ing alignments are difficult and may even bring copyright issues. Thus, automatically generating background music for videos is of great value to a wide range of creators.
Recently, tremendous progress has been made with text-to-image systems [37, 39], revealing the powerful generative capacity of state-of-the-art models. Though available to the same family of generative models, the area of video-to-music generation is still in the preliminary stage. We attribute this to the following three key challenges from the aspects of dataset, method, and evaluation, respectively: (1) Large-scale datasets of high-quality music with paired videos are absent and cumbersome to collect. (2) Video-music correspondence is complex and tricky to integrate effectively into current generative models. (3) There is a lack of objective metrics to evaluate the correspondence between video and music. We give detailed explanations and provide the corresponding solutions for each challenge in the rest of our paper. Our framework is illustrated in Fig. 1.
Existing video-music datasets [20, 32, 56] are either lim-ited in size or weak in video-music correspondence due to noisy data pairs. More importantly, these datasets only include music in audio format, which is complex, compu-tationally expensive, and difficult to impose control signals from videos. On the contrary, symbolic music, representing music in discrete sequence [24, 21], contains rich semantic information and helps to explore video-music relationships.
To fill this gap, we introduce a novel video and symbolic music dataset named Symbolic Music Videos (SymMV). It contains piano covers of popular music with their official music videos carefully collected from the Internet. Overall, it contains 1140 video-music pairs of more than 10 genres with a total length of 76.5 hours. As an advantage of symbolic music, we provide chord, melody, accompaniment, and other metadata for each music. The detailed annotations allow model to decouple the music generation process into stages and better control the generated music. Note that music in
SymMV is of high quality and can also be directly used for unconditional music generation without video modality.
We then explore the method for video background music
Figure 1: Overview of our framework. We solve the task of video background music generation from three perspectives.
Left: We present the first video and symbolic music dataset with detailed annotations. Middle: We decouple music generation into three progressive stages: chord, melody, and accompaniment (accom.), and extract various video features to guide different generation stages. Right: We propose a novel evaluation metric, named Video-Music CLIP Precision (VMCP), to measure the correspondence between generated music and input video. generation. It is challenging since the correspondence be-tween music and video is not a deterministic one-to-one map-ping but a more complex one related to aesthetic style. Mod-els are required to create music that is not only coherent and melodious but also harmonic with the given video in terms of both rhythm and style. Some initial attempts [45, 56] solve the motion-to-music generation via rhythmic control by mo-tion features in videos. They suffer from serious limitations on applied video types, i.e., requiring additional key points annotation. Prior work CMT [9] designs rule-based rhythmic relationships to generate video background music due to lack of paired data. It ignores the semantic-level correspondence, which sometimes results in conflicting styles.
To build a benchmark model, we propose a Video Music generation framework with Progressive decoupling control (V-MusProd). V-MusProd decouples music generation into three progressive transformer stages: chord, melody, and accompaniment.
It first predicts a chord sequence, then generates melody conditioned on chords and finally gen-erates accompaniment conditioned on chords and melody.
We extract semantic and color features to control the Chord
Transformer since these features reflect the theme and emo-tion of a video. Motion features are extracted as rhythmic control for Melody and Accompaniment Transformers since their outputs require rhythmic alignment with the input video.
V-MusProd can generate melodious music corresponding to the input video based on the two controlling processes.
Lastly, another bottleneck of video background music generation lies in the inadequate objective metrics. Previous methods [9, 56, 45] use metrics for unconditional music gen-eration or subjective evaluation to examine the video-music correspondence. Common metrics for unconditional music generation can only assess the quality and diversity of the generated samples but ignores the alignment between paired music and videos. Besides, conducting human listening tests for subjective evaluation is cumbersome and sometimes biased. Recent text-to-image generation models [34, 39] leverage the powerful pretrained vision-language CLIP [36] model to compute the similarity between input prompts and generated images, opening up new opportunities for eval-uating sample correspondence. Therefore, we propose a new evaluation metric, named Video-Music CLIP Precision (VMCP), which extends the vision-language CLIP model to video and music domain to measure the video-music cor-respondence. With VMCP and subjective evaluation, V-MusProd demonstrates better results than CMT on SymMV.
Our contributions are summarized as follows: (1) We present SymMV, the first video and symbolic music dataset with detailed annotations tailored for video background mu-sic generation. (2) We propose V-MusProd, a benchmark framework that utilizes music priors of chords, melody, and accompaniment along with video-music relations of seman-tic, color, and motion features. (3) We propose an objective metric VMCP based on the video-music CLIP model. Both objective metrics and subjective evaluation demonstrate that
V-MusProd surpasses the state-of-the-art model in video-music correspondence and music quality. 2.