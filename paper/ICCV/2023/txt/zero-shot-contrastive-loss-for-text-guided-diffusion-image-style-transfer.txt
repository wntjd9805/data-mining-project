Abstract 1.

Introduction
Diffusion models have shown great promise in text-guided image style transfer, but there is a trade-off between style transformation and content preservation due to their stochastic nature. Existing methods require computation-ally expensive fine-tuning of diffusion models or additional neural network. To address this, here we propose a zero-shot contrastive loss for diffusion models that doesn’t re-quire additional fine-tuning or auxiliary networks. By lever-aging patch-wise contrastive loss between generated sam-ples and original image embeddings in the pre-trained diffu-sion model, our method can generate images with the same semantic content as the source image in a zero-shot manner.
Our approach outperforms existing methods while preserv-ing content and requiring no additional training, not only for image style transfer but also for image-to-image trans-lation and manipulation. Our experimental results validate the effectiveness of our proposed method. Code is available at https://github.com/YSerin/ZeCon.
Style transfer is the task that converts the style of a given image into another style while preserving its con-tent. Over the past few years, GAN-based methods such as pix2pix [19], cycleGAN [42], and contrastive un-paired image-to-image translation (CUT) have been de-veloped [28]. Recently, joint use of a pretrained image generator and image-text encoder enabled text-guided im-age editing which requires little or no training of the net-works [31, 6, 30, 14, 25].
Inspired by the success of diffusion models for image generation [16, 35], image editing [27], in-painting [1], super-resolution [5], etc., many researchers have recently investigated the application of the diffusion models for image-to-image style transfer [33, 36]. For example, [33, 34] proposed conditional diffusion models that require paired dataset for image-to-image style transfer. One of the limitations of these approaches is that the diffusion mod-els need to be trained with paired data set with matched source and target styles. As collecting matched source and target domain data is impractical, many recent researchers have focused on unconditional diffusion models. Uncondi-tional diffusion models have limitations in maintaining con-tent due to the stochastic nature of the reverse sampling pro-cedure that doesn’t explicitly impose content consistency.
As a result, content and styles can change simultaneously, creating challenges for maintaining content.
To tackle this problem, the dual diffusion implicit bridge (DDIB) [36] exploits two score functions that have been in-dependently trained on two different domains. Although
DDIB can translate one image into another without any ex-ternal condition, it also requires training of two diffusion models for each domain which involves additional training time and a large amount of dataset. On the other hand, Dif-fusionCLIP [24] leverages the pretrained diffusion models and CLIP encoder to enable text-driven image style transfer without additional large training data set. Unfortunately,
DiffusionCLIP still requires additional fine-tuning of the model for the desired style. Furthermore, DiffuseIT [26] uses disentangled style and content representation inspired by the slicing Vision Transformer [37]. Although Diffu-seIT has shown its superiority in preserving content, it still suffers from the trade-off between transforming the texture of images and maintaining the content. Also, an additional network is required for computing content losses in Diffu-seIT.
To address this problem, here we propose a simple yet effective Zero-shot Contrastive (ZeCon) loss for diffusion models to transfer the style of a given image while pre-serving its semantic content in a zero-shot manner. Our approach is based on the observation that a pre-trained dif-fusion model already contains spatial information in its em-bedding that can be used to maintain content through patch-wise contrastive loss between the input image and generated images. Unlike DiffusionCLIP, our method doesn’t require additional training.
In other words, we could effectively preserve the content in a zero-shot manner by leveraging the patch-wise contrastive loss. Furthermore, unlike Diffu-seIT, our method achieves more accurate texture modifica-tion while preserving the content.
To demonstrate the effectiveness of our proposed method, we show a text-driven style transfer using
CLIP [31]. However, our method can be extended for gen-eral guidance beyond text inputs. Furthermore, we demon-strate that our method can be applied to text-driven image-to-image translation and image manipulation tasks, illus-trating its wide applicability. 2.