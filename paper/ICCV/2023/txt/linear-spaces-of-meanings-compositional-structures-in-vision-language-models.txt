Abstract
We investigate compositional structures in data embed-dings from pre-trained vision-language models (VLMs).
Traditionally, compositionality has been associated with al-gebraic operations on embeddings of words from a pre-existing vocabulary.
In contrast, we seek to approxi-mate representations from an encoder as combinations of a smaller set of vectors in the embedding space. These vectors can be seen as “ideal words” for generating con-cepts directly within embedding space of the model. We
ﬁrst present a framework for understanding compositional structures from a geometric perspective. We then explain what these compositional structures entail probabilistically in the case of VLM embeddings, providing intuitions for why they arise in practice. Finally, we empirically explore these structures in CLIP’s embeddings and we evaluate their use-fulness for solving different vision-language tasks such as classiﬁcation, debiasing, and retrieval. Our results show that simple linear algebraic operations on embedding vec-tors can be used as compositional and interpretable meth-ods for regulating the behavior of VLMs. 1.

Introduction
In natural language, few primitive concepts or words can be used compositionally to generate a large number of com-plex meanings. For example, Figure 1 shows a simple ex-ample of composed phrases morning, evening
, to which one could add more factors in the form
} of adjectives or attributes. The hidden representations pro-vided by a neural model, on the other hand, a priori do not have a similar compositional structure. In contextual text embeddings, in particular, the representation of a string of rainy, sunny
{
}⇥{
Figure 1: Words and concepts in natural language can be com-posed to generate complex meanings efﬁciently. Embeddings from transformer-based models a priori do not have a similar struc-In this paper, we argue that representations of composite ture. concepts admit a linear decomposition based on embedding vec-tors that can be viewed as “ideal words.” text is jointly affected by all of its tokens simultaneously, which means that there is no simple relationship between the representations of the entire text and the words that ap-pear in it.
In this paper, we investigate the existence of latent com-positional structures in the embedding space. That is, we aim to decompose composite concepts as linear combina-tions of embedding vectors associated with different factors,
If such vectors exist, they can as illustrated in Figure 1. be treated as ideal words for composing new concepts di-rectly within the representation space of the model. The ﬁrst
application that we envision is for vision-language models (e.g., CLIP [41]) where embeddings of text labels are often used for image classiﬁcation or retrieval.
In this setting, linear compositionality would imply that we could clas-sify an image with n1 . . . nk composite labels—where ni indicates the number of options for each factor—by com-paring each image with only n1 + . . . + nk ideal words, since by linearity the inner product of an image with a com-posed label is the sum of the product with the corresponding ideal words. Moreover, linear decompositions can be used for “post-hoc” manipulations of pre-trained data represen-tations (e.g., amplifying or reducing the importance of cer-tain factors), which can be helpful to control the behavior of neural models.
In general, the meaning of words in language is always contextual, in the sense that their interpretation depends on any text that surrounds them. However, language would be completely impractical if words did not also have some sta-bility in their meaning. The main beneﬁt of the usage of words is, in fact, that meaning can be mostly inferred com-positionally by combining meanings of words or phrases.
There is, therefore, a natural tension between composition-ality and contextuality: the former requires some amount of independence from context, while the latter allows for general dependencies.
In a sense, our goal in this work is to consider representations of meanings that were orig-inally learned as contextual, and to later approximate them as needed with compositional ones based on ideal words.
This combines the ﬂexibility and expressiveness of con-textuality with the structural efﬁciency of compositionality.
Our main contributions can be summarized as follows:
• We describe compositional linear structures from a geometric perspective and explain how these struc-tures can be approximately recovered from arbitrary collections of vectors associated with a product of
“factors.” We also relate these structures with pre-vious deﬁnitions of disentangled representations that were based on mathematical representation theory [26] (Section 3).
• We consider embeddings arising from visual-language models (VLMs) and show that the existence of lin-early factored embeddings is equivalent to the condi-tional independence of the factors for the probability deﬁned by the model. We also discuss some relax-ations of this result that illustrate how linear structures may emerge even when if true data distribution satis-ﬁes weaker “disentanglement” conditions (Section 4).
• We empirically show that embeddings of compos-ite concepts can often be well-approximated as linear compositional structures, and that this leads to simple but effective strategies for solving classiﬁcation and re-trieval problems in a compositional setting. We also vi-sualize manipulations of factored embeddings using a
CLIP-guided diffusion model (Stable Diffusion [42]). 2.