Abstract
We introduce LiDAR-UDA, a novel two-stage self-training-based Unsupervised Domain Adaptation (UDA) method for LiDAR segmentation. Existing self-training methods use a model trained on labeled source data to generate pseudo labels for target data and refine the pre-dictions via fine-tuning the network on the pseudo labels.
These methods suffer from domain shifts caused by dif-ferent LiDAR sensor configurations in the source and tar-get domains. We propose two techniques to reduce sen-sor discrepancy and improve pseudo label quality: 1) Li-DAR beam subsampling, which simulates different LiDAR scanning patterns by randomly dropping beams; 2) cross-frame ensembling, which exploits temporal consistency of consecutive frames to generate more reliable pseudo la-bels. Our method is simple, generalizable, and does not incur any extra inference cost. We evaluate our method on several public LiDAR datasets and show that it outper-forms the state-of-the-art methods by more than 3.9% mIoU on average for all scenarios. Code will be available at https://github.com/JHLee0513/lidar_uda. 1.

Introduction
Modern approaches to perception for robotics and au-tonomous driving rely on supervised LiDAR segmentation methods that can accurately identify objects and scenes from 3D point clouds. These methods have advanced sig-nificantly thanks to large public datasets [2, 5, 35, 29] that enable the development of efficient and powerful deep neu-ral networks [47, 8, 43], and they have inspired applications in other domains such as off-road navigation [25, 34], loco-motion navigation [13], and construction site mapping [14].
However, supervised LiDAR segmentation often struggles to adapt to new domains (i.e., domains that the model is not trained on) due to distributional shifts [32] between source and target datasets. LiDAR perception poses a unique chal-lenge in this regard because different sensor configurations
*Equal Contribution. (e.g., beam patterns, reflectivity estimates, mounting posi-introduce significant distributional shifts [39]. tion, etc.)
To mitigate such concerns, several Unsupervised Domain
Adaptation (UDA) approaches [21, 17, 44, 4] have been proposed, transferring the knowledge of a model trained on one domain to another without requiring additional labels.
UDA plays an essential role in LiDAR segmentation since it relieves the necessity of an expensive and labor-intensive labeling process.
Domain adaptation methods based on self-training, which work by iteratively generating pseudo labels on tar-get data and retraining the model with these labels, have achieved great success in reducing covariate shift in image-based semantic segmentation tasks [26, 48, 12, 1]. These self-training methods operate under the assumption that a model trained on source data yields mostly accurate predic-tions on at least a subset of the target dataset, enabling the model to adapt and refine its predictions iteratively through fine-tuning over the pseudo labels. However, in the case of
LiDAR segmentation, the beam pattern gap between differ-ent LiDAR sensors hampers the source model from predict-ing reasonably good pseudo labels in the target domain for initializing the self-training approach.
To overcome this gap between the source and target datasets, we propose a simple yet effective structured point cloud subsampling method that simulates different LiDAR beam patterns. Specifically, we randomly subsample rows in the range image [27] of a high-beam LiDAR sensor to simulate low-beam LiDAR sensors. Additionally, we pro-pose cross-frame ensembling, a temporal ensembling mod-ule, to ensure consistency of pseudo labels across LiDAR scans within each sequence. Cross-frame ensembling ag-gregates predictions from multiple scans and uses nearest neighbors to refine the pseudo labels. While we could sim-ply calculate the average with uniform weights, this method ignores the temporal (i.e., time from the reference scan) and spatial variations (i.e., distance to sensor origin for each scan) of points captured by the LiDAR sensor when aggre-gating multiple scans. We address this issue by training a
Learned Aggregation Model (LAM) that resembles graph convolution [38, 40]. LAM learns how to aggregate pseudo
labels within a sequence and weigh labels for each point differently according to its importance. Adopting this ap-proach eliminates the need for ad-hoc approaches to deal with special cases such as moving objects [44, 21].
We show that the combination of proposed modules achieves state-of-the-art performance in domain adaptation scenarios for urban and off-road driving. Moreover, our framework is applicable to off-the-shelf LiDAR segmen-tation networks since it does not require any architectural modifications or impose additional computational costs dur-ing inference.
In contrast to previous work [21, 44] that uses aggregated LiDAR scans within a sequence as a dense and sensor-agnostic representation for the segmentation net-work, our approach maintains the sparsity of the point cloud during the network forward pass. This characteristic en-ables us to use state-of-the-art network architectures that fa-vor sparse convolutions for efficient LiDAR segmentation. 2.