Abstract
We present CaPhy, a novel method for reconstructing an-imatable human avatars with realistic dynamic properties for clothing. Specifically, we aim for capturing the geomet-ric and physical properties of the clothing from real obser-vations. This allows us to apply novel poses to the human avatar with physically correct deformations and wrinkles of the clothing. To this end, we combine unsupervised training with physics-based losses and 3D-supervised training using scanned data to reconstruct a dynamic model of clothing that is physically realistic and conforms to the human scans.
We also optimize the physical parameters of the underlying physical model from the scans by introducing gradient con-straints of the physics-based losses. In contrast to previous work on 3D avatar reconstruction, our method is able to generalize to novel poses with realistic dynamic cloth de-formations. Experiments on several subjects demonstrate that our method can estimate the physical properties of the garments, resulting in superior quantitative and qualitative results compared with previous methods.
1.

Introduction
Digital human avatars are the backbone of numerous ap-plications in the entertainment industry (e.g., special effects in movies, characters in video games), in e-commerce (vir-tual try-on), as well as in immersive telecommunication applications in virtual or augmented reality. Digital hu-man avatars not only have to reassemble the real human in shape, appearance, and motion, but also have to con-form to physics. Clothing must move consistently with the underlying body and its pose.
In the past years, we have seen immense progress in digitizing humans to re-trieve such digital human avatars by using neural render-ing techniques [45, 46] or other 3D representations. Espe-cially, recent methods [38, 24, 4, 51, 20, 27, 31] that rely on deep neural networks to represent appearance and geome-try information show promising results. These data-driven methods store the body and clothing in a unified represen-tation and can be animated using the underlying body prior that exhibits a kinematic deformation tree. While the meth-ods aim at generalized animation of the captured human, the results often lack realistic deformations of the garment as they reproduce the deformation states seen during train-ing. This is due to limited training data, as not all possible poses can be captured as well as occlusions during the scan-ning procedure. In contrast to data-driven methods, recent works [2, 40] have focused on incorporating physical con-straints into dynamic clothing simulation and generating re-alistic simulation results of clothing under various complex human poses using unsupervised training. However, most of these methods rely on a fixed virtual physical model for formulating dynamic clothing, which prevents them from representing the physical properties of real-world clothing from real captures.
In this paper, we propose CaPhy, a clothing simulation and digital human avatar reconstruction method based on an optimizable cloth physical model. Our goal is to learn realistic garment simulations that can be effectively gen-eralized to untrained human poses using a limited set of 3D human scans. First, unlike most existing digital human construction works [38, 24, 4], we model the human body and clothing separately to retain their different dynamic physical properties. We train a neural garment deforma-tion model conditioned on the human pose that can produce realistic dynamic garment animation results by combining supervised 3D losses and unsupervised physical losses built upon existing real-world fabric measurements. This allows our network to generate simulated clothing results in vari-ous human poses even when insufficient scan data is avail-able. In contrast to SNUG [40], we do not assume fixed physical garment properties and optimize the fabric’s phys-ical parameters from human scans, thus, capturing its physi-cal characteristics. By combining the optimization of cloth-ing physical parameters and dynamic clothing training with both physical and 3D constraints, our method can generate highly realistic human body and clothing modeling results.
To summarize, we reconstruct dynamic clothing models from real-world observations that conform to the physical constraints of the input by combining unsupervised train-ing with physics-based losses and supervised training. The contributions of this work are as follows:
• For garment reconstruction, we introduce a physical model formulation based on real-world fabric mea-surement results, to better represent the physical prop-erties of real-captured garments (see Sec. 3.2).
• Using this physics prior and supervised 3D losses, we reconstruct an animatable avatar composed of body and garment layers including a neural garment defor-mation model which allows us to generalize to unseen poses (see Sec. 3.3).
• Instead of using the given fixed physical parameters of the fabric, we propose to optimize the parameters of the prior to obtain better physical properties of the 3D scans (see Sec. 3.4). 2.