Abstract
An authentic face restoration system is becoming in-creasingly demanding in many computer vision appli-cations, e.g., image enhancement, video communication, and taking portrait. Most of the advanced face restora-tion models can recover high-quality faces from low-quality ones but usually fail to faithfully generate realis-tic and high-frequency details that are favored by users.
To achieve authentic restoration, we propose IDM, an
Iteratively learned face restoration system based on denois-ing Diffusion Models (DDMs). We deﬁne the criterion of an authentic face restoration system, and argue that denoising diffusion models are naturally endowed with this property from two aspects: intrinsic iterative reﬁnement and extrin-sic iterative enhancement. Intrinsic learning can preserve the content well and gradually reﬁne the high-quality de-tails, while extrinsic enhancement helps clean the data and improve the restoration task one step further. We demon-strate superior performance on blind face restoration tasks.
Beyond restoration, we ﬁnd the authentically cleaned data by the proposed restoration system is also helpful to im-age generation tasks in terms of training stabilization and sample quality. Without modifying the models, we achieve better quality than state-of-the-art on FFHQ and ImageNet generation using either GANs or diffusion models. 1.

Introduction
Face images convey critical information and are used in every corner of our daily lives, e.g., social networks, confer-ence calls, and taking portrait, to name a few. Humans are very sensitive to delicate facial features. High-quality faces are the most preferred and typically are the key to the suc-cess of these applications. However, unknown and compli-cated image degradations are inevitable in real-world sce-narios due to e.g. camera defocus, encoding artifacts, mo-tion blur, and noises.
In recent years, rapid progress has been made in developing a blind face restoration capability as a post-processing step to restore low-quality photos into high-quality ones faithfully.
Most existing popular works exploit pretrained facial prior networks [43, 54, 3, 47, 4, 3] in assisting recover-ing photorealistic facial features even if the input is much deﬁcient. Their success derived from the advanced power of generative models in encapsulating high quality priors, e.g., StyleGANs [9, 19] and VQVAE [42, 31]. Other works use auxiliary information as guidance, such as face land-marks [6], component features [23], and high-quality exem-plars [24, 25]. Advanced architectures, including CNNs and
Transformers [45, 26] have been explored to alleviate face degradation issues. These works are based on building U-net models that restore low-quality faces into high-quality counterparts. The performance difference is often primarily attributed to the model design.
Although these works have achieved notable progress by generating perceptually realistic faces and reporting good numeric results on the benchmarks like PSNR (Peak Signal-to-Noise Ratio) and FID (Fr´echet Inception Distance) [13], they still struggle over several important factors, e.g., fail to preserve delicate identity features, hallucinate uncanny ar-tifacts especially in the presence of severe degradation, lose desired high-frequency details such as around facial hair. A quick test is to let the model perform restoration on slightly degraded faces that can make us distinguish delicate differ-ences before and after restoration. As shown in Figure 1, we evaluate two state-of-the-art works CodeFormer [54] and GFPGAN [43] on four examples from CelebA-HQ data [17, 27]. Both models either lose identity details, e.g., freckles and eyelashes or unexpectedly damage the origi-nal high-quality components like mustache and hair. The former issue sometimes raises ethical concerns, and the lat-ter holds users from applying the restoration model broadly to fulﬁll their demands as the regression is unsatisfactory.
We conjecture the aforementioned limitations not only from the design choices of their model, but also the quality of training data, as some ground-truth images are still noisy as seen in Figure 1. In some sense, it contradicts the goal of recovering low-quality images into high-quality images if the ground-truth data is not clean, implying that training a restoration model is an ill-posed problem. Moreover, calcu-lating pairwise metrics like PSNR between the predictions and the ground truth could also be problematic in evalua-tion. Intuitively, noisy training data can harm the restoration model’s performance compared with clean data.
Source
Codeformer [54]
GFPGAN[43]
IDM (Ours)
Figure 1: Qualitative comparison on original CelebA-HQ test examples (without degradations). Baselines are biased by losing face details and generating hazy facial features. Our method authentically restores faces and preserves details much better regardless of the input quality. Zoom in to see richer details.
Questions To address above challenges, there are two main questions to answer, which are also the primary factors that should be considered when building a machine learn-ing system. (i) Model: When processing above examples, why current model designs are unsatisfactory in balancing degradation removal and detail reﬁnement? (ii) Data: Can we automatically clean the data without expensive labor an-notation and pave the path for authentic restoration and ac-curate evaluation?
Solution To this end, we propose to use conditional de-noising diffusion models (DDMs) [14] with well-designed iterative learning as a uniﬁed solution to both questions. Be-cause of the inspiring and unprecedented results from image
Source
Restored
Figure 2: Restoring original ImageNet 256 256 image with the proposed iterative DDMs. (Zoom in to see differ-ences before and after.)
⇥ synthesis e.g., text-to-image synthesis [32, 35] and image editing [33], DDMs have attracted more and more atten-tion from research community and shown great potentials to land in industry applications. There also have been works exploring DDMs for image restoration [20, 49, 34], how-ever, none of them yields a solution to address the above concerns. In this paper, the proposed iterative DDMs have the following properties: (i) Intrinsic iterative reﬁnement: DDMs compromise of it-erative Markov chains in both forward diffusion and reverse denoising. The very dense U-Net architecture design and it-erative reﬁnement procedure make it a better choice to natu-rally preserve high-quality contents and remove degradation patterns. (ii) Extrinsic iterative enhancement: As shown in Figure 1,
DDMs are found to balance degradation removal and de-tail preservation very well. Hence, the DDMs trained for restoration are ready for enhancing the training data which is then used for the next iteration of restoration training and more accurate evaluation.
Beyond restoration Besides, we would rather think about using a model elsewhere than just for a speciﬁc goal, here, restoration. If above challenges are well resolved, we are able to get a new copy of higher-quality data in terms of resolution and cleanness. This consequently beneﬁts gen-erative models for training stabilization by reducing the number of outliers in terms of data quality and generating higher-ﬁdelity samples. For example, as is known to all, the resolution of ImageNet can be as small as 75 56 and encoding compression is also noticeable in many images.
Figure 2 shows qualitative restored examples using a model trained on ImageNet restoration. Consistently, the sample quality has been increased a lot. More inspiring results from generative models will be detailed in later sections.
⇥
Contribution The main contributions of this paper are as follows. First, we point out the issues of existing face restoration systems and formulate the deﬁnition of authentic restoration that can quickly examine the effectiveness of any face restoration model. Second, we build an iterative diffu-sion model (IDM) to tackle the raised concerns. Finally, we demonstrate strong empirical results on two benchmarks, blind face restoration and image generation, showing that the proposed model consistently outperforms state-of-the-art methods and can be used to beneﬁt the learning of gen-erative models beyond restoration. 2.