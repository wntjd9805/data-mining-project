Abstract
We propose a novel unsupervised object localization method that allows us to explain the predictions of the model by utilizing self-supervised pre-trained models with-out additional finetuning. Existing unsupervised and self-supervised object localization methods often utilize class-agnostic activation maps or self-similarity maps of a pre-trained model. Although these maps can offer valuable in-formation for localization, their limited ability to explain how the model makes predictions remains challenging. In this paper, we propose a simple yet effective unsupervised object localization method based on representer point selec-tion, where the predictions of the model can be represented as a linear combination of representer values of training points. By selecting representer points, which are the most important examples for the model predictions, our model can provide insights into how the model predicts the fore-ground object by providing relevant examples as well as their importance. Our method outperforms the state-of-the-art unsupervised and self-supervised object localization methods on various datasets with significant margins and even outperforms recent weakly supervised and few-shot methods. Our code is available at: https://github. com/yeonghwansong/UOLwRPS 1.

Introduction
Object localization is one of the most fundamental prob-lems in computer vision which aims to find a bounding box of a particular object category in a given image. Re-cent object localization methods achieve impressive per-formances thanks to advances in deep neural networks (DNNs) [19, 13] and large-scale datasets [33, 37]. Despite their successes, the biggest concern in this task is that col-lecting datasets with precise box-level annotations is labor-intensive and time-consuming. Recently, object localiza-tion methods with less supervision, i.e. weakly-supervised methods [25, 52, 48, 12, 53, 46, 31, 44, 3], have been well-studied to mitigate those problems, nevertheless, they re-quire image-level class labels.
Figure 1. GT-Known Loc scores of various methods with respect to different levels of supervision on ImageNet-1K dataset. All meth-ods use ViT-S [13] as a backbone and TC denotes TokenCut [39].
Beyond learning with less supervision, most recent works [42, 41, 51, 1, 21, 27, 36, 45, 35, 39] focus on the object localization task using self-supervised or unsuper-vised learning that does not require any human annotated la-bels. These works address the problem of identifying which regions are more likely to contain the foreground object, which is a salient object in an image. In order to discover foreground object regions, several methods [1, 21, 27, 36] attempt to use the magnitude of feature vectors as a clue for class-agnostic activation maps (CAAM) [1]. Most of these methods rely heavily on pre-trained models designed for the image classification task. However transferring these mod-els to the object localization task is challenging, since fore-ground features are not easily distinguished from the back-ground. To address this problem, these works have been focusing on learning to discriminate foreground and back-ground representations from the pre-trained model.
Recent approaches for unsupervised object localization face a limitation in terms of the explainability of the pre-dictions. DNNs are powerful predictors across various do-mains, but their complicated structure often makes them black-box functions. Class activation maps (CAM) [56] are frequently used to provide visually interpretable informa-tion about a specific class the model has learned, but a de-tailed explanation of how the model makes its predictions
remains unclear. This limitation becomes even more severe for CAAM, which is commonly used in self-supervised ob-ject localization methods [1, 21, 27, 36].
In this paper, we propose a simple yet effective unsu-pervised object localization method based on the represen-ter theorem using self-supervised representation learning models. The proposed method formulates the solution to the object localization task based on representer point se-lection [47]. This strategy is derived from the representer theorem, where the predictions of a neural network for a given test point find expression through a linear combina-tion of the activations originating from training examples.
The importance of each feature, which measures the sig-nificance of training examples in contributing to the final prediction, is computed by its norm. By selecting repre-senter points that have the most influence on the model’s predictions, our model gains valuable insights into how the model identifies foreground regions. In addition, we intro-duce a simple extension to weakly supervised object local-ization, demonstrating zero-shot transferability across dif-ferent datasets without the need for additional training.
Our experiments are conducted on various datasets for the object localization task to prove the effectiveness of our method and show outstanding performances compared to the state-of-the-art methods with substantial margins. Fur-thermore, as shown in Figure 1, our method even outper-forms some recent weakly supervised and few-shot ob-ject localization methods without any fine-tuning or post-processing using additional refinement networks.
We summarize our contributions as follows:
• We propose a novel unsupervised object localization method that leverages the representer theorem on self-supervised representation learning models. The pro-posed method helps to enhance the interpretability of predictions by providing relevant examples as well as their importance.
• We show that the representer point selection, which has not been extensively explored in previous research on deep neural networks, can be effectively applied to object localization.
• Our method is simple to implement and works well with a variety of self-supervised visual representation learning models. It is also an effective solution due to its good transferability across datasets and easy scala-bility to weakly supervised object localization.
• Our method shows outstanding results on various localization task and also datasets for the object achieves comparable performances to the state-of-the-art weakly-supervised and few-shot methods. 2.