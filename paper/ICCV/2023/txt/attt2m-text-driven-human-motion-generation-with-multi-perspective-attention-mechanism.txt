Abstract
Generating 3D human motion based on textual descrip-tions has been a research focus in recent years.
It re-quires the generated motion to be diverse, natural, and conform to the textual description. Due to the complex spatio-temporal nature of human motion and the difficulty in learning the cross-modal relationship between text and motion, text-driven motion generation is still a challeng-ing problem. To address these issues, we propose AttT2M, a two-stage method with multi-perspective attention mech-anism: body-part attention and global-local motion-text attention. The former focuses on the motion embedding perspective, which means introducing a body-part spatio-temporal encoder into VQ-VAE to learn a more expres-sive discrete latent space. The latter is from the cross-modal perspective, which is used to learn the sentence-level and word-level motion-text cross-modal relationship.
The text-driven motion is finally generated with a gen-erative transformer. Extensive experiments conducted on
HumanML3D and KIT-ML demonstrate that our method outperforms the current state-of-the-art works in terms of qualitative and quantitative evaluation, and achieve fine-grained synthesis and action2motion. Our code is in https://github.com/ZcyMonkey/AttT2M. 1.

Introduction
With the expansion of application scenarios and de-mands, cross-modal human motion synthesis has become a hot research topic in computer vision and graphics. Among them, text-driven human motion synthesis aims to synthe-size natural human motion that matches a given text descrip-tion, which can be applied to intelligent animation produc-tion, virtual reality, game and film industry, human-robotics interaction, etc.
*Equal contribution
†Corresponding author
Figure 1. Text-driven generation visualization. Our method achieves high-quality motion that is precisely consistent with se-mantically rich text descriptions.
However, introducing natural language descriptions as constraints for motion synthesis, which means using cross-modal high-level semantics to control the motion synthesis process, is a difficult interdisciplinary problem involving natural language processing, motion modeling, and cross-modal relationship learning. With the availability of pre-trained language models such as CLIP [35], BERT [10], and GPT [36] for natural language processing, there are still two challenges that remained: (1) human motion is high-dimensional data with irregular spatial structure and highly dynamic temporal characteristics. It is challenging
to perform text-driven synthesis in pose space directly; (2) there are complex local (specific words correspond to spe-cific motion sub-sequences) and global (the overall text de-scription corresponds to the order and connection of these sub-sequences) correspondence between text and motion.
How to better learn this cross-modal relationship is still a problem to be solved.
Compared to some works that directly generated text-driven motion in the pose space[4, 3, 8, 53, 41], others attempted to learn the low-dimensional representation of motion first using auto-encoder [13], VAE [49] and VQ-VAE [52] with temporal encoding. However, since hu-man motion has both spatial and temporal characteristics, it is necessary to consider both when dealing with latent representation learning [56, 50]. Therefore, we propose
Vector Quantised-Variational AutoEncoder(VQ-VAE) with
Body-Part attention-based Spatio-Temporal(BPST) en-coder to learn an expressive latent space. As for the motion-text cross-modal relationship, some works used cross-modal translation to learn the shared representation of text and motion and directly generated motion from text feature [4, 3, 33, 6]. However, the vast gaps between mo-tion and text data make learning an adequate shared rep-resentation difficult. Other researchers attempted to treat text information as a condition during the motion genera-tion [13, 52, 49, 53, 41, 14]. We observe two levels of corre-spondence in the cross-modal relationship between motion and text: 1). the local correspondence between words of text and sub-segments of motion sequences; 2). the global correspondence between the overall semantics of text and the whole motion sequence. Previous research typically fo-cused on either local [53] or global [52, 41] text information or combined them by simply concatenating [13]. We pro-pose Global and Local Attention(GLA) to consider both levels more reasonably better to learn the cross-modal rela-tionship between text and motion.
Specifically, we propose AttT2M, a two-stage text-driven motion generation model with multi-perspective at-tention(Figure 2(a)). The first attention focuses on motion embedding perspective. We use a spatial transformer [44] based on body-part attention and TCN to extract the spatio-temporal features, and then a VQ-VAE [43] is used
In the to quantize the features into a discrete codebook. text-diven generation stage, we propose global and local attention from the perspective of cross-modal relationship learning. After the global (sentence-level) and local (word-level) text features are extracted using CLIP, we calculate the motion-word cross-attention to learn the local cross-modal relationship and learn the global cross-modal rela-tionship by motion-sentence conditional self-attention. Fi-nally, a generative transformer is used to generate motion sequences.
We conduct extensive qualitative and quantitative exper-iments on two widely used datasets, KIT-ML [34] and Hu-manML3D [13]. The experimental results show that our work achieves better text-driven motion generation results than previous work in qualitative and quantitative compar-isons. The generated motions highly match the given nat-ural language descriptions and maintain reliable diversity while being highly realistic and natural(seeing Figure 1).
Our work can also achieve fine-grained generation and ac-tion to motion.
Our contributions can be summarized as follows:
• 1. We propose a Body-Part attention-based Spatio-Temporal VQ-VAE to map motion sequences into a better discrete code book, resulting in a more expres-sive low-dimensional motion representation.
• 2. We introduce Global and Local Attention to learn the global-local cross-modal relationship between text and motion, achieving precise correspondences be-tween motion and text.
• 3. We conduct extensive qualitative and quantitative experiments on KIT-ML and HumanML3D to demon-strate that our method outperforms the previous state-of-the-art methods, which can also handle fine-grained and action-to-motion generation. 2.