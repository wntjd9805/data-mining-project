Abstract
Recently, neural network (NN)-based image compres-sion studies have actively been made and has shown im-pressive performance in comparison to traditional methods.
However, most of the works have focused on non-scalable image compression (single-layer coding) while spatially scalable image compression has drawn less attention al-though it has many applications. In this paper, we propose a novel NN-based spatially scalable image compression method, called COMPASS, which supports arbitrary-scale spatial scalability. Our proposed COMPASS has a very flex-ible structure where the number of layers and their respec-tive scale factors can be arbitrarily determined during in-*Corresponding author. ference. To reduce the spatial redundancy between adjacent layers for arbitrary scale factors, our COMPASS adopts an inter-layer arbitrary scale prediction method, called LIFF, based on implicit neural representation. We propose a com-bined RD loss function to effectively train multiple lay-ers. Experimental results show that our COMPASS achieves
BD-rate gain of -58.33% and -47.17% at maximum com-pared to SHVC and the state-of-the-art NN-based spatially scalable image compression method, respectively, for var-ious combinations of scale factors. Our COMPASS also shows comparable or even better coding efficiency than the single-layer coding for various scale factors.
1.

Introduction
Recently, image compression has become increasingly important with the growth of multimedia applications. The exceptional performance of neural network (NN)-based methods in computer vision has led to active research on
NN-based image compression methods [38, 3, 36, 4, 27, 18, 8, 19, 21, 28, 14, 2, 26], resulting in remarkable improve-ments in coding efficiency. However, although the same content is often consumed in various versions in multime-dia systems, most existing NN-based image compression methods must separately compress an image into multiple bitstreams for their respective versions, thus leading to low coding efficiency. To resolve this issue, there have been a few recent studies [37, 34, 16, 43, 13, 24, 23, 25] on NN-based scalable image compression, where various versions of an image are encoded into a single bitstream in a hierar-chical manner with multiple layers. Each layer is in charge of en/decoding one corresponding version of the image, and typically, redundancy between adjacent layers is reduced by a prediction method for higher coding efficiency.
The scalable coding methods are divided into two classes: quality scalable codecs for the images of different quality levels and spatially scalable codecs for the images of different sizes. In this paper, we focus on the spatially scalable coding that has not been actively studied compared with the quality scalable coding. Upon our best knowledge, only one previous study [25] deals with the spatially scal-able coding in the recent deep NN-based approach.
In conventional tool-based scalable coding, SVC [31] and SHVC [6] have been standardized by MPEG [17] for video coding standards, as extensions to H.264/AVC [40] and H.265/HEVC [35], respectively. Despite of signifi-cant coding efficiency improvement compared with sepa-rate single-layer compression of different versions (simul-cast coding), the scalable coding has not yet been widely adopted for real-world applications [6, 32]. One reason may be lower coding efficiency of the accumulated bitstream for the larger version compared with the single-layer coding of the same size. The scalable coding often yields lower cod-ing efficiency due to its insufficient redundancy removal ca-pability between the layers.
In addition, for the existing NN-based method [25], only one fixed scale factor 2 is used between adjacent layers as shown in Figure 1. This limitation makes it not practical for real-world applications that require a variety of scale combinations. For example, an image of 4,000×2,000 size needs to be encoded into SD (720×480), HD (1,280×720) and FHD (1,920×1,080) versions which are not in powers of 2 scales compared to the input size. Therefore, in order to support for the one-source-multiple-use (OSMU) with spa-tially scalable image compression, it is worthwhile for spa-tially scalable image compression to support arbitrary scale factors between the different layers.
To address the aforementioned issues, we propose a novel NN-based image COMPression network with
Arbitrary-scale Spatial Scalability, called COMPASS. Our
COMPASS supports spatially scalable image compression that encodes multiple arbitrarily scaled versions of an image into a single bitstream in which each version of the image is encoded with its corresponding layer. Inspired by LIIF [7] and Meta-SR [15], we adopt an inter-layer arbitrary scale prediction method in the COMPASS, called Local Implicit
Filter Function (LIFF), based on implicit neural represen-tation that can effectively reduce the redundancy between adjacent layers and also supports arbitrary scale factors. In addition, it should be noted that our COMPASS exploits only one shared prediction/compression module for all the enhancement layers, thus it effectively provides the exten-sibility in terms of the number of layers and also reduces the number of model parameters. For effective and stable optimization of the hierarchically recursive architecture of
COMPASS, we introduce a combined RD loss function.
Based on its superior inter-layer prediction capability, our COMPASS significantly improves the coding efficiency compared to the existing scalable coding methods [6, 25], and achieves comparable or even better coding efficiency compared to the single-layer coding for various scale fac-tors. Note that the coding efficiency of the single-layer cod-ing has been regarded as the upper bound of the scalable coding efficiency. Furthermore, to the best of our knowl-edge, our COMPASS is the first NN-based spatially scalable image compression method that supports arbitrary scale fac-tors with high coding efficiency. Our contributions are sum-marized as:
• The COMPASS is the first NN-based spatially scalable image compression method for arbitrary scale factors.
• The COMPASS adopts an inter-layer arbitrary scale prediction, called LIFF, which is based on implicit neural representation to reduce redundancy effectively as well as to support the arbitrary scale factors. Addi-tionally, we propose a combined RD loss function to effectively train multiple layers.
• Our COMPASS significantly outperforms the existing spatially scalable coding methods [6, 25]. Further-more, to the best of our knowledge, the COMPASS is the first work that shows comparable or even bet-ter performance in terms of coding efficiency than the single-layer coding for various scale factors, based on a same image compression backbone. 2.