Abstract
Semantic segmentation of LiDAR point clouds has been widely studied in recent years, with most existing methods focusing on tackling this task using a single scan of the en-vironment. However, leveraging the temporal stream of ob-servations can provide very rich contextual information on regions of the scene with poor visibility (e.g., occlusions) or sparse observations (e.g., at long range), and can help
In this reduce redundant computation frame after frame. paper, we tackle the challenge of exploiting the informa-tion from the past frames to improve the predictions of the current frame in an online fashion. To address this chal-lenge, we propose a novel framework for semantic segmen-tation of a temporal sequence of LiDAR point clouds that utilizes a memory network to store, update and retrieve past information. Our framework also includes a novel regu-larizer that penalizes prediction variations in the neigh-borhood of the point cloud. Prior works have attempted to incorporate memory in range view representations for semantic segmentation, but these methods fail to handle occlusions and the range view representation of the scene changes drastically as agents nearby move. Our proposed framework overcomes these limitations by building a sparse 3D latent representation of the surroundings. We eval-uate our method on SemanticKITTI, nuScenes, and Pan-daSet. Our experiments demonstrate the effectiveness of the proposed framework compared to the state-of-the-art.
For more information, visit the project website: https:
//waabi.ai/research/memoryseg. 1.

Introduction
Semantic segmentation of LiDAR point clouds is a key component for the safe deployment of self-driving vehi-cles (SDV). It enables SDVs to enhance their understand-ing of the surrounding environment by categorizing every 3D point into specific classes of interest, such as vehicles, pedestrians, traffic signs, buildings, roadways, etc. This rich and precise 3D representation of the environment can then be used for various applications such as generating online or
Figure 1. Objects could be partially occluded in single frame
LiDAR point cloud. Our approach learns a 3D latent memory rep-resentation for better contextualizing the online observations. We apply PCA [14] to reduce the latent dimension to 3 and plot as
RGB. (Best viewed in color and zoomed-in.) offline semantic maps, building localization priors, or mak-ing the shapes of an object tracker more precise.
LiDAR data is typically captured as a continuous stream of data, where every fraction of a second (typically 100ms) a new point cloud is available. Despite this fact, most Li-DAR segmentation approaches process each frame indepen-dently [15, 32, 28, 22, 34, 5, 29] due to the computational and memory complexity associated with processing large amounts of 3D point cloud data. However, reasoning about a single frame suffers from the sparsity of the observations, particularly at range, and has difficulty handling occluded objects. Furthermore, the absence of motion information can make categorizing certain objects difficult.
Several approaches [20, 19] utilize a sliding window ap-proach, where a small set of past frames is processed inde-pendently at every time step. The main shortcoming of this approach is its limited temporal context (typically under 1 second) due to resource constraints, as processing multiple
LiDAR scans at once is expensive. TemporalLidarSeg [8] proposed to utilize a latent spatial memory to retain infor-mation about the past while avoiding redundant computa-tion every time a new scan is available, in a range view (RV) representation. However, the RV of the scene changes dras-tically as the SDV or the other actors move due to changes in perspective. As a result, the memory can be dominated by nearby objects, which appear larger in the RV represen-tation, making it challenging to be updated from frame to frame. Occlusion can be particularly challenging as the oc-cluder now occupies the same spatial region that was pre-viously describing the occluded object. This limits the use-fulness of the memory. In contrast to RV, 3D is a metric space where the distances between points are preserved re-gardless of the viewpoint or relative distance to the SDV.
Thus, representing the memory in 3D enables learning size priors for different classes.
It allocates equal representa-tion power to them regardless of the distance to the SDV, and enables easy understanding of motion. Moreover, pre-viously observed regions that are currently occluded can be remembered in the memory as the occluder and occluded objects occupy different 3D regions, even though they share the same space in RV. Despite these advantages, 3D mem-ory has been overlooked in LiDAR semantic segmentation.
In this paper we propose MEMORYSEG, a novel on-line LiDAR segmentation model that recurrently updates a sparse 3D latent memory as new observations are received, efficiently and effectively accumulating evidence from past observations. Fig. 1 illustrates the expressive power of such memory. In a single scan, objects are hard to identify due to sparsity (particularly at range) and lack of semantics, and occluded areas have no observations. In contrast, our latent memory is much denser, providing a rich context to separate different classes, especially in currently occluded regions.
To achieve an effective memory update that takes into account both the past and the present for accurate decoding, our method addresses several challenges. First, we align the previous memory with the current observation in the latest ego frame, to compensate for the SDV motion. Second, the sparsity level of the memory and the observation embed-dings are different, making fusion non-trivial. The latent memory is denser, and thus some currently unobserved lo-cations may be present in the memory. Complementarily, some regions in the current scan have not been observed before, such as new observations appearing as the SDV drives further. For this reason, we propose a mechanism to fill in missing regions in the current observations and add new observations to the memory during the update. Third, other objects are also moving (potentially in opposite direc-tion), thus fusing the memory and observation embeddings requires a large receptive field. We address this via a care-fully designed architecture that achieves a large receptive field yet preserves sparsity for efficiency. Aditionally, we introduce a novel point-level neighbourhood variation reg-ularizer which penalizes significant differences in semantic predictions within local 3D neighborhoods.
Extensive experiments in SemanticKITTI [2], nuScenes
[4] and PandaSet [27] demonstrate that MEMORYSEG outperforms current state-of-the-art semantic segmentation methods that rely purely on LiDAR on multiple bench-marks. 2.