Abstract
Part-prototype networks (e.g., ProtoPNet, ProtoTree, and
ProtoPool) have attracted broad research interest for their intrinsic interpretability and comparable accuracy to non-interpretable counterparts. However, recent works find that the interpretability from prototypes is fragile, due to the se-mantic gap between the similarities in the feature space and that in the input space. In this work, we strive to address this challenge by making the first attempt to quantitatively and objectively evaluate the interpretability of the part-prototype networks. Specifically, we propose two evaluation metrics, termed as “consistency score” and “stability score”, to evaluate the explanation consistency across images and the explanation robustness against perturbations, respectively, both of which are essential for explanations taken into prac-tice. Furthermore, we propose an elaborated part-prototype network with a shallow-deep feature alignment (SDFA) mod-ule and a score aggregation (SA) module to improve the interpretability of prototypes. We conduct systematical eval-uation experiments and provide substantial discussions to uncover the interpretability of existing part-prototype net-works. Experiments on three benchmarks across nine ar-chitectures demonstrate that our model achieves signifi-cantly superior performance to the state of the art, in both the accuracy and interpretability. Our code is available at https://github.com/hqhQAQ/EvalProtoPNet. 1.

Introduction
Part-prototype networks are recently emerged deep self-explainable models for image classification, which achieve 1† Corresponding author.
Figure 1. (a) Inconsistency. A prototype may mistakenly corre-spond to different object parts in different images. (b) Instability.
A prototype may mistakenly correspond to different object parts in the original image and the slightly perturbed image. The samples are from ProtoPNet trained on ResNet34 backbone [12]. excellent performance in an interpretable decision-making manner. In particular, ProtoPNet [7] is the first part-prototype network, with the follow-up part-prototype networks (e.g.,
ProtoTree [31], ProtoPool [33], TesNet [44], and ProtoP-Share [34]) built upon its framework. At its core, part-prototype networks define multiple trainable prototypes that represent specific object parts and emulate human perception by comparing object parts across images to make predictions.
Currently, part-prototype networks have been extended to various domains (e.g., graph neural network [54], deep rein-forcement learning [19], and image segmentation [35]).
However, the current part-prototype networks only demonstrate their interpretability with only a few visualiza-tion examples, and ignore the problem that the learned pro-totypes of part-prototype networks do not have adequately credible interpretability [13, 20, 36]. The reasons for such un-reliable interpretability are twofold: (1) Inconsistency. The basic design principle of part-prototype networks [7] is that each prototype is associated with a specific object part, but it is not guaranteed that the corresponding object part of a
prototype is consistent across images, as shown in Fig. 1 (a); (2) Instability. Previous interpretability methods [1, 50, 55] claim that the explanation results should be stable, but the prototype in part-prototype networks is easily mapped to a vastly different object part in a perturbed image [13], as shown in Fig. 1 (b). Recently, Kim et al. [20] have proposed a human-centered method named HIVE to evaluate the inter-pretability of part-prototype networks. Nevertheless, HIVE requires redundant human interactions and the evaluation re-sults are subjective. Therefore, for the further research on the part-prototype networks, there is an urgent need for more for-mal and rigorous evaluation metrics that can quantitatively and objectively evaluate their interpretability.
In this work, we strive to take one further step towards the interpretability of part-prototype networks, by making the first attempt to quantitatively and objectively evaluate the interpretability of part-prototype networks, rather than the qualitative evaluations by several visualization examples or subjective evaluations from humans. To this end, we pro-pose two evaluation metrics named “consistency score” and
“stability score”, corresponding to the above inconsistency and instability issues. Specifically, the consistency score evaluates whether and to what extent a learned prototype is mapped to the same object part across different images.
Meanwhile, the stability score measures the robustness of the learned prototypes being mapped to the same object part if the input images are slightly perturbed. In addition, our evalu-ation metrics generate objective and reproducible evaluation results using object part annotations in the dataset. With the proposed metrics, we make the first systematic quantitative evaluations of existing part-prototype networks in Sec. 4.2.
Experiments demonstrate that current part-prototype net-works are, in fact, not sufficiently interpretable.
To strengthen the interpretability of prototypes, we pro-pose an elaborated part-prototype network built upon a re-vised ProtoPNet with two proposed modules: a shallow-deep feature alignment (SDFA) module and a score ag-gregation (SA) module. These two modules aim to accu-rately match the prototypes with their corresponding object parts across images, benefiting both consistency and stability scores. Part-prototype networks match prototypes with ob-ject parts in two steps: (1) feature extraction of object parts; (2) matching between prototypes and features of object parts.
SDFA module improves the first step by promoting deep feature maps to spatially align with the input images. Specif-ically, SDFA module aligns the spatial similarity structure of shallow and deep feature maps with the observation that shal-low feature maps retain spatial information that deep feature maps lack (Fig. 2 (a)). Meanwhile, SA module improves the second step based on the observation that the matching of each prototype with its corresponding object part is disturbed by other categories (Fig. 2 (b)). To mitigate this problem, SA module aggregates the activation values of prototypes only
Figure 2. (a) Shallow feature maps of an image retain spatial in-formation that deep feature maps lack (the features are from one channel of the feature map). (b) A prototype from Chipping Spar-row tends to match the wing, but meanwhile it has to paradoxically ignore almost the same wing of Field Sparrow. The samples are from ProtoPNet trained on ResNet34 backbone. into their allocated categories to concentrate the matching between prototypes and their corresponding object parts.
We perform extensive experiments to validate the perfor-mance of our proposed model. Experiment results demon-strate that without using any object part annotations in training, our model achieves the state-of-the-art perfor-mance in both interpretability and accuracy on CUB-200-211 dataset [43], Stanford Cars dataset [23] and PartImageNet dataset [11], over six CNN backbones and three ViT back-bones. Furthermore, experiment results show that the pro-posed consistency and stability scores are strongly positively correlated with accuracy in part-prototype networks, nicely reconciling the conflict between interpretability and accuracy in most prior interpretability methods.
To sum up, the key contributions of this work can be summarized as follows:
• We establish a benchmark to quantitatively evaluate the interpretability of prototypes of part-prototype net-works with the proposed evaluation metrics (consis-tency score and stability score), uncovering pros and cons of various part-prototype networks.
• We propose an elaborated part-prototype network built upon ProtoPNet with a shallow-deep feature align-ment (SDFA) module and a score aggregation (SA) module to enhance its interpretability.
• Experiment results verify that our proposed model sig-nificantly outperforms existing part-prototype networks by a large margin, in both accuracy and interpretability.
Besides, the consistency and stability scores are posi-tively correlated with accuracy, nicely reconciling the conflict between interpretability and accuracy.
Figure 3. Overview of our proposed model (only two categories are presented for brevity). Backbone is a deep convolutional network to extract the features of input image x. SDFA module incorporates spatial information from shallow layers into deep layers by aligning the spatial similarity structure in deep layers with that in shallow layers using an alignment loss Lalign. The last feature map zd is fed into prototype layer for different categories. In the prototype layer, each prototype pi generates an activation map on zd and selects the maximum value as the activation value gpi (x). Finally, SA module aggregates the activation values of prototypes into their allocated categories for classification. Note that D
·Ds, Lclst, Lsep and the loss back propagation of Lce are omitted for brevity in the figure.
′ s = Hs
Hd
· Ws
Wd 2.