Abstract
The recent amalgamation of transformer and convolu-tional designs has led to steady improvements in accuracy
In this work, we introduce and efficiency of the models.
FastViT, a hybrid vision transformer architecture that ob-tains the state-of-the-art latency-accuracy trade-off. To this end, we introduce a novel token mixing operator, RepMixer, a building block of FastViT, that uses structural reparam-eterization to lower the memory access cost by removing skip-connections in the network. We further apply train-time overparametrization and large kernel convolutions to boost accuracy and empirically show that these choices have minimal effect on latency. We show that – our model faster than CMT, a recent state-of-the-art hybrid is 3.5 transformer architecture, 4.9 faster than EfficientNet, and faster than ConvNeXt on a mobile device for the same 1.9 accuracy on the ImageNet dataset. At similar latency, our model obtains 4.2% better Top-1 accuracy on ImageNet than MobileOne. Our model consistently outperforms com-peting architectures across several tasks – image classifica-tion, detection, segmentation and 3D mesh regression with significant improvement in latency on both a mobile de-vice and a desktop GPU. Furthermore, our model is highly robust to out-of-distribution samples and corruptions, im-proving over competing robust models. Code and mod-els are available at https://github.com/apple/ ml-fastvit
×
×
× 1.

Introduction
Vision Transformers [14] have achieved state-of-the-art performance on several tasks such as image classification, detection and segmentation [35]. However, these models have traditionally been computationally expensive. Recent works [66, 39, 41, 57, 29] have proposed methods to lower the compute and memory requirements of vision transform-ers. Recent hybrid architectures [41, 17, 10, 62] effectively corresponding authors: {panasosaluvasu, anuragr}@apple.com combine the strengths of convolutional architectures and transformers to build architectures that are highly competi-tive on a wide range of computer vision tasks. Our goal is to build a model that achieves state-of-the-art latency-accuracy trade-off.
Recent vision and hybrid transformer models [51, 17, 42, 41] follow the Metaformer [65] architecture, which con-sists of a token mixer with a skip connection followed by
Feed Forward Network (FFN) with another skip connection.
These skip connections account for a significant overhead in latency due to increased memory access cost [13, 55].
To address this latency overhead, we introduce RepMixer, a fully reparameterizable token mixer that uses structural reparameterization to remove the skip-connections. The
RepMixer block also uses depthwise convolutions for spa-tial mixing of information similar to ConvMixer [53]. How-ever, the key difference is that our module can be reparam-eterized at inference to remove any branches.
To further improve on latency, FLOPs and parameter count, we replace all dense k k convolutions with their fac-× torized version, i.e. depthwise followed by pointwise con-volutions. This is a common approach used by efficient ar-chitectures [26, 46, 25] to improve on efficiency metrics, but, naively using this approach hurts performance as seen in Table 1. In order to increase capacity of the these layers, we use linear train-time overparameterization as introduced in [13, 11, 12, 55, 18]. These additional branches are only introduced during training and are reparameterized at infer-ence.
In addition, we use large kernel convolutions in our net-work. This is because, although self-attention based to-ken mixing is highly effective to attain competitive accu-racy, they are inefficient in terms of latency [39]. There-fore, we incorporate large kernel convolutions in Feed For-ward Network (FFN) [14] layer and patch embedding lay-ers. These changes have minimal impact on overall latency of the model while improving performance.
Thus, we introduce FastViT that is based on three key design principles– i) use of RepMixer block to remove skip connections, ii) use of linear train-time overparameteriza-(a) (b)
Figure 1: (a) Accuracy vs. Mobile latency scaling curves of recent methods. The models are benchmarked on an iPhone 12
Pro following [55]. (b) Accuracy vs. GPU latency scaling curves of recent methods. For better readability only models with
Top-1 accuracy better than 79% are plotted. See supplementary materials for more plots. Across both compute fabrics, our model has the best accuracy-latency tradeoff. tion to improve accuracy, iii) use of large convolutional ker-nels to substitute self-attention layers in early stages.
FastViT achieves significant improvements in latency compared to other hybrid vision transformer architectures while maintaining accuracy on several tasks like – image classification, object detection, semantic segmentation and 3d hand mesh estimation. We perform a comprehensive analysis by deploying recent state-of-the-art architectures on an iPhone 12 Pro device and an NVIDIA RTX-2080Ti desktop GPU.
×
×
× faster than EfficientNetV2-S [49], 3.5
In Figure 1, we show that, at ImageNet Top-1 accu-faster than EfficientNet-racy of 83.9%, our model is 4.9
B5 [48], 1.6
× faster than CMT-S [17] and 1.9 faster than ConvNeXt-B [36] on an iPhone 12 Pro mobile device. At ImageNet
Top-1 accuracy of 84.9% our model is just as fast as NFNet-F1 [1] on a desktop GPU while being 66.7% smaller, us-ing 50.1% less FLOPs and 42.8% faster on mobile de-vice. At latency of 0.8ms on an iPhone 12 Pro mobile device, our model obtains 4.2% better Top-1 accuracy on
ImageNet than MobileOne-S0. For object detection and in-stance segmentation on MS COCO using Mask-RCNN [20] head, our model attains comparable performance to CMT-S [17] while incurring 4.3 lower backbone latency. For semantic segmentation on ADE20K, our model improves over PoolFormer-M36 [65] by 5.2%, while incurring a 1.5
× lower backbone latency on an iPhone 12 Pro mobile device.
On 3D hand mesh estimation task, our model is 1.9 faster than MobileHand [16] and 2.8 faster than recent state-of-the-art MobRecon [4] when benchmarked on GPU.
×
×
×
In addition to accuracy metrics, we also study the ro-bustness of our models to corruption and out-of-distribution samples which does not always correlate well with accu-racy. For example, PVT [58] achieves highly competitive performance on ImageNet dataset, but has very poor ro-bustness to corruption and out-of-distribution samples as re-ported in Mao et al. [38]. In real world applications, using a robust model in such a scenario can significantly improve user experience. We demonstrate the robustness of our ar-chitecture on popular benchmarks and show that our models are highly robust to corruption and out-of-distribution sam-ples while being significantly faster than competing robust models. In summary, our contributions are as follows:
• We introduce FastViT, a hybrid vision transformer that uses structural reparameterization to obtain lower mem-ory access cost and increased capacity, achieving state-of-the-art accuracy-latency trade-off.
• We show that our models are the fastest in terms of la-tency on two widely used platforms – mobile device and desktop GPU.
• We show that our models generalize to many tasks – im-age classification, object detection, semantics segmenta-tion, and 3D hand mesh regression.
• We show that our models are robust to corruption and out-of-distribution samples and significantly faster than com-peting robust models. 2.