Abstract
Despite tremendous progress in generating high-quality images using diffusion models, synthesizing a sequence of animated frames that are both photorealistic and tempo-rally coherent is still in its infancy. While off-the-shelf billion-scale datasets for image generation are available, collecting similar video data of the same scale is still chal-lenging. Also, training a video diffusion model is compu-tationally much more expensive than its image counterpart.
In this work, we explore finetuning a pretrained image dif-fusion model with video data as a practical solution for the video synthesis task. We find that naively extending the image noise prior to video noise prior in video diffu-sion leads to sub-optimal performance. Our carefully de-signed video noise prior leads to substantially better perfor-mance. Extensive experimental validation shows that our
*Work done during an internship at NVIDIA. model, Preserve Your Own COrrelation (PYoCo), attains
SOTA zero-shot text-to-video results on the UCF-101 and
MSR-VTT benchmarks. It also achieves SOTA video gener-ation quality on the small-scale UCF-101 benchmark with a 10× smaller model using significantly less computation than the prior art. The project page is available at https:
//research.nvidia.com/labs/dir/pyoco/. 1.

Introduction
Large-scale diffusion-based text-to-image models [38, 42, 2] have demonstrated impressive capabilities in turning com-plex text descriptions into photorealistic images. They can generate images with novel concepts unseen during train-ing. Sophisticated image editing and processing tasks can easily be accomplished through guidance control and em-bedding techniques. Due to the immense success in several applications [30, 67, 5], these models are established as pow-1
(a) (b)
Figure 2: Visualizing the noise map correlations. (a) visualizes the t-SNE plot of the noise maps corresponding to input frames randomly sampled from videos. These noise maps are obtained by running a diffusion ODE [49, 48] on the input frames using a trained text-to-image model, but in the opposite direction of image synthesis (σ : 0 → σmax). The green dots in the background denote the reference noise maps sampled from an i.i.d. Gaussian distribution. The red dots and yellow dots are noise maps corresponding to input frames coming from different videos. We found they are spread out and share no correlation. On the other hand, the noise maps corresponding to the frames coming from the same video (shown in blue dots) are clustered together. (b) Using an i.i.d. noise model (orange dots) for finetuning text-to-image models for video synthesis is not ideal since temporal correlations between frames are not modeled. To remedy this, we propose a progressive noise model in which the correlation between different noise maps is injected along the temporal axis. Our progressive noise model (blue dots) aptly models the correlations present in the video noise maps. erful image synthesis tools for content generation. As image synthesis is largely democratized with the success of these text-to-image models, it is natural to ask whether we can repeat the same success in video synthesis with large-scale diffusion-based text-to-video models.
Multiple attempts have been made to build large-scale video diffusion models. Ho et al. [17] proposed a UNet-based architecture for the video synthesis task that is trained using joint image-video denoising losses. Imagen video [14] extends the cascaded text-to-image generation architecture of Imagen [42] for video generation. In both works, the authors directly train a video generation model from scratch.
While these approaches achieve great success and produce high-quality videos, they are inherently expensive to train, requiring hundreds of high-end GPUs or TPUs and several weeks of training. After all, video generators not only need to learn to form individual images but should also learn to synthesize coherent temporal dynamics, which makes the video generation task much more challenging. While the formation of individual frames is a shared component in an image and video synthesis, these works disregard the exis-tence of powerful pretrained text-to-image diffusion models and train their video generators from scratch.
We explore a different avenue for building large-scale text-to-video diffusion models by starting with a pretrained text-to-image diffusion model. Our motivation is that most of the components learned for the image synthesis task can effectively be reused for video generation, leading to knowl-edge transfer and efficient training. A similar idea is adopted by several recent works [46, 69, 4]. Without exception, when finetuning, they naively extend the image diffusion noise prior (i.i.d. noise) used in the text-to-image model to a video diffusion noise prior by adding an extra dimen-sion to the 2D noise map. We argue that this approach is not ideal as it does not utilize the natural correlations in videos that are already learned by the image models. This is illustrated in Figure 2, where we visualize the t-SNE plot of noise maps corresponding to different input frames as obtained from a pretrained text-to-image diffusion model.
The noise maps corresponding to different frames coming from the same video (blue dots in Figure 2a are clustered together, exhibiting a high degree of correlation. The use of i.i.d. noise prior does not model this correlation, which would impede the finetuning process. Our careful analysis of the video diffusion noise prior leads us to a noise prior that is better tailored for finetuning an image synthesis model to the video generation task. As illustrated in Figure 2b, our proposed noise prior (shown in blue dots) aptly captures the correlations in noise maps corresponding to video frames.
We then proceed to build a large-scale diffusion-based text-to-video model. We leverage several design choices from the prior works, including the use of temporal atten-tion [17], joint image-video finetuning [17], a cascaded gen-eration architecture [14], and an ensemble of expert denois-ers [2]. Together with these techniques and the proposed video noise prior, our model establishes a new state-of-the-art for video generation outperforming competing methods on several benchmark datasets. Figure 1 shows our model can achieve high-quality zero-shot video synthesis capability with SOTA photorealism and temporal consistency.
In short, our work makes the following key contributions. 1. We propose a video diffusion noise tailored for finetuning text-to-image diffusion models for text-to-video. 2. We conduct extensive experimental validation and verify the effectiveness of the proposed noise prior. 3. We build a large-scale text-to-video diffusion model by finetuning a pretrained eDiff-I model with our noise prior and achieve state-of-the-art results on several benchmarks. 2.