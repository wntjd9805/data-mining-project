Abstract
Reconstructing the shape and spatially varying sur-face appearances of a physical-world object as well as its surrounding illumination based on 2D images (e.g., pho-tographs) of the object has been a long-standing problem in computer vision and graphics. In this paper, we intro-duce an accurate and highly efﬁcient object reconstruction pipeline combining neural based object reconstruction and physics-based inverse rendering (PBIR). Our pipeline ﬁrstly leverages a neural SDF based shape reconstruction to pro-duce high-quality but potentially imperfect object shape.
Then, we introduce a neural material and lighting distil-lation stage to achieve high-quality predictions for mate-rial and illumination. In the last stage, initialized by the neural predictions, we perform PBIR to reﬁne the initial results and obtain the ﬁnal high-quality reconstruction of object shape, material, and illumination. Experimental re-sults demonstrate our pipeline signiﬁcantly outperforms ex-isting methods quality-wise and performance-wise. Code: https://neural-pbir.github.io/ 1.

Introduction
Reconstructing geometry, material reﬂectance, and light-ing from images, also known as inverse rendering, is a long-standing challenge in computer vision and graphics. Con-ventionally, the acquisition of the three intrinsic compo-nents has been mainly studied independently. For instance, multiview-stereo (MVS) [26, 27, 9] and time-of-ﬂight [44] methods only focus on recovering object geometry, usually based on diffuse reﬂectance assumption. Classical mate-rial acquisition methods typically assume known or simple geometries (e.g., a planar surface) with highly controlled il-luminations [22, 34, 43], usually created with a light stage or gantry. This signiﬁcantly limits their practicality when such capturing conditions are unavailable.
Recently, the advent of novel techniques enables us to jointly reconstruct shape, material, and lighting from 2D images of an object. At a high level, these techniques can be classiﬁed into two categories. Neural reconstruction meth-ods encode the appearance of objects into a multi-layer per-ceptron (MLP) and optimize the network by minimizing the rendering errors from different views through differen-tiable volume ray tracing. NeRF [18] reconstructs a density
ﬁeld-based radiance ﬁeld that allows high-quality view syn-thesis but not relighting. A series of methods [31, 35, 24] compute the density ﬁeld from the signed distance function to achieve high-quality geometry reconstruction. Recent works [40, 41, 3, 4, 20, 11, 42] seek to fully decompose shapes, materials, and lighting from input images. How-ever, due to the high computational cost of volume ray trac-ing and neural rendering, those methods take hours [20, 11] or a day [3, 42] to run and usually cannot model more com-plex indirect illumination [40, 41, 3, 4, 20, 11], causing shadows and color bleeding to be baked into the material re-ﬂectance. Several new methods [28, 19, 7] signiﬁcantly re-duce the computational cost of radiance ﬁeld reconstruction using hybrid volume representations and efﬁcient MLPs.
We are among the ﬁrst that adopt these novel techniques for efﬁcient joint recovery of geometry, material, and lighting.
On the contrary, physics-based inverse rendering (PBIR)
Figure 2: Our pipeline for joint shape, material, and lighting estimation.
[6, 16, 23, 1] optimizes shape, material, and lighting by computing unbiased gradients of image appearance with respect to scene parameters. Leveraging physics-based differentiable renderers [38, 12, 15], state-of-the-art PBIR pipelines can efﬁciently handle complex light transport ef-fects such as soft shadow and interreﬂection. Such com-plex light transport effects cannot be easily handled through volume-based neural rendering. On the other hand, since
PBIR methods rely on gradient-based optimization to reﬁne intrinsic components, they can be prone to local minima and overﬁtting. Therefore, they may require a good initializa-tion to achieve optimal reconstruction quality.
In this paper, we present a highly efﬁcient and accurate inverse rendering pipeline with the advantages of both neu-ral reconstruction and PBIR methods. Our pipeline attempt to estimate geometry, spatially varying material reﬂectance, and an HDR environment map from multiple images of an object captured under static but arbitrary lighting. As shown in Fig. 2, our pipeline consists of three stages. In the ﬁrst stage, we propose a hybrid neural volume-based method for fast neural SDF and radiance ﬁeld reconstruction, which achieves state-of-the-art geometry accuracy and efﬁciency.
In the next stage, based on the reconstructed geometry and radiance ﬁeld, we design an efﬁcient optimization method to distill materials and lighting by ﬁtting the surface light
ﬁeld. Our method relies on a radiance ﬁeld to handle vis-ibility and indirect illumination but avoids expensive vol-ume ray tracing to signiﬁcant computational cost compared to some recent works. Finally, we use an advanced PBIR framework [38, 12] to jointly reﬁne the geometry, materi-als, and lighting. Note that our PBIR framework models complex light transport effects such as visibility, occlusion, soft shadows and interreﬂection in a physically correct and unbiased way while still being much faster than recent in-verse rendering methods.
Concretely, our contributions include the following:
• A hybrid volume representation for fast and accurate ge-ometry reconstruction.
• A efﬁcient optimization scheme to distill high-quality initial material and lighting estimation from the recon-structed geometry and radiance ﬁeld.
• An advanced PBIR framework that jointly optimizes ma-terials, lighting and geometry with visibility and inter-reﬂection handled in a physically unbiased way.
• A end-to-end pipeline that achieves state-of-the-art geom-etry, material and lighting estimation that enables realistic view synthesis and relighting. 2.