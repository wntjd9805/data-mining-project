Abstract
Face aging is an ill-posed problem because multiple plausible aging patterns may correspond to a given input.
Most existing methods often produce one deterministic esti-mation. This paper proposes a novel CLIP-driven Pluralis-tic Aging Diffusion Autoencoder (PADA) to enhance the di-versity of aging patterns. First, we employ diffusion models to generate diverse low-level aging details via a sequential denoising reverse process. Second, we present Probabilistic
Aging Embedding (PAE) to capture diverse high-level aging patterns, which represents age information as probabilis-tic distributions in the common CLIP latent space. A text-⋆Corresponding author guided KL-divergence loss is designed to guide this learn-ing. Our method can achieve pluralistic face aging condi-tioned on open-world aging texts and arbitrary unseen face images. Qualitative and quantitative experiments demon-strate that our method can generate more diverse and high-quality plausible aging results. 1.

Introduction
Face aging aims to model facial appearance changes across different ages meanwhile maintaining identity infor-mation. It is an ill-posed learning problem due to multiple plausible aging results for a given input. Given various ag-ing images in the last row of Fig. 1, which one meets your
imagination of aging? Since human aging process is influ-enced by a variety of factors, including genetics and social environment, there may be significant differences in both general aging trends and local details. Pluralistic face aging aims to generate multiple and diverse plausible face aging images from a single input.
Deep generative models, such as generative adversar-ial networks (GANs) [9] and variational autoencoders (VAEs) [17], have shown impressive performance in terms of face aging [24, 28, 26, 1, 10]. Unfortunately, most pre-vious methods can only produce one “optimal” aging pat-tern, which is inconsistent with human cognition. Recently, diffusion models [6, 16] show comparable or even better generation quality compared to GANs, which learn the re-verse of a particular Markov diffusion process and cover the modes of data distribution better. Inspired by this, we intend to employ diffusion models to generate aging faces with low-level subtle stochastic variations, such as diverse wrinkles, as shown in the lower right of Fig. 1 (d).
In addition to low-level stochastic details, the aging pro-cess is accompanied by high-level age semantic changes, such as, getting fatter or thinner, getting darker or whiter, as shown in Fig. 1 (c). Previous face aging methods [28, 1, 10, 8] directly represent the target age as a deterministic point or direction in the latent space, ignoring the personalized age characteristics. So here comes a key challenge for pluralis-tic face aging: how to learn high-level age representations with stochastic variations. To address it, we draw support from the pre-trained CLIP [32] model and propose Proba-bilistic Aging Embedding (PAE), which represents age in-formation as a distribution rather than a deterministic point.
The intuition to leverage CLIP is illustrated in Fig. 2. In the well-aligned image-text latent space, there are likely to be multiple image-based age features for a coarse text-based age feature of “Man’s face in his forties”. Inspired by it, we attempt to model PAE in CLIP latent space to capture the stochastic high-level age semantics.
In this paper, we explore pluralistic face aging based on both text and image conditions. We propose a CLIP-driven
Pluralistic Aging Diffusion Autoencoder (PADA) to simul-taneously model low-level stochastic variations and high-level age semantic variations in the aging process. For the low-level age variations, our method is based on the diffusion model, which can generate stochastic low-level face details via a sequential denoising procedure. For the high-level age variations, we propose Probabilistic Aging
Embedding (PAE) by representing the age information as probabilistic distributions in the CLIP space. Specifically, we represent the age information as a multivariate Gaus-sian distribution rather than a deterministic point, where the mean of the distribution indicates average age information, while the variance indicates the personalized aging patterns of the image. Then we feed back the PAE into the diffu-Figure 2: One-to-many correspondences between coarse text-based age feature and image-based age features in the
CLIP latent space. The solid line indicates the text-based age feature while the dashed line indicates the image-based age features. sion model with an adaptive modulation for pluralistic face aging. Since our goal is to learn the diverse aging patterns and achieve face aging with preservation of age-irrelevant information (i.e., identity and background), three types of losses are employed: 1) Text-guided KL-divergence loss; 2) Age fidelity loss; 3) Preservation loss. To summarize, our contributions are four-fold:
• We propose a novel CLIP-driven Pluralistic Aging Dif-fusion Autoencoder (PADA) for pluralistic face ag-ing, which can generate diverse aging results with both high-level age semantic variations and low-level stochastic variations.
• Probabilistic Aging Embedding (PAE) is proposed in the common CLIP space to represent the diverse high-level aging patterns as probabilistic distribution, where a text-guided KL-divergence loss is employed to guide this learning.
• A more user-friendly interaction way for face aging is provided, which can achieve age manipulation condi-tioned on both the open-world age descriptions and ar-bitrary unseen face images in the wild.
• Extensive qualitative and quantitative experiments show that our method outperforms the state-of-the-art
aging methods and can generate plausible and diverse aging patterns. 2.