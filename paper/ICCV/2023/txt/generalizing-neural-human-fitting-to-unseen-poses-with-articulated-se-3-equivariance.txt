Abstract 1.

Introduction
We address the problem of fitting a parametric human body model (SMPL) to point cloud data. Optimization-based methods require careful initialization and are prone to becoming trapped in local optima. Learning-based meth-ods address this but do not generalize well when the in-put pose is far from those seen during training. For rigid point clouds, remarkable generalization has been achieved by leveraging SE(3)-equivariant networks, but these meth-ods do not work on articulated objects. In this work we ex-tend this idea to human bodies and propose ArtEq, a novel part-based SE(3)-equivariant neural architecture for SMPL model estimation from point clouds. Specifically, we learn a part detection network by leveraging local SO(3) invari-ance, and regress shape and pose using articulated SE(3) shape-invariant and pose-equivariant networks, all trained end-to-end. Our novel pose regression module leverages the permutation-equivariant property of self-attention lay-ers to preserve rotational equivariance. Experimental re-sults show that ArtEq generalizes to poses not seen during 44% training, outperforming state-of-the-art methods by in terms of body reconstruction accuracy, without requir-ing an optimization refinement step. Furthermore, ArtEq is three orders of magnitude faster during inference than prior work and has 97.3% fewer parameters. The code and model are available for research purposes at https:
//arteq.is.tue.mpg.de.
∼
The three-dimensional (3D) capture of humans in var-ied poses is increasingly common and has many applica-tions including synthetic data generation [35], human health analysis [57], apparel design and sizing [51], and avatar cre-ation [10, 36, 50, 55]. Existing 3D body scanners output un-ordered point clouds, which are not immediately useful for the above applications. Consequently, the first step in pro-cessing such data is to register it; that is, to transform it into a canonical and consistent 3D representation such as a mesh with a fixed topology. For human bodies, this is typically done by first fitting a parametric model like SMPL [31] to the data; see Figure 1. Such a process should be efficient and general; that is, it should work for any input body scan, no matter the complexity of the pose. However, this is chal-lenging given the articulated structure of the body and the high degree of variation in shape and pose.
Traditional optimization-based methods for fitting bod-ies to point clouds [2, 7, 8, 21, 25] are usually based on
ICP [11] or its variants [2, 38, 61]. These approaches can recover accurate results even for complex poses, but require a good initialization, are computationally expen-sive, and may require significant manual input.
Inspired by progress made in neural architectures for rigid 3D point clouds [41, 42, 48, 58], learning-based approaches have been proposed to solve the registration task. Previous ap-proaches directly regress model parameters [24, 30, 54], in-termediate representations such as correspondences [5, 6],
or mesh vertex positions [19, 39, 59]. While less ac-curate than optimization, they can be used to initialize an optimization-based refinement step for improved accu-racy [15].
A major limitation of learning-based approaches, as re-ported in several recent papers [5, 6, 54], is their poor gen-eralization to body poses that lie outside the training dis-tribution. To understand why, let us first consider how a parametric model such as SMPL explains the input point cloud. Given the shape parameters, the model first gen-erates the overall body structure by deforming a template mesh in a canonical pose. Pose-dependent offsets are then added to the mesh. This deformed mesh then undergoes an articulated transformation that poses the body parts rigidly, and linear blend skinning is applied to smooth the result.
Therefore, the observed point cloud is modeled as a com-bination of a canonical body shape, a part-based articulated model, and non-rigid pose-corrective deformations. When training networks to fit SMPL to point clouds, the networks are tasked with capturing the joint distribution of canonical shape and pose deformation, entangling these factors while learning a prior over plausible body shape and pose. This data-dependent prior is useful to infer new, in-distribution samples, but becomes a limitation when it comes to poses that are far from the training set. Ideally, if the networks were designed to be equivariant to articulated body pose transformations, then unseen body poses at test time would not be a problem.
T
T
∈
, f (
X) =
G f (X), X
A function (network) f : V ariant with respect to a group
W is said to be equiv-→ if, for any transformation
V . This property can
T ∈ G aid in generalization since it allows one to train with only
“canonical” inputs f (X), while generalizing, by design, f (X). For example, to any transformation of the group
SE(3)-equivariant networks have been used to address the out-of-distribution (OOD) generalization problem in rigid point cloud tasks, see for example [9, 16, 40]. However, ex-tending this to the human body is far from straightforward, due to 1) its high degree of articulation, 2) the entangle-ment of pose and shape, and 3) deformations that are only approximately rigid.
T equivariant networks, all trained in an end-to-end man-ner. We further propose a novel pose regression module that leverages the permutation-equivariant property of self-attention layers to preserve rotational equivariance. Finally, to facilitate generalization to unseen poses, we cast pose re-gression as a weight prediction task, in which the predicted weights are used to calculate a weighted average over each of the discretized SO(3) rotations to obtain the final result.
Our empirical studies demonstrate the importance of in-troducing SE(3) equi-/in-variance for the task of SMPL pose and shape estimation from point cloud data. For out-of-distribution data, we show significant improvement over competing methods [5, 6, 54] in terms of part segmentation, as well as accuracy in pose and shape estimation, even when others are trained with SO(3) data augmentation. Notably, we outperform methods that require an optimization step, while ArtEq is purely regression-based. Our method also shows strong performance for in-distribution samples, sur-passing all previous (optimization-based) methods although it only uses regression. Finally, we demonstrate how em-ploying the right symmetries can lead to a lightweight net-work that is more than thirty times smaller than prior mod-els, as well as a thousand times faster at inference time, making it easy to deploy in real-world scenarios.
In summary, we make the following contributions: (1)
We propose a new framework for human shape and pose estimation from point clouds that integrates SE(3)-equi-/in-variant properties into the network architecture. (2) We propose a novel SE(3)-equivariant pose regression module that combines SE(3) discretization with the permutation-equivariant property of self-attention layers. (3) We show state-of-the-art performance on common benchmarks and datasets based on only regression, particularly for out-of-distribution poses. Additionally, our framework results in a much lighter model that performs three orders of magnitude faster than competitors at inference time. Our code and pre-trained models are available at https://arteq.is. tue.mpg.de. 2.