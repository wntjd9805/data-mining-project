Abstract
We present FUS3D, a fast and lightweight system for real-time 3D object detection and tracking on edge devices.
Our approach seamlessly integrates stages for 3D object detection and multi-object-tracking into a single, end-to-end trainable model. FUS3D is specially tuned for indoor 3D human behavior analysis, with target applications in
Ambient Assisted Living (AAL) or surveillance. The system is optimized for inference on the edge, thus enabling sensor-near processing of potentially sensitive data. In addition, our system relies exclusively on the less privacy-intrusive 3D depth imaging modality, thus further highlighting the potential of our method for application in sensitive areas.
FUS3D achieves best results when utilized in a joint detec-tion and tracking configuration. Nevertheless, the proposed detection stage can function as a fast standalone object de-tection model if required. We have evaluated FUS3D ex-tensively on the MIPT dataset and demonstrated its supe-rior performance over comparable existing state-of-the-art methods in terms of 3D object detection, multi-object track-ing, and, most importantly, runtime. 1.

Introduction
We present a Fast Unified System for 3D Object Detec-tion and Tracking, abbreviated as FUS3D Object Detection and Tracking. The proposed approach performs both 3D object detection and multi-object-tracking in a joint frame-work that solely relies on 3D depth data. Our work is the first to demonstrate that such a system can be sufficiently optimized to achieved real time inference speeds on edge devices such as the Nvidia Jetson Nano singleboard com-puter. Despite the tight integration our system is flexible in its use. If required, our object detection stage can be used as a standalone model, thus allowing for even faster runtime at the cost of a small drop in detection accuracy.
The FUS3D system is best suited to small scale indoor environments and a static camera setting. Typical examples of such settings are human monitoring systems in the do-mains of ambient assisted living (AAL) and surveillance, which can be particularly well served with a combination of 3D depth sensors and edge-based model inference due to the intrinsic characteristics of the respective technologies.
Depth sensors can operate continuously and largely inde-pendently of lighting condition, as they do not require an ex-ternal source of illumination. The absence of color intensity information also renders the invasion of people’s privacy a less prominent issue, and is further reduced when poten-tially sensitive data can be analyzed sensor-near. FUS3D is decidedly application-oriented, and we believe it can bring immediate benefits to human-centered support and monitor-ing systems in AAL and related fields.
Our work is evaluated on the MIPT [12] dataset which consists of sequences of depth data focused on human indoor activity. We outperform comparable previous state-of-the-art methods in terms of object detection perfor-mance, tracking metrics, and, most importantly, runtime.
The main contributions of our paper are:
• FUS3D is more than 10 times faster than state-of-the-art 3D object detection methods while also incorpo-rating a transformer-based tracking stage. FUS3D is the first method to demonstrate a unification of 3D ob-ject detection and multi-object-tracking on severely re-source constrained hardware. Model code is publicly available. 1
• We present several approaches to improve 3D de-tection performance with little to no computational overhead, including an auxiliary orientation estimation loss, use of global context for tracking, and a novel dense target assignment (DTA) scheme.
• A seamlessly integrated transformer-based tracking stage that outperforms existing trackers and allows for end-to-end training along with a simplified approach to track association and acquisition.
Section 2 gives an overview over related work in the ar-eas of joint detection-and-tracking and transformer-based 1https://github.com/theitzin/FUS3D
systems in vision tasks. We continue in Section 3 with a de-tailed description of the proposed pipeline. The method is evaluated in Section 4 by giving ablation studies to validate design choices and comparisons with the current state-of-the art. Finally we concluded in Section 5 with a discussion of limitations and reflection on the presented work. 2.