Abstract
The rendering scheme in neural radiance field (NeRF) is effective in rendering a pixel by casting a ray into the scene.
However, NeRF yields blurred rendering results when the training images are captured at non-uniform scales, and produces aliasing artifacts if the test images are taken in distant views. To address this issue, Mip-NeRF proposes a multiscale representation as a conical frustum to encode scale information. Nevertheless, this approach is only suit-able for offline rendering since it relies on integrated po-sitional encoding (IPE) to query a multilayer perceptron (MLP). To overcome this limitation, we propose mip voxel grids (Mip-VoG), an explicit multiscale representation with a deferred architecture for real-time anti-aliasing render-ing. Our approach includes a density Mip-VoG for scene geometry and a feature Mip-VoG with a small MLP for view-dependent color. Mip-VoG represents scene scale us-ing the level of detail (LOD) derived from ray differentials and uses quadrilinear interpolation to map a queried 3D location to its features and density from two neighboring down-sampled voxel grids. To our knowledge, our approach is the first to offer multiscale training and real-time anti-aliasing rendering simultaneously. We conducted experi-ments on multiscale dataset, results show that our approach outperforms state-of-the-art real-time rendering baselines. 1.

Introduction
The realm of computer vision and graphics is marked by the captivating yet formidable challenge of novel view synthesis.
In recent times, neural volumetric representa-tions, most notably the neural radiance field (NeRF) [35], have emerged as a promising breakthrough in reconstruct-ing intricate 3D scenes from multi-view image collections.
*Equal contribution
Method
Mip-NeRF [3]
SNeRG [21]
MobileNeRF [10]
Ours
Multiscale
Training
✓
Real-time
Rendering
Anti-aliasing
Rendering
✓
✓
✓
✓
✓
✓
✓
Table 1: Our method is the first one concurrently addresses multiscale training, real-time and anti-aliasing rending.
NeRF employs a coordinate-based multilayer perceptron (MLP) architecture to map a 5D input coordinate (includ-ing 3D spatial position and 2D viewing direction) to in-trinsic scene attributes (namely, volume density and view-dependent emitted radiance) at that precise location. The pixel rendering process in NeRF involves casting a ray through the pixel into the scene, extracting the scene rep-resentation for points sampled along the ray, and ultimately fusing these components to produce the final color output.
While this rendering methodology excels when the train-ing and testing images share a uniform resolution, chal-lenges arise when the training images encompass varying resolutions. This discrepancy in resolutions leads NeRF to produce blurred rendering outputs due to the altered pixel footprints originating from diverse scales. Besides, in case that test viewpoints significantly deviate from the spatial distance of the training views, the sample rate for per pixel would be inadequacy, thereby results in aliasing artifacts.
To surmount this challenge, Mip-NeRF [3] emerges as a noteworthy solution, presenting a continuously-valued scale representation for coordinate-based models. Mip-NeRF in-troduces a pioneering technique, known as integrated po-sitional encoding (IPE), which facilitate the scene repre-sentation with the knowledge of scale. Departing from the conventional ray-based approach, Mip-NeRF adopts a novel rendering strategy involving conical frustums. This repre-sentation not only enables effective multiscale training but also tangibly mitigates the persisting issue of aliasing ar-tifacts. However, it’s important to note that this approach relies on querying the network with the scale-variant IPE.
As a consequence, the integration of pre-cache techniques that circumvent positional encoding [21], remains elusive within the current Mip-NeRF’s formulation.
This paper introduces a novel multiscale representation, termed ”Mip-VoG” (Mip Voxel Grids, drawing inspiration from ”mipmap”), which addresses the challenge of train-ing on images with varying scales and enables real-time anti-aliasing rendering during the inference stage (Tab. 1).
Our approach commences by unveiling a ”deferred” NeRF variant, wherein both scene geometry and color attributes are explicitly stored within the Mip-VoG framework. The intricate task of decoding view-dependent effects is exe-cuted using a compact multilayer perceptron (MLP) that ef-ficiently processes each pixel only once.
Instead of pre-training and baking a continuous NeRF into grids [21], we treat voxel values as parameters and directly optimize them as seen in the work by [16, 57]. Despite the “mip” nomenclature, Mip-VoG only maintains one density voxel grid and one feature voxel grid that represent the high-frequency spatial attributes. This structure permits the infer-ence of lower frequency representations through a progres-sive down-sampling process, achieved via low-pass filters and interpolation algorithms [14]. Given a single 3D point sampled from a camera ray, Mip-VoG intelligently deter-mines the level of detail (LOD) via ray differentials [22].
The LOD calculation is pivotal, as it establishes a pixel-to-voxel ratio that represents the point’s footprint on the full-resolution voxel grids. As a consequence, scene proper-ties corresponding to this point are sampled by interpolating between two adjacent down-sampled level grids. Notably, camera rays cast from a low-resolution frame are spatially represented over a wider area, yielding a higher sample rate and capturing more lower-frequency information. During inference phrase, we pre-compute the voxel grids at each integer level and subsequently convert them to the sparse voxel grid data structure used by SNeRG [21] to increase the rendering speed.
We conducted a comprehensive evaluation of our ap-proach using well-established NeRF datasets, including
Synthetic-NeRF [35] and Multiscale-NeRF [3], in line with the multi-scale framework outlined in Mip-NeRF [3]. Our findings underscore the validity of our multiscale represen-tation in effectively addressing complex multiscale training scenarios, while successfully preserving both low and high-frequency components inherent in the multiscale dataset.
Comparative analysis against state-of-the-art real-time tech-niques reveals that our proposed approach excels in mitigat-ing multiscale challenges, yielding impressive results. Fur-thermore, our method proves instrumental in achieving re-markable accuracy in anti-aliasing rendering, bolstering its applicability and potential impact. 2.