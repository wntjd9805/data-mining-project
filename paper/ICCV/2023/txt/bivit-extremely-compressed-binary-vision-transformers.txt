Abstract
Model binarization can significantly compress model size, reduce energy consumption, and accelerate inference through efficient bit-wise operations. Although binarizing convolutional neural networks have been extensively stud-ied, there is little work on exploring binarization of vision
Transformers which underpin most recent breakthroughs in visual recognition. To this end, we propose to solve two fun-damental challenges to push the horizon of Binary Vision
Transformers (BiViT). First, the traditional binary method does not take the long-tailed distribution of softmax atten-tion into consideration, bringing large binarization errors in the attention module. To solve this, we propose Softmax-aware Binarization, which dynamically adapts to the data distribution and reduces the error caused by binarization.
Second, to better preserve the information of the pretrained model and restore accuracy, we propose a Cross-layer Bi-narization scheme that decouples the binarization of self-attention and multi-layer perceptrons (MLPs), and Param-eterized Weight Scales which introduce learnable scaling factors for weight binarization. Overall, our method per-forms favorably against state-of-the-arts by 19.8% on the
TinyImageNet dataset. On ImageNet, our BiViT achieves a competitive 75.6% Top-1 accuracy over Swin-S model. Ad-ditionally, on COCO object detection, our method achieves an mAP of 40.8 with a Swin-T backbone over Cascade Mask
R-CNN framework. 1.

Introduction
Vision Transformer (ViT) [21] and its variants have achieved great success in a variety of computer vision tasks, such as image classification [21, 49, 24], object detec-tion [38, 22, 11], semantic segmentation [82, 64, 12], etc.
However, massive parameters and calculations of the Trans-former models hinder their applications on portable devices such as mobile phones. To tackle the efficiency bottlenecks,
†H. Zhou and B. Zhuang are corresponding authors.
Figure 1. An illustration of attention binarization. Data is col-lected from pretrained Nest-T model and “Bi-Attention” denotes (a) Pre-softmax attention binarized by for binarized attentions.
BiBERT [60]. (b) Softmax attention binarized by our method. various model compression algorithms have been widely studied, such as distillation [65, 67, 33], pruning [58, 84, 75] and quantization [43, 45, 42]. Among them, binary neural networks (BNN) [15, 62, 8] aggressively compress weights and activations to a single bit, which delivers 32× savings on memory consumption, and enables efficient XNOR-popcount bit-wise operations to greatly accelerate model inference and reduce energy consumption.
However, the performance degradation restricts the wide application of BNNs, which is mainly caused by the lim-ited representational ability and difficulty in optimization.
To improve the performance of BNNs, binarized convo-lutional neural networks (CNNs) literature has been ex-tensively studied to design accurate binarization functions
[62, 83, 8], enhance the representation ability [54, 52, 86] and relieve the gradient approximation error in optimiza-tion [61, 30, 4]. Also, many attempts have been made in previous studies to binarize BERT [19] for natural language processing (NLP) tasks, such as calibrating the attention value range mismatch [60], customizing knowledge distil-lation and techniques in binary CNNs to Transformers [50].
However, there are few studies on the binarization of ViTs so far. Therefore, it is highly critical and imperative to ex-plore BiViT for diverse edge devices to infer ViT at low-latency and low-power for real-world flexibility. it
Firstly,
In addition to the common challenges mentioned above, binarizing Transformers presents two new exclusive chal-lacks effective methods for accu-lenges. rately binarizing softmax attention. Self-attention mod-ule aims to encode pairwise similarity between all the to-kens [69], which is very different from convolutional or fully-connected layers. Specifically, the values of atten-tion scores are all positive values between (0, 1) and exhibit long-tailed distributions after Softmax operation (See Fig-ures 1 (b) and 3). In contrast, the ordinary weights have both positive and negative values and follow a bell-shaped distribution. Moreover, attention scores are generated dur-ing inference while the ordinary weights are fixed after fin-ishing training. Consequently, the functionality and data distribution of softmax attention differ significantly from ordinary weights. As to this problem, the recent study BiB-ERT [60] proposes to maximize the information entropy of binary attention scores by applying Bool function on pre-softmax attentions, resulting in the balanced number of ze-ros and ones, as shown in Figure 1 (a). However, the soft-max attention scores are actually dominated by few ele-ments, thus the number of ones in binary attentions should be much less than the number of zeros to ensure a low quan-tization error (see Figure 1 (b)). In other words, BiBERT follows a softmax-agnostic approach and overlooks the ef-fect of Softmax on the distribution, resulting in mismatched attention score distributions before and after binarization and leading to significant quantization errors (See Table 1).
Another study BiT [50] proposes to learn both scales and thresholds for weight and attention binarization, making them fixed during inference. While this method works well for weight binarization, it neglects the dynamics of attention scores and cannot adapt well to the changing distribution of attentions, as this approach remains softmax-agnostic at in-ference time as well.
Secondly, how to preserve the information in the pre-trained ViTs during binarization is under explored. Un-like binary CNNs that perform well when training from scratch [61, 68, 52], we observe that BiViTs heavily rely on pretrained models and are sensitive to quantization, as shown in Figure 2. Even if the initial weights are derived from the pretrained model, directly binarizing all parame-ters still causes a huge loss of pretrained information, which then leads to a severe performance drop. Also, the loss of pretrained information is difficult for Transformers to re-cover through quantization-aware training (QAT). In partic-ular, MLP modules account for nearly half of the computa-tions and parameters within a Transformer [47]. They are mainly composed of 1 × 1 convolutions, which are widely
Figure 2. Impact of pretrained model when binarizing Trans-formers. The experiment is conducted on TinyImageNet dataset.
Initiating Transformers from the pretrained models greatly boosts the accuracy. recognized to be difficult to binarize due to the limited rep-resentational capability [86, 23, 7]. Therefore, the effective binarization of softmax attention and the retention of infor-mation from pretrained models remain open questions.
To reduce the quantization error in binarizing attentions, we first analyze the long-tailed distribution of softmax at-tention scores and discover their differing patterns across different attention vectors. To adaptively search the opti-mal thresholds for binarization, we propose an optimiza-tion algorithm based on sparse coding and coordinate de-scent, and further propose an efficient approximation called
Softmax-aware Binarization (SAB) to avoid conducting the optimization on each forward pass. Moreover, to retain pre-trained information and further enhance the model repre-sentational capability, we then propose Cross-layer Bina-rization (CLB) to decouple the quantization of self-attention and MLPs to avoid mutual interference and introduce Pa-rameterized Weight Scales (PWS) for weights binarization.
To our best knowledge, we are the pioneering work to probe binarizing Transformers for vision tasks.
In summary, our contributions are as follows:
• We are the pioneering work that explores binary vision
Transformers, a demanding recipe for efficient ViT in-ference.
• We design a Softmax-aware Binarization scheme for the self-attention module, which adapts to the long-tailed attention scores distribution and greatly reduces the quantization error.
• We propose Cross-layer Binarization and Parameter-ized Weight Scales to retain pretrained information and further enhance the representational ability of BiViTs, improving convergence and accuracy.
• Experiments on TinyImageNet and ImageNet for im-age classification, and COCO for object detection, demonstrate that it consistently outperforms current
state-of-the-arts by large margins, serving as a strong baseline for future research. 2.