Abstract
Scene boundary detection breaks down long videos in-to meaningful story-telling units and plays a crucial role in high-level video understanding. Despite signiﬁcant ad-vancements in this area, this task remains a challenging problem as it requires a comprehensive understanding of multimodal cues and high-level semantics. To tackle this issue, we propose a multimodal high-order relation trans-former, which integrates a high-order encoder and an adap-tive decoder in a uniﬁed framework. By modeling the mul-timodal cues and exploring similarities between the shot-s, the encoder is capable of capturing high-order relations between shots and extracting shot features with context se-mantics. By clustering the shots adaptively, the decoder can discover more universal switch pattern between successive scenes, thus helping scene boundary detection. Extensive experimental results on three standard benchmarks demon-strate that the proposed model performs favorably against state-of-the-art video scene detection methods. 1.

Introduction
Cognitive science has discovered that humans usually comprehend a long video by breaking down it into mean-ingful units and reasoning about their relationships [20].
Therefore, dividing a long video into a series of meaningful story-telling video scenes, i.e. video scene detection, turns out to be a crucial task towards high-level video understand-ing. In ﬁlmmaking and video production, the term ‘scene’ is a basic unit for story-telling, consisting of a series of seman-tic cohesive shots, while the term ‘shot’ is a series of frames captured by the same camera over an uninterrupted period of time [4]. The task of video scene detection has drawn remarkable attention and can be widely adopted to various tough applications, including video caption, content-driven video search, scene classiﬁcation, human-centric storyline construction and so on [8, 15, 31]. Although signiﬁcan-t progress has been achieved in recent years, video scene (a) Two consecutive scenes in movie Blade Runner. (b) Histograms of similarities between shots. (c) Diverse scene boundaries in movie Blade Runner.
Figure 1: (a) There is minimal visual alteration at the scene boundary, while the audio content undergoes signiﬁcan-(b) Direct similarities between shots near the t changes. boundary can all have high scores, while the trend of simi-larity variation can provide more discriminative signals. (c)
Scene boundaries can vary greatly even in the same movie.
Therefore, learning shot-level representations alone is in-sufﬁcient for this class-agnostic task. detection remains challenging due to requiring semantic un-derstanding in a long-term video.
Recently, researchers have proposed methods like [4, 15, 20, 38] that leverage visual cues in unlabeled videos to model scene boundary. These approaches mainly em-ploy an unsupervised contrastive learning strategy to dif-ferentiate shots from distinct scenes. Although these meth-ods have taken a big step forward, they simply use visual appearance cues to generate pseudo labels for contrastive
learning, resulting in the model learning the differences of shots at the visual apparent level rather than semantic lev-el. In order to address above limitations, approaches such as [18, 19, 23, 27] introduce various expert networks to model multimodal signals like visual, audio, cast and place in various video understanding tasks. These methods stil-l struggle to accurately identify scene boundaries in long-term videos, particularly those with complex story-telling structures. This is due to that modeling context of the shots plays a crucial role in identifying boundaries. Furthermore, all previous methods [1, 4, 23, 20, 27, 38] primarily em-phasize learning discriminative shot features and overlook that scene boundary detection is a class-agnostic task. Thus they fail to identify more universal switch pattern between different scenes which is less dependent on appearance.
Based on the above analysis and the characteristics of scene boundary detection, we believe following aspects are important in this task: (1) The multimodal shot represen-tations can provide comprehensive cues to detect scene boundary. (2) The high-order relation provides correction-s for the context of shots. (3) The scene-level representa-tions, derived from adaptively aggregation of shots features, can effectively indicate the switch pattern between differ-ent scenes. Firstly, as shown in Figure 1a, there is mini-mal alteration in the visual appearance between Scene A and scene B. However, the audio signal undergoes a signiﬁcant change, making it easier to discern the transition between scenes. Secondly, as shown in Figure 1b, the shots before and after the scene boundary, such as shot 48 and shot 49, exhibit high similarities, suggesting that the scene switch can occur seamlessly. Neither the environment nor the char-acters change, only the two men’s behavior changes. Un-der this circumstances, considering only the similarities be-tween shots (ﬁrst-order relation) can lead to undetectable blurred boundaries. Fortunately, by representing shots 47-49 as 10-dimensional similarity vectors based on their sim-ilarities with reference shots 43-52, we can observe a sig-niﬁcant change near the scene boundary. This motivates us that the similarities between these similarity vectors, de-noted as high-order relation, can correct for the ﬁrst-order relation. Thirdly, unlike the common video action detection task, video scene detection is a category-agnostic task [5].
As shown in Figure 1c, even within the same movie, the appearances of the ‘scene boundary shots’ (i.e. shots 48, 52, 63, 80) still vary greatly. It is more important to dis-cover the switch pattern between successive scenes rather than learn shot-level representations. This encourages us to learn more contextual and category-agnostic scene-level representations, which can be adaptively aggregated from shot-level features.
Take these observations together, we propose the Mul-timodal High-order Relation Transformer for video scene detection, which can model multimodal cues, high-order relation and scene adaptive clustering in a uniﬁed struc-ture. Speciﬁcally, we ﬁrst take multimodal expert networks and clip encoders to extract multimodal clip-level shot em-beddings for the input shot sequence. Then our designed multimodal high-order relation transformer, consisting of a high-order encoder and an adaptive decoder, is employed to model the high-order relation for the shots and gener-ate contextual scene-level representation in a uniﬁed frame-work. In detail, the high-order encoder primarily employs the multi-head and self-attention mechanism on the multi-modal shot embeddings to produce high-order relationships (‘relation in relation’), thereby enhancing the appropriate correlations between shots and suppressing erroneous ones.
And the adaptive decoder merges contextual shot embed-dings into scene representations with learnable scene pro-totypes and cross-attention. Finally, we take those contex-tualized shot embeddings for scene boundary classiﬁcation and scene embeddings for boundary position regression.
The major contributions of this work can be summa-rized as follows: (1) We propose the Multimodal High-order Relation Transformer, which can model multimodal cues, high-order relation and scene adaptive clustering in a uniﬁed structure. (2) In our transformer, we design the high-order encoder to enhance the context of shots and rec-tify noises in the correlation map of shots caused by their imperfect features. We further employ the adaptive de-coder to aggregate shot features, which is better at captur-ing switch pattern between scenes rather than visual appear-ance. (3) Extensive experimental results on three standard benchmarks including MovieNetSSeg [12], OVSD [27] and
BBC planet earth [1] demonstrate that the proposed model can outperform previous works by a large margin, and can be even competitive with those pre-training methods which consume more than 5 times training data. 2.