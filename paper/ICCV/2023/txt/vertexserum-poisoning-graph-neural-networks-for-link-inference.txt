Abstract
Graph neural networks (GNNs) have brought superb performance to various applications utilizing graph struc-tural data, such as social analysis and fraud detection. The graph links, e.g., social relationships and transaction his-tory, are sensitive and valuable information, which raises privacy concerns when using GNNs. To exploit these vul-nerabilities, we propose VertexSerum, a novel graph poi-soning attack that increases the effectiveness of graph link stealing by amplifying the link connectivity leakage. To infer node adjacency more accurately, we propose an at-tention mechanism that can be embedded into the link de-tection network. Our experiments demonstrate that Ver-texSerum signiﬁcantly outperforms the SOTA link infer-ence attack, improving the AUC scores by an average of 9.8% across four real-world datasets and three different
GNN structures. Furthermore, our experiments reveal the effectiveness of VertexSerum in both black-box and on-line learning settings, further validating its applicability in real-world scenarios. The source code is available at https://github.com/RollinDing/VertexSerum. 1.

Introduction
Graph Neural Networks (GNNs) have been widely adopted in various domains, such as ﬁnancial fraud detec-tion [25], social network analysis [19], and heart-failure prediction [6], thanks to their capabilities to model high-dimensional features and complex structural relationships between entities [30]. However, with the increasing use of graph data, concerns about data privacy are also growing
[1, 7, 27]. This is particularly relevant in industries such as ﬁnance and healthcare, where sensitive relationships are often embedded in graph-structured data.
Recently, there has been a rise in privacy attacks on
GNNs [11, 28] that infer link existence between nodes in graphs by only querying the graph model, thus posing a threat to the conﬁdentiality of GNNs. For a graph node pair, the similarity of their posterior distributions (abbreviated as
“posteriors” [11]) is measured to deduce the link existence.
For instance, in federated learning scenario [10], where dif-ferent parties keep private data locally but contribute to the
GNN training in the cloud based on their data, a malicious contributor can infer the link belonging to other contributors by querying trained GNN models. In this context, the risks of link information leakage lie in the joint training of GNNs and the available GNN inference APIs on graph data.
In this work, we identiﬁed a limitation of the existing link-inferring attacks: they do not perform well if the inter-ested node pairs are from the same category (intra-class).
This is due to the high similarity of the posterior distribu-tions between node pairs in the same category. To overcome this limitation, we propose a novel approach to signiﬁcantly improve link inference attacks, particularly on intra-class node pairs, by allowing a malicious contributor to poison the graph during GNN training in an unnoticeable way.
This paper proposes a novel privacy-breaching data poi-soning attack on GNNs, VertexSerum1, with a new anal-ysis strategy. The attack aims to amplify the leakage of private link information by modifying nodes/vertices. This work makes the following contributions: 1. We propose a new evaluation metric, intra-class AUC score, for link inference attacks, by considering only node pairs from the same class. It overcomes the bias in prior works not differentiating between inter-class and intra-class, bringing valuable insights for our approach. 2. We introduce the ﬁrst privacy-breaching data poisoning attack on GNNs, which injects adversarial noise into a small portion (< 10%) of the training graph to amplify the graph’s link information leakage. We constructively employ a self-attention-based network to train the link detector and propose a pre-training strategy to overcome the overﬁtting issue of limited training data. 3. We demonstrate the effectiveness of the proposed link inference attack on popular GNN structures and graph datasets. The attack improves the link stealing AUC score by 9.8% compared to the SOTA method in [11].
*These authors contributed equally to this work. 1The name is inspired by Veritaserum in the Harry Potter series.
4. We consider the practicality of applying VertexSerum by evaluating its homophily noticeability of the poisoned graph and the victim model accuracy. The experimental results show that VertexSerum increases model privacy leakage without affecting the GNN performance. 2.