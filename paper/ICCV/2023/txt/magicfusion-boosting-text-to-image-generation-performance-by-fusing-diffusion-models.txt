Abstract
The advent of open-source AI communities has produced a cornucopia of powerful text-guided diffusion models that are trained on various datasets. While few explorations have been conducted on ensembling such models to com-bine their strengths.
In this work, we propose a simple yet effective method called Saliency-aware Noise Blend-ing (SNB) that can empower the fused text-guided diffusion models to achieve more controllable generation. Specifi-cally, we experimentally find that the responses of classifier-free guidance are highly related to the saliency of gen-erated images. Thus we propose to trust different mod-*Work done during an internship at JD Explore Academy.
†Corresponding authors. els in their areas of expertise by blending the predicted noises of two diffusion models in a saliency-aware man-ner. SNB is training-free and can be completed within a
DDIM sampling process. Additionally, it can automati-cally align the semantics of two noise spaces without re-quiring additional annotations such as masks. Extensive experiments show the impressive effectiveness of SNB in various applications. The project page is available at https://magicfusion.github.io/. 1.

Introduction
In recent years, significant progress has been made in image generation [29, 6, 24, 23, 27, 20, 29, 5] thanks to breakthroughs in diffusion models [32, 29, 33] and large-scale training [24, 20, 29, 5], as well as the contributions of open-source AI communities. Pre-trained large models have become an invaluable resource in this field. One of the most exciting developments has been text-guided dif-fusion models [24, 20, 29, 27]. A wide range of powerful text-guided diffusion models trained on various datasets has been publicly released. For example, general models (e.g., stable diffusion v1-4, v1-5, v2-1, etc. [27]) trained on large-scale multimodal datasets like LAION 5B [31], as well as more specialized models (e.g., Anything-v3) trained on car-toon and anime datasets or fine-grained categories such as cars [38]. There are even fine-tuned models designed for specific objects [28]. The vast amounts of data and com-putational cost have enabled these models to achieve im-pressive capabilities in various fields. However, few explo-rations have been conducted on ensembling such models to combine their strengths.
Some works propose to add special symbols or signa-ture phrases when fine-tuning models on new datasets [1].
This approach enables the model to generate novel image distributions while retaining its ability to generate from the original data distribution. However, there has been limited discussion on how to effectively combine the generation ca-pabilities of these two distributions. One intuitive method for integrating the capabilities of two models involves tak-ing a weighted average of their predicted noises [2]. How-ever, such kind of fusions often fails to fully preserve the strengths of each model. Blended diffusion [3] proposes to spatially blend a noisy image and a predicted one, which has been explored in image editing tasks. However, this typically requires specifying a mask to edit particular ob-jects, and few discussions have been conducted to blend the noises of two diffusion models.
In this work, we propose a simple yet effective method called Saliency-aware Noise Blending (SNB) that can em-power the fused text-guided diffusion models to achieve more controllable generation. Specifically, we integrate two diffusion models by spatially blending the predicted noises.
Our insight is to trust different models in their areas of ex-pertise, thus the strengths of each individual model can be preserved. To obtain diffusion models’ areas of expertise, we revisit the classifier-free guidance [13], which is widely adopted in text-guided diffusion models to enhance the dif-ference between a given text and a null text in the predicted noise space. We experimentally find that the responses of classifier-free guidance are highly related to the saliency of generated images. To this end, we propose Saliency-aware Noise Blending that blends the predicted noises of two diffusion models based on their responses to classifier-free guidance.
SNB is training-free and can be completed within a
DDIM sampling [33] process. Additionally, it can auto-matically align the semantics of two noise spaces without requiring additional annotations such as masks. Our main contributions can be summarised as follows:
• We propose to fuse two well-trained diffusion models to achieve more powerful image generation, which is a novel and valuable topic.
• We propose a simple yet effective Saliency-aware
Noise Blending method for text-guided diffusion mod-els fusion, which can preserve the strengths of each in-dividual model.
• We conduct extensive experiments on three chal-lenging applications (i.e., a general model + a car-toon model, a fine-grained car model, and a Dream-Booth [28] model), and prove that SNB can signifi-cantly empower pre-trained diffusion models.
The remainder of the paper is organized as follows. We describe related work in Section 2 and introduce our pro-posed SNB method in Section 3. An evaluation of three ap-plications is presented in Section 4, followed by the results and comparisons with other methods in Section 5. Finally, a comprehensive summary of the paper is presented and an analysis of the limitations is provided in Section 6. 2.