Abstract
Incremental object detection (IOD) task requires a model to learn continually from newly added data. However, di-rectly fine-tuning a well-trained detection model on a new task will sharply decrease the performance on old tasks, which is known as catastrophic forgetting. Knowledge dis-tillation, including feature distillation and response distil-lation, has been proven to be an effective way to alleviate catastrophic forgetting. However, previous works on feature distillation heavily rely on low-level feature information, while under-exploring the importance of high-level seman-tic information. In this paper, we discuss the cause of catas-trophic forgetting in IOD task as destruction of semantic feature space. We propose a method that dynamically dis-tills both semantic and feature information with consider-ation of both between-class discriminativeness and within-class consistency on Transformer-based detector. Between-class discriminativeness is preserved by distilling class-level semantic distance and feature distance among various categories, while within-class consistency is preserved by distilling instance-level semantic information and feature information within each category. Extensive experiments are conducted on both Pascal VOC and MS COCO bench-marks. Our method outperforms all the previous CNN-based SOTA methods under various experimental scenar-ios, with a remarkable mAP improvement from 36.90% to 39.80% under one-step IOD task. (a) Teacher (b) Fine-Tune ep1 (c) Fine-Tune ep12 (d) LwF baseline (e) DMD (f) DMD + IFD
Figure 1. Visualization of semantic feature space of old cate-gories. (a) represents the semantic feature space of old categories before adding new categories. (b)-(f) represent the semantic fea-ture space of old categories after adding new categories. (b) is (c) is at epoch 12 using at epoch 1 using fine-tuning method. fine-tuning method. The top three figures illustrate the cause of catastrophic forgetting as destruction of within-class con-sistency and between-class discrimnativeness. (d) using LwF baseline method. (f) using DMD (e) using our DMD method. method and IFD method at the same time. The bottom three fig-ures prove that our method can alleviate catastrophic forget-ting via maintaining the within-class consistency and between-class discrimnativeness from teacher to student. 1.

Introduction
In real-world scenarios, learning often occurs incremen-tally from streaming data. However, traditional object de-*Corresponding author tection models lack this capability. They usually make im-plicit assumptions about a fixed or stationary data distribu-tion [9]. Directly fine-tuning a model based on newly added data may result in a sharp decrease of its performance on the old data, which is well-known as catastrophic forget-ting. Catastrophic forgetting is the key problem for incre-mental learning/continual learning task [50]. Depending on whether the task identity is explicitly given or must be in-ferred, incremental learning (IL) is divided into three types: task/domain/class IL [29]. In this paper, we focus on the most complicated scenario: class incremental learning.
Incremental image classification task has been studied thoroughly [20, 32, 34, 37, 43], but only a few researches focus on incremental object detection (IOD) task [29, 9].
Unlike incremental classification task where there is only one category of objects within each image, IOD task may contain various categories of objects within each image.
When adding new categories, only annotations for the new objects are provided and all the old objects are categorized as backgrounds. The “background” labels interfere with the memory of the previously-learnt old labels, thus resulting in catastrophic forgetting of old categories during the in-cremental process. Here we do a visualization of semantic feature space before and after adding new categories to il-lustrate the idea above. As shown in Fig.1(a)-(c), adding new categories results in a much messier and severely dis-torted semantic feature space of the old categories. We thus innovatively propose a method of maintaining the semantic feature space in order to alleviate catastrophic forgetting of the old categories in IOD task.
Previous researches mostly utilize knowledge distillation
[14] to reduce catastrophic forgetting under IOD task [4].
There are three kinds of knowledge distillation, including feature-based distillation, response-based distillation, and relation-based distillation. Most works [31, 30, 45] design feature distillation by manually selecting specific layers to mimic the low-level features of old categories. For exam-ple, fine-grained feature distillation method [41] and multi-view correlation distillation method [45] selectively utilized intermediate layers to preserve the pattern of old classes.
However, this kind of methods heavily relies on low-level feature selection, while under-exploring the importance of high-level semantic information.
In this paper, we focus on how to take advantage of the high-level semantic feature space to improve the knowl-edge distillation methods. Generally, the representation of instance includes class-specific semantic knowledge, con-sisting of both within-class knowledge and between-class knowledge. Within-class knowledge represents the consis-tency of feature expressions in a certain category, while between-class knowledge represents the distinction of fea-ture expressions among various categories. Previous work
[15] shows the potential of using within-class and between-class knowledge in incremental classification task. During incremental object detection, all old classes are categorized as the same background, thus affecting their original dis-tinct feature distributions, and destroying both within-class consistency and between-class discriminativeness. How-ever, previous works in feature distillation have not explic-itly discussed the cause of catastrophic forgetting via these two components. Therefore, maintaining both semantic differences among various categories and semantic con-sistency within each category should be fully considered, in order to mitigate the issue of catastrophic forgetting in incremental object detection task.
To tackle the problem of within-class consistency, IFD (Interactive Feature Distillation) method is proposed to force information of the same category to remain close-by via mimicking information within the same category from teacher to student. The information includes instance-wise interaction between high-level semantics and low-level fea-tures. Moreover, to tackle the problem of between-class discrimnativeness, DMD (Distance Matrix Distillation) method is proposed to keep the class-wise between-class semantic difference and feature difference of student the same as that of teacher. Between-class semantic difference and feature difference are here represented as between-class semantic distance and feature distance between each two classes. Here we distill between-class semantic distance pattern and feature distance pattern from teacher to student, so as to keep between-class dissimilarity.
The main contributions of this work can be summarized below. (i) To the best of our knowledge, we are the first to discuss catastrophic forgetting in IOD task as the destruc-tion of within-class consistency and between-class discrim-nativeness. (ii) We propose a novel instance-wise feature distillation method based on the interaction between high-level semantics and low-level features to keep the within-class consistency. (iii) We propose a novel class-wise dis-tance distillation method based on distance matrix of high-level semantics and low-level features to keep the between-class discriminativeness. 2.