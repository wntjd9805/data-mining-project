Abstract
Visual question answering (VQA) requires systems to perform concept-level reasoning by unifying unstructured (e.g., the context in question and answer; “QA context”) and structured (e.g., knowledge graph for the QA context and scene; “concept graph”) multimodal knowledge. Ex-isting works typically combine a scene graph and a con-cept graph of the scene by connecting corresponding visual nodes and concept nodes, then incorporate the QA context representation to perform question answering. However, these methods only perform a unidirectional fusion from unstructured knowledge to structured knowledge, limiting their potential to capture joint reasoning over the heteroge-neous modalities of knowledge. To perform more expressive reasoning, we propose VQA-GNN, a new VQA model that performs bidirectional fusion between unstructured and structured multimodal knowledge to obtain unified knowl-edge representations.
Specifically, we inter-connect the scene graph and the concept graph through a super node that represents the QA context, and introduce a new mul-timodal GNN technique to perform inter-modal message passing for reasoning that mitigates representational gaps between modalities. On two challenging VQA tasks (VCR and GQA), our method outperforms strong baseline VQA methods by 3.2% on VCR (Q-AR) and 4.6% on GQA, sug-gesting its strength in performing concept-level reasoning.
Ablation studies further demonstrate the efficacy of the bidi-rectional fusion and multimodal GNN method in unifying unstructured and structured multimodal knowledge. 1.

Introduction
The visual question answering (VQA) task aims to pro-vide answers to questions about a visual scene. It is cru-cial in many real-world tasks including scene understand-ing, autonomous vehicles, search engines, and recommen-dation systems [1, 2, 9, 15]. To solve VQA, systems need to
* Work done while at Stanford University.
Figure 1. Overview of VQA-GNN. Given an image and QA sen-tence, we obtain unstructured knowledge (e.g., QA-concept node p and QA-context node z) and structured knowledge (e.g., scene-graph and concept-graph), and then unify them to perform bidirec-tional fusion for visual question answering. perform concept-level reasoning by unifying unstructured (e.g., the context in question and answer; “QA context”) and structured (e.g., knowledge graph for the QA context and scene; “concept graph”) multimodal knowledge.
Most of the high-performing VQA methods [6, 20, 26, 37, 49, 51, 52] pretrain a multimodal transformer model on a large-scale dataset to obtain unstructured multimodal knowledge from image and language contexts, and then finetune the pretrained model to reason on downstream tasks (e.g., visual commonsense reasoning (VCR) task
[50]). Existing methods (e.g., SGEITL [42]) also incor-porate structured knowledge into these transformer-based models by including a scene graph in the input of a pre-trained multimodal transformer model. More recent meth-ods [27, 54] further combine the scene graph and the con-cept graph by inter-connecting corresponding visual nodes and concept nodes through graph neural networks (GNNs), and then incorporate the unstructured QA context repre-sentation to perform question answering. However, these methods only perform late fusion or unidirectional fusion from unstructured knowledge to structured knowledge and do not train the model to mutually aggregate information from both sides. This can limit their potential to perform joint reasoning over the heterogeneous modalities of knowl-edge. As unstructured knowledge and structured knowledge have complementary benefits—pretrained unstructured rep-resentations capture broader knowledge and structured rep-resentations offer scaffolds for reasoning— [48], this moti-vates the development of models that deeply fuse the two modalities of knowledge for visual question answering.
We propose VQA-GNN (Figure 1), a new visual ques-tion answering model performing bidirectional fusion be-tween unstructured and structured multimodal knowledge to obtain a unified, more expressive knowledge representa-tion. VQA-GNN extracts a scene graph from the given in-put image using an off-the-shelf scene graph generator [38] and then retrieves a relevant concept graph for the input im-age and QA context from a general knowledge graph like
ConceptNet [36], obtaining a structured representation of the scene. Simultaneously, to obtain an unstructured knowl-edge representation for the scene, (1) we use pretrained
RoBERTa [23] to encode the context in question and an-swer (“QA-context”) as QA-context node, and (2) we re-trieve relevant visual regions from a general scene graph
VisualGenome [18] and take their mean pooled representa-tion as a QA-concept node, which we connect to the scene graph. We then connect the scene graph and the concept graph through QA-context node to build a multimodal se-mantic graph.
To achieve bidirectional fusion across the multimodal se-mantic graph, we introduce a new multimodal GNN tech-nique that performs inter-modal message passing. The mul-timodal GNN consists of two modality-specialized GNN modules, one for each modality, which perform inter-message aggregation between the QA-context node and nodes in structured graphs, aiming to reduce representa-tional gaps between modalities. Meanwhile, by leveraging the robust transformer-based architecture of RoBERTa, we unfreeze and finetune the weights of the QA-context node to enable mutual information aggregation from modality-specialized GNN modules.
We evaluate VQA-GNN on two challenging VQA tasks,
VCR [50] and GQA [13]. These tasks require systems to perform conceptual and compositional reasoning to answer diverse questions (e.g., multiple-choice question answering and rationale selection in VCR; open-domain question an-swering in GQA). Our model outperforms strong baseline
VQA methods [12,42] by 3.2% on VCR (Q-AR) and 4.6% on GQA. Moreover, ablation studies show the efficacy of our two main techniques, bidirectional fusion and multi-modal GNN message passing. On VCR, our multimodal
GNN technique that reduces multimodal gaps outperforms existing works that use generic GNNs [27,54] by 4.5%. On
GQA, bidirectional fusion outperforms a unidirectional fu-sion variant by 4%. These results confirm the promise of
VQA-GNN in unifying unstructured and structured multi-modal knowledge for reasoning. 2. Problem Setup
This work focuses on multiple-choice and open-domain visual question answering, respectively. Each data point consists of an image c, and a natural language question q.
For the multiple-choice setting, each question corresponds to a set of candidate answers A, where only one candidate acorrect ∈ A is the correct answer to the question. Given a
QA example (c, q, A), we assume we have access to its rele-vant joint graph G(vcr) and our goal is to identify the correct answer acorrect ∈ A. For the open-domain setting, all ques-tions correspond to a large set of common answer classes
B, where only one candidate bcorrect ∈ B is the best answer to each question. Given a data example (c, q) with relevant scene graph G(gqa), the goal is to identify bcorrect ∈ B. 3.