Abstract
Domain adaptation (DA) aims to transfer knowledge from a fully labeled source to a scarcely labeled or to-tally unlabeled target under domain shift. Recently, semi-supervised learning-based (SSL) techniques that leverage pseudo labeling have been increasingly used in DA. De-spite the competitive performance, these pseudo labeling methods rely heavily on the source domain to generate pseudo labels for the target domain and therefore still suffer considerably from source data bias. Moreover, class dis-tribution bias in the target domain is also often ignored in the pseudo label generation and thus leading to fur-ther deterioration of performance. In this paper, we pro-pose GeT that learns a non-bias target embedding distri-bution with high quality pseudo labels. Specifically, we formulate an online target generative classifier to induce the target distribution into distinctive Gaussian compo-nents weighted by their class priors to mitigate source data bias and enhance target class discriminability. We fur-ther propose a structure similarity regularization frame-work to alleviate target class distribution bias and further improve target class discriminability. Experimental results show that our proposed GeT is effective and achieves con-sistent improvements under various DA settings with and without class distribution bias. Our code is available at: https://lulusindazc.github.io/getproject/. 1.

Introduction
Despite the remarkable advances of deep learning in the last decade [19, 58, 21, 27], the success of most deep learning-based works is based on the assumption that the data distributions of the train and test sets are similar, i.e. no domain shift. However, it is difficult to ensure no do-main shift in the data distributions for many practical real-world scenarios. Consequently, many domain adaptation (DA) works [11, 25, 54, 50] have been proposed to allevi-ate the domain shift problem. Unsupervised DA (UDA) is the most commonly studied DA setting, where the goal is to transfer knowledge from the labeled source data to the unla-Figure 1. Our GeT is designed to generate source domain and class distribution debiased and discriminative pseudo labels for the target domain in various domain adaptation tasks. beled target data with domain shift. Other more challenging variants of UDA include partial-set DA (PDA) [5, 68, 12] where the target label space is a subset of the source label space, semi-supervised DA (SSDA) [55, 22, 33, 26] which assumes partial target data are labeled, etc.
Most DA approaches are often either based on learning domain-invariant feature representations or directly adopt-ing SSL techniques for knowledge transfer.
It is shown in [3, 43, 44] that the target error is bounded by the source error and the divergence between marginal distributions in the source and target domains.
Inspired by the theoret-ical analysis, many works [38, 65, 56, 15, 61] propose to learn domain-invariant feature representations using a shared feature extractor to align the source and target do-mains. Nonetheless, feature alignment-based methods usu-ally suffer from the potential risk of damaging intrinsic target data discrimination. On the other hand, some re-cent works [9, 72, 51] investigate the application of SSL techniques, e.g. MixMatch [4] in [51], Label Propaga-tion [74] in [72], etc to strengthen discriminability on the unlabeled target domain. Although SSL-based DA meth-ods can achieve competitive performance, they often suffer from source domain bias due to over reliance on the source domain for pseudo label generation. A recent work [36] proposes to deal with data bias using an auxiliary target domain-oriented classifier (ATDOC) based on pseudo la-beling. It is shown that the proposed SSL regularization can work quite well in most DA scenarios.
In addition to source data bias, many DA approaches (in-cluding ATDOC) suffer significant performance drop due to class distribution bias in the target domain. Several meth-ods [66, 60, 23, 59] are proposed to alleviate class dis-tribution bias with class-conditioned sampling [23], class-balanced self-training [60], etc. However, as discussed in [10, 73], these methods rely on domain-invariant rep-resentation learning that can hurt intrinsic data discrimi-nation in the target domain. Consequently, a naive adop-tion of these methods on SSL-based DA can lead to unreli-able pseudo labels that greatly degrade performance. Fur-thermore, many existing DA methods are often designed to be task-specific and may not be versatile enough to handle complex variants of the DA problem, e.g. PDA and SSDA.
As illustrated in Fig. 1, we propose GeT to generate de-biased and discriminative pseudo labels to train the network on the DA tasks in this paper. Our GeT consists of an online target generative classifier and a structure similarity regular-ization. 1) Our online target generative classifier is a Gaus-sian mixture model (GMM). The class priors (i.e. mixture coefficients) and the means of the Gaussian components are the target features class distribution and prototypes, re-spectively. Intuitively, our generative classifier induces the target feature distribution into distinctive Gaussian compo-nents weighted by their respective class priors and thus al-leviating source data bias and enhancing target class dis-criminability. We introduce a memory bank that resembles a replay buffer to efficiently store and update the class pri-ors and feature prototypes for the classifier online in each mini-batch. 2) Our structure similarity regularization alle-viates target class distribution bias and further improves tar-get class discriminability. To this end, we introduce an aux-iliary distribution implicitly constrained with entropy max-imization to encourage balanced and discriminative pseudo labels. The final pseudo labels are obtained as a mixup of the pseudo labels generated by the target oriented genera-tive classifier and the auxiliary distribution. We jointly op-timize the auxiliary distribution, the pseudo labels and the network parameters in an iterative classification expectation maximization scheme.
We summarize our contributions as follows: 1) An on-line target oriented generative classifier is proposed to in-duce the distribution of the target features into distinctive
Gaussian components weighted by the class priors to avoid class distribution and source data biases while enhancing class discriminability. 2) We introduce a structure simi-larity regularization that leverages an auxiliary distribution implicitly constrained with entropy maximization to avoid the severely biased model predictions. 3) A classification expectation maximization framework is designed to jointly optimize the generative classifier with the structure similar-ity regularization for pseudo labels generation and train the network with the generated pseudo labels. 4) Competitive results are achieved in various DA settings on several stan-dard benchmark datasets. 2.