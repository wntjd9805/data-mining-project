Abstract 1.

Introduction
In this work, we focus on synthesizing high-quality tex-tures on 3D meshes. We present Point-UV diffusion, a coarse-to-ﬁne pipeline that marries the denoising diffu-sion model with UV mapping to generate 3D consistent and high-quality texture images in UV space. We start with introducing a point diffusion model to synthesize low-frequency texture components with our tailored style guid-ance to tackle the biased color distribution. The derived coarse texture offers global consistency and serves as a condition for the subsequent UV diffusion stage, aiding in regularizing the model to generate a 3D consistent UV tex-ture image. Then, a UV diffusion model with hybrid con-ditions is developed to enhance the texture ﬁdelity in the 2D UV space. Our method can process meshes of any genus, generating diversiﬁed, geometry-compatible, and high-ﬁdelity textures. Code is available at https://cvmi-lab.github.io/Point-UV-Diffusion.
†: Corresponding authors 1
Texturing 3D meshes is a fundamental task in computer
It enhances the visual richness of vision and graphics. 3D objects, thereby facilitating their application in various
ﬁelds such as video games, 3D movies, and AR/VR tech-nologies. However, generating high-quality textures can be daunting and time-consuming, often requiring special-ized knowledge and resources. As such, there is a pressing need for an efﬁcient approach to automatically create high-quality textures on 3D meshes.
Despite the substantial progress the community has made in 2D image synthesis and 3D shape generation using
GANs [13, 20, 44, 11] or diffusion models [34, 36, 35, 19], crafting realistic textures on mesh surfaces remains chal-lenging. One major difﬁculty stems from the need for suit-able 3D representations for texture synthesis. Early ap-proaches investigate the use of voxels [5, 45, 6] or point clouds [10] and synthesize point/voxel colors. However, they can only afford to synthesize low-resolution results with low-ﬁdelity textures due to memory and model com-plexity constraints. In response, Texture Fields [29] adopts an implicit representation with the potential to synthesize
high-resolution textures, but still could not yield satisfac-tory results as shown in Figure 2 (a): over-smoothed results.
Most recently, Siddiqui et al. [38] propose to parameterize the shape as tetrahedral meshes and introduce tetrahedral mesh convolution to enhance local details. Albeit improv-ing results, tetrahedral parameterization inevitably destroys geometric details of the input mesh and thus cannot faith-fully preserve the original structure. As shown in Figure 2 (b), the delicate structures of the chair’s back are absent.
Moreover, the generative models utilized in these methods are limited to GANs [29, 38] and VAEs [29, 12]. The more advanced diffusion model, which could potentially open up new avenues for high-quality texture generation, remains insufﬁciently explored.
In this paper, we delve into a novel texture representa-tion based on UV maps and investigate the advanced dif-fusion model for texture generation. The 2D nature of the
UV map enables it to circumvent the cost of high-resolution point/voxel representations. Besides, the UV map is com-patible with arbitrary mesh topologies, thereby preserving the original geometric structures. However, while promis-ing, direct integration of the UV map representation with a 2D diffusion model presents challenges in synthesizing seamless textures, leading to severe artifacts, as shown in
Figure 2 (c). This occurs because the UV mapping process fragments the continuous texture on the 3D surface into iso-lated patches on the 2D UV plane (see Figure 3).
To this end, we introduce Point-UV diffusion, a two-stage coarse-to-ﬁne framework consisting of point diffu-sion and UV diffusion. Speciﬁcally, we initially design a point diffusion model to generate color for sampled points that act as low-frequency texture components. This model is equipped with a style guidance mechanism that allevi-ates the impact of biased color distributions in the dataset and facilitates diversity during inference. Next, we project these colorized points onto the 2D UV space with 3D coor-dinate interpolation, thereby generating a coarse texture im-age that maintains 3D consistency and continuity. Given the coarse textured image, we develop a UV diffusion model with elaborately designed hybrid conditions to improve the quality of the textures (see Figure 2 (ours) and Figure 1).
In short, our contributions are as follows: 1) We propose a new framework for texture generation for given meshes.
Our representation can handle meshes with arbitrary topol-ogy and is able to faithfully preserve geometric structures. 2) To the best of our knowledge, we are the ﬁrst to train a diffusion model speciﬁcally for mesh texture generation.
Our coarse-to-ﬁne framework allows us to enjoy the efﬁ-ciency of 2D representation while enhancing 3D consis-tency. 3) We compare our approach with multiple meth-ods in unconditional generation and achieve state-of-the-art results. Furthermore, we demonstrate that our method can be easily extended to scenarios with text-conditioning and (cid:2)(cid:9)(cid:3)(cid:1)(cid:8)(cid:13)(cid:23)(cid:21)(cid:22)(cid:19)(cid:13)(cid:1)(cid:6)(cid:15)(cid:13)(cid:16)(cid:12)(cid:20)(cid:1)(cid:2)(cid:1) (cid:7)(cid:22)(cid:19)(cid:20) (cid:2)(cid:10)(cid:3)(cid:1)(cid:8)(cid:13)(cid:23)(cid:21)(cid:22)(cid:19)(cid:15)(cid:14)(cid:24) (cid:2)(cid:1) (cid:7)(cid:22)(cid:19)(cid:20) (cid:2)(cid:11)(cid:3)(cid:1)(cid:4)(cid:5)(cid:1)(cid:12)(cid:15)(cid:14)(cid:14)(cid:22)(cid:20)(cid:15)(cid:18)(cid:17)(cid:1)(cid:2)(cid:1) (cid:7)(cid:22)(cid:19)(cid:20)
Figure 2. Comparisons with different methods. Our generative results (a) possess high-quality details, (b) faithfully preserve the mesh structure, and (c) are better consistent with the given shape, compared with Texture Fields [29], Texturify [38] and 2D diffu-sion [16], respectively.
Figure 3. Illustration of UV mapping process. It establishes con-nections between the 2D texture map and the surface appearance of 3D shape. image-conditioning. 2.