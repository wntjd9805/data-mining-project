Abstract
Boundary localization is a challenging problem in Tem-poral Action Detection (TAD), in which there are two main issues. First, the submergence of movement feature, i.e. the movement information in a snippet is covered by the scene information. Second, the scale of action, that is, the propor-tion of action segments in the entire video, is considerably variable. In this work, we first design a Movement Enhance
Module (MEM) to highlight movement feature for better ac-tion location, and then, we propose a Scale Feature Pyra-mid Network (SFPN) to detect multi-scale actions in videos.
For Movement Enhance Module, firstly, Movement Feature
Extractor (MFE) is designed to get the movement feature.
Secondly, we propose a Multi-Relation Enhance Module (MREM) to grasp valuable information correlation both lo-cally and temporally. For Scale Feature Pyramid Network, we design a U-Shape Module to model different scale ac-tions, moreover, we design the training and inference strat-egy of different scales, ensuring that each pyramid layer is only responsible for actions at a specific scale. These two innovations are integrated as the Movement Enhance
Network (MENet), and extensive experiments conducted on two challenging benchmarks demonstrate its effectiveness.
MENet outperforms other representative TAD methods on
ActivityNet-1.3 and THUMOS-14. 1.

Introduction
Temporal Action Detection (TAD) is a significant video understanding task, which aims to extract video segments with specific action labels from untrimmed long videos. It remains a challenging problem because action boundaries are difficult to determine precisely, which adversely affects the performance in most works.
Two critical issues stand in the way of detecting action boundaries robustly from untrimmed videos. The first issue, which could be called as movement feature submergence, exists in where either context but not movement itself dom-inate feature expression, or movement is small in pixel size.
*Corresponding author (a) Strong class-specific context. (b) Small movement in pixel size. (c) Multi-scale actions.
Figure 1. Movement feature submergence and multi-scale actions. (a) Strong class-specific context of the accordion leads to video feature generated by TAC lacking the difference between action (b) Small movement of Futsal results in less and background. difference between action area and background area. (c) Multi-scale actions have different feature richness and action pattern.
For the first case, as shown in Fig 1(a), the action of Play-ing Accordion can be easily judged by Accordion, while this strong class-specific context causes the real movement information in action area to be submerged. For the second case, as shown in Fig 1(b), for Futsal, the stadium scene takes up a large proportion of pixels in the frame, while the movement information is ignored. This movement fea-ture submergence leads to the feature in background to be similar with the feature in action area, which blues the ac-tion boundaries. If the movement information can be high-lighted, the boundary will be easier to be located. However, this problem has been ignored by prior works [12, 29, 22].
The second issue is the multi-scale of actions in an
untrimmed video. As shown in Fig 1(c), the feature rich-ness varies greatly with the scale. There are rare features for action segments that account for a small proportion of the entire video, that is the small-scale action, and abundant features for those segments that account for a higher propor-tion, that is the large-scale action. We contend that there are different action patterns between different scales. Specifi-cally, compared with small-scale action, large-scale action contains more obviously action process (i.e. start phase, ac-tion phase and end phase). Some researches [13, 7, 11] re-sort feature pyramid network (FPN) to solve the problem.
However, there are two aspects that are ignored. Firstly, these works detect actions at different feature scales, but, they do not adaptively learn action features of different scales. Secondly, the information flow in FPN is insuffi-cient, lacking interactions between different pyramid layers.
In this paper, as an effort to overcome the above chal-lenges, we investigate the feature of TAD. (1) In order to overcome the movement feature submergence, and high-light the difference between foreground and background snippets. We design the Movement Enhance Module (MEM) to enhance the movement information in a video snippet, which includes two crucial parts. First, we propose a siamese network named Movement Feature Extractor (MFE), which uses the dynamic information of the frame sequence and the static information of the frame to extract movement feature from video snippets. And then, we pro-pose the Multi-Relation Enhance Module (MREM) to es-tablish temporal and local correlations between video snip-pets. (2) In order to learn specific representations for differ-ent scale actions, we propose the Scale FPN (SFPN), which uses a U-shaped network to produce multi-scale video fea-tures and facilitate the information flow between different layers. Moreover, for each layer in SFPN to take charge of detecting action segments of the corresponding scale, we design a two-stage learning strategy. In the former General-ization stage, each layer is trained with all action segments; in the latter Specialization stage, each layer is biased to-ward actions in a specific scale range. During inference, every layer takes charge of detecting action segments of the corresponding scale.
In summary, this work explores feature representation and action segment representation that are more suitable for
TAD task. Its contributions are summarized as follows. 1. To alleviate the movement feature submergence, we design the Movement Enhance Module (MEM) to highlight the movement feature in a video snippet, and explore local and temporal relations between snippets. 2. For the multi-scale actions in an untrimmed video with different feature patterns, we design the Scale FPN (SFPN) to learn different scale actions respectively, where targeted training and inference strategies are adopted. Consequently, each layer in the SFPN spe-cializes in actions at a certain scale range. 3. Extensive experiments conducted on two datasets ver-ify the effectiveness of our proposed method. On Ac-tivityNet1.3, MENet promotes the best average mAP from 36.6% to 37.7%, and boosts the mAP@0.7 from 31.8% to 34.0% on THUMOS-14. 2.