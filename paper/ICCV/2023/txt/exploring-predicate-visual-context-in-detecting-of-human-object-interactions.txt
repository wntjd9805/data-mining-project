Abstract
Recently, the DETR framework has emerged as the dom-inant approach for human–object interaction (HOI) re-In particular, two-stage transformer-based HOI search. detectors are amongst the most performant and training-efficient approaches. However, these often condition HOI classification on object features that lack fine-grained con-textual information, eschewing pose and orientation infor-mation in favour of visual cues about object identity and box extremities. This naturally hinders the recognition of com-plex or ambiguous interactions. In this work, we study these issues through visualisations and carefully designed experi-ments. Accordingly, we investigate how best to re-introduce image features via cross-attention. With an improved query design, extensive exploration of keys and values, and box pair positional embeddings as spatial guidance, our model with enhanced predicate visual context (PViC) outperforms state-of-the-art methods on the HICO-DET and V-COCO benchmarks, while maintaining low training cost. 1.

Introduction
Detecting human–object interactions (HOI) is the task of localising and recognising interactive human–object pairs.
It extends the detection of objects to include their rela-tionships and facilitates a deeper understanding of visual scenes. Recent developments in the detection of human– object interactions have largely adhered to the encoder– decoder style introduced by the detection transformers (DETR) [2], where learnable queries are randomly ini-tialised with Gaussian noise at the start of training, and pro-gressively decoded into the desired human-predicate-object triplets. Such one-stage detectors [33, 15, 47, 4, 42, 13] require pre-trained DETR weights for initialisation to facil-itate stable convergence. As we will demonstrate empiri-cally, the pre-trained encoder features have overfitted to ob-ject cues and lack the necessary information for recognising
†Work done at Microsoft Research Asia. (a) Attn. for human (DETR) (b) Attn. for bike (DETR) (c) Attn. for triplet (QPIC) (d) Attn. for predicate (Ours)
Figure 1. Visual context for the (a, b) two-stage HOI detector
UPT [44], (c) one-stage HOI detector QPIC [33] and (d) our method. Cross-attention weights from the last decoder layer are used for visualisation. UPT uses coarse object features that favour visual cues about object identity and box extremities. QPIC fails to detect the triplet person-washing-bike as it struggles to locate the relevant visual context (person, bike, and the water hose). The box pair with the highest IoUs against the ground truth is selected for display. Our two-stage method with pre-detected objects success-fully recognises the predicate washing as it pinpoints the location of the image region containing the water hose. human–object interactions. This means that the transformer encoder weights need to change significantly to produce discriminative features for such tasks. Together with the need to repurpose the decoder to detect HOI triplets rather than unary objects, this results in long training schedules that often amount to hundreds of GPU hours. On the other hand, two-stage detectors adopt a different methodology, wherein an object detector is fine-tuned and then frozen, which only introduces a one-off cost. These approaches fo-cus on the extraction and exploitation of the rich informa-tion residing in the frozen detector. Naturally, two-stage de-tectors require significantly less time and resources to train,
(a) Object feature replacement for predicate riding. (b) Object feature replacement for predicate throwing.
Figure 2. Object features from a frozen object detector (DETR [2]) are extracted from image regions that are indicative of the object identity and are near the bounding box extremities, as shown by the cross-attention weights overlaid on each detected object. These features often lack the fine-grained information to recognise HOIs. As a result, replacing object features with those from an object in a different (a, b) pose, orientation or even (a) identity does not impact the classification score significantly. Experiments are conducted on UPT [44], where the spatial configuration for pair (1, 2) is used for all pairs in each set of images while the object features are replaced. facilitating more model analysis and experimentation.
Current state-of-the-art two-stage detector UPT [44] employs a fine-tuned DETR detector, and performs self-attention on unary (object) and pairwise (human–object) to-kens. Despite its overall high performance and low cost, it only utilises object features from the frozen detector, com-plemented by hand-crafted spatial features, to construct the final representations. As we show in Figures 1a and 1b, these frozen features are obtained by attending to image re-gions indicative of object identity and box extremities, thus lacking the necessary information for recognising HOIs.
In Figure 2, we study the impacts of this lack of informa-tion by replacing the object features with those of a different object. For predicates with a distinct spatial pattern, such as riding, we observe that the predicted scores do not change significantly when object features are swapped, suggesting that the spatial information dominates the visual informa-tion, as shown in Figure 2a. Even when the replacement features come from objects of a different identity, UPT still gives confident predictions for the same predicate, such as (4, 2) horse–riding–horse. Yet, the majority of predicates do not exhibit a prominent spatial pattern, e.g. throwing. In such cases, visual context plays a crucial role. Naturally, replacing object features amounts to a more tangible im-pact (Figure 2b). However, the score drop when replacing the human features with those of a non-interactive person is still not significant, indicating that such features do not contain enough visual cues to differentiate interactiveness.
As we will show in Figure 3, failure cases of UPT often re-quire much richer visual context. In particular, we identify two types of context that coarse object features lack: fine-grained visual information, such as the human body parts, and information about other relevant context in the scene, such as another object involved in the interaction.
To address the aforementioned issues, we investigate how to enrich the contextual cues for human–object pair representations. Our contribution is twofold. We conduct thorough analysis with abundant visualisations to charac-terise the two types of visual contexts lacking in current two-stage models and the damage this causes. Accordingly, we develop a superior two-stage detector with a lightweight decoder, where we improve the query design with a more streamlined architecture, explore various choices and com-positions of keys/values and introduce positional embed-dings tailored for bounding box pairs.
In particular, we demonstrate that the positional embeddings function as spa-tial guidance in cross-attention, and shed light on this mech-anism with rich visualisations. 2.