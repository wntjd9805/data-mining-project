Abstract
Large vision-language models (VLMs), such as CLIP, learn rich joint image-text representations, facilitating ad-vances in numerous downstream tasks, including zero-shot classification and text-to-image generation. Nevertheless, existing VLMs exhibit a prominent well-documented limita-tion – they fail to encapsulate compositional concepts such as counting. We introduce a simple yet effective method to improve the quantitative understanding of VLMs, while maintaining their overall performance on common bench-marks. Specifically, we propose a new counting-contrastive loss used to finetune a pre-trained VLM in tandem with its original objective. Our counting loss is deployed over automatically-created counterfactual examples, each con-sisting of an image and a caption containing an incorrect object count. For example, an image depicting three dogs is paired with the caption “Six dogs playing in the yard” as a negative example. Our loss encourages discrimination between the correct caption and its counterfactual variant which serves as a hard negative example. To the best of our knowledge, this work is the first to extend CLIP’s ca-pabilities to object counting. Furthermore, we introduce
“CountBench” – a new image-text counting benchmark for evaluating object counting capabilities. We demonstrate a significant improvement over state-of-the-art baseline mod-els on this task. Finally, we leverage our counting-aware
CLIP model for image retrieval and text-conditioned image generation, demonstrating that our model can produce spe-cific counts of objects more reliably than existing ones. 1.

Introduction
Since the advent of CLIP [38], training large vision-language models (VLMs) has become a prominent paradigm for representation learning in computer vision.
By observing huge corpora of paired images and cap-tions crawled from the Web, these models learn powerful
and rich joint image-text embedding spaces, which have been employed in numerous visual tasks, including clas-sification [58, 59], segmentation [27, 53], motion gener-ation [47], image captioning [30, 48], text-to-image gen-eration [12, 29, 32, 41, 44] and image or video edit-ing [4, 6, 9, 17, 25, 35, 49, 18, 7]. Recently, VLMs have also been a key component in text-to-image generative mod-els [5, 39, 41, 43], which rely on their textual representa-tions to encapsulate the semantic meaning of the input text. such as
CLIP [38] and BASIC [36], are known to possess a weak understanding of the number of objects present in an im-age [3, 36, 38]. This is demonstrated in Fig. 1, where, when given a caption of the template “a photo of ănumberą
ăobjectsą”, CLIP often fails to retrieve images that cor-rectly match the described number. Downstream applica-tions that rely on VLM-based representations inherit these limitations, e.g., image generation models struggle to reli-ably produce specific counts of objects [43, 51, 52].
Despite their power, prominent VLMs,
In this work, we introduce a novel method that enhances the quantitative understanding of large-scale VLMs by en-couraging them to produce representations that are sensitive to the number of objects in the image and text.
We hypothesize that the reason existing VLMs fail to learn the concept of counting is severalfold: (i) Captions that accurately specify the number of objects become ex-tremely rare in the data as the number of objects increases.
For example, we found that for more than five objects, cap-tions typically contain a general form of quantity, e.g., “a group of...” or “many...”, rather than an accurate count. This can be attributed to the fact that people cannot instantly identify numerical quantities larger than four without ex-plicitly counting the objects [24]. (ii) Numbers in the cap-tion often refer to attributes that are NOT related to count-ing – e.g. age, time, address, temperature, model number (“This is an iPhone 5”), etc. (iii) The task of counting (as-sociating the visible number of objects in an image with the number in the caption), is not explicitly enforced in current
VLM training objectives. Therefore, current VLMs are able to count reasonably well only up to two or three (for which there are sufficient image-caption examples).
We thus suggest to mitigate each of these problems by: (i) Creating suitable training data in which the captions con-tain accurate numbers of objects. (ii) Designing a training objective whereby understanding object counts is critical for discriminating between the correctly associated caption and incorrect ones.
More specifically, as illustrated in Fig. 2, we automat-ically create a clean and diverse counting training set by curating image-text examples where the image depicts mul-tiple objects and its caption is verified to express their count.
We then finetune a pretrained VLM by formulating count-ing as a discriminative task – for each example, we create a counterfactual caption by swapping the spelled number associated with the object count with a different randomly selected number. The model’s objective is then to associate the image correctly with its true count caption, discriminat-ing it from the counterfactual one.
To evaluate counting capabilities, we introduce “Count-Bench” – a carefully curated object counting benchmark, consisting of 540 diverse, high quality image-text exam-ples. We evaluate our method on two prominent contrastive
VLMs: CLIP [38] and BASIC [36], and demonstrate a significant improvement in accuracy in the task of zero-shot count classification over baseline models. Importantly, we achieve this while maintaining the original knowledge learned by the VLM, as demonstrated by an extensive evalu-ation of our model on standard zero-shot downstream tasks.
The quantitative understanding of our model is further evi-dent by our text-to-image retrieval results (e.g., Fig. 1(a)), as well as by the relevancy maps of our model, which demon-strate that the model correctly attends to all visible objects whose count is specified in the text (e.g., Fig. 1(b)). Finally, we train a large-scale text-to-image generative model [43] which incorporates our counting training set and counting-aware CLIP text encoder. The generated images from this model exhibit higher fidelity to the number of objects spec-ified in the input prompts (Fig. 8).
To summarize, our main contributions are: 1. A novel training framework for tackling the task of vision-language counting – an important limitation of current VLMs. 2. A new benchmark, “CountBench”, carefully filtered and validated for evaluating VLMs on the counting task. 3. We apply our method to the widely-adopted VLMs
CLIP and BASIC, demonstrating significant improve-ment on the counting task, while maintaining general (non-counting) performance on common benchmarks. 4. We utilize our counting-aware VLMs for downstream tasks including image retrieval and text-to-image gener-ation, demonstrating more reliable results when the text prompt contains a specific number of objects. 2.