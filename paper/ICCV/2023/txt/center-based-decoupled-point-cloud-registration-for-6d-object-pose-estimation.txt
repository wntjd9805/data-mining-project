Abstract 1.

Introduction
In this paper, we propose a novel center-based decou-pled point cloud registration framework for robust 6D ob-ject pose estimation in real-world scenarios. Our method decouples the translation from the entire transformation by predicting the object center and estimating the rotation in a center-aware manner. This center offset-based translation estimation is correspondence-free, freeing us from the dif-ficulty of constructing correspondences in challenging sce-narios, thus improving robustness. To obtain reliable center predictions, we use a multi-view (bird’s eye view and front view) object shape description of the source-point features, with both views jointly voting for the object center. Addi-tionally, we propose an effective shape embedding module to augment the source features, largely completing the miss-ing shape information due to partial scanning, thus facili-tating the center prediction. With the center-aligned source and model point clouds, the rotation predictor utilizes fea-ture similarity to establish putative correspondences for
SVD-based rotation estimation.
In particular, we intro-duce a center-aware hybrid feature descriptor with a nor-mal correction technique to extract discriminative, part-aware features for high-quality correspondence construc-tion. Our experiments show that our method outperforms the state-of-the-art methods by a large margin on real-world datasets such as TUD-L, LINEMOD, and Occluded-LINEMOD. Code is available at https://github.com/Jiang-HB/CenterReg.
∗Corresponding authors
Haobo Jiang, Shuo Gu, Jin Xie, and Jian Yang are with PCA Lab,
Key Lab of Intelligent Perception and Systems for High-Dimensional In-formation of Ministry of Education, and Jiangsu Key Lab of Image and
Video Understanding for Social Security, School of Computer Science and
Engineering, Nanjing University of Science and Technology, China.
Accurate 6D object pose estimation (position and ori-entation in 3D space) is a crucial task in many real-world applications, such as robotics grasping [13, 59, 73], aug-mented reality [43, 44], and autonomous navigation [8, 22, 62]. While great progress has been made when exploiting
RGB or RGB-D data as input [34, 54, 51, 59, 49, 69, 58], the advances in 3D sensors and deep point-cloud learning archi-tectures have led to the development of increasingly accu-rate point cloud registration algorithms [61, 67, 32, 19, 14].
Nevertheless, the current state-of-the-art object-level 3D registration methods [56, 67, 19] prioritize achieving high performance on synthetic data, and yet still struggle with the challenges present in real-world data [25, 24, 6], such as full-range transformations, natural noise interference, and severe occlusions. A promising direction to alleviate this consists of decoupling the rotation and translation solutions, so as to reduce their interference. This was first investigated in [41, 42, 57] via the use of handcrafted rotation-invariant and translation-invariant feature descriptors. More recently, this idea was translated to the deep learning realm by aiming to learn representations that disentangle rotation and trans-lation [10].
In this paper, we introduce a drastically different ap-proach to decoupled registration for robust 6D object pose estimation in real-world scenarios. We advocate using the center offset between the source and model point clouds to decouple the translation from the entire transformation. In contrast to common correspondence-based translation esti-mation [61, 67, 14], our method is correspondence-free. It just requires regressing the independent center of the point cloud itself, freeing us from the difficulty of correspondence construction in challenging scenarios and thus significantly enhancing robustness. As the model point cloud can always be centered at the referential origin, our translation decou-pling can be further simplified to only predicting the posi-tion of the center in the source point cloud. We therefore
decouple 3D registration into a center prediction (for trans-lation estimation) and a center-aware rotation prediction.
Specifically, our method consists of a multi-view center predictor and a center-aware rotation predictor. The center predictor extracts a bird’s eye view (BEV) and a front view description of the object shape from the source point cloud features, which jointly vote for the object center. Notably, we augment the source-point features with rich object-shape information from the model point cloud via a model-shape embedding module. This lets us largely complement the missing shape information of the partially-scanned source point cloud, thus facilitating the center prediction. With the center-aligned source and model point clouds, the rotation predictor utilizes feature similarity to establish putative cor-respondences for SVD-based rotation estimation [61].
In this context, we develop a center-aware hybrid feature de-scriptor to inhibit wrong correspondences caused by mis-matched points with similar local structures. Specifically, this descriptor characterizes a part-aware representation of points, highlighting the intra-object location of each point (i.e., which part of the object the point belongs to), so as to distinguish locally-similar yet mismatched points. In ad-dition, we propose a center-aware normal-orientation cor-rection technique to account for normal consistency in the construction of the correspondences.
To summarize, our main contributions are as follows:
• We propose a novel center-based decoupled registra-tion framework for robust real-world 6D object pose estimation, which first identifies the object center to decouple translation and rotation prediction.
• We develop a robust multi-view center predictor, us-ing BEV and front-view source-feature projections to jointly vote for the object center. Notably, the source features are augmented with a proposed model-shape embedding module to complement the missing object-shape information due to partial scanning.
• We propose an effective center-aware rotation estima-tion method, designing a center-aware hybrid feature descriptor and a normal correction strategy to improve the robustness of extracted feature correspondences.
Our extensive experimental results on the real-world TUD-L [25], LINEMOD [24] and Occluded-LINEMOD [6] datasets evidence that our method outperforms the state of the art by a large margin. 2.