Abstract
Neural ﬁelds, which represent signals as a function pa-rameterized by a neural network, are a promising alterna-tive to traditional discrete vector or grid-based represen-tations. Compared to discrete representations, neural rep-resentations both scale well with increasing resolution, are continuous, and can be many-times differentiable. However, given a dataset of signals that we would like to represent, having to optimize a separate neural ﬁeld for each signal is inefﬁcient, and cannot capitalize on shared information or structures among signals. Existing generalization methods view this as a meta-learning problem and employ gradient-based meta-learning to learn an initialization which is then
ﬁne-tuned with test-time optimization, or learn hypernet-works to produce the weights of a neural ﬁeld. We instead propose a new paradigm that views the large-scale training of neural representations as a part of a partially-observed neural process framework, and leverage neural process algorithms to solve this task. We demonstrate that this approach outperforms both state-of-the-art gradient-based meta-learning approaches and hypernetwork approaches. 1.

Introduction
Neural ﬁelds, also known as neural implicit represen-tations or coordinate-based neural networks, have shown great promise as an alternative to traditional discrete repre-sentations. Neural ﬁelds consider signals and other objects as functions (ﬁelds) mapping coordinates to values, and ap-proximate these functions with neural networks. They have been used to represent a myriad of signals including 3D ob-jects/scenes [25, 34], CT scans [35], and 3D human mo-tion [36]. This approach has several advantages over dis-crete representations, including being continuous and hav-ing non-zero derivatives [32] (depending on the choice of neural network architecture), and scale much more efﬁ-ciently than traditional grid-based representations with in-creasing resolution [1, 4, 15, 23, 28, 29, 34].
Typically, the ﬁeld quantities are not themselves directly observable, but partial sensor observations are available.
The function relating the ﬁeld quantities to sensor obser-vations is called the forward map. The most common algo-rithm for training neural ﬁelds is thus follows the following paradigm [37]: ﬁrst, coordinates are sampled and passed through a neural network to predict the corresponding ﬁeld quantities. Then the forward map maps the ﬁeld quantities to the sensor domain, where we can calculate a reconstruc-tion loss between the neural network’s predictions and our ground-truth partial observations in the sensor domain. This reconstruction loss is then used to supervise the training of the neural ﬁeld (see Figure 1).
However, one of the major drawbacks of this neural ﬁeld training paradigm is that it is computationally expensive for a large dataset of signals, since a separate neural net-work must be trained from scratch for each signal, requir-ing a large amount of memory and computation [20], which we will henceforth refer to as the neural ﬁeld generaliza-tion problem. Previous approaches to this problem include conditioning, hypernetworks [3], and gradient-based meta-learning [20, 35]. The ﬁrst two approaches [11, 31, 32] seek to directly learn some or all of the parameters of a neural ﬁeld. The third approach views the problem as a meta-learning problem, where each individual signal is con-sidered a different task [20, 31, 35], and apply standard gradient-based meta-learning algorithms such as MAML
[9] or Reptile [26] to ﬁnd a parameter initialization that can be quickly ﬁne-tuned for any speciﬁc signal. Previ-ous results are mixed: [31] ﬁnds that gradient-based meta-learning can outperform concatenation and MLP hypernet-works, whereas [3] ﬁnds that hypernetworks with a vi-sion transformer [6] encoder can outperform gradient-based meta-learning [35] on tasks where the available partial ob-servations are 2D images.
However, gradient based meta-learning suffers from many problems, such as underﬁtting on large datasets and hyperparameter sensitivity. Recent research [12] ﬁnds that neural processes, another meta-learning framework, are more ﬂexible and efﬁcient for complex visual regression tasks, which we hypothesize will carry over to the neural
ﬁeld domain. Additionally, one of our key observations is that the encoder-decoder structure of neural processes is an extension of the hypernetwork approach, making a neu-ral process-style framework a natural choice for the neu-ral ﬁeld generalization problem. However, the usual neural process framework can not be applied directly to most neu-ral ﬁeld tasks, since in most neural ﬁeld tasks the training of the neural ﬁeld is supervised by partial sensor observa-tions, whereas neural processes generally assume that we have direct observations of the ﬁeld available. Therefore, we propose a simple partially-observed framework to adapt the neural process framework to the neural ﬁeld domain that incorporates the forward map relating ﬁeld observations to sensor observations.
In this work, we propose neural processes as an alterna-tive to the traditional gradient-based meta-learning and hy-pernetwork approaches for neural ﬁeld generalization. Our contributions are as follows: 1. We propose to use neural processes to tackle the neural
ﬁeld generalization problem. In particular, we adapt the traditional neural process framework to the com-mon neural ﬁeld setting where only partial observa-tions of the ﬁeld are available, and our framework is agnostic to the neural process architecture, allowing us to incorporate new advances in neural processes. 2. We show that neural processes can outperform both state-of-the-art gradient-based meta learning and state-of-the-art hypernetwork approaches on typical neural
ﬁeld generalization benchmarks, including 2D image regression and completion [3, 35], CT reconstruction from sparse views [35], and recovering 3D shapes from 2D image observations [3, 35]. 2.