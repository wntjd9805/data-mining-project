Abstract
Neural Radiance Fields (NeRFs) have achieved great success in the past few years. However, most current meth-ods still require intensive resources due to ray marching-based rendering. To construct urban-level radiance fields efficiently, we design Deformable Neural Mesh Primi-tive (DNMP), and propose to parameterize the entire scene with such primitives. The DNMP is a flexible and com-pact neural variant of classic mesh representation, which enjoys both the efficiency of rasterization-based render-ing and the powerful neural representation capability for photo-realistic image synthesis. Specifically, a DNMP con-* equal contribution. This work was done when Fan interned at Shang-hai AI Laboratory. Fan’s contributions: 1) Code implementation; 2) Novel database construction for latent space training and Loss Eq. (1); 3) Con-ducted most of the experiments. Yan’s contributions: 1) Proposed DNMP.
The structure design of DNMP and latent space construction for shape con-trol; 2) Proposed representing the entire scene with local primitives as well as the idea of hierarchical DNMP representation. 3) Drafted the paper and designed most of the experiments.
†corresponding authors. sists of a set of connected deformable mesh vertices with paired vertex features to parameterize the geometry and radiance information of a local area. To constrain the degree of freedom for optimization and lower the storage budgets, we enforce the shape of each primitive to be de-coded from a relatively low-dimensional latent space. The rendering colors are decoded from the vertex features (in-terpolated with rasterization) by a view-dependent MLP.
The DNMP provides a new paradigm for urban-level scene representation with appealing properties: (1) High-quality rendering. Our method achieves leading performance for novel view synthesis in urban scenarios. (2) Low com-putational costs. Our representation enables fast ren-dering (2.07 ms/1K pixels) and low peak memory usage (110 MB/1K pixels). We also present a lightweight version that can run 33× faster than vanilla NeRFs, which is com-parable to the highly-optimized Instant-NGP. Project page: https://dnmp.github.io/.
1.

Introduction
Synthesizing photo-realistic images of 3D scenes is a long-standing problem in computer vision, and has been the focus of research in the past decades. However, even with years of effort, the current paradigms still face great challenges especially in urban outdoor scenarios, due to the increased representation complexity and demanding com-putational resources.
To achieve high-quality image rendering, the com-puter graphics community has explored various techniques for scene representation, including point clouds [23], meshes [20, 22], voxels [46], implicit functions [6, 21], etc.
The mesh-based representation is widely used in modern rendering pipelines due to its compact and efficient nature.
However, constructing water-tight mesh models of urban scenes for modern graphic engines is still difficult. Besides, the textures and illuminations are difficult to be realistically recovered with the classic techniques.
The recent neural rendering methods circumvent the mesh construction step and represent the scene with im-plicit neural functions. NeRF [31] and its advanced vari-ants [29, 39, 54, 59, 4] proposed to store the density and radiance information of a volume inside multi-layer percep-trons (MLPs), and adopt volumetric rendering for view syn-thesis. Recently, some researchers [50] also made efforts to extend the NeRF models to large-scale scenes by inde-pendently representing a city block-by-block and merging the representations together thereafter. Although remark-able progress has been made, their rendering process is still computationally intensive as the implicit functions need to be evaluated thousands of times to densely sample the space during volumetric rendering. Most of the computational re-sources are wasted on the samples in empty spaces, and the situation will further escalate in outdoor scenes where empty space dominates.
More recently, researchers have identified this issue and propose to combine neural rendering with explicit point-cloud reconstruction to improve the efficiency [55, 36]. In this way, the empty spaces can be skipped by taking the explicit reconstruction as a reference, which significantly saves computational resources. These methods associate point-wise learnable high-dimensional features to the re-constructed point clouds for spatial radiance encoding. Dur-ing rendering, only the points around the intersections of the view ray with the point clouds will be sampled for feature aggregation. Based on such an aggregation mechanism, a dense and perfect reconstruction is vital for photo-realistic rendering. However, the reconstructed points are usually not uniformly distributed from the current reconstruction algorithms [18] and missing regions may be also ubiqui-tous. The noisy reconstruction will increase the learning difficulty of the implicit function and degrade the final ren-dering quality.
In this work, we propose an efficient radiance field rep-resentation for large-scale environments by combining effi-cient mesh-based rendering and powerful neural represen-tations. Specifically, we develop Deformable Neural Mesh
Primitive (DNMP) and propose to represent the entire ra-diance field in a bottom-up manner with such primitives, where each DNMP parameterizes the geometry and radi-ance of a local area. A DNMP consists of a set of con-nected deformable mesh vertices and each vertex is paired with a feature vector for radiance modeling. To constrain the degree of freedom for shape optimization and decrease the storage budgets, we enforce the mesh vertices of each
DNMP to be decoded from a relatively low-dimensional la-tent code. The latent code will be optimized to deform the primitive shapes for 3D structure modeling during training.
Different from previous methods [55, 36] that rely on in-efficient k-Nearest Neighbors (k-NN) algorithm to gather related features for rendering, we can directly leverage the rasterization pipeline for feature interpolation, which is more efficient. Based on rasterization, for each view ray, a set of features is collected by interpolations from the tri-angulated vertex features. These interpolated features are thereafter input to an implicit function (implemented with
MLPs) to get the corresponding radiance and opacity val-ues for volumetric rendering.
To represent the entire scene, we first coarsely voxelize the scene according to the 3D reconstruction results (from
Multi-View Stereo (MVS) [45] or hardware sensors), and then parameterize the geometry and radiance of each voxel with a DNMP. Considering the practical 3D reconstruction results may be noisy and full of missing regions, we further propose to voxelize the scene with hierarchical resolutions and separately represent the radiance fields accordingly with hierarchically-sized DNMPs. The rendering results from different hierarchy levels will be blended. Based on our DNMP-based hierarchical representation, we achieve more robustness against noisy 3D reconstructions compared with the previous point-cloud-based methods [55, 36].
We evaluate our method on two urban datasets, i.e.,
KITTI-360 [26] and Waymo Open Dataset [49]. Our method enables photo-realistic rendering and achieves lead-ing performance for novel view synthesis. We achieve a much faster speed and produce fewer peak memory foot-prints compared with vanilla NeRFs. We also present a lightweight version to further accelerate the rendering.
This lightweight version can run at an interactive rate only with limited sacrifices on rendering quality, the speed of which is even comparable with the highly-optimized
Instant-NGP’s [32]. Moreover, our method can be easily embedded into modern graphic rendering pipelines and nat-urally supports scene editing, which provides the potential for possible applications such as VR/AR.
2.