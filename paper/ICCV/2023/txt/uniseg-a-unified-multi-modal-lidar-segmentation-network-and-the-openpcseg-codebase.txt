Abstract
Point-, voxel-, and range-views are three representative forms of point clouds. All of them have accurate 3D measure-ments but lack color and texture information. RGB images are a natural complement to these point cloud views and fully utilizing the comprehensive information of them ben-efits more robust perceptions. In this paper, we present a unified multi-modal LiDAR segmentation network, termed
UniSeg, which leverages the information of RGB images and three views of the point cloud, and accomplishes seman-tic segmentation and panoptic segmentation simultaneously.
Specifically, we first design the Learnable cross-Modal
Association (LMA) module to automatically fuse voxel-view and range-view features with image features, which fully utilize the rich semantic information of images and are ro-bust to calibration errors. Then, the enhanced voxel-view and range-view features are transformed to the point space, where three views of point cloud features are further fused adaptively by the Learnable cross-View Association mod-ule (LVA). Notably, UniSeg achieves promising results in three public benchmarks, i.e., SemanticKITTI, nuScenes, and Waymo Open Dataset (WOD); it ranks 1st on two chal-lenges of two benchmarks, including the LiDAR semantic segmentation challenge of nuScenes and panoptic segmen-tation challenges of SemanticKITTI. Besides, we construct the OpenPCSeg codebase, which is the largest and most comprehensive outdoor LiDAR segmentation codebase. It contains most of the popular outdoor LiDAR segmentation algorithms and provides reproducible implementations. The
OpenPCSeg codebase will be made publicly available at https://github.com/PJLab-ADG/PCSeg. 1.

Introduction
LiDAR-based semantic segmentation, whose objective is to assign a semantic label to each input point, acts as an
*Work performed during an internship at Shanghai AI Laboratory.
†Corresponding authors. essential component in autonomous driving, digital cities, and service robots [16, 18, 21, 37]. With the advent of deep learning, an enormous amount of methods [38, 62, 32, 31, 20, 48, 9, 52, 6, 5, 23, 22] have been proposed and quickly dominate various benchmarks, such as SemanticKITTI [1] and nuScenes [3, 15].
Point cloud and RGB images are two frequently used modalities. As depicted in Fig. 1 (a), different modalities have their own merits and drawbacks. Point cloud provides reliable and accurate depth information, and can be pro-cessed in different views, e.g., point-view, voxel-view, and range-view. Specifically, point-view representation main-tains the complete point information but is inefficient in capturing the neighboring point features due to the unstruc-tured point locations. Voxel-view methods rasterize the point cloud into voxel cells that retain regular structure but suffer from severe voxelization loss especially when the voxel size is large. Range-view representations are dense and compact, which can be efficiently processed by highly optimized 2D convolution. However, the spherical projection inevitably destroys the original 3D geometric information. As for the
RGB image, it embraces rich color and texture information, but can not provide precise spatial information.
Apparently, the input data from multi-modality and mul-tiple views of the point cloud are supplementary to each other. Therefore, fully utilizing the comprehensive infor-mation benefits a more robust perception. However, such a cross-modal and cross-view fusion paradigm is not fully explored in LiDAR segmentation [14, 28, 48, 52]. Current multi-modal fusion methods are concentrated on the fusion of RGB and range images [14, 28, 24]. Other representa-tions such as voxel- and point-views of the LiDAR point cloud, which maintain original data structure and provide fine-grained spatial information, are ignored in prior meth-ods. Besides, they typically fuse the image and point cloud in a hard association manner through calibration matrices, thus being vulnerable to calibration errors [25].
In this paper, to address the aforementioned problems, we make the first attempt to dynamically fuse four differ-Figure 1: (a) Merits of different modalities. RGB images provide rich color, texture, and semantic information while point cloud embraces precise 3D positions of various objects. The pedestrian highlighted by the red rectangle is hard to find in the image but is visible in the point cloud. The combination of multi-modality and multi-views benefit a more robust and comprehensive perception. (b) Comparison of UniSeg with various competitive LiDAR segmentation algorithms on three challenges of SemanticKITTI and nuScenes benchmarks. The red pentagram, blue triangles, yellow circles, and green squares denote UniSeg, multi-view methods, uni-modal methods, and multi-modal ones, respectively. The selected baselines include state-of-the-art algorithms such as 2DPASS [52], RPVNet [48], Panoptic-PHNet [30], and LidarMultiNet [53]. ent modalities of data (voxel-, range-, and point-views of the point cloud and RGB images) for more robust and ac-curate perception. More formally, we propose a Learnable cross-Modal Association (LMA) and a Learnable cross-View
Association module (LVA) to effectively fuse the different modalities inputs. Specifically, we first fuse the image fea-tures with range- and voxel-view point features through the
LMA in a soft association schema with the deformable cross-attention [59] operation and alleviate calibration errors. Next, the image-enhanced range- and voxel-view features are trans-ferred into the point-view feature, and all three views of point cloud features are fused adaptively by the LVA module.
Equipped with LMA and LVA, we design a unified net-work, dubbed UniSeg, for various semantic scene under-standing tasks, i.e., semantic, and panoptic segmentation.
Extensive experimental results verify the generalizability of UniSeg across different tasks. As shown in Fig. 1 (b),
UniSeg ranks 1st in two open challenges.
It achieves 75.2 mIoU (semantic segmentation) and 67.2 PQ (panoptic segmentation) in SemanticKITTI; and 83.5 mIoU (seman-tic segmentation) and 78.4 PQ (panoptic segmentation) in nuScenes. The appealing performance strongly demonstrates the efficacy of our multi-modal fusion framework.
Besides, considering that many popular outdoor LiDAR segmentation methods [9, 48, 20, 30] either do not provide official implementations or the performance is difficult to reproduce, we construct the OpenPCSeg codebase which aims to provide reproducible and uniform implementations.
We have benchmarked 14 competitive LiDAR segmenta-tion algorithms and the reproduced performance of these algorithms all surpasses the reported value.
The contributions of our work are summarized as follows.
• We propose a unified multi-modal fusion network for
LiDAR segmentation, leveraging the information of
RGB images and three views of the point cloud for more accurate and robust perception.
• Our approach ranks 1st on two challenges of Se-manticKITTI and nuScenes, strongly demonstrating the efficacy of the proposed multi-modal network.
• The largest and most comprehensive outdoor LiDAR
segmentation codebase dubbed OpenPCSeg will be re-leased to facilitate related research. 2.