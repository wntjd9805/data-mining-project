Abstract
An important challenge in vision-based action recogni-tion is the embedding of spatiotemporal features with two or more heterogeneous modalities into a single feature. In this study, we propose a new 3D deformable transformer for action recognition with adaptive spatiotemporal recep-tive fields and a cross-modal learning scheme. The 3D de-formable transformer consists of three attention modules: 3D deformability, local joint stride, and temporal stride at-tention. The two cross-modal tokens are input into the 3D deformable attention module to create a cross-attention to-ken with a reflected spatiotemporal correlation. Local joint stride attention is applied to spatially combine attention and pose tokens. Temporal stride attention temporally reduces the number of input tokens in the attention module and sup-ports temporal expression learning without the simultane-ous use of all tokens. The deformable transformer iterates
L-times and combines the last cross-modal token for clas-sification. The proposed 3D deformable transformer was tested on the NTU60, NTU120, FineGYM, and PennAction datasets, and showed results better than or similar to pre-trained state-of-the-art methods even without a pre-training process.
In addition, by visualizing important joints and correlations during action recognition through spatial joint and temporal stride attention, the possibility of achieving an explainable potential for action recognition is presented. 1.

Introduction
Spatiotemporal feature learning is a crucial part of ac-tion recognition, which aims to fuse not only the spatial features of each frame but also the temporal correlation be-tween input sequences. Previous studies on action recog-nition [19, 6, 5, 42, 9, 48] investigated the application of 3D convolutional kernels with an additional temporal space beyond the 2D spatial feature space. Since then, 3D con-volutional neural networks (CNN) have achieved a promis-ing performance and have eventually become the de facto
*Corresponding author
Figure 1: Comparison between (a) Full attention and (b) the proposed 3D deformable attention. Full attention considers all tokens against a specific query in a complete sequence. By contrast, 3D deformable attention considers only intense tokens with adaptive receptive fields. standard for various action recognition tasks using sequen-tial data. Vision transformers (ViTs) for action recognition, which have peaked in popularity, have recently been used to explore a 3D token embedding to fuse the temporal space within a single token. However, ViTs-based action recogni-tion methods [1, 34] are limited in that they can only con-duct spatiotemporal feature learning within restricted recep-tive fields.
To avoid this problem, several studies [15, 57, 47] have been conducted to allow more flexible receptive fields for deep learning models. Deformable CNN leverages dynamic kernels to capture the intense object regions. First, they determine the deformable coordinates using embedded fea-tures. The kernel is then applied to the features extracted from the deformable coordinates. Deformable ViTs [47, 57] encourage the use of an existing attention module to learn deformable features. The query tokens are projected onto the coordinates to obtain deformable regions from the key and value tokens. The deformed value tokens are then ap-plied to the attention map, which is generated through a scaled dot product of the input query and deformed key to-kens. These methods suggest a new approach that can over-come the limitations of existing standardized feature learn-ing. However, despite some impressive results, these stud-ies are still limited in that they are only compatible with the spatial dimensions. Therefore, as a primary challenge, there is a need for the development of novel and deformable
ViTs that can learn spatiotemporal features from image se-quences.
Another challenge is the efficient application of multi-modal input features to an action recognition model. Ac-tion recognition is classified into three categories based on the feature type. The first is a video-based approach
[56, 4, 46, 29, 20, 43, 33], which has traditionally been used for action recognition. This approach is limited by a degraded performance caused by noise, such as varying object sizes, occlusions, or different camera angles. The second is a skeleton-based approach [51, 25, 12, 13, 11], which mainly converts poses into graphs for recognizing ac-tions through a graph neural network (GNN). Although this approach is robust against noise, its performance is highly dependent on the pose extraction method. To overcome the shortcomings of the previous two approaches, the third method aims to simultaneously fuse heterogeneous domain features using multimodal or cross-modal learning. With this approach, video and skeleton features are jointly trained simultaneously. However, because most related studies use a separate model composed of a GNN + CNN or CNN +
CNN for each modality, there is a limit in constructing an effective single model.
To alleviate the drawbacks stated above, we propose the use of transformer with 3D deformable attention for dy-namically utilizing the spatiotemporal features for action recognition. In this way, the proposed model applies flexi-ble cross-modal learning, which handles the skeletons and video frames in a single transformer model. The skeletons are projected onto sequential joint tokens, and each joint to-ken contains an activation at every joint coordinate. To pro-vide effective cross-modal learning between each modal-ity, the proposed method adopts a cross-modal token that takes the role of mutually exchanging contextual informa-tion. Therefore, the proposed model is capable of achieving a boosted performance without an auxiliary submodel for the cross-modalities. Figure 1 shows a comparison between the previous full attention and the proposed 3D deformable
In the case of the full attention shown in Fig. attention. 1 (a), all tokens within a spatiotemporal space are covered against a specific query token. By contrast, our proposed 3D deformable attention scheme, shown in Fig. 1 (b), con-siders only tokens with high relevance within the entire spa-tiotemporal space. The main contributions of this study are as follows:
• We propose the first 3D deformable attention that adaptively considers the spatiotemporal correlation within a transformer as shown in Fig. 1 (b), break-ing away from previous studies that consider all tokens against a specific query in a complete sequence.
• We propose a cross-modal learning scheme based on complementary cross-modal tokens. Each cross-modal token delivers contextual information between the dif-ferent modalities. This approach can support a sim-ple yet effective cross-modal learning within a single-transformer model structure.
• We present qualitative evidence for 3D deformable at-tention with visual explanations and prove that the proposed model outperforms several previous state-of-the-art (SoTA) methods. 2.