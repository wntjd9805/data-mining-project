Abstract
Previous research has studied the task of segmenting cine-matic videos into scenes and into narrative acts. However, these studies have overlooked the essential task of multi-modal alignment and fusion for effectively and efficiently processing long-form videos (> 60min). In this paper, we in-troduce Multimodal alignmEnt aGgregation and distillAtion (MEGA) for cinematic long-video segmentation. MEGA tack-les the challenge by leveraging multiple media modalities.
The method coarsely aligns inputs of variable lengths and different modalities with alignment positional encoding. To maintain temporal synchronization while reducing compu-tation, we further introduce an enhanced bottleneck fusion layer which uses temporal alignment. Additionally, MEGA employs a novel contrastive loss to synchronize and transfer labels across modalities, enabling act segmentation from labeled synopsis sentences on video shots. Our experimental results show that MEGA outperforms state-of-the-art meth-ods on MovieNet dataset for scene segmentation (with an
Average Precision improvement of +1.19%) and on TRI-POD dataset for act segmentation (with a Total Agreement improvement of +5.51%). 1.

Introduction
In the world of video production, movies are composed of smaller units called shots, scenes, and acts. Shots are a continuous set of frames, a scene is a sequence of shots that tell a story, and an act is a thematic section of a narra-tive [14]. While computer vision has made significant strides in shot detection [39], scene and act segmentation remain a challenge, despite their potential for smart video navigation, advertisement insertion, and movie summarization. Cine-matic content comprises of different data sources, including audio, visual, and text data, as well as derivative data sources from the narrative, including location, appearance, tone, or acoustic events. In this work, we will refer to all of these
Figure 1: MEGA works well on both scene segmentation and act segmentation tasks, outperforming previous work with significant margin. V,T*,T,A denotes video, screenplay, subtitle and audio respectively. input components as “modalities” of cinematic content. Pre-vious work has not fully explored how to align and aggregate these modalities which have different granularities.
We propose to address scene and act segmentation tasks with an unified multimodal Transformer. However, this approach presents two main challenges. Firstly, there is the issue of cross modality information synchronization and fusion at the shot level. Previous studies which use multimodal fusion for scene and act segmentation perform early [8, 32] or late fusion of features [35], and have not explored fusion strategies which utilize multimodal tempo-ral alignment. Additionally, the fusion strategies that utilize temporal alignment such as merged attention or cross modal-ity attention [9, 21] are computationally expensive and not generalizable to a large number of modalities. Secondly, due to the challenges associated with labeling a long video on act segmentation, the labels for act segmentation are provided on synopsis sentences [30] which do not provide timestamps.
To avoid the more challenging task of cross-modal synchro-nization, previous studies on act segmentation [30, 32] rely
on textual screenplay to transfer the labels from synopsis to movie, ignoring the rich multimodal information from the video, and introducing an additional dependency on screen-play data which is not always available.
To address these challenges, we introduce Multimodal alignmEnt aGgregation and distillAtion (MEGA). MEGA includes a novel module called alignment positional encod-ing which aligns inputs of variable lengths and different modalities at a coarse temporal level. To fuse the aligned embeddings of different modalities in an efficient and effec-tive manner, we adopt the bottleneck fusion tokens [28] and append a set of fusion tokens to each modality. These tokens share the same sequence length as the normalized positional encoding for different modalities, allowing us to inject them with the coarse temporal information, enabling information fusion in a better aligned embedding space. To address the issue of cross-domain knowledge transfer, we introduce a cross-modal synchronization approach. This method allows us to transfer the manually labeled act boundaries from syn-opsis level to movie level using rich multimodal information, enabling us to train MEGA directly on videos without re-lying on screenplay – which was a hard requirement for previous works [30, 32].
We test our proposed alignment and aggregation modules on the Movienet-318 [18] and the TRIPOD datasets [30], and we test our cross modality synchronization module on
TRIPOD alone, as the labels are provided on a different modality during training. Our proposed MEGA outperforms previous SoTA on scene segmentation on the Movienet-318 dataset (by +1.19% in AP) and on act segmentation on the
TRIPOD dataset (by +5.51% in TA). Our contributions are: 1. Alignment positional encoding module and a fusion bottleneck layer that performs multimodal fusion with aligned multi-modality inputs. 2. A cross-domain knowledge transfer module that syn-chronizes features across-domain, and enables knowl-edge distillation without requiring extra information. 3. SoTA performance on scene and act segmentation tasks, with detailed ablations, which can be used as reference for future work. 2.