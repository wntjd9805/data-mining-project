Abstract
Recent leading zero-shot video object segmentation (ZVOS) works devote to integrating appearance and mo-tion information by elaborately designing feature fusion modules and identically applying them in multiple fea-ture stages. Our preliminary experiments show that with the strong long-range dependency modeling capacity of
Transformer, simply concatenating the two modality fea-tures and feeding them to vanilla Transformers for fea-ture fusion can distinctly benefit the performance but at a cost of heavy computation. Through further empirical analysis, we find that attention dependencies learned in
Transformer in different stages exhibit completely dif-ferent properties: global query-independent dependency in the low-level stages and semantic-specific dependency in the high-level stages. Motivated by the observations, we propose two Transformer variants: i) Context-Sharing
Transformer (CST) that learns the global-shared contextual information within image frames with a lightweight com-ii) Semantic Gathering-Scattering Transformer putation. (SGST) that models the semantic correlation separately for the foreground and background and reduces the computa-tion cost with a soft token merging mechanism. We ap-ply CST and SGST for low-level and high-level feature fu-sions, respectively, formulating a level-isomerous Trans-* Corresponding author: wyfan@dlut.edu.cn. former framework for ZVOS task. Compared with the base-line that uses vanilla Transformers for multi-stage fusion, ours significantly increase the speed by 13× and achieves new state-of-the-art ZVOS performance. Code is available at https://github.com/DLUT-yyc/Isomer. 1.

Introduction
Zero-shot Video Object Segmentation (ZVOS) aims at discovering the most visually attractive objects in a video sequence and serves as a fundamental computer vision tech-nique. Different from image segmentation that mainly relies on static appearance features, ZVOS further explores tem-poral motion information to achieve reliable and temporally consistent results. One popular pipeline [39, 17, 60, 43] is integrating appearance and motion information by identi-cally applying feature fusion modules in multiple stages as shown in Fig. 1 (a). While great efforts have been made, de-signing effective multi-stage appearance-motion fusion ap-proaches for ZVOS is still an open problem.
Transformers [49] have made remarkable breakthroughs in many computer vision tasks [6, 3, 56, 28] due to its strong capability in modeling long-range dependencies and unique flexibility for cross-modal feature fusion. Nevertheless, their merits have not been fully explored in the ZVOS field.
A straightforward way is adopting Transformer blocks as the appearance-motion fusion modules. In our preliminary
tic Gathering-Scattering Transformer (SGST), which com-putes foreground and background attentions separately for the corresponding query tokens with some selected repre-sentative key/value tokens. In addition, a soft token merg-ing mechanism is adopted to enable the token selection pro-cess differentiable. Compared to the vanilla Transformer, the proposed SGST is able to model the semantic-specific dependency more explicitly and more efficiently.
Upon the above findings and the two proposed Trans-former blocks, a level-Isomerous Transformer (Isomer)
ZVOS framework is formulated by applying CST and
SGST blocks for early and late fusion stages, respectively.
Compared with the baseline network that applies vanilla
Transformer uniformly for all the feature fusion stages, ours treats the different fusion levels distinctively based on the observed Transformers properties, and achieves bet-ter segmentation results with 13× inference speed. Com-pared with the existing ZVOS works, our method equipped with Swin-Tiny[28] backbone (a comparable model size to
ResNet50[14]) obtains significantly superior performance with real-time inference (see Fig. 2).
The main contributions of this work are as follows: 1) We analyze the properties of vanilla Transformers in terms of attention dependencies hierarchically learned from the ZVOS task, and propose two Transformer vari-ants, i.e. Context-Sharing Transformer (CST) and Seman-tic Gathering-Scattering Transformer (SGST), to model the contextual dependencies from different levels effectively and efficiently. 2) We propose a level-isomerous Transformer paradigm for the ZVOS task, which applies the developed CST and
SGST for low-level early fusion and high-level late fu-sion, respectively. Different from the prior works that fuse appearance-motion information for all stages in an identical way, ours performs different fusion levels differentially and better fits the properties of the ZVOS network. 3) Extensive experiments demonstrate the superiority of our method compared to the existing works as well as the strong vanilla Transformer-based baseline. To our best knowledge, this is the first successful attempt at developing a real-time Transformer-based work in the ZVOS field. 2.