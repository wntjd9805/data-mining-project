Abstract
The decentralized and privacy-preserving nature of fed-erated learning (FL) makes it vulnerable to backdoor at-tacks aiming to manipulate the behavior of the resulting model on specific adversary-chosen inputs. However, most existing defenses based on statistical differences take effect only against specific attacks, especially when the malicious gradients are similar to benign ones or the data are highly non-independent and identically distributed (non-IID). In this paper, we revisit the distance-based defense methods and discover that i) Euclidean distance becomes meaning-less in high dimensions and ii) malicious gradients with di-verse characteristics cannot be identified by a single met-ric. To this end, we present a simple yet effective defense strategy with multi-metrics and dynamic weighting to iden-tify backdoors adaptively. Furthermore, our novel defense has no reliance on predefined assumptions over attack set-tings or data distributions and little impact on benign per-formance. To evaluate the effectiveness of our approach, we conduct comprehensive experiments on different datasets under various attack settings, where our method achieves the best defensive performance. For instance, we achieve the lowest backdoor accuracy of 3.06% under the most dif-ficult Edge-case PGD, showing significant superiority over previous defenses. The experiments also demonstrate that our method can be well-adapted to a wide range of non-IID degrees without sacrificing the benign performance. 1.

Introduction
Federated learning (FL) [26, 35] is a distributed machine learning paradigm that enables multiple participants to train a quality model collaboratively without exchanging their lo-cal data. During each round of training, the central server
*Corresponding Author distributes the global model to a subset of clients. Each client updates the model with the local data and submits the parameters to the server for aggregation. By training an efficient and quality model in a decentralized manner with-out sacrificing the privacy of participants, FL alleviates the possible conflict between technological developments and regulations for data privacy protection(e.g., General Data
Protection Regulation). FL has already been applied and achieved success in multiple fields, such as image process-ing [32], word prediction [36], medical imaging [30], and edge computing [56, 51].
However, FL is vulnerable to backdoors manipulat-ing the model towards the targeted behavior on specific adversary-chosen inputs [24, 4, 6, 47, 50, 11, 33, 39, 19, 31].
The backdoor attack is more difficult to be detected com-pared with the untargeted poisoning attacks [6, 7, 33, 11, 19] since it does not affect the regular function of the model and its gradient is more similar to benign ones [47, 38]. Multi-ple defenses have been proposed to improve the robustness of FL, such as the scoring-based methods [17, 8, 53, 41, 10, 55, 16, 29, 2, 20], which leverage a particular metric to dis-tinguish malicious gradients from the benign ones. Despite its effectiveness against some backdoors, researchers dis-covered that well-designed attacks (termed stealthy back-doors) whose gradients are indistinguishable from benign ones (through scaling [4, 47] or trigger split [50]) could eas-ily bypass these defenses. Differential privacy (DP) -based method [46, 40, 38, 49] builds upon the observation that the
DP method [14], traditionally used against DP attacks, is also effective against backdoors. By adding Gaussian noise to the global model, these methods can dilute the impact of potentially poisoned model updates. Surprisingly, DP-based methods show great ability in resisting stealthy back-doors (e.g., Edge-case PGD [47]). Despite its ability to re-sist the stealthy backdoors, the noises added by the DP sig-nificantly decrease the overall performance and the conver-gence speed. In comparison, the distance-based method less
impacts the global model by aggregating only the benign gradients. Consequently, a natural question arises: can we defend the stealthy backdoors without sacrificing the per-formance of the FL model? To accomplish this, we turn to the distance-based methods that don’t sacrifice the benign performance and promote the research question: how can we successfully leverage distance metrics to discriminate hostile updates from benign ones?
To this end, we revisit the distance-based defense and discover two limitations: 1. Euclidean distance (i.e., L2 distance) suffers from the curse of dimensionality. Param-eters of Neural Networks (NNs) can be viewed as high-dimensional vectors, and Euclidean distance generally fails to discriminate between malicious and benign gradients in high-dimensional space. 2. Single metric takes effect only against particular attacks with detailed assumptions regard-ing the malicious gradients. For instance, cosine distance detects malicious gradients with sizeable angular deviations while euclidean distance detects malicious gradients with a large L2 norm scaled by the attacker to impact the global models. Moreover, backdoor attacks are conducted with different data and scenarios, resulting in malicious gradi-ents with various characteristics that a single metric cannot handle (i.e., both gradients with a large norm and gradients with sizeable angular deviations exist in one round of aggre-gation). What’s worse is that the defender has no knowledge regarding the attacker and the underlying data distributions due to privacy requirements by FL, which makes detecting with a single metric even more difficult.
To address the above two problems, we first introduce the Manhattan distance, which we theoretically prove more meaningful in high dimensional space than Euclidean dis-tance. Empirically, with the Manhattan distance, our pro-posed distance-based defense shows remarkable perfor-mance against the stealthy backdoors. To cope with gra-dients with various properties, we leverage multiple met-rics cooperatively to identify the malicious gradients. We demonstrate in Section 5.4 and Figure 7 that malicious gra-dients of some characteristics are better identified by spe-cific metrics, which justify our motivation. To handle the different attacks and environments, we further propose to apply a whitening transformation and generate dynamic weights to handle the non-IID distribution of participants and different scales brought by the different distances. Fi-nally, we compute the score for each submitted gradient and aggregate only the benign ones based on the scoring. Exten-sive experiments illustrate that our defense maintains a high model performance and robustness simultaneously, which has never been achieved before. At a high level, we sum-marize the main contributions of this paper as follows:
• We present a novel defense with multi-metrics to adap-tively identify backdoors, which is applicable in a generic adversary model without predefined assump-tions over the attack strategy or data distribution.
• We show that by introducing the Manhattan distance, our defense alleviates the “meaningfulness” problem of Euclidean distance in high dimensions. By utilizing multiple metrics with dynamic weighting, our defense can resist backdoor attacks under a wide range of at-tack settings and data distributions.
• We empirically evaluate our method on various datasets and different attack settings. By showing that our defense maintains both high robustness and benign performance under the stealthy backdoors that previ-ously have not been successfully defended, we demon-strate the effectiveness of our approach. 2.