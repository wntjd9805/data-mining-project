Abstract
This paper proposes a novel approach for rendering a pre-trained Neural Radiance Field (NeRF) in real-time on resource-constrained devices. We introduce Re-ReND, a method enabling Real-time Rendering of NeRFs across
Devices. Re-ReND is designed to achieve real-time perfor-mance by converting the NeRF into a representation that can be efficiently processed by standard graphics pipelines.
The proposed method distills the NeRF by extracting the learned density into a mesh, while the learned color infor-mation is factorized into a set of matrices that represent the scene’s light field. Factorization implies the field is queried via inexpensive MLP-free matrix multiplications, while us-ing a light field allows rendering a pixel by querying the field a single time—as opposed to hundreds of queries when employing a radiance field. Since the proposed representa-tion can be implemented using a fragment shader, it can be directly integrated with standard rasterization frame-works. Our flexible implementation can render a NeRF in real-time with low memory requirements and on a wide range of resource-constrained devices, including mobiles and AR/VR headsets. Notably, we find that Re-ReND can achieve over a 2.6-fold increase in rendering speed versus the state-of-the-art without perceptible losses in quality. 1.

Introduction
Neural Radiance Fields (NeRFs) [20] have revolution-ized the field of novel view synthesis, as demonstrated by their impressive capacity to reconstruct complex objects and scenes with remarkable detail [2, 31]. The impres-sive performance of NeRFs casts them as a decisive tool for capturing and representing 3D objects and scenes. As a result, NeRFs hold great promise for countless practi-cal applications, including video games, movies, and Aug-mented/Virtual Reality (AR/VR).
However, the impressive performance of NeRFs comes with significant computational costs when rendering novel views. This slow rendering is mainly due to two limit-Rendering NeRFs in real-time on resource-Figure 1. constrained devices, such as AR/VR headsets and mobiles. We present Re-ReND, a method for rendering a pre-trained NeRF in real-time on a variety of devices with constrained computational resources. Re-ReND preserves remarkable photo-metric quality even when rendering at over 1,013 FPS on a desktop browser, or at the capped 74 FPS of a VR headset. Please, refer to the accom-panying video for a demonstration of Re-ReND’s capacities. ing properties of NeRFs, which dramatically increase their computational requirements. Firstly, they use a volumet-ric representation to model scenes [9], implying that ren-dering a single pixel requires hundreds of queries in space.
Secondly, they leverage a Multilayer Perceptron (MLP) to model radiance and density across space, meaning that each of those spatial queries involves evaluating an expensive
MLP. These properties make it challenging to render NeRFs in real-time on resource-constrained devices, which hinders their practical deployment.
A plethora of research efforts target the efficiency of
NeRFs. A line of work focused on shortening training times [27, 8, 21], while another looked at accelerating rendering times [22, 26]. Notably, some methods pre-tabulate a NeRF’s output on a sparse grid, and achieve real-time rendering by leveraging powerful GPUs [36, 13]. However, these methods are still incompatible with widely-available graphics pipelines that enable rendering on resource-constrained devices via popular frameworks such as WebGL and OpenGL ES. This incompatibility stems
from the rendering approach inherited from NeRFs, as vol-ume rendering demands ray marching, which is dramati-cally more expensive than mesh rasterization.
In this paper, we present Re-ReND, a method that en-ables Real-time Rendering of NeRFs across Devices, in-cluding resource-constrained devices such as VR headsets and mobile phones. To attain this end goal, we define (i) enabling compatibility and achieve three objectives: with widely-available graphics pipelines, (ii) obtaining ray color with a single query, and (iii) avoiding MLP evalua-tions for such queries. Given a pre-trained NeRF as input,
Re-ReND renders it in real-time by transforming the knowl-edge learned by the NeRF into an alternative representation.
In particular, Re-ReND distills the NeRF by extracting the learned density into a mesh, and the learned color infor-mation into a set of matrices that efficiently factorize the scene’s light field. Re-ReND is thus capable of rendering a
NeRF in real-time, making it deployable on a wide range of devices. When rendering challenging unbounded real scenes, our method achieves over 2.6-fold speed improve-ments above the state-of-the-art, while maintaining compa-rable quality. Furthermore, given that our rendering entirely disposes of MLPs, we can easily deploy it on other con-strained devices, like VR headsets, where other methods cannot adapt. Please refer to Figure 1 for an overview of the capabilities of Re-ReND.
In summary, our contributions are threefold: (i) We introduce Re-ReND, a method enabling real-time render-ing of a pre-trained NeRF on resource-constrained devices.
Re-ReND works on a wide variety of NeRFs, achieves remarkable rendering speeds at negligible costs to photo-metric quality, and is compatible with popular graphics pipelines available in common devices. (ii) Re-ReND en-ables fast rendering of a NeRF by transforming it into a rep-resentation with three fundamental qualities: it resembles a graphics-friendly representation (i.e. a mesh-texture pack-age), it obtains ray color via light fields (avoiding expensive volume rendering), and it factorizes the light field compu-tation as an MLP-free matrix multiplication. (iii) We con-duct a comprehensive empirical study of Re-ReND across resource-constrained devices. Our results find remarkable boosts in rendering time that come at insignificant costs to image quality. Notably, we find Re-ReND boosts rendering speeds by 2.6× at low memory expenses in challenging real scenes, enabling real-time rendering on various devices.
Striving for reproducibility, we provide our implementa-tion of Re-ReND, written in PyTorch [24], in the Appendix. 2.