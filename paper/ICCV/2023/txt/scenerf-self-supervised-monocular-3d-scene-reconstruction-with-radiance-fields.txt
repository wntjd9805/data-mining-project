Abstract 3D reconstruction from a single 2D image was exten-sively covered in the literature but relies on depth supervi-sion at training time, which limits its applicability. To re-lax the dependence to depth we propose SceneRF, a self-supervised monocular scene reconstruction method using only posed image sequences for training. Fueled by the re-cent progress in neural radiance ﬁelds (NeRF) we optimize a radiance ﬁeld though with explicit depth optimization and a novel probabilistic sampling strategy to efﬁciently han-dle large scenes. At inference, a single input image suf-ﬁces to hallucinate novel depth views which are fused to-gether to obtain 3D scene reconstruction. Thorough ex-periments demonstrate that we outperform all baselines for novel depth views synthesis and scene reconstruction, on indoor BundleFusion and outdoor SemanticKITTI. Code is available at https://astra-vision.github.io/SceneRF . 1.

Introduction
Humans evolve in a 3D physical world where even the slightest motion requires a thorough understanding of their surroundings to avoid collisions. While binocular vision is an evident evolutionary edge, physiological studies sug-gest that humans can sense depth even with monocular vi-sion [31]. Despite a long-standing line of research [68, 80, 63] this is yet unequaled by computer vision algorithms, which mostly rely on multiple-views to reconstruct complex scenes [56]. However, estimating 3D from a single view would unveil novel applications in a world ﬂooded with consumer cameras where mobile robots, like autonomous cars, still require costly depth sensors [6, 4].
A small portion of the 3D ﬁeld addressed reconstruc-tion of complex scenes from a single image [26, 81, 8, 12] but they all require depth supervision which discourage acquisition of image-only datasets. Meanwhile, Neural
Radiance Field [42] (NeRF), which optimizes a radiance
ﬁeld self-supervisedly from one or more views, unraveled
Figure 1: SceneRF overview. From a single input image,
SceneRF synthesizes novel depth/views, at arbitrary poses, which are then fused to estimate 3D reconstruction. It re-lies on an image-conditioned NeRF (here, f (·)) trained self-supervisedly on image sequences with pose. many descendants [74] with unprecedented performance on novel views synthesis. They are however, mostly limited to objects when it comes to single-view input [40, 48, 44].
For complex scenes, besides [34] all train on synthetic data [60] or require additional geometrical cues to train on real data [54, 14, 56]. Reducing the need of supervision on complex scenes would lower our dependency to costly-acquired datasets.
In this work, we address single-view reconstruction of complex (and possibly large) scenes, in a fully self-supervised manner. SceneRF trains only with sequences of posed images to optimize a large neural radiance ﬁelds (NeRF). Fig. 1 illustrates inference where a single RGB im-age sufﬁces to reconstruct the 3D scene from the fusion of synthesized novel depths/views, sampled at arbitrary loca-tions. We build upon PixelNeRF [77] and propose spe-ciﬁc design choices to explicitly optimize depth. Because
large scenes hold their own challenges, we introduce a novel probabilistic ray sampling to efﬁciently choose the sparse locations to optimize within the large radiance volume, and introduce a Spherical U-Net, which aims is to enable hallu-cination beyond the input image ﬁeld of view. We summa-rize our contributions below:
• We build on custom design choices to explicitly opti-mize depth (Sec. 3.1) with a Spherical U-Net (Sec. 3.3) – altogether allowing use of our radiance ﬁeld for scene reconstruction (Sec. 3.4),
• Our probabilistic ray sampling (Sec. 3.2) learns to model the continuous density volume with a mixture of
Gaussians – boosting both performance and efﬁciency,
• To the best of our knowledge, we propose the ﬁrst self-supervised large scene reconstruction method using a single-view as input. Results on indoor and driving scenes show that SceneRF even outperforms depth-supervised baselines (Sec. 4). 2.