Abstract
Federated learning (FL) is a distributed learning paradigm that enables multiple clients to learn a powerful global model by aggregating local training. However, the performance of the global model is often hampered by non-i.i.d. distribution among the clients, requiring extensive efforts to mitigate inter-client data heterogeneity. Going beyond inter-client data heterogeneity, we note that intra-client heterogeneity can also be observed on complex real-world data and seriously deteriorate FL performance. In this paper, we present a novel FL algorithm, i.e., FedIns, to handle intra-client data heterogeneity by enabling instance-adaptive inference in the FL framework. Instead of huge instance-adaptive models, we resort to a parameter-efficient fine-tuning method, i.e., scale and shift deep features (SSF), upon a pre-trained model. Specifically, we first train an
SSF pool for each client, and aggregate these SSF pools on the server side, thus still maintaining a low communica-tion cost. To enable instance-adaptive inference, for a given instance, we dynamically find the best-matched SSF subsets from the pool and aggregate them to generate an adaptive
SSF specified for the instance, thereby reducing the intra-client as well as the inter-client heterogeneity. Extensive experiments show that our FedIns outperforms state-of-the-art FL algorithms, e.g., a 6.64% improvement against the top-performing method with less than 15% communication cost on Tiny-ImageNet. 1.

Introduction
The availability of large-scale data dramatically pro-motes the development of deep learning models. How-ever, as these abundant data tend to be distributed across
*Corresponding author.
Figure 1: Illustration of inter- and intra-client data heterogeneity with t-SNE visualizations (see (a) and (b)) on DomainNet and their effect on accuracy of different FL algorithms (see (c) and (d)).
Please refer to the Suppl. for details on the settings to increase data heterogeneity. One can see that both inter- and intra-client data heterogeneity degrades FL performance. While FedBN [27] is able to alleviate the effect of inter-client data heterogeneity, both
FedAvg [30] and FedBN [27] are limited in handling intra-client data heterogeneity. many devices due to logistical and privacy concerns, de-centralized training is often required to train the deep neural network [1]. As a promising distributed learning paradigm, federated learning (FL) can train global mod-els in a distributed manner without sharing local private data [25, 30, 29]. During this process, each client first trains a local model on their private data and then sends the model parameters to the server for aggregation and distribution back to the client [30].
However, each client collects the local data in its own
manner (e.g., different devices, different ways). Conse-quently, the data distribution among different clients is het-erogeneous (see Fig. 1 (a)), resulting in serious perfor-mance degradation of conventional algorithms [4] (e.g., Fe-dAvg [30]). Many FL algorithms have been suggested to solve the problem of inter-client data heterogeneity for mak-ing them more robust in the non-i.i.d. setting [25, 16, 27, 24, 9] (see Fig. 1 (c)).
Actually, even for an individual client, the data may be collected by different devices [7], under different environ-mental conditions, etc. Therefore, intra-client data hetero-geneity can also be observed on complex real-world data [3] (see Fig. 1 (b)), and give rise to serious degradation of
FL performance (see Fig. 1 (d)). A naive solution is to build a specific model for each instance [42]. However, such a scheme results in significant challenges in the train-ing, storage and communication of the instance-adaptive models. Instead, we resort to the parameter-efficient fine-tuning, e.g., prompt tuning [14], of pre-trained models. As for prompt tuning, we can simply freeze the backbone of pretrained models, only learn and communicate a small number of learnable prompts between the server and the client [6, 36, 35, 14]. Nonetheless, instance-wise model-ing remains an unstudied issue for parameter-efficient fine-tuning. Additionally, in comparison to the original pre-trained model, prompt tuning also introduces additional pa-rameters and increases the computation cost in the inference stage [28].
To handle the intra-client data heterogeneity, instead of instance-adaptive models, this paper resorts to instance-adaptive inference, and presents a novel FL algorithm, i.e., FedIns, built upon pre-trained models. For parameter-effective fine-tuning, following [28], we scale and shift the deep features (SSF) to fine-tune the pre-trained model in the local training phase and merge them into the original pre-trained model weights by reparameterization in the infer-ence phase. To enable instance-adaptive inference, we train a SSF pool for each client and aggregate them on the server, and thus low storage and communication costs can still be maintained. For example, compared with FedAvg [30],
FedIns only has less than 15% communication costs. Our federated SSF pool is aggregated from multiple groups of local SSF, which implicitly add client-relevant knowledge to the federated SSF pool and dynamically guide the client to handle the corresponding response in an instance-wise fashion. During inference, for a given instance, we dynam-ically find the best-matched SSF subsets from the pool and aggregate them to generate an adaptive SSF specified for the instance. As such, the model can reduce both inter- and intra-client heterogeneity, achieving a 6.64% gain against
FedAvg on CIFAR-100.
In a nutshell, our contributions are three-fold:
• A novel FL algorithm, i.e., FedIns, is presented to han-dle intra-client data heterogeneity, which has been over-looked by the existing FL literature.
• Federated SSF is proposed and extended by allowing each client to have an SSF pool, and instance-adaptive infer-ence is fulfilled by dynamically finding and aggregating the best matched SSF subsets for each test instance.
• Extensive experimental results show FL performance can be effectively improved by alleviating the intra- and inter-client data heterogeneity. 2.