Abstract
Visible-infrared person re-identification (VI-ReID) aims to match a specific person from a gallery of images cap-tured from non-overlapping visible and infrared cameras.
Most works focus on fully supervised VI-ReID, which re-quires substantial cross-modality annotation that is more expensive than the annotation in single-modality. To re-duce the extensive cost of annotation, we explore two practi-cal semi-supervised settings: uni-semi-supervised (annotat-ing only visible images) and bi-semi-supervised (annotating partially in both modalities). These two semi-supervised settings face two challenges due to the large cross-modality discrepancies and the lack of correspondence supervision it is diffi-between visible and infrared images. Thus, cult to generate reliable pseudo-labels and learn modality-invariant features from noise pseudo-labels. In this paper, we propose a dual pseudo-label interactive self-training (DPIS) for these two semi-supervised VI-ReID. Our DPIS integrates two pseudo-labels generated by distinct models into a hybrid pseudo-label for unlabeled data. However, the hybrid pseudo-label still inevitably contains noise. To eliminate the negative effect of noise pseudo-labels, we in-troduce three modules: noise label penalty (NLP), noise correspondence calibration (NCC), and unreliable anchor learning (UAL). Specifically, NLP penalizes noise labels,
NCC calibrates noisy correspondences, and UAL mines the hard-to-discriminate features. Extensive experimen-tal results on SYSU-MM01 and RegDB demonstrate that our DPIS achieves impressive performance under these two semi-supervised settings.
*Equal contribution.
†Corresponding author.
Figure 1. The figure illustrates the differences between fully super-vised, uni-semi-supervised, and bi-semi-supervised learning set-tings, which mainly lie in the availability of labeled data. 1.

Introduction
Person re-identification (ReID) aims to retrieve a person corresponding to a given query across multi-disjoint cam-era views[11, 43, 30]. ReID has recently gained increas-ing attention due to its wide range of applications in au-tomated tracking and surveillance systems[16]. The exist-ing ReID methods might fail to achieve encouraging results under poor illumination environments, which limits the ap-plicability of ReID in a real-world scenario. To overcome this problem, the visible-infrared person re-identification (VI-ReID) has been proposed, which aims at retrieving in-frared person images of the same identity as the given visi-ble query and vice versa.
Though significant studies [42, 41, 37] have been made in supervised VI-ReID, they are built upon substantial la-beled data. It is challenging to manually annotate every im-age due to the enormous number of identities. Furthermore, some images are difficult for humans to recognize, espe-cially the color information is lost in the infrared images.
For example, the scale of existing VI-ReID datasets (e.g.,
SYSU-MM01 [36] and RegDB [25]) is relatively small due to the difficulty of annotating cross-modality images, which limits the development of VI-ReID. In this paper, we argue that it is also necessary to study a VI-ReID model trained with a small number of labels. We explore two practi-cal semi-supervised settings: uni-semi-supervised (annotat-ing only visible images) and bi-semi-supervised (annotating partially in both modalities). The instructions for these two settings are shown in Fig. 1 (b) and (c). Our goal is to train a VI-ReID model with limited labeled data to achieve com-parable performance with the fully supervised methods.
Several pseudo-label-based methods [33, 4] involve pseudo-label generation and self-training, which show ef-fectiveness in VI-ReID. But, all of them only focus on how to generate reliable pseudo-labels or mitigate the negative impacts of noise pseudo-labels while ignoring how to cali-brate noise pseudo-labels.
In this paper, we propose a dual pseudo-label interac-tive self-training (DPIS) framework. Our DPIS consid-ers both the generation of reliable pseudo-labels and the calibration of noise pseudo-labels. We employ two dis-tinct models to generate two pseudo-labels, i.e., Clustering
[4] and OTLA [14]. Then, we integrate the two pseudo-labels to obtain a more accurate hybrid pseudo-label. OTLA
[33] is a classifier with optimal transport, which considers both accuracy and even distribution of prediction results.
This prevents most unlabeled images from being classified into a few categories, ultimately resulting in a high pro-portion of noise in the generated pseudo-labels. Cluster-ing [4] is a pseudo-label refinement module designed for specific modality characteristics. Despite all this, there are inevitable noise labels in the hybrid pseudo-labels. In or-der to learn knowledge from noise pseudo-labels, we intro-duce three modules: noise label penalty (NLP), noise corre-spondence calibration (NCC), and unreliable anchor learn-ing (UAL). Specifically, following the work [38], we intro-duce a two-component Gaussian Mixture Model (GMM) to compute confidence for each pseudo-label and divide them into reliable and unreliable anchors (these anchors will be used to construct triplet [43]). Then, we penalize the noise labels according to confidence, which eliminates the neg-ative effects. Finally, for reliable anchors, NCC calibrates false positive samples for corresponding anchors to remove the detrimental effects of noise correspondence. For un-reliable anchors, UAL utilizes the unreliable anchors that were discarded by NCC to mine hard-to-discriminate fea-tures through unsupervised contrastive learning.
To summarize, our contributions are three-fold:
• We propose a dual pseudo-label interactive self-training framework for semi-supervised visible-infrared person ReID, which leverages the intra- and inter-modality characteristics to obtain hybrid pseudo-labels for unlabeled data.
• We introduce three modules: noise label penalty (NLP), noise correspondence calibration (NCC), and unreliable anchor learning (UAL). These modules help to penalize noise labels, calibrate noisy correspon-dences, and exploit hard-to-discriminate features.
• We provide comprehensive evaluations under these two semi-supervised VI-ReID. Extensive experiments on two popular VI-ReID benchmarks demonstrate that our DPIS achieves impressive performance. 2.