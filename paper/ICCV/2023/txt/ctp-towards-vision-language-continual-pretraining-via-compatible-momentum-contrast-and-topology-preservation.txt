Abstract
Vision-Language Pretraining (VLP) has shown impres-sive results on diverse downstream tasks by offline train-ing on large-scale datasets. Regarding the growing na-ture of real-world data, such an offline training paradigm on ever-expanding data is unsustainable, because models lack the continual learning ability to accumulate knowl-edge constantly. However, most continual learning studies are limited to uni-modal classification and existing multi-modal datasets cannot simulate continual non-stationary data stream scenarios. To support the study of Vision-Language Continual Pretraining (VLCP), we first con-tribute a comprehensive and unified benchmark dataset
P9D which contains over one million product image-text pairs from 9 industries. The data from each industry as an independent task supports continual learning and conforms to the real-world long-tail nature to simulate pretraining on web data. We comprehensively study the characteris-tics and challenges of VLCP, and propose a new algorithm:
Compatible momentum contrast with Topology Preserva-tion, dubbed CTP. The compatible momentum model ab-sorbs the knowledge of the current and previous-task mod-els to flexibly update the modal feature. Moreover, Topology
Preservation transfers the knowledge of embedding across tasks while preserving the flexibility of feature adjustment.
The experimental results demonstrate our method not only achieves superior performance compared with other base-lines but also does not bring an expensive training burden.
Dataset and codes are available at https://github. com/KevinLight831/CTP. 1.

Introduction
Benefiting from the remarkable generalization ability de-rived from large-scale pretraining, Vision-Language Pre-training (VLP) [33, 23] has emerged as the prevalent ap-proach for addressing downstream vision-language tasks.
The recent advancements in artificial intelligence such as
CLIP [33] and ChatGPT [1] have further fueled this trend
* This work was done when Hongguang Zhu worked as an research intern in Peng Cheng Laboratory. Email: kevinlight831@gmail.com
† Corresponding author: yzhao@bjtu.edu.cn
Figure 1: The traditional Class-Incremental Learning (CIL) is inflexible in the continual learning of visual concepts, which needs ever-expanding classifier parameters and end-less human annotation. Moreover, it is difficult for single-class labeling to cover all visual concepts in an image. e.g.,
CIL only focuses on the foreground class dog and ignores the background class flower, while Vision-Language Con-tinual Pretraining (VLCP) can flexibly represent the image content by text. Compared with CIL, which fixes the label space and only updates the image encoder, VLCP updates the image and text encoders simultaneously in the fixed dimension. Meanwhile, previous-task data also cannot be used as contrast samples in the continual pretraining. of using larger models and more data.
In the long term, this computational headlong rush does not seem reason-able to move toward sustainable solutions and actually also excludes academic laboratories with limited resources.
Current VLP paradigms all train on prepared data in ad-vance. Nevertheless, the world is ever-changing. Offline-trained models can not evolve in a dynamic environment to continually acquire, integrate and accumulate new knowl-edge. Moreover, repeated offline pretraining on the ever-expanding dataset will impose growing and endless training costs. Only finetuning on new data will also suffer severe
degradation due to catastrophic forgetting [32]. Hence, in practical application scenarios, it is significant for VLP to continually integrate knowledge from the incoming data.
Prior studies on continual learning [20, 55, 42, 53, 51] focused on supervised class-incremental learning (CIL), as
Figure 1(a), which aims to maintain discriminative features for known classes and expand new classifiers to learn new classes. However, this paradigm is inflexible due to the constant demand for laborious annotation and increasing classifier parameters.
In contrast, VLP allows for learn-ing open-world visual patterns without explicit “class” con-cepts, which can capture more comprehensive visual con-cepts rather than just category-based features. Moreover, massive web weak-aligned image-text pairs can be used as training data without extra human annotation, and no extra parameters are needed for category expansion as the output dimension is fixed for image and text encoders.
Nevertheless, Vision-Language Continual Pretraining (VLCP) remains understudied due to the lack of datasets that satisfy both massive image-text pairs and continual tasks with discrepant knowledge. Therefore, we contribute the first VLCP dataset P9D, which contains more than 1 million product image-text pairs and over 3800 categories from 9 industries. Different task data are split according to industry (e.g. food and clothing) to support the contin-ual pretraining. P9D not only is larger than previous CIL datasets both in terms of both class number and data size. but also conforms to real-world long-tailed distributions.
As shown in the Figure 1(b), VLCP, as a new paradigm, also suffers new challenges compared with traditional CIL. 1) Fixed-dimensional embedding: CIL methods typically address the stability-plasticity dilemma by preserving old logits [27] or freezing old backbone [31] and finetuning new classifiers. Without explicit class supervision and in-creasing embedding dimension, the VLCP can only ad-just the fixed-dimensional shared embedding to incorporate both old and new knowledge. 2) Missing contrast sam-ples: CIL still can use the gradient from negative logits of old classes [27, 34, 17] even if the old data is unseen. But the lack of contrast samples from old tasks leads to sub-optimal shared embedding in VLCP. 3) Multi-modal/task optimization: Unlike CIL has fixed label space to optimize image encoder, VLCP involves the complicated joint opti-mization of image, text, and multi-modal encoders.
Therefore, we propose a simple yet effective method,
Compatible momentum contrast with Topology Preser-vation (CTP), which maintains a compatible momentum model that absorbs both new and old knowledge to sepa-rately adjust uni-modal and multi-modal encoders. More-over, different from CIL methods that distill visual features across tasks, topology preservation keeps consistent sample relationship across tasks.
It not only transfers the topol-ogy knowledge of the old embedding while preserving the flexibility of feature adjustment. Meanwhile, to systemati-cally investigate the vision-language continual pretraining, we extend a series of traditional CIL methods to VLCP and evaluate them in a unified setting.
Interestingly, we find that the multi-modal fusion feature by masked modeling pretraining has a strong anti-forgetting ability, and the per-formance of continual finetuning approximates that of joint training in multi-modal retrieval. Oppositely, due to the lack of contrastive samples from different tasks, the cross-modal alignment ability suffers serious forgetting in continual pre-training and has a big gap with joint training. Meanwhile,
The experimental results show our method is not only com-patible with both memory-buffer and memory-free situa-tions, but also achieve leading performance without incur-ring expensive training time costs.
Our contributions are as follows:
• We build the first Vision-Language Continual Pretraining (VLCP) benchmark dataset P9D to support the study of
VLCP. which contains massive image-text pairs and the continual non-stationary task data.
• We systematically study the characteristics and chal-lenges of VLCP, and establish baseline library for VLCP by extending popular continual learning methods and evaluating them in a unified setting.
• We propose a simple yet effective method CTP for VLCP, which achieve both superior performance and efficient training. 2.