Abstract
Autonomous vehicles (AV) require that neural networks used for perception be robust to different viewpoints if they are to be deployed across many types of vehicles without the repeated cost of data collection and labeling for each. AV companies typically focus on collecting data from diverse scenarios and locations, but not camera rig configurations, due to cost. As a result, only a small number of rig varia-tions exist across most fleets. In this paper, we study how
AV perception models are affected by changes in camera viewpoint and propose a way to scale them across vehicle types without repeated data collection and labeling. Using bird’s eye view (BEV) segmentation as a motivating task, we find through extensive experiments that existing percep-tion models are surprisingly sensitive to changes in camera viewpoint. When trained with data from one camera rig, small changes to pitch, yaw, depth, or height of the camera at inference time lead to large drops in performance. We introduce a technique for novel view synthesis and use it to transform collected data to the viewpoint of target rigs, allowing us to train BEV segmentation models for diverse target rigs without any additional data collection or label-ing cost. To analyze the impact of viewpoint changes, we leverage synthetic data to mitigate other gaps (content, ISP, etc). Our approach is then trained on real data and evalu-ated on synthetic data, enabling evaluation on diverse tar-get rigs. We release all data for use in future work. Our method is able to recover an average of 14.7% of the IoU that is otherwise lost when deploying to new rigs. 1.

Introduction
Neural networks (NNs) are becoming ubiquitous across domains. Safety critical applications, such as autonomous vehicles (AVs), rely on these NN to be robust to out of dis-tribution (OOD) data. Yet, recent work has drawn atten-tion to the susceptibility of NNs to failure when exposed
*Work done during an internship at NVIDIA.
†Corresponding author: Jose M. Alvarez (josea@nvidia.com).
Figure 1. Impact of Changed Camera Viewpoint: We find that the performance of state-of-the-art methods for bird’s eye view (BEV) segmentation quickly drop with small changes to viewpoint at inference. Above we see predictions from Cross View Trans-formers [29] trained on data from a source rig (top). The target rig pitch is reduced by 10◦ (bottom), leading a 17% drop in IoU. to OOD data, such as adversarial corruptions [10], unseen weather conditions [15], and new geographic regions [6].
While each of these pose a significant challenge for safety critical applications, we focus on another distribution shift, which, thus far, has been understudied in the research liter-ature – changes in camera viewpoint between train data and test data. Because camera viewpoint changes are realistic in
AVs, we study their impact on AV perception tasks.
AVs use cameras around the ego-vehicle to perceive their surroundings. Using images from each camera, NNs detect and segment objects in the scene, such as vehicles, pedestri-ans, roads, and more. This information is used by trajectory planners to decide how the ego-vehicle navigates. Camera viewpoint for AVs may differ between train and test in sev-eral real-world scenarios. First, the camera viewpoint may change over time due to wear and tear or damage. Second, camera viewpoint may change due to installation variation.
Third, and most relevant for our work, if a single NN is to be deployed across different types of vehicles, it must be able to generalize to the camera viewpoints of each car.
Collecting and labeling train data for each target rig is not scalable and quickly becomes intractable for AV companies wishing to scale across many types of vehicles due to cost, thus motivating our work to transform collected data into the viewpoint of diverse target rigs to use for training.
The goal of this paper is to bring understanding and a
first approach to a real-world problem in the AV space that has yet to receive attention in the research literature – gen-eralization from a source to target camera rig. We focus on bird’s eye view (BEV) segmentation from RGB data to mo-tivate how changing camera viewpoint can affect AV per-ception models. We study this problem by conducting an in-depth analysis on the impact changing the camera view-point at inference time has on recent BEV segmentation models. Our findings indicate that even small changes in camera placement at inference time degrade BEV segmen-tation accuracy, as illustrated in Fig. 1. We then propose a method to improve generalization to a target rig by sim-ulating views in the target perspective. We show that in-corporating data generated from novel view synthesis into training can significantly reduce the viewpoint domain gap, bringing the BEV segmentation model to the same level of accuracy as when there is no change in camera viewpoint, without having to collect or label any additional data. We compare our approach with other strategies, such as aug-menting the camera extrinsics and labels during training, and find that our approach leads to better accuracy. Lit-tle work has focused on the impact of viewpoint changes for AV perception, and, to the best of our knowledge, we are the first to study the impact of diverse camera viewpoint changes on 3D AV perception tasks, such as BEV segmenta-tion. We hope that this paper will encourage more research on the important problem of viewpoint robustness in AV.
Our paper makes the following contributions:
• We highlight the understudied problem of viewpoint robustness in bird’s eye view segmentation for au-tonomous vehicles (AV) through an in-depth analysis revealing that recent models fail to generalize to differ-ent camera viewpoints at inference time.
• We propose a viewpoint augmentation framework for
AV; we develop a novel view synthesis method that can be used to transform training data to target viewpoints and show that it improves the robustness of bird’s eye view segmentation models to viewpoint changes.
• We provide datasets that can be used to benchmark fu-ture work on viewpoint robustness in AV.
Because real-world AV datasets from a diverse set of cam-era rigs are not publicly available, we use simulated data both for (1) training and evaluation in our analysis and (2) evaluation of our proposed technique. Our synthetic datasets can be used for future efforts to benchmark the gen-eralization abilities of different AV perception methods to viewpoint changes. Datasets are publicly available on our project page. The first dataset, rendered from CARLA, con-sists of both training and testing data, allowing for isolated analysis of the impact of viewpoint changes on BEV seg-mentation models (example images in Fig. 2). The second dataset, rendered with NVIDIA DRIVE Sim [20], is signif-icantly more photorealistic and consists of test sets from a diverse set of camera viewpoints. Thus, it can be used to evaluate models trained on real data, as we show in Section 5. Both datasets include 3D bounding box labels. 2.