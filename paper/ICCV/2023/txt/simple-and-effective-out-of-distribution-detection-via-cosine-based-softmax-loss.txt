Abstract
Deep learning models need to detect out-of-distribution (OOD) data in the inference stage because they are trained to estimate the train distribution and infer the data sampled from the distribution. Many methods have been proposed, but they have some limitations, such as requiring additional data, input processing, or high computational cost. More-over, most methods have hyperparameters to be set by users, which have a significant impact on the detection rate. We propose a simple and effective OOD detection method by combining the feature norm and the Mahalanobis distance obtained from classification models trained with the cosine-based softmax loss. Our method is practical because it does not use additional data for training, is about three times faster when inferencing than the methods using the input processing, and is easy to apply because it does not have any hyperparameters for OOD detection. We confirm that our method is superior to or at least comparable to state-of-the-art OOD detection methods through the experiments. 1.

Introduction
Deep learning models are generally designed to target the closed world. Models are trained based on the assump-tion that the inputs of the inference stage will be sampled from the same distribution as the train distribution. This means that the model may not make correct predictions if inputs are sampled from other distributions. The model must be able to distinguish whether the input is from the train distribution or not. This problem has been established as the task of out-of-distribution (OOD) detection.
Some researchers tried to detect OOD samples using confidence scores from classification models. Hendrycks and Gimpel simply used the confidence scores, but this approach was not very effective because of the overconfi-*Corresponding Author dence of deep learning models [9]. Hendrycks, Mazeika, and Dietterich (2019) and Papadopoulos et al. tried to lower the confidence of OOD samples using auxiliary large OOD datasets [10, 21]. However, collecting a massive OOD dataset is highly expensive, and training classification mod-els with such a dataset takes a long time. Liang, Li and
Srikant (2018) simply adopted the input processing tech-nique, which added a small perturbation to the input sam-ples to increase the confidence of in-distirubtion (InD) sam-ples, but they assumed OOD samples were given in ad-vance, which is not practical [16].
Some researchers defined measures to detect OOD sam-ples based on input features learned by classification mod-els. Lee et al. proposed a measure based on Mahalanobis distance, but they also assumed that OOD samples were given in advance as Liang et al. did [15, 16]. Sastry and
Oore proposed a measure based on various correlations be-tween features, but their approach was very time-consuming in evaluating the OOD measure [23]. Hsu et al. tried to ob-tain the OOD probability by modifying the network struc-ture, which caused the degradation of classification perfor-mances [12].
Additional popular technique is the input processing, which was adopted by most approaches [16, 15, 23]. The in-put processing is to generate xâ€² by adding a small perturba-tion to input data x to make the boundary of in-distribution compact.
It may be helpful to increase the detection ac-curacy under certain conditions. However, generating the perturbation takes almost twice of the inference time, so the total inference becomes three times longer. The OOD de-tection performance is very susceptible to the size of the perturbation, so it should be carefully determined.
Some OOD detection approaches had various hyperpa-rameters, including the perturbation size of input process-ing. The hyperparameters were usually tuned by using ad-ditional datasets to maximize the detection performance.
Liang et al. [16] and Lee et al. [15] tuned them with given
OOD samples, and Hsu et al. [12] and Sastry and Oore [23] used the validation samples. However, it is hard to find out
hyperparameter values generally applicable to any distribu-tions of OOD samples. When the values are found in a spe-cific dataset, the model is easy to be biased to the dataset.
This paper proposes a simple and effective approach that can detect OOD samples without hyperparameters, addi-tional datasets, and input processing. We detect OOD sam-ples by combining norm and Mahalanobis distances of fea-tures. We find that the models trained with the cosine-based softmax loss are more advantageous than the ones trained with the regular softmax loss for OOD detection. We math-ematically conjecture that the feature norm from the mod-els with the cosine-based softmax loss can be a measure for the probability of in-distribution, and confirm its effective-ness experimentally. We also develop a robust Mahalanobis distance-based measure for the model with the cosine-based softmax loss. We propose an OOD score function by com-bining the feature norm and the Mahalanobis distance with-out any hyperparmaeters.
Our method do not need the input processing because it shows high accuracy in OOD detection without it. So, the additional computation for OOD detection by our method is very small. Feature norms and the Mahalanobis distances can be obtained with tiny additional calculations from clas-sification models. Since the cosine-based softmax loss is generally applicable to any deep models, our OOD detec-tion approach can be easily extended to various structures without a classification accuracy loss. Our method is also practical because it does not use any additional datasets.
We confirm the OOD detection performance on various im-age datasets through experiments. Our method is compared with others using additional datasets, performing the input processing, or tuning hyperparameters to maximize perfor-mance. The experiment shows that our approach is superior to or at least comparable to state-of-the-art OOD detection methods. 2.