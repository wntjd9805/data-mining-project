Abstract
Continual Learning (CL) is the constant development of complex behaviors by building upon previously acquired skills. Yet, current CL algorithms tend to incur class-level forgetting as the label information is often quickly overwrit-ten by new knowledge. This motivates attempts to mine instance-level discrimination by resorting to recent self-supervised learning (SSL) techniques. However, previous works have pointed out that the self-supervised learning ob-jective is essentially a trade-off between invariance to dis-tortion and preserving sample information, which seriously hinders the unleashing of instance-level discrimination.
In this work, we reformulate SSL from the information-theoretic perspective by disentangling the goal of instance-level discrimination, and tackle the trade-off to promote compact representations with maximally preserved invari-ance to distortion. On this basis, we develop a novel alter-nate learning paradigm to enjoy the complementary mer-its of instance-level and category-level supervision, which yields improved robustness against forgetting and better adaptation to each task. To verify the proposed method, we conduct extensive experiments on four different benchmarks using both class-incremental and task-incremental settings, where the leap in performance and thorough ablation stud-ies demonstrate the efﬁcacy and efﬁciency of our modeling strategy. 1.

Introduction
Humans learn from visual inputs with everchanging sce-narios, both rapidly and ﬂexibly absorbing new knowledge with constantly emerging concepts, and robustly accumulat-Figure 1: Illustration of the trade-off that is widely embod-ied in recent self-supervised learning techniques. X denotes the given sample, V A and Z A stand for the augmented view and the corresponding embedding obtained from distortion
T A ∼ T and network fψ, respectively. ing previously acquired experiences. Modeling such pow-erful capability is the central target of continual learning (CL), and would be of substantial utility in real-world com-puter vision settings [44].
To this end, a variety of CL algorithms [7, 8, 19, 28, 40] have been developed to get rid of the requirement of i.i.d samples, and attempt to alleviate catastrophic forgetting
[23] when learning a continuum of training data. Broadly speaking, studies on this topic can be categorized into three schools: (i) rehearsal-based methods [4, 14, 33], which store samples in raw or generative format, and replay them to alleviate forgetting; (ii) regularization-based methods
[32, 36] that impose extra constraints to prediction, gradi-ent, etc. to consolidate previously learned contents; (iii) parameter isolation strategies [13, 22] that dedicate or mask a part of the model parameters for the training of each task.
Albeit the differences, the typical solution is to utilize the label, i.e., category-level supervision to acquire new knowl-edge while accumulating previously learned contents. Al-though this usually leads to fast adaptation, such strategy, i.e., supervised learning (SL), is prone to incur class-level
forgetting and quickly fades out with the label information overwritten by new concepts [23, 34]. Moreover, they also tend to produce overﬁtted and biased models when only a small amount of datum (e.g., incremental tasks, or samples stored in exemplar memory) is accessible for training.
The deﬁciencies mentioned above motivate a trend in CL community to resort to instance-level discrimination to al-leviate forgetting. For example, [21] directly applies SSL constraints to a pre-trained model, [25] deploys a parallel self-supervised branch in addition to a supervised partner.
Unfortunately, as indicated in BarlowTwins [41], the ob-jective of SSL is essentially a trade-off between preserv-ing sample information and being invariant to distortions, but both of which are necessary and beneﬁcial for continual learning. As a consequence, continual learners equipped with such strategy can barely achieve instance-level dis-crimination, and are heavily dependent on an extra super-vised partner. The above factors seriously limit the effec-tiveness of the self-supervised continual learner and have a detrimental effect on the performance of the entire CL framework, especially in task-agnostic CL settings [38].
In this work, we tackle the trade-off by reformulating the SSL objective from the information-theoretic perspec-tive. More speciﬁcally, we ﬁrst disentangle the principal tar-get of instance-level discrimination into two terms, i.e., (i) maximizing sample information without intensifying varia-tion caused by distortion; (ii) promoting invariance to yield compact representations. On this basis, our SSL strategy exhibits superiority in preserving instance-level discrimina-tion, and yields improved robustness against forgetting.
To enjoy both complementary merits of category-level and instance-level supervision, we develop a novel paradigm for continua learning. Concretely, it includes two updates to the continual learner, i.e., an “inner-loop” which alternately conducts SSL and SL, and an “outer-loop” which uses a momentum update to accumulate knowledge from previous tasks. In such cases, SSL serves as a pre-training procedure and maintains stability, while SL is uti-lized to generate task-speciﬁc parameters for plasticity.
To validate the proposed method, we conduct extensive experiments on four benchmarks, including CIFAR-100
[16], Tiny-ImageNet [17], ImageNet-100 and ImageNet-1K
[5], and provide a comprehensive ablation on each compo-nent to show the qualitative characteristics. Our contribu-tions can be summarized as follows: (i) We reformulate the SSL objective by disentangling it into two terms, which promote invariance against distortion while simultaneously producing compact representations. (ii) We design a novel alternate paradigm for continual learning, which fully exploits the complementary advan-tages of both category-level and instance-level supervision, demonstrating signiﬁcant superiority in achieving both sta-bility and plasticity. (iii) The leap in performance compared with all competi-tors on various benchmarks demonstrates its efﬁcacy, while substantial qualitative evidence veriﬁes each of our designs. 2. Preliminary and