Abstract
We introduce Domain-Adaptive Prompt (DAP), a novel method for continual learning using Vision Transformers (ViT). Prompt-based continual learning has recently gained attention due to its rehearsal-free nature. Currently, the prompt pool, which is suggested by prompt-based contin-ual learning, is key to effectively exploiting the frozen pre-trained ViT backbone in a sequence of tasks. However, we observe that the use of a prompt pool creates a domain scal-ability problem between pre-training and continual learn-ing. This problem arises due to the inherent encoding of group-level instructions within the prompt pool. To address this problem, we propose DAP, a pool-free approach that generates a suitable prompt in an instance-level manner at inference time. We optimize an adaptive prompt gener-ator that creates instance-specific fine-grained instructions required for each input, enabling enhanced model plas-ticity and reduced forgetting. Our experiments on seven datasets with varying degrees of domain similarity to Im-ageNet demonstrate the superiority of DAP over state-of-the-art prompt-based methods. Code is publicly available at https://github.com/naver-ai/dap-cl. 1.

Introduction
Humans can learn and solve continuously emerging tasks by leveraging knowledge from past experiences. In-spired by it, continual learning (CL) methods aim at tack-ling a sequence of tasks using a single model without ex-periencing performance deterioration in previously learned tasks [1, 56, 59]. Typically, rehearsal-based methods [5, 7, 22, 42], motivated by the complementary learning systems of humans [35], store a data subset of past tasks to al-leviate forgetting while acquiring new information. By maintaining a replay buffer of reasonable size, this ap-proach has shown superiority over regularization [8, 25]
*Work was done while working at NAVER AI Lab.
†Corresponding author.
Figure 1. Average accuracy changes of prompt-based CL methods including DAP across five datasets with varying levels of domain similarity to ImageNet. The datasets are sorted with domain simi-larity in descending order from left to right. and architecture-based methods [2, 33] in various settings.
However, the rehearsal-based approach is reluctant to use when data privacy is concerned, or the memory budget is
In this regard, lately, prompt-based rehearsal-free tight.
CL methods, such as L2P [58] and DualPrompt [57], have been presented and proved to outperform existing rehearsal-based methods without relying on a replay buffer.
Prompt-based learning was initially introduced in the field of natural language processing for effective transfer learning [31].
Instead of fine-tuning entire weights, it is able to condition an untouched (frozen) pre-trained back-bone such that it performs well to a specific downstream task. That is, only small additional weights (prompts) are trained to adjust learned representations from a source task to a new target task. Recently, with the recent advent of
Vision Transformers (ViT) [15], the notion of prompt-based learning has been adapted in CL [14, 47, 51, 55, 57, 58].
L2P and DualPrompt maintain a prompt pool, which is a constant number of prompts to learn shared prompts across tasks to mitigate forgetting as well as to benefit from pre-viously learned task knowledge. Hence, the prompt pool is considered a set of instructions to tune the frozen backbone to adapt to a sequence of new tasks.
Currently, most pre-trained backbones frozen in prompt-based learning are assumed to be trained on a large-scale natural image collection like ImageNet; L2P and Dual-(a) Domain-Adaptive Prompt (DAP). (b) Adaptive Prompt Generator (G in (a)).
Figure 2. Overview of DAP (left) and the architecture of the proposed adaptive prompt generator (G) (right).
Prompt also utilize the ImageNet-pretrained ViT as a frozen feature extractor. However, testing benchmarks in prior lit-erature [57, 58] are still confined only to natural images, despite no assumptions about data domain in CL. Data with varying levels of domain similarity to natural images can be provided as target tasks in the CL setup, e.g., Eu-roSAT data of satellite images [19] and ISIC data of skin diseases [12]. As such, prompts should be designed to en-code more domain-relevant knowledge and have the abil-ity to finely instruct the model to properly leverage learned representations. Our findings in Figure 1 reveal that exist-ing prompt-based CL methods exhibit two weaknesses con-cerning domain generalization, and thus they cannot be con-sistently effective across images with varying domains.
Their weaknesses arise since they rely on the prompt pool. First, they require hyperparamter tuning for the target domain in advance. The prompt pool can support varying levels of domain similarity by adjusting the pool size, and controlling the domain coverage of prompts. However, it is unrealistic to assume prior knowledge of the target domain in CL. Additionally, an expansion of the prompt pool leads to an increase in the memory budget. As shown in Figure 1, the two representative prompt-based CL methods show a significant performance degradation with a decrease in do-main similarity. The best pool sizes (w/ Tune in Figure 1), which were found via a grid search for each data domain in advance, improve the accuracy of the model, but a large performance drop still appears. Second, since the number of prompts in the pool is typically much smaller than the number of total training instances, each prompt is forced to be optimized in a group-level fashion. This makes it diffi-cult to provide delicate instructions to the model more suit-able per data instance, possibly resulting in better domain generalization.
In this paper, we propose a pool-free prompt-based CL framework named DAP, supporting domain-adaptive CL using the frozen backbone pre-trained on ImageNet. As shown in Figure 2(a), without relying on the pool of a constant number of prompts, it adaptively generates a sin-gle prompt per instance from input tokens; hence, DAP does not need to choose learned prompts from the pool, unlike L2P and DualPrompt. The adaptive prompt en-codes domain-relevant knowledge corresponding to the tar-get task, delicately steering the frozen backbone’s represen-tation via the attention mechanism in ViT. Specifically as shown in Figure 2(b), a feed-forward network and a linear transformation are utilized to extract instance-specific in-formation conditioned on a transposed input, creating the adaptive prompt in a timely manner. With the poolless instance-level prompts, DAP becomes versatile for various
CL tasks without domain restrictions, even when applied to a large benchmark. Our main contributions are:
• This is the first study to pose and examine the do-main scalability problem of the current prompt-based CL methods on benchmarks with varying levels of domain similarity to natural images.
• We propose a novel framework named DAP, which no longer relies on the prompt pool and generates each prompt in an instance-level manner, facilitating enhanced plasticity and reduced forgetting.
• We conduct extensive experiments on seven datasets with varying domains, including satellite, dermatology, and radiology images. DAP significantly outperforms the state-of-the-art prompt-based CL methods. 2.