Abstract
Visual-Language Models (VLMs) have significantly ad-vanced video action recognition. Supervised by the seman-tics of action labels, recent works adapt the visual branch of VLMs to learn video representations. Despite the effec-tiveness proved by these works, we believe that the potential of VLMs has yet to be fully harnessed. In light of this, we exploit the semantic units (SU) hiding behind the action la-bels and leverage their correlations with fine-grained items in frames for more accurate action recognition. SUs are entities extracted from the language descriptions of the en-tire action set, including body parts, objects, scenes, and motions. To further enhance the alignments between vi-sual contents and the SUs, we introduce a multi-region at-tention module (MRA) to the visual branch of the VLM.
The MRA allows the perception of region-aware visual fea-tures beyond the original global feature. Our method adap-tively attends to and selects relevant SUs with visual fea-tures of frames. With a cross-modal decoder, the selected
SUs serve to decode spatiotemporal video representations.
In summary, the SUs as the medium can boost discrim-inative ability and transferability. Specifically, in fully-supervised learning, our method achieved 87.8% top-1 ac-curacy on Kinetics-400. In K=2 few-shot experiments, our method surpassed the previous state-of-the-art by +7.1% and +15.0% on HMDB-51 and UCF-101, respectively. 1.

Introduction
Video action recognition is the fundamental task to-ward intelligent video understanding. Facilitated by deep learning, great advancements have been made in designing end-to-end network architecture, including two-stream net-works [50, 54, 67], 3D convolutional neural networks (3D-CNN) [9, 16, 17, 23, 46, 52, 53, 59] and transformer-based
† Corresponding author. ⋆ Ruijin Liu and Hao Li were interns at
Huawei during the project. networks [4, 14, 35, 44, 62]. The visual-languages models (VLM)[47, 25] have achieved great success in the last few years. Taking inspiration from this, recent works[55, 26, 41] started exploiting video representation learning under se-mantic supervision.
In particular, these works propose a paradigm that trains video representations to align with action-name text embeddings, which provide richer seman-tic and contextual information than conventional one-hot la-bels. Besides showing competitive performance in closed-set action recognition scenarios, adopting VLM demon-strates great learning effectiveness toward recognizing un-seen or unfamiliar categories compared to methods that do not utilize text embeddings.
Despite the achieved improvements, we believe that rely-ing solely on the semantic information of action labels is far from enough to fully harness the advantages of the VLMs.
This is because action labels are abstract concepts, and di-rectly mapping a video clip to them may confront a notice-able gap. We propose alleviating the problem by relating the action labels with some common entity, ranging from static scenes to moving body parts. For example, “playing golf” can be associated with people, golf equipment, and grass, which could provide fine-grained and specific guidance for visual alignments. We refer to these action-related basic en-tities as semantic units (SU). In practice, SUs are extracted from discriminative language descriptions of the whole ac-tion set, including body parts, objects, scenes, and motions.
Our basic idea is to use visual features to select related SUs and then use SUs to guide the decoding of spatiotemporal video features. Compared with the previous approaches, utilizing SUs can bring two advantages: (1) The text corre-sponding to semantic units can better explain which factors determine an action. (2) The re-usability of semantic units can greatly alleviate learning difficulty in the zero-shot/few-shot scenario.
Another concern arises when we explore fine-grained visual-language correlations through SUs: the visual branch of current VLMs produces frame-level representations, which may hinder the sensitive correspondences to SUs. To
Figure 1. Left: Compared with conventional video learners that encode frames directly, our method utilizes attentive semantic units (SU) to guide the representation learning of videos. The learned representation contains rich semantics and thus can be effectively adapted to the target representation given by the text representation of the action name. Right: With the utilization of semantic units (SU), our method can achieve impressive results on zero/few-shot learning. With the model trained on Kinetics-400, we achieve 48.1% and 70.8% top-1 accuracy in zero-shot and K=16 few-shot experiments, which is 3.5% and 6.6% higher than the second best methods. leverage the local visual appearance, we further introduce multi-region attention (MRA) modules upon the pre-trained visual encoders. MRA spatially perceives sub-regions of each image and enhances the visual features with region-aware info, therefore enabling visual features to select SUs globally and locally. The final video representation is pro-duced by a cross-modal video decoder with the selected SUs as the queries and the enhanced visual features from frames as the keys and values. The cross-modal decoder is a stack of cross-attention and 1D temporal convolution hybrid mod-ules. Cross-attention makes the representation focus on the most critical visual appearance by attentive semantic units, while the temporal convolution exploits the motion cue for video representation.
In summary, our contributions are threefold: (1) We utilize semantic units to guide the rep-resentation learning of actions. Because the semantic units are fine-grained and reusable, the learned representation can improve both the discriminative ability and the transferabil-ity. (2) We introduce multi-region attention (MRA) to the visual branch of VLM, which originally provided global representations. MRA perceives region-aware features for each image, enabling sensitive correspondence to multiple fine-grained semantic cues behind actions. (3) We propose a cross-modal decoder that generates the final video repre-sentation. This module utilizes attentive semantic units to highlight critical visual appearances while exploiting mo-tion cues. 2.