Abstract
Knowledge-based visual question answering (VQA) in-volves questions that require world knowledge beyond the image to yield the correct answer. Large language mod-els (LMs) like GPT-3 are particularly helpful for this task because of their strong knowledge retrieval and reasoning capabilities. To enable LM to understand images, prior work uses a captioning model to convert images into text. However, when summarizing an image in a single caption sentence, which visual entities to describe are often underspeciﬁed.
Generic image captions often miss visual details essential for the LM to answer visual questions correctly. To address this challenge, we propose PROMPTCAP (Prompt-guided image Captioning), a captioning model designed to serve as a better connector between images and black-box LMs. Dif-ferent from generic captions, PROMPTCAP takes a natural-language prompt to control the visual entities to describe in the generated caption. The prompt contains a question that the caption should aid in answering. To avoid extra annotation, PROMPTCAP is trained by examples synthesized with GPT-3 and existing datasets. We demonstrate PROMPT-CAP’s effectiveness on an existing pipeline in which GPT-3 is prompted with image captions to carry out VQA. PROMPT-CAP outperforms generic captions by a large margin and achieves state-of-the-art accuracy on knowledge-based VQA tasks (60.4% on OK-VQA and 59.6% on A-OKVQA). Zero-shot results on WebQA show that PROMPTCAP generalizes well to unseen domains.1 1.

Introduction
Knowledge-based visual question answering (VQA) [37] extends traditional VQA tasks [3] with questions that require
*Equal contribution. Correspondance to <Yushi Hu: yushihu@uw.edu>,
<Hang Hua: hhua2@cs.rochester.edu> 1All codes, data, and demos are available on the project page.
HF checkpoint: https://huggingface.co/tifa-benchmark/ promptcap-coco-vqa
What is the time of the day?
PromptCap
A street with trafﬁc lights in the evening.
ChatGPT
Evening.
What kind of food does the  restaurant on the sign serve?
PromptCap
A trafﬁc light on a street corner with a 
McDonalds sign.
ChatGPT
Fast food. Speciﬁcally, McDonald's serves  burgers, fries, and other fast food items. v.s. COCO caption: A trafﬁc light on a pole on a city street.
Figure 1.
Illustration of VQA with PROMPTCAP and ChatGPT.
PROMPTCAP is designed to work with black-box language models (e.g., GPT-3, ChatGPT) by describing question-related visual infor-mation in the text. Different from generic captions, PROMPTCAP customizes the caption according to the input question prompt, which helps ChatGPT understand the image and give correct an-swers to the user. In contrast, ChatGPT cannot infer the answers from the vanilla human-written caption from MSCOCO. broad knowledge and commonsense reasoning to yield the correct answer. Existing systems on knowledge-based VQA retrieve external knowledge from various sources, including knowledge graphs [13, 36, 63], Wikipedia [36, 63, 12, 15, 29], and web search [35, 63]. Recent work [67] ﬁnds that modern language models (LMs) like GPT-3 [5] are particu-larly useful for this task because of their striking knowledge retrieval and reasoning abilities. The current state-of-the-art methods [67, 15, 29, 1] all make use of recent large language models (GPT-3 or Chinchilla).
One key challenge is to allow LMs to understand images.
Many top-performing LMs (e.g., GPT-3, ChatGPT) are only
accessible via APIs, making it impossible to access their in-ternal representations or conduct ﬁne-tuning [49]. A popular solution is to project images into texts that black-box LMs can process, via a generic image captioning model [7] or an image tagger [67]. This framework has been successful on multiple tasks, including VQA [67, 15, 29], image para-graph captioning [65], and video-language tasks [70, 60].
Despite promising results, converting visual inputs into a generic, ﬁnite text description risks excluding information necessary for the task. As discussed in PICa [67], when used for VQA tasks, the generic caption might miss the detailed visual information needed to answer the question, such as missing the “McDonald’s" in Figure 1.
To address the above challenges, we introduce PROMPT-CAP, a question-aware captioning model designed to serve as a better connector between images and a black-box LM.
PROMPTCAP is illustrated in Figure 2. PROMPTCAP takes an extra natural language prompt as input to control the vi-sual content to describe. The prompt contains the question that the generated caption should help to answer. LMs can better answer visual questions by using PROMPTCAP as their
“visual front-end". For example, in Figure 1, when asked
“what is the time of the day?", PROMPTCAP includes “in the evening" in its image description; when asked “what kind of food does the restaurant on the sign serve?", PROMPTCAP includes “McDonald’s” in its description. Such visual in-formation is critical for ChatGPT to reply to the user with the correct answers. In contrast, the generic COCO [28] caption often contains no information about the time or the sign, making ChatGPT unable to answer the questions.
One major technical challenge is PROMPTCAP training.
The pipeline of “PROMPTCAP + black-box LM" cannot be end-to-end ﬁne-tuned on VQA tasks because the LM param-eters are not exposed through the API. Also, there are no training data for question-aware captions. To avoid extra annotation, we propose a pipeline to synthesize and ﬁlter training samples with GPT-3. Speciﬁcally, we view existing
VQA datasets as pairs of question and question-related vi-sual details. Given a question-answer pair, we rewrite the corresponding image’s generic caption into a customized caption that helps answer the question. Following 20 human-annotated examples, GPT-3 synthesizes a large number of question-aware captions via few-shot in-context learning [5].
To ensure the sample quality, we ﬁlter the generated captions by performing QA with GPT-3, checking if the answer can be inferred given the question and the synthesized caption.
Notice that GPT-3 is frozen in the whole pipeline. Its strong few-shot learning ability makes this pipeline possible.
We demonstrate the effectiveness of PROMPTCAP on knowledge-based VQA tasks with the pipeline in PICa [67].
Details of the pipeline are illustrated in §4. The images are converted into texts via PROMPTCAP, allowing GPT-3 to perform VQA via in-context learning. This pipeline, despite
Original VQA sample
Question:  
What type plane  is this? 
Answer: 747 
Prompt please describe this  image according to the  following question:  what type plane is this? 
Generic caption: 
A passenger jumbo jet  taxiing at an airport. 
GPT-3 rewrite
GPT-3
Training 
Target
A 747 taxiing  at an airport 
PromptCap
Question-aware caption  (synthesized by GPT-3)
Figure 2. Overview of PROMPTCAP training. PROMPTCAP takes two inputs, including an image and a natural language prompt.
The model is trained to generate a caption that helps downstream
LMs to answer the question. During training, we use GPT-3 to synthesize VQA samples into captioning examples. The original caption is rewritten into a caption that helps answer the question.
PROMPTCAP is trained to generate this synthesized caption given the image and the prompt. its simplicity, achieves state-of-the-art results on knowledge-based VQA tasks (60.4% on OK-VQA [38] and 59.6% on
A-OKVQA [46]). We also conduct extensive ablation stud-ies on the contribution of each component, showing that
PROMPTCAP gives a consistent performance gain (3.8% on
OK-VQA, 5.3% on A-OKVQA, and 9.2% on VQAv2) over a generic captioning model that shares the same architecture and training data. Finally, we investigate PROMPTCAP’s generalization ability on WebQA [6], showing that PROMPT-CAP, without any training on the compositional questions in
WebQA, outperforms the generic caption approach and all supervised baselines.
In summary, our contributions are as follows:
• We propose PROMPTCAP, a novel question-aware cap-tioning model that uses natural language prompt to control the visual content to be described. (§3)
• To the best of our knowledge, we are the ﬁrst to propose a pipeline to synthesize and ﬁlter training samples for vision-language tasks via GPT-3 (§3.1).
• PROMPTCAP helps GPT-3 in-context learning (§4) achieve state-of-the-art results on OK-VQA and A-OKVQA, substantially outperforming generic captions on various VQA tasks. (§5). 2.