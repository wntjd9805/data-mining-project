Abstract
Large vision-language models are generally applicable to many downstream tasks, but come at an exorbitant train-ing cost that only large institutions can afford. This pa-per trades generality for efﬁciency and presents Curation in Training (CiT), a simple and efﬁcient vision-text learning algorithm that couples a data objective into training. CiT automatically yields quality data to speed-up contrastive image-text training and alleviates the need for an ofﬂine data ﬁltering pipeline, allowing broad data sources (includ-ing raw image-text pairs from the web). CiT contains two loops: an outer loop curating the training data and an inner loop consuming the curated training data. The text encoder connects the two loops. Given metadata for tasks of interest, e.g., class names, and a large pool of image-text pairs, CiT alternatively selects relevant training data from the pool by measuring the similarity of their text embeddings and em-beddings of the metadata. In our experiments, we observe that CiT can speed up training by over an order of magni-tude, especially if the raw data size is large. 1.

Introduction
Vision-language models have demonstrated success for
ﬁne-tuning and zero-shot transfer to downstream tasks[21, 12, 26] by training on a general-purpose large-scale dataset instead of a small task-level dataset. While general, large-scale pre-training is computationally expensive (e.g.
CoCa[36] trains on 2048 TPUs for 5 days) and typically per-formed on a pre-ﬁltered dataset (e.g. WIT400M [21] used by CLIP [21] is created by searching for image-text pairs with text containing a set of 500,000 queries from Word-Net (includes ImageNet taxonomy) and Wikipedia, and [24] uses this model to create the LAION dataset).
Such ﬁltering pipelines usually involve manual labor-intensive efforts to remove data that is unlikely useful for downstream tasks [12, 21]. Recent effort has been made to curate data for high-quality image-text pairs (such as
CC3M[25], CC12M[3], YFCC15M[29, 21], WIT400M[21]
CLIP Pre-training
Raw Data
Static Training Data
Filtering Pipeline
Curation in Training
Text 
Model
Dynamic Training Data
Raw Data
Metadata
Text 
Model
Vision 
Model
Training Loop
Text 
Model
Vision
Model
Data Curation Loop
Training Loop
Figure 1: A conceptual illustration of CLIP/LiT training vs. CiT.
Vanilla CLIP training uses static data from ofﬂine human ﬁlter-ing (e.g. cleaned YFCC15M or WIT400M [21]) and optimizes the model. Instead, our CiT incorporates dynamic data curation into training in two loops: (i) an outer curation loop improving data (for downstream tasks) given the current model; (ii) an inner loop op-timizing the model given the curated data. The trained text model connects the loops by providing embeddings for curation. and LAION[24, 23]). Nevertheless, research is typically tied to the static datasets or model weights (if the data is not released) and is not able to access or change the data pipelines or model architectures. Further, work is limited by the prohibitive cost of training on these large image-text datasets (e.g. the CLIP model is trained on WIT400M for 12 days using 256 GPUs).
In this work, our goal is to empower training with the ca-pability of adjusting the data distribution. Our intention is to dynamically curate the data during training and our key idea is to use the learned text representation of vision-language models to measure relevance of the data w.r.t. the task of in-terest. Given metadata (from downstream tasks e.g. a class name such as “chicken”), we measure its embedding simi-larity to the training data. This similarity can guide us for the decision of including this data into our training process.
For example a caption containing the word “giraffe” will
have higher embedding similarity to “chicken” than a cap-tion such as “throwback Thursday”.
Driven by this idea, we presents a simple algorithm that incorporates data Curation in Training (CiT), aiming at im-proving both data efﬁciency and model performance. CiT works as follows. Given a large source of image-text pairs and metadata (e.g. a list of class names used in this paper),
CiT alternatively performs curation of the data and training on that curated data. As shown in Figure 1, CiT contains two loops: an outer loop to curate data given the current model and an inner loop trains the model given the curated data. Similar as Locked image Tuning (LiT [38]), CiT uses pre-trained image and text encoders and freezes the image one. The text model connects the two loops by serving cu-rated data to inner loop for training which in turn learns good representations for the outer loop for curation.
CiT can speed up training by multiple orders of mag-nitude, especially if the raw data size is large; e.g. when trained on LAION-400M data, CiT reaches similar Ima-geNet zero-shot1 accuracy as OpenCLIP [31], while being 37.7 faster in training. Since CiT changes the training data distribution that focuses on one or more tasks of in-terest, it can even handle image-text pairs from any (noisy) source with unknown distribution. Our experiments reveal that vanilla CLIP/LiT training fails on raw random image-text pairs crawled from the web, while CiT trains easily.
⇥ 2.