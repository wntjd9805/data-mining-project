Abstract
Point clouds are naturally sparse, while image pixels are dense. The inconsistency limits feature fusion from both modalities for point-wise scene flow estimation. Previ-ous methods rarely predict scene flow from the entire point clouds of the scene with one-time inference due to the mem-ory inefficiency and heavy overhead from distance calcula-tion and sorting involved in commonly used farthest point sampling, KNN, and ball query algorithms for local fea-ture aggregation. To mitigate these issues in scene flow learning, we regularize raw points to a dense format by storing 3D coordinates in 2D grids. Unlike the sampling operation commonly used in existing works, the dense 2D representation 1) preserves most points in the given scene, 2) brings in a significant boost of efficiency, and 3) elimi-nates the density gap between points and pixels, allowing us to perform effective feature fusion. We also present a novel warping projection technique to alleviate the informa-tion loss problem resulting from the fact that multiple points could be mapped into one grid during projection when com-puting cost volume. Sufficient experiments demonstrate the efficiency and effectiveness of our method, outperforming the prior-arts on the FlyingThings3D and KITTI dataset.
Our source codes will be released on https://github. com/IRMVLab/DELFlow. 1.

Introduction
Scene flow represents the point-wise motion between a pair of frames, specifically the magnitude and direction of the 3D motion. As a low-level task, 3D scene flow estima-tion is beneficial to numerous high-level scene understand-ing tasks in autonomous driving, such as LiDAR odometry
[46], object tracking [47], and semantic segmentation [18].
*Corresponding Authors. The first two authors contributed equally. (a) 3D grid-based methods.
Figure 1. Comparison of current point cloud processing frame-works. (b) 3D point-based meth-ods. (c) 2D perspective grid-based methods. (d) Our outlier-aware point-based methods, where outliers are filtered out.
Early works [7, 31] convert point cloud into 3D voxel grids and process them with 3D Convolutional Neural Net-works (CNNs). However, the computational costs of such voxel-based methods grow drastically with the resolution.
Besides voxelization (Fig. 1a), recent studies [9, 17, 51, 43] resort to learning scene flow directly from the raw coordi-nates of point clouds in 3D space. These point-based meth-ods [17, 51, 45, 40] (Fig. 1b) relies on feature aggregation from neighborhood points with a PointNet [28, 29] struc-ture. Nevertheless, the most commonly used K Nearest
Neighbors (KNN) and ball query algorithms in solutions
[9, 17, 51, 41] require frequent distance calculation and sorting between point clouds, which are memory-inefficient and time-consuming. Therefore, only a limited number of points are taken as inputs because large-scale input point clouds indicate more GPU memory consumption. The in-efficiency led to the proposal of projection-based methods
[4, 48, 49], where 2D convolutions are applied to the pro-jected point cloud on image plane. As shown in Fig. 1c, a convolution kernel is used to perform feature extraction among nine neighbors. Such 2D perspective grid-based methods suffer from the neighboring outliers and are not flexible since effective 3D methods cannot be applied.
To deal with the heavy computational cost of 3D meth-ods and the limited accuracy of 2D methods, we propose a projection-based framework for scene flow learning from dense point clouds. We store raw 3D point clouds (n × 3) in the corresponding 2D pixels by projecting them onto im-age plane (H × W × 3). Such representation allows us to take the complete point clouds from input scene. For the second stage, we propose Kernel Based Grouping to capture 3D geometric information on 2D grid. The oper-ations are conducted on 2D grids, but the output is in the 3D space. Different from 3D point-based methods, we are able to process point clouds with less memory consump-tion as more prior spatial information from the 2D format reduces the local query complexity. By reducing the group-ing range in search of neighboring points, our proposed network achieves single-frame feature extraction and inter-frame correlation for tens of thousands of LiDAR point clouds. In addition, the far-away points can be removed us-ing our outlier-aware method (Fig. 1d), solving the problem of 2D methods caused by outliers.
The second challenge pertains to the feature fusion be-tween point clouds and images, as raw point clouds in 3D space are naturally sparse, unordered and unstructured, while pixels of images are dense and adjacent to each other.
CamLiFlow [16] projects a small number of sparse points onto image plane to fuse features from these two modalities.
Instead, we take all points as input using the dense format of projected point clouds, which eliminates the density gap between sparse points and dense pixels, enabling effective feature fusion. As a result, we explore the potential of at-tentive feature fusion between dense points and pixels for more accurate scene flow prediction.
In addition, we adopt a warping operation in cost vol-ume to refine the predicted flow. During the refinement pro-cess, multiple warped points can be projected into the same grid on 2D plane. Previous methods merge extra points, but cause information loss which affects the final accuracy. On the contrary, our proposed cost volume module equipped with a novel warping projection technique enables us to avoid such information loss by using the warped coordi-nates as an intermediate indexing variable.
Overall, our contributions are as follows:
• We propose an efficient scene flow learning framework operated on projected point clouds to process points in 3D space, which takes the entire point clouds as input and predicts flow with one-time inference.
• We present a novel cost volume module with a warping projection technique, allowing us to compute the cost volume without information loss caused by merging points during the refinement process.
• We design a pixel-point feature fusion module, which encodes color information from images to guide the decoding of point-wise motion in point clouds, im-proving the accuracy of scene flow estimation. 2.