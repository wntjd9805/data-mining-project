Abstract
The vision-based perception for autonomous driving has undergone a transformation from the bird-eye-view (BEV) representations to the 3D semantic occupancy. Compared with the BEV planes, the 3D semantic occupancy further provides structural information along the vertical direction.
This paper presents OccFormer, a dual-path transformer network to effectively process the 3D volume for semantic occupancy prediction. OccFormer achieves a long-range, dynamic, and efficient encoding of the camera-generated 3D voxel features. It is obtained by decomposing the heavy 3D processing into the local and global transformer path-ways along the horizontal plane. For the occupancy de-coder, we adapt the vanilla Mask2Former for 3D seman-tic occupancy by proposing preserve-pooling and class-guided sampling, which notably mitigate the sparsity and class imbalance. Experimental results demonstrate that Oc-cFormer significantly outperforms existing methods for se-mantic scene completion on SemanticKITTI dataset and for
LiDAR semantic segmentation on nuScenes dataset. Code is available at https://github.com/zhangyp15/
OccFormer. 1.

Introduction
The accurate perception of 3D surroundings constitutes the foundation of modern autonomous driving systems.
Though LiDAR-based methods [23, 44, 62, 43, 39, 53], with explicit depth measurements, have been dominating the leading performance on public datasets [15, 3, 46, 2], vision-based approaches still offer advantages in terms of cost-effectiveness, stability, and generality. The past years have witnessed the prosperity of Bird-Eye-View represen-tations for vision-based 3D perception. With the multi-view camera images as input, various attempts for 2D-to-3D transformation [38, 30, 19, 28] have been proposed for applications including 3D object detection [19, 30, 33], se-mantic map construction [38, 60, 42, 37], and motion pre-*Corresponding author. diction [18, 1, 58]. Considering these tasks require either rigid bounding boxes or BEV-oriented predictions, the col-lapse of 3D scenes into 2D ground planes has demonstrated an excellent trade-off between performance and efficiency.
However, the holistic understanding of the 3D scene, es-pecially for real-world obstacles with variable shapes, can hardly be recovered with the condensed BEV feature maps.
To this end, this paper focuses on building a fine-grained 3D representation, namely 3D semantic occupancy, for the surrounding environment with multi-view images.
The task of 3D semantic occupancy prediction aims to reconstruct the surrounding 3D environment with fine-grained geometry and semantics, which is also known as 3D semantic scene completion when the LiDAR point cloud is taken as input. For the driving scenes, most existing methods [43, 11, 8, 25, 50] still rely on the expensive Li-DAR sensors for explicit depth measurements. The seminar work MonoScene [4] proposed the first monocular frame-work for 3D semantic occupancy prediction. It first con-structs the 3D feature with sight projection and then pro-cesses it with a classical 3D UNet. However, the 3D con-volution suffers from several limitations. First, it reasons the semantics within a relatively fixed receptive field, while different semantic classes may distribute following various patterns. Also, its spatial invariance cannot well process the sparse and discontinuous 3D features, generated from the state-of-the-art practices for image-to-3D transforma-tion [38, 19, 28]. Finally, the 3D convolution filters can consume massive parameters. Therefore, we believe a long-range, dynamic, and efficient method for encoding 3D fea-tures is needed to pave the way.
Inspired by the widespread success of vision transform-ers [13, 34] in various vision tasks [5, 59, 16, 54, 34, 29], we are motivated to utilize the attention mechanism for building the encoder-decoder network for 3D semantic oc-cupancy prediction. For the encoder part, we propose the dual-path transformer block to unleash the capacity of self-attention while limiting the quadratic complexity. Specifi-cally, the local path operates along each 2D BEV slice with the shared windowed attention to capture the fine-grained
details, while the global path performs on the collapsed
BEV feature to obtain scene-level understanding. Finally, the dual-path outputs are adaptively fused to generate the output 3D feature volume. The dual-path designs appropri-ately break down the challenging processing of 3D feature volumes and we demonstrate its clear advantage over the classic 3D convolutions. For the decoder part, we are the first to adapt the state-of-the-art method Mask2Former [9] for 3D semantic occupancy prediction. We further pro-pose to use max-pooling rather than the default bilinear for computing the masked regions for attention, which can better preserve the minor classes. Additionally, the class-guided sampling is proposed to capture the foreground ar-eas for more effective optimization. Experimental results demonstrate the superiority of OccFormer over existing state-of-the-art methods. For 3D semantic scene comple-tion on SemanticKITTI [2] dataset, OccFormer outperforms
MonoScene by 1.24% mIoU, which makes an 11% relative improvement and ranks first on the test leaderboard among all monocular methods. We also evaluate OccFormer on nuScenes [3] dataset for LiDAR semantic segmentation, following TPVFormer [20]. Our method surpasses TPV-Former by 1.4% mIoU and generates more complete and realistic predictions for 3D semantic occupancy prediction. 2.