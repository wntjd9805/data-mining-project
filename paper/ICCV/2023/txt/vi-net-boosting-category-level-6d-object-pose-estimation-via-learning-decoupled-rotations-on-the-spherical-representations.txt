Abstract
Rotation estimation of high precision from an RGB-D object observation is a huge challenge in 6D object pose es-timation, due to the difficulty of learning in the non-linear space of SO(3).
In this paper, we propose a novel rota-tion estimation network, termed as VI-Net, to make the task easier by decoupling the rotation as the combination of a viewpoint rotation and an in-plane rotation. More specifi-cally, VI-Net bases the feature learning on the sphere with two individual branches for the estimates of two factorized rotations, where a V-Branch is employed to learn the view-point rotation via binary classification on the spherical sig-nals, while another I-Branch is used to estimate the in-plane rotation by transforming the signals to view from the zenith direction. To process the spherical signals, a Spherical Fea-ture Pyramid Network is constructed based on a novel de-sign of SPAtial Spherical Convolution (SPA-SConv), which settles the boundary problem of spherical signals via fea-ture padding and realizes viewpoint-equivariant feature ex-traction by symmetric convolutional operations. We apply the proposed VI-Net to the challenging task of category-level 6D object pose estimation for predicting the poses of unknown objects without available CAD models; exper-iments on the benchmarking datasets confirm the efficacy of our method, which outperforms the existing ones with a large margin in the regime of high precision. 1.

Introduction
The task of 6D object pose estimation [35, 30, 13, 16, 31] from an RGB-D object observation is to learn the trans-formation from the canonical object system to the camera system, represented by a 3D rotation R ∈ SO(3) and a 3D translation t ∈ R3. It is demanded in many real-world applications, such as robotic grasping [34, 24], augmented reality [1], and autonomous driving [15, 33, 6, 7].
*Correspondence to Kui Jia <kuijia@gmail.com>
Figure 1. An illustration of the factorization of rotation R into a viewpoint (out-of-plane) rotation Rvp and an in-plane rotation
Rip (around Z-axis). Notations are explained in Sec. 3.
For the 6D object pose, translation is easier to be esti-mated, e.g., initialized as the centroid of the object points, while learning rotation in the whole space of SO(3) is more challenging due to the non-linearity of SO(3). Things be-come more complicated when estimating rotations of un-known objects without available CAD models. Taking the task of category-level 6D object pose estimation as an ex-ample, one typical way [31, 28, 32, 4] is to learn the cor-respondence between the camera system and the canonical one to solve the poses via Umeyama algorithm[29], with the surrogate objectives of canonical coordinates rather than the true ones of object poses. Another strategy is to learn in the
SO(3) space with rotation-aware features extracted from specially designed encoders, e.g., 3D Graph Convolutional (3DGC) autoencoder [5], Spherical CNN (in the spectral domain) [9, 20], and 3D Steerable CNN [18], whose re-sults, however, are less satisfactory than those based on cor-respondence learning.
Recently, OVE6D [3] narrows the learning space of rota-tion R by factorizing it, as shown in Fig. 1, into a viewpoint rotation Rvp and an in-plane rotation Rip around the canon-ical zenith direction (the positive direction of Z-axis in Fig. 1), and renders images of object CAD models with various of discrete rotations to construct the codebook for viewpoint
rotation retrieval. However, object CAD models are often not available in real-world applications for rendering, e.g., on the focused task of category-level 6D object pose esti-mation. In this paper, we correlate the factorized rotations to the sphere, and propose a new method of VI-Net, which employs two carefully designed heads of V-Branch and I-Branch to estimate Rvp and Rip, respectively, based on spherical representations with no use of object CAD mod-els; Fig. 2 gives an illustration of VI-Net.
Specifically, given an object observation (centered at the origin with known translation), VI-Net processes its point-wise attributes as the signals assigned on the sphere, rep-resented as a 2D spherical map, and constructs a Spheri-cal Feature Pyramid Network (FPN) to extract high-level spherical features along hierarchy; on top of the Spherical
FPN, two individual heads of V-Branch and I-Branch are employed to learn the viewpoint rotation Rvp and the in-plane rotation Rip, respectively. For V-Branch, Rvp is gen-erated by learning the intersection point of the canonical zenith direction with the unit sphere, which is recognized via binary classification on the high-level spherical feature map. For I-Branch, VI-Net rotates the object through the transformation of its spherical feature map with Rvp, and could thus estimate Rip from the perspective of the canon-ical zenith direction. Finally, the rotation R is given as the multiplication of Rvp and Rip.
Another problem is how to build the Spherical FPN. For convenience, the spherical signals are processed in the form of representations with regular 2D spatial sizes, which yet results in the boundary problem. We thus propose in this pa-per a novel design of SPAtial Spherical Convolution, termed as SPA-SConv, which settles the above problem via sim-ple feature padding, and furthermore, extracts viewpoint-equivariant features through symmetric convolutional op-erations to support the feature transformation in I-Branch.
Our SPA-SConv could be flexibly adapted to the existing convolutional architectures by replacing the convolutions, and we thus construct the spherical version of FPN [21].
To confirm the efficacy of our VI-Net on rotation estima-tion, we apply it to the challenging task of category-level 6D object pose estimation. Experiments are conducted on the benchmarking REAL275 dataset [31], which show that our method outperforms the existing ones with a large margin in the regime of high precision, e.g., an improvement of 4.0% on the metric of 5◦2cm over the state-of-the-art method of
DPDN [19]. Ablation studies also confirm the efficacy of our novel designs. 2.