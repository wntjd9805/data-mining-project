Abstract
Anomaly detection in the video is an important research area and a challenging task in real applications. Due to the unavailability of large-scale annotated anomaly events, most existing video anomaly detection (VAD) methods focus on learning the distribution of normal samples to detect the substantially deviated samples as anomalies. To well learn the distribution of normal motion and appearance, many auxiliary networks are employed to extract foreground ob-ject or action information. These high-level semantic fea-tures effectively ﬁlter the noise from the background to de-crease its inﬂuence on detection models. However, the ca-pability of these extra semantic models heavily affects the performance of the VAD methods. Motivated by the impres-sive generative and anti-noise capacity of diffusion model (DM), in this work, we introduce a novel DM-based method to predict the features of video frames for anomaly de-tection. We aim to learn the distribution of normal sam-ples without any extra high-level semantic feature extrac-tion models involved. To this end, we build two denoising diffusion implicit modules to predict and reﬁne the features.
The ﬁrst module concentrates on feature motion learning, while the last focuses on feature appearance learning. To the best of our knowledge, it is the ﬁrst DM-based method to predict frame features for VAD. The strong capacity of
DMs also enables our method to more accurately predict the normal features than non-DM-based feature prediction-based VAD methods. Extensive experiments show that the proposed approach substantially outperforms state-of-the-art competing methods. The code is available atFPDM. 1.

Introduction
Video anomaly detection (VAD) aims at identifying the unusual events that rarely appear and are different from nor-mal behaviors in videos. Successfully detecting the anoma-lies, such as trafﬁc accidents, violence, and stampedes, is of great importance in ubiquitous video surveillance for pub-*Guansong Pang is the corresponding author, e-mail: pangguan-song@gmail.com
Object
Detection
Action
Recognition
Discriminator 0/1 z
Generator
GAN-based approach
Images/Features
Images
Optical Flow
Encoder
Decoder
Auto-encoder-based approach
Images/Features
Encoder
Diffusion
Diffusion
Diffusion model-based approach
Features
Images
Figure 1. Overview of three generative VAD approaches. Exist-ing state-of-the-art GAN-based or auto-encoder-based approaches heavily rely on foreground object or action information extracted by auxiliary models, such as object detection, action recognition or optical ﬂow network, to generate features/images for effective performance. By contrast, our proposed diffusion model-based ap-proach does not have this reliance and can make accurate feature prediction using only simple networks as encoder to extract basic image features as input. lic safety. However, VAD is a challenging task since the anomaly events are unbounded in real-world applications and it is difﬁcult to collect large-scale labeled data.
There have been many VAD methods [9,10,14,19,23,33, 34,39,45,46,57,62,68,71] proposed over the years to handle this issue, in which one-class learning-based methods are preferred due to the relatively accessible normal training set and their capacity to achieve better performance [44]. The one-class VAD methods assume the availability of the train-ing data in which all samples are normal, and build different models to learn the distribution of normal data. Genera-tive modeling is a widely-used technique in this line since the normal samples can be better generated than anoma-lies after training. Generative Adversarial Networks (GAN)
[22, 52, 71, 72] and Auto-Encoder (AE) [24, 29, 40, 46] are two popular frameworks. Although these generative ap-proaches achieve promising performance in VAD, there are three main challenges: (1) the GAN/AE-based methods suf-fer from the weak generative capacity, leading to more noise from the low-quality generated image, which reduces the
performance, (2) current SOTA methods often employ some auxiliary models, e.g., object detection and action recogni-tion models, to capture the features of the foreground ob-ject or action information, and as a result, the performance relies heavily on the representation capacity of these high-level semantic models, and (3) the anomaly events are often characterized by the novel appearance and/or abnormal mo-tion, which increase the difﬁculty of the generative models to capture the normality/abnormality in both aspects.
In very recent years, diffusion models (DMs) [26, 53], have been attracting increasing attention due to their pow-erful generative capacity and excellent performance in var-ious tasks [5, 17, 49, 53]. Different from GAN and AE, the diffusion models inject Gaussian noise into the training data and then learn to recover the samples from those noisy data.
The DMs are featured by minor modiﬁcations and rectiﬁca-tion of the generated samples in each step, enabling a more stable generation of more realistic samples. Therefore, the
DM-based approaches outperform the GAN/AE-based ap-proaches in many generative tasks [6, 12, 50, 63].
Motivated by the powerful generative capacity of these diffusion models, we propose a novel DM-based approach for anomaly detection. For the second and third challenges mentioned above, as shown in Fig. 1, we devise two com-plementary denoising DM modules to learn the distribution of normal samples. One module emphasizes learning the distribution of motion, while another module focuses on ap-pearance learning. We employ a simple neural network as the encoder for extracting basic 2D features. Many previous studies [21, 49] have shown that many simple pre-trained networks can effectively extract basic texture information of images even if there are novel classes beyond the train-ing data. Therefore, we adopt a basic 2D feature extractor, which is different from many existing generative methods that utilize high-level semantic models for 3D feature ex-traction. Based on this, our goal is to learn the distribution of normal features and predict the features of samples for anomaly detection.
In summary, there are three main contributions:
• We introduce a novel diffusion model-based method to predict the features of each sample for VAD. To the best of our knowledge, it is the ﬁrst work in utilizing
DMs for VAD.
• We design two types of DDIM module for respective motion and appearance learning from the normal sam-ples to guarantee the generative quality of predicted features.
• The proposed model takes 2D images as input with no auxiliary semantic networks, while achieving a highly comparable performance to the methods utilizing high-level 3D semantic features.
Experimental results on four publicly available video anomaly detection datasets demonstrate that our method substantially outperforms the image feature-based VAD counterparts and performs comparably well to methods us-ing 3D semantic features. 2.