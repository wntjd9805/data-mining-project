Abstract
Model quantization, which aims to compress deep neu-ral networks and accelerate inference speed, has greatly facilitated the development of cumbersome models on mo-bile and edge devices. There is a common assumption in quantization methods from prior works that training data is available.
In practice, however, this assumption can-not always be fulfilled due to reasons of privacy and se-curity, rendering these methods inapplicable in real-life sit-uations. Thus, data-free network quantization has recently received significant attention in neural network compres-sion. Causal reasoning provides an intuitive way to model causal relationships to eliminate data-driven correlations, making causality an essential component of analyzing data-free problems. However, causal formulations of data-free quantization are inadequate in the literature. To bridge this gap, we construct a causal graph to model the data genera-tion and discrepancy reduction between the pre-trained and quantized models.
Inspired by the causal understanding, we propose the Causality-guided Data-free Network Quan-tization method, Causal-DFQ, to eliminate the reliance on data via approaching an equilibrium of causality-driven in-tervened distributions. Specifically, we design a content-style-decoupled generator, synthesizing images conditioned on the relevant and irrelevant factors; then we propose a discrepancy reduction loss to align the intervened distribu-tions of the pre-trained and quantized models. It is worth noting that our work is the first attempt towards introduc-ing causality to data-free quantization problem. Extensive experiments demonstrate the efficacy of Causal-DFQ. The code is available at Causal-DFQ. 1.

Introduction
There have been significant advances in deep learning models in the fields of computer vision [9, 14] and nat-ural language processing [33, 45]. To accommodate the increasing demand for equipping cumbersome models on resource-constrained edge devices, researchers have pro-*Corresponding author posed several network quantization methods [18, 57], in which high-precision parameters are converted into low-precision ones. To mitigate the performance degradation induced by model quantization, fine-tuning approaches are extensively studied to optimize quantized models on the full training datasets [19, 41, 42, 49]. However, original training data is sometimes inaccessible in real-world situations due to the privacy and security concerns. A patient’s electronic health record, for instance, is typically inaccessible because the information contained is private. Hence, the fine-tuning methods requiring training data are no longer applicable in such real-life scenarios.
To address this issue, researchers have proposed data-free quantization to quantize models without requiring ac-cess to real data [1, 2, 24, 43, 51, 54]. For example, Ze-roQ [2] is proposed to generate ‘optimal’ fake data, which learns an input data distribution to best match the batch normalization statistics of the FP32 model. Nevertheless, most data-free quantization methods attempt to reconstruct the original data from the pre-trained model utilizing prior statistical distribution information of the underlying data, such as BNS [2, 51, 52], Dirichlet distribution [28] and cat-egory information [3]. However, those methods ignore a powerful tool in the human cognition, i.e., causal reason-ing, which commonly aids humans in learning without re-lying upon data collection. Human cognitive systems are immune to the data deficiency because humans are more sensitive to causal relations than data-driven statistical asso-ciations [10, 55]. Using causal language, causal reasoning can extract causal relationship from the pre-trained models and ignore irrelevant factors by interventions [34].
There are two significant challenges that need to be over-come before causality can be introduced to eliminate the reliance on data during the quantized model training. First, constructing an informative causal graph is the fundamental premise for causal reasoning [30,34], but how causal graphs should be constructed in a data-free situation is still inade-quate in the literature. Second, using causal language to formalize data generation and network alignment is the key to connecting causality with data-free quantization, but it also remains unsolved. These two challenges are the funda-mental obstacles that prevent us from employing causality 1
in data-free quantization.
To address these challenges, we construct a causal graph to model the data-free quantization process, including data-generation and discrepancy reduction mechanisms, where the irrelevant factors in the pre-trained models are taken into consideration. Based on the causal graph, we pro-pose a novel Causality-Guided Data Free Network Quan-tization method, Causal-DFQ, to remove the reliance on data during quantized model training. Specifically, we de-sign a content-style-decoupled generator, synthesizing im-ages conditioned on the relevant and irrelevant factors (con-tent and style variables). Then we propose a discrepancy reduction loss to align the intervened distributions of the outputs from pre-trained and quantized models.
Overall, the contributions of this paper are four-fold: (i)
We provide a causal perspective on data-free quantization, which is the first attempt towards using causality to facili-tate data-free network compression; (ii) To leverage causal-ity to facilitate data-free quantization, we construct a causal graph to model data generation process and discrepancy re-duction process in data-free quantization mechanism; (iii)
We propose a novel quantization method called Causality
Guided Data-free Network Quantization, Causal-DFQ, in which we generate fake images conditioned on style and content variables, and align style-intervened distributions of pre-trained and quantized models. (iv) Extensive exper-iments demonstrate that the proposed method can signifi-cantly improve the performance of data-free low-bit mod-els. Importantly, it is the first method where data-free fine-tuned models outperform the models fine-tuned with data on the ImageNet. 2.