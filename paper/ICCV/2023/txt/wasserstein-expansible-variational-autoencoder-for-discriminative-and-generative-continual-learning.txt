Abstract
Task-Free Continual Learning (TFCL) represents a chal-lenging learning paradigm where a model is trained on the non-stationary data distributions without any knowledge of the task information, thus representing a more practical approach. Despite promising achievements by the Varia-tional Autoencoder (VAE) mixtures in continual learning, such methods ignore the redundancy among the probabilis-tic representations of their components when performing model expansion, leading to mixture components learning similar tasks. This paper proposes the Wasserstein Ex-pansible Variational Autoencoder (WEVAE), which evalu-ates the statistical similarity between the probabilistic rep-resentation of new data and that represented by each mix-ture component and then uses it for deciding when to ex-pand the model. Such a mechanism can avoid unneces-sary model expansion while ensuring the knowledge diver-sity among the trained components. In addition, we pro-pose an energy-based sample selection approach that as-signs high energies to novel samples and low energies to the samples which are similar to the model’s knowledge.
Extensive empirical studies on both supervised and unsu-pervised benchmark tasks demonstrate that our model out-performs all competing methods. The code is available at https://github.com/dtuzi123/WEVAE/. 1.

Introduction
The variational Autoencoder (VAE) [33] is one of the most popular deep generative models, underlined by a sym-metric network structure in which an input x is transferred into a pseudo-similar data x′ through an encoding-decoding process. Due to its powerful inference mechanisms, the
VAE has been successfully used in many applications, in-cluding few-shot learning [54], semi-supervised learning
[1], image synthesis [41], image-to-image translation [40], and density estimation [59]. However, using the VAE model in continual learning has not been sufficiently investigated so far. Similar to other deep neural networks in contin-ual learning, VAEs suffer from a significant performance loss when continually learning new data domains, which is known as catastrophic forgetting [45].
Constructing a fixed-length memory buffer that stores some past samples and then replays them during the sub-sequent learning stages [14, 9] was shown to relieve catas-trophic forgetting in continual learning. Meanwhile, other approaches focus on regulating the optimization procedure of the model by imposing a penalty term in the objective function for freezing certain parameters, [34, 39]. These approaches usually train a static model with a fixed capac-ity and cannot achieve good performance when learning a growing number of tasks [71]. The dynamic expansion model [52, 71] builds new hidden layers to handle incoming tasks, showing promising results in continual learning due to its scalability and generalization performance. However, despite their impressive performance in continual learning, most dynamic expansion methods still require the knowl-edge of the task boundaries, which limits their applicability in a more realistic scenario such as the Task-Free Continual
Learning (TFCL) [6].
The dynamic expansible VAE framework was recently shown to provide good performance in TFCL, [48, 73]. The
Continual Unsupervised Representation Learning (CURL)
[48] dynamically builds new VAE inference models to cap-ture the distribution shift over time. CURL relieves forget-ting by retraining a generator (decoder) to reproduce past samples, inevitably leading to forgetting, [71]. A similar idea was proposed in [38], which uses the Dirichlet pro-cess for the component expansion in a mixture of VAEs in a method called the Continual Neural Dirichlet Process Mix-ture (CN-DPM). Unlike CURL, CN-DPM does not rely on the generative replay mechanism (GRM) [84] and therefore can preserve the best information for all previously learned samples. More recently, the dynamic VAE mixture model was upgraded by using the Online Cooperative Memoriza-tion (OCM) [73], which manages two cooperative memory buffers to preserve both the short- and long-term informa-tion. However, none of these models have theoretical guar-antees for their expansion mechanisms. Moreover, they do
not explicitly design a mechanism for ensuring the informa-tion diversity among the VAE mixture components.
In this paper, we propose a new approach called the Wasserstein Expansible Variational Autoencoder (WE-VAE), aiming to learn a compact dynamic expansible VAE model for TFCL, which evaluates the knowledge corre-lation between the given information and the previously learned components, as an expansion criterion. The primary motivation for this expansion mechanism is to promote the knowledge diversity among mixture’s components.
For ensuring a stable expansion process, we evaluate the
Wasserstein distance for representing the dynamic change of the correlations between the currently trained and all pre-viously learnt components over time. The proposed mech-anism avoids the frequent expansion caused by outliers in the data, while ensuring the information diversity among components. In addition, autoencoders have naturally been employed in the Energy-Based Model (EBM) [85] aiming to learn an energy map in which the low energy is attributed to the data manifold [85]. Inspired by the EBM, we propose an energy-based sample selection approach, which aims to assign low energies to the samples that share similar infor-mation to that already learnt by the model.
Empirical validations show that the proposed Wasser-stein Expansible Variational Autoencoder (WEVAE) can train statistically diverse components while outperforming the state-of-the-art using a compact structure. Our contri-butions consists of : (1) We propose a new model, WEVAE, for TFCL, which dynamically expands its capacity through the proposed Wasserstein expansion mechanism, ensuring the information diversity among components; (2) We for-mulate the dynamic expansion as the evaluation of time se-ries data to avoid frequent expansion. This is the first work that employs the time series analysis for model expansion under TFCL; (3) We propose an energy-based sampling ap-proach to manage the memory buffer, which can further promote knowledge diversity among model’ s components.
This is the first work employing the energy function for sample selection in TFCL; (4) We provide theoretical guar-antees for the proposed expansion mechanism and analyze the forgetting behaviour of the proposed WEVAE. 2.