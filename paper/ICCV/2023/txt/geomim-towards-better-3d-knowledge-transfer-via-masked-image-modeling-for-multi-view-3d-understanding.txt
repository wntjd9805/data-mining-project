Abstract
Multi-view camera-based 3D detection is a challenging problem in computer vision. Recent works leverage a pre-trained LiDAR detection model to transfer knowledge to a camera-based student network. However, we argue that there is a major domain gap between the LiDAR BEV features and the camera-based BEV features, as they have different char-acteristics and are derived from different sources. In this paper, we propose Geometry Enhanced Masked Image Mod-eling (GeoMIM) to transfer the knowledge of the LiDAR model in a pretrain-finetune paradigm for improving the multi-view camera-based 3D detection. GeoMIM is a multi-camera vision transformer with Cross-View Attention (CVA) blocks that uses LiDAR BEV features encoded by the pre-trained BEV model as learning targets. During pretraining,
GeoMIM’s decoder has a semantic branch completing dense perspective-view features and the other geometry branch re-constructing dense perspective-view depth maps. The depth branch is designed to be camera-aware by inputting the camera’s parameters for better transfer capability. Exten-sive results demonstrate that GeoMIM outperforms existing methods on nuScenes benchmark, achieving state-of-the-art performance for camera-based 3D object detection and 3D segmentation. 1.

Introduction
Multi-view camera-based 3D detection is an emerging critical problem in computer vision [20, 46, 47, 19, 28, 29, 36, 26, 31, 32, 40, 41]. To improve the detection perfor-mance, recent works [9, 27, 21] often choose to use a pre-trained LiDAR model as the teacher and transfer its knowl-edge to a camera-based student network. Various techniques, such as LIGA-Stereo [13], CMKD [17], and BEVDistill [8], have been proposed to leverage the rich geometry informa-tion of the LiDAR model’s BEV (bird’s eye view) features.
Utilizing a pretrained LiDAR model to provide auxiliary supervision has become a widely adopted design that can (cid:0) Corresponding author. 4CPII under InnoHK
Pretrain
Supervision
Finetune
Finetune +
LiDAR BEV
SL [33]
SSL [30]
GeoMIM BEV Feature
Classes
RGB Pixels 40.6 44.3 47.2 41.7 43.9 45.4
Table 1: The effects of LiDAR BEV feature distillation on ImageNet-pretrained (SL), self-supervised (SSL), and our GeoMIM pretraining-finetuning settings for BEVDet in nuScenes 3D detection. Naively distilling LiDAR BEV features in finetuning introduces domain gaps and harms the performance when the pretrianed model is powerful enough. enhance the performance of camera-based models. How-ever, we contend that this design is not optimal due to a significant domain gap between the BEV features of the
LiDAR model and those of the camera-based model. This domain gap arises from the 3D and sparse characteristics of LiDAR point clouds compared to the dense 2D images captured by the camera. Additionally, the LiDAR model’s
BEV features are grounded in ground truth depth, while those of the camera-based model are typically inferred from 2D images, a problem that is often ill-posed. We empirically demonstrate their domain gap with a pilot study as shown in Tab. 1. We find that utilizing a LiDAR teacher to pro-vide auxiliary supervision can indeed improve an ImageNet-pretrained [33] camera-based model, but is unable to im-prove a stronger camera-based model initialized by recent powerful self-supervised pretraining. In other words, di-rectly utilizing the pretrained LiDAR model to distill the final camera-based model might not be an optimal design and does not necessarily lead to performance gain.
To better take advantage of the LiDAR model, in this paper, we propose Geometry Enhanced Masked Image Mod-eling (GeoMIM) to transfer the knowledge of the LiDAR model in a pretrain-finetune paradigm for improving the multi-view camera-based 3D detection.
It is built upon a multi-camera vision transformer with Cross-View Atten-tion (CVA) blocks and enables perspective-view (PV) repre-sentation pretraining via BEV feature reconstruction from masked images. Specifically, during pretraining, we parti-tion the training images into patches and feed a portion of them into the encoder following Masked Autoencoder [14].
Our GeoMIM decoder then uses these encoded visible to-kens to reconstruct the pretrained LiDAR model’s BEV fea-ture in the BEV space instead of commonly used RGB pix-els [49, 14, 30] or depth points [3] as in existing MAE frame-works. To achieve this PV to BEV reconstruction, we first devise two branches to decouple the semantic and geometric parts, with one branch completing dense PV features and the other reconstructing the depth map. The dense PV features can then be projected into the BEV space with the depth distribution following Lift-Splat-Shoot (LSS) [37]. We fur-ther equip the two branches with the proposed CVA blocks in their intermediate layers to allow each patch to attend to tokens in other views. It enhances the decoder’s capabil-ity of joint multi-view inference which is especially critical for BEV feature reconstruction. Finally, the depth branch is designed to be camera-aware with the additional encod-ing of cameras’ parameters as input, making the pretrained
GeoMIM better adapt to downstream tasks with different cameras.
To demonstrate the effectiveness of GeoMIM, we fine-tune the pretrained backbone to conduct multi-view camera-based 3D detection and 3D segmentation on the nuScenes [7] dataset. We achieve state-of-the-art results of 64.4 NDS (NuScenes Detection Score) and 70.5 mIoU (mean intersec-tion over union) for 3D detection and segmentation on the
NuScenes test set, which are 2.5% and 1.1% better than previously reported best results [36, 22]. Additionally, we verify that the backbone pretrained on nuScenes dataset can be successfully transferred to Waymo Open dataset [42], im-proving the mAP (mean average precision) of the ImageNet-initialized 3D detector by 6.9%. 2.