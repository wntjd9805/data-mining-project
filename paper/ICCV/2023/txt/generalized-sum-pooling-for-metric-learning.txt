Abstract
A common architectural choice for deep metric learn-ing is a convolutional neural network followed by global average pooling (GAP). Albeit simple, GAP is a highly effective way to aggregate information. One possible explanation for the effectiveness of GAP is considering each feature vector as representing a different seman-tic entity and GAP as a convex combination of them.
Following this perspective, we generalize GAP and pro-pose a learnable generalized sum pooling method (GSP).
GSP improves GAP with two distinct abilities: i) the ability to choose a subset of semantic entities, effec-tively learning to ignore nuisance information, and ii) learning the weights corresponding to the importance of each entity. Formally, we propose an entropy-smoothed optimal transport problem and show that it is a strict generalization of GAP, i.e., a specific realization of the problem gives back GAP. We show that this optimization problem enjoys analytical gradients enabling us to use it as a direct learnable replacement for GAP. We further propose a zero-shot loss to ease the learning of GSP.
We show the effectiveness of our method with extensive evaluations on 4 popular metric learning benchmarks.
Code is available at: GSP-DML Framework 1.

Introduction
Distance metric learning (DML) addresses the prob-lem of finding an embedding function such that the semantically similar samples are embedded close to each other while the dissimilar ones are placed rel-atively apart in the Euclidean sense. Although the prolific and diverse literature of DML includes various architectural designs [9, 24, 33], loss functions [39], and data-augmentation techniques [47, 55], many of these methods have a shared component: a convolutional neu-ral network (CNN) followed by a global pooling layer, mostly global average pooling (GAP) [39].
Common folklore to explain the effectiveness of GAP
†Affiliated with OGAM-METU during the research. is considering each pixel of the CNN feature map as corresponding to a separate semantic entity [14]. For example, spatial extent of one pixel can correspond to a "tire" object making the resulting feature a represen-tation for "tireness" of the image. If this explanation is correct, the representation space defined via output of
GAP is a convex combination of semantically indepen-dent representations defined by each pixel in the feature map. Although this folklore is later empirically studied in [64,70,71, and references therein] and further verified for classification in [13, 62], its algorithmic implications are not clear. If each feature is truly representing a different semantic entity, should we really average over all of them? Surely, some classes belong to the back-ground and should be discarded as nuisance variables.
Moreover, is uniform average of them the best choice?
Aren’t some classes more important than others? In this paper, we try to answer these questions within the context of metric learning. We propose a learnable and generalized version of GAP which learns to choose the subset of the semantic entities to utilize as well as weights to assign them while averaging.
In order to generalize the GAP operator to be learn-able, we re-define it as a solution of an optimization problem. We let the solution space to include 0-weight effectively enabling us to choose subset of the features as well as carefully regularize it to discourage degen-erate solution of using all the features. Crucially, we rigorously show that the original GAP is a specific case of our proposed optimization problem for a certain re-alization. Our proposed optimization problem closely follows optimal transport based top-k operators [6] and we utilize its literature to solve it. Moreover, we present an algorithm for an efficient computation of the gradi-ents over this optimization problem enabling learning.
A critical desiderata of such an operator is choosing sub-set of features which are discriminative and ignoring the background classes corresponding to nuisance variables.
Although supervised metric learning losses provide guid-ance for seen classes, they carry no such information to generalize the behavior to unseen classes. To enable such a behavior, we adopt a zero-shot prediction loss
as a regularization term which is built on expressing the class label embeddings as a convex combination of attribute embeddings [7, 62].
In order to validate the theoretical claims, we design a synthetic empirical study. The results confirm that our pooling method chooses better subsets and improve generalization ability. Moreover, our method can be applied with any DML loss as GAP is a shared com-ponent of them. We applied our method on 6 DML losses and test on 4 datasets. Results show consistent improvements with respect to direct application of GAP as well as other pooling alternatives. 2.