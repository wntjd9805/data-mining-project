Abstract
View Transformation Module (VTM), where transfor-mations happen between multi-view image features and
Bird-Eye-View (BEV) representation, is a crucial step in camera-based BEV perception systems. Currently, the two most prominent VTM paradigms are forward projection and backward projection. Forward projection, represented by
Lift-Splat-Shoot, leads to sparsely projected BEV features without post-processing. Backward projection, with BEV-Former being an example, tends to generate false-positive
BEV features from incorrect projections due to the lack of utilization on depth. To address the above limitations, we propose a novel forward-backward view transformation module. Our approach compensates for the deficiencies in both existing methods, allowing them to enhance each other to obtain higher quality BEV representations mutu-ally. We instantiate the proposed module with FB-BEV, which achieves a new state-of-the-art result of 62.4% NDS on the nuScenes test set. Code and models are available at https://github.com/NVlabs/FB-BEV .
BEV-based 3D detection models have gained popularity due to their unified and comprehensive representation abili-* Work done during an internship at NVIDIA. ties for multi-camera inputs, enhancing the performance of both vision-only and multi-modality perception models for autonomous driving [1–9]. A typical BEV-based detection model comprises an image backbone, a View Transforma-tion Module (VTM), and a detection head. The VTMs pri-marily function to project multi-view camera features onto the BEV plane. There are two main categories of existing mainstream VTMs based on the projection methods used: forward projection and backward projection. 1.

Introduction
Forward projection. The most intuitive method for pro-jecting camera features onto the BEV plane involves esti-mating the depth value of each pixel in the image and using the camera calibration parameters to determine the corre-sponding position of each pixel in 3D space [10], as shown in Figure 1 (left). We refer to this process as forward pro-jection, where the 2D pixels take the initiative in projection and the 3D space passively accepts features from the im-ages. The accuracy of the predicted depth for each pixel is critical to achieving high-quality BEV features. However, accurately estimating the depth value of each pixel is chal-lenging [11]. To address this challenge, Lift-Splat-Shoot (LSS) pioneered the use of depth distribution to model the uncertainty of each pixel’s depth [1]. One limitation of 1
(a) (b) (a) (b)
Figure 2: (a) Projection points on BEV plane with forward projection on nuScenes dataset, and different colors repre-(b) BEV feature map of LSS [1] sent different cameras. with a shape of 200×200. We can observe that forward pro-jection has an extremely low utilization rate for BEV space.
Figure 3: (a) Detection of BEVFormer. (b) The correspond-ing BEV features of BEVFormer. Since BEVFormer cannot use depth for distinction, the features of each object on the
BEV tend to be ray-shaped. The model thus predicts multi-ple boxes for one object along the longitudinal direction.
LSS is that it generates discrete and sparse BEV represen-tation [1, 12]. As shown in Figure 2, the density of BEV features decreases with distance. When using the default settings of LSS on the nuScenes dataset, only 50% of the grids can receive valid image features through projection.
Backward projection. The motivation behind backward projection is opposite to that of forward projection. For the backward projection paradigm, the points in 3D space take the initiative [2, 3, 8, 13]. For instance, BEVFormer sets the coordinates of the 3D space to be filled in advance and then projects these 3D points back onto the 2D image [3], as shown in Figure 1 (middle). As a result, each predefined 3D space position can obtain its corresponding image fea-tures. The BEV representation obtained by this method is denser than that of LSS, with each BEV grid filled with the corresponding image features.
The drawbacks of backward projection are also appar-ent, as shown in Figure 3. Although yielding a denser BEV representation, it comes at the cost of establishing numer-ous false correspondences between 3D and 2D space due to occlusion and depth mismatch [14]. The absence of depth information during the projection process is the main cause.
Without depth as a reference, each 3D coordinate on the ray is equally related to the same 2D coordinate, equivalent to having a uniform depth distribution for this pixel in forward projection. As a result, the distance prediction of the objects along the longitudinal direction become ambiguous. Back-ward projection thus tends to be inferior to forward projec-tion in depth utilization. Recently, the advantage of forward projection has been further highlighted since more accurate depth distribution obtained from depth supervision is shown to improve 3D perception [15, 16].
Considering the pros and cons discussed above, we pro-pose forward-backward view transformation to address the limitations of existing VTMs, as shown in Figure 1 (right).
To address the issue of sparse BEV representations in for-ward projection, we leverage backward projection to re-fine the sparse region from forward projection. Meanwhile, backward projection is prone to false-positive features due to the lack of depth guidance. We thus propose a depth-aware backward projection design to suppress false-positive features by measuring the quality of each projection re-lationship through depth consistency. The depth consis-tency is determined by the distance of depth distributions between a 3D point and its corresponding 2D projection point. Using this depth-aware method, unmatched projec-tions are given lower weights, which reduces the interfer-In addition, ence caused by false-positive BEV features. for the objection detection task, we only care about fore-ground objects, so we densify only the foreground regions of the BEV plane while using backward projection. This not only reduces the computational burden but also avoids the introduction of false-positive features in the background ar-eas. With the sparse regions refined for forward projection and false-positives features reduced for backward projec-tion, our forward-backward projection not only solves the defects of existing projection methods but also realizes the effective ensemble of existing projection methods. Our con-tributions can be summarized as follows:
• We propose a forward-backward projection strategy that generates dense BEV features with strong representa-tion ability through bidirectional projection. Our ap-proach addresses the limitations of existing projection methods, which result in either sparse BEV features or false-positive features caused by inaccurate projection.
• To address the pitfalls of existing forward projection methods for producing sparse BEV representations, we employ backward projection to refine the blank grid that not be activated by forward projection. This makes the model more suitable for large-scale BEVs.
• We propose a novel depth-aware backward projection method that overcomes the limitations of existing meth-ods in effectively utilizing depth information. Our ap-proach integrates depth consistency into the projection process to establish a more accurate mapping relation-ship between the 3D and 2D spaces.
• Our FB-BEV model has been extensively evaluated on the nuScenes dataset. The results demonstrate that it outperforms other methods for camera-based 3D object detection and achieves the state-of-the-art 62.4% NDS on the nuScenes test set. 2.