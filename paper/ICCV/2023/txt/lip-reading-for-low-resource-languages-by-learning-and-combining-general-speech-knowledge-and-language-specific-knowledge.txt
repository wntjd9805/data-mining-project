Abstract
This paper proposes a novel lip reading framework, es-pecially for low-resource languages, which has not been well addressed in the previous literature.
Since low-resource languages do not have enough video-text paired data to train the model to have sufficient power to model lip movements and language, it is regarded as challenging to develop lip reading models for low-resource languages.
In order to mitigate the challenge, we try to learn general speech knowledge, the ability to model lip movements, from a high-resource language through the prediction of speech units. It is known that different languages partially share common phonemes, thus general speech knowledge learned from one language can be extended to other languages.
Then, we try to learn language-specific knowledge, the ability to model language, by proposing Language-specific
Memory-augmented Decoder (LMDecoder). LMDecoder saves language-specific audio features into memory banks and can be trained on audio-text paired data which is more easily accessible than video-text paired data. Therefore, with LMDecoder, we can transform the input speech units into language-specific audio features and translate them into texts by utilizing the learned rich language knowl-edge. Finally, by combining general speech knowledge and language-specific knowledge, we can efficiently develop lip reading models even for low-resource languages. Through extensive experiments using five languages, English, Span-ish, French, Italian, and Portuguese, the effectiveness of the proposed method is evaluated. 1.

Introduction
It is a fascinating ability to understand the conversation by only looking at the speaker’s lip movements without listening [1].
If this were possible, we could easily hold conversations in crowded places, such as at concerts, and even with people who have trouble speaking up. With the
*Both authors have contributed equally to this work.
†Corresponding author great advance of deep learning, a technology called lip read-ing has made it possible to accurately infer what a speaker is saying without having to approach the speaker’s voice.
In recent years, the performance of lip reading has signif-icantly improved from 60.1% Word Error Rate (WER) to 26.9% WER [2, 3] in LRS3 [4], a popular English bench-mark database.
Such rapid progress could be made with large-scale audio-visual datasets [4–8], improved neural network ar-chitecture [9–15], enhanced multi-modal learning strate-gies [3, 16–22], and carefully designed training methods
[23–25]. Among these progresses, self-supervised learning methods using audio-visual data show remarkable advance-ment in both audio-visual speech recognition and lip read-ing. Recently, AV-HuBERT [3] which pre-trains the trans-former encoder with multi-modal inputs (i.e., audio and video) through masked prediction in a self-supervised man-ner, outperforms other previous lip reading methods once it is finetuned on lip reading tasks. Despite these advances, lip reading technologies have been developed primarily in
English rather than in other languages. One main reason for this is the lack of enough labeled video-text paired data in other languages. For example, the popular lip reading dataset in English, LRS3 [4], consists of about 443 hours of video, while the available video-text paired dataset in Ital-ian [26] is only about 47 hours, which is not enough for the model to learn the characteristics of both lip movements and language. Therefore, to build lip reading models for other languages rather than English, a new method considering the insufficient training data should be developed.
In this paper, we focus on developing a novel lip reading method for low-resource languages which has not been well explored in previous literature. Specifically, we propose a novel training method for low-resource language lip reading that learns 1) general speech knowledge and 2) language-specific knowledge, and combines the two learned knowl-edge. First, general speech knowledge refers to the knowl-edge of modelling short-term speech that can be regarded as speech units (i.e., phonemes or visemes). Since differ-ent languages partially share common phonemes [27–29],
learning to model accurate speech units from high-resource language can be beneficial in modelling speech representa-tions for low-resource language. To this end, we train the visual encoder to predict speech units from input lip move-ments through masked prediction using a high-resource language, English. Second, language-specific knowledge refers to the knowledge of translating learned speech rep-resentations into text, which can be regarded as the lan-guage modelling ability of a model. Since learning a lan-guage requires large-scale data [30–32], it might be insuf-ficient to only utilize the video-text paired data of low-resource language. To mitigate the problem, we propose a Language-specific Memory-augmented Decoder (LMDe-coder) which can be trained from audio-text paired data in the target language and be applied for lip reading. The input of LMDecoder is set to speech units derived from audio, and
Language-specific Memory (LM) saves language-specific audio features into memory banks, which are for trans-forming speech units into language-specific speech repre-sentations. Finally, after learning the two knowledge, we cascade the two modules (i.e., visual encoder and LMDe-coder) and we can employ both the accurate lip move-ments modelling ability (i.e., general speech knowledge) of the visual encoder and the rich language modelling ability (i.e., language-specific knowledge) of LMDecoder, for low-resource language lip reading.
The effectiveness of the proposed method is evaluated with five languages, English (EN), Spanish (ES), French (FR), Italian (IT), and Portuguese (PT). Especially, English is utilized as a high-resource language so employed to learn general speech knowledge, and other languages are utilized as low-resource languages thus LMDecoder is trained on each low-resource language data. Through comprehensive experiments, we show the proposed method is effective in developing lip reading models not only for low-resource languages but also effective for the high-resource language, by achieving state-of-the-art performance on English data.
The contributions of this paper can be summarized as:
• To the best of our knowledge, this is the first attempt to analyze the effectiveness of different pre-training methods, self-supervised pre-training of encoders, su-pervised pre-training in a high-resource language, and pre-training of decoders with audio-text data, in build-ing low-resource lip reading model.
• We propose a novel method of learning and combin-ing general speech knowledge and language-specific knowledge to effectively develop lip reading models for low-resource languages. method in developing lip reading models for different nationalities, even with a small-scale dataset. 2.