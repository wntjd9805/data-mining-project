Abstract
The Automated Model Evaluation (AutoEval) framework entertains the possibility of evaluating a trained machine learning model without resorting to a labeled testing set.
Despite the promise and some decent results, the existing
AutoEval methods heavily rely on computing distribution shifts between the unlabelled testing set and the training set.
We believe this reliance on the training set becomes another obstacle in shipping this technology to real-world ML de-velopment. In this work, we propose Contrastive Automatic
Model Evaluation (CAME), a novel AutoEval framework that is rid of involving training set in the loop. The core idea of CAME bases on a theoretical analysis which bonds the model performance with a contrastive loss. Further, with extensive empirical validation, we manage to set up a pre-dictable relationship between the two, simply by deducing on the unlabeled/unseen testing set. The resulting frame-work CAME establishes a new SOTA results for AutoEval by surpassing prior work significantly. 1 1.

Introduction
During the last decade, the technological advancement of artificial intelligence and machine learning has attained unprecedented achievements, affecting a variety of domains or verticals. Ubiquitously, off these milestones, to prop-erly evaluate, assess and benchmark the trained models is undoubtedly pivotal, particularly when considering the de-ployment towards the production in real-world scenarios.
To do that, the traditional means often relies on a pre-split and static testing set for model evaluation, which is principally left out of the sight during training or valida-tion phase. However, several recent works has pointed out
*Both authors contributed equally to this research.
†Corresponding author. 1Our code is publicly available at: https://github.com/ pengr/Contrastive_AutoEval the drawback of this standardized scheme due to its re-quirement of careful sample selection, randomization due to the sample set split, the OOD gap between the deploy-ment environment, and (somewhat) expensive label annota-tion [14, 13, 5], etc. Most recently, we see that Automated
Model Evaluation (AutoEval) has emerged to tackle these problems [14].
In particular, the vanilla prototype of the Automated
Model Evaluation approaches aim at estimating a provided model’s performance on an unlabeled testing set. Notably, these approaches first generate a few meta-sets by adopt-ing pre-selected data augmentations on the training set. In what follows, one can estimate a certain distance — for in-stance, the Frechet distance [16] — averaged between the meta-sets with the testing set. As a result, the prior work has proactively shown that this averaged distance measure-ment is related to the final model performance on the testing set. Indeed, we believe this setup of AutoEval on the testing set possesses positive prospects because it manifests a high similarity towards real production — where the testing set is acquired on the fly in the real-world, leaving no time/space for these samples to be annotated or persist. A graphical illustration of the AutoEval against the conventional static testing set evaluation is depicted in Figure 1.
Despite its promise and prospect, we realize that the cur-rent paradigm of AutoEval may still fail in its real-world deployment, under certain conditions. On one hand, it is widely acknowledged that the prior works are dedicated to avoiding annotating the testing samples and to amortizing the vexing randomness through the massive generation of meta-sets offline. On the other hand, however, these tech-niques still demand the full presence of the sample input from the training set, which in many — if not most — of the occasions probably imply expensive storage and computa-tion cost. Hence, we argue that this requirement cannot be easily ensured in many scenarios, most notably on limited-capacity, limited-storage, or low-power platforms such as edge devises for IOT or autonomous driving. Hereby, we
To this regard, we briefly introduce our framework,
CAME. It is prerequisite composed of two conditions: (i)-the model is trained jointed of a normal task loss together with a contrastive loss and (ii)-the model performance is not affected by jointly contrastive learning. Based on the model yielded this way, we conduct a rigorous empirical study — guided by the theories we pose above — that we prove the correlation between the contrastive loss on the testing set with its performance truly exists. The AutoEval established this way enjoys the following two major merits: (i)-it shreds the need for the training set during the evaluation phase which further extends the AutoEval technique towards pro-duction in real-world; (ii)-CAME sets a new record of test-ing performance estimation by exceeding the prior work by significant margins. 2.