Abstract
Nonlinear Activation (Act) models which help fit the un-derlying mappings are critical for neural representation learning. Neuronal behaviors inspire basic Act functions, e.g., Softplus and ReLU. We instead seek improved explain-able Act models by re-interpreting neural feature Act from a new philosophical perspective of Multi-Criteria Decision-Making (MCDM). By treating activation models as selec-tive feature re-calibrators that suppress/emphasize features according to their importance scores measured by feature-filter similarities, we propose a set of specific properties of effective Act models with new intuitions. This helps us identify the unexcavated yet critical problem of mismatched feature scoring led by the differentiated norms of the fea-tures and filters. We present the Instantaneous Importance
Estimation Units (IIEUs), a novel class of interpretable Act models that address the problem by re-calibrating the fea-ture with the Instantaneous Importance (II) score (which we refer to as) estimated with the adaptive norm-decoupled feature-filter similarities, capable of modeling the cross-layer and -channel cues at a low cost. The extensive experi-ments on various vision benchmarks demonstrate the signif-icant improvements of our IIEUs over the SOTA Act models and validate our interpretation of feature Act. By replacing the popular/SOTA Act models with IIEUs, the small ResNet-26s outperform/match the large ResNet-101s on ImageNet with far fewer parameters and computations. 1.

Introduction
Nonlinear Act models are the foundation for the un-precedented success of neural networks in pattern recogni-tion tasks [11, 46, 43, 7]. The choice of the Act model is a decisive yet non-trivial factor in the performance of a neu-ral network. Basic methods such as ReLU [38] and Soft-plus [16] are originated from neuronal behaviors [44, 27].
Based on them, past works have proposed to improve
Act models with channel context (e.g., FReLU [32], Dy-ReLU [10] and ACONs [31]), statistical strategies (e.g.,
GELU [22], Pserf [4], and SMU [5]), and task-specific
Figure 1. ImageNet Top-1 Accuracy (Acc.) relative improvements compared with the ReLU [38] baselines and SOTAs (Swish [40],
ACONs [31](CVPR’21), and SMU [5](CVPR’22)) with (1) Mo-bileNetV2 [42] (MNv2) 0.17ˆ and 1.0ˆ; (2) ShuffleNetV2 [33] (SNv2) 1.0ˆ; (3) ResNet-14, -26, and 50 [21]. We show the
ReLU baseline results by “(Acc.(%); parameters(M))”. Our IIEUs achieve the new SOTA improvements to the ReLU baselines and outperform the SOTAs remarkably, with negligible/marginal addi-tional parameters to ReLU (shown by the relative areas of the cir-cular patterns, where each ReLU network denotes the unit area). periodic functions [35, 45]. Existing methods, however, still leave critical problems in the optimal decision on Act models. As a major reason, although several past ef-forts [37, 2, 19] suggested extending Act models with dy-namic approximators, it still lacks tailored interpretations to help specify the properties of effective Act models for pattern recognition. These specific properties, however, are difficult to be identified from pure biological intuitions.
To explore new improvements in feature Act, we rethink neural operations from MCDM (a typical problem in oper-ational research) [41, 39, 26, 15, 9, 50]. As a core of our interpretation, we treat a nonlinear Act model as a selec-tive re-calibrator that suppresses or emphasizes features ac-cording to their importance. Such importance, in fact, is first modeled by the feature-filter inner product which sup-poses to indicate the similarity of the feature to the filter.
However, differentiated feature and filter norms can signifi-cantly bias the similarities modeled with feature-filter inner-products, thus likely interfering with the estimation of ac-tual feature importance. We identify this as a critical yet
unexcavated problem, namely mismatched feature scoring (as discussed in Figure 2(a)), which we infer from our in-terpretation and otherwise invisible to past explanations.
To address the problem, we propose a set of specific properties of effective Act models with new intuitions and introduce the initial solution i.e., a novel kind of explicable
Act models which we refer to as the IIEUs, to selectively re-calibrate features with an adaptive norm-decoupled im-portance measure. Specifically, we first treat each feature-filter inner product (suppose without biases and normaliza-tion layers) as a Transitive Importance (TI) score, as its in-put feature vector is de facto determined by a series of prior learning factors (e.g., the initial input, filters and Acts of the prior layers) and transmits their cues. We then estimate the corresponding norm-decoupled Instantaneous Importance (II) score with a low-cost adaptive shift term that incor-porates mild learning adjustments. Finally, the feature Act is realized by multiplying each TI-score with the II-score.
This feature re-calibration preserves meaningful prior learn-ing information carried by the TI-scores yet eliminates the negative effect led by the mismatched feature scoring prob-lem. Note that we formalize the mismatched feature scoring problem and TI-, II-scores in Section 2.
The contributions of this work are 3-fold: (1) We pro-pose to interpret neural feature Act from MCDM, where we identify the unexplored problem of mismatched feature scoring and introduce a set of specific properties with our intuitions to help explain the working mechanism of Act (2) We present explainable IIEU(s) as the ini-models. (3) We tial solution to the problem identified from (1). extensively validate the (a) effectiveness and versatility of
IIEUs with various vision benchmarks, where IIEUs signif-icantly improve the SOTA Act models; (b) our interpreta-tion with targeted ablation studies. Code is disseminated at https://github.com/SudongCAI/IIEU. 2. Rethinking Feature Activation from MCDM
We aim to interpret neural feature Act from MCDM, find the unexplored critical problem, and propose our novel Act model by addressing the new problem. We first clarify our
Intuitions and their induced Properties. We then present our
IIEU constructed on them. With the preliminaries, we for-malize the Properties with Definitions and Propositions, where the Deductions and Proofs are detailed in the Ap-pendix (in Supp, marked by
). For coherence, we dis-cuss the related works in Section 3 with our interpretation. (cid:66) 2.1. Preliminaries
We consider the simple settings with image inputs: (1)
A network has T sequential learning layers indexed by τ “ 1, 2, . . . , T . Let Xτ P RCτ ˆH τ ˆLτ which has C τ channels and a spatial resolution of H τ ˆ Lτ denote the input feature map of the layer-τ . (2) Let xτ `1 c ph, lqq de-ph, lq :“ ϕ p˜xτ c
, where ˜xτ note the learning of the layer-τ at a given location ph, lq P
ΩH τ ˆLτ with the c-th filter wτ pcq P RCτ and feature vec-tor xτ ph, lq P RCτ c ph, lq “ xwτ pcq, xτ ph, lqy denotes the inner product and ΩH τ ˆLτ is the spatial lat-tice of Xτ . Note that the layer-τ includes a total of Cτ `1 filters. ϕ : R Ñ R is a given Act function and we suppose c ph, lq, where ρ : R Ñ R
ϕ p˜xτ defines the reweighting function of ϕ about ˜xτ c ph, lqq “ ρ p˜xτ c ph, lqq ˜xτ c ph, lq.
Note that (1) we first leave aside normalization layers (e.g., BN [25] and LN [3]) and biases for simplicity and will consider them in Section 2.3 (Method). (2) for region-dependent learning with a K ˆ K convolution, we meet the supposed settings by vectorizing the neighborhood of features/filters from size C τ ˆ K ˆ K to C τ ¨ K 2. From
MCDM, we treat (1) a filter wτ pcq as an updatable ideal candidate1 of the c-th group of criteria (i.e., the C τ channels of wτ pcq); (2) a feature vector xτ ph, lq as an alternative candidate whose importance score about a group of criteria is measured by the feature-filter similarity, i.e., Alternative-Ideal (A-I) similarity. Following we omit the layer index τ and spatial coordinate ph, lq to simplify the notations for the operations of layer-τ (e.g., we denote xτ ph, lq, wτ pcq, and ˜xτ c ph, lq by x, w, and ˜x, respectively).
For clarity, as for ˜x “ xw, xy, we suppose the influ-ence of a candidate on the inferencing and filter updating can be quantified as ˜x and x, where the corresponding in-tensities are |˜x| and }x}, respectively, as (1) the difference of two vectorial candidates in a standard neural network can be measured by Euclidean distance; (2) the influence of x on the updating of w can be controlled by ∇w xw, xy “ x.
In particular, the supposed settings can be extended to the case: ϕ p˜x, ‚q “ ρ p˜x, ‚q ˜x, ϕ, ρ : 9D Ñ R, where 9D de-notes the extended domain of ˜x with other given real vari-ables/constants (denoted by ‚), if (1) ϕ and ρ are still func-tions about ˜x when the values of other variables are given; (2) ρ is continuous and differentiable about ˜x or at most has a finite number of points where the left- and right-hand lim-its exist but are unequal; (3) for a non-differentiable point on
ρ, let the single-side derivative of the side where the point is defined be the derivative for calculation. In the following, we omit “‚” (e.g., denote ϕ p˜x, ‚q as ϕ p˜xq) if not specified. 2.2. IIEU: Intuitions and Properties
We begin by rethinking “nonlinearity,” the foundation of neural feature Act, from MCDM:
Intuition 1. The “Nonlinearity” of feature Act can be inter-preted as a “loose selectivity” that is necessary but not suf-ficient to differentiate features by their importance scores.
“Nonlinearity” is indispensable for the learning of dis-criminative neural representations. The mathematically ab-1With the given conditions, the ideal candidate in MCDM [26, 49, 39, 41] denotes the acquirable/virtual optimal choice capable of quantitatively measuring the performance of an alternative candidate by the similarity.
Figure 2. Illustration of the intuitions for IIEU. Suppose w/o normalization layers and biases. The shades of colors denote the intensities (the darker the higher and positive if w/o “(–)”), where “orange,” “purple,” “aqua,” and “olive” denote features, filters, importance scores, and the parameters of the term-B. (a) Mismatched feature scoring problem: it is possible to find feature vectors x, y and filters w, u s.t. xu, yy " xw, xy and xw, yy " xw, xy, where y is far dissimilar to u and w compared with x to w, due to the significant differences of the norms. (b) Intuition 1: a “nonlinear” Act model does not be specified to suppress/emphasize candidates with their expected importance. (c) An example of typical Act model, where ˜x is directly applied as the approximated similarity ˆϱ p˜xq and the (a) is left unsolved. (d) and (e) IIEU eliminates the (a) by scoring feature with the adaptive norm-decoupled approximated similarity, such that the influence of x are relatively emphasized by assigning higher scores compared to y. (f) Properties of the term-B: u˚, ν ˚ denote the (virtual) optimal u, ν for u, ν to approach in training, respectively. we suppose ν to be updatable, positive, and bounded since (1) the perfectness of filters as ideal candidates cannot be ensured (as discussed with Intuition 3); (2) we identify the positive translation to the codomain of the approximated similarity ˆϱ p˜xq help to selectively suppress/emphasize the influence of targeted candidates; (3) a bounded ν ensures that the contribution of the bounded main term-S will not be neutralized by the auxiliary ν (as further discussed in Section 2.3 with the ablation study (4)). solute “nonlinearity,” however, can also be brought by other basic operations, e.g., BN [25], LN [3], and the biases of lin-ear layers. From MCDM, as for Act model, non-important candidates are likely to be scored with negative A-I inner products, where the candidates with intense negative inner products are possible to deteriorate the learning (
). This necessitates a selective re-calibration to suppress/preserve the harmful/positive influence, respectively. Our Intuition 1 aims at bridging the meaning of “Nonlinearity” to “Selec-tivity” with the Proposition 1: (cid:66)
Definition 1. For a function ρ : R Ñ R, we refer to this ρ as a function that holds Loose Selectivity (on R) if: D˜x, ˜y P R while ˜x, ˜y ‰ 0 and ˜x ‰ ˜y such that (s.t.) ρ p˜xq ‰ ρ p˜yq.
Proposition 1. then, ρ satisfies Definition 1 ðñ ϕ is nonlinear about ˜x.
For a given ρ and ϕ : ϕ p˜xq “ ρ p˜xq ˜x, (cid:66)
Proposition 1 helps us to identify the meaning of a nonlinear Act model as a selective re-calibrator, where its reweighting function ρ can assign unequal weights to dif-ferent A-I inner products. However, this “loose selectively” is not yet sufficiently specified to suppress/emphasize can-didates based on their measured importance scores (Fig-ure 2(b)), thus our goal is to suggest improved selectivity by proposing specific properties with further intuitions.
Intuition 2. There exists an A-I similarity measure ϱ p˜xq capable of completely reflecting the importance of ˜x about its criteria, which we refer to as the ideal similarity.
By assuming the existence of ϱ which satisfies:
• For any given alternative and ideal candidates x, y and w, v, suppose ˜x “ xw, xy and ˜y “ xv, yy. If ϱ p˜xq ě
ϱ p˜yq, then, x has higher/equal importance than y about their importance measure criteria, we specify the reweighting function as ρ p˜xq “ ς pϱ p˜xqq, where ς is an adjuster function that casts suitable con-straints on the codomain of ϱ such that:
Property 1. |ς pϱ p˜xqq| ě |ς pϱ p˜yqq| if ϱ p˜xq ě ϱ p˜yq.
Where ϱ p˜xq is continuous and differentiable at ˜x, @˜x P
R; ς pϱ p˜xqq is continuous and differentiable about ϱ p˜xq on the domain (or at most has finite points where the left-and right-hand limits of the function exist but are unequal).
Note that Property 1 is ensured by ς, as the monotonicity of
|ϱ p˜xq| about ϱ p˜xq is uncertain. Moreover, Property 1 can be met with a simplified condition, i.e.,
Property 1 ðñ (1) ς pϱxq is (monotoni-Proposition 2. cally) non-decreasing about ϱx ^ ς pϱxq ě 0 _ (2) ς pϱxq is (monotonically) non-increasing about ϱx ^ ς pϱxq ď 0 (ϱx denotes ϱ p˜xq; ^ denotes “and;” _ denotes “or”). (cid:66)
In particularly, we discuss ς pϱ p˜xqq ě 0 (i.e., ς pϱ p˜xqq is lower-bounded) without loss of generality.
As for real-world application, we identify the suitable approximation to the ideal similarity ϱ p˜xq (denoted by
ˆϱ p˜xq) a critical problem, as ϱ p˜xq is difficult to be deter-mined or may not exist, which lies in the fact that the underlying mappings of neural learning can be extremely complex. Accordingly, we propose our fundamental intu-ition to introduce the approximated similarity ˆϱ p˜xq of IIEU with Property 1:
Intuition 3. Mismatched feature scoring. Typical Act functions directly apply A-I inner product ˜x as ˆϱ p˜xq (sup-pose w/o normalization layers and biases, Figure 2(c)).
However, we identify A-I inner product can be largely bi-ased by the norms of features or/and filters. This can lead to unreliable importance scoring for features. We refer to this problem as the mismatched feature scoring (Figure 2(a)).
We clarify Intuition 3 with the Transitive and Instan-taneous Importance (TI and II) scores. We refer to each
A-I inner-product ˜x as an TI-score, as its input feature x is in fact determined by a series of prior learning factors (e.g., the initial input and filters of the prior layers). That is, TI-score which transmits prior layer information does not ex-actly measure the current importance of x about the criteria of w, as ˜x can be drastically biased by the norms }x} and
}w}. In contrast, we suppose II-score measures the current importance of x with its norm-independent similarity to w.
We identify the cosine similarity, i.e., cos θw,x “ ˜x
}w}}x} a suitable II-score, with the prerequisite that the filter w is a perfect representative for its criteria (i.e., channels). The perfectness of filters in reality, however, cannot be ensured, especially in the early/medium training stages where filters are far from being optimized. This weakens the reliability of the vanilla cosine similarity. To eliminate this critical problem, we propose IIEU equipped with an adaptive term to enable flexible II-score estimation (Figure 2(d) and 2(e)): comparable influence, i.e., the influence of the one with lower weight will not be covered by the higher one.
Intuition 6.
OD: We suppose the core of the Act model, i.e., the reweighting function ρ, has a sufficient capability to differentiate between important/non-important candidates. (cid:66)
The intuitions 4, 5, and 6 suggest three dependent con-straints on the influence of negative and positive candidates, which we formalize as three Properties, i.e., (CNI), (PPI), and (OD), and two corresponding Propositions that further specify the Properties for practical IIEUs, with supposing a set of simple constraints: (1) ϕ p´8q “ 0 (i.e., we adopt the boundedness constraint for self-gated Act functions [48] to ensure the stability and convergence of training, with the pre-condition that ρ p˜xq is lower-bounded; (2) ∇˜xϱ p˜xq is bounded; (3) ϱ p˜xq´1 ˜x is bounded at @ϱ p˜xq ‰ 0 (as de-tailed in the Appendix
). (cid:66)
Next, we present IIEU-B and IIEU-DC as two practical
IIEU derivatives, built based on the suggested intuitions and properties. In particular, as II-score built upon the proposed approximation to the ideal similarity (i.e., ˆϱ p¨q), we suppose a loosened Property 1 to bring additional learning flexibility, i.e., ς pˆϱ p˜xqq is possible to have small negative values and be non-monotonic about |ˆϱ p˜xq| at |ˆϱ p˜xq| ď |η|, where η denotes a given threshold close to 0.
ϕ p˜xq “ ς
ˆ
˜x
}x} }w}
˙
` ν
˜x , (1) 2.3. Practical Method
˜x
˜x
¯
´ where we suppose }x} }w} ą 0; ν is an updatable bias term with specific constraints (as described in Figure 2(f)) to per-form adaptive shifts, i.e., we propose to estimate II-score by
}x}}w} ` ν
ρ p˜xq “ ς
, where the approximated similar-ity ˆϱ p˜xq “ ˜x
}x}}w} `ν. Then, IIEU realises norm-decoupled feature Act by rectifying (i.e., multiplying) the TI-score ˜x with the II-score adapted to the training conditions. Partic-}x}}w} and ν as the main similarity ularly, we refer to these term (term-S) and auxiliary bias term (term-B), respec-tively. More generally, we let ˜x “ ψ pxw, xyq if with bi-ases or normalization layers (denoted by ψ) applied to the A-I inner product. This differs term-S from the co-sine similarity, yet not changes its meaning of importance scoring, as we suppose the decoupling of the norms of fea-tures/filters as the essence to eliminate the transitive biases.
Next, we discuss further properties to embody the term-B
ν and adjuster ς by specifying the relationships of the ideal similarity ϱ and its adjuster ς, with the preceding deduc-tions and new intuitive assumptions, termed as Constraint on Negative Influence (CNI), Preservation on Positive
Influence (PPI), and Oriented Discriminativeness (OD):
Intuition 4. date have constrained influence. (cid:66)
CNI: We suppose any non-important candi-Intuition 5.
PPI: We suppose any important candidates x, y with close importance scores ϱ p˜xq and ϱ p˜yq will have (cid:66)
We present IIEU-B as the initial practical IIEU (Figure 3) and IIEU-DC (Dynamic Coupler) Figure 4 as a tailored enhancement to IIEU-B. In the subsequent, we introduce
IIEU-B and IIEU-DC in detail.
Formulation. We propose IIEU-B built on the proto-type of IIEU (Equation (1)) described in Section 2.2, by embodying the term-B ν and the adjuster ς with the pro-posed properties. Specifically, for IIEU-B, we let term-B be
´
´
ν “ δ
LN avgpool
¯¯¯
´
˜Xc
, (2) where LN denotes the LayerNorm [3] to perform flexible channel-dependent scaling and shift to channel statistics with negligible cost. δ is Sigmoid function to cast upper-bounded positive constraint on channel statistics to help meet the supposed properties (Figure 2(f)). Moreover, with the prior that ˆϱ p˜xq of IIEU-B is bounded, we propose a suit-able conditional adjuster ς to meet the proposed Properties:
"
ς pˆϱxq “
ˆϱx,
η exp pˆϱx ´ ηq ,
ˆϱx ě η
ˆϱx ă η
, (3) where ˆϱx denotes ˆϱ p˜xq and η a learnable threshold shared initialized by a small value (0.05 within each channel, by default). Note that (1) ς pˆϱxq is continuous about ˆϱx if the domain of ˆϱx is continuous, as lim ˆϱx´ηÑ0´ “ lim ˆϱx´ηÑ0` “ η; (2) we suppose the right-hand derivative
Figure 3. Operational illustration of IIEU-B. “Elem” and “Mult” denote “Element-wise” and “Multiplication,” respectively. as the derivative at ˆϱx “ η (Section 2.1); (3) the influence of any candidate with ˆϱx ď η will be silenced if η “ 0. (cid:66)
Boundedness of ρ p˜xq. We suppose the boundedness of II-score ρ p˜xq as a pre-condition for Intuitions 4, 5, and 6 to ensure training stability. As for IIEU-B, as ν is bounded and ς is conditionally linear about ˆϱx for ˆϱx ą η, the boundedness of ρ p˜xq is solely determined by the term-S.
For generality, we discuss the common case that Batch-Norm [25] is applied, i.e., with the channel scaling and shift factors γ, β P R (extensible to LayerNorm [3]). Let
E “ }x} }w} ‰ 0, the codomain of term-S is calculated as:
´ |r| `
β ´ rµ
E
ď
˜x
E
ď |r| `
β ´ rµ
E
, (4) where r “ γ
σ ; σ ‰ 0 and µ denote the standard deviation and mean of ˜x for channel-c. That is, we can calculate both the upper- and lower-bound of term-S with the factors γ and
β whose values are constrained by the weight-decay (i.e.,
L2-regularization) in the training phase. Unlike the cosine similarity with a range r0, 1s, the range of term-S can be broader. Moreover, as the adjuster ς constrains ρ pˆϱxq ă η for ˆϱx ă η, the II-score can adaptively emphasize/suppress the informative/meaningless candidates.
¯
´ (cid:66)
˜x “ ˜x
}x}}w} ` ν
Analysis for term-S and -B from filter updating. We suppose term-B to be bounded to prevent it from neutral-izing the contribution of the term-S (Figure 2(f)). In this paragraph, we first discuss the case that ˆϱx ě η, i.e.,
˜x
ςpˆϱxq “ ˆϱx s.t. ϕ p˜xq “ ς
}x}}w} ˜x`ν ˜x.
Further, we simplify the case of comparing term-B and -S by considering ˜x “ xw, xy without loss of generality, as they share the same BN layers. Note that we discuss the derivatives about w and denote the term-S and -B by
}x}}w} and ν pwq “ δ pLN p¯˜xqq, respectively, s pwq “ where ¯˜x denotes the mean statistic for channel-c and δ is the Sigmoid function. Moreover, we approximate the op-eration of LN by LN p¯˜xq “ 9γ p¯˜xq ` 9β, where 9γ and 9β are the scaling and shift factors of the LN layer. Then, we can calculate the (partial) derivative about w of s pwq as:
˜x
∇ws pwq “
}w}2 x ´ wwTx
}x} }w}3
, (5) where T denotes matrix/vector transpose. Correspondingly,
´ 9γwT ¯x ` 9β
¯ ´ we calculate the derivative about w for term-B as:
´
¯¯ 1 ´ δ
∇wν pwq “ δ 9γ ¯x , (6) where ¯x “ avgpool pXq P RC denotes the vectorial chan-nel mean statistics of the feature map X. Particularly, we can expand the top-right term in Equation (5) as: 9γwT ¯x ` 9β wwTx “ wcxc w . (7)
¯
´ÿC c“1
That is, we identify term-S enabling each neuron to model detailed cross-channel feature-filter interactions at every spatial coordinate and leverage these informative cues to improve the filter updating. In contrast, as a control group, we calculate the derivative about w of ReLU [38] as:
∇wReLU p˜xq |xw,xyą0“ x , (8) where ReLU is shown to model channel-independent infor-mation only and lacks the capability to improve filter updat-ing with inter-channel relationships.
Next, we discuss the function of term-B from filter up-dating, which de facto realizes aligned adaptive adjustments to the term-S with statistical inter-channel information. As the representative of long-range channel cues, term-B, how-ever, does not provide the instance details about the fea-tures, hence it may dilute the contribution of the term-S if has excessive derivative about the filter. We propose to eliminate this problem by casting a positive constraint on the term-B with Sigmoid. Specifically, as the terms
}w}2 x and wwTx in Equation (5) are composed of the same member vectors (i.e., w and x), without loss of gen-erality, we suppose wwTx “ ´α }w}2 x, α P R and we have ∇ws pwq “ }w}2x`α}w}2x
“ 1`α
}x}}w} x. Then, we
}x}}w}3 can calculate the average contribution of term-S to the up-ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ “
¯∇ws pwq
ˇ 1`α
ˇ | ¯x|. With dating of filter w as:
} ¯x}}w} the preceding conditions, we have an conditional corollary:
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
¯∇ws pwq
ˇ 1`α
ě |∇wν pwq|, because
} ¯x}}w}
∇w |ν pwq| ď 1 4 | 9γ| | ¯x|. In particular, with the two criti-cal priors: (1) the range of value of learnable parameters are tightly constrained by the L2-regularization of a small weight-decay (e.g., 1ˆ10´4 for ImageNet experiments); (2)
| 9γ| is usually a small value fallen in 10´1 level, we suppose
ˇ
ˇ
ˇ ą |1 ` α| ě
} ¯x} , }w} ă 1 in common, so that
ˇ
ˇ
ˇ ě 1 4 | 9γ| ùñ
ˇ
ˇ
ˇ 1`α
} ¯x}}w}
Figure 4. DC operations. ¯˜x, ¯˜q P R denote the vectorial channel statistics of the main branch feature map ˜X and the residual feature map ˜Q.
ˇ
ˇ
ˇ
ˇ
¯∇ws pwq 1 4 | 9γ| (i.e.,
This ensures the applicability of the term-B to IIEU-B.
ě |∇wν pwq|) can be met easily.
Moreover, for ˆϱx ă η, the relative relationship of the term-S and -B about any given w preserves, as both have
Bp ˆϱx´ηq
Bς
“ η exp ˆϱx´η, which still pre-B ˆϱx
B ˆϱx serves the applicability of the term-B to IIEU-B.
Bpη exp ˆϱx´ηq
Bp ˆϱx´ηq
“
Dynamic Coupler. Recent neural networks usually leverage the shortcut (i.e., residual) to transmit details of the lower layers to the main branch of the current layer. The estimated II-scores of the features from the main branch and the shortcut, however, are un-calibrated before fusion for their cross-layer relationships such that they possibly meet compromised comparability in terms of importance measure as we propose IIEU mainly to score alternative candidates within the same layer. To address this prob-lem, we propose the Dynamic Coupler (DC) module as a tailor-made enhancement tool for IIEU-B. DC module is a new lightweight joint-feature-gating model that dynami-cally rectifies features of the main branch and the shortcut with the channel contexts such that the cross-layers features can be adaptively fused with calibrated intensities. In par-ticular, we refer to the enhanced IIEU-B as IIEU-DC.
DC module works at a low-cost, which only employs a joint-channel LayerNorm [3] with a small MLP (with a re-duction ratio r defaulted by 16) to project the global chan-nel statistics of the input main and shortcut features to the adaptive channel weights. Specifically, DC aims to estimate the channel-wise combination weights dynamically for the effective fusion of the main and shortcut features by extend-ing the channel attention mechanism [24] from
˘Xc “ λ1,c ˜Xc ‘ ˜Qc , (9) i.e., a single-side channel weights estimation without in-volving the contextual information of residual features, to
˘Xc “ λ1,c ˜Xc ‘ λ2,c ˜Qc , (10) i.e., the double-side channel weights estimation that jointly exploits the dual contextual cues of the main branch and the residual features in an interactive manner, where ‘ de-notes the element-wise summation. ˜Xc, ˜Qc P RHˆL de-note the main branch and residual feature matrices of the c-th channel (i.e., the channel slices of the corresponding feature maps), respectively. λ1,c, λ2,c P R denote the esti-mated weights for the c-th main branch and shortcut feature matrices, respectively. ˘Xc P RHˆL denotes the fused fea-ture matrix of the c-th channel. In particular, we constrain
λ1,c ` λ2,c “ 1 by Softmax function. Note that besides the clear differences in operations, the motivation of our
DC, i.e., to realize targeted dynamic weighted mixing of the main branch and shortcut features, is also different from the
SK-Net [29] which generalizes SE-Net to merge multi-scale features. The DC operations is illustrated in Figure 4 3.