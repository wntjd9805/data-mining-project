Abstract 1.

Introduction
We present a novel approach to single-view face relight-ing in the wild. Handling non-diffuse effects, such as global illumination or cast shadows, has long been a challenge in face relighting. Prior work often assumes Lambertian sur-faces, simplified lighting models or involves estimating 3D shape, albedo, or a shadow map. This estimation, how-ever, is error-prone and requires many training examples with lighting ground truth to generalize well. Our work by-passes the need for accurate estimation of intrinsic com-ponents and can be trained solely on 2D images without any light stage data, multi-view images, or lighting ground truth. Our key idea is to leverage a conditional diffusion implicit model (DDIM) for decoding a disentangled light encoding along with other encodings related to 3D shape and facial identity inferred from off-the-shelf estimators. We also propose a novel conditioning technique that eases the modeling of the complex interaction between light and ge-ometry by using a rendered shading reference to spatially modulate the DDIM. We achieve state-of-the-art perfor-mance on standard benchmark Multi-PIE and can photore-alistically relight in-the-wild images. Please visit our page: https://diffusion-face-relighting.github.io.
The ability to relight face images under any lighting con-dition has a wide range of applications, such as in Aug-mented Reality, where consistent lighting for all individuals in the scene is essential to achieve realism. Another use is in portrait photography, where one may aim to soften cast shadows to create a more pleasing, diffuse appearance. Yet, relighting single-view face images remains unsolved.
Relighting a face image requires modeling the physical interactions between the geometry, material, and lighting, which are not inherently present in a 2D image and difficult to estimate accurately. Earlier work [4, 53, 26, 68, 56] thus often assumes Lambertian surfaces and a simplified lighting model, which struggle to model complex light interactions like global illumination, subsurface scattering, or cast shad-ows. Using multi-view, multi-illumination data from a light stage or a simulation, [38, 72] proposed relighting pipelines that predict surface normals, albedo, and a set of diffuse and specular maps with neural networks given a target HDR map. Some recent methods aim to specifically model cast shadows by predicting a shadow map with a neural network
[23, 35] or rendering a shadow map through physical ray tracing with estimated geometry [22].
Figure 2: Pipeline overview. We use off-the-shelf estimators to encode the input image into encodings of light, shape, camera, face embedding, shadow scalar, and background image, which are then fed to DDIM via “spatial” and “non-spatial” conditioning techniques. For spatial conditioning, the modified SH, 3D shape, and camera encodings are rendered to a shading reference, which is then concatenated with the background image. This concatenated image is fed into Modulator to produce spatial modulation weights for DDIM’s first half. Non-spatial conditioning feeds a stack of 3D shape, camera, face embedding, and a modified shadow scalar to a set of MLPs for modulating the DDIM with our modified version of adaptive group normalization (AdaGN).
These approaches share a common scheme in which they first intrinsically decompose the face image into its surface normals, albedo, and lighting parameters, then use them along with a shadow or visibility map to render a relit out-put. However, one major issue of this scheme stems from its over-reliance on the accuracy of the estimated compo-nents, which are difficult to estimate correctly in real-world scenarios. For instance, when an input image contains cast shadows that need to be removed, these approaches often leave behind shadow residuals in the predicted albedo map, which in turn produces artifacts in the final output (Figure 3). Estimating the geometry for other areas like hair and ears is also extremely challenging, and they are often omit-ted from relighting pipelines, resulting in unrealistic final composites (Figure 3, 4 and 5).
This paper introduces an alternative approach that does not rely on accurate intrinsic decomposition of the face and can be trained exclusively on 2D images, without any 3D face scan, multi-view images, or lighting ground truth, once given a few off-the-shelf estimators. The general idea of our method is simple: we first encode the input image into a feature vector that disentangles the light information from other information about the input image. Then, we mod-ify the light encoding in the feature vector and decode it.
The challenge, however, is how to disentangle the light en-coding well enough so that the decoding will only affect the shading without altering the person’s shape and identity.
Our key idea is to leverage a conditional diffusion implicit model [59] with a novel conditioning technique for this task and learn the complex light interactions implicitly via the generative model trained on a real-world 2D face dataset.
Our method relies on mechanisms recently introduced in Denoising Diffusion Implicit Models (DDIM) [59] and
Diffusion Autoencoders (DiffAE) [41]. By exploiting the deterministic reversal process of DDIM proposed by Song et al.[59], DiffAE shows how one can encode an image into a meaningful semantic code and disentangle it from other information, which includes stochastic variations. By mod-ifying the semantic code and decoding it, DiffAE can ma-nipulate semantic attributes in a real image. Relighting can be thought of as a manipulation of the “light” attribute in the input image. But unlike DiffAE, which discovers semantic attributes automatically and encodes them in a latent code, our method requires an explicit and interpretable light en-coding that facilitates lighting manipulation by the user.
To solve this problem without access to the lighting ground truth, we use an off-the-shelf estimator, DECA [13], to encode the lighting information as spherical harmonic (SH) coefficients and rely on a conditional DDIM to decode and learn to disentangle the light information in the process.
Unlike prior work, our use of SH lighting is not for direct rendering of the output shading, as this would be restricted by the limited capacity of SH lighting to express complex illumination. Rather, it is used to condition a generative process that learns the complex shading prior to reproduce real-world 2D face images. To help preserve the input’s identity during relighting, we also condition the DDIM on other attributes, such as the face shape and deep feature em-beddings from a face recognition model, ArcFace [8].
Another key component is our novel technique for con-ditioning the DDIM. Instead of treating the SH lighting as a global, non-spatial condition vector as in DiffAE or other diffusion models, we render a shading reference using the known SH equation and feed it to another network called
Modulator, which computes layer-wise spatial modulation weights for the DDIM. This conditioning technique helps retain spatial information in the shading reference and pro-vides an easy-to-learn conditioning signal as the pixel inten-sities in the shading reference correlate more directly with the output RGB pixels.
With our novel framework, the visibility of cast shad-ows can also be modeled with a simple modification: add one conditioning scalar that indicates the “degree” of cast shadows to the DDIM. At test time, we can strengthen or attenuate cast shadows by modifying this scalar. Since our diffusion-based framework does not directly use this flag or the shape and SH parameters in a physical image formation model, imprecise estimation of these parameters can be tol-erated and does not significantly compromise our quality.
Our method produces highly plausible and photorealis-tic results and can convincingly strengthen or attenuate cast shadows. Moreover, we can reproduce the original facial details with high fidelity, which is difficult for competing methods that predict an albedo map with neural networks.
We conduct qualitative and quantitative evaluations and achieve state-of-the-art performance on a standard bench-mark, Multi-Pie [17]. To summarize, our contributions are:
• A state-of-the-art face relighting framework based on a conditional DDIM that produces photorealistic shad-ing without requiring accurate intrinsic decomposition or 3D and lighting ground truth.
• A novel conditioning technique that converts a shading reference rendered from the estimated light and shape parameters into layer-wise spatial modulation weights. 2.