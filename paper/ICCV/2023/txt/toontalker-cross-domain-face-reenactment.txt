Abstract
We target cross-domain face reenactment in this paper, i.e., driving a cartoon image with the video of a real per-son and vice versa. Recently, many works have focused on one-shot talking face generation to drive a portrait with a real video, i.e., within-domain reenactment. Straightfor-wardly applying those methods to cross-domain animation will cause inaccurate expression transfer, blur effects, and even apparent artifacts due to the domain shift between cartoon and real faces. Only a few works attempt to set-tle cross-domain face reenactment. The most related work
AnimeCeleb [13] requires constructing a dataset with pose vector and cartoon image pairs by animating 3D charac-ters, which makes it inapplicable anymore if no paired data
In this paper, we propose a novel method is available. for cross-domain reenactment without paired data. Specif-ically, we propose a transformer-based framework to align the motions from different domains into a common latent space where motion transfer is conducted via latent code addition. Two domain-specific motion encoders and two learnable motion base memories are used to capture do-main properties. A source query transformer and a driving one are exploited to project domain-specific motion to the canonical space. The edited motion is projected back to the domain of the source with a transformer. Moreover, since no paired data is provided, we propose a novel cross-domain training scheme using data from two domains with the de-signed analogy constraint. Besides, we contribute a cartoon dataset in Disney style. Extensive evaluations demonstrate the superiority of our method over competing methods. 1.

Introduction
Online video conferences and live streaming are spread-ing rapidly since the population of smartphones and com-munication techniques. Speakers sometimes present them-*Corresponding authors.
Figure 1. Cross-domain reenactment examples of our method.
Given a cartoon face, we animate it by transferring the pose and expression from a real face. selves as cartoon characters for privacy and entertainment consideration. However, conventional cartoon animation depends on a complex pipeline of 3D modeling and retar-geting based on a powerful render engine, which always takes a long period [24]. This limitation makes it difficult to drive a new model or directly drive a single comic picture.
There are many works [17, 19, 10, 23, 27] that focus on one-shot talking face generation, i.e., driving a portrait with a video of a real person, which belongs to within-domain reenactment. Their models are trained with a large amount of talking videos and perform well in the same-identity and cross-identity driving tasks. However, when those models are straightforwardly applied to cross-domain reenactment, i.e., driving a cartoon face with a real video as in Fig. 1, they persistently encounter the following issues, including inac-curate expression transfer, blur effects, and even apparent artifacts around facial components and the background. The reason is the distribution shift between different domains.
There is a large appearance and motion gap between the cartoon and real datasets. Though fine-tuning a pre-trained model on a cartoon dataset can improve the visual quality to a certain extent, those issues still cannot be wiped out.
Only a few methods focus on visually-driven cross-domain face reenactment. The early method Recycle-GAN [1] overfits two videos for unsupervised video trans-lation, which cannot be used for one-shot reenactment.
MAA [28] uses keypoints as an intermediate representation and designs an angle consistency loss to mimic the driving motion. But only focusing on keypoints will result in ignor-ing the subtle motion of facial components, especially the mouth and eyes. AnimeCeleb [13] is based on pre-defined 3D Morphable Models (3DMMs) [2] and additional pose annotaions1 of cartoon images. However, it is inapplicable when paired data is not available, which limits its range of use. Moreover, the pre-defined pose vector has limited ca-pability of capturing fine-grained facial expressions.
In this paper, we propose a novel method for cross-domain reenactment without the requirement of paired data.
All we need is a set of real videos and another set of cartoon videos. To solve the domain shift problem, we propose a transformer-based framework to align the motions from two domains in a shared canonical latent space. Specifically, two motion encoders and two motion bases are designed to discover the domain-specific properties of appearance and motion. To align the domain-specific motions in a common latent space, a source query transformer and a target take the responsibility of projecting the motions into a canoni-cal motion space, where motion transfer can be conducted via the addition of motion codes. The edited motion is then projected from the canonical space back to the domain of the source by a transformer. A generator takes the projected motion and the source features as input to synthesize a face in the domain of the source.
Moreover, to overcome the lack of paired data, we pro-pose a novel cross-domain training scheme. The core idea is the analogy constraint (see Fig. 2). Specifically, given two real faces and a cartoon face, we use the relative motion between the two real faces to drive the cartoon one, produc-ing a synthetic cartoon face. The relative motion from one real face to the other real one is analogous to that from the cartoon face to the synthetic one. We measure the distance between the two types of analogical motions in the aligned canonical motion space. They are supposed to be identical.
To conduct cross-domain reenactment experiments, we collect a cartoon video dataset in Disney style, which con-tains 344 videos and about 47k frames in total. The cartoon dataset will be released for research purposes.
Our contributions are as follows:
• We propose a novel framework for cross-domain face reenactment without using paired data. Several trans-formers are designed to align the motions of different domains in a common latent space.
• We propose a training scheme to compensate for the 1Note that Pose in [13] refers to a 20 dimension (20D) vector that contains the status of facial expression and head pose.
Figure 2. Visualization of analogy constraint. ∆ωreal and
∆ωcartoon represent the relative motion in the real and cartoon domain, respectively. ∆ωcan represents the projected motion in the canonical space where we can perform cross-domain motion transfer by the addition of motion codes. Given a cartoon face, we utilize the relative motion between two real faces to animate the cartoon one and generate a synthetic cartoon face. We hold that the relative motion between two real faces and that between the cartoon face and the synthetic are equivalent within the canonical motion space. lack of paired data by using the analogy constraint.
• We collect a cartoon video dataset and conduct exten-sive experiments to demonstrate the superiority of our method in the cross-domain reenactment. 2.