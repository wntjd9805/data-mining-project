Abstract
Recent advances in whole-slide image (WSI) scanners and computational capabilities have significantly propelled the application of artificial intelligence in histopathology slide analysis. While these strides are promising, cur-rent supervised learning approaches for WSI analysis come with the challenge of exhaustively labeling high-resolution slides—a process that is both labor-intensive and time-consuming.
In contrast, self-supervised learning (SSL) pretraining strategies are emerging as a viable alterna-tive, given that they don’t rely on explicit data annotations.
These SSL strategies are quickly bridging the performance disparity with their supervised counterparts. In this con-text, we introduce an SSL framework. This framework aims for transferable representation learning and semantically meaningful clustering by synergizing invariance loss and clustering loss in WSI analysis. Notably, our approach out-performs common SSL methods in downstream classifica-tion and clustering tasks, as evidenced by tests on the Came-lyon16 and a pancreatic cancer dataset. The code and addi-tional details are accessible at https://github.com/ wwyi1828/CluSiam. 1.

Introduction
Histopathology slide analysis remains the gold stan-dard for cancer diagnosis and prognosis. In recent years, researchers have seen the burgeoning adoption of digital histopathology slides in pathology laboratories, thanks to the availability of digital pathology scanners and advance-ments in computer vision, revolutionizing computational pathology [24]. While adoption of digital slides has ac-celerated, progress has been hindered by the exceptionally high resolution of whole slide images (WSIs), often exceed-ing 40, 000 × 40, 000 pixels, which makes directly applying standard computer vision models to WSIs not feasible. Fur-thermore, downsampling WSIs to a more manageable mag-nification level results in a substantial loss of fine-grained visual information.
Figure 1: The framework of CluSiam. View 1 and View 2 are distinct augmentations of the same images, pooled together for clustering. The invariance loss (solid line) aligns representations of the two views, while the cluster loss (dashed line) pushes cluster centroids apart.
To address the challenges, WSIs are commonly sub-divided into more manageable patches through sliding-window techniques. These patches are then labeled using annotations, forming training data for a patch-level classi-fier. Features extracted by the trained patch-level classifiers are aggregated to infer slide-level label [21, 42, 43, 40, 33].
However, this annotation-reliant approach has a signifi-cant drawback. It’s heavily dependent on precise annota-tions, which are expensive to obtain. Annotating WSIs is a painstaking and error-prone task that requires pixel-by-pixel scrutiny from highly skilled pathologists. The elusive
borders between different tissue patterns introduce variabil-ity among pathologists. Additionally, tissue morphology’s inherent variability often further diminishes the accuracy of annotations. Therefore, obtaining precise and consistent an-notations remains an uphill battle, even with substantial ex-pertise and time invested by trained pathologists. Inaccurate annotations can potentially lead to inaccurate and inconsis-tent WSI analysis models [39, 1].
To mitigate the impact of inaccurate annotations, noise-aware learning models have emerged. These methods im-prove the performance of patch-level feature extractors by either filtering or down-weighting the noisy patches [1, 28, 13]. However, these models are still constrained by the an-notation bottleneck. Even acquiring noisy annotations for
WSIs demands substantial time and expertise, which moti-vates the need for annotation-free techniques. They reduce costs and save time. They also eliminate the impact of inac-curate annotations.
In this challenging landscape, annotation-free techniques have emerged as a promising solution. By requiring only whole-slide labels, they not only cut costs and save time but also eliminate the effects of annotation inaccuracies.
Among these, Chen et al. [9] proposed a method that lever-ages a unified memory mechanism to train convolutional neural networks (CNNs) directly with numerous images.
However, this approach is constrained to lower magnifi-cation levels, restricting pixel sizes to above 2 µm. Con-versely, other studies [33, 30, 14, 15] have shown that achieving better results is possible by employing higher or multi-scale magnification levels across a variety of model designs.
Weakly supervised techniques have gained popularity as an annotation-free approach that retains high-resolution details by utilizing slide-level labels instead of exhaustive patch-level annotations. Obtaining slide-level labels is less laborious compared to exhaustive patch-level annotations.
Thus, weakly supervised learning has become particularly popular for histology slide classification tasks [36, 37, 46].
These methods employ slide-level labels as weak supervi-sion for all patches within a slide. Multiple instance learn-ing (MIL) models leverage this by treating slides as positive or negative bags, with patches as instances [29, 30, 22, 46, 3, 34]. However, MIL models have some limitations. They of-ten neglect important contextual cues across a whole slide.
Additionally, off-the-shelf feature extractors pretrained on natural images fail to sufficiently capture tissue morphol-ogy. These drawbacks motivate exploring self-supervised approaches for histology slides.
Self-supervised learning (SSL) enables models to learn feature representations without the need for labels. SSL methods are rapidly closing the performance gap with su-pervised approaches. However, SSL typically requires a large sample size. but this is mitigated for high-resolution histopathology images by splitting WSIs into numerous small patches. In computational pathology, self-supervision methods become an appealing solution for annotation-free
WSI analysis [30, 27, 26, 44, 7]. These methods uti-lize multiple-instance learning to aggregate self-supervised patch representations. They have demonstrated the capabil-ity to match the performance of state-of-the-art supervised methods while reducing the annotation burden on patholo-gists by eliminating the need for manual annotations.
One of the key paradigms of SSL is contrastive-based
SSL [19, 11, 35, 38, 25]. They may not be the most ef-fective in histopathology image analysis because adjacent patches from a WSI can be very similar in their morpho-logical features, making them unsuitable as negative sam-ple pairs. These methods also rely on a large number of negative pairs. To avoid the need for negative pairs, some knowledge-distillation-based methods [12, 6, 18] concen-trate solely on positive sample pairs, which are defined us-ing augmented views of the same image. However, only fo-cusing on positive pairs might prevent them from learning global information, as their objective functions only con-sider augmentations from the same image.
Apart from SSL representation learning, another pivotal technique gaining attention is clustering. Clustering is an unsupervised learning approach where similar samples are grouped to ensure intra-cluster cohesion and inter-cluster separation. In the domain of WSI retrieval, clustering could be instrumental. Wang et al. [41] employed a K-Means clustering-driven architecture, while Chen et al. [8] inte-grated a self-supervised variational autoencoder with the K-Means algorithm, both for WSI retrieval systems. Given the growing prominence of such methods in WSI retrieval, there’s an increasing demand to refine these clustering algo-rithms within computational pathology. Clustering shares similarities with representation learning. This has inspired clustering-based SSL methods that use pseudo-labels from iterative K-Means clustering algorithms for training feature encoders. Although these methods can learn effective im-age representations, they may not improve the performance of the actual clustering tasks as they cluster images into thousands of groups, which might hinder their direct use for histopathology image retrieval. Large cluster counts make identifying relevant groups challenging.
To address the shortcomings, we propose Cluster-Siam (CluSiam), a framework that decouples clustering from representation learning and retains only the most relevant and interpretable clusters for medical applications. Clu-Siam takes advantage of an existing self-supervised back-bone to extract representations. We introduce a cluster loss to guide the backbone in learning effective representations while generating accurate, interpretable cluster assignments for histopathology images. (Figure 1). Our experiments demonstrate that CluSiam outperforms baselines on down-stream classification tasks. Additionally, our adaptive clus-tering algorithm outperforms K-Means in clustering, result-ing in improved cluster assignments. In addition, our clus-ter assigner emerges as a by-product of the representation learning process, thus introducing only a small additional computational cost once the training is complete.
The contributions of this paper can be summarized as follows:
• We propose CluSiam, a SSL framework for image rep-resentation learning and clustering that combines in-variance loss and cluster loss (Figure 2)
• We compare the performance of different SSL frame-works and demonstrate that CluSiam outperforms pop-ular SSL methods on multiple histopathology datasets.
• CluSiam provides an efficient and accurate way to cluster histopathology images without either patch-level annotations or slide-level labels, with cluster-ing performance better than the widely used K-Means clustering in digital pathology.
Figure 2: The details of the CluSiam framework. The in-variance loss maximizes the on-diagonal elements of the similarity matrix between views. The cluster assigner takes the concatenated views as input and generates cluster as-signments. The cluster loss minimizes the off-diagonal ele-ments of the similarity matrix between cluster centroids. 2.