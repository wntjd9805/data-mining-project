Abstract
Grounding textual expressions on scene objects from first-person views is a truly demanding capability in de-veloping agents that are aware of their surroundings and behave following intuitive text instructions. Such capabil-ity is of necessity for glass-devices or autonomous robots to localize referred objects in the real-world. In the conven-tional referring expression comprehension tasks of images, however, datasets are mostly constructed based on the web-crawled data and don’t reflect diverse real-world structures on the task of grounding textual expressions in diverse ob-jects in the real world. Recently, a massive-scale egocen-tric video dataset of Ego4D was proposed. Ego4D cov-ers around the world diverse real-world scenes including numerous indoor and outdoor situations such as shopping, cooking, walking, talking, manufacturing, etc. Based on egocentric videos of Ego4D, we constructed a broad cov-erage of the video-based referring expression comprehen-sion dataset: RefEgo. Our dataset includes more than 12k video clips and 41 hours for video-based referring expres-sion comprehension annotation. In experiments, we com-bine the state-of-the-art 2D referring expression compre-hension models with the object tracking algorithm, achiev-ing the video-wise referred object tracking even in difficult conditions: the referred object becomes out-of-frame in the middle of the video or multiple similar objects are presented in the video. 1.

Introduction
It is a truly demanding task to identify surrounding ob-jects in real world scenes from video clips of egocentric viewpoints with free-form language supervisions. Such a task is necessary for glass-devices or autonomous robots that help with daily-life tasks and communicate with us in language because they need to understand the intuitive ex-pressions of languages and ground them into the surround-*Corresponding author.
Figure 1. Sample frames of RefEgo for “the large white bowl with broccoli inside that is used to load the pan of broccoli.” The re-ferred object of the bowl in green and other bowls in brown. ing world. It is an ultimate goal for the referring expression comprehension (REC) or shortly “visual grounding” task because it maps the referred entities in text to the corre-sponding objects identified and tracked from the observed sequence of images.
Extensive efforts are being made in 2D image reference expression comprehension [12, 21, 33, 19]. Recent semi-supervised approaches contribute to the open-vocabulary object detection from 2D images [11, 35]. However, com-pared to these extensive studies on 2D image referring ex-pression comprehension, we notice that comparably less efforts are taken in video-based referring expression com-prehension [16, 13, 24]. Video clips in such datasets are mostly collected in the Internet and aren’t suitable for the real-world daily-task understandings. The number of video clips are also limited. Ideally, video clips for such tasks are collected in embedded form in our daily lives and cover va-riety domains such as walking streets, shopping, chatting with others, staying in indoor, cleaning laundries, or cook-ing foods, when we pursue general purpose models in our daily scenes. However, it was nearly prohibitive to create datasets on such tasks because of the lack of the collection of real-world setting egocentric videos.
Recently, Ego4D [9], a massive-scale collection of ego-centric video and annotation is proposed. Ego4D videos are gathered by 931 unique participants in 74 locations world-Video REC
Base Dataset
# Clips
# Object Annotations
# Objects
# Categories
Lingual OTB99 [16]
Lingual ImageNet Videos [16]
OTB100 [18]
ImageNet VID [23] 99 100 58,733 23,855 99 100
-25
Video Object Segmentation
Base Dataset
# Clips
# Object Annotations
# Objects
# Categories
ReferDAVIS-16 [13]
ReferDAVIS-17 [13]
Refer-Youtube-VOS [24]
DAVIS [20]
DAVIS [20]
Youtube-VOS [30] 50 90 3,252 3,440 13,540 133,886 50 205 6,048
RefEgo (ours) 12,038
Table 1. Comparison of the video-based referring expression comprehension datasets.
Ego4D [9] (First-person video) 226,319 12,038
--78 505
REC dataset
# Images
# Object Annotations
RefCOCO [33]
RefCOCO+ [33]
RefCOCOg [19] 19,994 19,992 26,711
RefEgo (ours) 226,319 50,000 49,856 54,822 226,319
Table 2. Comparison of the RefEgo dataset to 2D REC datasets.
RefCOCO/+/g datasets are based on MSCOCO [5] images while ours is based on real-world egocentric video of Ego4D. wide. They are captured by a head-mounted camera device, intending to capture various daily-life activities in first per-son vision, covering hundreds of daily life activities includ-ing various locations: in-store, office-space, houses, wire-house, street and so on. Based on Ego4D videos, we con-structed the novel in a margin larger RefEgo video-based referring expression comprehension dataset with the help of object detection and human annotation, aiming to ground intuitive language expressions on various contexts in real-world first person perception.
Our RefEgo dataset exhibits unique characteristics that make it challenging to localize the textually referred ob-jects. It is based on egocentric videos and hence includes frequent motions in video clips. The referred objects are often surrounded by other similar objects of the same class.
The referred object may appear at the edge of the image frame or even goes out-of-frame in some frames, requiring models to discriminate images that contain and don’t con-tain the referred object. Fig. 1 presents the selective frames from a single video clip with a referred expression of “the large white bowl with broccoli inside that is used to load the pan of broccoli.” There are several other bowls in image frames and the referred object goes out-of-frame in the fifth frame. In this case, the models are expected to predict that the referred object is not presented in the image, illustrating the challenging characteristics of the proposed RefEgo.
We prepared valuable baseline models from multiple ap-proaches for RefEgo. We applied the state-of-the-art REC models of MDETR [11] and OFA [28] for RefEgo. We in-troduced MDETR models trained with all images includ-ing image frames with no annotated referred objects, and observed it performs better at discriminating images with-out referred objects than other models. We also introduce
MDETR with a special binary head to discriminate images without the referred object presented. We finally apply the object tracking of ByteTrack [34] for combining multiple detection results, allowing the models to spatio-temporal lo-calization of the referred object. 2.