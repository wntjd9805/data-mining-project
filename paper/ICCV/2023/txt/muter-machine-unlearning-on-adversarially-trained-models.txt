Abstract
Machine unlearning is an emerging task of removing the inﬂuence of selected training datapoints from a trained model upon data deletion requests, which echoes the widely enforced data regulations mandating the Right to be Forgot-ten. Many unlearning methods have been proposed recently, achieving signiﬁcant efﬁciency gains over the naive baseline of retraining from scratch. However, existing methods focus exclusively on unlearning from standard training models and do not apply to adversarial training models (ATMs) despite their popularity as effective defenses against adversarial examples. During adversarial training, the training data are involved in not only an outer loop for minimizing the training loss, but also an inner loop for generating the adversarial perturbation. Such bi-level optimization greatly complicates the inﬂuence measure for the data to be deleted and ren-ders the unlearning more challenging than standard model training with single-level optimization. This paper proposes a new approach called MUter for unlearning from ATMs.
We derive a closed-form unlearning step underpinned by a total Hessian-related data inﬂuence measure, while existing methods can mis-capture the data inﬂuence associated with the indirect Hessian part. We further alleviate the compu-tational cost by introducing a series of approximations and conversions to avoid the most computationally demanding parts of Hessian inversions. The efﬁciency and effectiveness of MUter have been validated through experiments on four datasets using both linear and neural network models. 1.

Introduction
Machine learning (ML) models are increasingly applied to a broad range of applications, accompanied by growing concerns about their privacy and robustness issues. Both issues are actively studied in recent years [45]. On the
*Equal contribution.
†Corresponding Author. privacy side, model inversion attack [16] and membership inference attack [51] reveal that the trained ML model con-tains sensitive information of its training data, which can cause privacy loss for individuals contributing their data for model training. On the robustness side, adversarial exam-ple attack is one of the most well-recognized robustness attacks [24, 30, 39, 62]. It can easily fool an undefended model, e.g., standard training model (STM), to misclassify by a small adversarial perturbation on the input [9, 24, 39].
Many works focus either on the privacy [1, 35, 37, 42, 51] or on the robustness aspect [9, 33, 38, 53–56, 66], few works studied both. Yet, it is critical to consider privacy and robust-ness jointly [20, 27, 36, 41, 47, 48, 52] to build ML models to simultaneously meet data privacy regulations and ensure robustness against adversarial threats.
In this paper, we target a new joint privacy-robustness problem to simultaneously meet emerging privacy regula-tions and ensure adversarial robustness of the model, which has not been examined before: how to efﬁciently and effec-tively remove the inﬂuence of a training datapoint from an adversarially trained model upon data deletion request?
The privacy need is driven by the widely enacted user data regulations that enforce the Right to be Forgotten, for example, the European Union’s GDPR [17], the California
Consumer Privacy Act (CCPA), and Canada’s proposed Con-sumer Privacy Protection Act (CPPA). These regulations mandate the deletion of personal data upon user requests and can even include the deletion of models and algorithms de-rived from the user data, e.g., the Federal Trade Commission
[15]. Machine unlearning [5, 8] aims to obtain an updated model with the inﬂuence of the target datapoint removed in an effective and efﬁcient way. That is, the updated model should be similar to the model obtained by the computation-ally expensive retraining-from-scratch approach, while con-suming less computation [21–23,26,31,43,44,58,60,61,65].
The robustness is achieved by the adversarial training model (ATM) [3, 24, 30, 39, 59], which is a popular and ef-fective defense for enhancing the model robustness against adversarial examples by creating and incorporating adver-Unlearing Request
Model parameters 
Direct Hessian-only unlearning
Unable to sufficiently  capture data influence
Direct Hessian
Total Hessian-based unlearning
Catch information  hidden in perturbations
Adversarial perturbations
Indirect Hessian
Figure 1: ATM has two interdependent sets of optimization variables, model parameters and adversarial perturbations (yellow arrows), both of which contain nested training data inﬂuence (blue arrows). To remove such nested data inﬂuence, ATM unlearning requires the total Hessian that consists of both direct and indirect Hessian components (green solid lines). Existing unlearning methods designed for standard training models are inapplicable to ATM unlearning, because they only use direct
Hessian information during the unlearning update and do not capture sufﬁcient data inﬂuence (red dashed lines). sarial examples into the training process. ATM is trained by a bi-level optimization with the model parameters as the outer-level variable and the adversarial perturbations as the inner-level variable. To the best of our knowledge, all exist-ing unlearning methods focus solely on data removal from
STM. As our analysis will reveal in Section 3, the existing methods cannot be applied to ATM unlearning without mis-capturing the data inﬂuence to be removed, due to ATM’s bi-level optimization structure.
In this paper, we propose a new unlearning approach called MUter : Machine Unlearning for data removal from adversarial training models. First, we deﬁne the ATM un-learning task in accordance with the mainstream machine unlearning standard. Second, we convert it to an ATM un-learning criteria derived from the optimality condition of
ATM, which facilitates the derivation of the unlearning up-date for ATM. Meanwhile, existing unlearning methods de-signed for STM cannot satisfy the ATM unlearning criteria.
As illustrated in Figure 1, ATM, as a bi-level optimization, has two sets of variables: model parameters and adversarial perturbations, which are interdependent and both contain training data information. In contrast, STM, as a single-level optimization, has only one set of variables: model param-eters, which have a direct inﬂuence dependence on each training datapoint. Existing unlearning methods cannot sufﬁ-ciently remove data inﬂuence from ATM because they fail to account for the indirect inﬂuence on the model parameters (i.e., miss the indirect Hessian part in Figure 1). That is, due to the coupled outer-inner optimization, any updates to the outer model parameters will incur further updates for both the adversarial perturbations and the model parameters. To address this, we derive a new closed-form unlearning update for ATM, which is underpinned by the total Hessian-based inﬂuence measure. Third, based on the ATM unlearning update, we propose a three-stage unlearning framework to support successive unlearning requests for ATM, which en-hances efﬁciency by selectively storing certain computations in memory. Last, we leverage Schur complement conversion and Neumann series approximation to avoid the computa-tionally demanding and numerical unstable computations of
Hessian matrix and its inversions.
To summarize, our main contributions are: 1. We introduce the problem of unlearning from adversarial training models, which is a new and pressing challenge to si-multaneously meet emerging privacy regulations and ensure the adversarial robustness of the model. To the best of our knowledge, it has not been studied in the existing literature. 2. We derive a new unlearning update tailored to ATM un-learning, which has the total Hessian-based data inﬂuence measure to sufﬁciently account for the interdependence be-tween inner and outer optimizations of ATM. 3. We propose MUter based on the proposed unlearning update, which supports successive unlearning requests for
ATM and introduces a series of conversions and approxima-tions for Hessian inversions to alleviate the computational cost and improve the numerical stability. 4. We perform a comprehensive evaluation to verify that
MUter achieves effective and efﬁcient unlearning perfor-mance while maintaining the model accuracy and adversarial robustness, under two unlearning settings on four datasets. 2. Preliminaries and