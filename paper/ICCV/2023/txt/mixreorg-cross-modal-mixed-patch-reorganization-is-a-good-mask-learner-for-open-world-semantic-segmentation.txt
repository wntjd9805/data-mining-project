Abstract
Recently, semantic segmentation models trained with image-level text supervision have shown promising results in challenging open-world scenarios. However, these mod-els still face difficulties in learning fine-grained semantic alignment at the pixel level and predicting accurate ob-ject masks. To address this issue, we propose MixReorg, a novel and straightforward pre-training paradigm for se-mantic segmentation that enhances a model’s ability to re-organize patches mixed across images, exploring both lo-cal visual relevance and global semantic coherence. Our approach involves generating fine-grained patch-text pairs data by mixing image patches while preserving the corre-spondence between patches and text. The model is then trained to minimize the segmentation loss of the mixed im-ages and the two contrastive losses of the original and re-stored features. With MixReorg as a mask learner, conven-tional text-supervised semantic segmentation models can achieve highly generalizable pixel-semantic alignment abil-ity, which is crucial for open-world segmentation. After training with large-scale image-text data, MixReorg mod-els can be applied directly to segment visual objects of ar-bitrary categories, without the need for further fine-tuning.
Our proposed framework demonstrates strong performance on popular zero-shot semantic segmentation benchmarks, outperforming GroupViT by significant margins of 5.0%, 6.2%, 2.5%, and 3.4% mIoU on PASCAL VOC2012, PAS-CAL Context, MS COCO, and ADE20K, respectively. 1.

Introduction
Image segmentation has important applications in sce-narios such as virtual presence, virtual try-on, movie post-*Equal contribution.
†Corresponding author.
Figure 1: Comparison between GroupViT [38] and MixReorg. (a)
GroupViT obtains image segmentation implicitly from image-text pairs to achieve cross-modal semantic alignment. (b) MixReorg explicitly constructs the fine-grained patch-text pairs data from the image-text pairs for free by mixing the patches from different im-ages and preserving the correspondence between patches and text. production, and autonomous driving. Currently, state-of-the-art semantic segmentation methods [35, 30, 6] bene-fit from a large number of densely annotated data. How-ever, the assumption of this closed-world setting requires that all categories of objects that appear in the test set are included in the training set. This heavy dependence on an-notations limits that they can only work well in closed-set settings. However, considering ubiquitous new concepts in real-world scenarios, learning an open-world segmentation model is more practical, but it is also more challenging. The open-world segmentation model is required to segment all entities and objects class-agnostically and exhaustively dur-ing training and be highly generalizable for aligning pixels with new semantics during testing.
Early methods achieve open-world semantic segmenta-tion through few-shot learning [3] or unsupervised cluster-ing [26]. The former actually still assumes that the train-ing and testing classes are in the same latent feature space,
lem. The contextual mixing strategy allows each patch in the mixed image to obtain the global semantics of its origi-nal image in advance by adding a transformer layer before the mixing operation, thereby forcing the model to learn the mixed image reorganization from high-level semantics.
Furthermore, to further enhance the global information in the mixed image features, we propose to use the original image features to enhance the global semantics in the mixed image features. For the second challenge, we present a mix-ing restoration strategy. It guarantees the semantic associ-ation of each patch token in the mixed image with the text through contrastive learning between the image recovered from mixed image and the text.
In this way, the mutual interference between patches from different images in the mixed image can be effectively suppressed.
In general, MixReorg constructs a set of fine-grained patch-text pairs for free from image-text pair data, and successfully builds a cross-modal mixed image patch re-organization mask learner for open-world segmentation tasks. The proposed MixReorg as a good mask learner also shows strong performance compared with the popular zero-shot semantic segmentation baselines, achieving the perfor-mance of 50.5%, 25.4%, 23.6% and 10.1% mIoU on multi-scale evaluations on PASCAL VOC2012, PASCAL Con-text, MS COCO and ADE20K, respectively. The visual-ization in Figure 2 shows that MixReorg significantly out-performs GroupViT [38] on open-world segmentation. Our contributions can be summarized as follows:
• We propose a novel and simple method that can eas-ily construct patch-text data with fine-grained match-ing relationships from image-text data, thereby pro-viding densely supervised information for open-world segmentation.
• For the constructed patch-text data, we propose a cross-modal mixed patch reorganization method.
It successfully addresses the challenge of model failure due to mixed image segmentation susceptible to low-level features and irrelevant patches.
• The proposed MixReorg exhibits strong open-world segmentation performance and significantly outper-forms current state-of-the-art zero-shot segmentation baselines. 2.