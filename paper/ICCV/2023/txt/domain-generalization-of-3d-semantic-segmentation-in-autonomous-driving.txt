Abstract
Using deep learning, 3D autonomous driving semantic segmentation has become a well-studied subject, with meth-ods that can reach very high performance. Nonetheless, because of the limited size of the training datasets, these models cannot see every type of object and scene found in real-world applications. The ability to be reliable in these various unknown environments is called domain generaliza-tion.
Despite its importance, domain generalization is rela-tively unexplored in the case of 3D autonomous driving se-mantic segmentation. To fill this gap, this paper presents the first benchmark for this application by testing state-of-the-art methods and discussing the difficulty of tackling Laser
Imaging Detection and Ranging (LiDAR) domain shifts.
We also propose the first method designed to address this domain generalization, which we call 3DLabelProp. This method relies on leveraging the geometry and sequential-ity of the LiDAR data to enhance its generalization perfor-mances by working on partially accumulated point clouds.
It reaches a mean Intersection over Union (mIoU) of 50.4% on SemanticPOSS and of 55.2% on PandaSet solid-state Li-DAR while being trained only on SemanticKITTI, making it the state-of-the-art method for generalization (+5% and
+33% better, respectively, than the second best method).
The code for this method is available on GitHub: https://github.com/JulesSanchez/3DLabelProp. 1.

Introduction
Since the release of SemanticKITTI [2], 3D semantic segmentation for autonomous driving has been a field of growing interest, resulting in the emergence of a large vari-ety of open-source datasets [3, 27, 21] and high-performing methods [47, 29].
Using LiDAR data instead of the typical 2D images ob-tained from cameras has several advantages. First, LiDAR data are resilient to some sources of 2D domain shifts, such
Figure 1: Illustration of the 3DLabelProp algorithm. With the past inferred sequence and the current registered LiDAR frame given as inputs, 3DLabelProp geometrically propa-gates the static object labels. Then, it clusterizes the current scan and populates it with points from the past sequence to feed small but dense point clouds to the KPConv mod-ule. Finally, predictions from the geometric propagation and deep learning module are combined to obtain the in-ferred LiDAR frame. as the change in illumination. Furthermore, these data yield precise geometric information and reliable distance infor-mation relative to the objects in scenes (down to a few centimeters in error). Nevertheless, deep learning meth-ods dealing with LiDAR data are still sensitive to domain shifts such as scene-type change (urban vs. suburban) or sensor change [17]. This observation is especially relevant for autonomous driving semantic segmentation because the source of the domain shift varies and originates from hard-ware, such as the number of fibers in the LiDAR and its positioning on the vehicles; from firmware such as reflec-tivity calculations; and from the scene itself. LiDAR data are also costly to acquire and label (1,700 hours of labeliza-tion for SemanticKITTI [2]). The accumulation of these issues has clarified the need for domain adaptation and do-main generalization methods to, on the one hand leverage 1
the availability of synthetic data [8, 36] and, on the other hand, perform well on unseen data because it is not feasible to cover every scene type and corner case.
Data adaptation aims to bridge the domain shift with a learning transferable model; hence, data adaptation-methods usually train a model on source data and then per-form an adaptation step, for which they have access to either sparsely labeled or fully unlabeled target data. Conversely, domain generalization methods work in a setup where the target data are fully unavailable.
Although domain adaptation has gained a lot of traction in 3D scene understanding, domain generalization is still relatively unexplored. Furthermore, the disparity between the label sets of various available datasets makes it diffi-cult to compare methods across datasets. In this work, we propose a simple method to evaluate domain generalization performance. With it, we benchmark current state-of-the-art approaches for autonomous driving semantic segmentation.
Furthermore, we propose a novel method to perform se-mantic segmentation while improving generalization per-formances.
Specifically, we develop a domain align-ment method, which accumulates the sequences of LiDAR frames rather than examining scans individually, as is usu-ally done. Although working on such point clouds is slow, we propose several geometry-based mechanisms to speed up the method by propagating the labels of static objects to new points and processing small clusters of points rather than the full point cloud. This method is called 3DLabel-Prop.
We can summarize our contributions as follows:
• We properly introduce the domain generalization prob-lem of 3D semantic segmentation in the autonomous driving field and evaluate current state-of-the-art meth-ods.
• We propose a domain generalization-oriented method for autonomous driving semantic segmentation called 3DLabelProp (Figure 1). 2.