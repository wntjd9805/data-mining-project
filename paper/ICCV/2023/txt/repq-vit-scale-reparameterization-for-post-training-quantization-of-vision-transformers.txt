Abstract
Post-training quantization (PTQ), which only requires a tiny dataset for calibration without end-to-end retraining, is a light and practical model compression technique. Re-cently, several PTQ schemes for vision transformers (ViTs) have been presented; unfortunately, they typically suffer from non-trivial accuracy degradation, especially in low-bit cases. In this paper, we propose RepQ-ViT, a novel PTQ framework for ViTs based on quantization scale reparame-terization, to address the above issues. RepQ-ViT decouples the quantization and inference processes, where the former employs complex quantizers and the latter employs scale-reparameterized simplified quantizers. This ensures both accurate quantization and efficient inference, which dis-tinguishes it from existing approaches that sacrifice quan-tization performance to meet the target hardware. More specifically, we focus on two components with extreme dis-tributions: post-LayerNorm activations with severe inter-channel variation and post-Softmax activations with power-law features, and initially apply channel-wise quantization and log 2 quantization, respectively. Then, we reparame-terize the scales to hardware-friendly layer-wise quantiza-tion and log2 quantization for inference, with only slight accuracy or computational costs. Extensive experiments are conducted on multiple vision tasks with different model variants, proving that RepQ-ViT, without hyperparameters and expensive reconstruction procedures, can outperform existing strong baselines and encouragingly improve the ac-curacy of 4-bit PTQ of ViTs to a usable level. Code is avail-able at https://github.com/zkkli/RepQ-ViT.
√ 1.

Introduction
With the powerful representational capabilities of the self-attention mechanism, vision transformers (ViTs) have recently demonstrated surprising potential in a range of vi-*Corresponding author. sion applications, including image classification [5, 23], ob-ject detection [2, 38], semantic segmentation [28], etc., and are thus being widely investigated as new vision backbones
[8]. However, ViTs rely on heavy and intensive computa-tions, resulting in intolerable memory footprint, power con-sumption, and inference latency, which hinders their de-ployment on resource-constrained edge devices [10, 29].
Consequently, compression techniques for ViTs are essen-tial in real-world applications, particularly where low-cost deployment and real-time inference are desired.
Model quantization, which reduces model complexity by decreasing the representation precision of weights and acti-vations, is an effective and prevalent compression approach
[7, 12]. A notable research line is based on quantization-aware training (QAT) [3, 6], which relies on end-to-end re-training to compensate for the accuracy of the quantized model. Despite the good performance, such retraining re-quires gradient backpropagation and parameter updates on the entire training dataset, which brings undesirably large time and resource costs [20, 33]. Fortunately, another fam-ily of methods, referred to as post-training quantization (PTQ), can overcome the above challenges [32, 16, 26]. It simply takes a tiny unlabeled dataset to calibrate the quan-tization parameters without retraining and thus is regarded as a promising and practical solution.
Although various PTQ methods for convolutional neu-ral networks (CNNs) have been proposed in previous works with good performance, they produce disappointing results on ViTs, with more than 1% accuracy drop even in 8-bit quantization [35]. To this end, several efforts identify the key components that limit the quantization performance of
ViTs, such as LayerNorm, Softmax, and GELU, and pro-pose PTQ schemes accordingly [22, 24]. Nevertheless, when performing ultra-low-bit (e.g., 4-bit) quantization, the performance of these schemes is still far from satisfactory
[4]. The core reason for their low performance is that they invariably follow the traditional quantization paradigm, in which the initial design of the quantizers must account for the future inference overhead. This forces previous meth-ods to carefully design simple quantizers to accommodate the characteristics of the target hardware, even at the cost of remarkably sacrificing accuracy.
Is the traditional quantization-inference dependency paradigm the only option? To answer this question, we ex-plore the feasibility of decoupling the quantization and in-ference processes, and reveal that complex quantizers and hardware standards are not always antagonistic; instead, the two can be explicitly bridged via scale reparameteri-zation. This potentially derives an interesting quantization-inference decoupling paradigm, in which complex quantiz-ers are employed in the initial quantization to adequately preserve the original parameter distributions, and then they are transformed to simple hardware-friendly quantizers via scale reparameterization for actual inference, resulting in both high quantization accuracy and inference efficiency.
With the above insights, we propose a novel PTQ frame-work for ViTs, called RepQ-ViT, in this paper. In RepQ-ViT, we focus on two components with extreme distribu-tions in ViTs that challenge the direct use of simple quan-tizers. Specifically, for post-LayerNorm activations, we ini-tially apply channel-wise quantization to maintain their se-vere inter-channel variation, and then reparameterize the scales to layer-wise quantization to match the hardware, which is achieved by adjusting the LayerNorm’s affine fac-tors and the next layer’s weights; for post-Softmax acti-vations, since our study shows that their power-law distri-2 butions and the properties of attention scores prefer log quantizers, we are interested in reparameterizing the scales to change the base to 2 to enable bit-shifting operations in inference. The overview of the RepQ-ViT framework is il-lustrated in Figure 1. Note that the scale reparameteriza-tion methods presented in this paper enjoy theoretical sup-port, with only a slight accuracy drop compared to complex quantizers or a slight computational overhead compared to simple quantizers, and thus have the potential to ensure in-terpretability and robustness.
√
Our main contributions are summarized as follows:
• We propose a novel PTQ framework for ViTs that es-capes from the traditional paradigm by decoupling the quantization and inference processes, with the former employing complex quantizers and the latter employ-ing scale-reparameterized simplified quantizers, which has great potential in quantizing components with ex-treme distributions in ViTs.
√
• For post-LayerNorm and post-Softmax activations, we 2 quantization, initially apply channel-wise and log respectively, to maintain the original data distribution, and then transform them to simple quantizers via in-terpretable scale reparameterization to match the hard-ware in inference.
• We evaluate RepQ-ViT on various vision tasks, includ-Figure 1. Overview of the RepQ-ViT framework. Building on the quantization-inference decoupling paradigm, for post-LayerNorm and post-Softmax activations, complex quantizers are employed in the quantization process and simple quantizers are employed in the inference process, with scale reparameterization bridging the two. ing image classification, object detection, and instance segmentation, and RepQ-ViT, without hyperparame-ters and expensive reconstruction procedures, can en-couragingly outperform existing baselines. 2.