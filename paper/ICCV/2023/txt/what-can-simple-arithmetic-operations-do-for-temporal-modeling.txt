Abstract
Temporal modeling plays a crucial role in understand-ing video content. To tackle this problem, previous studies built complicated temporal relations through time sequence thanks to the development of computationally powerful de-vices. In this work, we explore the potential of four sim-ple arithmetic operations for temporal modeling. Specifi-cally, we first capture auxiliary temporal cues by comput-ing addition, subtraction, multiplication, and division be-tween pairs of extracted frame features. Then, we extract corresponding features from these cues to benefit the orig-inal temporal-irrespective domain. We term such a simple pipeline as an Arithmetic Temporal Module (ATM), which operates on the stem of a visual backbone with a plug-and-play style. We conduct comprehensive ablation studies on the instantiation of ATMs and demonstrate that this mod-ule provides powerful temporal modeling capability at a low computational cost. Moreover, the ATM is compatible with both CNNs- and ViTs-based architectures. Our results show that ATM achieves superior performance over sev-eral popular video benchmarks. Specifically, on Something-Something V1, V2 and Kinetics-400, we reach top-1 accu-racy of 65.6%, 74.6%, and 89.4% respectively. The code is available at https://github.com/whwu95/ATM . 1.

Introduction
Owing to the rapid development of the Internet and mobile devices, researchers can now work with massive amounts of video data. The time-sequenced data provides substantial information in understanding visual tasks such as human activity recognition. While significant progress has been made in extracting the semantics from a single image, how to model the temporal information effectively becomes an essential problem for video recognition.
Historically, temporal information has been constructed either by utilizing low-level streams (e.g. optical flow, mo-tion vector) [14, 46, 58, 78], or by building segregated time sequence with recurrent networks using features ex-tracted from an image backbone [8, 25, 76]. After that,
Figure 1. (a),(b): A pair of consecutive video frames where a cyan rectangle highlights the area around a moving hand. (c)-(f): Vi-sualization of arithmetic operations (+, −, p, ÷) on consecutive frames. Zoom in for the best view. more works present 3D CNNs [5, 51] and factorized 2D+1D
CNNs [40, 53, 72] as the natural evolution from their 2D counterparts for dealing with 3D volumetric video data directly, thanks to the rapid improvement of computa-tional capacity. Along this research line, many recent pieces of research follow the “2D backbone + temporal interaction” paradigm to develop efficient temporal mod-ules [17, 23, 24, 27, 28, 29, 32, 34, 50, 57, 61, 68, 83]. The strong point of this approach is that, the pre-trained image backbone can serve as a reasonable initialization, reduc-ing training time and enabling the optimization of the en-tire video architecture in an end-to-end manner. Under this paradigm, the ways of temporal modeling can be roughly divided into two categories based on the type of interaction between frames: i) sequence-wise modeling (e.g., temporal convolution [13, 29, 60, 61], dynamic convolution [34, 68], and temporal transformer [1, 2, 33]); ii) pair-wise modeling (e.g., pseudo optical flow estimation [50], similarity estima-tion [23, 24, 56], etc.).
We bring the latter into focus. Precisely, pair-wise mod-eling employs the relationship between an anchor frame and the others. Such relationships are considered to contain ex-pert knowledge. For instance, a few previous works [23, 24, 56] have used correlation operations to estimate the similar-ity between two frames, while another work [50] has com-bined subtraction and Sobel operations to estimate optical flow. Despite the effectiveness of these limited attempts on
CNN backbones [18], the pair-wise modeling has not re-ceived enough attention and exploration when compared to sequence-wise modeling, especially in the current era of the rising popularity of vision transformer backbones [9, 31].
In this study, we aim to fill this gap by revisiting the pair-wise relationship with the most basic arithmetic operations:
Addition (+), Subtraction (−), Multiplication (p), and Di-vision (÷), without extensive elaborate design.
The arithmetic operations are commonly employed in or-dinary vision and time series tasks, and their outputs have explainable physical meaning. Briefly, addition is used to implement integral to obtain properties such as the average intensity of an image [7] or the accumulated status of a fea-ture sequence; On the contrary, subtraction detects changes over time and approximates the tendency of the motion such as optical flow [50]; Next, multiplication can be con-sidered as a measure of similarity or correlation between (regions of) vectorized features of frames [22], such that the features that are unchanged along time shall be high-lighted; Finally, on the opposite of multiplication, the divi-sion between frames locates the features that are changed intensely [55]. For easier understanding, we provide an in-tuition of the physical meaning of these operations using the original frames, as depicted in Figure 1. Notably, our work focuses on exploring these arithmetic operations on encoded feature representations, rather than on raw frames.
In this work, we raise a question: Can simple arithmetic operations be employed for temporal modeling? To answer this question, we first abstract a simple video framework that divides the neural network into a temporal-irrespective stem and a temporal-interactive block. This enables the implementation of the temporal-irrespective stem using ex-isting vision backbones, such as CNNs or Vision Trans-formers. We then propose an Arithmetic Temporal Module (ATM), which conducts arithmetic operations between pairs of frame features to generate auxiliary temporal cues and then transform them back onto the network stem. We or-ganize comprehensive ablation studies on the instantiation of ATMs, w.r.t the choice of operations, temporal context range, structures of feature extractor, and the attached loca-tions. Subsequently, we discover the composite of Subtrac-tion and Multiplication achieve the best performance among the candidates. Finally, we evaluate the proposed method by instantiating it on both representative CNN and Trans-former backbones (i.e., ResNet [18], ViT [9]), to show its generality and superiority. Our contributions are as follows:
• We explore the ascendency of employing fundamen-tal arithmetic operations for temporal modeling, and propose an ad-hoc block named Arithmetic Temporal
Module (ATM) that integrates pair-wise temporal in-teractions between pairs of frame features.
• We conduct a comprehensive exploration of the instan-tiation of ATMs on both CNN backbones and Vision
Transformer backbones.
• We demonstrate that existing backbones plugged with
ATM can achieve superior performance on a broad range of popular benchmarks. Specifically, our method achieves Top-1 accuracy of 65.6% on Something-Something V1, 74.6% on Something-Something V2, and 89.4% on Kinetics-400, respectively. 2.