Abstract
Controllable human image generation (HIG) has numer-ous real-life applications. State-of-the-art solutions, such as ControlNet and T2I-Adapter, introduce an additional learnable branch on top of the frozen pre-trained stable dif-fusion (SD) model, which can enforce various conditions, including skeleton guidance of HIG. While such a plug-and-play approach is appealing, the inevitable and uncer-tain conflicts between the original images produced from the frozen SD branch and the given condition incur signifi-cant challenges for the learnable branch, which essentially conducts image feature editing for condition enforcement.
*Equal contribution. ‡ Work done during an internship at IDEA.
†Corresponding author.
In this work, we propose a native skeleton-guided dif-fusion model for controllable HIG called HumanSD. In-stead of performing image editing with dual-branch dif-fusion, we fine-tune the original SD model using a novel heatmap-guided denoising loss. This strategy effectively and efficiently strengthens the given skeleton condition dur-ing model training while mitigating the catastrophic for-getting effects. HumanSD is fine-tuned on the assem-bly of three large-scale human-centric datasets with text-image-pose information, two of which are established in this work. Experimental results show that HumanSD out-performs ControlNet in terms of pose control and image quality, particularly when the given skeleton guidance is sophisticated. Code and data are available at: https://idea-research.github.io/HumanSD/.
1.

Introduction
Controllable human image generation (HIG) aims to generate human-centric images under given conditions such as human pose [24, 33, 45], body parsing [46, 56], and text [19, 36, 42]. It has numerous applications (e.g., anima-tion/game production [29] and virtual try-on [55]), attract-ing significant attention from academia and industry.
While earlier controllable HIG solutions based on gener-ative adversarial networks (GANs) [10,23–26,33,43,50,53] and variational auto-encoders (VAEs) [7, 14, 34, 45] have been successfully applied in certain applications (e.g., vir-tual try-on), they have not gained mainstream acceptance due to their training difficulties and poor multi-modality fu-sion and alignment capabilities [51]. Recently, diffusion models [12, 35, 40] have demonstrated unprecedented text-to-image generation performance [32] and quickly become the dominant technique in this exciting field. However, it is difficult to provide precise position control with text infor-mation, especially for deformable objects such as humans.
To tackle the above problem, two concurrent controllable diffusion models were proposed in the literature: Control-Net [52] and T2I-Adapter [27]. Both models introduce an additional learnable diffusion branch on top of the frozen pre-trained stable diffusion (SD) model [35]. The addi-tional branch enables the enforcement of various conditions such as skeleton and sketch during image generation, which greatly improves the original SD model in terms of control-lability, thereby gaining huge traction from the community.
However, the learnable branch in such dual-branch dif-fusion models is essentially performing a challenging im-age feature editing task and suffers from several limita-tions. Consider the skeleton-guided controllable HIG prob-lem that generates humans with specific poses. Given text prompts containing human activities, the SD branch may generate various images that are inconsistent with the skele-ton guidance, e.g., humans could present at different places with various poses. Therefore, the extra condition branch needs to learn not only how to generate humans according to the given skeleton guidance but also how to suppress var-ious inconsistencies, making training more challenging and inference less stable. Generally speaking, the larger the gap between skeleton guidance and original images produced by the frozen SD branch, the higher discrepancy between the given guidance and generated human images. Moreover, the inference cost of these dual-branch solutions largely in-creases compared to the original SD model.
In contrast to employing an additional trainable branch for controllable HIG, this work proposes a native skeleton-guided diffusion model, named HumanSD. By directly fine-tuning the SD model [35] with skeleton conditions concate-nated to the noisy latent embeddings, as shown in Figure 2 (a), HumanSD can natively guide image generation with the desired pose, instead of conducting a challenging image editing task. To mitigate the catastrophic forgetting effects caused by model overfitting during fine-tuning, we propose a novel heatmap-guided denoising loss for diffusion mod-els to disentangle between conditioned humans and uncon-ditioned backgrounds in the training stage. Such a disen-tanglement forces the fine-tuning process to concentrate on the generation of foreground humans while minimizing un-expected overrides of the pre-trained SD model parameters that hurt the model’s generation and generalization abilities.
Besides the algorithm, training data is another important factor determining model performance [38]. To improve the HIG quality of HumanSD, we fine-tune our model on three large-scale human-centric datasets containing high-quality images and the corresponding 2D skeletal infor-mation and text descriptions: GHI, LAION-Human, and
Human-Art. Specifically, GHI and LAION-Human are es-tablished in this work. GHI has 1M multi-scenario im-ages generated from SD with crafted prompts, and only the top 30% with the highest image quality are selected. For
LAION-Human, it selects 1M human-centric images from the LAION-Aesthetics [37] via filtering.
The main contributions of this work include:
• We propose a new HIG framework HumanSD with a novel heatmap-guided denoising loss, to natively gen-erate human images with highly precise pose control yet no extra computational costs during inference.
• We introduce two large-scale human-centric datasets with a standard development process, which facilitates multi-scenario HIG tasks with large quantities, rich data distribution, and high annotation quality.
• To demonstrate the effectiveness and efficiency of Hu-manSD, we apply a series of evaluation metrics cov-ering image quality, pose accuracy, text-image consis-tency, and inference speed to compare our model with previous works in a fair experimental setting.
With the above, HumanSD outperforms state-of-the-art solutions such as ControlNet regarding pose control and hu-man image generation quality, particularly when the given skeleton guidance is sophisticated. 2.