Abstract
Deep neural networks (DNNs) are vulnerable to back-door attack, which does not affect the network’s perfor-mance on clean data but would manipulate the network behavior once a trigger pattern is added. Existing de-fense methods have greatly reduced attack success rate, but their prediction accuracy on clean data still lags be-hind a clean model by a large margin.
Inspired by the stealthiness and effectiveness of backdoor attack, we pro-pose a simple but highly effective defense framework which injects non-adversarial backdoors targeting poisoned sam-ples. Following the general steps in backdoor attack, we detect a small set of suspected samples and then apply a poisoning strategy to them. The non-adversarial back-door, once triggered, suppresses the attacker’s backdoor on poisoned data, but has limited influence on clean data.
The defense can be carried out during data preprocessing, without any modification to the standard end-to-end train-ing pipeline. We conduct extensive experiments on multi-ple benchmarks with different architectures and representa-tive attacks. Results demonstrate that our method achieves state-of-the-art defense effectiveness with by far the lowest performance drop on clean data. Considering the surpris-ing defense ability displayed by our framework, we call for more attention to utilizing backdoor for backdoor defense.
Code is available at https://github.com/damianliumin/non-adversarial backdoor. 1.

Introduction
In recent years, deep neural networks (DNNs) have achieved impressive performance across tasks, such as ob-ject detection [36, 33], speech recognition [46, 2] and ma-chine translation [37, 41]. With the increasing usage of
DNNs, security of neural networks has attracted a lot of at-tention. Studies have shown that DNNs are especially vul-nerable to backdoor attack [43], a variant of data poisoning which fools the model to establish a false correlation be-tween inserted patterns and target classes. Specifically, the adversary injects a trigger pattern to a small proportion of
Figure 1: Representations under the effect of adversar-ial backdoor (AB) and non-adversarial backdoor (NAB), which are injected by attackers and defenders respectively.
“Stamp” is the trigger pattern for NAB. (a) Clean samples are not influenced by backdoor. (b) AB changes model be-havior on poisoned samples. (c) NAB is not triggered on clean samples. (d) NAB suppresses the effectiveness of AB on poisoned samples. the training data. A network trained on the poisoned data has normal behavior on benign data, but deviates from its expected output when the trigger pattern is implanted.
To ensure the security of DNN systems, a lot of novel defense methods have been proposed in the past few years.
Most of the defense methods try to either 1) avoid learning the backdoor during training or 2) erase it from a poisoned model at the end. Following idea 1), some studies detect and filter poisoned samples [38, 6]. Since a small num-ber of poisoned samples slipping from detection can lead to a successful attack, simply filtering the potentially poi-soned samples is not enough in most cases. A more realistic way is to adopt data separation as an intermediate procedure
[23, 14]. Some other works pre-process the input to depress the effectiveness of injected patterns [31, 13]. However, these methods have limited effects under the increasingly diverse attacking strategies. Another line of work follows idea 2) [24, 44]. Despite promising defense effectiveness,
erasing-based methods suffer from performance drop due to the additional erasing stage. Performance on clean data still lags behind a clean model by a large margin. Reducing the performance gap on clean data while maintaining satisfying defense effectiveness remains a challenging problem.
Under backdoor attack, representations of poisoned sam-ples are dominated by the trigger pattern as shown in Fig. 1.
Therefore, injecting the pattern can force a poisoned model to behave in a way expected by the attacker. Consider-ing the effectiveness of such strategies, a natural question is whether backdoor can be utilized for defense purpose, that is to say beating backdoor attack at its own game. To be more specific, a model might misbehave when only the trigger pattern is exposed, but the mishehavior should be suppressed once a benign pattern, which is called a stamp in this paper, is injected to the poisoned sample. There are three advantages behind this idea. First, the defender only needs a small set of poisoned training samples to in-ject a backdoor, which is a much easier requirement than filtering all the poisoned data. Second, a backdoor targeting poisoned data, ideally, will not influence the model perfor-mance on clean data. Finally, the backdoor can be injected during data pre-processing, without any modification to the standard end-to-end training pipeline.
In this work, we propose a novel defense framework,
Non-adversarial Backdoor (NAB), which suppresses back-door attack by injecting a backdoor targeting poisoned sam-ples. Specifically, we first detect a small set of suspected samples using existing methods such as [23, 14, 11]. Then we process these samples with a poisoning strategy, which consists of a stamping and a relabeling function. A pseudo label is generated for each detected sample and we stamp the samples with inconsistent orginal and pseudo labels.
In this way, we insert a non-adversarial backdoor which, once triggered, is expected to change model behaviors on poisoned data. Furthermore, NAB can be augmented with an efficient test data filtering technique by comparing the predictions with or without the stamp, ensuring the perfor-mance on poisoned data. We instantiated the NAB frame-work and conducted experiments on CIFAR-10 [16] and tiny-ImageNet [17] over several representative backdoor at-tacks. Experiment results show that the method achieves state-of-the-art performance in both clean accuracy and de-fense effectiveness. Extensive analyses demonstrate how
NAB takes effect under different scenarios.
Our main contributions can be summarized as follows:
• We propose the idea of backdooring poisoned samples to suppress backdoor attack. To the best of our knowl-edge, our work is the first to utilize non-adversarial backdoor in backdoor defense.
• We transform the idea into a simple, flexible and ef-fective defense framework, which can be easily aug-mented with a test data filtering technique.
• Extensive experiments are conducted and our method achieves state-of-the-art defense effectiveness with by far the lowest performance drop on clean data. 2.