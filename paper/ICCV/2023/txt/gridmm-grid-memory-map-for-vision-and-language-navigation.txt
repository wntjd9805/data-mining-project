Abstract
Vision-and-language navigation (VLN) enables the agent to navigate to a remote location following the nat-ural language instruction in 3D environments. To repre-sent the previously visited environment, most approaches for VLN implement memory using recurrent states, topolog-ical maps, or top-down semantic maps. In contrast to these approaches, we build the top-down egocentric and dynam-ically growing Grid Memory Map (i.e., GridMM) to struc-ture the visited environment. From a global perspective, his-torical observations are projected into a unified grid map in a top-down view, which can better represent the spatial rela-tions of the environment. From a local perspective, we fur-ther propose an instruction relevance aggregation method to capture fine-grained visual clues in each grid region. Ex-tensive experiments are conducted on both the REVERIE,
R2R, SOON datasets in the discrete environments, and the
R2R-CE dataset in the continuous environments, showing the superiority of our proposed method. The source code is available at https://github.com/MrZihan/GridMM. 1.

Introduction
Vision-and-language navigation (VLN) tasks [4, 33, 40] require an agent to understand natural language instructions and act according to the instructions. Two distinct VLN scenarios have been proposed, being navigation in discrete environments (e.g., R2R [4], REVERIE [40], SOON [62]) and in continuous environments (e.g., R2R-CE [32], RxR-CE [33]). The discrete environment in VLN is abstracted as the topology structure of interconnected navigable nodes.
With the connectivity graph, the agent can move to an ad-jacent node on the graph by selecting a direction from nav-igable directions. Different from the discrete environments,
VLN in continuous environments require the agent to move through low-level controls (i.e., turn left 15 degrees, turn
Figure 1. Illustration of different methods to represent the environ-ment with maps for VLN. right 15 degrees, or move forward 0.25 meters), which is closer to real-world robot navigation and more challenging.
Whether in discrete environments or continuous envi-ronments, historical information during navigation plays an important role in environment understanding and instruc-tion grounding. In previous works [4, 18, 49, 55, 25], re-current states are most commonly used as historical infor-mation for VLN, which encode historical observations and actions within a fixed-size state vector. However, such con-densed states might be insufficient for capturing essential information in trajectory history. Therefore, Episodic trans-former [39] and HAMT [12] propose to directly encode the trajectory history and actions as a sequence of previ-ous observations instead of using recurrent states. Further-more, in order to structure the visited environment and make global planning, a few recent approaches [10, 14, 34] struc-ture the topological map, as shown in Fig. 1(a). However, these methods are difficult to represent the spatial relations among objects and scenes in historical observations, thus a lot of detailed information is lost. As shown in Fig. 1(b),
more recent works [27, 19, 11, 26] model the navigation environment using the top-down semantic map, which rep-resents spatial relations more precisely. But the semantic concepts are extremely limited due to the pre-defined se-mantic labels. So the objects or scenes, which are not in-cluded in prior semantic labels, cannot be represented, such as the “refrigerator” in Fig. 1(b). Moreover, as illustrated in
Fig. 1(b), the objects with diverse attributes such as “wood table” and “blue couch” cannot be fully expressed by the semantic map which misses object attributes.
In contrast to the above works [10, 14, 19, 26], we pro-pose the Grid Memory Map (i.e., GridMM), a visual repre-sentation structure for modeling global historical observa-tions during navigation. Different from BEVBert [1], who applies local hybrid metric maps for short-term reasoning, our GridMM leverages both temporal and spatial informa-tion to depict the globally visited environment. Specif-ically, the grid map divides the visited environment into many equally large grid regions, and each grid region con-tains many fine-grained visual features. We dynamically construct a grid memory bank to update the grid map dur-ing navigation. At each step of navigation, the visual fea-tures from the pre-trained CLIP [43] model are saved into the memory bank, and all of them are categorized into the grid map regions based on their coordinates calculated via the depth information. To obtain the representation of each region, we design an instruction relevance aggrega-tion method to capture the visual features most relevant to instructions and aggregate them into one holistic fea-ture. With the help of N ×N aggregated map features, the agent is able to accurately conduct the next action planning.
A wealth of experiments illustrate the effectiveness of our
GridMM compared with the previous methods.
In summary, we make the following contributions:
• We propose the Grid Memory Map for VLN to struc-ture the global space-time relations of the visited en-vironment and adopt instruction relevance aggregation to capture visual clues relevant to instructions.
• We comprehensively compare different maps repre-senting the visited environment in VLN and analyze the characteristics of our proposed GridMM, which de-picts more fine-grained information and gives some in-sights into future works in VLN.
• Extensive experiments are conducted to verify the ef-fectiveness of our method in both discrete environ-ments and continuous environments, which show that our method outperforms existing methods on many benchmark datasets. 2.