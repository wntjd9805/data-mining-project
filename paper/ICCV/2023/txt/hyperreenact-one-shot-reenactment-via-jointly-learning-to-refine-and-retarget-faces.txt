Abstract
In this paper, we present our method for neural face reenactment, called HyperReenact, that aims to generate realistic talking head images of a source identity, driven by a target facial pose. Existing state-of-the-art face reen-actment methods train controllable generative models that learn to synthesize realistic facial images, yet producing reenacted faces that are prone to significant visual artifacts, especially under the challenging condition of extreme head pose changes, or requiring expensive few-shot fine-tuning to better preserve the source identity characteristics. We propose to address these limitations by leveraging the pho-torealistic generation ability and the disentangled proper-ties of a pretrained StyleGAN2 generator, by first inverting the real images into its latent space and then using a hy-pernetwork to perform: (i) refinement of the source iden-tity characteristics and (ii) facial pose re-targeting, elimi-nating this way the dependence on external editing meth-ods that typically produce artifacts. Our method oper-ates under the one-shot setting (i.e., using a single source frame) and allows for cross-subject reenactment, without requiring any subject-specific fine-tuning. We compare our method both quantitatively and qualitatively against several state-of-the-art techniques on the standard bench-marks of VoxCeleb1 and VoxCeleb2, demonstrating the su-periority of our approach in producing artifact-free im-ages, exhibiting remarkable robustness even under extreme head pose changes. We make the code and the pretrained models publicly available at: https://github.com/
StelaBou/HyperReenact. 1.

Introduction
The recent developments in deep learning and generative models [24, 25] have led to remarkable progress in the field of facial image synthesis and editing. Among the tasks that have drawn benefit from this progress is neural face reen-actment, that aims to synthesize photorealistic head avatars.
Specifically, given a source and a target image, the goal of face reenactment is to generate a new image that conveys the identity characteristics of the source face and the facial pose (defined as the 3D head orientation and facial expres-sion) of the target face. The key objectives of this task are three-fold: (i) creating realistic facial images that resemble the real ones, (ii) preserving the source identity character-istics, such as the facial shape, and (iii) faithfully transfer-ring the target facial pose. This technology is an essential component within numerous applications of augmented and virtual reality, as well as arts and entertainment industries.
However, despite the recent advancements, most of the ex-isting reenactment methods fail in producing realistic facial images in the one-shot setting (i.e., using a single source frame) or under extreme head pose movements (i.e., large differences in the head pose of the source and the target).
The majority of the state-of-the-art methods in neural face reenactment (e.g., [62, 29, 26]) train controllable mod-els that learn to synthesize realistic images. However, these methods are prone to severe visual artifacts, espe-cially when the source and the target faces have large head pose differences. Most of these methods rely on paired data training (i.e., images of the same identity), limiting their applicability in cross-subject reenactment. Several meth-ods [63, 11, 29, 21] require expensive few-shot fine-tuning (i.e., using multiple different views of the source face) in order to faithfully preserve the source identity and appear-ance. Another line of research leverages the exceptional generation ability of pretrained generative adversarial net-works (GANs) [9, 10, 61], achieving to effectively disentan-gle the identity from the facial pose. However, these works rely on external real image GAN inversion methods and, thus, are bounded by their limitations, such as poor identity reconstruction and image editability [46].
In this paper, we draw inspiration from recent works that combine a GAN generator with a hypernetwork [18] for real image inversion, namely HyperStyle [4] and HyperIn-verter [14]. These methods use a hypernetwork [18], con-ditioned on features derived from the original image and its initial inversion, that learns to modify the weights of the generator to obtain improved image reconstruction quality.
Subsequently, one can perform semantic editing of the re-fined image in the latent space and produce the edited im-age using the updated generator weights. In spite of their high-quality reconstruction results, HyperStyle [4] and Hy-perInverter [14] fail upon applying global editings on the inverted images and, consequently, they are not practically applicable to neural face reenactment.
In order to address the limitations of state-of-the-art works, we tackle the neural face reenactment task by lever-aging the photorealistic image generation and the disentan-gled properties of a pretrained StyleGAN2 [25], along with a hypernetwork [18]. We present a novel method that per-forms both faithful identity reconstruction and effective fa-cial image editing (as shown in Fig. 1) by learning to up-date the weights of a StyleGAN2 generator using a hyper-network approach. Specifically, our model effectively com-bines the appearance features of a source image and the fa-cial pose features of a target image to create new facial im-ages that preserve the source identity and convey the target facial pose.
Overall, the main contributions of this paper can be sum-marized as follows: 1. We present a novel framework for face reenact-ment that leverages the adaptive nature of hypernet-works [18] to alter the weights of a powerful Style-GAN2 [25], so as to perform both: (i) refinement of the source identity details and (ii) reenactment of the source face in the target facial pose. To the best of our knowledge, we are the first to show the effectiveness of merging the steps of inversion refinement and facial pose editing for robust and realistic face reenactment. 2. We demonstrate that our method is able to successfully operate under one-shot settings (i.e. using a single source frame) without requiring any fine-tuning, pre-serving the source identity characteristics on both self and cross-subject reenactment scenarios. This holds true even in challenging cases where the source face is partially self-occluded (i.e., in partial facial views due to highly non-frontal head poses). 3. We show that our method achieves state-of-the-art re-sults even on extreme head pose variations, generating artifact-free images and exhibiting remarkable robust-ness to large head pose shifts. 4. We conduct experiments on the standard benchmarks of VoxCeleb1 and VoxCeleb2 [30, 12], performing qualitative and quantitative comparisons with existing state-of-the-art reenactment techniques. We show that our proposed method achieves compelling results both on identity preservation and facial pose transfer. 2.