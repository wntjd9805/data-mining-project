Abstract
Recent implicit neural representations have shown great results for novel view synthesis. However, existing methods require expensive per-scene optimization from many views hence limiting their application to real-world unbounded urban settings where the objects of interest or backgrounds are observed from very few views. To mitigate this chal-lenge, we introduce a new approach called NeO 360, Neu-ral fields for sparse view synthesis of outdoor scenes. NeO 360 is a generalizable method that reconstructs 360◦ scenes from a single or a few posed RGB images. The essence of our approach is in capturing the distribution of complex real-world outdoor 3D scenes and using a hybrid image-conditional triplanar representation that can be queried from any world point. Our representation combines the best of both voxel-based and bird’s-eye-view (BEV) representa-tions and is more effective and expressive than each. NeO 360’s representation allows us to learn from a large collec-tion of unbounded 3D scenes while offering generalizability to new views and novel scenes from as few as a single image during inference. We demonstrate our approach on the pro-posed challenging 360◦ unbounded dataset, called NeRDS 360, and show that NeO 360 outperforms state-of-the-art generalizable methods for novel view synthesis while also offering editing and composition capabilities. Project page: zubair-irshad.github.io/projects/neo360.html 1.

Introduction
Advances in learning-based implicit neural representa-tions have demonstrated promising results for high-fidelity novel-view synthesis [64], doing so from multi-view images [60, 62]. The capability to infer accurate 3D scene representations has benefits in autonomous driving [42, 16] and robotics [22, 41, 21].
Despite great progress in neural fields [57] for indoor novel view synthesis, these techniques are limited in their ability to represent complex urban scenes as well as decom-pose scenes for reconstruction and editing. Specifically, previous formulations [6, 46, 73] have focused on per-scene optimization from a large number of views, thus increasing their computational complexity. This requirement limits their application to complex scenarios such as data cap-Object-Centric
DVR [40]
IDR [68]
NeUS [62]
NeDDF [60]
DM-NeRF [61]
Neural-Scene-Graphs [43]
Overfitting
Object-NeRF [66]
Neuman [24]
Panoptic Neural Fields [28]
MipNeRF360 [4]
NeRF-VAE [27]
NeRF [35]
Scene-centric
DeepSDF [45]
ShAPO [22]
CodeNeRF [23]
AutoRF [36]
CARTO [20]
NeO 360 (Ours)
Generalizable
SSSR [72]
IBRNet [63]
SRT [50]
DeFiNE [18]
PixelNeRF [69]
Figure 2: Taxonomy of implicit representation methods reconstructing appearance and shapes. The x and y corre-spond to (Generalizable vs Overfitting) and (Object-Centric vs Scene-Centric) dimensions, as discussed in Section 1. tured by a moving vehicle where the geometry of interest is observed in just a few views. Another line of work focuses on object reconstructions [36, 22] from single-view
RGB (Fig. 2). However, these approaches require accurate panoptic segmentation and 3D bounding boxes as input which is a strong supervisory signal and consists of multi-stage pipelines that can lead to error-compounding.
As shown in Fig. 1, our approach extends
To avoid the challenge of acquiring denser input views of a novel scene in order to obtain an accurate 3D repre-sentation as well as the computational expense of per-scene optimization from many views, we propose to infer the rep-resentation of 360◦ unbounded scenes from just a single or a few posed RGB images of a novel outdoor environment. the
NeRF++ [73] formulation by making it generalizable. At the core of our method are local features represented in the form of triplanes [9]. This representation is constructed as three perpendicular cross-planes, where each plane models the 3D surroundings from one perspective and by merg-ing them, a thorough description of the 3D scene can be achieved. NeO 360’s image-conditional tri-planar repre-sentation efficiently encodes information from image-level features while offering a compact queryable representation for any world point. We use these features combined with the residual local image-level features to optimize multiple unbounded 3D scenes from a large collection of images.
NeO 360’s 3D scene representation can build a strong prior for complete 3D scenes, which enables efficient 360◦ novel view synthesis for outdoor scenes from just a few posed
RGB images.
To enable building a strong prior representation of un-bounded outdoor scenes and given the scarcity of available multi-view data to train methods like NeRF, we also present
Figure 3: Samples from our large scale NeRDS 360: ”NeRF for Reconstruction, Decomposition and Scene Synthesis of 360◦ outdoor scenes” dataset comprising 75 unbounded scenes with full multi-view annotations and diverse scenes. a new large scale 360◦ unbounded dataset (Figure 3) com-prising of more than 70 scenes across 3 different maps. We demonstrate our approach’s effectiveness on this challeng-ing multi-view unbounded dataset in both few-shot novel-view synthesis and prior-based sampling tasks. In addition to learning a strong 3D representation for complete scenes, our method also allows for inference-time pruning of rays using 3D ground truth bounding boxes, thus enabling com-positional scene synthesis from a few input views. In sum-mary, we make the following contributions:
• A generalizable NeRF architecture for outdoor scenes based on tri-planar representation to extend the
NeRF formulation for effective few-shot novel view synthesis of 360◦ unbounded environments.
• A large scale synthetic 360◦ dataset, called NeRDS 360, for 3D urban scene understanding compris-ing multiple objects, capturing high-fidelity outdoor scenes with dense camera viewpoint annotations.
• Our proposed approach significantly outperforms all baselines for few-shot novel view synthesis on the
NeRDS 360 dataset, showing 1.89 PNSR and 0.11
SSIM absolute improvement number for the 3-view novel-view synthesis task. 2.