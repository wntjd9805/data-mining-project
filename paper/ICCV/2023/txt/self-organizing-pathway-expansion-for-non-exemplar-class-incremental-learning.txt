Abstract
Non-exemplar class-incremental learning aims to rec-ognize both the old and new classes without access to
The conflict between old and new old class samples. class optimization is exacerbated since the shared neural pathways can only be differentiated by the incremental samples. To address this problem, we propose a novel self-organizing pathway expansion scheme. Our scheme consists of a class-specific pathway organization strategy that reduces the coupling of optimization pathway among different classes to enhance the independence of the feature representation, and a pathway-guided feature optimization mechanism to mitigate the update interference between the old and new classes. Extensive experiments on four datasets demonstrate significant performance gains, outperforming the state-of-the-art methods by a margin of 1%, 3%, 2% and 2%, respectively. 1.

Introduction
Since deep neural networks have achieved good perfor-mance in fully supervised scenarios, how to extend this learning capability to open environment has attracted great attention. Particularly, it is essential to ensure that the network can continuously learn new knowledge while main-taining the abilities to identify old tasks (i.e., incremental learning [8, 18]). Fine-tuning the network directly with new data can lead to a serious bias of the representation and classifier, which is often referred to as catastrophic forgetting. Due to privacy and hardware limits, old samples are usually unavailable for joint training, making it more difficult to maintain the old class performance in the sub-In this paper, we focus on sequent optimization process. this ability to continuously learn new tasks without any old
∗Co-first Author. †Corresponding Author. samples or exemplars, which is called non-exemplar class-incremental learning (NECIL) [25, 27, 30, 31, 33].
Most methods maintain the feature representation of old classes by means of various distillation loss functions
[8, 10]. Although catastrophic forgetting is somewhat mitigated, incremental performance still suffers from the confusion between the old and new class in the feature space. Furthermore, in the absence of old class samples, the degree of forgetting is only related to the initial model and incremental samples [31]. Existing NECIL works
[25,30] mainly focus on enhancing the overall performance by improving the discrimination and generalization of the initial model, which brings a significant improvement on the incremental performance.
Instead, we focus on the impact of incremental samples on the optimization process.
Intuitively, since different incremental classes cause disparate feature confusion, the interference on the old class performance is also different even if initialized from the same model [31, 33]. To further explore the association, we estimate the inter-class confusion by measuring the status of feature activation [29] in existing incremental model. As shown in Fig. 1 (b), we filter out the positions of strongly activated modules as the class-specific pathways [19], and find that the pathway of incremental class is commonly confused with the previous ones in the baseline. Furthermore, it can be seen in Fig. 1 (a) that the degree of pathway overlap (i.e., similarity) between the old class and incremental class is positively correlated with the forgetting degree, which motivates us to address the interference problem from the perspective of pathway optimization.
Based on the above observation, we propose a self-organizing pathway expansion scheme to learn a pathway-aware representation, mitigating the feature interference during the subsequent incremental process. The scheme is mainly manifested in two aspect. Firstly, during the initial phase, we adopt the class-specific pathway organization
(a) Correlation statistics. (b) The t-SNE visualization of filtered pathways.
Figure 1. Motivation of our method. (a) The accuracy degradation of old classes (i.e. forgetting rates in the horizontal coordinate) is positively correlated to the corresponding pathway similarity with incremental classes. The concept of pathway [19] is formed from the aggregation of important modules, which are filtered out by the contribution to the final recognition performance. (b) In standard classification method (i.e., baseline in Sec. 3.2), the direct adoption of an activation-like rule [33] makes it hard to measure inter-class overlap as the distribution implicitly optimized for classes is haphazard. In contrast, the discriminative pathway in our method brings out lower inter-class overlap, which benefits the mitigation of feature confusion. All above experiments are conduct on ImageNet-Full dataset. strategy to enhance the independence of feature represen-tation by forcing the optimization pathways specific to different classes. A global pathway planner is utilized to explicitly select the most relevant modules, facilitating the pathway identification. It is noted that we do not modify the network structure, but only divide the output channels of each convolution module to match the output of the pathway planner. Secondly, during the incremental phases, we introduce a pathway-guided feature update mechanism to promote the effectiveness of new classes involved in incremental optimization by adjusting the classification weight with the pathway similarity. Since the pathway value is either 0 or 1, we calculate the intersection of union (i.e.,
IoU) value to better measure the class relevance, reducing the interference of vector normalization.
Furthermore, an incremental pathway update mechanism is proposed to ensure the long-term effect by alternating the optimization of the pathway planner and feature representation. To summarize, our main contributions are as follows: 1) A self-organizing pathway expansion scheme is pro-posed for non-exemplar incremental learning, in which a progressive decoupling optimization is accomplished by a class-specific pathway organization strategy, resulting in a pathway-aware representation. 2) A pathway-guided feature update mechanism is pro-posed, which utilizes the similarity of pathways to guide the optimization of incremental samples. 3) Extensive experiments are performed on benchmark including CIFAR-100, TinyImageNet,
ImageNet-Subset and ImageNet-Full datasets, and the results demonstrate the superiority of our method over the state-of-the-art. requires the network to learn new tasks without forgetting the old knowledge to achieve the stability-plasticity trade-off. Class-incremental learning (CIL [8, 9, 18, 23, 24]), a difficult type in continual learning, has attracted much attention due to the agnosticism to task identity [21, 22].
Recently, some works [25,27,30,31] focus on a challeng-ing but practical NECIL problem, where no past data can be stored due to equipment limits or privacy security. [27] estimates the semantic drift of the initial model inherited from the base phase, and compensates the prototypes in each test phase. [25] inverts the old samples from the initial model for the joint distillation. [30, 31] consider to enhance the generalization of the representation to learn more trans-ferable features for future tasks. We follow their NECIL settings. However, different from their work focusing on the utilization and enhancement of the initial model, we mainly consider the rectification of the incremental samples on joint classification and distillation process. 2.2. Neural Pathways
To enhance the adaptation of the network to new tasks, several continual learning methods [4, 17] have been pro-posed to decouple the learning process from the perspective of pathway. However, the targeted models are continuously expanded with the update of pathway, which is difficult to adapt to the standard classification network. The expansion direction of pathway tends to be selected randomly, making it hard to search for an explanation.
In this paper, we target on the pathway learning on the standard network without changing the structure, and guiding the incremental optimization based on the pathway relationship. 2.