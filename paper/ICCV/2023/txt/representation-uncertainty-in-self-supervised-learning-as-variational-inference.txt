Abstract
In this study, a novel self-supervised learning (SSL) method is proposed, which considers SSL in terms of vari-ational inference to learn not only representation but also representation uncertainties. SSL is a method of learning representations without labels by maximizing the similar-ity between image representations of different augmented views of an image. Meanwhile, variational autoencoder (VAE) is an unsupervised representation learning method that trains a probabilistic generative model with variational inference. Both VAE and SSL can learn representations with-out labels, but their relationship has not been investigated in the past. Herein, the theoretical relationship between
SSL and variational inference has been clariﬁed. Further-more, a novel method, namely variational inference SimSiam (VI-SimSiam), has been proposed. VI-SimSiam can predict the representation uncertainty by interpreting SimSiam with variational inference and deﬁning the latent space distribu-tion. The present experiments qualitatively show that VI-SimSiam could learn uncertainty by comparing input images and predicted uncertainties. Additionally, we described a re-lationship between estimated uncertainty and classiﬁcation accuracy. 1.

Introduction
Self-supervised learning (SSL) is a framework for learn-ing representations of data [6, 14, 20, 7, 5, 61, 50, 37, 28, 39].
This method enables training of high-performance models in downstream tasks (e.g., image classiﬁcation and object detection) without substantial manually labeled data through pre-training the network to generate features. It can mitigate the annotation bottleneck, one of the crucial barriers to the practical application of deep learning. Some state-of-the-*Equal contribution
Low uncertainty images g (cid:2018) (cid:3404) (cid:883)(cid:886)(cid:884)(cid:891) (cid:2018) (cid:3404) (cid:883)(cid:887)(cid:885)(cid:891) (cid:2018) (cid:3404) (cid:883)(cid:886)(cid:885)(cid:888)
High uncertainty images g (cid:2018) (cid:3404) (cid:886)(cid:886)(cid:884) (cid:2018) (cid:3404) (cid:887)(cid:884)(cid:890) (cid:2018) (cid:3404) (cid:886)(cid:889)(cid:887)
Figure 1: Input images and the predicted uncertainty parame-ter κ. Our method learns not only representations of images but also uncertainties in them without supervision. The lower the κ, the higher the uncertainty. The images with a low κ appear to have less salient features than those with a high κ. art SSL methods, such as SimSiam [6], SimCLR [5], and
DINO [4], train image encoders by maximizing the similar-ity between representations of different augmented views of an image.
The probabilistic generative models with variational in-ference provide another approach for representation learn-ing [27]. This approach learns latent representations in an unsupervised fashion by training inference and generative models (i.e., autoencoders) together. It can naturally incor-porate representation uncertainty by formulating them as probabilistic distribution models (e.g., Gaussian). However, the pixel-wise objective for reconstruction is sensitive to rare samples [32] in such methods. Furthermore, this repre-sentation learning is recently found to be less competitive than the SSL methods on the benchmarking classiﬁcation tasks [32, 37]. Although SSL and variational inference seem
highly related learning representations without supervision, their theoretical connection has not been fully explored.
In this study, we incorporate the variational inference concept to make the SSL uncertainty-aware and conduct a detailed representation uncertainty analysis. The contribu-tions of this study are summarized as follows.
• We clarify the relationship between SSL (i.e., SimSiam,
SimCLR, and DINO) and variational inference, gener-alizing the SSL methods as the variational inference of spherical or categorical latent variables (§4).
• We derive a novel SSL method called variational in-ference SimSiam (VI-SimSiam) by incorporating the above relationship. It learns to predict not only repre-sentations but also their uncertainty (§ 5).
• We demonstrate that VI-SimSiam successfully esti-mates uncertainty without labels while achieving com-petitive classiﬁcation performance with SimSiam. We qualitatively evaluate the uncertainty estimation capa-bility by comparing input images and the estimated uncertainty parameter κ, as shown in Fig. 1. In addi-tion, we also describe that the predicted representation uncertainty κ is related to the accuracy of the classiﬁca-tion task (§ 6).
A comparison of SimSiam and VI-SimSiam is illustrated in Fig. 2, where VI-SimSiam estimates the uncertainty by predicting latent distributions. 2.