Abstract
In our pursuit of advancing multi-modal AI assistants ca-pable of guiding users to achieve complex multi-step goals, we propose the task of ‘Visual Planning for Assistance (VPA)’. Given a succinct natural language goal, e.g., “make a shelf”, and a video of the user’s progress so far, the aim of VPA is to devise a plan, i.e. a sequence of actions such to realize the speci-as “sand shelf”, “paint shelf”, etc.
ﬁed goal. This requires assessing the user’s progress from the (untrimmed) video, and relating it to the requirements of natural language goal, i.e. which actions to select and in what order? Consequently, this requires handling long video history and arbitrarily complex action dependencies.
To address these challenges, we decompose VPA into video action segmentation and forecasting. Importantly, we ex-periment by formulating the forecasting step as a multi-modal sequence modeling problem, allowing us to leverage the strength of pre-trained LMs (as the sequence model).
This novel approach, which we call Visual Language Model based Planner (VLaMP), outperforms baselines across a suite of metrics that gauge the quality of the generated plans. Furthermore, through comprehensive ablations, we also isolate the value of each component – language pre-training, visual observations, and goal information. We have open-sourced all the data, model checkpoints, and training code. 1.

Introduction
Imagine assembling a new piece of furniture or follow-ing a new recipe for a dinner party. To achieve such a goal, you might follow a manual or a video tutorial, going back and forth as you perform the steps.
Instead of fumbling through a manual, imagine an assistive agent capable of be-ing invoked through natural language, having the ability to understand human actions, and providing actionable multi-step guidance for achieving your desired goal. Such multi-*equal mentoring
†corresponding author
Human
VLaMP
I want to build shelf.
“What should I  do next?” untrimmed  visual history
Your next steps:  1. cut shelf 2. sand shelf 3. paint shelf
Segmentation
Forecasting
The picture can't be  displayed.
The picture can't be  displayed.
Visual 
History t i l p
S y r o t s i
H
Observations
Actions
Obs. 
Encoder
Action 
Encoder
Goal Prompt: “Build a Shelf”
Pretrained
Language 
Model
Figure 1: Visual Planning for Assistance overview (top)
Given a user-and general methodology (bottom). speciﬁed, natural language goal (“build a shelf”) and corre-sponding visual history of the user’s progress till now, VPA involves predicting a sequence of actions, to assist the user towards the goal (“cut”, “sand”, “paint”). Our approach is based on multi-modal sequence modeling where we reuse pre-trained video segmentation and language models. modal assistive agents should be able to reason human ac-tivities from visual observations, contextualize them to the goal at hand, and plan future actions for providing guidance.
To quantify the progress and aid the development of such multi-modal neural models, we need a intuitive task, as il-lustrated in the example in Fig. 1 (top). We call this Visual
Planning for Assistance (VPA) that we detail next. Given a user-speciﬁed goal in natural language (“build shelf”) and corresponding video observations of the user’s progress towards this goal, the task objective should be to gener-ate the ordered sequence of next actions towards achiev-ing the goal (“cut paint”). We base VPA off
! instructional YouTube videos of such procedural activities from large, open-sourced datasets – CrossTask [89] and
COIN [73]. This is a natural choice as procedural hu-man activities (cooking, assembly, repair, etc.) are a per-fect source of multi-step and complex sequence of actions, where humans routinely seek guidance. Realistic nature sand
!
of VPA makes it particularly challenging. Operating on untrimmed videos requires dealing with potentially many ir-relevant background frames, chunking actions in the videos will also be imperative. Another challenge is the validity of the plan of actions i.e. they must respect the constraints of the activity – the shelf shouldn’t be painted before sanding.
The natural next question is – what’s a good way to tackle VPA? Marrying research from video forecasting & anticipation with embodied AI [19, 17], we cast VPA as goal-conditioned task planning. We approach VPA by utilizing video action segmentation and transformer-based neural sequence modeling – the former allows us to deal with long video history and the latter can handle arbitrary sequential constraints [86, 39]. The formulation (particu-larly, transformer-based neural sequence modeling) allows us to tap into the prowess of pre-trained language mod-els (PTLMs). These PTLMs contain useful priors about action-action similarity, action-goal association, and action ordering [7, 30, 58], that our formulation can piggy-back off. Particularly, our model – Visual Language Model Plan-ner (VLaMP) – conditions the generated plan onto the vi-sual history by using a transformer-based mapper network that projects embeddings corresponding to visual history into the input space of the LM. Using VPA as the testbed, we show that VLaMP outperforms a range of standardized baselines. We undertake head-on ablations to quantify ef-fect of each component of VLaMP – language pre-training of the LM, the visual history, and goal description. We be-lieve the modularity of our approach can allow researchers to swap components with their own, to made rapid progress towards VPA.
In summary, our main contributions are: (1) a new task VPA on human interaction videos capturing unique as-pects of real-world vision-powered assistive agents; (2) a general-purpose methodology for VPA, which allows lever-aging pre-trained multi-modal encoders and sequence mod-els; (3) an instantiation of this methodology (we call
VLaMP), where we reuse sequence priors from PTLM and investigate its efﬁcacy across two, well-studied datasets of procedural human activities. 2.