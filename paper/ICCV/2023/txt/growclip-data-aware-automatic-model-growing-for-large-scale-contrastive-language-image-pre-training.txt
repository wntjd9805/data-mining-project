Abstract
Cross-modal pre-training has shown impressive perfor-mance on a wide range of downstream tasks, benefiting from massive image-text pairs collected from the Internet.
In practice, online data are growing constantly, highlighting the importance of the ability of pre-trained model to learn from data that is continuously growing. Existing works on cross-modal pre-training mainly focus on training a net-work with fixed architecture. However, it is impractical to limit the model capacity when considering the continu-ously growing nature of pre-training data in real-world ap-plications. On the other hand, it is important to utilize the knowledge in the current model to obtain efficient training and better performance. To address the above issues, in this paper, we propose GrowCLIP, a data-driven automatic model growing algorithm for contrastive language-image pre-training with continuous image-text pairs as input. Spe-cially, we adopt a dynamic growth space and seek out the optimal architecture at each growth step to adapt to online learning scenarios. And the shared encoder is proposed in our growth space to enhance the degree of cross-modal fu-sion. Besides, we explore the effect of growth in different dimensions, which could provide future references for the design of cross-modal model architecture. Finally, we em-ploy parameter inheriting with momentum (PIM) to main-tain the previous knowledge and address the issue of the local minimum dilemma. Compared with the existing meth-ods, GrowCLIP improves 2.3% average top-1 accuracy on zero-shot image classification of 9 downstream tasks. As for zero-shot image retrieval, GrowCLIP can improve 1.2% for top-1 image-to-text recall on Flickr30K dataset.
*Corresponding author.
Figure 1. Top-1 accuracy (%) of zero-shot image classification on
ImageNet of GrowCLIP and baselines during training at step 4, where the horizontal dotted lines mean the process of supernet training. Our GrowCLIP has the best performance and is more efficient compared with other baselines. 1.

Introduction
Recently, large-scale pre-trained models illustrate the potential performance among different fields including computer vision (CV) [24, 25, 11] and natural language pro-cessing (NLP) [15, 44, 33]. Cross-modal models like CLIP
[52], ALIGN [28], FILIP [65], BLIP [36] also demonstrate remarkable success across various vision-language down-stream tasks. Note that these models are usually built in dual-stream architectures, which consist of the image and text encoders to extract the feature of image and text inputs, respectively. The alignment between these two features is then performed under the contrastive objective to learn the alignment between different modals.
However, training these cross-modal models requires a large amount of image-text pairs collected from the In-ternet, e.g., 400M image-text pairs for CLIP [52] and 300M for FILIP [65]. Most existing cross-modal methods
[52, 28, 65, 39, 36, 68, 63, 1] directly train the final large-CC12M 10% 20% 50% 100%
CLIP-0.5-ViT-B/16
CLIP-ViT-B/16 6.6 6.5 11.1 11.6 19.0 20.9 25.7 28.3
Table 1. Result of different model sizes with different data sizes:
Top-1 accuracy (%) of zero-shot image classification on ImageNet.
The depth and the width of 0.5-ViT-B/16 are both one half of ViT-B/16. The relative performance ranking is dependent on the size of the training data. scale model using the completed image-text pair datasets collected at some point in time. However, it is impracti-cal to regard cross-modal pre-training as “disposable” train-ing without considering the continuously growing nature of pre-training data or domain in real-world applications.
For instance, mountains of new knowledge is generated on the Internet every day, which can be used to further im-prove the performance of existing pre-trained cross-modal models. Therefore, in this paper, we consider the training of large-scale cross-modal pre-trained models as an online learning problem [17, 56, 57] with continuous image-text pairs as input. Different from the standard continual learn-ing setting [59, 55, 22, 31, 2, 7], the previous image-text pairs can also be achieved in our setting since we assume the size of training data is gradually increased over time.
One tough problem in the online learning scenario is that the model capacity should be related to the size of training data [16, 69, 29, 26]. For example, it’s observed that large
ViT models perform worse than ResNets when pre-trained on small datasets, while the result is opposite when they are both pre-trained on larger datasets [16]. To verify the re-lationship between model capability and data size, we split
Conceptual 12M (CC12M) dataset [9] and test the perfor-mance of ViT-B/16 with different scales [52]. As shown in
Table 1, the relative performance ranking is dependent on the size of the training data. Given a smaller dataset (e.g., 10% CC12M), the performance of 0.5-ViT-B/16 is compa-rable with ViT-B/16.
In contrast, when data is sufficient (e.g., 100% CC12M), the performance of ViT-B/16 is bet-ter than the smaller model. Thus, the unchanged model is not practical for this real-time learning setting and it’s still an open issue on how to modify and train our model with the growing data. Besides, how to efficiently make use of knowledge of previous model when new data is coming re-mains an open problem. One direct solution is to fine-tune the model with the updated training dataset. However, train-ing with previous pre-trained parameters of the same model will deteriorate the performance [4] due to the parameter inheriting issue. As shown in Figure 1, the CLIP training with pre-training (TWP) have worse performance than the one training from scratch (TFS).
To address the above issues, we propose a data-aware au-tomatic model growing method (denoted as GrowCLIP) for large-scale contrastive language-image pre-training, which performs a model growth process considering the grad-ually increased pre-training data. Specially, when train-ing data grows dynamically, we adopt different cross-modal network architectures via the customized neural ar-chitecture search (NAS) to make the network pre-training more efficient. Different from traditional NAS approaches
[70, 53, 64], we introduce a cross-modal customized NAS by defining a dynamic search space named growth space, which is enlarged when more data comes, and proposing a shared encoder search space to enhance the degree of cross-modal fusion. To utilize the architecture at the previous step more efficiently, the parameters of the new architecture are also inherited from the old one with momentum to maintain the previous knowledge and address the issue of local min-imum dilemma. Finally, growth architecture selection pro-cedure is performed to select the optimal model architecture at each step, considering the performance and model size.
Experiments are conducted by averagely dividing the
Conceptual 12M (CC12M) into 4 growth steps under the online learning setting. As depicted in Table 1, compared with the existing methods, our GrowCLIP has the best per-formance and is more efficient. Specially, experimental re-sults show that our GrowCLIP can improve up to 2.3% av-erage top-1 accuracy on zero-shot image classification of 9 downstream tasks compared with the existing methods. As for zero-shot image-text retrieval, GrowCLIP has a 1.2% improvement for top-1 image-to-text recall on Flickr30K
[50] dataset and 0.8% on MSCOCO [40].
To summarize, the contributions of this paper are listed as follows: (i) To adapt to the growing data scenario, we propose a data-aware automatic model growing method, named GrowCLIP. (ii) We provide some insights for the design of cross-modal model architecture. (iii) The exten-sive experiment results illustrate the effectiveness of our ap-proach on zero-shot classification and retrieval tasks. 2.