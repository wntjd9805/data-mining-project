Abstract
Large, pretrained models are commonly finetuned with imagery that is heavily augmented to mimic different condi-tions and scales, with the resulting models used for various tasks with imagery from a range of spatial scales. Such models overlook scale-specific information in the data for scale-dependent domains, such as remote sensing. In this paper, we present Scale-MAE, a pretraining method that ex-plicitly learns relationships between data at different, known scales throughout the pretraining process. Scale-MAE pre-trains a network by masking an input image at a known input scale, where the area of the Earth covered by the image deter-mines the scale of the ViT positional encoding, not the image resolution. Scale-MAE encodes the masked image with a standard ViT backbone, and then decodes the masked image through a bandpass filter to reconstruct low/high frequency images at lower/higher scales. We find that tasking the net-work with reconstructing both low/high frequency images leads to robust multiscale representations for remote sensing imagery. Scale-MAE achieves an average of a 2.4 − 5.6% non-parametric kNN classification improvement across eight remote sensing datasets compared to current state-of-the-art and obtains a 0.9 mIoU to 1.7 mIoU improvement on the
SpaceNet building segmentation transfer task for a range of evaluation scales. 1.

Introduction
Remote sensing data is captured from satellites and planes through a mixture of sensors, processing pipelines, and view-ing geometries. Depending on the composition and relative geometry of the sensor to the Earth, each image’s Ground
Sample Distance (GSD - the physical distance between two
*Denotes co-first authorship. Co-first authors will prioritize their names on their resumes/websites.
Figure 1. Scale-MAE learns better representations for multiscale tasks compared to vanilla MAE. (Column 1) The top image spans an area at 0.3m GSD and the bottom image shows the same region at a coarser GSD. (Columns 2-4) The following columns show a ground truth building segmentation, Scale-MAE segmentation from a finetuned UperNet, and segmentation from an analogously finetuned UperNet from a vanilla MAE, respectively. Scale-MAE demonstrates better performance across images at both scales. See the supplementary material for more examples. adjacent pixels in an image) can vary from 0.3m to 1km, so a 100x100 pixel image could span anywhere from an Olympic-size swimming pool (900 m2) to almost the entire country of
Jamaica (10,000 km2). The data within each image, and the corresponding objects and points of interest, can therefore vary across wide spatial ranges. Data from these multiscale sensors provide critical and complementary information for various operational and research applications in areas such as atmospheric, hydrologic, agricultural, and environmental monitoring [45, 52].
Few modern computer vision methods have explicitly ad-dressed multiscale remote sensing imagery [35]. Neverthe-less, the remote sensing vision community has increasingly used large, pretrained models [13, 20], where such appli-cations finetune a pretrained model for a single source of
Figure 2. Scale-MAE employs the Masked Autoencoder framework. An input image is patchified and masked before being passed into an
MAE encoder. A Ground Sample Distance Positional Encoding (GSDPE) is added to the encoder input, which scales the positional encodings to the area of ground covered. The Scale-MAE decoders has three stages: (1) Decoding, which uses a smaller number of transformer layers than MAE to decode the encoded values (2) Upsampling, which progressively deconvolves the decoded feature map to a larger size before being passed through the Laplacian Blocks (abbreviated LB, see Section 3), (3) Reconstruction, which then reconstructs low and high frequency features at different scales. These outputs are used to compute an aggregate loss with ground truth low and high frequency features, where following super resolution literature [2], an L1 loss is used for high frequency output to better reconstruct edges and an L2 loss is used for low frequency output to better reconstruct average values. data at a specific scale [13, 20, 22, 32, 41]. In this paper we present Scale-MAE, a masked reconstruction model that ex-plicitly learns relationships between data at different, known scales throughout the pretraining process. By leveraging this information, Scale-MAE produces a pretrained model that performs better across a wide range of GSDs and tasks.
Masked Autoencoders [26] offer self-supervised learn-ing without explicit augmentations. A standard Masked
Autoencoder resizes/crops an image, masks the majority of the transformed image, and then tasks a Vision Transformer (ViT) based autoencoder with embedding the unmasked com-ponents. A decoding ViT then decodes the full image from these learned embeddings, where the decoder is later dis-carded and the encoder is used to produce representations for an unmasked input image.
Existing MAE-based pretraining approaches fail to gen-eralize across domains with images at multiple scales.
Scale-MAE (Figure 1) overcomes this through a GSD-based positional encoding derived from the land area covered in the image. This informs the ViT of both the position and scale of the input image. Scale-MAE also uses a Laplacian-pyramid decoder to encourage the network to learn multiscale rep-resentations. The embeddings are decoded to two images containing low and residual high frequency information, re-spectively – see Figure 2. As we discuss in Section 3, this structure allows the ViT decoder to use fewer parameters than MAE while still producing strong representations across multiple scales.
We show that Scale-MAE leads to better performing, more robust multiscale representations than both a stan-dard MAE and a recently proposed, state-of-the-art MAEs
SatMAE [13] and ConvMAE [21] across remote sensing datasets with a variety of scale and resolution characteristics.
To the best of our knowledge Scale-MAE is the first self-supervised MAE to include scale-aware positional encoding and Laplacian pyramids. In our experiments, Scale-MAE achieves an average of a 5.6% nonparametric kNN classifica-tion improvement across eight remote sensing datasets com-pared to current state-of-the-art in addition to a 0.9 mIoU to 1.7 mIoU improvement on the SpaceNet building seg-mentation transfer task for a range of evaluation scales (see
Figure 1). 2.