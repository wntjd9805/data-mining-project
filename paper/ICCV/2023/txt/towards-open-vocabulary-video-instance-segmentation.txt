Abstract
Video Instance Segmentation (VIS) aims at segment-ing and categorizing objects in videos from a closed set of training categories, lacking the generalization ability to handle novel categories in real-world videos. To ad-dress this limitation, we make the following three con-tributions. First, we introduce the novel task of Open-Vocabulary Video Instance Segmentation, which aims to si-multaneously segment, track, and classify objects in videos from open-set categories, including novel categories unseen during training. Second, to benchmark Open-Vocabulary
VIS, we collect a Large-Vocabulary Video Instance Segmen-tation dataset (LV-VIS), that contains well-annotated ob-jects from 1,196 diverse categories, significantly surpass-ing the category size of existing datasets by more than one order of magnitude. Third, we propose an efficient
Memory-Induced Transformer architecture, OV2Seg, to first achieve Open-Vocabulary VIS in an end-to-end manner with near real-time inference speed. Extensive experi-ments on LV-VIS and four existing VIS datasets demonstrate the strong zero-shot generalization ability of OV2Seg on novel categories. The dataset and code are released here https://github.com/haochenheheda/LVVIS. 1.

Introduction
Despite impressive efforts, Video Instance Segmenta-tion (VIS) [39, 9, 36] is fundamentally constrained to seg-ment and classify objects from a closed set of training cate-gories, thus limiting the capacity to generalize to novel con-cepts, see Fig. 1 (a). In real-world scenarios, this closed-vocabulary paradigm lacks practical value, as the model usually encounters objects from novel categories unseen in the training stage.
In contrast, recent works Open-World
Tracking (OWTB)[27], UVO [35], and BURST [1] aim
*Equal contribution.
†Corresponding author. (a) Traditional VIS fails to tackle objects from novel
Figure 1. categories (unseen during training, e.g. walrus in the figure); (b)
Open-World Tracking aims to segment and track all objects in a class-agnostic manner, while lacking the ability for object cate-gorization in videos; (c) Open-Vocabulary VIS aims to simulta-neously segment, track, and classify objects for both training cate-gories, e.g., person, and novel categories, e.g., walrus in the figure.
Different colors in the figures indicate different object instances. to segment and track all visible objects, see Fig. 1 (b).
However, those works still have no capacity to classify the objects from open-set categories, which is significant for video-level tasks, such as video captioning and action recognition. To enhance the practicality of VIS, and make
VIS more suited to downstream video-level tasks, we pro-pose Open-Vocabulary Video Instance Segmentation to si-multaneously classify, track, and segment arbitrary objects from an open set of categories, as illustrated in Fig. 1 (c).
To properly benchmark the Open-Vocabulary VIS, a video dataset with large and diverse object categories is
Dataset UVO [35]YT19 [39]YT21 [39]OVIS [31]BURST [1]LV-VIS
Videos
Instances
Masks
Mask/Frame
Object/Video
Categories 11228 104898 593k 12.3 9.3 80∗ 2883 4883 131k 1.7 1.6 40 3859 8171 232k 2.0 2.1 40 901 5223 296k 4.7 5.8 25 2914 16089 600k 3.1 5.5 482 4828 25588 544k 4.9 5.3 1196
Table 1. Key statistics comparison between our LV-VIS dataset and published video segmentation datasets. ∗ indicates the UVO only provides category labels for objects from 80 common cate-gories defined in MS-COCO. Our LV-VIS dataset contains a sig-nificantly larger category set than the existing datasets. necessary. However, existing datasets Youtube-VIS [39],
OVIS [31], and UVO [35] are not sufficient as they con-tain only a few dozen of categories, see Tab.1. BURST [1] contains a relevant larger category set. Still, 81.7% objects in BURST are from the common categories in MS-COCO, shown in Fig. 2 (b), and thus not diverse enough for the open-vocabulary evaluation of novel categories.
In this work, we collect a Large-Vocabulary Video In-stance Segmentation dataset (LV-VIS). LV-VIS contains 4,828 videos and over 544k instance masks from 1,196 cat-egories, which is significantly larger than all existing video segmentation datasets. Notably, as illustrated in Fig. 2 (b), a large percentage of annotated objects in LV-VIS are distinct from the categories in the commonly used datasets such as
MS-COCO [25] and LVIS [17], making it well suited for evaluating the generalization ability on novel categories of
Open-Vocabulary VIS methods, and also a valuable supple-ment to the existing datasets.
Architecturally, a straightforward approach for Open-Vocabulary VIS is to associate per-frame results of open-vocabulary detectors [45, 43] with open-world trackers [27].
However, this propose-reduce-association approach desires intricate hand-crafted modules such as non-maximum sup-pression, and neglects video-level features for stable track-ing and open-vocabulary classification in videos, leading to sub-optimal performance and inference speed.
In this work, we propose the first end-to-end Open-Vocabulary Video Instance Segmentation model, OV2Seg, which simplifies the intricate propose-reduce-association paradigm and attains long-term awareness with a Memory-Induced Transformer. Specifically, it starts by proposing and segmenting all objects with a Universal Object Pro-posal module, then a set of Memory Queries are introduced to incrementally encode object features through time, en-abling long-term awareness for efficiently tracking all ob-jects through time. Lastly, given arbitrary category names as input, a language transformer encoder is adopted to clas-sify the tracked objects based on the Memory Queries. The
Memory Queries incrementally aggregate the object fea-tures from different frames, thus leading to robust video ob-ject classification. To our knowledge, OV2Seg is the first end-to-end model with the capability of segmenting, track-Figure 2. Figure (a) and (b) show the number of categories and ob-ject instances disjointed with a certain category set C, respectively.
CCOCO, CLVIS∗, and CLVIS represent all categories in MS-COCO, frequent/common categories in LVIS, and all categories in LVIS.
A larger percentage of categories and objects in LV-VIS are dis-jointed with the categories in commonly used datasets. ing, and classifying objects in videos from arbitrary open-set categories with near real-time inference speed.
We evaluate OV2Seg on LV-VIS as well as four existing video segmentation datasets: Youtube-VIS2021, Youtube-VIS2019, OVIS, and BURST. Without finetuning on down-stream video datasets, OV2Seg matches the performance of several fully-supervised competitors [39, 7] that have been trained on target video datasets, while exhibiting strong zero-shot generalization ability for novel categories unseen during the training phase.
To sum up, our main contributions are: (i) we introduce Open-Vocabulary VIS, which simulta-neously segments, tracks, and classifies objects of arbitrary open-set categories in videos, generalizing the traditional
VIS for closed-set training categories; (ii) we collect a large-scale, pixel-level annotated video dataset LV-VIS, with a significantly larger vocabulary set than the existing video datasets, thus being a suitable testbed for Open-Vocabulary VIS; (iii) we propose the first end-to-end Open-Vocabulary
VIS model, OV2Seg, that can segment, track, and clas-sify objects from novel categories with a Memory-Induced
Transformer architecture. 2.