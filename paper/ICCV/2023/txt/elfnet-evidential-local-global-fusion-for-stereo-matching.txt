Abstract
Although existing stereo matching models have achieved continuous improvement, they often face issues related to trustworthiness due to the absence of uncertainty estima-tion. Additionally, effectively leveraging multi-scale and multi-view knowledge of stereo pairs remains unexplored.
In this paper, we introduce the Evidential Local-global
Fusion (ELF) framework for stereo matching, which en-dows both uncertainty estimation and confidence-aware fu-sion with trustworthy heads.
Instead of predicting the disparity map alone, our model estimates an evidential-based disparity considering both aleatoric and epistemic uncertainties. With the normal inverse-Gamma distribu-tion as a bridge, the proposed framework realizes intra evidential fusion of multi-level predictions and inter evi-dential fusion between cost-volume-based and transformer-based stereo matching. Extensive experimental results show that the proposed framework exploits multi-view informa-tion effectively and achieves state-of-the-art overall per-formance both on accuracy and cross-domain generaliza-tion. The codes are available at https://github. com/jimmy19991222/ELFNet. 1.

Introduction
Stereo matching, which aims at estimating the dense dis-parity map given a pair of rectified images, is one of the most fundamental problems in various applications, such as 3D reconstruction, autonomous driving, and robotics navi-gation [15]. Benefiting from the rapid development of con-volutional neural networks, many stereo matching models have achieved promising performance by constructing cost volume and using 3D convolutions [14, 38â€“40]. Recently, with the support of transformer, approaches have been pro-posed to utilize global information using self- and cross-attention mechanisms, bringing an alternative way for the
*cheng jun@i2r.a-star.edu.sg (a) Left image (b) ELFNet (Ours) (c) PCWNet (Cost-volume based) (d) STTR (Transformer based)
Figure 1. Error map visualization and comparison of the pro-posed ELFNet, PCWNet (a cost-volume-based model) and STTR (a transformer-based model) on a case of the Scene Flow [25].
The regions colored in blue indicate low error, while the regions in white and red indicate relatively higher error. By fusing cost-volume-based and transformer-based models reliably, our ELFNet significantly reduces the error. stereo matching [13, 19].
Despite the improved performance, quantifying uncer-tainty of the stereo matching results has been overlooked.
The frequently occurring overconfident predictions in the existing stereo matching limit the deployment of the algo-rithms, especially in safety-critical applications. The deep learning models are prone to be unreliable due to the lack of interpretability, especially when facing out-of-domain, low-quality or perturbed samples. Things are even worse in the field of stereo matching where the model is first pretrained in a large scaled synthetic dataset [25] and fine-tuned in a much smaller dataset from real-world scenes. This makes uncertainty estimation an essential part of preventing poten-tially disastrous decisions based on stereo matching results.
In the meantime, multi-view complementary informa-tion widely exists in stereo matching, but it remains a chal-lenge to harness them to improve accuracy effectively and
efficiently. For instance, multi-scale pyramidal cost vol-umes are used to offer the coarse-to-fine knowledge ob-tained from the feature extractor [3, 4, 14, 31, 32, 38, 42], but the current fusion method fails to consider the uncer-tainties in different scales, which results in an untrustwor-thy and incomplete integration. In addition, cost-volume-based [32, 39] and transformer-based approaches [19] pro-vide entirely different strategies for dealing with stereo pairs: the former aggregates local features with convolu-tions, and the latter captures global information with trans-former for dense matching. We find that these two types of methods complementary to each other. For instance, as shown in the red blocks of Figure 1(c) and 1(d), cost-volume-based model is not robust in regions with large illu-mination changes while transformer-based model does not make full use of complex local textures. In such a scenario, uncertainty estimation is a potential module for endowing a trustworthy fusion strategy among multi-view information to alleviate the error risks without bringing much additional computational load.
With these motivations in mind, we propose an
Evidential Local-global Fusion (ELF) framework for stereo matching to kill two birds with one stone (see Figure 2).
The framework enables both uncertainty estimation and re-liable fusion by taking advantage of deep evidential learn-ing [1, 30, 44]. Specifically, we employ trustworthy heads in each branch of the model to compute the aleatoric and epistemic uncertainties [8, 16] along with the disparity. To integrate the multi-scale cost volume information and the complementary information between convolution based and transformer based approaches simultaneously, we propose an intra evidential fusion module and an inter evidential fu-sion module with a mixture of normal-inverse Gamma dis-tributions (MoNIG) [7, 23]. As shown in Figure 1(b), the proposed ELFNet attains a low disparity error in most re-gions by leveraging respective strengths of the cost-volume-based model [32] and the transformer-based model [19] ac-cording to the evidence dynamically.
Our contributions can be summarized as follows: 1. We introduce deep evidential learning to both cost-volume-based and transformer-based stereo matching to estimate both aleatoric and epistemic uncertainties; 2. We propose a novel evidential local-global fusion (ELF) framework, which enables both uncertainty esti-mation and two-stage information fusion based on ev-idence; 3. We conduct comprehensive experiments, which demonstrate that the designed ELFNet consistently boost the performance in terms of accuracy and cross-domain generalization. 2.