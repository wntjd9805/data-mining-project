Abstract
We focus on learning descriptive geometry and motion features from 4D point cloud sequences in this work. Exist-ing works usually develop generic 4D learning tools with-out leveraging the prior that a 4D sequence comes from a single 3D scene with local dynamics. Based on this obser-vation, we propose to learn region-wise coordinate frames that transform together with the underlying geometry. With such frames, we can factorize geometry and motion to fa-cilitate a feature-space geometric reconstruction for more effective 4D learning. To learn such region frames, we de-velop a rotation equivariant network with a frame stabiliza-tion strategy. To leverage such frames for better spatial-temporal feature learning, we develop a frame-guided 4D learning scheme. Experiments show that this approach sig-niﬁcantly outperforms previous state-of-the-art methods on a wide range of 4D understanding benchmarks. 1.

Introduction
We have recently witnessed a surge of interest in under-standing point cloud sequences in 4D (3D space + 1D time).
As the direct sensory input in a large number of modern AI applications including robotics and AR/VR, point cloud se-quences can faithfully depict the geometry and motion of a dynamic scene, and therefore become critical for an intelli-gent agent to perceive and interact with the physical world.
However, learning on such 4D data is very challenging and is still in an immature stage. 4D point cloud sequences usually couple 3D geometry and its dynamic motion to-gether, resulting in quite redundant data in a very high di-mensional space. This causes severe learning issues against an effective and compact spatial-temporal representation.
Some existing efforts tackle the challenge through novel 4D backbone designs [8, 30, 7]. However, most of these works treat the point cloud sequence as unstructured 4D data and exploit generic 4D learning methods without lever-aging the prior that the whole 4D sequence just depicts a single 3D scene with dynamic objects. As a result, such 4D learning is usually not super effective and the learned t-1 t t+1
Frame-unaware
Spatial-temporal Conv
Frame-aware
Spatial-temporal Conv
Figure 1. Frame-aware spatial-temporal convolution. We propose to learn frames for point cloud sequence. Upon obtaining the frames, we could easily align the geometry regardless of the un-derlying motion toward a more canonical and complete geometry understanding. The motion-agnostic geometric features also allow easier temporal association toward a better motion understanding. spatial-temporal feature barely outperforms the spatial fea-ture alone. Another line of works [7] uses self-supervised representation learning to encourage geometry and motion learning in a loosely decoupled manner. However, the de-coupling still happens on the whole-scene level with a spe-cial focus on camera ego-motion, restricting their efﬁcacy in modeling local dynamics on the object-level. We envi-sion that successful geometry and motion decoupling is the key toward effective 4D representation learning. This es-sentially requires depicting the low-dimensional manifold of the dynamic scenes from the redundant high-dimensional 4D data. This highly correlates to dynamic scene recon-struction which requires understanding both camera ego-motion and object motion and is an ongoing research
Instead of explicitly reconstructing the dy-topic itself. namic scene without a quality guarantee, we seek a more lightweight and ﬂexible solution.
Our solution is based upon the following observation.
For a speciﬁc region in the scene, we can understand its dynamic motion through establishing geometry-based and temporally-consistent local coordinate frames. Speciﬁcally, if we can establish a local coordinate frame based upon the 3D geometry in each timestamp and also make sure such
frame transforms in the same way as the underlying geome-try, then we can estimate the local rigid motion via compar-ing such frames among corresponding regions across times-tamps. Such local frames allow us to factorize the local geometry understanding from its motion in a similar spirit to equivariant 3D analysis [22, 14]. In particular, once we establish temporally-consistent local coordinate frames, we essentially have a way to align the geometry regardless of the underlying motion toward a more canonical and com-plete geometry understanding.
In addition, the motion-agnostic geometric features also allow easier temporal as-sociation toward a better motion understanding.
Based upon the above observations, we present a novel 4D learning framework named LeaF. LeaF contains a frame learning module and a frame-guided 4D learning module.
The frame learning module aims at producing geometry-based temporally consistent coordinate frames at different scales so that corresponding regions across timestamps can be well-aligned. This allows for factorizing geometry learn-ing from motion learning. And then the frame-guided 4D learning module leverages the learned frames to facilitate a more effective spatial-temporal feature aggregation. Specif-ically, in the frame learning module, we design a hierar-chical FrameNet which is essentially a rotation-equivariant neural network able to produce coordinate frames equivari-ant to the rotation of input geometry. The frame learning process is very challenging though since apart from rota-tions different observations from a point cloud sequence can vary signiﬁcantly due to sampling differences, density vari-ation, or sensor noises. To make sure the rotation equivari-ance of hierarchical FrameNet is not broken in practice, we introduce a frame stabilization scheme to further regularize the frame learning process. In the frame-guided 4D learn-ing module, we ﬁrst modify popular 4D operators (e.g. 4D point conv) into their frame-guided version and then lever-age frame-guided 4D convolution to process 4D point cloud sequences. Since the motion information is factorized away while convolving with the learned region frames, we addi-tionally process the 4D sequences with just a globally con-stant camera frame so that the motion information is faith-fully kept. We fuse the learned features both from using region frames and from using camera frames, allowing a very effective 4D feature learning.
To verify the effectiveness of LeaF, we conduct experi-ments on a wide range of 4D understanding tasks. And we demonstrate signiﬁcant improvements over previous state-of-the-art methods (+2.0% accuracy on HOI4D action seg-mentation [21], +1.51% accuracy on MSR action recog-nition [17], +2.4% mIoU on HOI4D indoor semantic seg-mentation [21], and +1.81% mIoU on Synthia4D outdoor semantic segmentation [25]).
Our contributions are threefold: 1) we propose to learn effective spatial-temporal 4D features via learning and ex-ploiting region-wise coordinate frames and our framework
LeaF achieves state-of-the-art performance on a wide range of 4D understanding benchmarks; 2) we design a hierar-chical FrameNet along with a frame stabilization scheme to learn equivariant region frames for motion-invariant ge-ometry feature learning; 3) we present a frame-guided 4D learning method that is able to beneﬁt from equivariant re-gion frames without losing the motion information. 2.