Abstract
Multi-modality image fusion aims to combine different modalities to produce fused images that retain the com-plementary features of each modality, such as functional highlights and texture details. To leverage strong genera-tive priors and address challenges such as unstable train-ing and lack of interpretability for GAN-based generative methods, we propose a novel fusion algorithm based on the denoising diffusion probabilistic model (DDPM). The fusion task is formulated as a conditional generation prob-lem under the DDPM sampling framework, which is fur-ther divided into an unconditional generation subproblem and a maximum likelihood subproblem. The latter is mod-eled in a hierarchical Bayesian manner with latent vari-ables and inferred by the expectation-maximization (EM) algorithm. By integrating the inference solution into the diffusion sampling iteration, our method can generate high-quality fused images with natural image generative priors and cross-modality information from source images. Note that all we required is an unconditional pre-trained gener-ative model, and no ﬁne-tuning is needed. Our extensive experiments indicate that our approach yields promising fusion results in infrared-visible image fusion and medical image fusion. The code is available at https://github. com/Zhaozixiang1228/MMIF-DDFM . 1.

Introduction
Image fusion integrates essential information from multi-ple source images to create high-quality fused images [37, 70, 27, 42], encompassing various source image types like digital [20, 67, 74], multi-modal [58, 72], and remote sens-ing [62, 76]. This technology provides a clearer repre-sentation of objects and scenes, and has diverse applica-∗Corresponding author.
Figure 1: (a) Existing GAN-based fusion method workﬂow. (b)
Graph of the hierarchical Bayesian model in likelihood rectiﬁcation, linking the MMIF loss and our statistical inference model. (c) Our
DDFM workﬂow: the unconditional diffusion sampling (UDS) module generates f t, while the likelihood rectiﬁcation module, based on (b), rectiﬁes UDS output with source image information.
Figure 2: Visualization of results on MSRS [51] and Road-Scene [59] in Tab. 1. Hexagons formed by lines of different colors represent the values of different methods across six metrics. Our
DDFM (marked in yellow) outperforms all other methods. tions such as saliency detection [43, 40, 41], object detec-tion [12, 2, 10, 55], and semantic segmentation [28, 11, 56].
Among the different subcategories of image fusion, Infrared-Visible image Fusion (IVF) and Medical Image Fusion (MIF) are particularly challenging in Multi-Modality Image Fusion (MMIF) since they focus on modeling cross-modality fea-tures and preserving critical information from all sensors and modalities. Speciﬁcally, in IVF, fused images aim to retain both thermal radiation from infrared images and detailed texture information from visible images, thereby avoiding the limitations of visible images being sensitive to illumi-nation conditions and infrared images being noisy and low-resolution. While MIF can assist in diagnosis and treatment by fusing multiple medical imaging modalities for precise detection of abnormality locations [16, 9].
There have been numerous methods devised recently to address the challenges posed by MMIF [26, 65, 29], and generative models [7, 38] have been extensively utilized to model the distribution of fused images and achieve satis-factory fusion effects. Among them, models based on Gen-erative Adversarial Networks (GANs) [34, 35, 33, 26] are dominant. The workﬂow of GAN-based models, illustrated in Fig. 1a, involves a generator that creates images contain-ing information from source images, and a discriminator that determines whether the generated images are in a similar manifold to the source images. Although GAN-based meth-ods have the ability to generate high-quality fused images, they suffer from unstable training, lack of interpretability and mode collapse, which seriously affect the quality of the generated samples. Moreover, as a black-box model, it is difﬁcult to comprehend the internal mechanisms and behav-iors of GANs, making it challenging to achieve controllable generation.
Recently, Denoising Diffusion Probabilistic Models (DDPM) [13] has garnered attention in the machine learning community, which generates high-quality images by mod-eling the diffusion process of restoring a noise-corrupted image towards a clean image. Based on the Langevin dif-fusion process, DDPM utilizes a series of reverse diffusion steps to generate promising synthetic samples [46]. Com-pared to GAN, DDPM does not require the discriminator network, thus mitigating common issues such as unstable training and mode collapse in GAN. Moreover, its generation process is interpretable, as it is based on denoising diffusion to generate images, enabling a better understanding of the image generation process [57].
Therefore, we propose a Denoising Diffusion image
Fusion Model (DDFM), as shown in Fig. 1c. We formu-late the conditional generation task as a DDPM-based pos-terior sampling model, which can be further decomposed into an unconditional generation diffusion problem and a maximum likelihood estimation problem. The former satis-ﬁes natural image prior while the latter is inferred to restrict the similarity with source images via likelihood rectiﬁca-tion. Compared to discriminative approaches, modeling the natural image prior with DDPM enables better generation of details that are difﬁcult to control by manually designed loss functions, resulting in visually perceptible images. As a generative method, DDFM achieves stable and control-lable generation of fused images without discriminator, by applying likelihood rectiﬁcation to the DDPM output.
Our contributions are organized in three aspects:
• We introduce a DDPM-based posterior sampling model for MMIF, consisting of an unconditional generation module and a conditional likelihood rectiﬁcation mod-ule. The sampling of fused images is achieved solely by a pre-trained DDPM without ﬁne-tuning.
• In likelihood rectiﬁcation, since obtaining the likeli-hood explicitly is not feasible, we formulate the op-timization loss as a probability inference problem in-volving latent variables, which can be solved by the
EM algorithm. Then the solution is integrated into the
DDPM loop to complete conditional image generation.
• Extensive evaluation of IVF and MIF tasks shows that
DDFM consistently delivers favorable fusion results, effectively preserving both the structure and detail in-formation from the source images, while also satisfying visual ﬁdelity requirements. 2.