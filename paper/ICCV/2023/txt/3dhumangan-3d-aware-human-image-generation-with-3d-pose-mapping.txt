Abstract
We present 3DHumanGAN, a 3D-aware generative ad-versarial network that synthesizes photo-like images of full-body humans with consistent appearances under different view-angles and body-poses. To tackle the representational and computational challenges in synthesizing the articulated structure of human bodies, we propose a novel generator architecture in which a 2D convolutional backbone is modu-lated by a 3D pose mapping network. The 3D pose mapping network is formulated as a renderable implicit function con-ditioned on a posed 3D human mesh. This design has several merits: i) it leverages the strength of 2D GANs to produce high-quality images; ii) it generates consistent images under varying view-angles and poses; iii) the model can incor-porate the 3D human prior and enable pose conditioning.
Our model is adversarially learned from a collection of web images needless of manual annotation.
† Work done as research engineer at Shanghai AI Laboratory.
Project page: https://3dhumangan.github.io/ 1.

Introduction
Human image generation is a long-standing topic in com-puter vision and graphics with applications across multiple areas of interest including movie production, social network-ing and e-commerce. Compared to physically-based meth-ods, data-driven approaches are preferred due to the photo-likeness of their results, versatility and ease of use [61]. In this work, we are interested in synthesizing full-body hu-man images with a 3D-aware generative adversarial network (GAN) that produces appearance-consistent images under different view-angles and body-poses.
Rapid developments have been seen in using 3D-aware
GANs to generate view-consistent images of human faces
[44, 56, 5, 4, 13, 68, 21]. However, these methods have limited capacity when dealt with complex and articulated objects such as human bodies. To begin with, methods based solely on neural volume rendering [5, 44, 56] are too memory inefficient. Rendering human bodies requires a volumetric representation that is much more dense than that of faces, which makes it computationally infeasible. A line of work improves the computational efficiency and rendering quality
of 3D-aware GANs by refining the rendered output with a convolutional neural network [68, 13, 4, 21]. However, we argue that this method is not optimal for generating full-body human images. This is because the 3D representation has to capture both the shape and the appearance of human bodies at the same time, which requires a high level of representa-tional capacity. Meanwhile, the potential of the 2D-network is not fully exploited.
As depicted in Figure 2a, our work introduces a novel generator architecture in which a 2D convolutional backbone is modulated by a 3D pose mapping network. This design is motivated by the observation that in a StyleGAN2 [27] model trained on human images certain layers of styles cor-relate strongly with the pose of the generated human [9] while others correlate more apparently with the appearance.
The 3D Pose Mapping Network is formulated as a render-able implicit function conditioned on a posed 3D human mesh derived with a parametric model [36]. In this way, the 3D representation handles the simplified task of parsing a geometric prior. As an additional benefit of explicitly con-ditioning on posed human mesh, the pose of the generated human can be specified. The output of the 3D pose mapping network is used to render a 2D low-resolution style map through ray integration [41]. The style map is passed into the first few layers of our backbone network. The appear-ance of the generated human is controlled by the Appearance
Mapping Network. It is formulated as an MLP following a common practice in style-based generators [26, 27]. For the network to learn to parse the 3D geometric prior, we use a segmentation-based GAN loss [59] calculated using a
U-Net [51] discriminator. This design enables the network to establish one-to-many mapping from 3D geometry to syn-thesized 2D textures using only collections of single-view 2D photographs without manual annotations.
Traditional CNN generator networks with 3 × 3 convo-lution and progressive upsampling are not equivariant and demonstrate inconsistency under geometric transformations
[25]. In our case, the appearance of the generated human may change when pose and view-angle vary. To preserve consistency, we propose a network design with two key as-pects: 1) Our backbone network is built entirely with 1 × 1 convolutions. This helps eliminate positional reference and promotes equalvariance. 2) Modulation from the pose map-ping network is passed into the backbone by means of spatial adaptive batch normalization instead of the commonly used instance normalization [27, 46], so that underlying struc-ture from the geometric information parsed by our 3D pose mapping network is preserved.
Our contributions can be summarized as follows: 1) We propose a 2D-3D hybrid generator which is both efficient and expressive. The model is supervised with segmentation-based GAN loss which helps establish a mapping between 3D coordinates and 2D human body semantics. 2) Our generator is carefully designed to preserve the consistency of appearance when pose and view-angle vary. 3) Our work achieves state-of-the-art fidelity of 3D-aware generation of full-body human images. 2.