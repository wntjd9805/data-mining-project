Abstract
Dataset distillation aims at a small synthetic dataset to mimic the training performance on neural networks of a given large dataset. Existing approaches heavily rely on an iterative optimization to update synthetic data and multiple forward-backward passes over thousands of neural network spaces, which introduce significant overhead for computa-tion and are inconvenient in scenarios requiring high effi-ciency. In this paper, we focus on few-shot dataset distilla-tion, where a distilled dataset is synthesized with only a few or even a single network. To this end, we introduce the no-tion of distillation space, such that synthetic data optimized only in this specific space can achieve the effect of those optimized through numerous neural networks, with dramat-ically accelerated training and reduced computational cost.
To learn such a distillation space, we first formulate the problem as a quad-level optimization framework and pro-pose a bi-level algorithm. Nevertheless, the algorithm in its original form has a large memory footprint in practice due to the back-propagation through an unrolled computational graph. We then convert the problem of learning the distilla-tion space to a first-order one based on image translation.
Specifically, the synthetic images are optimized in an arbi-trary but fixed neural space and then translated to those in the targeted distillation space. We pre-train the translator on some large datasets like ImageNet so that it requires only a limited number of adaptation steps on the target dataset.
Extensive experiments demonstrate that the translator af-ter pre-training and a limited number of adaptation steps achieves comparable distillation performance with state of the arts, with ∼ 15× acceleration. It also exerts satisfac-tory generalization performance across different datasets, storage budgets, and numbers of classes. 1.

Introduction
Given an original dataset T , Dataset distillation (DD) [41, 51, 35, 18, 47, 20, 22] aims at a much smaller
*Corresponding Author.
Figure 1. (a) Existing DD methods heavily rely on forward-backward processes through enormous neural networks. (b) More neural networks used in dataset distillation would result in more floating point operations (FLOPs) and higher latency. (c) We in-troduce the notation of distillation space for few-shot dataset dis-tillation in this paper and aim to learn a neural network such that distilled data only optimized in this specific space can approxi-mate the performance by multiple networks. (d) Given only a few or even a single network, our method achieves comparable per-formance with state-of-the-art baselines requiring enormous net-works. Results in this figure are concerning models trained with 1 image per class distilled from CIFAR100 dataset. OS denotes one-shot DD using only one network, FS denotes few-shot DD with a limited number of adaptation steps, and DS denotes using down-sampled parameterization, i.e., storing down-sampled dis-tilled images with the same storage budget. synthetic dataset S, such that the training effect of S on neural networks can match that of T . Such techniques have received considerable attention to alleviate the heavy bur-den on storage, transmission, and training of deep learning models brought by large-scale datasets. Recent advances on
DD have revealed that models trained with only a few syn-thetic samples, even 1 image per class in some cases, can retain most of the performance of those trained with real data [49, 50, 2, 28, 29, 40, 13, 52, 23, 21]. Thus, it becomes a research area receiving increasing attention from the aca-demic and industrial community recently as a potential so-lution to alleviate the concerns on the storage, transmission, pre-processing, and training effort for large datasets.
Nevertheless, as shown in Fig. 1(a), all existing meth-ods on DD work iteratively: the synthetic dataset is opti-mized in an outer loop, and real and synthetic data are sent to a newly-initialized neural network in each iteration. Al-though they have achieved remarkable performance, such forward-backward passes in multiple neural networks can introduce extensive computational overhead to generate a distilled dataset, as illustrated in Fig. 1(b). As a result, the burdensome distillation procedure in existing methods brings significant latency. For instance, one of the state-of-the-art works by Deng et al. [5] requires ∼ 18 days to distill only 1 image per class for CIFAR100 dataset [15] as shown in Fig. 1(d) and [47]. Even FRePo [52], with the most sat-isfactory efficiency, takes over 3 hours in the same setting requiring 500, 000 neural network steps. The long training process in current DD makes it inconvenient for scenarios requiring high efficiency, e.g., processing streaming data.
Focusing on these drawbacks, in this paper, we propose few-shot dataset distillation and are interested in the perfor-mance of DD if only a few or even a single neural network is available for training. We first try a naive solution by merely adopting one neural network for the existing method
FRePo [52] and visualize the accuracy and efficiency. As shown in Fig. 1(d), although the simple strategy improves the efficiency, the performance gap with the original version based on thousands of networks is significant, which sug-gests a trade-off between performance and the number of networks. A similar effect is also revealed by Cazenavette et al. [2], where the number of teacher training trajectories would affect the performance substantially. Thus, how to boost the performance by only a limited number of net-works becomes an interesting question in few-shot DD.
Currently, the networks used in Fig. 1(d) are randomly and independently initialized, which makes us curious: whether there exists a dedicated neural network, such that the synthetic dataset distilled in it can achieve similar per-formance to those distilled in multiple ones.
In this pa-per, we describe this concept through a quad-level defini-tion termed as distillation space, as shown in Fig. 1(c), and propose a bi-level algorithm to find it, to optimize the error on real data, for networks trained with the synthetic dataset, distilled in this space.
However, the bi-level optimization requires back-propagating through an unrolled computational graph over multiple steps of gradient descent, which imposes an inten-sive GPU footprint. Due to the bottleneck on GPU memory, the number of steps for inner optimization is limited, which further limits the performance in practice. To reduce the complexity, we free the dependency of higher-order gradi-ents via casting the problem as image translation, to make sure that the iterative optimization process is not nested in the computational graph: the synthetic dataset is distilled in a random but fixed neural network space and then translated to the desired space via a translator network.
Specifically, the translator is pre-trained on some large datasets like ImageNet [4]. In each iteration, a random sub-set is sampled. Distilled results after being translated from the original space are evaluated in some random networks, and the distillation loss is back-propagated to the transla-tor to update its parameters. We find that the distillation space translator after pre-training can generalize well across different datasets, storage budgets, and numbers of classes.
With only a few adaptation steps on the target dataset, the performance of distilled data after translation achieves can be on par with those distilled by multiple neural networks at ∼ 15× faster, as shown in Fig. 1(d).
In summary, our contributions are listed as follows:
• Tailoring for scenarios requiring high efficiency, we are the first to study the problem of few-shot dataset distillation, where only a limited number of neural net-works are available for DD, and introduce the quad-level definition of distillation space for it.
• An efficient translative modeling approach with a pre-training algorithm is proposed to learn an effective dis-tillation space;
• Extensive experiments demonstrate that our method is capable of generating dataset distillation results with
∼ 15× acceleration. 2.