Abstract
Humans can watch a continuous video stream and ef-fortlessly perform continual acquisition and transfer of new knowledge with minimal supervision yet retaining previously learnt experiences.
In contrast, existing con-tinual learning (CL) methods require fully annotated la-bels to effectively learn from individual frames in a video stream. Here, we examine a more realistic and challenging problem—Label-Efficient Online Continual Object Detec-tion (LEOCOD) in streaming video. We propose a plug-and-play module, Efficient-CLS, that can be easily inserted into and consistently improve existing CL algorithms for object detection in video streams with reduced data anno-tation costs and model retraining time. We show that our method has achieved significant improvement with minimal forgetting across all supervision levels on two challeng-ing CL benchmarks for streaming real-world videos. Re-markably, with only 25% annotated video frames, our pro-posed method still outperforms the state-of-the-art CL mod-els trained with 100% annotations on all video frames. The data and source code will be publicly available at https:
//github.com/showlab/Efficient-CLS. 1.

Introduction
Humans have the ability to continuously learn from an ever-changing environment, while retaining previously learnt experiences.
In contrast to human learning, prior works [3, 2, 11, 34, 7] show that deep neural networks are prone to catastrophic forgetting. To address the forgetting problem, existing works in continual learning (CL) primar-ily focus on class-incremental image classification or object detection. Their experiment settings are often idealistic and simplified, where i.i.d. static images are usually grouped by class and incrementally presented to computational models in sequence. To learn a particular task containing specific
*Corresponding Author. classes, an agent can go through all the data of current task over multiple epochs. After that, the learned classes in cur-rent task become unavailable, i.e., no overlaps between the sets of learned classes and unseen classes.
However, these experiment designs deviate from the on-line continual learning (OCL) setting in the real world, where an agent learns from temporally correlated non-i.i.d. video streams in one single pass. Given context regulari-ties in natural environments, an agent is likely to encounter cases when objects of previously learnt classes co-occur with unknown objects from unseen classes, e.g., a computer mouse and a computer monitor often co-occur. Taking these considerations, [34] introduces OCL on object detection in real-world video streams. They evaluate existing CL ap-proaches on this setting and report a huge performance gap compared with offline training.
Based on the setting in [34], we take a significant step further and introduce a novel problem setting called Label-Efficient Online Continual Object Detection (LEOCOD), which highlights two unique challenges. First, the setting in [34] is such that the CL algorithms are trained with ev-ery mini-batch over multiple passes. We tighten the training recipe in LEOCOD to strictly online, where data is allowed to have one single pass and models are trained on the entire video dataset for only one epoch. Second, existing CL mod-els require fully supervised training where box-level ground truth labels of every object on every video frame have to be obtained from human annotators. Unlike static images, ac-quiring human annotations for object detection on videos can be expensive and daunting. Thus, in LEOCOD, the video frames per mini-batch are sparsely annotated to al-leviate the burdens of extensive human labeling, making
LEOCOD one step closer to real world application.
Cognitive science works [35, 19] show that humans are efficient at continuously learning from very few annotated data samples. We get inspirations from the theory of Com-plementary Learning Systems (CLS) in human brains [18], and propose a plug-and-play module for the LEOCOD task, dubbed as Efficient-CLS. In Efficient-CLS, we introduce
(a) Problem introduction: An agent continuously learns from a never-ending online video stream over time. In each training
Figure 1. step, out of a mini-batch containing 16 consecutive video frames, only a fixed proportion of frames are labeled (green boundary), while the rest of the frames are unlabeled (orange boundary). Following [34], the video frame after every training mini-batch (transparent) is held out for testing. After every 100 training steps, the agent is evaluated on all the video frames from the test set for object detection. (b) Key results: Our proposed method (red) consistently outperforms the best competitive baseline (blue) by a margin of 5%. Remarkably, our model, trained at 25% annotation cost, surpasses the best baseline trained at 100% (grey line). The orange cross denotes the performance of the state-of-the-art model, which is 15% lower than our method. two feed-forward neural networks as slow and fast learners.
In the fast learner, memory is rapidly adapted to the current task. The weights of the slow learner change a little on each reinstatement, and are maintained by taking the exponential moving average (EMA) of the fast learner’s weights over time. Though a few continual learning models in previous works [4, 24] also use a similar source of inspiration, they miss the effect of reciprocal connections from slow learn-ers to fast learners, which we intend to address. Inspired by the bidirectional interaction in CLS [14], we reactivate the weights of the slow learners to predict meaningful pseudo labels from the unlabeled video frames and use these pseudo labels to guide the training of the fast learner, closing the loop between the two systems.
We demonstrate the versatility and effectiveness of our
Efficient-CLS on two standard real-world video datasets,
OAK [34] and EgoObjects [1]. Our proposed method can be easily integrated into existing CL models and consistently improve their performance by a large margin in LEOCOD.
It is worth noting that, with only 25% labeled data, our method surpasses the comparative baselines trained with full supervision (Figure 1(b)).
To summarize, we make the following key contributions:
• We introduce a new, challenging and important prob-lem of label-efficient online continual object detection (LEOCOD) in video streams. Solving this problem would greatly benefit real-world applications in reduc-ing annotation cost and model retraining time.
• We propose Efficient-CLS, a plug-and-play module inspired from the Complementary Learning Systems (CLS) theory, which can be integrated into existing CL models and learn efficiently and effectively with less supervision and minimal forgetting.
• We benchmark existing CL methods on the task of
LEOCOD and demonstrate the state-of-the-art perfor-mance of our method through extensive experiments. 2.