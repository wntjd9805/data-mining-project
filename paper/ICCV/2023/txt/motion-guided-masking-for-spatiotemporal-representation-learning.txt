Abstract
Several recent works have directly extended the image masked autoencoder (MAE) with random masking into video domain, achieving promising results. However, unlike im-ages, both spatial and temporal information are important for video understanding. This suggests that the random masking strategy that is inherited from the image MAE is less effective for video MAE. This motivates the design of a novel masking algorithm that can more efficiently make use of video saliency. Specifically, we propose a motion-guided masking algorithm (MGM) which leverages motion vectors to guide the position of each mask over time. Crucially, these motion-based correspondences can be directly obtained from information stored in the compressed format of the video, which makes our method efficient and scalable. On two chal-lenging large-scale video benchmarks (Kinetics-400 and
Something-Something V2), we equip video MAE with our
MGM and achieve up to +1.3% improvement compared to previous state-of-the-art methods. Additionally, our MGM achieves equivalent performance to previous video MAE us-ing up to 66% fewer training epochs. Lastly, we show that
MGM generalizes better to downstream transfer learning and domain adaptation tasks on the UCF101, HMDB51, and Diving48 datasets, achieving up to +4.9% improvement compared to baseline methods. 1.

Introduction
Video transformers [1, 24, 25, 28, 43] have achieved state-of-the-art for a variety of video understanding tasks, mirroring the success of image transformers such as ViT [9]. How-ever, many video transformers heavily rely on large-scale supervised pretraining from image datasets such as Ima-geNet21K [8] and JFT-300M [35], which is data-inefficient.
Self-supervised learning (SSL) [5, 15, 17, 29, 42] is one promising paradigm for eliminating the dependency on large-scale supervised pretraining. The denoising / masked autoen-coder (MAE) [41], which was originally popularized by
BERT [20] for natural language modeling, has recently re-Figure 1: Top-1 accuracy on Something-Something V2.
Our proposed MGM outperforms all previous masked-video methods (M3Video [36], MAM2 [33], BEVT [45], Omn-iMAE [13], MotionMAE [50], and VideoMAE [39]) with improved training efficiency. Marker shape denotes recon-struction target. emerged as a promising representation learning method for vision tasks. Image MAE [2, 16] has achieved state-of-the-art in image domain via learning to reconstruct randomly masked image patches. Recently, similar works [11, 39] that reconstruct randomly masked video have obtained encour-aging results in video domain. However, few SSL works focus on effectively learning video saliency. In this paper, we improve upon the previous video MAEs [11, 39] via video saliency.
We argue that the random masking strategy which is in-herited from image MAE is not optimal for video. The optimal random masking ratio for video MAE [11, 39] is higher than that for image MAE [16] (0.9 vs. 0.75). This can be understood as a consequence of the natural tempo-ral coherence within videos, which leads to the existence of similar video patches in other frames. When using ran-dom masking, many of these correlated video patches may be visible to the encoder which would make reconstruction easier. This necessitates the use of a high masking ratio in order to reduce redundancy and make the reconstruction task
sufficiently challenging [11]. However, increasing the mask-ing ratio has the side-effect of leaving fewer visible patches for the MAE encoder to learn spatiotemporal saliency from, which we hypothesize limits the learned representation.
We thus propose to guide the model to learn to reconstruct the most salient regions of video. As humans and objects are key to understanding video, one natural idea is to track the bounding boxes in each video frame and mask the content within. This is also consistent with the observation that video pixels evolve continuously frame-by-frame, and therefore, the MAE should be able to reconstruct this spatiotemporal continuity. However, generating bounding boxes for each frame is impractical for large-scale video datasets.
To this end, we hypothesize that motion is an effective guide for detecting spatiotemporal saliency. To test this hy-pothesis, we define the saliency score as rbbox / rnon bbox, or the ratio of average motion magnitude within bounding boxes to average motion magnitude outside bounding boxes.
The saliency score is 1.47 for Something-Something v2 [14] and 1.28 for Kinetics-400 [19] respectively, suggesting that regions of higher motion overlap with spatially salient re-gions more often than regions of lesser motion.
We thus propose an improved masking algorithm, MGM, which uses motion to continuously guide the mask to cover the most salient spatiotemporal regions. MGM obtains cheap motion correspondence by exploiting the H.264 codec [30] which prevails in popular video formats such as MP4. Dur-ing the encoding phase of H.264, motion vectors are pre-computed and stored as part of the codec. Thus during the decoding phase, motion vectors can be obtained “for free” along with RGB frames. The use of readily available mo-tion vectors instead of expensive optical flow enables us to efficiently achieve scale. While some supervised works have used motion vectors [21, 43, 47], we are the first to use motion vectors for MAE pretraining.
Our MGM outperforms previous VideoMAE [39] by 1.2% on Something-Something V2 (SSv2) [14] and 0.2% on
Kinetics-400 (K400) [19], demonstrating that our MGM is ef-fective. MGM can also achieve the same performance as [39] with 50% fewer training epochs as shown in Fig. 1, further making MAE pretraining more data-efficient. We also show that MGM generalizes well to small datasets (UCF101 [34],
HMDB51 [22], and Diving48 [23]) in both full finetune and linear probe evaluation as well as domain adaptation settings with up to 4.9% performance improvement over
VideoMAE [39]. This shows that the features learned by
MGM contain richer semantics that transfer well to video recognition tasks. In summary, our contributions are: 1. MGM, an efficient and effective self-supervised algo-rithm for 3D masking that continuously models motion trajectories. 2. Applying motion vectors which are directly available during video decoding - unlike optical flow - to provide efficient motion guidance in the MAE framework. 3. New state-of-the-art or comparable results on two large-scale datasets and various downstream tasks on three small datasets, as well as detailed ablations and insights. 2.