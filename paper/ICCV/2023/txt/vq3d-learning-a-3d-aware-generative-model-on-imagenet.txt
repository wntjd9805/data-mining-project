Abstract
Recent work has shown the possibility of training gener-ative models of 3D content from 2D image collections on small datasets corresponding to a single object class, such as human faces, animal faces, or cars. However, these models struggle on larger, more complex datasets. To model diverse and unconstrained image collections such as ImageNet, we present VQ3D, which introduces a NeRF-based decoder into a two-stage vector-quantized autoencoder. Our Stage 1 al-lows for the reconstruction of an input image and the ability to change the camera position around the image, and our
Stage 2 allows for the generation of new 3D scenes. VQ3D is capable of generating and reconstructing 3D-aware images from the 1000-class ImageNet dataset of 1.2 million training images, and achieves a competitive ImageNet generation
FID score of 16.8. Our project webpage is at this url. 1.

Introduction 3D assets are an important part of popular media for-mats such as video games, movies, and computer graphics.
Since 3D content can be time-consuming to create by hand, automatically generating 3D content using machine learn-ing is an active area of research. While machine learning techniques beneﬁt from training on large amounts of data, existing 3D datasets have noisy labels and are orders of magnitude smaller than those of 2D images.
To circumvent the limitations of 3D datasets, recent
GAN-based methods have explored learning generative mod-els of 3D scenes from images with limited or no 3D la-bels [26, 6, 18, 25]. These GAN-based approaches demon-strate the promise of learning 3D representations from 2D data. However, GANs are unstable and challenging to scale to large diverse datasets [3, 35]. Because of these issues, re-cent 3D-aware GANs mostly focus on single-class datasets, such as human faces [20], animal faces [8], or cars [48].
In order to move beyond single-class generation, we draw inspiration from recent advances in 2D image generation,
*Work completed at Google Research.
Figure 1: 3D-aware images generated by VQ3D on Ima-geNet. Please see supplemental materials for video results. where other formulations such as text-to-image generation models [51, 33, 28] and two-stage image models [14, 50] have begun to achieve impressive results on very large and diverse image collections. The most recent state-of-the-art 2D generative models leverage diffusion or vector quanti-zation rather than GANs to scale well to large datasets. In particular, the two-stage vector quantization approach, being a likelihood method, can model more diverse modes and is more stable during training. This motivates us to explore vec-tor quantization as an alternative to the popular GAN-based methods for 3D-aware generative models.
In this paper, we propose VQ3D, a strong 3D-aware gen-erative model that can be learnt from large and diverse 2D image collections, such as ImageNet [9]. To encourage stability and higher reconstruction quality, we forgo GAN-based [16] approaches [25, 5, 6, 26, 18], in favor of the two-stage autoencoder formulation of VQGAN [14] and ViT-VQGAN [50]. We build on this formulation with several novel architectural components and losses, and show through ablations that they are necessary for good performance and 3D-awareness. We learn 3D geometry by introducing a con-ditional NeRF decoder and modiﬁed triplane representation which can handle unbounded scenes, and training with a novel loss formulation which encourages high-quality geom-etry and novel views.
Our formulation has two advantages that ensure its ability to scale and correctly model ImageNet. First, we separate the training into two stages (reconstruction and generation).
This enables us to directly supervise the ﬁrst stage training via a novel depth loss that uses pseudo-GT depth. Note, this is possible because our conditional NeRF decoder in the ﬁrst stage learns to both reconstruct the input and predict the depth of each image.
Second, our two-stage formulation is simpler and more reliable than existing techniques for training 3D-aware gen-erative models. Our formulation does not use progressive growing [5, 6], a neural upsampler [6, 5, 26], pose condition-ing [6, 42], or patch-wise discriminators [36, 42], yet still learns meaningful 3D representations. Previous work [2, 35] found that 2D GANs cannot easily scale up to large diverse datasets (e.g., ImageNet) and signiﬁcant innovations in train-ing techniques are needed. Despite an exhaustive hyperpa-rameter search, we were unable to scale existing 3D GAN baselines to ImageNet, and future training innovations are needed to make 3D GANs work on ImageNet. By con-trast, our two-stage formulation scales to ImageNet stably like prior two-stage architectures [50, 14], and also achieves comparable or superior performance to existing 3D GAN baselines on simpler datasets such as CompCars [48].
We verify that baseline 3D-aware GAN methods [6, 18, 5, 26], while working well on single-object datasets, fail to learn good generative models for ImageNet. Compared to the best existing 3D-aware baseline, VQ3D attains a 75.9% relative improvement on FID scores for 3D-aware ImageNet images (69.8 for StyleNeRF [18] to 16.8 for VQ3D).
In summary, we make the following three contributions:
• We present a novel 3D-aware generative model that can be trained on large and diverse 2D image collections.
Whereas all previous methods are GANs, we are the
ﬁrst to show that a two-stage VQ formulation can work for 3D-aware generative models. Our two-stage inher-its the stability of prior VQ formulations and works reliably on both single-class and highly diverse datasets.
Our formulation also allows the use of pseudo-depth supervision in the ﬁrst stage.
• We obtain state-of-the-art generation results on Ima-geNet, demonstrating that our 3D-aware generative model is capable of ﬁtting a dataset at the scale and diversity of ImageNet. Our model signiﬁcantly outper-forms the next best baseline.
• The Stage 1 of our model enables 3D-aware image edit-ing and manipulation. One forward pass through our network converts a single RGB image into a manipu-lable NeRF, without relying on an expensive inversion optimization used in prior work [5, 6]. 2.