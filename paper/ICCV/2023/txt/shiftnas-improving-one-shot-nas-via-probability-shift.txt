Abstract
One-shot Neural architecture search (One-shot NAS) has been proposed as a time-efficient approach to obtain opti-mal subnet architectures and weights under different com-plexity cases by training only once. However, the subnet performance obtained by weight sharing is often inferior to the performance achieved by retraining. In this paper, we investigate the performance gap and attribute it to the use of uniform sampling, which is a common approach in supernet training. Uniform sampling concentrates training resources on subnets with intermediate computational re-sources, which are sampled with high probability. However, subnets with different complexity regions require different optimal training strategies for optimal performance.
To address the problem of uniform sampling, we propose
ShiftNAS, a method that can adjust the sampling probabil-ity based on the complexity of subnets. We achieve this by evaluating the performance variation of subnets with different complexity and designing an architecture genera-tor that can accurately and efficiently provide subnets with the desired complexity. Both the sampling probability and the architecture generator can be trained end-to-end in a gradient-based manner. With ShiftNAS, we can directly ob-tain the optimal model architecture and parameters for a given computational complexity. We evaluate our approach on multiple visual network models, including convolutional neural networks (CNNs) and vision transformers (ViTs), and demonstrate that ShiftNAS is model-agnostic. Experi-mental results on ImageNet show that ShiftNAS can improve the performance of one-shot NAS without additional con-sumption. Source codes are available at GitHub. 1.

Introduction
Deep neural networks (DNNs) have been widely ap-plied to the field of computer vision with remarkable suc-cess [6, 8]. However, the deployment of these vision net-*Xinyi Yu and Linlin Ou are corresponding authors.
Figure 1: A conceptual overview of our ShiftNAS. Focus-ing on the computational resource (e.g. FLOPs), Shift-NAS split a search space into several parts where subnets have close computational complexity. In stochastic super-net training, ShiftNAS first samples computational com-plexity according to probability, then an architecture gen-erator samples the subnet with desired computational com-plexity. The probability distribution is dynamically updated with stochastic training by estimating the performance vari-ation of each sub-space. works on edge devices still has some limitations, such as the massive model sizes and excessive computation overhead
[9, 10, 20, 24]. In addition, designing architectures artifi-cially in a trial-and-error manner is a resource-consuming task that requires not only architectural skills but also do-main expertise. Consequently, how to acquire optimal ar-chitectures that balance latency and accuracy efficiently is of paramount importance.
Recent advancements in neural architecture search (NAS) methods, such as [22, 7, 30, 35], have led to signifi-cant improvements in the performance of practical applica-tions by automatically searching for optimal architectures within a defined search space. However, traditional NAS methods typically require a substantial computation budget
[23, 35]. In order to speed up training and reduce the re-source consumption of the training process, one-shot NAS 1
methods [18, 7] have adopted a two-stage training approach based on weight sharing. Specifically, a supernet is trained in the first stage, and subnets with better performance are searched for in the second stage.
In some cases, post-processing methods (e.g., retraining and finetuning) are also necessary in the second stage since the performance of these subnets inherited from the supernet is often inferior to that of models trained from scratch. However, the downside of such methods is that the training consumption increases lin-early with the number of architectures, which can be prob-lematic. To address this issue, some one-shot NAS methods
[3, 4, 29, 33] have utilized a weight entanglement training strategy to share the weights in each operation, eliminat-ing the need for additional finetuning or retraining. Fur-thermore, a high-quality supernet is essential for candidate architectures to inherit weights directly [33], as well as for accurately ranking candidate architectures [14].
However, there is still potential for breakthroughs in training a better supernet. In the first stage, previous meth-ods [7, 4] assume that all candidate architectures are equally important and should be sampled with equal probability during training. However, subnets with different numbers of parameters require different amounts of training resources
[2]. For instance, subnets with 1.0 GFLOPs may converge after 30000 iterations, while subnets with 2.0 GFLOPs may require 50000 iterations. Additionally, only the optimal subnet will be deployed, while others will be ignored at the same computational complexity. Therefore, subnets that occupy more training resources may not be distributed in regions corresponding to resource constraints, resulting in sub-optimal performance of the final deployed model. We find that when all subnets are sampled with equal proba-bility, the resulting computational resource distribution is approximately normal. Consequently, subnets trained un-der this distribution may appear to be under-fitting or over-fitting in different regions.
To address the challenge of efficiently training a high-performance supernet, we propose a novel method, probability-Shift Neural Architecture search (ShiftNAS).
In ShiftNAS, the sampling probability of subnets is not uni-form and can be dynamically adjusted during the training process. The training sufficiency of each subnet is mea-sured by evaluating its performance variation under dif-ferent computational constraints. The subnets with high-performance variance are identified as undertrained, and their sampling probabilities are increased to tilt training re-sources dynamically towards them. This enables us to al-locate resources more effectively and efficiently to achieve better performance for the subnets that need more training resources.
In spite of having an optimal sampling distribution, effi-ciently and accurately sampling subnets with a desired com-putational constraint still poses a challenge. To address this issue, we propose an LSTM-based architecture generator (AG) that can be optimized differentiably with a resource constraint loss function. The AG’s output is then processed by Gumbel Softmax [12] to generate a one-hot vector policy for each searched operation. To suit the weight-entangled search space [32, 4], we employ a matrix mapping tech-nique that can convert the one-hot vector into a differen-tiable mask. The mask is multiplied by the operation to obtain a differentiable subnet. The AG and supernet can be jointly trained to learn how to generate the best subnets with desired computational constraints. During evaluation, the AG can generate a corresponding subnet immediately for any given computational constraint. The weights of the searched subnet can be directly inherited from the well-trained supernet, making ShiftNAS free from any additional search or retrain costs.
The overall contribution can be summarized as follows:
• A learnable sampling strategy, called probability shift, is proposed to relief the bias of uniform sampling which leads to performance gap between supernet training and subnet deployment.
• We propose an LSTM-based AG to precisely and effi-ciently offer the best subnet with desired resource con-straints. AG training can be differentiably trained with supernet under weight-entangled search space.
• We achieve state-of-the-art or competitive results on both CNN and ViT models. Therefore, ShiftNAS is a model-agnostic search method. 2.