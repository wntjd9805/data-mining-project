Abstract
Learning with noisy labels (LNL) is one of the most important and challenging problems in weakly-supervised learning. Recent advances adopt the sample selection strat-egy to mitigate the interference of noisy labels and use small-loss criteria to select clean samples. However, the one-dimensional loss is an over-simplified metric that fails to accommodate the complex feature landscape of various samples, and, hence, is prone to introduce classification er-rors during sample selection.
In this paper, we propose
RankMatch, a novel LNL framework that investigates ad-ditional dimensions of confidence and consistency in or-der to combat noisy labels. Confidence-wise, we propose a novel sample selection strategy based on confidence repre-sentation voting instead of the widely-used small-loss cri-terion. This new strategy is capable of increasing sam-ple selection quantity without sacrificing labeling accuracy.
Consistency-wise, instead of the widely adopted feature dis-tance metric for measuring the consistency of inner-class samples, we advocate that the rank of principal features is a much more robust indicator. Based on this metric, we propose rank contrastive loss, which strengthens the con-sistency of similar samples regardless of their labels and facilitates feature representation learning. Experimental results on noisy versions of CIFAR-10, CIFAR-100, Cloth-ing1M and WebVision have validated the superiority of our approach over existing state-of-the-art methods. 1.

Introduction
The remarkable success of DNNs stems from the avail-ability of large-scale datasets. However, it is highly labo-rious to obtain massive data with high-quality annotations.
*Corresponding author.
Figure 1: Motivation of RankMatch. After being trained with noisy labels, the model is validated by the augmented images. Noisy labels leads to high-entropy outputs, which lack confidence. We advocate for the use of confidence cri-terion for sample selection, as the confident samples are more likely to have clean labels. Moreover, The weakly augmented image is similar to training Image 1 (classi-fied as ‘dog’), while the strongly augmented one is simi-lar to training Image 2 (classified as ‘deer’). Considering the model tends to produce similar predictions for similar images, the two views of the same image have inconsis-tent predictions. Consequently, we introduce the rank con-trastive loss to enhance the semantic consistency.
Hence, training DNNs using inexpensive samples from the search engine or machine annotation [27, 33, 11] has be-come an attractive alternative. However, these methods in-evitably introduce erroneous labels, which cause poor per-formance as DNNs can easily overfit to noises [58]. Hence, learning with noisy labels (LNL) has become an important yet challenging task.
Sample selection is a widely adopted strategy in LNL that identifies clean samples from the dataset to alleviate the negative impact of noisy labels. The small-loss crite-rion is a popular sample selection strategy, which regards
samples with lower loss as clean ones. However, such a one-dimensional loss is too simplified to accommodate the sophisticated distribution of high-dimensional data, such as the image features. As a result, a plethora of easy-but-noisy samples with small loss are often wrongly grouped into the clean set by using the loss-based strategy. This confirmation bias would be gradually amplified in the subsequent loss minimization training and severely impact the final perfor-mance [24, 44].
Another challenge of LNL lies in the difficulty of obtain-ing robust and consistent representation for samples across categories and subjects [24, 26, 19].
In semi-supervised learning, this issue can be largely addressed by enforcing consistent predictions given different views of the same sample [21]. However, in LNL, due to the existence of wrong labels, different views of the same object could be clustered into different categories, leading to inconsistent estimations (see Figure 1). Though the sample selection mechanism can identify a part of clean samples, the consis-tent representation learned from this set of clean data only work on simple samples with high confidence. Hence, how to achieve consistent prediction on difficult samples with low confidence remains an open question.
In this work, we follow the line of sample selection to avoid the adverse impact of noisy labels. However, instead of relying on an over-simplified 1D loss function, we pro-pose to combat the noises by incorporating new perspec-tives from confidence and consistency. Confidence-wise, to ensure robust selection of clean samples, we leverage the advances of confidence learning (CL) [37] while avoiding its vulnerability to noisy network predictions. In particular, we investigate the performance of clean sample selection using fixed confidence thresholds at different noise levels.
The results (Figure 4) show that a fixed high threshold leads to a much more accurate sample selection even at the pres-ence of strong noises. Therefore, we leverage one fixed high confidence threshold for all classes to ensure the purity of confident samples. To address the lack of clean data by us-ing a high threshold, we further propose sample selection via confidence voting (SCV) to ensure an ample collection of clean labels. Specifically, we generate K clusters from confident samples for each class and treat the cluster center as confident prototype. Each sample is considered clean if the label is consistent with the results voted by its k near-est confident prototypes. We empirically find that the pro-posed confidence voting mechanism can generate reliable and clean samples in sufficient amounts, ensuring both the quality and quantity of clean samples.
Consistency-wise, in addition to the classic consistency regularization, i.e. enforcing consistent outputs of differ-ent views from the same sample, we propose to encourage consistent predictions between similar hard samples of the same category. To achieve this goal, we introduce rank con-trastive loss (RCL), a novel metric that fully leverages the rank of principal image features for robust measurement of similar samples. Empirical experiments show that the con-ventional l2 distance cannot exploit the inner structure of feature representation, and, hence, fails to foster consistent clustering in the complex feature space corrupted by noisy samples. Our key observation is that while similar hard samples (with low confidence) of the same class may have a large l2 distance in the feature space, the underlying rank of their principal features still remains consistent (see Fig-ure 2). Therefore, our proposed RCL loss is able to promote feature consistency by encouraging correct and discrimina-tive clustering of similar samples with low confidence.
We code our method as RankMatch as the two per-spectives of the proposed framework can mutually benefit each other. While more reliable data segmentation based on high confidence can benefit the subsequent represen-tation learning, the consistent representations further im-prove the confidence of uncertain data samples, and, in turn, strengthen our proposed confidence-based sample-selection mechanism. We validate the superiority of our method over the state-of-the-art approaches on several challenging benchmarks, including CIFAR, Clothing1M, and WebVi-sion. Our contributions can be summarized as follows.
• A method with new state-of-the-art performance that is specially tailored for LNL problems by fostering both confidence and consistency.
• A novel sample-selection via confidence voting (SCV) strategy that generates reliable and ample clean sam-ples for the subsequent training.
• Rank contrastive loss (RCL) based on the rank statis-tics of principal features that encourages consistency between hard samples of the same category. RCL can further benefit clean sample selection by promoting more discriminative features (see Fig. 1 in the supple-mentary material) for constructing intra-category clus-ters. 2.