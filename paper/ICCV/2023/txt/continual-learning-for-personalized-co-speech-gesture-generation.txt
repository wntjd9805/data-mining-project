Abstract
Co-speech gestures are a key channel of human communi-cation, making them important for personalized chat agents to generate. In the past, gesture generation models assumed that data for each speaker is available all at once, and in large amounts. However in practical scenarios, speaker data comes sequentially and in small amounts as the agent per-sonalizes with more speakers, akin to a continual learning paradigm. While more recent works have shown progress in adapting to low-resource data, they catastrophically for-get the gesture styles of initial speakers they were trained on. Also, prior generative continual learning works are not multimodal, making this space less studied. In this paper, we explore this new paradigm and propose C-DiffGAN: an approach that continually learns new speaker gesture styles with only a few minutes of per-speaker data, while retaining previously learnt styles. Inspired by prior continual learning works, C-DiffGAN encourages knowledge retention by 1) generating reminiscences of previous low-resource speaker data, then 2) crossmodally aligning to them to mitigate catas-trophic forgetting. We quantitatively demonstrate improved performance and reduced forgetting over strong baselines through standard continual learning measures, reinforced by a qualitative user study that shows that our method produces more natural, style-preserving gestures. Code and videos can be found at https://chahuja.com/cdiffgan 1.

Introduction
Human communication technologies for both verbal (e.g. spoken language) and nonverbal (e.g. co-speech gestures) have seen significant improvements which have made gen-erative models for human communication more natural and semantically relevant [32]. Advancements in speech-based personal assistants such as Cortana, Alexa, Siri, and more re-cently in text based conversational agents such as chatGPT1
[34], Meena [1], and Xiaoice [48] have paved the way for embodied personal assistants. As embodied agents have both 1https://openai.com/blog/chatgpt
Figure 1: Overview of the continual learning paradigm of co-speech gesture personalization task. We start with a source model G1 pre-trained on a source speaker. We personalize
G1 to target models G2, G3 and so on in a sequential manner using low-resource data (∼10 minutes) for each of the target speaker. verbal and nonverbal communicative channels, one techni-cal challenge is to be able to generate personalized visual co-speech gestures (i.e. nonverbal) using spoken language (i.e. verbal) [46, 2]. Previous works in co-speech gesture generation learn unique speaker styles in a static paradigm, where training is done on a fixed dataset consisting of data for all speakers available all at once. However, in practical scenarios of embodied agents learning in the wild, the agent would receive small amounts of training data sequentially -also known as a continual learning paradigm. The main goal of the paper is to learn a unified co-speech gesture genera-tion model with the ability to generate gestures in multiple different styles (see Figure 1). This goal is achieved in a challenging and practical continual learning setting where the model only has access to limited training data.
This problem setting brings a unique technical challenge, typically not studied in generative continual learning settings: crossmodal catastrophic forgetting. Due to the crossmodal
nature of our task, crossmodal catastrophic forgetting refers to the forgetting of the crossmodal grounding relationships between input spoken language modalities and output ges-ture modality of speakers that the model interacted with earlier. For example, consider a virtual agent that has the knowledge of generating gestures for one speaker. As it starts to interact with the new speakers in the world, it ex-periences new crossmodal grounding relationships between gestures and spoken language. While the gestures are heav-ily dependent on the spoken language, they are also heavily influenced by the new speakers’ idiosyncrasies. A practi-cal challenge here is that these interactions are often short, hence creating a low-resource setting for this agent. Another challenge is that the agent sequentially receives new data as it interacts with multiple new speakers over time. The goal of this virtual agent is to learn to generate personalized gestures for many different speakers without forgetting the crossmodal grounding of the speakers that it interacted with earlier in its life. The agent should be able to achieve these goals with the practical constraints of low-resource data, limited storage space, and faster training.
In this paper, we propose an approach, named C-DiffGAN, that can efficiently personalize co-speech gesture generation models from a high-resource source speaker to multiple low-resource target speakers. To the best of our knowledge, this is the first approach that is able to learn a personalized model for multiple speakers with only 2-10 minutes each of speaker data (i.e. as opposed to 10 hours [11, 2, 20, 14]) in a continual learning setting. Our
C-DiffGAN approach requires access to only 2-10 minutes of the input data (i.e. language and speech) for the prior speakers and 2-10 minutes of paired data (i.e. language, speech, and gestures) for the new speaker. For continually learning new speakers’ behaviors while not forgetting the prior speakers’, C-DiffGAN follows two steps: First, it di-rectly identifies shifts in crossmodal grounding relationships along with the shifts in the output domain from the pretrained source model. Based on these identified distribution shifts,
C-DiffGAN updates a few necessary parameters in a single layer of the source model, allowing efficient adaptation with low resources. Second, it utilizes the low-resource input data of prior speakers in tandem to prevent the model from drift-ing from the prior speakers’ crossmodal grounding, hence preventing crossmodal catastrophic forgetting. This is done via a novel proposed objective term Lccf . Our experiments study the effectiveness of our C-DiffGAN approach on a di-verse publicly available dataset and is substantiated through a myriad of quantitative and qualitative studies, which show that our proposed methodology significantly outperforms prior approaches for low resource continual learning of non-verbal grounding and personalization of gesture generation models. 2.