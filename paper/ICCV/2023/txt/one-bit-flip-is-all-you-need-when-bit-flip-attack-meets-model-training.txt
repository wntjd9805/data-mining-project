Abstract
Deep neural networks (DNNs) are widely deployed on real-world devices. Concerns regarding their security have gained great attention from researchers. Recently, a new weight modification attack called bit flip attack (BFA) was proposed, which exploits memory fault inject techniques such as row hammer to attack quantized models in the de-ployment stage. With only a few bit flips, the target model can be rendered useless as a random guesser or even be implanted with malicious functionalities. In this work, we seek to further reduce the number of bit flips. We propose a training-assisted bit flip attack, in which the adversary is involved in the training stage to build a high-risk model to release. This high-risk model, obtained coupled with a corresponding malicious model, behaves normally and can escape various detection methods. The results on bench-mark datasets show that an adversary can easily convert this high-risk but normal model to a malicious one on vic-tim’s side by flipping only one critical bit on average in the deployment stage. Moreover, our attack still poses a significant threat even when defenses are employed. The codes for reproducing main experiments are available at https://github.com/jianshuod/TBA. 1.

Introduction
Deep neural networks (DNNs) have been widely and successfully deployed in many mission-critical applica-tions, such as facial recognition [14, 31, 18] and speech recognition [36, 33, 29]. Accordingly, their security issues are of great significance and deserve in-depth explorations.
Currently, many studies have illustrated that DNNs are vulnerable to various attacks, such as data poisoning [26,
*Corresponding Author: Yiming Li (liyiming.tech@gmail.com).
Figure 1. The illustration of the optimization process for our training-assisted bit-flip attack. We alternately optimize the ob-jective of the released model and that of the flipped model. Ac-cordingly, this process will gradually move the original model Mo from the low-risk area to the high-risk state (i.e., M 3 r ), serving as the released model Mr passed to victims. The adversaries will turn it into malicious Mf for the attack in the deployment stage. 13, 17], and adversarial attacks [7, 1, 8]. Specifically, data poisoning is a training-stage attack, designed to implant ma-licious prediction behaviors in the victim model by manip-ulating some training samples. Adversarial attacks target the inference process of victim DNNs, leading to malicious predictions by adding small perturbations to target images.
Most recently, a few research [20, 21, 22, 6, 4, 3] demon-strated that DNNs are also vulnerable in the deployment stage. the adversaries can alter a victim model’s parameters in the memory of the devices where it is deployed by flipping their bit-level representations (e.g., ‘0’
→ ‘1’) to trigger malicious prediction behaviors. This threat
In particular,
is called the bit-flip attack (BFA). BFAs can achieve differ-ent goals including crushing DNNs’ accuracy to random-guess level [20, 35, 23], inserting trojans that can be ac-tivated via triggers (i.e., backdoor-oriented BFAs) [21, 6], and manipulating DNNs’ outputs via specific benign sam-ples (i.e., sample-wise BFAs) [22, 4]. Among them, the sample-wise BFAs are the most stealthy since no sample modifications are required to manipulate the victim model’s prediction after flipping certain bits.
Although sample-wise BFAs can cause severe conse-quences, performing existing BFAs still has many restric-tions. Particularly, state-of-the-art attacks still need to flip a relatively large number of bits, especially when the dataset is complicated and the model is large, since the benign (vic-tim) model may be far away from its malicious counterpart in the parameter space (as shown in Figure 1). However, as pointed in [35, 23, 19], flipping one bit in the memory of the target device is practical but flipping multiple bits is very challenging and sometimes infeasible (see more expla-nations in Section 2.1). As such, most existing BFAs are not practical. An intriguing question arises: Is it possible to de-sign an effective bit-flip attack where we only need to flip a few bits or even one bit of the victim model for success?
The answer to the aforementioned question is positive.
By revisiting bit-flip attacks, we notice that all existing methods concentrated only on the deployment stage, where the victim model was assumed to be trained on benign sam-ples with a standard process. In this paper, we demonstrate that it is possible to find a high-risk parameter state of the victim DNN during the training stage that is very vulnera-ble to bit-flip attacks. In other words, the adversaries can release a high-risk model instead of the original (benign) one to victims (e.g., open-sourced model communities or the company) to circumvent anomaly detection and acti-vate its malicious behaviors by flipping a few bits during the later deployment stage. This new BFA paradigm is called training-assisted bit-flip attack (TBA) in this paper.
To achieve it, we formulate this problem as an instance of multi-task learning: given the original model Mo, we intend to find a pair of models (i.e., released model Mr and flipped model Mf ) with minimal bit-level parameter distance such that the released model is benign while the flipped model is malicious. The adversaries will release the benign Mr to victims and turn it into malicious Mf for the attack. Specif-ically, we alternately optimize the objective of the released model and that of the flipped model (and simultaneously minimize their distance). This process will gradually move the original model Mo from the low-risk area to the high-risk state, as shown in Figure 1. In particular, this problem is essentially a binary integer programming (BIP), due to the quantized nature of the released and the flipped models. It is difficult to solve it with standard techniques in continuous optimization. To alleviate this problem, we convert the dis-crete constraint in the problem to a set of continuous ones and solve it effectively, inspired by ℓp-Box ADMM [32].
In conclusion, the main contributions of this paper are three-fold. (1) We reveal the potential limitations of exist-ing bit-flip attacks, based on which we propose the training-assisted bit-flip attack (TBA) as a new and complementary (2) We define and provide an effective
BFA paradigm. method to solve the problem of TBA. (3) We empirically show that our attack is effective, requiring flipping only one bit to activate malicious behaviors in most cases. 2.