Abstract
An important challenge for autonomous agents such as robots is to maintain a spatially and temporally consistent model of the world.
It must be maintained through oc-clusions, previously-unseen views, and long time horizons (e.g., loop closure and re-identification). It is still an open question how to train such a versatile neural representation without supervision. We start from the idea that the train-ing objective can be framed as a patch retrieval problem: given an image patch in one view of a scene, we would like to retrieve (with high precision and recall) all patches in other views that map to the same real-world location. One drawback is that this objective does not promote reusability of features: by being unique to a scene (achieving perfect precision/recall), a representation will not be useful in the context of other scenes. We find that it is possible to bal-ance retrieval and reusability by constructing the retrieval set carefully, leaving out patches that map to far-away lo-cations. Similarly, we can easily regulate the scale of the learned features (e.g., points, objects, or rooms) by ad-justing the spatial tolerance for considering a retrieval to be positive. We optimize for (smooth) Average Precision (AP), in a single unified ranking-based objective. This ob-jective also doubles as a criterion for choosing landmarks or keypoints, as patches with high AP. We show results cre-ating sparse, multi-scale, semantic spatial maps composed of highly identifiable landmarks, with applications in land-mark retrieval, localization, semantic segmentation and in-stance segmentation. 1.

Introduction
For an autonomous agent to be able to take useful ac-tions, it must maintain a spatially and temporally consistent
Code and model weights for this project can be found at https:
//www.robots.ox.ac.uk/˜vgg/research/locus.
Figure 1: Problem setting. Our goal is to train a network to extract features that are identifiable and 3D-consistent, so that features at image locations corresponding to the same region in 3D space, but viewed from different positions, are similar. This can be done at multiple scales, from large (e.g., the kitchen islands in the large green circles) to small (e.g., the drawers in the small red circles). However, simply op-timizing for “unique” representations at each location (e.g., via contrastive learning) runs the risk of over-fitting to the training scenes, as such objectives will discourage reuse of the same representation for different places.
Instead, we encourage reusable landmark representations, such as the concept of a kitchen island, which may appear in different scenes (top and bottom panels) with appearance variations.
world model. This model may comprise the agent itself (e.g., pose estimation), the environment (e.g., mapping), and dynamic objects (e.g., instance detection), and must be maintained when confronted with previously-unseen views, occlusions, and long time horizons (e.g., loop closure and re-identification). However, visual observations used by an agent for this purpose are inherently inconsistent: the same landmark may appear significantly different from different viewpoints in space and time, due to (self) occlusion, reflec-tions, lighting variations, and dynamic effects, among other factors. Errors in the estimation of the agent’s internal state compound these problems. Therefore, it is important for any vision-based agent to convert observations into some spatio-temporally consistent form.
Existing approaches [4, 21, 41] do this at the observation synthesis (mapping) stage, by aggregating or distilling vi-sual information in 3D. We argue that significant progress can be made before this point, at the observation processing stage, which lends itself to a more flexible image-centered representation that is useful for a range of tasks. The key is to encourage consistency between image features that un-project to the same region of 3D space, within a spatial tol-erance, defining a landmark at a given scale.
This can be achieved by formulating the problem as one of patch retrieval: given an image patch from one view of a scene, retrieve all patches in other views that correspond to the same 3D location, with high precision and recall.
To encourage reusability, so that the learned features are useful in new scenes, we exclude patches from the retrieval set if (when unprojected) they exceed a fixed distance from the query. Excluding such patches ensures that the representations of similar-looking landmarks in distant places are not pushed apart unnecessarily, which would promote over-fitting unique representations to the training scenes, thus making them non-reusable in new scenes.
Moreover, by adjusting the spatial tolerance that defines the positive set, we can regulate the scale of the learned features. This allows us to learn features at a small scale (e.g., local textures and structures), medium scale (e.g., household objects), and large scale (e.g., whole rooms or places) in the same framework.
We learn this representation by optimizing a ranking-based metric, (smooth) Average Precision (AP), which doubles as a criterion for choosing distinctive landmarks (keypoints). The resulting Location-Consistent Universal features are semantically-meaningful,
Stable (LoCUS) 3D-consistent at the selected scale, and balance distinctive-ness with reusability, producing sparse, multi-scale, and semantic maps. We demonstrate applications in landmark retrieval, localization, semantic segmentation and instance segmentation.
To summarize, our contributions are: 1. A framework for learning 3D-consistent features from posed images via retrieval, taking into account multiple scales and how to trade off retrieval performance vs. gen-eralization performance (reusability). 2. A unified ranking-based objective function that facilitates the selection of highly-identifiable landmarks. 3. An evaluation of the proposed features on real images of indoor environments, on the tasks of place recogni-tion, semantic segmentation, instance segmentation and re-identification, as well as relative pose estimation. 2.