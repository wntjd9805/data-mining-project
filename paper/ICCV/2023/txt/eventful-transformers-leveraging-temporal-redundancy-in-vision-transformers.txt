Abstract
Vision Transformers achieve impressive accuracy across a range of visual recognition tasks. Unfortunately, their accuracy frequently comes with high computational costs.
This is a particular issue in video recognition, where mod-els are often applied repeatedly across frames or tempo-ral chunks. In this work, we exploit temporal redundancy between subsequent inputs to reduce the cost of Trans-formers for video processing. We describe a method for identifying and re-processing only those tokens that have changed significantly over time. Our proposed family of models, Eventful Transformers, can be converted from ex-isting Transformers (often without any re-training) and give adaptive control over the compute cost at runtime. We eval-uate our method on large-scale datasets for video object detection (ImageNet VID) and action recognition (EPIC-Kitchens 100). Our approach leads to significant computa-tional savings (on the order of 2-4x) with only minor reduc-tions in accuracy. 1.

Introduction
Transformers, initially designed for language model-ing [57], have been recently explored as an architecture for vision tasks. Vision Transformers [16] have achieved im-pressive accuracy across a range of visual recognition prob-lems, attaining state-of-the-art performance in tasks includ-ing image classification [16], video classification [1, 2, 18], and object detection [8, 37, 40, 61].
One of the primary drawbacks of vision Transformers is their high computational cost. Whereas typical convo-lutional networks (CNNs) consume tens of GFlops per im-age [7], vision Transformers often require an order of mag-nitude more computation, up to hundreds of GFlops per im-age. In video processing, the large volume of data further amplifies these costs. High compute costs preclude vision
Transformers from deployment on resource-constrained or latency-critical devices, limiting the scope of this otherwise
Figure 1. Eventful Transformers. Our method exploits temporal redundancy between subsequent model inputs. (Top) Within each
Transformer block, we identify and update only those tokens with significant changes over time. Image: [5]. (Bottom) In addition to improving efficiency, our method gives fine-grained control over the compute cost at runtime. “Budget” refers to parameter r as described in Section 4.3. “Flush” refers to the initialization of all tokens on the first time step. This example shows the ViTDet [37] object detection model on a video from the VID [54] dataset. exciting technology. In this paper, we present one of the first methods to use temporal redundancy between subse-quent inputs to reduce the cost of vision Transformers when applied to video data.
Temporal redundancy. Consider a vision Transformer that is applied frame-by-frame or clip-by-clip to a video se-quence. This Transformer might be a simple frame-wise
model (e.g., an object detector) or an intermediate step in some spatiotemporal model (e.g., the first stage of the factorized model from [1]). Unlike in language process-ing, where one Transformer input represents a complete se-quence, we consider Transformers applied to several dis-tinct inputs (frames or clips) over time.
Natural videos contain significant temporal redundancy, with only slight differences between subsequent frames.
Despite this fact, deep networks (including Transformers) are commonly computed “from scratch” on each frame.
This approach is wasteful, discarding all potentially relevant information from previous inferences. Our key intuition is that we can reuse intermediate computations from earlier time steps to improve efficiency on redundant sequences.
Adaptive inference. For vision Transformers (and deep networks in general), the inference cost is typically fixed by the architecture. However, in real-world applications, the available resources may vary over time (e.g., due to com-peting processes or variations in power supply). As such, there is a need for models whose computational cost can be modified at runtime [45]. In this work, adaptivity is one of our primary design objectives; we design our method to al-low real-time control over the compute cost. See Figure 1 (bottom portion) for an example where we vary the compute budget throughout a video.
Challenges and opportunities. There are past works ex-ploring temporal redundancy [17, 23, 48] and adaptiv-ity [44, 58, 66] for CNNs. However, these methods are gen-erally incompatible with vision Transformers, owing to sub-stantial architectural differences between Transformers and
CNNs. Specifically, Transformers introduce a new prim-itive, self-attention, that does not conform to the assump-tions of many CNN-based methods.
Despite this challenge, vision Transformers also repre-sent a unique opportunity. In CNNs, it is difficult to trans-late sparsity improvements (i.e., the sparsity gained by con-sidering temporal redundancy) into concrete speedups. Do-ing so requires imposing significant restrictions on the spar-sity structure [23] or using custom compute kernels [48]. In contrast, the structure of Transformer operations (centered on manipulating token vectors) makes it easier to translate sparsity into reduced runtime using standard operators.
Eventful Transformers. We propose Eventful Transform-ers, a new class of Transformer that leverages temporal re-dundancy between inputs to enable efficient, adaptive in-ference. The term “Eventful” is inspired by event cam-eras [4, 39], sensors that produce sparse outputs based on scene changes. Eventful Transformers track token-level changes over time, selectively updating the token represen-tations and self-attention maps on each time step. Blocks in an Eventful Transformer include gating modules that allow controlling the number of updated tokens at runtime.
Our method can be applied to off-the-shelf models (gen-erally without re-training) and is compatible with a wide range of video processing tasks. Our experiments demon-strate that Eventful Transformers, converted from existing state-of-the-art models, significantly reduce computational costs while largely preserving the original model’s accu-racy. We publicly release our code, which includes Py-Torch modules for building Eventful Transformers. See our project page: wisionlab.com/project/eventful-transformers.
Limitations. We demonstrate wall-time speedups on both the CPU and GPU. However, our implementation (based on vanilla PyTorch operators) is likely sub-optimal from an en-gineering standpoint. With additional effort to reduce over-head (e.g., implementing a fused CUDA kernel for our gat-ing logic), we are confident that the speedup ratios could be further improved. Our method also involves some unavoid-able memory overheads. Perhaps unsurprisingly, reusing computation from previous time steps requires maintaining some tensors in memory. These memory overheads are rel-atively modest; see Section 6 for further discussion. 2.