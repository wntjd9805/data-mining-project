Abstract 3D lane detection usually builds a dense correspondence between the front-view space and the BEV space to estimate lane points in the 3D space. 3D lanes only occupy a small ratio of the dense correspondence, while most correspon-dence belongs to the redundant background. This sparsity phenomenon bottlenecks valuable computation and raises the computation cost of building a high-resolution corre-spondence for accurate results. In this paper, we propose a sparse point-guided 3D lane detection, focusing on points related to 3D lanes. Our method runs in a coarse-to-fine manner, including coarse-level lane detection and iterative fine-level sparse point refinements. In coarse-level lane de-tection, we build a dense but efficient correspondence be-tween the front view and BEV space at a very low resolution to compute coarse lanes. Then in fine-level sparse point refinement, we sample sparse points around coarse lanes to extract local features from the high-resolution front-view feature map. The high-resolution local information brought by sparse points refines 3D lanes in the BEV space hierar-chically from low resolution to high resolution. The sparse point guides a more effective information flow and greatly promotes the SOTA result by 3 points on the overall F1-score and 6 points on several hard situations while reducing almost half memory cost and speeding up 2 times. 1.

Introduction 3D lane detection is an indispensable part of the ad-vanced driver assistance system, supporting functionali-ties such as automated lane centering and lane departure warning. It relies on building a dense correspondence be-tween the front view space and BEV space to localize lane points in 3D space. Previous methods build a dense corre-spondence by directly projecting the 2D image or 2D fea-*Corresponding author.
Figure 1: The visualization of dense correspondence (a) and sparse correspondence (b) between font view and BEV. The green grids represent areas near lanes, while blue grids are redundant areas far from lanes. ture map into 3D space using Inverse Perspective Mapping (IPM) [18, 38], but this kind of correspondence is hard to handle the complex road conditions (e.g., uphill and down-hill). Recently, some methods [3, 21] build a dense cor-respondence using a learnable transformer to resolve this problem. The transformer-based correspondence builds a more effective information flow from the front view space to the BEV space, which results in better performance in extreme scenes.
Building such dense correspondence is actually redun-dant for 3D lane detection. As shown in Figure 1, even among a local window near the lane, only 2/5 correspon-dences are beneficial for 3D lane detection, while the oth-ers are redundant. Among the dense correspondence, 3D lanes only occupy a small ratio, while most correspon-dences are built for the background. This redundant prob-lem becomes more severe at high resolution, where both the high-resolution front-view and BEV feature maps are necessary for high-quality lane detection in most methods
[6, 7, 18, 10, 38, 15]. To this end, we propose to construct a sparse correspondence only focusing on a limited num-ber of points around 3D lanes. The sparse correspondence builds an effective yet efficient information flow from the front-view space to the BEV space. It directly brings the high-resolution lane information from the front-view space, ensuring fine-grained details in high-resolution BEV space.
Without redundant correspondence, the information flow learns to focus more on features beneficial for 3D lane re-sults. Meanwhile, it has a naturally efficient performance by only allocating computation to points related to lanes.
In this paper, we present a sparse-point-guided 3D lane detection method that decomposes the 3D lane detection into coarse-level lane detection and fine-level sparse point refinement. In coarse-level lane detection, we extract multi-scale feature maps from the front-view image. An efficient but dense BEV feature map is built from the front-view fea-ture map at the lowest resolution to detect coarse 3D lanes.
Then the fine-level sparse point refinement refines coarse re-sults hierarchically from low resolution to high resolution.
In each refinement, we first sample sparse points around coarse 3D lanes within a specific window. And then, we project sampled points onto the front-view plane to extract local features from the high-resolution front-view feature map. The local feature provides fine-grained information to refine the local structure of 3D lanes. At the same time, we compress the front-view feature map into a single vec-tor as the global features to ensure the global smoothness of 3D lanes. We fuse the global feature, the local features, and the sparse point coordinates to predict the location and category of each lane. The fusion of the local and global features refers to the point coordinates, which ensures both the global smoothness and local discrimination of lanes.
We validate our method on two anchor-based approaches and a widely used segmentation-based approach. The demonstrations are conducted on two real-world datasets (OpenLane [3] and ONCE [18]), including different weath-ers, lane structures, and road conditions. Our method out-performs the two anchor-based approaches, showing our feasibility to be seamlessly integrated with different proto-typical methods to offer consistent improvement. Besides, our method simply outperforms the SOTA anchor-based ap-proach by 2 points on the overall F1-score and more than 6 points on the F1-score in several extreme conditions while reducing half memory cost and speeding up 2 times. As for the segmentation-based approach, our method achieves comparable performance and reduces the memory cost of the 3D lane head by 80% and speeds up 2 times. 2.