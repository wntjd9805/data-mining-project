Abstract
Federated Learning (FL) is popular for its privacy-preserving and collaborative learning capabilities. Recently, personalized FL (pFL) has received attention for its ability to address statistical heterogeneity and achieve personalization in FL. However, from the perspective of feature extraction, most existing pFL methods only focus on extracting global or personalized feature information during local training, which fails to meet the collaborative learning and personal-ization goals of pFL. To address this, we propose a new pFL method, named GPFL, to simultaneously learn global and personalized feature information on each client. We conduct extensive experiments on six datasets in three statistically heterogeneous settings and show the superiority of GPFL over ten state-of-the-art methods regarding effectiveness, scalability, fairness, stability, and privacy. Besides, GPFL mitigates overfitting and outperforms the baselines by up to 8.99% in accuracy. 1.

Introduction
To maximize the value of data generated on massive clients while protecting privacy, Federated Learning (FL), an iterative machine learning scheme, comes along with various applications [25, 21, 38, 35, 34]. Traditional FL methods focus on collaborative learning and obtaining a reasonable global model. However, in practice, one single global model cannot meet the requirements of every client and performs poorly due to statistical heterogeneity [24, 1, 50].
Recently, personalized FL (pFL) has attracted increasing attention in addressing statistical heterogeneity and achiev-ing personalization in FL [50, 49, 63, 62]. From the view of each client, it joins FL for additional server information (e.g., global model parameters) to enhance its model and
*Corresponding author. address the data shortage problem. To obtain high-quality server information, each client also has to provide locally learned information for server aggregation. Thus, an ideal pFL is one kind of FL with two goals: (1) aggregating infor-mation for collaborative learning and (2) training reasonable personalized models. On the other hand, since every client is connected to the external environment and shares certain common information, the data present on each client com-prises both global and personalized feature information.
However, from a feature extraction perspective, exist-ing pFL methods only focus on one of these two goals on clients. For collaborative learning, FedRoD [8] trains the feature extractor to extract global feature information for its global objective, but it does not extract personalized fea-ture information for personalized tasks. For personalization,
FedPer [3] and FedRep [12] only use local data to train the model for the personalized objective, losing some global information during local training [10], which is not bene-ficial for collaborative learning. Although FedPHP [32]/
FedProto [51] utilizes global features/prototypes to guide personalized feature extraction, the quality of global fea-tures/prototypes depends on the quality of feature extractors, which is paradoxical. Poor global features/prototypes mis-lead feature extraction in turn.
To simultaneously learn global and personalized fea-ture information on each client, we propose a novel pFL framework, named GPFL. Inspired by the category anchors that introduce extra common information in domain adapta-tion [65], we learn the global feature information with the guidance of global category embeddings using the Global
Category Embedding layer (GCE). Besides, we learn per-sonalized feature information through personalized tasks.
However, learning two contrary (global vs. personalized) objectives is confusing, so we devise and insert the Con-ditional Valve (CoV) after the feature extractor to create a global guidance route and a personalized task route in the client model. With CoV, we learn global and personalized 1
feature information separately at the same time, unlike Fe-dRoD, FedPer, and FedRep, which only learn one kind of feature information. Besides, GPFL leverages trainable cat-egory embeddings to guide feature extraction at both the magnitude and angle levels, unlike FedPHP and FedProto, which rely on the well-trained feature extractor. Further-more, the global category embeddings in GPFL introduce extra global information besides local data, which can mit-igate the overfitting of personalized models and enhance fairness and privacy-preserving ability.
To evaluate GPFL regarding effectiveness, scalability, fairness, stability, and privacy, we compare GPFL with ten state-of-the-art (SOTA) methods on six datasets in Computer
Vision (CV), Natural Language Processing (NLP), and Inter-net of Things (IoT) domains. Besides, we consider the label skew [40, 36, 27], feature shift [31], and real world [66, 15] settings to simulate different kinds of statistical heterogene-ity in FL. Experimental results show that GPFL outperforms these baselines by up to 8.99% in accuracy. We provide the code in the supplementary materials. Overall, our key contributions are
• We emphasize the importance of achieving both collab-orative learning and individualized goals in pFL and propose a pFL method GPFL that simultaneously learns the global and personalized feature information.
• We learn the global feature information through train-able category embeddings, and the additional global information in GCE mitigates the overfitting of the personalized model to local data.
• We conduct extensive experiments in the CV, NLP, and
IoT domains under label skew, feature shift, and real world settings. The results show that our GPFL out-performs the SOTA method in terms of effectiveness, scalability, fairness, stability, and privacy. 2.