Abstract
In addition to the unprecedented ability in imaginary creation, large text-to-image models are expected to take customized concepts in image generation. Existing works generally learn such concepts in an optimization-based manner, yet bringing excessive computation or memory bur-den. In this paper, we instead propose a learning-based en-coder, which consists of a global and a local mapping net-works for fast and accurate customized text-to-image gen-eration. In specific, the global mapping network projects the hierarchical features of a given image into multiple
“new” words in the textual word embedding space, i.e., one primary word for well-editable concept and other aux-iliary words to exclude irrelevant disturbances (e.g., back-ground). In the meantime, a local mapping network injects the encoded patch features into cross attention layers to provide omitted details, without sacrificing the editability of primary concepts. We compare our method with existing optimization-based approaches on a variety of user-defined concepts, and demonstrate that our method enables high-fidelity inversion and more robust editability with a signifi-cantly faster encoding process. Our code is publicly avail-able at https://github.com/csyxwei/ELITE. 1.

Introduction
Recently, large-scale diffusion models [3, 29, 32, 34] have demonstrated impressive superiority in text-to-image generation. By training with billions of image-text pairs, large text-to-image diffusion models have exhibited excel-lent semantic understanding ability, and generate diverse and photo-realistic images being accordant to the given text prompts. Owning to their unprecedentedly creative capabil-ities, these models have been applied to various tasks, such as image editing [16, 23], data augmentation [24], and even artistic creation [26].
However, despite diverse and general generation, users may expect to create imaginary instantiates with undescrib-able personalized concepts [18], e.g., “corgi” in Fig. 1.
Figure 1. Given an input image, customized text-to-image gener-ation learns a pseudo-word (S*) in word embedding space to rep-resent the target concept. With S*, one can flexibly synthesize or edit the concept with text prompts. The running time to learn a new concept is also listed, and our method learns the new concept much faster than others.
To this end, many recent studies have been conducted for customized text-to-image generation [9, 15, 18, 33], which aims to learn a specific concept from a small set of user-provided images (e.g., 3∼5 images). Then, users can flex-ibly compose the learned concepts into new scenes, e.g., A
S* wearing sunglasses in Fig. 1. Given a small im-age set depicting the target concept, Textual Inversion [15] learned a new pseudo-word (i.e., S*) in the well-editable textual word embedding space of text encoder to represent the user-defined concept. DreamBooth [33] finetuned the entire diffusion model to accurately align the target concept with a unique identifier. Custom Diffusion [18] balanced the fidelity and memory by selectively finetuning K, V map-ping parameters in cross attention layers.
Albeit flexible generation has been achieved by [15, 18, 33], the computational efficiency remains a challenge to ob-tain the textual embedding of a visual concept. Existing methods usually adopt the per-concept optimization formu-lation, which requires several or tens of minutes to learn a single concept. As shown in Fig. 1, the Custom Diffu-sion [18], which is among the fastest existing algorithms, still takes around 6 minutes to learn one concept, which is infeasible for online applications. In contrast, in GAN in-version, many efficient learning-based methods [31] have
been proposed to accelerate the optimization process. An encoder can be trained to infer the latent codes, which only needs one step forward inference.
Driven by the above analysis, we propose a learning-based encoder for Encoding visuaL concepts Into Textual
Embeddings, termed as ELITE. As shown in Fig. 2, our
ELITE adopts a pre-trained CLIP image encoder [28] for feature extraction, followed by a global mapping network and a local mapping network to encode visual concepts into textual embeddings. Firstly, we train a global mapping net-work to map the CLIP image features into the textual word embedding space of the CLIP text encoder, which has su-perior editing capacity [15]. Since a given image contains both the subject and irrelevant disturbances, encoding them as a single word embedding severely degrades the editabil-ity of subject concept. Thus, we propose to separately learn them with a well-editable primary word and several auxil-iary words. Using the hierarchical features from CLIP inter-mediate layers, the word learned from the deepest features naturally links to the primary concept (i.e., the subject), while auxiliary words learned from other features describe the irrelevant disturbances (as shown in Fig. 5). When de-ploying to customized generation, we only use the primary word to avoid editability degradation from auxiliary words.
Usually, a visual concept is worth more than one word, and describing it with a single word may result in the in-consistency of local details [15]. For higher fidelity of the learned concept without sacrificing its editability, we fur-ther propose a local mapping network to inject finer de-tails. From Fig. 2, our local mapping network encodes the
CLIP features into the textual feature space (i.e., the output space of the text encoder). Compared with the textual word embeddings learned by global mapping network, the tex-tual feature embeddings focus on the local details of each patch in the given image. Then, the obtained textual fea-ture embeddings are injected through additional cross atten-tion layers, and the output feature is fused with the global part to improve the local details. Experiments show that our
ELITE can encode the target concept efficiently and faith-fully, while keeping control and editing abilities. The con-tributions of this work are summarized as follows:
• We propose a learning-based encoder, namely ELITE, for fast and accurate customized text-to-image genera-tion. It adopts a global and a local mapping networks to encode visual concepts into textual embeddings.
• Multi-layer features are adopted in global mapping to learn a well-editable primary word embedding, while the local mapping improves the consistency of details without sacrificing editability.
• Experimental results show that our ELITE can faith-fully recover the target concept with higher visual fi-delity, and enable more robust editing.
Figure 2. Inference pipeline of our proposed ELITE. Given a user-provided image x, our ELITE extracts hierarchical features with CLIP image encoder. Then, it uses global and local map-ping networks to encode the visual concept into textual word em-beddings (i.e., primary word w0 and auxiliary words w1···N ) and textual feature embeddings, respectively. The embeddings are in-jected with cross attention to guide customized generation. Note that, in generation, only w0 is used for better editability. 2.