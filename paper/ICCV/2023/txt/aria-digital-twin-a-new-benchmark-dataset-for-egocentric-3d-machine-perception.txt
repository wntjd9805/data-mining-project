Abstract
We introduce the Aria Digital Twin (ADT) 1 - an egocen-tric dataset captured using Aria glasses with extensive ob-ject, environment, and human level ground truth. This ADT release contains 200 sequences of real-world activities con-ducted by Aria wearers in two real indoor scenes with 398 object instances (344 stationary and 74 dynamic). Each sequence consists of: a) raw data of two monochrome cam-era streams, one RGB camera stream, two IMU streams; b) complete sensor calibration; c) ground truth data including continuous 6-degree-of-freedom (6DoF) poses of the Aria devices, object 6DoF poses, 3D eye gaze vectors, 3D hu-man poses, 2D image segmentations, image depth maps; and d) photo-realistic synthetic renderings. To the best of our knowledge, there is no existing egocentric dataset with a level of accuracy, photo-realism and comprehensiveness comparable to ADT. By contributing ADT to the research community, our mission is to set a new standard for evalu-ation in the egocentric machine perception domain, which includes very challenging research problems such as 3D ob-ject detection and tracking, scene reconstruction and un-derstanding, sim-to-real learning, human pose prediction -while also inspiring new machine perception tasks for aug-mented reality (AR) applications. To kick start exploration of the ADT research use cases, we evaluated several existing state-of-the-art methods for object detection, segmentation and image translation tasks that demonstrate the usefulness of ADT as a benchmarking dataset. 1.

Introduction
Egocentric data has become increasingly important to the machine perception community in the past several years due to the rapid emergence of AR applications. Such ap-plications require the co-existence of the real-world space and a virtual space along with a contextual awareness of 1Download dataset and tools: https://www.projectaria.com/datasets/adt/ the real surroundings. Complete contextual awareness can-not be achieved without a full and accurate 3D digitization of three fundamental elements in the real-world space: hu-mans, objects and the environment. Every object and en-vironmental component, including lighting, room structure and layout, has to be precisely digitized to unlock consis-tent rendering of the virtual space within the real world.
Dynamic object motion needs to be tracked in 3D to up-date the state of the space via physical interactions. The state of the human wearing AR glasses should be estimated and intersected with the digital space to derive the interac-tion in both physical and virtual spaces. Achieving all of this requires solutions to a number of core problems such as 3D object detection, human pose estimation, and scene reconstruction, where data is the key component.
Existing datasets that aim at progressing the field of AR do not focus holistically on the problem space, but rather on specific sub-problems. A significant amount of progress in large scale static scene datasets [7, 36, 6] has helped to advance 3D scene understanding tasks such as static object detection, scene reconstruction and room layout estimation.
Although the photo-realism of these reconstructed scenes is continuously improving [37], these datasets lack the mo-tion of objects introduced by hand interactions that com-monly occur in egocentric AR scenarios. Object-centric datasets [1, 42] that include increasingly complex occlu-sions between objects, also require that objects be station-ary to facilitate the annotation process. Dynamic object datasets [11, 12, 15] capture hand-object interaction but the data is captured in controlled, simplified environments.
Egocentric human motion datasets [30, 45] capture 3D hu-man poses with annotation of 3D joint positions but with-out the digitization of the environment. Most importantly, none of the discussed datasets leverage an AR-style sensing device that captures the unique challenges with egocentric data such as fast ego motion, sub-optimal viewpoint, low-power sensing hardware, etc. Although some egocentric datasets [13, 38, 10] have emerged recently, they present only either narrative annotation or 2D object annotation
(a) Top-down rendering of two spaces with the apartment on the left and the office on the right. (b) 2D visualization of ground truth projected onto Aria camera sensors. From top to bottom: the RGB, the left monochrome, the right monochrome camera sensors. From left to right: raw sensor image; photo-realistic synthetic rendering; 3D bounding boxes (cyan for stationary and red for dynamic objects), 2D bounding boxes, segmentation masks for all object instances; depth map.
Figure 1: An overview of the ADT dataset. without addressing the challenges in 3D space.
The availability of egocentric data capture devices has been surging in recent years, e.g., Vuzix Blade, Pupil Labs,
ORDRO EP6, etc. Among them, the popularity of Aria glasses [35] is quickly growing due to its standard glasses-like form factor and the full egocentric sensor suite includ-ing, but not limited to, a red-green-blue (RGB) camera, two monochrome cameras, two eye tracking cameras and two inertial measurement units (IMUs) which allows users to tackle a broad spectrum of machine perception tasks in real-world activities. The availability of Aria data has been ac-celerated by the recent release of the Aria Pilot Dataset [28].
Motivated by the gap in holistic egocentric 3D data highlighted above, we have created the Aria Digital Twin (ADT) dataset to accelerate egocentric machine perception research for AR applications. This dataset offers 200 se-quences collected by Aria-wearers performing real-world activities in two realistic spaces - an apartment and an of-fice, with a combined total of 344 stationary and 74 dy-namic object instances. Compared to existing work, each
ADT sequence offers more complete and accurate ground truth data for the digital space including: device calibra-tions, device and object 6-degree-of-freedom (6DoF) poses, human poses, eye gaze vectors, object segmentation, depth maps and photo-realistic synthetic images. Figure 1a shows top-down renderings of two spaces, and Figure 1b shows a 2D visualization of all object ground truth projected onto the Aria camera sensors. Figure 2 shows a snapshot im-age of the data capturing process and a 3D rendering of the human ground truth data.
To build this dataset, we reconstructed every object and the entire environment of the two spaces in a metric, photo-realistic pipeline. We integrated a motion capture system with the digitized space and precisely synchronized it with the Aria glasses to track objects and humans while record-ing egocentric data in a spatio-temporally aligned environ-ment. We demonstrate the quality of the 3D reconstruction via a qualitative evaluation and the accuracy of the object tracking via a novel quantitative evaluation. We performed evaluations on several existing state-of-the-art methods for object detection, segmentation and image translation tasks to demonstrate the usefulness of ADT when testing AR re-lated machine perception algorithms. Our contribution is the establishment of a new standard for both the quality and comprehensiveness of digitized real-world indoor spaces to advance fundamental AR research by means of an exem-plary dataset and methodology for the creation of such a dataset. 2.