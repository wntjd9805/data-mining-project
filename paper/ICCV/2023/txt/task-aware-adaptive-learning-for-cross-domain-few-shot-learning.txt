Abstract
Although existing few-shot learning works yield promis-ing results for in-domain queries, they still suffer from weak cross-domain generalization. Limited support data requires effective knowledge transfer, but domain-shift makes this harder. Towards this emerging challenge, researchers im-proved adaptation by introducing task-specific parameters, which are directly optimized and estimated for each task.
However, adding a fixed number of additional parameters fails to consider the diverse domain shifts between target tasks and the source domain, limiting efficacy. In this paper, we first observe the dependence of task-specific parameter configuration on the target task. Abundant task-specific pa-rameters may over-fit, and insufficient task-specific param-eters may result in under-adaptation – but the optimal task-specific configuration varies for different test tasks. Based on these findings, we propose the Task-aware Adaptive Net-work (TA2-Net), which is trained by reinforcement learning to adaptively estimate the optimal task-specific parameter configuration for each test task. It learns, for example, that tasks with significant domain-shift usually have a larger need for task-specific parameters for adaptation. We eval-uate our model on Meta-dataset. Empirical results show that our model outperforms existing state-of-the-art meth-ods. Our code is available at https://github.com/
PRIS-CV/TA2-Net. 1.

Introduction
Traditional deep learning models [42, 36, 12, 17] show excellent generalization performance when training on a large number of labeled samples. However, both abundant samples and reliable annotations are not always available in realistic applications, e.g., rare disease diagnosis, and fine-grained recognition. Few-shot learning [32, 33, 23, 4, 10], inspired by the fact that humans can quickly learn new
*Corresponding Author (a) and (b) reveal the accuracy of varying test tasks
Figure 1. when inference was performed in different models, in which TA-Modules are attached to each layer in the backbone network. (c) and (d) show the accuracy when TA-Modules are attached to dif-ferent layers in the backbone network. Four dashed boxes show the backbone and several TA-Modules. knowledge, aims to adapt the model to new classes by only a few labeled samples for each one.
Recently, meta-learning-based few-shot learning meth-ods have made great strides in the setting where the train and test tasks are sampled from the same domain [49, 22].
However, more and more studies have demonstrated that these existing works fail to generalize well to novel classes that are heterogeneous with the source domain [45, 9, 14].
This limitation is attributable to the fact that most previous few-shot learning models focus solely on how to quickly adapt to a set of novel classes with only a few labeled sam-ples per class but less effort has been made to understand and address the domain shift problem between the target (test) task and training domain. Recently, to address this problem, a series of research referred to as cross-domain few-shot learning are proposed [45, 14, 35, 3, 7, 20, 26, 6].
Generally, these existing methods address the domain
shift problem by introducing a set of task-specific param-eters to enable the feature extractor or classifier to adapt to the feature distributions of new tasks. According to the pa-rameter generation method, these methods can be loosely divided into the following two main streams. One kind of approach is to use an auxiliary network [35, 3, 43] to gener-ate task-specific parameters. The auxiliary network is first meta-trained with multiple tasks from the source domain, and then it utilizes the support set to estimate task-specific parameters for the target task. Other methods [21] directly attach a set of task-specific parameters to the pre-trained model on the source domain and then optimize them on a few labeled samples of the target task to well generalize the knowledge to new classes. Although cross-domain few-shot learning performance has improved, the fixed number of task-specific parameters may be too rigid to account for diverse tasks, resulting in less generality.
In this paper, from the perspective of parameter size, we studied the performance of test tasks when inference was performed in models equipped with varying sizes of task-specific parameters. Specifically, following the trend of in-troducing directly optimizing task-specific parameters [21], we designed several Task-specific Adapters (TA-Modules) with increasing parameters, which are respectively attached to the conv of pre-trained backbone network (e.g., ResNet-18) to construct cross-domain models. As shown in Fig-ure 1, the optimal task-specific parameters policy varies de-pending on the target task. More specifically, for differ-ent test tasks, the optimal task-specific parameters required in different layers of the backbone network for effectively adapting pre-learned knowledge to the target task’s feature distribution differ. Abundant task-specific parameters may over-fit on target tasks, whereas few task-specific parame-In order to ters will show the under-adaptation problem. effectively direct the feature extractor to adapt feature distri-butions, we must carefully design task-specific parameters in each layer for the target task.
Motivated by the findings, for improving the recognition performance of cross-domain few-shot learning, we pro-pose the Task-aware Adaptive Network (TA2-Net) to learn the optimal task-specific parameters policy for target tasks adaptively. As shown in Figure 2, our TA2-Net model con-sists of an Action Generation Network (“Agent”) trained by Reinforcement Learning to generate “actions” – adap-tive Task-specific Adapter execution decisions aimed at the target task, and an Adapted Network built on the “actions” to infer the target task, meanwhile, which can be viewed as the “Environment” to provide a “reward” for the Ac-tion Generation Network. The TA2-Net can learn optimally adapted network graphs aimed at the target task to transfer pre-learned knowledge efficiently.
In summary, our contributions are: (i) We propose the Task-aware Adaptive Network (TA2-Net), which can adaptively learn optimal task-specific pa-rameters policy for each target task. (ii) We evaluate our model on the Meta-dataset. Em-pirical results show that our model obviously outperforms existing state-of-the-art methods. (iii) We further analyze parameter distribution and dis-cover that domains with significant distribution shift (com-pared with the source domain) usually have a larger demand for task-specific parameters to effectively adapt pre-learned knowledge. Furthermore, layers in deep blocks necessitate more task-specific parameters than those in shallow blocks for learning task-specific features. 2.