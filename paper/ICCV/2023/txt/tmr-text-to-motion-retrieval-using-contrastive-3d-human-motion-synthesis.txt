Abstract e
A person is stretching legs
In this paper, we present TMR, a simple yet effective approach for text to 3D human motion retrieval. While previous work has only treated retrieval as a proxy evaluation metric, we tackle it as a standalone task. Our method extends the state-of-the-art text-to-motion synthesis model TEMOS, and incorporates a contrastive loss to better structure the cross-modal latent space.
We show that maintaining the motion generation loss, along with the contrastive training, is crucial to obtain good perfor-mance. We introduce a benchmark for evaluation and provide an in-depth analysis by reporting results on several protocols.
Our extensive experiments on the KIT-ML and HumanML3D datasets show that TMR outperforms the prior work by a signif-icant margin, for example reducing the median rank from 54 to 19. Finally, we showcase the potential of our approach on moment retrieval. Our code and models are publicly available at https://mathis.petrovich.fr/tmr. 1.

Introduction
The language of movement cannot be translated into words.
Barbara Mettler
We ask the question whether a cross-modal space exists between 3D human motions and language. Our goal is to retrieve the most relevant 3D human motion from a gallery, given a natural language query that describes the desired motion (as illustrated in Figure 1). While text-to-image retrieval is a well-established problem within the broader vision & language field [39], there has been less focus on the related task of text-to-motion retrieval. Searching an existing motion capture dataset based on text input can often serve as a viable alternative to text-to-human-motion synthesis in many applications, while also providing the added benefit that the retrieved motion is guaran-teed to be realistic. Additionally, once a cross-modal embedding is built to map text and motions into a joint representation space, both of the symmetrical text-to-motion and motion-to-text tasks rank 1 rank 2 rank 3 rank 4 rank 5 rank 6 rank 7 rank 8
Figure 1. Text-to-motion retrieval: We illustrate the task of text-based motion retrieval where the goal is to rank a gallery of motions according to their similarity to the given query in the form of a natural language description. can be performed. Such a retrieval-based solution has a range of applications, including automatically indexing large motion cap-ture collections, and helping to initialize the cumbersome text labeling process, by assigning the nearest text to each motion.
Let us first differentiate text-to-motion retrieval from text-to-motion synthesis. Motion synthesis [5, 9, 36, 49] involves generating new data samples that go beyond the existing training set, while motion retrieval searches through existing motion capture collections. For certain applications, reusing motions from a collection may be sufficient, provided the collection is large enough to contain what the user is searching for. Unlike generative models for motion synthesis, which struggle to produce physically plausible, realistic sequences [5, 35, 36], a retrieval model has the advantage that it always returns a realistic motion. With this motivation, we pose the problem as a nearest neighbor search through a cross-modal text-motion space.
Early work performs search through motion databases to build motion graphs [4, 23] by finding paths between existing motions and synthesizing new motions by stitching motions together with generated transitions. If the motion database is
labeled with actions, the user can specify a series of actions to combine [4]. In contrast, our search database is not labeled with text. Motion matching [8], on the other hand, seeks to find the animation that best fits the current motion by searching a database of animations, doing motion-to-motion retrieval [45].
Our framework fundamentally differs from these lines of work in that our task is multi-modal, i.e., user query is text, which is compared against motions. The most similar work to ours is the very recent model from Guo et al. [15], which trains for a joint embedding space between the two modalities. This model is only used to provide a performance measure for motion syn-thesis tasks, by querying a generated motion within a gallery of 32 descriptions (i.e., motion-to-text retrieval), and counting how many times the correct text is retrieved1. While this can be con-sidered as the first text-motion retrieval model in the literature, its main limitation is the low performance, in particular when the gallery contains fine-grained descriptions. We substantially improve over [15], by incorporating a joint synthesis & retrieval framework, as well as a more powerful contrastive training [34].
We get inspiration from image-text models such as BLIP [26] and CoCa [54], which formulate a multi-task objective. Besides the standard dual-encoder matching (such as CLIP [39] with two unimodal encoders for image and text), [26, 54] also employ a text synthesis branch, performing image captioning. Such a generative capability potentially helps the model go beyond
‘bag-of-words’ understanding of vision-language concepts, ob-served for the naive contrastive models [11, 55]. In our case, we depart from TEMOS [36] which already has a synthesis branch to generate motions from text. We incorporate a cross-modal contrastive loss (i.e., InfoNCE [34]) in this framework to jointly train text-to-motion synthesis and text-to-motion retrieval tasks.
We empirically demonstrate significant improvements with this approach when ablating the importance of each task.
Text-motion data differs from its text-image counterparts particularly due to the nature of motion descriptions. In fact, for an off-the-shelf large language model, sentences describing different motions tend to be similar, since they fall within the same topic of human motions. For example, the text-text cosine similarities [46] after encoding motion descriptions from the
KIT training set [37] are on average 0.71 on a scale between [0, 1], while this value is 0.56 (almost orthogonal) on a random sub-set of LAION [44] image descriptions with the same size. This poses several challenges. Typical motion datasets [15, 37, 38] contain similar motions with different accompanying texts, e.g.,
‘person walks’, ‘human walks’, as well as similar texts with dif-ferent motions, e.g., ‘walk backwards’, ‘walk forwards’. With naive contrastive training [34], one would make all samples within a batch as negatives, except the corresponding label for a given anchor. In this work, we take into account the fact that there are potentially significant similarities between pairs within a batch. Specifically, we discard pairs that have a text-text similarity in their labels that is above a certain threshold. Such careful negative sampling leads to performance improvements.
In this paper, we illustrate an additional use case for our retrieval model – zero-shot temporal localization – and high-light this task as a potential future avenue for research. Sim-ilar to temporal localization in videos with natural language queries [12, 13, 19, 25, 41], also referred to as “moment re-trieval”, we showcase the grounding capability of our model by directly applying it on long motion sequences to retrieve corre-sponding moments. We illustrate results on the BABEL dataset
[38], which typically contains a series of text annotations for each motion sequence. Note that the task is zero-shot, because the model has not been trained for localization, and at the same time has not seen BABEL labels, which come from a different domain (e.g., typically action-like descriptions instead of full sentences).
Our contributions are the following: (i) We address the little-studied problem of text-to-motion retrieval, and introduce a series of evaluation benchmarks with varying difficulty. (ii)
We propose a joint synthesis and retrieval framework, as well as negative filtering, and obtain state-of-the-art performance on text-motion retrieval. (iii) We provide extensive experiments to analyze the effects of each component in controlled settings.
Our code and models are publicly available2. 2.