Abstract
Recently, the RGB images and point clouds fusion meth-ods have been proposed to jointly estimate 2D optical flow and 3D scene flow. However, as both conventional RGB cameras and LiDAR sensors adopt a frame-based data acquisition mechanism, their performance is limited by the fixed low sampling rates, especially in highly-dynamic scenes. By contrast, the event camera can asynchronously capture the intensity changes with a very high temporal res-olution, providing complementary dynamic information of
In this paper, we incorporate RGB the observed scenes. images, Point clouds and Events for joint optical flow and scene flow estimation with our proposed multi-stage mul-timodal fusion model, RPEFlow. First, we present an at-tention fusion module with a cross-attention mechanism to implicitly explore the internal cross-modal correlation for 2D and 3D branches, respectively. Second, we intro-duce a mutual information regularization term to explic-itly model the complementary information of three modal-ities for effective multimodal feature learning. We also contribute a new synthetic dataset to advocate further re-search. Experiments on both synthetic and real datasets show that our model outperforms the existing state-of-the-art by a wide margin. Code and dataset is available at https://npucvr.github.io/RPEFlow. 1.

Introduction
Optical flow estimation, i.e., estimating the dense 2D motion between consecutive image frames, has been exten-sively studied and significantly advanced with the develop-ment of deep neural networks [1–3]. Scene flow estimation, on the other hand, aims to estimate the 3D motion field with various input configurations, ranging from monocu-lar images [4, 5], stereo images [6, 7], two frames of point clouds [8, 9], images combined with depth maps [10, 11]
† Corresponding author (daiyuchao@gmail.com). or point clouds [12, 13]. Both are fundamental to down-stream applications such as autonomous driving [14, 15], object tracking [16, 17], scene reconstruction [18, 19], etc.
Due to the strong correlation between 2D and 3D mo-tion, i.e., 2D motion can be regarded as the projection of 3D motion on the image plane, recent works [10, 12, 13] make efforts to jointly estimate optical flow and scene flow by combining RGB images and point clouds (or depth maps).
Their success indicates that joint 2D and 3D motion esti-mation within a framework can obtain more accurate re-sults than separate tasks. However, as both conventional
RGB cameras and LiDAR (or depth) sensors adopt a fixed frame-by-frame data acquisition mechanism, these methods show unsatisfactory performance when dealing with com-plex motion scenes (see Fig. 3), which motivates us to alle-viate this problem by introducing the event camera.
Event camera, as a bio-inspired imaging sensor, can asynchronously capture the brightness change with very high temporal resolution (in the order of µs) and output an event signal quickly [20]. As each pixel adapts its sampling rate according to the captured changes, the amount of out-put events usually depends on the complexity of motion (the faster the motion, the more triggered events), thus providing abundant motion information of the observed scene. Based on this, some works use event data alone to estimate op-tical flow [21, 22], but they show limitations in estimating reliable motion at regions with no events [23]. As compen-sation for this, image and event data are fused together to es-timate dense optical flow [24, 25]. As far as we know, there is no method to incorporate event data within a multimodal learning framework for both 2D and 3D motion estimation.
In this paper, we propose to fuse RGB images, point clouds and events for joint optical flow and scene flow es-timation. We find the ability of the event camera to asyn-chronously capture the brightness changes caused by mo-tion makes it complementary to image cameras and LiDAR sensors, especially for complex dynamics and high-contrast brightness changes. We believe that combining these three modalities together for 2D and 3D motion estimation meets the practical needs, which has been further confirmed by
existing datasets, such as MVSEC [26] and DSEC [22] that contain these data for driving scenarios.
We formulate this task as a representation-based mul-timodal learning problem, and exploit the complementary information between these three very different modalities implicitly and explicitly. We aim to exploit the relation-ships between multimodal and multi-dimensional space ob-servations (images and events in 2D with point clouds in 3D) and explore their contributions to 2D and 3D motion.
Specifically, in our RPEFlow framework, we first propose a multimodal attention fusion module with cross-attention mechanism to implicitly explore the correlations between three modalities, based on which a pyramid multi-stage fu-sion structure is introduced to extensively modeling. We observe that each modality can contribute a part to 2D and 3D motion estimation, making representation learning [27] suitable for our multimodal learning framework. Then we introduce cross-modal mutual information minimization in feature space to explicitly maximize the complementary in-formation. We also contribute a new synthetic dataset with simulations that conform to the gravity model and collision detection and contain a larger variety of moving objects and richer annotations than FlyingThings3D [28]. Extensive experimental results validate both our implicit multimodal attention fusion and explicit representation regularization towards effective multimodal learning, leading to a new benchmark on both synthetic and real-captured datasets.
Our main contributions are summarized as follows: 1) We propose to incorporate event cameras with RGB cameras and LiDAR sensors to jointly estimate optical flow and scene flow for complex dynamic scenes, which constitutes a new and practical problem. 2) An implicit multimodal attention fusion module and an explicit representation learning via mutual informa-tion regularization are presented in our RPEFlow model, achieving extensive cross-modal relationship modeling. 3) We contribute a large-scale synthetic dataset with ground-truth motion annotations. Experimental results on both synthetic and real datasets show that the pro-posed RPEFlow outperforms existing state-of-the-art and demonstrates the effectiveness of event data for mo-tion estimation of complex dynamics. 2.