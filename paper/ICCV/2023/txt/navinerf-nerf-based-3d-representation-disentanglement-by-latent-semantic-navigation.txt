Abstract 3D representation disentanglement aims to identify, de-compose, and manipulate the underlying explanatory fac-tors of 3D data, which helps AI fundamentally understand our 3D world. This task is currently under-explored and poses great challenges: (i) the 3D representations are com-plex and in general contains much more information than 2D image; (ii) many 3D representations are not well suited for gradient-based optimization, let alone disentanglement.
To address these challenges, we use NeRF as a differen-tiable 3D representation, and introduce a self-supervised
Navigation to identify interpretable semantic directions in the latent space. To our best knowledge, this novel method, dubbed NaviNeRF, is the first work to achieve fine-grained 3D disentanglement without any priors or supervisions.
Specifically, NaviNeRF is built upon the generative NeRF pipeline, and equipped with an Outer Navigation Branch and an Inner Refinement Branch. They are complemen-tary —— the outer navigation is to identify global-view semantic directions, and the inner refinement dedicates to fine-grained attributes. A synergistic loss is further devised to coordinate two branches. Extensive experiments demon-strate that NaviNeRF has a superior fine-grained 3D disen-tanglement ability than the previous 3D-aware models. Its performance is also comparable to editing-oriented models relying on semantic or geometry priors.* 1.

Introduction 3D reconstruction aims to create a virtual representa-tion of an object or scene based on point cloud, voxel, 3D mesh, and etc. Despite significant progress of explicit re-construction technologies such as Structure from Motion (SfM) [1], Multi-View Stereo (MVS) [2] and Structured
Light (SL) [3], it remains a critical problem that the recon-structed scenes typically lack interpretability and controlla-bility. Thus, it is important to study the 3D representation disentanglement, in which we can identify, decompose, and manipulate the underlying explanatory factors hidden in the observed 3D data.
However, 3D representation disentanglement is currently
*Denotes the corresponding author. Code is available at this link.
under-explored and faces great challenges: On one hand, the 3D representations are complex with the prohibitive storage costs, which in general contains much more in-formation than 2D image, like depth, viewpoint, etc. On the other hand, many high-dimensional 3D representations (e.g., discrete point cloud, mesh, voxel) are essentially not well suited for gradient-based optimization [4], which fur-ther increases the difficulty of disentanglement. All in all, how to efficiently and effectively achieve fine-grained 3D disentanglement without extra auxiliary priors or supervi-sions urgently needs to be solved.
Recently, the development of implicit representation learning has significantly promoted 3D reconstruction w.r.t the model flexibility and generalizability [5, 6, 7]. As a landmark of implicit 3D reconstruction, Neural Radi-ance Fields (NeRF) [4] maps scenes into a multi-layer perceptron (MLP) from limited views, which results in accurate, efficient, and differentiable 3D representations.
Moreover, as a deep neural model, NeRF has preliminary shown its capability w.r.t disentangled representation learn-ing in few studies [8, 9, 10]. Typically, the conditional
NeRFs [11, 12] achieve disentanglement with pre-defined extra latent codes, which inevitably limits the diversity of decomposed attributes. On the other branch, the editing-oriented NeRFs [13, 14] also achieve a controllable 3D syn-thesis. However, these approaches heavily relied on geo-metric priors and did not identify the underlying semantic representation, such specific priors largely limited the scope of practical applications.
Revisit the recent success of disentanglement in 2D im-age, we knew that traversing semantically meaningful direc-tions in the Generative Adversarial Network’s (GAN) [15] latent space leads to coherent variations in the generated 2D image [16, 17, 18, 19]. Typically, the way of smooth navigation [20, 21] is investigated for GAN-based semantic editing in the space of the generator’s parameters. These observations indicate that the underlying explanatory prop-erties are probably embedded in the generative latent space.
Based on the above discussions, in this paper, we ex-plore to use NeRF as a differentiable 3D representation
, and introduce a self-supervised navigation to identify interpretable semantic directions in the generative la-tent space. We name this novel method as NaviNeRF. As shown in Figure 1, NaviNeRF achieves a fine-grained 3D disentanglement by bridging 3D reconstruction and latent semantic manipulation. When shifting along the disentan-gled semantic direction that represents the mouth, we obtain a group of continuously changed visual results, look like a
“smile”.
In addition, the generated results of NaviNeRF could remain 3D consistency well across different views.
Specifically, Figure 2 showcases that NaviNeRF is com-posed of two main components: an outer navigation branch and an inner refinement branch. The outer navigation aims to identify the traversal directions as global-view factors in the latent space for disentangled representation learning
—— this process employs a learnable matrix to append a shift on a latent code. The shifted code, paired with the orig-inal one, are used to generate a pair of images through the pre-trained generator. A trainable decoder is then devised to predict the shift (i.e., semantic direction) based on such paired images, with a reconstruction loss [20]. Similarly, the inner branch dedicates to more fine-grained attributes by appending shifts on the specific dimensions of intermediate latent code. Finally, a synergistic loss function is further de-signed to combine these two complementary branches well.
Compared to off-the-shelf solutions, NaviNeRF does not re-sort to explicit conditional codes or any geometry priors. In summary, our contributions are: 1. To our best knowledge, the proposed NaviNeRF is the first work that could achieve fine-grained 3D disentan-glement at feature-level, without any priors and addi-tional supervision. 2. We take full advantage of both latent semantic nav-igation (the outer branch) and NeRF representation (the inner branch) in a complementary way. The outer branch learns to identify semantic directions for global disentangled representation learning, and the in-ner branch learns to focus on fine-grained attributes. 3. As a by-product, a simple synergistic loss is designed to collaborate well two outer-inner branches within
NaviNeRF.
We evaluate NaviNeRF on two popular benchmarks:
FFHQ [22] for the human face and AFHQ [23] for the an-imal face. NaviNeRF outperforms typical 3D-aware GANs including pi-GAN [24], GIRAFFE [25] and StyleNeRF [26] in attribute manipulation. Furthermore, the model obtains comparable performance to editing-oriented models which rely on semantic or geometric priors. Extensive ablation studies are also conducted to support our claims. 2.