Abstract
Prompt tuning and adapter tuning have shown great po-tential in transferring pre-trained vision-language models (VLMs) to various downstream tasks.
In this work, we design a new type of tuning method, termed as regularized mask tuning, which masks the network parameters through
Inspired by neural pathways, we a learnable selection. argue that the knowledge required by a downstream task already exists in the pre-trained weights but just gets concealed in the upstream pre-training stage. To bring the useful knowledge back into light, we first identify a set of parameters that are important to a given downstream task, then attach a binary mask to each parameter, and finally optimize these masks on the downstream data with the parameters frozen. When updating the mask, we introduce a novel gradient dropout strategy to regularize the parameter selection, in order to prevent the model from forgetting old knowledge and overfitting the downstream data. Experi-mental results on 11 datasets demonstrate the consistent superiority of our method over previous alternatives. It is noteworthy that we manage to deliver 18.73% performance improvement compared to the zero-shot CLIP via masking an average of only 2.56% parameters. Furthermore, our method is synergistic with most existing parameter-efficient tuning methods and can boost the performance on top of them. Project page can be found here. 1.

Introduction
The advent of large-scale pre-trained vision-language models (VLMs) [30] has ushered in a new era of incor-porating language features to supervise the image encoder for a wide range of downstream visual tasks, such as few-shot learning [47] and open-world detection [8]. Thanks to the multimodal architecture and millions of text-image pairs from the web, VLMs exhibit exceptional zero-shot transferability in downstream tasks. To further enhance
† indicates equal contribution.
Figure 1. Concept diagrams of (a) prompt tuning [47, 39], (b) adapter tuning [12, 43], and (c) our regularized mask tuning. The tables on the right of (a)(b) demonstrate the inference time and accuracy of the existing tuning method before and after combining with our regularized mask tuning method (R-AMT). The R-AMT significantly boosts their performance without introducing additional inference time. “Key Para.” refers to the identified key parameters (e.g., multi-head self-attetnion). Flames and snowflakes refer to learnable and frozen parameters, respectively. the transferability of VLMs, researchers have proposed efficient tuning methods, such as adapter tuning [12, 43] or prompt tuning [47, 39, 24]. These techniques incorporate a small number of task-specific parameters and train them solely on the downstream task, thus significantly improving the performance and reducing computational requirements.
The essence of efficient tuning methods lies in two fundamental components, i.e. leveraging the well-learned knowledge structure of VLMs and efficiently exploring the task-specific knowledge given few-shot data. Despite its potential, however, most existing efficient transfer learn-ing approaches direct utilize all parameters of pre-trained
VLMs and do not consider further unleashing the potential 1
Specifically, of the well-learned knowledge of VLMs. prompt tuning methods [47] use the frozen CLIP model and add the extra learnable parameters from the input side as shown in Fig. 1a. Adapter modules [12, 43] consist of a small set of the learnable module, further inserted into the frozen pre-trained model for adaptation as in Fig. 1b.
Despite the considerable efforts in efficient tuning methods from the prompt or adapter side, these methods do not explore the frozen CLIP parameters at all, choosing instead to add additional modules to learn task-specific knowledge.
Thus, as shown in Fig. 1c, we adopt mask tuning to explore the well-learned knowledge structure of VLMs and uncover the hidden knowledge in them for task-specific domains.
In the field of neural physiology [16, 9, 40], it has been discovered that neurons in the brain cortex exhibit diverse knowledge of various visual features such as shape, color, and depth. The knowledge is distributed in distinct neurons that have specific functions and work in conjunction with one another, termed neural pathways. When there is knowledge of a new environment coming, the neurons will compare it with the old knowledge learned in the past and then pick new conjunctions (i.e., neural pathways) to adapt to the new environment. Analogous to VLMs, parameters act as a manifestation of neurons and are responsible for memorizing knowledge from data. Thus, selecting suitable parameters as parameter pathways is beneficial for uncovering the key knowledge of downstream tasks.
Inspired by the neural pathways, we propose an efficient
Regularized Mask Tuning (R-MT) method to mask the parameters of the pre-trained VLMs under a learnable selection. Specifically, we first identify a subset of the parameters (e.g., multi-head self-attentive layer) based on the magnitude of the gradient changes as sensitive network parameters for downstream tasks. Then, we introduce a binary mask equipped with gradient dropout regularization to the selected parameters. Because few-shot training tends to cause overfitting, we introduce the logits from pre-trained
VLMs as the general knowledge to prevent mask tuning from forgetting. Concretely, the gradient dropout regular-ity as an effective regularizer introduces the probabilistic masking strategy that samples gradients based on the level of consistency of the downstream-related knowledge and the general knowledge, which can reject weak loss minima that may lead to overfitting. Intrinsic evaluations reveal that representations generated by our mask tuning can encode knowledge preferences for specific downstream tasks. The findings also indicate that selecting well-placed parameters is crucial for achieving successful transfer settings. More-over, our method is orthogonal to most existing parameter-efficient adaption methods (e.g., adapter and prompt) and endows them the ability to customization on downstream needs. Extensive experiments on 11 datasets demonstrate the effectiveness of the proposed method. 2.