Abstract
The “pre-training → downstream adaptation” presents both new opportunities and challenges for Continual Learn-ing (CL). Although the recent state-of-the-art in CL is achieved through Parameter-Efficient-Tuning (PET) adap-tation paradigm, only prompt has been explored, limiting its application to Transformers only. In this paper, we position prompting as one instantiation of PET, and propose a uni-fied CL framework with general PET, dubbed as Learning-Accumulation-Ensemble (LAE). PET, e.g., using Adapter,
LoRA, or Prefix, can adapt a pre-trained model to down-stream tasks with fewer parameters and resources. Given a PET method, our LAE framework incorporates it for CL with three novel designs. 1) Learning: the pre-trained model adapts to the new task by tuning an online PET mod-ule, along with our adaptation speed calibration to align different PET modules, 2) Accumulation: the task-specific knowledge learned by the online PET module is accumu-lated into an offline PET module through momentum up-date, 3) Ensemble: During inference, we respectively con-struct two experts with online/offline PET modules (which are favored by the novel/historical tasks) for prediction en-semble. We show that LAE is compatible with a battery of
PET methods and gains strong CL capability. For exam-ple, LAE with Adaptor PET surpasses the prior state-of-the-art by 1.3% and 3.6% in last-incremental accuracy on
CIFAR100 and ImageNet-R datasets, respectively. Code is available at https://github.com/gqk/LAE. 1.

Introduction
Continual Learning (CL) of new knowledge is an essen-tial ability for AI models in the constantly changing world.
However, neural networks often suffer from catastrophic forgetting [9, 39], in which previously learned knowledge is
† Corresponding authors.
This work was supported by Shenzhen General Research Project un-der Grant JCYJ20220531093215035, KAUST Office of Sponsored Re-search (OSR) under Award No. OSR-CRG2021-4648, and the SDAIA-KAUST Center of Excellence in Data Science and Artificial Intelligence.
Figure 1: The pipeline of our LAE framework vs. prompt-pool approaches.
Above: Prompt-pool ap-proaches, which query prompts from a pool of learn-able prompts. Below: Proposed LAE, where an online
Parameter-Efficient Tuning (PET) module attached to the pre-trained model to adapt a new task quickly, and an offline
PET module accumulates the learned knowledge slowly.
During inference, we use the ensemble of the predictions of the online and offline PET modules as the final prediction. forgotten when the model incorporates novel information.
Although many works have been devoted to reducing for-getting, such as dynamic networks [41, 50, 26, 18], regular-ization [28, 20, 52, 1], and memory replay [38, 15, 6, 30, 2], their performance still falls short of practical requirements.
Recently, pre-training and downstream adaptation tech-niques have opened up new opportunities and challenges for
CL. Basically, these techniques [4, 36, 12, 11, 48] pre-train a deep model on large-scale data and then adapt the pre-trained model to novel tasks. We observe that downstream adaptation and CL are important for each other. On the one hand, in realistic AI systems, pre-trained models sometimes needs to be adapted to multiple downstream tasks sequen-tially, yielding the need of CL. On the other hand, recent ef-forts [47, 46, 45] show that the “pre-training → downstream adaptation” techniques can boost CL performance.
Specifically, L2P [47], DualPrompt [46], and ESN [45] all use a popular adaptation technique named Parameter-Efficient-Tuning (PET). Generally, PET adapts pre-trained models to downstream tasks with much fewer learnable pa-rameters, as well as fewer resources. Though these ap-proaches have advanced the state-of-the-art in CL, they still have some limitations. 1) They are all constrained to a spe-1
cific PET method, i.e., prompt tuning, limiting their flexibil-ity, considering that prompt can only cooperate with trans-formers and does not accommodate other network architec-tures. 2) Most of them rely on selecting task-specific pa-rameters (the prompt tokens, in particular) for each indi-vidual task. The selection tends to be noisy with increasing task numbers and the task-specific prompts appear homoge-neous, according to our investigation in the supplementary. this paper proposes
Learning-Accumulation-Ensemble (LAE), a unified CL framework resort to the general Parameter-Efficient Tuning (PET). LAE is not restricted to Prompt, but can also utilize various other PET modules as shown in Fig. 2 (b). Given a
PET method, our LAE directly reshapes it for CL with three steps, i.e., learning, accumulation, and ensemble.
To circumvent these issues,
• 1) Learning with calibrated speed. The pre-trained model adapts to the new task by tuning an online PET mod-ule. To accommodate various PET methods, a key chal-lenge is that different PET modules have different adap-tation speeds (for novel tasks), as well as different forget-ting speeds (for historical tasks). In response, we design an adaptation calibration strategy, based on the gradient anal-ysis for different PET modules. We empirically show that this calibration strategy aligns different PET against each other and is critical for LAE to be a unified framework.
• 2) Accumulation of multi-task knowledge. After adapting the pre-trained model to a new task, the parameters in the online PET module are prone to the current novel task and may not fit historical tasks. Instead of memorizing mul-tiple sets of PET modules and selecting some subsets (as in
L2P and DualPrompt) for individual tasks, LAE accumu-lates all the knowledge of already-seen tasks into a single offline PET module through momentum update. This sim-ple accumulation avoids noisy selection and is competent for alleviating catastrophic forgetting, especially when the amount of learned tasks is large (Figs. 3 and 4 in Sec. 5).
• 3) Ensemble of two expert models. The online and offline PET modules respectively contain more novel and historical knowledge, therefore, two expert models con-structed with them are correspondingly better at handling newer and older tasks.
Instead of inference only using the online or offline expert model, we integrate the outputs of two expert models by an energy indicator (detailed in
Sec. 4.2) to obtain the prediction for an inference sample from any learned task. This expert ensemble strategy helps our framework to achieve a more robust performance com-pared to inference using one of the expert models alone.
The contributions of this paper are summarized as follows:
• We thoroughly investigate the novel Continual Learn-ing paradigm that constantly adapts a pre-trained tasks using general Parameter-model
Efficient Tuning (PET) methods, and propose a unified
Learning-Accumulation-Ensemble (LAE) framework. to novel
• Our LAE framework reshapes a given PET method into a competitive Memory-Free Continual Learning approach with three novel designs: Learning with cal-ibrated speed, Accumulation of multi-task knowledge, and Ensemble of two expert models constructed with online and offline PET modules.
• We conduct extensive experiments on CIFAR100 and ImageNet-R benchmarks, on all of which, our
LAE consistently achieves superior incremental per-formance than previous state-of-the-art approaches. 2.