Abstract
Visual localization is the task of estimating a 6-DoF camera pose of a query image within a provided 3D ref-erence map. Thanks to recent advances in various 3D sen-sors, 3D point clouds are becoming a more accurate and af-fordable option for building the reference map, but research to match the points of 3D point clouds with pixels in 2D images for visual localization remains challenging. Exist-ing approaches that jointly learn 2D-3D feature matching suffer from low inliers due to representational differences between the two modalities, and the methods that bypass this problem into classification have an issue of poor refine-ment. In this work, we propose EP2P-Loc, a novel large-scale visual localization method that mitigates such appear-ance discrepancy and enables end-to-end training for pose estimation. To increase the number of inliers, we propose a simple algorithm to remove invisible 3D points in the image, and find all 2D-3D correspondences without keypoint de-tection. To reduce memory usage and search complexity, we take a coarse-to-fine approach where we extract patch-level features from 2D images, then perform 2D patch classifica-tion on each 3D point, and obtain the exact corresponding 2D pixel coordinates through positional encoding. Finally, for the first time in this task, we employ a differentiable PnP for end-to-end training. In the experiments on newly cu-rated large-scale indoor and outdoor benchmarks based on 2D-3D-S and KITTI, we show that our method achieves the state-of-the-art performance compared to existing visual lo-calization and image-to-point cloud registration methods. 1.

Introduction
Visual localization [2, 5, 15, 22, 23, 42, 51] aims at esti-mating the precise 6-degree-of-freedom (DoF) camera pose (i.e. position and orientation) of a given 2D image within a 3D reference map. Since it can position the pose of a photo-taker in an environment without localization infor-Figure 1: Visual localization of EP2P-Loc. For a query im-age, we (1) retrieve top-4 point cloud submaps from the database, (2) perform 2D patch classification for each 3D point in the retrieved submaps, (3) find precise 2D pixel co-ordinates using a positional encoding, and (4) estimate the camera pose of the image using a learnable PnP solver [11]. mation such as GPS, it has a variety of applications includ-ing autonomous driving, robot navigation, and augmented reality. Previous works use the Perspective-n-Point (PnP) algorithm [17] to compute the camera pose of a query im-age based on the coordinates in the 3D reference map. The 3D reference map is typically built from a collection of images using Structure-from-Motion (SfM) reconstruction
[1, 24, 36, 44, 47], and the associated keypoints and descrip-tors are stored with the map in the form of point clouds to establish 2D-3D correspondences. Therefore, the accuracy of visual localization is highly dependent on the quality of the 3D reference map, and there are multiple challenges, in-cluding varying lighting, weather and seasonal conditions in outdoor environments, and repetitive patterns and insuf-ficient textures in indoor environments.
With the recent advance in 3D sensors, such as RGB-D scanners and LiDARs, it is possible to generate large-scale 3D point cloud maps with no help from SfM. This could be a more practical approach compared to SfM since the point clouds are denser and more accurate. However, visual localization has difficulty in matching descriptors from 2D images with those from 3D reference point clouds due to the big discrepancy between 2D and 3D representations. This is because 2D images are represented in the form of a grid of colors and texture, while 3D point clouds are relatively sparse and lack texture information due to empty spaces in the 3D space.
To tackle this problem, several recent studies have tried to match 2D pixels of an image with 3D points of a point cloud. 2D3D-MatchNet [16] is a pioneer work that pro-poses a deep neural network to embed the descriptors of 2D and 3D keypoints into a shared feature space. However, it still suffers from a low inlier problem, which means that the number of predicted 2D-3D correspondences used for pose estimation is not enough, since it is difficult to mitigate the severe appearance differences between the two modal-ities. P2-Net [53] detects keypoints and per-point descrip-tors based on the RGB-D scan datasets [20, 49], but every 2D pixel must have a corresponding 3D point. Moreover, obtaining features from all 2D pixels and 3D points can be inefficient in terms of memory usage and the amount of computation for the visual localization in large-scale envi-ronments. DeepI2P [29] replaces descriptor matching with classification to reduce computation while increasing accu-racy. However, it is limited for large-scale visual localiza-tion as will be shown in our experiments later, since it is challenging to find exact 2D-3D correspondences from the grid classification of each 3D point.
In this work, we propose EP2P-Loc, a novel approach to large-scale visual localization that prevents the low inlier problem by processing all 2D pixels in the query image and 3D points in the 3D reference maps, while reducing mem-ory usage and search space. Figure 1 outlines the pipeline of our method at inference. Initially, the 3D point cloud ref-erence map is divided into a set of submaps for effective candidate selection. Like a coarse-to-fine approach, we re-trieve relevant 3D point cloud submaps from the database for a query image. We then perform 2D patch classification to determine to which patch of the image the 3D points in the retrieved submaps belong. Next, we find the exact 2D pixel coordinates of the 3D points using positional encod-ing, instead of storing features for all 2D pixels. Finally, the obtained 2D-3D correspondences pass into the PnP layer to estimate the camera pose of the image.
The key novelty of our approach lies in its ability to ef-fectively learn features of both 2D pixels and 3D points while not only handling invisible 3D points in 2D images using our Invisible 3D Point Removal (IPR) algorithm but also finding all 2D-3D correspondences without the need for keypoint detection in a coarse-to-fine manner, resulting in a higher number of inliers. Moreover, for the first time in this task, we adopt an end-to-end learnable PnP solver [11] for high-quality 6-DoF pose estimation. We demonstrate that this approach is more accurate and efficient than se-lecting top k pairs and manually putting them into the PnP solver. For empirical validation in large-scale indoor and outdoor environments, we establish benchmarks based on the open-source Stanford 2D-3D-Semantic (2D-3D-S) [3] and KITTI [19] datasets, and show the state-of-the-art per-formance compared to existing image-based visual localiza-tion methods [2, 7, 14, 15, 22, 23, 42, 43] and image-to-point cloud methods [16, 29]. Our method uses only the coordi-nate values of the 3D point cloud and assumes that every 3D point is not associated with a pixel in the 2D image and vice versa. Through experiments, we also demonstrate that our method is applicable to various 3D global point cloud maps generated in different ways, including 3D Li-DAR data, RGB-D scans, and SfMs. 2.