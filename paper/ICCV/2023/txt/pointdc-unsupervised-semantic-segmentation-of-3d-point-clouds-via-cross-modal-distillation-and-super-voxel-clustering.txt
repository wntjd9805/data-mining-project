Abstract
Semantic segmentation of point clouds usually requires exhausting efforts of human annotations, hence it attracts wide attention to the challenging topic of learning from unlabeled or weaker forms of annotations. In this paper, we take the first attempt for fully unsupervised semantic segmentation of point clouds, which aims to delineate se-mantically meaningful objects without any form of annota-tions. Previous works of unsupervised pipeline on 2D im-ages fails in this task of point clouds, due to: 1) Clustering
Ambiguity caused by limited magnitude of data and imbal-anced class distribution; 2) Irregularity Ambiguity caused by the irregular sparsity of point cloud. Therefore, we pro-pose a novel framework, PointDC, which is comprised of two steps that handle the aforementioned problems respec-tively: Cross-Modal Distillation (CMD) and Super-Voxel
Clustering (SVC). In the first stage of CMD, multi-view vi-sual features are back-projected to the 3D space and ag-gregated to a unified point feature to distill the training of the point representation. In the second stage of SVC, the point features are aggregated to super-voxels and then fed to the iterative clustering process for excavating semantic classes. PointDC1 yields a significant improvement over the prior state-of-the-art unsupervised methods, on both the
ScanNet-v2 (+18.4 mIoU) and S3DIS (+11.5 mIoU) seman-tic segmentation benchmarks. 1.

Introduction
Semantic segmentation of 3D point cloud is a crucial problem that assigns each individual point to a known on-tology. The segmentation models can delineate objects at
*These authors contributes equally.
†Corresponding author. 1The code
SCUT-BIP-Lab/PointDC. released is at: https://github.com/
Figure 1. From unannotated point clouds, we would like a seg-mentation system to discover the semantic concepts automatically without any supervision. a fine granularity of object boundary, which is helpful for multiple downstream applications, such as robotic naviga-tion, autonomous vehicles, and scene parsing. Despite the immense progress of fully-supervised schemes in 3D se-mantic segmentation, the success crucially relies on large-scale datasets and annotations. Unfortunately, it requires exhausting efforts to conduct semantic-level per-point an-notations (e.g. ≈ 22.3 minutes per indoor scene for annota-tion [7]).
To reduce the efforts on the tedious process of seman-tic annotations, several works were proposed to create 3D semantic segmentation systems that can be trained from weaker forms of annotations, including projected 2D im-ages [32], subcloud-level [34], segment-level [28], and point-level annotations [23, 35, 14]. However, few works attempt to handle the great challenge of 3D semantic seg-mentation without any form of human annotations or mo-tion cues. In this paper, we aim to build an unsupervised 3D semantic segmentation framework that can automatically excavate meaningful semantic features from point clouds, as shown in Fig. 1.
Some efforts of unsupervised semantic segmentation
have been witnessed in the research field of 2D visual im-ages.
Independent Information Clustering (IIC) [16] and
PiCIE [5] try to excavate semantically meaningful features through self-supervised learning based on transformation invariance and equivariance, meantime conducting a clus-tering process to optimize the compactness of the learned semantic clusters. STEGO [12] further distills from a self-supervisedly pretrained Transformer to form compact se-mantic clusters of the corpora. Whereas the existing line of unsupervised clustering pipeline could not be simply mi-grated to 3D point clouds for the following reasons: 1) Clustering Ambiguity: For a 2D unsupervised sys-tem, the major premise is that the pictures are meaningful images collected by human (rather than meaningless case like an empty image with a single color). The nature prior during the collection of human enables the effectiveness of large-scale unsupervised clustering on 2D images as long as the dataset is large enough. However, for a 3D unsu-pervised system, on the one hand, the huge cost for data collection limits the magnitude and diversity of the dataset; on the other hand, the imbalanced occupancy of 3D space aggravates the long-tail distribution effect among different classes when clustering among points, resulting in the igno-rance of classes with fewer points. 2) Irregularity Ambiguity: Without a regular grid-like structure, point clouds might have variations in the density of local areas. During the calculation of clusters, the points from dense areas inherently weigh more than the points from sparse areas. As a result, the original K-means cluster-ing in [16, 5] might overly focus on the dense regions and ignores the sparse regions.
In this paper, we take the first attempt for unsuper-vised 3D semantic segmentation and introduce PointDC (Point cloud cross-modal Distillation and Super-Voxel
Clustering), which is capable of discovering and segment-ing semantic objects from point clouds without any human annotations. Directing to handle the aforementioned prob-lems of Clustering Ambiguity and Irregularity Ambiguity, we respectively adopt Cross-Modal Distillation (CMD) and
Super-Voxel Clustering (SVC) in our PointDC framework. 1) To handle the former problem, CMD could integrate the multi-view visual clues to distill the corresponding point feature in the point cloud. As cognitive scientists [8, 27] argue, humans are proficient at mapping the visual con-cepts learned from 2D images to understand the 3D world.
We obtain multi-view images by observing 3D point clouds from different viewpoints first and feed them to a self-supervisedly pretrained visual model such as DINO [4] to extract unsupervised visual features. By back-projecting the multi-view visual features into the corresponding points in 3D space, we can aggregate the features from different views to formulate a unified multi-view representation, and distill the learning of point representation. The involvement of multi-view visual cues can effectively diminish the ambi-guity during clustering among points, and provide a coarse understanding of 3D semantic features. 2) To handle the latter problem, instead of clustering on the original point space, SVC rasterizes the 3D space into super-voxels and assigns each point to the corresponding super-voxel. The features of points in the same voxel are aggregated together via Super-Voxel Pooling for a unified permutation-invariant representation. During each iteration phase of clustering process, we first assign the point fea-tures to the super-voxels and then cluster among these vox-els. Afterward, the feature of each super-voxel is assigned back to the occupying points, and assume these points in the local super-voxel share a common semantic feature.
The overall pipeline of our PointDC framework includes 2 steps: 1) CMD is utilized to distill the learning of point representation first; 2) Then SVC is applied iteratively to optimize the clustered semantic representation on point clouds. For evaluation, we conduct extensive experiments on the challenging ScanNet-v2 [7] and S3DIS [2]. Com-pared with state-of-the-art on existing unsupervised meth-ods for point cloud semantic segmentation, our PointDC achieves an improvement on both the ScanNet-v2 (+18.4 mIOU) and S3DIS (+11.5 mIOU).
In summary, our contribution is threefold.
• We take the first attempt for unsupervised 3D semantic segmentation without any kinds of human annotations.
• We propose PointDC, a novel framework for unsuper-vised 3D semantic segmentation.
It is comprised of 2 steps: 1) Cross-Modal Distillation that distills multi-view visual features to the point-based representations; 2) Super-Voxel Clustering that regularizes the point features with voxelized representation through super-voxel pooling, and iteratively clusters to optimize the semantic features on point clouds.
• The proposed method achieves superior improve-ment compared with existing unsupervised methods for 3D semantic segmentation on various challenging datasets, demonstrating its effectiveness. 2.