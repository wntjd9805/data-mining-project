Abstract
We study the problem of future step anticipation in proce-dural videos. Given a video of an ongoing procedural activ-ity, we predict a plausible next procedure step described in rich natural language. While most previous work focuses on the problem of data scarcity in procedural video datasets, another core challenge of future anticipation is how to ac-count for multiple plausible future realizations in natural settings. This problem has been largely overlooked in pre-vious work. To address this challenge, we frame future step prediction as modelling the distribution of all possible can-didates for the next step. Speciﬁcally, we design a gener-ative model that takes a series of video clips as input, and generates multiple plausible and diverse candidates (in nat-ural language) for the next step. Following previous work, we side-step the video annotation scarcity by pretraining our model on a large text-based corpus of procedural ac-tivities, and then transfer the model to the video domain.
Our experiments, both in textual and video domains, show that our model captures diversity in the next step prediction and generates multiple plausible future predictions. More-over, our model establishes new state-of-the-art results on
YouCookII, where it outperforms existing baselines on the next step anticipation. Finally, we also show that our model can successfully transfer from text to the video domain zero-shot, i.e., without ﬁne-tuning or adaptation, and produces good-quality future step predictions from video. 1.

Introduction
Anticipating future steps while performing a task is a natural human behaviour necessary to successfully accom-plish a task and cooperate with other humans. Thus, it is important for a smart AI agent to exhibit this behaviour too, in order to assist humans in performing procedural tasks
*Equal Contribution
Figure 1. Summary of the proposed GEPSAN model. Our model, given an initial video stream representing a sequence of past procedural steps, predicts multiple feasible alternatives for the next step in natural language. We ﬁrst train our model on text-only data, followed by zero-shot transfer to the video domain. (e.g., cooking, assembling furniture or setting up an elec-tronic device). For example, consider a cooking AI assistant that observes a user as they cook a dish. To be useful, this assistant needs to anticipate possible next steps in order to provide timely support with ingredients, cooking tools and actions. Anticipating future steps from a video stream is a challenging task, where simply recognizing the current ac-tion or objects is not sufﬁcient. To anticipate future actions, one needs to parse and recognize the human-object inter-actions from an unstructured and cluttered environment in the current frame, predict the possible task being performed (possibly leveraging the past observations) and ﬁnally antic-ipate plausible next steps. Given the importance and chal-lenges associated with this task, several research efforts tar-geted this application in the recent years [24, 25, 9, 37].
We follow recent work [24] and tackle future anticipa-tion in the realm of cooking activities. Given visual obser-vations of the ﬁrst t steps, our task is to predict the next step to be executed. This task entails recognizing the cur-rent step and the recipe being made, which is particularly challenging given the modest size of cooking video datasets
with annotations. Fortunately, such instructional knowledge is available in abundance in the text domain (think of all the dish recipes online) and can be leveraged to help video prediction. Prior work [24, 32] builds on this observation and proposes to ﬁrst pretrain the anticipation model on a large corpus of text-based recipes, i.e., Recipe1M+ [17] to acquire knowledge about the recipe domain, and then
ﬁne-tune the model on visual data. This line of work ef-fectively alleviates the video annotation problem, however, these works only predict a single future realization, and thus it does not take into account all the variability present in the recipes. For example, given the task of making a salad, and assuming the ﬁrst three observed step are: Chop Vegetable,
Add Tomatoes, Add Cucumber, the plausible next step can be: Add Olive Oil or Add Salt and Pepper (for those who like more seasoning) or simply Serve. This simple exam-ple highlights that the task’s output is, in fact, multi-modal.
This observation suggests that a good future step anticipa-tion model must be able to predict diverse and plausible future realizations. Moreover, it is known that in a multi-modal setup using a model that outputs a single prediction (as done by previous work [24, 32]) may harm the perfor-mance [35] even further by producing unrealistic samples that “fall between” the true modes.
In this work, we embrace the uncertainty inherent in the task of future anticipation and propose to learn a Generative
Procedure Step Anticipation model (GEPSAN), that cap-tures the distribution of possible next steps and allows to sample multiple plausible outputs given an initial observa-tion from video input. A summary of our proposed work is depicted in Figure 1. To achieve this goal, we design a model that consists of three modules: 1) a modality en-coder, that ingests a sequence of previous instruction step observations (in either text or video format), 2) a generative recipe encoder, that, given the observation history, proposes the next plausible instruction vector, and 3) an instruction decoder, that transforms the next step prediction (given by the recipe encoder) into rich natural language. The core component of the model is the generative recipe encoder; it combines the beneﬁts of the transformer model [28] (to process long input sequences) and Conditional Variational
AutoEncoder (CVAE) (to capture the uncertainty inherent to the task) and can produce multiple plausible alternatives for the next step in a procedure. Another key element of the pipeline is the input encoder; in contrast to the previ-ous works that learn it from scratch, we adapt a pretrained video-language feature extractor [16] to serve as our en-coder. Since the encoder has been trained to map video and text into a common embedding space, our model, trained only on the recipe text corpus, can generalize to future step anticipation from video zero-shot, without any ﬁnetuning.
Contributions. Our contributions are twofold:
• We propose GEPSAN, a new generative model for fu-ture step anticipation that captures the uncertainty in-herent in the task of next step prediction.
• We show that GEPSAN, only trained using text recipes, can generalize to video step anticipation zero-shot, without ﬁnetuning or adaptation.
Thanks to that, we achieve state-of-the-art results in next step anticipation from video on YouCookII [40] and show that our model can generate diverse plausible next steps, outperforming the baselines in modelling the distribution of next steps.1 2.