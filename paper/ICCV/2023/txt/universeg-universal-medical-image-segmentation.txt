Abstract
While deep learning models have become the predomi-nant method for medical image segmentation, they are typ-ically not capable of generalizing to unseen segmentation tasks involving new anatomies, image modalities, or labels.
Given a new segmentation task, researchers generally have to train or fine-tune models. This is time-consuming and poses a substantial barrier for clinical researchers, who of-ten lack the resources and expertise to train neural networks.
We present UniverSeg, a method for solving unseen med-ical segmentation tasks without additional training. Given a query image and an example set of image-label pairs that define a new segmentation task, UniverSeg employs a new
CrossBlock mechanism to produce accurate segmentation maps without additional training. To achieve generalization to new tasks, we have gathered and standardized a collec-tion of 53 open-access medical segmentation datasets with over 22,000 scans, which we refer to as MegaMedical. We used this collection to train UniverSeg on a diverse set of anatomies and imaging modalities. We demonstrate that Uni-*Denotes equal contribution verSeg substantially outperforms several related methods on unseen tasks, and thoroughly analyze and draw insights about important aspects of the proposed system. The Uni-verSeg source code and model weights are freely available at https://universeg.csail.mit.edu 1.

Introduction
Image segmentation is a widely studied problem in com-puter vision and a central challenge in medical image analy-sis. Medical segmentation tasks can involve diverse imaging modalities, such as magnetic resonance imaging (MRI), X-ray, computerized tomography (CT), and microscopy; differ-ent biomedical domains, such as the abdomen, chest, brain, retina, or individual cells; and different labels within a re-gion, such as heart valves or chambers (Figure 1). This di-versity has inspired a wide array of segmentation tools, each usually tackling one task or a small set of closely related tasks [17, 23, 41, 42, 87, 94]. In recent years, deep-learning models have become the predominant strategy for medical image segmentation [45, 74, 87].
A key problem in image segmentation is domain shift,
Figure 1: Medical segmentation involves many imaging types, biomedical domains, and target labels. We employ a large diverse set of training tasks (blue) to build a model that can segment unseen tasks (orange) without additional training.
Figure 2: Workflow for inference on a new task, from an unseen dataset. Given a new task, traditional models (left) are trained before making predictions. UniverSeg (right) employs a single trained model which can make predictions for images (queries) from the new task with a few labeled examples as input (support set), without additional fine-tuning. where models often perform poorly given out-of-distribution examples. This is especially problematic in the medical domain where clinical researchers or other scientists are con-stantly defining new segmentation tasks driven by evolving populations, and scientific and clinical goals. To solve these problems they need to either train models from scratch or fine-tune existing models. Unfortunately, training neural net-works requires machine learning expertise, computational resources, and human labor. This is infeasible for most clinical researchers or other scientists, who do not possess the expertise or resources to train models. In practice, this substantially slows scientific development. We, therefore, focus on avoiding the need to do any training given a new segmentation task.
Fine-tuning models trained on the natural image domain can be unhelpful in the medical domain [86], likely due to the differences in data sizes, features, and task specifications between domains, and importantly still requires substan-tial retraining. Some few-shot semantic segmentation ap-proaches attempt to predict novel classes without fine-tuning in limited data regimes, but mostly focus on classification tasks, or segmentation of new classes within the same input domain, and do not generalize across anatomies or imaging modalities.
In this paper, we present UniverSeg – an approach to learning a single general medical-image segmentation model that performs well on a variety of tasks without any retrain-ing, including tasks that are substantially different from those seen at training time. UniverSeg learns how to exploit an input set of labeled examples that specify the segmentation task, to segment a new biomedical image in one forward pass. We make the following contributions. information from the example set to the new image.
• We demonstrate that UniverSeg substantially outper-forms several models across diverse held-out segmen-tation tasks involving unseen anatomies and even ap-proaches the performance of fully-supervised networks trained specifically for those tasks.
• In extensive analysis, we show that the generalization capabilities of UniverSeg are linked to task diversity during training and image diversity during inference. 2.