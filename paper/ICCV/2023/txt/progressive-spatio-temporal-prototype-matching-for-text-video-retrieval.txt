Abstract
The performance of text-video retrieval has been sig-nificantly improved by vision-language cross-modal learn-ing schemes. The typical solution is to directly align the global video-level and sentence-level features during learn-ing, which would ignore the intrinsic video-text relations, i.e., a text description only corresponds to a spatio-temporal part of videos. Hence, the matching process should con-sider both fine-grained spatial content and various tempo-ral semantic events. To this end, we propose a text-video learning framework with progressive spatio-temporal pro-totype matching. Specifically, the matching process is de-composed into two complementary phases: object-phrase prototype matching and event-sentence prototype matching.
In the object-phrase prototype matching phase, the spa-tial prototype generation mechanism predicts key patches or words, which are aggregated into object or phrase proto-types. Importantly, optimizing the local alignment between object-phrase prototypes helps the model perceive spatial details.
In the event-sentence prototype matching phase, we design a temporal prototype generation mechanism to associate intra-frame objects and interact inter-frame tem-poral relations. Such progressively generated event proto-types can reveal semantic diversity in videos for dynamic matching. Validated by comprehensive experiments, our method consistently outperforms the state-of-the-art meth-ods on four video retrieval benchmark.1 1.

Introduction
Understanding multimodal information [13, 37, 40, 76, 6, 79, 41, 12, 53, 54] is an essential way for humans to perceive the world. As a fundamental task in multimodal learning [25, 62, 60], Text-Video Retrieval (TVR) [22, 70, 68, 24] has garnered huge interest with the rapid devel-*Interns at DAMO Academy, Alibaba Group
â€ Corresponding author 1Code is available at https://github.com/IMCCretrieval/ProST.
Figure 1. Visualization of event and object prototypes learned by 1) adaptive event matching w/o object-phrase prototypes and 2) object-phrase prototype matching. Object prototypes can focus on local patches (e.g., tail) and complement event-level matching. opment of short video platforms. TVR [45, 21] aims to search semantically relevant videos based on user-entered text queries. Unfortunately, the inherent modality gap phe-nomenon [42, 61, 38, 3, 67] increases the difficulty of asso-ciating multimodal data. Towards such a concern, pioneer-ing works [47, 65, 73] usually exploit multiple unimodal pre-trained models to extract features, and then use met-ric learning strategies [14] to strengthen the modality align-ment in the joint space. However, there are large differences in the initial distributions of multiple unimodal offline fea-tures, which inevitably brings the feature fusion challenge to affect the retrieval results.
Recently, encouraged by the success of vision-language pre-training [50, 66, 34, 27, 72, 35, 4], a series of canonical works [51, 20, 44, 16] are proposed by transferring knowl-matching stage, and then perform temporal aggregation.
To this end, we propose a novel Progressive Spatio-Temporal Prototype Matching (ProST) framework, which decomposes the matching process into complementary object-phrase and event-sentence prototype alignments. In the object-phrase prototype matching phase, a spatial pro-totype generation mechanism is first developed to focus on key image patches or word tokens. To prevent massive video redundancy, we use sparse weights to filter irrelevant background interference and aggregate important patch or word tokens into object or phrase prototypes. As shown in
Fig. 1(2), optimal local responses can be found by the max-imum similarity between object and phrase prototypes to help the model perceive spatial details.
In the event-sentence prototype matching phase, a tem-poral prototype generation mechanism is designed to de-code various event prototypes for dynamic semantic align-ment. Specifically, since the spatial details should not be ignored in the event generation process, we send object pro-totypes to the frame and event decoder for progressive pro-totype learning. Multiple different event prototypes are gen-erated to reveal the semantic diversity in videos and support dynamic matching. Finally, as shown in Fig. 2, different text sentences can match the most appropriate event prototype learned from the video. Overall, ProST encodes the rich video content by generating object-level, frame-level and event-level prototypes in a progressive manner, and con-structs dynamic and fine-grained spatio-temporal matching strategies. ProST not only strengthens the interaction of lo-cal spatial content, but also considers the intrinsic relations where video and text are only partially aligned.
The contributions of this paper are threefold: 1) we pro-pose a novel framework, ProST, to decompose the match-ing process into complementary object-phrase and event-sentence prototype alignments; 2) two prototype generation mechanisms are developed to learn sparse spatial and dy-namic temporal information respectively, which can fully explore fine-grained local details and video semantic diver-sity; 3) extensive experiments demonstrate that ProST out-performs state-of-the-art methods on MSRVTT, DiDeMo,
VATEX and LSMDC datasets. 2.