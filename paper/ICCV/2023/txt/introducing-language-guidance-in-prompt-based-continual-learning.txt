Abstract
Continual Learning aims to learn a single model on a sequence of tasks without having access to data from pre-vious tasks. The biggest challenge in the domain still re-mains catastrophic forgetting: a loss in performance on seen classes of earlier tasks. Some existing methods rely on an expensive replay buffer to store a chunk of data from previous tasks. This, while promising, becomes expensive when the number of tasks becomes large or data can not be stored for privacy reasons. As an alternative, prompt-based methods have been proposed that store the task information in a learnable prompt pool. This prompt pool instructs a frozen image encoder on how to solve each task. While the model faces a disjoint set of classes in each task in this set-ting, we argue that these classes can be encoded to the same embedding space of a pre-trained language encoder. In this work, we propose Language Guidance for Prompt-based
Continual Learning (LGCL) as a plug-in for prompt-based methods. LGCL is model agnostic and introduces language guidance at the task level in the prompt pool and at the class level on the output feature of the vision encoder. We show with extensive experimentation that LGCL consistently im-proves the performance of prompt-based continual learning methods to set a new state-of-the art. LGCL achieves these performance improvements without needing any additional learnable parameters. 1.

Introduction
In Class Incremental Continual Learning, we task a model to learn a sequence of non-overlapping tasks consisting of new classes being introduced at each task. This presents a challenge different from the common supervised learning setting as the data distribution is continuously changing, and the independent and identically distributed (i.i.d.) data as-sumption does not hold. As a result, a model trained with our usual training recipe of optimising a loss function on incom-ing data leads to catastrophic forgetting [33] i.e., the model forgets the previously seen classes since the loss only incen-tivises performance on the current task. There have been
Figure 1: In Computer Vision, Continual Learning in the class incremental setting aims to learn a single model on a sequence of tasks where each task consists of disjoint classes. While each task represents a disjoint set of classes, we argue that they can be mapped to the same semantic space of a pretrained language encoder. Based on this principle, we propose to introduce language guidance in a continual learner to mitigate catastrophic forgetting. several attempts to address this challenge. One popular line of works aims to identify model parameters most important for performance on each task and prevent them from chang-ing too much through subsequent tasks [19, 65, 25, 1]. These regularization-based methods, however, achieve sub-optimal performance as we move to very complex tasks where the model needs to share parameters between different tasks to learn a robust representation.
Another line of work takes a very simple but effective ap-proach of storing a chunk of training data. Rehearsal-based methods [7, 8, 12] maintain a rehearsal buffer which is a finite set of training data stored across each task. The key in-tuition is that to prevent forgetting on previously seen classes, the model simply uses the examples in the rehearsal buffer when optimising for new tasks. However, these methods require large buffer sizes as the number of tasks increases
and hence become expensive. Moreover, they have been criticised for being impractical in real-world settings where privacy concerns prevent storing data. Architecture-based methods [49, 64, 23, 28, 46, 66] take an orthogonal approach where these works reserve specialised parts of the network for each task and take an approach similar to multi-task learning. However, this can bring a significant increase in the number of learnable parameters. Moreover, this requires knowing the task identity at test time to select the relevant network module for each task which is not a realistic setting.
Recently, Learning to Prompt (L2P)[59] has proposed an exciting new direction for continual learning. Instead of learning the parameters of the model for each new task, the authors propose to use a pre-trained vision encoder and learn the prompts that can instruct this pre-trained model to solve new tasks. This technique is called Prompt Tun-ing and is popularized by its success in Natural Language
Processing (NLP). A learned prompt instructs the model on how to solve a new task using the wide set of knowledge it has stored during pre-training. These methods have shown incredible performance boosts at a fraction of learnable pa-rameters. L2P initialises a learnable prompt pool where each prompt is attached with a learnable key. The authors propose to use the CLS feature from the pre-trained vision encoder to perform a lookup with this learnable pool. The selected prompts are then appended with the patchwise embeddings of the image into the pre-trained model, and the output rep-resentations of the selected prompt tokens are used to learn a linear classification layer. This surprisingly simple formu-lation has brought impressive performance gains without the need to store any data in a rehearsal buffer.
While the sequence of data from each task in contin-ual learning has a changing distribution, we argue that the classes of each task can be mapped to the same semantic space. In this work, we argue that language represents such a robust representation space where all tasks can be suffi-ciently mapped to. Hence if we encode the features of the continual learner to map to a semantic space of language, this can present an avenue to mitigate catastrophic forgetting and result in a more robust continual learner. We use this insight to develop a novel method Language Guidance for
Prompt-based Continual Learning (LGCL) that can in-troduce language guidance into any prompt-based continual learning method. We achieve this by introducing language guidance at two levels. First, we introduce the task-level language guidance by incentivising the model to map the learnable keys of the prompt pool into a shared language representation of all classes in the task. Secondly, we in-centivise the model to map the output features of the visual encoder after prompting it to align with the language repre-sentation of its respective class. The model learns a robust representation of all tasks by aligning these representations with a pre-trained semantic space of language.
Our contributions are as follows: 1) We present a novel perspective that entails introducing language guidance in continual learning to mitigate catastrophic forgetting. 2) We propose Language Guidance for Prompt-based Continual
Learning (LGCL), a novel method that introduces language guidance in prompt-based continual learning methods. 3)
Without any additional learnable parameters or extra memory requirements at inference, LGCL improves the performance of prompt-based continual learning methods and achieves state-of-the-art performance on two challenging continual learning benchmarks. 2.