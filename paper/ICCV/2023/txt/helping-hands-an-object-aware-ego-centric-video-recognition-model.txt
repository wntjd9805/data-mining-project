Abstract
We introduce an object-aware decoder for improving the performance of spatio-temporal representations on ego-centric videos. The key idea is to enhance object-awareness during training by tasking the model to predict hand posi-tions, object positions, and the semantic label of the objects using paired captions when available. At inference time the model only requires RGB frames as inputs, and is able to track and ground objects (although it has not been trained explicitly for this).
We demonstrate the performance of the object-aware representations learnt by our model, by: (i) evaluating it for strong transfer, i.e. through zero-shot testing, on a number of downstream video-text retrieval and classification bench-marks; and (ii) by using the representations learned as in-put for long-term video understanding tasks (e.g. Episodic
Memory in Ego4D). In all cases the performance improves over the state of the art—even compared to networks trained with far larger batch sizes. We also show that by using noisy image-level detection as pseudo-labels in training, the model learns to provide better bounding boxes using video consistency, as well as grounding the words in the associ-ated text descriptions.
Overall, we show that the model can act as a drop-in replacement for an ego-centric video model to improve per-formance through visual-text grounding1. 1.

Introduction
In visual-language models there has been a recent move to explicitly build object awareness into the vision mod-ule by adding specialized and bespoke components, or us-ing entirely object-centric architectures. The motivation for this partly comes from the attractive compositional na-ture of objects and their inter-relationships in language, which enables inexhaustible novel combinations [10, 45], and partly from infant cognitive studies that stress the im-1Code available
Chuhanxx/helping_hand_for_egocentric_videos and models https://github.com/ at: portance of objects in early visual development [29, 56, 60].
Examples in the video domain include explicit internal ob-ject representations [2], e.g., through RoI-align [17] pooled features either from a pre-trained region-proposal network (RPN) [2, 52, 57, 62], or from bounding-box coordinates taken as input [19, 42, 48, 71]. This contrasts with the large body of work where standard representations are learnt end-to-end without any explicit factorization into ob-jects/entities, such as dual-encoder vision-language models in the image [21, 49] and video domains [4, 64].
In this paper, we take a different (middle) path and in-stead use a vanilla video transformer architecture and in-duce object-awareness into the video representation by task-ing the model to predict object-level properties, such as their localization and semantic categories, only during training.
Our target domain is ego-centric video [11, 16], and we tailor the object properties used to this. In ego-centric videos the actor [57] is often present through their hands, and we therefore task the network to predict both the hands and the principal objects they interact with. As will be seen, this simple object-aware training boosts the perfor-mance of pre-trained video-language architectures signifi-cantly, and leads to state-of-art performance across multi-ple ego-centric benchmarks. During inference, the model requires only RGB frames as input, and operates as a stan-dard video-language network.
In more detail, our model is built on top of a pre-trained video-language dual encoder architecture (where there are separate encoders for the video and text data). We add an additional, but vanilla, transformer decoder head [61], and train with DETR/Mask2former [7, 9] query vectors and ob-ject loss for hands and other objects. The intuition is that these additional query vectors help the model to attend to and track the hands and salient objects in the scene (these
Importantly, are the ‘helping hands’ of the paper title). we do not require dense frame level ground truth for this training. Rather, we obtain somewhat noisy and temporally sparse annotations automatically from a hand object detec-tor [53], and use these to provide prediction targets for the frames where they are available. This opportunistic use of annotations is pragmatic as object detectors trained on third-person datasets (such as COCO) do not perform so well on the ego-centric domain, where the scenes are more crowded and objects are often small and can be motion blurred. By only requiring annotations for a subset of frames, where they can be reliably produced automatically, we are able to train on large-scale data without requiring expensive manual supervision.
Although we train with noisy and sparse image-level ob-ject localization, our model can learn to predict better and denser bounding-box trajectories through large-scale train-ing due to the spatio-temporal consistency which naturally presents in videos. Also, it is able to predict semantic grounding by learning to map the object appearance to the nouns in the video captions.
It is worth noting that we are using hand detectors be-cause hands are a common and important object in ego-centric videos. However, the object-centric method we are proposing has greater scope than ego-centric videos and can be applied to other scenarios with other object types provid-ing the ‘helping-hand’.
In summary, we make the following contributions: (i) We propose a method to induce object-awareness in video-language models for an architecture composed of standard neural modules. The model only requires RGB frames as inputs, and thus is a drop-in replacement for any ego-centric video model. (ii) The model can be trained opportunistically using available and sparse frame-level and noisy annotations, pro-duced automatically. (iii) We demonstrate state-of-the-art strong (zero-shot) transfer to other ego-centric retrieval datasets namely, EpicKitchens-MIR and EGTEA improv-ing prior art by 2-4%. for cross-modal (iv) We evaluate the grounding quantitatively using the
EpicKitchens-VISOR dataset [11, 12] and find that the model outperforms the base hand-object detector used for training supervision. (v) Finally, we also demonstrate that the representations learned can be used as input in long-term video understand-ing tasks like EgoNLQ and EgoMQ. The objectiveness in the representation helps the model outperform other models trained on the same training set on these two tasks. 2.