Abstract
Siamese network has been a de facto benchmark framework for 3D LiDAR object tracking with a shared-parametric encoder extracting features from template and search region, respectively. This paradigm relies heav-ily on an additional matching network to model the cross-correlation/similarity of the template and search region. In this paper, we forsake the conventional Siamese paradigm and propose a novel single-branch framework, SyncTrack, synchronizing the feature extracting and matching to avoid forwarding encoder twice for template and search region as well as introducing extra parameters of matching network.
The synchronization mechanism is based on the dynamic affinity of the Transformer, and an in-depth analysis of the relevance is provided theoretically. Moreover, based on the synchronization, we introduce a novel Attentive Points-Sampling strategy into the Transformer layers (APST), re-placing the random/Farthest Points Sampling (FPS) method with sampling under the supervision of attentive relations between the template and search region. It implies connect-ing point-wise sampling with the feature learning, benefi-cial to aggregating more distinctive and geometric features for tracking with sparse points. Extensive experiments on two benchmark datasets (KITTI and NuScenes) show that
SyncTrack achieves state-of-the-art performance in real-time tracking. 1.

Introduction
With recent advances in autonomous driving, 3D vision tasks based on LiDAR are becoming increasingly popular in the visual community. Among these tasks, 3D LiDAR single object tracking (SOT) aims to track a specific target object in a 3D video with the knowledge of 3D bounding
*Equal Contribution
†Corresponding Author
Figure 1. The comparison of (a) Siamese network based trackers, (b) previous single-branch, two-stage framework M2Track [51] with (c) our SyncTrack, which is a single-branch and single-stage framework. box in the initial frame. This task meets numerous chal-lenges, such as LiDAR point cloud sparsity, occlusions, and fast motions.
Most existing methods [32, 50, 19, 12, 33, 52, 20, 9, 39] of 3D SOT mainly adopt a Siamese-like backbone and in-corporate an additional matching network to cope with the tracking challenges as shown in Fig. 1(a). Trackers based on the Siamese-like backbone separate feature extraction of template and search region, forwarding the two kinds of features with shared model parameters, respectively. Sub-sequently, an extra matching network is introduced to fuse
the extracted template and search region features to model the correlation or similarity between them. However, such a paradigm restricts the feature interaction to a post-matching network, correlating the template and search region insuf-ficiently with merely the high-level extracted features. In other words, the matching process posterior to the encoder is incapable of modeling the relations of multi-scale fea-tures intra-backbone. Moreover, a standalone matching net-work results in extra model parameters and computational overheads, let alone the double forwarding process of the
Siamese-backbone to extract the template and search re-gion features. M2Track [51] proposed a motion-centric paradigm to replace the Siamese-like structure, constructing a spatial-temporal point cloud to predict the motion. How-ever, they still rely heavily on an additional motion transfor-mation network, which requires extra training input, to inte-grate the extracted template features into the search region, and another two-stage refinement network is leveraged to ensure the performance as illustrated in Fig. 1(b). Based on aforementioned problems, we ask the question: Can feature extracting and matching be conducted simultaneously in a simple way?
The answer is Yes and the solution is implied in the dy-namic global reasoning property of Transformer [35, 7, 10]. Specifically, the affinity matrix of all tokens can be constructed dynamically via continuous computation of the key and query vectors in the attention mechanism. The spa-tial context is aggregated using affinity to attend features.
Intuitively, this affinity matrix can intrinsically serve as the matching matrix for intermediate feature interactions be-tween the template and search region if we merge them into one input of the Transformer layers. Therefore, we pro-pose a single-branch and single-stage framework equipped with a Transformer-based backbone instead of the conven-tional Siamese-like PointNet++ [31] backbone, as shown in Fig 1(c). The framework is dubbed as SyncTrack, as the Transformer backbone synchronizes the feature extract-ing and matching process. The SyncTrack is composed of a simple backbone and prediction head, omitting the com-plex matching network design and motion state estimation, depending merely on point-wise features.
However, 3D point clouds have unique properties such as sparsity [54, 24], density variance, and implicit geomet-ric features in data locality [29]. For example, 51% samples of KITTI [11] Car category have less than 100 points [19].
These problems are further aggravated when point clouds are grouped and down-sampled to formulate multi-scale point-wise feature maps. The phenomena stresses the vital-ity of point clouds sampling, aiming to improve the point-wise perception efficiency with limited points. PTTR [52] proposed to sample the input point clouds before the back-bone, utilizing the L2 distance as the similarity metric for sampling. However, as down-sampling layer by layer is essential in tracking backbone for multi-scale feature fu-sion to strengthen representation, it is reasonable to con-sider the sampling strategy in the backbone. Therefore, we propose the attentive sampling strategy based on the atten-tion map between the template and search region, and equip each Transformer layer with our sampling module as shown in Fig. 2(b). We name the Transformer containing Atten-tive Points-Sampling as APST. Specifically, the attentive response from template tokens to search region tokens is considered, as the positively respond tokens are more likely to be in the foreground and should be preserved for feature extracting. By contrast, as Fig. 3 shows, random sampling easily falls into the perceptive confusion as selected points hardly contain geometric features due to randomness.
The main contributions of our paper can be summarized as follows:
• We introduce a single-branch and single-stage frame-work for real-time 3D LiDAR SOT dubbed SyncTrack, without Siamese-like forward propagation and a stan-dalone matching network. We ingeniously leverage the dynamic affinity characteristic of the self-attention mechanism to synchronize the feature extracting and matching. A detailed analysis is provided to explain the synchronizing mechanism.
• We propose a novel APST to build the backbone, replacing the random/FPS1 down-sampling of point clouds with attentive sampling to preserve more target-relevant points, and thus improving the perceptive ca-pability of feature extracting.
• Extensive results show that our method has achieved new state-of-the-art performance on the KITTI and
NuScenes datasets in real-time tracking, up to 2.8% and 1.4% on mean results with a high-speed of around 45 fps. Besides, SyncTrack exhibits good scalability in both width and depth. 2.