Abstract
Semantic occupancy perception is essential for au-tonomous driving, as automated vehicles require a fine-grained perception of the 3D urban structures. However, existing relevant benchmarks lack diversity in urban scenes, and they only evaluate front-view predictions. Towards a comprehensive benchmarking of surrounding perception al-gorithms, we propose OpenOccupancy, which is the first surrounding semantic occupancy perception benchmark. In the OpenOccupancy benchmark, we extend the large-scale nuScenes dataset with dense semantic occupancy annota-tions. Previous annotations rely on LiDAR points superim-position, where some occupancy labels are missed due to sparse LiDAR channels. To mitigate the problem, we intro-duce the Augmenting And Purifying (AAP) pipeline to ∼2× densify the annotations, where ∼4000 human hours are involved in the labeling process. Besides, camera-based,
LiDAR-based and multi-modal baselines are established for the OpenOccupancy benchmark. Furthermore, con-sidering the complexity of surrounding occupancy percep-tion lies in the computational burden of high-resolution 3D predictions, we propose the Cascade Occupancy Network
*These authors contributed equally to this work.
†Corresponding authors. zhengzhu@ieee.org, xingang.wang@ia.ac.cn
‡https://github.com/JeffWang987/OpenOccupancy (CONet) to refine the coarse prediction, which relatively enhances the performance by ∼30% than the baseline. We hope the OpenOccupancy benchmark ‡ will boost the devel-opment of surrounding occupancy perception algorithms. 1.

Introduction
Accurately perceiving 3D structures of different objects and regions in urban scenes is a fundamental requirement for safe driving, thus there are growing interests in seman-tic occupancy perception [1, 35, 10, 36, 41, 16, 8]. Unlike 3D detection [13, 6, 34, 3, 38] and LiDAR segmentation
[1, 38, 11] that are designed for foreground objects or sparse scanned points, the occupancy task targets at assigning se-mantic labels to every spatially-occupied region within the perceptive range. Therefore, semantic occupancy percep-tion is a promising and challenging research direction in autonomous-driving perception.
Despite growing interests in semantic occupancy percep-tion, most of the relevant benchmarks [35, 10, 36, 41, 16, 8] are devised for indoor scenes. SemanticKITTI [1] extends the occupancy perception to driving scenarios, but its dataset is relatively small in scale and limited in diversity, which hinders the generalization and evaluation of the developed occupancy perception algorithms.
Besides,
SemanticKITTI only evaluates the front-view occupancy
Type
Surround Modality
Vol. Size
#Scenes
#Frames
Annotation
NYUv2 [35]
ScanNet [8]
SceneNN [16]
SUNCG [36]
SynthCity [14]
SemanticPOSS [30]
SemanticKITTI [1]
Indoor
Indoor
Indoor
Synthtic
Synthtic
Outdoor
Outdoor nuScenes-Occupancy Outdoor
✗
✗
✗
✗
✗
✓
✗
✓
C&D
C&D
C&D
C&D
L
L
C&L
C&L (144 × 240 × 240) (31 × 62 × 62)
-(144 × 240 × 240)
--(32 × 256 × 256) (40 × 512 × 512) 1.4K 1.5K 100 46K 9
-22 850 1.4K 1.5K
-140K
-3K 44K
Human
Human
Human
Synthtic
Synthtic
Human
Auto&Human 200K1
Auto&Human
Table 1: Comparison between nuScenes-Occupancy and other dense LiDAR/occupancy perception datasets. Surround=✓ represents datasets that use surround-view inputs. C, D, L denote camera, depth and LiDAR. Vol. Size is the volumetric size. 1Note that nuScenes-Occupancy has 34K key frames, where 6 images are in each frame (i.e., 200K image frames). predictions, while the surrounding perception is more criti-cal for safe driving. To address these problems, we propose
OpenOccupancy, which is the first surrounding semantic occupancy perception benchmark. In the OpenOccupancy benchmark, we introduce nuScenes-Occupancy, which extends the large-scale nuScenes [3] dataset with dense semantic occupancy annotation. As shown in Tab. 1, the number of annotated scenes and frames (of nuScenes-Occupancy) are ∼40× and ∼5× more than that of [1].
Notably, it is almost impractical to directly annotate large-scale occupancy labels by human labor. Therefore, the
Augmenting And Purifying (AAP) pipeline is introduced to efficiently annotate and densify the occupancy labels.
Specifically, we initialize annotation by multi-frame Li-DAR points superimposition, where the per-point semantic labels are from [11]. Considering the sparsity of the initial annotation (i.e., some occupancy labels are missed due to occlusion or limited LiDAR channels), we augment it with pseudo occupancy labels, which are constructed by the pre-trained baseline (see Sec. 3.4). To further reduce noise and artifacts, human endeavors are leveraged to purify the aug-mented annotation. Based on the AAP pipeline, we gener-ate ∼2× dense occupancy labels than the initial annotation.
Visualizations of the dense annotation are shown in Fig. 1.
To facilitate future research, we establish camera-based,
LiDAR-based and multi-modal baselines for the OpenOc-cupancy benchmark. Experiment results show that the camera-based method achieves better performance on small objects (e.g., bicycle, pedestrian, motorcycle), while the
LiDAR-based approach shows superior performance on large structured regions (e.g., drivable surface, sidewalk).
Notably, the multi-modal baseline adaptively fuses interme-diate features from both modalities, relatively improving the overall performance (of camera-based and LiDAR-based methods) by 46% and 34%. Considering the computa-tional burden of the surrounding occupancy perception, the proposed baselines can only generate low-resolution predictions. Towards an efficient occupancy perception, we propose the Cascade Occupancy Network (CONet) that builds a coarse-to-fine pipeline upon the proposed baseline, relatively improving the performance by ∼30%.
The main contributions are summarized as follows: (1)
We propose OpenOccupancy, which is the first benchmark designed for surrounding occupancy perception in driving scenarios. (2) The AAP pipeline is proposed to efficiently annotate and densify semantic occupancy labels of the nuScenes dataset, and the resulted nuScenes-Occupancy is the first dataset for surrounding semantic occupancy seg-mentation. (3) We establish camera-based, LiDAR-based and multi-modal baselines in the OpenOccupancy bench-mark. Besides, the CONet is introduced to alleviate the computational burden of high-resolution occupancy pre-dictions, which relatively improves the baseline by ∼30%. (4) Based on the OpenOccupancy benchmark, we conduct comprehensive experiments on the proposed baselines,
CONet, and modern occupancy perception approaches. 2.