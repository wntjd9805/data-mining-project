Abstract
Visual question answering is a task of predicting the an-swer to a question about an image. Given that different people can provide different answers to a visual question, we aim to better understand why with answer groundings.
We introduce the first dataset that visually grounds each unique answer to each visual question, which we call VQA-AnswerTherapy. We then propose two novel problems of predicting whether a visual question has a single answer grounding and localizing all answer groundings. We bench-mark modern algorithms for these novel problems to show where they succeed and struggle. The dataset and evalua-tion server can be found publicly at https://vizwiz.org/tasks-and-datasets/vqa-answer-therapy/. 1.

Introduction
Visual question answering (VQA) is the task of predict-ing the answer to a question about an image. A fundamental challenge is how to account for when a visual question has multiple natural language answers, a scenario shown to be common [10]. Prior work [2] revealed reasons for these dif-ferences, including due to subjective or ambiguous visual questions. However, it remains unclear to what extent an-swer differences arise because different visual content in an image is described versus because the same visual content is described differently (e.g., using different language).
Our work is designed to disentangle the vision problem from other possible reasons that could lead to answer dif-ferences. To do so, we introduce the first dataset where all valid answers to each visual question are grounded, mean-ing we segment for each answer the visual evidence in the image needed to arrive at that answer. This new dataset, which we call VQA-AnswerTherapy, consists of 5,825 vi-sual questions from the popular VQAv2 [9] and VizWiz [11] datasets. We find that 16% of the visual questions have mul-tiple answer groundings, and provide fine-grained analysis to better elucidate when and why this arises.
We also introduce two novel algorithmic challenges, which are exemplified in Figure 1. First, we introduce the
Single Answer Grounding Challenge, which entails pre-Figure 1: Examples from our VQA-AnswerTherapy dataset showing that visual questions with different natural lan-guage answers can have multiple answer groundings (first row) or all share the same answer grounding (second row). dicting for a visual question whether all valid answers will describe the same grounding or not. Next, the Grounding
Answer(s) Challenge entails localizing the answer ground-ings for all valid answers to a visual question. We bench-marked models for these novel tasks to demonstrate the baseline performance for modern architectures and to high-light where they are succeeding and struggling.
We offer this work as a valuable foundation for im-proving our understanding and handling of annotator dif-ferences. Success on our Single Answer Grounding Chal-lenge will enable solutions to notify users when there is un-certainty which visual evidence to consider, enabling users to clarify a visually ambiguous visual question. This can immediately benefit visually impaired individuals since a portion of our dataset comes from this population (i.e.,
VizWiz [11]). Success on the Answer(s) Grounding Chal-lenge can enable users of VQA solutions to better under-stand the varied reasoning processes that can lead to dif-ferent natural language answers, while also contributing to enhanced model explainability and trustworthiness. More generally, this work can inform how to account for anno-tator differences for other related tasks such as image cap-tioning, visual dialog, and open-domain VQA (e.g., VQAs found on Yahoo!Answers and Stack Exchange). This work also contributes to ethical AI by enabling revisiting how
VQA models are developed and evaluated to consider the diversity of plausible answer groundings rather than a sin-gle (typically majority) one. To facilitate future extensions, we publicly-share our dataset and a public evaluation server with leaderboard for our dataset challenges at the follow-ing link: https://vizwiz.org/tasks-and-datasets/vqa-answer-therapy/. 2.