Abstract
Model calibration usually requires optimizing some pa-rameters (e.g., temperature) w.r.t an objective function like negative log-likelihood. This work uncovers a significant but overlooked aspect that the objective function is influ-enced by calibration set difficulty: the ratio of misclassified to correctly classified samples1. If a test set has a dras-tically different difficulty level from the calibration set, a phenomenon out-of-distribution (OOD) data often exhibit: the optimal calibration parameters of the two datasets would be different, rendering an optimal calibrator on the calibra-tion set suboptimal on the OOD test set and thus degraded calibration performance. With this knowledge, we propose a simple and effective method named adaptive calibrator ensemble (ACE) to calibrate OOD datasets whose difficulty is usually higher than the calibration set. Specifically, two calibration functions are trained, one for in-distribution data (low difficulty), and the other for severely OOD data (high difficulty). To achieve desirable calibration on a new
OOD dataset, ACE uses an adaptive weighting method that strikes a balance between the two extreme functions. When plugged in, ACE generally improves the performance of a few state-of-the-art calibration schemes on a series of OOD benchmarks. Importantly, such improvement does not come at the cost of the in-distribution calibration performance.
Project Website: https://github.com/insysgroup/Adaptive-Calibrators-Ensemble.git. 1.

Introduction
Model calibration aims to connect the neural network out-put with uncertainty. A common practice is to find optimal parameters against certain objective functions on a held-out calibration set, to obtain an optimized calibrator. In this paper, we focus on post-hoc calibration methods, which 1Dataset difficulty (w.r.t a classifier) shares the same meaning of classi-fier accuracy on this dataset. We use “difficulty” to indicate the property of a dataset (i.e., its OOD degree to the classifier), instead of using “accuracy” which describes the performance of the classifier on a dataset. require training a calibration mapping function to rescale the confidence scores of a trained neural network to make it calibrated [9, 10, 18]. A popular technique is Temper-ature Scaling [9], which optimizes model temperature by minimizing the negative log-likelihood (NLL) loss.
Post-hoc calibration methods generally work well when calibrating in-distribution test sets. However, oftentimes their calibration performance drops significantly when being tested on an out-of-distribution (OOD) test set [24]. For example, temperature scaling has shown to be ineffective under distribution shift in some scenarios [24]. This problem happens because the test environment (OOD) is different from the training environment due to factors like sample bias and non-stationarity. This paper thus aims to improve post-hoc calibration methods by producing reliable and predictive uncertainty under distribution shifts.
In the community, there exist a few works studying the
OOD calibration problem [28, 31, 36]. They typically aim to make amendments to the calibration set to let it approximate the OOD data in certain aspects [28, 31]. Nevertheless, these techniques are typically not adaptive to the test dataset, that is, the calibration set transformation process cannot automatically adjust to the test set. In our experiment, we observe that they improve calibration on some OOD datasets but significantly lead to decreased in-distribution calibration performance. In this regard, while TransCal [36] can perform domain adaptation according to the test domain, it needs to be re-trained for every new test set.
In this paper, our contributions are mainly in two aspects.
First, we provide a new perspective to understand calibra-tion failure on out-of-distribution datasets. Specifically, we show that the calibration objective is dependent on the dataset difficulty. When the calibration set have the same distribution with the test set, it has low difficulty, and thus the calibrator learned on the calibration set would be effective on the test set [9, 10, 18]. Yet, out-of-distribution test sets usually exhibit a different (usually higher) difficulty level compared with the calibration set because of the distribution gap. Under this case, the optimal calibration functions are different for calibration and OOD datasets. That is, a calibra-tor that optimized on the calibration set would not be optimal on OOD data and thus achieve poor calibration performance.
Second, to achieve robust calibration under distribution shifts, we propose a simple but effective method named adaptive calibrator ensemble (ACE). It adaptively integrates two predefined calibrators: 1) one trained on an easy in-distribution dataset, and 2) the other trained on a severely
OOD data set with high difficulty. By estimating how much a new test set deviates from the high-difficulty calibration set, we compute a test adaptive weight to balance the force between the two calibrators. We show that our proposed
ACE method improves three existing post-hoc calibration algorithms such as Spline [10] on commonly used OOD benchmarks. Moreover, our method does not have compro-mised calibration performance for in-distribution data. 2.