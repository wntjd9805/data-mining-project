Abstract
In this case,
Label noise is one of the key factors that lead to the poor generalization of deep learning models. Existing label-noise learning methods usually assume that the ground-truth classes of the training data are balanced. How-ever, the real-world data is often imbalanced, leading to the inconsistency between observed and intrinsic class dis-tribution with label noises. it is hard to distinguish clean samples from noisy samples on the in-trinsic tail classes with the unknown intrinsic class dis-tribution.
In this paper, we propose a learning frame-work for label-noise learning with intrinsically long-tailed data. Specifically, we propose two-stage bi-dimensional sample selection (TABASCO) to better separate clean sam-ples from noisy samples, especially for the tail classes.
TABASCO consists of two new separation metrics that com-plement each other to compensate for the limitation of us-ing a single metric in sample separation. Extensive exper-iments on benchmarks demonstrate the effectiveness of our method. Our code is available at https://github. com/Wakings/TABASCO. 1.

Introduction
Under the support of a large amount of high-quality la-beled data, deep neural networks have achieved great suc-cess in various fields [27, 40, 7]. However, it is expen-sive and difficult to obtain a large amount of high-quality labeled data in many practical applications.
Instead, the commonly used large-scale training data is usually obtained from the Internet or crowdsourcing platforms like Amazon
Mechanical Turk, which is unreliable and may be misla-beled [52, 43]. The models trained on this kind of unreliable data, called noisy-labeled data, often produce poor general-ization performance because deep neural networks tend to overfit noisy samples due to their large model capacity. In
*Yang Lu is the corresponding author: luyang@xmu.edu.cn (a) Class distribution (b) Loss distribution
Figure 1: (a) An example of observed class distribution with noisy labels with long-tailed intrinsic class distribution. (b)
The training loss of each sample under noisy-labeled and long-tailed data. the literature, there are some works to obtain a robust model trained on noisy-labeled data [11, 44]. Among them, the most straightforward and effective way is to differentiate between clean and noisy samples based on their differences in specific metrics, such as training loss [12, 29, 23].
These label-noise learning methods generally assume that the intrinsic class distribution of the training data is balanced, where each class has almost the same number of samples in terms of their unknown ground-truth labels.
However, the data in real-world applications are often im-balanced, e.g., the LVIS dataset [9] and the iNaturalist dataset [17]. The class imbalance usually exhibits in the form of a long-tail distribution, where a small portion of classes possess a large number of samples, and the other classes possess a small number of samples only [38, 18]. In this case, the model training tends to the head classes and ignores the tail classes [60, 62]. When both noisy labels and long-tail distribution exist, training a robust model is even more challenging. There are two key challenges in this sce-(1) Distribution inconsistency: The observed and nario. intrinsic distributions are likely inconsistent due to noise la-bels, making the model more difficult to discover and focus on the intrinsic tail classes. As illustrated in Fig. 1(a), the in-trinsic class distribution of the dataset is long-tailed, while the existence of noisy labels makes the distribution more
balanced. The intrinsic tail classes, e.g., classes 6 and 9, are occupied by a large number of noisy data, making them no longer tail classes by observation. (2) Tail inseparability:
Even if the tail class is identified, it is more difficult than ever to distinguish between clean and noisy samples in the tail class because clean samples are overwhelmed by noises that make their values of the separation metric highly sim-ilar. As illustrated in Fig. 1(b), the training loss of clean and noisy samples in the tail class are generally inseparable compared with the ones in the other classes. Several pre-liminary works have studied the joint problem of label noise and long-tail distribution [21, 49, 24, 2]. These methods im-plicitly reduce the complexity of the problem by assuming a similar noise rate for each class. However, this assumption is too strong to apply because noisy samples from the head classes may be the majority, resulting in a higher noise rate in the tail class than in other classes.
In this paper, we propose a Two-stAge Bi-dimensionAl
Sample seleCtiOn (TABASCO) strategy to address the problem of label-noise learning with intrinsically long-tailed data. In the first stage, we propose to use two new sep-aration metrics for sample separation, i.e., weighted Jensen-Shannon divergence (WJSD) and adaptive centroid distance (ACD), which work corporately to separate clean samples from noisy samples in the tail classes. The proposed met-rics are complementary, where WJSD separates the sam-ples from the output perspective while ACD does that from the feature perspective. In the second stage, we determine the separation dimension with better separability for each class and perform sample selection. In order to evaluate the method uniformly and effectively, we introduce two bench-marks with real-world noise and intrinsically long-tailed distribution. The main contributions of our work can be summarized as follows:
• We present a more general problem of label-noise learning with intrinsically long-tailed data. The key challenges in this problem are distribution inconsis-tency and tail inseparability.
• We propose an effective solution called TABASCO.
With the help of two new separation metrics, it is able to effectively identify and select clean samples of the intrinsic tail class.
• We introduce two benchmarks with real-world noise and intrinsically long-tailed distribution. Extensive experiments on them show the effectiveness of our method and the limitations of existing methods. 2.