Abstract
Event-based motion deblurring has shown promising re-sults by exploiting low-latency events. However, current approaches are limited in their practical usage, as they as-sume the same spatial resolution of inputs and specific blur-riness distributions. This work addresses these limitations and aims to generalize the performance of event-based de-blurring in real-world scenarios. We propose a scale-aware network that allows flexible input spatial scales and en-ables learning from different temporal scales of motion blur.
A two-stage self-supervised learning scheme is then devel-oped to fit real-world data distribution. By utilizing the rel-ativity of blurriness, our approach efficiently ensures the re-stored brightness and structure of latent images and further generalizes deblurring performance to handle varying spa-tial and temporal scales of motion blur in a self-distillation manner. Our method is extensively evaluated, demonstrat-ing remarkable performance, and we also introduce a real-world dataset consisting of multi-scale blurry frames and events to facilitate research in event-based deblurring.
Multimedia Material
The Multi-Scale Real-world Blurry Dataset (MS-RBD) and our Pytorch implementation are available at: https:
//github.com/XiangZ-0/GEM. 1.

Introduction
Due to the fixed exposure time of frame-based cam-eras, motion blur often occurs in scenes with dynamic tar-gets or camera ego-motion, degrading the quality of the ac-quired images [15, 33]. Conventional motion deblurring ap-proaches attempt to resolve this by exploiting deconvolution and blur kernel estimation techniques [30, 16], and recent research further improves the deblurring performance with the advanced deep-learning methods [12, 36]. However, tra-(cid:66)Corresponding author
The research was partially supported by the National Natural Science
Foundation of China under Grants 62271354 and 61871297.
Figure 1: An illustrative example of motion deblurring via the state-of-the-art algorithm Motion-ETR [36] and our pro-posed method, which is trained on HR blurry frames and LR events in a self-supervised manner and can generalize to the inputs at different temporal and spatial scales. ditional frame-based methods usually assume specific mo-tion patterns, e.g., linear or quadratic motion trajectory, for blurry images and thus often face challenges in real-world scenarios with complex non-uniform motions. In addition, due to the motion ambiguity and texture erasure issues in blurry images [29, 35], frame-based approaches often strug-gle to extract the precise motion and restore the accurate latent images from severely blurred frames.
The advent of event cameras poses a paradigm shift in visual perception and information acquisition, benefiting a wide variety of applications [17, 6, 22, 34, 10, 37, 32, 7, 31]. For motion deblurring tasks, the microsecond-level low latency of events enables almost continuous observa-tion of dynamic scenes and alleviates the motion ambigu-ity in blurry frames [21, 26]. Moreover, the brightness
changes recorded in event streams inherently correspond to high-contrast edges, compensating for the intensity texture erased by motion blur [29, 35, 24, 13]. However, the per-formance of current event-based deblurring methods is usu-ally confined to the distribution of training data, e.g., frames with a certain range of blurriness and the same spatial reso-lution as events, posing limitations in real-world scenarios.
• Temporal Limitation: Most previous approaches synthesize or collect blurry frames in a fixed range of exposure time for training [26, 29], which implic-itly assumes motion blur with a specific distribution of blurriness. However, real-world motion blur often violates this assumption in highly dynamic scenes, re-sulting in a performance drop of pre-trained models.
• Spatial Limitation: Existing methods mainly take frames and events of the same spatial resolution as in-put, ignoring that frame-based cameras usually have larger spatial resolution than event-based ones in prac-tice [6]. Besides, due to the varying distributions of events at different spatial scales [9], how to effec-tively deblur High-Resolution (HR) frames with Low-Resolution (LR) events remains an open problem.
In this paper, we propose to address the above issues and generalize the performance of event-based motion de-blurring in both spatial and temporal domains, as shown in
Fig. 1.
In detail, a Scale-Aware Network (SAN) is first designed to extract high frame-rate HR sequences from a single HR blurry frame and its concurrent LR events. In-spired by implicit neural representation [3], we implement a Multi-Scale Feature Fusion (MSFF) module to represent frame and event features in a spatially continuous manner, which allows flexible setups of input spatial resolutions. In the temporal dimension, an Exposure-Guided Event Repre-sentation (EGER) is presented to enable the arbitrary selec-tion of target latent images without requiring model mod-ification or re-training. To fit real-world data distribution, a two-stage self-supervised learning framework is further proposed. In the first stage, we efficiently supervise the re-stored brightness and structure of latent images by utilizing the relativity of blurriness. Following that, a self-distillation strategy is applied to generalize the deblurring performance to handle varying spatial and temporal scales of motion blur.
Overall, our contributions are three-fold:
• A scale-aware network is presented to allow flexible setups of input spatial resolutions and output tempo-ral scales, which is able to restore high frame-rate HR sequences from HR blurry frames and LR events.
• A two-stage self-supervised learning framework is proposed to efficiently fit real-world data distributions and generalize deblurring performance to handle vary-ing spatial and temporal scales of motion blur.
• A real-world dataset MS-RBD containing HR blurry frames and LR events is built to facilitate deblurring research. Extensive experiments on both synthetic and real datasets validate the effectiveness of our approach. 2.