Abstract
Open-World Compositional Zero-Shot Learning (OW-CZSL) aims to recognize new compositions of seen at-tributes and objects.
In OW-CZSL, methods built on the conventional closed-world setting degrade severely due to the unconstrained OW test space. While previous works alleviate the issue by pruning compositions according to external knowledge or correlations in seen pairs, they in-troduce biases that harm the generalization. Some methods thus predict state and object with independently constructed and trained classifiers, ignoring that attributes are highly context-dependent and visually entangled with objects. In this paper, we propose a novel Distilled Reverse Attention
Network to address the challenges. We also model attributes and objects separately but with different motivations, cap-turing contextuality and locality, respectively. We further design a reverse-and-distill strategy that learns disentan-gled representations of elementary components in training data supervised by reverse attention and knowledge distil-lation. We conduct experiments on three datasets and con-sistently achieve state-of-the-art (SOTA) performance.
Figure 1: Motivation behind our disentangling strategy for OW-CZSL. When extracted features of objects and at-tributes are disentangled (images 3 and 5), their residual features (images 4 and 6) carry sufficient information about each other to classify correctly, and produce large overlap between the object residuals and the attribute features (im-ages 4 and 5). For entangled attribute-object features (im-ages 1 and 3), the phenomena are otherwise reversed (image 2: few object information; images 1 and 4: small overlap). 1.

Introduction
Humans can recognize complex concepts never seen be-fore (e.g., the pink elephant) by composing their knowledge of familiar visual primitives (elephants and other pink ob-jects). This ability of compositional learning is considered a hallmark of human intelligence [17] that deep learning methods clearly lack [18]. Deep learning often requires a large quantity of labeled examples to train. However, real-world instances follow a long-tailed distribution [34, 38], making it impractical to gather supervision for all cate-gories. Compositional Zero-shot Learning (CZSL) mimics the human ability to tackle these issues [31, 23, 29, 15].
CZSL learns the compositionality of seen objects (e.g. fruits, animals, etc.) and attributes (e.g. colors, sizes, etc.) as primitives to recognize unseen attribute-object pairs. For example, CZSL composes and generalizes Peeled-Orange and Sliced-Apple to Peeled-Apple (Fig. 1). Conventional
CZSL methods characterize closed-world (CW-CZSL) set-tings [31, 28, 30, 23], where unseen attribute-object pairs contained in test images are given as priors to restrict the search space. For example, the test space of the widely-used benchmark MIT-States [13] is simplified to 1,662 compo-sitions out of 28,175 possible pairs (115 attributes × 245 objects) for CW-CZSL. This setup fundamentally reduces the generalization ability of CZSL models. Therefore, in this work, we study a more realistic and challenging task: unconstrained Open-World CZSL (OW-CZSL) [14, 15, 26, 27], where arbitrary compositions may appear at test time.
A notable line of works for CW-CZSL projects attribute-object pairs and images onto a shared embedding space to perform similarity-based composition classification [41,
29, 39]. However, their performances severely degrade for
OW-CZSL [26] due to greatly expanded output space (e.g.,
∼ 17 times in MIT-States). Thus, some works adapt them to
OW-CZSL by pruning OW composition space based on fea-sibility scores calculated according to linguistic side infor-mation [15] or seen attribute-object dependencies [26, 27].
Such scores inevitably introduce biases caused by distribu-tion shifts between images and external linguistic knowl-edge bases or seen and unseen compositions, resulting in visual-semantic inconsistent or seen-biased predictions.
Therefore, for OW-CZSL, we follow another direction that adopts two parallel discriminative modules to infer objects and attributes respectively, reducing composition search to separate attribute and object search [15, 14, 19, 43].
Despite the success of separate modeling techniques in
CW- and OW-CZSL, these ignore the intrinsic differences between attributes and objects [19, 43, 15, 14]. Children, for instance, learn nouns faster than adjectives because they relate to context differently [7]. Similarly, visual primitives of attributes (often adjectives) are more context-sensitive than objects (usually nouns) [28, 30]. For example, Small in Small-Cat and Small-Building is not visually equivalent, while Tomato in Red-Tomato and Fresh-Tomato is simi-lar. Extracting attribute and object features using identical structures [15, 14] without considering the heavier context dependencies of attributes may impair the discrimination.
Another bottleneck for separate modeling is visual en-tanglement. Taking Fig. 1 as an example, given an image of the unseen composition, i.e., Peeled-Apple, it is hard to distinguish which visual features are Apple and which ones are Peeled. The extracted features of attributes and objects are highly entangled (images 1 and 3), leading to a wrong prediction biased towards the seen pairs, i.e., Sliced-Apple. Some efforts disentangle the embeddings in CW-CZSL [33, 1, 43, 19]. However, they either learn pair-wise attribute-object correlations in compositional space [32, 1] or adopt generative methods to synthesize samples for all pairs [33, 19], thus making them infeasible for OW-CZSL due to the drastically expanded output space.
To address these issues, we propose the Distilled Re-verse Attention Network (DRANet) that extracts and disen-tangles visual primitives of attributes and objects for OW-CZSL. First, we design attribute/object-specific networks to extract their features differently according to their charac-teristics. As suggested by [35], Convolutional Neural Net-works (CNNs), used to extract visual embeddings in CZSL, are built on top of local neighborhoods and thus cannot cap-ture long-range context. Therefore, we adapt non-local at-tention blocks [35, 6] to model spatial and channel contex-tual relationships for attribute learning while adopting local attention to focus on essential parts for object recognition.
Second, we design an attention-based disentangling strategy for OW-CZSL, namely Reverse-and-Distill. This strategy is based on the observation that humans can still recognize Apple after removing Peeled from the images of
Peeled-Apple. Intuitively, if learned primitives of attributes and objects are disentangled, removing either of them from the feature space will not affect the classification of the other. Thus, object predictions after erasing the attribute features (or attribute predictions after object removal) can indicate the unraveling degree of attribute and object fea-tures. For example, as shown in Fig. 1, models can still recognize Apple (image 6) after removing attribute features when primitives are disentangled (images 3 and 5) but fail (image 2) when entangled (images 1 and 3). Given that fea-ture removal is intractable in practice, we approximate it by reversing attention. We then achieve attribute and object feature disentanglement by supervising their residuals to crossly carry sufficient object and attribute information. Be-sides, when attribute and object features are disentangled, the overlaps between attribute features and object residu-als (or object features and attribute residuals) become large (seeing images 4 and 5 or images 3 and 6 in Fig. 1). We enlarge such overlaps by distilling primitives to learn from mutual residuals for further unraveling.
In summary, our contributions are as follows: 1) We pro-pose the DRANet for OW-CZSL. DRANet employs distinct extractors to capture attribute and object features, enhanc-ing contextuality and locality, simultaneously. 2) We design the reverse-and-distill strategy to disentangle the attribute and object embeddings in OW-CZSL, where existing dis-entangling methods in CW-CZSL are impractical. 3) We achieve SOTA performance on three benchmark datasets, and analyze the limitations and extensibility of our model. 2.