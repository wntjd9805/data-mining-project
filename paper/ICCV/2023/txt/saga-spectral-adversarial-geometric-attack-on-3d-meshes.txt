Abstract
Atta
Attack ack
A triangular mesh is one of the most popular 3D data representations. As such, the deployment of deep neural networks for mesh processing is widely spread and is in-creasingly attracting more attention. However, neural net-works are prone to adversarial attacks, where carefully crafted inputs impair the model’s functionality. The need to explore these vulnerabilities is a fundamental factor in the future development of 3D-based applications. Recently, mesh attacks were studied on the semantic level, where clas-siﬁers are misled to produce wrong predictions. Neverthe-less, mesh surfaces possess complex geometric attributes beyond their semantic meaning, and their analysis often in-cludes the need to encode and reconstruct the geometry of the shape.
We propose a novel framework for a geometric adversar-ial attack on a 3D mesh autoencoder. In this setting, an ad-versarial input mesh deceives the autoencoder by forcing it to reconstruct a different geometric shape at its output. The malicious input is produced by perturbing a clean shape in the spectral domain. Our method leverages the spectral de-composition of the mesh along with additional mesh-related properties to obtain visually credible results that consider the delicacy of surface distortions1. 1.

Introduction
A triangular mesh is the primary representation of 3D shapes, with applications in many safety-critical realms. In the medical ﬁeld, incorrect perception of the geometric sub-tleties of an organ can lead to life-threatening errors.
In robotics and automotive, a precise understanding of the ge-ometry of obstacles is essential to prevent accidents. The security of facial modeling is also dependent on the accu-racy of the processed geometry of the mesh.
Autoencoders (AEs) are one of the most prominent deep-learning tools to process the mesh’s geometry. They are de-signed to capture geometric features which enable dimen-1https://github.com/StolikTomer/SAGA
*Equal contribution
AE
AE
AEAE
AE
Figure 1. A result of our geometric mesh attack. A mesh of a sphere (top left) is perturbed into an adversarial example (top right). While the original mesh is accurately reconstructed by an autoencoder (AE) (bottom left), our attack fools the AE and changes the output geometry to a cube! (bottom right). sionality reduction for both storage and communication pur-poses [6, 4]. Mesh AEs are also used for segmentation, self-supervised learning, and denoising tasks [16, 19, 7].
Despite their tremendous achievements, neural networks are often found vulnerable to adversarial attacks. These at-tacks craft inputs that impair the victim network’s behavior.
Adversarial attacks were extensively studied in recent years, focusing especially on the semantic level, where the input to a classiﬁer is carefully modiﬁed in an imperceivable man-ner to mislead the network to an incorrect prediction. Se-mantic adversarial attacks are abundant in the case of 2D images [8, 20, 3], and recently, semantic attacks on 3D rep-resentations have also drawn much attention, both on point clouds [29, 10, 28] and meshes [30, 14, 23, 1].
Nonetheless, the vulnerabilities of networks that process geometric attributes, such as AEs, have not been thoroughly investigated. AEs may be imperative to many practical mesh deployments and their credibility and robustness de-pend on the study of geometric adversarial attacks.
We propose a framework of a geometric adversarial at-tack on 3D meshes. Our attack, named SAGA, is exempli-ﬁed in Figure 1. The input mesh of the sphere is perturbed and fed into an AE that reconstructs a geometrically differ-ent output, i.e., a cube! Ideally, the deformation of the input
should be unapparent and yet effectively modify the output geometry.
In our attack, we aim to reconstruct the geometry of a speciﬁc target mesh by perturbing a clean source mesh into a malicious input. We present a white-box setting, where we have access to the AE and we optimize the attack according to its output. A black-box framework is also explored by transferring the adversarial examples to other unseen AEs.
Mesh perturbations include shifts of vertices that af-fect their adjacent edges and faces and possibly result in noticeable topological disorders, such as self-intersections.
Therefore, concealed perturbations must address the inher-ent topological constraints of the mesh. To cope with the fragility of the mesh surface, we apply the perturbations in the spectral domain deﬁned by the eigenvectors of the
Laplace-Beltrami operator (LBO) [5]. Particularly, we fa-cilitate an accelerated attack by operating in a shared spec-tral coordinate system for all shapes in the dataset. The source’s distortions are retained by using low-frequency perturbations and additional mesh-related regularizations.
The attack is tested on datasets of human faces [24] and animals [32]. We evaluate SAGA using geometric and se-mantic metrics. Geometrically, we measure the similarity between shapes by comparing the mean curvature of match-ing vertices. Semantically, we use a classiﬁer to predict the labels of the adversarial reconstructions, and a detector net-work to demonstrate the difﬁculty of identifying the adver-sarial shapes. We also conduct a thorough analysis of the attack and a comprehensive ablation study.
To summarize, we are the ﬁrst to propose a geometric adversarial attack on 3D meshes. Our method is based on low-frequency spectral perturbations and regularizations of mesh attributes. Using these, SAGA crafts adversarial ex-amples that change an AE’s output into a different geomet-ric shape. 2.