Abstract
Video action segmentation involves categorizing each frame or short snippet of an untrimmed video into prede-ﬁned action categories. Despite notable advancements in recent years, a considerable number of current approaches still rely on frame-wise segmentation that tends to render fragmentary results. To address it, we present an inno-vative approach for video action segmentation, centered
Initially, around contextually reﬁned temporal keypoints. our method identiﬁes a set of sparse, over-complete tem-poral keypoints through non-local visual cues, with each keypoint representing a potential action segment candi-date. Subsequent enhancements to these initial keypoints are achieved through iterative reﬁning and re-assembling operations. Driven by the notion that optimal temporal key-points should collectively resemble the true ground-truth structurally, we introduce a module that conducts graph matching between the keypoint-derived graph and the ref-erence graph constructed from accurate annotations. This module effectively learns structural features used to fur-ther reﬁne the initial keypoints. Moreover, a set of pre-deﬁned rules is applied to re-assemble all temporal key-points. The unﬁltered temporal keypoints, resulting from these operations, are harnessed to generate the ﬁnal ac-tion segments. We extensively evaluate our method across three video benchmarks: 50salads, GTEA, and Breakfast.
Our proposed approach consistently demonstrates substan-tial improvements over existing methods, establishing its su-periority in video action segmentation. It achieves F 1@50 scores (one of the key performance metrics for this task) of 79.5%, 83.4%, and 60.5%, respectively, v.s. previous state-of-the-art 78.5%, 79.8% and 57.4%. 1.

Introduction
Understanding human actions from visual sensors has been regarded as a long-standing crucial task in a variety of real-world applications, such as surveillance video anal-*Corresponding author video
…
…
…
…
Ground truth via densely classification
Keypoints
Generation take open scoop boundary-misalign over-segmentation take open scoop
Refinement via graph relations
GCNs
Rule-based re-Assembling via RTK
Figure 1: Comparison of frame-wise classiﬁcation meth-ods and keypoints based segmentation methods.
The untrimmed video clip of making peanut butter bread con-tains three actions. Speciﬁcally, the colors indicate different categories of actions, while the white indicates background and grey is for boundary point. Frame-wise classiﬁcation methods are prone to over-segmentation and boundary mis-alignment, while keypoints based method effectively allevi-ates these problems by reﬁning keypoints using contextual information. ysis [4, 10, 11, 55], human-robot interaction [11, 43], au-tonomous driving, sports Analysis [50], etc.
In the past few years, researchers have made great efforts on segment-ing human actions in a frame-wise classiﬁcation manner.
Speciﬁcally, each frame of the video is densely labeled as a pre-deﬁned human action category, based on the complete video stream, which is shown in Figure 1. However, exist-ing methods [1,22,48,51] under this manner are still suffer-ing from several mis-classiﬁcation problems, such as over-segmentation or boundary-misalignment. We illustrate the above problems in two aspects:
Frame-wise classiﬁcation does not globally consider the semantics of each temporal action segment. For the frame-wise classiﬁcation loss, the goal of the function is always to assign the pre-deﬁned categories to each video frame inde-pendently, regardless of whether the arrangement or dura-tion of temporal actions are semantically appropriate. For example, in Figure 1, there is one frame mis-classiﬁed in scoop since the effects of video motion blur or other noises, which does not produce a large loss in frame-wise classi-ﬁcation. Moreover, no matter where the mis-classiﬁcation occurs, the loss is always the same. However, the overall meaning of the video changed considerably when the cen-tral mis-classiﬁed frame occurs, because the long scoop is split into three short actions (scoop, open, scoop). Although the smoothing-based methods [22,48] make great efforts on dynamically voting the predictions on a local region to al-leviate such errors (i.e., over-segmentation). However, due to the varying duration of temporal actions, the range of smoothness is often difﬁcult to determine.
On the other hand, capturing long-term dependencies frame-wisely is computationally expensive. As the com-mon practice, existing methods use dilated convolution [22] with a large dilation rate, or a local sliding-window based self-attention module [51], or casting multiple frames into several high ordered embeddings for further reﬁnement [1].
However, the above methods either suffer from the size and shape of the convolution kernel or sliding window [22, 51], or the inaccurate representation of video actions caused by over-segmentation in frame-level classiﬁcation.
In this paper, we propose a video action segmenta-tion framework via contextually Reﬁned Temporal Key-points (RTK), which treats actions as keypoints. Temporal keypoint-based methods, as ﬁrst explored and advocated in this work, use sparse candidate points to represent actions, which is a high-order semantic representation. In addition, by constructing graph relationships between temporal key-points, it is also easier to capture the long-term dependen-cies between actions. RTK has three main beneﬁts: (i) Detecting temporal keypoints implies localization of temporal actions, which implies that the model focuses on discriminating the global-to-local location of each action, rather than independently classifying each frames. (ii) Detecting keypoints facilitates further exploration of contextual information between actions. Compared to ana-lyze video frame-level relations, analyzing keypoint-level relations can provide higher-order semantic information, which can also help save the cost of capturing long-term dependencies. For speciﬁc, we construct the graph struc-ture on sparse temporal keypoints while adopting a graph matching module for assistance, in modeling ﬁne-grained action-level relationships in videos. (iii) For the reason that boundary points are directly lo-calized and optimized in the pipeline, RTK leads to precise action boundaries and avoids boundary-misalignment prob-lems, compared to frame-wise classiﬁcation.
[14] dataset.
We evaluate the framework on three popular datasets for temporal action segmentation: 50salads [38], GTEA [8], and Breakfast
RTK utilizes a modi-ﬁed ASFormer [51] architecture with several extra mod-ules(keypoints generatation heads, graph matching mod-ules) and achieves 83.4% F 1@0.50 scores on GTEA, 79.5% on 50salads, and 60.5% on Breakfast, which im-proves the segmentation baselines by about 2 ∼ 4% and signiﬁcantly alleviating the over-segmentation problems. 2.