Abstract
Shadows often occur when we capture the document with casual equipment, which influences the visual quality and readability of the digital copies. Different from the algo-rithms for natural shadow removal, the algorithms in doc-ument shadow removal need to preserve the details of fonts and figures in high-resolution input. Previous works ig-nore this problem and remove the shadows via approxi-mate attention and small datasets, which might not work in real-world situations. We handle high-resolution doc-ument shadow removal directly via a larger-scale real-world dataset and a carefully-designed frequency-aware network. As for the dataset, we acquire over 7k cou-ples of high-resolution (2462 × 3699) images of real-world documents pairs with various samples under dif-ferent lighting circumstances, which is 10 times larger the net-than existing datasets. work, we decouple the high-resolution images in the fre-quency domain, where the low-frequency details and high-frequency boundaries can be effectively learned via the carefully designed network structure. Powered by our net-work and dataset, the proposed method shows a clearly better performance than previous methods in terms of vi-sual quality and numerical results. The code, models, and dataset are available at https://github.com/
CXH-Research/DocShadow-SD7K.
As for the design of (a) Input (b) Shah et al. [27] (c) Fu et al. [8] 1.

Introduction
When occluders obstruct the light source, shadows are cast on the paper. These shadows reduce the readabil-ity and hinder subsequent tasks such as document enhancement, optical character recogni-tion (OCR), key information extraction, and semantic entity labeling [17, 36, 28], etc. text-related intelligent
Removing the cast shadow of the digital copy has a long need in the real world and also there are a lot of meth-*Equal contributions
†Corresponding authors (d) Liu et al. [25] (e) Hu et al. [15] (f) Ours
Figure 1. Visual results of input document shadow image (a), classic method (b), supervised method (c), weakly-supervised method (d), unsupervised method (e) and ours (f). Our model re-moves the shadow while preserving the original document’s con-tent and aspect ratio. ods, including the traditional methods that applied physics-based illumination models [16, 30, 32]. In reality, these ap-proaches are significantly constrained due to the fact that the assumptions utilized in illumination models [27] are often too rigid to account for real-world shadow. Thus, they may
incorrectly compute the exposure and reflectance as shown in Fig. 1.
Recent methods apply deep learning for this task since it has achieved remarkable progress in natural shadow re-moval [6] and other computer vision tasks [12, 13, 35].
However, several obstacles make it challenging to transfer previous natural shadow removal methods to the document domain. On the one hand, in contrast to natural shadow re-moval, documents are presented by high-resolution images in the real world, which needs to preserve fine-grained in-formation such as fonts and figures. However, the majority of document shadow removal methods [24, 4] are only de-signed for relatively low-resolution images and is hard to handle high-resolution situations directly since they are re-lying on the low-resolution approximation of attention for guidance. On the other hand, there are no large-scale and high-resolution datasets designed specifically for document shadow removal at this time. Consequently, given limited image pairs for training, existing methods usually result in different artifacts (as shown in Fig. 1 (c)). Although weakly-supervised [18] and unsupervised [15, 25] shadow removal methods alleviate the requirements for data, they hold a strong assumption of the statistical similarity be-tween shadowed and unshadowed domains. As illustrated in Fig. 1 (d) (e), once the training domain is very different from the test, these methods create hallucination contents and suffer from unstable results.
To solve the above problems, we first offer a large-scale real-world shadow dataset, Shadowed Document 7K (SD7K), which contains more than 7k real-world and high-resolution shadow and shadow-free document image pairs with also the manually annotated shadow masks for re-lated research, e.g., shadow detection [6]. Compared to ex-isting document shadow removal datasets, SD7K includes various document types, e.g., manga, papers, and figures.
Also, different from previous work which only considers one specific light environment, we use three distinct light sources to increase the diversity of the training samples. Be-sides the dataset, we also propose a robust network for high-resolution image processing specifically, named Frequency-aware Shadow Erasing Net (FSENet). In detail, inspired by the laplacian pyramid [2, 23], we divide the document im-age into two frequency portions, each with its own domain-specific properties. The low-frequency component is pri-marily responsible for illuminations or colors while high-frequency components are more related to content details.
As for the low-frequency deshading module, we involve the transformer [7] based network to model the global illumi-nations alternation and a high-frequency restoration module based on cascaded aggregations for adaptive pixel enhance-ment. For high-frequency modules, we involve several di-lated convolution-based texture recovery modules to restore the refined details similar to [6]. With the aforementioned techniques, our model achieves the state-of-the-art perfor-mance of previous methods on two common small bench-marks and the proposed SD7K.
We summarize the main contribution as follows:
• We provide SD7K, a large-scale real-world dataset consisting of high-resolution shadow and the associ-ated shadow-free images under various illumination conditions.
• We propose FSENet, a frequency-aware network with a carefully designed network structure to handle high-resolution document shadows.
• Both qualitative and quantitative results on all avail-able public datasets indicate that the proposed FSENet performs favorably against state-of-the-art methods. 2.