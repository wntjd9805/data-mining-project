Abstract
We present DySample, an ultra-lightweight and effective dynamic upsampler. While impressive performance gains have been witnessed from recent kernel-based dynamic up-samplers such as CARAFE, FADE, and SAPA, they intro-duce much workload, mostly due to the time-consuming dy-namic convolution and the additional sub-network used to generate dynamic kernels. Further, the need for high-res feature guidance of FADE and SAPA somehow limits their application scenarios. To address these concerns, we by-pass dynamic convolution and formulate upsampling from the perspective of point sampling, which is more resource-efficient and can be easily implemented with the standard built-in function in PyTorch. We first showcase a naive de-sign, and then demonstrate how to strengthen its upsam-pling behavior step by step towards our new upsampler,
DySample. Compared with former kernel-based dynamic upsamplers, DySample requires no customized CUDA pack-age and has much fewer parameters, FLOPs, GPU mem-ory, and latency. Besides the light-weight characteristics,
DySample outperforms other upsamplers across five dense prediction tasks, including semantic segmentation, object detection, instance segmentation, panoptic segmentation, and monocular depth estimation. Code is available at https://github.com/tiny-smart/dysample. 1.

Introduction
Feature upsampling is a crucial ingredient in dense pre-diction models for gradually recovering the feature reso-lution. The most commonly used upsamplers are nearest neighbor (NN) and bilinear interpolation, which follows fixed rules to interpolate upsampled values. To increase flexibility, learnable upsamplers are introduced in some spe-cific tasks, e.g., deconvolution in instance segmentation [13] and pixel shuffle [34] in image super-resolution [31, 12, 22].
*Corresponding author
Figure 1. Comparison of performance, inference speed, and GFLOPs of different upsamplers. The circle size in-dicates the GFLOPs cost. The inference time is tested by
×2 upsampling a feature map of size 256 × 120 × 120. The mIoU performance and additional GFLOPs are tested with
SegFormer-B1 [40] on the ADE20K data set [42].
However, they either suffer from checkerboard artifacts [32] or seem not friendly to high-level tasks. With the popular-ity of dynamic networks [14], some dynamic upsamplers have shown great potential on several tasks. CARAFE [37] generates content-aware upsampling kernels to upsample the feature by dynamic convolution. The following work
FADE [29] and SAPA [30] propose to combine both the high-res guiding feature and the low-res input feature to generate dynamic kernels, such that the upsampling process could be guided by the higher-res structure. These dynamic upsamplers are often of complicated structures, require cus-tomized CUDA implementation, and cost much more infer-ence time than bilinear interpolation. Particularly for FADE and SAPA, the higher-res guiding feature introduces even more computational workload and narrows their application
scenarios (higher-res features must be available). Different from the early plain network [27], multi-scale features are often used in modern architectures; therefore the higher-res feature as an input into upsamplers may not be necessary.
For example in Feature Pyramid Network (FPN) [23], the higher-res feature would add into the low-res feature after upsampling. As a result, we believe that a well-designed single-input dynamic upsampler would be sufficient.
Considering the heavy workload introduced by dynamic convolution, we bypass the kernel-based paradigm and re-turn to the essence of upsampling, i.e., point sampling, to reformulate the upsampling process. Specifically, we hy-pothesize that the input feature is interpolated to a continu-ous one with bilinear interpolation, and content-aware sam-pling points are generated to re-sample the continuous map.
From this perspective, we first present a simple design, where point-wise offsets are generated by linear projection and used to re-sample point values with the grid sample function in PyTorch. Then we showcase how to improve it with step-by-step tweaks by i) controlling the initial sam-pling position, ii) adjusting the moving scope of the offsets, and iii) dividing the upsampling process into several inde-pendent groups, and obtain our new upsampler, DySample.
At each step, we explain why the tweak is required and con-duct experiments to verify the performance gain.
Compared with other dynamic upsamplers, DySample i) does not need high-res guiding features as input nor ii) any extra CUDA packages other than PyTorch, and partic-ularly, iii) has much less inference latency, memory foot-print, FLOPs, and number of parameters, as shown in Fig. 1 and Fig. 8. For example, on semantic segmentation with
MaskFormer-SwinB [8] as the baseline, DySample invites 46% more performance improvement than CARAFE, but requires only 3% number of parameters and 20% FLOPs of
CARAFE. Thanks to the highly optimized PyTorch built-in function, the inference time of DySample also approaches to that of bilinear interpolation (6.2 ms vs. 1.6 ms when upsampling a 256 × 120 × 120 feature map). Besides these appealing light-weight characteristics, DySample re-ports better performance compared with other upsamplers across five dense prediction tasks, including semantic seg-mentation, object detection, instance segmentation, panop-tic segmentation, and monocular depth estimation.
In a nutshell, we think DySample can safely replace
NN/bilinear interpolation in existing dense prediction mod-els, in light of not only effectiveness but also efficiency. 2.