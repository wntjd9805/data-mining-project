Abstract
Modeling Neural Radiance Fields for fast-moving de-formable objects from visual data alone is a challenging problem. A major issue arises due to the high deforma-tion and low acquisition rates. To address this problem, we propose to use event cameras that offer very fast ac-quisition of visual change in an asynchronous manner. In this work, we develop a novel method to model the de-formable neural radiance fields using RGB and event cam-eras. The proposed method uses the asynchronous stream of events and calibrated sparse RGB frames. In our setup, the camera pose at the individual events –required to in-tegrate them into the radiance fields– remains unknown.
Our method jointly optimizes these poses and the radiance field. This happens efficiently by leveraging the collection of events at once and actively sampling the events during learning. Experiments conducted on both realistically ren-dered graphics and real-world datasets demonstrate a sig-nificant benefit of the proposed method over the state-of-the-art and the compared baseline. This shows a promising direction for modeling deformable neural radiance fields in real-world dynamic scenes. We release our code at: https://qimaqi.github.io/DE-NeRF.github.io/ 1.

Introduction
Neural Radiance Fields (NeRFs) have shown great suc-cess in synthesizing photorealistic images by implicitly rep-resenting rigid 3D scenes. Modeling non-rigid scenes in such manner is a much more difficult task. Recently, sev-eral methods have been proposed to model dynamic neural radiance fields. They aim to model rather slowly deforming radiance fields [5, 15, 16, 6, 30]. The slow deformation as-sumption is insufficient in scenarios involving fast-moving objects or, equivalently, low frame- rate cameras. In other words, the existing methods cannot capture fast-deforming radiance fields due to the limited frame rate of RGB cam-eras. To address this problem, we propose to add an event camera that provides information about the radiance change asynchronously.
Event cameras capture radiance changes, also in the
Figure 1. Our framework takes the aligned frames and events cap-tured by a dual RGB-Event camera setup as input. Our method captures fast-moving objects and is capable of rendering a free-viewpoint representation at given timestamps. The figures show the flames’ reconstruction with high quality and correct geometry. presence of fast motions. However, harnessing this bene-fit comes with its own challenges, mainly due to (i) the un-known absolute radiance at the event location and (ii) the unknown pose of the camera at the time of the event. The former challenge can be addressed by using a hybrid system of RGB and event cameras. We address the latter challenge of pose determination with a novel method.
Previous methods to deal with the pose of event cameras either do not treat the events to be asynchronous [10, 9] or assume that the event camera’s pose is known at all times [21]. We advocate that the event streams must be treated asynchronously to maximally utilize their temporal precision, in keeping with earlier work [23]. On the other hand, we argue that the assumption of known poses for all asynchronous events is simply impractical. Instead, we as-sume that only the poses of subsequent RGB frames are known. The poses of the events are derived from their as-sociated time stamps, by learning to map time to the evolv-ing camera poses. During this process, the known poses of the RGB frames and the non-rigid deformation prior of the
scene under investigation are jointly utilized.
In this work, we use moving calibrated stereo of RGB and event cameras. Using the known poses of sparse RGB frames only, we want to model the 3D radiance field of de-formable objects. To the best of our knowledge, there are thus far no methods leveraging event cameras to model de-formable neural radiance fields. Therefore, we first estab-lish a baseline method – which we refer to as DE-baseline – inspired by two notable works on deformable NeRF [15] and event-based NeRF [21]. Later, we propose a novel method that significantly improves this baseline. The pro-posed method learns to map the time stamp of an event to a camera pose such that each event’s ray can be backpro-jected to the 3D space, without requiring the continuous pose of the asynchronous events. The main idea of this paper is then to constrain the radiance field using the mea-sured events. To do so, we re-create the events solely from the radiance field. Any error due to mismatches between re-created and measured events is backpropagated to supervise the implicit radiance field represention. This radiance field is augmented by sparse and calibrated RGB image frames.
The major contributions of this paper are as follows,
• We show the benefit of using event cameras to model the deformable neural radiance fields for the first time.
• We develop a novel method that learns the continuous pose of event cameras which is robust also to inaccu-rate RGB poses, exploits a collection of events at once, and performs active sampling to maximally utilize the asynchronous event streams.
• The proposed method significantly outperforms exist-ing methods and our baseline on both realistically ren-dered, but artificial scenes and on real-world datasets. 2.