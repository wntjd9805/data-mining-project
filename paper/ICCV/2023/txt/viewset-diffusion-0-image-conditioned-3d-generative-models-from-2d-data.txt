Abstract
We present Viewset Diffusion, a diffusion-based genera-tor that outputs 3D objects while only using multi-view 2D data for supervision. We note that there exists a one-to-one mapping between viewsets, i.e., collections of several 2D views of an object, and 3D models. Hence, we train a diffusion model to generate viewsets, but design the neural network generator to reconstruct internally corresponding 3D models, thus generating those too. We fit a diffusion model to a large number of viewsets for a given category of objects. The resulting generator can be conditioned on zero, one or more input views. Conditioned on a single view, it performs 3D reconstruction accounting for the ambigu-ity of the task and allowing to sample multiple solutions compatible with the input. The model performs reconstruc-tion efficiently, in a feed-forward manner, and is trained us-ing only rendering losses using as few as three views per viewset. Project page: szymanowiczs.github.io/ viewset-diffusion. 1.

Introduction
Image-based 3D reconstruction, i.e., recovering the 3D shape of the world from 2D observations, is a fundamental problem in computer vision.
In this work, we study the problem of reconstructing the 3D shape and appearance of individual objects from as few as one image. In fact, we cast this as image-conditioned 3D generation, and also consider the case of unconditional generation (fig. 1).
Single-view 3D reconstruction is inherently ambiguous because projecting a 3D scene to an image loses the depth dimension. The goal is thus not to recover the exact 3D shape and appearance of the object, particularly of its oc-cluded parts, but to generate plausible reconstructions. This can only be achieved by learning a prior over the likely 3D shapes and appearances of the objects. Here, we do so for one category of objects at a time.
Leveraging 3D object priors for reconstruction has been explored by several works [11, 48]. Most of these tackle 3D reconstruction in a deterministic manner, outputting one reconstruction per object. This is limiting in the presence of ambiguity, as a deterministic reconstructor can only predict either (1) a single most likely solution, which is plausible but usually incorrect, or (2) an average of all possible re-constructions, which is implausible (fig. 2).
Thus, in this work, we tackle the problem of modelling ambiguity in few-view 3D reconstruction. Our goal is to learn a conditional generator that can sample all plausible 3D reconstructions consistent with a given image of an ob-ject from a given viewpoint.
We approach this problem using Denoising Diffusion
Probabilistic Models (DDPM) [14] due to their excellent performance for image generation [6]. However, while
DDPMs are trained on billions of images, 3D training data is substantially more scarce. We thus seek to learn a 3D
DDPM using only multi-view 2D data for supervision. The challenge is that DDPMs assume that the training data is in the same modality as the generated data. In our setting, the 3D model of the object can be thought of as an unob-served latent variable, learning which is beyond the scope of standard DDPMs. We solve this problem starting from the following important observation.
Given a 3D model of an object, we can render all possi-ble 2D views of it. Likewise, given a sufficiently large set of views of the object, called a viewset, we can recover, up to an equivalence class, the corresponding 3D model. Be-cause of this bijective mapping, generating 3D models is equivalent to generating viewsets. The advantage of the lat-ter is that we often have access to a source of suitable 2D multi-view data for supervision. Similarly to 2D image gen-eration, our corresponding DDPM takes as input a partially noised viewset and produces as output a denoised version of it. For generation, this denoising process is iterated starting from a Gaussian noise sample.
Our second key intuition is that the bijective mapping between viewsets and 3D models can be integrated in the denoising network itself. Namely, our DDPM is designed to denoise the input viewset by reconstructing a full radi-ance field of the corresponding 3D object (see fig. 3). This has the advantage of producing the 3D model we are after and ensuring that the denoised viewset is 3D consistent (the lack of 3D consistency is an issue for some multi-view gen-erators [22, 47]). Furthermore, by allowing different views in the viewset to be affected by different amounts of noise, the same model supports conditional generation from any number of input views (including zero). This conditional generation is achieved by setting the noise level of the avail-able conditioning images to zero.
We call our method Viewset Diffusion and, with it, make several contributions: (i) The idea of generating viewsets as a way to apply DDPMs to the generation of 3D objects even when only multi-view 2D supervision is available. (ii)
An ambiguity-aware 3D reconstruction model that is able to sample different plausible reconstructions given a sin-gle input image, and which doubles as an unconditional 3D generator. (iii) A network architecture that enables our reconstructions to match the conditioning images, aggre-gate information from an arbitrary number of views in an occlusion-aware manner and estimate plausible 3D geome-tries. (iv) A new synthetic benchmark dataset, designed for evaluating the performance of single-image reconstruction techniques in ambiguous settings.
Figure 2: Ambiguities. Under occlusion, deterministic meth-ods blur possible shapes (orange car’s back, Minecraft characters’ poses) and colours (black car’s back, occluded sides of Minecraft characters). Our method samples plausible 3D reconstructions. 2.