Abstract
Image diffusion models, trained on massive image col-lections, have emerged as the most versatile image genera-tor model in terms of quality and diversity. They support inverting real images and conditional (e.g., text) genera-tion, making them attractive for high-quality image editing applications. We investigate how to use such pre-trained image models for text-guided video editing. The critical challenge is to achieve the target edits while still preserving the content of the source video. Our method works in two simple steps: first, we use a pre-trained structure-guided (e.g., depth) image diffusion model to perform text-guided edits on an anchor frame; then, in the key step, we pro-gressively propagate the changes to the future frames via self-attention feature injection to adapt the core denoising step of the diffusion model. We then consolidate the changes by adjusting the latent code for the frame before continuing the process. Our approach is training-free and generalizes to a wide range of edits. We demonstrate the effectiveness of the approach by extensive experimentation and compare it against four different prior and parallel efforts (on ArXiv).
We demonstrate that realistic text-guided video edits are possible, without any compute-intensive preprocessing or https://duyguceylan. video-specific finetuning. github.io/pix2video.github.io/. 1.

Introduction
Diffusion-based algorithms [8, 18, 48] have emerged as the generative model of choice for image creation. They are stable to train (even over huge image collections), produce high-quality results, and support conditional sampling. Ad-ditionally, one can invert [31, 48] a given image into a pre-trained diffusion model and subsequently edit using only textual guidance [15]. Such a generic workflow, to handle real images and interact using semantic text prompts, is an exciting development and opens the door for many down-stream content creation tasks.
However, the same workflow is barely available for videos where the development of video diffusion models
*These authors contributed equally to this work
Figure 1: There has been exciting advancements in large scale image generation models [41]. When applied inde-pendently to a sequence of images (‘per-frame’), however, such methods produce inconsistent results across frames.
Our method uses a pre-trained and fixed image generation model to consistently edit a video clip based on a target text prompt. We show examples of two different edits (‘ours’). is still in its infancy [32, 46, 61]. Not surprisingly, naively applying an image-based workflow to each video frame pro-duces inconsistent results (see Figure 1). Alternately, while it is possible to use a single frame for style guidance and employ video stylization propagation [20], the challenge lies in stylizing new content revealed under changing oc-clusions across frames.
In this paper, we explore the feasibility of editing a video clip using a pre-trained image diffusion model and text in-structions with no additional training. We start by inverting the input video clip and expecting the user to edit, using tex-tual prompts, one of the video frames. The goal is then to consistently propagate the edit across the rest of the video.
The challenge is to balance between respecting the user edit and maintaining the plausibility and temporal coherency of the output video. Image generation models already gener-ate images faithful to the edit prompt. Hence, what remains
challenging is to propagate the edit in a temporally coherent manner.
Temporal coherency requires preserving the appearance across neighboring frames while respecting the motion dy-namics. Leveraging the fact that the spatial features of the self attention layers are influential in determining both the structure and the appearance of the generated images, we propose to inject features obtained from the previously edited frames into the self attention layer of the current frame. This feature injection notably adapts the self atten-tion layers to perform cross-frame attention and enables the generation of images with coherent appearance character-istics. To further improve consistency, we adopt a guided diffusion strategy in which we update the intermediate la-tent codes to enforce similarity to the previous frame before we continue the diffusion process. While the image gener-ation model cannot reason about motion dynamics explic-itly, recent work has shown that generation can be condi-tioned on static structural cues such as depth or segmenta-tion maps [63]. Being disentangled from the appearance, such structural cues provide a path to reason about the mo-tion dynamics. Hence, we utilize a depth-conditioned im-age generation model and use the predicted depth from each frame as additional input.
We term our method Pix2Video and evaluate it on various real video clips demonstrating both local (e.g., changing the attribute of a foreground object) and global (e.g., changing the style) edits. We compare with several state-of-the-art approaches including diffusion-based image editing meth-ods applied per frame [30], patch-based video based styliza-tion methods [20], neural layered representations that facil-itate consistent video editing [3], and concurrent diffusion based video editing methods [58]. We show that Pix2Video is on par with or better than the baselines without requir-ing any compute-intensive preprocessing [3] or any video-specific finetuning [58]. This can be seen in Figure 1 “ours” columns where the appearance of the foreground objects are more consistent across frames than per-frame editing.
In summary, we present a training free approach that uti-lizes pre-trained large scale image generation models for video editing. Our method does not require pre-processing and does not incur any additional overhead during infer-ence stage. This ability to use an existing image genera-tion model paves the way to bring exciting advancements in controlled image editing to videos at no cost. Source code is available at the project page. 2.