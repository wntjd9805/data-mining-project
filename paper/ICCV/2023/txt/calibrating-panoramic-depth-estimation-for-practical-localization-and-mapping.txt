Abstract
The absolute depth values of surrounding environments provide crucial cues for various assistive technologies, such as localization, navigation, and 3D structure estimation.
We propose that accurate depth estimated from panoramic images can serve as a powerful and light-weight input for a wide range of downstream tasks requiring 3D information.
While panoramic images can easily capture the surround-ing context from commodity devices, the estimated depth shares the limitations of conventional image-based depth estimation; the performance deteriorates under large do-main shifts and the absolute values are still ambiguous to infer from 2D observations. By taking advantage of the holistic view, we mitigate such effects in a self-supervised way and fine-tune the network with geometric consistency during the test phase.
Specifically, we construct a 3D point cloud from the current depth prediction and project the point cloud at various viewpoints or apply stretches on the current input image to generate synthetic panoramas.
Then we minimize the discrepancy of the 3D structure esti-mated from synthetic images without collecting additional data. We empirically evaluate our method in robot navi-gation and map-free localization where our method shows large performance enhancements. Our calibration method can therefore widen the applicability under various exter-nal conditions, serving as a key component for practical panorama-based machine vision systems. 1.

Introduction
Acquiring depth maps of the surrounding environment is a crucial step for AR/VR and robotics applications, as the depth maps serve as building blocks for mapping and localization. While dense LiDAR or RGB-D scan-ning [1, 2, 3, 4, 5] has been widely used for depth ac-quisition, the methods are often computationally expen-sive or require costly hardware. Panoramic depth estima-Figure 1: Motivation and overview of our approach. Panoramic perception enables efficient navigation due to the large field of view (top). Nevertheless, the performance drops due to the gaps between the training dataset with upright cameras in medium-sized rooms and the deployment scenarios with limited data and various domain shifts. The proposed solution suggests test-time training using geometric consistency to mitigate the gap (bottom). tion [6, 7, 8, 9, 10, 11, 12, 13], on the other hand, enables quick and cost-effective depth computation.
It outputs a dense depth map from a single neural network inference given only 360◦ camera input, which is becoming more widely accessible [14, 15]. Further, the large field of view
of panoramic depth maps can model the comprehensive 3D context from a single image capture. The holistic view pro-vides ample visual cues for robust localization, and allows efficient 3D mapping. An illustrative example is shown in
Figure 1a, where a robot navigation agent equipped with panorama view observes larger areas and builds more com-prehensive grid map than the agent with perspective view when deployed for the same trajectory.
While existing panoramic depth estimation methods can estimate highly accurate depth maps in trained environ-ments [11, 8, 7, 6], their performances often deteriorate when deployed in unseen environments with large domain gaps. For example, as shown in Figure 1b, depth estimation networks trained on upright panorama images in medium-sized rooms perform poorly in images containing large camera rotation or captured in large rooms. Such scenar-ios are highly common in AR/VR or robotics applications, yet it is infeasible to collect large amounts of densely an-notated ground-truth data for panorama images or perform data augmentations to realistically and thoroughly cover all the possible adversaries. Further, while numerous unsu-pervised domain adaptation methods have been proposed for depth estimation [16, 17, 18, 19], most of them mainly consider sim-to-real gap minimization and require the la-belled training dataset for adaptation which is infeasible for memory-limited applications.
In this paper, we propose a quick and effective calibra-tion method for panoramic depth estimation in challeng-ing environments with large domain shifts. Given a pre-trained depth estimation network, our method applies test-time adaptation [20, 21, 22] on the network solely using ob-jective functions derived from test data. Conceptually, we are treating depth estimation networks as sensors that out-put depth maps from images, which then makes the process similar to ‘calibration’ in depth or LiDAR sensing literature for accurate measurements. Our resulting scheme is flexi-bly applicable in either online or offline manner adaptation.
As shown in Figure 1c, the light-weight training calibrates the network towards making more accurate predictions in the new environment.
Our calibration scheme consists of two key components that effectively utilize the holistic spatial context uniquely provided by panoramas. First of all, our method operates using training objectives that impose geometric consisten-cies from novel view synthesis and panorama stretching.
To elaborate, as shown in Figure 2, we leverage the full-surround 3D structure available from panoramic depth esti-mation and generate synthetic panoramas. The training ob-jectives then minimize the geometric discrepancy between depth estimations from the synthesized panoramas and the original view. Second, we propose light-weight data aug-mentation to cope with offline scenarios where only a lim-ited amount of test-time training data is available. Specif-ically, we augment the test data by applying arbitrary pose shifts or synthetic stretches, similar to the techniques used for the training objectives.
Since our calibration method aims at adapting the net-work during the test phase using geometric consistencies, it is compute and memory efficient while being able to han-dle a wide variety of domain shifts. Our method does not require the computational demands of additional network pre-training [21, 23], or memory to store the original train-ing dataset during adaptation [16, 18, 17, 24]. Nevertheless, our method shows large amounts of performance enhance-ments when tested in challenging domain shifts such as low lighting or room-scale change. Further, due to the light-weight formulation, our method could easily be applied to numerous downstream tasks in localization and mapping.
We experimentally verify that our calibration scheme ef-fectively improves performance in two exemplary tasks, namely map-free localization and robot navigation. To sum-marize, our key contributions are as follows: (i) a novel test-time adaptation method for calibrating panoramic depth es-timation, (ii) a data augmentation technique to handle low-resource adaptation scenarios, and (iii) an effective applica-tion of our calibration method on downstream mapping and localization tasks. 2.