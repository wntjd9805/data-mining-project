Abstract
We propose a novel task for generating 3D dance move-ments that simultaneously incorporate both text and mu-sic modalities. Unlike existing works that generate dance movements using a single modality such as music, our goal is to produce richer dance movements guided by the instruc-tive information provided by the text. However, the lack of paired motion data with both music and text modalities limits the ability to generate dance movements that inte-grate both. To alleviate this challenge, we propose to uti-lize a 3D human motion VQ-VAE to project the motions of the two datasets into a latent space consisting of quantized vectors, which effectively mix the motion tokens from the two datasets with different distributions for training. Ad-ditionally, we propose a cross-modal transformer to inte-grate text instructions into motion generation architecture for generating 3D dance movements without degrading the performance of music-conditioned dance generation. To better evaluate the quality of the generated motion, we in-troduce two novel metrics, namely Motion Prediction Dis-tance (MPD) and Freezing Score (FS), to measure the co-herence and freezing percentage of the generated motion.
Extensive experiments show that our approach can gener-*Equal contribution: gongkehong@u.nus.edu, dongze@nus.edu.sg
â€ Corresponding author: xinchao@nus.edu.sg ate realistic and coherent dance movements conditioned on both text and music while maintaining comparable perfor-mance with the two single modalities. Code is available at https://garfield-kh.github.io/TM2D/. 1.

Introduction
The music-conditioned dance generation has become a topic of great interest in recent years. The ability to gen-erate dance movements that are synchronized with music has numerous applications, such as behavior understanding, simulation, and benefiting the community of dancers and musicians [25, 9, 29, 44]. Although music has been used as a guidance to generate dance movements, another impor-tant modality cue, text (or language), which provides richer actions and more flexible motion guidance, and is studied in other tasks such as image classification [38], detection
[15], segmentation [50], and text-driven image generation
[42], has not been fully explored in dance generation. To this end, we first propose a novel task for generating 3D dance movements that simultaneously incorporate both text and music modalities, enabling the generated human to per-form rich dancing movements in accordance with the music and text.
Designing a system pipeline for this bimodality driven 3D dance generation task is non-trivial. There exist two
i) the existing significant challenges to be considered: datasets only cater to either music-driven (music2dance)
[46, 5, 57, 29] or text-driven (text2motion) [36, 16] hu-man motion generation, and no paired 3D dance generation dataset exists that takes into account both music and text.
While building a new large-scale paired 3D dance dataset based on music and text is possible, it is time-consuming with fully annotated 3D human motion [29]; ii) the inte-gration of text into music-conditioned dance generation re-quires a suitable architecture. However, existing methods that use music as a driving force to generate dance move-ments might result in temporal-freezing frames or fail to generalize to in-the-wild scenarios [29, 44]. Therefore, sim-ple integration of text into the existing music-conditioned architecture might pose a risk of degraded dance generation quality in our new task.
To address the first challenge, we take advantage of ex-isting music-dance and text-motion datasets for this new task. However, directly mixing motions from these two datasets would result in inferior performance since the mo-tions from these two datasets are in completely different motion spaces. To overcome this, we propose to utilize a
VQ-VAE architecture to project the motions into a consis-tent and shared latent space. In particular, we build up a shared codebook for all the motions from the training set, and motions from both datasets are now represented as dis-crete tokens that are implicitly constrained to fall into a shared latent space. For the second challenge, we propose to utilize a cross-modal transformer architecture that for-mulates both music2dance and text2motion as sequence-to-sequence translation tasks. This architecture directly trans-lates audio and text features into motion tokens and enables bimodality driven ability by introducing a fusion strategy in the latent space with a shared motion decoder for both tasks.
With the shared decoder, audio and text information can be efficiently fused during inference. Our entire cross-modal transformer architecture is both effective and efficient, al-lowing for the integration of text instructions to generate coherent 3D dance motions, as illustrated in Figure 1.
To better evaluate the coherence of generated dance in our task, we propose a new evaluation metric, Motion Pre-diction Distance (MPD), which measures the distance be-tween the predicted motion and the ground truth at the time of integrating text, thereby providing a more accurate eval-uation of the coherence of frames. Additionally, we in-troduce a Freezing Score (FS) that quantifies the percent-age of temporal freezing frames in dance generation, which is a common problem in music-conditioned dance gener-ation. To better evaluate the performance of our method in real-world scenarios, we also collect some in-the-wild music data for evaluation. Our method successfully per-forms dance generation based on both text and music while maintaining comparable performance on the single modal-ity tasks (music2dance, text2motion) compared to other state-of-the-art methods.
In summary, our contributions are as follows: i) We pro-pose an interesting task of utilizing both music and text for 3D dance generation and propose a pipeline named TM2D (Text-Music to Dance) for this task. ii) Rather than collect-ing a new training set, we effectively combine the existing music2dance and text2motion datasets and employ a VQ-VAE framework to encode motions from all training sets to a shared feature space. iii) We propose a cross-modal transformer as well as a bimodal feature fusion strategy to encode both audio and text features, which is both effec-tive and efficient. iv) We propose two new metrics, MPD and FS, which efficiently reflect the quality of generated motion. v) We successfully generate realistic and coher-ent dance based on both music and text instructions while maintaining comparable performance on the single modal-ity tasks (music2dance, text2motion). 2.