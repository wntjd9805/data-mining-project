Abstract
This paper aims for high-performance offline LiDAR-based 3D object detection. We first observe that expe-rienced human annotators annotate objects from a track-centric perspective. They first label objects in a track with clear shapes, and then leverage the temporal coherence to infer the annotations of obscure objects. Drawing inspira-tion from this, we propose a high-performance offline de-tector in a track-centric perspective instead of the conven-tional object-centric perspective. Our method features a bidirectional tracking module and a track-centric learning module. Such design allows our detector to infer and re-fine a complete track once the object is detected at a cer-tain moment. We refer this characteristic to “onCe de-tecTed, neveR Lost” and name the proposed system CTRL.
Extensive experiments demonstrate the remarkable perfor-mance of our method, surpassing the human-level annotat-ing accuracy and previous state-of-the-art methods in the highly competitive Waymo Open Dataset leaderboard with-out model ensemble. The code is available at https:
//github.com/tusen-ai/SST. 1.

Introduction
As a fundamental task in autonomous driving, 3D object detection has made great progress in recent years, in both
LiDAR-based detectors [44, 17, 49, 31] and image-based detectors [40, 15, 20, 21]. Such success is primarily at-tributed to the data-driven paradigm, which requires a mas-sive amount of labeled data. As a result, there is growing interest in developing a high-performance offline detector for auto-labeling.
To achieve a high-performance offline detector, we first
Figure 1: Our method consistently outperforms human per-formance in Waymo Open Dataset. Human performance is reported by 3DAL [30]. We normalize the AP of human performance to 100 for better visualization. analyze the behavior of human annotators in a standard se-quence labeling process. We find that humans annotate ob-jects from a track-centric perspective, meaning that they uti-lize the temporal motion cues of objects to achieve precise labeling. In particular, annotators first label easy samples in a sequence, and then use temporal cues to propagate these high-confidence labels to other time steps, when the ob-jects may contain very few points and are hard to be ac-curately labeled individually. In this process, once an ob-ject is detected in a certain moment, human annotators con-tinuously track that object and predict its movements over time, even if there are some periods when the object is tem-porarily invisible. An important fact is exploited here: a detected object will not disappear unless it moves outside the perception range. Inspired by human labeling behav-ior, we develop a track-centric offline detector that aims to achieve two objectives: 1) Accurate labeling of well-observed tracks. 2) For tracks containing only a few high-quality frames, our detector propagates the predictions in high-quality frames to low-quality frames. We refer to this key property of the detector as “onCe detecTed, neveR Lost” and accordingly name our method CTRL.
In order to implement our vision, we first utilize a base detector to obtain detection results. Then we propose a bidi-rectional tracking module. During the tracking process, a simple motion model is adopted to fill in missing frames and bidirectionally extend the track, greatly extending the life cycle of the track. The bidirectional tracking offers the detector an opportunity to catch obscure objects which might be totally ignored by the base detector. However, such a heuristic frame filling or extension cannot obtain precise poses of objects. Therefore, we develop a track-centric learning module for refinement. In the design of this module, we follow a track-centric principle: tracks, rather than objects, should be regarded as first-class citizens in the workflow. This module takes the all points and all proposals in the entire track as input, and refines all their poses simul-taneously. In addition, after the refinement, we could op-tionally optimize the track coherence via a temporal coher-ence optimization module if needed. Particularly, we manu-ally specify a bounding box containing a clear object shape in a track, and then align other object shapes with the spec-ified one by point cloud registration[1, 7]. We summarize our contributions as follows.
• Based on the behavior of human annotators, we propose an offline detection system CTRL, following the philos-ophy of “track-centric” and “once detected, never lost”.
CTRL boosts the performance of auto-labeling.
• Single-model CTRL outperforms the previous state-of-the-art offline detector and all the online detectors. It is worth emphasizing that among millions of vehicles, only 0.48% of them would be completely missed by CTRL.
• We carefully relabel some diverged cases between our predictions and official ground truths. Our results demon-strate that our method even surpasses the ground-truth ac-curacy provided by Waymo human annotators in those cases.
• We keep our methodology simple and clean, greatly sim-plifying the workflow and reducing the resource require-ments of existing offline frameworks. 2.