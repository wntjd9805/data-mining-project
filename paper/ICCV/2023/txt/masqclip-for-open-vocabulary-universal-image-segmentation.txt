Abstract
We present a new method for open-vocabulary univer-sal image segmentation, which is capable of performing in-stance, semantic, and panoptic segmentation under a uni-fied framework. Our approach, called MasQCLIP, seam-lessly integrates with a pre-trained CLIP model by utilizing its dense features, thereby circumventing the need for exten-sive parameter training. MasQCLIP emphasizes two new aspects when building an image segmentation method with a CLIP model: 1) a student-teacher module to deal with masks of the novel (unseen) classes by distilling informa-tion from the base (seen) classes; 2) a fine-tuning process to update model parameters for the queries Q within the
CLIP model. Thanks to these two simple and intuitive de-signs, MasQCLIP is able to achieve state-of-the-art perfor-mances with a substantial gain over the competing methods by a large margin across all three tasks, including open-vocabulary instance, semantic, and panoptic segmentation.
Project page is at https://masqclip.github.io/. 1.

Introduction
By being universal and open-world, the traditional im-age segmentation methods that have been mainly trained in a supervised manner can be made less specific and more powerful. When an image segmentation method is open-world, it means that it can handle new concepts with novel classes or categories that are not labeled in its supervised training data. However, this is not something that can be achieved by the algorithm alone, as information about the novel classes that go beyond supervised labels cannot be generated out of thin air. Fortunately, the CLIP models [31] are trained on millions of image-text pairs, which provides a rich source of combinatorial object/scene information by mapping images and texts into the same semantic space.
*Equal Contribution.
Figure 1: We present the visualization and quantitative results of MasQCLIP on open-vocabulary segmentation with a universal model architecture. As shown, we achieve state-of-the-art perfor-mances across three segmentation tasks.
As CLIP models are trained contrastively at the global image-text level, they do not directly output segmentation maps at the pixel or region level. Recent works [9, 13, 55] demonstrate the feasibility of adopting and locking a pre-trained CLIP model for open-vocabulary image segmenta-tion by adopting a two-stage approach, which first generates class-agnostic mask proposals and subsequently classifies each mask region based on a CLIP model.
This decoupled two-stage design, involving localiza-tion and then classification, is considered advantageous for open-vocabulary universal segmentation. First, it does not depend on any task specificity. Recent works [7, 8] have at-tempted to unify instance, semantic, and panoptic segmen-tation through Transformer-based architectures [6], and the
CLIP-based region classification module itself is directly applicable to different types of masks (instance/semantic).
Second, it is a natural choice to utilize CLIP models ex-clusively for object/scene classification rather than localiza-tion. The reason is that the dense features within a CLIP visual encoder only carry semantic information, but can-not discriminate between different objects belonging to the same category.
However, recent works [9, 20] still exhibit limitations with regard to being open-world. Specifically, the class-agnostic mask proposal network is trained only on the su-pervision of a limited set of base classes, which restricts its ability to generate mask proposals beyond supervision.
Hence, the model performance is hindered when dealing with novel or unseen categories. In addition, although pre-vious works have attempted to introduce some additional modules for mask classification, they fail to fully bridge the gap between image-level and region-level representa-tion, thus lacking in adaptation to mask classification. The balance between maintaining generalization for more cat-egories and adapting CLIP models for mask classification tasks needs further exploration.
In this paper, we aim to conquer these weaknesses. We introduce a new approach called MasQCLIP that can per-form open-vocabulary instance, semantic, and panoptic seg-mentation under a single framework. To accomplish this,
MasQCLIP uses a similar two-stage model design: 1) a mask generator stage that extracts object/scene masks, and 2) an encoder-only module that performs mask classifica-tion. We are also inspired by the Mask Class Token strategy from MaskCLIP [9], which is tightly integrated with a given
CLIP model. MasQCLIP is different from MaskCLIP [9] in two key areas: 1) the use of a student-teacher self-training module that significantly enhances the generation of novel class masks, and 2) fine-tuning the query parameters in the
CLIP visual encoder, which better adapts the CLIP mod-els for mask region representations. The following are the contributions of our work:
• We develop MasQCLIP for open-vocabulary univer-sal image segmentation that demonstrates substantial performance improvement over the current state-of-the-art methods by a large margin across all three tasks including open-vocabulary instance, semantic, and panoptic segmentation.
• We design a progressive distillation process to generate more novel mask proposals beyond supervision, thus taking a step forward toward open-world mask gener-ation.
• We propose a parameter-efficient fine-tuning strategy,
MasQ-Tuning, which only tunes the query parameters.
When coupled with Mask Class Tokens, MasQ-Tuning is able to preserve the generalization of a pre-trained image-level CLIP model while greatly enhancing its adaptation for segmentation tasks. 2.