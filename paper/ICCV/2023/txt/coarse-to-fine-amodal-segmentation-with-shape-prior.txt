Abstract
Amodal object segmentation is a challenging task that involves segmenting both visible and occluded parts of
In this paper, we propose a novel approach, an object. called Coarse-to-Fine Segmentation (C2F-Seg), that ad-dresses this problem by progressively modeling the amodal segmentation. C2F-Seg initially reduces the learning space from the pixel-level image space to the vector-quantized la-tent space. This enables us to better handle long-range dependencies and learn a coarse-grained amodal segment from visual features and visible segments. However, this latent space lacks detailed information about the object, which makes it difficult to provide a precise segmentation directly. To address this issue, we propose a convolu-tion refine module to inject fine-grained information and provide a more precise amodal object segmentation based on visual features and coarse-predicted segmentation. To help the studies of amodal object segmentation, we cre-ate a synthetic amodal dataset, named as MOViD-Amodal (MOViD-A), which can be used for both image and video amodal object segmentation. We extensively evaluate our model on two benchmark datasets: KINS and COCO-A.
Our empirical results demonstrate the superiority of C2F-Seg. Moreover, we exhibit the potential of our approach for video amodal object segmentation tasks on FISHBOWL and our proposed MOViD-A. Project page at: https:
//jianxgao.github.io/C2F-Seg. 1.

Introduction
Amodal instance segmentation [24] aims to extract com-plete shapes of objects in an image, including both visible and occluded parts. This task plays a vital role in var-ious real-world applications such as autonomous driving
[25, 10], robotics [4], and augmented reality [23, 20]. For instance, in autonomous driving, partial understanding of the scene may result in unsafe driving decisions.
†: Co-corresponding authors.
Figure 1. Visualization of predicted amodal masks in KINS and
COCOA by C2F-Seg. Images in the top row are from COCOA, while the others are from KINS.
Typically, existing approaches [21, 29, 32, 13, 9] build new modules on the detection framework, by additionally introducing an amodal branch that predicts the complete mask perception of the target object. The central idea lies in imbibing a holistic understanding of shape (i.e., shape prior) through multi-task learning by harnessing the super-vised signals of the visible and full regions. While these ap-proaches have yielded promising outcomes in recent years, the task of amodal segmentation remains fraught with chal-lenges. One of the main challenges of amodal segmenta-tion is that it is an ill-posed problem, meaning that there are many non-unique and reasonable possibilities for per-ceiving occluded areas, particularly for elastic bodies like people and animals. On the other hand, there are intricate categories and shapes of objects in real-world scenarios, which would pose significant challenges to prior learning of shapes.
In this paper, we advocate that shape priors are essen-tial for amodal segmentation, since the shape of an object is usually determined by its function, physiology, and char-acteristics. For example, a carrot has a long shape, while an apple has a round shape. Thus, the potential distribution of this object can be learned via neural networks. Never-theless, we argue that while shape prior can only provide a basic outline and may not capture individual differences or highly local information. Meanwhile, it is possible for the shape prior to being inconsistent with the observed vis-ible area due to factors like pose and viewpoint. To this end, we in this paper propose to generate amodal segments progressively via a coarse-to-fine manner. Specifically, we divide the segmentation of amodal object into two phases: a coarse segmentation phase where we use the shape prior to generate a plausible amodal mask, and a refinement phase is adopted to refine the coarse amodal mask to get the precise segmentation.
In the coarse segmentation phase, as we only need to provide a coarse mask, we perform the segmentation in a low-dimension vector-quantized latent space to reduce the learning difficulty and accelerate the inference process.
The segmentation in such latent space is resorted to the popular mask prediction task adopted in BERT [15] and
MaskGIT [3]. Specifically, we adopt a transformer model which takes as inputs the ResNet visual feature, the vector-quantized visible segments, and the ground-truth amodal segments masked in a high ratio. Then the transformer is trained to reconstruct the masked tokens of the amodal seg-ments. This mask-and-predict procedure [3] leads to natu-ral sequential decoding in the inference time. Starting with an all-mask token sequence of amodal segments, our trans-former gradually completes the amodal segments. Each step increasingly preserves the most confident prediction.
In the second refinement phase, our model learns to in-ject details to the coarse-prediction and provide a more pre-cise amodal object segmentation. Our convolutional refine-ment module takes as inputs the coarse-predicted segments
Imitating the human activity for and the visual features. visual stimulus, we construct a semantic-inspired attention module as an initial stimulus, and then gradually inject the visual features to the segments through convolution layers.
With this coarse-to-fine architecture design, our C2F-Seg complements the latent space that is easier-to-learn, the transformer that has superiority of long-range dependency, and the convolutional model that can supplement details, and results in a better amodal object segmentation. Our framework is flexible to generalize to video-based amodal object segmentation tasks. Guided by the shape prior and visual features of related frames, our model can generate precise amodal segments, and is even capable of generat-ing amodal segments when the object is totally invisible, as shown in Figure 7.
In order to evaluate the performance of our C2F-Seg. We conduct experiments both on image and video amodal seg-mentation benchmarks. For image amodal segmentation, our model reaches 36.5/36.6 on AP, 82.22/80.27 on full mIoU and 53.60/27.71 on occluded mIoU for KINS and
COCOA respectively. For video amodal segmentation, our model reaches 91.68/71.30 on full mIoU and 81.21/36.04 on occluded mIoU for FISHBOWL and MOViD-A respec-tively. C2F-Seg outperforms all the baselines and achieves state-of-the-art performance.
Our contributions can be summarized as:
• We propose a novel coarse-to-fine framework, which con-sists of a mask-and-predict transformer module for coarse masks and a convolutional refinement module for refined masks. It imitates human activity and progressively gen-erates amodal segmentation, mitigating the effect of detri-mental and ill-posed shape priors.
• We build a synthetic dataset MOViD-A for amodal seg-mentation, which contains 838 videos and 12,299 objects.
We hope it will advance research in this field. We release the dataset on our project page.
• Extensive experiments are conducted on two image-based benchmarks, showing the superiority of our methods over other competitors. Moreover, our framework can be eas-ily extended to video-based amodal segmentation, achiev-ing state-of-the-art performance on two benchmarks. 2.