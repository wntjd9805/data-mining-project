Abstract
Recent few-shot video classification (FSVC) works achieve promising performance by capturing similarity across support and query samples with different temporal alignment strategies or learning discriminative features via
Transformer block within each episode. However, they ig-nore two important issues: a) It is difficult to capture rich intrinsic action semantics from a limited number of sup-port instances within each task. b) Redundant or irrelevant frames in videos easily weaken the positive influence of dis-criminative frames. To address these two issues, this pa-per proposes a novel Representation Fusion and Promotion
Learning (RFPL) mechanism with two sub-modules: meta-action learning (MAL) and reinforced image representation (RIR). Concretely, during training stage, we perform online learning for seeking a task-shared meta-action bank to en-rich task-specific action representation by injecting global knowledge. Besides, we exploit reinforcement learning to obtain the importance of each frame and refine the repre-sentation. This operation maximizes the contribution of dis-criminative frames to further capture the similarity of sup-port and query samples from the same category. Our RFPL framework is highly flexible that it can be integrated with many existing FSVC methods. Extensive experiments show that RFPL significantly enhances the performance of exist-ing FSVC models when integrated with them. 1.

Introduction
Due to the emergence of deep learning [43, 18, 15], video action recognition has obtained impressive improve-ment by learning informative semantics from raw videos
[23, 38]. These achievements heavily rely on the powerful supervision of considerable well-labeled videos to optimize deep neural networks [32, 41]. However, the collection and annotation of training samples become laborious and expen-sive, especially for video sequences. Hence, such a learning
*Work done during the internship at NEC Laboratories America. paradigm does not always feasible for many practical appli-cations [7, 29]. This conflict motivates investigations on few-shot video classification (FSVC) which aims to learn a model of good generalization performance from a few la-beled video samples [4, 35, 37].
The mainstream solutions to FSVC typically adopt metric-based meta-learning [30, 12] fashion by training model across multiple episodes where each includes one support set with a few annotated videos and the other query set with unlabeled instances. For inference, the well-trained model measures the similarities across support and query videos within each episode to determine which categories the query samples come from [17]. However, it is difficult to precisely assess frame-wise relationships within video sequences, since their temporal information has significant divergence or even mismatch. To overcome such a chal-lenge, the intuitive strategy is to achieve temporal align-ment among cross-video frames. For example, OTAM [4] develops a variant of dynamic time wrapping (DTW) [27] to find the optimal alignment path via the frame-level cu-mulative distance function. Similarly, ITANet [45] explores implicit temporal alignment via a traversal strategy. More-over, TRX [28] develops multiple tuples of sub-sequence to achieve action matching. And STRM [35] studies self-attention mechanisms to emphasize channel representation in a single video. HyRSM [37] presents a set matching con-cept to explore temporal alignment without frame-level or-dering across various videos.
Although these existing works have achieved appealing results, they tend to encounter performance bottlenecks due to ignorance of the following two issues. First, the lim-ited number of labeled samples within each individual task hardly provides sufficient support to assist model learning the essential action semantics for the recognition task. Sec-ond, there are redundant or irrelevant frames in videos; treating these less informative frames in the same way as the other informative ones likely weakens the discriminabil-ity of the learned video representations, and thus triggers a negative effect on solving the FSVC task.
In this paper, we propose a novel Representation Fusion
and Promotion Learning (RFPL) framework to address the two issues, by two novel modules, respectively. To address the deficiency of semantic information within each individ-ual task, we propose the meta-action learning (MAL) mod-ule which introduces external knowledge learned globally to enrich video representations learned locally within each individual task. Specifically, MAL maintains a global meta-action bank storing representations of atomic action snip-pets that can be used to compose various action categories of higher complexity. The global meta-action bank is shared by all action types and all individual FSVC tasks. We ex-plore it to enrich the video feature representations learned locally in each individual task via a novel Single Value De-composition (SVD) technique.
To address the redundant/irrelevant frame issue, we pro-pose the reinforced image representation (RIR) module which explores reinforcement learning to discover the im-portance of each frame to the FSVC task. We train the rein-forcement learning agent by exploiting the relationship be-tween support and query video pairs, striving to emphasize the influence of discriminative frames to help the model ac-curately capture sample-wise similarity. Our MAL and RIR can be easily plugged into the existing FSVC methods to promote their performance. Our main contributions in this work are summarized in three folds as:
• We propose the meta-action learning module which learns a global meta-action bank to enrich video rep-resentation learned locally in individual tasks, via a novel SVD technique.
• We address the negative effect of task-irrelevant frames on capturing cross-video relationships and de-velop a reinforced image representation module that promotes the contribution of discriminative frames.
• Our RFPL framework is highly flexible such that it can be integrated with various FSVC methods and gets im-proved performance. 2.