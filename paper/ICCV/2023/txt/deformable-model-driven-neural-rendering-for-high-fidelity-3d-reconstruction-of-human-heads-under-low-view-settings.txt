Abstract
Reconstructing 3D human heads in low-view settings presents technical challenges, mainly due to the pronounced risk of overﬁtting with limited views and high-frequency sig-nals. To address this, we propose geometry decomposition and adopt a two-stage, coarse-to-ﬁne training strategy, al-lowing for progressively capturing high-frequency geomet-ric details. We represent 3D human heads using the zero level-set of a combined signed distance ﬁeld, comprising a smooth template, a non-rigid deformation, and a high-frequency displacement ﬁeld. The template captures fea-tures that are independent of both identity and expression and is co-trained with the deformation network across mul-tiple individuals with sparse and randomly selected views.
The displacement ﬁeld, capturing individual-speciﬁc de-tails, undergoes separate training for each person. Our network training does not require 3D supervision or object masks. Experimental results demonstrate the effectiveness and robustness of our geometry decomposition and two-stage training strategy. Our method outperforms existing neural rendering approaches in terms of reconstruction ac-curacy and novel view synthesis under low-view settings.
Moreover, the pre-trained template serves a good initializa-tion for our model when encountering unseen individuals. 1.

Introduction
Accurately modeling and rendering human heads is cru-cial for various digital human-related applications as the head is one of the most distinguishing features that helps us identify individuals. Neural implicit functions [22, 34, 37, 35, 9, 46, 4] have recently emerged as a promising technique for synthesizing novel views of complex objects, includ-ing human heads, by learning a continuous function from multi-view images without being tied to a speciﬁc resolu-*Project page: https://github.com/xubaixinxbx/3dheads.
†Corresponding author: Y. He (yhe@ntu.edu.sg). tion. However, training such deep learning models often requires a large number of images as input, which can be costly and computationally inefﬁcient. Moreover, a single implicit ﬁeld may not generalize well to unseen heads, par-ticularly under setting of low or sparse views.
This paper aims to develop a robust method for learn-ing neural implicit functions that can accurately reconstruct 3D human heads with high-ﬁdelity geometry and appear-ance from low-view inputs, thereby reducing the need of extensive data collection and annotation. To achieve this goal, we learn a signed distance ﬁeld (SDF) that con-sists of a smooth template, a non-rigid deformation, and a high-frequency displacement map. The template represents identity-independent and expression-neutral features, while the deformation and displacement maps encode identity-dependent geometric details that are trained for each spe-ciﬁc individual. We represent 3D human heads as the zero level-set of the composed SDF. Our training involves two stages. The ﬁrst stage takes the whole set of persons as input, and learns an identity-independent and expression-neutral template head and a non-rigid deformation between each observed head and the template head. In the second stage, we learn an identity-dependent displacement map for further reﬁning the geometry. To train the proposed SDF without any 3D supervision, we adopt a volume rendering scheme [22, 37, 34] to minimize the difference between the rendered colors and the ground-truth colors. We also adopt regularization terms for the SDF, the deformation, the dis-placement map and the latent codes.
We evaluate our approach on both senior and young per-sons and demonstrate that it is robust and effective in learn-ing SDFs for both types of inputs, resulting in 3D surfaces with high-ﬁdelity geometry. Our experiments show that our method outperforms existing methods in terms of recon-struction accuracy and visual quality. We also demonstrate that our method can generalize to unseen identities by using the pre-trained template as a good initialization.
Input 3D head
Training views
Novel views
Figure 1. An example of a 3D head reconstructed by our method using 10 input views. Refer also to the supplementary material for a video demonstration. 2.