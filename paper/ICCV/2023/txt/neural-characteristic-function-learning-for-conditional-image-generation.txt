Abstract
The emergence of conditional generative adversarial net-works (cGANs) has revolutionised the way we approach and control the generation, by means of adversarially learning joint distributions of data and auxiliary information. De-spite the success, cGANs have been consistently put under scrutiny due to their ill-posed discrepancy measure between distributions, leading to mode collapse and instability prob-lems in training. To address this issue, we propose a novel conditional characteristic function generative adversarial network (CCF-GAN) to reduce the discrepancy by the char-acteristic functions (CFs), which is able to learn accurate distance measure of joint distributions under theoretical soundness. More specifically, the difference between CFs is first proved to be complete and optimisation-friendly, for measuring the discrepancy of two joint distributions. To relieve the problem of curse of dimensionality in calculating
CF difference, we propose to employ the neural network, namely neural CF (NCF), to efficiently minimise an upper bound of the difference. Based on the NCF, we establish the
CCF-GAN framework to explicitly decompose CFs of joint distributions, which allows for learning the data distribution and auxiliary information with classified importance. The experimental results on synthetic and real-world datasets verify the superior performances of our CCF-GAN, on both the generation quality and stability. 1.

Introduction
Generative adversarial network (GAN) has been the workhorse in deep generative models since its birth for image generation [16], and its popularity arises from the capability of generating clear and realistic images from merely small di-mensions. Despite success, the original architecture of GAN only allows for randomly generating images from Gaussian
*The corresponding author of this paper is Mai Xu, with email:
MaiXu@buaa.edu.cn. noise, and an important variant of GANs aims to control the generation by pre-defined auxiliary information (e.g., the class labels or texts), constituting the conditional GAN (cGAN). Taking advantages of the auxiliary information, cGANs have been proved to be capable of enhancing the realistic image generation that is conditioned on extra se-mantic cues [42, 32, 33]. Therefore, the past few years have witnessed the extensive applications of cGANs, including class-conditioned generation [31, 37], style transfer [55], text-to-image translation [42, 51], to name but a few.
Generally speaking, cGANs establish a joint distribu-tion between data X and auxiliary information Y, i.e.,
{X , Y} ∼ p(x, y). Most cGANs agreed on the design of the generator network, in which the auxiliary information is embedded to the input noise [31] or the inter-mediate lay-ers of the generator [11, 37, 9, 50, 40, 4, 33]. As such, the generator aims to sample from the joint distribution p(x, y).
On the other hand, for designing the discriminator, the way we formulate the conditional distribution tells the existing cGANs apart, because p(x, y) can be formulated by either p(x|y)p(y) or p(y|x)p(x). The former calls for transform-ing the auxiliary information Y into the discriminator so as to predict p(x|y), and this can be achieved by concatenating with X as input [31, 10, 44], or embedding Y to hidden layers of the discriminator [42, 51]. The latter, however, requires the discriminator to predict the auxiliary informa-tion p(y|x), by for example, additional explicit classifiers
[37, 15, 21] or implicit projections [33, 50, 32, 4]. Despite being able to control the generation by pre-defined auxil-iary cues, applying cGANs in practice has been significantly restricted owing to their mode collapse [48, 52, 23] and in-stability [37, 33] problems in training, thus impeding the consistent improvement in the realistic image generation.
Indeed, most discriminators of cGANs build upon the cross-entropy adversarial loss, with an equivalence to the
Jensen-Shannon (JS) divergence between generated and real data distributions [2]. Unfortunately, it has been verified both theoretically and empirically that the JS divergence,
which compares two distributions in a “bin-to-bin” man-ner [25], can easily max out when the two distributions are mis-aligned or supported by low dimensions [2]. Conse-quently, there exists an issue of gradient vanishing in the discriminator, which misleads the generator to simply learn fixed patterns or completely break down in training [2, 3].
For unconditional generation, this issue has been elegantly addressed by introducing a broad class of distance metrics called integral probability metric (IPM) [35]. Under the umbrella of the theoretical completeness of IPMs, the dis-criminator operates as certain bounded functions to compare distributions in a “cross-bin” style [25], such that smooth and sufficient gradient can be provided for unconditional generation.
Therefore, it is intuitive to apply IPMs to conditional generation, benefiting from the theoretical completeness of IPMs to stably and consistently improve the generation.
However, it is non-trivial to design an IPM-cGAN, due to the non-linear coupling between the data and auxiliary in-formation. In other words, the bounded function of the dis-criminator prohibits explicitly modelling p(x|y) or p(y|x) for conditional generation. Several attempts were proposed to concatenate X and Y as an augmented random variable (cid:98)X , and equivalently train the cGAN by an unconditional
IPM-GAN [54]. However, it is problematic to straightfor-wardly combine two random variables at different seman-tic levels, whereby its deficiency has been proved in many cGANs [28, 33]. Although several cGANs employed certain
IPMs, e.g., the Wasserstain distance in their implementations
[34, 44, 33], their very basic theories were established upon the cross-entropy form (equivalent to the JS divergence), thus still suffering from the mode collapse and instability problems caused by the “bin-to-bin” comparison. More importantly, the above cGANs are established upon the ex-istence of probability density functions (pdfs) of random variables. This premise, oftentimes taken for granted with-out verification, may not hold in practice, especially when real-world data such as images and videos essentially reside on low-dimensional manifolds [24, 36].
In this paper, we propose a novel cGAN architecture upon the characteristic function (CF) of random variables, i.e., conditional characteristic function GAN (CCF-GAN).
We also noticed several works [1, 30] built upon the CF to achieve enhanced unconditional generation. Those methods, however, by first embedding the data distributions into latent spaces, are problematic in learning joint distributions of the data and auxiliary information in the embedded spaces. In contrast, this paper explicitly establishes the CFs for both generated and real joint distributions. By inspecting that the
CF always exists and uniquely corresponds to one distribu-tion, we propose to calculate the difference between CFs as a vehicle to indicate the discrepancy of joint distributions.
However, the calculation of CFs requires excessively sam-pling in the complex domain, which is prohibitive to learn distributions of images that reside in high dimensions. We thus develop the neural network as a proxy to calculate an upper bound of the CF difference, called neural CF (NCF) metric. Based on the NCF, we establish the CCF-GAN by explicitly modelling the conditional distribution from the joint distribution, allowing for a classified treatment on the image and auxiliary information at different semantic levels.
Consequently, the superior performances of our CCF-GAN are verified on both synthetic and real-world datasets. 2.