Abstract
Point cloud registration is a task to estimate the rigid transformation between two unaligned scans, which plays an important role in many computer vision applications.
Previous learning-based works commonly focus on super-vised registration, which have limitations in practice. Re-cently, with the advance of inexpensive RGB-D sensors, sev-eral learning-based works utilize RGB-D data to achieve unsupervised registration. However, most of existing unsu-pervised methods follow a cascaded design or fuse RGB-D data in a unidirectional manner, which do not fully ex-ploit the complementary information in the RGB-D data. To leverage the complementary information more effectively, we propose a network implementing multi-scale bidirec-tional fusion between RGB images and point clouds gen-erated from depth images. By bidirectionally fusing vi-sual and geometric features in multi-scales, more distinc-tive deep features for correspondence estimation can be ob-tained, making our registration more accurate. Extensive experiments on ScanNet and 3DMatch demonstrate that our method achieves new state-of-the-art performance. Code will be released at https://github.com/phdymz/
PointMBF. 1.

Introduction
Point cloud registration [28] aims at aligning partial views of the same scene, which is a critical component of many computer vision tasks. Commonly, point cloud regis-tration starts from feature extraction [55, 10] and correspon-dence estimation [46, 43], followed by robust geometric fit-ting [16, 37, 3, 72]. Among them, feature extraction plays a vital role in point cloud registration, as distinctive fea-*Equal contribution.
†Corresponding author. tures can reduce the occurrence of outlier correspondences, thereby saving time on robust geometric fitting.
Many traditional methods rely on hand-crafted features
[55, 34], but they commonly show limited performance.
Benefiting from the rapid progress of deep learning, many learning-based features [10, 64, 25] have been proposed in recent years. Compared to hand-crafted features, they are distinctive enough to achieve robust performance in many challenging conditions such as low overlap. However, most deep learning-based features need supervision on poses or correspondences, which limits their practical applications.
For unannotated datasets with different distributions from the training set, they tend to suffer from performance degra-dation.
With the recent advance of inexpensive RGB-D sensors, it has become easier to simultaneously acquire both depth information and RGB images, which inspires unsupervised point cloud registration using additional color information.
UR&R [14] proposed a framework for unsupervised RGB-D point cloud registration. It utilizes a differentiable ren-derer to generate the projections of the transformed point clouds and calculates geometric and photometric losses be-tween the projections and the registration targets. Based on these losses, UR&R can train its deep descriptor without an-notations and achieve robust registration on RGB-D video.
Similar to UR&R, BYOC [15] proposed a teacher-student framework for unsupervised point cloud registration for
RGB-D data, which also shows competitive performance.
However, all these RGB-D-based methods use RGB images and depth information separately and do not further exploit the complementary information within RGB-D data. Re-cently, LLT [67] first utilized a linear transformer [35, 59] to fuse these complementary information and achieved new state-of-the-art performance. However, LLT focuses on us-ing depth information to guide RGB information and ne-glects the interaction between the two modalities, which hinders better performance.
To fully leverage these complementary modalities, we propose a multi-scale bidirectional fusion network named
PointMBF for unsupervised RGB-D point cloud registra-tion, which fuses visual and geometric information bidirec-tionally at both low and high levels. In this work, we pro-cess depth images in the form of point clouds and utilize two network branches for RGB images and point clouds, re-spectively. Both branches follow the U-Shape [54] structure to extract features for information fusion in multiple scales.
Unlike the fusion strategy in LLT [67], we perform cross-modalities fusion in all stages rather than only in the last few layers, making fused features more distinctive. Moreover, different from the unidirectional fusion strategy in LLT, we adopt a bidirectional design for more effective fusion.
Specifically, in each scale, we first find the regional cor-responding points/pixels for each query pixel/point. Then we sample the KNN points/pixels among them and gather their features to a set. The feature set is fed to a PointNet-style module to achieve permutation-invariant aggregation.
Finally, the information communication between different modalities can be achieved by fusing the aggregated fea-tures with the query feature using a shallow neural network with residue design.
To evaluate our method, we conduct experiments on two popular indoor RGB-D datasets, ScanNet [11] and 3DMatch [73]. Our PointMBF not only achieves new state-of-the-art performance but also shows competitive general-ization across different datasets. When tested on an unseen dataset ScanNet, our PointMBF trained on 3DMatch still shows comparable performance to recent advanced methods directly trained on ScanNet. We also conduct comprehen-sive ablation studies to further demonstrate the effectiveness of each component of our multi-scale bidirectional design.
To summarize, our contributions are as follows:
• We propose a multi-scale bidirectional fusion network for RGB-D point cloud registration, which fully lever-ages the information in the two complementary modal-ities. Compared to unidirectional fusion or fusion in the final stage, our fusion strategy can achieve the in-formation communication more effectively, so that it can generate more distinctive features for registration.
• We introduce a simple but effective module for bidi-rectional fusion, which adapts to density-variant point clouds generated by view-variant depth images.
• We provide a comprehensive comparison between dif-ferent fusion strategies to analyze their effect empiri-cally.
• Our method achieves new state-of-the-art results on
RGB-D point cloud registration on ScanNet [11] using weights trained either on ScanNet or 3DMatch [73]. 2.