Abstract
Query expression: “a kangaroo in the middle of two”
Referring Video Object Segmentation (R-VOS) is a chal-lenging task that aims to segment an object in a video based on a linguistic expression. Most existing R-VOS methods have a critical assumption: the object referred to must ap-pear in the video. This assumption, which we refer to as
“semantic consensus”, is often violated in real-world sce-narios, where the expression may be queried against false videos. In this work, we highlight the need for a robust R-VOS model that can handle semantic mismatches. Accord-ingly, we propose an extended task called Robust R-VOS (R2-VOS), which accepts unpaired video-text inputs. We tackle this problem by jointly modeling the primary R-VOS problem and its dual (text reconstruction). A structural text-to-text cycle constraint is introduced to discriminate semantic consensus between video-text pairs and impose it in positive pairs, thereby achieving multi-modal alignment from both positive and negative pairs. Our structural con-straint effectively addresses the challenge posed by linguis-tic diversity, overcoming the limitations of previous meth-ods that relied on the point-wise constraint. A new eval-uation dataset, R2-Youtube-VOS is constructed to measure the model robustness. Our model achieves state-of-the-art performance on R-VOS benchmarks, Ref-DAVIS17 and Ref-Youtube-VOS, and also our R2-Youtube-VOS dataset. 1.

Introduction
Referring video object segmentation (R-VOS) aims to segment a referred object in a video given a linguistic ex-pression. R-VOS has witnessed growing interest thanks to its promising potential in human-computer interaction, such as video editing and augmented reality.
In previous studies [1, 40, 7, 3], the R-VOS problem is addressed with the strict assumption that the referred object appears in the video, thus requiring object-level semantic consensus between the expression and the video. How-ever, this assumption is not always true in practice, leading
*This work was done when Xiang Li and Xiaohao Xu were interns at
MSRA. e v i t i s o
P e v i t a g e
N
R-VOS
CSC
False Pred.
True Pred.
Target Object (cid:1792)(cid:2779)-VOS
R-VOS
False Pred.
CSC (cid:1792)(cid:2779)-VOS
True Pred. (None)
Video
Input frame
Prediction
Figure 1. Illustration of the new R2-VOS task. A linguistic ex-pression is given to query a set of videos without the semantic consensus assumption in R2-VOS. Videos containing the referred object by the expression are positive, otherwise negative. Exist-ing R-VOS methods such as Referformer [40] easily segment ir-relevant objects in both positive and negative videos, as they do
In contrast, our R2-VOS method not investigate the consensus. achieves greater accuracy by incorporating cyclic structural con-sensus (CSC). to severe false-alarm problems in negative videos (referred object does not present in the video), as demonstrated in
Fig. 1. This limitation hinders the application of such meth-ods in scenarios where matched vision-language pairs are not available. We argue that the current R-VOS task is in-complete when assuming that the referred object always ex-ists in the video.
Even with semantic consensus in positive video-language pairs, locating the correct object is still challeng-ing due to its multimodal nature. Recent method MTTR
[1] employs multimodal transformer encoders to learn a joint representation, and localizes the object by ranking all presented objects in the video. ReferFormer [40] and the image method ReTR [15] adopt the linguistic expression as a query to the transformer decoder to avoid redundant ranking of all objects. However, these latest R-VOS meth-ods still suffer from a semantic misalignment of the seg-mented object and the linguistic expression, as shown in
Fig. 1. Some previous works [30, 2, 5] attempt to enhance the alignment by imposing cycle consistency between refer-ring expressions and their reconstructed counterparts, with-out considering negative vision-language pairs. Moreover, they restrict the two expressions to be identical or similar, which may not always be the case due to linguistic diversity.
Expressions that describe an object can vary from different perspectives, e.g., the target kangaroo in Fig. 1 can be de-scribed either as “ in the middle of two” or “ lying on its side”. Therefore, a more robust cycle constraint is required to better accommodate this diversity.
In this paper, we seek to investigate the semantic align-ment problem between visual and linguistic modalities in referring video segmentation. We extend the current R-VOS task to accept unpaired video-language inputs. This new task, termed Robust R-VOS (R2-VOS), goes beyond the limitation of the existing R-VOS task by analyzing the semantic consensus. It additionally discriminates between positive video-language pairs where the referred object ap-pears in the video, and negative pairs where it does not.
The R2-VOS task essentially corresponds to two in-terrelated problems: the primary problem of segmenting masks in videos with referring texts (R-VOS), and its dual problem of reconstructing text expressions from videos with object masks. To jointly optimize both problems, consistency in the text-to-text cycle is usually imposed if the video-language pairs are positive, as in most previous works [30, 2, 5]. We go further by learning from both pos-itive and negative pairs. Speciﬁcally, we investigate the se-mantic consensus using cycle consistency measure to dis-criminate the positive and negative pairs, while also im-posing consistency in positive pairs. Both the discrimina-tion and imposing procedure help with the video-language alignment learning. Furthermore, instead of using point-wise cycle consistency for individual samples [2, 5], we adopt a structural measurement to assess the relation con-sistency between the referring textual space and its recon-structed counterpart. This design accommodates linguistic diversity better, as it considers that an object may have dif-ferent text expressions but should be differentiated from the expressions describing other objects. In addition, we enable the end-to-end joint training of the primary and dual prob-lems by introducing an object localizing module (OLM) as a proxy to bridge the two problems. The dual text recon-struction can be conducted using the proxy features without waiting for the resulting masks from the primary task.
Our contributions are summarized as:
• We are the ﬁrst to address the severe false-alarm prob-lem faced by previous R-VOS methods with unpaired video-language inputs. We introduce the new R2-VOS task accepting unpaired inputs, as well as an eval-uation dataset and corresponding metrics.
• We introduce the cyclic structural consensus for both discrimination between positive and negative video-language pairs and enhancement of segmentation qual-ity, which better accommodates linguistic diversity.
• We propose a R2-VOS network that enables end-to-end training of the primary referring segmentation and dual expression reconstruction task by introducing the object localizing module as a proxy.
• Our method achieves state-of-the-art performance for both R-VOS and R2-VOS tasks on popular Ref-Youtube-VOS, Ref-DAVIS, and new R2-Youtube-VOS datasets. 2.