Abstract
We need billion-scale images to achieve more general-izable and ground-breaking vision models, as well as mas-sive dataset storage to ship the images (e.g., the LAION-5B dataset needs 240TB storage space). However, it has become challenging to deal with unlimited dataset storage with limited storage infrastructure. A number of storage-efﬁcient training methods have been proposed to tackle the problem, but they are rarely scalable or suffer from severe damage to performance.
In this paper, we pro-pose a storage-efﬁcient training strategy for vision clas-siﬁers for large-scale datasets (e.g., ImageNet) that only uses 1024 tokens per instance without using the raw level pixels; our token storage only needs <1% of the original
JPEG-compressed raw pixels. We also propose token aug-mentations and a Stem-adaptor module to make our ap-proach able to use the same architecture as pixel-based ap-proaches with only minimal modiﬁcations on the stem layer and the carefully tuned optimization settings. Our exper-imental results on ImageNet-1k show that our method sig-niﬁcantly outperforms other storage-efﬁcient training meth-ods with a large gap. We further show the effectiveness of our method in other practical scenarios, storage-efﬁcient pre-training, and continual learning. Code is available at https://github.com/naver-ai/seit 1.

Introduction
We need billion-scale data points for more generaliz-able and ground-breaking vision models, e.g., 400M image-text pairs [49], 1.8B image-text pairs [31], or 3.6B weakly-annotated images [43, 60]. However, designing and op-erating a high-performance but fault-tolerant generic dis-tributed dataset is a very expensive and challenging prob-lem [75]. This problem has become more challenging for vision datasets compared to language datasets. For exam-ple, training GPT-2 with 8M documents only need 40GB of storage [50], while the larger GPT-3 is trained with 410B to-kens with 570GB of storage [11]. On the other hand, storing
Figure 1. Training data storage vs.
ImageNet 1k Accuracy.
Comparisons on ImageNet-1k [55] using ViT-B/16 backbone [20] are shown. Our SeiT (red lines) signiﬁcantly outperforms other storage-efﬁcient methods with the same storage size, achieving 74.0% and 78.4% top-1 acc with only 1.36GB utilizing tokeniz-ers trained with ImageNet-1k and OpenImages, respectively. Note that the original pixel-based image storage requires 140GB of stor-age to achieve 81.8% top-1 accuracy. Details are in Table B.5. images requires signiﬁcantly more storage space than stor-ing language. For example, the ImageNet-21k dataset [55] with 11M images requires a 1.4TB storage size, 2.5 times larger than GPT-3 storage despite containing fewer data points. Larger-scale datasets for large-scale pre-training re-quire even more massive storage, e.g., 240TB for 5B images
[57]. Consequently, storage remains a major bottleneck in scaling up vision models compared to language models.
Why do images require a large storage size than text?
This is because while the nature of language is discrete, im-ages are continuous in nature. Also, while the text quality is independent of document length, the image quality directly affects the storage size; better quality images require larger storage sizes. Although a lossy JPEG compression can re-duce the storage size, as witnessed by Rombach et al., still
“most bits of a digital image corresponds to imperceptible details” [53]. Such imperceptible details (e.g., ﬁne-grained details or high-frequency information of images) could be unnecessary for our desired vision classiﬁers. However,
deep vision models are vulnerable to imperceptible high-frequency perturbations [23, 42, 17] or unreasonably local areas [22, 6, 59], implying that deep vision models attend too much to imperceptible details instead of the true prop-erty of objects. Therefore, we can expect that we can still achieve a high-performing vision model with the reduced image dataset by removing the imperceptible details.
There are two major directions to storage-efﬁcient vision model training. The ﬁrst direction aims to reduce the total number of data points by discarding less important samples
[45, 47, 32] or synthesizing more “condensed” images than natural images [78, 77]. However, this approach shows a signiﬁcant performance drop compared to the full dataset (the blue and yellow lines in Fig. 1) or cannot be applied to large-scale datasets due to their high complexity. Also, as the sampled or synthesized images are still normal im-ages, these methods still suffer from an inefﬁcient compres-sion ratio to express imperceptible details. Furthermore, these methods need to compute the importance score or the sample-wise gradient of each sample by learning models with the full dataset. It makes these approaches not appli-cable to unseen datasets or newly upcoming data streams.
The other approach involves reducing the size of each image while keeping the total number of images. For ex-ample, by learning a more efﬁcient compression method
[7, 8]. However, the neural compression methods have been mostly studied on extremely small-scale datasets (e.g., 24 images [15] or 100 images [4]), and their generalizability to large-scale datasets is still an open problem. Moreover, the goal of neural compression is to compress an image and recover the original image as perfectly as possible, not to extract the most discriminant features for object recognition tasks. In response to these limitations, no neural compres-sion method has been used to compress large-scale datasets like ImageNet [55] to train deep vision models.
⇥
Due to the difﬁculty of the practical usage of neural com-pression, practitioners have attempted to reduce storage us-age by controlling image quality. For example, the LAION 256 resolution, dataset [58, 57] stores each image at 256 which takes up only 36% of ImageNet images (469 387 resolution on average). Similarly, adjusting the JPEG com-pression quality can reduce the overall storage. As shown in
Fig. 1 (green and purple lines), these approaches work well in practice compared to sampling-based methods. How-ever, these methods have a limited compression ratio; if the compression ratio becomes less than 25%, the performances drop signiﬁcantly. By adjusting the image resolution with a 4% compression ratio and JPEG quality with a 7% com-pression ratio, we achieve 63.3% and 67.8% top-1 accura-cies, respectively. In contrast, our approach achieves 74.0% top-1 accuracy with only a 1% compression ratio.
⇥
All shortcomings of the previous methods originate from the fact that too many imperceptible bits are assigned to
⇥ store a digital image, which is misaligned with our target task. Our approach overcomes this limitation by storing images as tokens rather than pixels, using pre-trained vision tokenizers, such as VQGAN [21] or ViT-VQGAN tokenizer
[72]. Introducing Storage-efﬁcient Vision Training (SeiT), we convert each image to 32 32 tokens. The number of possible cases each token can have (the codebook) is 391, which takes only 1.15KB to store each token (assuming that the number of 391 cases can be expressed in 9 bits). It costs only less than 1.5GB for storing 140GB pixel-based storage of ImageNet. We train Vision Transformer (ViT) models on our tokenized images with minimum modiﬁcations. First, a 1024-length tokenized image is converted to a 32 32 tensor by using pre-trained 32-dimensional codebook vec-tors from ViT-VQGAN. Next, we apply random resized crop (RRC) to the tensor to get a 32 28 tensor. Then, to convert the tensor into a form that ViT can handle, we introduce Stem-Adapter module that converts the RRC-ed tensor into a tensor of size 768 14, the same as the ﬁrst layer input of ViT after the stem layer. Because the image-based augmentations are not directly applicable to tokens, we propose simple token-speciﬁc augmentations, including
Token-EDA (inspired from easy data augmentation (EDA)
[71] for language), Emb-Noise and Token-CutMix (inspired from CutMix [73]). In our experiment, we achieve 74.0% top-1 accuracy with 1.36GB token storage, where the full image storage requires 140GB to achieve 81.8% [65]. 32 14 28
⇥
⇥
⇥
⇥
⇥
⇥
SeiT has several advantages over previous storage-efﬁcient methods. First, as we use a frozen pre-trained to-kenizer that only requires forward operations to extract to-kens from images, we do not need an additional optimiza-tion for compressing a dataset, such as importance score-based sampling [32], image synthesis methods [78, 77], or neural compression [7, 8]. Hence, SeiT is easily applica-ble to newly upcoming data streams directly. Second, un-like previous works that use pre-trained feature extractors (e.g., HOG [19] or Faster-RCNN [51, 2]), SeiT can use the same architecture as pixel-based approaches with only min-imal modiﬁcations on the stem layer, as well as the carefully tuned optimization settings, such as DeiT [65]. It becomes a huge advantage when using SeiT as an efﬁcient pre-training method; we can achieve 82.6% top-1 accuracy by ﬁne-tuning the token pre-trained model with images. Moreover, applying an input augmentation for feature extractor-based approaches is not straightforward, limiting their generaliz-ability. Finally, SeiT shows a signiﬁcant compression ratio, with a 1% compression ratio for ImageNet.
We show the effectiveness of SeiT on three image classi-ﬁcation scenarios: (1) storage-efﬁcient ImageNet-1k bench-mark (2) storage-efﬁcient large-scale pre-training, and (3) continual learning. The overview of storage-efﬁcient results is shown in Fig. 1: SeiT outperforms comparison methods with a signiﬁcant gap with the same storage size, 74.0% ac-curacy on ImageNet under 1% of the original storage, where comparison methods need 40% (uniform sampling, C-score sampling [32]), 6% (adjusting image resolution), and 8% (adjusting JPEG quality) of the original storage to achieve the similar performance. We also demonstrate that SeiT can be applied to large-scale pre-training for an image-based ap-proach; we pre-train a ViT-B/16 model on the tokenized
ImageNet-21k (occupying only 14.1GB) and ﬁne-tune the
ViT model on the full-pixel ImageNet-1k. By using slightly more storage (156GB vs. 140GB), our storage-efﬁcient pre-training strategy shows 82.8% top-1 accuracy, whereas the full-pixel ImageNet-1k training shows 81.8%. Finally, we observe that our token-based approach signiﬁcantly outper-forms the image-based counterpart in the continual learning scenario [52] by storing more data samples in the same size of the memory compared to full-pixel images. (1) We compress an image to 1024 dis-Contributions. crete tokens using a pre-trained visual tokenizer. By ap-plying a simple lossless compression for the tokens, we achieve only 0.97% storage size compared to images stored in pixels. (2) We propose Stem-Adapter module and aug-mentation methods for tokens such as Token-RRC, Token-CutMix, Emb-Noise, and Token-EDA in order to enable
ViT training with minimal change to the protocol and hy-(3) Our storage-perparameters of existing ViT training. efﬁcient training pipeline named Storage-efﬁcient Vision
Training (SeiT) shows great improvements on the low-storage regime. With only 1% storage size, SeiT achieves 74.0% top-1 ImageNet 1k validation accuracy. (4) We addi-tionally show that SeiT can be applied to a storage-efﬁcient pre-training strategy, and continual learning tasks. 2.