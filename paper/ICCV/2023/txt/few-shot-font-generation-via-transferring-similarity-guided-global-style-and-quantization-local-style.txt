Abstract
Automatic few-shot font generation (AFFG), aiming at generating new fonts with only a few glyph references, re-duces the labor cost of manually designing fonts. However, the traditional AFFG paradigm of style-content disentan-glement cannot capture the diverse local details of different fonts. So, many component-based approaches are proposed to tackle this problem. The issue with component-based approaches is that they usually require special pre-defined glyph components, e.g., strokes and radicals, which is in-feasible for AFFG of different languages. In this paper, we present a novel font generation approach by aggregating styles from character similarity-guided global features and stylized component-level representations. We calculate the similarity scores of the target character and the referenced samples by measuring the distance along the correspond-ing channels from the content features, and assigning them as the weights for aggregating the global style features. To better capture the local styles, a cross-attention-based style transfer module is adopted to transfer the styles of reference glyphs to the components, where the components are self-learned discrete latent codes through vector quantization without manual definition. With these designs, our AFFG method could obtain a complete set of component-level style representations, and also control the global glyph charac-teristics. The experimental results reflect the effectiveness and generalization of the proposed method on different lin-guistic scripts, and also show its superiority when com-pared with other state-of-the-art methods. The source code can be found at https://github.com/awei669/VQ-Font. 1.

Introduction
Font design techniques can benefit many critical appli-cations, such as logo designs, data augmentation for text-related tasks, handwriting imitation and identification, etc.
*Corresponding author
However, traditional font design heavily depends on expert designers rendering the glyph styles for each character man-ually, making the creation of fonts extremely expensive and labor-intensive, especially for glyph-rich scripts.
Recently, with the development of deep learning tech-niques, many automatic few-shot font generation (AFFG) methods have been proposed. They have been created us-ing Convolution Neural Networks (CNNs) [22], Genera-tive Adversarial Networks (GANs) [27], Transformers [32], etc. The AFFG methods use only a few reference font images for generating different glyphs automatically. The typical strategy follows the style and content disentangle-ment and combination paradigm [38, 23], and either adopts global style representation or component-wise style repre-sentation. The global style representation [13, 38] is learned and extracted from all the references of each style, which could capture the global characteristics, such as character size and stroke space. However, it lacks the representation of diverse local details, such as the shape and length of lo-cal strokes and serif size. On the contrary, the component-wise style representation category [11, 6, 20] generally de-composes each reference sample sharing the same font into pre-defined components and radicals. It either conditions the style encoders jointly on the glyph image and the corre-sponding component labels or adopts component-label clas-sification losses to train the style encoder. This can be in-feasible because each character should be manually associ-ated with a certain set of components, which requires more preparation when applying for new scripts. Additionally, for different content images, their local relations with ref-erence samples can vary. That means the local style rep-resentations for different content characters are required to be recomputed when given fixed reference samples, which increases the computational cost.
To tackle the above issue, we propose a hybrid global and local style transferring approach for AFFG in this paper.
Since the global style representation of fonts controls more intra-style consistent properties, e.g., the locations, sizes, stroke thickness, and spaces of characters, while the lo-cal style representation focuses on capturing inter-style in-consistent component details, e.g., stroke shape, serif-ness, stroke deformation. Therefore, we leverage both the global and local styles for feature complementation. In order to obtain the global style feature representation, we calculate the similarity scores of the target glyph and the referenced samples by measuring their content feature distances, and then assign them as the weights for aggregating the style features. For local style feature representation, the glyph components are first learned automatically, which are dis-crete latent codes decomposed from a set of glyphs by vec-tor quantization. Then, a cross-attention transformer is em-ployed to transfer component-wise styles, with the repre-sentation of the learned components as the queries and the style representations of the reference glyphs as the keys and values. Contrastive learning is used to learn the local styles in an unsupervised way. For each forward pass, the styles from the reference samples can be transferred onto all the components. So, this local style extraction process is inde-pendent of the content glyph, avoiding multiple component-wise representation calculations for different inputs. Fi-nally, the global and local style representations are com-bined with content features, and then decoded into the target glyph. Moreover, we adopt GAN and a self-reconstruction strategy for training the model without strong supervision.
Therefore, it can be easily applied for different script font generation.
We demonstrate the effectiveness of the proposed method on the Chinese mainly. The experimental results reflect the necessity of combing global and local representa-tions, and also tell that our method outperforms other state-of-the-art (SOTA) AFFG methods given very limited refer-ence examples.
In summary, the contributions of this paper are as fol-lows:
• We propose a novel AFFG method leveraging comple-mentary global and local representations, which is able to capture intra-style consistent properties and intra-style inconsistent structures of reference glyphs.
• Similarity of content is used to obtain global styles.
It takes a similar degree of glyph structures into con-sideration. This strategy can better transfer styles for glyphs owning the same components with reference.
• Pre-trained Vector Quantization-based Variational Au-toencoder (VQ-VAE) is adopted to extract components automatically, component labels are not required. The local styles can be transferred to all the components via cross-attention in one-forward pass. it is efficient for font library creation because it is content irrelevant.
A style contrastive loss is proposed to unsupervised transfer the component-level styles.
• Experimental results show great generalizability of our model for unseen fonts, unseen characters, and differ-ent scripts.
It achieves SOTA performance for font generation even with very limited reference samples.
Additionally, it can transfer styles onto cross-linguistic in the zero-shot manner. 2.