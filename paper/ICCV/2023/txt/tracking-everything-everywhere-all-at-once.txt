Abstract 1.

Introduction
We present a new test-time optimization method for esti-mating dense and long-range motion from a video sequence.
Prior optical flow or particle video tracking algorithms typi-cally operate within limited temporal windows, struggling to track through occlusions and maintain global consistency of estimated motion trajectories. We propose a complete and globally consistent motion representation, dubbed OmniMo-tion, that allows for accurate, full-length motion estimation of every pixel in a video. OmniMotion represents a video using a quasi-3D canonical volume and performs pixel-wise tracking via bijections between local and canonical space.
This representation allows us to ensure global consistency, track through occlusions, and model any combination of camera and object motion. Extensive evaluations on the
TAP-Vid benchmark and real-world footage show that our approach outperforms prior state-of-the-art methods by a large margin both quantitatively and qualitatively. See our project page for more results: omnimotion.github.io.
Motion estimation methods have traditionally followed one of two dominant approaches: sparse feature tracking and dense optical flow [53]. While each type of method has proven effective for their respective applications, neither representation fully models the motion of a video: pairwise optical flow fails to capture motion trajectories over long temporal windows, and sparse tracking does not model the motion of all pixels.
A number of approaches have sought to close this gap, i.e., to estimate both dense and long-range pixel trajectories in a video. These range from methods that simply chain together two-frame optical flow fields, to more recent ap-proaches that directly predict per-pixel trajectories across multiple frames [22]. Still, these methods all use limited context when estimating motion, disregarding information that is either temporally or spatially distant. This locality can result in accumulated errors over long trajectories and spatio-temporal inconsistencies in the motion estimates. Even when prior methods do consider long-range context [53], they op-erate in the 2D domain, resulting in a loss of tracking during occlusion events. All in all, producing both dense and long-range trajectories remains an open problem in the field, with three key challenges: (1) maintaining accurate tracks across long sequences, (2) tracking points through occlusions, and (3) maintaining coherence in space and time.
In this work, we propose a holistic approach to video motion estimation that uses all the information in a video to jointly estimate full-length motion trajectories for every pixel. Our method, which we dub OmniMotion, uses a quasi-3D representation in which a canonical 3D volume is mapped to per-frame local volumes through a set of local-canonical bijections. These bijections serve as a flexible relaxation of dynamic multi-view geometry, modeling a combination of camera and scene motion. Our representation guarantees cy-cle consistency, and can track all pixels, even while occluded (“Everything, Everywhere”). We optimize our representa-tion per video to jointly solve for the motion of the entire video “All at Once”. Once optimized, our representation can be queried at any continuous coordinate in the video to receive a motion trajectory spanning the entire video.
In summary, we propose an approach that: 1) produces globally consistent full-length motion trajectories for all points in an entire video, 2) can track points through occlu-sions, and 3) can tackle in-the-wild videos with any combi-nation of camera and scene motion. We demonstrate these strengths quantitatively on the TAP video tracking bench-mark [14], where we achieve state-of-the-art performance, outperforming all prior methods by a large margin. 2.