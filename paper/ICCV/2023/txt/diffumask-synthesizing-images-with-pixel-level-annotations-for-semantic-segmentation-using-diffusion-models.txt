Abstract
Collecting and annotating images with pixel-wise labels is time-consuming and laborious.
In contrast, synthetic data can be freely available using a generative model (e.g.,
DALL-E, Stable Diffusion).
In this paper, we show that it is possible to automatically obtain accurate semantic masks of synthetic images generated by the Off-the-shelf
Stable Diffusion model, which uses only text-image pairs during training. Our approach, termed DiffuMask, ex-ploits the potential of the cross-attention map between text and image, which is natural and seamless to extend the text-driven image synthesis to semantic mask generation.
DiffuMask uses text-guided cross-attention information to
∗ Corresponding author localize class/word-specific regions, which are combined with practical techniques to create a novel high-resolution and class-discriminative pixel-wise mask. The methods help to significantly reduce data collection and annotation costs. Experiments demonstrate that the existing segmenta-tion methods trained on synthetic data of DiffuMask can achieve a competitive performance over the counterpart of real data (VOC 2012, Cityscapes). For some classes (e.g., bird), DiffuMask presents promising performance, close to the state-of-the-art result of real data (within 3% mIoU gap). Moreover, in the open-vocabulary segmenta-tion (zero-shot) setting, DiffuMask achieves new state-of-the-art results on the Unseen classes of VOC 2012. The project website can be found at DiffuMask.
1.

Introduction
Semantic segmentation is a fundamental task in vision, and existing data-hungry semantic segmentation models usually require a large amount of data with pixel-level an-notations to achieve significant progress. Unfortunately, pixel-wise mask annotation is a labor-intensive and expen-sive process. For example, labeling a single semantic ur-ban image in Cityscapes [14] can take up to 60 minutes, underscoring the level of difficulty involved in this task Ad-ditionally, in some cases, it may be challenging or even im-possible to collect images due to existing privacy and copy-right. To reduce the cost of annotation, weakly-supervised learning has become a popular approach in recent years.
This approach involves training strong segmentation mod-els using weak or cheap labels, such as image-level la-bels [2, 33, 59, 61, 51, 52], points [3], scribbles [37, 63], and bounding boxes [34]. Although these methods are free of pixel-level annotations, still suffer from several disadvan-tages, including low-performance accuracy, complex train-ing strategy, indispensable extra annotation cost (e.g., edge), and image collection cost.
With the great development of computer graphics (e.g., generative model), an alternative way is to utilize synthetic data, which is largely available from the virtual world, and the pixel-level ground truth can be freely and automati-cally generated. DatasetGAN [65] firstly exploits the fea-ture space of a trained GAN and trains a shallow decoder to produce pixel-level labeling. BigDatasetGAN [35] ex-tends DatasetGAN to handle the large class diversity of Im-ageNet. However, both methods suffer from certain draw-backs, the need for a small number of pixel-level labeled examples to generalize to the rest of the latent space and suboptimal performance due to imprecise generative masks.
Recently, large-scale language-image generation (LLIG) models, such as DALL-E [48], and Stable Diffusion [49], have shown phenomenal generative semantic and composi-tional power, as shown in Fig. 1. Given one language de-scription, the text-conditioned image generation model can create corresponding semantic things and stuff, where vi-sual and textual embedding are fused using spatial cross-attention. We dive deep into the cross-attention layers and explore how they affect the generative semantic object and structure of the image. We find that cross-attention maps are the core, which binds visual pixels and text tokens of the prompt text. Also, the cross-attention maps contain rich class (text token) discriminative spatial localization infor-mation, which critically affects the generated image.
Can the attention map be used as mask annotation?
Consider semantic segmentation [19, 14]—a ‘good’ pixel-level semantic mask annotation should satisfy two condi-tions: (a) class-discriminative (i.e., localize and distinguish the categories in the image); (b) high-resolution, precise mask (i.e., capture fine-grained detail). Fig. 2b presents a (a) Cross attention maps of different text tokens. (b) Cross attention maps of different resolutions. (c) Binarization Mask with different thresholds γ in Equ. (3).
Figure 2 – Cross-attention maps of a text-conditioned diffu-sion model (i.e., Stable Diffusion [49]). Prompt language: ‘a horse on the grass’. visualization of cross attention map between text token and vision. 8 × 8, 16 × 16, 32 × 32, and 64 × 64, as four dif-ferent resolutions, are extracted from different layers of the
U-Net of Stable Diffusion [49]. 8×8 feature map is the low-est resolution, including obvious class-discriminative loca-tion. 32 × 32 and 64 × 64 feature maps include high-resolution and highlight fine-grained details. The average map shows the possibility for us to use for semantic seg-mentation, where it is class-discriminative and fine-grained.
To further validate the potential of the attention map of the generative task, we convert the probability map to a binary map with fixed thresholds γ, and refine them with Dense
CRF [31], as shown in Fig. 2c. With the 0.35 threshold, the mask presents a wonderful precision on fine-grained de-tails (e.g., foot, ear of the ‘horse’).
Based on the above observation, we present DiffuMask, an automatic procedure to generate a massive high-quality image with a pixel-level semantic mask. Unlike Dataset-GAN [65] and BigDatasetGAN [35], DiffuMask does not require any pixel-level annotations. This approach takes full advantage of powerful zero-shot text-to-image genera-tive models such as Stable Diffusion [49], which are trained on web-scale image-text pairs. DiffuMask mainly includes two advantages for two challenges: 1) Precise Mask. An adaptive threshold of binarization is proposed to convert the probability map (attention map) to a binary map, as the mask annotation. Besides, noise learning [44, 56] is used to filter noisy labels. 2) Domain Gap: retrieval-based prompt (various and verisimilar prompt guidance) and data augmentations (e.g., Splicing [7]), as two effective solu-tions, are designed to reduce the domain gap via enhancing the diversity of data. With the above advantages, DiffuMask can generate infinite images with pixel-level annotation for any class without human effort. These synthetic data can
then be used for training any semantic segmentation archi-tecture (e.g., mask2former [11]), replacing real data.
To summarize, our contributions are three-folds:
• We show a novel insight that it is possible to automat-ically obtain the synthetic image and mask annotation from a text-supervised pre-trained diffusion model.
• We present DiffuMask, an automatic procedure to gen-erate massive image and pixel-level semantic annota-tion without human effort and any manual mask an-notation, which exploits the potential of the cross-attention map between text and image.
• Experiments demonstrate that segmentation methods trained on DiffuMask perform competitively on real data, e.g., VOC 2012. For some classes, e.g., dog, the performance is close to that of training with real data (within 3% gap). Moreover, in the open-vocabulary segmentation (zero-shot) setting, DiffuMask achieves new SOTA results on the Unseen classes of VOC 2012. 2.