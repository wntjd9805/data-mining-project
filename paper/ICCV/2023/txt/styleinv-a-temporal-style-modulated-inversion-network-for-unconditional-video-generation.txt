Abstract
Unconditional video generation is a challenging task that involves synthesizing high-quality videos that are both coherent and of extended duration. To address this chal-lenge, researchers have used pretrained StyleGAN image generators for high-quality frame synthesis and focused on motion generator design. The motion generator is trained in an autoregressive manner using heavy 3D convolutional discriminators to ensure motion coherence during video generation.
In this paper, we introduce a novel motion generator design that uses a learning-based inversion net-work for GAN. The encoder in our method captures rich and smooth priors from encoding images to latents, and given the latent of an initially generated frame as guidance, our method can generate smooth future latent by modu-lating the inversion encoder temporally. Our method en-joys the advantage of sparse training and naturally con-strains the generation space of our motion generator with the inversion network guided by the initial frame, elimi-nating the need for heavy discriminators. Moreover, our method supports style transfer with simple fine-tuning when the encoder is paired with a pretrained StyleGAN gener-ator. Extensive experiments conducted on various bench-marks demonstrate the superiority of our method in gener-ating long and high-resolution videos with decent single-frame quality and temporal consistency. Code is available at https://github.com/johannwyh/StyleInV. 1.

Introduction
Unconditional video generation aims at learning a gener-ative model to create novel videos from latent vectors. De-spite extensive studies [42, 32, 33, 38, 11, 50] in address-ing this problem, it remains challenging to generate high-resolution videos with both favorable quality and motion coherence over a long-term duration. The core difficulties in this task lie in modeling consistent motion and managing the high memory consumption introduced by the addition of the temporal dimension.
To ensure high single-frame resolution and quality, many
Figure 1: A comparison between autoregressive and non-autoregressive pipeline: (a) Previous autoregressive mo-tion generators require generating the whole clip for a 3D-convolution-based discriminator. (b) Our non-autoregressive motion generator, StyleInV, is an inversion network modulated by temporal style (as a random function of t), which enjoys sparse training using a 2D-convolution-based discriminator. existing studies, such as MoCoGAN-HD [36], employ a powerful image generator such as StyleGAN [22] as a back-bone to serve as a strong generative prior. This approach shifts the focus towards developing a robust motion gener-ator that can capture temporally coherent motion. Most of these methods model motion in an auto-regressive manner, where the next latent is sampled conditioned on the previ-ous one (see Fig. 1). However, this design has two main drawbacks. First, while good performance requires seeing
fined by the initial latent code. As demonstrated in Fig. 2, this leads to a significant benefit. Second, thanks to the flexibility of the inversion network in accepting tempo-ral styles of arbitrary timestamps, the framework allows non-autoregressive generation and sparse training [52, 35].
These merits help alleviate the need for heavy discrimina-tors to ensure temporal consistency, as is required in exist-ing approaches. In our implementation, we only need to use a 2D convolutional discriminator instead of a 3D discrim-inator like MoCoGAN-HD. Third, Unlike existing state-of-the-art methods [52, 35, 7] that couple content and mo-tion decoding in one synthesis network, our framework can naturally support content decoder fine-tuning on different image datasets. Specifically, after fine-tuning the decoder (e.g., StyleGAN2) on another image dataset with the map-ping layers and low-resolution synthesis layers fixed, given the same sequence of synthesized motion latents, the gen-erated video can possess the new style of the fine-tuning dataset while preserving the motion patterns of the video generated by the parent content decoder.
The main contribution of this work is a novel motion generator that modulates a GAN inversion network. This is the first attempt to build such a generator, and it offers several advantages in a unified framework over existing ap-proaches. These advantages include consistent generation, sparse training, and flexibility in supporting style transfer with simple fine-tuning. We additionally contribute a refor-mulation to the conventional sparse training, through first-frame-aware acyclic positional encoding (FFA-APE) and first-frame-aware sparse training (FFA-ST), to ensure that our motion generator can faithfully reconstruct the initial frame and that the generated video is smooth and continu-ous. Extensive experiments on DeeperForensics [17], Face-Forensics [31], SkyTimelapse [47] and Tai-Chi-HD [34] datasets show that our model is comparable to or even better than state-of-the-art unconditional video generation meth-ods [36, 52, 35] both qualitatively and quantitatively. 2.