Abstract
Recent research in language-guided visual navigation has demonstrated a significant demand for the diversity of traversable environments and the quantity of supervi-sion for training generalizable agents. To tackle the com-mon data scarcity issue in existing vision-and-language navigation datasets, we propose an effective paradigm for generating large-scale data for learning, which ap-plies 1200+ photo-realistic environments from HM3D and
Gibson datasets and synthesizes 4.9 million instruction-trajectory pairs using fully-accessible resources on the web.
Importantly, we investigate the influence of each component in this paradigm on the agent’s performance and study how to adequately apply the augmented data to pre-train and fine-tune an agent. Thanks to our large-scale dataset, the performance of an existing agent can be pushed up (+11% absolute with regard to previous SoTA) to a significantly new best of 80% single-run success rate on the R2R test split by simple imitation learning. The long-lasting gener-alization gap between navigating in seen and unseen envi-ronments is also reduced to less than 1% (versus 8% in the previous best method). Moreover, our paradigm also facil-itates different models to achieve new state-of-the-art navi-gation results on CVDN, REVERIE, and R2R in continuous environments. 1.

Introduction
Vision-and-Language Navigation (VLN) [10] is a chal-lenging task that requires an agent to navigate in photo-realistic environments, following human natural language instructions such as “Walk downstairs, move towards the dining table, turn left to the kitchen, and stop in front
˚Equal contribution. :Project lead.
♠Research done during internship at Shanghai AI Lab.
Figure 1: Agent success rate with increasing data size on addressing the R2R navigation task. Our proposed method creates 4.9M instruction-trajectories pairs for learn-ing, which greatly boosts the agent’s performance, and for the first time approaching human results. of the fridge.” Addressing VLN relies heavily on cor-rectly interpreting the instructions, perceiving the envi-ronments, and learning from interaction, which demands a large amount of diverse visual-language data for learn-ing. Recent research shows that scaling up the diver-sity of environments and the quantity of demonstration for training VLN agents are promising in improving gener-alization to unseen scenes [17, 37]. Compared to previ-ous approaches of addressing data scarcity by augmenting agent’s observations [47, 72] or employing large vision-linguistic models pre-trained with image-text data from the web [27, 31, 53, 70, 71], utilizing additional traversable environments allows the agents to learn from in-domain visual-language data and physical interaction in the space.
In light of this, recent large datasets which contain hun-dreds of interactive scenes have been created [20, 65, 81], as well as a vast amount of human demonstrations have been
collected [45, 66] for learning visual navigation, leading to significant improvement in agent’s performance. How-ever, the process towards such large-scale training involves solving a series of key sub-problems such as how to build navigation graphs [10, 30, 37], how to recover corrupted rendered images [8, 43], and how to generate navigational instructions [23, 25, 76, 80], which significantly influ-ence the quality of collected data and should be investi-gated thoroughly. Meanwhile, an agent capable of under-standing human natural language and navigating in photo-realistic environments is a complex and modularized sys-tem [3, 14, 18, 32, 74, 77, 87, 90], and it is important to study how to effectively utilize the large-scale data to bene-fit the training of navigational agents adequately.
In this paper, we propose an effective paradigm for large-scale vision-and-language navigation (VLN) training and quantitatively evaluate the influence of each component in the pipeline. Specifically, we utilize environments in both the HM3D [65] and the Gibson [81] datasets, build nav-igation graphs for the environments based on the Habitat simulator [69], sample new trajectories and generate cor-responding instructions [72], and train agents [16, 18] for solving downstream navigation tasks [10, 35, 44, 59, 73].
Different from previous methods such as AutoVLN [17] and MARVAL [37], we build navigation graphs using an excessive viewpoint sampling and aggregation algorithm, following the graph construction heuristic proposed in [30], which results in fully-connected graphs with high coverage in open space. Additionally, we address the issue of cor-rupted rendered images from HM3D and Gibson environ-ments with the Co-Modulated GAN [84], which we train to generate photo-realistic images from the faulty rendered images with broken, distorted, or missing regions, to miti-gate the noise in visual data. Unlike MARVAL, which uses a non-public language generation model Marky [76] and visual encoder MURAL [34], as well as synthesizes ob-servations from novel viewpoints with an image-to-image
GAN [40], our large-scale training regime is fully repro-ducible and straightforward to execute, while leading to a significant improvement on agent’s performance.
Through comprehensive experiments, we find that a fully traversable navigation graph is crucial to improve the agent’s performance for downstream tasks with detailed instructions like R2R. Besides, we show that recovering photo-realistic images from the rendered images is very beneficial, especially for the low-quality 3D scans from the
Gibson environments. Results also suggest that an agent can consistently benefit from having more diverse visual data, and learning from additional scenes helps agents to generalize better to unseen environments than simply learn-ing from more data. Moreover, we validate that an agent trained with augmented instructions generated by a simple
LSTM-based model [72] can achieve good performance on multiple navigation tasks [10, 59, 73]. Last but not least, we find that appropriately combining our augmented data with the original data in pre-training and fine-tuning can benefit the agent’s generalization ability.
Remarkably, by following the above analysis as data augmentation and agent training guidelines, our result-ing VLN model achieves 80% success rate (SR) on the
R2R test split by simple imitation learning without pre-exploration [26, 72, 89], beam search [25, 53, 82] or model ensembling [62], and successfully eliminates the gap between navigating in seen and unseen environments.
This result significantly outperforms previous best method (73%) [3], and reduces the difference towards human per-formance (86% SR3) [10] to 6%. Our method also achieves new state-of-the-art results on different language-guided visual navigation problems, including CVDN [73] and
REVERIE [59]. Moreover, although the augmented data is discrete, it helps boost VLN performance in continuous en-vironments (R2R-CE) [5, 30, 44], a much more realistic but difficult scenario, by 5% SR. All the results demonstrate the great effectiveness and generalization potential of our train-ing regime. In summary, our main contributions include: 1. A simple, effective, fully automated and reproducible large-scale training paradigm for vision-and-language navigation. 2. Comprehensive analysis of the entire data augmenta-tion pipeline and utilizing the large data for training. 3. New state-of-the-art results on navigation tasks includ-ing R2R, CVDN, REVERIE, and R2R-CE. 2.