Abstract
Recent studies indicate that Vision Transformers (ViTs) are robust against out-of-distribution scenarios. In partic-ular, the Fully Attentional Network (FAN) - a family of ViT backbones, has achieved state-of-the-art robustness. In this paper, we revisit the FAN models and improve their pre-training with a self-emerging token labeling (STL) frame-work. Our method contains a two-stage training frame-work. Specifically, we first train a FAN token labeler (FAN-TL) to generate semantically meaningful patch token labels, followed by a FAN student model training stage that uses both the token labels and the original class label. With the proposed STL framework, our best model based on FAN-L-Hybrid (77.3M parameters) achieves 84.8% Top-1 ac-curacy and 42.1% mCE on ImageNet-1K and ImageNet-C, and sets a new state-of-the-art for ImageNet-A (46.1%) and ImageNet-R (56.6%) without using extra data, outper-forming the original FAN counterpart by significant mar-gins. The proposed framework also demonstrates signifi-cantly enhanced performance on downstream tasks such as semantic segmentation, with up to 1.7% improvement in ro-bustness over the counterpart model. 1.

Introduction
Vision Transformers (ViTs) [1] have recently achieved remarkable success in visual recognition tasks. Such suc-cess is not only attributed to their self-attention representa-tion but also to the newly developed training recipes. For instance, refinements in training techniques such as strong data augmentation and knowledge distillation [2] greatly alleviate ViT’s issue of being data-hungry and make them more accessible for training on ImageNet-1K.
Another important development in the training recipe is token labeling [3], where patch tokens are assigned with labels to ViTs in a dense manner. In some sense, token la-beling can also be considered as an alternative form of hard knowledge distillation. However, the dense nature of token
*Work done during an internship at NVIDIA.
†Corresponding author.
Figure 1. Results of zero-shot robustness against ImageNet-A and ImageNet-R. Models trained on ImageNet-1K with self-emerging token labels from FAN show superior robustness to the out-of-distribution data. Our best model (with only 77.3M param-eters) achieves robust accuracy of 46.1% and 56.6% and sets a new record on ImageNet-A and ImageNet-R. labeling allows ViTs to leverage more fine-grained infor-mation in an image and take different categories and object localization into account. Compared to traditional knowl-edge distillation methods, token labeling enables ViTs to exploit a wider range of information in the image, lead-ing to more accurate results. The success of token la-beling depends on carefully-designed token-level annota-tors (i.e., token-labelers) that can provide accurate location-specific information (i.e., token labels) to patch tokens.
In [3], this is done by a special re-labeling process [4] us-ing convolutional neural networks (CNNs) [5] pre-trained on ImageNet-1K. While Vision Transformers have shown great promise in representation learning, less exploration has been conducted on modeling them as token-labelers.
This raises two interesting questions: 1. Can Transformer-based models self-produce meaning-ful token labels? 2. Can one improve the pre-training of ViTs with self-produced knowledge instead of external teachers?
Our approach:
In this paper, we aim to answer the above questions. We propose a self-emerging token label-ing (STL) framework that employs the self-produced token labels by ViT token-labelers instead of relying on CNNs.
Our work is built on the recently proposed Fully Atten-tional Network (FAN) [6] for two reasons. First, FAN ex-hibits excellent self-emerging visual grouping on token fea-tures, which can be leveraged to generate high-quality to-ken labels. Second, FAN is a family of ViT backbones with state-of-the-art accuracy and robustness. We aim to further improve this family of powerful backbones through a prin-cipled token-labeling design and validate its effectiveness.
Our contributions can be summarized as follows:
• Our work demonstrates that ViT models can be effective token-labelers. We propose a simple yet effective way to train a FAN token-labeler that can produce semantically meaningful token labels.
• We perform an in-depth analysis and show critical con-tributors to the accuracy of token labels. On top of the observations, we design a solution that retains more accurate token labels of the target object for improved model pre-training.
• Our models trained with STL set a new record on out-of-distribution datasets without using extra data than
ImageNet-1K. Our best model achieves robust accuracy of 46.1% on ImageNet-A and 56.6% on ImageNet-R with only 77M parameters, as shown in Fig. 1.
• Experiments on downstream tasks demonstrate that the improved performance in backbone models is transfer-able to semantic segmentation and object detection.
Our STL framework is akin to the teacher-student train-ing strategy introduced in knowledge distillation and con-sists of two stages:
First stage: We train a FAN token-labeler (FAN-TL) model to generate token-level annotations. Our task is essentially a “chicken or the egg” problem since there is no explicit su-pervision on how the token labels are generated. We tackle this by assigning supervising both the class token and the global average-pooled token. This produces semantically meaningful token labels as shown in Fig. 2(b).
Second stage: We train a FAN student model using the original class labels and the token labels from FAN-TL. Ob-serving the imperfect quality of token labels, we introduce a token selection approach based on Gumbel-Softmax that adaptively selects tokens with high confidence. Labels of the selected tokens are of better quality and object ground-ing in general, leading to improved pre-training. 2.