Abstract
Large-scale noisy web image-text datasets have been proven to be efficient for learning robust vision-language models. However, to transfer them to the task of video re-trieval, models still need to be fine-tuned on hand-curated paired text-video data to adapt to the diverse styles of video descriptions. To address this problem without the need for hand-annotated pairs, we propose a new setting, text-video retrieval with uncurated & unpaired data, that uses only text queries together with uncurated web videos during training without any paired text-video data. To this end, we propose an approach, In-Style, that learns the style of the text queries and transfers it to uncurated web videos. More-over, to improve generalization, we show that one model can be trained with multiple text styles. To this end, we introduce a multi-style contrastive training procedure, that improves the generalizability over several datasets simulta-neously. We evaluate our model on retrieval performance over multiple datasets to demonstrate the advantages of our style transfer framework on the new task of uncurated & un-paired text-video retrieval and improve state-of-the-art per-formance on zero-shot text-video retrieval. 1 1.

Introduction
Vision-language retrieval refers to the task of retrieving an image or a video from a large data pool given a textual description of the content. Especially the field of text-image retrieval has seen remarkable progress, mainly spurred by the combination of image and text models trained on large-scale web collections [47, 31] of image-text pairs. While advances in video retrieval also rely on pre-trained image-language models, which serve for better task transfer, most systems still require a fine-tuning on downstream data. This requires hand-annotated text-video pairs, namely a trimmed segment of a larger video that is precisely described by the
*Equal contribution. 1github.com/ninatu/in style
Figure 1: Training data for supervised and uncurated & un-paired settings for text-video retrieval. Left: standard super-vised text-video retrieval, aligned and paired data is given for each target setting of the same distribution as target test set; Right: our uncurated & unpaired text-video retrieval setting. No paired data is available during training, only text queries, whereas to support training, we use uncurated web videos. corresponding text pair, for the training and testing of each target downstream dataset. Collecting such aligned pairs of text and videos can be time and cost intensive, and particu-larly gathering videos that comply with national regulations and copyright can be a challenge. Also, in case of relying on free web content, some videos can become unavailable over time while the respective curated annotations stay available for download but do not have matching videos.
To address this problem, we propose a new setup, text-video retrieval with uncurated & unpaired data, assum-ing the availability of text queries only and without related videos during training (Figure 1). The setting is motivated by the fact that it can be considered easier to collect or gen-erate text data, e.g. by producing topic-specific text queries, rather than providing a video to match a specific context.
To allow the training of a text-video retrieval system based on the given text, we assume to have access to an uncurated video collection as the only source of available videos.
As different domains and datasets contain diverse styles of textual descriptions of videos, we propose a novel method, In-Style, to transfer the caption style of given text queries to uncurated web videos, which can be from a devi-ating distribution compared to the given text queries. To
transfer the style of the text queries, we leverage large image-language models [31, 47] by creating pseudo pairs that correspond to the given text queries and videos from the uncurated collection by matching them in the shared embedding space [47]. Thus, we identify a subset of videos that have more similarity to the text queries than the rest of the videos. We then adopt an image-to-text captioning model (captioner) to mimic the style of our text queries by training with these pseudo pairs. The stylized captioner is now capable of producing relevant video descriptions in the desired style; therefore, we re-annotate the web videos with the captioner to obtain aligned paired data; we call them generated pairs. Finally, we show that generated pairs help to adapt models pre-trained on large-scale web data [31, 52] to the desired single or multiple styles of given text queries.
We evaluate our model on text-video retrieval over 5 benchmark datasets. Specifically, we demonstrate the advantages of the In-Style method on the new task of uncurated & unpaired text-video retrieval with image-language [31] and video-language [52] pre-trained back-bones. We show the generalization of the proposed ap-proach by training a single model for multiple datasets at once leading to an improved state-of-the-art zero-shot text-video retrieval performance.
We summarize our contributions in the following: (i) we introduce a new task of text-video retrieval with un-curated & unpaired data where during training, only text queries are available, whereas for standard text-video re-trieval task, paired text-video data is used; (ii) we propose a novel method, In-Style, to transfer the style of text queries in an unsupervised way, showing that style is an important component for language-based retrieval tasks; therefore, we repurpose large pre-trained image-language models to gen-erate pseudo-captions of the same style for uncurated web videos; (iii) we demonstrate the advantages of our In-Style method for the new task over 5 different datasets with in-dividual models for each dataset as well as one generalized model and we achieve state-of-the-art performance on zero-shot text-video retrieval. 2.