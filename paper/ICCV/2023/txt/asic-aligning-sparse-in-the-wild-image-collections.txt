Abstract 1.

Introduction
We present a method for joint alignment of sparse in-the-wild image collections of an object category. Most prior works assume either ground-truth keypoint annotations or a large dataset of images of a single object category. How-ever, neither of the above assumptions hold true for the long-tail of the objects present in the world. We present a self-supervised technique that directly optimizes on a sparse col-lection of images of a particular object/object category to obtain consistent dense correspondences across the collec-tion. We use pairwise nearest neighbors obtained from deep features of a pre-trained vision transformer (ViT) model as noisy and sparse keypoint matches and make them dense and accurate matches by optimizing a neural network that jointly maps the image collection into a learned canonical grid. Experiments on CUB, SPair-71k and PF-Willow bench-marks demonstrate that our method can produce globally consistent and higher quality correspondences across the image collection when compared to existing self-supervised methods. Code and other material will be made available at https://kampta.github.io/asic.
Given an image of a car, we as humans, can easily map corresponding pixels between this car and an arbitrary collec-tion of car images. Our visual system is able to achieve this (rather impressive) feat using a multitude of cues - low level photometric consistency, high level visual grouping and our priors on cars as an object category (shape, pose, materials, illumination etc.). The above is also true for an image of a
“never-before-seen” object (as opposed to a common object category such as cars) where humans demonstrate surpris-ingly robust generalization despite lacking an object or cate-gory speciﬁc priors [7]. These correspondences in turn in-form downstream inferences about the object such as shape, affordances, and more. In this work, we tackle this problem of “low-shot dense correspondence” – i.e. given only a small in-the-wild image collection (∼10–30 images) of an object or object category, we recover dense and consistent corre-spondences across the entire collection.
Prior works addressing this problem of dense alignment in “in-the-wild” image collections assume availability of an-notated keypoint matches and image pairs [13, 55], a mesh of the object [42, 86], or a very large collection of images
of the object [58, 61]. These assumptions often do not hold for the long-tail of objects that exist in real world imagery.
This long-tail is unavoidable; no matter how many new im-ages we annotate, we will keep uncovering new and rare cat-egories of objects. In our work, we show that it is possible to achieve dense correspondence of small in-the-wild image collections without any manual annotations by leveraging the power of large self-supervised vision models. Aligning these image sets can be useful for a wide range of applica-tions such as edit propagation for images and videos, as well as downstream problems such as pose and shape estimation.
In the presence of a limited number of samples and a high-dimensional search space, dense correspondence and joint alignment of an image set is a challenging optimization prob-lem. We draw inspiration from classical image alignment methods [44, 74] where images are warped (or congealed) to a consistent canonical pose before classiﬁcation using sim-ple transformations, as well as recent works on per-image-set optimization [39, 53, 80], where the inductive model bi-ases coupled with additional regularization allows for learn-ing a good solution with self-supervision. Our framework, dubbed ASIC, consists of a small image-to-image network which predicts a dense per-pixel mapping from the image to a two-dimensional canonical grid. This canonical grid is parameterized as a multi-channel learned embedding and stores an RGB color along with an alpha value indicating whether the location represents the object or the background.
We devise a novel contrastive loss function to ensure that se-mantic keypoints from different images map to a consistent location in canonical space.
The key contribution of our work is to exploit noisy and sparse pseudo-correspondences between a pair of images and extend them to learn consistent dense correspondences across the image collection. These pseudo-correspondences can be obtained using any of the large self-supervised learn-ing (SSL) models [10–12, 22, 24, 57, 62] which learn with-out explicit labels on large internet-scale data. In order to make them accurate, we enforce pair-wise consistency across the image collection with an alignment network and a self-supervised keypoint consistency loss. Further, we introduce additional regularization via equivariance and reconstruction terms to get dense correspondences across collection.
Fig. 1 demonstrates the dense and consistent mapping learned by our model for two image sets. We also evalu-ate our method on 18 image categories in SPair-71k [56] dataset, 4 categories in PF-Willow [23], 3 ﬁne-grained cate-gories in the CUB [81], as well as 5 collections in SAMU-RAI [9] datasets and show that ASIC is competitive against unsupervised keypoint correspondence approaches, and of-ten outperforms them. An additional advantage of learning a joint canonical mapping is that our method suffers signiﬁ-cantly less drift when propagating keypoints on a sequence of images (instead of just a single image pair). In order to evaluate the keypoint consistency over a sequence of k im-ages, we propose a new metric k-CyPCK (or k-cycle PCK) in Sec. 4.4 and show that our method outperforms existing methods by over 20% at both the low and high precision set-tings. In summary, our contributions are as follows:
• We introduce a test-time optimization technique to recover consistent dense correspondence maps over a small col-lection of in-the-wild images.
• We design a novel loss function and several regularization terms to encourage mapping to be consistent across multi-ple images in a given collection.
• We perform extensive quantitative and qualitative eval-uations on 4 different datasets (spanning 30 object cate-gories) to show that our method is competitive with the unsupervised methods, often outperforming them.
• We propose a novel metric k-CyPCK to evaluate consis-tency of keypoint propagation over a sequence of images, which is not captured by traditional metrics such as PCK. 2.