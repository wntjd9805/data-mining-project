Abstract
Scene flow estimation provides the fundamental motion perception of a dynamic scene, which is of practical im-portance in many computer vision applications.
In this paper, we propose a novel multi-scale bidirectional recur-rent architecture that iteratively optimizes the coarse-to-fine scene flow estimation. In each resolution scale of es-timation, a novel bidirectional gated recurrent unit is pro-posed to bidirectionally and iteratively augment point fea-tures and produce progressively optimized scene flow. The optimization of each iteration is integrated with the hy-brid correlation that captures not only local correlation but also semantic correlation for more accurate estima-tion. Experimental results indicate that our proposed archi-tecture significantly outperforms the existing state-of-the-art approaches on both FlyingThings3D and KITTI bench-marks while maintaining superior time efficiency. Codes and pre-trained models are publicly available at https:
//github.com/cwc1260/MSBRN . 1.

Introduction
Scene flow estimation is a fundamental task that esti-mates the dense 3D motion field of points from two con-secutive frames [32, 19]. As it provides the basic motion understanding of a dynamic environment, it is meaningful in a variety of high-level applications such as autonomous driving, augmented reality, and robotics [15].
Early scene flow estimation approaches rely on 2D rep-resentations such as RGB images [14, 11, 13, 28, 12]. They basically estimate optical flow and disparity map separately in the 2D space instead of directly estimating scene flow vectors in the 3D space. Recently, with the advances in
LiDAR sensors and point cloud-based learning technolo-gies, learning scene flow directly from point clouds has been extensively studied. The pioneering work known as
FlowNet3D [19] is the first to introduce hierarchical Point-Figure 1. Illustration of the multi-scale bidirectional recurrent net-work for scene flow estimation. The multi-scale features extracted from each input frame are fed into novel bidirectional gated recur-rent units (BGRUs) that respectively iterates K times for optimiz-ing scene flows on each scale. Introducing hybrid correlation (HC) further improves the performance by searching correspondence in both the Euclidean and latent feature space. The estimated scene flows are warped with the source frame for a clear comparison with the target frame.
Net++ [26] to directly predict the 3D scene flow based on the point cloud. Following this work, a diverse variety of architectures [33, 36, 9, 32, 15] have been proposed and significantly enhanced performance.
Recently, there is a rising trend of iteratively optimiz-ing estimated scene flow by utilizing the recurrent scheme
[15, 9, 31] to progressively improve the estimation accu-racy. However, they only focus on optimizing single resolu-tion scale which causes large computation latency. On the other hand, another series of works [28, 4] presented their superior efficiency by estimating multi-scale scene flow in a coarse-to-fine manner. However, their single-shot estima-tion on each scale restricts their performance. To achieve high estimation accuracy while holding high efficiency, we propose an effective and efficient architecture, Multi-Scale
Bidirectional Recurrent Network (MSBRN), that iteratively optimizes coarse-to-fine scene flow. Moreover, in our view, the optimization phases of scene flow can be regarded as temporal sequences, which can benefit from the bidirec-The key contributions of this paper are summarized as follows:
• We propose a novel multi-scale bidirectional recurrent architecture used for a 3D scene flow estimation task based on point cloud. The model can iteratively and bidirectionally enhance features and scene flow esti-mations in a coarse-to-fine manner in order to sig-nificantly improve the performance while maintaining high efficiency.
• We propose a hybrid correspondence grouping that collects corresponding points from the other point frame in both the latent feature space and Euclidean space.
• The proposed model achieves state-of-the-art perfor-mance and generality on the synthetic FlyingThings3D and real-world KITTI benchmarks under both oc-cluded and non-occluded conditions. 2.