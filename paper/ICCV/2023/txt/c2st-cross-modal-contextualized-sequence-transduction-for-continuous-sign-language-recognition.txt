Abstract
Continuous Sign Language Recognition (CSLR) aims to transcribe the signs of an untrimmed video into written words or glosses. The mainstream framework for CSLR consists of a spatial module for visual representation learn-ing, a temporal module aggregating the local and global temporal information of frame sequence, and the connec-tionist temporal classification (CTC) loss, which aligns video features with gloss sequence. Unfortunately, the language prior implicit in the gloss sequence is ignored throughout the modeling process. Furthermore, the contex-tualization of glosses is further ignored in alignment learn-ing, as CTC makes an independence assumption between glosses. In this paper, we propose a Cross-modal Contextu-alized Sequence Transduction (C2ST) for CSLR, which ef-fectively incorporates the knowledge of gloss sequence into the process of video representation learning and sequence transduction. Specifically, we introduce a cross-modal con-text learning framework for CSLR, in which the linguistic features of gloss sequences are extracted by a language model, and recurrently integrate with visual features for video modelling. Moreover, we introduce the contextualized sequence transduction loss that incorporates the contextual information of gloss sequences in label prediction, without making any independence assumptions between the glosses.
Our method sets the new state of the art on three widely used large-scale sign language recognition datasets: Phoenix-2014, Phoenix-2014-T, and CSL-Daily. On CSL-Daily, our approach achieves an absolute gain of 4.9% WER com-pared to the best published results. 1.

Introduction
Sign language, which utilizes signals like hand/arm po-sitions, and body postures to aid individuals with hearing impairments globally, has become a powerful communica-tion tool that enhances their quality of life. Due to the crit-ical role of sign language and the increased availability of
Figure 1: Illustration of the framework of conventional sign language recognition methods (a) and our proposed Cross-modal Contextualized Sequence Transduction (C2ST) (b).
The proposed C2ST incorporates the textual information to the original visual branch by the introduced fusion module.
In addition, we introduce a novel Contextualized Sequential
Transduction (CST) loss function to consider the relation-ship between the labels in training process. sign language datasets, continuous sign language recogni-tion (CSLR) [28, 11, 40, 15, 14, 6] has gained significant attention in recent years, which enables communication be-tween hearing-impaired people and persons without specific knowledge of sign language.
With the aim of automatically recognizing gloss se-quences (the smallest semantic unit in sign language) from untrimmed sign videos, various CSLR methods [15, 14] have been proposed. Fig.1a shows the mainstream CSLR framework, which consists of a spatial module and a tem-poral module to learn the spatial and temporal visual rep-resentations of sign language videos, and a Connection-ist Temporal Classification (CTC) [10] loss to align the extracted visual features with the corresponding gloss se-quences. For example, VAC [28] employs 2D Convolution
Neural Network (CNN), 1D-TCN [23] and BiLSTM [34] to extract the visual features and uses CTC as the alignment module. C2SLR [41] introduces a spatial attention consis-tency module and a temporal sentence embedding consis-tency module to learn a better spatial-temporal representa-tion. SMKD [11] forces the visual and contextual module to focus on short-term and long-term information by the self-mutual knowledge distillation method.
Existing methods have achieved promising CSLR per-formance by utilizing spatial-temporal features for visual representation and CTC for alignment. However, the lan-guage prior implicit in the gloss sequence is neglected dur-ing the video modeling process. Without understanding the linguistic knowledge of glosses, existing models may per-form unsatisfactorily in complex scenarios, for example, in long sequence prediction. Additionally, contextual infor-mation provided by the predicted glosses is also disregarded during alignment learning, as the widely-used CTC method makes the assumption of independence between glosses.
Signs that present different semantics in different contexts may be misrecognized.
To address this issue, we propose a Cross-modal Contex-tualized Sequence Transduction (C2ST) method for CSLR, which effectively incorporates the knowledge of gloss se-quence into the process of video representation learning and sequence transduction. We first present the cross-modal context learning framework to equip visual representation with language prior of glosses. As shown in Fig.1b, we introduce a language model to extract linguistic features from gloss sequences, which are then combined with lo-cal and global temporal visual features with our recurrent cross-modal context fusion strategy. Specifically, the lan-guage model is first pre-trained on gloss sequences of train-ing data. At the start of the training, the video and a blank gloss are as inputs to the spatial module and language model, respectively. Then, as an example shown in Fig.3, the gloss tokens predicted at each time step are collected and feed it into the language model recurrently. In addition, we present a novel contextualized sequence transduction method that further incorporates the context of gloss se-quence into sequence transduction by making a dependence assumptions between the glosses. Specifically, we present a conditional gloss decoder that adopts all the predicted gloss as additional input for the prediction of the next gloss. We also introduce a sequence-level transduction calibration to counter the exposure bias [33] of sequence mapping meth-ods with dependence assumptions.
Extensive experiments conducted on three large-scale sign language recognition datasets: Phoenix-2014 [20],
Phoenix-2014-T [3], and CSL-Daily [39], demonstrate that our proposed C2ST effectively utilizes gloss sequences and achieves a significant improvement over the state-of-the-art approach.
The main contributions are summarized as follows:
• A cross-modal context learning framework is proposed for CSLR, which effectively incorporates knowledge of gloss sequences into visual representations for bet-ter sign video modeling.
• A contextualized sequence transduction loss is intro-duced for CSLR, which leverages the contextual infor-mation of the previous gloss sequence to predict the current one, rather than making the conditional inde-pendence assumption in gloss prediction.
• The proposed C2ST method achieves state-of-the-art performance on three large-scale sign language recog-nition datasets and outperforms state-of-the-art meth-ods by a large margin. 2.