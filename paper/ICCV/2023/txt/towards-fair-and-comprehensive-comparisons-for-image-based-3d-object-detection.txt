Abstract
In this work, we build a modular-designed codebase, for-mulate strong training recipes, design an error diagnosis toolbox, and discuss current methods for image-based 3D object detection. In particular, different from other highly mature tasks, e.g., 2D object detection, the community of image-based 3D object detection is still evolving, where methods often adopt different training recipes and tricks resulting in unfair evaluations and comparisons. What is worse, these tricks may overwhelm their proposed de-signs in performance, even leading to wrong conclusions.
To address this issue, we build a module-designed code-base and formulate uniﬁed training standards for the com-munity. Furthermore, we also design an error diagnosis toolbox to measure the detailed characterization of detec-tion models. Using these tools, we analyze current meth-ods in-depth under varying settings and provide discus-sions for some open questions, e.g., discrepancies in con-clusions on KITTI-3D and nuScenes datasets, which have led to different dominant methods for these datasets. We hope that this work will facilitate future research in image-based 3D object detection. Our codes will be released at https://github.com/OpenGVLab/3dodi. 1.

Introduction
As a new and rapidly developing research ﬁeld, vision-based 3D object detection [49] shows promising potential in autonomous driving and attracts lots of attention from both academia and industry. Thanks to the unremitting efforts of numerous researchers, lots of advanced technologies, such as model designs [3, 46, 29], detection pipelines [95, 83, 27], and challenging datasets [21, 6, 75], are continuously
Corresponding author proposed, which signiﬁcantly promotes the development of this research ﬁeld.
However, although lots of breakthroughs in detection ac-curacy and inference speed have been achieved, there are still some critical problems to be solved, especially the stan-dardization of evaluation protocols. Speciﬁcally, compared with the encouraging developments at the technique level, the conventional rules in model building, training recipes, and evaluation are not well-deﬁned. As shown in Table 1, existing methods generally adopt different settings, e.g. backbones, training strategies, augmentations, etc., to build and evaluate their models, and the commonly used bench-marks (and most of the papers) only record the ﬁnal accu-racy. This makes the comparison of the detectors unfair and may lead to misleading conclusions.
In this paper, we aim to provide a uniﬁed platform for image-based 3D object detectors and standardize the proto-cols in model building and evaluation. Speciﬁcally, simi-lar to the advanced codebases in 2D detection [9, 86], we decompose the image-based 3D detection frameworks into several separate components, e.g. backbones, necks, etc., and provide a uniﬁed implementation for current methods.
Furthermore, we fully investigate existing algorithms and formulate several efﬁcient training protocols, e.g. training schedules, data augmentation, etc. Our summarized train-ing recipes not only provide a fair environment for evalua-tion but also signiﬁcantly improve the performance of cur-rent methods, particularly in the KITTI-3D dataset. For ex-ample, the methods [51, 46] published two years ago trained with our recipes can achieve better (or similar) performance than the recent works [29, 55], emphasizing the importance of establishing a standard training recipe.
Furthermore, to systematically analyze the bottlenecks and issues in image-based 3D object detection, we urgently require a tool to thoroughly examine detection results. In-spired by TIDE [2], we propose an error diagnosis toolbox, named TIDE3D, to measure the detailed characterization of
GUPNet [46]
FCOS3D [81]
PGD [80]
BEVDet [27] backbone
DLA34-DLAUp
ResNet101-DCN-FPN
ResNet101-DCN-FPN
ResNet50-FPN-LSS
# epochs 140
-48
-KITTI data aug.
ﬂip, crop
-ﬂip
-others
--TTA
-# epochs
-12 24 24 nuScenes data aug.
-ﬂip
ﬂip
ﬂip, BEV aug. others
-TTA
TTA
TTA
Table 1: Overview of example methods on KITTI-3D and nuScenes benchmarks. The existing methods generally adopt different settings, e.g. backbones, epochs, augmentations, etc., in model training. Besides, KITTI-3D and nuScenes apply different evaluation metrics and are dominated by different detection pipelines. TTA denotes the test-time augmentation. detection algorithms. Speciﬁcally, we decouple the detec-tion errors into seven types, and then independently quan-tify the impact of each error type by calculating the over-all performance improvement of the model after ﬁxing the speciﬁed errors. In this way, we can analyze a speciﬁc as-pect of given algorithms while isolating other factors. In addition to characterizing the detailed features of the mod-els, the proposed TIDE3D also has other useful applica-tions. For example, the effectiveness and action mechanism of a speciﬁc design/module can be explored by analyzing the change of error distribution of the baseline model with or without the target design/module.
Besides, we also ﬁnd the most concerned two datasets, i.e. KITTI-3D [21] and nuScenes [6], are dominated by dif-ferent detection pipelines, and even gradually become sep-arate research communities. For example, none of the pop-ular Bird’s Eye View (BEV) detection methods provide the
KITTI-3D results, and the top-performing methods in the
KITTI-3D leaderboard also hard to achieve good results in nuScenes. We apply the cross-metric evaluation (i.e. ap-plying the nuScenes-style metrics on the KITTI-3D dataset and vice versa) on the representative models and report the adopted metric is the main factor causing this phenomenon.
We also provide TIDE3D analyses for this issue.
To summarize, the contributions of this work are as fol-lows: First, we build a modular-designed codebase for the community of image-based 3D object detection, which can serve as a foundation for future research and algorithm im-plementation. Second, we investigate the training settings and formulate standard training recipes for this task. Third, we provide an error diagnosis toolbox that can quantita-tively analyze the detection models at a ﬁne-grained level.
Last, we discuss some open problems in this ﬁeld, which may provide insights for future research. We hope our code-base, training recipes, error diagnosis toolbox, and discus-sions will promote better and more standardized research practices within the image-based 3D object detection com-munity. 2.