Abstract
The recent introduction of the large-scale, long-form
MAD and Ego4D datasets has enabled researchers to in-vestigate the performance of current state-of-the-art meth-ods for video grounding in the long-form setup, with inter-esting findings: current grounding methods alone fail at tackling this challenging task and setup due to their in-ability to process long video sequences. In this paper, we propose a method for improving the performance of natu-ral language grounding in long videos by identifying and pruning out non-describable windows. We design a guided grounding framework consisting of a Guidance Model and a base grounding model. The Guidance Model emphasizes describable windows, while the base grounding model ana-lyzes short temporal windows to determine which segments accurately match a given language query. We offer two de-signs for the Guidance Model: Query-Agnostic and Query-Dependent, which balance efficiency and accuracy. Ex-periments demonstrate that our proposed method outper-forms state-of-the-art models by 4.1% in MAD and 4.52% in Ego4D (NLQ), respectively. Code, data and MAD’s audio features necessary to reproduce our experiments are available at: https://github.com/waybarrios/guidance-based-video-grounding. 1.

Introduction
The task of grounding natural language in long-form videos within large-scale datasets, such as MAD [33] and
Ego4D [12], can be particularly challenging due to the pos-sibility of encountering many video segments that do not contain any interesting or relevant moments to search for.
Given the large search space, it is critical to develop ap-proaches that can quickly scan the video and identify query-able moments via natural language.
For instance, Figure 1 illustrates various moments oc-curring in a long-form video, where certain moments could have greater relevance than others based on the video’s sto-rytelling. The dialogue moment, which spans more than
Figure 1: Describable Window. We depict the difference be-tween describable and non-describable windows. The former are temporal windows with relevant visual and auditory events that are likely to contain one or more noteworthy moments. The lat-ter can be categorized as “boring” video segments (temporal win-dows) where little happens and no moment of interest happens for grounding. three minutes, would only be described as “two people sit-ting at a table”, whereas the action scene contains many moments that can be described, such as “a girl falling into a pool”, “a dog opening a fence”, and “a dog jumps to rescue a little girl”. This demonstrates that certain slices within a video can contain numerous notable moments that users would be highly interested in searching for.
In consequence, we introduce the concept of Describ-able windows (as shown in Figure 1), which are video slices shorter than the original long-form video and have a high probability of containing remarkable and relevant visual moments that can be queried through natural lan-guage. Conversely, video slices that do not contain such visually-rich moments that cannot be queried through natural language are considered non-describable windows.
Our work builds on the hypothesis that certain video segments (non-describable windows) are difficult to narrate effectively by natural language, which can lead to noisy predictions when retrieving moments via natural language queries, particularly for long videos. In fact, state-of-the-art grounding models achieve remarkable performance when analyzing short videos [31, 16, 26]; however, their capabili-ties seem to saturate when tested on longer videos [33]. For example, VLG-Net [34] achieves only marginal gains com-pared to a simple yet effective zero-shot approach based on
CLIP [29], and some models outright fail to achieve reason-able performance1. The above statements highlight the need to detect video slices corresponding to the “important” parts of a long-form video, i.e., those that contain as many mo-ments as possible. Our intuition is that when analyzing the entire long-form video, grounding models might overlook the most critical segments of the video, which represents a gap in the current research and motivates us to develop more sophisticated techniques that can capture certain intricacies using language cues or audio cues on top of the visual in-formation.
To address this issue, we propose a guided grounding framework [1, 10] comprised of two core components: a
Guidance Model that specializes emphasizing describable windows and a base grounding model that analyzes short temporal windows to determine which temporal segments match a given query accurately.
Observing multimodal cues is key for detecting describ-able windows. For instance, let us suppose we want to find the moment when “the dog jumps to rescue the little girl” (Figure 1). Here, the water splash (sound) and dog jump-ing (visual) are hints suggesting that lots of visual activi-ties are happening in the scene. This intuition motivates the multimodal design of our Guidance Model. In practice, our model jointly encodes video and audio over an extended temporal window using a transformer encoder [35].
The proposed Guidance Model can also be used in two different frameworks: Query Agnostic and Query Depen-dent. The former pre-computes which parts of a video have a low-probability of containing describable windows, making it suitable for real-time applications or limited computational resources.
In contrast, the latter provides more precise results by identifying irrelevant parts of a video based on a given text query, at the cost of being more computationally expensive as the number of queries grows.
We also highlight the fact that the two-stage approach can be adapted to any grounding method, making it a flexible and versatile option for a wide range of applications. 1We trained Moment-DETR [17] from scratch on the MAD [33] dataset and found that, even though it achieves good grounding performance in short videos, it utterly fails in long videos. More details can be found in the supplementary material.
In summary, our contributions are three-fold: 1. We propose a two-stage guided grounding framework: a general approach for boosting the performance of current state-of-the-art grounding models in the long-form video setting. 2. We introduce a Guidance Model that is capable of de-tecting describable windows: a temporal window that contains one or more noteworthy moments. 3. Through extensive experiments, we empirically show the effectiveness of our two-stage approach. In partic-ular, we validate the benefits of leveraging a Guidance
Model that specializes in finding describable windows.
We improve state-of-the-art performance on the MAD dataset [33] by 4.1% and improve the performance of an Ego4D baseline model [12, 44] in the NLQ task by 4.52%. 2.