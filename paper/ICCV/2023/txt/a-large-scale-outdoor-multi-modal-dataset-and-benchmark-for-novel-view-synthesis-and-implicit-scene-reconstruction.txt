Abstract
Neural Radiance Fields (NeRF) [24] has achieved impres-sive results in single object scene reconstruction and novel view synthesis, as demonstrated on many single modality and single object focused indoor scene datasets like DTU [14],
BMVS [42], and NeRF Synthetic [24]. However, the study of NeRF on large-scale outdoor scene reconstruction is still limited, as there is no unified outdoor scene dataset for large-scale NeRF evaluation due to expensive data acquisition and calibration costs. In this work, we propose a large-scale outdoor multi-modal dataset, OMMO dataset, containing complex objects and scenes with calibrated images, point clouds and prompt annotations. A new benchmark for sev-eral outdoor NeRF-based tasks is established, such as novel view synthesis, diverse 3D representation, and multi-modal
NeRF. To create the dataset, we capture and collect a large number of real fly-view videos and select high-quality and high-resolution clips from them. Then we design a quality review module to refine images, remove low-quality frames and fail-to-calibrate scenes through a learning-based au-tomatic evaluation plus manual review. Finally, volunteers are employed to label and review the prompt annotation for each scene and keyframe. Compared with existing NeRF datasets, our dataset contains abundant real-world urban and natural scenes with various scales, camera trajectories, and lighting conditions. Experiments show that our dataset can benchmark most state-of-the-art NeRF methods on dif-ferent tasks. The dataset can be found at the following link: https://ommo.luchongshan.com/ .
Figure 1. A city scene example from our dataset captured with low illuminance and circle-shaped camera trajectory. We show multi-view calibrated images, the camera track, and text descriptions of the scene. Some details in colored boxes are zoomed in to indicate that our dataset can provide real-world high-fidelity texture details. 1.

Introduction
Recent advances in implicit neural representations have achieved remarkable results in photo-realistic novel view synthesis and high-fidelity surface reconstruction [44, 43].
Unfortunately, most of the existing methods focus on single
*Corresponding author. objects or indoor scenes [44, 43, 9, 15, 5], and their synthesis performance will decrease drastically if migrated to outdoor scenes. Although some very recent methods try to solve this problem and are well-designed for large scenes [36, 40], their performance is difficult to compare due to the lack of large-scale outdoor scene datasets and uniform benchmarks.
At present, the existing outdoor scene datasets are either
Table 1. Comparison with existing NeRF datasets, especially those outdoor datasets related to ours. The first group is single object datasets, the second group is large scenes datasets (or outdoor parts), and the last row is our dataset. For each dataset, we show the number of scenes and images, the diversity of scene types (DT ), camera trajectories (DC ), and lighting conditions (DL), and whether they are real-world scenes (Real), and whether they have multi-modal data (M modal).
Datasets
#Scenes
#Images DT DC DL Real M modal
DTU [14]
NeRF [24]
Scannet [7]
T & T [18]
BMVS [42]
Urban3D [22]
Quad 6k [6]
Mill 19 [36]
Block-NeRF [35]
Ours 124 18 1.5K 6 28 16 1 2 an entire city 33 4.2K 3551 2.5M 88k 5k 10.4K 5.1K 3.6K 2.8M 14.7K
No
No Yes
No Yes No
No Yes No
No
No
No
No
Yes No
Yes No
No
No Yes No
Yes No
No
Yes No Yes
Yes Yes Yes
Yes
Yes
Yes
Yes
Part
Part
Yes
Yes
Yes
Yes
No
No
No
No
No
No
No
No
No
Yes collected with a small geographical scale, or rendered from not-real virtual scenes. For example, Tanks and temples [18] provides a benchmark of realistic outdoor scenes captured by a high-precision industrial laser scanner, but its scene scale is still too small (463m2 on average) and only focuses on a sin-gle outdoor object or building. The BlendedMVS [42] and
UrbanScene3D [22] datasets contain scene images rendered from reconstructed or virtual scenes, which deviate from the real scene in both texture and appearance details. Collecting images from the Internet can theoretically build very effec-tive datasets [13, 1], like ImageNet [8] and COCO [21], but these methods are not suitable for NeRF-based task evalua-tion due to the changes of objects and lighting conditions in the scene at different times. Our dataset acquisition method is similar to Mega-NeRF [36], which captures large real-world scenes by drones. But Mega-NeRF only provides two monotonic scenes, which hinders it from being a widely used baseline. Therefore, to our knowledge, no uniform and widely recognized large-scale scene dataset is built for
NeRF benchmarking, causing large-scale NeRF research for outdoor far fall behind that for single objects or indoor scenes [14, 42, 24, 7].
To address the lack of large-scale real-world outdoor scene datasets, we introduce a well-selected fly-view multi-modal dataset. The dataset contains totally 33 scenes with prompt annotations, tags, and 14K calibrated images (cf.
Fig. 4). Different from the existing methods mentioned above, the sources of our scenes are very extensive, includ-ing those collected on the Internet and captured by ourselves.
Meanwhile, the collection indicators are also comprehen-sive and representative, including various scene types, scene scales, camera trajectories, lighting conditions, and multi-modal data that are not available in existing datasets (cf.
Tab. 1). More importantly, we provide a generic pipeline to generate real-world NeRF-based data from drone videos on the Internet, which makes our dataset easily to be extensible by the community.
Further, to evaluate the applicability and performance
Figure 2. Visual comparison with existing large-scale satellite-view outdoor datasets [40] acquired from Google Earth Studio. The top row is from [40], and the bottom row is corresponding scenes from our fly-view dataset, which is more realistic with clear textures and rich details (zoom-in for the best of views). of the built dataset for evaluating mainstream NeRF meth-ods, we build all-around benchmarks including novel view synthesis, scene representations, and multi-modal synthesis based on the dataset. Moreover, we provide several detailed sub-benchmarks for each above task, according to different scene types, scene scales, camera trajectories and lighting conditions, to give a fine-grained evaluation of each method.
We aim to establish a general large-scale outdoor NeRF-based benchmark and promote NeRF-related algorithm re-search on novel view synthesis, surface reconstruction, and multi-modal. To summarize, our main contributions include:
• Aiming at advancing the large-scale NeRF research, we introduce an outdoor scene dataset captured from the real world with multi-modal data, which surpasses all existing relative outdoor datasets in both quantity and diversity, see Tab. 1 and Sec. 3.3.
• To form a uniform benchmarking standard for outdoor
NeRF methods, we create multiple benchmark tasks for mainstream outdoor NeRF methods. Extensive experi-ments show that our dataset can well support common
NeRF-based tasks and provide prompt annotations for future research, see Sec. 4.
• We provide a cost-effective pipeline for converting videos that can be flexibly accessed from the Internet to
NeRF-purpose training data, which makes our dataset easily scalable, see Sec. 3.1 and Sec. 3.2. 2.