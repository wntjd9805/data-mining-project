Abstract
Recent work in unsupervised multi-object segmentation shows impressive results by predicting motion from a sin-gle image despite the inherent ambiguity in predicting mo-tion without the next image. On the other hand, the set of possible motions for an image can be constrained to a low-dimensional space by considering the scene structure and moving objects in it. We propose to model pixel-wise ge-ometry and object motion to remove ambiguity in recon-structing ﬂow from a single image. Speciﬁcally, we divide the image into coherently moving regions and use depth to construct ﬂow bases that best explain the observed ﬂow in each region. We achieve state-of-the-art results in unsuper-vised multi-object segmentation on synthetic and real-world datasets by modeling the scene structure and object motion.
Our evaluation of the predicted depth maps shows reliable performance in monocular depth estimation. 1.

Introduction
Finding objects on visual data is one of the oldest prob-lems in computer vision, which has been shown to work to great extent in the presence of labeled data. Achieving it without supervision is important given the difﬁculty of obtaining pixel-precise masks for the variety of objects en-countered in everyday life. In the absence of labels, motion provides important cues to group pixels corresponding to objects. The existing solutions use motion either as input to perform grouping or as output to reconstruct as a way of verifying the predicted grouping. The current methodology fails to incorporate the underlying 3D geometry creating the observed motion. In this work, we show that modeling ge-ometry together with object motion signiﬁcantly improves the segmentation of multiple objects without supervision.
Unsupervised multi-object discovery is signiﬁcantly more challenging than the single-object case due to mutual occlusions. Therefore, earlier methods in unsupervised seg-mentation focused on separating a foreground object from the background whereas multi-object methods have been mostly limited to synthetic datasets or resorted to additional supervision on real-world data such as sparse depth [15].
While sparse-depth supervision can be applied to driving scenarios [15], depth information is not typically available on common video datasets. Moreover, video segmentation datasets such as DAVIS [48, 49] contain a wide variety of categories under challenging conditions such as appearance changes due to lighting conditions or motion blur. The mo-tion information can be obtained from video sequences via optical ﬂow. Optical ﬂow not only provides motion cues for grouping [64] but can also be used for training on synthetic data without suffering from the domain gap while transfer-ring to real data [63]. The problems in optical ﬂow predic-tion on real-world data can be mitigated to some extent by relating ﬂow predictions from multiple frames [63].
In addition to problems in predicting optical ﬂow, requir-ing ﬂow as input prohibits the application of the method on static images. Another line of work [11, 33] uses motion for supervision at train time only. Based on the observation that objects create distinctive patterns in ﬂow, initial work [11]
ﬁts a simple parametric model to the ﬂow in each object re-gion to capture the object motion. This way, the network can predict object regions that can potentially move coher-ently from a single image at test time. There is an inherent ambiguity in predicting motion from a single image. There-fore, the follow-up work [33] predicts a distribution of pos-sible motion patterns to reduce this ambiguity. This also allows extending it to the multi-object case by mitigating the over-segmentation problem of the initial work [11].
In this work, we propose to model pixel-wise geometry to remove ambiguity in reconstructing ﬂow from a single image. Optical ﬂow is the difference between the 2D pro-jections of the 3D world in consecutive time steps. By mod-eling the 3D geometry creating these projections, we di-rectly address the mutual occlusion problem due to interac-tions of multiple objects. This problem has been crudely ad-dressed by previous work with a depth-ordered layer repre-sentation [63]. Instead of assuming a single depth layer per object, we predict pixel-wise depth which provides more expressive power in explaining the observed motion. Fur-thermore, we do not use ﬂow as input during inference, al-lowing us to evaluate our method on single-image datasets.
Recent work [5] showed that motion resides in a low-dimensional subspace, and its reconstruction can be used to supervise monocular depth prediction. Despite many possi-ble ﬂow ﬁelds, the space of possible ﬂow ﬁelds is spanned by a small number of basis ﬂow ﬁelds related to depth and independently moving objects. While [5] focuses on mod-eling camera motion for quantitatively evaluating depth in static scenes, it also points to the fact that the object motion can be similarly modeled in a low-dimensional subspace by simply masking the points in the object region. Given the difﬁculty of predicting pixel-wise masks, simple object em-beddings are used to cluster independently moving objects.
We instead predict the object regions jointly with depth to
ﬁnd the low-dimensional object motion that best explains the observed ﬂow in each region.
Our approach works extremely well on synthetic Multi-Object Video (MOVi) datasets [23], signiﬁcantly outper-forming previous work, especially in more challenging
MOVi-{C,D,E} partitions and performing comparably on visually simpler MOVi-A due to difﬁculty of estimating depth. We use motion only for supervision at train time, therefore our method can be successfully applied to still im-ages of CLEVR [31] and CLEVRTEX [34] and shows state-of-the-art performance. More impressively, our method can segment multiple objects on real-world videos of DAVIS-2017 [49] from a single image at test time, exceeding the performance of the state-of-the-art that uses ﬂow from mul-tiple frames as input [63]. In addition to evaluating segmen-tation, we show that our method can also reliably predict depth in real-world self-driving scenarios on KITTI [21]. 2.