Abstract
In this paper, we propose a novel task IntentQA, a special
VideoQA task focusing on video intent reasoning, which has become increasingly important for AI with its advantages in equipping AI agents with the capability of reasoning be-yond mere recognition in daily tasks. We also contribute a large-scale VideoQA dataset for this task. We propose a
Context-aware Video Intent Reasoning model (CaVIR) con-sisting of i) Video Query Language (VQL) for better cross-modal representation of the situational context, ii) Con-trastive Learning module for utilizing the contrastive con-text, and iii) Commonsense Reasoning module for incor-porating the commonsense context. Comprehensive experi-ments on this challenging task demonstrate the effectiveness of each model component, the superiority of our full model over other baselines, and the generalizability of our model to a new VideoQA task. The dataset and codes are open-sourced at: https://github.com/JoseponLee/IntentQA.git. 1.

Introduction
Among the recent flourishing studies on cross-modal vision-language understanding, video question answering (VideoQA) is one of the most prominent to support interac-tive AI with the ability to understand and communicate dy-namic visual scenarios via natural languages [75]. Despite its popularity, VideoQA is still quite challenging, because it demands the models to comprehensively understand the videos to correctly answer questions, which include not only factual but also inferential ones. The former (factoid
VideoQA) directly asks about the visual facts (e.g., humans, objects, actions, etc.), while the latter (inference VideoQA) requires logical reasoning of latent variables (e.g., the spa-tial, temporal and causal relationships among entities, men-tal states, etc.) beyond observed visual facts [75]. The
Figure 1: Illustration of challenges brought by varied con-texts in video intention reasoning. The same action under different contexts could mean different underlying intents. future trend for AI is to study inference VideoQA beyond factoid VideoQA [75], requiring more reasoning ability be-yond mere recognition. In this paper, we propose a new task called IntentQA, i.e., a special kind of inference VideoQA that focuses on intent reasoning.
Intent understanding is a key building block of human intelligence. Humans have a strong inclination to interpret events as a series of goals driven by intentions [10, 58, 59].
In fact, humans do not encode the entirety of action details but rather interpret actions in terms of intentions and store these interpretations for later retrieval [3]. As a fundamen-tal organizing principle that regulates how humans compre-hend one another and act in the environment, the concept of intent has been awarded a central position within social intelligence and should thus be an essential component of future AI [76, 17]. However, as far as we know, there is no
VideoQA work focusing on intent understanding. There-fore, we believe our proposed new task is a great contribu-tion to the development of intent reasoning in VideoQA.
The biggest challenge for video intent reasoning is con-text because intent understanding is quite context-sensitive.
As illustrated in Fig. 1, humans can interpret different in-tents underlying the same action ‘point to a cup’ given dif-ferent video contexts along with commonsense knowledge.
The intent is more likely to be ‘give me water’ if the given context is ‘a table at a restaurant’, and ‘clean the cup’ if the context is ‘a sink full of dirty cups’, and ‘look at the cup’ if the context is ‘a store selling beautiful cups’. The uncertainty does not come from the protruding finger, but from the context, which is the key to solving the overloaded signal and the ‘dark matter’ mystery. Here, the context in-cludes the immediate communicative context, the shared experience, and the commonsense. Context-aware reason-ing ability plays a significant role in human intelligence.
We contribute a new dataset for IntentQA, as detailed in Section 3. We also propose a model with three key modules that deals with three major contexts respectively: (I) Situational Context; (II) Contrastive Context; (III)
Commonsense Context. Module I (Video Query Language (VQL)) integrates cross-modal contextual information from both videos and languages. Module II (Contrastive Learn-ing) learns to reason from contrasting a triplet of anchor, positive and negative samples. Module III (Commonsense
Reasoning) further incorporates the commonsense knowl-edge from the large language model.
Our main contributions can be summarized as follows.
First, we propose a new task IntentQA, a special VideoQA task focusing on intent reasoning. Given a video and a ques-tion, the aim is to select the correct answer with the under-standing of intent. Second, we collect and annotate a large-scale VideoQA dataset with natural social scene videos. Fi-nally, we propose a Context-aware Video Intent Reasoning model (CaVIR) and provide benchmark results. 2.