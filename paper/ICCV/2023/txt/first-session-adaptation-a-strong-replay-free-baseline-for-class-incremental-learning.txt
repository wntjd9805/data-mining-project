Abstract that can be applied to a set of unlabelled inputs which is predictive of the benefits of body adaptation.
In Class-Incremental Learning (CIL) an image classifi-cation system is exposed to new classes in each learning session and must be updated incrementally. Methods ap-proaching this problem have updated both the classification head and the feature extractor body at each session of CIL.
In this work, we develop a baseline method, First Session
Adaptation (FSA), that sheds light on the efficacy of exist-ing CIL approaches, and allows us to assess the relative performance contributions from head and body adaption.
FSA adapts a pre-trained neural network body only on the first learning session and fixes it thereafter; a head based on linear discriminant analysis (LDA), is then placed on top of the adapted body, allowing exact updates through CIL. FSA is replay-free i.e. it does not memorize examples from pre-vious sessions of continual learning. To empirically moti-vate FSA, we first consider a diverse selection of 22 image-classification datasets, evaluating different heads and body adaptation techniques in high/low-shot offline settings. We find that the LDA head performs well and supports CIL out-of-the-box. We also find that Featurewise Layer Modulation (FiLM) adapters are highly effective in the few-shot setting, and full-body adaption in the high-shot setting. Second, we empirically investigate various CIL settings including high-shot CIL and few-shot CIL, including settings that have pre-viously been used in the literature. We show that FSA sig-nificantly improves over the state-of-the-art in 15 of the 16 settings considered. FSA with FiLM adapters is especially performant in the few-shot setting. These results indicate that current approaches to continuous body adaptation are not working as expected. Finally, we propose a measure 1.

Introduction
Continual learning (CL) is needed to bring machine learning models to many real-life applications. After a model is trained on a given set of data, and once deployed in its test environment, it is likely that new classes will naturally emerge. For example, in an autonomous driving scenario, new types of road transportation and traffic sig-nage can be encountered. The deployed model needs to efficiently acquire this new knowledge with minimal cost (e.g. annotation requirements) without deteriorating the per-formance on existing classes of objects. The process of ef-ficiently acquiring new knowledge while preserving what is already captured by the model is what continual learning methods target.
Continual learning can be mainly divided into task in-cremental learning and class incremental learning. Task incremental learning (TIL) sequentially learns independent sets of heterogeneous tasks and is out of this paperâ€™s scope.
Class incremental learning (CIL), in contrast, assumes that all classes (already learnt and future ones) form part of a sin-gle classification task. In class-incremental learning, data arrive in a sequence of sessions with new classes appearing as the sessions progress. Critically, the data in each ses-sion cannot be stored in its entirety and cannot be revisited.
The goal is to train a single classification model under these constraints.
In both task incremental and class incremental learning,
the data distribution will change as the sessions progress.
However, these changes tend to be smaller in real-world class incremental learning settings than in the task incre-mental setting. For, example consider a human support robot learning about new objects in a home in an incremen-tal manner (CIL) vs. the same robot learning to adapt to different homes (TIL).
Practically, CIL has two major uses. First, in situations where large amounts of data arrive in each session and so retraining the model is computationally prohibitive. Sec-ond, in situations where data are not allowed to be revisited due to privacy reasons such as under General Data Protec-tion Regulation (GDPR). The latter situation is relevant for applications such as personalization where small numbers of data points are available i.e. few-shot continual learning.
So-called replay-free methods are necessary for such set-tings, as samples of previous sessions are not allowed to be memorized, but CIL is known to be challenging in such settings.
Current SOTA methods for class incremental learning start from a pre-trained backbone [13, 19, 35, 34] and then adapt the features at each session of continual learning. The use of a pre-trained backbone has been shown to lead to strong performance especially in the few-shot CIL setting due to the lack of data [21]. However, it is unclear to what extent continuous adaption of the features in each session is helpful. In general, in CIL there is a trade-off between adaptation (which helps adapt to the new statistics of the target domain) and catastrophic forgetting (whereby earlier classes are forgotten as the representation changes). When memorization of previous samples is restricted, the bene-fits from adapting the feature extractor body to learn better features might well be out-weighted by the increase in for-getting of old classes. Moreover, body adaptation in earlier sessions is arguably more critical (e.g. adapting the back-bone to the new domain in the first session), whilst it is less essential in later sessions where the changes in the ideal rep-resentation between sessions are smaller.
This work explores under what conditions continuous adaptation of the body is beneficial, both in principle and in current practice. In order to do this, we develop a new replay-free method, inspired by the first encoding method of [2], called First Session Adaptation (FSA). FSA adapts the body of a pre-trained neural network on only the first session of continual learning. We investigate adapting all body parameters and the use of Feature-wise Layer Modula-tion (FiLM) adapters [23] which learn only a small number of parameters. The head, in contrast, is adapted at each ses-sion using an approach that is similar to Linear Discriminate
Analysis (LDA), which suffers from no forgetting, and im-proves over the Nearest Class Mean classifier (NCM) [18] approach. The efficacy of this general approach, includ-ing comparisons to a standard linear head, is first motivated through experiments in the offline setting (Sec. 4.2). We then carry out experiments under three CIL settings. First, we consider the high-shot setting (high-shot CIL). The other two settings consider few-shot continual learning. Specifi-cally, one setting follows previous approaches that employ an initial session with a large number of data points and few-shot sessions thereafter (few-shot+ CIL) while the other ex-clusively includes sessions with only a small amount of data (few-shot CIL).
The contributions of the paper are as follows: (1) We develop a replay-free CIL baseline, namely FSA, that is ex-tremely simple to implement and performs well in many dif-ferent scenarios. (2) We empirically motivate FSA through a set of offline experiments that evaluate different forms of neural network head and body adaptation; (3) We then compare FSA to a set of strong continual learning baseline methods in a fair way using the same pre-trained backbone for all methods. (4) We show that the FSA-FiLM baseline performs well in the high-shot CIL setting, outperforming the SOTA whilst avoiding data memorization. (5) In the few-shot learning settings, FSA outperforms existing con-tinual learning methods on eight benchmarks, often by a substantial margin, and is statistically tied with the best per-forming method on the one remaining dataset. (6) Finally, we propose a measure that can be applied to a set of un-labelled inputs which is predictive of the benefits of body adaptation. 2.