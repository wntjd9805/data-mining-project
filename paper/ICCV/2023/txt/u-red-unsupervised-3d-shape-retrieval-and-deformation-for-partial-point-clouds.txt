Abstract
In this paper, we propose U-RED, an Unsupervised shape REtrieval and Deformation pipeline that takes an ar-bitrary object observation as input, typically captured by
RGB images or scans, and jointly retrieves and deforms the geometrically similar CAD models from a pre-established database to tightly match the target. Considering existing methods typically fail to handle noisy partial observations,
U-RED is designed to address this issue from two aspects.
First, since one partial shape may correspond to multiple potential full shapes, the retrieval method must allow such an ambiguous one-to-many relationship. Thereby U-RED learns to project all possible full shapes of a partial target onto the surface of a unit sphere. Then during inference, each sampling on the sphere will yield a feasible retrieval.
Second, since real-world partial observations usually con-tain noticeable noise, a reliable learned metric that mea-sures the similarity between shapes is necessary for stable retrieval. In U-RED, we design a novel point-wise residual-guided metric that allows noise-robust comparison. Exten-sive experiments on the synthetic datasets PartNet, Comple-mentMe and the real-world dataset Scan2CAD demonstrate that U-RED surpasses existing state-of-the-art approaches by 47.3%, 16.7% and 31.6% respectively under Chamfer
Distance. 1.

Introduction 3D semantic scene perception [36, 56] involves the de-composition of a scene into its constituent objects, under-standing and reconstructing all the detected objects, and putting them in place to formulate a holistic scene represen-tation. Significant progress has been made recently in at-taining the comprehensive analysis of multiple objects’ ge-*Authors with equal contributions.
Codes: https://github.com/ZhangCYG/U-RED
Figure 1. Given a segmented RGB image with estimated depth or a partial noisy object scan, U-RED utilizes an unsupervised joint
R&D network to retrieve the most suitable CAD model from the database and deform it to tightly fit the target object. After aligning all deformed shapes to the target scene via predicted poses [18], a compact CAD scene representation is generated. ometry [17, 30, 42, 57, 61], dynamic reconstruction of both scene and objects [11, 12], structure-aware scene comple-tion [5], etc. These methods demonstrate promising re-sults in overall reconstruction quality but typically fail in preserving fine-grained geometric structures. To address this problem, Retrieval and Deformation (R&D) meth-ods [8, 25, 35, 40, 46, 47, 49, 54] are proposed. They lever-age a pre-prepared 3D shape database (usually represented as CAD models) as prior knowledge and typically follow a two-stage scheme to generate a clean and compact scene representation. First, based on manually picked metrics, the most similar shape of the target is selected from the database. Then, the retrieved shape is scaled, aligned and rotated to match the target.
However, these methods suffer from two challenges, making them vulnerable to noise and partial observations.
First, a partial shape may correspond to multiple full shapes.
For example, if only a plane is observed, it may be the back
or the seat of a chair. Without additional prior information, the correspondence is totally ambiguous. Directly applying supervision using a single ground truth may result in erro-neous or undesired results. Therefore, the retrieval network should allow a one-to-many (OTM) retrieval. Second, due to challenging illumination conditions and inherent sensor limitations, noisy observations are common in real-world scenarios. Thereby, a learned noise-robust metric to mea-sure similarity among shapes is essential for retrieving the most similar source shapes to the observed shape.
To handle these challenges, we propose U-RED, a novel
Unsupervised joint 3D shape REtrieval and Deformation framework that is capable of effectively handling noisy, par-tial, and unseen object observations. We develop U-RED upon large-scale synthetic data that provides plenty of high-quality CAD models. We simulate real-world occlusions, sensor noise, and scan errors to generate partial point clouds of each shape as our network’s input for training, then di-rectly apply our method to challenging real-world scenes without fine tuning, since collecting 3D annotations in real scenes is laborious and requires considerable expertise.
To enable one-to-many retrieval, we propose to encapsu-late possible full shapes of the target partial observation in the surface of a high-dimensional unit sphere. Specifically, during joint training, a supplementary branch for process-ing the full shape is incorporated to extract the normalized global feature, which corresponds to a point on the surface of the sphere. The full-shape feature is concatenated with the target partial-shape feature as an indicator for individ-ual retrieval. In this manner, the retrieval network learns to interpolate different full shapes on the sphere. During in-ference, we sample uniformly on the sphere surface to yield multiple retrievals and collect unique ones as the final re-sults. Moreover, cross-branch geometric consistencies can be established based on the joint learning scheme to help the partial branch learn structure-aware features and improve robustness against noise.
For similarity metrics, existing methods like [47] directly estimate a single probabilistic score based on Chamfer Dis-tance. However, this score depends heavily on the train-ing set and is vulnerable to noise due to the unstable near-est neighbor search in Chamfer Distance [16], limiting the generalization ability of these methods, especially in real-world scenes. We take a step further by designing a novel point-wise residual-guided metric. For each point inside the target partial observations, we predict a residual vector de-scribing the discrepancy between the coordinates of its own and its nearest neighbor in the source shape. By aggregating all residual vectors and removing outliers, we calculate an average norm of the remaining vectors as the final metric.
We demonstrate that our residual-guided metric is robust to noise and can be directly applied to real-world scenes while trained only with synthetic data.
Our main contribution are summarized as follows:
• U-RED, a novel unsupervised approach capable of conducting joint 3D shape R&D for noisy, partially-observed and unseen object observations in the real world, yielding state-of-the-art performance on public synthetic PartNet [34], ComplementMe [44] and real
Scan2CAD [2] datasets.
• A novel OTM module that leverages supplementary full-shape cues to enable one-to-many retrieval and en-force geometric consistencies.
• A novel Residual-Guided Retrieval technique that is robust to noisy observations in real-world scenes. 2.