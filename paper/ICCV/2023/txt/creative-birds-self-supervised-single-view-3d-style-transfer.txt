Abstract
In this paper, we propose a novel method for single-view 3D style transfer that generates a unique 3D object with both shape and texture transfer. Our focus lies primarily on birds, a popular subject in 3D reconstruction, for which no existing single-view 3D transfer methods have been de-veloped. The method we propose seeks to generate a 3D mesh shape and texture of a bird from two single-view im-ages. To achieve this, we introduce a novel shape trans-fer generator that comprises a dual residual gated network (DRGNet), and a multi-layer perceptron (MLP). DRGNet extracts the features of source and target images using a shared coordinate gate unit, while the MLP generates spa-tial coordinates for building a 3D mesh. We also intro-duce a semantic UV texture transfer module that imple-ments textural style transfer using semantic UV segmen-tation, which ensures consistency in the semantic mean-ing of the transferred regions. This module can be widely adapted to many existing approaches. Finally, our method constructs a novel 3D bird using a differentiable renderer.
Experimental results on the CUB dataset verify that our method achieves state-of-the-art performance on the single-view 3D style transfer task. Code is available at https:
//github.com/wrk226/creative_birds. 1.

Introduction
Neural image style transfer has received increasing at-tention in computer vision communities due to its remark-able success in many automatic creations. These applica-tions mainly belong to a 2D style transfer, which transfers the artistic style of one reference image to another content image. Recently, it is extended to transfer the shape and tex-ture style of one 3D object to another for editing 3D content in augmented reality and virtual reality [41]. However, this extension needs to acquire the 3D information on objects which suffers from the significant problem of being ardu-ous, high-cost, and time-consuming. In contrast, taking a single-view object image is easier and cheaper than 3D data as cameras (e.g., mobile phones) are used widely in every-day life. Therefore, we address a new task of generating a 3D object with novel shape and texture by transferring one single-view object image to another, instead of acquiring 3D input information. We refer to this task as a single-view 3D style transfer.
Prior works have primarily handled either 3D style trans-fer or single-view 3D reconstruction but have not simulta-neously considered both for the aforementioned task. The former heavily depends on the 3D information of the object as an input, which is often expensive and time-consuming.
For example, 3DStyleNet [41] requires training on a large dataset of 3D shapes, while Neural Renderer [20] transfers the style of an image onto a 3D mesh. In addition, 3D scene stylization [12, 27] generates stylized novel view synthesis from multiple RGB images given an arbitrary style. The lat-ter approach can easily implement 3D object reconstruction using single-view images. Classical methods predict the re-constructed information of a 3D object from the single-view
images [18, 10, 19, 39, 11]. Specifically, category-Specific mesh reconstruction (CMR) [18] learns to recover the 3D mesh shape, texture, and camera pose of an object from the single images of one object category. Following CMR, unsupervised mesh reconstruction (UMR) [22] uses a self-supervision with semantic part consistency to predict a 3D object. As UMR does not require 3D information, a union between the two approaches becomes a viable strategy to tackle the single-view 3D style transfer task.
In this paper, we primarily focus on the novel 3D bird generation as it is a popular example [18, 2, 38, 22] in single-view 3D reconstruction, especially UMR [22].
Specifically, given a pair of single-view source and target bird images within and between species, we explore plausi-ble 3D bird creation by transferring the geometric and tex-tural styles of the target bird to the source, as shown in Fig. 2. First, we employ the reconstruction network in UMR
[22] as an encoder to output the shape features, UV textures, and camera poses of the target and source bird images, re-spectively. Second, we propose a shape transfer genera-tor for implementing the geometric transfer which consists of a dual residual gated network (DRGNet), a scale factor, and a multi-layer perceptron (MLP). DRGNet is designed with a shared coordinate gate unit to select both the target and source shape features, separately cascading single-layer networks and incorporating residual connections into their networks in Fig. 3. The scale factor controls the degree of the transformation from the source to the target. MLP gen-erates the spatial coordinates necessary for building a 3D mesh. Third, we introduce a semantic UV texture transfer that exploits switched gates to respectively control whether the part textural transfer or not using AdaIN [13], LST [21] and EFDM [44]. Finally, a novel 3D bird is constructed by using a differentiable renderer [24] with the pose of the source, the mesh, and the semantic stylized UV map. Over-all, our contributions are summarized as follows:
• We propose a shape transfer generator consisting of a dual residual gated network, a scale factor, and a multi-layer perceptron for the geometric style transfer.
• We present a semantic UV transfer method to utilize switched gates to respectively control whether the part textural transfer or not by using AdaIN [13], LST [21], and EFDM [44], and employing semantic UV segmen-tation for the texture style transfer.
• To the best of our knowledge, we are the first to address the single-view 3D style transfer task by combining the shape and texture transfer to automatically create novel 3D birds from images in Fig. 1. 2.