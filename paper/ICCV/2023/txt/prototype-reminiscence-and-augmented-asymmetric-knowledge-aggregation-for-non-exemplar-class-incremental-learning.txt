Abstract
Non-exemplar class-incremental learning (NECIL) re-quires deep models to maintain existing knowledge while continuously learning new classes without saving old class samples. In NECIL methods, prototypical representations are usually stored, which inject information from former classes to resist catastrophic forgetting in subsequent in-cremental learning. However, since the model continuously learns new knowledge, the stored prototypical representa-tions cannot correctly model the properties of old classes in the existence of knowledge updates. To address this prob-lem, we propose a novel prototype reminiscence mechanism that incorporates the previous class prototypes with arriv-ing new class features to dynamically reshape old class fea-ture distributions thus preserving the decision boundaries of previous tasks. In addition, to improve the model gen-eralization on both newly arriving classes and old classes, we contribute an augmented asymmetric knowledge aggre-gation approach, which aggregates the overall knowledge of the current task and extracts the valuable knowledge of the past tasks, on top of self-supervised label augmentation.
Experimental results on three benchmarks suggest the supe-rior performance of our approach over the SOTA methods. 1.

Introduction
In recent years, deep neural networks have achieved great success on various tasks. In dynamic and open en-vironments, deep models also require the ability to continu-ously learn new tasks as the input stream is updated. Hence, class-incremental learning (CIL), which aims to learn a uni-ﬁed classiﬁer that can classify all seen classes under pro-gressive changes in the classes to be learned, has attracted extensive attention [30, 41, 16, 47, 31, 28].
As new data becomes available, it is computationally ex-pensive to jointly retrain the model with new and old class
*Corresponding Author: Mang Ye (yemang@whu.edu.cn) prototype feature decision  boundary (a) Joint training
Task t
Task t+1 (b) Baseline inject information (cid:2020) (cid:2020)
Task t
Task t+1
Task t inject information
Task t+1 (c) Gaussian Noise Augmentation  (d) Prototype Reminiscence
Figure 1. Idea illustration. (a) Joint training with abundant sam-ples. (b) Baseline: Updating the model with the memorized pro-totype and new data, which narrows the decision boundary of the old classes. (c) Gaussian Noise Augmentation [66]: The decision boundary of the old class is retained, but it introduces an over-lap between the old and new class distributions due to the change in representation space. (d) With prototype reminiscence, newly learned information can be injected while reshaping the feature distribution of past data to reduce overlap and resist forgetting. samples. Worse still, the old class samples could not be fully accessible. In this case, an alternative is to ﬁne-tune the model on new data, yet catastrophic forgetting [38, 15] will be a serious challenge. The decision boundary of the uniﬁed classiﬁer would be signiﬁcantly changed and biased towards the new classes. Conversely, another direction is to
ﬁx the feature embedding space of a trained model, which suffers from frustrating generalization ability and thus per-forms poorly on new tasks, i.e., the plasticity of the model is greatly degraded.
To overcome the catastrophic forgetting issue, many CIL methods [42, 8, 6, 56, 2, 53] store a fraction of the old data in memory and replay them in subsequent incremen-tal phases to maintain the existing knowledge. Unfortu-nately, storing data poses privacy concerns and comes at a sharp cost to memory and computation. In this paper, we follow a paradigm holding for more extensive applications,
termed non-exemplar class-incremental learning (NECIL)
[66, 67], which solves the catastrophic forgetting problem in CIL scenario without preserving old class samples.
For NECIL, a natural substitute for storing data is to gen-erate pseudo-samples of previous classes by deep generative models [62, 54, 46, 49, 57] such as GAN [4, 17]. However, it is unstable and ineffective to train generative models for non-stationary data streams [65]. Catastrophic forgetting can also have a negative impact on the generative model resulting in a simultaneous decrease in the effectiveness of both models. Instead of focusing on old data, some works turn to estimating model parameters that are important for previous tasks and constraining their changes [23, 61, 3].
Nevertheless, the constraints on the model parameters lead to poor generalization ability to long-sequence tasks. Be-sides, several studies propose to dynamically expand the network structure during the process of incremental learn-ing [44, 58, 59, 37, 64]. Although this strategy can efﬁ-ciently handle long sequences of tasks and ultimately main-tain the performance of the old classes, the computational resource requirements associated with creating and storing additional network components and reasoning about multi-ple forward propagations are frustrating.
Recently, some prototype-based NECIL methods have achieved impressive performance [66, 67, 60, 50]. They use prototypical representations (typically the class mean in the deep feature space) memorized for each old class to model the feature distribution of past data and inject information from the previous classes in subsequent incremental learn-ing. Rather than storing samples, this strategy is more mem-ory efﬁcient and privacy secure. Nevertheless, as shown in
Fig. 1 (b), direct training with saved prototypes and current data struggles to prevent the collapse of decision bound-aries, due to the lack of old class features. Some works augment the prototypes by adding Gaussian noise [66] or over-sampling [67] to enrich old class features. However, updates of the model on continuous data streams could lead to inevitable changes in the representation of old classes, making the saved prototypes increasingly outdated.
The feature distribution simulated by the above strate-gies cannot accommodate such changes due to the missing consideration of knowledge updates. It could result in over-lap between the distributions of different classes, especially between new and old classes, as shown in Fig. 1 (c). Conse-quently, combining newly acquired information with stored prototypes to dynamically model past data distributions is crucial to resist catastrophic forgetting in NECIL.
To address this challenge, we propose a prototype remi-niscence mechanism to track the evolution of the old class representations by injecting new knowledge from the up-dating network while reshaping the feature distribution.
Speciﬁcally, we perform a random bidirectional interpola-tion operation between the extracted new class features and
E x t r a c t o r
F e a t u r e (Cat, 0(cid:28733)) (Cat, 90(cid:28733)) (Cat, 180(cid:28733)) (Cat, 270(cid:28733)) label augmentation task t
Cat Dog d e n i f e r r e i f i s s a l c
E x t r a c t o r
F e a t u r e (cid:28703)Cat(cid:28704) prototype (Cat, 0(cid:28733))
Knowledge Aggregation (Dog, 0(cid:28733)) (Dog, 90(cid:28733)) (Dog, 180(cid:28733)) (Dog, 270(cid:28733)) (Cat, 0(cid:28733)) (Cat, 90(cid:28733)) (Cat, 180(cid:28733)) (Cat, 270(cid:28733)) label augmentation task t+1
Figure 2. We introduce self-supervised label augmentation to learn generalizable and transferable representations. The knowledge of the self-supervised classiﬁer is aggregated and transferred to an-other classiﬁer to take full advantage of it. the saved old class prototypes to enrich old class features.
As shown in Fig. 1 (d), it expands the prototype to pro-tect the decision boundaries of old classes and to counter-act catastrophic forgetting. Since the feature distribution of past tasks is dynamically adjusted to the current represen-tation space, the overlap between the old and new classes is reduced. Thus, the discrimination and balance between the old and new classes are maintained. Cooperating with the well-known knowledge distillation (KD) [20, 19], the mismatch between the preserved prototypes and the contin-uously updated network is alleviated.
In addition to dealing with catastrophic forgetting, when new data arrives, performance on the current task is also of great concern, necessitating the plasticity of the incre-mental learner. This mainly involves two aspects: learning generalizable and transferable representations, fully utiliz-ing the information from new data. Previous works [66, 55] have achieved good progress on the ﬁrst aspect by self-supervised label augmentation [27], however they ordinar-ily disregard the second. The new task contains abundant information that the network has never encountered and will have a stronger inﬂuence on model updates. Improv-ing the plasticity of the model from continuous data streams requires a simultaneous approach from both aspects.
To solve this bottleneck, we contribute an augmented asymmetric knowledge aggregation approach to enhance the plasticity of the model noninvasively. Taking inspira-tion from [24, 27], we ﬁrst augment the new classes with  
rotation as self-supervision, which requires the model to acquire task-agnostic representations to improve its gener-alization ability. Furthermore, as illustrated in Fig. 2, we selectively aggregate the valuable knowledge in the aug-mented classiﬁer—valid weights of past tasks are extracted, and the information captured on the current task is suf-ﬁciently exploited. This asymmetric knowledge aggrega-tion scheme can condense the knowledge learned via self-supervised label augmentation (SLA) to make the classiﬁer more puriﬁed. It further improves the incremental learner’s performance on the new tasks without discrimination scari-ﬁcation on old classes.
To summarize, our main contributions are as follows:
• We propose a simple yet effective method of proto-type reminiscence for NECIL, which models feature distribution for the past data in a continuously updated representation space to resist catastrophic forgetting.
• We contribute augmented asymmetric knowledge ag-gregation, which learns task-agnostic representations and fully captures the newly acquired knowledge to improve the plasticity of incremental learners.
• Extensive experiments on three benchmarks demon-strate that our method achieves state-of-the-art perfor-mance. We also provide a detailed ablation study to analyze the inﬂuence of each component. 2.