Abstract requires
However,
Accomplishing household tasks to plan step-by-step actions considering the consequences of previous actions. the state-of-the-art em-bodied agents often make mistakes in navigating the environment and interacting with proper objects due to learning by imitating experts or algorithmic imperfect planners without such knowledge. To improve both vi-sual navigation and object interaction, we propose to consider the consequence of taken actions by CAPEAM (Context-Aware Planning and Environment-Aware Memory) that incorporates semantic context (e.g., appropriate ob-jects to interact with) in a sequence of actions, and the changed spatial arrangement and states of interacted objects (e.g., location that the object has been moved to) in inferring the subsequent actions. We empirically show that the agent with the proposed CAPEAM achieves state-of-the-art performance in various metrics using a challenging interactive instruction following benchmark in both seen and unseen environments by large margins (up to
+10.70% in unseen env.). 1.

Introduction
For decades, the research community has been pursuing the goal of building a robotic assistant that can perform ev-eryday tasks through language directives. Recent advance-ments in computer vision, natural language processing, and embodied AI have led to the development of several bench-marks aimed at encouraging research on various compo-nents of such robotic agents. These benchmarks include navigation [2, 7, 8, 24], object interaction [28, 42], and inter-active reasoning [11, 16] in visually rich 3D environments
[6,23,40]. However, for realistic assistants to be built, active research in interactive instruction following [16, 28, 36, 42] has been in progress. This requires agents to navigate, in-*Work done while YK was an intern at Yonsei University. †: Corre-sponding author.
Figure 1: Overview of the proposed ‘Context-Aware Planning (CAP)’ and ‘Environment-Aware Memory (EAM)’. The CAP incorporates ‘context’ (i.e., task-relevant objects) of the task (de-noted by ✓ in generating a sequence of sub-goals, compared with the output without the CAP, denoted by ✗). The detailed planners then predict a sequence of agent-executable actions for each re-spective sub-goal. The agent keeps the state changes of objects and their masks in the EAM and utilizes them when necessary.
Even when the agent may not predict the mask of the plate due to occlusion, it can still interact with the plate thanks to the mask remembered in EAM, leading to successful task completion. teract with objects, and complete long-horizon tasks by fol-lowing natural language instructions with egocentric vision.
To accomplish a given task, the agent needs to plan a se-quence of actions to interact with specific task-relevant ob-jects. However, the agent often plans to interact with irrele-vant objects to the task. For instance, for the task “put an ap-ple slice on the table”, after slicing an apple, the agent might plan to pick up a bread slice, which can lead to the failure of the entire task, mainly due to a lack of contextual memory.
To address this issue, we first propose a novel approach that
divides the long-horizon planning process into two distinct phases: (1) task-relevant prediction, treated as a context pre-diction, and (2) detailed action planning that considers the contextual memory. We refer to the term ‘context’ as the objects that the command instructs the agent to manipulate.
All actions performed by the agent need to focus on these objects, making them the overarching context for the entire plan’s actions. By prioritizing the prediction of the context, we improve the agent’s ability to plan a sequence of ac-tions with less loss of environmental knowledge including objects and their receptacles. We then combine the gener-ated actions with the context to boost the agent’s efficiency in accomplishing long-term objectives by concentrating on interactive objects related to the task.
In addition, changing the object states poses an addi-tional challenge to the agent’s ability to successfully com-plete tasks that involve object interaction [16, 42]. Failure to track the dynamic object states (e.g., if an object has been already moved or not) can result in unintended interactions and often lead to task failure. For example, for the task
“move two apples in the table,” once the agent moves an apple, the agent might try to move the same apple twice if the agent does not know the apple has already been moved and eventually fails at the task.
To address the additional challenge, we further propose to use an environment-aware memory that stores informa-tion about the states of objects, as well as their masks for changed visual appearances mainly due to occlusion. This approach allows the agent to interact with objects in their proper states over time. By keeping track of object states and appearances, the agent can ensure interacting with the correct objects and conducting the appropriate actions, ulti-mately leading to more successful task completion.
For training and evaluation, we use the widely used challenging benchmark for interactive instruction follow-ing [36]. We achieve the state-of-the-art success rates and the goal-condition success rates in seen and unseen envi-ronments by large margins (up to +10.70% in unseen SR) and rank the first place in the leaderboard at the moment of submission. Also, CAPEAM with the templated approach with minor engineering won the 1st generalist language grounding agents challenge at the Embodied AI Workshop in CVPR 2023.1
We summarize our contributions as follows:
• We propose context-aware planning that plans a sub-goal sequence with ‘context’ and conducts respective sub-goals with the corresponding detailed planners.
• We propose environment-aware memory that stores states in spatial memory and object masks for better navigation and interaction with changed object states. 1See our entry ‘[EAI23] ECLAIR’ in https://leaderboard. allenai.org/alfred/submissions/public
• We achieve a state-of-the-art in a challenging interac-tive instruction following benchmark [36] in all met-rics with better generalization to novel environments. 2.