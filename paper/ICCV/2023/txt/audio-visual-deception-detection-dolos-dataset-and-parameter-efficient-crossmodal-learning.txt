Abstract 1.

Introduction
Deception detection in conversations is a challenging yet important task, having pivotal applications in many
ﬁelds such as credibility assessment in business, multime-dia anti-frauds, and custom security. Despite this, decep-tion detection research is hindered by the lack of high-quality deception datasets, as well as the difﬁculties of learning multimodal features effectively. To address this issue, we introduce DOLOS1, the largest gameshow de-ception detection dataset with rich deceptive conversations.
DOLOS includes 1,675 video clips featuring 213 subjects, and it has been labeled with audio-visual feature anno-tations. We provide train-test, duration, and gender pro-tocols to investigate the impact of different factors. We benchmark our dataset on previously proposed deception detection approaches. To further improve the performance by ﬁne-tuning fewer parameters, we propose Parameter-Efﬁcient Crossmodal Learning (PECL), where a Uniform
Temporal Adapter (UT-Adapter) explores temporal atten-tion in transformer-based architectures, and a crossmodal fusion module, Plug-in Audio-Visual Fusion (PAVF), com-bines crossmodal information from audio-visual features.
Based on the rich ﬁne-grained audio-visual annotations on
DOLOS, we also exploit multi-task learning to enhance per-formance by concurrently predicting deception and audio-visual features. Experimental results demonstrate the de-sired quality of the DOLOS dataset and the effectiveness of the PECL. The DOLOS dataset and the source codes are available at here.
*Equal contribution
†Corresponding author 1The name “DOLOS” comes from Greek mythology.
Deception is a pervasive and complex phenomenon that occurs in all areas of life, and understanding its nature and impact is crucial in preventing negative consequences. Ef-fective deception detection has crucial implications in the area of border security, anti-fraud, business negotiations and etc. [14, 12, 1, 13, 9, 27, 44]. Deep learning algorithms have achieved comparable or even better performance than hu-man in many complex tasks [22, 41, 40, 7, 38]. One may expect AI models can also bring signiﬁcant breakthroughs in deception detection. Albeit the fruitful progress in com-puter vision [48, 10, 31, 46] and audio representation learn-ing [5, 18, 28], it remains a signiﬁcant challenge to efﬁ-ciently explore AI ability to process multimodal informa-tion in perceiving and predicting human deceptive behav-iors.
The performance of AI models in deception detection is heavily reliant on the availability of authentic and effec-tive deception samples from the real world. The deceptive subjects must be spontaneous and motivated [8, 49] such that certain behavioral cues (e.g., vocal pitch and chin raise) could be more pronounced. Public benchmark datasets col-lected from court trials [36], game shows [42], and lab-based scenarios [15], have contributed to spurring interest and progress in deception detection research. Deception can be affected by a range of factors in diverse real-world situa-tions. Thus, it is necessary to create deception datasets from different scenarios. However, current datasets are still insuf-ﬁcient to drive further progress and inspire novel ideas due to their limitations on both quantity and quality. These lim-itations include (1) the small number of deceptive samples and subjects, (2) the lack of rich annotated visual and speech attributes, and (3) the variety of protocols. It is imperative to build a larger and richer deception detection dataset. In par-ticular, more deceptive samples and subjects, better anno-tations for facial movements, gestures, and audio attributes,
and more types of protocols for investigating factors affect-ing deception detection.
In addition to a high-quality dataset, effective methods are equally important to deception detection. A variety of works have been done towards using visual and acoustic information in videos for deception detection [42, 15, 36].
Current methods fall into two categories: unimodal learn-ing and multimodal fusion. However, both approaches have limitations as they do not fully exploit unimodal features or integrate complementary information from multiple modal-ities. To make better use of available information, ﬁne-tuning large pre-trained models has shown promising re-sults [39, 26, 20, 47]. However, fully ﬁne-tuning pre-trained models, such as W2V2 [5] and ViT [10], can be inefﬁ-cient and lead to overﬁtting, especially if the downstream dataset is limited. Therefore, it is important to consider parameter efﬁciency when ﬁne-tuning pre-trained models.
Adapters [17, 21] offer an efﬁcient approach for ﬁne-tuning models. While originally proposed for language [17] or vi-sion [21] tasks, adapters have not yet been applied to multi-ple modalities for temporal feature extraction.
Contributions. In this paper, we establish a new decep-tion detection dataset and propose parameter-efﬁcient cross-modal learning for audio-visual deception detection.
As our ﬁrst contribution, we introduce DOLOS, a new gameshow dataset for audio-visual deception detection.
DOLOS has several merits compared with current public datasets. First, the gameshow is a reliable source for col-lecting deception detection data because all the participants are motivated to cheat and the ground truths are available.
Second, the proposed dataset is in a conversational setup, where the deception behaviors are more naturally presented.
Third, our dataset is the largest in terms of the number of subjects and also the largest non-lab based dataset in terms of the number of video clips. The dataset has been la-beled with ﬁne-grained audio-visual annotations. We also benchmark our dataset on previous deception detection ap-proaches that involve both unimodal and multimodal fea-tures. In addition, we offer three distinct protocols, namely train-test, duration, and gender, to explore various factors that may impact deception detection.
Our second contribution is Parameter-Efﬁcient Cross-modal Learning (PECL), a method for deception detec-tion that achieves high performance by ﬁne-tuning a small number of extra learnable weights. Speciﬁcally, we intro-duce a Uniform Temporal Adapter (UT-Adapter) that ex-plores the temporal attention between input embeddings for both visual and audio modalities without the need for delicate modiﬁcations. For multimodal fusion, we pro-pose Plug-in Audio-Visual Fusion (PAVF), which utilizes the complementary information between visual and audio features to enhance the overall performance. PECL is de-signed to be parameter efﬁcient, with only the UT-Adapter,
PAVF modules, and the classiﬁer being trainable. Fur-thermore, we explore the beneﬁts of multi-task learning, a proven method that enhances performance in audio and visual tasks [54, 45]. By simultaneously predicting decep-tion, facial movements, and phonetic features, our proposed method can be further improved.
To compare and show the advantages of our dataset and the proposed method, we conducted extensive experiments.
In the cross-testing with the current gameshow benchmark dataset [42], DOLOS performs better on several test sets in different scenarios. The experimental results on DO-LOS also showed that PECL yields superior performance on different protocols. Through our experiments, we un-cover valuable insights into multimodal deception detec-tion. We believe that DOLOS, the proposed method, and the benchmarking experiments provide valuable resources to other researchers in advancing research in this area. 2.