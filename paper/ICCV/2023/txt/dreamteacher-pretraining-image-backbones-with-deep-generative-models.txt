Abstract
In this work, we introduce a self-supervised feature rep-resentation learning framework DreamTeacher that utilizes generative networks for pre-training downstream image backbones. We propose to distill knowledge from a trained generative model into standard image backbones that have been well engineered for speciﬁc perception tasks. We investi-gate two types of knowledge distillation: 1) distilling learned generative features onto target image backbones as an al-ternative to pretraining these backbones on large labeled datasets such as ImageNet, and 2) distilling labels obtained from generative networks with task heads onto logits of target backbones. We perform extensive analyses on multiple gener-ative models, dense prediction benchmarks, and several pre-training regimes. We empirically ﬁnd that our DreamTeacher signiﬁcantly outperforms existing self-supervised represen-tation learning approaches across the board. Unsupervised
ImageNet pre-training with DreamTeacher leads to signiﬁ-cant improvements over ImageNet classiﬁcation pre-training on downstream datasets, showcasing generative models, and diffusion generative models speciﬁcally, as a promising ap-proach to representation learning on large, diverse datasets without requiring manual annotation. 1.

Introduction
Self-supervised representation learning is becoming an ef-fective way of pre-training vision backbones [7,12,13,27,29].
⇤ Equal Contribution.
The premise of this line of work is to leverage large unla-beled datasets as additional source of training data in order to boost performance of downstream networks, and to re-duce the need for large labeled target datasets. Recent works have shown that self-supervised pre-training on ImageNet can now come close to supervised pre-training, even outper-forming it on some downstream datasets and tasks such as pixelwise semantic and instance segmentation [13, 29, 63].
One of the dominant approaches to self-supervised repre-sentation learning are variants of contrastive learning, where the target backbone is trained to map transformed views of an image closer in latent space than images randomly drawn from the dataset [12]. Improvements to this paradigm include introducing spatial losses [63, 70, 71, 73], and im-proving training stability with fewer or no negative exam-ples [13, 14, 27, 29].
Another line of work pursues reconstruction losses for supervision, where certain regions get masked from an input image, and backbones get trained to reconstruct them [21, 28, 64, 72], also known as Masked Image Modeling (MIM).
This task is mostly treated as deterministic, ie supervising a single explanation for the masked region. This line of work typically investigates masking strategies, architecture design and training recipes to train better backbones. These methods have achieved state-of-the-art (SoTA) performance when applied to Vision Transformer-based backbones; however, recently sparse CNN-based image backbones [58] have been shown to be as performant.
In this paper, we argue for generative models as repre-sentation learners: for the simplicity of the objective – to 1
generate data, and intuitive representational power – gen-erating high quality samples as an indication of learning semantically capable internal representations. Using gen-erative networks as representation learners is not a novel concept. DatasetGAN and variants [4, 40, 82] proposed to add task-dependent heads on top of StyleGAN’s or a diffu-sion model’s features, and used these augmented networks as generators of labeled data, on which downstream networks are then trained. SemanticGAN [41] instead used StyleGAN with an additional task decoder as the task network itself – by encoding images into the latent space of the generative model and using the task head for producing perception output.
We introduce DreamTeacher, a representation learning framework that leverages generative models for pre-training downstream perception models via distillation. We inves-tigate two types of distillation: 1) feature distillation, where we propose methods for distilling generative features to target backbones, as a general pre-training mechanism that does not require any labels. 2) label distillation: using task-heads on top of generative networks for distilling knowledge from a labeled dataset onto target backbones, in a semi-supervised regime. We focus our work on diffusion models [35, 54, 56] and GANs [26, 36, 37] as the choice of generative models. For target backbones, we focus on CNNs, for two major reasons. 1) CNN-based backbones have been shown to achieve SoTA representation learning performance for both contrastive and MIM approaches [44, 58, 62, 66], 2) SoTA generative models today (GANs and diffusion models) primarily still use CNNs internally. In preliminary experiments, we also explored vision transformer backbones, but found it challenging to distill features from CNN-based generative models into vision transformers. Generative models built with vision transformer architectures are nascent [2, 48], and hence we leave a thorough exploration of DreamTeacher with these architectures to future work.
We experimentally show that DreamTeacher outperforms existing self-supervised learning approaches on various benchmarks and settings. Most notably, when pre-trained on ImageNet without any labels, our method signiﬁcantly outperforms methods that are pre-trained on ImageNet with full supervision, on several dense prediction benchmarks and tasks such as semantic segmentation on ADE20K [84], instance segmentation on MSCOCO [43] and on the au-tonomous driving dataset BDD100K [77]. On object-focused datasets with millions of unlabeled images [78, 82], our method, when trained solely on the target domain, signif-icantly outperforms variants that are pre-trained on Ima-geNet with label supervision, and achieves new SoTA re-sults. These results highlight generative models, especially diffusion-based generative models [20, 35, 56], as powerful representation learners that can effectively leverage diverse unlabeled datasets at scale. 2.