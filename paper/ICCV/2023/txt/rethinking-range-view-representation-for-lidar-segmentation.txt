Abstract
LiDAR segmentation is crucial for autonomous driving perception. Recent trends favor point- or voxel-based meth-ods as they often yield better performance than the tradi-tional range view representation. In this work, we unveil several key factors in building powerful range view mod-els. We observe that the “many-to-one” mapping, semantic incoherence, and shape deformation are possible impedi-ments against effective learning from range view projec-tions. We present RangeFormer – a full-cycle framework comprising novel designs across network architecture, data augmentation, and post-processing – that better handles the learning and processing of LiDAR point clouds from the range view. We further introduce a Scalable Training from
Range view (STR) strategy that trains on arbitrary low-resolution 2D range images, while still maintaining satis-factory 3D segmentation accuracy. We show that, for the first time, a range view method is able to surpass the point, voxel, and multi-view fusion counterparts in the competing
LiDAR semantic and panoptic segmentation benchmarks, i.e., SemanticKITTI, nuScenes, and ScribbleKITTI. 1.

Introduction
LiDAR point clouds have unique characteristics. As the direct reflections of real-world scenes, they are often diverse and unordered and thus bring extra difficulties in learning
[26, 40].
Inevitably, a good representation is needed for efficient and effective LiDAR point cloud processing [64].
Although there exist various LiDAR representations as shown in Tab. 1, the prevailing approaches are mainly based on point view [32, 61], voxel view [15, 60, 81, 28], and multi-view fusion [41, 72, 51]. These methods, however, re-quire computationally intensive neighborhood search [50], 3D convolution operations [43], or multi-branch networks
[2, 24], which are often inefficient during both training and inference stages. The projection-based representations, such as range view [68, 46] and bird’s eye view [78, 80],
Figure 1: Three detrimental factors observed in the LiDAR range view representation: 1) the “many-to-one” problem; 2) “holes” or empty grids; and 3) shape distortions.
Table 1: Comparisons for different LiDAR representations.
View
Formation
Complexity
Representative
Raw Points
Range View
Bird’s Eye View
Voxel (Dense)
Voxel (Sparse)
Voxel (Cylinder)
Multi-View
Bag-of-Points
Range Image
Polar Image
Voxel Grid
Sparse Grid
Sparse Grid
Multiple
O(N · d)
O( H·W r2
O( H·W r2
O( H·W ·L
· d)
· d)
· d) r3
O(N · d)
O(N · d)
O((N + H·W r2 ) · d)
RandLA-Net, KPConv
SqueezeSeg, RangeNet++
PolarNet
PVCNN
MinkowskiNet, SPVNAS
Cylinder3D
AMVNet, RPVNet are more tractable options. The 3D-to-2D rasterizations and mature 2D operators open doors for fast and scalable in-vehicle LiDAR perception [46, 71, 64]. Unfortunately, the segmentation accuracy of current projection-based methods
[79, 13, 78] is still far behind the trend [73, 72, 75].
The challenge of learning from projected LiDAR scans comes from the potential detrimental factors of the LiDAR data representation [46]. As shown in Fig. 1, the range view projection1 often suffers from several difficulties, includ-ing 1) the “many-to-one” conflict of adjacent points, caused 1We show a frustum of the LiDAR scan for simplicity; the complete range view projection is a cylindrical panorama around the ego-vehicle.
by limited horizontal angular resolutions; 2) the “holes” in the range images due to 3D sparsity and sensor disrup-tions; and 3) potential shape deformations during the ras-terization process. While these problems are ubiquitous in range view learning, previous works hardly consider tack-ling them. Stemming from the image segmentation com-munity [77], prior arts widely adopt the fully-convolutional networks (FCNs) [44, 8] for range view LiDAR segmenta-tion [46, 79, 13, 35]. The limited receptive fields of FCNs cannot directly model long-term dependencies and are thus less effective in handling the mentioned impediments.
In this work, we seek an alternative in lieu of the cur-rent range view LiDAR segmentation models. Inspired by the success of Vision Transformer (ViT) and its follow-ups
[19, 67, 70, 42, 57], we design a new framework dubbed
RangeFormer to better handle the learning and processing of LiDAR point clouds from the range view. We formulate the segmentation of range view grids as a seq2seq problem and adopt the standard self-attention modules [66] to cap-ture the rich contextual information in a “global” manner, which is often omitted in FCNs [46, 1, 13]. The hierarchi-cal features extracted with such global awareness are then fed into multi-layer perceptions (MLPs) for decoding. In this way, every point in the range image is able to establish interactions with other points – no matter whether close or far and valid or empty – and further lead to more effective representation learning from the LiDAR range view.
It is worth noting that such architectures, albeit straight-forward, still suffer several difficulties. The first issue is related to data diversity. The prevailing LiDAR segmen-tation datasets [7, 21, 5, 59] contain tens of thousands of
LiDAR scans for training. These scans, however, are less diverse in the sense that they are collected in a sequential way. This hinders the training of Transformer-based archi-tectures as they often rely on sufficient samples and strong data augmentations [19]. To better handle this, We design an augmentation combo that is tailored for range view. In-spired by recent 3D augmentation techniques [80, 36, 47], we manipulate the range view grids with row mixing, view shifting, copy-paste, and grid fill. As we will show in the following sections, these lightweight operations can signifi-cantly boost the performance of SoTA range view methods.
The second issue comes from data post-processing. Prior works adopt CRF [68] or k-NN [46] to smooth/infer the range view predictions. However, it is often hard to find a good balance between the under- and over-smoothing of the 3D labels in unsupervised manners [34]. In contrast, we design a supervised post-processing approach that first sub-samples the whole LiDAR point cloud into equal-interval
“sub-clouds” and then infer their semantics, which holisti-cally reduces the uncertainty of aliasing range view grids.
To further reduce the overhead in range view learning, we propose STR – a scalable range view training paradigm.
STR first “divides” the whole LiDAR scan into multiple groups along the azimuth direction and then “conquers” each of them. This transforms range images of high hor-izontal resolutions into a stack of low-resolution ones while can better maintain the best-possible granularity to ease the
“many-to-one” conflict. Empirically, We find STR helpful in reducing the complexity during training, without sacri-ficing much convergence rate and segmentation accuracy.
The advantages of RangeFormer and STR are demon-strated from aspects of LiDAR segmentation accuracy and efficiency on prevailing benchmarks. Concretely, we achieve 73.3% mIoU and 64.2% PQ on SemanticKITTI
[5], surpassing prior range view methods [79, 13] by sig-nificant margins and also better than SoTA fusion-based methods [73, 30, 75]. We also establish superiority on the nuScenes [21] (sparser point clouds) and ScribbleKITTI
[65] (weak supervisions) datasets, which validates our scal-ability. While being more effective, our approaches run 2× to 5× faster than recent voxel [81, 60] and fusion [72, 73] methods and can operate at sensor frame rate. 2.