Abstract
Pre-trained vision-language models, e.g., CLIP, work-ing with manually designed prompts have demonstrated great capacity of transfer learning. Recently, learnable prompts achieve state-of-the-art performance, which how-ever are prone to overfit to seen classes, failing to gen-eralize to unseen classes.
In this paper, we propose a
Knowledge-Aware Prompt Tuning (KAPT) framework for vision-language models. Our approach takes the inspira-tion from human intelligence in which external knowledge is usually incorporated into recognizing novel categories of objects. Specifically, we design two complementary types of knowledge-aware prompts for the text encoder to lever-age the distinctive characteristics of category-related ex-ternal knowledge. The discrete prompt extracts the key in-formation from descriptions of an object category, and the learned continuous prompt captures overall contexts. We further design an adaptation head for the visual encoder to aggregate salient attentive visual cues, which establishes discriminative and task-aware visual representations. We conduct extensive experiments on 11 widely-used bench-mark datasets and the results verify the effectiveness in few-shot image classification, especially in generalizing to un-seen categories. Compared with the state-of-the-art Co-CoOp method, KAPT exhibits favorable performance and achieves an absolute gain of 3.22% on new classes and 2.57% in terms of harmonic mean. 1.

Introduction
Recently, large-scale pre-trained vision-language mod-els, e.g., CLIP [29], ALIGN [15], and FLIP [44], have demonstrated remarkable performance in zero/few-shot learning tasks. Unlike traditional vision-only frameworks
∗ Equal contribution.
† Corresponding author. Work done when
Baoshuo Kan visited to Feng Zheng Lab in SUSTech.
Figure 1: A motivating example. The textual description of the “Gentiana Acaulis” conduces to the recognition of the corresponding image (b) of Gentiana Acaulis. that are trained mainly by a closed set of single-modal data, vision-language models train two uni-modal encoders on massive amounts of image-text pairs to exploit cross-modal alignments in the semantic space. By leveraging large-scale web-scale image-text data, pre-trained vision-language models are endowed with the ability to solve zero/few-shot downstream tasks and even recognize open-set visual concepts [29, 15, 44]. Expressly, when a new classification task arrives, the CLIP text-encoder encodes manually designed textual prompt (e.g., “a photo of a [la-bel].”), and then cosine similarity between textual features and image features is computed. However, identifying ap-propriate manually designed prompts is an art that requires both domain expertise and laborious prompt engineering.
To avoid the manual prompt design, some recent re-search (e.g., CoOp [49]) on visual representations are mainly inspired by prompt tuning approaches [47, 18, 21] in Natural Language Processing (NLP), like learn-able prompts. By optimizing their models with learnable prompts in closed datasets, these methods achieve outstand-ing performance in seen classes. However, the learned prompts are usually prone to overfit to the seen classes
ing a large number of entity descriptions. To take full ad-vantage of category-related external knowledge, we design two complementary types of prompts: discrete and learn-able continuous prompts. Discrete prompts carry the sum-marized texts that directly describe the visual appearance of the category, and learnable continuous prompts carry con-textual information that may cover a broader background of the category. As shown in Figure 2, a preliminary experi-ment verifies that the proposed discrete knowledge-aware prompt improves performance of CLIP on several image datasets. Meanwhile, to further adapt the visual represen-tation towards a specific task for inhibiting disturbance of task-irrelevant visual concepts, we propose an adaptation head that refines the image features by attending to the salient visual cues relevant to categories of the target task.
The main contributions can be summarized as follows:
• We propose a novel prompt tuning framework for vision-language models by incorporating external knowledge, which greatly improves the generalizabil-ity on unseen object categories.
• We design two complementary types of knowledge-aware prompts, which enables the model to fully ex-ploit category-related dense knowledge retrieved from the Wikipedia Encyclopedia.
• We further propose a task-aware visual adaptation head to aggregate the attentive visual features condi-tioned on linguistic description of categories, which maximally capture task-related visual cues, while sup-pressing the disturbance caused by task-irrelevant vi-sual concepts.
• Extensive experimental results on 11 popular image datasets demonstrate the effectiveness of the proposed method. Our method significantly outperforms state-of-the-art methods on the overall metric in the base-to-new generalization setting. 2.