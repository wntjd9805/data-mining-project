Abstract
Multi-modality image fusion and segmentation play a vital role in autonomous driving and robotic operation.
Early efforts focus on boosting the performance for only one task, e.g., fusion or segmentation, making it hard to reach ‘Best of Both Worlds’. To overcome this issue, in this paper, we propose a Multi-interactive Feature learn-ing architecture for image fusion and Segmentation, namely
SegMiF, and exploit dual-task correlation to promote the performance of both tasks. The SegMiF is of a cascade structure, containing a fusion sub-network and a commonly used segmentation sub-network. By slickly bridging inter-mediate features between two components, the knowledge learned from the segmentation task can effectively assist the fusion task. Also, the benefited fusion network sup-ports the segmentation one to perform more pretentiously.
Besides, a hierarchical interactive attention block is estab-lished to ensure fine-grained mapping of all the vital in-formation between two tasks, so that the modality/semantic features can be fully mutual-interactive. In addition, a dy-namic weight factor is introduced to automatically adjust the corresponding weights of each task, which can balance the interactive feature correspondence and break through the limitation of laborious tuning. Furthermore, we con-struct a smart multi-wave binocular imaging system and collect a full-time multi-modality benchmark with 15 anno-tated pixel-level categories for image fusion and segmenta-tion. Extensive experiments on several public datasets and our benchmark demonstrate that the proposed method out-puts visually appealing fused images and perform averagely 7.66% higher segmentation mIoU in the real-world scene than the state-of-the-art approaches. The source code and benchmark are available at https://github.com/
JinyuanLiu-CV/SegMiF. 1.

Introduction
Accurate and robust scene parsing[1, 2] is a fundamen-tal technology for autonomous driving. However, in com-plex environments, e.g., inclement weather, only using visi-ble sensors may fail to accurately recognize targets. On the contrary, infrared sensors are free from the aforementioned issues but limited in low spatial resolution. Consequently, fusing the infrared and visible image [3, 4, 5, 6, 7] has be-come a mainstream solution for better scene understanding.
Multi-modality fusion for scene parsing needs to pro-vide: (i). robust visual appealing image: they require con-tinually generating high-quality images in dynamic scenes. (ii). accurate semantic segmentation: they demand to assign category labels to each pixel. Towards these goals, jointly solving multi-modality image fusion and segmentation be-comes an urgent issue.
Numerous learning-based multi-modality image fusion methods have been fast development [8, 9, 10, 11, 12].
However, most of them concentrate on developing vari-ous networks for generating visual-appealing images rather than considering the follow-up high-level vision tasks, pos-ing an obstacle to better scene parsing. Recently, few studies[13, 14, 15, 16] have attempted to design multi-task learning-based loss functions by cascading the fusion net-work and high-level tasks. Unfortunately, seeking unified appropriate features for either task simultaneously is still a tough issue.
Moreover, exploring multi-modality fusion and seg-mentation demands a comprehensive collection of well-alignment image pairs with pixel-level annotated labels.
Also, as for one image, the annotated needs to cover a wide range of pixels. Unfortunately, existing multi-modality data collections either focus on image fusion or lack whole im-age annotated segmentation labels, placing an obstacle to exploring the correlation of the fusion and segmentation.
This paper proposes a multi-interactive feature learn-ing architecture for the joint problem of multi-modality fu-sion and segmentation, namely SegMiF. SegMiF is con-structed by a fusion network and a segmentation network, in which the intrinsic features of either one interact via a new proposed hierarchical interactive attention (HIA).
HIA fully integrates semantic-/modality-oriented features by fine-grained mapping. We also derive a dynamic weight-ing factor and seamless it in the interactive training scheme, to automatically learn the optimal parameters for either task.
Figure 1 demonstrates that our SegMiF assigns the category to each pixel from the visual-friendly fused result more ac-curately than the state-of-the-arts (SOTAs). Our contribu-tions can be distilled into four main aspects as follows:
• We formulate both image fusion and segmentation in a joint manner, in which the semantic and pixel-based features can mutually interact. To this end, two tasks can achieve the ‘Best of Both Worlds’, generat-ing visual-appealing fused images along with accurate scene parsing.
• A hierarchical interactive attention is introduced to bridge the feature gap between the fusion network and the segmentation one. Establishing the seman-tic/modality multi-head attention mechanism in HIA simultaneously preserves intrinsic modality features and brings more attention to semantic features.
• An interactive feature training scheme is proposed to overcome the shortcoming of insufficient feature inter-action between fusion and segmentation. Seamlessly integrating a dynamic weighting factor allows the ex-ploration of the optimal parameters of each task in an automatic manner.
• We construct a smart multi-wave binocular imag-ing system, and introduce a full-time multi-modality benchmark, namely FMB, to promote the research of both image fusion and segmentation. FMB contains 1500 well-registered infrared and visible image pairs with 15 annotated pixel-level categories (see the left part of Figure 1). Also, it covers a wide range of pixel variations and various severe environments, e.g., dense fog, heavy rain, and low-light condition. 2.