Abstract
Event cameras offer many advantages over standard cameras due to their distinctive principle of operation: low power, low latency, high temporal resolution and high dy-namic range. Nonetheless, the success of many downstream visual applications also hinges on an efficient and effective scene representation, where Neural Radiance Field (NeRF) is seen as the leading candidate. Such promise and poten-tial of event cameras and NeRF inspired recent works to in-vestigate on the reconstruction of NeRF from moving event cameras. However, these works are mainly limited in terms of the dependence on dense and low-noise event streams, as well as generalization to arbitrary contrast threshold val-ues and camera speed profiles. In this work, we propose
Robust e-NeRF, a novel method to directly and robustly re-construct NeRFs from moving event cameras under vari-ous real-world conditions, especially from sparse and noisy events generated under non-uniform motion. It consists of two key components: a realistic event generation model that accounts for various intrinsic parameters (e.g. time-independent, asymmetric threshold and refractory period) and non-idealities (e.g. pixel-to-pixel threshold variation), as well as a complementary pair of normalized reconstruc-tion losses that can effectively generalize to arbitrary speed profiles and intrinsic parameter values without such prior knowledge. Experiments on real and novel realistically sim-ulated sequences verify our effectiveness. Our code, syn-thetic dataset and improved event simulator are public. 1.

Introduction
Event cameras are bio-inspired sensors that represent a paradigm shift in visual acquisition and processing. This is attributed to its fundamentally distinctive principle of opera-tion, where its pixels independently respond to log-intensity changes in an asynchronous manner, yielding a stream of events, rather than measuring absolute linear intensity syn-Figure 1. Existing works on NeRF reconstruction from moving event cameras heavily rely on (a) temporally dense and low-noise events generated under roughly (b) uniform-speed camera motion.
In contrast, our method, Robust e-NeRF is able to directly and robustly reconstruct NeRFs from (c) sparse and noisy events gen-erated under (d) non-uniform camera motion, as shown in (e). chronously at a constant rate, as done in standard cameras.
Such unique properties contribute to their multitude of ad-low power, low la-vantages over standard cameras [9]:
tency, high temporal resolution and high dynamic range, thereby the recent success of event-based [15, 37, 10, 7, 38] or event-image hybrid [48, 47, 11] applications.
The success of many downstream visual applications in robotics, computer vision, graphics and virtual/augmented reality also hinges on an efficient and effective representa-tion to encode various information of the scene being inter-acted with. Neural scene representations [24, 28, 49, 54, 20], especially neural fields [52], have recently emerged as promising candidates for future applications, owing to their continuous nature and memory efficiency. This trend is further driven by the exceptional capabilities and photo-realism of Neural Radiance Field (NeRF) [24]-based works
[49, 23, 45, 30, 31, 32].
Motivated by such promise and potential of event cam-eras and NeRF, we are interested in studying the follow-ing research question: How to robustly reconstruct a NeRF from a moving event camera under general real-world con-ditions? One simple way is to retrofit an events-to-video re-construction method [38, 44, 29] to NeRF. However, such a na¨ıve approach is inherently limited by the low photometric accuracy and consistency of the reconstructed video frames, since they are assumed to be true observations of the scene.
On the contrary, recent works, such as EventNeRF [41],
Ev-NeRF [12] and E-NeRF [18], have proposed to recon-struct NeRFs directly using events via alternative recon-struction losses inspired or derived from an event generation model. Nonetheless, these works heavily rely on a tempo-rally dense and low-noise event stream, which is generally inaccessible in practice due to the presence of refractory period (i.e. pixel dead-time after generating an event) and pixel-to-pixel variation in the contrast threshold (i.e. min-imum log-intensity change for event generation). Such a limitation can be partly attributed to the accumulation of successive events at each pixel over time intervals, as per-formed in these works. Moreover, the effective reduction in contrast sensitivity and amplification of threshold variation resulting from the event accumulation also leads to a less detailed and robust reconstruction, respectively.
In addition, these methods do not directly and effectively generalize to arbitrary contrast threshold values and camera speed profiles, as their optimal hyper-parameter configura-tion greatly depends on the contrast threshold and speed of motion. EventNeRF and E-NeRF assumes symmetric posi-tive and negative thresholds, which generally does not hold true in practice. While joint optimization of the contrast threshold is supported in Ev-NeRF, an additional regular-ization is necessary to prevent degeneracy. Furthermore, the assumption of time-varying thresholds made in Ev-NeRF and E-NeRF, which is not well supported by the litera-ture, also leads to a reduction in reconstruction accuracy as shown in our experiments.
Contributions. We propose Robust e-NeRF, a novel method to directly and robustly reconstruct NeRFs from moving event cameras under various real-world conditions, especially from temporally sparse and noisy event streams given by event cameras in non-uniform motion.
In particular, we incorporate a more realistic event gen-eration model that accounts for various intrinsic parame-ters (e.g. time-independent, asymmetric contrast threshold and refractory period) and non-idealities (e.g. pixel-to-pixel threshold variation). Furthermore, we introduce two com-plementary normalized reconstruction losses that are not only effectively invariant to the camera motion speed and threshold scale, but also minimally influenced by asymmet-ric thresholds. This allows for their effective generaliza-tion, as well as the regularization-free joint optimization of unknown contrast threshold and refractory period from poor initializations. The first loss serves as the primary loss for high-fidelity reconstruction, while the second acts as a smoothness constraint for better regularization of texture-less regions. As both loss functions do not involve event ac-cumulation, detailed and robust reconstruction from sparse and noisy events can be achieved. Our experiments on novel sequences, simulated using an improved variant of ESIM
[36], and real sequences from TUM-VIE [17] verify the effectiveness of Robust e-NeRF. We publicly release our code, synthetic event dataset and improved ESIM. 2.