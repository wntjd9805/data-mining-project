Abstract
Jointly processing information from multiple sensors is crucial to achieving accurate and robust perception for re-liable autonomous driving systems. However, current 3D perception research follows a modality-specific paradigm, leading to additional computation overheads and ineffi-cient collaboration between different sensor data. In this paper, we present an efficient multi-modal backbone for outdoor 3D perception named UniTR, which processes a variety of modalities with unified modeling and shared parameters. Unlike previous works, UniTR introduces a modality-agnostic transformer encoder to handle these view-discrepant sensor data for parallel modal-wise repre-sentation learning and automatic cross-modal interaction without additional fusion steps. More importantly, to make full use of these complementary sensor types, we present a novel multi-modal integration strategy by both consider-ing semantic-abundant 2D perspective and geometry-aware 3D sparse neighborhood relations. UniTR is also a fun-damentally task-agnostic backbone that naturally supports different 3D perception tasks.
It sets a new state-of-the-art performance on the nuScenes benchmark, achieving
+1.1 NDS higher for 3D object detection and +12.0 higher mIoU for BEV map segmentation with lower inference la-tency. Code will be available at https://github. com/Haiyang-W/UniTR. 1.

Introduction
Perceiving the physical world in 3D space is criti-cal for reliable autonomous driving systems [2, 54]. As self-driving sensors become more advanced, integrating the complementary signals captured from different sensors (e.g., Cameras, LiDAR, and Radar) in a unified manner is
*Equal contribution.
†Corresponding author.
Figure 1. 3D object detection performance (NDS) vs speed (Hz) on nuScenes [3] validation set. Latency is measured on an A100
GPU with AMD EPYC 7513 CPU. Blue and green lines are the operating frequency of the camera and LiDAR in nuScenes. essential. To achieve this goal, we propose UniTR, a uni-fied yet efficient multi-modal transformer backbone that can process both 3D sparse point clouds and 2D multi-view dense images in parallel to learn the unified bird’s-eye-view (BEV) representations for boosting 3D outdoor perception.
Data obtained from multi-sensory systems are repre-sented in fundamentally different modalities: e.g., cameras capture visually rich perspective images, while LiDARs ac-Inte-quire geometry-sensitive point clouds in 3D space. grating these complementary sensors is an ideal solution for achieving robust 3D perception. However, due to the view discrepancy in raw data representations, developing an ef-fective fusion approach is non-trivial. Previous works can be broadly classified into point-based, proposal-based, and
BEV-based fusion methods. Point-based [51, 52, 22, 70, 53] and proposal-based approaches [4, 72, 1, 30, 5, 66, 28] en-rich the LiDAR points and object proposals with seman-tic features from 2D images separately. BEV-based meth-ods [36, 32] unify the representations of camera and lidar into a shared BEV space and fuse them with subsequent
2D convolutions. Though performant, these fusion schemes generally require modality-specific encoders that process different sensor data in a sequential manner, leading to in-creased inference latency and hindering their real-world ap-plicability. Thus, developing a modality-agnostic encoder has the potential to efficiently align the multi-modal fea-tures while facilitating the learning of generic representa-tions for better 3D scene understanding.
Transformers have emerged as a powerful tool for multi-modal processing in various research fields [6, 25, 47, 77]. However, its application to image-LiDAR data presents unique challenges due to the view discrepancy (i.e., 2D dense images and 3D sparse point clouds). Exist-ing transformer-based fusion strategies [1, 5, 27] rely on modality-specific encoders followed by additional query-based late fusion, incurring non-negligible computational overheads. Hence, building an efficient and unified multi-modal backbone that can automatically learn the intra- and inter-modal representations from both image and LiDAR data is the main challenge we aim to address in this paper.
In this paper, we introduce UniTR, a unified multi-modal transformer backbone for outdoor 3D perception. As shown in Figure 2, unlike previous modality-specific encoders, our UniTR processes the data from multi-sensors in par-allel with a modal-shared transformer encoder and inte-grates them automatically without additional fusion steps.
To achieve these goals, we design two major transformer blocks extended on DSVT [56] (i.e., a powerful yet flexible transformer architecture for sparse data). One is the intra-modal block that facilitates parallel computation of modal-wise representation learning for the data from each sensor, and the other one is an inter-modal block to perform cross-modal feature interaction by considering both 2D perspec-tive and 3D geometric neighborhood relations.
Specifically, given single- or multi-view images and point clouds, we first convert them into unified token se-quences with lightweight modality-specific tokenizers, i.e., 2D Convolution [23] for images and voxel feature encoding layer [75] for point clouds. To jointly process the modal-wise representation learning of each sensor, these sequences are then partitioned to size-equivalent local sets in their cor-responding modal space separately, which are then assigned to different samples in the same batch for parallel computa-tion by several modality-agnostic DSVT blocks. This strat-egy maximizes the parallel computing capabilities of mod-ern GPU, which greatly reduces the inference latency (about 2× faster) while also achieving better performance by shar-ing weights among different modalities (see Table 4).
Secondly, we present a powerful yet efficient cross-modal transformer block for camera-lidar fusion. To re-solve the view discrepancy, previous methods adopt two view transformations, i.e., LiDAR-to-camera [1, 66, 5, 30] or camera-to-LiDAR projection [36, 32, 41], to unify multi-Figure 2. Comparison between sequential modality-specific en-coders and our proposed UniTR, which processes various modali-ties in parallel with a single model and shared parameters. modal features in a shared space. However, these two fu-sion spaces are actually complementary due to their dis-tinct neighborhood relations among the inputs, i.e., 2D cam-era view preserves dense semantic relations, while 3D li-dar space provides sparse geometric structures. Combin-ing them can benefit the performance of both geometric-and semantic-oriented 3D perception. More importantly, the time cost brought by additional late fusion behind the modality-specific encoders is normally considerable. To ad-dress these problems, we design an inter-modal transformer block to bridge different modalities according to 2D and 3D structural relations. Equipped with this block, our UniTR can integrate multi-modal features with alternative 2D-3D neighborhood configurations during backbone processing, and the performance gains (see Table 3) demonstrate its ef-fectiveness. Notably, this block is also built upon DSVT and can be seamlessly inserted into our multi-modal backbone.
In a nutshell, our contribution is four-fold. 1) We present a weight-sharing intra-modal transformer block for effi-cient modal-wise representation learning in parallel. 2)
To bridge sensors with disparate views, a powerful cross-modal transformer block is designed for integrating dif-ferent modalities by considering both 2D perspective and 3D geometric structural relations. 3) With the above de-signs, we introduce a novel multi-modal backbone for out-door 3D perception named UniTR, which processes a vari-ety of modalities with shared parameters in a unified man-ner. 4) Our UniTR achieves state-of-the-art performance on nuScenes [3] benchmark of various 3D perception tasks, i.e., 3D Object Detection (+1.1) and BEV Map Segmenta-tion (+12.0), with lower latency, as shown in Figure 1.
We hope the observed strong performance of UniTR can serve as an encouraging benchmark for future efforts toward integrating visual and geometric signals with unified archi-tectures for more generic outdoor 3D perception. 2.