Abstract
Domain adaptation has been vastly investigated in com-puter vision but still requires access to target images at train time, which might be intractable in some uncommon condi-tions. In this paper, we propose the task of ‘Prompt-driven
Zero-shot Domain Adaptation’, where we adapt a model trained on a source domain using only a general descrip-tion in natural language of the target domain, i.e., a prompt.
First, we leverage a pretrained contrastive vision-language model (CLIP) to optimize afﬁne transformations of source features, steering them towards the target text embedding while preserving their content and semantics. To achieve this, we propose Prompt-driven Instance Normalization (PIN). Second, we show that these prompt-driven augmen-tations can be used to perform zero-shot domain adapta-tion for semantic segmentation. Experiments demonstrate that our method signiﬁcantly outperforms CLIP-based style transfer baselines on several datasets for the downstream task at hand, even surpassing one-shot unsupervised do-main adaptation. A similar boost is observed on object de-tection and image classiﬁcation. The code is available at https://github.com/astra-vision/PODA . 1.

Introduction
The last few years have witnessed tremendous success of supervised semantic segmentation methods towards bet-ter high-resolution predictions [5, 6, 9, 31, 50], multi-scale processing [30,57] or computational efﬁciency [56]. In con-trolled settings where segmentation models are trained us-ing data from the targeted operational design domains, the accuracy can meet the high industry-level expectations on in-domain data; yet, when tested on out-of-distribution data, these models often undergo drastic performance drops [35].
This hinders their applicability in real-world scenarios for critical applications like in-the-wild autonomous driving.
To mitigate this domain-shift problem [2], unsuper-vised domain adaptation (UDA) [16, 18, 45, 46, 48, 59] has emerged as a promising solution. Training of UDA meth-ods requires labeled data from source domain and unlabeled
PØDA w/ prompt
"driving at night"
PØDA w/ prompt
"driving through fire" segmenter (trained on
Cityscapes
PØDA w/ prompt
"driving in old movie"
PØDA w/ prompt
"driving in sandstorm"
Figure 1. Zero-shot adaptation with prompt. PØDA enables the adaptation of a segmenter model (here, DeepLabv3+ trained on the source dataset Cityscapes) to unseen conditions with only a prompt. Source-only predictions are shown as smaller segmenta-tion masks to the left or right of the test images. data from target domain. Though seemingly effortless, for some conditions even collecting unlabeled data is complex.
For example, as driving through ﬁre or sandstorm rarely oc-curs in real life, collecting raw data in such conditions is non-trivial. One may argue on using Internet images for
UDA. However, in the industrial context, the practice of us-ing public data is limited or forbidden. Recent works aim to reduce the burden of target data collection campaigns by de-vising one-shot [33,54] UDA methods, i.e., using one target image for training. Pushing further this line of research, we frame the challenging new task of prompt-driven zero-shot domain adaptation where given a target domain description in natural language (i.e., a prompt), our method accordingly adapts the segmentation model to this domain of interest.
Figure 1 outlines the primary goal of our work with a few qualitative examples. Without seeing any ﬁre or sandstorm images during training, the adapted models succeed in seg-menting out critical scene objects, exhibiting fewer errors than the original source-only model.
Our method, illustrated in Fig. 2, is made possible by leveraging the vision-language connections from the sem-inal CLIP model [39]. Trained on 400M web-crawled image-text pairs, CLIP has revolutionized multi-modal rep-resentation learning, bringing outstanding capability to
zero-shot image synthesis [15, 24, 37], zero-shot multi-modal fusion [20], zero-shot semantic segmentation [26, 58], open-vocabulary object detection [34], few-shot learn-ing [10], etc.
In our work, we exploit the CLIP’s latent space and propose a simple and effective feature styliza-tion mechanism that converts source-domain features into target-domain ones (Fig. 2, left), which can be seen as a spe-ciﬁc form of data augmentation. Fine-tuning the segmenta-tion model on these zero-shot synthesized features (Fig. 2, middle) helps mitigating the distribution gap between the two domains thus improving performance on unseen do-mains (Fig. 2, right). Owing to the standard terminology of
“prompt” that designates the input text in CLIP-based im-age generation, we coin our approach Prompt-driven Zero-shot Domain Adaptation, PØDA in short.
To summarize, our contributions are as follows:
• We introduce the novel task of prompt-driven zero-shot domain adaptation, which aims at adapting a source-trained model on a target domain provided only an ar-bitrary textual description of the latter.
• Unlike other CLIP-based methods that navigate CLIP latent space using direct image representations, we al-ter only the features, without relying on the appearance in pixel space. We argue that this is particularly use-ful for downstream tasks such as semantic segmentation where good features are decisive (and sufﬁcient). We present a simple and effective Prompt-driven Instance
Normalization (PIN) layer to augment source features, where afﬁne transformations of low-level features are optimized such that the representation in CLIP latent space matches the one of target-domain prompt.
• We show the versatility of our method by adapting source-trained semantic segmentation models to differ-ent conditions: (i) from clear weather/daytime to ad-verse conditions (snow, rain, night), (ii) from synthetic to real, (iii) from real to synthetic. Interestingly, PØDA outperforms state-of-the-art one-shot unsupervised do-main adaptation without using any target image.
• We show that PØDA can also be applied to object de-tection and image classﬁcation. 2.