Abstract
The field of explainable artificial intelligence (XAI) aims to uncover the inner workings of complex deep neural mod-els. While being crucial for safety-critical domains, XAI inherently lacks ground-truth explanations, making its au-tomatic evaluation an unsolved problem. We address this challenge by proposing a novel synthetic vision dataset, named FunnyBirds, and accompanying automatic evalua-tion protocols. Our dataset allows performing semantically meaningful image interventions, e.g., removing individual object parts, which has three important implications. First, it enables analyzing explanations on a part level, which is closer to human comprehension than existing methods that evaluate on a pixel level. Second, by comparing the model output for inputs with removed parts, we can esti-mate ground-truth part importances that should be reflected in the explanations. Third, by mapping individual explana-tions into a common space of part importances, we can ana-lyze a variety of different explanation types in a single com-mon framework. Using our tools, we report results for 24 different combinations of neural models and XAI methods, demonstrating the strengths and weaknesses of the assessed methods in a fully automatic and systematic manner. 1.

Introduction
Even though deep learning models have achieved break-through results in computer vision, their inner workings re-main largely opaque. As a result, deep networks sometimes receive only limited user trust and cannot be applied blindly in safety-critical domains. To overcome this issue, a grow-ing interest in the field of explainable artificial intelligence (XAI) has emerged, attempting to explain the inner work-ings of deep neural models in a human-comprehensible way. However, since there are generally no ground-truth ex-planations, evaluating XAI methods remains an open chal-Dataset and code available at github.com/visinf/funnybirds/.
Figure 1. Schematic illustration of our proposed dataset and anal-ysis framework. For each input image, we render the correspond-ing part annotation and part interventions (e.g., removed beak).
The interventions are used to estimate ground-truth importance scores of each input part, which are then compared to the estimated part importances from the explanation method under inspection (here [44]). We evaluate multiple dimensions of explainability to draw a more conclusive picture. Please refer to Sec. 4.1 for details. lenge. In fact, a third of XAI papers lack sound quantitative evaluation [38], while other work has limited comparabil-ity [31] or problematic evaluation protocols [23, 38].
To overcome the issue of missing ground-truth explana-tions, automatic evaluations are often done via proxy tasks that adhere to the idea of performing image interventions by removing certain input features to then measure the re-sulting impact on the model output [21, 23, 55]. As im-age interventions are non-trivial to perform on existing vi-sion datasets, they are usually applied on a pixel level, e.g., masking out single pixels [21, 23, 27, 52, 55]. However, this and related approaches share several downsides. First, per-forming interventions, as well as evaluating explanations on a pixel level, is disconnected from the downstream task of providing human-understandable explanations since hu-mans perceive images in concepts rather than pixels. Sec-ond, existing automatic evaluation protocols are developed for specific explanation types, e.g., pixel-level attribution maps, and thus, cannot be extended to other explanation
types like prototypes [12]. Third, by performing unreal-istic interventions in image space, e.g., masking out pixels, they introduce domain shifts compared to the training dis-tribution [10, 23, 28], which can cause the model to behave unexpectedly, and thus negatively affects the evaluation.
In this work, we address the above and more chal-lenges to contribute an important step toward a more rig-orous quantitative evaluation of XAI methods by propos-ing a thorough, dedicated evaluation/analysis tool. We do so by building a fully controllable, synthetic classification dataset consisting of renderings of artificial bird species.
This approach to analyzing XAI methods is analogous to controlled laboratory research, where we have full control over all variables, eliminating the potential influence of ir-relevant factors, and therefore, providing clearer evidence of the observed behavior [5]. Our proposed dataset allows us to make the following main contributions: (1) We cover a wide range of dimensions of explainability by consider-ing a collection of different evaluation protocols. (2) We al-low to automatically compare various explanation types in a shared framework. (3) We avoid the out-of-domain issue of previous image-space interventions by introducing seman-tically meaningful interventions at training time. (4) We re-duce the gap between the downstream task of human com-prehension and XAI evaluation by proposing metrics that operate at a semantically meaningful part level rather than (5) We au-the semantically less meaningful pixel level. tomatically analyze the coherence of explanations. (6) We analyze 24 different combinations of existing XAI methods and neural models, highlighting their strengths and weak-nesses as well as identifying new insights that may be of general interest to the XAI community. 2.