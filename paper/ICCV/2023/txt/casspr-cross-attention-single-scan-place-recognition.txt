Abstract
Place recognition based on point clouds (LiDAR) is an important component for autonomous robots or self-driving vehicles. Current SOTA performance is achieved on accu-mulated LiDAR submaps using either point-based or voxel-based structures. While voxel-based approaches nicely in-tegrate spatial context across multiple scales, they do not exhibit the local precision of point-based methods. As a result, existing methods struggle with fine-grained match-ing of subtle geometric features in sparse single-shot Li-DAR scans. To overcome these limitations, we propose
CASSPR as a method to fuse point-based and voxel-based approaches using cross attention transformers. CASSPR leverages a sparse voxel branch for extracting and ag-gregating information at lower resolution and a point-wise branch for obtaining fine-grained local information.
CASSPR uses queries from one branch to try to match structures in the other branch, ensuring that both extract self-contained descriptors of the point cloud (rather than one branch dominating), but using both to inform the out-put global descriptor of the point cloud. Extensive exper-iments show that CASSPR surpasses the state-of-the-art by a large margin on several datasets (Oxford RobotCar,
TUM, USyd). For instance, it achieves AR@1 of 85.6% on the TUM dataset, surpassing the strongest prior model by
∼15%. Our code is publicly available.1 1.

Introduction 3D place recognition and localization in a city-scale map is a fundamental challenge in allowing autonomous agents to operate effectively in realistic applications, such as au-tonomous driving [20, 21, 32, 33, 47, 30] and sidewalk or indoor robot navigation [17, 40, 49, 10, 29, 28].
While GPS signals can provide reliable geo-location un-der optimal conditions, they require external satellite infor-mation and are bound to fail when there is no direct line-†Corresponding author. * Equal contribution. 1https://github.com/Yan-Xia/CASSPR
Figure 1. (Top) Voxelization, which is necessary for sparse 3D convolutions [59], loses a large amount of geometric detail (visu-alized in red). In this example, 64% of the points are lost. (Bottom) Place recognition performance on the TUM dataset. The proposed CASSPR delivers consistently better performance across all top retrieval numbers. Notably, at top 1 retrieval, it outperforms the best baseline by ∼15%. of-sight and the signal is absent, such as in tunnels, parking garages, or among tall buildings and vegetation [4]. It is therefore important to develop the capability to perform lo-calization from the on-board sensors only.
Calibrated depth sensors such as LiDAR (producing point clouds) can be used to localize the currently-observed scene by matching it against a pre-built point cloud database. Being focused on the geometry of the scene, point cloud recognition sidesteps several factors that make pure
visual (RGB image-based) matching difficult, such as vari-ations in lighting, weather, and season, where the same ge-ometric structure may appear entirely different.
In the past decade, a variety of point-cloud-based so-lutions has been proposed that achieve excellent perfor-mance. They can be classified into two broad categories: point-based descriptors and voxel-based descriptors. With several works building on PointNetVlad [2], point-based descriptors[2, 53, 35, 50] are dedicated to improving the global context aggregation (e.g. with pooling or self-attention units). However, processing points individually can miss some local context, and does not make use of flex-ible and robust local pattern matchers such as the filters of convolutional neural networks (CNNs). The voxel-based descriptors [27, 59] generally consist of CNNs leveraging 3D sparse convolutions over a sparse volumetric represen-tation, which can be implemented efficiently using hash ta-bles, most notably with the Minkowski Engine [8]. They achieve good performance with fewer parameters than other models and comparatively fast training. However, sparse voxels can only represent 1 point per voxel cell. Therefore, for larger cells, several points (possibly containing impor-tant geometric information) are necessarily lost – for exam-ple, 64% of the points in Fig. 1 (a) are lost during voxeliza-tion. Small cells, on the other hand, consume large amounts of memory – for example, 82.6 GB of GPU memory would be needed to keep 90% of the points on average [34]. It re-mains a difficult challenge to capture fine-grained geometric information with point representations, while enjoying the robust pattern matching at multiple context scales of voxel approaches, and without consuming too much memory.
Another challenge is that most previous descriptors have performed place recognition when given dense maps as a reference. While this is an important setting, accurate point cloud maps may not always be available, in unfamil-iar scenes and when an environment changes. Ideally, one would therefore prefer localization to be performed from sparse single-shot scans, which can be collected during nor-mal online operation of an autonomous system, instead of requiring a carefully-curated densely captured point cloud to be created first. A single LiDAR such as Ouster OS1-128 [59] can cover an area between 160 and 200 meters in diameter, collecting a relatively sparse set of points. Aggre-gated LiDAR scans are typically at least 10 times denser.
The larger area and lower density represent a problem for performing place recognition from single LiDAR scans.
To tackle these problems, we propose a novel cross at-tention transformer for single-scan-based place recognition, named CASSPR, aiming to compensate for the quantization losses and integrating long-range spatial relationships. The key to maintaining geometric detail even with coarse vox-elization is to use a second branch to compute features in-dependently for each point, inspired by PointNet [42]. This branch is free to partition the input space into regions with boundaries that are not necessarily aligned with voxels, and associate different features with each one. One problem with this strategy is that naively applying a sparse convo-lution network to such features would still lose geomet-ric detail by removing co-located points due to each voxel only supporting features from a single point. Therefore, we propose to fuse information from both branches, namely point-wise and sparse voxelized, with a hierarchical cross-attention transformer [46] (HCAT). This transformer can flexibly aggregate local and global information into sparse voxel features, and do so very efficiently (see Sec. 6.5). This flexibility and efficiency allows us to surpass the state-of-the-art performance in several challenging point cloud lo-calization tasks (Sec. 6.3).
To summarize, the main contributions of this work are:
• We study the extreme sparsity of single (non-aggregated)
LiDAR scans, in the context of voxel and point-based neural networks for place recognition, and analyze the complementary roles of each approach.
• We propose a 2-stage hierarchical cross-attention trans-former (HCAT) module that is designed to compensate for the shortcomings of point-wise and voxel-wise fea-tures, compensating for the loss of the geometric detail caused by the spatial quantization and integrating long-range spatial relationships.
• We assess the computation and memory trade-offs of the different approaches, finding that our proposal reduces in-ference time and memory consumption by up to 62% and 91%, respectively, when compared to previous attention units used for LiDAR-based place recognition.
• We conduct extensive experiments on several benchmark datasets, including USyd Campus [56], Oxford Robot-Car [38] and TUM City Campus datasets [58] and show that the proposed CASSPR greatly improves over the state-of-the-art methods. 2.