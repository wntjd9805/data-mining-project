Abstract
Monocular depth estimation is known as an ill-posed task in which objects in a 2D image usually do not con-tain sufficient information to predict their depth. Thus, it acts differently from other tasks (e.g., classification and segmentation) in many ways.
In this paper, we find that self-supervised monocular depth estimation shows a di-rection sensitivity and environmental dependency in the feature representation. But the current backbones bor-rowed from other tasks pay less attention to handling different types of environmental information, limiting the overall depth accuracy. To bridge this gap, we propose a new Direction-aware Cumulative Convolution Network (DaCCN), which improves the depth feature representa-tion in two aspects. First, we propose a direction-aware the feature extrac-module, which can learn to adjust tion in each direction, facilitating the encoding of differ-ent types of information. Secondly, we design a new cu-mulative convolution to improve the efficiency for aggre-gating important environmental information. Experiments show that our method achieves significant improvements on three widely used benchmarks, KITTI, Cityscapes, and
Make3D, setting a new state-of-the-art performance on the popular benchmarks with all three types of self-supervision. https://github.com/wencheng256/DaCCN. 1.

Introduction
Monocular depth estimation is an important vision task for autonomous driving, which can generate a depth map for the image from a single camera. Unlike stereo-matching methods [38, 9, 25, 19], monocular depth estimation does not require rectified images, making it easier to be applied for self-driving cars. Because of this, monocular depth esti-mation methods attract much more attention from both the
Corresponding author†: Jianbing Shen. This work was supported in part by the FDCT grants 0154/2022/A3 and SKL-IOTSC(UM)-2021-2023, the MYRG-CRG2022-00013-IOTSC-ICI grant and the SRG2022-00023-IOTSC grant.
Figure 1. Illustration of the direction sensitivity of self-supervised monocular depth estimation. In (a), we translate the same car into different positions, and their depth values are shown in the right figure. In (b), we illustrate the connection region of the car and analyze the importance of this region. academic and the industrial societies, and many represen-tative monocular depth estimation methods [23, 24, 3, 10] have been proposed during the last decade.
The pioneering work of Eigen et al. [6] first developed a CNN-based network and trained the model in a fully su-pervised manner. To alleviate the need for the ground truth,
Grag et al. [10] proposed a self-supervised method based on the stereo images. Zhou et al. [53] proposed a pose network to predict the relative position between two consecutive frames and only employ the sequence captured by a single camera in the training phase. Based on these works, a se-ries of monocular depth estimation methods based on self-supervised learning have been proposed [33, 45, 18, 33]. In this paper, we mainly focus on the self-supervised monoc-ular depth estimation task by fully exploring the direction sensitivity and environmental dependency information of this task.
Monocular depth estimation is an ill-posed task since the pixels of one object do not contain enough information to predict its depth. Therefore, the models highly rely on the
Settings original (640 × 192)
Horizon Stretch (1280 × 192)
Vertical Stretch (640 × 384)
Equal Stretch (1280 × 384)
Abs Rel ↓ 0.115 0.118 0.108 0.109
Metrics
RMSE ↓ 4.863 4.875 4.622 4.723
F LOP s 8B 16B 16B 32B
Table 1. Analysis about different input ratios with mon-odepth2. We adopt Abs Rel, and RMSE as our metrics. For the two metrics, lower values are better. We also provide the FLOPs of each setting. interrelationships between the objects and environments.
Previous depth estimation backbones [39, 41, 14, 40] sel-dom considered the depth-aware environmental encoding efficiency, which will lead to the lack of important depth clues, thereby limiting the overall performance of models.
In Fig. 1(a), the car is translated to different positions in the image, and their depth values are visualized in the right figure. From the visualization results, we find that even with the same pixels, these objects in different positions own different depth values. This demonstrates that depth prediction relies on the environment of objects. We further observed that the horizontally translated objects have little depth variance from the original object, but the depth of vertically translated objects changed a lot. Based on these observations, we infer that information from different direc-tions plays different roles in depth estimation. The informa-tion along the view line contributes more to the depth vari-ations, and the information from the horizontal lines keeps the depth consistency between objects. Therefore feature extraction from each direction could show different pref-erences. To explore their differences, we prepare a more detailed analysis in Table 1.
As mentioned in previous works [11, 47], increasing the input resolution will facilitate detailed information extrac-tion, and a small resolution is helpful for global information encoding. Thus, we change the horizon and vertical reso-lutions, respectively, and train the model to compare their performances. If the feature extraction from the two direc-tions contributes equally to the final accuracy, models with the large horizon and vertical resolution will perform simi-larly. As shown in Table 1, the depth estimator gets a sig-nificant performance drop when increasing the horizon res-olution, indicating that the global information is preferred in this direction for better performance. While the model with a large vertical resolution obviously outperforms the one with the original resolution, which performs closely to the model with equally stretched inputs. This demonstrated that detailed information is more critical in the vertical di-rection for performance. Along this direction, we infer that information from the connection region is an important clue for depth estimation. As shown in Fig. 1(b), the ground line is a crucial reference for the depth estimation of the car [15], while the depth of the ground line largely relies on the region between it and the camera, which is named the connection region in this paper.
Although the depth estimation task is direction-sensitive and environmentally dependent, current backbones cannot fully use these properties. Traditional convolutional net-works usually have the same receptive fields for every direc-tion and encode the information from them similarly. This would lead to less efficiency in extracting various types of features. Moreover, convolutional operations equally ag-gregate information from the receptive fields into the center position. This aggregating strategy cannot efficiently utilize the critical information encoded in the connection regions.
To solve these problems, we propose a new Direction-aware Cumulative Convolution Network (DaCCN) for depth feature encoding. Our DaCCN improves the feature representation in two aspects. The first improvement is for feature extraction. DaCCN can learn to adjust the feature extraction from different directions, facilitating their infor-mation encoding. As discussed above, the encoded infor-mation from different directions in the images plays diverse roles during depth estimation. Therefore, the feature extrac-tion from each direction should be adjusted according to the features it carries. Instead of manually adjusting the feature extraction, we design a learnable and direction-aware mod-ule to optimize it in an end-to-end manner during the offline training. for
Another improvement feature aggregation. is
DaCCN can efficiently aggregate environmental informa-tion from the connection regions. The connection regions are the areas that contain all the spaces between the cam-era and objects and are critical for depth estimation. To efficiently aggregate information from these areas, we pro-pose a new cumulative convolution operation, which can accumulate the environmental features from the connection regions and learn to fuse them efficiently. We integrate our DaCCN into a state-of-the-art baseline model of self-supervised depth estimation and evaluate the performance on three representative benchmarks. Experimental results show that our method achieves significant improvements with a new start-of-the-art performance.
In conclusion, the main contributions of this paper could be summarized into four folds:
• We carefully analyze the direction sensitivity and en-vironmental dependency of self-supervised monocular depth estimation and propose the new Direction-aware
Cumulative Network for better feature representation in depth estimation.
• We find that features extracted from different direc-tions in the image play distinct roles during depth pre-diction and propose a learnable module to adjust the sample density and receptive fields for each direction.
• We propose a new convolutional operation for encod-ing the critical environmental information from the connection regions.
• Experiments on three datasets show the improvements of the proposed methods, and we set a new state-of-the-art performance on three widely used benchmarks. 2.