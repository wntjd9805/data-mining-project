Abstract
Reading text in real-world scenarios often requires un-derstanding the context surrounding it, especially when dealing with poor-quality text. However, current scene text recognizers are unaware of the bigger picture as they op-In this study, we harness erate on cropped text images. the representative capabilities of modern vision-language models, such as CLIP, to provide scene-level information to the crop-based recognizer. We achieve this by fusing a rich representation of the entire image, obtained from the vision-language model, with the recognizer word-level features via a gated cross-attention mechanism. This component grad-ually shifts to the context-enhanced representation, allow-ing for stable fine-tuning of a pretrained recognizer. We demonstrate the effectiveness of our model-agnostic frame-work, CLIPTER (CLIP TExt Recognition), on leading text recognition architectures and achieve state-of-the-art re-sults across multiple benchmarks. Furthermore, our anal-ysis highlights improved robustness to out-of-vocabulary words and enhanced generalization in low-data regimes. 1.

Introduction
Recognizing text in real-world settings often involves leveraging contextual information from the scene, particu-larly when dealing with blurry, low-resolution, corrupted, or occluded text, as showcased in Fig. 1. Conversely, learning-based methods typically detect text in the image and then perform recognition solely on the cropped detected regions, neglecting valuable scene information [8, 38, 20, 9, 2, 1, 6, 48, 11, 39, 70, 72, 46]. As a result, the practice of operating on cropped text images is inherently suboptimal.
To overcome this limitation, we explore the use of vision-language models. These models, pretrained on a vast corpus of image-caption pairs, exhibit powerful representa-tion capabilities and can be used for numerous downstream tasks [51, 63, 44, 14, 4, 34, 65, 35]. Unlike models trained only on visual data, such as MAE [23], vision-language models are also supervised by the corresponding textual de-*Corresponding author aaberdam@amazon.com.
†Work done during an Amazon internship.
Figure 1: The Importance of Seeing the Bigger Picture.
Scene context often assists in reading text in real-world sce-narios, and in certain cases, it is even vital. Thus, current crop-based text recognizers are inherently limited (Top). To address this limitation, our method, CLIPTER, provides the recognizer with scene information (Bottom).
Figure 2: CLIPTER – Incorporating Scene Context into
Text Recognizers. Our novel approach employs a frozen vision-language model, such as CLIP, to extract rich fea-tures of the entire scene image. These features are then fused with the crop-level features using our gated cross-attention mechanism, which gradually shifts the pretrained recognizer to the context-enriched features. scription. This description brings focus to the crucial details in the scene, which in turn can assist in reading poor-quality text, as we show later. Moreover, the image caption can even contain actual text words in the image, such as busi-ness logos and street names, due to their necessity for de-scribing the scene. Hence, leveraging vision-language mod-els can facilitate in recognizing such words, which are typ-ically unique, categorized as out-of-vocabulary, and there-fore pose greater difficulty to text recognition models [60].
In this work, we introduce CLIPTER (CLIP TExt Recog-nition), a general framework for integrating image-level knowledge into crop-based text recognizers. To this end, our method first extracts a rich visual representation of the entire image using a vision-language image encoder. As depicted in Fig. 2, this representation is then merged with the word-level features of the cropped text instance using a cross-attention-based operation. Additionally, we incor-porate a gating mechanism, which gradually shifts between the word-level features and the merged representations dur-ing training. This mechanism provides a more stable train-ing process and enables the adaptation of pre-existing mod-els, including those pretrained on synthetic data. As a re-sult, CLIPTER can effectively enhance any pretrained rec-ognizer with scene context awareness.
We design our method as a versatile framework consist-ing of modular blocks of varying sizes that can support vari-ous text recognition architectures and adapt to diverse com-putational constraints. In particular, we explore a range of vision and vision-language image encoders, pooling oper-ators, light-to-heavy fusion schemes, and different integra-tion points between word-level and image-level representa-tions. This integration point is critical and highly dependent on the underlying architecture, and therefore we study two types of integration point: early fusion within the vision model, which considers the image representation as addi-tional visual content, and late fusion at the decoding stage, which utilizes the image features as supplementary contex-tual information to condition the prediction on.
Throughout extensive experimentation on twelve highly-diverse datasets, our method exhibits consistent improve-ments on top of various leading text recognition methods,
In such as TRBA [8], ABINet [20], and PARSeq [11]. particular, implementing CLIPTER on PARSeq achieves state-of-the-art (SoTA) results on all benchmarks, includ-ing dense text and challenging street-view images. Further in-depth analysis reveals that incorporating CLIPTER im-proves robustness to out-of-vocabulary words and enhances generalization capability in low-data regimes.
To account for all the computations involved in adding
CLIPTER to an existing recognizer, we perform an end-to-end evaluation, in which we cascade the recognizer after an existing text detector. This setting not only reveals perfor-mance gains over two-stage and end-to-end approaches, but also demonstrate a marginal impact in the overall latency.
Finally, through a comprehensive ablation study, we de-velop a recipe for integrating CLIPTER in other text recog-nition architectures, including future ones.
To summarize, our main contributions are:
• Introducing CLIPTER, a framework for enhancing text recognition performance by incorporating scene con-text through the use of vision-language models.
• The design of a computationally efficient and flexible framework that can be incorporated with various exist-ing text recognition architectures.
• Demonstrating consistent improvements of leading text recognizers on diverse datasets, achieving state-of-the-art results, and enhancing robustness to out-of-vocabulary and generalization in low-data regimes. 2.