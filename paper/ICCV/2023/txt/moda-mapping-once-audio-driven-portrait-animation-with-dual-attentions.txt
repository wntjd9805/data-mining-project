Abstract 1.

Introduction
Audio-driven portrait animation aims to synthesize por-trait videos that are conditioned by given audio. Animating high-fidelity and multimodal video portraits has a variety of applications. Previous methods have attempted to cap-ture different motion modes and generate high-fidelity por-trait videos by training different models or sampling signals from given videos. However, lacking correlation learning between lip-sync and other movements (e.g., head pose/eye blinking) usually leads to unnatural results. In this paper, we propose a unified system for multi-person, diverse, and high-fidelity talking portrait generation. Our method con-tains three stages, i.e., 1) Mapping-Once network with Dual
Attentions (MODA) generates talking representation from given audio. In MODA, we design a dual-attention module to encode accurate mouth movements and diverse modal-ities. 2) Facial composer network generates dense and detailed face landmarks, and 3) temporal-guided renderer syntheses stable videos. Extensive evaluations demonstrate that the proposed system produces more natural and realis-tic video portraits compared to previous methods.
*Corresponding author.
Given an input audio, talking portrait animation is to syn-thesize video frames of a person whose poses and expres-sions are synchronized with the audio signal [2, 3, 4, 18].
This audio-driven portrait video generation task has gained increasing attention recently and has a wide range of ap-plications in digital avatars, gaming, telepresence, virtual reality (VR), video production etc. Conventional portrait video generation consumes intensive labor and time during setting up the background, make-up, lighting, shooting, and editing. Moreover, a re-shot is always required when there exists new textual content. In contrast, audio-driven talking video generation is more convenient and attractive which only requires a new audio clip to render a new video.
Previous methods [7, 29, 52] try to learn the correspon-dence between audio and frames. However, these meth-ods usually ignore the head pose as it is hard to sepa-rate head posture from facial movement. Many 3D face reconstruction algorithm-based and GAN-based [8] meth-ods estimate intermediate representations, such as 3D face shapes [6, 50], 2D landmarks [22, 54], or face expression parameters [49], to assist the generation process. However, such sparse representations usually lost facial details, lead-ing to over-smooth [44]. Recently, the neural radiance field 1
(NeRF) [10, 44] has been widely applied in talking head generation for high-fidelity results. However, the implicit neural representation is hard to interpret and control.
In addition, these methods are usually person-specific and re-quire extra training or adaptation time for different persons.
Although quite a number of attempts and progresses have been made in recent years, it is still challenging to gen-erate realistic and expressive talking videos. As humans are extremely sensitive to identifying the artifacts in the syn-thesized portrait videos, it sets a very high standard for this technique to become applicable. We summarize the follow-ing key points that affect human perceptions: 1) Correct-ness. The synthesized talking portrait video should be well synchronized with the driven audio. 2) Visual quality. The synthesized video should have high resolution and contain fine detail components. 3) Diversity. Besides the lip mo-tion needing to be exactly matched to the audio content, the motion of other components like eye blinking and head movement are not deterministic. They should move natu-rally as a natural human does.
To achieve these goals, previous approaches either map the mouth landmarks and the head pose separately by learn-ing different sub-networks [22, 50], or only model the mouth movement while the head pose is obtained from the existing video [29, 52]. However, lacking correlation learn-ing between lip-sync and other movements usually leads to unnatural results. In this paper, we propose a mapping-once network with dual attentions (MODA), which is a unified architecture to generate diverse representations for a talk-In or-ing portrait, simplifying the computational steps. der to combine synchronization and diversity of the talk-ing portrait generation, we carefully design a dual-attention module to learn deterministic mappings (i.e., the accurate mouth movements driven by audio) and probabilistic sam-pling (i.e., the diverse head pose/eye blinking from time-to-time), respectively. To summarize, our contributions can be listed as follows:
• We propose a talking portrait system that generates multimodal photorealistic portrait videos with accurate lip motion. Comprehensive evaluations demonstrate our system can achieve state-of-the-art performance.
• We propose a unified mapping-once with dual atten-tion (MODA) network for generating portrait represen-tation from subject conditions and arbitrary audio.
• We propose 3 technical points for taking portrait gen-eration: 1) A transformer-based dual attention mod-ule for generating both specific and diverse represen-tations. 2) A facial composer network to get accurate and detailed facial landmarks. 3) A temporally guided renderer to synthesize videos with both high quality and temporal stabilization. 2.