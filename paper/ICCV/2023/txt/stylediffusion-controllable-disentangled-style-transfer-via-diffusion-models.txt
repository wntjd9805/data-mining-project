Abstract
Content and style (C-S) disentanglement is a fundamen-tal problem and critical challenge of style transfer. Existing approaches based on explicit deﬁnitions (e.g., Gram matrix) or implicit learning (e.g., GANs) are neither interpretable nor easy to control, resulting in entangled representations and less satisfying results. In this paper, we propose a new
C-S disentangled framework for style transfer without using previous assumptions. The key insight is to explicitly extract the content information and implicitly learn the comple-mentary style information, yielding interpretable and con-trollable C-S disentanglement and style transfer. A simple yet effective CLIP-based style disentanglement loss coor-dinated with a style reconstruction prior is introduced to disentangle C-S in the CLIP image space. By further lever-aging the powerful style removal and generative ability of diffusion models, our framework achieves superior results than state of the art and ﬂexible C-S disentanglement and trade-off control. Our work provides new insights into the
C-S disentanglement in style transfer and demonstrates the potential of diffusion models for learning well-disentangled
C-S characteristics. 1.

Introduction
Given a reference style image, e.g., Starry Night by Vin-cent Van Gogh, style transfer aims to transfer its artistic style, such as colors and brushstrokes, to an arbitrary con-tent target. To achieve such a goal, it must ﬁrst properly separate the style from the content and then transfer it to an-other content. This raises two fundamental challenges: (1)
“how to disentangle content and style (C-S)” and (2) “how to transfer style to another content”.
To resolve these challenges, valuable efforts have been devoted. Gatys et al. [18] proposed A Neural Algorithm of Artistic Style to achieve style transfer, which explicitly deﬁnes the high-level features extracted from a pre-trained
Convolutional Neural Network (CNN) (e.g., VGG [71]) as
∗This work was done when Zhizhong Wang was an intern at Huawei.
†Corresponding author. content, and the feature correlations (i.e., Gram matrix) as style. This approach acquires visually stunning results and inspires a large number of successors [32, 27, 50, 1, 90].
Despite the successes, by diving into the essence of style transfer, we observed three problems with these approaches: (1) The C-S are not completely disentangled. Theoreti-cally, the C-S representations are intertwined. For exam-ple, matching the content representation of an image may also match its Gram matrix, and vice versa. (2) What CNN learned is a black box rugged to interpret [92], which makes the C-S deﬁnitions [18] uninterpretable and hard to control. (3) The transfer process is modeled as a separate optimiza-tion of content loss and style loss [18], so there lacks a deep understanding of the relationship between C-S. These prob-lems usually lead to unbalanced stylizations and disharmo-nious artifacts [6], as will be shown in later Fig. 3.
On the other hand, disentangled representation learn-ing [24] provides other ideas to implicitly disentangle C-S, either supervised [44, 34] or unsupervised [9, 93]. For style transfer, Kotovenko et al. [42] utilized ﬁxpoint triplet style loss and disentanglement loss to enforce a GAN [20]-based framework to learn separate C-S representations in an unsupervised manner. Similarly, TPFR [74] learned to disentangle C-S in latent space via metric learning and two-stage peer-regularization, producing high-quality im-ages even in the zero-shot setting. While these approaches successfully enforce properties “encouraged” by the corre-sponding losses, they still have three main problems: (1)
Well-disentangled models seemingly cannot be identiﬁed without supervision [54, 64], which means the unsupervised learning [42, 74] may not achieve truly disentangled C-S, as will be shown in later Fig. 3. (2) These approaches are all based on GANs and thus often conﬁned to the GAN pre-deﬁned domains, e.g., a speciﬁc artist’s style domain [69]. (3) The implicitly learned C-S representations are still black boxes that are hard to interpret and control [54].
Facing the challenges above, in this paper, we propose a new C-S disentangled framework for style transfer with-out using previous assumptions such as Gram matrix [18] or GANs [42]. Our key insight stems from the fact that the deﬁnition of an image’s style is much more complex
than its content, e.g., we can easily identify the content of a painting by its structures, semantics, or shapes, but it is in-tractable to deﬁne the style [61, 21, 35, 82]. Therefore, we can bypass such a dilemma by explicitly extracting the con-tent information and implicitly learning its complementary style information. Since we strictly constrain style as the complement of content, the C-S can be completely disen-tangled, and the control of disentanglement has been trans-formed into the control of content extraction. It achieves both controllability and interpretability.
However, achieving plausible and controllable content extraction is also non-trivial because the contents extracted from the content images and style images should share the same content domain, and the details of the extracted con-tents should be easy to control. To this end, we resort to recent developed diffusion models [25, 73] and introduce a diffusion-based style removal module to smoothly dispel the style information of the content and style images, extract-ing the domain-aligned content information. Moreover, ow-ing to the strong generative capability of diffusion models, we also introduce a diffusion-based style transfer module to better learn the disentangled style information of the style image and transfer it to the content image. The style disen-tanglement and transfer are encouraged via a simple yet ef-fective CLIP [62]-based style disentanglement loss, which induces the transfer mapping of the content image’s content to its stylization (i.e., the stylized result) to be aligned with that of the style image’s content to its stylization (i.e., the style image itself) in the CLIP image space. By further co-ordinating with a style reconstruction prior, it achieves both generalized and faithful style transfer. We conduct compre-hensive comparisons and ablation study to demonstrate the effectiveness and superiority of our framework. With the well-disentangled C-S, it achieves very promising styliza-tions with ﬁne style details, well-preserved contents, and a deep understanding of the relationship between C-S.
In summary, our contributions are threefold:
• We propose a novel C-S disentangled framework for style transfer, which achieves more interpretable and controllable C-S disentanglement and higher-quality stylized results.
• We introduce diffusion models to our framework and demonstrate their effectiveness and superiority in con-trollable style removal and learning well-disentangled
C-S characteristics.
• A new CLIP-based style disentanglement loss coordi-nated with a style reconstruction prior is introduced to disentangle C-S in the CLIP image space. 2.