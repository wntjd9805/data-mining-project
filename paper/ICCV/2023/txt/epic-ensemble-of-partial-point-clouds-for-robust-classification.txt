Abstract
Robust point cloud classification is crucial for real-world applications, as consumer-type 3D sensors often yield partial and noisy data, degraded by various artifacts.
In this work we propose a general ensemble framework, based on partial point cloud sampling. Each ensemble member is exposed to only partial input data. Three sam-pling strategies are used jointly, two local ones, based on patches and curves, and a global one of random sampling.
We demonstrate the robustness of our method to various local and global degradations. We show that our frame-work significantly improves the robustness of top classifica-tion netowrks by a large margin. Our experimental setting uses the recently introduced ModelNet-C database by Ren et al.[24], where we reach SOTA both on unaugmented and on augmented data. Our unaugmented mean Corruption
Error (mCE) is 0.64 (current SOTA is 0.86) and 0.50 for augmented data (current SOTA is 0.57). We analyze and explain these remarkable results through diversity analy-sis. Our code is availabe at: https://github.com/ yossilevii100/EPiC (a) Random (b) Patch (c) Curve
Figure 1: EPiC concept. Three sampling mechanism are used in our ensemble: Random captures global informa-tion, Patch holds full local resolution, and Curve is more exploratory in nature. Blue - anchor point, Red - sam-pled points. We show and explain why such ensembles are highly robust to various corruptions. 1.

Introduction
A major obstacle in data-driven algorithms is the strong relation between performance and precise knowledge of in-put statistics. A way to test network robustness is to create corrupted test sets, where training is unaware of the spe-cific corruptions. In recent years this has been investigated for images, with the creation of corrupted benchmarks e.g.:
ImageNet-C, CIFAR10-C, CIFAR100-C [11] and MNIST-C [17]. In [24] the idea is extended to point cloud classi-fication, with the introduction of ModelNet-C. In our work we present a generic framework, based on sampling, which is light, flexible and robust to Out-Of-Distribution (OOD) samples.
Ensemble learning is a long-standing concept in robust machine learning [8, 10, 6]. We would like to obtain an en-semble of learners, which are loosely correlated, that gener-alize well also to OOD input. There are two main questions: 1) How to form these learners? 2) How to combine their re-sults?
One classical way to form ensembles in deep learning (see for instance [2]), is to rely on the partial-randomness of the training process. In this setting, the ensemble consists of networks of the same architecture trained on the same training-set. Variations of networksâ€™ output stem from the different random parameter initialization and the stochas-tic gradient descent process. This induces Stochastic diver-sity. Another major approach to generate ensembles (both in classical machine learning and in deep learning [8, 2, 39]) is to change the sampling distribution of the training set for each learner. A mixture-of-experts approach is to train dif-ferent types of classifiers, in neural networks this is accom-plished by different network architectures. This induces
ModelNet-C [24] (mCE = 0.646 using PCT[9] and mCE = 0.501 using augmented with WolfMix[25, 4] ver-sion of RPC [24]), even with a small ensemble of size 12. 2.