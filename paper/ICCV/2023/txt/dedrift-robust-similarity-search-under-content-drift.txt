Abstract
The statistical distribution of content uploaded and searched on media sharing sites changes over time due to seasonal, sociological and technical factors. We investigate the impact of this “content drift” for large-scale similarity search tools, based on nearest neighbor search in embedding space. Unless a costly index reconstruction is performed frequently, content drift degrades the search accuracy and efficiency. The degradation is especially severe since, in general, both the query and database distributions change.
We introduce and analyze real-world image and video datasets for which temporal information is available over a long time period. Based on the learnings, we devise
DEDRIFT, a method that updates embedding quantizers to continuously adapt large-scale indexing structures on-the-fly.
DEDRIFT almost eliminates the accuracy degradation due to the query and database content drift while being up to 100× faster than a full index reconstruction. 1.

Introduction
The amount of content available online is growing expo-nentially over the years. Various online content sharing sites collect billions to trillions of images, videos and posts over time. Efficient Nearest Neighbor Search (NNS) techniques enable searching these vast unstructured databases based on content similarity. NNS is at the core of a plethora of practical machine learning applications including content moderation [17], retrieval augmented modeling [10, 24], keypoint matching for 3D reconstruction [1], image-based geo-localisation [12], content de-duplication [44], k-NN clas-sification [4, 11], defending against adversarial attacks [18], active learning [15] and many others.
NNS techniques extract high dimensional feature vectors (a.k.a. “embeddings”) from each item to be indexed. These embeddings may be computed by hand-crafted techniques
[35, 28] or, more commonly nowadays, with pre-trained neu-ral networks [7, 37, 36]. Given the database of embedding
*The work is partially done during the internship at Meta AI vectors D={x1, . . . , xN } ⊂ Rd and a query embedding q ∈ Rd, NNS retrieves the closest database example to q from D according to some similarity measure, typically the
L2 distance: argminx∈D∥q − x∥2 (1)
The exact nearest neighbor and its distance can be computed by exhaustively comparing the query embeddings against the entire database. On a single core of a typical server from 2023, this brute-force solution takes a few milliseconds for databases smaller than few tens of thousand examples with embeddings up to a few hundred dimensions.
However, practical use cases require real time search on millions to trillion size databases, where brute-force NNS is too slow [14]. Practitioners resort to approximate NNS (ANNS), trading some accuracy of the results to speed up the search. A common approach is to perform a statistical analysis of D to build a specialized data structure (an “index” in database terms) that performs the search efficiently. Like any unsupervised machine learning task, the index is trained on representative sample vectors from the data distribution to the index.
A tool commonly used in indexes is vector quantiza-tion [22]. It consists in representing each vector by the near-est vector taken in a finite set of centroids {c1, . . . , cK} ⊂
Rd. A basic use of quantization is compact storage: the vec-tor can be reduced to the index of the centroid it is assigned to, which occupies just ⌈log2 K⌉ bits of storage. The larger
K, the better the approximation. As typical for machine learning, the training set T is distinct from D (usually T is a small subset of D). The unsupervised training algorithm of choice for quantization is k-means which guarantees Lloyd’s optimality conditions [32].
In an online media sharing site, the index functions as a database system. After the initial training, the index in-gests new embeddings by batches as they are uploaded by users and removes content that has been deleted (Fig-ure 1). Depending on the particular indexing structure, addi-tion/deletion may be easy [27], or more difficult [40]. How-ever, a more pernicious problem that appears over the time is content drift. In practice, over months and years, the sta-tistical distribution of the content changes slowly, both for
Figure 1. Overview of the dynamic index developed in this work. Images are continuously uploaded to a online sharing platform and their embeddings are added to an index. At any moment, the index can be used to search for similar images. The index quantizes the embeddings into centroids. However, as the content drifts over time, the centroids do not match the data distribution anymore. DEDRIFT introduces a lightweight update procedure to adapt to the new data distribution. the data inserted in the index and the items that are queried.
The drift on image content may have a technical origin, e.g. camera images become sharper and have better contrast; post-processing tools evolve as many platforms offer edit-ing options with image filters that are applied to millions of images. The drift may be seasonal: the type of photos that are taken in summer is not the same as in winter, see Fig-ure 2. Sociological causes could be that people post pictures of leaderboards of a game that became viral, or there is a lockdown period where people share only indoor pictures without big crowds. In rare cases, the distribution may also change suddenly, for example because of an update of the platform that changes how missing images are displayed.
The problem posed by this distribution drift is that new in-coming vectors are distributed differently from T . Indeed, by design, feature extractors are sensitive to semantic differ-ences in the content. This mismatch between training and indexed vectors impacts the search accuracy negatively. To address this, practitioners monitor the indexing performance and initiate a full index reconstruction once the efficiency degrades noticeably. By definition, this is the optimal update strategy since it adapts exactly to the new data distribution that contains both old and recent vectors. However, at larger scales, this operation becomes a resource bottleneck since all N vectors need to be re-added to the index, and disrupts the services that relies on the index.
Our first contribution is to carefully investigate tempo-ral distribution drift in the context of large-scale nearest neighbor search. For this purpose, we introduce two real-world datasets that exhibit drift. We first measure the drift in an index-agnostic way, on exact search queries (Section 3).
Then we measure the impact on various index types that are commonly used for large-scale vector search (Section 4). (Section 5). DEDRIFT modifies the index slightly to adapt to the evolution of the vector distribution, without re-indexing all the stored elements, which would incur an O(N ) cost.
This adaptation yields search results that are close to the rein-dexing topline while being orders of magnitude faster. This modification is done while carefully controlling the accuracy degradation. Sections 6 reports and analyzes DEDRIFT’s results. 2.