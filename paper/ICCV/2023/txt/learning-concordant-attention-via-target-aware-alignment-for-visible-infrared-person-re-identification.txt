Abstract
Owing to the large distribution gap between the hetero-geneous data in Visible-Infrared Person Re-identification (VI Re-ID), we point out that existing paradigms often suffer from the inter-modal semantic misalignment issue and thus fail to align and compare local details properly.
In this paper, we present Concordant Attention Learning (CAL), a novel framework that learns semantic-aligned representations for VI Re-ID. Specifically, we design the
Target-aware Concordant Alignment paradigm, which al-lows target-aware attention adaptation when aligning het-erogeneous samples (i.e., adaptive attention adjustment ac-cording to the target image being aligned). This is achieved by exploiting the discriminative clues from the modality counterpart and designing effective modality-agnostic cor-respondence searching strategies. To ensure semantic con-cordance during the cross-modal retrieval stage, we further propose MatchDistill, which matches the attention patterns across modalities and learns their underlying semantic cor-relations by bipartite-graph-based similarity modeling and cross-modal knowledge exchange. Extensive experiments on VI Re-ID benchmark datasets demonstrate the effective-ness and superiority of the proposed CAL. 1.

Introduction
Person Re-identification (Re-ID) aims to associate per-son identities across non-overlapping cameras. It has gained increasing attention in recent years due to its practical ap-plications in real-world surveillance systems. Conventional person Re-ID methods mainly focus on retrieving the same identity across visible (RGB) cameras [48, 29, 16, 23, 10].
Despite their remarkable success, they have limited appli-cability since visible cameras cannot capture discriminative information under poor-lighting conditions (e.g., at night).
To improve the illumination robustness, infrared (IR) cam-eras are widely applied to cooperate with visible ones in
Figure 1. A high-level overview of typical local feature learning paradigms. (a) Splitting-based: The semantics of the hand-craft (b) Auxiliary-model-stripes are not always properly aligned. based: Pretrained auxiliary models are often error-prone due to the domain shifts (especially for infrared image). (c) Attention-based: Due to the inter-modal distribution gap, existing style-sensitive attention module with less descriptive learnable pro-totypes often fails to attend semantically consistent regions. (d) CAL (Ours): Our method can learn concordant attention by discriminative region mining, target-aware style-agnostic atten-tion, and part-aligned knowledge exchange. real-world surveillance systems. This increases the need to explore the Visible-Infrared Person Re-identification (VI
Re-ID) problem, which aims to associate the person images taken by different spectrum cameras to achieve long-term person tracking in 24-hour surveillance systems.
Compared to traditional RGB-based re-identification, VI
Re-ID is much more challenging due to the differences in spectral properties between visible and infrared images.
This data heterogeneity can lead to severe misalignment in the feature space and large intra-class discrepancy, result-ing in significant degradation in performance. In addition, similarly to RGB-based Re-ID tasks, VI Re-ID can also be
impacted by the changes in pose or background, resulting in increased difficulties.
Recent years have witnessed a surge of creative work on mitigating the modality gap for VI Re-ID [38, 43, 18, 44, 4, 20]. However, as most existing methods only consider learning global representations from the whole image, they fail to compare local details. In addition, owing to the short-cut learning characteristics [6], global-feature-based meth-ods would tend to learn modality-specific shortcut patterns, making the learned features susceptible to concentrating on divergent regions in each modality and resulting in sub-optimal performance. A seemingly straightforward solu-tion is to learn semantic-aligned local features with either splitting-based [29, 8, 20], auxiliary-model-based meth-ods [26, 19, 28, 1], or attention-base paradigms [18, 16, 33, 27] instead of the global ones. However, these methods also fail to achieve inter-modal semantic alignment, as demon-strated in Figure 1, and forcibly aligning these semantic-misaligned embeddings would inevitably injure the training process and compromise the performance.
Different from these deep learning methods, human vi-sual systems can naturally avoid the misalignment issue thanks to their target-aware comparison strategy. Consid-ering the scenario of comparing two images of persons (re-ferred to as “base image” and “target image”, respectively), human vision systems would first identify multiple distinc-tive key regions (such as facial and clothing details) in the target image and then direct their attention to correspond-ing regions in the base image.
In this manner, humans can always make perfect part-to-part comparisons1 since they are able to adaptively and accurately adjust their at-tention by referencing the key regions of the target image.
This suggests that exploiting clues from the target image of the modality counterpart and exploring an effective cross-modal corresponding region-searching strategy can benefit in mitigating the inter-modal semantic misalignment issue.
In this paper, we present Concordant Attention Learn-ing (CAL), a novel framework that mimics the target-aware comparison behavior of human vision systems to learn semantic-aligned representations for VI Re-ID. Firstly, we devise the Target-aware Concordant Alignment (TCA) paradigm, which aims to exploit discriminative local clues from the target modality (i.e., the modality counterpart) when aligning heterogeneous embeddings. The proposed
TCA consists of three components: (1) Discriminative Re-gion Mining: identifying diverse and discriminative key regions from the feature maps of each training sample; (2) Target-aware Style-agnostic Attention Adapter: tak-ing the selected key regions from the target modality as part queries, and applying target-aware refinement to adapt the feature attention and generate part-aligned embeddings; 1Note that we use the term “region” and “part” interchangeably to de-note the same thing. (3) Part-aligned Metric Learning: clustering or separat-ing the part-aligned embeddings across modalities accord-ing to their identity labels. Even though this target-aware refinement scheme can mitigate the modality discrepancy by leveraging the clues from the target modality, it brings higher computational costs during inference. Because it needs to be carried out for all query-gallery pairs, and the gallery set is generally large. To this end, we further pro-pose MatchDistill. First, we need to associate the generated part queries of different modalities to guarantee that the sub-sequent distillation is conducted on semantic-aligned fea-tures. This is achieved by modeling the correlations of their corresponding attention maps with bipartite graphs and con-ducting Cross-modal Query Matching (CQM) to find the optimal matches. After that, a dual-level knowledge dis-tillation loss is designed to allow cross-modality knowl-edge exchange between the best-matched queries. This can facilitate the learning of underlying relationships be-tween the visible and infrared modalities. After training with MatchDistill, each modality can learn knowledge from its modality counterpart, and no cross-modal interaction is needed during inference.
Overall, our contributions are summarized as follows:
• We propose Concordant Attention Learning (CAL), a target-aware training paradigm that mimics human behav-ior and learns concordant attention to alleviate the inter-modal attention bias issue for VI Re-ID.
• To enable part-aligned metric learning, we present Target-aware Concordant Alignment (TCA), which leverages cross-modal clues and allows adaptive attention adjust-ment when aligning heterogeneous embeddings.
• We propose MatchDistill, which matches the attention patterns across modalities and learns their underlying semantic correlations by bipartite-graph-based similarity modeling and cross-modal knowledge exchange.
• Extensive experiments demonstrate that the proposed
CAL archives state-of-the-art performance on both the
SYSU-MM01 [38] and RegDB [25] datasets. 2.