Abstract
Neural networks trained on distilled data often produce over-confident output and require correction by calibration methods. Existing calibration methods such as temperature scaling and mixup work well for networks trained on orig-inal large-scale data. However, we find that these meth-ods fail to calibrate networks trained on data distilled from large source datasets. In this paper, we show that distilled data lead to networks that are not calibratable due to (i) a more concentrated distribution of the maximum logits and (ii) the loss of information that is semantically meaningful but unrelated to classification tasks. To address this prob-lem, we propose1 Masked Temperature Scaling (MTS) and
Masked Distillation Training (MDT) which mitigate the lim-itations of distilled data and achieve better calibration re-sults while maintaining the efficiency of dataset distillation. 1.

Introduction
Dataset distillation (DD) has recently gained growing at-tention because of its ability to reduce the need for large amounts of data during deep neural network (DNN) train-ing, thereby reducing training time and storage burden [40].
Despite the efficiency of training, studies have pointed out that DD still has multiple limitations. On the one hand, the distillation process is found to be time-consuming, compu-tationally expensive, and storage intensive [40, 53, 52, 7, 27, 28, 14, 49]. On the other hand, DNNs trained on DD data are said to be poorly generalizable to different models 1Code available at https://github.com/DongyaoZhu/calibrate-networks-trained-on-distilled-datasets
Figure 1. ECE (red area, smaller is better) of different calibrations on an over-confident ConvNet trained on MTT [3] distilled CI-FAR10 and CIFAR100. Our proposed techniques achieve the best calibration results compared to the over-calibration of other meth-ods. Focal: Focal loss. LS: Label Smoothing. or downstream tasks [40, 53, 52]. Efforts have been con-ducted to address these issues [3, 50, 24]. However, the calibration of DD has been overlooked, which is important for deploying DD safely in real-world applications.
An increasing number of studies are investigating cal-ibration as an important property of DNNs, which means that a DNN should know when it is likely to be wrong [10, 26, 1]. In other words, the confidence (probability related to the predicted category label) of a model should reflect its ground truth correctness likelihood (accuracy). Previous work has found that DNNs are often too confident to real-ize when they are making mistakes [10, 30], which leads to safety issues, especially in safety-critical tasks, e.g., auto-mated healthcare and self-driving cars [6, 32].
We for the first time identify and study the calibration problem of DNNs trained on distilled data (DDNNs).
Problem 1. We find that DDNNs still suffer from over-confidence problem.
We evaluate the calibration quality of DDNNs by Ex-pected Calibration Error (ECE) [10], which is a common metric to quantitatively measure the difference between confidence and accuracy. Specifically, to calculate the ECE, we categorize the output probability and accuracy into dif-ferent levels and calculate the average absolute difference.
The lower the ECE, the better the calibration. As shown in
Figure 1, the ECE (red area) of DDNNs is quite visible in the figures of the first column, which means that the prob-ability of DDNNs’ output is usually higher than the actual accuracy of its prediction. Thus, it is desirable to calibrate
DDNNs for reliable prediction and decision-making.
Problem 2. We find that DDNNs are not calibratable when using existing calibration methods.
There are calibration methods designed to align the con-fidence and accuracy of DNNs trained on full datasets (FDNNs). They either modify loss term during network training [21], use soft labels [47, 36], or scale down the logits after training [10]. However, when training on dis-tilled data, we find that most of the existing methods tend to over-calibrate DDNNs. As shown in Figure 1, a DDNN trained on distilled CIFAR10 (the first column) has an ini-tial ECE of 6.17% (red area). After calibrating with fo-cal loss (the second column), mixup (the third column), or label smoothing (the fourth column), the DDNN becomes under-confident with increased ECE of 7.79%, 14.09%, and 26.18% respectively, as shown by the inverted and enlarged red bars. This over-calibration problem also occurs for var-ious distillation methods on common datasets (Table 1).
In order to address the issues mentioned above, we raise the following questions:
Question 1. Why are DDNNs not calibratable when us-ing existing calibration methods?
We first dive deep into the differences between the source full data and the distilled data. We find that the distilled data tend to retain information relevant to the classifica-tion task while discarding other distributional information in the full data, which may result in limiting DDNNs to pursuing higher accuracy in the classification task while losing more abilities in latent representation learning of
FDNNs [37, 29]. By decomposing distilled and full data into smaller components and studying their corresponding significance to model training accuracy, we show that dis-tilled data contains very condensed information, implying a loss of information and leading to harder during-training calibration. Then, we also investigate the differences be-tween DDNNs and FDNNs. We observe that DDNNs have a more concentrated distribution of logit values, leading to less room for after-training calibration methods such as temperature scaling.
Question 2. How to calibrate DDNNs efficiently?
To enable DDNNs to be calibratable, we propose (i)
Mask Temperature Scaling and (ii) Masked Distillation
Training that can be applied both during and after the training of DDNNs. We design a binary masking method for synthetic input when training for distillation objection, which effectively forces the distillation model to extract richer information from the source dataset into distilled datasets, leading to better encoding abilities and thus bet-ter calibration of DDNNs. We also show that our proposed masked temperature scaling better improves after-training calibration results on DDNNs by introducing more dynam-ics to network outputs. Our proposed techniques thus allow for more powerful and more calibratable DDNNs. We sum-marize contributions as follows:
• We for the first time study the calibration of DDNNs and find that DDNNs are not calibratable.
• We find that DD discards semantically meaningful in-formation and that DDNNs produce a concentrated logit distribution, which explains the difficulty of cali-brating DDNNs.
• We propose two masking techniques that can improve the calibration of DDNNs better than existing cali-bration methods, i.e., masked distillation training and masked temperature scaling. In addition, our proposed techniques can be readily deployed in existing dataset distillation methods with minimal extra cost.
• We perform extensive experiments on multiple bench-mark datasets, model architectures, and data distilla-tion methods. Our techniques reduce ECE values by up to 91.05% with comparable accuracy. 2.