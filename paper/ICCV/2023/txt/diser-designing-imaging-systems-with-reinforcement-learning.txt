Abstract
Imaging systems consist of cameras to encode visual in-formation about the world and perception models to in-terpret this encoding. Cameras contain (1) illumination sources, (2) optical elements, and (3) sensors, while per-ception models use (4) algorithms. Directly searching over all combinations of these four building blocks to design an imaging system is challenging due to the size of the search space. Moreover, cameras and perception models are often designed independently, leading to sub-optimal task per-formance.
In this paper, we formulate these four build-ing blocks of imaging systems as a context-free grammar (CFG), which can be automatically searched over with a learned camera designer to jointly optimize the imaging system with task-specific perception models. By transform-ing the CFG to a state-action space, we then show how the camera designer can be implemented with reinforce-ment learning to intelligently search over the combinato-rial space of possible imaging system configurations. We demonstrate our approach on two tasks, depth estimation and camera rig design for autonomous vehicles, showing that our method yields rigs that outperform industry-wide standards. We believe that our proposed approach is an important step towards automating imaging system design.
Our project page is https://tzofi.github.io/diser. 1.

Introduction
Cameras are ubiquitous across industries.
In au-tonomous vehicles, camera rigs provide information on the ego-vehicle’s surroundings so it can navigate; in biology, microscopy allows new viruses to be studied and vaccines to be developed; and in AR/VR systems, advanced headsets provide immersive reconstructions of the user’s surround-ings. In each of these applications, camera configurations must be carefully designed to capture relevant information for downstream tasks, often done with perception models (PMs). PMs are typically implemented as neural networks and use the output of cameras to predict information such as where other vehicles are on the road, what type of molecule
* Equal contribution.
Figure 1: Overview: The camera designer selects imaging hard-ware candidates, which are used to capture observations in sim-ulation. The perception model is then updated and computes the reward for the camera designer using the captured observations. In our paper, we implement the camera designer with reinforcement learning and the perception model with a neural network. is present in a biological sample, or where the user is located within a virtual environment. Yet, despite their interdepen-dence, cameras and PMs are often designed independently.
Designing camera systems is non-trivial due to the vast number of engineering decisions to be made. For example, consider designing a camera rig on an autonomous vehicle.
Suppose the ego-vehicle is limited to up to 5 lidar sensors, 5 radars, and 5 RGB sensors, with 1,000 possible spatio-temporal resolutions. If there are 1,000 discrete candidate camera positions on the ego-vehicle, the search space ex-pands to 108 different configurations. In practice, the search space can become many orders larger with more possibili-ties for each imaging system building block. Furthermore, because the search space is non-differentiable, there exists a need to develop efficient methods to effectively traverse the search space for an optimal imaging configuration.
In our paper, we propose using reinforcement learning (RL) to automate search over imaging systems. We first define a language for imaging system design using context-free grammar (CFG), which allows imaging systems to be represented as strings. The CFG serves as a search space for which search algorithms can then be used to automate imag-ing system design. We refer to such an algorithm as a cam-era designer (CD) and implement it with RL. RL allows us to search over imaging systems without relying on differen-tiable simulators and can scale to the combinatorially large search space of the CFG. Inspired by how animal eyes and brains are tightly integrated [28], our approach jointly trains
Figure 2: Approach: Our approach allows a camera configuration and perception model (PM) to be co-designed for task-specific imaging applications. At every step of the optimization, the camera designer (CD), implemented with reinforcement learning, proposes candidate camera configurations (1-2), which are used to capture observations and labels in a simulated environment (3-4). The observations and labels are added to the perception buffer (5) and used to compute the loss and reward, while the N most recent observations in the perception buffer are used to train the PM. The reward is propagated to the CD agent which proposes additional changes to the candidate camera configuration. After the episode terminates, the CD agent is trained using proximal policy optimization (PPO) [39] until convergence. the CD and PM, using the accuracy of the PM to inform how the CD is updated in training (Fig. 1). Because searching over the entire CFG is infeasible with available simulators, we take the first step of validating that RL can be used to search over subsets of the CFG, including number of cam-eras, pose, field of view (FoV), and light intensity. First, we apply our method to depth estimation, demonstrating the vi-ability of jointly learning imaging and perception. Next, we tackle the practical problem of designing a camera rig for
AVs and show that our approach can create rigs that lead to higher perception accuracy than industry-standard rig de-signs. While AV camera rigs are one of many potential ap-plications of our method, to the best of our knowledge, we are among the first to propose a way to optimize AV camera rigs. Our paper makes the following contributions:
• Imaging CFG: We introduce a context-free grammar (CFG) for imaging system design, which enumerates possible combinations of illumination, optics, sensors, and algorithms. The CFG can be used as a search space and theoretical framework for imaging system design.
• Co-Design: We demonstrate how task-specific cam-era configurations can be co-designed with the percep-tion model by transforming the CFG into a state-action space and using reinforcement learning (Fig. 2). Our approach can converge despite the reward function be-ing jointly trained with the policy and value functions.
• Experimental Validation: We demonstrate our method for co-design by applying it to (1) the task of depth estimation using stereo cues, and (2) optimizing camera rigs for autonomous vehicle perception, show-ing in both cases that camera configuration and percep-tion model can be learned together. 2.