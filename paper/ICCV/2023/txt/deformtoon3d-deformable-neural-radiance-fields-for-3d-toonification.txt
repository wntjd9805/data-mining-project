Abstract
In this paper, we address the challenging problem of 3D toonification, which involves transferring the style of an artistic domain onto a target 3D face with stylized ge-ometry and texture. Although fine-tuning a pre-trained 3D
GAN on the artistic domain can produce reasonable perfor-mance, this strategy has limitations in the 3D domain. In particular, fine-tuning can deteriorate the original GAN la-tent space, which affects subsequent semantic editing, and requires independent optimization and storage for each new style, limiting flexibility and efficient deployment. To over-come these challenges, we propose DEFORMTOON3D, an effective toonification framework tailored for hierarchical 3D GAN. Our approach decomposes 3D toonification into
*Equal contribution. subproblems of geometry and texture stylization to better preserve the original latent space. Specifically, we devise a novel StyleField that predicts conditional 3D deformation to align a real-space NeRF to the style space for geome-try stylization. Thanks to the StyleField formulation, which already handles geometry stylization well, texture styliza-tion can be achieved conveniently via adaptive style mixing that injects information of the artistic domain into the de-coder of the pre-trained 3D GAN. Due to the unique design, our method enables flexible style degree control and shape-texture-specific style swap. Furthermore, we achieve effi-cient training without any real-world 2D-3D training pairs but proxy samples synthesized from off-the-shelf 2D tooni-fication models. Code is released at https://github. com/junzhezhang/DeformToon3D.
1.

Introduction
Artistic portraits are prevalent in various applications such as comics, animation, virtual reality, and augmented reality. In this work, our main objective is to propose an ef-fective approach for 3D-aware artistic toonification, a criti-cal problem that involves transferring the style of an artistic domain onto a target 3D face with stylized geometry and texture. The task opens up potential applications for quick high-quality 3D avatar creation based on a photograph with the style of a designated artwork, which would typically re-quire highly professional handcraft skills.
Substantial progress has been made in automatic por-Starting with im-trait style transfer over 2D images. age style transfer [14, 52, 34, 28] and image-to-image translation [27, 33, 10, 6, 55], recent advancements in
StyleGAN-based generators [23, 24] have shown their po-tential in high-quality toonification via efficient transfer learning [47]. Specifically, a pre-trained StyleGAN genera-tor on face images is fine-tuned to transfer to the artistic por-trait domain. With the progress of 3D-aware GANs [8, 40], researchers have extended this pipeline to 3D with well-designed domain adaptation frameworks [21, 31, 26, 1], en-abling remarkable 3D portrait toonification.
Although fine-tuning the pre-trained StyleGAN-based model for toonification achieves superior quality, it has sev-eral limitations. First, fine-tuning the pre-trained genera-tor shifts its generative space from the real face domain to the artistic portrait domain at the cost of deteriorating the original GAN latent space. With tremendous off-the-shelf tools [53, 61] trained for the original GAN space, altering the well-learned style space would affect the performance of downstream applications over the toonified portrait, e.g., semantic editing. Second, despite fine-tuning-based do-main adaptations have been thoroughly investigated for 2D
GANs [48, 64], applying this technique to 3D GANs fails to leverage the full potential of the architecture of 3D genera-tor models [8, 40] for characterizing view-consistent shape and high-frequency textures in the artistic domain. Third, it is inevitable to fine-tune a heavy generator for each new style, which requires hours of training time and additional storage. This limitation affects scalability when deploying dozens of fine-tuned generators for real-time user interac-tions. Therefore, 3D toonification remains a challenging task that requires further exploration.
To better preserve the pre-trained GAN latent space and to better exploit the 3D GAN generator, we propose
DEFORMTOON3D that decomposes geometry and texture stylization into more manageable subproblems. In partic-ular, unlike conventional 3D toonification approaches that fine-tune the whole 3D GAN generator following existing 2D fine-tuning schemes, we carefully consider the charac-teristics of 3D GANs to decompose the stylization of geom-etry and texture domains. To achieve geometry stylization, we introduce a novel StyleField on top of a pre-trained 3D generator to deform each point in the style space to the pre-trained real space guided by an instance code. This allows for easy extension to multiple styles with a single stylization field by introducing a style code to guide the deformation.
Since StyleField already handles geometry stylization well, texture stylization can be easily achieved through adaptive style mixing which injects artistic domain information into the network for effective texture toonification. Notably, our unique design enables training of the method at minimal cost using synthetic paired data with realistic faces gener-ated by a pre-trained 3D GAN and corresponding paired stylized data generated by an off-the-shelf 2D toonification model [66].
The proposed DEFORMTOON3D achieves high-quality geometry and texture toonification over a vast variety of styles, as demonstrated in Fig.1. Additionally, our approach preserves the original GAN latent space, enabling compati-bility with existing tools built on the real face space GAN, including inversion [31], editing [53], and animation [61].
Furthermore, our design significantly reduces the storage footprint by requiring only a small stylization field with a set of AdaIN parameters for artistic domain stylization. In summary, our work makes the following contributions:
• We propose a novel StyleField that separates geometry toonification from texture, providing a more efficient method for modeling 3D shapes than fine-tuning and enabling flexible style control.
• We present an approach to achieve multi-style toonifi-cation with a single model, facilitating cross-style ma-nipulation and reducing storage footprint.
• We introduce a full synthetic data-driven training pipeline that offers an efficient and cost-effective solu-tion to training the model without requiring real-world 2D-3D training pairs. 2.