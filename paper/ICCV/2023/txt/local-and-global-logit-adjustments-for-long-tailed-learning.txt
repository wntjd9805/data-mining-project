Abstract
Multi-expert ensemble models for long-tailed learning typically either learn diverse generalists from the whole dataset or aggregate specialists on different subsets. How-ever, the former is insufficient for tail classes due to the high imbalance factor of the entire dataset, while the latter may bring ambiguity in predicting unseen classes. To address these issues, we propose a novel Local and Global Logit Ad-justments (LGLA) method that learns experts with full data covering all classes and enlarges the discrepancy among them by elaborated logit adjustments. LGLA consists of two core components: a Class-aware Logit Adjustment (CLA) strategy and an Adaptive Angular Weighted (AAW) loss.
The CLA strategy trains multiple experts which excel at each subset using the Local Logit Adjustment (LLA). It also trains one expert specializing in an inversely long-tailed distribution through Global Logit Adjustment (GLA). More-over, the AAW loss adopts adaptive hard sample mining with respect to different experts to further improve accuracy.
Extensive experiments on popular long-tailed benchmarks manifest the superiority of LGLA over the SOTA methods. 1.

Introduction
Deep learning has brought profound improvements to various vision tasks, including classification, detection, seg-mentation, etc. The success of deep learning is undoubtedly inseparable from large-scale well-designed datasets, such as
ImageNet [9], COCO [25] and Places [51], which usually exhibit approximately uniform distribution over different classes. However, constructing these artificially balanced datasets is extremely difficult: sufficient instances must be collected for the tail classes whose samples are few by na-ture. According to the inherently existing power law [43], most real-world data follows a long-tailed distribution: a few head classes occupy a large portion of samples, while most tail classes only have small portions. It is challenging to learn directly from these long-tailed data, because deep
*Equal contribution
†Corresponding author
Figure 1: Our LGLA exhibits advantageous performance, i.e., Top-1 Acc (%), over existing SOTA approaches on most long-tailed benchmarks, all under the same settings for fair comparisons. On iNaturalist 2018, though surpassed on Many-shot, LGLA achieves superiority over others on
Medium-/Few-shot and the overall dataset (“All”). learning models tend to be dominated by the head classes that appear most during training, while resulting in poor performance on tail classes. This paper aims to mitigate such problems in model training on long-tailed data.
Among the existing literature coping with the long-tail problem, some design class re-balancing strategies for train-ing, including re-sampling [3, 12, 17, 33] or cost-sensitive learning [24, 20, 2, 40, 34]. In addition, decoupled learn-ing proposes a two-stage training process that decouples the representation learning and the classifier learning [18, 23] to preserve the broken feature representation caused by the re-balancing methods. Most recent efforts rely on ensemble learning to achieve state-of-the-art performances on long-tail visual recognition, where multiple experts are trained in a complementary manner, then aggregated together for in-(a) Training generalists with full data. (b) Training specialists with partial data. (c) Ours: LGLA with full data.
Figure 2: (a) Some ensemble methods train generalists (ψΩ1,...,3 ) on the entire dataset S that is severely imbalanced. (b) Some others train specialists on individual subsets (S1,...,3) that are less imbalanced, but these specialists may suffer from limited perceptions. (c) Our method trains all experts on the entire dataset S, with a novel CLA strategy to ensure an adaptive local/global awareness for the experts. Assuming this is a three-expert model, the first two experts ψl controlled by LLA
Ω1,2 excel at different subsets (marked with smile symbols), while the last expert (ψg
) optimized by GLA will further boost the
Ω3 ensemble results and delivers superior performance. ference. The ensemble process involves averaging the pre-dictions of each expert to make the final decision. These experts are commonly trained either using the entire dataset to create generalists [39, 21] (Figure 2a) or by combin-ing specialists trained on different subsets [1, 6, 41] (Fig-ure 2b). However, the former approach experiences a high level of data imbalance during training, while the latter re-stricts the vision of each model seeing only a subset of the training data, which impairs the capabilities of each special-ist as well as the overall ensemble model since they only en-counter a limited number of classes and lack effective col-laboration during training.
Inspired by the above insights, we propose a novel the Local and framework for long-tailed recognition:
Global Logit Adjustments (LGLA) framework. LGLA the Class-aware Logit Adjust-contains two core parts: ment (CLA) strategy and the Adaptive Angular Weighted (AAW) loss. CLA has two adjustment strategies, namely
Local Logit Adjustment (LLA) and Global Logit Adjust-ment (GLA). LLA trains multiple experts on the whole dataset but specializing in non-overlapping subsets and pos-sessing the most comprehensive knowledge in their respec-tive areas. Meanwhile, GLA trains one expert that achieves a global perspective and excels at handling inversely long-tailed distributions. AAW further improves classification performance by introducing adaptive hard sample mining, which enhances discriminative ability by capturing and re-weighting hard samples for each expert during training.
The core insight of LGLA is simple: each skill-diverse expert should always have access to the whole data during training. The various skills of each expert controlled by
LLA within a certain subset, as well as the capability of the last expert learned by GLA, should be obtained through an adaptive approach, instead of arbitrarily restricting their training data. Besides, the sharing of the same vision for all experts would also eliminate possible ambiguity during their ensemble.
As illustrated in Figure 2, instead of training generalists on the full dataset (Figure 2a), or training specialists on par-tial subsets (Figure 2b), LGLA (Figure 2c) combines the ad-vantages of both the generalist and the specialists by train-ing experts all on the full data, leading to diversely skilled recognition ability w.r.t both fine-grained subsets and the whole data distribution.
To sum up, LGLA inherits the strengths of generalists and specialists, resulting in improved classification perfor-mance on long-tailed data. As illustrated by Figure 1, under a fair setting ensuring the same backbone and same train-ing data, our approach achieves superior performance over existing methods on most public benchmarks. Overall, our contributions can be summarized as follows:
• We propose a novel Local and Global Logit Adjust-ments (LGLA) method to boost long-tailed recognition tasks. LGLA possesses the merits of more fine-grained optimization, higher diversity among experts, and an entire feature space learning.
• We propose a Class-aware Logit Adjustment (CLA) strategy to instruct differentiated learning among ex-perts, and an Adaptive Angular Weighted (AAW) loss for adaptive instance re-weighting, which better han-dles samples of different difficulties.
• Extensive experiments on popular long-tailed bench-marks, including CIFAR-10/100-LT [8], ImageNet-LT[27], iNaturalist 2018 [36] and Places-LT [51] have demonstrated the superiority of LGLA over the SOTA competitors, as shown by Figure 1.
2.