Abstract
Vision-based reinforcement learning (RL) depends on discriminative representation encoders to abstract the ob-servation states. Despite the great success of increas-ing CNN parameters for many supervised computer vi-sion tasks, reinforcement learning with temporal-difference (TD) losses cannot benefit from it in most complex envi-ronments.
In this paper, we analyze that the training in-stability arises from the oscillating self-overfitting of the heavy-optimizable encoder. We argue that serious oscilla-tion will occur to the parameters when enforced to fit the sensitive TD targets, causing uncertain drifting of the latent state space and thus transmitting these perturbations to the policy learning. To alleviate this phenomenon, we propose a novel asymmetric interactive cooperation approach with the interaction between a heavy-optimizable encoder and a supportive light-optimizable encoder, in which both their advantages are integrated including the highly discrimina-tive capability as well as the training stability. We also present a greedy bootstrapping optimization to isolate the visual perturbations from policy learning, where represen-tation and policy are trained sufficiently by turns. Finally, we demonstrate the effectiveness of our method in utilizing larger visual models by first-person highway driving task
CARLA and Vizdoom environments. 1.

Introduction
Learning complex control from high-dimensional obser-vations such as images is significant for many real-world applications [37, 28]. It puts forward higher requirements for the representation capability of visual encoder models,
*Corresponding author.
Figure 1. Comparison of DeepMDP agents on CARLA with light- and heavy-weight visual encoders in episode returns (a) and parameter oscillation (PT t ∇θt∥ (b). Com-pared with 4-layer CNN, larger models like ResNet-18 do not im-prove the RL performance as in supervised learning. The deteri-oration results from the oscillating self-overfitting of the larger model, which occurs in neither supervised learning nor RL with a lightweight encoder. t ∥∇θt∥)/∥ PT especially in some complex scenes such as self-driving [5] and robot controlling [29]. The last decade has witnessed impressive progress in computer vision by training large-scale networks [18] as they increase the search space of pos-sible solutions. However, such parameter increment of vi-sual models cannot directly benefit the reinforcement learn-ing, and even leads to deterioration of training. For ex-ample, as shown in Fig. 1(a), we illustrated the training curves of DeepMDP [7] agents that use different CNN net-works as visual encoders on the self-driving environment
CARLA [5]. The result shows that using a larger model, e.g., ResNet-18 [18], leads to unstable training and achieves distinctly lower returns than the lighter model with only four convolutional layers.
To investigate this phenomenon, we quantified the os-cillation of the network parameters by introducing an in-t ∥∇θt∥)/∥ PT dicator calculated by the ratio of accumulation of modulus length of the gradient to the module lengths of cumulative gradient within T training steps, (PT t ∇θt∥, where ∇ is gradient operator and θt is parameters of the last convolution layer in the t-th step. A higher indicator means worse oscillation and instability of parameters dur-ing training. As shown in Fig. 1(b), we compared the os-cillation in three experiments including i) training ResNet-18 on RL, ii) training 4-layer CNN on RL, and iii) train-ing ResNet-18 on supervised learning (SL) with CIFAR-100 [24]. ResNet-18 suffers much more serious oscilla-tion when training on RL, resulting in deteriorated perfor-mance. However, such oscillation occurs neither on SL with ResNet-18 nor on RL with the lighter 4-layer CNN.
We name this phenomenon as oscillating self-overfitting, which particularly results from a pathological concurrence of the overfitting capability of large models and the sensitive learning targets of the temporal-difference (TD) loss [25].
Specifically, when the heavyweight parameters are enforced to fit the sensitive TD targets which are partially generated by themselves with a bootstrapping formulation, contradic-tory gradients will propagate back to oscillate the param-eters. This phenomenon results in uncertain drifting of the state space and transmits these perturbations to policy learn-ing. Therefore, stabilizing visual encoders with amounts of parameters from TD losses for performance gain in RL is still an open challenge.
From the perspective of the relation between the encoder and the TD objective, existing representation learning for
RL can be categorized into two groups, as illustrated in Fig. 2. One approach jointly learns representation with policy by the TD loss [16, 7, 23]. It efficiently learns the long-term expected returns with light encoders while is incom-patible with heavy-optimizable encoders due to the oscillat-ing self-overfitting. Another group of approaches decouples representation learning from RL to avoid instability, where the encoder is learned only by auxiliary dynamic predictive losses [11, 32]. However, it is inaccessible to the expected returns from the bootstrapping objective in RL. In this pa-per, we propose a novel asymmetric interactive cooperation for representation learning in RL. To take both the advan-tages of representation capability and the stability for TD targets, it separately trains a main heavy-optimizable en-coder and a supportive light-optimizable encoder by aux-iliary tasks and TD losses, respectively. And the asymmet-ric interaction is simultaneously conducted between them to effectively exchange their knowledge from each other, where the heavy one transfer the representation capability by parameter momentum and the light one transfer the long-term expected returns by topological distillation. Hence, the heavy encoder is equipped with the capability of latent state abstraction without oscillating by the TD objective. More-over, we present a greedy bootstrapping optimization for
Illustrations of different learning paradigms of visual
Figure 2. encoders. a) Joint Learning trains the visual encoder end-to-end with reinforcement loss. b) Decoupled learning trains the encoder only by auxiliary tasks. c) The proposed AIC trains a supportive light-optimizable encoder with the expected returns information from TD targets and transfers it to the main visual encoder. further stability of training, where representation and pol-icy are trained sufficiently by turns.
The main contributions of this paper can be summa-rized in three aspects. First, it investigates the phenomenon of oscillating self-overfitting that leads to deterioration in
RL with heavy-optimizable encoders, and proposes a novel asymmetric interactive cooperation to alleviate it. Second, it presents a topological distillation between latent state spaces to learn state abstraction by interactive cooperation without any explicit labels. Third, it presents a greedy boot-strapping optimization for further stability of training. Ex-periments demonstrate a significant performance gain over the complex and realistic environments of CARLA and Viz-doom. 2.