Abstract
The ability to recognize and reason about text embed-ded in visual inputs is often lacking in vision-and-language (V&L) models, perhaps because V&L pre-training methods have often failed to include such an ability in their train-ing objective. In this paper, we propose PRESTU, a novel pre-training recipe dedicated to scene-text understanding (STU). PRESTU introduces OCR-aware pre-training ob-jectives that encourage the model to recognize text from an image and connect it to the rest of the image content.
We implement PRESTU using a simple transformer-based encoder-decoder architecture, combined with large-scale image-text datasets with scene text obtained from an off-the-shelf OCR system. We empirically demonstrate the effec-tiveness of this pre-training approach on eight visual ques-tion answering and four image captioning benchmarks. 1.

Introduction
Understanding the role of text as it appears in the context of a visual scene is important in various real-world appli-cations, e.g., from automatically organizing images of re-ceipts, to assisting visually-impaired users in overcoming challenges related to comprehension of non-Braille writ-ing in their surroundings, to enabling autonomous robots to make safe decisions in environments designed for humans.
As a result, scene-text understanding (STU) has received increased attention in vision-and-language (V&L) under-standing tasks, such as visual question answering (VQA)
[46, 5, 40, 55, 38, 37, 36] or image captioning [45, 16, 30].
Please see Figure 1 for an illustration.
We identify two distinct capabilities that models target-ing STU must address: (i) recognizing text in a visual scene and (ii) connecting the text to its context in the scene. Pre-vious solutions that target STU tasks [46, 45, 19, 59] of-ten delegate scene-text recognition to off-the-shelf OCR
* Work done at Google Research.
Figure 1: Example of scene-text understanding (STU) tasks. NOPRESTU (baseline) and PRESTU share the same V&L model, but PRESTU is pre-trained on our pro-posed pre-training objectives. Scene texts are highlighted by bounding boxes. Unlike the baseline, PRESTU cor-rectly predicts the title of the book on scene-text VQA (TextVQA [46]) and even generates a more detailed scene-text caption (e.g., “united states space shuttle”) than the ground-truth annotated by humans (TextCaps [45]). (Optical Character Recognition) systems [45, 7] and model the visual context using pre-computed object-detection fea-tures. These two streams of information (noisy OCR strings and visual features on detected objects) are used as input into a V&L model. While achieving decent results, these methods heavily rely on the quality of the upstream OCR system and lack a direct connection between the text being recognized and a high-fidelity representation of its context.
More concretely, previous methods have not fully ex-plored pre-training objectives that specifically target STU.
In general, V&L pre-training objectives (e.g., masked lan-guage modeling, image-text matching [33], etc.) have been proven effective for learning and became the go-to approach in V&L research. However, these objectives typically do
not require a model to understand the role of text embedded in a visual context. For instance, LaTr [4] ignores the visual context during pre-training and instead focuses on modeling the co-occurrence statistics of layout-aware text-only OCR tokens. Even in systems that do perform STU pre-training, such as TAP [59], their models are built upon the afore-mentioned pipeline. Specifically, TAP represents the visual input by a set of object features detected and extracted by
FRCNN [43]. As a result, it may lose some visual contexts that cannot be captured by objectness (e.g., activities) but are relevant to understand the role of recognized text.
In this paper, we address such a challenge by incorpo-rating an OCR-aware learning objective in the context of a high-fidelity representation of the image context. We adopt a Transformer-based [48] encoder-decoder V&L architec-ture, using a T5 [42] backbone. The model takes both im-age and text inputs. For the former, we extract fine-tunable visual features directly from image pixels using a ViT [12] encoder, rather than adopting frozen visual features from pre-detected objects [43]. For the latter, we concatenate task-specific text tokens (e.g., task prompts) with tokens ex-tracted from an off-the-shelf OCR system, in a manner that allows the model to interpret (via the prompt) the OCR to-kens in the context of the image.
Building upon this model, we propose PRESTU, a novel recipe for Pre-training for Scene-Text Understanding (Fig-ure 2). PRESTU consists of two main steps. First, it teaches the model to recognize scene text from image pixels1 and at the same time connect scene text to the visual context.
Specifically, given an image and the “part” of the scene texts in the image, the model is pre-trained to predict the “rest” of the scene texts. We call this step SPLITOCR. Second, it teaches the model to further strengthen the connection between scene text and visual context by pre-training with
OCR-aware downstream tasks (e.g., VQA and CAP). For pre-training, we leverage large-scale image-text resources
[44, 8, 5], with the (noisy) scene text extracted by the off-the-shelf OCR system (Google Cloud OCR2).
We validate PRESTU on eight VQA (ST-VQA [5],
TextVQA [46], VizWiz-VQA [15], VQAv2 [14], OCR-VQA [40], DocVQA [38], ChartQA [36], AI2D [26]) and four image captioning (TextCaps [45], VizWiz-Captions [16], WidgetCap [30], Screen2Words [51]) bench-marks. Our OCR-aware objectives SPLITOCR, VQA, and
CAP are significantly beneficial. For instance, compared with strong baselines which take OCR signals as input, we observe more than 10% absolute gain on TextVQA and 42
CIDEr point gains on TextCaps (Figure 1). Finally, we con-duct comprehensive experiments to understand which fac-tors contribute to effective STU pre-training. In summary, our contributions are as follows: 1This makes our model more robust to the quality of OCR systems. 2https://cloud.google.com/vision/docs/ocr
• We propose PRESTU, a simple and effective pre-training recipe with OCR-aware objectives designed for scene-text understanding (§2).
• We show that our objectives consistently lead to im-proved scene-text understanding on twelve diverse down-stream VQA / image captioning tasks (§3.1) and even on cases when OCR signals are absent during downstream tasks (§3.2).
• We perform detailed analyses to understand the effect of our design choices on STU performance (§3.2). 2. PreSTU: Pre-Training for Scene-Text Un-derstanding
Figure 2 provides an overview of PRESTU OCR-aware objectives and their input-output format. In what follows, we first describe our starting point: model architecture and
OCR signals (§2.1). Then, we describe our recipe for pre-training (§2.2), including the objectives, SPLITOCR, VQA, and CAP (§2.2.1), and data sources (§2.2.2). Finally, we describe the fine-tuning stage and target benchmarks (§2.3). 2.1. Setup
V&L model architecture. Our main architecture is illus-trated in Figure 3. We start from an encoder-decoder V&L architecture which unifies image-to-text (e.g., image cap-tioning) and image+text-to-text (e.g., VQA) tasks. The pre-trained vision encoder is ViT-B/16 [12], and the pre-trained language encoder-decoder is mT5-Base [58]. Specifically,
ViT is a transformer-based encoder that takes a sequence of image patches as input, pre-trained on an image classi-fication task. mT5 is a multilingual variant of text-to-text transformers T5 [42], pre-trained on a massive multilingual text corpus with the span corruption objective. See more details in the supplementary material.
As mentioned in LaTr [4], this starting point leads to modeling advantages over existing model architectures for
STU tasks. First, we believe that understanding the role of OCR text in the visual context is much easier from im-age pixels, making ViT a natural choice. Second, mT5 uses wordpiece vocab to encode and decode text tokens; thus a certain level of robustness to the noise in the input OCR texts comes with it by default. On the other hand, M4C [19] and TAP [59] resort to a more complicated solution of us-ing fastText [6] and Pyramidal Histogram of Characters fea-tures [2]. Third, mT5 is an encoder-decoder model which enables to generate the open-ended text. This is suitable for general image captioning and scene-text VQA where the answers tend to be out-of-vocab.
In contrast, most prior works [46, 19, 59, 52, 34] treat VQA as answer vocab-based classification. Lastly, our model is built upon well-developed vanilla unimodal building blocks in vision and
NLP. We deliberately choose this general encoder-decoder
Figure 2: Our proposed pipeline. Left: Comparison between PRESTU and NOPRESTU (baseline) we want to com-pare against. Green denotes the PRESTU pre-training phase and yellow the downstream/fine-tuning phase. SPLITOCR encourages scene-text recognition as well as the learning of the connection between scene text and its visual context;
VQA and CAP further strengthen that connection. Right: The text input and output for each objective. All objectives utilize OCR signals. See Figure 3 for the architecture of PRESTU. 2.2. Pre-Training Stage 2.2.1 PreSTU Objectives
We consider two sets of OCR-aware pre-training objectives for scene-text understanding.
Task-agnostic objective: SplitOCR. Inspired by the im-language modeling pressive performance of the visual pre-training objective for image+text-to-text downstream tasks [56], we propose an OCR-aware pre-training objective called SPLITOCR. This objective is designed to be down-stream task-agnostic, focusing on teaching the two core ca-pabilities for STU: recognizing scene text and connecting it to the visual context.
We randomly split the OCR texts into two parts and use the first part as additional input and the second part as a target. Recall that we have ordered the OCR texts based on their locations such that the model can recognize them in a consistent manner. Note that if the splitting point is right at the beginning of the OCR sequence, the model per-forms a simplified version of the traditional Optical Char-acter Recognition task (i.e., predicting the whole OCR to-kens). We denote this by OCR in Table 6 and also compare it with SPLITOCR in our ablation studies.
Why SPLITOCR? SPLITOCR equips the model with the abilities to recognize scene text and connect it to the visual context in a unified, seamless manner. Specifically, operat-ing SPLITOCR upon the “first part” of OCR tokens and the image pixels (not pre-extracted global or object detection features) and predicting the “second part” of OCR tokens requires the model to (i) identify which scene text in the im-age still needs to be recognized, inherently connecting the input scene text to its visual context; (ii) perform the OCR task, inherently acquiring the scene-text recognition skill.
Task-specific objectives: VQA and CAP. We propose
Figure 3: V&L model architecture used in all of our ex-periments. We use a simple transformer-based encoder-decoder (pre-trained ViT [12] + mT5 [58]) transforming image and text inputs to the text output. Green box: text input/output. Blue box: visual input. Yellow box: model blocks. See Figure 2 for the input-output pairs for different objectives. architecture to push for the applicability of our objectives.
Such a design choice allows us to develop less model-dependent pre-training objectives.
Image resolution. Unless stated otherwise, we use the im-age resolution of 640x640 in all of our experiments.
OCR signals. We obtain OCR signals from Google Cloud
OCR for all pre-training and downstream datasets in our experiments. They come in the form of a set of texts and their corresponding box coordinates in the image (i.e., ob-ject detection-like). We order OCR texts based on their lo-cations, top-left to bottom-right and concatenate them with the T5 separator </S>. This allows models to implicitly learn the scene text’s spatial information and standarize the target output sequence during training. Unless stated oth-erwise, we use these sorted silver OCR texts in all of our experiments.
OCR-aware downstream-task-specific pre-training objec-tives on top of SPLITOCR. We consider two objectives based on our downstream tasks: (i) VQA which predicts the tar-get answer from the question prompt, the visual question, and OCR texts and (ii) CAP which predicts the target cap-tion from the caption prompt and OCR texts. This is similar to previous approaches to STU, except that we encode the image pixels, not features from pre-detected regions.
Why VQA or CAP? Task-specific objectives aim to achieve two goals. First, they further encourage the learning of the relationship between scene text and its visual context through direct interaction between input image pixels and input OCR texts. Second, it eases the knowledge transfer from pre-training to fine-tuning since task-specific objec-tives share the same input format as that of the downstream tasks (§2.3). See Figure 2 for more details. 2.2.2 Pre-Training Data
Our main pre-training data is CC15M, the union of two pop-ular image-text datasets: Conceptual Captions (CC3M) [44] and Conceptual 12M (CC12M) [8].3 CC3M consists of 3.3M ⟨image, caption⟩ pairs, obtained by processing raw alt-text descriptions from the Web. CC12M extends CC3M by relaxing its over-restrictive filtering pipeline. We use
CC15M for SPLITOCR and CAP pre-training. Note that the captions of CC15M are not used for SPLITOCR and their im-ages are not necessarily scene text-related. See more details in the supplementary material.
Since CC15M does not have data in the form of visual questions and their answers for us to leverage, we resort to
ST-VQA [5]. It is a scene-text VQA dataset whose images are collected from 6 diverse data sources (COCO-Text [50],
Visual Genome [27], VizWiz [15], ICDAR [25, 24], Im-ageNet [10], IIIT-STR [39]). We use its training set for pre-training. We use ST-VQA as pre-training data for other
VQA benchmarks as well as a downstream benchmark for testing SPLITOCR (§2.3). 2.3. Fine-tuning Stage
In all of our downstream scene-text V&L tasks, the input-output pairs follow the same format as either VQA or
CAP ( with OCR text tokens as input.) The only difference from the task-specific pre-training is the training data.
We validate PRESTU on twelve datasets related to
VQA and image captioning tasks. ST-VQA, TextVQA, and TextCaps are the main benchmarks for STU. We also consider other scene-text domains, including book (OCR-VQA), document (DocVQA), illustration (ChartQA), di-agram (AI2D), and screenshot domains (WidgetCap and
Screen2Words). VizWiz-VQA and VizWiz-Captions are for 3Due to expired URLs, only 13M ⟨image, caption⟩ pairs are used in our experiments. the blind and heavily involve STU. VQAv2 is a general
VQA dataset. See complete details in the supplementary material. 2.4. Discussion
We compare PRESTU with two well-known prior STU works TAP [59] and LaTr [4]. In terms of modeling, TAP leverages two conventional V&L objectives: visual-region masked language modeling and image-text matching, as well as the objective of learning the relative spatial position of two OCR text detections. TAP models the image using object-based features [43], which we believe is a subopti-mal visual context. Besides, TAP adopts vocab-based clas-sification, less suitable for some STU tasks which are full of out-of-vocab words. LaTr overcomes those weaknesses by adopting a similar V&L architecture to ours (ViT-B/16
/ T5large). However, its pre-training objective does not in-volve the visual component (ViT). Instead, it only pre-trains its language component to learn the co-occurrence statis-tics of layout-aware OCR tokens. As the visual component is distorted or absent during pre-training, these models do not inherently learn the two essential STU capabilities, and would likely suffer in a case when OCR signals are absent during downstream tasks. In contrast, PRESTU fully em-braces the visual component. As shown in §3.2, this brings a huge benefit especially when OCR signals are not avail-able. See a more detailed comparison in §3.1.4.
In terms of pre-training data, TAP aggregates scene-text dedicated downstream data, including ST-VQA, TextVQA,
TextCaps, and OCR-CC. Thus, while it aligns well with the corresponding downstream tasks, it is less generalizable to other V&L tasks. In contrast, PRESTU adopts general pre-training data (i.e., CC15M), providing a more flexible inter-face for V&L tasks. Besides, LaTr argues that pre-training on document images is a better choice since acquiring large quantities of natural images with scene text for pre-training is challenging and hard to scale, and the amount of text is of-ten sparse. Our work challenges this assumption and shows that one can pre-train effectively for STU on natural images with minimal preprocessing. (i.e., nothing beyond extract-ing OCR signals).
Finally, in terms of evaluation as we will show next, our experiments are done on a much wider range of benchmarks than before. This is in stark contrast to existing works which often focus on three benchmarks at most. 3. Experimental Results
Baselines. We denote by NOPRESTU our main baseline. It is the same pre-trained V&L model as PRESTU (i.e., ViT-B/16 / mT5) but not pre-trained with any of our pre-training objectives.
Metrics. For VQA tasks, we use standard VQA accuracy
Model
Pre-training
Objective
Test Benchmark
ST-VQA TextVQA VizWiz-VQA
ANLS
Acc
Acc
VQAv2
Acc
NOPRESTU
-VQA
PRESTU
SPLITOCR
SPLITOCR→VQA 56.7
N/A 65.5
N/A 44.8 48.3 55.2 56.3 57.7 / 57.2 74.8 / 75.2 58.3 / 57.6 61.9 / 61.3 62.5 / 62.0 75.0 / 75.0 76.0 / 76.2 76.1 / 76.1
Table 1: Effectiveness of PRESTU objectives on VQA. Our pre-training objectives (VQA, SPLITOCR, SPLITOCR→VQA) show consistent gains over the baseline on all VQA benchmarks. We use CC15M for SPLITOCR pre-training and ST-VQA for VQA pre-training. Since ST-VQA for VQA pre-training, we mark VQA and SPLITOCR→VQA as “N/A”. Results are reported on the test set for
ST-VQA, test-std for TextVQA, and test-dev/test-std for VizWiz-VQA and VQAv2.
Model
NOPRESTU
Pre-training
Objective
-CAP
PRESTU
SPLITOCR
SPLITOCR→CAP
TextCaps test-std
VizWiz-Captions test-std
B
M
R
S
C
B
M
R
S
C 23.4 21.0 45.0 13.6 96.9 29.4 22.6 49.9 18.5 87.2 31.6 28.5 32.8 25.6 23.9 26.2 51.5 48.9 52.2 18.7 16.3 19.1 133.1 126.1 139.1 33.7 29.8 34.3 24.5 22.6 24.7 52.8 50.3 53.4 20.8 18.6 21.1 103.1 90.2 105.6
Table 2: Effectiveness of PRESTU objectives on image captioning. Our pre-training objectives (CAP, SPLITOCR, SPLI-TOCR→CAP) show significant gains over the baseline on all image captioning benchmarks, with SPLITOCR→CAP performing best. We use CC15M for both SPLITOCR and CAP pre-training. B: BLEU@4, M: METEOR, R: ROUGE-L, S: SPICE, C: CIDEr. 3 following [46, 59, 53]. It is the average score over nine sub-sets of the ground-truth ten answers, where each score is: min( #answer occurrences
, 1). For ST-VQA/DocVQA, we use Average Normalized Levenshtein Similarity (ANLS), softly penalizing the model’s mistakes on scene-text recog-nition. For ChartQA, we report its official metric, a relaxed accuracy that allows a minor inaccuracy for numeric an-swers. For image captioning tasks, we use their standard evaluation metrics, including BLEU [41], METEOR [11],
ROUGE-L [31], SPICE [3], and CIDEr [49]. 3.1. Main Results
The main goal of our experiments is to assess the utility of our pre-training objectives SPLITOCR and VQA/CAP in
VQA (§3.1.1) and image captioning (§3.1.2) tasks. 3.1.1 VQA
Table 1 summarizes our main results on VQA tasks, includ-ing ST-VQA, TextVQA, VizWiz-VQA, and VQAv2. SPLI-TOCR outperforms the baseline (i.e., without our STU pre-training) by a large margin on scene-text-heavy VQA tasks, more than +8.8 ANLS on ST-VQA, +10.4% on TextVQA, and +4.1% on VizWiz-VQA. With SPLITOCR→VQA, we slightly but significantly improve the performance further on TextVQA and VizWiz-VQA, +1.1% and 0.7%, respec-tively. These results show the utility and applicability of our pre-training objectives for improving scene-text under-standing.
SPLITOCR and VQA are complementary on scene-text-heavy VQA tasks (TextVQA/VizWiz-VQA), where each of them alone underperforms SPLITOCR→VQA. Addition-ally, we observe the first-stage pre-training via SPLITOCR is more beneficial than the second-stage task-specific pre-training VQA. This could be due to the superiority of SPLI-TOCR or the lack of large-scale scene-text VQA pre-training data, or both. We identify data development for scene-text
VQA as an open research question.
Our results also highlight the importance of STU in gen-eral real-world VQA (i.e., not specially designed for STU).
We observe a slight but significant improvement over the baseline on VQAv2 and a more significant improvement on
VizWiz-VQA for blind people. We attribute this to a sub-set of questions that require text recognition and reasoning skills [60]. We believe this is an important step since these questions are considered “hard to learn” or even “outliers” that work against VQA algorithms [47, 23]. 3.1.2
Image Captioning
Table 2 summarizes our main results on image caption-ing tasks, TextCaps and VizWiz-Captions. Aligned with the VQA results, SPLITOCR significantly improves over the baseline across all evaluation metrics, with SPLI-TOCR→CAP performing best. The gain is notably 42.2
CIDEr points on TextCaps, and 18.4 on VizWiz-Captions.
Overall, we highlight the usefulness of SPLITOCR across
V&L tasks with different input-output formats.
Model
Widget Screen2
OCR Doc
VQA VQA
Cap Words
%Acc %ANLS %RelaxedAcc %Acc CIDEr CIDEr
Chart
QA
AI2D
NoPreSTU 71.5
PreSTU-SplitOCR 72.2 47.5 50.1 40.5 50.7 64.5 63.9 98.5 69.3 125.6 113.8
Table 3: PreSTU on other scene-text domains (Val split). See
§3.1.3 for a detailed discussion.
Similar to the VQA results, SPLITOCR and CAP are com-plementary. However, CAP alone is more beneficial than
SPLITOCR alone. We attribute this to our large-scale web-based image-text data that is already suitable for CAP pre-training. Despite such a strong CAP model, SPLITOCR still provides an additional benefit. 3.1.3 Applicability to Other Scene-Text Domains
Unlike prior STU literature [59, 52, 34, 4, 54, 13], we fur-ther explore other scene-text domains (Table 3). We show that PreSTU is also effective on book (OCR-VQA), docu-ment (DocVQA), illustration (ChartQA), diagram (AI2D), and screenshot domains (WidgetCap & Screen2Words).
This demonstrates the applicability of PRESTU to many different real-world STU problems. 3.1.4 Comparison to Prior Works
So far our results provide strong evidence for the benefit of our proposed objectives. In this section, we provide a comparison to prior works as further context. While apples-to-apples comparison has become increasingly difficult, we make our best attempt to analyze our results in the context of these works. For example, TAP’s objective has coupled the use of object detection signals, which we do not resort to. More importantly, many prior works [4, 1, 53] do not release code, rely on private data, and/or require too large-scale pre-training that is prohibitively costly to reproduce.
We first compare PRESTU to recent works focusing on
STU tasks (Rows Non-TAP to LaTr in Table 4). Overall,
PRESTU establishes strong results on all tasks. Concretely,
PreSTU achieves better results than all prior smaller-scale works (i.e., TAP, TAG, LOGOS). More interestingly, with much less data, we even outperform two larger models Con-Cap/UniTNT (139.1 vs. 105.6/109.4 in CIDEr) on TextCaps and (56.3% vs. 55.4%) on TextVQA.
PreSTU, however, performs worse than another larger model LaTr on TextVQA/ST-VQA. We attribute this to the superiority of LaTr’s V&L backbones. As shown in Table 5,
LaTrbase with no pre-training significantly outperforms our baseline (NOPRESTU) on TextVQA (52.3% vs. 45.2%).
LaTr and PRESTU use different scene-text pre-training data: LaTr uses five times larger data than PRESTU (64M vs. 13M in Table 4), which covers more diverse scene text.
This is particularly beneficial to TextVQA/ST-VQA, which contain scene text from multiple domains (e.g., brand, vehi-cle, etc.) and may explain why LaTr outperforms PRESTU.
In contrast, OCR-VQA [40] only covers book-related scene text. Thus, pre-training data becomes less important than pre-training approaches, and PRESTU outperforms
LaTr (72.2% vs. 67.5% in Table 5). Moreover, while LaTr only shows its effectiveness on VQA tasks, PreSTU shows on both VQA and image captioning tasks.
We further compare PRESTU to extremely large-scale
V&L models pre-trained on more than 2B ⟨image, text⟩ pairs. Interestingly, our best model even outperforms two much larger models Flamingo [1] and GIT2 [53] on some tasks; using much less data, we achieve better results than
Flamingo (56.3% vs. 54.1%, Table 4) on TextVQA and than
GIT2 (72.2% vs. 69.9%, Table 5) on OCR-VQA.
Recently, PaLI [9], a large-scale V&L model (ViT-e/mT5-XXL) pre-trained on 10B ⟨image, text⟩ pairs, re-ports SOTA results on all major V&L tasks, except for
VizWiz-Captions (Table 4). It is worth noting that PRESTU (specifically, our OCR) was an ingredient in the pre-training objective of PaLI to tackle OCR and STU tasks, demon-strating OCR’s utility in large-scale SOTA models.
The closest to PRESTU in terms of model/data sizes is GITL, a smaller-scale version of GIT2 (347M parame-ters and 20M ⟨image, text⟩ pairs). As shown in Table 5,
PRESTU outperforms (or is on par with) GITL on all tasks, demonstrating efficiency with respect to model/data sizes.
See more comparisons in the supplementary material. 3.2. Analysis
We aim to understand PRESTU in detail. We show (a) the importance of different components of our design choice, (b) its zero-shot transferability, (c) the effect of pre-training image resolution, (d) the effect of pre-training data size, and (e) the effect of downstream OCR quality.
Detailed ablation. As shown in Figure 2, our PRESTU consists of two (optional) pre-training stages, followed by fine-tuning on downstream tasks. Here, we aim to under-stand the gain brought by each component. We consider different combinations of the design choices at each stage and organize the results stage-by-stage into Table 6. We have the following three major observations.
First, SPLITOCR is significantly and consistently better than OCR (Rows with SPLITOCR vs. Rows with OCR in their
Stage-1). OCR is a “pure” OCR prediction task, a variant of our main SPLITOCR (OCR-conditioned OCR prediction) in which the splitting point is always at the beginning. At first glance, such a result may seem counterintuitive: pre-dicting the entire scene text is strictly harder than predicting part of the OCR text given the other part. When thought of carefully, this result indicates that OCR may put too much emphasis on recognizing scene text, at the expense of con-necting scene text to its visual context. In other words, this
Model
Vision / Language Model Data
Size
Size
Pre-training
Objective
TextCaps VizWiz-Captions ST-VQA TextVQA VizWiz-VQA VQAv2
Test Benchmark
Non-TAP [59]
TAP [59]
TAG [52]
LOGOS [34]
ConCap [54]
UniTNT [13]
LaTr [4]
NOPRESTU ViT-B/16 / mT5base 473M 0
PRESTU
ViT-B/16 / mT5base 473M 13M
FRCNN / BERTbase 146M 0 1.5M* 88K* 88K*
-SPLITOCR
SPLITOCR→VQA/CAP
-MLM+ITM+RPP
MLM+ITM+RPP
ROILOCAL
BLIP 559M 129M
VLM+ITM+ITC
ViT-B/16 / T5large 831M 64M
MLM
Flamingo [1] NFNet / Chinchilla
DaViT / TransDec
ViT-e / mT5-XXL
GIT2 [53]
PaLI [9]† 80B 5B 16B 2.3B 12.9B 10B
VLM
VLM our OCR w/ others
CIDEr
CIDEr
ANLS 96.9 126.1 139.1 93.4 109.7
--105.6 109.4
--145.0 160.4 87.2 90.2 105.6
--------120.8
-56.7 65.5
N/A 51.7 59.7 60.2 57.9
-66.0 69.6
-75.8 79.9
Acc 44.8 55.2 56.3 44.8 54.0 53.7 51.1
-55.4 61.6 54.1 67.3 73.1
Acc 57.2 61.3 62.0
-------65.4 70.1 73.3
Acc 75.2 76.2 76.1
-----80.1
-82.1 81.9 84.3
Table 4: Comparison to prior works. See §3.1.4 for a detailed discussion. FRCNN: Faster R-CNN, TransDec: 6-layer transformer decoder, MLM: Masked Language (visual region) Modeling, ITM: Image-Text Matching, RPP: Relative Position Prediction, VLM: Visual
Language Modeling, ITC: Image-Text Contrastive Loss, ROILOCAL: ROI localization. *: dedicated scene-text understanding data, includ-ing ST-VQA, TextVQA, TextCaps, and OCR-CC. †: our objective OCR is an ingredient in their pre-training objectives.
Model
Vision / Language Model Data
Size Size
Pre-training
Objective
TextCaps VizWiz-Captions ST-VQA TextVQA VizWiz-VQA VQAv2 OCR-VQA
Val or test-dev Benchmark
CIDEr
CIDEr
ANLS
NOPRESTU ViT-B/16 / mT5base 473M 0
PRESTU ViT-B/16 / mT5base 473M 13M
-SPLITOCR
SPLITOCR→VQA/CAP 100.0 134.6 141.7
LaTrbase [4] ViT-B/16 / T5base
LaTrbase [4] ViT-B/16 / T5base 281M 0 281M 64M
GITL [53] ViT-L/14 / TransDec 347M 20M 5B 12.9B
GIT2 [53] DaViT / TransDec
-MLM
VLM
VLM
--106.3 148.6 87.7 90.3 105.6
--96.1 119.4 55.6 62.7
N/A
-67.5 44.6 75.1
Acc 45.2 55.6 56.7 52.3 58.0 37.5 68.4
Acc 57.7 61.9 62.5
--62.5 71.0
Acc 74.8 76.0 76.1
--75.5 81.7
Acc 71.5 72.2
--67.5 62.4 69.9
Table 5: Comparison to GITL (similar model/data sizes to PRESTU). PreSTU outperforms (or is on par with) GITL on all tasks.
GIT2/LaTrbase-64M are for reference to show that PreSTU even outperforms these large-scale works on OCR-VQA. highlights how SPLITOCR is able to balance the two capa-bilities that we identify as important for STU (§1).
Second, SPLITOCR (or OCR) makes the visual compo-nent (ViT) inherently better at recognizing text (gap be-tween “Yes” and “No” Rows with Stage-1 pre-training vs. gap between “Yes” and “No” Rows without Stage-1 pre-training). Without Stage-1 (e.g., VQA/CAP), removing OCR signals during fine-tuning leads to more than a 33% drop on
TextVQA and a 49 CIDEr point drop on TextCaps. With
Stage-1, these drops become less than 17% and 26 CIDEr points, respectively. For TextCaps, SPLITOCR with “No”
OCR input tokens during fine-tuning even outperforms the baseline with OCR input (116.6 vs. 100.0 in CIDEr).
In summary, recognizing scene text via Stage-1 pre-training is important (i.e., cannot be achieved via VQA or CAP alone).
Third, having two sources of OCR signals is ben-eficial. OCR signals by pre-trained ViT (Row SPLI-TOCR→VQA/CAP with “No”) and OCR signals by the off-the-shelf system (Row NOPRESTU "Yes") are complemen-tary; we achieve the best result when leveraging both OCR signal sources (Row SPLITOCR→VQA/CAP with “Yes”).
See more ablation studies in the supplementary material.
Zero-shot transferability on scene-text VQA. Table 7 shows zero-shot transferability of SPLITOCR on TextVQA.
We observe that performing SPLITOCR and then fine-tuning on ST-VQA (SPLITOCR→VQA) already leads to a strong model; SPLITOCR→VQA without fine-tuning (44.3%) is competitive to NOPRESTU with fine-tuning on TextVQA training set (45.2%), while ST-VQA alone (VQA) only achieves 35.7%. This suggests that SPLITOCR enables gen-eralization for STU and may remove the need to collect
TextVQA data entirely!
Effect of image resolutions during pre-training. We hy-pothesize that pre-training with high-resolution images is important for scene-text recognition; Table 8 supports this argument. Further, pre-training with the 224x224 image resolution (standard resolution for many vision tasks) al-most does not help; it achieves the accuracy of 47.1%, close to 45.2% of NOPRESTU baseline (Table 6 Row 2), sug-gesting non-standard resolution must be considered to reap the benefit of STU pre-training.
Pre-training
Stage-1
Stage-2
Fine-tuning TextVQA TextCaps
OCR input Val Acc Val CIDEr
Model
Pre-training
TextVQA
Objective Proportion # of Data Val Acc
---VQA/CAP
OCR
-OCR
VQA/CAP
SPLITOCR
-SPLITOCR VQA/CAP
No
Yes
No
Yes
No
Yes
No
Yes
No
Yes
No
Yes 19.5 45.2 13.7 47.2 35.8 49.9 38.6 51.9 39.4 55.6 44.3 56.7 40.1 100.0 81.1 130.2 110.4 126.7 108.9 134.4 116.6 134.6 118.4 141.7
Table 6: Main ablation studies for validating the importance of our main components: SPLITOCR, VQA/CAP, and having OCR input during fine-tuning. See §3.2 for a detailed discussion. OCR refers to predicting the entire OCR text.
Model
Pre-training
Objective
Fine-tuning
TextVQA
Val Acc
NOPRESTU
--TextVQA
PRESTU
VQA
SPLITOCR→VQA
--0.04 45.2 35.7 44.3
Table 7: Zero-shot transferability on TextVQA. Our zero-shot
SPLITOCR→VQA (without fine-tuning on TextVQA) is competi-tive to supervised NOPRESTU (with fine-tuning on TextVQA).
Model
Pre-training
Fine-tuning TextVQA
Objective Resolution Resolution Val Acc
PRESTU SPLITOCR 224 384 480 640 640 47.1 50.2 53.1 55.6
Table 8: Effects of image resolutions. TextVQA accuracy goes up as the pre-training image resolution increases, emphasizing the necessity of high-resolution images during pre-training.
Effect of pre-training data scale. How much data do we need to learn to recognize text? Table 9 shows the perfor-mance of TextVQA given checkpoints pre-trained on 1%, 3%, 10%, and 30% subsets of CC15M. We find that the
TextVQA performance goes up as more pre-training data is included. This highlights the importance of data scale in acquiring transferable scene-text recognition skills.
PRESTU SPLITOCR 1% 3% 10% 30% 100% 130K 390K 1.3M 3.9M 13M 42.3 45.4 50.6 53.0 55.6
Table 9: Importance of pre-training data scale. TextVQA per-formance improves as more pre-training data, showing the impor-tance of data scale in learning transferable scene-text recognition.
Model
Pre-training
Objective
Fine-tuning
TextVQA
OCR System Val Acc
NOPRESTU
-PRESTU
SPLITOCR
TextOCR [45]
Rosetta [7] gOCR
TextOCR [45]
Rosetta [7] gOCR 44.0 36.7 45.2 54.8 50.7 55.6
Table 10: Effect of downstream OCR systems on TextVQA.
SPLITOCR makes the model more robust to the change in OCR systems during fine-tuning.
NOPRESTU. Indeed, SPLITOCR + Rosetta can even per-form better than NOPRESTU + gOCR. This result is con-sistent with Table 6, where we experiment with removing
OCR texts entirely during fine-tuning. We also find that gOCR is the most effective.
Interestingly, it is even bet-ter than human-annotated TextOCR; we hypothesize this is because TextOCR only provides word-level annotation whereas gOCR provides some grouping. 4.