Abstract
Gait recognition has emerged as a promising technique for the long-range retrieval of pedestrians, providing nu-merous advantages such as accurate identification in chal-lenging conditions and non-intrusiveness, making it highly desirable for improving public safety and security. How-ever, the high cost of labeling datasets, which is a prereq-uisite for most existing fully supervised approaches, poses a significant obstacle to the development of gait recogni-tion. Recently, some unsupervised methods for gait recog-nition have shown promising results. However, these meth-ods mainly rely on a fine-tuning approach that does not sufficiently consider the relationship between source and target domains, leading to the catastrophic forgetting of source domain knowledge. This paper presents a novel per-spective that adjacent-view sequences exhibit overlapping views, which can be leveraged by the network to gradu-ally attain cross-view and cross-dressing capabilities with-out pre-training on the labeled source domain. Specifically, we propose a fine-grained Unsupervised Domain Adapta-tion (UDA) framework that iteratively alternates between two stages. The initial stage involves offline clustering, which transfers knowledge from the labeled source domain to the unlabeled target domain and adaptively generates pseudo-labels according to the expressiveness of each part.
Subsequently, the second stage encompasses online train-ing, which further achieves cross-dressing capabilities by continuously learning to distinguish numerous features of source and target domains. The effectiveness of the pro-posed method is demonstrated through extensive experi-ments conducted on widely-used public gait datasets. 1.

Introduction
Gait recognition, a biometric technology [37, 41, 15] that identifies individuals based on their unique walking pat-terns, has gained significant attention for its ability to recog-*Corresponding Authors
Figure 1. (a) The prior knowledge of the existence of angle overlap between adjacent-view. (b) The proposed Fine-grained Unsuper-vised Domain Adaptation (UDA) framework for gait recognition. nize people at long distances, regardless of their attire. The applications [6, 31, 12] of gait recognition have been greatly facilitated by advances in deep learning and the availability of vast amounts of labeled data. However, accurately anno-tating large amounts of data remains a daunting challenge, especially under long-distance cameras, where critical in-formation such as the human face is often blurred, making it difficult to label gait sequences of the same person in differ-ent clothing, views, and carrying conditions. As a solution, we propose a fine-grained Unsupervised Domain Adapta-tion (UDA) framework that transfers knowledge from a la-beled source domain to an unlabeled target domain.
In the gait recognition task, an essential prior knowledge is the existence of overlapping angles in adjacent-view se-quences as shown in Fig. 1 (a), which enables networks to build cross-view capabilities through a chain-like approach.
Leveraging this premise, the UDA framework for gait recog-nition can gradually learn cross-dressing capabilities in the target domain through knowledge transfer and clustering, without the need for pre-training the network. We aim to reveal it by addressing the following aspects: (i) Offline clustering stage. In the offline clustering phase,
as shown in Fig. 1, the primary goal is to assign accurate pseudo-labels to unlabeled data by leveraging cross-view chaining relationships. To achieve this, the network extracts essential insights for gait from a labeled source domain and subsequently transfers this acquired knowledge to the un-labeled target domain. Following this knowledge transfer, the precise pseudo-label is assigned to the cross-view and cross-dressing data present within the target domain. This assignment hinges on the observed global feature space dis-tribution within the aforementioned target domain. Further-more, previous methods [3, 10, 16, 25, 19, 28] demonstrate that part-based methods outperform other approaches. To this end, we leverage the varied expressive capabilities of different body parts and adaptively generate a global fea-ture space, which results in a more accurate assignment of pseudo-labels to unlabeled data. (ii) Online training stage. During the online training stage, the core objective is to continually learn the gait features in both the source and target domains. As depicted in Fig. 1 (a), the angles of the sequences vary continuously during human walking, with identical angles and actions appear-ing in sequences from adjacent viewpoints (indicated by the blue and red arrows). Consequently, the network can estab-lish associations of neighboring perspectives by focusing on temporal features and gain cross-view capabilities through the association of all perspectives. Moreover, as shown in
Fig. 1 (b), the network is trained by continuously selecting features from the unlabelled target domain that have been assigned the same pseudo-label as the training set. By con-tinually adding data to the target domain in this way, the net-work focuses on more robust motion patterns to distinguish features from different domains. Prior methods [25, 26, 28] have demonstrated the criticality of motion pattern extrac-tion in cross-dressing recognition. Consequently, the net-work gradually develops the cross-dressing capability, even in cases where it is absent from the source domain data.
Driven by this analysis, we propose a simple yet ef-fective fine-grained UDA framework, wherein knowledge learned from the labeled source domain is transferred to ob-tain pseudo-labels for the unlabeled target domain. Sub-sequently, we utilize both labeled source and target domain data to extract fine-grained spatio-temporal motion patterns.
As illustrated in Fig. 3, the offline clustering stage calculates the confidence scores for each part-based feature. These confidence scores are employed to derive the global feature space and assign pseudo-labels for unlabeled data. And the hybrid memory is initialized based on these pseudo-labels. Additionally, information scores are utilized to su-pervise the hybrid memory and facilitate the computation of the part-based contrastive loss. Furthermore, the on-line training stage is shown in Fig. 2, we propose a novel
Spatio-Temporal Aggregation Network (STANet) for learn-ing global motion patterns, which contains spatial designed
Cycle Temporal Shift convolution (CTS-Conv) and Global context block. At each iteration, the hybrid memory is dy-namically updated using the global motion patterns.
Our contributions can be summarized in three aspects.
• We propose a unified UDA framework structure based on the prior knowledge of the existence of an over-lap between adjacent-view sequences, which enables our network to gradually acquire cross-view and cross-dressing capabilities without pre-training.
• We present an innovative STANet that effectively cap-tures fine-grained motion patterns while enabling the acquisition of cross-view and cross-dressing capabili-ties. Our approach demonstrates superior performance than current state-of-the-art methods, particularly un-der the cross-dressing condition.
• The effectiveness of the proposed UDA framework is demonstrated through experimental results, which show superior performance on the CASIA-B [41],
OUMVLP [32], and GREW [48] datasets. Moreover, comprehensive ablation experiments performed on the
CASIA-B dataset further corroborate the efficacy of the proposed method within the UDA framework. 2.