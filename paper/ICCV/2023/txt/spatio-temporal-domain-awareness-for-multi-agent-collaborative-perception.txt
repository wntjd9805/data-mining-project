Abstract
Multi-agent collaborative perception as a potential ap-plication for vehicle-to-everything communication could significantly improve the perception performance of au-tonomous vehicles over single-agent perception. However, several challenges remain in achieving pragmatic informa-tion sharing in this emerging research. In this paper, we propose SCOPE, a novel collaborative perception frame-work that aggregates the spatio-temporal awareness char-acteristics across on-road agents in an end-to-end man-ner. Specifically, SCOPE has three distinct strengths: i) it considers effective semantic cues of the temporal con-text to enhance current representations of the target agent; ii) it aggregates perceptually critical spatial information from heterogeneous agents and overcomes localization er-rors via multi-scale feature interactions; iii) it integrates multi-source representations of the target agent based on their complementary contributions by an adaptive fusion paradigm. To thoroughly evaluate SCOPE, we consider both real-world and simulated scenarios of collaborative 3D object detection tasks on three datasets. Extensive ex-periments show the superiority of our approach and the necessity of the proposed components. The project link is https://ydk122024.github.io/SCOPE/. 1.

Introduction
Perceiving complex driving environments is essential for ensuring road safety [23] and traffic efficiency [21] in au-tonomous driving. Despite the rapid development in the perception ability of intelligent vehicles through deep learn-ing technologies [5, 6, 14, 41, 43], the conventional single-agent perception paradigm remains challenging. Single-agent perception system is generally vulnerable and defec-tive due to the inevitable practical difficulties on the road,
†Equal contributions in no particular order. The work was done when
Dingkang Yang was at the Institute of Meta-Medical, IPASS. The two first authors thank Runsheng Xu for providing constructive suggestions.
*Corresponding author. such as restricted detection range and occlusion [47, 48]. To alleviate the inadequate observation constraints of isolated agents, multi-agent collaborative perception has emerged as a promising solution in recent years [17, 29]. With Vehicle-to-Everything (V2X) oriented communication applications, collaborative perception effectively promotes perspective complementation and information sharing among agents.
Based on recent LiDAR-based 3D collaborative percep-tion datasets [35, 36, 37, 46], several impressive works have achieved a trade-off between collaborative perception per-formance and communication bandwidth of Autonomous
Vehicles (AVs) via well-designed mechanisms. These re-markable efforts include knowledge distillation-based in-termediate collaboration [17], handshake communication mechanism [20], V2X-visual transformer [36], and spatial information selection strategy [10]. However, there is still considerable room for progress in achieving a pragmatic and effective collaborative perception. Concretely, existing methods invariably follow a single-frame static perception pattern, suffering from the data sparsity shortcoming of 3D point clouds [47] and ignoring meaningful semantic cues in the temporal context [49]. The insufficient representation of the moving objects in the current frame potentially restricts the perception performance of target vehicles.
For the collaborator-shared message fusion,
Moreover, spatial information aggregation in collab-orative perception systems has exposed several prob-lems. the per-agent/location-based fusion strategies of previous at-tempts [16, 17, 20, 28, 36] fail to deal with feature map misalignment from heterogeneous agents due to localiza-tion errors. Consequently, misleading features from collab-orators could lead to object position misjudgment of the ego vehicle (i.e., receiver) and harm its detection preci-sion. For the ego vehicle message refinement, existing ap-proaches [10, 17, 20, 21, 28, 33] rely on fused representa-tions with collaborator information to implement detection, abandoning the natural perception advantages of the ego ve-hicle and introducing the potential noises. The ego-centered characteristics may contain locally critical spatial informa-tion that is not disturbed by noise from collaborative agents
Figure 1. The overall architecture of the proposed SCOPE. The framework consists of five parts: metadata conversion and feature extrac-tion, context-aware information aggregation, confidence-aware cross-agent collaboration, importance-aware adaptive fusion, and detection decoders. The details of each individual component are illustrated in Section 3. with asynchronous sensor measurements. To this end, how to effectively break the aforementioned limitations becomes a priority for achieving robust collaborative perception.
Motivated by the above observations, we present
SCOPE, a Spatio-temporal domain awareness multi-agent
CollabOrative PErception approach to tackle existing chal-lenges jointly. From Figure 1, SCOPE achieves effective in-formation collaboration among agents and precise percep-tion of the ego agent via three proposed core components.
Specifically, (i) for the data sparsity challenge in single-frame point clouds, the context-aware information aggre-gation is proposed to aggregate the ego agent’s context in-formation from preceding frames. We employ selective in-formation filtering and spatio-temporal feature integration modules to capture informative temporal cues and historical context semantics. (ii) For the collaborator-shared message fusion challenge, the confidence-aware cross-agent collab-oration is introduced to guarantee that the ego agent ag-gregates complementary information from heterogeneous agents. Based on the confidence-aware multi-scale feature interaction, we facilitate holistic communication and miti-gate feature map misalignment due to localization errors of collaborators. (iii) For the multi-feature integration chal-lenge of the ego agent, the importance-aware adaptive fu-sion is designed to flexibly incorporate the advantages of diverse representations based on distinctive perception con-tributions. We evaluate the superiority of SCOPE on var-ious collaborative 3D object detection datasets, including
DAIR-V2X [46], OPV2V [37], and V2XSet [36]. Exten-sive experiments in real-world and simulated scenarios pro-vide favorable evidence that our approach is competitive in multi-agent collaborative perception tasks.
The main contributions can be summarized as follows:
• We present SCOPE, a novel framework for multi-agent collaborative perception. The framework fa-cilitates information collaboration and feature fusion among agents, achieving a reasonable performance-bandwidth trade-off. Comprehensive experiments on collaborative detection tasks show that SCOPE outper-forms previous state-of-the-art methods.
• To our best knowledge, we are the first to consider the temporal context of the ego agent in collaborative per-ception systems. Based on the proposed context-aware component, the target frame of point clouds efficiently integrates historical cues from preceding frames to capture valuable temporal information.
• We introduce two spatial information aggregation components to address the challenges of collabora-tion heterogeneity and fused representation unicity.
These tailored components effectively perform multi-grained information interactions among agents and multi-source feature refinement of the ego agent. 2.