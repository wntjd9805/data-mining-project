Abstract
We consider the problem of simultaneously clustering and learning a linear representation of data lying close to a union of low-dimensional manifolds, a fundamental task in machine learning and computer vision. When the manifolds are assumed to be linear subspaces, this reduces to the clas-sical problem of subspace clustering, which has been stud-ied extensively over the past two decades. Unfortunately, many real-world datasets such as natural images can not be well approximated by linear subspaces. On the other hand, numerous works have attempted to learn an appro-priate transformation of the data, such that data is mapped from a union of general non-linear manifolds to a union of linear subspaces (with points from the same manifold be-ing mapped to the same subspace). However, many existing works have limitations such as assuming knowledge of the membership of samples to clusters, requiring high sampling density, or being shown theoretically to learn trivial repre-sentations. In this paper, we propose to optimize the Maxi-mal Coding Rate Reduction metric with respect to both the data representation and a novel doubly stochastic cluster membership, inspired by state-of-the-art subspace cluster-ing results. We give a parameterization of such a repre-sentation and membership, allowing efﬁcient mini-batching and one-shot initialization. Experiments on CIFAR-10, -20,
-100, and TinyImageNet-200 datasets show that the pro-posed method is much more accurate and scalable than state-of-the-art deep clustering methods, and further learns a latent linear representation of the data.4 1.

Introduction 1.1. Clustering: from Linear to Non-linear Models
Clustering is a fundamental problem in machine learn-ing, allowing one to group data into clusters based on as-sumptions about the geometry of each cluster. As early as the 1950s, the classic k-means [43, 23, 30, 46] algo-1Mathematical Institute for Data Science, Johns Hopkins University,
USA 2Department of Electrical Engineering and Computer Sciences, Uni-versity of California, Berkeley, USA 3The Hong Kong University of Sci-ence and Technology (Guangzhou), PRC 4Code is available at https://github.com/tianjiaoding/mlc. (a) (b) (c) (d)
Figure 1: (a) Input data X where 100 points in green lie on a curve and 100 in blue lie close to a point. (b) Stage 0: Features f✓(X) from a neural network f✓ whose pa-rameters ✓ are randomly initialized. (c) Stage 1: Features after self-supervised learning. (d) Stage 2: Features further improved by the proposed Manifold Linearizing and Clus-tering (MLC). rithm emerged to cluster data that concentrate around dis-tinct centroids, with numerous variants [8, 2, 4] follow-ing. This assumption of distinct centroids was later gen-eralized in subspace clustering methods, which aim to clus-ter data lying close to a union of low-dimensional linear (or afﬁne) subspaces5. This motivated numerous lines of research in the past two decades, leading to various formu-lations [20, 22, 44, 41, 28, 75, 40, 33] with efﬁcient algo-rithms [76, 75, 15] and theoretical guarantees on the cor-rectness of the clustering [58, 59, 71, 72, 36, 63, 74, 55, 69].
Subspace clustering has been used in a wide range of appli-cations, such as segmenting image pixels [45, 70, 42], video frames [65, 61, 37], or rigid-body motions [66, §11], along 5Note, a centroid can be seen as a 0-dimensional afﬁne subspace.
with clustering face images [24, 29, 22] or human actions
[73, 25, 50].
However, while subspace clustering methods have achieved state-of-the-art performance for certain tasks and datasets, the geometric assumption upon which they rely (namely that the datapoints lie on a union of linear or afﬁne subspaces) is often grossly violated for common high-dimensional datasets. For instance, even in a dataset as simple as MNIST hand-written digits [34], images of a single digit do not lie close to a low-dimensional sub-space; directly applying subspace clustering methods thus fails. Instead, a more natural idea is to assume that each cluster is deﬁned by a non-linear low-dimensional manifold and to learn or design a non-linear transformation of the data so that points from one manifold are mapped to one lin-ear subspace. In some cases, one may be able to hand-craft an appropriate transformation of the data, with polynomial or exponential kernel mappings being examples that have been explored in the literature [21], and the authors of [39] show that a subspace clustering method achieves 99% clus-tering accuracy on MNIST when the data is passed through the scattering-transform [9].
Unfortunately though, hand-crafted design requires one to assume speciﬁc and simple families of manifolds which is often unrealistic and challenging to apply on complicated data such as natural images. On the other hand, the authors of [21] propose to cluster data via treating a local neigh-borhood of the manifold approximately as a linear subspace and applying subspace clustering techniques to local neigh-borhoods. However, this method requires sufﬁcient sam-pling density to succeed, which implies a prohibitive num-ber of samples when the manifolds have moderate dimen-sion or curvature. More recently, numerous works propose to learn an appropriate transformation of the data via deep networks and then perform subspace clustering in a latent feature space [53, 31, 1, 80, 32]. Unfortunately, it has been shown that many of these formulations are ill-posed and provably learn trivial representations6, with much of the claimed beneﬁt coming from ad-hoc post-processing rather than the method itself [27]. This motivates the one of the primary questions we consider here:
Question 1. Can we efﬁciently transform data near a union of low-dimensional manifolds, so that the transformed data lie close to a union of low-dimensional linear subspaces to allow for easy clustering? 1.2. Learning Diverse and Discriminative Features: from Supervised to Unsupervised Learning
Meanwhile, learning a compact representation from multi-modal data has been a topic of its own interest in 6In this paper, we use ‘representation’ and ‘feature’ interchangeably to mean the image of the data under a (learned) transformation. machine learning [7]. An ideal property of the learned representation is between-cluster discrimination, namely, features from different clusters should be well separated, which is often pursued via a loss such as the classic cross-entropy (CE) objective. However, an important yet often ig-nored property is that the learned representation maintains within-cluster diversity. This allows distances of samples within a cluster to be preserved under the learned trans-formation, which may facilitate downstream tasks such as generation [16], denoising [68], and semantic interpretation
[78, §B.3.1] (see also §A). Unfortunately, the representa-tion learned by CE fails to achieve this property and exhibits neural collapse, a phenomenon discovered by [51] with ex-tensive theoretical and empirical analysis [47, 85, 62, 83] (even for non-CE objectives [84]), where latent features from one cluster tend to collapse to a single point.
In contrast, [78] recently proposed Maximal Coding Rate Re-duction (MCR2) as an objective to pursue both of the men-tioned ideal properties. In particular, MCR2 learns a union-of-orthogonal-subspaces representation: features from each cluster spread uniformly in a low-dimensional subspace (compact & within-cluster diverse), and the subspaces cor-responding to different clusters are orthogonal to each other (between-cluster discriminative). Nevertheless, MCR2 re-quires ground-truth labels to learn such a representation.
This leads to our second question of interest:
Question 2. For data lying close to a union of manifolds, can we learn a union-of-orthogonal-subspaces representa-tion, without access to the ground-truth labels? 1.3. Our Contributions
To address the two interrelated questions, we start with the basic idea of blending the philosophies from MCR2 and subspace clustering to explore the best of both worlds. This idea leads us to the following contributions.
• Formulation (§2): We propose Manifold Linearizing and Clustering (MLC) objective (4), which optimizes the MCR2 loss over both the representation and a novel doubly stochastic cluster membership . The latter con-sists of pair-wise similarities between samples, and it is constrained to be doubly stochastic, inspired by state-of-the-art subspace clustering results [39, 18].
• Algorithm (§3): We describe how to parameterize and initialize the representation and membership, as well as to optimize MLC (4). Even though the membership is doubly stochastic, which may appear large in size and hard to constrain, we give an efﬁcient parameter-ization of it that allows for mini-batching and notably one-shot initialization. That is, the membership is ini-tialized with no additional training whatsoever lever-aging already-initialized representation, which is sta-ble, structured, and efﬁcient.
• Experiments (§4): On CIFAR-10, we demonstrate that MLC learns a union-of-orthogonal-subspaces rep-resentation, and achieves more accurate clustering than state-of-the-art subspace clustering methods. More-over, on CIFAR-10, -20, -100, and TinyImageNet-200, we show that MLC yields higher clustering accuracy using less running time than state-of-the-art deep clus-tering methods, even when there are many or imbal-anced clusters. 1.4. Additional