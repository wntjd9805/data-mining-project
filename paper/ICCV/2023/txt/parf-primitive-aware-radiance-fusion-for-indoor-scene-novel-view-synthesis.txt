Abstract
This paper proposes a method for fast scene radiance field reconstruction with strong novel view synthesis perfor-mance and convenient scene editing functionality. The key idea is to fully utilize semantic parsing and primitive extrac-tion for constraining and accelerating the radiance field re-construction process. To fulfill this goal, a primitive-aware hybrid rendering strategy was proposed to enjoy the best of both volumetric and primitive rendering. We further con-tribute a reconstruction pipeline conducts primitive parsing and radiance field learning iteratively for each input frame which successfully fuses semantic, primitive, and radiance information into a single framework. Extensive evaluations demonstrate the fast reconstruction ability, high rendering quality, and convenient editing functionality of our method. 1.

Introduction
Indoor scene 3D reconstruction and novel view synthesis (NVS) is a long-lasting classical topic in the field of com-puter vision for decades, which is widely used in virtual
†The corresponding authors are Lu Fang (fanglu@tsinghua.edu.cn, http://www.luvision.net/) and Tao Yu (ytrock@tsinghua.edu.cn). reality, robot perception, and visualization. Classic indoor scene reconstruction methods [22, 23] focus on geometric registration and fusion [23] based on feature matching, bun-dle adjustment [6], and multi-view stereo [37] algorithms.
However, these methods rely on discrete point clouds or voxels for scene representation, which results in high mem-ory overhead and limited ability to describe scene details, making it challenging to achieve realistic NVS effects.
The emergence of implicit continuous representations based on neural networks has revolutionized 3D vision tasks. NeRF [18] represents the density and color fields of the scene using an implicit representation. Coupled with volume rendering techniques [19, 8, 13], NeRF achieves a simple but effective pipeline for end-to-end radiance field reconstruction. NeRF not only enables realistic novel view synthesis, but also facilitates 3D structure, material, and ap-pearance recovery. However, NeRF-based methods tend to fit a diffused density field to the ground truth geome-try surface for achieving view-dependent volume rendering effects, which may not be suitable for view extrapolation due to the lack of a sharp geometry constraint. Although incorporating depth information can constrain the learn-ing of implicit geometry field, generating accurate sam-ples for view extrapolation and achieving faster conver-gence remains challenging as NeRF requires relatively re-dundant sampling around the surface for volume rendering
[29]. Additionally, training NeRF in a manner of pixel-independent strategy neglects the global geometric consis-tency of the whole scene, which introduces noise and arti-facts in the final reconstruction.
To overcome this challenge, primitive-based methods such as NeurMiPs [16] use global plane prior extracted from traditional primitive detection methods [3, 11, 30].
These methods typically use a fixed number of planes to fit the reconstructed point cloud obtained from other meth-ods [1, 15, 28]. This global structure prior effectively regularizes the implicit density field in the planar regions, thereby improving view extrapolation performance. How-ever, for regions that are difficult to describe with planes, such as curved surfaces and thin structures, the boundary of the fitted plane suffers from obvious discontinuity artifacts.
In this paper, we aim to establish an incremental radiance field reconstruction pipeline based on NeRF and semantic parsing for much higher performance, no matter view inter-polation or extrapolation, with an order of magnitude fewer training iterations than SOTA methods. Our key innovation is a divide-and-conquer strategy that makes the representa-tion ultra-simple in global primitive regions while keeping it complex in non-plane local details.
In light of this, we propose Primitive-Aware Radiance
Fusion, named PARF, for indoor scene novel view synthe-sis. Our key idea is that: Indoor scene always contains many planar regions, and by leveraging the global prim-itive prior of planar regions and the local implicit repre-sentation for non-planar regions, we can achieve much bet-ter performance with strong semantic guidance. However, representing, fusing, and training both primitive and non-primitive representation in the same radiance field from se-quential RGB-D inputs in real-time is non-trivial. In order to solve the problems above, PARF proposes a hybrid repre-sentation that uses discrete semantic volume as a medium to integrate planar semantics into the continuous and implicit scene radiance field. This allows for a primitive-aware sam-pling process in volume rendering, resulting in improved efficiency and quality. Additionally, PARF dynamically maintains a global scene plane representation and can fuse and differentiate planar regions through dynamic fusion and adaptive update. This enables efficient and noise-robust op-timization of the radiance field, as well as direct semantic editing capabilities. Overall, PARF successfully incorpo-rates semantic parsing and primitive merging into a radiance fusion framework, enabling efficient training, high-quality rendering, and semantic editing.
The contributions of PARF can be summarized as:
• We propose PARF, a novel hybrid scene representation to decompose the radiance field into primitive-based and volume-based components in a unified form.
• We contribute an incremental reconstruction frame-work for primitive-aware radiance fusion, which ef-fectively leverages the benefits of semantic parsing, primitive merging, and neural representation for in-door scene reconstruction.
• Extensive evaluations demonstrate that our method en-joys fast convergence, robust view extrapolation per-formance, and convenient scene editing ability. 2.