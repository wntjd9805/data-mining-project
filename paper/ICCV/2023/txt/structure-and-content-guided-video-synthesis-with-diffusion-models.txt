Abstract
Text-guided generative diffusion models unlock powerful image creation and editing tools. Recent approaches that edit the content of footage while retaining structure require expensive re-training for every input or rely on error-prone propagation of image edits across frames.
In this work, we present a structure and content-guided video diffusion model that edits videos based on descrip-tions of the desired output. Conflicts between user-provided content edits and structure representations occur due to in-sufficient disentanglement between the two aspects. As a so-lution, we show that training on monocular depth estimates with varying levels of detail provides control over structure and content fidelity. A novel guidance method, enabled by joint video and image training, exposes explicit control over temporal consistency. Our experiments demonstrate a wide variety of successes; fine-grained control over output char-acteristics, customization based on a few reference images, and a strong user preference towards results by our model. 1.

Introduction
Demand for more intuitive and performant video edit-ing tools has increased as video-centric platforms have been popularized. But editing in the format is still complex and time-consuming due the temporal nature of video data.
State-of-the-art machine learning models have shown great promise in improving editing workflows.
Generative approaches for image synthesis recently ex-perienced a rapid surge in quality and popularity due to the introduction of powerful diffusion models trained on large-scale datasets. Text-conditioned models, such as DALL-E 2 [34] and Stable Diffusion [38], enable novice users to generate detailed imagery given only a text prompt as input.
Latent diffusion models especially enable efficient methods for producing imagery via synthesis in a perceptually com-pressed space.
Motivated by this progress, we investigate generative models suited for interactive applications in video editing. 1
Current methods repurpose existing image models by ei-ther propagating edits with approaches that compute ex-plicit correspondences [5] or by finetuning on each individ-ual video [63]. We aim to circumvent expensive per-video training and correspondence calculation to achieve fast in-ference for arbitrary videos.
We propose a controllable structure and content-aware latent video diffusion model trained on a large-scale dataset of uncaptioned videos and images. We opt to represent structure with monocular depth estimates, and content with embeddings predicted by a pre-trained neural network. Our approach offers several powerful modes of control. First, we train our model such that the content of inferred videos, e.g. their appearance or style, match user-provided images or text prompts (Fig. 1). Second, we vary the fidelity of the structure representation during training to allow select-ing the strength of the structure preservation at test-time.
Finally, we also adjust the inference process via a custom guidance method, inspired by classifier-free guidance, to enable control over temporal consistency.
In summary, we present the following contributions:
• We extend latent diffusion models to video generation by introducing temporal layers into a pre-trained im-age model and by joint training on images and videos.
• We present a structure and content-aware model that edits videos given example images or text. Our method does not require per-video training or pre-processing.
• We demonstrate full control over temporal, content and structure consistency. We show for the first time that joint image-video training enables control over tempo-ral stability. And, training on varying levels of detail in the structure representation allows choosing the de-sired level of preservation during inference.
• We show that our approach is preferred over several other approaches in a user study. We further improve the accuracy of previously unseen content by finetun-ing on a small set of images of the desired subject. 2.