Abstract
Continual learning aims to learn a model from a contin-uous stream of data, but it mainly assumes a fixed number of data and tasks with clear task boundaries. However, in real-world scenarios, the number of input data and tasks is con-stantly changing in a statistical way, not a static way. Al-though recently introduced incremental learning scenarios having blurry task boundaries somewhat address the above issues, they still do not fully reflect the statistical proper-ties of real-world situations because of the fixed ratio of disjoint and blurry samples.
In this paper, we propose a new Stochastic incremental Blurry task boundary scenario, called Si-Blurry, which reflects the stochastic properties of the real-world. We find that there are two major challenges in the Si-Blurry scenario: (1) intra- and inter-task forget-tings and (2) class imbalance problem. To alleviate them, we introduce Mask and Visual Prompt tuning (MVP). In
MVP, to address the intra- and inter-task forgetting issues, we propose a novel instance-wise logit masking and con-trastive visual prompt tuning loss. Both of them help our model discern the classes to be learned in the current batch.
It results in consolidating the previous knowledge. In ad-dition, to alleviate the class imbalance problem, we intro-duce a new gradient similarity-based focal loss and adap-tive feature scaling to ease overfitting to the major classes and underfitting to the minor classes. Extensive experi-ments show that our proposed MVP significantly outper-forms the existing state-of-the-art methods in our challeng-ing Si-Blurry scenario. The code is available at https:
//github.com/moonjunyyy/Si-Blurry 1.

Introduction
Continual learning involves constantly learning from a stream of data while having limited access to previously
*Equally contributed
†Corresponding author seen information. In this scenario, unlike humans who can retain and apply their prior knowledge to new situations, modern deep neural networks face a challenge of catas-trophic forgetting [30, 11]. To overcome this challenge, various approaches are being explored [32, 31, 13, 21, 8].
However, these traditional continual learning scenarios have clear task boundaries, where one can distinguish tasks with input data, unlike in the real-world. In real-world applica-tions, clear task boundaries are often absent and access to data is limited to small portions at a time. This is referred to as online learning with blurry task boundaries [3].
There are many cases of class emerging or disappearing like the stock market or e-commerce. To address this, the i-Blurry scenario [18] has been recently proposed, which combines disjoint continual learning and blurry task-free continual learning. Although i-Blurry somewhat alleviates the above issue, it does not fully capture the complexity of real-world data, because i-Blurry has the fixed number of classes between tasks. Figure 1(a) shows the i-Blurry sce-nario that contains the static number of classes in each task.
In the real-world scenarios, the number of classes and tasks vary dynamically as illustrated in Figure 1(c) and 1(d). That is, as samples of a specific class continuously disappear or appear in the data stream, distribution of the data is dynam-ically changing.
To reflect the dynamic distribution of the real-world data, we propose a novel Stochastic incremental Blurry (Si-Blurry) scenario. We adopt a stochastic approach to imitate the chaotic nature of the real-world. As shown in Figure 1(b), our Si-Blurry scenario is capable of effectively sim-ulating not only newly emerging or disappearing data but also irregularly changing data distribution. In the Si-Blurry scenario, we find that there are two main reasons for perfor-mance degradation: (1) intra- and inter-task forgettings, and (2) class imbalance problem. First, the continuous change of classes between batches causes intra- and inter-task for-gettings, which make the model difficult to retain previously learned knowledge. Second, ignorance of minor classes 1
(a) Visualization of i-Blurry scenario [18]. (b) Visualization of our Si-Blurry scenario. (c) Nomalized search history from December 28, 2021, to December 28, 2022, NAVER Search Trend API : https://www.ncloud.com/ product/applicationService/searchTrend. (d) Example of task generated from real-world data.
Figure 1. Comparision of (a) i-Blurry scenario, (b) Si-blurry scenario, and (c), (d) real-world data. (a) i-Blurry scenario consists of the static number of classes in each task. all the tasks have the same number of disjoint classes and blurry classes. (b) The Si-Blurry scenario has an everchanging number of classes. The disjoint classes and blurry classes are different with each task. It denotes the unpredictable traits of the real-world. (c) and (d) show the stream of real data. Comparing (a), (b), and (d), Si-blurry is alike real-world task configuration. and overfitting to major classes worsen the class imbalance problem in the Si-Blurry scenario. Minor class ignorance occurs by insufficient consideration of a few samples which belong to minor classes in training, and overfitting on major classes makes the model biased to a large number of sam-ples that belong to major classes or disjoint classes.
To deal with the aforementioned problems, we propose a novel online continual learning method called Mask and Vi-sual Prompt tuning (MVP). We propose instance-wise logit masking and contrastive visual prompt tuning loss to alle-viate intra- and inter-task forgettings by making classifica-tion easier and allowing prompts to learn the knowledge for each task effectively. Moreover, we propose a gradi-ent similarity-based focal loss to prevent the problem of mi-nor class ignorance. This method boosts learning of the ig-nored samples of minor classes in a batch, so that the sam-ples for minor classes can be considered intensively. We also propose adaptive feature scaling to address the prob-lem of overfitting to major classes. This method measures the marginal benefit [7] of learning from a sample and pre-vents our model from learning already sufficiently trained samples.
We summarize our main contributions as follows:
• We introduce a new incremental learning scenario, coined Si-Blurry, which aims to simulate a more re-alistic continual learning setting that the neural net-works continually learn new classes online while a task boundary is stochastically varying.
• We propose an instance-wise logit masking and con-trastive visual prompt tuning loss to prevent the model from intra-task and inter-task forgettings.
• To solve the class imbalance problem, we propose a new gradient similarity-based focal loss and adaptive feature scaling for minor-class ignorance and overfit-ting on major classes.
• We experimentally achieved significantly high per-formance compared to existing methods, supporting that our proposed method shows overwhelming perfor-mance and solves the problems of Si-Blurry in CIFAR-100, Tiny-ImageNet, and ImageNet-R.
2.