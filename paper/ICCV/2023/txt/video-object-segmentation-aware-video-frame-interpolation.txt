Abstract
Video frame interpolation (VFI) is a very active re-search topic due to its broad applicability to many ap-plications, including video enhancement, video encoding, and slow-motion effects. VFI methods have been advanced by improving the overall image quality for challenging se-quences containing occlusions, large motion, and dynamic texture. This mainstream research direction neglects that foreground and background regions have different impor-tance in perceptual image quality. Moreover, accurate syn-thesis of moving objects can be of utmost importance in computer vision applications.
In this paper, we propose a video object segmentation (VOS)-aware training frame-work called VOS-VFI that allows VFI models to interpolate frames with more precise object boundaries. Specifically, we exploit VOS as an auxiliary task to help train VFI mod-els by providing additional loss functions, including seg-mentation loss and bi-directional consistency loss. From extensive experiments, we demonstrate that VOS-VFI can boost the performance of existing VFI models by render-ing clear object boundaries. Moreover, VOS-VFI displays its effectiveness on multiple benchmarks for different appli-cations, including video object segmentation, object pose estimation, and visual tracking. The code is available at https://github.com/junsang7777/VOS-VFI 1.

Introduction
Video frame interpolation (VFI) is a low-level video pro-cessing task synthesizing intermediate frames between con-secutive frames to increase the frame rate. VFI has been widely used in many applications, including slow motion generation [44, 46, 2, 27], video restoration [67, 31], video compression [65], and novel view synthesis [17, 29]. Al-though VFI has been extensively studied in past decades, there is still room for improvement because moving objects, occlusions, and cluttered backgrounds make it challenging.
Like other vision tasks, deep learning-based meth-*Corresponding author.
Figure 1: Examples of video frame interpolation and video object segmentation results on the AdaCoF baseline [35].
The proposed VOS-VFI is a training framework that can improve the performance of the baseline model without in-creasing the number of parameters and inference time. ods dominate VFI, which can be classified into kernel-based [35, 46, 15, 55] and flow-based [14, 27, 70, 32, 40] approaches. The former learns interpolation kernels for two consecutive frames as an output of a convolutional neu-ral network (CNN) to synthesize an intermediate frame; whereas the latter finds optical flow between two frames in the pixel-space or feature-space and generates an inter-mediate frame by motion compensated prediction. Both approaches have been advanced by adopting novel archi-tectures or algorithms, including coarse-to-fine architec-tures [56, 72, 32], attention mechanisms [11, 30], de-formable convolutions [21, 35], and Transformers [55, 40].
Although we have witnessed significant performance im-provements in several VFI benchmarks [11, 71, 57, 50], VFI methods have been competing to improve their performance in terms of the overall quality of interpolated images, e.g.,
PSNR and SSIM [24]. This bias in the benchmarks neglects the fact that foreground and background regions have differ-ent levels of importance in perceptual image quality [53]. In addition, VFI can contribute to the performance improve-ment of vision applications, such as video object tracking and object pose estimation. However, it is undiscovered whether VFI methods depicting high performance in global image quality perform consistently well in vision applica-tions.
In this paper, we propose to perform video object seg-mentation (VOS) as an auxiliary task during the training of the VFI model such that the interpolated frames have clear object boundaries without artifacts. Specifically, the proposed VOS-aware VFI, called VOS-VFI, performs VOS using the existing and interpolated frames and uses the ac-curacy of VOS as an auxiliary loss term. Moreover, the
VOS accuracy is measured in both forward and backward directions, and the forward-backward consistency is further enforced considering the temporal coherence of VFI.
Extensive experimental results demonstrate that the pro-posed VOS-VFI can be applied to existing state-of-the-art
VFI models, including AdaCoF [35], CDFI [15], and IFR-Net [32], making them produce interpolated frames with sharper and clearer objects. These improved results lead to the performance improvement of not only VOS but also other related vision tasks.
The main contributions are summarized as follows:
• We propose a new training framework for VFI called
VOS-VFI. To the best of our knowledge, VOS-VFI is the first work that exploits VOS to assist in training
VFI models.
• We design segmentation and bi-directional consistency loss terms of VOS that allow VFI models to render frames with precise object boundaries with temporal consistency.
• Comprehensive experimental results demonstrate the effectiveness of VOS-VFI on several vision applica-tions, including video object segmentation, video ob-ject tracking, and object pose estimation. 2.