Abstract
Standard unsupervised domain adaptation methods adapt models from a source to a target domain using la-beled source data and unlabeled target data jointly.
In model adaptation, on the other hand, access to the la-beled source data is prohibited, i.e., only the source-trained model and unlabeled target data are available. We in-vestigate normal-to-adverse condition model adaptation for semantic segmentation, whereby image-level correspon-dences are available in the target domain. The target set consists of unlabeled pairs of adverse- and normal-condition street images taken at GNSS-matched locations.
Our method—CMA—leverages such image pairs to learn condition-invariant features via contrastive learning.
In particular, CMA encourages features in the embedding space to be grouped according to their condition-invariant semantic content and not according to the condition under which respective inputs are captured. To obtain accurate cross-domain semantic correspondences, we warp the nor-mal image to the viewpoint of the adverse image and lever-age warp-confidence scores to create robust, aggregated features. With this approach, we achieve state-of-the-art se-mantic segmentation performance for model adaptation on several normal-to-adverse adaptation benchmarks, such as
ACDC and Dark Zurich. We also evaluate CMA on a newly procured adverse-condition generalization benchmark and report favorable results compared to standard unsupervised domain adaptation methods, despite the comparative hand-icap of CMA due to source data inaccessibility. Code is available at https://github.com/brdav/cma. 1.

Introduction
Adverse visual conditions, such as fog, heavy rain, or snowfall, represent a challenge for autonomous systems ex-pected to navigate “in the wild”. To achieve full auton-omy, systems require perception algorithms that perform robustly in every condition. However, due to their in-Figure 1. CMA exploits image-level correspondences to learn condition-invariant features. Two images of the same location (but captured under different visual conditions) are encoded, and the normal-condition image features are warped to get spatially aligned with the adverse-condition image features. Our contrastive loss then creates an embedding space where patches of adverse features (black) are closer to their corresponding normal patches (green) than to other adverse patches (red). frequent occurrence, inclement weather conditions are of-ten underrepresented in common, finely annotated outdoor datasets (e.g., BDD100K [44] or Mapillary Vistas [28]).
As a result, state-of-the-art recognition methods are bi-ased towards “normal” visual conditions (i.e., daytime and clear weather), which causes them to fail for edge cases.
Furthermore—in particular for detailed, pixel-level tasks like semantic segmentation—high-quality annotations for adverse-condition images are difficult and expensive to ob-tain. In fact, they require specialized annotation protocols due to ambiguities arising from aleatoric uncertainty [33].
To bypass these issues, researchers have investigated unsu-pervised domain adaptation (UDA) from normal to adverse conditions as an alternative to full supervision [6, 11, 14, 30, 41], where a model is jointly trained on labeled source-domain data and unlabeled target-domain data.
This paper instead targets the more general problem of source-free domain adaptation—also known as model adaptation—for semantic segmentation. In model adapta-tion, only (i) the model pre-trained on source images and (ii) unlabeled target images are available. This pertains to many real-world use cases when the labeled source data is proprietary or inaccessible due to privacy concerns. The complete absence of fine ground-truth annotations repre-sents a significant challenge, as the model can easily drift and unlearn important concepts during adaptation. To bol-ster the adaptation process, we leverage another form of weak supervision, which is far easier and cheaper to col-In particular, lect than pixel-wise semantic annotations. multiple recent driving datasets—such as RobotCar [26],
ACDC [33] and Boreas [3]—traverse the same route sev-eral times under varying weather conditions, and provide
GNSS-matched frames. Each adverse-condition target im-age can thus be paired with a corresponding reference im-age depicting roughly the same scene under normal condi-tions. While also unlabeled, the reference images bridge the domain gap between the source and target domain by overlapping both with the source domain in terms of visual condition and with the target domain in terms of geography and sensor characteristics.
Our proposed method, named Contrastive Model Adap-tation (CMA), leverages the reference predictions through a unified embedding space. Assuming the reference and target images are sufficiently aligned, co-located features should be similar between the two—neglecting dynamic ob-jects and slight shifts in static content (e.g., missing leaves on a tree). Accordingly, we posit that for a given target fea-ture, its reference feature at the same spatial location should be closer in the embedding space than most other target fea-tures. An embedding space fulfilling this assumption would effectively eliminate condition-specific information, but si-multaneously preserve semantic content. We aim to cre-ate such an embedding space through contrastive learning, where dense spatial embeddings of the target image serve as anchors (black patch in Fig. 1). Each anchor is pulled to-wards a single positive, i.e., the embedding of the reference image corresponding to the same location (green patch in
Fig. 1). Since the pre-trained source model is expected to produce semantic features of higher quality on the reference images than on the target images (for a qualitative compar-ison see Sec. E of the suppl. material), this clustering step helps to correct less reliable anchor semantics. Conversely, the anchor is pushed apart from the negatives, which are simply target embeddings at other spatial locations (or from other target images, red patches in Fig. 1), to counteract mode collapse. Through spatial alignment of the reference and target images and custom, confidence-modulated fea-ture aggregation, we create robust embeddings for optimiza-tion with our cross-domain contrastive loss.
CMA yields state-of-the-art results for model adaptation on several normal-to-adverse semantic segmentation bench-marks. It even outperforms recent standard UDA methods on these benchmarks, despite its data handicap compared to the latter methods. Attesting to our successful cross-domain embedding alignment, CMA delivers exceptionally robust results, as shown by evaluations on the newly compiled
Adverse-Condition Generalization (ACG) benchmark. 2.