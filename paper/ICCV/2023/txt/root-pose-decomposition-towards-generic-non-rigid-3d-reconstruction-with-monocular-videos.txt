Abstract
This work focuses on the 3D reconstruction of non-rigid objects based on monocular RGB video sequences. Con-cretely, we aim at building high-fidelity models for generic object categories and casually captured scenes. To this end, we do not assume known root poses of objects, and do not utilize category-specific templates or dense pose priors. The key idea of our method, Root Pose Decomposition (RPD), is to maintain a per-frame root pose transformation, mean-while building a dense field with local transformations to rectify the root pose. The optimization of local transfor-mations is performed by point registration to the canonical space. We also adapt RPD to multi-object scenarios with object occlusions and individual differences. As a result,
RPD allows non-rigid 3D reconstruction for complicated scenarios containing objects with large deformations, com-plex motion patterns, occlusions, and scale diversities of different individuals. Such a pipeline potentially scales to diverse sets of objects in the wild. We experimentally show that RPD surpasses state-of-the-art methods on the chal-lenging DAVIS, OVIS, and AMA datasets. We provide video results in https://rpd-share.github.io. 1.

Introduction
The reconstruction of non-rigid (or deformable) 3D ob-jects with monocular RGB videos is a long-standing and challenging task in computer vision and graphics [34]. It is needed for a variety of applications ranging from XR to robotics. Traditionally, a typical prior pipeline lever-ages template-based models such as human skeleton models
SMPL [19], SMPL-X [25], GHUM(L) [42], and reconstruct models for specific categories like the human body or hu-man face. These methods do not scale to diverse categories. (cid:66) Corresponding author: Fuchun Sun.
With the success of Neural Radiance Field (NeRF) [21], several representative works [5, 23] unify frames to a canon-ical space and do not rely on pre-defined skeleton mod-els. Whilst a variety of methods have been proposed to improve the reconstruction fidelity, the performance de-generates when encountering large object deformations or movements. Meanwhile, they are not suitable for casual videos when background Structure from Motion (SfM) does not provide root poses for the object. As a typical work to address the issues, BANMo [41] initializes approximate camera poses by leveraging continuous surface embeddings (CSE) [22] or pre-trained DensePose models [12]. Never-theless, CSE is acquired by annotations and only applies to specific categories in the training set, e.g., quadruped an-imals. DensePose is learned with the aid of manual UV fields from SMPL and is thus also highly category-limited.
Such a pipeline that relies on off-the-shelf pose or surface models of a certain categories does not generalize to recon-struct generic object categories.
As a result, it is an open problem when considering the non-rigid construction in the wild that might contain com-plicated factors, e.g., multiple categories, complex motion patterns, individual diversities, or object occlusions. Based on casually captured monocular videos, our effort is de-voted to building articulated models for generic categories, without explicitly incorporating priors that might limit the generalization of categories. Towards this goal, we pro-pose Root Pose Decomposition (RPD), a method for non-rigid 3D reconstruction based on monocular RGB videos.
RPD does not rely on known camera poses or poses com-pensated by background-SfM, category-specific skeletons, or pre-trained dense pose models (e.g., DensePose, CSE), while could achieve articulated reconstruction for objects with rapid object deformations, complex motion patterns, and large pose changes.
Concretely, RPD follows the common approach for non-rigid reconstruction that builds a canonical space for dif-Method
Dependency (âœ— is preferred)
Known camera poses