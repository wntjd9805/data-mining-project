Abstract
Text-to-image generative models often reflect the biases of the training data, leading to unequal representations of underrepresented groups. This study investigates inclusive text-to-image generative models that generate images based on human-written prompts and ensure the resulting images are uniformly distributed across attributes of interest. Un-fortunately, directly expressing the desired attributes in the prompt often leads to sub-optimal results due to linguistic ambiguity or model misrepresentation. Hence, this paper proposes a drastically different approach that adheres to the maxim that “a picture is worth a thousand words”. We show that, for some attributes, images can represent concepts more expressively than text. For instance, categories of skin tones are typically hard to specify by text but can be eas-ily represented by example images. Building upon these in-sights, we propose a novel approach, ITI-GEN1, that lever-ages readily available reference images for Inclusive Text-to-Image GENeration. The key idea is learning a set of prompt embeddings to generate images that can effectively represent all desired attribute categories. More importantly,
ITI-GEN requires no model fine-tuning, making it computa-tionally efficient to augment existing text-to-image models.
Extensive experiments demonstrate that ITI-GEN largely improves over state-of-the-art models to generate inclusive images from a prompt. 1.

Introduction
In recent years we have witnessed a remarkable leap in text-based visual content creation, driven by breakthroughs in generative modeling [69, 27, 59, 58, 63] and the access to large-scale multimodal datasets [67, 35]. Particularly, pub-licly released models, such as Stable Diffusion [63], have matured to the point where they can produce highly realis-tic images based on human-written prompts.
However, one major drawback of existing text-to-image models is that they inherit biases from the training data [6, 1Project page: https://czhang0528.github.io/iti-gen
Figure 1. (a) Given a human-written prompt (“a headshot of a per-son”), existing text-to-image models [63] can hardly synthesize pictures representing minority groups (i.e., people with eyeglasses (b) Conventional hard prompt searching [18] in this example). is sub-optimal due to linguistic ambiguity. (c) We address these problems by leveraging a small set of reference images for inclu-sive text-to-image generation (ITI-GEN). 58, 63, 12, 5] and thus have yet to exhibit inclusiveness — the generated images based on the input text may reflect stereotypes, leading to the exclusion of certain attributes or minority groups. For instance, given the prompt “a headshot of a person”, Figure 1(a) shows how a state-of-the-art sys-tem generates about 92% images of subjects without eye-glasses, and only 8% with eyeglasses, showing a clear bias towards people without eyeglasses. Alternatively, as shown in Figure 1(b), one could specify the attribute in the prompt, resulting in better outcomes; however, this will still result in a sub-optimal solution due to linguistic ambiguity. While inclusiveness has been critical to responsible AI, existing text-to-image models are still lagging [12, 5, 55, 53, 46]. In this work, we propose a new method that achieves inclusive-ness2 in text-to-image generation using only a few example images, as illustrated in Figure 1(c).
To advance inclusive generation, a straightforward way is to retrain or fine-tune the model upon request, using truly inclusive training data [17, 82]. Doing so, however, is in-surmountably challenging as collecting large-scale train-ing data that is balanced/inclusive across all attributes of interest is impractical, and training generative models is highly compute-intensive [67, 65, 17]. Another principled approach towards inclusiveness is to specify or enumerate each category in natural language (i.e., hard prompt search-ing) [18, 55]. However, many categories are difficult to specify with natural language (e.g., skin tone) or cannot be well synthesized by the existing models due to linguistic ambiguity or model misrepresentation [29].
At first glance, these seem to paint a grim picture for in-clusive text-to-image generation. However, we argue that instead of specifying attributes explicitly using descriptive natural language, images can represent specific concepts or attributes more efficiently. Observing the availability of a shared vision-language embedding in many multimodal generative models [56], we raise the question: can we learn inclusive prompt embeddings using images as guidance?
To achieve this goal, we introduce ITI-GEN, a novel and practical framework that creates discriminative prompts based on readily available reference images for Inclusive
Text-to-Image GENeration. Concretely, we leverage the vision-language pre-trained CLIP model [56] to obtain the embeddings of the reference images and learnable prompts.
In the joint embedding space, we design a new training ob-jective to align the directions of the image and prompt fea-tures. The core idea is to translate the visual attribute dif-ferences into natural language differences such that the gen-erated images based on the learned prompts can effectively represent all desired categories. By equalizing the sampling process over the learned prompts, our method guarantees inclusiveness for text-to-image generation.
We validate our framework with Stable Diffusion [63].
ITI-GEN can leverage reference images from different do-mains, including human faces [43, 34, 20] and scenes [68], to achieve inclusive generation in single or multiple at-tributes of interest. ITI-GEN needs neither prompt speci-fication nor model fine-tuning, bypassing the problems of linguistic ambiguity as well as computational complexity.
Moreover, ITI-GEN is compatible with the existing text-based image generation models (e.g., ControlNet [81] and instruction-based image editing models [7]) in a plug-and-play manner. To the best of our knowledge, this is the first method that allows inclusive text-to-image generation over a frozen model and obtains competitive results throughout. 2Few works [12, 5] have studied fairness issues in text-to-image genera-tion but mainly focused on social biases (e.g., perceived gender, ethnicity).
This paper incorporates a broader spectrum of attributes. 2.