Abstract
Fully immersive and interactive audio-visual scenes are dynamic such that the listeners and the sound emitters move and interact with each other. Reconstruction of an immer-sive sound experience, as it happens in the scene, requires detailed reconstruction of the audio perceived by the lis-tener at an arbitrary location. The audio at the listener lo-cation is a complex outcome of sound propagation through the scene geometry and interacting with surfaces and also the locations of the emitters and the sounds they emit. Due to these aspects, detailed audio reconstruction requires ex-tensive sampling of audio at any potential listener location.
This is usually difficult to implement in realistic real-time dynamic scenes. In this work, we propose to circumvent the need for extensive sensors by leveraging audio and visual samples from only a handful of A/V receivers placed in the scene. In particular, we introduce a novel method and end-to-end integrated rendering pipeline which allows the lis-tener to be everywhere and hear everything (BEE) in a dy-namic scene in real-time. BEE reconstructs the audio with two main modules, Joint Audio-Visual Representation, and
Integrated Rendering Head. The first module extracts the
*Corresponding author: shlizee@uw.edu informative audio-visual features of the scene from sparse
A/V reference samples, while the second module integrates the audio samples with learned time-frequency transforma-tions to obtain the target sound. Our experiments indicate that BEE outperforms existing methods by a large margin in terms of quality of sound reconstruction, can generalize to scenes not seen in training and runs in real-time speed. 1.

Introduction
It is Friday night, and your favorite jazz band is perform-ing at the Birdland Jazz Club in New York. Your friends will be attending but you cannot attend in person. Imagine that instead, it would be possible to join them virtually, as in the scenario illustrated in Figure 1. Enabling such an im-mersive experience requires high-fidelity real-time spatial audio reconstruction of the scene and could unlock novel experiences in applications of virtual reality, mixed reality, and immersive live-streaming.
While the dynamic aspects of such scenes are the ones that make them immersive, these same aspects make au-In particular, dio reconstruction a challenging problem. for these scenes, sound reconstruction is an outcome of (i) scene properties related to sound propagation, such as 1
room geometry, surface materials, etc., and (ii) actions of the emitters, such as their positions and emitted sounds at every time step. Due to these, a possible direct approach for audio reconstruction at arbitrary listener locations could be to place a microphone at each such location, e.g., designing a dense mesh of microphones. Such a solution is usually impractical in realistic scenes. An alternative approach to deal with multiple moving emitters at each time could be to track the moving emitters and to design high-end equip-ment to collect clean emitter sounds of each emitter so that they can be integrated and synthesized for each possible lo-cation of the listener. With known emitter locations, this approach can utilize common audio reconstruction tech-niques to render the Room Impulse Response (RIR) for each emitter-listener pair, then perform convolution of the emit-ter sound with the corresponding RIR, and integrate the out-comes to obtain the reconstructed audio at arbitrary listener location [13, 27, 12, 21, 15, 29, 4, 22, 23, 24, 16, 17]. While this approach is plausible, beyond the requirement to design novel equipment, it also implies an extensive computational cost of the integration, which is expected to increase with the number of emitters.
Due to the above described challenges, both approaches are generally impractical and warrant the development of methods that synthesize the audio for an arbitrary listener location from a sparse set of fixed sensors. Indeed, recently developed neural sound synthesis methods have been shown to synthesize audio through generative neural network mod-eling conditioned on A/V samples from sensors for a single listener location [28, 25, 8, 9, 6, 10, 30]. While promising, they appear to depend on the training samples on which the network has been trained on, since scene properties that de-termine sound propagation are not explicitly captured dur-ing training. This restricts the reconstruction accuracy and the generalization to various dynamic scenes.
To address these limitations, in this work, we propose a novel neural sound reconstruction and synthesis system that leverages samples from a sparse set of fixed A/V sen-sors that sample the audio (waveforms captured by micro-phones) and in addition the visual information (egocen-tric images captured by cameras) at any given time. Our proposed end-to-end integrated audio rendering pipeline is capable to render high-quality audio and generalize the audio reconstruction to arbitrary listener locations, effec-tively allowing the listener to be everywhere and hear ev-erything (BEE). BEE contains two modules, namely, the
Joint Audio-Visual Representation module (JAVR) and In-tegrated Rendering Head (IRH). JAVR learns and repre-sents the properties of the scene by projecting visual sam-ples into a world coordinate system and obtains a 3D visual feature volume for the acoustic propagation space of the scene. Through this space the A/V receivers and the listen-ers are associated and then correlated by injecting audio fea-tures into the 3D visual representation and employing cross-attention. This constitutes the audio-visual representation of the scene. The target listener sound is synthesized by the
IRH module within BEE, which learns time-frequency (TF) transformations after integrating the audio-visual features generated from JAVR and the received audio samples on different levels through two decoupled branches.
In summary, our main contributions in this work are as follows: 1) We develop an end-to-end integrated ren-dering pipeline, named BEE, to address audio reconstruc-tion at arbitrary listener locations for dynamic scenes by sparse audio-visual samples. 2) BEE constructs an effective
Joint Audio-Visual Representation module that can learn an audio-visual representation of the scene. Such repre-sentation along with an Integrated Rendering Head module implicitly untangles the contribution of each emitter to the sound at an arbitrary listener location. 3) Experiments on the SoundSpaces dataset [7] with Replica and Matterport3D scenes demonstrate that BEE outperforms existing methods by a large margin in quality, ability to generalize to various scenes, and runs in real-time. 2.