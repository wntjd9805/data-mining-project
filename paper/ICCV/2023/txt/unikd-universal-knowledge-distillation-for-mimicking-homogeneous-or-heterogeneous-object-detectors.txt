Abstract
Knowledge distillation (KD) has become a standard method to boost the performance of lightweight object de-tectors. Most previous works are feature-based, where stu-dents mimic the features of homogeneous teacher detectors.
However, distilling the knowledge from the heterogeneous teacher fails in this manner due to the serious semantic gap, which greatly limits the flexibility of KD in practical appli-cations. Bridging this semantic gap now requires case-by-case algorithm design which is time-consuming and heavily relies on experienced adjustment. To alleviate this problem, we propose Universal Knowledge Distillation (UniKD), in-troducing additional decoder heads with deformable cross-attention called Adaptive Knowledge Extractor (AKE). In
UniKD, AKEs are first pretrained on the teacher’s output to infuse the teacher’s content and positional knowledge into a fixed-number set of knowledge embeddings. The fixed
AKEs are then attached to the student’s backbone to en-courage the student to absorb the teacher’s knowledge in these knowledge embeddings. In this query-based distilla-tion paradigm, detection-relevant information can be dy-namically aggregated into a knowledge embedding set and transferred between different detectors. When the teacher model is too large for online inference, its output can be stored on disk in advance to save the computation overhead, which is more storage efficient than feature-based meth-ods. Extensive experiments demonstrate that our UniKD can plug and play in any homogeneous or heterogeneous teacher-student pairs and significantly outperforms conven-tional feature-based KD. 1.

Introduction
Object detection is a fundamental computer vision task that has been widely applied to many practical applications.
*Work done during internship at SenseTime Research.
†Corresponding authors.
Figure 1: The feature density of different models. First row: four teachers. Second row: students distilled with FitNet.
Third row: students distilled with UniKD.
In recent years, various frameworks for object detection have been proposed to improve tdetection performance such as Faster R-CNN [19], RetinaNet [13], FCOS [21], and De-formable DETR [37]. In practical applications, different de-tectors often favor different devices due to constraints on parameter number, inference latency, and even the detector framework. For example, the high-performance Faster R-CNN is not friendly to some edge devices due to the RoI pooling operator but is often deployed in many cloud de-vices. Therefore, boosting the accuracy of deployable mod-els in different detector frameworks as much as possible is the core problem in practical applications.
Knowledge Distillation (KD) [20, 6, 31] methods have significantly boosted the performance of lightweight stu-dent via learning from a high-capacity teacher model. This learning paradigm explored by researchers on various visual tasks including high-level visual understanding tasks [19, 13], as well as low-level visual tasks [25, 8]. Current detec-tion KD methods are mostly feature-based [20, 28, 29, 26].
For example, FGD [28] improves the RetinaNet-Res50 from 37.4 mAP to 39.6 mAP by mimicking the RetinaNet-Res101. However, these methods are not general enough because they only consider homogeneous pairs of teach-ers and students with the same framework. How to flex-ibly transfer the knowledge in heterogeneous teacher-student pairs, such as from RetineNet-Res101 to De-formable DETR-Res18, is a practical and challenging topic for object detection. Unfortunately, conventional feature-based distillation fails in directly distilling knowl-edge from the heterogeneous teacher due to a serious se-mantic gap. As shown in Fig. 1, activation maps from dif-ferent detection frameworks indicate significantly different semantics. We further show the feature maps of students after feature-based KD in the second row, which demon-strates that it’s hard for students to imitate the teachers’ out-put completely and this even impairs the students’ perfor-mance. To alleviate this dilemma, several works [18, 23] have tried to bridge this semantic gap by introducing assis-tants to guide the optimization of the student detector. How-ever, these assistants require a case-by-case algorithm de-sign to adapt to different teacher-student pairs, which heav-ily relies on experienced adjustment and greatly limits its flexibility.
Additionally, to obtain supervision from the teacher in each iteration, large amounts of computing resources and training time are consumed when training data pass through the giant teacher networks, which is inefficient and costly.
A straightforward solution, called offline KD, is to store the multi-scale teacher features of each training image in advance and then reuse them during distillation to avoid the storage-consuming online inference of teachers. How-ever, the storage cost of the current feature-based distillation method is unacceptable.
In this paper, we propose a query-based distillation paradigm called Universal Knowledge Distillation (UniKD) to flexibly transfer knowledge in any homogeneous or het-erogeneous teacher-student pairs. The advantages of such a query-based paradigm are threefold: (1) Given a high-capacity teacher model trained in any popular detection frameworks, we can directly boost the performance of lightweight detectors, whether they’re homogeneous or het-erogeneous. (2) UniKD is a general knowledge distillation paradigm with zero-cost algorithm adjustment in different practical applications without time-consuming case-by-case design. (3) In contrast to distilling the whole feature map, query-based UniKD extracts the teachers’ knowledge into a small number of knowledge embeddings, which requires significantly less storage than feature-based methods in of-fline KD and even performs better. See Tab. 4.
In UniKD, we introduce Adaptive Knowledge Extractor (AKE) modules, which are additional transformer decoder heads with deformable cross-attention. The AKE modules use content queries (qct) and positional queries (qpos) as probes to extract the detection-relevant knowledge from the network’s intermediate output for distillation. Specifically, this paradigm has two stages. In the first stage, given a high-capacity teacher model, qct and qpos are first generated to absorb the teacher’s knowledge via the AKE modules. AKE modules are pre-trained at this stage to be capable of ex-tracting detection-relevant knowledge. In the second stage, the AKEs with frozen qct, qpos and parameter weights are attached to the output multi-scale features of the student to imitate the teacher’s output extracted by the same AKEs.
This encourages the student to absorb the teacher’s knowl-edge in knowledge embeddings. The proposed AKE mod-ule is detector-agnostic and can be applied to arbitrary de-tectors for extracting detection-relevant knowledge, which can be transferred to any student detectors to boost their performance.
We conduct extensive experiments to demonstrate the generalization of our method, which achieves better or com-parable results on homogeneous or heterogeneous teacher-student pairs compared to existing works. To our best knowledge, we are the first to implement the knowledge transferring between traditional detectors and end-to-end
Deformable DETR. We boost the performance of RetinaNet by 2.0 mAP on MS-COCO 2017 dataset by mimicking De-formable DETR, and in turn, Deformable DETR can also obtain 2.0 mAP improvement by imitating the RetinaNet.
Even in homogeneous teacher-student pairs, the proposed
UniKD still outperforms the previous state-of-the-art meth-ods and establishes new advanced results. In summary, the contributions of this paper are as follows:
• We propose the query-based Universal Knowledge
Distillation, which is a new knowledge distillation paradigm for transferring information in homogeneous or heterogeneous teacher-student pairs.
• We introduce AKE modules with content queries and positional queries to extract detection-relevant knowl-edge. It requires zero-cost algorithm adjustment when applied to different detectors and has high storage effi-ciency for offline KD.
• We conduct extensive experiments on various teacher-student pairs and model architectures to verify the ef-fectiveness and universality of UniKD. Especially, to our best knowledge, we are the first to effectively trans-fer the detection-relevant knowledge between conven-tional detectors and end-to-end Deformable DETR. 2.