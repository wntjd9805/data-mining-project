Abstract
Guided depth completion aims to recover dense depth maps by propagating depth information from the given pix-els to the remaining ones under the guidance of RGB im-ages. However, most of the existing methods achieve this using a large number of iterative refinements or stacking repetitive blocks. Due to the limited receptive field of con-ventional convolution, the generalizability with respect to different sparsity levels of input depth maps is impeded.
To tackle these problems, we propose a feature point cloud aggregation framework to directly propagate 3D depth in-formation between the given points and the missing ones.
We extract 2D feature map from images and transform the sparse depth map to point cloud to extract sparse 3D fea-tures. By regarding the extracted features as two sets of fea-ture point clouds, the depth information for a target location can be reconstructed by aggregating adjacent sparse 3D features from the known points using cross attention. Based on this, we design a neural network, called as PointDC, to complete the entire depth information reconstruction pro-cess. Experimental results show that, our PointDC achieves superior or competitive results on the KITTI benchmark and
NYUv2 dataset. In addition, the proposed PointDC demon-strates its higher generalizability to different sparsity levels of the input depth maps and cross-dataset evaluation. 1.

Introduction
In recent years, dense depth maps have shown great im-portance in various computer vision tasks, including au-tonomous driving [39, 7], 3D object detection [48, 47], aug-mented reality[22, 9, 55] and 3D reconstruction [11, 35].
*Corresponding author. (a) A diagram of our feature point cloud aggregation module. (b) An example scene for comparison.
Figure 1. The diagram of feature point cloud aggregation module of our PointDC. It reconstructs 3D information for the 2D feature map by aggregating from the 3D features, where the 2D and 3D features are viewed as two sets of feature point clouds. Compared with the state-of-the-art depth completion approach NLSPN [31], our PointDC can still achieve better results although the details in the RGB image are hard to discriminate by human eyes.
However, commercially available depth sensors, such as Li-DARs or RGB-D cameras, typically produce highly sparse depth maps that cannot accurately capture the full 3D infor-mation of the scene. To address this limitation, recent re-searches [20, 42, 10] have focused on directly reconstruct-ing dense depth maps from sparse observations. Despite significant progress, this approach remains challenging due to the ill-posed nature of the problem, which often leads to unsatisfactory accuracy. In comparison, a more promising solution is to incorporate an additional RGB image captured
in the same scene. Based on the auxiliary structural infor-mation, it is much easier to complete the sparse depth map.
This approach, known as guided depth completion, has be-come one of the important steps for the aforementioned vi-sion applications.
Given a sparse depth image, guided depth completion es-sentially aims to propagate depth information from known pixels to the remaining ones under the guidance of an RGB image [5, 31]. Generally, it can be classified into two categories. The first one [5, 6, 31, 27, 16, 54, 41, 52] treats sparse depth maps as ordinary images and formu-lates guided depth completion as a guided image restoration task, where depth values are regarded as pixel intensities.
In this case, the information is propagated by learning vari-ous types of affinities among neighboring pixels from RGB images [5, 6, 31, 27] or constructing content-adaptive neu-ral networks [54, 41, 52]. However, these methods are pri-marily designed to operate in 2D space and therefore strug-gle to fully exploit the 3D geometry information that has been demonstrated to be beneficial for depth estimation in both multi-view stereo (MVS) and stereo matching meth-ods [14, 13]. To explicitly consider 3D geometry informa-tion, the second category of methods [23, 21, 4, 17] extracts 3D features using point cloud convolutions [46, 1] or by in-terpreting depth information with plane-residual representa-tion [23, 21]. This category of methods propagate informa-tion by employing either 2D or 3D convolution. In a word, both types of methods propagate depth information in a pro-gressive or an iterative manner due to limited receptive field.
Consequently, they may be less robust in cases where there are varying levels of point sparsity, as it becomes increas-ingly difficult to propagate information between distant pix-els when the densities of sparse points decrease.
In this work, we propose a feature point cloud aggrega-tion framework to directly propagate the given sparse depth information to the entire image. In this way, our framework can overcome the limited receptive field of conventional convolutions and generalize well to different sparsity levels of the input depth maps. Given the inputs, we transform the depth map to point cloud using the camera intrinsic matrix.
Then we extract sparse 2D features from the images and 3D features from the point cloud. We hypothesize that the 2D features only give visual descriptions of the scene while the 3D features contain the extra 3D information. Gener-ally, similar visual contents tend to have similar depth val-ues within neighboring regions. Therefore, the 3D depth information of a target location can be reconstructed from the adjacent sparse 3D features using cross-attention strat-egy. By referring the 2D and sparse 3D features as the 2D and the sparse 3D feature point clouds, the reconstruction process can be achieved in a cross-attention manner with higher flexibility.
Based on the above analysis, we design a neural network, called as PointDC, to handle the depth completion task.
First, PointDC generates the 2D and 3D feature point clouds with a UNet [36] and several stacked local self-attention transformer blocks, respectively. Then, for a target loca-tion, its 3D depth information is reconstructed based on its neighboring 3D feature points by the feature point cloud aggregation module which is mainly a local cross-attention transformer block. A diagram of this module is shown in
Fig. 1(a). Finally, from the reconstructed dense 3D feature point cloud, PointDC generates the final dense depth map.
In summary, the main contributions of this work are as follows:
• We propose a feature point cloud aggregation frame-work which extracts both 2D and sparse 3D features for depth completion. It reconstructs the depth infor-mation for a target location by the adjacent sparse 3D feature points, in which each location can capture 3D information from the sparse 3D features directly re-gardless of the sparsity level of the input depth maps.
• We design a novel local transformer by regarding the extracted features as two sets of point clouds, which is used to exploit 3D geometry information and recon-struct the depth information for each target location.
• Experimental results show that our PointDC achieves better or comparable results compared to state-of-the-art depth completion methods.
In addition, our
PointDC demonstrates its higher generalizability to different sparsity levels of the input depth maps and cross-dataset evaluation. 2.