Abstract
This paper presents OxfordTVG-HIC (Humorous Im-age Captions), a large-scale dataset for humour genera-tion and understanding. Humour is an abstract, subjec-tive, and context-dependent cognitive construct involving several cognitive factors, making it a challenging task to generate and interpret. Hence, humour generation and un-derstanding can serve as a new task for evaluating the abil-ity of deep-learning methods to process abstract and sub-jective information. Due to the scarcity of data, humour-related generation tasks such as captioning remain under-explored. To address this gap, OxfordTVG-HIC offers ap-proximately 2.9M image-text pairs with humour scores to train a generalizable humour captioning model. Contrary to existing captioning datasets, OxfordTVG-HIC features a wide range of emotional and semantic diversity resulting in out-of-context examples that are particularly conducive to generating humour. Moreover, OxfordTVG-HIC is curated devoid of offensive content. We also show how OxfordTVG-HIC can be leveraged for evaluating the humour of a gen-erated text. Through explainability analysis of the trained models, we identify the visual and linguistic cues influen-tial for evoking humour prediction (and generation). We observe qualitatively that these cues are aligned with the benign violation theory of humour in cognitive psychology. 1.

Introduction
Humour has been recorded as a universal and high-level cognitive perception since Sumerians wrote down the first joke and remains a complicated concept due to its depen-dence on culture, visual and linguistic stimuli, as well as fundamental affective factors. Generating humorous con-tent poses a significant challenge. As the arousal-reduction mechanism [31] suggests, the optimal level of novelty for humour should be neither too low nor too high, so that most audience can comprehend and appreciate it. Achiev-ing this balance can be difficult. Another study from neu-*These authors contributed equally to this work. Correspondence to
Shuyang Sun (kevinsun@robots.ox.ac.uk). samples
Image-text from OxfordTVG-Figure 1:
HIC and COCO [9]. In OxfordTVG-HIC , the captions for a cat image do not describe the physical features of the cat, but rather the situations that could elicit the cat’s fa-cial expression. These situations create a humorous effect with the expression, as they are not offensive and violate the audience’s everyday-life expectations (Benign violation theory [32]). On the other hand, the captions for a similar cat image in COCO [9] explicitly describe the facts in the image. roscience [14] states: “successful jokes involve a cognitive juxtaposition of mental sets, followed by an affective feel-ing of amusement”. Linguistically, semantic and phono-logical violations of mental sets interact to generate hu-mour [32]. When multiple modalities such as vision and language are involved, the complexity of juxtaposition will significantly increase. Thus, Humour captioning, as a multi-modal humour generation task, can be a useful task to inves-tigate the upper limit of deep learning to handle high-level abstraction and creativity.
The definition of humour captioning is not clear and the task is under-explored. We argue that humour captioning should be distinguished from conventional image caption-ing [9, 45, 51], which mainly describes the objects or scenes in the image. The main goal of humour caption-ing should be to elicit a cognitive perception of humour
Dataset
#Images #Captions
OxfordTVG-HIC
COCO [9]
CC3M [45]
Flicker30k [51]
ArtEmis [2]
ArtELingo [35] 54k 123k 3334k 32k 80k 80k 2885k 617k 3334k 159k 455k 1224k
#Captions
/image
Dataset type 53.7 Humour 5 Object 1 Object 5 Object 5.7 Emotion 15.3 Emotion
Table 1: Image captioning dataset statistics compari-son. The average captions per image of OxfordTVG-HIC is significantly higher than other popular captioning datasets, and the total number of captions is larger than most of the datasets. rather than to provide factual information. This implies that a humorous sentence may not be relevant to the im-age content and still be a valid example. Hence, standard metrics [39, 12, 46, 25] that assess the quality of captions and conventional image captioning training framework may not be appropriate for humour captioning.
Humour captioning has received little attention. An initial attempt [41] used a relatively simple model [47] and a small-scale dataset and followed the conventional image captioning framework. Another recent image-text dataset [19] that aims to evaluate how well deep-learning models can comprehend humour has too few images (1.6k) with limited diversity to train a robust model.
To overcome the limitation faced by previous research, we introduce OxfordTVG-HIC (Humorous Image Cap-tions), a large-scale image-text dataset for humour caption-ing. To our knowledge, it is the first dataset of this kind that contains 2.9 million image-text pairs with a humour score to measure their funniness. In OxfordTVG-HIC, each im-age has 53.7 captions on average, while the conventional
[9, 45, 51]) have less than image caption datasets (e.g., 5 captions per image.
Based on OxfordTVG-HIC, we develop humour generators that can automatically produce humorous captions given any images. the humour genera-tors are trained on our proposed position-conditioned loss to address the issues of low diversity and humour caused by using cross-entropy loss on OxfordTVG-HIC. Specifically, since OxfordTVG-HIC is a dataset with high grammatical and semantic diversity, the cross-entropy loss tends to gen-erate captions that are close to all ground truths given an input image, which is detrimental for training. The reason is that a generated sentence that is close to a set of distinct sentences is also far from those sentences. Therefore, the humour level and diversity are reduced with cross-entropy loss. Our position-conditioned loss solves this problem with promising results.
The qualitative nature of human thoughts makes it hard to evaluate and understand humour quantitatively. How-Figure 2: OxfordTVG-HIC are much greater than the other traditional image captioning datasets [9, 45, 51] in terms of the mean and variance for the number of captions per image. ever, there are still some theories [32, 6, 37] proposed by psychologists to explain and measure humour. Drawing in-sights from one of the theories: Benign violation [32], we propose the benign violation humour score based on a hu-mour classifier trained on OxfordTVG-HIC and a benign level classifier. Together with other linguistic metrics such as diversity and fluency, we benchmark the performance of our model on OxfordTVG-HIC.
Moreover, the learned ability to perceive humour as digi-tal signals sheds light on the quantitative understanding and explainability of humour. From our observation and analy-sis, the model regards the abnormal and emotionally intense parts of images and sentences as the most important fac-tors for creating humour. We hypothesise our observation is consistent with the Benign violation theory [32].
In summary, our main contributions are as follows: 1. We introduce OxfordTVG-HIC, a large-scale humour-oriented image-text dataset that addresses the lack of data in image-text-based humour generation and detection. 2. We show the diversity and richness of OxfordTVG-HIC and design the position-conditioned loss to better train a humour caption generator on the diverse data of
OxfordTVG-HIC. 3. By explaining the learned humour models, we analyse the visual and linguistic stimuli that evoke humour and pro-vide insights regarding humorous cues within the data. We further observe that the insights are aligned with the Benign violation theory [32].
2.