Abstract
Federated learning (FL) is an emerging distributed ma-chine learning method that empowers in-situ model training on decentralized edge devices. However, multiple simul-taneous FL tasks could overload resource-constrained de-vices. In this work, we propose the first FL system to effec-tively coordinate and train multiple simultaneous FL tasks.
We first formalize the problem of training simultaneous FL tasks. Then, we present our new approach, MAS (Merge and Split), to optimize the performance of training multiple simultaneous FL tasks. MAS starts by merging FL tasks into an all-in-one FL task with a multi-task architecture. After training for a few rounds, MAS splits the all-in-one FL task into two or more FL tasks by using the affinities among tasks measured during the all-in-one training. It then continues training each split of FL tasks based on model parameters from the all-in-one training. Extensive experiments demon-strate that MAS outperforms other methods while reducing training time by 2× and reducing energy consumption by 40%. We hope this work will inspire the community to fur-ther study and optimize training simultaneous FL tasks. 1.

Introduction
Federated learning (FL) [34] has attracted considerable attention as it enables privacy-preserving distributed model training among decentralized devices.
It is empowering growing numbers of applications in both academia and in-dustry, such as medical imaging analysis [29, 41], Google
Keyboard [16], and autonomous vehicles [55, 39]. Among them, some applications contain multiple application tasks.
For example, autonomous vehicles are related to multiple resource-intensive computer vision (CV) tasks, including lane detection, object detection, and segmentation [20].
In fact, the majority of edge devices (e.g., NVIDIA Je-ston TX2 and AGX Xavier) can only support one FL task at a time [30]. Multiple simultaneous FL tasks on the same de-vice could overwhelm its memory, computation, and power
*most of the work done in S-Lab, Nanyang Technological University
Figure 1: Existing methods suffer from a trade-off between training time and test loss (lower test loss means better per-formance) when training 9 simultaneous FL tasks, whereas our method navigates a sweet point, achieving the best test loss with 2.1× training time reduction. One-by-one trains
FL tasks one after another; All-in-one combines tasks into a multi-task learning network before training it in FL. capacities. Thus, it is important to navigate solutions to well coordinate these simultaneous FL tasks.
A plethora of research on FL are mainly devoted to ad-dressing challenges such as statistical heterogeneity [28, 48], system heterogeneity [6, 51, 56], communication ef-ficiency [23, 61, 27, 50], and privacy issues [2, 19]. Most of the existing works only focus on one FL task, overlooking the fact that certain applications, such as self-driving cars or intelligent manufacturing robots, need to tackle multi-ple FL tasks simultaneously [20, 13]. To address this issue,
Bonawitz et.al [4] designed multi-tenancy to prevent simul-taneous FL tasks from overloading devices. However, their proposed method is slow in training because it regards these tasks as independent training tasks and trains them sequen-tially. This one-by-one training method only considers the differences among tasks, neglecting potential synergies.
Another intuitive solution is to adopt multi-task learning (MTL) to train multiple FL tasks by combining these tasks into an all-in-one neural network. This network has one encoder shared among tasks and multiple task-specific de-coders. It could prevent overloading devices and speeds up the training process as only one neural network is trained.
However, it could result in worse performance because not
all tasks are beneficial to the others when training together
[22, 60]. Simply combining FL tasks together only takes into account their synergies while overlooking their distinc-tions. Figure 1 shows that either the all-in-one (only consid-ers task synergies) or the one-by-one method (only consid-ers task differences) suffers from a trade-off between train-ing time and test loss.
In this work, we propose MAS (i.e. Merge and Split), the first FL system to effectively coordinate and train multiple simultaneous FL tasks under resource constraints by con-sidering both synergies and differences among these tasks.
We first formalize the problem of training multiple simul-taneous FL tasks. To address this problem, we introduce
MAS to optimize the performance. Specifically, MAS starts by merging these FL tasks into an all-in-one FL task with a multi-task architecture, which shares common layers and has specialized layers for each task. After training the all-in-one FL task for certain rounds, MAS splits this all-in-one task into two or more FL tasks based on their synergies and differences measured by affinity scores during training.
Lastly, MAS continues training each split of FL tasks with models trained in the all-in-one process.
Figure 1 shows that MAS achieves the best test loss with 2.1× training time reduction compared to the one-by-one method on training nine FL tasks. We also demonstrate that it reduces energy consumption by over 40% while achieving superior performance to other methods via extensive exper-iments on three different sets of FL tasks. We believe that
MAS is beneficial for many real-world applications such as autonomous vehicles and robotics. We summarize our con-tributions as follows:
• We formalize the problem of training multiple simul-taneous FL tasks. To the best of our knowledge, we are the first to conduct an in-depth investigation into the training of multiple simultaneous FL tasks.
• We propose MAS, a new FL system to effectively coor-dinate and train simultaneous FL tasks by considering both synergies and differences among these tasks.
• We establish baselines for training multiple simultane-ous FL tasks and demonstrate that MAS elevates per-formance with significantly less training time and en-ergy consumption via extensive empirical studies. 49, 66, 53, 58, 65, 45, 57, 11, 64], system heterogeneity
[6, 51, 31], communication efficiency [34, 25, 23, 61], and privacy concerns [2, 19]. Numerous methods are proposed to cluster FL clients into groups to address statistical het-erogeneity [14, 37, 63]. They aim to cluster models that are trained on clients with similar distribution, whereas our proposed MAS differs fundamentally from these methods as it splits simultaneous FL tasks into groups. Several other attempts have been made [42, 33] on federated multi-task learning in order to learn personalized models to tackle statistical heterogeneity. These personalized FL methods mainly focus on training one FL task of an application in a client. Training multiple simultaneous FL tasks is rarely ex-plored. The prior work [4] designs multi-tenancy in an FL system to schedule and train these tasks sequentially. This one-by-one method is slow in training and only considers the differences among these FL tasks.
Multi-task Learning is a popular machine learning ap-proach to learn models that can generalize on multiple tasks
[46, 59]. A plethora of studies investigate parameter shar-ing approaches that share common layers of a similar archi-tecture [5, 10, 3, 36]. Besides, many studies employ new techniques to address the negative transfer problem [22, 60] among tasks, including soft parameter sharing [9, 35], neu-ral architecture search [32, 18, 47, 15, 44], and dynamic loss reweighting strategies [24, 7, 52]. Instead of training all tasks together, task grouping trains only similar tasks to-gether. The early works of task grouping [22, 26] are not adaptable to DNN. Recently, several studies analyze task similarity [43] and task affinities [12] for task grouping. The state-of-the-art task grouping methods [43, 12], however, are unsuitable for training multiple simultaneous FL tasks because they mainly focus on inference efficiency. They would train a task multiple times as their task groups al-ways contain overlapped tasks. This motivates us to exploit task merging and task splitting to group and train multiple simultaneous FL tasks. 3. Method
In this section, we start by providing problem definition for training multiple simultaneous FL tasks. Then, we pro-pose Merge and Split (MAS) method that first merges tasks into an all-in-one FL task and then splits it into two or more splits for further training. 2.