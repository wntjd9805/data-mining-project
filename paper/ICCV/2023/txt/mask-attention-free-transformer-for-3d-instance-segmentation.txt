Abstract
Recently, transformer-based methods have dominated 3D instance segmentation, where mask attention is com-monly involved.
Specifically, object queries are guided by the initial instance masks in the first cross-attention, and then iteratively refine themselves in a similar man-ner. However, we observe that the mask-attention pipeline usually leads to slow convergence due to low-recall ini-tial instance masks. Therefore, we abandon the mask at-tention design and resort to an auxiliary center regres-sion task instead. Through center regression, we effectively overcome the low-recall issue and perform cross-attention by imposing positional prior. To reach this goal, we de-velop a series of position-aware designs. First, we learn a spatial distribution of 3D locations as the initial po-sition queries. They spread over the 3D space densely, and thus can easily capture the objects in a scene with a high recall. Moreover, we present relative position en-coding for the cross-attention and iterative refinement for more accurate position queries. Experiments show that our approach converges 4 faster than existing work, sets a new state of the art on ScanNetv2 3D instance segmen-tation benchmark, and also demonstrates superior perfor-mance across various datasets. Code and models are avail-able at https://github.com/dvlab-research/
Mask-Attention-Free-Transformer.
× 1.

Introduction
Nowadays 3D point clouds can be conveniently col-lected. They have benefited various applications, such as autonomous driving, robotics, and augmented reality. As a fundamental task, 3D instance segmentation also poses great challenges simultaneously, such as geometric occlu-sion and semantic ambiguity.
Many works have been proposed to solve the 3D instance segmentation task. Grouping-based methods [25, 55, 5, 75] rely on heuristic clustering algorithms such as DBSCAN or Breadth-First Search (BFS) to generate instance pro-posals. They thus require sophisticated hyper-parameters
*Corresponding Author
Figure 1. Validation curve of the baseline and ours on ScanNetv2 val set. With only 128-epoch training, ours outperforms the base-line trained with 512 epochs. tuning and are prone to wrongly segment instances that are close to each other. Recently, transformer-based meth-ods [49, 50] develop a fully end-to-end pipeline. With trans-former decoder layers, a fixed number of object queries attend to global features iteratively and directly output in-stance predictions.
It requires no post-processing for du-plicate removal such as NMS, since it adopts one-to-one bipartite matching during training. Moreover, it employs mask attention, which uses the instance masks predicted in the last layer to guide the cross-attention.
However, we point out that current transformer-based methods suffer from the issue of slow convergence. As shown in Fig. 1, the baseline model manifests slow conver-gence and lags behind our method by a large margin, par-ticularly in the early stages of training. We dive further and find that the issue is potentially caused by the low recall of the initial instance masks. Specifically, as shown in Fig. 2 (a), the initial instance masks are produced by the similarity map between the initial object queries and the point-wise mask features. Since the initial object queries are unstable in early training, we notice that the recall of initial instance masks is substantially lower than ours in Fig. 3, especially at the beginning of training (i.e., the 32-th epoch). The low-quality initial instance masks increase the training difficulty,
Figure 2. The framework of (a) existing works (based on mask at-tention) and (b) ours. Existing works have the issue of low-recall initial instance masks (i.e., M0). Our approach resorts to an aux-iliary center regression task to circumvent this issue. thereby slowing down convergence.
Given the low recall of the initial instance masks, we abandon the mask attention design and instead construct an auxiliary center regression task to guide cross-attention, as depicted in Fig. 2 (b). To enable center regression, we de-velop a series of position-aware designs. Firstly, we main-tain a set of learnable position queries, each of which de-notes the position of its corresponding content query. They are densely distributed over the 3D space, and we require each query to attend to its local region. As a result, the queries can easily capture the objects in a scene with a higher recall, which is crucial in reducing training difficulty and accelerating convergence.
In addition, we design the contextual relative position en-coding for cross-attention. Compared to the mask attention used in previous works, our solution is more flexible since the attention weights are adjusted by relative positions in-stead of hard masking. Furthermore, we iteratively update the position queries to achieve more accurate representa-tion. Finally, we introduce the center distances between predictions and ground truths in both matching and loss.
In total, our contribution is three-fold.
• We observe that existing transformer-based methods suffer from the low recall of initial instance masks, which causes training difficulty and slow convergence.
• Instead of relying on mask attention, we construct an auxiliary center regression task to overcome the low-recall issue and design a series of position-aware com-ponents accordingly. Our approach manifests faster convergence and demonstrates higher performance.
• Experiments show our approach achieves a new state-of-the-art result and demonstrates superior perfor-mance on various datasets including ScanNetv2, Scan-Net200, and S3DIS. 2.