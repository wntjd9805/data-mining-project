Abstract
Computer vision datasets frequently contain spurious correlations between task-relevant labels and (easy to learn) latent task-irrelevant attributes (e.g. context). Mod-els trained on such datasets learn “shortcuts” and under-perform on bias-conflicting slices of data where the corre-lation does not hold. In this work, we study the problem of identifying such slices to inform downstream bias mit-igation strategies. We propose First Amplify Correlations and Then Slice (FACTS), wherein we first amplify corre-lations to fit a simple bias-aligned hypothesis via strongly regularized empirical risk minimization. Next, we perform correlation-aware slicing via mixture modeling in bias-aligned feature space to discover underperforming data slices that capture distinct correlations. Despite its simplic-ity, our method considerably improves over prior work (by as much as 35% precision@10) in correlation bias identifi-cation across a range of diverse evaluation settings. Code: https://github.com/yvsriram/FACTS. 1.

Introduction
Real-world datasets frequently exhibit correlation bi-ases, wherein a task-irrelevant attribute (say, the image background) is correlated with the task label of inter-est [1, 2, 3]. Consider the task of distinguishing images of chickens from airplanes (see Fig. 1). Naturally, most images of airplanes are in the sky, whereas chickens are typically found on the ground. A naive classifier trained on this biased dataset may conceivably learn to over-rely on the (relatively easier to learn) background and consequently un-derperform on bias-conflicting slices of the data (e.g. chick-ens in the air). Indeed, deep models trained with standard empirical risk minimization [4] are notorious for exploiting such “shortcuts” [5, 6], which may have serious repercus-sions in high-stakes applications like medicine [7, 8], face recognition [3], and autonomous driving [9].
In this work, given a potentially biased dataset, we at-tempt to automatically identify such bias-conflicting slices: dataset subsets wherein the spurious correlation does not hold. Such identification could inform downstream mitiga-Figure 1: Consider the task of classifying images of chick-ens and airplanes. Real-world datasets typically also con-tain spurious correlations between (easy to learn) task-irrelevant attributes and labels: e.g. most airplane images are airborne, whereas chickens are usually photographed on the ground. Naively training on such a dataset would lead to overfitting to the majority context within each class, and to underperforming on bias-conflicting slices of data (e.g. chickens in the sky). We study the problem of automati-cally identifying such bias-conflicting slices (dashed ovals), which can then inform downstream mitigation strategies.
Our method, FACTS, first amplifies correlations to learn a context-aligned decision boundary, and then clusters sam-ples that are confidently misclassified by this biased model to uncover bias-conflicting slices. tion strategies based on reweighting [10, 11, 12, 13, 14] or annotating [15, 16, 17] more instances of underrepresented populations. Importantly, such a method should be able to discover slices that represent semantically coherent and dis-tinct bias-conflicting subpopulations, that may optionally be
“named” (say, using an image captioning model [18]), and presented to a practitioner.
Why is discovering bias-conflicting slices hard? Presum-ably, one could annotate and control for potentially spuri-ous attributes [19, 20, 21], but this is challenging to scale to larger datasets. Further, tasks labels are sometimes spu-riously correlated with latent [6] attributes which may be unknown apriori [22], or not correspond to a clean, “human-interpretable” concept [23] to begin with!
Some recent works have focused on fully automated so-lutions to this problem by posing it as a problem of dis-covering systematic error modes on a held out validation set (via “error-aware” mixture modeling [24] or distilling model failures as directions in latent space [15]), or by us-ing external pretrained models such as CLIP [25] for im-age captioning [26]. However, we find that these methods can either only diagnose a single bias-conflicting slice per-class [15], fail to generalize to settings with severe correla-tion bias [24], or do not control for the bias in the external pretrained model [25, 26] itself. Further, by focusing exclu-sively on failure modes, such methods may miss underrep-resented subpopulations which an overparameterized model may have memorized but still does not understand [27].
To address these limitations, we propose a simple algo-rithm we call First Amplify Correlations and Then Slice (FACTS). Our method first amplifies the model’s reliance on the underlying spurious correlation by training with heavily regularized empirical risk minimization. By do-ing so, we force the model to fit a simple, bias-aligned hy-pothesis that maximally separates bias-conflicting and bias-aligned samples within each class, making them easier to segregate. Next, we propose a novel slicing strategy, that fits per-class mixture models in bias-amplified feature space (to ensure correlation-aware clustering) with an additional coherence prior (to ensure semantic coherence). By mak-ing limited assumptions, our method is able to generalize to challenging but practical evaluation settings not previously considered in the literature [24, 15]: containing multiple mi-nority groups per-class, or containing a class without a mi-nority group. We make the following contributions:
• We study the problem of automatically discovering co-herent and distinct slices of a dataset containing corre-lation bias where the correlation does not hold.
• We propose FACTS, a novel two-stage algorithm that clusters in correlation-amplified feature space to un-cover bias-conflicting slices, without assuming access to additional annotations.
• We report results for slice discovery on a range of diverse evaluation settings constructed from the Wa-terBirds [11], CelebA [28], and the newly introduced
NICO++ [29] datasets, and demonstrate strong gains over prior work (with absolute gains of as much as 35% precision points across datasets!). 2.