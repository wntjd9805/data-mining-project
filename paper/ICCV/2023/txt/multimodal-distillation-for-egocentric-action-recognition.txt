Abstract
The focal point of egocentric video understanding is modelling hand-object interactions. Standard models, e.g.
CNNs or Vision Transformers, which receive RGB frames as input perform well, however, their performance improves further by employing additional input modalities (e.g. ob-ject detections, optical ﬂow, audio, etc.) which provide cues complementary to the RGB modality. The added com-plexity of the modality-speciﬁc modules, on the other hand, makes these models impractical for deployment. The goal of this work is to retain the performance of such a multi-modal approach, while using only the RGB frames as input at inference time. We demonstrate that for egocentric ac-tion recognition on the Epic-Kitchens and the Something-Something datasets, students which are taught by multi-modal teachers tend to be more accurate and better cal-ibrated than architecturally equivalent models trained on ground truth labels in a unimodal or multimodal fashion.
We further adopt a principled multimodal knowledge dis-tillation framework, allowing us to deal with issues which occur when applying multimodal knowledge distillation in a na¨ıve manner. Lastly, we demonstrate the achieved re-duction in computational complexity, and show that our ap-proach maintains higher performance with the reduction of the number of input views. We release our code at: https://github.com/gorjanradevski/multimodal-distillation 1.

Introduction
The purpose of egocentric vision is enabling machines to interpret real-world data taken from a human’s perspec-tive. Its applications are numerous, ranging from recogniz-ing [63] or anticipating [15] actions, to more complex tasks such as recognizing egocentric object-state changes, local-izing action instances of a particular video moment [22], etc. The focal point of egocentric vision is hand-object interactions. Usually, these hand-object interactions take
* Authors contributed equally.
Multimodal Action
Recognition Model
Multimodal Fusion
Module
Student Taught by a
Multimodal Teacher
Video Swin-Transformer
Optical Flow Swin-Transformer
Audio Swin-Transformer
Spatial-Temporal Layout
Transformer (STLT)
Video Swin-Transformer
...
...
Figure 1: Motivation: The multimodal action recognition model is powerful, but too slow to be used in practice. The distilled stu-dent is signiﬁcantly faster yet achieves competitive performance. place in cluttered environments, where the object of interest is often occluded, or occurs only during a short time period.
Furthermore, egocentric vision often suffers from motion blur – due to the movement of the scene objects or the cam-era itself – and thus, understanding video content from RGB frames alone may be challenging.
To cope with these challenges, various egocentric action recognition methods [25, 29, 33, 43, 56, 59, 61] demonstrate that explicitly modelling hand-object interactions (usually represented via bounding boxes & object categories) signif-icantly improves the action recognition performance, most notably in a compositional generalization setup [43]. Simi-larly, other works show that leveraging multiple modalities (optical ﬂow, audio, etc.) at inference time yields improved performance [16, 28, 36, 57]. The assumptions these meth-ods make are (i) that all modalities used during training are also available at inference time, and (ii) the compute budget at inference time would be sufﬁcient to obtain and process the additional modalities. Such assumptions make these methods cumbersome or even impossible to use in practice, e.g. on a limited compute budget such as in the case of em-bedded devices. Namely, using dedicated models for each additional modality (e.g. object detector, object tracker and a transformer when using bounding boxes & object cate-gories as input [43]), increases both the memory footprint as well as the inference time. Ideally, video understanding
models would leverage additional modalities during train-ing, while the resulting model would use only RGB frames at inference time, i.e. when deployed in practice.
One way to achieve the aforementioned goal is training
Omnivorous models, i.e. models trained jointly on multiple modalities, which have been shown to generalize better [20] than unimodal counterparts. In this work, we take a differ-ent route and transfer multimodal knowledge to models sub-sequently used in a unimodal setting. Namely, we distill the knowledge from a multimodal ensemble – exhibiting supe-rior performance, but unviable for deployment – to a stan-dard RGB-based action recognition model [5] (see Fig. 1).
Contributions. We employ state-of-the-art knowledge distillation practices [6] and 1 show that a student [31] taught by a multimodal teacher, is both more accurate and better calibrated than the same model trained from scratch or in an omnivorous fashion (§4.1); 2 We provide moti-vation and establish a simple but reliable multimodal dis-tillation approach, which overcomes the issue of poten-tially suboptimal modality-speciﬁc teachers (§4.2); 3 We demonstrate that the distilled student performs on par with signiﬁcantly larger models, and maintains performance in computationally cheaper inference setups (§4.3). 2.