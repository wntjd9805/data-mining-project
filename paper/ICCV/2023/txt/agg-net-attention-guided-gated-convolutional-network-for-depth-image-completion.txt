Abstract
Recently, stereo vision based on lightweight RGBD cam-eras has been widely used in various fields. However, lim-ited by the imaging principles, the commonly used RGB-D cameras based on TOF, structured light, or binocular vision acquire some invalid data inevitably, such as weak reflec-tion, boundary shadows, and artifacts, which may bring ad-verse impacts to the follow-up work. In this paper, we pro-pose a new model for depth image completion based on the
Attention Guided Gated-convolutional Network (AGG-Net), through which more accurate and reliable depth images can be obtained from the raw depth maps and the corresponding
RGB images. Our model employs a UNet-like architecture which consists of two parallel branches of depth and color features. In the encoding stage, an Attention Guided Gated-Convolution (AG-GConv) module is proposed to realize the fusion of depth and color features at different scales, which can effectively reduce the negative impacts of invalid depth data on the reconstruction. In the decoding stage, an Atten-tion Guided Skip Connection (AG-SC) module is presented to avoid introducing too many depth-irrelevant features to the reconstruction. The experimental results demonstrate that our method outperforms the state-of-the-art methods on the popular benchmarks NYU-Depth V2, DIML, and
SUN RGB-D. https://github.com/htx0601/AGG-Net 1.

Introduction
Depth sensing is critical in applications such as au-tonomous driving [4], robot navigation [22], and scene re-construction [1]. The commonly used depth sensors include
LiDAR, Time-of-Flight, or binocular camera. However, most of the acquired depth images will inevitably be ac-h t p e d w a
R s r u
O (a) NYU-Depth V2 (b) DIML (c) SUN RGB-D
Figure 1. Typical raw depth images with invalid data in the popular benchmark datasets (a) NYU-Depth V2, (b) DIML, and (c) SUN
RGB-D, and the completion results of our method. companied by many invalid areas caused by weak or direct reflections, far distance, bright light, and other environmen-tal noises, as shown in Fig. 1. These invalid data will have severe diverse impacts on the following process. Therefore, depth completion has become necessary for most applica-tions based on depth images.
Although many approaches based solely on raw depth images have been proposed, their performance is severely limited because of the absence or uncertainty of invalid data. Therefore, researchers consider introducing RGB in-formation to guide the depth completion [7] through two typical manners. The traditional one fills invalid pixels based on their valid neighbors according to some given rules, such as joint bilateral filters [2], fast marching [24], and Markov random field [16]. However, these methods are generally not fast nor sufficiently accurate. The other ap-proach predicts invalid pixels with the deep neural network, which usually employs an auto-encoder to extract depth and
color features from the RGB-D data and fuse them to com-plete depth map [5, 18, 19, 20, 25, 31, 29, 14, 23]. This ap-proach has shown extraordinary progress compared to the traditional one and is widely applied in recent works.
However, there are two challenges in the deep learning approach. Firstly, the vanilla convolution operation treats all inputs as valid values [30]. The raw depth images con-tain a lot of invalid values, which can defile the latent fea-tures extracted by convolution kernels and lead to various visual artifacts during the reconstruction, such as cavities, contradiction, and blurred edges. To ameliorate the limita-tion, partial convolution has been proposed [17] to distin-guish invalid pixels automatically and calculates the output based only on valid pixels. Moreover, the output pixel will be marked as valid if the receptive field contains at least one valid pixel. This method improves the reliability of the features, but it still has irreconcilable issues. For instance, considering a convolution kernel that covers 3x3 pixels, and no matter how many valid pixels are contained in this re-gion, the kernel outputs are marked equally valid. Never-theless, the truth is that the confidence of the outputs is to-tally different in these cases. Going beyond, Gated Convo-lution (GConv) and De-convolution (De-GConv) [30] was proposed, which can learn a gating mask via additional con-volution kernels to suppress invalid features and strengthen the reliable ones. These operations are workable in extract-ing features from raw depth images with invalid pixels but fail to handle large missing areas, thus their depth comple-tion results are still not trustworthy. A plausible approach to complete depth images with big holes is taking both color and depth information into account.
Here comes another challenge, using color information for depth completion has both positive and negative effects.
Most existing models implement the fusion of color and depth by concatenating latent features directly on the bot-tleneck of the auto-encoder. However, the involvement of depth-irrelevant color features may mislead the depth pre-diction results, such as neighboring surfaces in the same color and planes with rich textures. Therefore, a mecha-nism of screening the interference from the fusion of depth and color features certainly benefits the task of depth com-pletion. Unfortunately, most comparative research has not addressed this problem.
Based on the above observations, we propose a new framework for depth completion based on an UNet-like [21] architecture, in which the depth and color features are ex-tracted in two parallel encoding branches and then merge into one branch with skip connections in the decoding stage.
Specifically, the fusion of the two branches is conducted by stages based on our proposed Attention Guided Gated Con-volution (AG-GConv), which learns joint contextual atten-tion from both color and depth values to guide the extraction of depth features. Furthermore, the Attention Guided Skip
Connection module is designed to filter out irrelevant color features from the reconstruction of depth. Our main contri-butions can be summarized as follows:
• We propose a dual-branch multi-scale encoder-decoder network that combines depth and color fea-tures to achieve high-quality completion of the depth image.
• An Attention Guided Gated Convolution (AG-GConv) module is proposed to alleviate the adverse impacts of invalid depth values on feature learning.
• A new Attention Guided Skip Connection (AG-SC) module is presented to reduce the interference from depth-irrelevant color features to the decoder.
• Experimental results indicate that our model outper-forms state-of-the-art on three popular benchmarks, including NYU-Depth V2, DIML and SUN RGB-D datasets. 2.