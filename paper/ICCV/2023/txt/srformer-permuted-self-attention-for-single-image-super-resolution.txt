Abstract
Previous works have shown that increasing the window size for Transformer-based image super-resolution models (e.g., SwinIR) can significantly improve the model perfor-mance but the computation overhead is also considerable.
In this paper, we present SRFormer, a simple but novel method that can enjoy the benefit of large window self-attention but introduces even less computational burden.
The core of our SRFormer is the permuted self-attention (PSA), which strikes an appropriate balance between the channel and spatial information for self-attention. Our
PSA is simple and can be easily applied to existing super-resolution networks based on window self-attention. With-out any bells and whistles, we show that our SRFormer achieves a 33.86dB PSNR score on the Urban100 dataset, which is 0.46dB higher than that of SwinIR but uses fewer parameters and computations. We hope our simple and effective approach can serve as a useful tool for future research in super-resolution model design. Our code is available at https://github.com/HVision-NKU/
SRFormer. 1.

Introduction
Single image super-resolution (SR) endeavors to re-cover a high-quality image from its degraded low-resolution counterpart. The pursuit of efficient and proficient super-resolution algorithms has been a hot research topic in com-puter vision, which has a variety of applications [2, 25, 63].
Since the pioneer works [9, 26, 30, 38, 52, 82], CNN-based methods have been mainstream for image super-resolution for a long time. These methods mostly take advantage of residual learning [26, 30, 32, 38, 55, 80], dense connec-tions [58, 67, 87], or channel attention [72, 86] to construct network architectures, making substantial contributions to the advancement of super-resolution models.
*Corresponding author.
WS : 24 × 24
PSNR : 33.51
WS : 16 × 16
PSNR : 33.40
WS : 16 × 16
PSNR : 33.26
WS : 12 × 12
PSNR : 33.28
WS : 12 × 12
PSNR : 33.08
WS : 8 × 8
PSNR : 33.09
[37]
Figure 1. Performance comparison between SwinIR and our SR-Former when training 200k iterations with different window sizes (WS) for 200k iterations. Our SRFormer enjoys a large window size of 24 × 24 with even fewer computations but higher PSNR scores.
Despite the success made by CNN-based models in super-resolution, recent works [5, 37, 79, 85] have shown that Transformer-based models perform better. They ob-serve that the ability of self-attention to build pairwise rela-tionships is a more efficient way to produce high-quality super-resolution images than convolutions. One typical work among them should be SwinIR [37] which introduces
Swin Transformer [41] to image super-resolution, greatly improving the state-of-the-art CNN-based models on vari-ous benchmarks. Later, a variety of works, such as Swin-FIR [79], ELAN [85], and HAT [6], further develop SwinIR and use Transformers to design different network architec-tures for SR.
The aforementioned methods reveal that properly en-larging the windows for the shifted window self-attention in SwinIR can result in clear performance gain (see Fig-ure 1). However, the computational burden is also an im-portant issue as the window size goes larger.
In addi-tion, Transformer-based methods utilize self-attention and require networks of larger channel numbers compared to previous CNN-based methods [26, 86, 87]. To explore ef-ficient and effective super-resolution algorithms, a straight-forward question should be: How would the performance go if we reduce the channel number and meanwhile increase the window size?
Motivated by the question mentioned above, in this pa-per, we present permuted self-attention (PSA), an efficient way to build pairwise relationships within large windows (e.g., 24 × 24). The intention is to enable more pixels to get involved in the attention map computation and at the same time introduce no extra computational burden. To this end, we propose to shrink the channel dimensions of the key and value matrices and adopt a permutation operation to convey part of the spatial information into the channel dimension.
In this way, despite the channel reduction, there is no loss of spatial information, and each attention head is also allowed to keep a proper number of channels to produce expressive attention maps [60]. In addition, we also improve the orig-inal feed-forward network (FFN) by adding a depth-wise convolution between the two linear layers, which we found helps in high-frequency component recovery.
Given the proposed PSA, we construct a new network for
SR, termed SRFormer. We evaluate our SRFormer on five widely-used datasets. Benefiting from the proposed PSA, our SRFormer can clearly improve its performance on al-most all five datasets. Notably, for ×2 SR, our SRFormer trained on only the DIV2K dataset [38] achieves a 33.86
PSNR score on the challenging Urban100 dataset [20].
This result is much higher than those of the recent SwinIR (33.40) and ELAN (33.44). A similar phenomenon can also be observed when evaluating on the ×3 and ×4 SR tasks.
In addition, we perform experiments using a light version of our SRFormer. Compared to previous lightweight SR models, our method also achieves better performance on all benchmarks.
To sum up, our contributions can be summarized as fol-lows:
• We propose a novel permuted self-attention for image super-resolution, which can enjoy large-window self-attention by transferring spatial information into the chan-nel dimension. By leveraging it, we are the first to im-plement 24x24 large window attention mechanism at an acceptable time complexity in SR.
• We build a new transformer-based super-resolution net-work, dubbed SRFormer, based on the proposed PSA and an improved FFN from the frequency perspective (Con-vFFN). Our SRFormer achieves state-of-the-art perfor-mance in classical, lightweight, and real-world image SR tasks. 2.