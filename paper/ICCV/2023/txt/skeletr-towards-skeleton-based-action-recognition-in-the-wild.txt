Abstract
We present SkeleTR, a new framework for skeleton-based
In contrast to prior work, which fo-action recognition. cuses mainly on controlled environments, we target more general scenarios that typically involve a variable number of people and various forms of interaction between people.
It first mod-SkeleTR works with a two-stage paradigm. els the intra-person skeleton dynamics for each skeleton sequence with graph convolutions, and then uses stacked
Transformer encoders to capture person interactions that are important for action recognition in general scenarios.
To mitigate the negative impact of inaccurate skeleton as-sociations, SkeleTR takes relative short skeleton sequences as input and increases the number of sequences. As a uni-fied solution, SkeleTR can be directly applied to multiple skeleton-based action tasks, including video-level action classification, instance-level action detection, and group-level activity recognition. It also enables transfer learning and joint training across different action tasks and datasets, which result in performance improvement. When evaluated on various skeleton-based action recognition benchmarks,
SkeleTR achieves the state-of-the-art performance. 1.

Introduction
Skeleton-based action recognition has received increas-ing attention in recent years due to its robustness against background and illumination changes [64, 16]. Existing methods [36, 7, 8, 15] mostly use Graph Convolutional Net-works (GCNs) as backbones and focus on modeling spatio-temporal information from long skeleton sequences sepa-rately. While observing promising results in simplified and controlled environments [46, 32], they are less effective in more realistic scenarios with a variable number of people performing diverse actions. First, considering that linking skeletons (e.g., using skeletonsâ€™ confidence scores [64]) in a long time span may inevitably generate more association errors [12, 48], requiring long sequences as input can bring
*The work was done during an Amazon internship.
Figure 1: SkeleTR is a general framework for skeleton action recognition which can handle different tasks, including video-level action classification, instance-level action detection, and group-level activity recognition. It adopts a two-stage paradigm: the intra-sequence and inter-sequence modeling, respectively. lots of noises. Second, prior works pay little attention to modeling the relations among a group of people, which plays an important role in recognizing both person inter-actions (e.g., talking to someone [22]) and group activities (e.g., pass/win-point in a volleyball game [24]).
In this paper, we propose SkeleTR for skeleton action recognition in real-world scenarios and handle multiple ac-tion tasks (i.e., video-level, instance-level, and group-level action recognition) in a general framework. SkeleTR sam-ples a large number of short skeleton sequences instead of a few long ones (practice of prior works [64, 16]) from each video. Each short sequence directly corresponds to an atomic action of a single person, thus the collection of mul-tiple short sequences in space and time can represent hier-archical actions (with a longer time span) of a single person or a video-level action involving multiple people. The new input paradigm can be applied to instance-level and video-level recognition of actions with different temporal granu-larities, and it has several unique advantages. First, associ-ation errors are less likely to happen in a shorter skeleton sequence [12, 48]. We show that the improved sequence consistency leads to better intra-sequence modeling. Sec-ond, this new format is more flexible and helps to create comprehensive yet compact inputs. With a finer temporal 1
granularity, the same input size budget can cover more hu-man skeletons.
It reduces redundant zero-paddings for a small number of people, and mitigates information loss dur-ing input sampling for a large number of people.
SkeleTR adopts a two-stage paradigm as shown in Fig 1.
Stage 1 performs intra-sequence modeling to capture the motion patterns of each individual skeleton sequence with a GCN backbone. The GCN backbone outputs a spatio-temporal feature for each sequence. Stage 2 concatenates all sequence features and feeds them into Transformer en-coders to model inter-sequence relationships. At the end of stage 2, average pooling aggregates all features belonging to the same sequence into a single feature vector. By lever-aging inter-sequence modeling, the output sequence feature incorporates the interactions between people and the global context of the video, which is helpful in both instance-level and video-level action recognition. To reduce the dimension of spatio-temporal skeleton features and keep the computa-tions of Transformer tractable, we insert a novel Mix Pool-ing module before stage 2. It generates a compact represen-tation with fine granularity for each sequence by applying multiple partial dimension reduction strategies in parallel.
SkeleTR easily enables transfer learning and joint learn-ing across different datasets and tasks. With a unified input format and model architecture, one can jointly train multi-ple tasks with different datasets, each with its own loss func-tion and ground truth annotations. This allows for leverag-ing auxiliary datasets for pre-training or joint training of a target task with limited labeled data.
In experiments, we find that joint training serves as a strong regularizer and improves recognition performance, especially in scenarios with limited training data.
To evaluate the effectiveness of SkeleTR, we conduct comprehensive study on three skeleton-based action recog-action classification [6, 65, 46], spatio-nition tasks: temporal action detection [22], and group activity recog-nition [24]. Experimental results show that SkeleTR works well with different GCN backbones and significantly out-performs prior works on all three tasks. For example, with ST-GCN++ [15] backbone, SkeleTR outperforms the state-of-the-art methods by 3.8%, 3.8%, 0.7%, and 1.1% on Posetics [64], AVA [22], Volleyball [24], and NTU-Inter [40]. With Posetics joint training, the AVA perfor-mance is further improved by 4% mAP. The great capa-bility of SkeleTR leads to another noteworthy result: we first observe the strong complementarity of skeleton action recognition. Combining SkeleTR prediction and the RGB state-of-the-art [2, 39] with simple late fusion leads to great performance boost (2.3% Posetics Top-1, 3% AVA mAP). 2.