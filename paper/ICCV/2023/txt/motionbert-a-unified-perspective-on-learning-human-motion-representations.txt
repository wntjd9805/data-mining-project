Abstract
We present a unified perspective on tackling various human-centric video tasks by learning human motion rep-resentations from large-scale and heterogeneous data re-sources. Specifically, we propose a pretraining stage in which a motion encoder is trained to recover the underly-ing 3D motion from noisy partial 2D observations. The motion representations acquired in this way incorporate geometric, kinematic, and physical knowledge about hu-man motion, which can be easily transferred to multiple downstream tasks. We implement the motion encoder with a Dual-stream Spatio-temporal Transformer (DSTformer) neural network. It could capture long-range spatio-temporal relationships among the skeletal joints comprehensively and adaptively, exemplified by the lowest 3D pose estimation error so far when trained from scratch. Furthermore, our proposed framework achieves state-of-the-art performance on all three downstream tasks by simply finetuning the pre-trained motion encoder with a simple regression head (1-2 layers), which demonstrates the versatility of the learned motion representations. Code and models are available at https://motionbert.github.io/ 1.

Introduction
Perceiving and understanding human activities have long been a core pursuit of machine intelligence. To this end, researchers define various tasks to estimate human-centric semantic labels from videos, e.g. skeleton keypoints [13, 33], action classes [60, 116], and surface meshes [42, 66]. While significant progress has been made in each of these tasks, they tend to be modeled in isolation, rather than as intercon-nected problems. For example, Spatial Temporal Graph Con-â€ Yizhou Wang is with Center on Frontiers of Computing Studies, School of Computer Science, Peking University and Institute for Artificial Intelli-gence, Peking University.
Figure 1. Framework overview. We utilize a motion encoder to learn human motion representations via recovering 3D human mo-tion from corrupted 2D skeleton sequences. To adapt to different downstream tasks, we finetune the pretrained motion representa-tions with a linear layer or a simple MLP. volutional Networks (ST-GCN) have been applied to model-ing spatio-temporal relationship of human joints in both 3D pose estimation [12, 109] and action recognition [89, 116], but their connections have not been fully explored. Intu-itively, these models should all have learned to identify typi-cal human motion patterns, despite being designed for dif-ferent problems. Nonetheless, current methods fail to mine and utilize such commonalities across the tasks. Ideally, we could develop a unified human-centric video representation that can be shared across all relevant tasks.
One significant challenge to developing such a represen-tation is the heterogeneity of available data resources. Mo-tion capture (Mocap) systems [36, 71] provide high-fidelity 3D motion data obtained with markers and sensors, but the appearances of captured videos are usually constrained to simple indoor scenes. Action recognition datasets provide annotations of the action semantics, but they either contain no human pose labels [15, 88] or feature limited motion of daily activities [59, 60, 86]. In contrast, in-the-wild hu-man videos offer a vast and diverse range of appearance and motion. However, obtaining precise 2D pose annotations requires considerable effort [3], and acquiring ground-truth (GT) 3D joint locations is almost impossible. Consequently,
most existing studies focus on a specific task using a single type of human motion data, and they are not able to enjoy the advantages of other data resources.
In this work, we provide a new perspective on learning human motion representations. The key idea is that we can learn a versatile human motion representation from hetero-geneous data resources in a unified manner, and utilize the representation to handle different downstream tasks in a unified way. We present a two-stage framework, consist-ing of pretraining and finetuning, as depicted in Figure 1.
In the pretraining stage, we extract 2D skeleton sequences from diverse motion data sources and corrupt them with ran-dom masks and noises. Subsequently, we train the motion encoder to recover the 3D motion from the corrupted 2D skeletons. This challenging pretext task intrinsically requires the motion encoder to i) infer the underlying 3D human struc-tures from its temporal movements; ii) recover the erroneous and missing observations. In this way, the motion encoder implicitly captures human motion commonsense such as joint linkages, anatomical constraints, and temporal dynam-ics. In practice, we propose Dual-stream Spatio-temporal
Transformer (DSTformer) as the motion encoder to capture the long-range relationship among skeleton keypoints. We suppose that the motion representations learned from large-scale and diversified data resources could be shared across different downstream tasks and benefit their performance.
Therefore, for each downstream task, we adapt the pretrained motion representations using task-specific training data and supervisory signals with a simple regression head.
In summary, the contributions of this work are three-fold: 1) We provide a new perspective on solving various human-centric video tasks through a shared framework of learning human motion representations. 2) We propose a pretraining method to leverage the large-scale yet heterogeneous human motion resources and learn generalizable human motion representations. Our approach could take advantage of the precision of 3D mocap data and the diversity of in-the-wild
RGB videos at the same time. 3) We design a dual-stream
Transformer network with cascaded spatio-temporal self-attention blocks that could serve as a general backbone for human motion modeling. The experiments demonstrate that the above designs enable a versatile human motion represen-tation that can be transferred to multiple downstream tasks, outperforming the task-specific state-of-the-art methods. 2.