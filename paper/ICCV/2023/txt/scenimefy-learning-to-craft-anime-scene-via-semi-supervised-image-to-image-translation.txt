Abstract
Automatic high-quality rendering of anime scenes from complex real-world images is of significant practical value.
The challenges of this task lie in the complexity of the scenes, the unique features of anime style, and the lack of high-quality datasets to bridge the domain gap. De-spite promising attempts, previous efforts are still in-competent in achieving satisfactory results with consis-tent semantic preservation, evident stylization, and fine de-tails.
In this study, we propose Scenimefy, a novel semi-supervised image-to-image translation framework that ad-dresses these challenges. Our approach guides the learning with structure-consistent pseudo paired data, simplifying the pure unsupervised setting. The pseudo data are derived uniquely from a semantic-constrained StyleGAN leveraging rich model priors like CLIP. We further apply segmentation-guided data selection to obtain high-quality pseudo super-vision. A patch-wise contrastive style loss is introduced to improve stylization and fine details. Besides, we contribute a high-resolution anime scene dataset to facilitate future research. Our extensive experiments demonstrate the su-∗ Equal contribution. periority of our method over state-of-the-art baselines in terms of both perceptual quality and quantitative perfor-mance. Project page: https://yuxinn-j.github. io/projects/Scenimefy.html. 1.

Introduction
Crafting anime scenes requires significant artistic skill and time, making developing learning-based techniques for automatic scene stylization of unquestionable practical and commercial value. Recent advances in Generative Adver-sarial Networks (GANs) have led to a significant improve-ment in automatic stylization, but most research in this area has focused primarily on human faces [41, 33, 22, 20, 42].
Despite its high research value, generating high-quality anime scenes from complex real-world scene images re-mains underexplored.
Transferring real scene images into anime styles remains a formidable challenge due to several factors. 1) The nature of a scene. Scenes are typically composed of multiple ob-jects with complex relationships among them, and there is an inherent hierarchy between foreground and background elements, as shown in Figure 2. 2) The features of anime.
Anime is characterized by unique textures and intricate de-tails, such as the pre-designed brush strokes used in natural landscapes like grass, trees, and clouds, as illustrated in Fig-ure 2. These textures are typically organic and hand-drawn, making their style much more difficult to mimic than the sharp edges and smooth color patches defined in previous studies [5, 36]. 3) The domain gap and lack of data. There is a large domain gap between real and anime scenes, and a high-quality anime scene dataset is essential in bridging this gap. However, existing datasets contain many human faces and other foreground objects, whose style is different from that of the background scene, leading to their low quality.
Unsupervised image-to-image translation [46, 21, 11, 13, 27, 14] is a typical solution for complex scene styliza-tion without paired training data. Despite promising results, existing methods [5, 3, 36, 8] that focus on anime styles fall short in several ways. First, the absence of pixel-wise correspondence in complex scenes hinders existing meth-ods [5, 3] from effectively performing evident texture styl-ization while preserving semantic content, resulting in po-tentially unnatural results with notable artifacts. Second, some approaches [36, 8] fall short of generating fine details of anime scenes. This is due to their handcrafted anime-specific losses or pre-extracted representations that impose edge and surface smoothness.
To address the challenges discussed above, we pro-pose a novel semi-supervised image-to-image (I2I) transla-tion pipeline, named Scenimefy, for producing high-quality anime-style renderings of scene images, as shown in Fig-ure 1. Our key idea is to incorporate a new supervised training branch into the unsupervised framework using gen-erated pseudo paired data to overcome the difficulties of unsupervised training. Specifically, we leverage the desir-able properties of StyleGAN [16, 17] by fine-tuning it to generate coarse paired data between real and anime, which we call pseudo paired data. We propose a novel semantic-constrained fine-tuning strategy that leverages rich pre-trained model priors, such as CLIP [29] and VGG [31], to guide StyleGAN to capture complex scene features and alleviate overfitting. We further introduce a segmentation-guided data selection scheme to filter low-quality data. With the pseudo paired data, Scenimefy learns effective pixel-wise correspondence and generates fine details between the two domains, guided by a novel patch-wise contrastive style loss. Together with the unsupervised training branch, our semi-supervised framework seeks a desired trade-off be-tween the faithfulness and fidelity of scene stylization.
To facilitate training, we also collected a high-quality pure anime scene dataset. We conducted comprehensive experiments that demonstrate the effectiveness of Scen-imefy, surpassing state-of-the-art baselines in both percep-tual quality and quantitative evaluation. In summary, our key contributions are as follows:
• We propose a novel semi-supervised image-to-image
Figure 2: Characteristics of anime scenes. A scene frame from Shinkai’s film ’Children Who Chase Lost Voices’ (2011) shows the presence of hand-drawn brush strokes of grass and stones (foreground), as well as trees and clouds (background), as opposed to clear edges and flat surfaces. translation framework for scene stylization that gen-erates high-quality complex anime scene images from real ones. Our framework incorporates a new patch-wise contrastive style loss to improve stylization and fine details.
• The training supervision is derived from structure-consistent pseudo paired data generated by a newly designed semantic-constrained StyleGAN fine-tuning strategy with rich pre-trained prior guidance, followed by a segmentation-guided data selection scheme.
• We collected a high-resolution anime scene dataset to facilitate future research in scene stylization. 2.