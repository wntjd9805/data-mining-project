Abstract
Weakly-supervised action localization aims to recognize and localize action instancese in untrimmed videos with only video-level labels. Most existing models rely on multi-ple instance learning(MIL), where the predictions of unla-beled instances are supervised by classifying labeled bags.
The MIL-based methods are relatively well studied with co-gent performance achieved on classification but not on lo-calization. Generally, they locate temporal regions by the video-level classification but overlook the temporal varia-tions of feature semantics. To address this problem, we pro-pose a novel attention-based hierarchically-structured la-tent model to learn the temporal variations of feature se-mantics. Specifically, our model entails two components, the first is an unsupervised change-points detection mod-ule that detects change-points by learning the latent repre-sentations of video features in a temporal hierarchy based on their rates of change, and the second is an attention-based classification model that selects the change-points of the foreground as the boundaries. To evaluate the effec-tiveness of our model, we conduct extensive experiments on two benchmark datasets, THUMOS-14 and ActivityNet-v1.3. The experiments show that our method outperforms current state-of-the-art methods, and even achieves compa-rable performance with fully-supervised methods. 1.

Introduction
Action localization is one of the most challenging tasks in video analytics and understanding [16, 43, 50, 17]. The goal is to predict the accurate start and end time stamps of different human actions. Owing to its wide application (e.g., surveillance [40, 42], video summarization [27], high-†Work is done during the internship at ACS Lab, Huawei Technologies.
*Corresponding author (guoqinghai@huawei.com).
Figure 1. Visualization of the change-point detection component and the co-occurrence component(attention-based classification module) decoupled by our method from a snippet representation.
The change-point component helps to detect change-points of tem-poral variations, which include the change-points of foreground in a video(i.e. the highlighted frames in the left part). Collaborating with the attention module, the points of the foreground are cho-sen as boundaries of action(i.e. the highlighted frames in the right part).
nificant improvement has been made in prior MIL-based work, there is still a huge performance gap between the weakly-supervised and fully-supervised settings. In consid-eration of this issue, diverse solutions have been proposed in the literature. For instance, [47, 16, 17] try to erase the most discriminative parts for learning action completeness,
[43, 50, 12] learn with pseudo labels generated by manual thresholds and iterative refinement, and [33, 37, 54] formu-late the WSAL problem as a video recognition problem and introduce an attention mechanism to construct video-level features, then apply an action classifier to recognize videos.
All the above approaches largely rely on the video-level classification model, which aims at learning the effective classification functions to identify action instances from bags of action instances and non-action frames, but over-look the significance of feature variations.
In fact, fea-tures usually contain intense semantic information [5, 34], mainly stemming from the temporal and spatial variations in video actions. The variation of features is useful for cor-recting the wrong action region and adjusting the imprecise boundary of the temporal proposal. Existing solutions often neglect such semantics and thus largely suffer from deviated action boundaries and inaccurate detection.
To better learn the semantics in a given video sample, the model should be able to encode the temporal variations of different time factors. Intuitively, these variations in dif-ferent timescales disentangle different video fragments, and detection on such variations automatically leads to the de-tection of change-points, which provide the candidates for action boundaries.
Derived from the above idea, we propose a novel
Attention-based Hierarchically-structured Latent Model (AHLM) to model the spatial and temporal features for
WSAL task. Specifically, we detect the action boundaries as the change-points of a generative model, where those change-points are determined at the time points with in-accurate generation. Such generative model, is trained by learning the hierarchical representations of the feature se-mantics in the latent space based on the video inputs. By using an attention-based classification model to select the change-points of the foreground, AHLM localizes the exact action boundaries, see Figure 1 for an illustration.
To our best knowledge, we are the first to consider the temporal variation of feature semantics and study the change-point detection mechanism in WSAL. We design an
AHLM that prominently boosts WSAL performance. Our main contributions are summarized as follows:
• To leverage the temporal variations of feature seman-tics for WSAL, we propose a hierarchically-structured generative latent model that explores spatiotemporal representations and leverages the temporal feature se-mantics to detect the change-points of videos.
• We build a new framework, AHLM, which firstly pro-poses the use of an unsupervised change-points detec-tor in a latent space to complement weakly supervised learning, with a novel hierarchical generative model-baed change-point detector for complex datasets.
• Based on extensive experiments, we demonstrate that, on two popular action detection datasets, our novel
AHLM provides considerable performance gains. On
THUMOS14 especially, our method achieves an aver-age mAP of 47.2% when IOU is from 0.1 to 0.7, which is the new state-of-the-art (SotA). On ActivityNet v1.3, our method also achieves the new SotA, with an aver-age mAP of 25.9% when IOU is from 0.5 to 0.95. 2.