Abstract
This study focuses on using large language models (LLMs) as a planner for embodied agents that can follow natural language instructions to complete complex tasks in a visually-perceived environment. The high data cost and poor sample efficiency of existing methods hinders the development of versatile agents that are capable of many
In this work, we tasks and can learn new tasks quickly. propose a novel method, LLM-Planner, that harnesses the power of large language models to do few-shot planning for embodied agents. We further propose a simple but ef-fective way to enhance LLMs with physical grounding to generate and update plans that are grounded in the current environment. Experiments on the ALFRED dataset show that our method can achieve very competitive few-shot per-formance: Despite using less than 0.5% of paired train-ing data, LLM-Planner achieves competitive performance with recent baselines that are trained using the full training data. Existing methods can barely complete any task suc-cessfully under the same few-shot setting. Our work opens the door for developing versatile and sample-efficient em-bodied agents that can quickly learn many tasks. 1 1.

Introduction
Building versatile embodied agents such as robots that can follow natural language commands to do different tasks as well as learn to do new tasks quickly has long been de-sired. However, contemporary language-driven agents still require a large number of labeled examples (pairs of lan-guage instructions and gold trajectories) to learn each task, which is highly costly and hinders the development of truly versatile agents [35, 31, 26, 8, 38, 18, 42, 29, 11, 2, 17]. Re-Figure 1: An illustration of LLM-Planner for high-level planning. After receiving the natural language instruction (t = 0), LLM-Planner first generates a high-level plan by prompting a large language model (e.g., GPT-3). When the embodied agent gets stuck during the execution of the current plan (t = 5 and 20), LLM-Planner re-plans based on observations from the environment to generate a more grounded plan, which may help the agent get unstuck. The commonsense knowledge in the LLM (e.g., food is often stored in a fridge) allows it to produce plausible high-level plans and re-plan based on new environmental perception. cently, an array of seminal work has shown the remarkable potential of large language models (LLMs) such as GPT-3 [4] as a few-shot planner for embodied AI agents [1, 13, 22, 36]. Agents equipped with LLM-based planners have started to show the ability to learn a new task with a few training examples. 1https://osu-nlp-group.github.io/LLM-Planner/
While showing great promises as proof of concepts, ex-isting work still presents significant limitations that may prevent larger-scale applications beyond their limited eval-uation setting. As an example, SayCan [1], one of the pioneering work on using LLMs for embodied instruction following, is evaluated on two environments with only 15 object types. The agent is assumed to be able to enu-merate all admissible skills (i.e., [action, object] pairs) up front so it can use an LLM to rank the skills. This as-sumption could break easily in partially-observable envi-ronments when deploying an agent to new environments.
The cost could also quickly pile up in more complex envi-ronments with more objects because the agent needs to call the LLM to evaluate every admissible skill at every step; efficiency deteriorates at the same time. Finally, most ex-isting work [1, 36, 13, 25] uses LLMs to generate a single static plan from the language instruction and then executes on the entire plan. However, the optimal plan for the same language instruction is dependent on the environment; dif-ferent environments may need different plans. There lacks a way to dynamically adjust the plan from LLMs based on environmental perception.
Building on existing work, we propose LLM-Planner, an LLM-based planner for embodied instruction following.
An important design goal is to be able to handle a wide range of tasks in diverse, partially-observable environments, and can dynamically adjust the plan based on perceptions from the environment. Therefore, different from SayCan, we use LLMs to directly generate plans instead of rank-ing admissible skills, obviating the need to have sufficient knowledge about the environment a priori while also sig-nificantly reducing the number of calls to LLMs. Another unique strength of LLM-Planner is its ability to dynamically re-plan based on what the agent observes in the current en-vironment, which produces more grounded plans.
More specifically, we adopt hierarchical planning mod-els (e.g., [39, 34]), which consist of a high-level planner and a low-level planner. We use LLMs to generate high-level plans (HLPs), i.e., a sequence of subgoals (e.g., [Navigation potato, Pickup potato, Navigation microwave, ...]) that the agent needs to achieve, in the specified order, to accomplish the final goal specified by the language instruction. The low-level planner then maps each subgoal into a sequence of primitive actions for achieving that subgoal in the current environment and state. An important observation is that, given a high-level plan, low-level planning becomes condi-tionally independent of the natural language instruction. It becomes the classic object localization and navigation prob-lem [6] (for navigation subgoals) or simply executing the specified interaction action with the right objects (for inter-action subgoals). The low-level planner can be trained with data synthesized from the simulator (see, e.g., [28, 3]).
Furthermore, we learning follow the paradigm [4, 21] and only use a small number of paired in-context
In addition, no parameter update is needed, examples. which saves development time. For the example in Fig-ure 1, at the beginning of an episode (t = 0), given a natural language instruction, we directly prompt the LLM to generate the HLP by giving it several exemplar pairs of (instruction, HLP) in its context. We also leverage established techniques such as dynamic in-context example retrieval [30, 32, 9, 20] and logit biases [10] to further improve the in-context learning performance.
While the HLPs generated by LLMs are already plausi-ble at first glance, they still lack a fundamental aspect of embodied agents â€” physical grounding; i.e., the generated
HLP needs to be grounded to the environment the agent is in. Previous approaches [1, 36, 13] train a separate model that translates the LLM plans to the grounded admissible actions. However, this is possible under the assumption that the LLM plan can be matched to a reasonable admissible ac-tion. If the LLM plans are not contained in the list of admis-sible action, which is the case in the diverse environments, this creates an undetermined behavior for those agents. To overcome this problem, we propose a novel grounded re-planning algorithm to empower LLM-Planner with physi-cal grounding. Specifically, as an agent is executing the ini-tial HLP, whenever it has taken too many steps to reach the current subgoal or has made too many failed attempts, we dynamically prompt the LLM again to generate a new con-tinuation of the partial HLP that has been completed at that point. For grounding, we add the list of objects perceived in the environment so far into the prompt as a simple but effective description of the current environment. Figure 1 demonstrates how our grounded re-planning algorithm can help the agent overcome a plan that is unattainable. For the example at t = 5, the agent is taking too long to find a potato. It re-prompts the LLM with the object fridge ob-served in the environment, and LLM-Planner generates a new HLP from scratch (because no subgoal has been com-pleted so far) that directs the agent to look for a potato in the fridge. By introducing a way to incorporate feedback from the environment, we aim to create a closed-loop between the environment and the LLMs where LLMs can dynamically adapt the generated high-level plans to the environment.
While most existing work [1, 14, 13, 36, 25] is evalu-ated under a limited setting (e.g., limited/known environ-ments, short-horizon tasks, or simple environments with a small number of objects), we evaluate LLM-Planner on
ALFRED [35], a large-scale dataset with diverse partially-observable environments and a wide variety of tasks and objects. We test our LLM-Planner by integrating it with the perception module and low-level planner from a strong baseline model, HLSM [3]. Using less than 0.5% of paired training data, LLM-Planner achieves competitive perfor-mance compared with HLSM and outperforms multiple other baselines, which are trained with the full training
set. Under the same few-shot setting, existing methods can barely complete any task successfully. Our work opens a new door for developing versatile and extremely sample-efficient embodied agents by harnessing the power of large language models and grounding. 2.