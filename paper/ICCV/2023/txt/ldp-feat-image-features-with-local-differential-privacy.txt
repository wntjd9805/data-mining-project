Abstract
Modern computer vision services often require users to share raw feature descriptors with an untrusted server. This presents an inherent privacy risk, as raw descriptors may be used to recover the source images from which they were ex-tracted. To address this issue, researchers [11] recently pro-posed privatizing image features by embedding them within an affine subspace containing the original feature as well as adversarial feature samples. In this paper, we propose two novel inversion attacks to show that it is possible to (approximately) recover the original image features from these embeddings, allowing us to recover privacy-critical image content. In light of such successes and the lack of theoretical privacy guarantees afforded by existing visual privacy methods, we further propose the first method to pri-vatize image features via local differential privacy, which, unlike prior approaches, provides a guaranteed bound for privacy leakage regardless of the strength of the attacks. In addition, our method yields strong performance in visual lo-calization as a downstream task while enjoying the privacy guarantee. 1.

Introduction
The extraction and matching of image keypoints with descriptors is an essential building block for many vi-sion problems, such as 3D reconstruction [2], image re-trieval [28] and recognition [39]. Thus, modern com-puter vision services often require the users to share feature descriptors and/or raw images to a centralized server for downstream tasks, such as visual localization [37]. How-ever, recent works [30, 36] show that high-quality images may be recovered from the keypoint descriptors [30] or their spatial information [36], raising serious concerns regarding the potential leakage of private information via inversion at-tacks.
This in turn inspires great interest in researching feature obfuscation with a view to concealing the privacy critical information in the image, mainly by perturbing either the feature descriptors or their locations. One of the recent
Figure 1. Our novel privacy attacks and LDP-based privacy method. Top row: image reconstruction attacks against adver-sarial affine subspace embeddings [11] and our LDP-FEAT when they have comparable performance in the downstream utility task.
Bottom row: overviews of the adversarial affine subspace embed-dings algorithm [11] and our LDP-FEAT algorithm works [11] represents a feature descriptor point as an affine subspace passing through the original point, as well as a number of other adversarial descriptors randomly sampled from a database of descriptors, as shown in Fig. 1. These adversarial descriptors serve as confounders to conceal the raw descriptor. Another line of research [37, 38, 15, 35] aims to conceal the location of 2D or 3D keypoints by lift-ing the point to a line passing through that point, which pre-vents a direct attack of the sort in [30, 36].
Despite their success, these works are primarily eval-uated on the basis of empirical performance of a chosen attacker, without rigorous understanding of the attacker-independent, intrinsic privacy property. This causes hin-drance for a method to claim privacy protection safely since there is no theoretical guarantee to assure practical appli-cations. For instance, [7] re-investigates the privacy claim in [37] and designs a stronger attack to reveal that a sig-nificant amount of scene geometry information in fact still exists in the lifted line clouds, which can be leveraged to re-cover sensitive image content. In this paper, we focus on the
feature descriptor and, similar in spirit to [7], we reveal the privacy leakage in the affine subspace lifting [11]. Consid-ering the drawbacks of the visual privacy-based method, we present the first attempt of its kind to formulate the privacy protection of image features through the lens of differential privacy [43], which permits theoretic privacy characteriza-tion, enjoys a guaranteed bound on privacy loss, and has become a gold standard notion of privacy.
More specifically, we firstly introduce two novel attacks against the adversarial affine subspace embedding [11], namely the database attack and the clustering attack. In the database attack, we assume that the database used to sam-ple the adversarial descriptors is accessible to the attacker, while in clustering attack we relax this assumption. At its core, both attacks are established based upon the following key assumption: the low-dimensional (e.g. 2,4,8) affine sub-space very likely only intersects with the manifold of high-dimensional (e.g. 128 for SIFT [23]) descriptors at those points that were intentionally selected to construct the sub-space in the beginning, i.e. the raw descriptor to be con-cealed and the adversarial ones chosen from the database.
Tha main idea of our attacks lies in identifying these inter-sections and further eliminating the adversarial ones. As shown in Fig. 1, our attacks recover images of higher qual-ity than the direct inversion attack shown in [11].
Next, we propose LDP-FEAT, a novel descriptor priva-tization method that rests on the notion of local differential privacy (LDP) [49], as illustrated in Fig. 1. In contrast to the original centralized differential privacy which prevents pri-vate information in the database from releasing to queries, we instead aim to protect privacy in the query itself, i.e. the image descriptors to be sent. We propose to formulate the feature obfuscation by local differential privacy, with the so-called ω-subset mechanism [43] – we effectively replace each raw descriptor with a random set of descriptors un-der predefined probability distribution that endows the rig-orous and quantifiable differential privacy guarantee. Fur-ther, our database and clustering attack are not applicable to
LDP-FEAT, and the direct inversion attack largely fails on
LDP-FEAT, as shown in Fig. 1. We demonstrate strong per-formance in visual localization as a downstream task while enjoying the advantageous privacy guarantee.
In summary, our contributions include:
• Two novel attacks on adversarial affine subspace em-beddings [11] that enable (approximate) recovery of the original feature descriptors.
• A novel method for image feature privatization that rests on local differential privacy with favorable pri-vacy guarantees.
• Advantageous privacy-utility trade-offs achieved via empirical results to support practical applications. 2.