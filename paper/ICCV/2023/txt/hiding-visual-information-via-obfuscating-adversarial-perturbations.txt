Abstract
Growing leakage and misuse of visual information raise security and privacy concerns, which promotes the de-velopment of information protection. Existing adversar-ial perturbations-based methods mainly focus on the de-identification against deep learning models. However, the inherent visual information of the data has not been well protected.
In this work, inspired by the Type-I adversar-ial attack, we propose an Adversarial Visual Information
Hiding (AVIH) method to protect the visual privacy of data.
Specifically, the method generates obfuscating adversar-ial perturbations to obscure the visual information of the data. Meanwhile, it maintains the hidden objectives to be correctly predicted by models. In addition, our method does not modify the parameters of the applied model, which makes it flexible for different scenarios. Experimental re-sults on the recognition and classification tasks demonstrate that the proposed method can effectively hide visual infor-mation and hardly affect the performances of models. The code is available at https://github.com/suzhigangssz/AVIH. 1.

Introduction
Deep neural networks (DNNs) have been widely applied in the computer vision [26, 10, 18]. However, the increas-ing leakage and misuse of visual information has raised se-rious concerns [45, 20], especially in fields such as face
[27, 28, 51] and medicine [5, 1, 12]. A representative case is the security issue of data stored in the cloud environment
[19, 4, 35]. Due to potential vulnerabilities in cyberspace
[3], uploaded private images can be easily stolen and used maliciously [38]. Therefore, it is meaningful and urgent to explore effective strategies to protect visual information.
A classic strategy is visual information hiding, which
*Equal contributions. † Corresponding author.
Figure 1. Illustration of visual information hiding for face recog-nition systems in cloud environments. The gallery set is protected and provided to the DNN in the cloud. Protected image has quite different visual information from the original image, but it is still correctly identified. The protected gallery set can be recovered by a key model by the owner. mainly consists of two types [40]: Homomorphic Encryp-tion (HE)-based methods [2, 46, 32], Perceptual Encryption (PE)-based methods [42, 40, 39, 8, 16] and steganography method [24, 50]. Affected by the nonlinear activation func-tions in DNNs, HE-based methods are difficult to perform well on advanced DNNs [40]. Although PE-based methods apply to DNNs, existing methods typically require retrain-ing with data in the encrypted domain to guarantee accuracy on encrypted data [42, 40]. This affect the performance of service model on raw data and cause additional resource consumption (especially for large models). The steganog-raphy method can hide the visual information of sensitive data into another image, but it does not guarantee that the service model can correctly recognize the protected image.
To alleviate these negative effects, we expect to hide the visual information without making any modifications to ser-vice model. Namely, we hide sensitive visual information only by varying the input image. Previous researches have made some explorations in this regard. The transformation network-based methods [16], which try to protect the orig-inal image via a transformation function parameterized by
a neural network, share the same philosophy. However, the method cannot easily recover the original image from the protected image for other purposes. They may suffer from adversarial vulnerability [9, 14] because the introduced neu-ral network may be destroyed by adversarial attacks.
In fact, the negative effects of adversarial attacks can be utilized positively to protect privacy. Some works have ex-ploited adversarial attacks for de-identification [36, 34, 48].
These methods add imperceptible perturbations or non-suspicious patches generated by adversarial attacks to orig-inal images, hindering DNNs to extract effective features and recognize identities, thus protecting the identity pri-vacy in the image. However, in this work, we focus on visual information hiding, which means that the protected image is entirely different from the original image visually but can still be correctly predicted by DNNs (see Figure. 1).
Fortunately, we observe a special type of adversarial attack (called Type-I attack) [43] quite different from the type used in previous methods. This type of attack guides DNNs to make consistent predictions on two distinct samples.
Inspired by the Type-I adversarial attack, in this pa-per, we propose an Adversarial Visual Information Hiding (AVIH) method. The proposed method hides the visual in-formation in images while preserving their functional (e.g., recognition and classification) features for service model. It can recover original images from protected images for their owners. Specifically, we reduce the visual correlation be-tween the protected and original image while minimizing their distance in the feature space of service model, to gen-erate the protected image. Meanwhile, we exploit a genera-tive model pre-trained in a private training setting as the key model, then optimize the protected image based on it so that the recovered image is similar to the original image. Fur-thermore, to break through the tough trade-off between the capability of privacy protection and the quality of restored image, we design the variance consistency loss to enhance privacy protection without compromising image recovery (see Section. 3.3). Note that the protected image generated by our method can only be accurately recovered by the own key model, other models (even if the model architecture is the same) are difficult to recover well (see Section. 4.3).
AVIH can significantly improve the security and flexibil-ity of image storage, which is extremely obvious in the pro-tection of gallery sets for metric learning. Take the cloud-based face recognition system as an example. According to the service face recognition model, the face database man-ager can generate protected images locally or in the cloud and save the key model locally. The protected image con-tains no visual information and can be used by the service model to extract features correctly. Moreover, these pro-tected images cannot be recognized by other models. Then, these protected images can be stored in the gallery set in the cloud for normal face recognition tasks. These protected images can be recovered using the key model when needed (such as maintaining the dataset or using face images for other tasks, etc.). The process is shown in Figure. 1.
Taking visual information hiding of gallery set images for cloud-deployed face recognition systems as the basic task, we conduct a comprehensive evaluation of our pro-posed method in terms of both effectiveness and security. In order to compare with existing methods suitable for infor-mation hiding tasks, we extend our method to classification tasks. Experimental results on multiple service models and datasets show that the proposed method is effective. In ad-dition, to prove the effectiveness of our proposed loss, we conduct an ablation study about it to further present the ad-vantages of our proposed method.
Our main contributions are as follows:
• Inspired by Type-I adversarial attacks, we propose a visual information hiding method AVIH. To alleviate the difficult trade-off between capability of informa-tion hiding and quality of recovered image, we design a variance consistency loss.
• Our proposed method has following properties: 1) The visual information in the image is clearly obfuscated. 2) Our method does not require retrain. 3) The pro-tected image can be recovered by the own key model, but external models are difficult to recover.
• We validate the effectiveness of the proposed method on the face recognition task and the classification task.
In addition, we conduct qualitative and quantitative ab-lation studies to show efficiency of the proposed loss. 2.