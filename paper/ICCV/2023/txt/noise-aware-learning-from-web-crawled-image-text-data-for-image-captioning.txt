Abstract
Image captioning is one of the straightforward tasks that can take advantage of large-scale web-crawled data which provides rich knowledge about the visual world for a captioning model. However, since web-crawled data con-tains image-text pairs that are aligned at different levels, the inherent noises (e.g., misaligned pairs) make it diffi-cult to learn a precise captioning model. While the filter-ing strategy can effectively remove noisy data, it leads to a decrease in learnable knowledge and sometimes brings about a new problem of data deficiency. To take the best of both worlds, we propose a Noise-aware Captioning (NoC) framework, which learns rich knowledge from the whole web-crawled data while being less affected by the noises. This is achieved by the proposed alignment-level-controllable captioner, which is learned using alignment levels of the image-text pairs as a control signal during training. The alignment-level-conditioned training allows the model to generate high-quality captions by simply set-ting the control signal to the desired alignment level at inference time. An in-depth analysis shows the effective-ness of our framework in handling noise. With two tasks of zero-shot captioning and text-to-image retrieval using gen-erated captions (i.e., self-retrieval), we also demonstrate our model can produce high-quality captions in terms of descriptiveness and distinctiveness. The code is available at https://github.com/kakaobrain/noc. 1.

Introduction
The recent introduction of large-scale data of image-text pairs [5, 42, 20] has brought remarkable advances in com-puter vision, e.g., CLIP [38] for multi-modal representation learning and DALL·E [40] for the text-to-image generation task. This is mainly thanks to the scalability of the data col-lection process as well as the rich knowledge described in alt-texts of web-crawled data. Inspired by this, research on image captioning is also moving towards exploiting large-*These authors contributed equally.
Figure 1: Zero-shot captioning performance curve on MSCOCO when varying the ratio of noise injection to data of CC3M. To deliberately make noise data, we replace captions up to the speci-fied ratio with ones from randomly selected images in the dataset.
Models learned without consideration of noises suffer from perfor-mance degradation, even with a data filtering scheme.1 In contrast, our model is more robust to noises and provides more accurate captions, indicating the necessity for noise-aware learning. scale web-crawled image-text paired data [49, 47, 23, 54].
While web-crawled data is effective in learning rich knowledge about the visual world, it inherently suffers from noise issues as some text may be unrelated to its paired im-age. According to our observation from Fig. 1, the quality of captions generated by a standard captioning model, when learned without consideration of noises, dramatically dete-riorates as more noisy data is included during training.
One straightforward approach to tackle noises in large-scale web-crawled data is the CLIP-based filtering strat-egy [42] where image-text pairs are filtered out according to their CLIP similarity2. As shown in Fig. 1, the filtering strategy improves the quality of captions by leaving only relatively well-aligned image-text pairs for training. How-ever, in general, filtering methods without oracle criteria in-1For a fair comparison with the filtering scheme reducing the number of training samples, we train all models for the same steps rather than epochs. 2Throughout the paper, the term CLIP similarity is used to denote image-text cosine similarity calculated by the CLIP model, indicating the quality of the caption for a paired image as described in [18].
Figure 2: Examples of web-crawled image-text pairs in CC3M, where numbers within each image indicate CLIP similarity. Filtering with a threshold of 0.3, a selected threshold value on [42] after human evaluations, effectively leaves well-aligned samples (right) and removes noise samples (left). However, according to our observation, it often discards informative ones (middle) as well. evitably discard data informative for training models with
CLIP similarity below a certain threshold. Fig. 2 illustrates such unintentionally filtered cases. In addition, the inability to access filtered data reduces learnable knowledge, limiting the power of expression during caption generation. Thus, the filtering strategy may not be optimal for handling noises.
Considering the observation from Fig. 2, we argue the necessity for the noise-robust image captioning model to fully exploit web-crawled image-text data without filtering; note that, despite its importance, it is unexplored yet so far as we know. More specifically, we set our main goal to de-sign a noise-robust model so that the model 1) can generate highly-aligned captions like non-filtered samples in Fig. 2 and 2) also takes advantage of informative knowledge in the data that would be removed with filtering method.
We introduce Noise-aware Captioning (NoC) framework based on an alignment-level-controllable captioner. In the framework, we first assign alignment levels to web-crawled data by discretizing CLIP similarities of image-text pairs.
Then, the model is trained using the alignment level as an additional control signal, enabling the model to generate captions with the desired alignment level. At inference time, high-quality captions can be generated by feeding a control signal indicating the top level of alignment.
We conduct comprehensive experiments to validate the effectiveness of our model. First, from the experiments on zero-shot image captioning and self-retrieval tasks, our model outperforms comparative methods, indicating the su-perior quality of the generated captions in both descrip-tiveness and distinctiveness. Second, we observe that NoC framework enhances the pre-training→fine-tuning scheme thanks to the more advanced level of visual-language un-derstanding achieved by noise-aware pre-training. Third, we show that NoC framework provides consistent perfor-mance gain compared to the filtering strategy when scaling up dataset sizes up to 125M. Finally, we further analyze how the noise issue is addressed in the proposed method by in-vestigating the memorization effect of noisy pairs.
Our main contributions are summarized as follows:
• We propose a novel Noise-aware Captioning (NoC) framework for handling the noise issue, which is un-derexplored despite its potential importance.
• We propose an alignment-level-controllable captioner that utilizes alignment levels of data as a control signal, thus effectively addressing noise issues and being able to generate highly correlated captions by adjusting the control signal at inference time.
• We show the effectiveness of the proposed noise-aware learning through extensive experiments, where our model outperforms competing methods on both image captioning and self-retrieval tasks with large margins. 2.