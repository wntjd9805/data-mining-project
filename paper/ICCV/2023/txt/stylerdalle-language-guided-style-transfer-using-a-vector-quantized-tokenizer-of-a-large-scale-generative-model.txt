Abstract
Despite the progress made in the style transfer task, most previous work focus on transferring only relatively sim-ple features like color or texture, while missing more ab-stract concepts such as overall art expression or painter-speciﬁc traits. However, these abstract semantics can be captured by models like DALL-E or CLIP, which have been trained using huge datasets of images and textual
In this paper, we propose StylerDALLE, a documents. style transfer method that exploits both of these models and uses natural language to describe abstract art styles.
Speciﬁcally, we formulate the language-guided style trans-fer task as a non-autoregressive token sequence transla-tion, i.e., from input content image to output stylized im-age, in the discrete latent space of a large-scale pretrained vector-quantized tokenizer, e.g., the discrete variational auto-encoder (dVAE) of DALL-E. To incorporate style in-formation, we propose a Reinforcement Learning strategy with CLIP-based language supervision that ensures styl-ization and content preservation simultaneously. Experi-mental results demonstrate the superiority of our method, which can effectively transfer art styles using language in-structions at different granularities. Code is available at https://github.com/zipengxuc/StylerDALLE. 1.

Introduction
In the last few years, a lot of work has focused on the style transfer task using a reference image as a representa-tive of the target style, where the goal is to transfer the style of the reference to a content image [14, 16, 28, 3, 22, 8, 9, 40]. Recent improvements in this ﬁeld include: reduc-ing the artifacts [2, 25, 5, 40], modeling the style-content relationship [28, 43, 24], increasing the generation diver-sity [21, 37, 46] and many others. However, art styles are usually abstract concepts, e.g., pop art, fauvism, and the style of Van Gogh. To transfer these abstract art styles to a content image, the low-level features (e.g., textures and col-ors), which are commonly extracted from a single reference image, are not enough. A possible solution is to collect a set of reference images that can be used, e.g., to train a Gen-erative Adversarial Network (GAN) for artist-speciﬁc style transfer [36, 47, 19]. The disadvantage of this set-based representation is the effort required to collect sufﬁciently large style-speciﬁc data for training.
Recently, large-scale image generative models [32, 31, 35, 45] have shown their power to generate high-quality im-ages of various types, e.g., realistic photo, cartoon, ukiyo-e print, or painting of a speciﬁc artist. Moreover, CLIP [30], which is trained with 400 million text-image pairs, learns good joint knowledge between language and vision. Can we leverage the power of these large-scale models for style transfer? In this paper, we propose StylerDALLE, a language-guided style transfer method that uses both the vector-quantized tokenizer of large-scale image gener-ative methods and CLIP. There are three advantages of our method. Firstly, as compared to images, language is a more natural and efﬁcient way to describe abstract art styles and enables more ﬂexibility. Language can directly indicate art styles at different levels, e.g., “Van Gogh style oil painting” and “Van Gogh starry night style oil painting”. Secondly, using CLIP and the language description of style saves the effort of collecting style images, as CLIP already learns style-related knowledge and thus can be used to provide supervision. Thirdly, using a large-scale pretrained vector-quantized tokenizer potentially enables style transfer results to be closer to real artworks, as the model is trained from an enormous number of real-world images.
In concrete, we propose a Non-Autoregressive Transformer (NAT) model
[15], which translates tokens of a content image to tokens of a stylized image in the discrete latent space of a pretrained visual tokenizer, and a two-stage training paradigm.
First of all, since both the input content image and output stylized image can be represented by a sequence of tokens via the large-scale pretrained vector-quantized tokenizer, we formulate the language-guided style transfer task as a token sequence translation. Speciﬁcally, we design a NAT model that learns to translate a token-based representation of a low-resolution image into a full-resolution representa-tion, where the ﬁnal token sequence contains appearance details specialized for the target style. The most important advantage of using NAT in image generation, with respect to the more common autoregressive transformer (AT) based generation, is that NAT is much faster than AT at inference time, as it allows a parallel token generation while AT gen-erates in a token-by-token way.
Secondly, we propose to use CLIP-based language su-pervision to ensure stylization and content preservation si-multaneously, saving the effort to collect data and design dedicated losses. A similar solution that uses CLIP to do style transfer is CLIPStyler [20]. Since merely maximizing the CLIP similarity with respect to a textual style descrip-tion is not enough for a style transfer task, which should also preserve the content of the source image, CLIPstyler intro-duces a hybrid of losses, including a content loss measured in an external pre-trained VGG network. Conversely, we propose an alternative direction, which avoids the need to tune the coefﬁcients of different loss functions so as to keep the balance between style and content. Speciﬁcally, we in-troduce a two-stage training paradigm: 1) a self-supervised pretraining stage, where the model learns to add semanti-cally coherent image details from a low-resolution image to a high-resolution image; and 2) a style-speciﬁc ﬁne-tuning stage, where the model learns to incorporate style into the high-resolution image. Since the ﬁne-tuning phase is built on top of the ﬁrst stage, the translator is able to keep the semantic consistency with respect to the input im-age as learned during pretraining. Moreover, we create a textual prompt by concatenating both the style and the tex-tual description of the image content (i.e., its caption). This way, the prompt simultaneously models both the target ap-pearance (i.e., the style description) and the image content which should be preserved by the translation process. Fi-nally, since our translator’s output is discrete and there is no ground-truth tokens for a stylized image, we introduce a Reinforcement Learning (RL) approach to ﬁne-tune the translator using a reward based on the CLIP similarity be-tween the stylized image and textual prompt, enabling the model to explore the answers in the latent space of a pre-trained vector-quantized tokenizer.
We call our network StylerDALLE and we show that it can generate stylized images driven by different types of language guidance. Compared with previous language-guided and reference image-based transfer methods, our generated images are less inclined to produce artifacts or se-mantic errors. Moreover, they can capture abstract concepts related to the target style (e.g., the typical brushstrokes of the artist) besides low-level features like texture and colors.
We illustrate the effectiveness of our method through quali-tative results, quantitative results, and a user study.
To conclude, our main contributions are:
• We propose StylerDALLE, a language-guided style transfer method that manipulates the discrete latent space of a pretrained vector-quantized tokenizer using a token sequence translation approach.
• We propose a non-autoregressive translation network that translates a low-resolution content image into a full-resolution image with style-speciﬁc details.
• We propose a two-stage training procedure, including an RL strategy to ensure stylization and content preser-vation using CLIP-based language supervision.
• Experimental results show that StylerDALLE can ef-fectively transfer abstract style concepts, going beyond simple texture and color features while simultaneously preserving the semantic content of the translated scene. 2.