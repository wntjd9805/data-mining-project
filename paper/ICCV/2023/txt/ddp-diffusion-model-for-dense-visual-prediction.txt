Abstract
We propose a simple, efficient, yet powerful framework for dense visual predictions based on the conditional dif-fusion pipeline. Our approach follows a “noise-to-map” generative paradigm for prediction by progressively remov-ing noise from a random Gaussian distribution, guided by the image. The method, called DDP, efficiently extends the denoising diffusion process into the modern perception pipeline. Without task-specific design and architecture cus-tomization, DDP is easy to generalize to most dense pre-diction tasks, e.g., semantic segmentation and depth esti-mation. In addition, DDP shows attractive properties such as dynamic inference and uncertainty awareness, in con-trast to previous single-step discriminative methods. We show top results on three representative tasks with six di-verse benchmarks, without tricks, DDP achieves state-of-the-art or competitive performance on each task compared to the specialist counterparts. For example, semantic seg-mentation (83.9 mIoU on Cityscapes), BEV map segmenta-tion (70.6 mIoU on nuScenes), and depth estimation (0.05
REL on KITTI). We hope that our approach will serve as a solid baseline and facilitate future research. 1.

Introduction
Dense prediction tasks are the foundation of computer vision research, including a wide range of perceptual tasks such as semantic segmentation [19, 91], depth estimation
[28, 63, 67], and optical flow [26, 28]. These tasks require correctly predicting the discrete labels or continuous values for all pixels in the image, which provides detailed contex-tual understanding and enables various applications.
Numerous methods have rapidly improved the result of perception tasks over a short period of time.
In general terms, these methods can be divided into two paradigms:
∗ Equal contribution.
† Corresponding author.
Figure 1. Conditional diffusion pipeline for dense visual pre-dictions. Specifically, a conditional diffusion model is employed, where q is the forward diffusion process and pθ is the inverse pro-cess. The framework iteratively transforms the noise sample yT , drawn from a standard Gaussian distribution, into the desired tar-get prediction y0 under the guidance of the input image x. discriminative-based [27, 88, 78, 17] and generative-based
[77, 31, 35, 40, 81]. The former approach, which directly learns the mapping between input-output pairs and predicts in a single forward step, has become the current de-facto choice due to its simplicity and efficiency. Whereas, gen-erative models aim at modeling the underlying distribution of the data, conceptually having a greater capacity to han-dle challenging tasks. However, they are often restricted by complex architecture customization as well as various train-ing difficulties [60, 37, 6].
These challenges have been largely addressed by the dif-fusion and score-based models [32, 64, 68]. The solutions, based on denosing diffusion process, are conceptually sim-ple: they apply a continuous diffusion process to transform data into noise and generate new samples by simulating the time-reversed diffusion process. These methods now en-able easy training and achieve superior results on various generative tasks [50, 58, 56, 53]. Witnessing these great successes, there has been a recent surge of interest to intro-duce diffusion models to dense prediction tasks, including
Figure 2. The proposed DDP framework. The image encoder extracts feature representation from the input image x as the condition.
The map decoder takes the noisy map yt as input and produces the denoised prediction under the guidance. During training, the noisy map yt is constructed by adding Gaussian noise to the encoded ground truth. In inference, the noisy map yt is randomly sampled from the
Gaussian distribution and iteratively refined to obtain the desired prediction y0. semantic segmentation [1, 13, 75, 74] and depth estimation
[61]. However, these methods simply transfer the heavy frameworks from image generation tasks to dense predic-tion, resulting in low efficiency, slow convergence, and sub-optimal performance.
In this paper, we introduce a general, simple, yet effec-tive diffusion framework for dense visual prediction. Our method named as DDP, which extends the denoising diffu-sion process into the modern perception pipeline effectively (see Figure 2). During training, the Gaussian noise con-trolled by a noise schedule [51] is added to the encoded ground truth to obtain the noisy maps. Then these noisy maps are fused with the conditional features from the im-age encoder, e.g., Swin Transformer [45]. Finally, these fused features are fed to a lightweight map decoder to pro-duce the predictions without noise. At the inference phase,
DDP generates predictions by reversing the learned diffu-sion process, which adjusts a noisy Gaussian distribution to the learned map distribution under the guidance of the test images (see Figure 1).
Compared to previous cumbersome diffusion perception models [75, 74, 61], DDP decouples the image encoder and map decoder. The image encoder runs only once, while the diffusion process is performed only in the lightweight de-coder head. With this efficient design, our proposed method can easily be applied to modern perception tasks. Further-more, unlike previous single-step discriminative models,
DDP is capable of performing iterative inference multiple times using the shared parameters and exhibits the follow-ing appealing properties: (1) dynamic inference to trade off computation and prediction quality and (2) natural aware-ness of the prediction uncertainty.
We evaluate DDP on three representative dense predic-tion tasks, including semantic segmentation, BEV map seg-mentation, and depth estimation, using six popular datasets (ADE20K [91], Cityscapes [19], nuScenes [7], KITTI [28],
NYU-DepthV2 [63], and SUN RGB-D [67]). Our experi-mental results demonstrate that DDP significantly outper-forms existing state-of-the-art methods. Specifically, on
ADE20K, DDP achieves 46.1 mIoU with a single sampling step, which is significantly better than UperNet [76] and
K-Net [87]. On nuScenes, DDP yields an mIoU of 70.3, which is clearly better than the BEVFusion [47] baseline that achieves an mIoU of 62.7. Furthermore, by increasing the sampling steps, DDP can achieve even higher perfor-mance on both ADE20K and nuScenes, reaching anmIoU of 47.0 and 70.6, respectively. Moreover, the gains are more versatile for different model architectures as well as model sizes. DDP achieves 83.9 mIoU on Cityscapes with the ConvNeXt-L backbone and produces a leading REL of 0.05 on KITTI with the Swin-L backbone.
Overall, our contributions in this work are three-fold.
• We formulate the dense visual prediction tasks as a general conditional denoising process, with simple yet highly effective designs.
• Our “noise-to-map” generative paradigm offers several appealing properties, such as the ability to perform dy-namic inference and uncertain awareness.
• We conduct extensive experiments on three represen-tative tasks with six diverse benchmarks. The results demonstrate that our method, which we refer to as
DDP, achieves competitive performance when com-pared to previous discriminative methods. 2.