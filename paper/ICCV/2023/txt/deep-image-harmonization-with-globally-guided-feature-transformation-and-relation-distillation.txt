Abstract
Given a composite image, image harmonization aims to adjust the foreground illumination to be consistent with background. Previous methods have explored transforming foreground features to achieve competitive performance.
In this work, we show that using global information to guide foreground feature transformation could achieve sig-nificant improvement. Besides, we propose to transfer the foreground-background relation from real images to com-posite images, which can provide intermediate supervision for the transformed encoder features. Additionally, con-sidering the drawbacks of existing harmonization datasets, we also contribute a ccHarmony dataset which simulates the natural illumination variation. Extensive experiments on iHarmony4 and our contributed dataset demonstrate the superiority of our method. Our ccHarmony dataset is released at https://github.com/bcmi/Image-Harmonization-Dataset-ccHarmony. 1.

Introduction
Image composition [31, 6] is an essential editing oper-ation to combine regions from different images to produce composite images, which has a variety of vision applica-tions like advertisement propaganda and digital entertain-ment [34, 28]. Nevertheless, when pasting the foreground extracted from one image on another background image, the resultant composite image may have inconsistent illumina-tion statistics. Image harmonization [10, 29] aims to adjust the foreground illumination for visual consistency within the composite image. In recent years, deep learning based image harmonization methods [10, 36, 18, 29, 20, 15] have sprouted out and achieved remarkable progress.
Existing methods [33, 49, 11, 34, 22] have developed myriads of techniques to address the mismatch between
*Corresponding author. foreground and background illumination. Among them, some works [29, 11] revealed that specially transforming the foreground features can enhance the performance, since the main goal of image harmonization is adjusting the fore-ground. For example, [29, 17] transfer the feature statis-tics (e.g., mean, variance) from background to foreground.
[11, 36] process foreground and background features sep-arately using different channel attentions. These methods only leverage the information within the same feature map to transform the foreground features, lacking the guidance of global information. However, the global information of the whole composite image is very likely to be beneficial for foreground feature transformation.
In this work, we extract the bottleneck feature from en-coder as the global feature, which is used to guide the trans-formation of foreground features in each feature map. Re-garding the transformation manner, we opt for modulated convolution kernel proposed in [21], and other transfor-mation manners are also applicable. In particular, we use global feature to obtain the modulated convolution kernel, which is applied to the foreground features. We name our method as GiftNet (globally guided feature transformation).
It is worth noting that our method shares similar spirit with
CDTNet [9]. CDTNet uses global feature to predict color transformation, while our GiftNet uses global feature to predict feature transformation.
In practice, we observe that globally guided feature transformation is effective for decoder features, but inef-fective for encoder features. We conjecture that one rea-son is the lack of intermediate supervision for encoder fea-tures. Specifically, most of previous harmonization meth-ods [33, 49, 11, 34, 22] only use the ground-truth image as the final supervision, without any intermediate supervi-sion. In the auto-encoder based network, the encoder fea-tures may not be sufficiently harmonized, which could have negative impact on the harmonization performance. To ad-dress the above issue, we provide useful guidance for en-coder features, by distilling foreground-background relation from the encoder feature maps of harmonious images to those of composite images. In particular, we design a two-branch network, in which the top branch reconstructs the harmonious real images and the bottom branch harmonizes the inharmonious composite image. We add a distillation loss to pull close the foreground-background relation in en-coder feature maps between two branches.
Another contribution of this work is a new harmoniza-tion dataset. Training deep harmonization models requires abundant pairs of composite images and harmonious im-ages. Due to the high cost of manually harmonizing com-posite images, the prevalent way to construct harmonization dataset is the inverse approach [10, 34, 43], i.e., adjusting the foreground of real images to create synthetic compos-ite images. However, this manner may not faithfully reflect natural illumination variation, that is, the same foreground object captured under different illumination conditions. To faithfully reflect natural illumination variation, we need to capture a group of images for the same scene under varying illumination conditions, as Hday2night subdataset in iHar-mony4 [10]. However, such data collection process is ex-tremely expensive. In this work, we propose a novel way to construct harmonization dataset, aiming to simulate natural illumination variation. Specifically, we utilize ex-isting datasets [7, 12], in which each image is associated with its illumination information recorded by color checker (see Figure 2). Based on the recorded illumination infor-mation, we can transfer each image across different illumi-nation conditions to simulate natural illumination variation.
We name our dataset as color-checker harmonization (ccHa-rmony) dataset, which offers a new perspective to construct harmonization dataset.
Our contributions can be summarized as follows. 1) We propose globally guided feature transformation to adjust the foreground features, which demonstrates the importance of global guidance for foreground feature transformation; 2)
We propose to distill foreground-background relation from harmonious feature map to composite feature map, which provides useful intermediate supervision for image harmo-nization task. 3) We contribute a new dataset named ccHa-rmony to approximate natural illumination variation, which offers a new perspective for harmonization dataset construc-tion. 4) Extensive experiments on the benchmark dataset iHarmony4 [10] and our contributed dataset show that our method significantly outperforms the existing methods. 2.