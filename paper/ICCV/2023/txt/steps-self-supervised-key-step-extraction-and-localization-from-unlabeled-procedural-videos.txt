Abstract
We address the problem of extracting key steps from un-labeled procedural videos, motivated by the potential of
Augmented Reality (AR) headsets to revolutionize job train-ing and performance. We decompose the problem into two steps: representation learning and key steps extraction. We propose a training objective, Bootstrapped Multi-Cue Con-trastive (BMC2) loss to learn discriminative representa-tions for various steps without any labels. Different from prior works, we develop techniques to train a light-weight temporal module which uses off-the-shelf features for self supervision. Our approach can seamlessly leverage infor-mation from multiple cues like optical flow, depth or gaze to learn discriminative features for key-steps, making it amenable for AR applications. We finally extract key steps via a tunable algorithm that clusters the representations and samples. We show significant improvements over prior works for the task of key step localization and phase classi-fication. Qualitative results demonstrate that the extracted key steps are meaningful and succinctly represent various steps of the procedural tasks. Our code can be found at https://github.com/anshulbshah/STEPs. 1.

Introduction
Rapid shifts in technology and business models have led to a mismatch between the skills needed by employers and the skills possessed by the labor force. It has been estimated that this mismatch will reduce manufacturing output by $2.4 trillion over ten years in the US alone [31, 33]. Increased attention has been placed on effective methods of “reskilling” workers [2]. Unfortunately, reskilling will not be easy: hu-man expertise in performing a complex task takes years of training and mastery of domain-specific knowledge [10, 27].
Augmented Reality (AR) headsets can play an important role in collective reskilling efforts. AR headsets are known to improve the efficiency of front line workers during training
Figure 1. An illustrative example of automatically generated key steps using our approach STEPs for the task of changing the car-tridge in a printer. Data was captured using a Microsoft HoloLens 2 and extracted using a publicly available repository. Our approach leads to plausible key steps for sub-tasks of ‘opening cartridge lid’,
‘taking cartridge’, ‘placing cartridge’ and ‘closing cartridge lid‘ and
‘end recording’. Note that the associated labels and arrows are for visualization purposes only and were not used for training. and on the job, across industries as diverse as food service, manufacturing, medicine, and warehousing [1,14,74,76]. AR plays a strikingly similar role across these diverse use cases: to assist the user in completing a complex task, the headset renders a sequence of visual cues on real-world objects.
Our approach focuses on extracting key-steps of a com-plex task which is the most crucial component needed for automatic AR content creation. We employ a “learning-from-observation”-style framework [60], where an instructor is recorded while performing a complex task. The goal is to automatically parse the recording into key steps (KSs) that succinctly represent the complete task. This greatly stream-lines the content creation process, as the trainer no longer has to manually edit the recording to find the key steps.
Consider the task of changing the cartridge in a printer.
Using tools from the public repository [19], we captured data on a Microsoft HoloLens 2 of an “expert” undertaking this task. Using multiple cues – hand pose, head pose, eye gaze and first person video – we automatically generate the key-steps shown in Figure 1. Using multiple cues when ob-serving experts perform procedural tasks is important when generating training materials for novices [12, 28, 32].
The key step extraction problem for complex procedural
tasks is challenging: (1) Recordings of tasks performed by experts are limited in number; (2) Supervision for key steps is hard due to the subjective nature of what constitutes a key step; (3) There are no large-scale datasets for real world procedural tasks. Unlike typical web-crawled videos used in video representation learning, procedures are often minutes long; and (4) Visual information alone might not adequately represent all the information in a scene. None of the prior works address all of the challenges mentioned above.
Our solution involves a two-stage approach to the KS extraction problem. First, we train a task-specific model to produce a context-rich feature vector for each frame of the recording without any labels. To overcome paucity of data, our Multi-cue key steps (STEPs) approach, (Figure 2 and
Sec. 3), uses multiple temporal feature sequences correspond-ing to different cues as input. Different from prior works in self-supervised learning, we only train a per-modality tem-poral model which is applied on pre-extracted ‘raw’ features using a pre-trained feature extractor. To learn rich features, we propose a novel loss termed BMC2 (Bootstrapped Multi-Cue Contrastive Loss). The loss enforces temporally adapted features from each modality within the window to be close in a common latent space. Instead of using a fixed temporal window as used by related works, our loss first bootstraps a temporal window around each anchor using ‘raw’ features thus reducing the effect of false negatives. After training, the per-frame representations are clustered and KSs are sampled from the clusters. Use of off-the-shelf features enables fast temporal encoder training with extremely long temporal con-texts; we can train a model on 17 Meccano bike assembly videos [8] for 300 epochs in under 6 minutes on a single
GPU. Unlike recent works in self-supervised video repre-sentation learning [23, 37, 56], we do not rely on alignment between multiple recordings as a proxy task. Inter-video alignment as a learning loss requires multiple recordings of the same task, thus limiting its practical applicability to the problem of learning from few expert recordings.
Our approach is suitable for not only multi-modal data computed by first-person wearable devices such as Oculus
Quest, Microsoft HoloLens and others, but also conventional procedural task videos. In this case, we use the visual stream to extract rich targeted modalities like optical flow for train-ing.
We provide quantitative and qualitative results on seven datasets (both first-person and third person). Unfortunately,
KS extraction is a subjective task and there are no publicly available large-scale annotated procedural learning datasets to evaluate KSE. Thus for quantatitive benchmarking, we compare our learned representations for the proxy task of
Key Step localization (KSL). On that task, we outperform [7] by 3-18 points on F1 score. We also provide results on phase classification and outperform prior works by a wide margin.
In summary, following are our key contributions: 1. We propose an intra-video SSL-approach for proce-dure learning to train a light-weight temporal encoder with pre-extracted multi-cue features. We can train on long temporal sequences (1000s of frames) unlike prior works that train expensively large backbones. 2. We employ a novel loss, termed BMC2, that bootstraps information from pre-extracted multi-cue features to create a temporal window around an anchor, which is used to force representations from multiple modalities to be close in a common representation space. To the best of our knowledge, ours is the first work to employ multiple synchronized sensor and vision-derived cues often available in AR devices for key step extraction. 3. We obtain superior performance compared to state-of-the-art on several tasks across various datasets: KS localization, phase classification and Kendall’s Tau with a low-complexity SSL module. 2.