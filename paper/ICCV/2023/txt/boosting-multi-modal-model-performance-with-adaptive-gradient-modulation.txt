Abstract 1.

Introduction
While the field of multi-modal learning keeps growing fast, the deficiency of the standard joint training paradigm has become clear through recent studies. They attribute the sub-optimal performance of the jointly trained model to the modality competition phenomenon. Existing works attempt to improve the jointly trained model by modulating the training process. Despite their effectiveness, those meth-ods can only apply to late fusion models. More importantly, the mechanism of the modality competition remains unex-plored. In this paper, we first propose an adaptive gradient modulation method that can boost the performance of multi-modal models with various fusion strategies. Extensive ex-periments show that our method surpasses all existing mod-ulation methods. Furthermore, to have a quantitative un-derstanding of the modality competition and the mechanism behind the effectiveness of our modulation method, we in-troduce a novel metric to measure the competition strength.
This metric is built on the mono-modal concept, a function that is designed to represent the competition-less state of a modality. Through systematic investigation, our results confirm the intuition that the modulation encourages the model to rely on the more informative modality. In addi-tion, we find that the jointly trained model typically has a preferred modality on which the competition is weaker than other modalities. However, this preferred modality need not dominate others. Our code will be available at https:
//github.com/lihong2303/AGM_ICCV2023.
*These authors contributed equally.
†Corresponding author: Yi Zhou.
Recent years have seen tremendous progress in deep multi-modal learning. Despite these advances, integrating information from multiple modalities remains challenging.
Many efforts have been made to design sophisticated fu-sion methods for better performance. However, adding ad-ditional modalities only slightly improves accuracy in some multi-modal tasks. For example, trained on the CMU-MOSEI [5] dataset, the accuracy of the text-based single-modal model is only about 1% point lower than that of the multi-modal model based on both text and audio modalities.
Similar phenomena have also been observed across a wide variety of multi-modal datasets [25, 4].
Such an inefficiency in exploiting and integrating infor-mation from multiple modalities presents a great challenge to the multi-modal learning field. It is commonly believed that this inefficiency is a consequence of the existence of the dominant modality, which prevents the model from fully exploiting the other relatively weak modalities [18, 14]. Re-cent studies [1, 15, 10] theoretically investigate the training process of late fusion models and explain the production of the dominant modality with the concept of modality com-petition.
In addition to the theoretical studies, there is a group of empirical works that attempts to develop methods to modulate the training of a multi-modal model and bal-ance the learning of different modalities and, thus, achieve better performance. To our best knowledge, existing mod-ulation methods are confined to late fusion models which greatly limits their application. More importantly, little ef-fort has been paid to the study of the mechanism behind the effectiveness of those modulation methods.
Figure 1. Schematic diagram of the adaptive gradient modulation (AGM) method. Firstly, based on the full input and corresponding muted inputs, the Shapley module produces mono-modal outputs ϕm, which disentangle the responses of the multi-modal model to individual modalities. Next, ϕm are used to compute the mono-modal cross-entropy sm that reflect the amount of information in modality m. At last, sm and their running average ˆsm are fed to the Discrepancy Ratio module to compute the modulation coefficients κm for each modality, which in turn modulate the strength of corresponding gradient signals during back-propagation.
It is natural to ask Can we design a modulation method that applies to more complex fusion strategies? and Is it possible to understand the working mechanism of modu-lation in terms of modality competition? To this end, we propose an adaptive gradient modulation method, which utilizes a Shapley value-based attribution technique, that can in principle apply to any fusion strategy. Our ap-proach achieves better performance compared with the cur-rent modulation methods. Moreover, we introduce the mono-modal concept to represent the competition-less state of a modality in a multi-modal model and build a metric on top of it to directly measure the competition strength of a modality in this multi-modal model. This novel metric lay the base for us to quantitatively study the behavior of modality competition and the working mechanism of our adaptive gradient modulation method.
Our main contributions are three-fold: 1. We propose an adaptive gradient modulation method that can boost the performance of multi-modal models with various fusion strategies and justify its effective-ness through extensive experiments. 2. We introduce the mono-modal concept to capture the competition-less state of a modality and build a novel metric to measure the modality competition strength. 3. We systematically analyze the behavior of modality competition and study the mechanism of how our mod-ulation method works. 2.