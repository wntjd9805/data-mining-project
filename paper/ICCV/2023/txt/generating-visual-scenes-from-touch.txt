Abstract
An emerging line of work has sought to generate plau-sible imagery from touch. Existing approaches, however, tackle only narrow aspects of the visuo-tactile synthesis problem, and lag signiﬁcantly behind the quality of cross-modal synthesis methods in other domains. We draw on re-cent advances in latent diffusion to create a model for syn-thesizing images from tactile signals (and vice versa) and apply it to a number of visuo-tactile synthesis tasks. Using this model, we signiﬁcantly outperform prior work on the tactile-driven stylization problem, i.e., manipulating an im-age to match a touch signal, and we are the ﬁrst to success-fully generate images from touch without additional sources of information about the scene. We also successfully use our model to address two novel synthesis problems: gen-erating images that do not contain the touch sensor or the hand holding it, and estimating an image’s shading from its reﬂectance and touch. Project Page: https:// fredfyyang.github.io/vision-from-touch/ 1.

Introduction
Humans rely crucially on cross-modal associations be-tween sight and touch to physically interact with the world [57]. For example, our sense of sight tells us how the ground in front of us will feel when we place our feet on it, while our sense of touch conveys the likely visual appear-ance of an unseen object in our hand. Translating between these modalities requires an understanding of physical and material properties. Models trained to solve this problem must, for instance, learn to associate rapid changes in vi-sual shading with rough microgeometry, and smooth visual textures with soft surfaces.
Touch is arguably the most important sensory modal-ity for humans [47, 42, 39], due to its role in basic sur-vival [39, 9, 22] and physical interaction. Yet touch sens-ing has received comparably little attention in multimodal learning. An emerging line of work has addressed the prob-lem of translating touch to sight, such as by learning joint embeddings [63, 38], manipulating visual styles to match a tactile signal [63], or adding a plausible imagery of a robotic arm to an existing photo of a scene [37]. While these tasks each capture important parts of the cross-modal pre-diction problem, each currently requires a separate, special-purpose method. Existing methods also lag signiﬁcantly be-hind those of other areas of multimodal perception, which provide general-purpose methods for cross-modal synthe-sis, and can translate between modalities without the aid of extra conditional information.
In this paper, we generate plausible images of natural scenes from touch (and vice versa), drawing on recent ad-vances in diffusion models [50, 12, 20, 21, 44]. We adapt latent diffusion models to a variety of visuo-tactile synthe-sis problems. Our proposed framework obtains strong re-sults on several novel synthesis problems, and uniﬁes many previously studied visuo-tactile synthesis tasks.
First, we study the problem of generating images from touch (and vice versa). We address the task of generating images from touch without any image-based conditioning, where we are the ﬁrst method to successfully generate im-ages for natural scenes (Fig. 1a). We also address the task of adding an arm to a photo of an existing scene, where we signiﬁcantly outperform prior work [37].
Second, we address the recently proposed tactile-driven image stylization task, i.e., the problem of manipulating an image to match a given touch signal [63] (Fig. 1b), using an approach based on guided image synthesis [43]. Our ap-proach obtains results that are higher ﬁdelity and that match the tactile signal signiﬁcantly more closely than those of prior work. It also provides the ability to control the amount of image content preserved from the input image.
Finally, we show that we can augment our model with additional conditional information. Taking inspiration from the classic problem of intrinsic image decomposition [40, 3], we perform tactile-driven shading estimation, predict-ing an image after conditioning on reﬂectance and touch (Fig. 1c). Since changes in tactile microgeometry often manifest as changes in shading (i.e., the information miss-ing from reﬂectance), this tests the model’s ability to link the two signals. We also use segmentation masks to create
“hand-less” images that contain the object being pressed but not the tactile sensor or arm that pressed it.
We demonstrate our framework’s effectiveness using natural scenes from the Touch and Go dataset [63], a col-lection of egocentric videos that capture a wide variety of materials and objects using GelSight [27], and using robot-collected data from VisGel [37]. 2.