Abstract
Most model-free visual object tracking methods formu-late the tracking task as object location estimation given by a 2D segmentation or a bounding box in each video frame. We argue that this representation is limited and in-stead propose to guide and improve 2D tracking with an ex-plicit object representation, namely the textured 3D shape and 6DoF pose in each video frame. Our representation tackles a complex long-term dense correspondence problem between all 3D points on the object for all video frames, in-cluding frames where some points are invisible. To achieve that, the estimation is driven by re-rendering the input video frames as well as possible through differentiable rendering, which has not been used for tracking before. The proposed optimization minimizes a novel loss function to estimate the best 3D shape, texture, and 6DoF pose. We improve the state-of-the-art in 2D segmentation tracking on three differ-ent datasets with mostly rigid objects. 1.

Introduction
Visual object tracking is an important task in com-puter vision. A wide range of challenges [20, 21], bench-marks [22], and methods [34, 33, 49] have been proposed in recent years. Given an input video and an annotation of the object in the first frame, tracking methods localize the object with a 2D segmentation (or a 2D bounding box) in all following frames. However, they do not attempt to re-construct the 3D shape and its 6DoF motion and settle for 2D segmentation as a low-level representation of the object state. On the one hand, this makes those methods general in terms of prior assumptions on object shape and motion. On the other hand, the information extracted by the segmenta-tion is limited, e.g. to whole object editing, and does not support many applications.
In this paper, we propose a generic approach for tracking that is fundamentally different from what standard trackers perform [22, 21]. Instead of a mere 2D segmentation, we estimate the latent object 3D shape, texture, and 6DoF pose in each video frame to improve 2D tracking. This formula-t u p n
I s r u
O
. j o r p
B
G
R s i v
Sequence
Texture
Figure 1. Improving 2D tracking with explicit 3D modeling.
Starting from initial segmentations given by a tracker [49] (top row), we jointly optimize all object parameters to re-render the input video as close as possible. Long-term correspondences are visualized by colored points and their visibility by colored lines underneath. In contrast, the standard tracker reports essentially no motion (just slightly changing 2D segmentation). We use S2DNet features [8] for reprojection error (middle row) and back-project
RGB values only for visualization (bottom row). tion of tracking solves a more complex long-term dense cor-respondence problem between all 3D points on the object, even of points that are not visible (Fig. 1, colored lines).
This is useful in many applications, e.g. augmented real-ity, motion interpretation and analysis, point matching, per-pixel editing, robotics, object manipulation and grasping.
The core of the approach is to find the object shape, pose, and texture – the parameters that define the object – whose reprojection is most consistent with input video frames. The parameter estimation is driven by re-rendering the input video via differentiable rendering instead of directly pre-dicting the 2D segmentation in the next frame based on fea-tures extracted in the previous frames. This modern differ-entiable rendering way of thinking was demonstrated to be successful in various applications, such as 3D reconstruc-tion [28, 3], structure-from-motion [26], reconstruction of clothed humans [16], and deblurring of motion-blurred ob-jects [39, 40], but never in tracking. To initialize our op-timization process, we use segmentations given by a stan-dard 2D tracker. We demonstrate experimentally that the proposed tracking method improves segmentation accuracy of this initial tracker.
Another related line of research is multi-view 3D ob-ject reconstruction [5, 35, 52]. Those methods solve the task with given additional information, usually with cam-era poses (or alternatively object poses), camera calibration matrix, depth information, object segmentation, etc. More-over, these methods do not process the sequences causally and reconstruct the whole scene at once. Most methods as-sume either a perfect segmentation or objects in front of a blank background [5, 38]. They usually require distinctive texture and are trained on large datasets, in some cases even for a particular object class [38].
A third group, methods for 6DoF pose estimation [13, 14, 23], require another type of additional information as in-put – the 3D shape of the object appearing in the video [13] (or a small set of 3D shapes [27, 23]). Some methods recon-struct higher than 6DoF poses [51] or poses of articulated objects [30]. In contrast, our method requires no additional information and only uses RGB information as input. To summarize, we make the following contributions: (1) we propose the first model-free object tracking method that explicitly estimates the latent object 3D shape, tex-ture, and 6DoF motion from an RGB sequence. The tracker is aware of which part of its surface is visible, (2) we introduce differentiable rendering to tracking to op-timize object parameters to re-render the input video, (3) we introduce deep surface textures to improve tracking robustness under view-dependent illumination changes, (4) experiments demonstrate improved tracking accuracy compared to the initial tracker, more accurate 2D seg-mentation, and dense long-term correspondences on the object surface, even after self-occlusion.
It sets new state-of-the-art results on tracking datasets with rigid objects such as CDTB (part of VOT challenge), coin tracking, and In-hand Object Manipulation. 2.