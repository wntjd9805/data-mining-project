Abstract
Component 2
The embedding spaces of image models have been shown to encode a range of social biases such as racism and sex-ism. Here, we investigate specific factors that contribute to the emergence of these biases in Vision Transformers (ViT).
Therefore, we measure the impact of training data, model architecture, and training objectives on social biases in the learned representations of ViTs. Our findings indicate that counterfactual augmentation training using diffusion-based image editing can mitigate biases, but does not eliminate them. Moreover, we find that larger models are less biased than smaller models, and that models trained using discrim-inative objectives are less biased than those trained using generative objectives. In addition, we observe inconsisten-cies in the learned social biases. To our surprise, ViTs can exhibit opposite biases when trained on the same data set using different self-supervised objectives. Our findings give insights into the factors that contribute to the emergence of social biases and suggests that we could achieve substantial fairness improvements based on model design choices. 1.

Introduction
In recent studies, state-of-the-art self-supervised image models such as SimCLR [9] and iGPT [8] have been shown to encode a range of social biases, such as racism and sex-ism [34]. This can lead to representational harm [4] and ethical concerns in different socio-technical application sce-narios [41]. The distributional nature of these models is suspected to be an important factor contributing to the emer-gence of social biases, as it has been demonstrated that these models tend to encode common co-occurrences of objects associated with social biases (e. g. women are more often set in “home or hotel” scenes, whereas men are more often de-picted in “industrial and construction” scenes [36]). More-over, it has been demonstrated that self-supervised train-ing objectives can impact the distribution of social biases in models that share the same ResNet50 [14] architecture [33].
* Corresponding author.
The associated code is available at github.com/jannik-brinkmann/social-biases-in-vision-transformers. cos(fe m ale,fa m ily)
) r e e r a c
, e l e m a f ( s o c ale, c a r e e r) c o s( m family) cos(male,
Component 1
Male
Female
Career
Family
Figure 1: Gender bias in image embedding from ViTMAE: t-SNE (n=2) reveals that “female” is more closely associ-ated with “family” rather than “career”, whereas “male” has a comparable association with both attributes.
However, existing work has done little investigation into other factors that contribute to the emergence of social bi-ases in image models.
Contributions Here, we seek to better understand the fac-tors that contribute to the emergence of social biases in image models. Therefore, we investigate social biases in embedding spaces, which, despite not being observable for end-users, could propagate into downstream tasks during fine-tuning. This can help to make informed choices about the model to select for a downstream task, and to develop effective strategies to mitigate social biases. In detail, the contributions of our work are:
• Training ViTs with counterfactual data augmentation us-ing diffusion-based image editing can reduce social bi-ases, but is not sufficient to eliminate them.
• ViTs trained using discriminative objectives are less bi-ased than those trained using generative objectives.
• Scaling ViTs can help to mitigate social biases.
• ViTs can exhibit opposite biases despite being trained on the same data set, which indicates that biases are not just a result of simple object co-occurrences.
Figure 2: Selected counterfactual images on ImageNet. In each case, we show the original image (left), and the generated counterfactual image (right). 2.