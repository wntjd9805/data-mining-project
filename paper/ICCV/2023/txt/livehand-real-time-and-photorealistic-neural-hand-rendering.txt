Abstract
The human hand is the main medium through which we interact with our surroundings, making its digitization an important problem. While there are several works model-ing the geometry of hands, little attention has been paid to capturing photo-realistic appearance. Moreover, for appli-cations in extended reality and gaming, real-time rendering is critical. We present the first neural-implicit approach to photo-realistically render hands in real-time. This is a chal-lenging problem as hands are textured and undergo strong articulations with pose-dependent effects. However, we show that this aim is achievable through our carefully de-signed method. This includes training on a low-resolution rendering of a neural radiance field, together with a 3D-consistent super-resolution module and mesh-guided sam-pling and space canonicalization. We demonstrate a novel application of perceptual loss on the image space, which is critical for learning details accurately. We also show a live demo where we photo-realistically render the human hand in real-time for the first time, while also modeling pose-and view-dependent appearance effects. We ablate all our design choices and show that they optimize for rendering speed and quality. Video results and our code can be ac-cessed from https://vcai.mpi-inf.mpg.de/projects/LiveHand/ 1.

Introduction
As the popularity of VR/AR technology rises, provid-ing a natural interface with these digital contents becomes vital. Undoubtedly, hands are the most intuitive mode of in-teraction for users in a 3D environment. Therefore, it is quintessential to digitize the users’ hands to render their personalized, controllable, and photorealistic counterparts in the virtual world. Achieving this is a challenging task since hand appearance is a complex function varying with both pose and viewing direction. Moreover, ensuring real-time performance of such a system is key to enabling appli-cations such as telepresence, teleoperation, and computer-aided design.
While the creation of photorealistic hand models is pos-sible to some extent using traditional computer graphics techniques, it typically requires extensive manual efforts from experienced artists. Therefore, recent research has started to investigate whether hand models can be directly derived from 2D imagery. Here, most existing meth-ods use some data-driven explicit model to constrain the hand geometry and appearance to a low dimensional space
for the sake of tractability and robustness to occlusions
[35, 32, 24, 16, 17]. Reconstruction is then formulated as a search in this space for the best fitting parameters. Although these approaches can rapidly provide plausible results, the reconstruction is constrained to the space spanned by the registered hand mesh data used to create the model, thus limiting the visual quality and level of personalization.
More recently, neural implicit representations [23] have shown impressive results on static scenes for novel-view synthesis. Some works have extended these formulations beyond static scenes to enable photorealistic renderings of articulated objects such as the human body [38, 30, 26, 18, 28, 42, 10, 9]. Despite their successes, very little work has been done applying these ideas to hands. In contrast to bod-ies, hand motions exhibit more severe self-occlusions and more self-contact, which hinders the learning of scene rep-resentation that is consistent across different articulations.
One particular work of interest is LISA [6], which proposed a method to create neural hand avatars. Although their ap-proach shows promising results, it does not support real-time rendering during inference and the results lack high-frequency details.
In this paper, we propose the first method for creating a photorealistic neural hand avatar, which achieves real-time performance while being solely learned from segmented multi-view videos of an articulated hand and respective hand pose annotations (see Fig. 1). To this end, we intro-duce a hybrid hand model representation using the MANO hand model as a coarse proxy, which is surrounded by a neural radiance field. The idea is to simplify the learn-ing problem by bounding the learnable volume through the canonicalization of global coordinates into a texture cube.
These normalized coordinates can then be fed into a shal-low coordinate-based MLP to regress the scene color and density. This formulation can also leverage the coarse mesh proxy for more efficient sampling of a low-resolution NeRF representation of the scene; we show that this, when com-bined with a CNN-based super-resolution module carefully designed for efficient upsampling, can achieve real-time performance. Moreover, we found that our highly efficient representation allows training not only on a few ray sam-ples per iteration but on full images. Therefore, we can for the first time supervise an implicit scene representation us-ing a perceptual loss on full images during training. Again our experiments show that this greatly improves our results over the baseline, which runs perceptual supervision on a patch basis. Together, these design choices allow us to ren-der and re-enact photo-realistic hands in real-time detailed enough to capture even pose- and view-dependent appear-ance changes.
In summary, our contributions are:
• We propose LiveHand, the first method for real-time photorealistic neural hand rendering.
Methods
HTML [32]
NIMBLE [17]
LISA [6]
Ours
Real-time
✓
✓
✗
✓
Photo-real
✗
✓
✗
✓
Pose-dep. View-dep. app.
✗
✗
✓
✓ app.
✗
✗
✗
✓
Table 1: Conceptual comparison of our method with other hand-modeling approaches.
• The real-time performance is achieved with our careful combination of design choices, namely, a mesh-guided 3D sampling strategy, a low-resolution neural radiance field, and a 3D-consistent super-resolution module.
• With these computationally-efficient design choices, we for the first time demonstrate that a perceptual loss on the full image can be effectively used for super-vising implicit representations and that it out-performs the commonly used patch-based loss.
Our results demonstrate that we clearly outperform the state of the art in terms of visual quality and runtime perfor-mance. Moreover, we show a live demo of our approach, which convincingly shows the straightforward use of our method in daily life scenarios. 2.