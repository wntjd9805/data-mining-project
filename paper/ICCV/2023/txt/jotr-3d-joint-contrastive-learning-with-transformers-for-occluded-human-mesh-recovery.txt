Abstract
In this study, we focus on the problem of 3D human mesh recovery from a single image under obscured conditions.
Most state-of-the-art methods aim to improve 2D alignment technologies, such as spatial averaging and 2D joint sam-pling. However, they tend to neglect the crucial aspect of 3D alignment by improving 3D representations. Furthermore, recent methods struggle to separate the target human from occlusion or background in crowded scenes as they optimize the 3D space of target human with 3D joint coordinates as lo-cal supervision. To address these issues, a desirable method would involve a framework for fusing 2D and 3D features and a strategy for optimizing the 3D space globally. There-fore, this paper presents 3D JOint contrastive learning with
TRansformers (JOTR) framework for handling occluded 3D human mesh recovery. Our method includes an encoder-decoder transformer architecture to fuse 2D and 3D repre-sentations for achieving 2D&3D aligned results in a coarse-to-fine manner and a novel 3D joint contrastive learning approach for adding explicitly global supervision for the 3D
† Jiahao Li worked on this at his Alibaba internship.
‡ Yi Yang is the corresponding author. feature space. The contrastive learning approach includes two contrastive losses: joint-to-joint contrast for enhancing the similarity of semantically similar voxels (i.e., human joints), and joint-to-non-joint contrast for ensuring discrimi-nation from others (e.g., occlusions and background). Quali-tative and quantitative analyses demonstrate that our method outperforms state-of-the-art competitors on both occlusion-specific and standard benchmarks, significantly improving the reconstruction of occluded humans. Code is available at https://github.com/xljh0520/JOTR. 1.

Introduction
The estimation of 3D human meshes from single RGB images is an active area of research in computer vision with a broad range of applications in robotics, AR/VR, and hu-man behavior analysis. In contrast to estimating the pose of general objects [69], human mesh recovery is more chal-lenging due to the complex and deformable structure of the human body. Nevertheless, enhancing human-centric tasks can be achieved by combining visual features and prior knowledge about human anatomy through construct-ing multi-knowledge representations [66]. Generally, the
human mesh recovery task takes a single image as input and regresses human model parameters such as SMPL [46] as output.
Driven by deep neural networks, this task has achieved rapid progress [10, 21, 25, 28, 31–33, 41, 42, 56, 57, 72, 76]. Recent studies have focused on regressing accurate human meshes despite occlusions. To achieve this, most of them employ 2D prior knowledge (e.g., UV maps [76], part segmentation masks [31] and 2D human key points [28]) to focus the model on visible human body parts for enhancing the 2D alignment of the predicted mesh. Additionally, some methods [10, 57] introduce 3D representations to locate 3D joints and extract 2D features from the corresponding regions of the 2D image.
Even though the above methods have achieved signifi-cant progress in occluded human mesh recovery, they still remain constrained to these two aspects: the pursuit of 2D alignment and local supervision for 3D joints. (i) As shown in Fig. 1a, the above methods employing 2D prior knowl-edge mainly focus on 2D alignment technologies, includ-ing spatial averaging and 2D joint sampling. However, in crowded or occluded scenarios, solely focusing on 2D align-ment may acquire ambiguous features for the entire mesh due to the lack of estimation of hidden parts. Accordingly, the invisible human body parts would be aligned based on prior knowledge of the standard SMPL template, resulting in misalignment with visible parts and leading to inaccurate 3D reconstructions. (ii) Furthermore, creating a comprehensive and precise 3D representation from a single RGB image is an ill-posed problem as the inherently limited information.
As illustrated in Fig. 1b, some methods that use 3D repre-sentations rely on localized 3D joints as local supervision, ignoring the rich semantic relations between voxels across different scenes. These “local” contents (i.e., human joints) occupy only a small portion of the 3D space, while most voxels are often occupied by occlusions and background.
Consequently, the lack of explicit supervision for the entire 3D space makes it difficult to differentiate target humans from other semantically similar voxels, resulting in ambigu-ous 3D representations.
Therefore, to improve occluded human mesh recovery, we consider investigating a fusion framework that integrates 2D and 3D features for 2D&3D alignment, along with a global supervision strategy to obtain a semantically clear 3D feature space. By leveraging the complementary infor-mation from both 2D and 3D representations, the network could overcome the limitations of using only a single 2D rep-resentation, enabling obscured human parts to be detected in 3D representations and achieving 2D&3D alignment. Given a global supervision strategy, we could explicitly supervise the entire 3D space to highlight the representation of tar-get humans and distinguish them from other semantically similar voxels, resulting in a semantically clear 3D feature space.
Based on the above motivation, this paper proposes a novel framework, 3D JOint contrastive learning with TRans-formers (JOTR), for recovering occluded human mesh using a fusion of multiple representations as shown in Fig. 1a.
Unlike existing methods such as 3DCrowdNet [10] and
BEV [57] that employ 3D-aware 2D sampling techniques,
JOTR integrates 2D and 3D features through transformers
[60] with attention mechanisms. Specifically, JOTR utilizes an encoder-decoder transformer architecture to combine 3D local features (i.e., sampled 3D joint features) and 2D global features (i.e., flatten 2D features), enhancing both 2D and 3D alignment. Besides, to obtain semantically clear 3D represen-tations, the main objective is to strengthen and highlight the human representation while minimizing the impact of irrele-vant features (e.g., occlusions and background). Accordingly, we propose a new approach, 3D joint contrastive learning (in Fig. 1b), that provides global and explicit supervision for 3D space to improve the similarity of semantically similar voxels (i.e., human joints), while maintaining discrimination from other voxels (e.g., occlusions). By carefully designing 3D joint contrast for 3D representations, JOTR can mitigate the effects of occlusion and acquire semantically meaning-ful 3D representations, resulting in accurate localization of 3D human joints and acquisition of meaningful 3D joint features.
We conduct extensive experiments on both standard 3DPW benchmark [61] and occlusion benchmarks such as 3DPW-PC [56, 61], 3DPW-OC [61, 76], 3DPW-Crowd [10, 61], 3DOH [76] and CMU Panoptic [22], and JOTR achieves state-of-the-art performance on these datasets. Es-pecially, JOTR outperforms the prior state-of-the-art method 3DCrowdNet [10] by 6.1 (PA-MPJPE), 4.9 (PA-MPJPE), and 5.3 (MPJPE) on 3DPW-PC, 3DPW-OC, and 3DPW respectively. Moreover, we carry out comprehensive ab-lation experiments to demonstrate the effectiveness of our framework and 3D joint contrastive learning strategy. Our contributions are summarized as follows:
• We propose JOTR, a novel method for recovering occluded human mesh using a fusion of 2D global and 3D local features, which overcomes limitations caused by person-person and person-object occlu-sions and achieves 2D&3D aligned results.
JOTR achieves state-of-the-art results on both standard and occluded datasets, including 3DPW, 3DPW-PC, 3DPW-OC, 3DOH, CMU Panoptic, and 3DPW-Crowd.
• We develop a 3D joint contrastive learning strategy that supervises the 3D space explicitly and globally to obtain semantically clear 3D representations, minimizing the impact of occlusions and adapting to more challenging scenarios with the help of cross-image contrast.
2.