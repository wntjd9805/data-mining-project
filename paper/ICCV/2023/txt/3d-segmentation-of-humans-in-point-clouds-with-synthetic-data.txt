Abstract
Segmenting humans in 3D indoor scenes has become increasingly important with the rise of human-centered robotics and AR/VR applications. To this end, we pro-pose the task of joint 3D human semantic segmentation, instance segmentation and multi-human body-part segmen-tation. Few works have attempted to directly segment hu-mans in cluttered 3D scenes, which is largely due to the lack of annotated training data of humans interacting with 3D scenes. We address this challenge and propose a frame-work for generating training data of synthetic humans in-teracting with real 3D scenes. Furthermore, we propose a novel transformer-based model, Human3D, which is the first end-to-end model for segmenting multiple human in-stances and their body-parts in a unified manner. The key advantage of our synthetic data generation framework is its ability to generate diverse and realistic human-scene in-teractions, with highly accurate ground truth. Our exper-iments show that pre-training on synthetic data improves performance on a wide variety of 3D human segmentation tasks. Finally, we demonstrate that Human3D outperforms even task-specific state-of-the-art 3D segmentation meth-ods. 1

Introduction
In this work, we address the task of segmenting humans
∗,† indicate equal contribution.
In particular, we focus on 3D semantic in point clouds. segmentation (humans vs. background), 3D instance seg-mentation (masking multiple humans) and 3D multi-human body-part segmentation (segmenting human instances to-gether with their body parts) as shown in Fig. 1 (right).
As human-centered robotics and embodied AI are be-coming more popular, there has been a growing interest in the development of methods for 2D human segmenta-tion [11, 23, 25, 29, 81, 82, 87] and 3D human detection and segmentation [14, 36, 38, 64, 78]. While image-based meth-ods have inherent limitations in their ability to reason in 3D, existing 3D methods mainly focus on simplified sce-narios in which they only consider individual humans with pre-defined foreground segmentation masks and minimal occlusions. Real-life 3D scenarios, however, are typically cluttered, which can lead to strong occlusions when humans interact closely with each other and their environment. 3D segmentation of humans in point clouds (or depth maps) is a critical aspect of perceiving humans in vari-ous applications, such as AR/VR and robotics, in which depth sensors are commonly available and heavily used.
For such applications, using point clouds has certain advan-tages. First, point clouds provide accurate scale and ge-ometry, and are robust against illumination changes. Sec-ond, in the realm of human-related computer vision, point clouds are less biased towards visual appearance of humans.
This can improve model fairness, and ensures better privacy when collecting data of real humans.
Although there have been significant advancements in 3D scene understanding methods that operate directly on point clouds and segment indoor objects [15, 56, 63, 68], these advancements have not yet translated to the task of 3D human segmentation due to a lack of annotated humans in popular 3D indoor training datasets [1, 8, 16]. These in-door datasets usually lack diverse scenarios involving in-teractions between humans and cluttered real-world indoor environments. While outdoor datasets [4, 6] provide labels for pedestrians, they are limited in terms of human poses, actions, and occlusion patterns, making them less practical for indoor applications where humans closely interact with their surroundings. More recently, new datasets (BEHAVE
[5], RICH [34], EgoBody [84]) provide depth recordings of humans interacting with their surroundings and other peo-ple. They are labeled with pseudo-ground truth human body meshes [49, 58] via multi-view registration processes rely-ing on image segmentation and manual cleaning. To facili-tate the labeling process, these datasets are often limited in terms of scene complexity, the number of people and poses, as well as occlusion and truncation patterns. Nevertheless, while tedious to annotate, these datasets can serve as realis-tic pseudo-labels for training 3D human segmentation tasks.
The key issue of recording and labeling real humans in complex indoor scenes is the time-consuming annotation process and thus its limited scalability. A promising alter-native is synthesizing virtual humans as training data. Syn-thetic training data contains perfect labels that are impossi-ble to annotate manually, and the creators have full control over dataset variation and diversity. Compared to gener-ating color images, where it is challenging to render photo-realistic humans [77], generating depth scans of 3D humans in 3D scenes is significantly easier, as the domain gap be-tween real and synthetic point clouds is much smaller.
In this work, we describe a framework for synthesizing virtual humans in realistic environments, and show that it is possible to create synthetic training data that helps to improve 3D human segmentation in-the-wild. In addition, we propose a novel transformer-based model, called Hu-man3D, that performs a wide variety of 3D human seg-mentation tasks in a unified manner. Human3D is the first model that directly addresses 3D multi-human body-part segmentation in point clouds of realistic environments. Hu-man3D relies on a novel mechanism using two-level queries to jointly segment human instance masks and their asso-ciated body parts. Our experiments consistently demon-strate that pre-training models with synthetic data and fine-tuning with real data yields significant improvements over models trained exclusively on real data. Furthermore, our
Human3D model trained for multi-human body-part seg-mentation achieves superior performance compared to task-specific state-of-the-art models for both 3D semantic and instance segmentation.
In summary, our contributions are as follows:
• Human3D, the first multi-human body-part segmenta-tion model, that operates directly on real-world clut-tered indoor 3D scenes.
• An approach for generating synthetic data of humans in 3D scenes and its use for synthesizing training data to improve 3D human segmentation.
• Manual annotation of 3D human instances on
EgoBody [84] to evaluate human segmentation tasks.
• Extensive analysis showing the benefits of pre-training on synthetic data on multiple baselines and tasks. 2