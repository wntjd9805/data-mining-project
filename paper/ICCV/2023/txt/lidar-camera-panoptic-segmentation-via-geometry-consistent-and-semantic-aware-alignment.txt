Abstract 3D panoptic segmentation is a challenging perception task that requires both semantic segmentation and instance segmentation.
In this task, we notice that images could provide rich texture, color, and discriminative information, which can complement LiDAR data for evident performance improvement, but their fusion remains a challenging prob-lem. To this end, we propose LCPS, the first LiDAR-Camera
Panoptic Segmentation network. In our approach, we con-duct LiDAR-Camera fusion in three stages: 1) an Asyn-chronous Compensation Pixel Alignment (ACPA) module that calibrates the coordinate misalignment caused by asyn-chronous problems between sensors; 2) a Semantic-Aware
Region Alignment (SARA) module that extends the one-to-one point-pixel mapping to one-to-many semantic rela-tions; 3) a Point-to-Voxel feature Propagation (PVP) mod-ule that integrates both geometric and semantic fusion in-formation for the entire point cloud. Our fusion strategy improves about 6.9% PQ performance over the LiDAR-only baseline on NuScenes dataset. Extensive quantitative and qualitative experiments further demonstrate the effective-ness of our novel framework. The code will be released at https://github.com/zhangzw12319/lcps.git. 1.

Introduction 3D scene perception has become an increasingly impor-tant task for a wide range of applications, including self-driving and robotic navigation. Lying in the heart of 3D vi-sion, 3D panoptic segmentation is a comprehensive percep-tion task composed of semantic and instance segmentation
[15]. This is still challenging since it not only requires pre-dicting semantic labels of each point for Stuff classes, such as tree, road, but also needs recognizing instances for Thing classes, e.g., car, bicycle, and pedestrian simultaneously.
*Corresponding authors; † equal contributions
Figure 1. The distinctions between LiDAR point cloud and im-ages. (a) The red box displays a vehicle segment (orange points) in the point cloud, where points are sparsely and unevenly dis-tributed. (b) The lower-right green mask demonstrates a vehicle with dense texture and color features, effectively detected via [40].
The upper-left blue mask (partly occluded) shows image features that help detect small objects in the distance. Better zoomed in.
Currently, the leading 3D panoptic methods use LiDAR-only data as input sources. However, We have observed that using only LiDAR data for perception has some insufficien-cies: 1) LiDAR point cloud is usually sparse and unevenly distributed, as illustrated in Figure 1 (a), making it chal-lenging for 3D networks to capture the notable difference between the foreground and the background; 2) distant ob-jects that occupy just a few points appear to be small in the view and cannot be effectively detected. On the con-trary, images provide rich texture and color information, as shown in Figure 1 (b). This observation motivates us to use images as an additional input source to complement LiDAR sensors for scene perception. Moreover, most autonomous driving systems come equipped with RGB cameras, which makes LiDAR-Camera fusion studies more feasible.
Although LiDAR sensors and cameras complement each other, their fusion strategy remains challenging. Existing fusion strategy could be generally split into proposal-level fusion [16], result-level fusion [27], and point-level fusion
[33, 12, 34], as summarized in PointAugmenting [35]. Yet, proposal-level and result-level fusion focus on integrating
2D and 3D proposals (or bounding box results) for ob-ject detection, which limits their generalizability in dense predictions like segmentation tasks. The previous point-fusion methods also suffer: 1) the asynchronous working frequency between LiDAR and camera sensors is not con-sidered, which may result in misaligned feature correspon-dence; 2) point-fusion is a one-to-one fusion mechanism, and large image areas are unable to be mapped to sparse
LiDAR points, resulting in the waste of abundant informa-tion from dense pixel features; e.g., for a 32-beams LiDAR sensor, only about 5% pixels can be mapped to correlated points, while the 95% of pixel features would be dropped
[23]. 3) previous point-level fusion methods [33, 12, 34] of-ten use simple concatenation, which excludes points whose projections fall outside the image plane, as image features cannot support them.
Motivated by these insufficiencies, we propose the first
LiDAR-Camera Panoptic Segmentation (LCPS) network to exploit the complementary information from multiple sensors.
In this work, we propose a novel three-stage fusion strategy involving the Asynchronous Compensa-tion Pixel Alignment (ACPA) module, Semantic-Aware
Region Alignment (SARA) module, and Point-to-Voxel feature Propagation (PVP) module. The ACPA module employs ego-motion compensation operations to achieve spatial-temporal alignment between the LiDAR and cam-era modalities, overcoming asynchronous issues in point fusion. Then, our novel SARA module extends the one-to-one point-pixel mapping to one-to-many semantic rela-tions, highly improving the image utilization rate. Specifi-cally, SARA introduces Class Activation Maps (CAM) for image branch to localize semantic-related image regions for each point. Next, the PVP module replaces simple concate-nation with local attention to propagate information from point-aligned pixels and regions to the entire point cloud.
Points outside camera frustums can also be preserved and attached to image features. Finally, we design a Foreground
Object selection Gate (FOG) module to enforce the network to learn a class-agnostic foreground object mask in addition to the semantic prediction head. This gate effectively re-duces incorrect predictions and stabilizes the training pro-cess. To sum up, our main contributions are:
• To the best of our knowledge, this is the first LiDAR-Camera fusion network for 3D panoptic segmentation, which effectively exploits the complementary informa-tion of the LiDAR and image data.
• We have improved the former point-fusion approach with our novel Asynchronous Compensation Pixel
Alignment (ACPA), Semantic-Aware Region Align-ment (SARA), and Point-to-Voxel feature Propagation (PVP) modules. These contribute to the geometry-consistent and semantic-aware alignment between Li-DAR and Camera sensors.
• We present the Foreground Object selection Gate (FOG) to reduce the incorrect predictions of confusing points, further boosting panoptic segmentation quality.
• Extensive quantitative and qualitative experiments demonstrate the effectiveness of our approach. Our fu-sion approach improves performance at 6.9% PQ on
NuScenes and 3.3% PQ in SemanticKITTI compared to the LiDAR-only baseline. 2.