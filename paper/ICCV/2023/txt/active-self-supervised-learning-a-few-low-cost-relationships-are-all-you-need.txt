Abstract
Self-Supervised Learning (SSL) has emerged as the so-lution of choice to learn transferable representations from unlabeled data. However, SSL requires to build sam-ples that are known to be semantically akin, i.e. posi-tive views. Requiring such knowledge is the main lim-itation of SSL and is often tackled by ad-hoc strategies e.g. applying known data-augmentations to the same in-put. In this work, we formalize and generalize this princi-ple through Positive Active Learning (PAL) where an ora-cle queries semantic relationships between samples. PAL achieves three main objectives. First, it unveils a theoreti-cally grounded learning framework beyond SSL, based on similarity graphs, that can be extended to tackle supervised and semi-supervised learning depending on the employed oracle. Second, it provides a consistent algorithm to em-bed a priori knowledge, e.g. some observed labels, into any SSL losses without any change in the training pipeline.
Third, it provides a proper active learning framework yield-ing low-cost solutions to annotate datasets, arguably bring-ing the gap between theory and practice of active learning that is based on simple-to-answer-by-non-experts queries of semantic relationships between inputs. 1.

Introduction
Learning representations of data that can be used to solve multiple tasks, out-of-the-box, and with minimal post-processing is one of the main goals of current AI research
[39, 35, 24]. Such representations are generally found by processing given inputs through Deep neural Networks (DNs). The main question of interest around which con-temporary research focuses on deals with the choice of the training setting that is employed to tune the DN’s parame-ters. A few different strategies have emerged such as lay-erwise [6], reconstruction based [48], and more recently, based on Self-Supervised Learning (SSL) [14, 37]. In fact,
*Equal contribution. due to the cost of labeling and the size of datasets constantly growing, recent methods have tried to drift away from tra-ditional supervised learning [42]. From existing training solutions, joint-embedding SSL has emerged as one of the most promising ones [34]. It consists in learning represen-tations that are invariant along some known transformations while preventing dimensional collapse of the representa-tion. Such invariance is enforced by applying some known
Data-Augmentation (DA), e.g. translations for images, to the same input and making sure that their corresponding representations are the same.
Despite tremendous progress, several limitations remain in the way of a widespread deployment of SSL. In particu-lar, it is not clear how to incorporate a priori knowledge into
SSL frameworks beyond the usual tweaking of the loss and
DAs being employed, although some efforts are being made
[16, 53, 52]. Indeed, it is not surprising that vision-language pre-training has replaced SSL as the state-of-the-art to learn image representation [26], as those models are better suited to incorporate information stemming from captions that of-ten come alongside images collected on the Internet.
In this study, we propose to redefine existing SSL losses in terms of a similarity graph –where nodes represent data samples and edges reflect known inter-sample relationships.
Our first contribution stemming from this formulation pro-vides a generic framework to think about learning in terms of similarity graph: it yields a spectrum on which SSL and supervised learning can be seen as two extremes. Within this realm, those two extremes are connected through the similarity matrix, and in fact can be made equivalent by varying the similarity graph. Our second contribution natu-rally emerges from using such a similarity graph, unveiling an elegant framework to reduce the cost and expert require-ment of active learning summarized by:
Tell me who your friends are, and I will tell you who you are.
Active learning, which aims to reduce supervised learning cost by only asking an oracle for sample labels when needed
[41, 20, 27, 31], can now be formulated in term of relative sample comparison, rather than absolute sample labeling.
Active Learning
Given an input, return its label e.g. for Imagenet:
- Tinca tinca
- Carassius auratus
- Carcharodon carcharias
- ... query1 query2
Positive Active Learning (PAL)
Given inputs, choose if they are semantically related: yes/no query1 query2 and and
) a h c t p a c e r ( r o response1 aligator response2 aligator
Expansive oracle (expert knowledge) response1 yes response2 no
Low-cost relationships information (reduced expertise)
Figure 1. Active Self-Supervised Learning introduces PAL (right box), an alternative to active learning (left box) where the oracle is asked if a collection of inputs are semantically related or not. As opposed to active learning, expert knowledge is reduced as one need not to know all the possible classes but only how to distinguish inputs from different classes. PAL querying is flexible, as an illustrative example we exhibit an `a la captcha version where a given input is presented along with a collection of other inputs, and the oracle can select among those inputs the positive ones.
This much more efficient and low-cost approach is exactly the active learning strategy stemming from our framework: rather than asking for labels, one rather asks if two (or more) inputs belong to the same classes or not, as depicted in
Fig. 1. We coin such a strategy as Positive Active Learning (PAL), and we will present some key analysis on the bene-fits of PAL over traditional active learning. We summarize our contributions below:
• We provide a unified learning framework based on the concept of similarity graph, which encompasses both self-supervised learning, supervised learning, as well as semi-supervised learning and many variants.
• We derive a generic PAL algorithm based on an oracle to query the underlying similarity graph Algorithm 1.
The different learning frameworks (SSL, supervised, and so forth) are recovered by different oracles, who can be combined to benefit from each framework dis-tinction.
• We show how PAL extends into an active learn-ing framework based on similarity queries that pro-vides low-cost efficient strategies to annotate a dataset (Fig. 1).
All statements of this study are proven in Appendix B, code to reproduce experiments is provided at https:// github.com/VivienCabannes/rates. 2.