Abstract
Positive-unlabeled learning (PU learning) in hyperspec-tral remote sensing imagery (HSI) is aimed at learning a bi-nary classifier from positive and unlabeled data, which has broad prospects in various earth vision applications. How-ever, when PU learning meets limited labeled HSI, the un-labeled data may dominate the optimization process, which makes the neural networks overfit the unlabeled data. In this paper, a Taylor variational loss is proposed for HSI
PU learning, which reduces the weight of the gradient of the unlabeled data by Taylor series expansion to enable the network to find a balance between overfitting and underfit-ting. In addition, the self-calibrated optimization strategy is designed to stabilize the training process. Experiments on 7 benchmark datasets (21 tasks in total) validate the ef-fectiveness of the proposed method. Code is at: https:
//github.com/Hengwei-Zhao96/T-HOneCls. 1.

Introduction
Positive-unlabeled learning is aimed at learning a binary classifier from positive and unlabeled data [21, 17, 3]. Due to the lack of negative samples, PU learning is a challenging task, but play an important role in machine learning appli-cations, including product recommendation [16], deceptive reviews detection [30], and medical diagnosis [39].
PU learning in HSI is a powerful tool for environmental monitoring [43, 23]. For example, when mapping the in-vasive species in complex forestry, PU learning only needs positive labels of invasive species; however, traditional hy-perspectral classification [19, 37, 46] requires the vari-ous negative classes to be labeled to obtain a discriminate boundary, which is labor-intensive, even impossible, to in-vestigate the negative objects and annotate them in high species richness areas [43].
Few releated works have focused on PU learning in HSI.
Compared to other tasks, the training data size in HSI is
*Corresponding author. much smaller [9], and the deep models are more likely to be over-fitting and susceptible to unalabeled data. These characteristics make hyperspectral PU learning a more chal-lenging task.
PU learning methods can be divided into two categories, according to whether the class prior (πp, i.e., the proportion of positive data) is assumed to be known. (1) Due to the limited supervision information from PU data, most stud-ies assume that the class prior is available [43, 23], but in reality, the class prior is hard to be estimated accurately, especially for HSIs, due to the severe inter-class similar-ity and intra-class variation. (2) Class prior-free PU learn-ing is a recent research focus of the machine learning com-munity [3, 17], where variational principle-based PU learn-ing [3] is one of the state-of-the-art in theory. It approxi-mates the positive distribution by optimizing the posterior probability, i.e., the classifier, and does not require knowing the class prior. However, the unlabeled data may dominate the optimization process, which makes it difficult for neu-ral networks to find a balance between the underfitting and overfitting of positive data, especially when the variational principle meets limited labeled HSI data (discussed later in
Section 3 in detail).
In this paper, a Taylor series expansion-based variational framework—T-HOneCls—is proposed to solve the limited labeled hyperspectral PU learning problem without class prior. The contributions of this paper are summarized as follows:
• A novel insight is proposed in terms of the dynamic change of the loss, which demonstrates that the unla-beled data dominating the training process is the bot-tleneck of the variational principle-based classifier.
• Taylor variational loss is proposed to tackle the prob-lem of PU learning without a class prior, which reduces the weight of the gradient of the unlabeled data and si-multaneously satisfy the variational principle by Tay-lor series expansion, to alleviate the problem of unla-beled data dominating the training process.
• Self-calibrated optimization is proposed to take advan-tage of the supervisory signals from the network itself to stabilize the training process and alleviate the poten-tial over-fitting problem caused by limited labeled data with a large pool of unlabeled data.
• Extensive experiments are conducted on 7 benchmark datasets, including 5 hyperspectral datasets (19 tasks in total), CIFAR-10 and STL-10, where the proposed method outperforms other state-of-the-art methods in most cases. 2.