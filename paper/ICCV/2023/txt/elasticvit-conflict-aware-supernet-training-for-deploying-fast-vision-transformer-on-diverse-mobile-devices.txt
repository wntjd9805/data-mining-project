Abstract
Neural Architecture Search (NAS) has shown promising performance in the automatic design of vision transformers (ViT) exceeding 1G FLOPs. However, designing lightweight and low-latency ViT models for diverse mobile devices re-mains a big challenge. In this work, we propose ElasticViT, a two-stage NAS approach that trains a high-quality ViT supernet over a very large search space for covering a wide range of mobile devices, and then searches an optimal sub-network (subnet) for direct deployment. However, current supernet training methods that rely on uniform sampling suffer from the gradient conflict issue: the sampled sub-nets can have vastly different model sizes (e.g., 50M vs. 2G
FLOPs), leading to different optimization directions and infe-rior performance. To address this challenge, we propose two novel sampling techniques: complexity-aware sampling and performance-aware sampling. Complexity-aware sampling limits the FLOPs difference among the subnets sampled across adjacent training steps, while covering different-sized subnets in the search space. Performance-aware sampling further selects subnets that have good accuracy, which can reduce gradient conflicts and improve supernet quality. Our discovered models, ElasticViT models, achieve top-1 accu-racy from 67.2% to 80.0% on ImageNet from 60M to 800M
FLOPs without extra retraining, outperforming all prior
CNNs and ViTs in terms of accuracy and latency. Our tiny and small models are also the first ViT models that surpass state-of-the-art CNNs with significantly lower latency on mo-bile devices. For instance, ElasticViT-S1 runs 2.62× faster than EfficientNet-B0 with 0.1% higher accuracy.
*Equal contribution
§Work was done during the internship at Microsoft Research
‡Corresponding author (lzhani@microsoft.com)
Figure 1: We train a high-quality ViT supernet for a wide range of mobile devices. Our discovered ViTs outperform SOTA CNNs and
ViTs with higher accuracy, fewer FLOPs and faster speed. 1.

Introduction
Vision Transformers (ViTs) have achieved remarkable success in various computer vision tasks [14, 31, 51, 5, 64].
However, the success comes at a significant cost - ViTs are heavy-weight and have high inference latency costs, posing a great challenge to bring ViTs to resource-limited mobile devices [56]. Designing accurate and low-latency ViTs be-comes an important but challenging problem.
Neural Architecture Search (NAS) provides a powerful tool for automating efficient DNN design. Recently, two-stage NAS such as BigNAS [59] and AutoFormer [7], de-couple training and searching process and achieves remark-able search efficiency and accuracy. The first stage trains a weight-shared supernet assembling all candidate architec-tures in the search space, and the second stage uses typical search algorithms to find best sub-networks (subnets) under various resource constraints. The searched subnets can di-rectly inherit supernet weights for deployment, achieving comparable accuracy to those retrained from scratch. Such two-stage NAS can eliminate the prohibitively expensive cost for traditional NAS to retrain each subnet, making it a practical approach for efficient deployment.
The success of two-stage NAS heavily relies on the qual-ity of the supernet training in the first stage. However, it’s extremely challenging to train a high-quality ViT supernet
for mobile devices, due to the vast mobile diversity: mobile applications must support a wide range of mobile phones with varying computation capabilities, from the latest high-end devices to older ones with much slower CPUs. For instance, Google Pixel 1 runs 4× slower than Pixel 6. As a result, the supernet must cover ViTs that range from tiny size (< 100M FLOPs) for weak devices to large size for strong ones. However, including both tiny and large ViTs results in an overwhelmingly larger search space compared to typical search spaces in two-stage NAS [3, 59, 15]. Training a super-net over such a search space has been known to suffer from performance degradation due to optimization interference caused by subnets with vastly different sizes [11, 60, 63].
While existing works [28, 62, 7, 47] circumvent this issue by manually designing multiple separate normal-sized search spaces, the multi-space approach can be costly. Moreover, there has been limited discussion regarding the root causes of this problem and how to effectively address it.
In this work, we introduce ElasticViT, a novel approach for training a high-quality vision transformer supernet that can efficiently serve both strong and weak mobile devices.
Our approach is built upon a single very large search space optimized for mobile devices, containing a wide range of vi-sion transformers with sizes ranging from 37M to 3G FLOPs.
This search space is 107× larger than typical two-stage NAS search spaces, allowing us to accommodate a broad range of mobile devices with various resource constraints.
We start by investigating the root causes of poor per-formance when training a ViT supernet over our exces-sively large search space. We found that the main reason is that prior supernet training methods rely on uniform sam-pling [7, 15, 59, 3], which can easily sample subnets with vastly different model sizes (e.g.,50M vs. 1G FLOPs) from our search space. This leads to conflicts between subnets’ gradients and creates optimization challenges. We make two key observations: (i) the gradient conflict between two subnets increases with the FLOPs difference between them; and (ii) gradient conflict between same-sized subnets can be significantly reduced if they are good subnets.
Inspired by the above observations, we propose two key techniques to address the gradient conflict issue. First, we propose complexity-aware sampling to limit the difference in FLOPs between sampled subnets across adjacent training steps, while ensuring that different-sized subnets within the search space are sampled. We achieve this by constraining the FLOPs level of the sampled subnets to be close to that of the previous step. Furthermore, we employ a multiple-min strategy to sample the nearest smallest subnet based on the
FLOPs sampled at each step, thus ensuring performance bounds without introducing a large FLOPs difference with other subnets. Second, we introduce performance-aware sampling that further reduces the gradient conflicts among subnets with similar FLOPs. Our method samples subnets with higher potential accuracy at each step from a prior distri-bution that is dynamically updated based on an exploration and exploitation policy. The policy leverages a memory bank and a ViT architecture preference rule. The preference rule guides the exploration of subnets with wider width and shallower depth, which are empirically preferred by ViT ar-chitectures. The memory bank stores historical good subnets for each FLOPs level using prediction loss as a criterion.
Our contributions are summarized as follows:
• We propose ElasticViT to automate the design of accu-rate and low-latency ViTs for diverse mobile devices.
For the first time we are able to train a high-quality ViT supernet over a vast and mobile-regime search space.
• We conduct thorough analysis on the poor-quality su-pernet trained by existing approaches, and find that uniform sampling results in subnets of vastly different sizes, leading to gradient conflicts.
• Inspired by our analysis, we propose two methods, complexity-aware sampling and performance-aware sampling, to effectively address the gradient issues by sampling good subnets and limiting their FLOPs differ-ences across adjacent training steps.
• Extensive experiments on ImageNet [12] and four mo-bile devices demonstrate that our discovered models achieve significant improvements over SOTA efficient
CNN and ViT models in terms of both inference speed and accuracy. For example, ElasticViT-T3 achieves the same accuracy of 75.2% as MobileNetV3 with only 160 MFLOPs, while be 1.2× faster. This is the first time that ViT outperforms CNN with a faster speed on mobile devices within the 200 MFLOPs range, to the best of our knowledge. ElasticViT-L achieves 80.0% accuracy with 806 MFLOPs, which is 5.3% higher than
Autoformer-Tiny [7] while using 1.61× fewer FLOPs.
We also prove that ElasticViT substantially enhance the quality of supernet training, resulting in a noteworthy 3.9% accuracy improvements for best-searched models. 2.