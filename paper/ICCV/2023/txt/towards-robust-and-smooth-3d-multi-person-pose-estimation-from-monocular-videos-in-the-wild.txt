Abstract 3D pose estimation is an invaluable task in computer vision with various practical applications. Especially, 3D pose estimation for multi-person from a monocular video (3DMPPE) is particularly challenging and is still largely uncharted, far from applying to in-the-wild scenarios yet.
We pose three unresolved issues with the existing meth-ods: lack of robustness on unseen views during training, vulnerability to occlusion, and severe jittering in the out-put. As a remedy, we propose POTR-3D, the ﬁrst realiza-tion of a sequence-to-sequence 2D-to-3D lifting model for 3DMPPE, powered by a novel geometry-aware data aug-mentation strategy, capable of generating unbounded data with a variety of views while caring about the ground plane and occlusions. Through extensive experiments, we verify that the proposed model and data augmentation robustly generalizes to diverse unseen views, robustly recovers the poses against heavy occlusions, and reliably generates more natural and smoother outputs. The effectiveness of our approach is veriﬁed not only by achieving the state-of-the-art performance on public benchmarks, but also by qualita-tive results on more challenging in-the-wild videos. Demo videos are available at https://www.youtube.com/@potr3d. 1.

Introduction 3D pose estimation aims to reproduce the 3D coordinates of a person appearing in an untrimmed 2D video.
It has been extensively studied in literature with many real-world applications, e.g., sports [3], healthcare [38], games [17], movies [1], and video compression [36]. Instead of fully rendering 3D voxels, we narrow down the scope of our dis-cussion to reconstructing a handful number of body key-points (e.g., neck, knees, or ankles), which concisely repre-sent dynamics of human motions in the real world.
Depending on the number of subjects, 3D pose esti-mation is categorized into 3D Single-Person Pose Esti-mation (3DSPPE) and 3D Multi-Person Pose Estimation
*Corresponding author (3DMPPE). In this paper, we mainly tackle 3DMPPE, re-producing the 3D coordinates of body keypoints for every-one appearing in a video. Unlike 3DSPPE that has been extensively studied and already being used for many appli-cations, 3DMPPE is still largely uncharted whose models are hardly applicable to in-the-wild scenarios yet. For this reason, we pose 3 unresolved issues with previous models.
First, existing models are not robustly applicable to un-seen views (e.g. unusual camera angle or distance). Trained on a limited amount of data, most existing models per-form well only on test examples captured under similar views, signiﬁcantly underperforming when applied to un-seen views. Unfortunately, however, most 3D datasets pro-vide a limited number of views (e.g., 4 for Human 3.6M [12] or 14 for MPI-INF-3DHP [24]), recorded under limited conditions like the subjects’ clothing or lighting due to the high cost of motion capturing (MoCap) equipment [8].
This practical restriction hinders learning a universally ap-plicable model, failing to robustly generalize to in-the-wild videos.
Second, occlusion is another long-standing challenge that most existing models still suffer from. Due to the in-visible occluded keypoints, there is unavoidable ambiguity since there are multiple plausible answers for them. Occlu-sion becomes a lot more severe when a person totally blocks another from the camera, making the model output incon-sistent estimation throughout the frames.
Third, the existing methods often produce a sequence of 3D poses with severe jittering. This is an undesirable byproduct of the models, not present in the training data.
In this paper, we propose a 3DMPPE model, called
POTR-3D, that works robustly and smoothly on in-the-wild videos with severe occlusion cases. Our model is pow-ered by a novel data augmentation strategy, which gener-ates training examples by combining and adjusting existing single-person data, to avoid actual data collection that re-quires high cost. For this augmentation, we decouple the core motion from pixel-level details, as creating a realistic 3D video at the voxel level is not a trivial task. Thereby, we adopt the 2D-to-3D lifting approach, which ﬁrst de-tects the 2D body keypoints using an off-the-shelf model and trains a model to lift them into the 3D space. Beneﬁt-ing from a more robust and generalizable 2D pose estima-tor, 2D-to-3D lifting approaches have been successfully ap-plied to 3DSPPE [29, 41]. Observing that we only need the body keypoints, not the pixel-level details, of the subjects for training, we can easily generate an unlimited number of 2D-3D pairs using given camera parameters under various conditions, e.g., containing arbitrary number of subjects un-der various views. Translating and rotating the subjects as well as the ground plane itself, our augmentation strategy makes our model operate robustly on diverse views.
To alleviate the occlusion problem, we take a couple of remedies. The main reason that existing models suffer from occlusion is that they process a single frame at a time (frame2frame). Following a previous work, MixSTE [41], which effectively process multiple frames at once (seq2seq) for 3DSPPE, our POTR-3D adopts a similar Transformer-based 2D-to-3D structure, naturally extending it to multi-person. Lifting the assumption that there is always a single person in the video, POTR-3D tracks multiple people at the same time, equipped with an additional self-attention across multiple people appearing in the same frame. We infer the depth and relative poses in a uniﬁed paradigm, helpful for a comprehensive understanding of the scene. To the best of our knowledge, POTR-3D is the ﬁrst seq2seq 2D-to-3D lift-ing method for 3DMPPE. Alongside, we carefully design the augmentation method to reﬂect occlusion among peo-ple with a simple but novel volumetric model. We gener-ate training examples with heavy occlusion, where expected outputs of off-the-shelf models are realistically mimicked, and thus the model is expected to learn from these noisy examples how to conﬁdently pick useful information out of confusing situations.
The seq2seq approach also helps the model to reduce jit-tering, allowing the model to learn temporal dynamics by observing multiple frames at the same time.
In addition, we propose an additional loss based on MPJVE, which has been introduced to measure temporal consistency [29], to further smoothen the prediction across the frames.
In summary, we propose POTR-3D, the ﬁrst realization of a seq2seq 2D-to-3D lifting model for 3DMPPE, and de-vise a simple but effective data augmentation strategy, al-lowing us to generate an unlimited number of occlusion-aware augmented data with diverse views. Putting them together, our overall methodology effectively tackles the aforementioned three challenges in 3DMPPE and adapts well to in-the-wild videos. Speciﬁcally,
• Our method robustly generalizes to a variety of views that are unseen during training, overcoming the long-standing data scarcity challenge.
• Our approach robustly recovers the poses in situations with heavy occlusion.
• Our method produces a more natural and smoother se-quence of motion compared to existing methods.
Trained on our augmented data, POTR-3D outperforms ex-isting methods both quantitatively on several representative benchmarks and qualitatively on in-the-wild videos. 2.