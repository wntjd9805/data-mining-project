Abstract
Explanation supervision is a technique in which the model is guided by human-generated explanations during training. This technique aims to improve the predictability of the model by incorporating human understanding of the prediction process into the training phase. This is a chal-lenging task since it relies on the accuracy of human an-notation labels. To obtain high-quality explanation annota-tions, using multiple annotations to do explanation supervi-sion is a reasonable method. However, how to use multiple annotations to improve accuracy is particularly challenging due to the following: 1) The noisiness of annotations from different annotators; 2) The lack of pre-given information about the corresponding relationship between annotations and annotators; 3) Missing annotations since some images are not labeled by all annotators. To solve these challenges, we propose a Multi-annotated explanation-guided learn-ing (MAGI) framework to do explanation supervision with comprehensive and high-quality generated annotations. We first propose a novel generative model to generate anno-tations from all annotators and infer them using a newly proposed variational inference-based technique by learning the characteristics of each annotator. We also incorporate an alignment mechanism into the generative model to infer the correspondence between annotations and annotators in the training process. Extensive experiments on two datasets from the medical imaging domain demonstrate the effective-ness of our proposed framework in handling noisy annota-tions while obtaining superior prediction performance com-pared with previous SOTA. 1.

Introduction
In medical imaging, deep learning models are often used to provide predictions for tasks such as diagnosing diseases and they have shown exceptional performance [11, 40, 42, 33, 16, 44, 45]. However, these models can be seen as a
*Corresponding author.
Figure 1: An example showing the challenges present in explanation supervision: (a) Model-generated explanations still lack accuracy compared with human annotation. (b)
The model performance heavily relies on the quality of hu-man explanation labels, which is often inconsistent and in-complete among different annotators.
“black box”, making it difficult for clinicians to understand why a particular diagnosis was made [1]. An important line of research to tackle such limitations is to provide post-hoc explanations of the model behavior [12, 27, 31]. A popular way to achieve this in computer vision is through the use of attention maps, which highlight specific regions of an image that are most relevant for the model’s predic-tion [31]. Despite the attention mechanism being an in-creasingly prominent component in deep neural networks (DNNs) as a means of explanation, little research has been done to analyze whether attention is trustworthy and how to further improve it until recently.
Recently, explanation supervision, a technique that jointly optimizes prediction loss and explanation loss be-tween ground-truth annotations and model-generated expla-nations, has started to show promising effects in improving both the predictability and interpretability of deep neural networks [9, 13, 34]. By injecting explanation as a supervi-sion signal, models aim to explain the decision-making pro-cess at either the local or global level. Global explanations provide a general understanding of how the model works across the entire dataset, while local explanation techniques are applied for each input data and are therefore more com-monly used [12]. Depending on the type of data, local ex-planation techniques can be further summarized into several categories: 1) visual explanation, 2) rationale attention, and 3) feature attribution alignment [12]. While most research focuses on text and tabular data, supervising explanations on image data is relatively under-explored [36, 29, 28].
Human annotations are often subjective, which gives rise to the need for multiple annotations on a single image from different annotators. Learning from multiple annota-tions from diverse annotators can provide a more diverse range of insights, which can help to improve the accuracy and reliability of the annotations [18]. However, the chal-lenge with multi-annotation is that different annotators may have different levels of expertise, and therefore their anno-tations are often inconsistent. While previous works aim to improve model performance by incorporating annotations from multiple annotators (e.g., Figure 1), these approaches suffer from several limitations: 1) The noisiness of anno-tations from different annotators due to the personality attributes. In medical imaging, the quality of annotations can vary based on the personality of annotators, such as at-tention to detail and experience. For example, as shown in Figure 1, annotator 1 acts more aggressively than an-notator 2 when marking the nodule location. 2) The lack of prior information about the corresponding relation-ship between annotations and annotators. Due to privacy concerns and difficulty in data collection, there is often no pre-given information about the relationship between anno-tations and annotators. This makes it difficult to learn the persistent behavioral characteristics specific to each anno-tator. 3) Missing annotations as not all the annotators will label each image. Multiple annotations for a medical image (e.g., CT scan) may be missing due to various factors such as technical limitations, time constraints, or insuffi-cient resources. However, the lack of multiple annotations can limit the accuracy and reliability of medical image in-terpretation, which may impact patient diagnosis and treat-ment outcomes. To address the above challenges, we pro-pose a novel explanation supervision framework that deals with the inconsistent quality of annotation labels among dif-ferent annotators. This work makes several contributions, which can be summarized as follows:
• Introducing a novel framework for explanation super-vision that incorporates multiple explanation annota-tions. The proposed framework is supervised by class la-bels and multiple explanation annotations aggregated by learnable weights for each annotator.
• Proposing a new generative model to generate missing annotations. The generative model is inferred based on a newly proposed variational inference technique and can learn the characteristics of each annotator when generat-ing annotations.
• Developing a novel alignment mechanism and incorpo-rated into proposed generative model to infer the cor-respondence between annotations and annotators in the training process. This novel alignment mechanism can convert the correspondence inference problem to a linear sum assignment problem.
• Conducting extensive experiments with a variety of eval-uation metrics on two medical datasets demonstrates our effectiveness in improving model predictability and gen-erating robust consensus labels via our generative model to noisy annotations. 2.