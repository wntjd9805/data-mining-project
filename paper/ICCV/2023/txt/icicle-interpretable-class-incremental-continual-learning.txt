Abstract (cid:44)(cid:81)(cid:83)(cid:88)(cid:87)(cid:3)(cid:76)(cid:80)(cid:68)(cid:74)(cid:72) (cid:55)(cid:68)(cid:86)(cid:78)(cid:3)(cid:20)(cid:3)
Continual learning enables incremental learning of new tasks without forgetting those previously learned, result-ing in positive knowledge transfer that can enhance per-formance on both new and old tasks. However, contin-ual learning poses new challenges for interpretability, as the rationale behind model predictions may change over time, leading to interpretability concept drift. We address this problem by proposing Interpretable Class-InCremental
LEarning (ICICLE), an exemplar-free approach that adopts a prototypical part-based approach.
It consists of three crucial novelties: interpretability regularization that distills previously learned concepts while preserving user-friendly positive reasoning; proximity-based prototype initialization strategy dedicated to the ﬁne-grained setting; and task-recency bias compensation devoted to prototypical parts.
Our experimental results demonstrate that ICICLE reduces the interpretability concept drift and outperforms the ex-isting exemplar-free methods of common class-incremental learning when applied to concept-based models. 1.

Introduction
With the growing use of deep learning models in diverse domains, including robotics [10], medical imaging [17], and autonomous driving [43], there is a pressing need to develop models that can adapt to ever-changing conditions and learn new tasks from non-stationary data. However, a signiﬁcant challenge with neural networks is their ten-dency to suffer from catastrophic forgetting [26, 30, 44], where performance on previous tasks deteriorates rapidly as new ones are acquired. Continual Learning (CL) [19] has emerged as a promising technique to address this challenge by enabling models to learn new tasks without forgetting those learned before.
While existing CL approaches signiﬁcantly reduce catas-trophic forgetting, they are often difﬁcult for humans to understand. It is especially problematic because deep net-(cid:21) (cid:3) (cid:78) (cid:86) (cid:68) (cid:55) (cid:22) (cid:3) (cid:78) (cid:86) (cid:68) (cid:55) (cid:23) (cid:3) (cid:78) (cid:86) (cid:68) (cid:55) (cid:41)(cid:76)(cid:81)(cid:72)(cid:16)(cid:87)(cid:88)(cid:81)(cid:76)(cid:81)(cid:74)(cid:3) (cid:40)(cid:58)(cid:38) (cid:47)(cid:58)(cid:41) (cid:47)(cid:58)(cid:48) (cid:44)(cid:38)(cid:44)(cid:38)(cid:47)(cid:40)
Figure 1: We process the input image (top left) through the network and visualize how its speciﬁc areas are similar to one of the prototypes. The interpretability concept drift oc-curs when such a similarity map differs between tasks. ICI-CLE performs best, preserving similarity maps better than the other continual learning methods. works often predict the right answer for the wrong reason (the “Clever Hans” phenomenon), leading to excellent per-formance in training but poor performance in practice [63].
This results in serious societal problems that deeply affect health, freedom, racial bias, and safety [11]. As a result, some initial steps were taken in the literature to introduce explainable posthoc methods into the CL setup [32, 52, 64].
However, explaining black boxes, rather than replacing them with interpretable (self-explainable) models, can es-calate the problem by providing misleading or false char-acterizations [69] or adding unnecessary authority to the black box [12]. Therefore, there is a clear need for inno-vative machine-learning models that are inherently inter-pretable [11]. To the best of our knowledge, no interpretable
CL approach has been proposed so far.
In this work, we introduce Interpretable Class-Incremental Learning (ICICLE), an interpretable approach to class-incremental learning based on prototypical parts methodology. Similarly to This looks like that reason-ing [16], ICICLE learns a set of prototypical parts repre-senting reference concepts derived from the training data and makes predictions by comparing the input image parts to the learned prototypes. However, the knowledge transfer between tasks in continual learning poses new challenges for interpretability. Mainly because the rationale behind model predictions may change over time, leading to inter-pretability concept drift and making explanations inconsis-tent (see Figure 1 and Table 1). Therefore, ICICLE contains multiple mechanisms to prevent this drift and, at the same time, obtain satisfactory results. First, we propose an inter-pretability regularization suited for prototypical part-based models to retain previously gained knowledge while main-taining model plasticity. It ensures that previously learned prototypical parts are similarly activated within the current task data, which makes explanations consistent over time.
Moreover, considering the ﬁne-grained nature of considered datasets, we introduce proximity-based prototype initializa-tion for a new task. It searches for representative concepts within the new task data close to previously learned con-cepts, allowing the model to recognize high-level features of the new task and focusing on tuning details. Thirdly, to overcome task-recency bias in class-incremental learn-ing scenarios, we propose a simple yet effective method that balances the logits of all tasks based on the last task data. Finally, we reduce multi-stage training while preserv-ing user-friendly positive reasoning.
We evaluate ICICLE on two datasets, namely CUB-200-2011 [83] and Stanford Cars [46], and conduct exhaustive ablations to demonstrate the effectiveness of our approach.
We show that this problem is challenging but opens up a promising new area of research that can further advance our understanding of CL methods. Our contributions can be summarized as follows:
• We are the ﬁrst to introduce interpretable class-incremental learning and propose a new method ICI-CLE, based on prototypical part methodology.
• We propose interpretability regularization that pre-vents interpretability concept drift without using ex-emplars.
• We deﬁne a dedicated prototype initialization strategy and a method compensating for task-recency bias. 2.