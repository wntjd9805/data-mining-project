Abstract
Automatic 3D content creation has achieved rapid progress recently due to the availability of pre-trained, large language models and image diffusion models, forming the emerging topic of text-to-3D content creation. Existing text-to-3D methods commonly use implicit scene represen-tations, which couple the geometry and appearance via vol-ume rendering and are suboptimal in terms of recovering finer geometries and achieving photorealistic rendering; consequently, they are less effective for generating high-quality 3D assets. In this work, we propose a new method of Fantasia3D for high-quality text-to-3D content creation.
Key to Fantasia3D is the disentangled modeling and learn-ing of geometry and appearance. For geometry learning, we rely on a hybrid scene representation, and propose to encode surface normal extracted from the representation as the input of the image diffusion model. For appearance modeling, we introduce the spatially varying bidirectional reflectance distribution function (BRDF) into the text-to-3D task, and learn the surface material for photorealistic ren-dering of the generated surface. Our disentangled frame-work is more compatible with popular graphics engines, supporting relighting, editing, and physical simulation of the generated 3D assets. We conduct thorough experiments that show the advantages of our method over existing ones under different text-to-3D task settings. Project page and source codes: https://fantasia3d.github.io/. 1.

Introduction
Automatic 3D content creation [43, 18, 33, 44] powered by large language models has drawn significant attention recently, due to its convenience to entertaining and gaming industries, virtual/augmented reality, and robotic applica-tions. The traditional process of creating 3D assets typically involves multiple, labor-intensive stages, including geome-*Equal contribution.
†Corresponding author.
Figure 1. Provided with a textual description of “a highly de-tailed stone bust of Theodoros Kolokotronis”, our method pro-duces high-quality geometry as well as disentangled materials, and enables photorealistic rendering. try modeling, shape baking, UV mapping, material creation, and texturing, as described in [15], where different software tools and the expertise of skilled artists are often required.
Imperfections would also accumulate across these stages, resulting in low-quality 3D assets. It is thus desirable to au-tomate such a process, and ideally to generate high-quality 3D assets that have geometrically fair surfaces, rich materi-als and textures, and support photorealistic rendering under arbitrary views.
In this work, we focus on automatic 3D content cre-ation given text prompts encoded by large language mod-els, i.e., the text-to-3D tasks [18, 33]. Text-to-3D is in-spired by the tremendous success of text-to-image research
[34, 36, 30, 35]. To enable 3D generation, most existing methods [33, 23] rely on the implicit scene modeling of
Neural Radiance Field (NeRF) [25, 4, 27], and learn the
NeRFs by back-propagating the supervision signals from image diffusion models. However, NeRF modeling is less effective for surface recovery [43, 46], since it couples the learning of surface geometry with that of pixel colors via volume rendering. Consequently, 3D creation based on
NeRF modeling is less effective for recovery of both the fine surface and its material and texture. In the meanwhile, explicit and hybrid scene representations [43, 46, 38] are proposed to improve over NeRF by modeling the surface explicitly and performing view synthesis via surface ren-dering.
In this work, we are motivated to use 3D scene represen-tations that are more amenable to the generation of high-quality 3D assets given text prompts. We present an auto-matic text-to-3D method called Fantasia3D. Key to Fanta-sia3D is a disentangled learning of geometry and appear-ance models, such that both a fine surface and a rich mate-rial/texture can be generated. To enable such a disentangled learning, we use the hybrid scene representation of DMTET
[38], which maintains a deformable tetrahedral grid and a differentiable mesh extraction layer; deformation can thus be learned through the layer to explicitly control the shape generation. For geometry learning, we technically propose to encode a rendered normal map, and use shape encoding of the normal as the input of a pre-trained, image diffusion model; this is in contrast different from existing methods that commonly encode rendered color images. For appear-ance modeling, we introduce, for the first time, the spa-tially varying Bidirectional Reflectance Distribution Func-tion (BRDF) into the text-to-3D task, thus enabling mate-rial learning that supports photorealistic rendering of the learned surface. We implement the geometry model and the BRDF appearance model as simple MLPs. Both models are learned through the pre-trained image diffusion model, using a loss of Score Distillation Sampling (SDS) [33]. We use the pre-trained stable diffusion [35, 40] as the image generation model in this work.
We note that except for text prompts, our method can also be triggered with additional inputs of users’ prefer-ences, such as a customized 3D shape or a generic 3D shape of a certain object category; this is flexible for users to better control what content is to be generated. In addition, given the disentangled generation of geometry and appearance, it is convenient for our method to support relighting, editing, and physical simulation of the generated 3D assets. We con-duct thorough experiments to verify the efficacy of our pro-posed methods. Results show that our proposed Fantasia3D outperforms existing methods for high-quality and diverse 3D content creation. We summarize our technical contribu-tions as follows.
• We propose a novel method, termed Fantasia3D, for high-quality text-to-3D content creation. Our method disentangles the modeling and learning of geometry and appearance, and thus enables both a fine recovery of geometry and photorealistc rendering of per-view images.
• For geometry learning, we use a hybrid representation of DMTET, which supports learning surface deforma-tion via a differentiable mesh extraction; we propose to render and encode the surface normal extracted from
DMTET as the input of the pre-trained image diffusion model, which enables more subtle control of shape generation.
• For appearance modeling, to the best of our knowl-edge, we are the first to introduce the full BRDF learning into text-to-3D content creation, facilitated by our proposed geometry-appearance disentangled framework. BRDF modeling promises high-quality 3D generation via photorealistic rendering. 2.