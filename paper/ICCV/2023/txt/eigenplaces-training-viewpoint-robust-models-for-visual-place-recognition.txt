Abstract
Visual Place Recognition is a task that aims to predict the place of an image (called query) based solely on its visual features. This is typically done through image re-trieval, where the query is matched to the most similar images from a large database of geotagged photos, us-ing learned global descriptors. A major challenge in this task is recognizing places seen from different viewpoints.
To overcome this limitation, we propose a new method, called EigenPlaces, to train our neural network on im-ages from different point of views, which embeds viewpoint robustness into the learned global descriptors. The un-derlying idea is to cluster the training data so as to ex-plicitly present the model with different views of the same points of interest. The selection of this points of inter-est is done without the need for extra supervision. We then present experiments on the most comprehensive set of datasets in literature, finding that EigenPlaces is able to outperform previous state of the art on the majority of datasets, while requiring 60% less GPU memory for train-ing and using 50% smaller descriptors. The code and trained models for EigenPlaces are available at https:
//github.com/gmberton/EigenPlaces, while results with any other baseline can be computed with the codebase at https://github.com/gmberton/auto_VPR. 1.

Introduction
Visual Place Recognition (VPR) is a task that aims to predict the place where a photo (i.e. query) was taken, quickly and accurately, based solely on its visual fea-tures. This is typically done with an image retrieval ap-proach [51, 14, 27, 16, 5, 15, 50, 23, 20, 31, 19, 21, 26, 57, 53, 22, 25, 60, 36, 1, 8, 10, 2, 29]: first, a deep neural network is used to extract global descriptors from the query and from a database of geo-referenced images; then, a nearest neighbor search is performed in this features space [5, 31, 27, 21, 8, 2, 1]. While such approaches have shown great potential in partially solving known problems such as scalability (by using ever more compact descriptors
Figure 1. Most previous works [5, 31, 27, 61] train their models through metric learning, using as positive the most similar image to the query, which naturally would have same or similar orien-tation to the query. Other works split the dataset in classes, with images within a class having similar [1, 2] or exactly the same [8] orientation. EigenPlaces goes against this trend, creating classes in which all images are oriented towards the same point, leading to viewpoint robust models able to correctly localize highly chal-lenging queries, for example the ones collected from a sidewalk.
[62, 8, 2]) and illumination changes (through the synthetic generation of night images [11, 41, 3] or strong data aug-mentation [21]), recognizing images under heavy viewpoint shifts is still an open challenge. A popular strategy to handle this problem is to follow up the similarity search performed on global feature descriptors with a post-processing phase that re-ranks the retrieved results using either spatial verifi-cation [12, 30, 22] or matching densely extracted local fea-principal components within each given class.
To empirically show the soundness of our method, we run a benchmark on the largest number of VPR datasets ever. The results show that a model trained with Eigen-Places is able to outperform previous SOTA on numerous datasets, while using 50% smaller descriptors and requiring 60% less GPU memory for training.
Our contributions are summarized as follows:
• we propose EigenPlaces, a novel training protocol whose ultimate goal is to render the model robust to viewpoint changes that it may encounter at test time;
• we perform a rigorous VPR benchmark on the most complete set of datasets in literature, to highlight not only the strengths but also the weaknesses of Eigen-Places and its predecessors;
• while our exploration shows that there is no one-win-all solution on every scenario, we note that Eigen-Places outperforms previous state of the art on a large number of datasets, while needing 60% less GPU memory to train and using 50% more compact descrip-tors.
The code are available
EigenPlaces. and trained models for EigenPlaces at https://github.com/gmberton/
We also created and released a codebase to run experi-ments with a number of trained models (namely NetVLAD,
SFRS, CosPlace, Conv-AP, MixVPR and EigenPlaces), which automatically downloads each model’s weights from their official repository, to be able to run experiments within a fair and standardized framework. The codebase is avail-able at https://github.com/gmberton/auto_VPR. 2.