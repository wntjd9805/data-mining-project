Abstract
Existing inverse rendering combined with neural ren-dering methods can only perform editable novel view syn-thesis on object-specific scenes, while we present intrinsic neural radiance fields, dubbed IntrinsicNeRF, which intro-duce intrinsic decomposition into the NeRF-based neural rendering method and can extend its application to room-scale scenes. Since intrinsic decomposition is a funda-mentally under-constrained inverse problem, we propose a novel distance-aware point sampling and adaptive re-flectance iterative clustering optimization method, which enables IntrinsicNeRF with traditional intrinsic decompo-sition constraints to be trained in an unsupervised manner, resulting in multi-view consistent intrinsic decomposition results. To cope with the problem that different adjacent instances of similar reflectance in a scene are incorrectly clustered together, we further propose a hierarchical clus-tering method with coarse-to-fine optimization to obtain a fast hierarchical indexing representation. It supports com-pelling real-time augmented applications such as recolor-ing and illumination variation. Extensive experiments and editing samples on both object-specific/room-scale scenes
∗indicates equal contribution.
† indicates corresponding author. and synthetic/real-word data demonstrate that we can ob-tain consistent intrinsic decomposition results and high-fidelity novel view synthesis even for challenging sequences. 1.

Introduction
Recently neural rendering techniques have gained in-creasing attention and demonstrated tremendous perfor-mance in novel view synthesis, ranging from small ob-jects [37, 41, 46, 63] to large outdoor scenes [41, 60], but they struggle to perform further intuitive editing like realis-tic scene recoloring, relighting, etc, for the scenes are usu-ally represented as neural fields implicitly and required to be decomposed into the editable properties explicitly.
Several works have proposed to fulfill this goal by intro-ducing inverse rendering into neural rendering [74, 76, 77], where the scene is decomposed into geometry, reflectance, and illumination. However, since inverse rendering is fun-damentally ambiguous and highly ill-posed, these NeRF-based inverse rendering works [74, 77] introduce many prior assumptions preventing the modeling of mutual oc-clusion, inter-reflection, and indirect light propagation of different objects in the scene. An accurate 3D surface re-covery is also required as a prerequisite. All these factors limit their applications to object-specific scenarios.
To empower such editable capabilities to the scene-level neural rendering, we present intrinsic neural radiance fields, which introduce intrinsic decomposition into neural render-ing, based on the fact that intrinsic decomposition can be considered as a simplification of inverse rendering designed to provide interpretable intermediate representations (i.e., reflectance and shading) that are relatively easy to solve for both in small objects and large scenes. A potential naive so-lution may use the trained NeRF model to generate multi-view images and then perform multi-view intrinsic decom-position, where these two tasks are separated. In contrast, extending from NeRF [46], IntrinsicNeRF (see Sec. 3.1 and Fig. 2) takes the sampled spatial coordinate point x = (x, y, z) and the direction d = (θ, ϕ) as input and regresses them into the density σ, view-independent reflectance r and shading s (Lambertian reflectance assumption) and addi-tional view-dependent residual term re [42, 61] (Eq. 2), which naturally guarantees the multi-view consistency of decomposition after training, thanks to neural rendering.
However, it is nontrivial to design such a framework due to huge gaps in optimization between traditional intrinsic decomposition and NeRF-based methods. Traditional in-trinsic decomposition methods optimize the energy equa-tion by establishing constraints related to the image pixels, while NeRF-based methods optimize the view-dependent densities and colors of several sampled 3D points through volume rendering, which makes it hard to exploit the com-monly used prior knowledge in intrinsic decomposition (see
Sec. 3.2) such as chromaticity prior, reflectance sparsity, etc. To address this problem, we propose a distance-aware sampling method (see Fig. 3) that allows the sampled points not only to be random but also to establish local and global relationships between points.
In this way, IntrinsicNeRF satisfies both the novel view synthesis and the better recov-ery of the intrinsic properties of the scene.
Moreover, to deal with the inconsistencies of similar re-flectance regions [44], we present an adaptive reflectance it-erative clustering method (see Sec. 3.3) with mean shift [13] to adaptively cluster color points with similar reflectance based on the scene itself, rather than K-Means used in [44], which limits the number of specific classes. A continu-ously updated clustering operation with the voxel grid fil-ter is constructed to map similar reflectance colors to the same target reflectance color and then obtain the clustered category for each color point (see Fig. 5).
To settle the problem of different adjacent instances of similar reflectance in a scene being clustered together, we propose a semantic-aware reflectance sparsity con-straint during training.
Inspired by Semantic-NeRF [79], we add an additional semantic branch to IntrinsicNeRF, along with reflectance clustering, which yields a hierarchi-cal reflectance iterative clustering and indexing method (see
Fig. 6), optimizing the network from coarse to fine. Ex-tensive experiments on the Blender Object and the Replica
Scene dataset demonstrate our method can obtain consistent intrinsic decomposition results and high-fidelity novel view synthesis even for challenging sequences. We also develop video editing software to facilitate users to perform online scene recoloring, illumination variation, and editable novel view synthesis on both real-world and synthetic data on the
CPU (see Fig. 1). 2.