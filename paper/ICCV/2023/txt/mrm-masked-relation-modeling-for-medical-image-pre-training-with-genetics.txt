Abstract
Modern deep learning techniques on automatic multi-modal medical diagnosis rely on massive expert annota-tions, which is time-consuming and prohibitive. Recent masked image modeling (MIM)-based pre-training methods have witnessed impressive advances for learning meaning-ful representations from unlabeled data and transferring to downstream tasks. However, these methods focus on nat-ural images and ignore the specific properties of medical data, yielding unsatisfying generalization performance on downstream medical diagnosis.
In this paper, we aim to leverage genetics to boost image pre-training and present
Instead a masked relation modeling (MRM) framework. of explicitly masking input data in previous MIM methods leading to loss of disease-related semantics, we design rela-tion masking to mask out token-wise feature relation in both self- and cross-modality levels, which preserves intact se-mantics within the input and allows the model to learn rich disease-related information. Moreover, to enhance seman-tic relation modeling, we propose relation matching to align the sample-wise relation between the intact and masked fea-tures. The relation matching exploits inter-sample relation by encouraging global constraints in the feature space to render sufficient semantic relation for feature representa-tion. Extensive experiments demonstrate that the proposed framework is simple yet powerful, achieving state-of-the-art transfer performance on various downstream diagnosis tasks. Codes are available at https://github.com/
CityU-AIM-Group/MRM . 1.

Introduction
In the medical diagnosis [44, 34, 38, 28, 6, 5], the large-scale multimodal biobank data, e.g., images and genet-ics, is necessary for a reliable diagnosis, overcoming the limited scale and disease information of a single-modality
*Corresponding author.
This work was supported by Hong Kong Research Grants Council (RGC) General Research Fund 14220622, 14204321, and Innovation and
Technology Commission-Innovation and Technology Fund ITS/100/20.
Figure 1. Comparison of different masking strategies on natu-ral and medical data. (a): Existing MIM methods mask out in-put natural images and infer the missing content to learn semantic representations via reconstruction task. (b): Recent pre-training approaches for disease diagnosis explicitly employ MIM on in-put medical data (e.g., medical images and genome), whilst they are prone to lose tiny disease regions and cause non-tractable re-construction. (c): Our method masks token-wise feature relation across multimodal data and matches sample-wise relation between the intact and masked features, preserving intact semantic regions and enriching relation information. dataset. However, the prohibitive expert annotations of large-scale datasets make it difficult to train a conventional deep model [44, 45, 41, 46, 47]. Particularly, in this multi-modal scenario, the requirements of the experts in various medical fields prevent enough annotation access, severely limiting the grounding of the automatic diagnosis system.
To address this, the most prospering trend [16, 43, 4, 34, 13, 35, 36, 12, 27] is self-supervised pre-training, e.g., masked image modeling (MIM) [16, 43, 13, 42], aiming to train a label-free model with adequate generalization capac-ity. Existing MIM methods [16, 43, 4, 25, 13, 42, 49] mask out a high portion of patches within input images and in-fer the missing content, as suggested in Figure 1 (a). They make use of contextual information to glance the semantics and reconstruct the entire images, which performs a mask-and-reconstruct task to pre-train the model without anno-tation and transfer meaningful representations to various
downstream tasks for improving label-efficient fine-tuning.
Though achieving great success, most works [16, 43, 4, 40, 10, 25, 19] are designed for natural images, ignor-ing the essential differences between medical data and nat-ural images. Therefore, we empirically find that existing
MIM CANNOT works well in medical data (see Table 1), and even totally fails to reconstruct diseases (see Figure 3).
The reasons derive from a critical observation regarding the significant data differences, which can be summarized into two challenges. Firstly, compared with natural images, there are limited semantic regions in medical data. As shown in Figure 1 (a), the semantic-rich foreground is al-ways the main body of the natural image, while the rest non-informative background region only represents a mi-nority part. Differently, in the medical image (Figure 1 (b)), the majority of regions are backgrounds, and the in-formative disease regions are usually on a tiny scale. Un-der the strategy of masking entire tokens in existing MIM methods [16, 43, 4, 40, 10, 25, 19], if the disease tokens are masked out, the disease-related semantic is totally miss-ing with a catastrophic information loss, leading to a non-tractable reconstruction. This issue also exists between ge-nomics and natural images. The semantic regions in ge-nomics, i.e., the disease-related patterns, mainly lie in a mi-nority of genome segments [28, 5, 7]. Hence, instead of masking the whole input token, these observations motivate us to delve into the masking of token-level relation, which preserves abundant semantic discriminability and adequate self-supervision, as illustrated in Figure 1 (c) Left.
The second challenge is the limited semantic relation.
In a natural image, the background and foreground relation-ship, e.g., the bird in the sky and the person in a room, tends to be prosperous and abundant, serving a critical role in semantic-level learning [39, 24]. In contrast, in each med-ical data sample, the disease-aware relation is limited and insufficient to provide enough discriminative evidence. The reason lies in that the medical datasets are usually collected from the same human organ, e.g., the fundus, containing redundant and similar anatomical patterns, e.g., the capil-lary, which severely prevent the relation modeling between the disease and the complex medical scene. This chal-lenge hampers reliable relation learning in existing MIM methods, and may incur an inevitable overfitting to non-informative relation within the background [40, 25, 23, 3].
Hence, considering the limited semantic relation in each data sample, we are committed to going beyond the self-supervised learning for an independent and individual data sample, and propose to encourage global constraints for ex-ploiting inter-sample relation (see Figure 1 (c) Right).
To combat above challenges, as shown in Figure 1 (c), we present MRM, a masked relation modeling from a uni-fied view of relation, containing relation masking and re-lation matching, to rationally pre-train multimodal medical images with genetics. To preserve intact semantic informa-tion within raw input, we devise relation masking strategy to allow the model to learn disease-related semantics. In-stead of masking out input data, the relation masking inves-tigates token-wise relation within feature representations in both self- and cross-modality levels and masks out the rela-tion among all multimodal tokens. The relation masking en-dows the model to explicitly learn global dependency from raw data without missing disease-related semantic informa-tion. Furthermore, to improve the semantic relation model-ing, the relation matching is designed to provide global con-straint by aligning the feature relation across multiple sam-ples. Specifically, relation matching exploits sample-wise relation in both self- and cross-modality levels to encour-age the relation consistency between the intact and masked features. This enjoys the complementary advantages of per-sample pixel-wise reconstruction loss and boosts the trans-fer ability of the model. With the pre-trained model, we can obtain the feature representation that can be transferred to supervised downstream diagnosis tasks for boosting label-efficient fine-tuning, alleviating the severe demand for spe-cialized annotations.
In summary, our contributions fall into four parts:
• We identify the challenges of current MIM methods on medical data, and present MRM, a masked relation modeling using multimodal medical data to facilitate image representation learning.
• Towards the issue of limited semantic regions in med-ical data, we design relation masking to mask token-wise feature relation across self- and cross-modality.
Different from MIM explicitly masking inputs, rela-tion masking preserves disease semantics within in-puts, endowing a powerful mask-and-construct task.
• Moreover, to enrich the semantic relation among dis-eases, the relation matching is proposed to capture abundant disease-related relation by aligning sample-wise feature relation between intact and masked fea-tures in both self- and cross-modality levels.
• Extensive transfer evaluation on various downstream tasks using two public medical pre-training datasets demonstrate that our framework performs superior transfer ability over state-of-the-art methods. 2.