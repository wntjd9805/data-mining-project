Abstract
Text-guided human motion generation has drawn signifi-cant interest because of its impactful applications spanning animation and robotics. Recently, application of diffusion models for motion generation has enabled improvements in the quality of generated motions. However, existing ap-proaches are limited by their reliance on relatively small-scale motion capture data, leading to poor performance
In this paper, we on more diverse, in-the-wild prompts. introduce Make-An-Animation, a text-conditioned human motion generation model which learns more diverse poses and prompts from large-scale image-text datasets, enabling significant improvement in performance over prior works.
Make-An-Animation is trained in two stages. First, we train on a curated large-scale dataset of (text, static pseudo-pose) pairs extracted from image-text datasets. Second, we fine-tune on motion capture data, adding additional layers to model the temporal dimension. Unlike prior diffusion mod-els for motion generation, Make-An-Animation uses a U-Net architecture similar to recent text-to-video generation models. Human evaluation of motion realism and alignment with input text shows that our model reaches state-of-the-art performance on text-to-motion generation. Generated samples can be viewed at https://azadis.github.io/make-an-animation. 1.

Introduction
As the world shifts towards virtual spaces, and as em-bodied agents become more capable, it will be increasingly important to be able to generate plausible human motion.
Speech or text are perhaps the most natural ways to prompt generative models, which is one reason behind the explo-sion in text-to-x generation research. Human motion gener-ation from text has diverse applications both in virtual and real worlds. For instance, it enables control of robots with speech, faster video game development, creation of special effects featuring humans, as well as novel metaverse user-interaction modes whereby users can control the actions of their or others’ avatars via voice or text.
Figure 1. Samples generated by Make-An-Animation for text con-ditional motion generation. The lighting of the body models rep-resents progress across time. Darker color indicates later frames in the sequence. In the top image, for a better visualization, frames are distributed horizontally.
Text-conditioned human motion generation is challeng-ing for a number of reasons. Firstly, there exists a huge diversity of human motions and text descriptions which de-scribe those motions (i.e., the task is many-to-many distri-bution matching). Secondly, because motion capture data is expensive to acquire, human motion datasets are limited in scale and diversity.
Recently, text-to-motion generation has seen rapid im-provements in the quality and diversity of poses generated.
In particular, diffusion models, which have yielded impres-sive results in text-conditioned image and video genera-tion, have proven effective in generating human body poses
[35, 16, 38]. However, these models struggle with in-the-wild prompts that are outside of the distribution of motion capture data.
To address these shortcomings, we present Make-An-Animation, a text-conditioned human motion generation model which learns to map diverse human poses to nat-ural language descriptions through in-the-wild image-text datasets and significantly improves on prior state-of-the-art models which only rely on motion capture data.
Core to our approach is a large-scale dataset of human poses extracted from image-text datasets. Our key obser-vation is that we should not be limited to learning human motion from motion capture data when we have access to large-scale in-the-wild video and images of human poses.
So, to address the limited scale and diversity of motion capture datasets, we extract a large-scale Text Pseudo-Pose (TPP) dataset from image-text datasets filtered for images containing humans. This TPP dataset contains 35M (text, static pose) pairs.
Make-An-Animation is trained in two stages: (1) We train a text-conditioned static 3D pose generation diffu-sion model on the TPP dataset.
In this stage, Make-An-Animation learns the distribution of human poses and their alignment with text. (2) Then, we extend the pre-trained diffusion model to motion generation via the addition of temporal convolution and attention layers which model the new temporal dimension and train on widely-used motion capture data. In this second stage, the model learns motion, i.e., how to connect poses in a temporally coherent man-ner. Crucially, it does not have to re-learn the distribution of feasible poses or their alignment with text.
The Make-An-Animation architecture is a U-Net, simi-lar to recent text-to-video diffusion models. We condition the U-Net on text representations extracted from a language model trained on large-scale language data. We represent human motion as a sequence of 3D SMPL body param-eters, with a 6D continuous SMPL representation for 21 body joints and the root orient. We additionally represent the global position per frame via a 3D vector indicating the position in each of the x, y, z dimensions.
Through human evaluation on a collected set of 400 di-verse text prompts, we demonstrate that our method outper-forms prior works in terms of generated pose realism and text alignment.
To summarize, our main contributions are:
• We present Make-An-Animation – a text-conditioned human motion generation model which improves on prior state-of-the-art models, especially on diverse, in-the-wild text prompts.
• We show, for the first time, how to leverage large-scale image datasets to learn in-the-wild human poses for generation. We show through ablations that pre-training on our collected Text Pseudo-Pose dataset sig-nificantly improves the generalization to prompts out-side the distribution of widely-used motion capture datasets while showing a comparable performance on the mocap test set.
• We present a U-Net architecture for human motion generation which leverages a language model pre-trained on large-scale language data and naturally ex-tends a static text-to-pose generation diffusion model to motion generation via the addition of temporal con-volution and attention layers. 2.