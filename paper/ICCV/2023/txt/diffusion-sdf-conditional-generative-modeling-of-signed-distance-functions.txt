Abstract
Probabilistic diffusion models have achieved state-of-the-art results for image synthesis, inpainting, and text-to-image tasks. However, they are still in the early stages of generating complex 3D shapes.
This work for shape proposes Diffusion-SDF, a generative model completion, single-view reconstruction, and reconstruction of real-scanned point clouds. We use neural signed distance functions (SDFs) as our 3D representation to parameterize the geometry of various signals (e.g., point clouds, 2D images) through neural networks. Neural SDFs are implicit functions and diffusing them amounts to learning the reversal of their neural network weights, which we solve using a custom modulation module. Extensive experiments show that our method is capable of both realistic uncondi-tional generation and conditional generation from partial inputs. This work expands the domain of diffusion models from learning 2D, explicit representations, to 3D, implicit representations. Code is released at https://github. com/princeton-computational-imaging/
Diffusion-SDF . 1.

Introduction
Diffusion probabilistic models [51, 18] have become a popular choice for generative tasks and can produce im-pressive results, such as the images generated by DALLE-2 [44] and Stable Diffusion [46] from text input. Diffusion models are a type of likelihood-based models whose train-ing objective can be expressed as a variational lower bound
[18, 53]. On a high level, they learn to gradually remove noise from a signal and repeat this process to generate sam-ples from Gaussian noise. Recent advances [38, 10, 44, 46] show that diffusion models produce images with quality on par with state-of-the-art generative adversarial networks (GANs) [15] without the common drawbacks of mode col-lapse [38, 37] and unstable training [36, 2]. Diffusion has also been applied to 3D tasks although these works are still
In this in the early stages of producing complex shapes. work, we investigate the generation of 3D shapes of neural signed distance functions via diffusion. 3D modeling and generation are essential to vision and graphics tasks. 3D generation of high-quality assets and large volumes of realistic data is often essential where train-ing data is expensive to collect [24, 27, 47, 34, 48, 3].
Additionally, generation can be applied to 3D reconstruc-tion of imperfect visual observations as there exists a one-to-many mapping that requires a probabilistic approach to solve. This has applications in self-driving [54, 67, 39] and robotics grasping [4, 22, 65] where occlusion and camera measurement errors are common.
We propose Diffusion-SDF, a generative model for shape completion, single-view reconstruction, and reconstruction of real-scanned point clouds. We choose neural signed dis-tance functions (SDFs) [40] as the 3D representation to pa-rameterize the surfaces described by various input signals such as point clouds and 2D images. They implicitly en-code an object surface by the signed distances between 3D coordinate queries to their closest surface point through a coordinate-based MLP [40, 8]. Compared to discrete 3D representations [25, 30, 43, 13, 16], SDFs have proven to be a versatile representation that supports arbitrary resolu-tion during test-time [55], small memory footprints [9], and strong generalization [8].
We make the following two key insights. First, im-plicit functions can directly be used as data and diffusing them amounts to learning the reversal of the neural network weights. Furthermore, we introduce geometrical constraints to produce complex shapes and outputs consistent with the geometry of conditioned inputs. Very recently, Dupont et al. [11] similarly diffuse implicit functions but do not ad-dress SDFs nor geometric constraints. Second, by using
SDFs as a unified 3D representation, we condition training to learn a mapping between various input types and their possible reconstructions. Then, we leverage a probabilis-tic diffusion model to generate diverse completions. Thus, our work can be applied to shape synthesis and multi-modal shape completion.
The proposed method consists of two steps, shown in
Fig. 2. First, we create a compressed representation of
SDFs using modulation [11, 5, 31]. We find that diffusing
SDFs is impractical due to the large number of parameters and the lack of a smoothed data distribution. Our modu-Figure 1. Our method generates clean meshes with diverse geometries. (Top) Unconditional generations from training on multiple classes. (Bottom) Conditional generation given various visual inputs, such as partial point clouds (same point cloud overlaid on sample), real-scanned point clouds, and 2D images. Our method captures details of conditioned geometry, such as the handle of the pitcher. lation module consists of learning a generalizable encoder and a regularized latent space to create latent vectors that map to individual SDFs when combined with an SDF base network. Second, we train a diffusion model with the pre-viously created latent vectors as data points. We follow the conventional approach of learning the reverse diffusion process [18, 44], but we combine it with our modulation scheme to introduce geometric information. We show that this geometric constraint is essential for the method to com-plete shapes consistent with guided inputs. Furthermore, we experiment with various input types for guiding genera-tion. Shown in Fig. 1, we validate the method by shape gen-eration and completion with conditioning of partial point clouds from Acronym [12], real-scanned point clouds from
YCB [4], and 2D images from ShapeNet [6]. Our method generates diverse and realistic shapes for multiple tasks. We make the following contributions:
• We propose a probabilistic generative model that cre-ates clean and diverse 3D meshes.
• We solve a learning problem of diffusing the weights of implicit neural functions while providing geometric guidance through our modulation module.
• Our method reconstructs plausible outputs from vari-ous imperfect observations such as sparse, partial point clouds, single images, and real-scanned point clouds.
• Extensive experiments show that our method achieves favorable performance in shape generation and com-pletion compared to existing methods. 2.