Abstract
We present a new method for generating realistic and view-consistent images with fine geometry from 2D im-age collections. Our method proposes a hybrid explicit-implicit representation called OrthoPlanes, which encodes fine-grained 3D information in feature maps that can be efficiently generated by modifying 2D StyleGANs. Com-pared to previous representations, our method has better scalability and expressiveness with clear and explicit in-formation. As a result, our method can handle more chal-lenging view-angles and synthesize articulated objects with high spatial degree of freedom. Experiments demonstrate that our method achieves state-of-the-art results on FFHQ and SHHQ datasets, both quantitatively and qualitatively.
Project page: https://orthoplanes.github.io/.
∗ Equal contribution.
† Interns at Shanghai AI Laboratory.
‡ Work done as research engineer at Shanghai AI Laboratory. 1.

Introduction
Recovering 3D world from 2D images, known as in-verse rendering, is a typical problem in computer vision and computer graphics. It has many practical uses in VR/AR and other domains like movie production and virtual try-on. Physically-based methods [9, 12] can yield superior outcomes, but they are associated with notable costs and intensive labor requirements. On the other hand, data-centric approaches stand out for their adaptability, ease of use, and photorealism [56]. In this study, our objective is to develop a novel data-driven representation that enhances the 2D GAN’s understanding of 3D geometry, enabling it to produce more detailed and realistic images of diverse objects. 3D-aware GANs have advanced rapidly to synthesize images that are consistent across multiple views from 2D image collections [6, 5, 18, 43, 41, 49, 62, 22, 65, 51]. These methods combine unsupervised learning, neural rendering and super resolution to produce realistic results. However, they continue to face challenges in accurately reconstructing 3D shapes from 2D images, a crucial step in rendering 2D
GAN genuinely aware of 3D contexts. Certain methods at-tempt to address this issue utilizing a tri-plane representation
[5] or through the use of multiple parallel images [64] for the reconstruction of 3D entities. However, these strategies fall short when dealing with non-rigid entities like human figures that exhibit asymmetrical poses and varied appear-ances, which cannot be adequately represented by merely three orthogonal or parallel projections.
Inspired by successful scene-specific representations [38], we present a method for efficiently upscaling the generaliz-able tri-plane representation [5]. The main idea of our work is that a representation indexed via sectional projection can better represent 3D world than one that is indexed via or-thogonal or parallel projection, because it has more explicit 3D information such as projection direction and distance.
Based on this intuition, we introduce a novel hybrid explicit-implicit 3D representation, OrthoPlanes, as shown in Fig. 2 (d). We introduce a pre-defined location embedding to each feature map [28] to make them location-aware. The renderings exhibit better view consistency owing to this ex-plicit data. Moreover, the geometry can be more detailed since the codebook size and density surpasses that of the representations based on orthogonal projections.
With acceptable increase of computing costs over EG3D
[5], our approach can improve image quality, deal with more difficult viewing perspectives and generate articulated ob-jects with high spatial variability, e.g., human bodies, as shown in Fig. 1. Our experiment show state-of-the-art re-sults for 3D-aware image synthesis from 2D collections on diverse datasets including FFHQ [27] and SHHQ [15].
We summarize our contributions as the follows: 1) We present a 3D representation termed orthoplanes aimed at en-hancing the 3D awareness of 2D GANs, which significantly improves view-consistency and geometry. 2) we add a new branch to the 2D generator ensuring both training efficiency and heightened scalability. 2.