Abstract
The popularity of Contrastive Language-Image Pre-training (CLIP) has propelled its application to diverse downstream vision tasks.
To improve its capacity on downstream tasks, few-shot learning has become a widely-adopted technique. However, existing methods either ex-hibit limited performance or suffer from excessive learnable parameters. In this paper, we propose APE, an Adaptive
Prior rEfinement method for CLIP’s pre-trained knowl-edge, which achieves superior accuracy with high compu-tational efficiency. Via a prior refinement module, we ana-lyze the inter-class disparity in the downstream data and decouple the domain-specific knowledge from the CLIP-extracted cache model. On top of that, we introduce two model variants, a training-free APE and a training-required APE-T. We explore the trilateral affinities between the test image, prior cache model, and textual represen-tations, and only enable a lightweight category-residual module to be trained. For the average accuracy over 11 benchmarks, both APE and APE-T attain state-of-the-art and respectively outperform the second-best by +1.59% and +1.99% under 16 shots with ×30 less learnable pa-rameters. Code is available at https://github.com/ yangyangyang127/APE. 1.

Introduction
The advent of contrastive visual-language pre-training has provided a new paradigm for multi-modal learning [16, 17, 22, 42].
Its popularity has been observed across di-verse downstream vision tasks, including 2D or 3D classifi-cation [14, 39, 41, 9], segmentation [27, 48, 36, 44], and de-tection [38, 45, 29]. CLIP [26] is one of the most acknowl-Figure 1: Comparison of Accuracy, Training GFLOPs, and Learnable Parameters on 16-shot ImageNet [3] clas-sification. We compare the training GFLOPs including gra-dient back-propagation, and the icon sizes denote the num-ber of learnable parameters. Our APE and APE-T achieve superior performance with high implementation efficiency. edged contrastive visual-language models and has attained widespread attention for its simplicity and superiority. Pre-trained by massive image-text pairs sourced from the Inter-net, CLIP exhibits remarkable aptitude in aligning vision-language representations with favorable zero-shot perfor-mance on downstream tasks. To further enhance CLIP in low-data regimes, many efforts propose few-shot learn-ing techniques with additional learnable modules upon the frozen CLIP for new semantic domains.
As shown in Figure 2 (a) and (b), existing CLIP-based few-shot methods can be categorized as two groups con-cerning whether to explicitly construct learnable modules by CLIP’s prior knowledge. 1) Non-prior Methods ran-Figure 2: Comparison of Existing CLIP-based Few-shot Methods. We only show the training-required model variants of prior-based methods and our APE-T. EV , ET denote CLIP’s pre-trained visual and textual encoders, respectively. domly initialize the learnable modules without CLIP’s prior, and optimize them during few-shot training. For instance,
CoOp series [47, 46] adopt learnable prompts before CLIP’s textual encoder, and CLIP-Adapter [7] instead learns two residual-style adapters after CLIP. Such networks only in-troduce lightweight learnable parameters but suffer from limited few-shot accuracy, since no pre-trained prior knowl-edge is explicitly considered for the additional modules. 2)
Prior-based Methods construct a key-value cache model via CLIP-extracted features from the few-shot data and are able to conduct recognition in a training-free manner, in-cluding Tip-Adapter [40], Tip-X [33], and CaFo [43]. Then, they can further regard the cache model as a well-performed initialization and fine-tune the cache keys for better classi-fication accuracy. These prior-based methods explicitly in-ject prior knowledge into the training process but are cum-bersome due to the large cache size with enormous learn-able parameters. We then ask the question, can we integrate their merits to make the best of both worlds, namely, not only equipping efficient learnable modules, but also bene-fiting from CLIP’s prior knowledge?
To this end, we propose Adaptive Prior rEfinement, termed as APE, which efficiently adapts CLIP for few-shot classification by refining its pre-trained knowledge in visual representations. APE can not only achieve superior perfor-mance via CLIP’s prior, but also consumes less computa-tion resource than non-prior methods, as shown in Figure 1.
We observe that not all CLIP’s prior, i.e., the extracted vi-sual features of the cache model or test image, are signifi-cant for downstream tasks along the channel dimension. In
Figure 3, we divide the feature channels of CLIP-extracted visual representations into two groups, and respectively vi-sualize their similarity maps with the textual representation in ImageNet [3]. Features in the first group (a) can observe much better vision-language alignment than the second one (b). Motivated by this, we propose a prior refinement mod-ule to adaptively select the most significant feature channels by two criteria, inter-class similarity and variance. By max-imizing the inter-class disparity in few-shot training data, the refined feature channels can discard redundant informa-Figure 3: Similarity Maps for Vision-language Align-ment. We utilize CLIP with ResNet-50 [11] visual encoder and refine 512 feature channels from 1024 ones, where the refined features are more attentive towards object targets. tion and reduce the cache size with less memory cost.
On top of this, we present two variants of our approach, denoted as APE and APE-T. The first one is a training-free model that directly utilizes the refined cache model for inference. APE novelly explores the trilateral affini-ties between the test image, the refined cache model, and the textual representations for robust training-free recogni-tion. The second one, APE-T (Figure 2(c)), simply trains lightweight category residuals on top, other than costly fine-tuning the entire cache model. Such category residuals further update the refined cache model and are shared be-tween modalities to ensure the vision-language correspon-dence. Our APE and APE-T respectively achieve state-of-the-art performance compared with existing training-free and training-required methods on 11 few-shot benchmarks, surpassing the second-best by +1.59% and +1.99% for the average 16-shot accuracy.
The contributions of our work are summarized below:
• We propose Adaptive Prior rEfinement (APE), an adaption method of CLIP to explicitly utilize its prior knowledge while remain computational efficiency.
• After prior refinement, we explore the trilateral affini-ties among CLIP-extracted vision-language represen-tations for effective few-shot learning.
• Our training-free APE and APE-T exhibit state-of-the-art performance on 11 few-shot benchmarks, demon-strating the superiority of our approach. 2.