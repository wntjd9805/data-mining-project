Abstract
Plentiful adversarial attack researches have revealed the fragility of deep neural networks (DNNs), where the imper-ceptible perturbations can cause drastic changes in the out-put. Among the diverse types of attack methods, gradient-based attacks are powerful and easy to implement, arousing wide concern for the security problem of DNNs. Howev-er, under the black-box setting, the existing gradient-based attacks have much trouble in breaking through DNN mod-els with defense technologies, especially those adversari-ally trained models. To make adversarial examples more transferable, in this paper, we explore the ﬂuctuation phe-nomenon on the plus-minus sign of the adversarial pertur-bations’ pixels during the generation of adversarial exam-ples, and propose an ingenious Gradient Relevance Attack (GRA). Speciﬁcally, two gradient relevance frameworks are presented to better utilize the information in the neighbor-hood of the input, which can correct the update direction adaptively. Then we adjust the update step at each iteration with a decay indicator to counter the ﬂuctuation. Exper-iment results on a subset of the ILSVRC 2012 validation set forcefully verify the effectiveness of GRA. Furthermore, the attack success rates of 68.7% and 64.8% on Tencen-t Cloud and Baidu AI Cloud further indicate that GRA can craft adversarial examples with the ability to transfer across both datasets and model architectures. Code is released at https://github.com/RYC-98/GRA. 1.

Introduction
Deep neural networks (DNNs) have made numerous achievements [13, 14, 10, 3, 32, 6, 35]. However, especially in the computer vision ﬁeld, recent researches on the mod-el robustness verify that DNNs are extremely susceptible to human-imperceptible malicious perturbations [1, 11, 31, 4], attracting many researchers to dive into the generation of adversarial examples [7, 5, 22, 20]. Furthermore, crafting
∗Corresponding author: Yuchen Ren
Predict: Cart (99.7%)
Predict: Fountain (52.6%)
Figure 1. Attention maps [28] of a clean image and its adversarial example crafted by GRA on Inc-v3. The target model is Res-152. adversarial examples with strong transferability can expose security defects and explore the inner mechanism of current
DNNs [11, 23, 33], and it has become a vital task in com-puter vision.
From the perspective of the attackers’ knowledge, there are two types of attack settings, i.e., black-box setting and
In the white-box setting, all informa-white-box setting. tion about the target model can be acquired by the attacker-s, and many previous attack methods can already hoax the source model with a nearly 100% attack success rate under this setting [16]. Conversely, in the black-box setting, on-ly the model output is available, which will often degrade the attacking performance on target models, especially for models with defense mechanisms [26, 15, 12, 21, 24]. To deal with this issue, diverse gradient-based attack meth-ods [18, 7, 9, 36, 39], input augmentation transformation-s [8, 38, 37], and ensemble strategy [19] are presented in recent years. Among them, variance tuning (VT) [36] is one of the most promising attack methods, which introduces neighborhood information of the input at the last iteration to stabilize the current update direction. Unfortunately, it ig-nores the gradient relevance between the input and its neigh-borhood, failing to make full use of the neighborhood infor-mation.
In this research, we propose a new gradient-based at-tack named Gradient Relevance Attack (GRA). An exam-ple is provided to show the misdirection capacity of GRA in Figure 1. Concretely, inspired by the framework of dot-product attention [2, 34], we ﬁrst devise two gradient rel-evance frameworks to dig out neighborhood information.
We view the current gradient as the query vector [34] and the gradients calculated from the neighborhood as the key vectors [34], then establish relevance between them through cosine similarity. With the inner relevance information, the update direction is determined by a group of samples’ gra-dients adaptively.  (a) 1K Adversarial examples’ average pixel change proportion 
)
% ( n o i t r o p o r
P 98.5 98.0 97.5 97.0 60.0 40.0 20.0 0.0 (b) 1K Adversarial perturbations’ average sign change proportion 
Admix
MI-FGSM 1 2 3 4 5 6
Iteration 7 8 9 10
Figure 2. The illustration for two kinds of pixel change tenden-cy: (a) Adversarial examples’ pixel value changes; (b) Adversarial perturbations’ sign changes. Note that adversarial examples are crafted on Inc-v3.
Besides, we calculate the mean pixel changes of the ad-versarial examples compared with their clean images on a subset (1k) of ILSVRC 2012 validation set [27]. Five popular gradient-based attacks (MI-FGSM [7], NI-FGSM
[18], VTMI-FGSM, VTNI-FGSM [36] and Admix [37]) are taken into consideration. The result indicates the mean pixel changes are all between 10 and 11 under the maxi-mum constraint ε = 16 and the maximum iteration number
T = 10. Current methods typically add adversarial per-turbations with the magnitude of ε/T on the input at each iteration to craft adversarial examples. We conclude two reasons may result in this fact. One is that many pixels’ val-ues remain unchanged after certain iterations, and we name it the early stop. While the other is caused by the frequent plus-minus sign changes on the pixels of adversarial per-turbations (we simply call it the adversarial perturbations’ sign changes in the following context without ambiguity).
To ﬁgure out the real reason, we study the adversarial ex-amples’ pixel changes and adversarial perturbations’ sign changes between two adjacent iterations. Admix and MI-FGSM are selected as examples, their results are displayed in Figure 2. Figure 2 (a) shows that more than 95% pixels keep changing from the beginning to the end, therefore, the
ﬁrst early stop is impossible. Figure 2 (b) certiﬁes the fre-quent ﬂuctuation of the sign (see Figure 3), because more than half of the adversarial perturbations’ signs are chang-ing even at the end of the generation. In fact, the ﬂuctuation phenomenon in adversarial perturbations’ sign is not always bad, because it can help us ﬁnd the optimum. Whereas the step size is ﬁxed during the generation of adversarial ex-amples, and it keeps us from getting closer to the optimum when facing frequent ﬂuctuation. Consequently, we further integrate a decay indicator to adjust the step size and counter the ﬂuctuation. Combining MI-FGSM with the gradient rel-evance framework and decay indicator, we propose the gra-dient relevance attack (GRA). (cid:2238)(cid:2202)(cid:2879)(cid:2778) 1.6 (cid:2238)(cid:2202)
-1.6 (cid:2238)(cid:2202)(cid:2878)(cid:2778) 1.6 (cid:2183)(cid:2186)(cid:2204) (cid:2206)(cid:2202)(cid:2879)(cid:2778) (cid:2183)(cid:2186)(cid:2204) (cid:2206)(cid:2202) (cid:2183)(cid:2186)(cid:2204) (cid:2206)(cid:2202)(cid:2878)(cid:2778) (cid:2183)(cid:2186)(cid:2204) (cid:2206)(cid:2202)(cid:2878)(cid:2779)
... 20.0 21.6 20.0 21.6
...
Figure 3. The illustration for adversarial perturbations’ sign changes. The xadv and δt are input and adversarial perturbations at the t-th iteration severally. t
Experiment results persuasively verify that GRA has bet-ter performance than other advanced attacks, and becomes the state-of-the-art gradient-based attack method. For ex-ample, the average attack success rate of our method can reach 83.0% on models with defense technologies [33, 26, 15, 12, 21, 24], and it achieves at least 12.3.% improvement over other advanced attack methods.
Our main contributions are summarized as follows:
• We explore the ﬂuctuation on adversarial perturbation-s’ plus-minus sign during the generation of adversarial examples, and devise a decay indicator of the step size to counter the ﬂuctuation.
• We present two kinds of gradient relevance frame-works, which can make full use of the neighbor infor-mation by establishing the gradient relevance between the input and its neighborhood at each iteration.
• We propose an ingenious Gradient Relevance At-tack (GRA) combing with current input augmentation transformations, which can boost the transferability of adversarial examples largely.  
• Comprehensive experiments on normal classiﬁcation classiﬁers, defended classiﬁers and practical online classiﬁers verify that GRA is superior to the latest state-of-the-art gradient-based attacks. 2.