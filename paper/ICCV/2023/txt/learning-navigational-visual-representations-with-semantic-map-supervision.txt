Abstract
Being able to perceive the semantics and the spatial structure of the environment is essential for visual naviga-tion of a household robot. However, most existing works only employ visual backbones pre-trained either with in-dependent images for classification or with self-supervised learning methods to adapt to the indoor navigation domain, neglecting the spatial relationships that are essential to the learning of navigation. Inspired by the behavior that hu-mans naturally build semantically and spatially meaningful cognitive maps in their brains during navigation, in this pa-per, we propose a novel navigational-specific visual repre-sentation learning method by contrasting the agent’s ego-centric views and semantic maps (Ego2-Map). We apply the visual transformer as the backbone encoder and train the model with data collected from the large-scale Habitat-Matterport3D environments. Ego2-Map learning transfers the compact and rich information from a map, such as ob-jects, structure and transition, to the agent’s egocentric rep-resentations for navigation. Experiments show that agents using our learned representations on object-goal naviga-tion outperform recent visual pre-training methods. More-over, our representations significantly improve vision-and-language navigation in continuous environments for both high-level and low-level action spaces, achieving new state-of-the-art results of 47% SR and 41% SPL on the test server. 1.

Introduction
Visual representations for navigation should capture the rich semantics and the complex spatial relationships of the observations, which helps the agent to recognize visual en-tities and its transition in space for effective exploration.
However, previous works usually adopt visual backbones
:Intern at Adobe Research. which only focus on capturing the semantics of a static image, ignoring its connection to the agent or the corre-spondence to other views in a continuous environment [3, 9, 40, 42, 51, 68, 98]. One common approach is to ap-ply encoders pre-trained for object/scene classification (e.g.
ResNet [39] on ImageNet [80]/Places365 [102]), object de-tection (e.g. RCNN [38, 78] on VisualGenome [52]/MS-COCO [57]) or semantic segmentation (e.g. RedNet [45] on SUN RGBD [87]), or more recently, to use CLIP [72], which is trained for aligning millions of images and texts to encode agent’s RGB observations [31, 46, 84]. Despite the increasing generalization ability of the features and rising zero-shot performance on novel targets, there still exists a large visual domain gap between these features and the fea-tures suitable for navigation since they lack the expressive-ness of spatial relationships. For example, the connection between time-dependent observations and the correspon-dence between egocentric views and the spatial structure of an environment, which are important to decision making during navigation.
We suggest that there are two main difficulties in learn-ing navigational-specific visual representations in previ-ous research; First, there lacked a large-scale and realis-tic dataset of indoor environments. Popular datasets such as Matterport3D [8] and Gibson [95] provide traversable scenes rendered from real photos, but either the number of scenes is very limited, or the quality of the 3D scan is low. Synthetic datasets such as ProcTHOR [22] contain 10K generated scenes, but they are unrealistic and simple and do not capture the full complexity of real-world envi-ronments. Second, fine-tuning visual encoders while learn-ing to navigate is very expensive because of the long travel-ing horizon, especially in tasks that require extensive explo-ration [4, 53, 63, 86]. Moreover, due to the scene scarcity, fine-tuning visual encoders is unlikely to generalize well to the novel environments. To address these problems, a large amount of research has been dedicated to augment-ing the environments, either editing the agent’s observations
[56, 90] or adding objects [62], building new spatial struc-tures [58], applying web-images to replace the views of the same categories [35] or generating new scenes [47, 48, 55].
Unlike augmentation, which essentially requires the agent’s policy network to adapt new visual data, another strategy is to first pre-train the visual encoder before learning to navigate. Those methods usually apply images captured in 3D scenes and perform self-supervised learning or tune the encoder with proxy tasks (e.g. masked region modeling, angular prediction) to mitigate the domain gap and intro-duce spatial awareness [15, 17, 96, 98]. Furthermore, recent works tend to apply CLIP [72] to process visual inputs, as the model can provide powerful representations for ground-ing semantic and spatial concepts, hence facilitating learn-ing [1, 31, 46, 83, 84, 85]. Although these methods have demonstrated significant improvements on navigation per-formance, they either only use more data without address-ing the learning problem, not generalizable across different tasks, or ignore the spatial correspondence between consec-utive frames which the agent will capture as it moves.
Inspire by the research in cognitive science that humans naturally build virtual maps from perspective observations that are helpful to track semantics, spaces and movements during navigation [29, 91, 92]; we propose a contrastive learning method between the agent’s Egocentric view pairs and top-down semantic Maps (Ego2-Map) for training vi-sual encoders appropriate for navigation. Specifically, we first sample RGBD images and create corresponding se-mantic maps [7] from the large-scale Habitat-Matterport3D environments (HM3D) [74, 82] which contains hundreds of photo-realistic scenes, obtaining abundant and effective visual data. Then, we encode the sampled RGBD obser-vations and the semantic maps with two separate visual backbones, respectively, and train the entire model with our
Ego2-Map contrastive learning. Once trained, the RGBD encoder will be plugged into an agent’s network to facilitate visual perception. We argue that the map contains very rich and compact visual clues such as spatial structure, acces-sible areas and unexplored regions, object entities and their arrangement, as well as the agent’s transition in the environ-ment that are essential to navigation. Compared to existing online mapping-based approaches [10, 14, 32, 44], Ego2-Map learning efficiently produces visual features that imply a complete map of the space known a view priori, while the online map is only a partial map. Importantly, Ego2-Map explores a new possibility of modeling semantics and struc-tures from simple RGBD inputs, which its resulting features are directly applicable to non-mapping-based navigational models and effectively generalizable to different tasks.
We mainly evaluate the features learned from Ego2-Map on the Room-to-Room Vision-and-Language Navigation in
Continuous Environments (R2R-CE) [3, 51] task, which requires an agent to navigate in photo-realistic environ-ments following human natural language instructions such as “Leave the bedroom and enter the kitchen. Walk for-ward and take a left at the couch. Stop in front of the window.” Addressing R2R-CE highly relies on exploit-ing the correspondence between contextual clues and the agent’s observations, hence it is crucial for the visual en-coder to provide semantic and structural meaningful rep-resentations. We found that the proposed Ego2-Map fea-tures significantly boost the agent’s performance, obtaining
+3.56% and +5.10% absolute SPL improvements over the
CLIP baseline [72] under the settings of high-level and low-level action spaces, respectively, and achieves the new best results on the R2R-CE test server. Moreover, our experi-ments show that Ego2-Map learning also outperforms other visual representation learning methods such as OVRL [96] on the Object-Goal Navigation task (ObjNav), suggesting the strong generalization potential of the proposed methods. 2.