Abstract
Recently, feature-based methods for Online Action De-tection (OAD) have been gaining traction. However, these methods are constrained by their fixed backbone de-sign, which fails to leverage the potential benefits of a trainable backbone.
This paper introduces an end-to-end learning network that revises these approaches, in-corporating a backbone network design that improves ef-fectiveness and efficiency. Our proposed model utilizes a shared initial spatial model for all frames and maintains an extended sequence cache, which enables low-cost in-ference. We promote an asymmetric spatiotemporal model that caters to long-form and short-form modeling. Ad-ditionally, we propose an innovative and efficient infer-ence mechanism that accelerates extensive spatiotempo-ral exploration. Through comprehensive ablation studies and experiments, we validate the performance and effi-ciency of our proposed method. Remarkably, we achieve an end-to-end learning OAD of 17.3 (+12.6) FPS with 72.4% (+1.2%), 90.3% (+0.7%), and 48.1% (+26.0%) mAP on THMOUS’14, TVSeries, and HDD, respectively.
The source code is available at https://github. com/sqiangcao99/E2E-LOAD. 1.

Introduction
Online Action Detection (OAD)[10] has become a crit-ical domain in computer vision, driven by its extensive ap-plicability spanning surveillance, autonomous driving, and more. Recent research endeavors[29, 3, 26, 32] have be-gun embracing the Transformer architecture [24] for this task. By leveraging the attention mechanism’s capability for long-range interactions, these methods manifest marked improvements over their RNN-based counterparts [28, 5].
Nevertheless, most existing studies [28, 29, 3] rely on fea-*Authors contributed equally.
†Corresponding author.
Figure 1: Comparison of Performance (mAP and FPS).
Methods like GateHUB [3] and LSTR [29] have eliminated computation-intensive optical flow inputs to speed up infer-ence, albeit at the cost of a considerable decline in perfor-mance. In contrast, our E2E-LOAD, benefiting from back-bone design and efficient inference mechanism, achieves superior mAP and FPS. tures from pre-trained networks. The dependency on a frozen backbone progressively constrains improvements in both speed and precision. Although there are efforts [30, 4] to fine-tune the backbone directly, they often fall short in balancing outstanding performance with acceptable compu-tation costs. This is primarily because these feature-based methods adopt a paradigm that employs a heavy spatiotem-poral backbone for individual local chunks coupled with a lightweight temporal model for chunk-wise interactions.
Such an architecture often results in a less-than-ideal bal-ance between performance and efficiency. Specifically, the localized employment of the heavy spatiotemporal model might not fully exploit the backbone’s full potential in mod-eling long-term dependencies. Additionally, the subsequent lightweight temporal model often struggles to capture long-term relationships effectively. Moreover, this design im-poses challenges for end-to-end training, as it requires the parallel execution of multiple backbone networks for fea-ture extraction from each video chunk, resulting in substan-tial GPU memory consumption. As a response, this pa-per proposes the design of an end-to-end learning Trans-former for OAD, enhancing its scalability for practical ap-plications.
Specifically, we introduce a novel method named the
End-to-End Long-form Transformer for OAD task, abbre-viated as E2E-LOAD. Our approach employs the “Space-then-Space-time” paradigm. Initially, raw video frames are processed by the spatial model and transformed into fea-tures, which are subsequently cached in a buffer. This tech-nique is instrumental in managing streaming video data, as it allows for the re-utilization of these buffered features across diverse time steps, thereby significantly decreasing computational overhead. Furthermore, the buffering mech-anism boosts the model’s capability to process extended historical sequences, as it retains most frames as compact representations within the buffer, alleviating the computa-tional burden. Next, we partition the sequences conserved in the cache into long-term and short-term histories and con-duct spatiotemporal modeling independently. We imple-ment a shallow branch for the long-term stream and a deep branch for the short-term stream. This asymmetric archi-tecture promotes efficient long-form feature extraction. Fi-nally, we introduce a token re-usage strategy to mitigate the high computation costs of spatiotemporal interactions on extended video clips, achieving a 2× speed enhancement.
Regarding implementation, we train with shorter history sequences and then increase the sequence length for infer-ence. This technique mitigates the training expenses associ-ated with long-term videos while enabling us to leverage the benefits of long-term context. The experiments demonstrate that this strategy effectively reduces training costs without compromising the model’s effectiveness.
Through these architectural innovations and efficiency techniques, E2E-LOAD addresses the limitations inher-ent in feature-based methods, achieving both superior ef-fectiveness and efficiency. A comparison of E2E-LOAD with other feature-based methods is illustrated in Figure 1.
The results underscore that our model excels in efficiency and effectiveness compared to other methods. We per-form comprehensive experiments on three public datasets:
THUMOS14 [13], TVSeries [10], and HDD [20]. E2E-LOAD yields mAP of 72.4 (+1.2)%, mcAP of 90.3 (+0.7)%, and mAP of 48.1 (+26.0)% respectively, showcasing sub-stantial improvements. Notably, E2E-LOAD is roughly 3× faster than these methods in terms of inference speed. (i) We propose
In summary, our key contributions are: a unique end-to-end learning framework that integrates a stream buffer between the spatial and spatiotemporal mod-els, thereby enhancing the effectiveness and efficiency of (ii) We introduce an efficient in-online data processing. ference mechanism that accelerates spatiotemporal atten-tion processing through token re-usage, achieving a 2× re-(iii) Our method achieves sig-duction in running time. nificant accuracy and inference speed advancements using only RGB frames on three public datasets, highlighting its promise for practical use in real-world scenarios. 2.