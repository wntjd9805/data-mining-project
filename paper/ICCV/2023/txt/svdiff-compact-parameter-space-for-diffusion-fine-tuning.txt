Abstract
Diffusion models have achieved remarkable success in text-to-image generation, enabling the creation of high-quality images from text prompts or other modalities. How-ever, existing methods for customizing these models are lim-ited by handling multiple personalized subjects and the risk of overfitting. Moreover, their large number of parameters is inefficient for model storage. In this paper, we propose a novel approach to address these limitations in existing text-to-image diffusion models for personalization. Our method involves fine-tuning the singular values of the weight ma-trices, leading to a compact and efficient parameter space that reduces the risk of overfitting and language-drifting.
We also propose a Cut-Mix-Unmix data-augmentation tech-nique to enhance the quality of multi-subject image genera-tion and a simple text-based image editing framework. Our proposed SVDiff method has a significantly smaller model size compared to existing methods (≈2,200 times fewer pa-rameters compared with vanilla DreamBooth), making it more practical for real-world applications. 1.

Introduction
Recent years have witnessed the rapid advancement of diffusion-based text-to-image generative models [19, 53, 20, 47, 51], which have enabled the generation of high-quality images through simple text prompts. These models are capable of generating a wide range of objects, styles, and scenes with remarkable realism and diversity. These models, with their exceptional results, have inspired re-searchers to investigate various ways to harness their power for image editing [31, 37, 79].
In the pursuit of model personalization and customiza-tion, some recent works such as Textual-Inversion [16],
DreamBooth [52], and Custom Diffusion [32] have further unleashed the potential of large-scale text-to-image diffu-sion models. By fine-tuning the parameters of the pre-trained models, these methods allow the diffusion models to be adapted to specific tasks or individual user preferences.
*Work done during an internship at Google Research.
Figure 1. Applications of SVDiff . Style-Mixing: mix styles from personalized objects and create novel renderings; Multi-Subject: generate multiple subjects in the same scene; Single-Image Edit-ing: text-based editing from a single image.
Despite their promising results, there are still some limi-tations associated with fine-tuning large-scale text-to-image diffusion models. One limitation is the large parameter space, which can lead to overfitting or drifting from the original generalization ability [52]. Another challenge is the difficulty in learning multiple personalized concepts es-pecially when they are of similar categories [32].
To alleviate overfitting, we draw inspiration from the ef-ficient parameter space in the GAN literature [50] and pro-pose a compact yet efficient parameter space, spectral shift, for diffusion model by only fine-tuning the singular val-ues of the weight matrices of the model. This approach is inspired by prior work in GAN adaptation showing that constraining the space of trainable parameters can lead to improved performance on target domain [48, 36, 41, 64].
Comparing with another popular low-rank constraint [22], the spectral shifts utilize the full representation power of the weight matrix while being more compact (e.g. 1.7MB for 1
StableDiffusion [51, 15, 69], full weight checkpoint con-sumes 3.66GB of storage). The compact parameter space allows us to combat overfitting and language-drifting issues, especially when prior-preservation loss [52] is not applica-ble. We demonstrate this use case by presenting a simple
DreamBooth-based single-image editing framework.
To further enhance the ability of the model to learn mul-tiple personalized concepts, we propose a simple Cut-Mix-Unmix data-augmentation technique. This technique, to-gether with our proposed spectral shift parameter space, en-ables us to learn multiple personalized concepts even for semantically similar categories (e.g. a “cat” and a “dog”).
In summary, our main contributions are:
• We present a compact (≈2,200× fewer parameters compared with vanilla DreamBooth [52], measured on
StableDiffusion [51]) yet efficient parameter space for diffusion model fine-tuning based on singular-value decomposition of weight kernels.
• We present a text-based single-image editing frame-work and demonstrate its use case with our proposed spectral shift parameter space.
• We present a generic Cut-Mix-Unmix method for data-augmentation to enhance the ability of the model to learn multiple personalized concepts.
This work opens up new avenues for the efficient and ef-fective fine-tuning large-scale text-to-image diffusion mod-els for personalization and customization. Our proposed method provides a promising starting point for further re-search in this direction. 2.