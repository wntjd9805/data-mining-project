Abstract
Inferring affordance for 3D articulated objects is a chal-lenging and practical problem. It is a primary problem for applying robots to real-world scenarios. The exploration can be summarized as figuring out where to act and how to act. Correspondingly, the task mainly requires produc-ing actionability scores, action proposals, and success like-lihood scores according to the given 3D object informa-tion and robotic information. Current works usually di-rectly process multi-modal inputs with early fusion and ap-ply critic networks to produce scores, which leads to in-sufficient multi-modal learning ability and inefficiently it-erative training in multiple stages. This paper proposes a novel Multimodality-Aware Autoencoder-based affordance
Learning (MAAL) for the 3D object affordance problem. It is an efficient pipeline, trained in one go, and only requires a few positive samples in training data. More importantly,
MAAL contains a MultiModal Energized Encoder (MME) for better multi-modal learning. It comprehensively models all multi-modal inputs from 3D objects and robotic actions.
Jointly considering information from multiple modalities, the encoder further learns interactions between robots and objects. MME empowers the better multi-modal learning ability for understanding object affordance. Experimental results and visualizations, based on a large-scale dataset
PartNet-Mobility, show the effectiveness of MAAL in learn-ing multi-modal data and solving the 3D articulated object affordance problem. 1.

Introduction
Recently, robots have been widely used in various appli-cations in manufacturing, transportation, and other indus-tries. Toward diverse tasks, a fundamental requirement is to interact with objects by robots. To this end, the robots
Figure 1. Comparison of methods. MAAL contains a MME mod-ule, which provides better multi-modal learning ability. Besides, previous methods with critics or decoders require multiple train-ing stages. MAAL pipeline only contains one step and is trained in one go, which is more efficient. need to understand real-world objects, use grippers or other manipulators in the robotic system, and interact with given objects in a given scenario. As a primary problem, the ob-ject affordance problem [21, 14] is conceptualized and sum-marized as the first step for the interaction of robots and objects.
It aims to figure out where and how to interact with an object by the robot in a given environment. Many works [4, 29] propose various solutions to solve the affor-dance problem. However, due to the diversity of instances and complexity of practical robotic scenarios, the problem is still far from being resolved.
Specifically, recent works focus on the affordance prob-lem of interacting with 3D articulated objects [30, 9]. Mo et al. [28] introduce a solid benchmark for learning to ma-nipulate articulated objects. They construct a large-scale
3D articulated object dataset and formulates a standard benchmark for the 3D articulated object affordance prob-lem. Wang et al. [45] consider the kinematic and dynamic uncertainties of objects. They design multiple critics to im-prove the understanding of hidden kinematic information in articulated objects. More works [29, 53] continuously emerge, pushing the frontier of solving the 3D object affor-dance problem.
Moreover, previous works can be concluded as early fusion [22] for learning multi-modal data and critic-based learning [28, 45] for 3D object affordance. Specifically, they usually concatenate all data (e.g., the point cloud of a 3D object, the robot gripper direction, etc.) as inputs. Then, multiple critics or decoders, trained by classification loss according to labels (negative or positive) initially, are intro-duced to leverage supervision for other networks.
The straightforward idea leads to significant advance-ments but still has two defeats. First, learning of inputs ne-glects the correlation between multi-modal data. In the 3D object affordance problem, the input data are from various modalities (i.e., object modality and robot modality). The relationships and interactions between objects and robots are valuable clues for understanding affordance [14, 21].
However, as shown in Fig 1, direct concatenation, as in
[28, 45], considering as an early fusion operation [22], would miss the correlation between inputs [27, 49]. This leads that the multi-modal inputs and their interaction may not be sufficiently learned by the previous works. Second, the critic-based pipeline is not efficient enough. It requires adequately labeled samples to teach the critics to distinguish the difference between negative samples and positive sam-ples [51, 52]. However, as in [28], training data of articu-lated object affordance are sampled from SE(3) space, and most actions fail during manipulation. This means most of the samples are negative. For example, sometimes, only 1% [28] of the data are positive samples for pulling action.
Training of critic-based methods needs all the samples for training and consumes larger training time. Moreover, crit-ics or decoders need to be trained independently. Then, they will be fixed or iteratively updated with the training of other networks, as shown in Fig 1. The training procedure with multiple stages further increases the overall training time.
To overcome above defeats, we present a novel solu-tion named Multimodality-Aware Autoencoder-based affor-dance Learning (MAAL). In MAAL, a MultiModal Ener-gized Encoder (MME) is introduced to handle multi-modal inputs in the affordance problem. MME energizes the multi-modal learning ability to understand 3D object affordance.
Then, rather than a critic-based designation, MAAL lever-ages the deep autoencoder (AE) [11, 16] to solve the affor-dance problem and achieve better training efficiency.
Toward better multi-modal learning, MME is proposed to comprehensively understand data from various modali-ties and fused features at different levels. Specifically, it in-volves three branches, carefully designed for learning infor-mation in object modality, robot modality, and their interac-tions. This empowers MAAL to pursue a better understand-ing of affordance from different perspectives in modalities.
Moreover, rather than directly concatenating all data and applying early fusion for various modalities, our encoder considers the correlation between inputs and fuses multi-level features according to the modalities. This can formu-late better multi-modal learning than simply early fusion, as proved in [49, 31, 5].
Furthermore, MAAL introduces AE [11] pipeline to solve the 3D affordance problem more efficiently. AE can learn the valuable pattern [51, 42, 52] in high-dimensional data points without labeled examples [15, 13, 6]. This prop-erty leads AE can only use positive samples to learn specific valuable patterns from datasets. This also induces the bet-ter computational efficiency of the AE pipeline in solving the affordance problem. Besides, rather than learning repre-sentations with multiple critics, it only uses reconstruction loss [52, 51] as supervision. The overall pipeline can be trained in one go without multiple training steps for differ-ent parts. All these advantages lead that MAAL can achieve better training efficiency than previous critic-based works.
In addition to the above encoder, our MAAL has an ac-tion memory and an action decoder, which are used to for-mulate the AE pipeline. More than applying AE, MAAL specifically considers the properties of 3D object affor-dance, which takes object information as known conditions and aims to produce action proposals. Correspondingly,
MAAL takes multi-modal data as inputs and only recon-structs action proposals as outputs. This leads the network to concentrate on learning action information and the in-teraction between robots and objects rather than remem-bering object information and overfitting to some points in objects. Overall, MAAL fully considers the multi-modal inputs, leverages the AE pipeline, and formulates a novel framework for learning 3D articulated object affordance.
Our main contribution can be summarized as follows: 1. We propose a novel pipeline named Multimodality-Aware Autoencoder-based affordance Learning (MAAL).
It is an efficient framework for solving the 3D object af-fordance problem. MAAL does not need multiple training steps and only requires a few data samples compared to pre-vious methods. 2. We propose MultiModal Energized Encoder (MME) to handle the multi-modal information and their interaction in the 3D object affordance problem. The proposed encoder comprehensively learns data in all modalities and provides better multi-modal learning ability. 3. Without bells and whistles, our method outperforms all current methods in both F-score and sample success rate.
Visualizations also show the effectiveness of our MAAL.
2.