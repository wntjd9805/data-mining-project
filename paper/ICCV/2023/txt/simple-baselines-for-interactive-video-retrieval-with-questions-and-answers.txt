Abstract
To date, the majority of video retrieval systems have been optimized for a “single-shot” scenario in which the user submits a query in isolation, ignoring previous interactions with the system. Recently, there has been renewed inter-est in interactive systems to enhance retrieval, but existing approaches are complex and deliver limited gains in perfor-mance. In this work, we revisit this topic and propose several simple yet effective baselines for interactive video retrieval via question-answering. We employ a VideoQA model to simulate user interactions and show that this enables the pro-ductive study of the interactive retrieval task without access to ground truth dialogue data. Experiments on MSR-VTT,
MSVD, and AVSD show that our framework using question-based interaction significantly improves the performance of text-based video retrieval systems. Code is available at https://github.com/kevinliang888/IVR-QA-baselines. 1.

Introduction
Given the surging popularity of video content, there is a pressing need to develop tools that enable users to search video collections accurately and efficiently. The text-video re-trieval task aims to meet this need by ranking videos among a gallery according to how well they match a given text query.
This task has seen tremendous progress in recent years, ben-efiting from large-scale pretraining datasets [47, 41], mul-timodal architectures [46, 38, 17] and robust optimisation strategies for cross-modal representation learning [45].
Much of the work to date has focused on a “single-shot” scenario in which a user submits a single query in isolation.
While this may be applicable in some scenarios, there are many in which it would be useful to allow refinements to the initial ranking (particularly when the pool of valid re-sponses to the initial query is large). To a large degree, this focus single-shot systems can be explained by practicality: obtaining the data required to develop and evaluate interac-tive retrieval systems is extremely challenging. User studies that engage individuals in interactive sessions are difficult to scale, while production data from product deployments is
Figure 1: Interactive video retrieval with questions and answers. We revisit interactive video retrieval with a sim-ple framework that employs Video Question Answering (VideoQA) to simulate user responses, sidestepping the need to collect dialogue data for development and evaluation. challenging to adapt to the development of new systems.
Although it has received less attention, the topic of inter-active retrieval is far from new—it has been studied for more than two decades [53]. Following an initial query, various mechanisms have been offered to the user to enable them to refine results: relevance scores [53] (corresponding to highly relevant, irrelevant, etc.), relative attributes [29] (e.g.
“shiner than these”), natural language descriptions [19, 57] (e.g. “unlike the given result, the one I want has fur on the back”) and question-answer dialog [13, 43] (e.g. “Q: are there any people in the shot?” A: No”).
Among dialogue-based approaches, the recent work of
Madasu et al. [43] highlighted the potential of question-answering with free-form text questions to enhance video retrieval performance. At the heart of this approach is a question-generating system that probes the user for more details about their desired target in order to refine the re-trieval ranking. In such a framework, user responses can be simulated by employing a question-answering model to synthesise plausible answers to the questions (Fig. 1). Our
work draws inspiration from ViReD [43], but differs in two key aspects. (1) The ViReD answer generator works by first captioning, then extracting the answer from the text of the caption. The disadvantage of this approach is that the answer indirectly depends on the visual content. Instead, we adopt a video question-answering model to simulate the answer, incorporating the information from the video di-rectly with the question. (2) ViReD fine-tunes on AVSD with the human-collected dialogue while our approach does not require any fine-tuning. We show that our approach gen-eralises well to multiple datasets, and obtains significantly better performance than ViReD.
In this work, we make the following contributions: (1)
We propose a simple, yet effective framework for interac-tive video retrieval based on video question answering, and explore the design space of question generators within this framework. (2) We develop three simple question genera-tors that leverage heuristics and learned models to gather discriminative information beyond objects (scenes, actions, colour etc.) in the target video. (3) Through careful experi-ments, we demonstrate that our system generalizes well to several video datasets and obtains significant improvement over baseline without fine-tuning. These results, which can be interpreted as leveraging an approximate interactive or-acle, highlight the potential of question-based interactive retrieval frameworks to substantially enhance video search. 2.