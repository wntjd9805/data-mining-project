Abstract
Many studies in vision tasks have aimed to create ef-fective embedding spaces for single-label object prediction within an image. However, in reality, most objects possess multiple specific attributes, such as shape, color, and length, with each attribute composed of various classes. To apply models in real-world scenarios, it is essential to be able to distinguish between the granular components of an ob-ject. Conventional approaches to embedding multiple spe-cific attributes into a single network often result in entangle-ment, where fine-grained features of each attribute cannot be identified separately. To address this problem, we pro-pose a Conditional Cross-Attention Network that induces disentangled multi-space embeddings for various specific attributes with only a single backbone. Firstly, we employ a cross-attention mechanism to fuse and switch the infor-mation of conditions (specific attributes), and we demon-strate its effectiveness through a diverse visualization ex-ample. Secondly, we leverage the vision transformer for the first time to a fine-grained image retrieval task and present a simple yet effective framework compared to ex-isting methods. Unlike previous studies where performance varied depending on the benchmark dataset, our proposed method achieved consistent state-of-the-art performance on the FashionAI, DARN, DeepFashion, and Zappos50K benchmark datasets. 1.

Introduction
ImageNet [2] is a representative benchmark dataset to verify the visual feature learning effects of deep learning models in the vision domain. However, each image has only one label, which cannot fully explain the various features of real objects. For example, a car can be identified with various attributes such as category, color, and length, as in
Figure 1. As shown in Figure 1 (a), the general method of forming embeddings for objectsâ€™ various attributes involves constructing neural networks equal to the number of spe-*Corresponding author
Figure 1: Multiple Networks vs Single Network for Multi-space embedding. CCA means our proposed Conditional
Cross Attention Network. cific attributes, and creating multiple embeddings for vision tasks such as image classification [6, 22, 8] and retrieval
[10, 20]. Unlike conventional methods, this study presents a technique that embeds various attributes into a single net-work. We refer to this technique as multi-space attribute-specific embedding Figure 1 (b).
Embedding space aims to encapsulate feature similari-ties by mapping similar features to close points and dis-similar ones to farther points. However, when the model attempts to learn multiple visual and semantic concepts si-multaneously, the embedding space becomes complex, re-sulting in entanglement; thus, points corresponding to the same semantic concept can be mapped in different regions.
Consequently, embedding multiple concepts in an image into a single network is very challenging. Although previ-ous studies attempted to solve this problem using convolu-tional neural networks (CNNs) [26, 13, 4, 20], they have required intricate frameworks, such as the incorporation of multiple attention modules or stages, in order to identify specific local regions that contain attribute information.
Recently, there has been an increase in research related to ViT [11], which outperforms existing CNN-based mod-Figure 2: Previous works (CSN, ASEN, CAMNet) vs. Ours (CCA) els in various vision tasks, such as image classification [11], retrieval [21], and detection [1]. In addition, research ana-lyzing how ViT learns representations compared to CNN is underway [17, 15, 14]. Raghu et al. [17] demonstrated that the higher layers of ViT are superior in preserving spatial locality information, providing improved spatially discrim-inative representation than CNN. Some attributes of an ob-ject are more easily distinguished when focusing on specific local areas. So, we tailor the last layer of ViT to recognize specific attributes based on their spatial locality, which pro-vides fine-grained information about a particular condition.
Figure 2 summarizes the difference between existing CNN-based and proposed ViT-based methods. This study makes the following contributions: 1. Entanglement occurs when embedding an object con-taining multiple attributes using a single network. The proposed CCA that applies a cross-attention mecha-nism can solve this problem by adequately fusing and switching between the different condition information (specific attributes) and images. 2. This is the first study to apply ViT to multi-space embedding-based image retrieval tasks. In addition, it is a simple and effective method that can be applied to the ViT architecture with only minor modification.
Moreover, it improves memory efficiency by forming multi-space embeddings with only one ViT backbone rather than multiple backbones. 3. Most prior studies showed good performance only on specific datasets. However, the proposed method yields consistently high performance on most datasets and effectively learns interpretable representations.
Moreover, the proposed method achieved state-of-the-art (SOTA) performance on all relevant benchmark datasets compared to existing methods. 2.