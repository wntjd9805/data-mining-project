Abstract
Image captioning is conventionally formulated as the task of generating captions for images that match the dis-tribution of reference image-caption pairs. However, refer-ence captions in standard captioning datasets are short and may not uniquely identify the images they describe. These problems are further exacerbated when models are trained directly on image-alt text pairs collected from the inter-net. In this work, we show that it is possible to generate more speciﬁc captions with minimal changes to the train-ing process. We implement classiﬁer-free guidance [14] for an autoregressive captioning model by ﬁne-tuning it to estimate both conditional and unconditional distributions over captions. The guidance scale applied at decoding controls a trade-off between maximizing p(caption|image) and p(image|caption). Compared to standard greedy de-coding, decoding with a guidance scale of 2 substantially improves reference-free metrics such as CLIPScore (0.808 vs. 0.775) and caption→image retrieval performance in the CLIP embedding space (recall@1 44.6% vs. 26.5%), but worsens standard reference-based captioning metrics (e.g., CIDEr 78.6 vs 126.1). We further explore the use of language models to guide the decoding process, obtaining small improvements over the Pareto frontier of reference-free vs. reference-based captioning metrics that arises from classiﬁer-free guidance, and substantially improving the quality of captions generated from a model trained only on minimally curated web data. 1.

Introduction
Image captioning is both a difﬁcult task for computer vi-sion systems to perform and a difﬁcult task to evaluate. Al-though automated captioning metrics rank the best caption-ing systems higher than humans, human raters still show a strong preference for human-generated captions [20], sug-gesting shortcomings in both captioning models and met-rics. One shortcoming relates to the lack of speciﬁcity in generated captions. Conventional maximum likelihood-∗Work performed while at Google.
†Work performed as a student researcher at Google.
γ=1.0 a man riding a blue motorcycle on a dirt road
γ=1.5 a man riding a blue motorcycle on a straw bale
γ=2.0 rider on blue suzuki motorcycle near straw bales
γ=3.0 racer on blue suzuki motorcycle leaning up against straw bales
GT A person riding a baby blue motorcycle near haystacks
Figure 1. Using classiﬁer-free guidance (γ > 1) results in more speciﬁc captions that are farther from the reference distribution.
Left: Example of captions generated at different guidance scales for a single image. Right: Caption→image recall@1 with CLIP
ViT-B/32 vs. CIDEr score, for captions generated with different guidance scales γ on MS-COCO. Higher scales improve retrieval accuracy at the expense of CIDEr. based image captioning models attempt to generate cap-tions such that the p(caption|image) is high. However, captions from the ground truth distribution are often non-speciﬁc, e.g., human annotators will usually describe a Ger-man Shepard only as a dog. Moreover, previous work has emphasized “reference-based” captioning metrics that measure the match between generated captions and human-provided ground truth captions [28, 23, 42]. These metrics intrinsically penalize captions that are more speciﬁc than ground truth.
In this work, we explore strategies to guide image cap-tioning models to produce more speciﬁc captions by mod-ifying the decoding distribution, and explore the trade-offs in captioning metrics that result. We ﬁrst investigate the application of classiﬁer-free guidance (CFG) [14] to image captioning with autoregressive models. Classiﬁer-free guidance increases p(image|caption) at the expense of p(caption|image). Although CFG hurts reference-based image captioning metrics such as BLEU [28], ROUGE [23], and CIDEr [42], it improves “reference-free” metrics that measure captions’ speciﬁcity via the similarity between the
image and the generated caption in the embedding space of image-text models [13] or caption→image retrieval perfor-mance. Qualitatively, we ﬁnd that captions generated with
CFG are more speciﬁc than both the ground truth captions and captions generated without CFG, but they are less gram-matical, particularly at high CFG scales.
Beyond classiﬁer-free guidance, we experiment with guiding image captioning models using the probability distribution obtained from a few shot-prompted language model (LM). We ﬁnd that using a language model to guide a captioning model trained on MS-COCO [24] with descriptive manually written captions can allow it to achieve slightly better trade-offs between reference-free vs. reference-based captioning metrics than those observed with CFG. LM guidance also substantially improves the captions produced by a model trained exclusively on min-imally curated web data. Although this model achieves a
CIDEr score of only 21.8 without guidance, this CIDEr score improves to 57.4 when guided by a language model prompted with 20 captions from the MS-COCO training set.
In summary, our contributions are as follows:
• We propose two strategies to guide image captioning models to produce more speciﬁc captions: classiﬁer-free guidance and language model guidance.
• We demonstrate that classiﬁer-free guidance yields cap-tions that are closer to the corresponding image in the embedding space of image-text models, but are farther from human-provided reference captions.
• We show that language model guidance can alter cap-tion styles, substantially improving captions produced by a model trained only on minimal curated web data and marginally improving the trade-off between captioning metrics observed with classiﬁer-free guidance. 2.