Abstract
The joint visual-language model CLIP has enabled new and exciting applications, such as open-vocabulary seg-mentation, which can locate any segment given an arbi-In our research, we ask whether it is trary text query. possible to discover semantic segments without any user guidance in the form of text queries or predefined classes, and label them using natural language automatically? We propose a novel problem zero-guidance segmentation and the first baseline that leverages two pre-trained general-ist models, DINO and CLIP, to solve this problem with-out any fine-tuning or segmentation dataset. The general idea is to first segment an image into small over-segments, encode them into CLIP’s visual-language space, translate them into text labels, and merge semantically similar seg-ments together. The key challenge, however, is how to encode a visual segment into a segment-specific embed-ding that balances global and local context information, both useful for recognition. Our main contribution is a novel attention-masking technique that balances the two contexts by analyzing the attention layers inside CLIP. We also introduce several metrics for the evaluation of this new task. With CLIP’s innate knowledge, our method can precisely locate the Mona Lisa painting among a museum crowd (Figure 1). More results are available at https:
//zero-guide-seg.github.io/. 1.

Introduction
Semantic segmentation is a core computer vision prob-lem that seeks to partition an image into semantic regions.
Traditionally, the semantic classes of interest need to be pre-defined and are limited in number [22]. Earlier methods thus cannot generalize beyond the training classes. With re-cent advances in joint vision-language representation learn-ing, e.g., CLIP [25], newer methods [21, 19, 36] can suc-cessfully predict segments corresponding to arbitrary text queries in a novel task called open-vocabulary segmenta-*Equal contribution
Figure 1. Zero-guidance Segmentation segments input images and generated text labels for all segments without any guidance or prompting. Our method produces these results using only pre-trained networks with no fine-tuning or annotations. tion. These segmentation methods are guided by a text query, which describes what already exists in the image and must be provided by the user. Another meaningful mile-stone, however, is how we can segment an image with-out user input or guidance like text queries or predefined classes, and label such segments automatically using natu-ral language. Our work provides the first baseline for this novel problem, referred to as zero-guidance segmentation.
Our work is inspired by a recent research direction that solves segmentation by leveraging CLIP [36, 39]; how-ever, our key distinction is that we require no segmentation datasets, no text query guidance, and no additional training or fine-tuning. This problem is challenging partly because
CLIP has been trained with image captions that globally de-scribe the scenes and provide no spatially specific informa-tion for learning segmentation. Surprisingly, we show that it is possible to distill the learned knowledge from two gener-alist models: a self-supervised visual model, DINO [2], and a visual-language model, CLIP [25], to solve zero-guidance segmentation without further training.
The overall idea is to first over-segment an image into small segment candidates, then translate each segment into
words, and finally join semantically similar segments to form the output segments. In particular, we identify seg-ment candidates by clustering deep pixel-wise features from a DINO that takes as input our image. Despite using no training labels, DINO has been shown to produce class-discriminative features, allowing unsupervised segmenta-tion of the primary object in an image [2] or part co-segmentation across images [1]. However, our main chal-lenge lies in the next step, which maps each segment into a meaningful representation that can be later translated to words. We leverage CLIP’s joint space to model this inter-mediate representation.
A naive way to project a segment to CLIP’s joint space is to input the segment directly into CLIP’s image encoder, but this entirely ignores the surrounding context needed for object recognition and disambiguation [38, 5, 4]. Alterna-tively, masking can be applied inside the attention layers, as done with a transformer-based segmentation network [6].
However, when applied to CLIP’s encoder, which was not trained for segmentation, these techniques struggle to pro-duce segment-specific embeddings due to the domination of global context information. Evidence in [40, 37, 13] also suggests that a transformer trained with image-level annota-tions, such as CLIP, may lose local information in its tokens in later layers. We discovered similar issues: masking in earlier layers removes global contexts and hurts recognition, whereas masking in later layers fails to focus the embedding on a given segment, resulting in all embeddings describing the same dominant object in the image.
Another difficulty in balancing global and local contexts is that different objects may require different degrees of context balancing. For small objects, their CLIP embed-dings can be dominated by global contexts, which describe other prominent objects in the scene. This phenomenon matches the characteristics of CLIP’s training captions, which often ignore unimportant objects in the image. As a result, less prominent objects may require less of the global contexts to highlight their semantics and local contexts.
To solve this, we introduce a novel attention-masking technique called global subtraction, which helps adjust the influence of global contexts in the output embedding. The key idea is to first estimate the saliency or the presence of a given segment in the global contexts by analyzing CLIP’s attention values. Then, this saliency value will be used to determine how much global contexts should be attenu-ated in the segment’s embedding. The resulting embedding in CLIP’s joint vision-language space allows us to readily translate it to text labels with an existing image-to-text gen-eration algorithm [30]. And finally, we merge semantically similar segments with simple thresholding by considering both their visual and text similarities.
To evaluate our algorithm that can output arbitrary text labels, we also propose new evaluation metrics. Evaluating an algorithm under this setup is not straightforward as pre-dicted labels may not necessarily match predefined labels in the test set but can still be correct. This may result from the use of synonyms, such as “cat” vs. “feline,” or differ-ences in label granularity, such as “cat” vs. “orange cat” or
“cat’s nose” or “kitten.” Generally, there is no single correct level of granularity, and each dataset may arbitrarily adopt any level. To address this, we propose to first map the pre-dicted semantic labels to the existing ones in a given test set. After that, we can use standard measurements, such as segment IoU, to evaluate the results as if the algorithm performs segmentation with the predefined test classes. We also introduce Segment Recall, which measures how often ground-truth objects are discovered, and Text Generation
Quality, which tests the quality of our embedding technique given ground-truth oracle segmentation.
Our technique can automatically segment an image into meaningful segments as shown in Figure 1 without any supervision or text guidance. There are still performance gaps between our technique and other supervised methods or methods fine-tuned on segmentation datasets—but none can specifically solve our problem that lacks user guidance.
Nonetheless, we provide a detailed analysis on obstacles that lie ahead as well as ablation studies for the first ap-proach to this problem. In summary, our contributions are:
• We introduce the first baseline to a novel problem, zero-guidance segmentation, which aims to segment and label an input image in natural language without predefined classes or text query guidance. Our method does not require a segmentation dataset or fine-tuning.
• We propose a novel attention-masking technique to convert a segment into an embedding in CLIP’s joint space by balancing global and local contexts.
• We present evaluation metrics for the proposed setup. 2.