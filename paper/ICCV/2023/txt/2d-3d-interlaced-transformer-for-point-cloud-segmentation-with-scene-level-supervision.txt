Abstract door:(cid:1905)(cid:1905) chair:(cid:1905)(cid:1905)… bathtub: 3D 2D data token class token
We present a Multimodal Interlaced Transformer (MIT) that jointly considers 2D and 3D data for weakly supervised point cloud segmentation. Research studies have shown that 2D and 3D features are complementary for point cloud seg-mentation. However, existing methods require extra 2D an-notations to achieve 2D-3D information fusion. Consider-ing the high annotation cost of point clouds, effective 2D and 3D feature fusion based on weakly supervised learning is in great demand. To this end, we propose a transformer model with two encoders and one decoder for weakly su-pervised point cloud segmentation using only scene-level class tags. Speciﬁcally, the two encoders compute the self-attended features for 3D point clouds and 2D multi-view images, respectively. The decoder implements interlaced 2D-3D cross-attention and carries out implicit 2D and 3D feature fusion. We alternately switch the roles of queries
It turns out and key-value pairs in the decoder layers. that the 2D and 3D features are iteratively enriched by each other. Experiments show that it performs favorably against existing weakly supervised point cloud segmenta-tion methods by a large margin on the S3DIS and Scan-Net benchmarks. The project page will be available at https://jimmy15923.github.io/mit_web/. 1.

Introduction
Point cloud segmentation offers rich geometric and se-mantic information of a 3D scene, thereby being essen-tial to many 3D applications, such as scene understand-ing [5, 10, 16, 26, 36], augmented reality [2, 35], and au-tonomous driving [7, 8, 13]. However, developing reliable models is time-consuming and challenging due to the need for vast per-point annotations and the difﬁculty in capturing detailed semantic clues from textureless point clouds.
Research efforts have been made to address the afore-mentioned issues. Several methods are proposed to derive point cloud segmentation models using various weak su-pervisions, such as sparsely labeled points [24, 33, 58, 65], encoders decoder odd layers even layers (cid:20) (cid:14) (cid:25) 3D point cloud (cid:485) (cid:485) multi-view 2D images (cid:485) (cid:485)
[table, chair]
[chair, door]
Overview of the Multimodal
Figure 1:
Interlaced
Transformer (MIT). The input includes a 3D point cloud, multi-view 2D images, and class-level tags of a scene. Our method is a transformer model with two encoders and one decoder. The two encoders compute features for 3D voxel tokens and 2D view tokens, respectively. The decoder con-ducts interlaced 2D-3D attention and carries out 2D and 3D feature fusion. In its odd layers, 3D voxels serve as queries and are enriched by the semantic features of 2D views, act-ing as key-value pairs. In the even layers, the roles of 3D voxels and 2D views switch: 2D views are described by ad-ditional 3D geometric features. bounding box labels [9], subcloud-level annotations [54], and scene-level tags [40, 61]. These weak annotations are cost-efﬁcient and can signiﬁcantly reduce the annotation burden. On the other hand, recent studies [19, 20, 23, 32, 41, 52, 53, 62] witness the remarkable success of 2D vision, and utilize 2D image features to enhance the 3D recognition task. They show promising results because 2D detailed tex-ture clues are well complementary to 3D geometry features.
Although 2D-3D fusion is effective, current methods re-quire extra annotation costs for 2D images. To the best of our knowledge, no prior work has explored fusing 2D-3D features under extremely weak supervision, where only scene-level class tags of the 3D scene are given. It is chal-lenging to derive a segmentation model that leverages both 2D and 3D data under scene-level supervision, as no per-point/pixel annotations or per-image class tags are avail-able to guide the learning process. Furthermore, existing 2D-3D fusion methods require camera poses or depth maps to establish pixel-to-point correspondences, adding extra burdens on data collection and processing.
In this work, we address these difﬁculties by proposing a Multimodal
Interlaced Transformer (MIT) that works with scene-level supervision and can implicitly fuse 2D, and 3D features without camera poses and depth maps.
Our MIT is a transformer model with two encoders and one decoder, and can carry out weakly supervised point cloud segmentation. As shown in Figure 1, the input to our method includes the 3D point cloud, multi-view images, and scene-level tags of a scene. The two encoders utilize the self-attention mechanism to extract the features of the 3D point cloud and the 2D multi-view images, respectively.
The decoder computes the proposed interlaced 2D-3D at-tention and can implicitly fuse the 2D and 3D data.
Speciﬁcally, one encoder is derived for 3D feature ex-traction, where the voxels of the input point cloud yield the data tokens. The other encoder is for 2D multi-view images, where images serve as data tokens. Also, the multi-class to-kens [57] are included to match the class-level annotations.
The encoders capture long-range dependencies and aggre-gate class-speciﬁc features for their respective modalities.
The decoder comprises 2D-3D interlaced layers, and is developed to fuse 2D and 3D features, where the corre-spondences between 3D voxels and 2D views are implic-itly computed via cross-attention. In odd layers of the de-coder, 3D voxels are enriched by 2D image features, while in even layers, 2D views are augmented by 3D geometric features. Speciﬁcally, in each odd layer, each 3D voxel serves as a query, while 2D views act as key-value pairs.
Through cross-attention, a query is a weighted combination of the values. Together with residual learning, this query (3D voxel) is characterized by the fused 3D and 2D fea-tures. In each even layer, the roles of 3D voxels and 2D views switch: 3D voxels and 2D views become key-value pairs and queries, respectively. This way, 2D views are de-scribed by the augmented 2D and 3D features.
By leveraging multi-view information without extra an-notation effort, our proposed MIT effectively fuses the 2D and 3D features and signiﬁcantly improves 3D point cloud segmentation. The main contribution of this work is three-fold. First, to the best of our knowledge, we make the ﬁrst attempt to fuse 2D-3D information for point cloud segmen-tation under scene-level supervision. Second, we enable this new task by presenting a new model named Multimodal
Interlanced Transformer (MIT) that implicitly fuses 2D-3D information via interlaced attention, which does not rely on camera pose information. Besides, a contrastive loss is de-veloped to align the class tokens across modalities Third, our method performs favorably against existing methods on the large-scale ScanNet [11] and S3DIS [3] benchmarks. 2.