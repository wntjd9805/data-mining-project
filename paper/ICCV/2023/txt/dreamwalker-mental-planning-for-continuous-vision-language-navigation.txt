Abstract
VLN-CE is a recently released embodied task, where AI agents need to navigate a freely traversable environment to reach a distant target location, given language instructions.
It poses great challenges due to the huge space of possible strategies. Driven by the belief that the ability to anticipate the consequences of future actions is crucial for the emer-gence of intelligent and interpretable planning behavior, we propose DREAMWALKER — a world model based VLN-CE agent. The world model is built to summarize the visual, to-pological, and dynamic properties of the complicated conti-nuous environment into a discrete, structured, and compact representation. DREAMWALKER can simulate and evaluate possible plans entirely in such internal abstract world, be-fore executing costly actions. As opposed to existing model-free VLN-CE agents simply making greedy decisions in the real world, which easily results in shortsighted behaviors,
DREAMWALKER is able to make strategic planning through large amounts of “mental experiments.” Moreover, the ima-gined future scenarios reﬂect our agent’s intention, making its decision-making process more transparent. Extensive ex-periments and ablation studies on VLN-CE dataset conﬁrm the effectiveness of the proposed approach and outline fruit-ful directions for future work. 1.

Introduction
For decades, the AI community has strived to develop in-telligent robots that can understand human instructions and carry them out. As a small step towards this long-held goal, vision-language navigation (VLN) [6] — the task of entail-ing autonomous agents to navigate in never-before-seen 3D environments with language instructions — gained growing attention. In the standard VLN setting, agent’s movement is constrained to a small set of pre-deﬁned sparse locations.
As pointed out by [45], such over-simpliﬁed, discrete task setup involves many unrealistic assumptions such as known
*Corresponding authors. (cid:109)(cid:97)(cid:82)(cid:64) (cid:112)(cid:64)(cid:72)(cid:93)(cid:77) (cid:57)(cid:97)(cid:108)(cid:93)(cid:75)(cid:3)(cid:33)(cid:97)(cid:75)(cid:77)(cid:93) sofaa table (cid:118)(cid:85)(cid:109)(cid:85)(cid:112)(cid:77)(cid:75)(cid:3) (cid:119)(cid:64)(cid:121)(cid:105)(cid:97)(cid:85)(cid:95)(cid:112) (cid:85)(cid:94)(cid:64)(cid:83)(cid:85)(cid:95)(cid:83) (cid:85)(cid:94)(cid:64)(cid:83)(cid:85)(cid:95)(cid:77)(cid:75)(cid:3) (cid:119)(cid:64)(cid:121)(cid:105)(cid:97)(cid:85)(cid:95)(cid:112) continuous  environment
Instruction: Head out and turn left. Pass a fireplace and continue  towards the sofa. Enter the room and stop when seeing a table. 
Figure 1: In partially observable, continuous VLN environments,
DREAMWALKER maps its surrounding into a discrete and structured abstraction. In this internal world, it is able to conduct mental plan-ning (
) by imagining future scenarios, before taking real action. topology, perfect localization, and deterministic transition.
To better reﬂect the challenges of real world navigation,
Krantz et al.[45] update the discrete VLN to a continuous ver-sion – VLN-CE (VLN in continuous environments), where the agent is free to traverse any unobstructed location with low-level actions. VLN-CE proved much more challenging than its discrete counterpart: the performance gap between the state-of-the-arts in the two settings is more than 20%, in terms of episode success rate. The main challenge posed by
VLN-CE lies in the demand of strategic planning in conti-nuous environments with low-level actions.
As a direct response, we developed a world model based
VLN-CE agent, called DREAMWALKER. Previous studies in cognitive science [17, 34, 35] suggest that humans build a mental model of the local surrounding, based on our limited
senses. This internal world model summarizes our knowle-dge about the environment and serves as the basis for many high-level meta-skills, e.g., reasoning, planning, decision-making, and interpretation. The world model theory is one source of the idea of model-based Reinforcement Learning (RL) [73] and promotes many recent advances in robot con-trol [69, 62, 58, 91]. Keeping this grand idea in head, we let
DREAMWALKER explicitly abstract crucial characteristics of its continuous surrounding environment to a discrete, struc-tured representation (Fig.1). This allows DREAMWALKER to
“imagine” a lot of future possible navigation plans and eva-luate the corresponding consequences entirely in the mind, before taking actual low-level actions in the real world. In this way, DREAMWALKER takes the challenge of VLN-CE head-on: mental planning with discrete world model enables efﬁcient navigation behavior in continuous environments.
Technically, the world model is built upon agent’s past ex-periences and can make predictions about the future. It con-tains two parts: i) An environment graph (EG) is constructed as a composition of selected or predicted waypoints and their typological relations. EG collects agent’s temporary know-ledge about its surrounding. ii) A learnable scene synthesizer (SS) predicts future observations from a waypoint with mul-tiple steps. SS embeds agent’s stable knowledge about envi-ronments, such as general room layout rules and transition dynamics, into its network parameters. Based on the world model, DREAMWALKER synthesizes various future naviga-tion trajectories, and assesses their progress towards the ﬁnal target location. Then, the best mental plan is found by Monte
Carlo Tree Search [41] and executed in the continuous world with low-level actions. With the navigation proceeds, EG is further updated for making a new round of mental planning.
Notably, our DREAMWALKER signiﬁcantly distinguishes itself from prior VLN-CE solutions [67, 29, 43, 44] in the following aspects: i) Recent advanced solutions are essen-tially model-free methods. While in principle a representa-tion of the environment could be implicitly learned through model-free RL, the reinforcement signal may be too weak to quickly learn such a representation and how to make use of it. In contrast, our agent plans its actions within an explicit, and abstract model of the continuous environment. ii) Ex-isting agents navigate by greedily and reactively choosing between a small set of nearby waypoints, based on their hidden state which compresses past observations. They tend to be shortsighted, due to the absence of reliable strategies for capturing information for achieving the future [23]. Yet
DREAMWALKER can use the world model to anticipate the impacts of possible actions and plan strategic behavior. iii)
The future scenarios created by the world model explain the intention of DREAMWALKER in a way that human can un-derstand, making its behaviors more interpretable [9, 79].
Extensive experiments on VLN-CE dataset [45] conﬁrm that our DREAMWALKER gains promising performance with the appealing ability of real-time behavioral interpretation.
This work is expected to foster future research in developing more strategic, robust, and interpretable VLN-CE agents. 2.