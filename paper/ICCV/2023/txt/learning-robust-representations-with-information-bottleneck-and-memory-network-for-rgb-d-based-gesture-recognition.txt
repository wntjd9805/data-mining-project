Abstract
Although previous RGB-D-based gesture recognition methods have shown promising performance, researchers often overlook the interference of task-irrelevant cues like illumination and background. These unnecessary factors are learned together with the predictive ones by the net-work and hinder accurate recognition.
In this paper, we propose a convenient and analytical framework to learn a robust feature representation that is impervious to gesture-irrelevant factors. Based on the Information Bottleneck theory, two rules of Sufficiency and Compactness are de-rived to develop a new information-theoretic loss function, which cultivates a more sufficient and compact represen-tation from the feature encoding and mitigates the impact of gesture-irrelevant information. To highlight the predic-tive information, we further integrate a memory network.
Using our proposed content-based and contextual memory addressing scheme, we weaken the nuisances while pre-serving the task-relevant information, providing guidance for refining the feature representation. Experiments con-ducted on three public datasets demonstrate that our ap-proach leads to a better feature representation and achieves better performance than state-of-the-art methods. The code of our method is available at: https://github.com/
Carpumpkin/InBoMem. 1.

Introduction
Gesture recognition based on RGB-D video has raised much attention of researchers since it has many applica-tions, such as visual surveillance, intelligent transportation, and particularly, human-computer interaction (HCI) [48].
The development of CNN family [35, 5, 26], RNN fam-ily [27, 11], and Transformer-based methods [15, 57, 54]
*Corresponding author
Figure 1. Diagram of our principle. The feature of a gesture sample can be decomposed into the essential gesture feature and gesture-irrelevant ones, such as the background, illumination, and the performer’s appearance. The former provides predictive infor-mation whereas the latter is superfluous. Based on the information bottleneck theory, we can eliminate these disturbing nuisances by minimizing the superfluous information. Meanwhile, as the pre-dictive feature in different samples is all the same, we leverage the memory network to store and “overlay” them, and then highlight the gesture-relevant predictive feature as the guidance for learning a robust feature representation. promote the improvement of recognition performance sig-nificantly.
Although great progress has been made in this field, the attention has always been drawn to boost performance via better network structure [50, 48, 55, 56] or by introduc-ing extra modalities of data [6, 18, 23, 19]. The interfer-ence of gesture-irrelevant factors is hardly noticed. In most real-world gesture recognition scenarios, the dynamic vari-ations are in two aspects. One is based on the motion of the gesture itself like trajectories of hands/arms, and is cru-cial to the recognition task. The other is related to environ-mental influences like illumination, backgrounds, and the performers’ appearances as depicted in Fig.1. The gesture performing-related factors, such as the velocity, performer’s
concentration, and understanding of the gesture can also affect the quality of gesture presentation. When training a network for gesture recognition, these gesture-irrelevant factors may be learned as a kind of feature, and thus re-sult in some inner-class differences hindering the recogni-tion performance. Therefore, it is necessary to disentangle the recognition-relevant and redundant information to refine the feature representation.
In order to disentangle the recognition task-relevant in-formation and the disturbing gesture-irrelevant factors, we refer to the theory of Information Bottleneck (IB) [34], which engages mutual information (MI) and provides an information-theoretic objective for solving this problem.
The essential idea of IB is mapping the observation of an input to a sophisticated representation, which retains the de-sired characteristics with respect to the prediction of label and simultaneously minimizes the redundant information.
Many methods leverage IB to learn robust representations for downstream tasks like unsupervised multi-view learning
[8], Person Re-identification [33] and human pose estima-tion [20]. In this process, the sophisticated representation is critical since it ensures a compact feature encoding that avoids the interference of task-irrelevant factors. However, it is difficult to obtain such a representation directly with the original IB theory. Approximating MI in high dimensions is hard [25], and thus the task-irrelevant distractors may not be removed. Even though some recent techniques ease the constraint by transforming this issue to a network optimiza-tion problem without explicitly estimating MI [33], the joint optimization of feature encoding and such a representation encoding blindly may fail to reach a satisfied representa-tion since no explicit standard of “good representation” is given. Therefore, to refine the feature representation, we should explicitly highlight what the predictive information is, and take it as guidance for the refinement of the feature representation.
Combining these concerns, we propose an analytic framework to learn a robust representation for RGB-D-based gesture recognition. To address gesture-irrelevant factors, we employ the Information Bottleneck (IB) princi-ple to unify them as ”superfluous information”, in contrast to predictive information like motion trajectories. Then we derive a new objective that compresses the superfluous in-formation in the encoding space and emphasizes the pre-dictive one. To achieve a sufficient yet compact feature representation required by IB, a memory network is incor-porated for explicit guidance. We create a large external memory bank, where the shared predictive information is highlighted by overlaying the features in different memory slots. This process is achieved through memory manipula-tions of writing and reading. Then with the derived objec-tive, the distribution of encoding features is pulled towards that of the robust representation, resulting in improved ges-ture recognition performance.
The contributions of our method can be summarized as three-fold: 1. A framework to develop a robust feature representation for gesture recognition, along with the theoretical anal-ysis based on IB. We extend the existing theoretical analysis and optimize the feature encoding to mitigate the interference of gesture-irrelevant factors. To the best of our knowledge, we are the first to provide in-sights from an information-theoretic view in this field. 2. A scheme to explicitly distill the predictive informa-tion. Utilizing the memory network, we learn the pre-dictive information from various samples to derive a sufficient and compact feature representation. 3. Experiments prove the effectiveness of our design and demonstrate that the proposed method achieves the state-of-the-art performance on three public RGB-D gesture datasets of IsoGD [40, 38], EgoGesture [4, 52] and THU-READ [31, 32]. 2.