Abstract
Visual Speech Recognition (VSR) differs from the com-mon perception tasks as it requires deeper reasoning over the video sequence, even by human experts. Despite the re-cent advances in VSR, current approaches rely on labeled data to fully train or finetune their models predicting the target speech. This hinders their ability to generalize well beyond the training set and leads to performance degenera-tion under out-of-distribution challenging scenarios. Un-like previous works that involve auxiliary losses or com-plex training procedures and architectures, we propose a simple approach, named Lip2Vec that is based on learn-ing a prior model. Given a robust visual speech encoder, this network maps the encoded latent representations of the lip sequence to their corresponding latents from the audio pair, which are sufficiently invariant for effective text decod-ing. The generated audio representation is then decoded to text using an off-the-shelf Audio Speech Recognition (ASR) model. The proposed model compares favorably with fully-supervised learning methods on the LRS3 dataset achieving 26 WER. Unlike SoTA approaches, our model keeps a rea-sonable performance on the VoxCeleb2-en test set. We be-lieve that reprogramming the VSR as an ASR task narrows the performance gap between the two and paves the way for more flexible formulations of lip reading. 1.

Introduction
The process of inferring visual cues from a speaker’s fa-cial expressions and lip movements to interpret speech in a silent setting is refereed to as lip-reading or visual speech recognition (VSR). VSR is mostly useful in environments where the speech is unclear or difficult to hear due to some
Code: https://github.com/YasserdahouML/Lip2Vec
Correspondence: yasser.djilali@tii.ae confounding factors [8, 7]. Hearing and speech-impaired individuals also greatly benefit from VSR [57]. Albeit the small variations around the mouth area, the space of spoken words can be large due to the phonemes composition mech-anism. This makes the task highly ambiguous as several phonemes incur similar visual characteristics. Moreover,
VSR needs to be robust to variations w.r.t. multiple speak-ers, head pose movements, non-verbal facial expressions and imaging conditions. Furthermore, lip-reading requires the integration of visual features and contextual information (i.e., topic, key words search, environment and place, etc.)
[53, 33, 6]. Over the last few years, computational methods for VSR has seen a surge with the recent proposed datasets, and can be grouped into (i) word-level prediction that clas-sifies a silent video segment into a pre-defined vocabulary of words; (ii) continuous visual speech recognition, which predicts sentences for varying length video sequences.
Most existing VSR approaches employ a common pipeline, where lip sequences are spatially encoded us-ing a convolution-based backbone and passed to a con-textual encoder (i.e., transformer [59] or conformer [19]) to model temporal dependencies. Finally, auto-regressive transformer decoder cross-attends to these representations for predicting the text. Previous works focused on enhanc-ing the video representations for better decoding, while early approaches pretrained the backbone on word-level
LRW dataset [14] for better convergence on continuous
In contrast, [30, 3] exploit audio informa-VSR [1, 28]. tion as an extra supervision for an auxiliary task. Recently, cross-modal self-supervised pretraining has been a domi-nant paradigm for a smoother supervised finetuning after-wards [50, 51, 20].
Alternatively, the audio latent space exhibits the prop-erties of local smoothness between input and its represen-tation, is temporally coherent over a sequence of obser-vations, has simple dependencies among its factors and is sparsely activated for a specific input, leading to robust and performing models [5, 4, 42, 45]. Whereas the lip sequence
is more ambiguous, with complex dependencies over the sequences as the movements are only a partial observation of a larger system that includes tongue, and other facial muscles[18]. Thus, this highlights a fundamental question about supervised learning on lip-reading data that is likely to result in local generalization, while lacking robustness on out-of-distribution data. In this work, we study these ques-tions, uncovering key representational analogies between audio and lip sequences, the ways in which these analogies can act as a robust support for downstream task transfer, allowing for reprogramming the VSR using off-the-shelf
ASR models. Specifically, our contributions are:
• We propose Lip2Vec framework that simulates VSR as an ASR task by learning a prior network that maps lip sequence features to audio-like representations, which can then be decoded to text using an ASR model.
• Through extensive evaluation, we show that learning the prior network can be exploited for decoding text.
Furthermore, it performs on par with fully-supervised methods on the LRS3 [2] test set and generalizes better on the VoxCeleb2-en [13] test set.
• Our approach addresses the generalization and robust-ness challenges encountered by VSR models. The de-sign explicitly bridges the gap between the VSR and
ASR performances, that is proportional to the quality of the learned prior network.
• Our approach benefits from CTC-only decoding of
ASR models and is 4-10× faster compared to standard
VSR approaches, which decode text auto-regressively. 2.