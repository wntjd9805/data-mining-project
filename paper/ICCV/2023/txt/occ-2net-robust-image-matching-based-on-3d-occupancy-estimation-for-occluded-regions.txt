Abstract
Image matching is a fundamental and critical task in var-ious visual applications, such as Simultaneous Localization and Mapping (SLAM) and image retrieval, which require accurate pose estimation. However, most existing methods ignore the occlusion relations between objects caused by camera motion and scene structure. In this paper, we pro-pose Occ2Net, a novel image matching method that models occlusion relations using 3D occupancy and infers match-ing points in occluded regions. Thanks to the inductive bias encoded in the Occupancy Estimation (OE) module, it greatly simpliﬁes bootstrapping of a multi-view consis-tent 3D representation that can then integrate information from multiple views. Together with an Occlusion-Aware (OA) module, it incorporates attention layers and rotation alignment to enable matching between occluded and visible points. We evaluate our method on both real-world and sim-ulated datasets and demonstrate its superior performance over state-of-the-art methods on several metrics, especially in occlusion scenarios.
Figure 1. Schematic diagram of the Occ2Net. (a) and (b) are im-ages taken from different viewpoints, while (c) shows the match-ing process for occluded regions. In (c), two monitors are shown with green and red masks indicating areas that are visible in (b) but occluded in (a). By using Occ2Net to extract consistent occupancy features and match them between (a) and (b), the monitors that are occluded in (a) can still be matched in (b), thus enabling Occ2Net to have the ability to perform matching under occlusion. 1.

Introduction
Image matching is a fundamental and critical task in various visual applications, such as SLAM and image re-trieval. It aims to identify and correspond to the same or
Im-similar structure/contents from two or more images. age matching can be divided into two categories: feature-based methods [36, 22, 11, 37] and dense methods [40, 16].
Feature-based methods extract sparse keypoints and de-scriptors from images and match them based on similar-ity metrics, while dense methods estimate dense correspon-dences between pixels or patches of images.
However, both types of methods struggle with occlusion scenarios, which are frequent in real-world environments.
Fig. 1 shows an example of these challenges. The two images have a large disparity due to camera motion. Al-though there are considerable overlapping regions, the large disparity causes occlusion, which signiﬁcantly reduces the number of visible matching pairs. Moreover, in this exam-ple, both the ground and wall in the scene have low texture, and large areas are visible in image (b) but occluded in im-age (a), such as the two distinguishable monitors marked as green and red. These factors make it difﬁcult for exist-ing algorithms to extract enough matching pairs for camera pose estimation. Scenarios like these are common in indoor navigation or autonomous driving. To tackle these prob-lems, we propose a novel image matching method called
Occ2Net, which matches not only the visible point pairs but also the occluded points and the visible points.
Based on this observation, we design Occ2Net to match 3D points. Following NeRF [26], we treat each pixel as a ray emitted from the corresponding camera. NeRF [26] obtains 3D points along the ray by sampling at equal inter-vals and learns their information by differentiable render-ing. However, in the matching algorithm, we do not have pose information at inference time, so we simplify the sam-pling along the ray to two points: one visible point and one occluded point. At training time, we use ground truth depths and poses to reproject and determine whether a 3D point is occluded or visible.
Based on these simpliﬁcations, Occ2Net extends the matching between visible-visible points to matching be-tween visible-occluded points. To achieve this goal, we use a 3D Occupancy Estimation (OE) Module, which greatly simpliﬁes the multi-view 3D representation, following [5, 25]. Due to the difﬁculty of 3D matching, the large GPU memory for occupancy and the error in the occupancy es-timation, we do not use the occupancy of the whole image to estimate the matching. We use a coarse-to-ﬁne structure instead. The Occlusion-Aware (OA) module is used in the coarse stage to obtain the matching between patches and the
OE module is used to obtain the ﬁne matching points.
We evaluate our proposed method on two datasets: Scan-Net [7] and TartanAir [45], containing real-world and simu-lated scenes with various degrees of occlusion. We compare our method with several state-of-the-art feature-based and dense methods on several metrics. Experiments show that our method achieves superior accuracy on both datasets, outperforming existing methods by large margins. More-over, our method demonstrates robustness and efﬁciency under occlusion scenarios.
In conclusion, we propose an image matching algorithm that is aware of occluded points and outperforms state-of-the-art methods on both real-world and synthetic datasets.
Speciﬁcally, our contributions are:
• We propose Occ2Net, a novel occlusion-aware image matching algorithm that uses 3D occupancy to model the occlusion relations between objects and infer the location of matching points in occluded regions.
• We combine an Occupancy Estimation (OE) mod-ule with an Occlusion-Aware (OA) module to enable visible-occluded matching using a coarse-to-ﬁne struc-ture with occupancy estimation.
• Our experiments show Occ2Net achieves state-of-the-art pose estimation accuracy on both the real-world dataset ScanNet and the simulated dataset TartanAir. 2.