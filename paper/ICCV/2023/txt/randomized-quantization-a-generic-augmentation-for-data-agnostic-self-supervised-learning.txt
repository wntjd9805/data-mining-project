Abstract
Self-supervised representation learning follows a the data and paradigm of withholding some part of tasking the network to predict it from the remaining part.
Among many techniques, data augmentation lies at the core for creating the information gap. Towards this end, tool masking has emerged as a generic and powerful where content is withheld along the sequential dimension, e.g., spatial in images, temporal in audio, and syntactic in language.
In this paper, we explore the orthogonal channel dimension for generic data augmentation by exploiting precision redundancy. The data for each chan-nel is quantized through a non-uniform quantizer, with the quantized value sampled randomly within randomly sampled quantization bins.
From another perspective, quantization is analogous to channel-wise masking, as it removes the information within each bin, but preserves the information across bins. Our approach significantly surpasses existing generic data augmentation methods, while showing on par performance against modality-specific augmentations. We comprehensively evaluate our approach on vision, audio, 3D point clouds, as well as the DABS benchmark which is comprised of various
The code is available at https: data modalities.
//github.com/microsoft/random_quantize. 1.

Introduction
We are witnessing a convergence of multi-modal AI [23, 6] where the architecture and the learning algorithm are unified for various data modalities. This exciting direction abandons the domain-specific knowledge for an individual data modality, but rather pursues a solution far more gener-alizable.
For self-supervised representation learning, masked modeling [23] or simply masking input as an augmenta-tion [75] has emerged as an effective approach. The in-Figure 1. We represent data as a matrix with a sequential di-mension and a channel dimension. As a generic data augmenta-tion, masking drops tokens along the sequential dimension. The proposed randomized quantization instead withholds information along the channel dimension. In this figure, we use 1D data of 10 sequential tokens for illustration. Data values are coded in grayscale. put data is represented by a 2D tensor with a sequential di-mension and a channel dimension in a modality-agnostic way [3]. The sequential dimension can be spatial in images, temporal in audio, and syntactic in languages. The masking mechanism withholds information along the sequential di-mension, and exploits it for supervision. As a result, models learned from the masking supervision demonstrate strong capability for capturing correlations between sequential to-kens [38].
The channel dimension describes the data feature at each sequential location, for example, RGB color at a spatial lo-cation or spectrogram frequency at a time step. Despite be-ing generic, masking approaches have neglected to exploit supervision along the channel dimension. While the num-ber of channels for images is as small as three, the channels for audio and tabular data can be as many as hundreds. For-mulating the self-supervision from the channel dimension holds much potential for representation learning.
*Equal contribution. Work done during an internship at MSRA.
In this paper, we draw a connection between the sequen-tial masking operation and quantization by exploring quan-tization as a novel form of masking along the channel di-mension. The data in each channel is dynamically quan-tized through a non-uniform quantizer, with the quantiza-tion value randomly sampled from randomly sampled quan-tization bins.
In this way, information within each quan-tization bin is masked out, yet information across bins is retained. The information removed by quantization is con-trolled by the number of bins and the size of the bins, which has been rigorously studied in theory [62]. The larger the distortion rate, the stronger the quantization when it is used as an augmentation for representation learning. The ex-treme case of using only a single bin is equivalent to drop-ping the entire channel. We systematically study various quantization configurations for their effects as a data aug-mentation, for example, with respect to the number bins, uniform or non-uniform bins, and methods to select quanti-zation values.
We apply the randomized quantizer as the only augmen-tation, or in conjunction with augmentations along the se-quential dimension on state-of-the-art Siamese representa-tion learning methods MoCo-v3 [16] and BYOL [34]. In comparisons with domain-agnostic augmentations based on
MixUp [85], our approach achieves state-of-the-art results by a large margin on vision, audio, 3d point cloud, and the DABS benchmark. Compared with domain-specific augmentations, our approach achieves competitive perfor-mance against handcrafted augmentations on vision, and state-of-the-art performance on audio and 3d point clouds.
Our contributions can be summarized as follows:
- We propose a simple yet effective data-agnostic aug-mentation for contrastive learning, based on quantiza-tion along the channel dimension.
- We design a randomization technique by varying the quantization value and the quantization bins, to en-hance data augmentation.
- We demonstrate the generality and strong performance of our channel-wise randomized quantization for vi-sion, audio, 3D point clouds and the DABS benchmark in a data-agnostic way. 2.