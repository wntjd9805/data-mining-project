Abstract
Event-based cameras are ideal for line-based motion es-timation, since they predominantly respond to edges in the scene. However, accurately determining the camera dis-placement based on events continues to be an open prob-lem. This is because line feature extraction and dynamics estimation are tightly coupled when using event cameras, and no precise model is currently available for describing the complex structures generated by lines in the space-time volume of events. We solve this problem by deriving the cor-rect non-linear parametrization of such manifolds, which we term eventails, and demonstrate its application to event-based linear motion estimation, with known rotation from an Inertial Measurement Unit. Using this parametrization, we introduce a novel minimal 5-point solver that jointly es-timates line parameters and linear camera velocity projec-tions, which can be fused into a single, averaged linear ve-locity when considering multiple lines. We demonstrate on both synthetic and real data that our solver generates more stable relative motion estimates than other methods while capturing more inliers than clustering based on spatio-In particular, our method consistently temporal planes. achieves a 100% success rate in estimating linear veloc-ity where existing closed-form solvers only achieve between 23% and 70%. The proposed eventails contribute to a bet-ter understanding of spatio-temporal event-generated ge-ometries and we thus believe it will become a core building block of future event-based motion estimation algorithms.
Project page: https://mgaoling.github.io/eventail/ 1.

Introduction
Event-based cameras are bio-inspired vision sensors that naturally react to edges moving in the scene with microsec-ond temporal resolution and minimal motion blur. These intrinsic properties make events ideal for accurate rela-tive motion estimation, especially under challenging motion and lighting conditions where standard cameras often fall short. Nevertheless, estimating motion from event measure-*indicates equal contribution 1 0.75 0.50 s t 0.25 0 8 2 1 1 0.75 0.50 s t 0.25 2 3 00 6 4 3 2 1 2 8 9 6 0 8 2 1 6 9 4 6 2 3 6 4 3 2 0 0 1 2 8 9 6 px py px 6 9 4 6 py
Figure 1. An event camera observing two non-parallel lines and moving with constant linear and angular velocity. The events trig-gered by each line lie on a manifold, which we call an eventail. We derive a minimal 5-point solver to estimate the parameters of the manifold, which includes both camera motion and scene geometry.
Clustering these events based on spatio-temporal planes as done in previous work [24, 33] generates many spurious clusters (colorful points) with many outliers (grey points). Instead, eventails result in two large clusters with fewer outliers, and a velocity direction error of only 0.01 rad. ments is an open challenge, as motion cues need to be in-ferred from the complex spatio-temporal structures formed by events, which typical vision-based algorithms struggle to grasp. Although event-based cameras have recently demon-strated unprecedented performance [41], the recent devel-opment of autonomous systems has created an increased de-mand for more accurate and reliable solutions which could better exploit the opportunity of improved motion modeling offered by these sensors.
However, while with a traditional camera, solving for relative motion simply means aligning two views with suf-ﬁcient overlap, this problem is not as straightforward to de-ﬁne for an event-based camera since views are not even available in the ﬁrst place. Furthermore, even if the ﬁelds of view of the camera share substantial overlap at two dif-ferent points in time, the structure of the perceived events at those two moments remains very much a function of local
camera dynamics. In the worst case, if the camera ceases to move at all, no more events are triggered, and relative pose estimation becomes an ill-posed problem. It is intuitively clear that—for a dynamic vision sensor—the most funda-mental problem of relative motion estimation is therefore given by the determination of local camera dynamics from a relatively short interval of events. The present paper intro-duces a geometric, deterministic solution to this problem.
The sparse and noisy nature of events has pushed the ge-ometric vision community towards semi-dense approaches that make use of or optimize edge maps [37, 13, 52]. Based on the assumption that the gradient map contains straight lines, a promising area of research, therefore, looks at line features as a possible alternative to assist the geometric solution of relative event camera motion. Works in this area [24, 33], however, have inherent limitations that stem from a wrong assumption made during the initial feature ex-traction step. Indeed, they perform feature extraction inde-pendently of the relative camera displacement information, and they rely on a simple clustering strategy that models the space-time volume of events generated by a line under motion as a plane. However, as will be explained in detail in this work, lines do not form ﬂat planes in the space-time volume of events, even if the camera undergoes constant linear and angular velocity, as evident from Figure 1.
It is thus clear that the problem of line feature extraction in the space-time volume of events can no longer be consid-ered apart from the problem of dynamics estimation. In the present paper, we depart from this approximation and intro-duce a novel feature extractor that relies on a rigorously de-rived geometrical model of line-generated manifolds. Clus-tering the events of one manifold entails the identiﬁcation of the manifold parameters, thereby leading to an implicit so-lution of the linear camera velocity from given angular rates measured by an Inertial Measurement Unit (IMU). Speciﬁ-cally, we make the following contributions:
• We introduce a minimal geometric parametrization of the manifolds that contain all events generated by the observation of a single line under the assumption of locally constant, linear velocity. The parametrization involves the velocity components that are non-parallel to the line, as well as a minimal 3D parametrization of the line itself.
• Based on this incidence relationship, we propose a minimal 5-point solver for the manifold parameters, and demonstrate its application in robust clustering for line feature detection and partial camera dynamics de-termination.
• We conclude with an averaging scheme that fuses the partial camera dynamics observations from each line-generated cluster into a complete estimate of the linear camera velocity, and thereby presents a rigorous theory for deterministic event camera motion initialization.
The present paper focuses on a theoretical understand-ing of line-generated manifold features, and thereby con-tributes to a better understanding of the geometry of the temporally dense-sampling event cameras. The theory is thoroughly evaluated on simulated data, and the advantage of the method is also demonstrated in a few concluding real-world examples. In particular, we show that using our method can consistently achieve a 100% success rate in es-timating linear velocity where existing closed-form solvers achieving a success rate 23% and 70%. 2.