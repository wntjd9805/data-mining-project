Abstract
Recent DETR-based video grounding models have made the model directly predict moment timestamps without any hand-crafted components, such as a pre-defined proposal or non-maximum suppression, by learning moment queries.
However, their input-agnostic moment queries inevitably overlook an intrinsic temporal structure of a video, pro-viding limited positional information.
In this paper, we formulate an event-aware dynamic moment query to en-able the model to take the input-specific content and po-sitional information of the video into account. To this end, we present two levels of reasoning: 1) Event rea-soning that captures distinctive event units constituting a given video using a slot attention mechanism; and 2) moment reasoning that fuses the moment queries with a given sentence through a gated fusion transformer layer and learns interactions between the moment queries and video-sentence representations to predict moment times-tamps. Extensive experiments demonstrate the effectiveness and efficiency of the event-aware dynamic moment queries, outperforming state-of-the-art approaches on several video grounding benchmarks. The code is publicly available at https://github.com/jinhyunj/EaTR. 1.

Introduction
Over the decade, online video platforms have been explo-sively developed, with the number of videos uploaded every day growing exponentially. Accordingly, the amount of work for video search (e.g. video summarization [52, 22], video retrieval [53, 71], text-to-video retrieval [6, 54]) has been ex-plored to enable users to efficiently browse the information they want. While they have presented an efficient way to search videos by considering the whole content, providing a user-defined moment in a video is a different desire. As an alternative to this way, video grounding [1, 31, 41, 46] has been explored in recent years.
*Corresponding author
This research was supported by the National Research Founda-tion of Korea (NRF) grant funded by the Korea government (MSIP) (NRF2021R1A2C2006703). (a) Video grounding (b) Previous DETR-based approach (c) Ours
Figure 1. (a) Video grounding aims to localize timestamps of a moment referring to a given sentence. Each video is composed of its own set of event units with varying lengths. (b) Previous DETR-based methods learn input-agnostic moment queries, providing fixed referential search areas. (c) The proposed method learns event-aware moment queries, providing reliable referential search areas according to the given video.
Video grounding (also called natural language video mo-ment localization) aims to localize timestamps of a moment referring to a given natural language sentence in a video, as shown in Fig. 1a. A key to identifying sentence-relevant mo-ments is to 1) align video-language information; and 2) pre-cisely reason temporal area. Most works have accomplished this by aligning sentence and heuristically pre-defined tem-poral proposals (e.g. sliding windows [1, 16, 37], temporal anchors [5, 77, 87]) or directly learning sentence-frame inter-actions [44, 7, 79]. However, they highly rely on the quality of hand-crafted components (e.g. proposals, non-maximum suppression) to achieve a promising result.
The recent success of detection transformer (DETR) [3]
has inspired approaches to integrate transformers [66] into video grounding framework [26, 2, 70, 14]. They learn a referential search area with a set of trainable embeddings, called moment queries, as an alternative to the heuristically designed proposals. Each moment query probes and ag-gregates video-sentence representations through the cross-attention mechanism where the final moment queries are used to predict the timestamps of the sentence-relevant mo-ments. While they have achieved outperforming perfor-mance over the CNN-based approaches, the design choices of moment queries are still underexplored, exhibiting several drawbacks. Specifically, since moment queries are learned to contain general positional information, they produce a fixed input-agnostic search area during inference, as shown in Fig. 1b. However, a video is a complex visual stream that consists of multiple semantic units (i.e., events) with varying lengths [21, 61, 59, 23]. In addition, the moment queries are controlled with equal contributions to aggregate video-sentence representations in the decoder layers, missing salient information and resulting in slow convergence.
In this paper, we propose a novel Event-aware Video
Grounding TRansformer (EaTR) that formulates a video as a set of event units and treats video-specific event units as dynamic moment queries. Our EaTR performs two differ-ent levels of reasoning: 1) Event reasoning that identifies the event units comprising the video and produces the con-tent and positional queries; and 2) Moment reasoning that fuses the moment queries with the given sentence, and in-teracts with the video-sentence representations to predict the final timestamps for video grounding. Specifically, the randomly initialized learnable event slots identify the distinc-tive event units from the given video using a slot attention mechanism [42]. The identified event units are then used as the moment queries in moment-level reasoning. While the moment queries in EaTR provide the input-specific referen-tial search area as shown in Fig. 1c, the interaction between the video-sentence representations and the sentence-relevant moment queries should be properly captured to predict an accurate moment timestamps. To this end, we introduce a gated fusion transformer layer to effectively minimize the impact of the sentence-irrelevant moment queries and cap-ture the most informative referential search area. In the gated fusion transformer layer, we fuse the moment queries and sentence representation according to their similarity in order to adaptively aggregate sentence information to the infor-mative moment queries. The fused moment queries interact with the video-sentence representations in the transformer decoder to make the final decision for video grounding.
Extensive experiments on several video grounding bench-marks [26, 16, 24] demonstrate the effectiveness of the event-aware video grounding framework, achieving a new state-of-the-art performance over the previous methods [26, 41, 90].
In summary, our key contributions are as follows: (i) We present a novel Event-aware Video Grounding Transformer (EaTR) that enhances the temporal reasoning capability of the moment queries by learning the video-specific event information. (ii) We introduce effective event reasoning and the gated fusion that highlight the distinctive events in a given video and sentence. (iii) We conduct extensive experiments to validate the effectiveness of the proposed method, and outperform state-of-the-art approaches on three video grounding benchmarks, including QVHighlights [26],
Charades-STA [16], and ActivityNet Captions [24]. 2.