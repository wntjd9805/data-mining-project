Abstract
Generating life-like whole-body human grasping has garnered significant attention in the field of computer graphics. Existing works have demonstrated the effec-tiveness of keyframe-guided motion generation framework, witch focus on modeling the grasping motions of humans in temporal sequence when the target objects are placed in front of them. However, the generated grasping poses of the human body in the key-frames are limited, failing to capture the full range of grasping poses that humans are capable of.
To address this issue, we propose a novel framework called COOP (DeCOupling and COupling of Whole-Body
GrasPing Pose Generation) to synthesize life-like whole-body poses that cover the widest range of human grasping capabilities. In this framework, we first decouple the whole-body pose into body pose and hand pose and model them separately, which allows us to pre-train the body model with out-of-domain data easily. Then, we couple these two gen-â€ Corresponding author. erated body parts through a unified optimization algorithm.
Furthermore, we design a simple evaluation method to evaluate the generalization ability of models in gener-ating grasping poses for objects placed at different po-sitions.
The experimental results demonstrate the effi-cacy and superiority of our method. And COOP holds great potential as a plug-and-play component for other domains in whole-body pose generation. Our models and code are available at https://github.com/ zhengyanzhao1997/COOP. 1.

Introduction
Grasping is a common activity in human daily life. To make virtual humans act realistically in a 3D scene, an avatar needs to be able to perform diverse grasping poses, similar to those of real humans, adapting to the targets from different positions.
In this paper, we focus on the task of synthesizing life-like whole-body grasping poses with objects located at var-Figure 2. Whole-body grasping poses generated by GNet [36] (Left) and COOP (Right) for 10 testing samples with 2 genders. ious heights in the vicinity of a human. It is worth empha-sizing that this task does not require modeling any tempo-ral aspects, such as moving to the object. Although exist-ing works [36, 40] can generate life-like grasping poses of humans, the objects in the generated key-frames are often concentrated close to the human body due to limitations in training data and generation methods. As shown on the left side of Fig. 2, [36] generates the body and hand pose as a whole. When the position of the given object is far away from the human or beyond the scope of the training data, ei-ther the body pose is distorted and unbalanced or the grasp-ing hand is far away from the target object in the generated grasping key-frame. Although IK (Inverse Kinematic) [30] can force the hand to touch the given object by adjusting the generated body pose, it can also result in some side effects such as unrealistic body pose or floating feet.
Since whole-body grasping involves multiple body parts such as the body, feet, and hands, the realism of a grasping pose needs to be evaluated from multiple aspects. Firstly, the generated hand should fit the given 3D object with high contact and low penetration. Secondly, the generated body pose should resemble that of a real human while avoiding ground penetration or floating. Finally, the body poses need to be balanced.
To address these challenges, we propose a novel frame-work called COOP (for an overview, see Fig. 3). Due to the decoupling and subsequent coupling of body parts, COOP can simultaneously consider the precision of hand grasp-ing pose and the fidelity of the body pose. In the whole-body pose generation stage, we design two networks (HNet and BNet) to generate hand poses and body poses sepa-rately. In HNet, we introduce a fine-grained representation of hand-object contact, which we call Point2Finger Contact
Map (PF-map), as an important intermediate information for guiding the generation of the grasping hand pose.
In
BNet, we propose the Body Graph Transformer (BGT) to generate the body pose conditioned on the target position of the right wrist. The Body Graph Transformer mainly consists of the Pose Graph Layer we proposed, which can explicitly encode the hierarchy of body joints. Based on the decoupling, we can easily pre-train the BGT with out-of-domain data, witch significantly improves the performance.
In the unified optimization stage, we design the Stitch Loss to couple these two generated body parts. We further uti-lize the sampled PF-map as a contact constraint to improve the hand grasping and introduce the Body Balance Loss to ensure the generated body pose is balanced.
Contributions. Our work makes the following contribu-tions: 1. COOP, a novel generation framework that can synthe-size life-like whole-body grasping pose, given different 3D objects in various positions; 2. Body Graph Transformer, a body pose generation model that explicitly encodes the hierarchy of body joints, along with a unique pre-training method that can use a large amount of out-of-domain data for pre-training; 3. Our framework and pre-training method can be easily applied to other domains in whole-body pose generation or downstream tasks as a plug-and-play component. 4. We propose an evaluation method that can test the gen-eralization ability of different whole-body grasping pose generation methods on object positions. The experimen-tal results demonstrate the efficacy and superiority of our method. 2.