Abstract
This paper accelerates video perception, such as seg-mentation and human pose estimation, by levering cross-frame redundancies. Unlike the existing approaches, which avoid redundant computations by warping the past features using optical-flow or by performing sparse convolutions on frame differences, we approach the problem from a dif-ferent perspective: low-bit quantization. We observe that residuals, as the difference in network activations between two neighboring frames, exhibit properties that make them highly quantizable. Based on this observation, we propose a novel quantization scheme for video networks coined as
Residual Quantization. ResQ extends the standard, frame-by-frame, quantization scheme by incorporating temporal dependencies that lead to better performance in terms of accuracy vs. bit-width. Furthermore, we extend our model to dynamically adjust the bit-width proportionally to the amount of changes in the video. We showcase the superi-ority of our model, against the standard quantization and existing efficient video perception models, using various ar-chitectures on semantic segmentation, video object segmen-tation and human pose estimation benchmarks. 1.

Introduction
Despite the great progress made in optimizing the com-putational efficiency of deep neural networks, real-time video inference is still an open challenge in many cases, especially when deploying on resource-constrained de-vices [46, 15, 68, 14, 9, 21, 25]. The most effective strat-egy to accelerate video inference is to exploit the temporal redundancy, i.e. by leveraging what has been processed al-ready in the past. Early works relied on feature warping and adaptation using optical flow [68, 25] or self-attention [21].
More recently, some encouraging results have been ob-tained by decomposing video snippets into a keyframe fol-lowed by residuals, that can be more efficiently processed
*Qualcomm AI Research is an initiative of Qualcomm Technologies,
Inc.
Frame quantization. err.
Residual quantization. err.
Figure 1: Visualization of the quantization error (bottom) when using frame representations (left) and residual representations (right). The residual quantization. error is typically lower (blue) than the one on individual frames (red). with distilled networks [15] or sparse operators [14, 46, 9].
For many applications, neural network quantization has emerged over the years as one of the key techniques for ac-celerating floating point models [10, 42, 30, 39, 2, 23], and for their deployment in integer precision. However, when dealing with video inputs, quantized models still operate by processing every frame independently, neglecting the op-portunity to exploit redundancy among frames.
In this paper, we observe that residuals, as the differ-ence in network activations between adjacent frames, ex-hibit properties that make them highly quantizable. Specif-ically, we illustrate that residuals typically exhibit a smaller variance w.r.t. the frame activations, which results in a re-duction in quantization error as illustrated in Fig. 1. Fol-lowing this observation, we propose Residual Quantization, coined as ResQ, a novel quantization scheme tailored for video perception. ResQ employs two sets of quantizers: one at a higher precision to quantize keyframes, one at a lower precision to quantize the residuals for subsequent frames.
Both quantizers interact during the inference to combine the
high-precision details from keyframes with complementary information from residuals. Furthermore, motivated by the fact that the range of residuals depends on the scene, we ex-tend ResQ to dynamically adjust the quantization bit-width on the fly. More specifically, we propose a lightweight pol-icy function that assigns a minimally acceptable bit-width when the residuals are small, such as in static scenes.
We extensively evaluate our proposals on three percep-tion tasks, namely semantic segmentation, video object seg-mentation and human pose estimation. We experiment with various architectures and quantization techniques, i.e. post-training quantization and quantization-aware training.
ResQ and its dynamic counterpart consistently outperform the standard quantization schemes, and perform favorably compared to state-of-the-art in efficient video processing.
We summarize our contributions as follows: i) We for-mally and empirically show the benefits of using frame residuals in reducing the quantization error (Sec. 3); ii) We propose ResQ , a novel quantization scheme for video net-works leveraging the residual quantization (Sec. 4); iii) We extend our model to dynamically adjust the quantization level based on the residual content (Sec. 4.2); iv) We vali-date our proposals on three tasks, where our models achieve an optimal trade-off of accuracy vs. efficiency. 2.