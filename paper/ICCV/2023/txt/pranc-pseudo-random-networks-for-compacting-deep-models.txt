Abstract a
We can demonstrate deep model be that reparametrized as a linear combination of several randomly initialized and frozen deep models in the weight space. During training, we seek local minima that reside within the subspace spanned by these random models (i.e., ‘basis’ networks). Our framework, PRANC, enables significant compaction of a deep model. The model can be reconstructed using a single scalar ‘seed,’ employed to generate the pseudo-random ‘basis’ networks, together with the learned linear mixture coefficients.
In practical applications, PRANC addresses the challenge of efficiently storing and communicating deep models, a common bottleneck in several scenarios, including multi-agent learning, continual learners, federated systems, and edge
In this study, we employ PRANC devices, among others. to condense image classification models and compress images by compacting their associated implicit neural networks. PRANC outperforms baselines with a large margin on image classification when compressing a deep model almost 100 times. Moreover, we show that PRANC enables memory-efficient inference by generating layer-wise weights on the fly. The source code of PRANC is here: https://github.com/UCDvision/PRANC 1.

Introduction
The prevailing notion is that larger deep models yield improved accuracy. Yet, it remains unclear if the better generalization of larger models stems from the increased complexity of the architecture or more parameters. More-over, among numerous good local minima in the loss func-tion, training finds one. In this paper, we introduce a fresh
*Equal contribution approach: viewing a deep model as a linear combination within the weight space of several randomly initialized and frozen models. During learning, our goal shifts to finding a minimum that exists within the subspace defined by these initial models. Our findings highlight the potential to sig-nificantly compact deep models by retaining only the seed value of the pseudo-random generator and the coefficients for weight combination.
This efficient reparameterization benefits AI and ML ap-plications by reducing deep model size for easier storage or communication.
In modern neural networks with mil-lions to billions of parameters, storage, and communica-tion become costly. This issue worsens in low-bitrate en-vironments due to physical constraints or adversarial dis-ruption. For instance, underwater applications might have as low as 100 bits per second bandwidth, then, transferring
ResNet18’s 11M parameter model takes more than 40 days in such conditions. Moreover, in distributed learning with many agents, high-bandwidth WiFi networks still face con-gestion issues.
Going beyond communications, loading or storing these large models on edge devices poses another significant chal-lenge. Edge devices often come with small memories un-suitable for storing large neural networks and may want to run the model less frequently (on-demand). Hence, they may benefit from compacting a deep model to fewer param-eters to construct the model layer-by-layer or even kernel-by-kernel on-demand to run each inference. This will result in significantly less I/O cost.
One may compact the model by distilling it into a smaller model [18], pruning the model parameters [29], quantizing the parameters [26], or sharing the weights as much as pos-sible [40, 6]. More recently, dataset distillation [48] is pro-posed. It can be seen as an alternative to model compression since one can store or communicate the distilled dataset and then train the model again when needed. However, most of
coefficients useless to unauthorized parties. This design choice facilitates secure communication and storage, espe-cially in cybersecurity or privacy-sensitive applications.
Theoretically, overparametrization is vital in contem-porary neural networks, enhancing their representational power and simplifying optimization due to an improved loss landscape [33, 30]. Solutions of these over-parameterized systems often form a positive-dimension manifold [8], with larger systems lacking non-global minima [34]. Consider-ing the abundance of good solutions, we examine if we can confine the solution search to low-dimensional subspaces defined by random vectors in the weight space (i.e., the
‘basis’ networks). Our experiments confirm the possibil-ity of finding good solutions in very low-dimensional ran-dom subspaces in the weight space of overparametrized net-works, urging further theoretical investigations.
Contributions: Below are our specific contributions:
• Introducing PRANC, a simple but effective network reparameterization framework that is memory-efficient during both the learning and reconstruction phases,
• Assessing the effectiveness of PRANC in compact-ing image recognition models for various benchmark datasets and model architectures, showing higher ac-curacy with a much fewer parameters compared to ex-tensive recent baselines,
• Demonstrating the effectiveness of PRANC for image compression by compacting implicit neural represen-tations for both natural and medical images,
• Showcasing the potential of PRANC in applications requiring encrypted communication of models (or data represented via models). 2.