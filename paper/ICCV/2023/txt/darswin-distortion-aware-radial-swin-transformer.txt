Abstract
Wide-angle lenses are commonly used in perception tasks requiring a large field of view. Unfortunately, these lenses produce significant distortions making conventional models that ignore the distortion effects unable to adapt to wide-angle images. In this paper, we present a novel transformer-based model that automatically adapts to the distortion pro-duced by wide-angle lenses. We leverage the physical char-acteristics of such lenses, which are analytically defined by the radial distortion profile (assumed to be known), to de-velop a distortion aware radial swin transformer (DarSwin).
In contrast to conventional transformer-based architectures,
DarSwin comprises a radial patch partitioning, a distortion-based sampling technique for creating token embeddings, and an angular position encoding for radial patch merging.
We validate our method on classification tasks using syntheti-cally distorted ImageNet data and show through extensive ex-periments that DarSwin can perform zero-shot adaptation to unseen distortions of different wide-angle lenses. Compared to other baselines, DarSwin achieves the best results (in terms of Top-1 accuracy) with significant gains when trained on bounded levels of distortions (very-low, low, medium, and high) and tested on all including out-of-distribution distortions. The code and models are publicly available at https://lvsn.github.io/darswin/
Swin [28]
DarSwin (ours)
Figure 1: Illustration of (cartesian) Swin [28] (left) and our (radial) DarSwin (right) given a wide-angle image (middle).
While Swin [28] computes attention on the predefined win-dows over square image patches (bottom left orange region),
DarSwin performs radial transformations using distortion-aware radial patches and computes the attention on windows defined over radial patches (shown in orange region bot-tom right), which enables greater generalization capabilities across different lenses. 1.

Introduction
Wide field of view (FOV) lenses are becoming increas-ingly popular because their increased FOV minimizes cost, energy, and computation since fewer cameras are needed to image the entire environment. They are having a positive impact on many applications, including security [22], aug-mented reality (AR) [38], healthcare and more particularly, autonomous vehicles [9, 49], which require sensing their surrounding 360◦ environment.
Unfortunately, such wide angle lenses create significant distortions in the image since the perspective projection model no longer applies: straight lines appear curved, and the appearance of the same object changes as a function of its position on the image plane. This distortion breaks the translation equivariance assumption implicit in convolution neural networks (CNNs) and therefore limits their applica-bility. This problem is further exacerbated by the diversity in lens distortion profiles: as we will demonstrate, a network naively trained for a specific lens tends to overfit to that specific distortion, and does not generalize well when tested on another lens. Just as methods are needed to address the
“domain gap” [16] from dataset bias [42, 20], we believe we must also bridge the “distortion gap” to truly make wide angle imaging applicable.
One popular strategy to bridge the distortion gap is to can-cel the effect of distortion on the image plane by warping the input image back to a perspective projection model accord-ing to calibrated lens parameters. Conventional approaches can then be trained and tested on the resulting “un-distorted” images. A wide array of such methods, ranging from classi-cal [5, 35, 52, 30, 19] to deep learning [47, 45], have been proposed. Unfortunately, warping a very wide angle image to a perspective projection tends to create severely stretched images and restricts the maximum field of view since, in the limit, a point at 90◦ azimuth projects at infinity. Reducing the maximum field of view defeats the purpose of using a wide angle lens in the first place. Other projections are also possible (e.g., cylindrical [33]) but these also tend to create unwanted distortions.
Recently, methods that break free from the “undistort first” strategy aim to reason directly about the images with-out undistorting them. For example, methods like [34, 1] use deformable convolutions [8, 54] to adapt convolution kernels to the lens distortion. However, the high computational cost of deformable CNNs constrains the kernel adaptation to a few layers inside the network. Other approaches like spher-ical CNNs [6] or gauge equivariant CNN [7] can adapt to different manifolds but their applicability for lens distortion has not been demonstrated. Finally, vision transformers [11] and their more recent variants [53, 28, 44] could also better bridge the “distortion gap” since they do not assume any prior structure other than permutation equivariance, but their cartesian partitioning of the image plane do not take lens geometry into account (see fig. 1).
In this paper, we present DarSwin, a transformer-based architecture that adapts its structure to the lens distortion the camera profile, which is assumed to be known (i.e. is calibrated). Our method, inspired by the recent Swin transformer architecture [28], leverages a distortion aware sampling scheme for creating token embeddings, employs polar patch partitioning and merging strategies, and relies on angular relative positional encoding. This explicitly em-beds knowledge of the lens distortion in the architecture and makes it much more robust to the “distortion gap” created by training and testing on different lenses.
The main contribution of this paper is a novel transformer-based encoder that automatically adapts to the (known) lens distortion profile, which relies on a combination of the fol-lowing novel distortion aware components: polar patch parti-tioning, distortion-based sampling scheme for creating token embeddings along with a jittering technique for better gener-alization, and angular relative position encoding for radial patch merging. We show, through extensive classification experiments, that DarSwin can perform zero-shot adaptation (without pretraining) across different lenses. Indeed, when our method is trained on a restricted set of distortions, we observe that it is much more robust to changes in distortions at test time than all of the compared alternatives, including baseline Swin [28] (applied on both distorted and undistorted images) and deformable attention transformer [44]. 2.