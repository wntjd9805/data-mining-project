Abstract
Recent advancements in deep generative models have fa-cilitated the creation of photo-realistic images across var-ious tasks. However, these generated images often exhibit perceptual artifacts in specific regions, necessitating man-⋆ indicates equal contribution. ♠ work done when Lingzhi is a grad-uate student at University of Pennsylvania. ual correction.
In this study, we present a comprehen-sive empirical examination of Perceptual Artifacts Local-ization (PAL) spanning diverse image synthesis endeavors.
We introduce a novel dataset comprising 10,168 generated images, each annotated with per-pixel perceptual artifact labels across ten synthesis tasks. A segmentation model, trained on our proposed dataset, effectively localizes arti-facts across a range of tasks. Additionally, we illustrate 1
its proficiency in adapting to previously unseen models us-ing minimal training samples. We further propose an in-novative zoom-in inpainting pipeline that seamlessly recti-fies perceptual artifacts in the generated images. Through our experimental analyses, we elucidate several invaluable downstream applications, such as automated artifact recti-fication, non-referential image quality evaluation, and ab-normal region detection in images. The dataset and code are released here: https://owenzlz.github.io/PAL4VST 1.

Introduction
Generative models have made significant progress in a myriad of image synthesis tasks, including unconditional generation [5, 21, 19, 17, 12], image inpainting [66, 49, 32, 24, 67, 61], image-to-image translation [37, 41, 48, 44, 53], and text-to-image synthesis [13, 60, 35, 40, 42, 45, 2], among others. However, even cutting-edge models occa-sionally generate implausible content or display unpleasant artifacts in specific regions of the image, which we refer to as perceptual artifacts. These artifacts are easily detectable by the human eye. Therefore, in typical image editing pro-cesses, users often retouch generated images, masking and re-editing these regions to achieve perfection.
The manual retouching of perceptual artifacts is time-consuming and iterative. Such artifacts also pose challenges for generative models in achieving full automation in im-age synthesis, editing, or batch processing without human oversight. These challenges drive our exploration into the feasibility of training AI oracle models to identify and seg-ment these perceptual artifacts. A successful implementa-tion would present users with an automatically delineated mask of potential artifact areas, eliminating manual mask-ing. Moreover, we could offer users the option to deploy established editing techniques, like inpainting, to these de-tected regions, thereby enhancing the automation of the re-touching process.
Technically, the ideal goal is to generate a flawless im-age in a single pass. However, today’s leading large-scale diffusion models often struggle to capture intricate details like subtle facial features, hands, and other object-specific nuances. While integrating more training data or using weighted loss might appear as potential solutions to these issues, they could compromise image quality in broader contexts. Until we achieve perfect single-pass outputs, au-tomating the localization and refinement of perceptual arti-facts stands as a promising direction to improve image syn-thesis quality.
To meet this objective, we’ve amassed a dataset of gener-ated images, complemented with per-pixel artifact segmen-tation labels across a range of synthesis tasks. Using this dataset, we trained a segmentation model adept at localizing perceptual artifacts across various tasks. Our pretrained ar-tifact detector showcases its versatility across multiple new models, adapting with enhanced accuracy even with limited training samples.
In conjunction with our artifact detection, we also unveil several practical applications. The foremost of these is the automatic refinement of artifacts in generated images using inpainting. However, it’s observed that leading diffusion inpainting models, like DALL-E [40] and Stable Diffusion
[42], sometimes falter in generating high-fidelity object de-tails, such as facial features. We hypothesize this may stem from an unsuitable inpainting context. Consequently, we introduce a zoom-in inpainting pipeline, presenting a more apt input context before inpainting. This simple approach effectively mitigates challenges tied to object detail genera-tion, without necessitating model training or alterations.
Our primary contributions include:
• A novel high-quality dataset comprising 10,168 im-ages with per-pixel artifact annotations from humans, spanning ten diverse image synthesis tasks.
• A segmentation model adept at localizing perceptual artifacts across multiple synthesis tasks. Our pre-trained model exhibits a rapid adaptation capability to new techniques with minimal training examples.
• An novel zoom-in inpainting pipeline for the auto-mated refinement of intricate details in generated im-ages.
• Demonstrated applications of our artifact detector, automatic artifact refinement; reference-free image quality evaluation; and 3). which include: 1). 2). anomaly detection in natural images.
We will release the dataset and the code. 2.