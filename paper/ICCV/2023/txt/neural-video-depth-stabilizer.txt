Abstract
Video depth estimation aims to infer temporally consis-tent depth. Some methods achieve temporal consistency by finetuning a single-image depth model during test time us-ing geometry and re-projection constraints, which is inef-ficient and not robust. An alternative approach is to learn how to enforce temporal consistency from data, but this re-quires well-designed models and sufficient video depth data.
To address these challenges, we propose a plug-and-play framework called Neural Video Depth Stabilizer (NVDS) that stabilizes inconsistent depth estimations and can be ap-plied to different single-image depth models without extra effort. We also introduce a large-scale dataset, Video Depth in the Wild (VDW), which consists of 14,203 videos with over two million frames, making it the largest natural-scene video depth dataset to our knowledge. We evaluate our method on the VDW dataset as well as two public bench-marks and demonstrate significant improvements in consis-tency, accuracy, and efficiency compared to previous ap-proaches. Our work serves as a solid baseline and provides a data foundation for learning-based video depth models.
We will release our dataset and code for future research. 1.

Introduction
Monocular video depth estimation is a prerequisite for various video applications, e.g., bokeh rendering [24, 25, 47], 2D-to-3D video conversion [12], and novel view syn-thesis [16, 17]. An ideal video depth model should out-put depth results with both spatial accuracy and temporal consistency. Although the spatial accuracy has been greatly improved by recent advances in single-image depth mod-els [15, 27, 28, 39, 45] and datasets [18, 41, 42], how to obtain temporal consistency, i.e., removing flickers in the
*Corresponding author.
Figure 1: (a) Performance and efficiency comparisons.
Circle area represents inference time. Smaller circles mean faster speed. The X-axis represents δ1 on Sintel [4] dataset for spatial accuracy. The Y-axis represents consistent met-ric OP W . Lower OP W means better temporal consis-tency. Our framework outperforms prior arts by large mar-gins. (b) Dataset comparisons. Larger circles mean larger amounts of frames. We present VDW dataset, the largest video depth dataset in the wild with diverse scenes. predicted depth sequences, is still an open question. The prevailing video depth approaches [13, 23, 48] require test-time training (TTT). During inference, a single-image depth model is finetuned on the testing video with geometry con-straints and pose estimation. These TTT-based methods have two main issues: limited robustness and heavy compu-tation overhead. Due to the heavy reliance on camera poses, e.g., CVD [23] shows erroneous predictions and robust-CVD [13] produces obvious artifacts for many videos when camera poses [13, 29] are inaccurate. Moreover, test-time
training is extremely time-consuming. CVD [23] takes 40 minutes for 244 frames on four NVIDIA Tesla M40 GPUs.
This motivates us to build a learning-based model that learns to enforce temporal consistency from video depth data. However, like all the deep-learning models, learning-based paradigm requires proper model design and suffi-cient training data. Previous learning-based methods [5, 34, 40, 46] show worse performance than the TTT-based ones.
Video depth data is also limited in scale and diversity.
To address the two aforementioned challenges, we first propose a flexible learning-based framework termed Neural
Video Depth Stabilizer (NVDS), which can be directly ap-plied to different single-image depth models. NVDS con-tains a depth predictor and a stabilization network. The depth predictor can be any off-the-shelf single-image depth model. Different from the previous learning-based meth-ods [5, 34, 40, 46] that function as stand-alone models,
NVDS is a plug-and-play refiner for different depth predic-tors. Specifically, the stabilization network processes ini-tial flickering disparity estimated by the depth predictor and outputs temporally consistent results. Therefore, our frame-work can benefit from the cutting-edge depth models with-out extra effort. As for the design of stabilization network, inspired by attention [35] in other video tasks [1, 14, 22, 33], we adopt a cross-attention module in our framework. Each frame can attend relevant information from adjacent frames for temporal consistency. We also design a bidirectional inference strategy to further improve the consistency. As shown in Fig. 1(a), our NVDS outperforms the previous ap-proaches in terms of consistency, accuracy, and efficiency significantly.
Moreover, we collect a large-scale natural-scene video depth dataset, Video Depth in the Wild (VDW), to support the training of robust learning-based models. Current video depth datasets are mostly closed-domain [7, 9, 31, 32, 37].
A few in-the-wild datasets [4, 36, 38] are still limited in quantity, diversity, and quality, e.g., Sintel [4] only contains 23 animated videos. In contrast, our VDW dataset contains 14,203 stereo videos of over 200 hours and 2.23M frames from four different data sources, including movies, anima-tions, documentaries, and web videos. We adopt a rigor-ous data annotation pipeline to obtain high-quality dispar-ity ground truth for these data. As shown in Fig. 1(b), to the best of our knowledge, VDW is the largest in-the-wild video depth dataset with diverse scenes.
We conduct evaluations on the VDW and two pub-lic benchmarks: Sintel [4] and NYUDV2 [31]. Our method achieves state-of-the-art in both the accuracy and the consistency. We also fit three different depth predic-tors [27, 28, 45] into our framework and evaluate them on
NYUDV2 [31]. The results demonstrate the flexibility and effectiveness of our plug-and-play manner. Our main con-tributions can be summarized as follows:
• We propose a plug-and-play and bidirectional learning-based framework termed Neural Video Depth Stabilizer (NVDS), which can be directly adapted to different single-image depth predictors to remove flickers.
• We propose VDW dataset, which is currently the largest video depth dataset in the wild with the most diverse video scenes. 2.