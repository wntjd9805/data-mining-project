Abstract
Audio-driven talking-head synthesis is a popular re-search topic for virtual human-related applications. How-ever, the inflexibility and inefficiency of existing methods, which necessitate expensive end-to-end training to transfer emotions from guidance videos to talking-head predictions, are significant limitations.
In this work, we propose the
Emotional Adaptation for Audio-driven Talking-head (EAT) method, which transforms emotion-agnostic talking-head models into emotion-controllable ones in a cost-effective and efficient manner through parameter-efficient adapta-tions. Our approach utilizes a pretrained emotion-agnostic talking-head transformer and introduces three lightweight adaptations (the Deep Emotional Prompts, Emotional De-formation Network, and Emotional Adaptation Module) from different perspectives to enable precise and realistic emotion controls. Our experiments demonstrate that our approach achieves state-of-the-art performance on widely-used benchmarks, including LRW and MEAD. Addition-ally, our parameter-efficient adaptations exhibit remarkable generalization ability, even in scenarios where emotional training videos are scarce or nonexistent. Project website: https://yuangan.github.io/eat/ 1.

Introduction
Recently, there has been increasing attention on syn-thesizing realistic talking heads due to their wide-ranging applications in industry, such as digital human anima-tion [16, 18, 47], visual dubbing [36], and video content creation [43]. Audio-driven talking-head generation aims to produce realistic talking-head videos synchronized with speech. However, unlike speech, humans convey intentions through emotional expressions. Therefore, generating emo-tional talking heads is important to improve the fidelity of talking heads for real-world applications. To address this open problem, various forms of knowledge (such as human head models, emotions, audio, and vision) must be consid-*Corresponding author.
Figure 1. Efficient emotional talking-head generation. (a)
Previous work trains or finetunes the whole network with aug-mented emotional driving videos. (b) Our EAT transforms emotion-agnostic talking-head models into emotion-controllable ones through flexible guidance, including emotional prompts or text-guided CLIP [38] supervision, by lightweight adaptations. ered in constructing multi-knowledge representations [49].
Previous one-shot talking-head generation methods [57, 36, 47] focus on achieving audio-visual synchronization for emotion-agnostic talking-heads, which is a special case of realistic talking-heads. More recent works [19, 18, 28] pay attention to generating emotion-aware talking-heads. GC-AVT [28] and EAMM [18] are two methods that generate emotional videos using driven emotional videos and pose-guiding videos. GC-AVT [28] achieves explicit control over the expression, speech content, and pose of talking heads through granular pre-processing design. EAMM [18] syn-thesizes one-shot emotional talking-heads by adding aug-mented emotional source videos. Since the driven videos can introduce semantic ambiguity to mouth shapes, GC-AVT replaces the mouth part with the neighbor frame, while
EAMM ignores the mouth part of driven emotional videos by using data augmentation. Additionally, these methods require training or finetuning the entire network at high costs for emotional talking-heads generation.
Although emotion-aware methods have made progress in the one-shot talking-head generation, they lack in-depth thinking in two key aspects. (1) Architecture efficiency.
As a sub-task of talking-head generation, it is parameter-inefficient to train or finetune the entire emotional talking-head generation network. Furthermore, since large-scale emotion-agnostic talking-head data is more readily avail-able than emotional data, it is worthwhile to consider how to efficiently reuse the knowledge learned from emotion-(2) Guidance flexibility. Previous meth-agnostic data. ods prefer to transfer the driving videos to the target talk-ing heads rather than directly learning the emotional repre-sentation. In practice, finding appropriate driven emotional videos requires taking into account factors such as reso-lution, occlusion, and even the length of the driven emo-tional videos and audio. Furthermore, prior research has neglected to consider lip shapes, which can result in un-realistic emotional expressions. For instance, according to
FACS [14, 23], depressed lip corners are one of the key components of the sad expression.
To address the above limitations, a desirable approach should enable an efficient and flexible transfer of pretrained talking-head models to emotional talking-head generation tasks with lightweight emotional guidance, as illustrated in
Fig. 1. There are two key advantages. Firstly, with the reused knowledge, we can readily and effortlessly apply the talking-head model to emotional talking-head genera-tion tasks. Secondly, obtaining lightweight guidance is sim-pler and more adaptable in practical scenarios, such as text-guided zero-shot expression editing.
To realize the aforementioned paradigm, we propose an efficient Emotional Adaptation framework for audio-driven
Talking-head (EAT) generation, which involves two stages.
In the first stage, we enhance the unsupervised 3D latent keypoints representation [48] to capture emotional expres-sions. Then, we introduce the Audio-to-Expression Trans-former (A2ET), which learns to map audio to enhanced 3D latent keypoints using large-scale talking-head datasets. In the second stage, we propose learnable guidance and adap-tation modules for steering emotional expression genera-tion. These include Deep Emotional Prompts for parameter-efficient emotional adaptation, a lightweight Emotional De-formation Network (EDN) for learning the emotional de-formation of facial latent representation, and a plug-and-play Emotional Adaptation Module (EAM) for enhancing visual quality. Our approach enables rapid transfer of tra-ditional talking-head models to emotional generation tasks with high-quality results and supports zero-shot expression editing with image-text models [38].
We conduct extensive experiments to assess the ef-fectiveness of EAT on emotional talking-head generation.
Compared to baseline competitors, EAT achieves superior performance without guiding emotional videos. Moreover, based on the pretrained talking-head model, we can attain state-of-the-art (SOTA) performance in 2 hours with only 25% training data. The results indicate that our method is capable of generating more realistic talking-head videos.
And with only text descriptions of emotions, we can achieve zero-shot talking-head editing.
In summary, the main contributions of our work are listed below:
• Our study introduces a new two-stage paradigm, called
EAT, for addressing emotional talking-head tasks. Our experiments demonstrate that this paradigm outper-forms previous methods with respect to both emotion manipulation and video quality in one-shot talking-head generation tasks.
• Our proposed architecture includes deep emotional prompts, an emotional deformation network, and an emotional adaptation module. This design enables the efficient transfer from generating talking heads with-out emotional expression to generating talking heads with emotional expression.
• To the best of our knowledge, our study is the first to introduce flexible guidance for talking-head adap-tation. By utilizing image-text models, we can achieve zero-shot expression editing of talking-head videos, surpassing the capabilities of previous methods. 2.