Abstract
Current unsupervised video deraining methods are in-efficient in modeling the intricate spatio-temporal proper-ties of rain, which leads to unsatisfactory results. In this paper, we propose a novel approach by integrating a bio-inspired event camera into the unsupervised video derain-ing pipeline, which enables us to capture high temporal res-olution information and model complex rain characteris-tics. Specifically, we first design an end-to-end learning-based network consisting of two modules, the asymmetric separation module and the cross-modal fusion module. The two modules are responsible for segregating the features of the rain-background layer, and for positive enhancement and negative suppression from a cross-modal perspective, respectively. Second, to regularize the network training, we elaborately design a cross-modal contrastive learning method that leverages the complementary information from event cameras, exploring the mutual exclusion and similar-ity of rain-background layers in different domains. This en-courages the deraining network to focus on the distinctive characteristics of each layer and learn a more discrimina-tive representation. Moreover, we construct the first real-world dataset comprising rainy videos and events using a hybrid imaging system. Extensive experiments demonstrate the superior performance of our method on both synthetic and real-world datasets. 1.

Introduction
Rain is the most common bad weather which introduces the serious degradation in captured videos and images. It not only causes the poor visual quality but also seriously deteriorates the performance of some outdoor vision tasks that assume clean video as input, e.g., object tracking [15], object detection [12], person re-identification (Re-ID) [43] and segmentation [26]. Thus, it is of great importance to develop an effective video rain removal algorithm to restore
*Corresponding author
Figure 1: Our proposed cross-modal contrastive learning method includes intra-modal and inter-modal contrastive learning. In intra-modal contrastive learning, we aim to es-tablish the mutually exclusive relationship between the rain and background layers by pushing them far away in both the event and frame domains.
In inter-modal contrastive learning, we pull together the rain layer shared in two do-mains. Moreover, we push the rain layer in the frame do-main and background layer in the event domain for sup-pressing the negative information such as the moving edges as shown in Fig. 3(f). the high-quality rain-free videos. Recently, many methods
[42, 36, 35, 41] are proposed for rain removal and achieved significant successes in synthetic datasets. Unfortunately, most of these methods are supervised, which heavily rely on paired rain-clean data. The large domain gap between the synthetic and real rain makes them perform poorly in real-world rainy scenes.
To address this issue, the semi-supervised deraining methods [40, 8] are proposed. They commonly employed
the labeled synthetic data for good initialization and intro-duce the real rains for generalization. Although the char-acteristics of real rains are taken into account, they can-not achieve satisfying results when the gap between syn-thetic and real rainy images is large. To further improve robustness, the unsupervised deraining methods have at-tracted more attention. Existing unsupervised deraining methods demonstrated that satisfying deraining results can be achieved by using either the temporal correlation and consistency [37] or the unpaired adversarial learning and cycle-consistency [29, 45, 39]. More recently, some meth-ods [38, 2] exploited the underlying mutually exclusive rela-tionship and correlation from rainy inputs to remove rains in a contrastive learning manner. Despite remarkable improve-ment, these frame-based methods are limited to the imaging mechanism of the conventional RGB cameras, which fail to model the complex spatio-temporal distribution of rain and present unsatisfying results.
In this paper, we introduce a novel neuromorphic sen-sor called event camera [4] to approach the unsupervised video deraining task.
In contrast to conventional frame-based cameras that capture images at a fixed frame rate, event cameras asynchronously respond to intensity changes of each pixel in high temporal resolution and have been used for many applications [9, 30, 10, 24, 21]. We delve into the exploration of the role of event cameras in video deraining and demonstrate that event cameras can contribute to video deraining from two perspectives.
Firstly, event cameras are well-suited for modeling the complex spatio-temporal properties of rain, making it eas-ier to distinguish between the rain layer and the background layer. The moving rain streaks produce noticeable inten-sity changes that match the dynamic perception of event cameras. With their high temporal resolution and high dy-namic range perception of rain, event cameras can capture the fast motions of both rain and background, preserving the details of rain-free regions. Secondly, event cameras can provide complementary modality information. We can obtain both absolute intensity information and the intensity changes produced by the motion of rain and moving back-ground objects. This way, the contrastive learning can be enhanced by exploring multiple relationships in two modal-ity. In comparison to the conventional contrastive learning of single modality, we propose a new contrastive learning framework called cross-modal contrastive learning. It forms positive and negative pairs from both frame and event for utilizing the mutually exclusive and similar relationships between rain and background in the frame and event do-mains. Fig. 1 illustrates the main idea of proposed cross-modal contrastive learning. Furthermore, to enable real-world evaluations, we build a hybrid imaging system to col-lect a dataset of rainy videos and event streams. The main contributions are summarized as follows:
• We make the first attempt to approach unsupervised video deraining with an event camera by exploiting its effective perception of motion information.
• We formulate a cross-modal contrastive learning frame-work to distinguish the rain layer and background layer by exploiting their mutually exclusive and similar rela-tionship in frame and event domains.
• We collect a real-world dataset containing rainy videos and events using a hybrid camera system.
• We achieve superior performance over existing state-of-the-art methods on both synthetic and self-collected real-world datasets. 2.