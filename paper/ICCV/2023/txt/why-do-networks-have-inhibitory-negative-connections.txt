Abstract
Why do brains have inhibitory connections? Why do deep networks have negative weights? We propose an an-swer from the perspective of representation capacity. We believe representing functions is the primary role of both (i) the brain in natural intelligence, and (ii) deep networks in artificial intelligence. Our answer to why there are in-hibitory/negative weights is: to learn more functions. We prove that, in the absence of negative weights, neural net-works with non-decreasing activation functions are not uni-versal approximators. While this may be an intuitive result to some, to the best of our knowledge, there is no formal theory, in either machine learning or neuroscience, that demonstrates why negative weights are crucial in the context of representation capacity. Further, we provide insights on the geometric properties of the representation space that non-negative deep networks cannot represent. We expect these insights will yield a deeper understanding of more so-phisticated inductive priors imposed on the distribution of weights that lead to more efficient biological and machine learning. 1.

Introduction
Are inhibitory connections necessary for a brain to func-tion? Many studies on mammals answer yes: a balanced excitatory/inhibitory (E/I) ratio is essential for memory [26], unbalanced E/I lead to either impulsive or indecisive behav-iors [22], such balance is closely tracked throughout learning
[33, 31], and imbalance is hypothesized to be the driving force behind epilepsy [6, 18, 32]. These simulation results and disease studies provide compelling evidence that E/I balance is necessary for brains to function stably.
Intriguingly, when neurons first came into existence in the long history of evolution, they were exclusively excita-tory [20]. The Cnidarian jellyfish has a well-defined nervous system that is made of sensory neurons, motor neurons, and interneurons. Different from mammals, their synaptic
*Correspondence: qwang88@jhu.edu
§Current address: United States Military Academy, Department of Math-ematical Sciences, West Point NY US connections are exclusively excitatory. Even though these jellyfish are not equipped with inhibitory neurons, they are perfectly capable of performing context-dependent behav-iors: when they feed, they swim slowly through a weak, rhythmic contraction of the whole bell; when they sense a strong mechanical stimulus, they escape through a rapid, much stronger contraction [20]. Cnidarian jellyfish behave adaptively, not through sophisticated excitatory-inhibitory circuits, but instead by modifying voltage-gated channel properties (i.e., conductance). Cnidarian jellyfish prove to us that without inhibitory connections the brain can still function, albeit likely through alternative mechanisms. This re-raises the fundamental question: are inhibitory connec-tions necessary for brains to function?
In this paper, we explore the necessity of inhibitory con-nections from the perspective of representation capacity. In-stead of viewing the brain as a dynamical system to discuss its functional stability, we think about the brain as a feed-forward network capable of representing functions. For a certain network structure, we are interested in characteriz-ing the repertoire of functions such networks are capable of representing. Specifically, to understand the importance of inhibitory connections in networks’ representation capacity, we ask what functions can non-negative networks represent?
We do so with the help of Deep Neural Networks (DNNs).
DNNs allow us to work at a level of abstraction where we can stay focused on the connectivity between computation units; they also allow us to completely take out inhibitory connections by setting all weights to be non-negative. They further allow us to build upon the well-celebrated DNN theoretical result that DNNs are universal approximators
[16, 9, 15] (Theorem 3.2). We prove in this paper that DNNs with all non-negative weights are not universal approxima-tors (sec 3). We further prove three geometric properties of the representation space for non-negative DNNs (sec 4).
These results show that networks without negative connec-tions not only lose universality; in fact, they have extremely limited representation space. We further extend our results to convolutional neural networks (CNNs) and other struc-tural variants (sec 5). Such theoretical results serve as a plausible explanation for why purely excitatory nervous sys-tems, along the history of evolution, were largely overtaken
by brains containing both inhibitory and excitatory connec-tions; the simpler systems may be able to perform interesting functions, but they also have limited representation capac-ity. Our work thus concludes that inhibitory connections are necessary for a brain to represent more functions. 2.