Abstract
Semantic segmentation is a computer vision task that associates a label with each pixel in an image. Modern approaches tend to introduce class embeddings into se-mantic segmentation for deeply utilizing category seman-tics, and regard supervised class masks as final predictions.
In this paper, we explore the mechanism of class embed-dings and have an insight that more explicit and mean-ingful class embeddings can be generated based on class masks purposely. Following this observation, we propose
ECENet, a new segmentation paradigm, in which class em-beddings are obtained and enhanced explicitly during in-teracting with multi-stage image features. Based on this, we revisit the traditional decoding process and explore in-verted information flow between segmentation masks and class embeddings. Furthermore, to ensure the discrim-inability and informativity of features from backbone, we propose a Feature Reconstruction module, which combines intrinsic and diverse branches together to ensure the con-currence of diversity and redundancy in features. Ex-periments show that our ECENet outperforms its counter-parts on the ADE20K dataset with much less computational cost and achieves new state-of-the-art results on PASCAL-Context dataset. The code will be released at https:
//gitee.com/mindspore/models and https:// github.com/Carol-lyh/ECENet. 1.

Introduction
Semantic segmentation is a fundamental task in com-puter vision, which aims to predict the corresponding classes for each pixel of the input image. Typically, pix-els that share common semantic categories are aggregated together to form regions on each slice of predicted masks,
*Work done during an internship at Huawei Noah’s Ark Lab.
†Corresponding author.
Figure 1. Explicit Class Extraction module. Each slice of predicted masks naturally presents what the model has learned on this cate-gory. By extracting class information into embeddings, we allow the information flow reversely and obtain the explicit class embed-dings upon spatial prior knowledge on each region. which naturally presents the description of each category the model has learned.
Traditional semantic segmentation methods are domi-nated by Fully Convolutional Networks (FCN) [33] based models. With stacked convolutional layers, the semantics in input images are gradually extracted. The 1 × 1 con-volutional layer, which serves as semantic kernels, is usu-ally applied to the representative feature maps in the end.
Previous works [4, 5, 16, 30] focus on enlarging receptive filed [28, 4, 5], integrating attention modules [16, 51, 24] or fusing multi-stage features [34, 30, 27]. However, the CNN architectures are lack of long-range dependencies, which hinders the performances of FCNs. (ViT)
Recently, transformer [43] using self-attention mech-anism is introduced into the field of computer vision.
Inspired by Vision Transformer
[13] and Seg-menter [40], class tokens/embeddings have aroused the in-terest of many researchers [29, 10, 9, 56]. Generally, class embeddings are randomly initialized and passed into the decoder to interact with feature maps, then it would be used to get final segmentation masks [10, 9]. However, the class embeddings here are defined implicit and meaningless ini-tially, which means that much spatial prior knowledge is ignored and lost.
Our key insight: class embeddings can be made explicit and meaningful by taking use of predicted masks.
Intu-itively, accurate regions on each slice of masks which the model has learned become the most natural description of each category. In fact, MaskFormer [10] first realizes the importance of per mask and replaces this per-pixel classifi-cation task with a set of binary masks prediction, each as-sociated with a single category. Class embeddings which serve as object queries are passed into transformer decoder and then used to get the predicts. Given this promising attempt, a natural question emerges: can we reverse this process? Or can predicted masks assist the generation of meaningful class embeddings conversely?
To address this question, we consider a simple approach to utilize the predicted masks in turn and propose our
ECENet, which is composed of Feature Reconstruction (FR), Explicit Class Extraction (ECE), Semantics Atten-tion & Updater (SAU). More specifically, we design a FR module which is used to ensure the discriminative and in-formative capability of backbone features. Then Explicit
Class Extraction (ECE) module consisting of spatial pool-ing and a single linear projection is used to extract class embeddings from masks. The class embeddings extracted are then passed into a transformer block that takes the early features as queries and class embeddings as keys and val-ues. Through this, we encourage the interaction between early stage features and meaningful class semantics.
In-spired by SegViT [53], we utilize the similarity maps as an extra masks. Then we employ gated mechanism [11] to further enhance previous class embeddings with the newly obtained semantics from the masks, dubbed Semantics At-tention & Updater (SAU). Since the masks are a byproduct of the regular attention calculations, negligible computation is involved. We get the final segmentation masks by aggre-gating the enhanced outputs from multi-stage features and employ a convolutional layer onto it, assisted by class em-beddings and intermediate masks.
Building upon this effective paradigm, the regions be-longing to the same category tend to group together with the assistance of highly consistent semantics. Furthermore, the semantic gap between different layers is bridged. Exper-iments show that our ECENet achieves promising perfor-mance on common segmentation datasets and outperforms its counterparts with less computational cost.
We summarize our main contributions as follows:
• We reverse the general decoding process which departs from randomly initialized embeddings. Our class em-beddings are explicit and consistently meaningful. It is the first attempt to uncover the correlations between segmentation masks and class embeddings and explore possible inverted information flow between both.
• We propose a new network, ECENet based on this in-sight, which is composed of Feature Reconstruction (FR), Explicit Class Extraction (ECE), Semantics At-tention & Updater (SAU), which is demonstrated to be effective and efficient.
• We conduct extensive experiments on challenging benchmarks. Results show that our ECENet achieves competitive mIoU 55.2% on the ADE20K dataset with much less computational cost and parameters. We also demonstrate that our method yields state-of-the-art results on PASCAL-Context dataset (65.9% mIoU) and is compelling on Cityscapes dataset. 2.