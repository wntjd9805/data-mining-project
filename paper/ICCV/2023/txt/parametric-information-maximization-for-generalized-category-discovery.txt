Abstract
We introduce a Parametric Information Maximiza-tion (PIM) model for the Generalized Category Discov-ery (GCD) problem.
Specifically, we propose a bi-level optimization formulation, which explores a param-eterized family of objective functions, each evaluating a information between the features and weighted mutual the latent labels, subject to supervision constraints from the labeled samples.
Our formulation mitigates the class-balance bias encoded in standard information max-thereby handling effectively both imization approaches, short-tailed and long-tailed data sets. We report ex-tensive experiments and comparisons demonstrating that our PIM model consistently sets new state-of-the-art per-formances in GCD across six different datasets, more so when dealing with challenging fine-grained problems.
Our code: https://github.com/ThalesGroup/ pim-generalized-category-discovery. 1.

Introduction
Deep learning methods are driving progress in a wide span of computer vision tasks, particularly when large la-beled datasets are easily accessible for training. Obtaining such large datasets is a cumbersome process, which is often a limiting factor impeding the scalability of these models.
To alleviate this limitation, semi-supervised learning (SSL) has emerged as an appealing alternative, which leverages both labeled and unlabeled data to boost the performance of deep models. Despite recent success, SSL approaches work under the closed-set assumption, in which the cate-*Corresponding author: florent.chiaroni.ai@gmail.com (permanent address) gories in the labeled and unlabeled subsets share the same underlying class label space. Nevertheless, this assumption rarely holds in real scenarios, where novel categories may emerge in conjunction with known classes, which typically results in significant drops in the performances of standard supervised deep learning models. Thus, the ability to detect whether the input of a deep learning model belongs or not to a set of known classes seen during training is essential for robust deployment in a breadth of critical application ar-eas, such as medicine, security, finance, agriculture, market-ing and engineering [4, 34]. Thus, devising novel learning models that can address the realistic open-set scenario is of paramount importance.
Novel category discovery (NCD) [18, 1] tackles this problem by exploiting the knowledge learned from a set of relevant known classes to improve clustering into un-known categories. Nevertheless, NCD assumes the two sets of classes to be disjoint, which means that the unla-beled dataset contains only instances belonging to the set of novel categories. Generalized category discovery (GCD)
[42] considers a more general scenario, where unlabeled data contain instances from both seen and novel classes.
This scenario is particularly challenging, as learning is per-formed under class distribution mismatch, and the unla-beled data may contain categories never encountered in the available labeled set.
Contributions:
In this work, we address the generalized category discovery task from an information-theoretic per-spective. Our contributions are summarized as follows:
• We introduce a Parametric Information Maximization (PIM) model for GCD. Specifically, we propose a bi-level optimization formulation, which explores a pa-rameterized family of objective functions, each evalu-ating a weighted mutual information between the fea-tures and the latent labels, subject to supervision con-straints from the labeled samples. Our formulation mitigates the class-balance bias encoded in standard information maximization, deals effectively with both short-tailed and long-tailed data sets, and is model-agnostic (i.e., could be used in conjunction with any feature extractor).
• We report extensive experiments and comparisons demonstrating that PIM consistently sets new state-of-the-art performances across six different datasets, with larger gaps on the more challenging fine-grained
It outperforms both specialized GCD benchmarks. methods and standard information-maximization ap-proaches. 2.