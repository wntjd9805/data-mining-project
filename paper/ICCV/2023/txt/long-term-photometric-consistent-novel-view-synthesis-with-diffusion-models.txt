Abstract
Novel view synthesis from a single input image is a chal-lenging task, where the goal is to generate a new view of a scene from a desired camera pose that may be separated by a large motion. The highly uncertain nature of this syn-thesis task due to unobserved elements within the scene (i.e. occlusion) and outside the field-of-view makes the use of generative models appealing to capture the variety of pos-sible outputs. In this paper, we propose a novel generative model capable of producing a sequence of photorealistic images consistent with a specified camera trajectory, and a single starting image. Our approach is centred on an au-toregressive conditional diffusion-based model capable of interpolating visible scene elements, and extrapolating un-observed regions in a view, in a geometrically consistent manner. Conditioning is limited to an image capturing a single camera view and the (relative) pose of the new cam-era view. To measure the consistency over a sequence of generated views, we introduce a new metric, the thresholded symmetric epipolar distance (TSED), to measure the num-ber of consistent frame pairs in a sequence. While previous methods have been shown to produce high quality images and consistent semantics across pairs of views, we show empirically with our metric that they are often inconsistent with the desired camera poses.
In contrast, we demon-strate that our method produces both photorealistic and view-consistent imagery. Additional material is available on our project page: https://yorkucvil.github. io/Photoconsistent-NVS/. 1.

Introduction
Novel view synthesis (NVS) methods are generally tasked with generating new scene views, given a set of ex-isting views. NVS has a long history in computer vision
[6, 18, 2] and has recently seen a resurgence of interest with
the advent of NeRFs [23, 46, 41]. Most current approaches to NVS (e.g. NeRFs) focus on problem settings where gen-erated views remain close to the input and whose content is largely visible from some subset of the given views. This restricted setting makes these methods amenable to direct supervision. In contrast, we consider a more extreme case, where a single view is given as input, and the goal is to gen-erate plausible image sequence continuations from a trajec-tory of provided camera views. By plausible, we mean that visible portions of the scene should evolve in a 3D consis-tent fashion, while previously unseen elements (i.e. regions occluded or outside of the camera field-of-view) should ap-pear harmonious with the scene. Moreover, regions not vis-ible in the input view are generally highly uncertain; so, there are a variety of plausible continuations that are valid.
To address this challenge, we propose a novel NVS method based on denoising diffusion models [12] to sam-ple multiple, consistent novel views. We condition the dif-fusion model on both the given view, and a geometrically informed representation of the relative camera settings of both the given and target views. The resulting model is able to produce multiple plausible novel views by simply gener-ating new samples from the model. Further, while the model is trained to generate a single novel view conditioned on an existing view and a target camera pose, we demonstrate that this model can generate a sequence of plausible views, in-cluding final views with little or no overlap with the starting view. Fig. 1 shows the outputs of our model for several dif-ferent starting views, with two samples of plausible sets of views.
Existing NVS techniques have been evaluated primar-ily in terms of generated image quality (e.g. with FrÂ´echet
Inception Distance (FID) [11]) but have generally ignored measuring consistency with the camera poses. Based on the epipolar geometry defined by relative camera poses [10], we introduce a new metric which directly evaluates the geomet-ric consistency of generated views independently from the quality of generated imagery. The proposed metric does not require any knowledge of scene geometry, making it widely applicable even on purely generated images. We evaluate the proposed method on both real and synthetic datasets in terms of both generated image quality and geometric consistency. Further, previous work only evaluates perfor-mance based on in-distribution camera trajectories. Here, we evaluate the generalization ability of extant models and our own by generating sequences based on novel trajecto-ries (i.e. trajectories that differ significantly from those in the training data). 2.