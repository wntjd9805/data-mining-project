Abstract
This paper presents a novel network structure with illumination-aware gamma correction and complete image modelling to solve the low-light image enhancement prob-lem. Low-light environments usually lead to less informa-tive large-scale dark areas, directly learning deep repre-sentations from low-light images is insensitive to recover-ing normal illumination. We propose to integrate the ef-fectiveness of gamma correction with the strong modelling capacities of deep networks, which enables the correction factor gamma to be learned in a coarse to elaborate man-ner via adaptively perceiving the deviated illumination. Be-cause exponential operation introduces high computational complexity, we propose to use Taylor Series to approxi-mate gamma correction, accelerating the training and in-ference speed. Dark areas usually occupy large scales in low-light images, common local modelling structures, e.g.,
CNN, SwinIR, are thus insufﬁcient to recover accurate il-lumination across whole low-light images. We propose a novel Transformer block to completely simulate the depen-dencies of all pixels across images via a local-to-global hi-erarchical attention mechanism, so that dark areas could be inferred by borrowing the information from far informa-tive regions in a highly effective manner. Extensive experi-ments on several benchmark datasets demonstrate that our approach outperforms state-of-the-art methods. 1.

Introduction
Images captured in low-light environments usually con-tain large scales of dark areas. They are of low con-trast and intensities, which submerge the useful image con-tents, making them invisible to humans as well as damaging the performances of numerous computer vision algorithms
*Equal contribution
†Corresponding author
Input
SCI [28]
URetinexNet [41]
SNR [43]
Ours
GT
Figure 1. This ﬁgure shows a challenging low-light image, whose middle areas are completely swallowed by the dark. Our IAGC obtains higher illumination and recovers more image details than
SOTA methods. This is attributed to illumination-aware gamma correction which effectively enhances illumination as well as
COMO-ViT which recovers better image details by completely modelling the dependencies among all pixels of images.
[12, 21, 25, 44]. Many attempts have been made to solve the
Low-Light Image Enhancement (LLIE) task. Besides the early methods, e.g., histogram equalization [33], Retinex-based optimization methods [8, 31, 34, 23, 10], CNNs are introduced to learn illumination-recovering deep represen-tations from low-light images. Many CNN-based meth-ods [49, 51, 50, 41, 40] solve LLIE by combining Retinex theory [14], which decomposes an image I into reﬂectance
R and illumination L:
I = R (cid:2) L (1) where (cid:2) is point-wise multiplication. There still exist some other deep networks learning a mapping from low-light images to normal-light ones, e.g., the supervised meth-ods [2, 27, 15, 45, 18, 53, 4], and unsupervised methods
[7, 13, 28].
However, the convolutional mechanism is usually lim-ited by its inductive biases [30]. The locality is unable to
handle long-range dark areas well, and the spatial invari-ance regards dark and bright areas in the same manner, so that some useful information in the bright areas cannot be properly used. Therefore, CNN-based methods are insensi-tive to learning effective illumination-recovering deep rep-resentations, leading to still low illumination and inaccurate color recovery for some images.
Transformers are introduced to enhance low-light im-ages by modelling longer-range context dependencies [52, 43]. An effective paradigm is to combine self-attention and convolution in a complementary way. However, these meth-ods usually downsample an image before calculating self-attention to reduce computation. The dependencies among some pixels are inevitably ignored, losing some useful in-formation and leading to unsatisfactory recovery results for some low-light images.
Allowing for the insensitivity of CNNs to handling low-light images, we propose to make the best use of the ef-fectiveness of Gamma Correction to illumination enhance-ment. Existing methods usually neglect the differences be-tween different image contents and even different images by using single empirical gamma values [8, 10, 23, 31, 34].
We propose to adaptively learn correction factors for differ-ent image contents according to their unique brightness by introducing an Illumination-Aware Gamma Correction net-work (IAGC). IAGC integrates the effectiveness of gamma correction with the strong modelling capacities of deep neu-ral networks, so that more effective gamma can be evaluated via perceiving different image contents.
IAGC adopts a coarse-to-ﬁne strategy to learn correc-tion factors. Coarsely, we globally recover the brightness of low-light images by learning a whole correction factor with a Global Gamma Correction Module (GGCM) per-ceiving global illumination. Finely, we further build a Lo-cal Gamma Correction Module (LGCM) to locally explore pixel dependencies to elaborately enhance illumination ac-cording to different local image contents. An example is shown in Fig. 1 to illustrate our improvement on visual qual-ity over existing state-of-the-art LLIE works.
In our work, gamma is learnable, gamma correction thus introduces exponential operation in both forward and back-ward propagation, bringing high computational complexity.
We propose to use Taylor Series to approximate gamma cor-rection so that exponential operations are avoided in both forward and backward propagation.
Low-light images usually contain large-scale dark ar-eas, which are ineffective to be enhanced by CNNs due to their locality bias. Similarly, calculating attention patch-wisely [24] or on a downsampled image [43, 52] is also veriﬁed to be not enough to handle such large dark areas.
We propose to completely model the dependencies of all pixels in an image, so that information from far bright re-gions can be used to recover dark areas. To reach such a goal, we propose a COmpletely MOdelling Vision Trans-former (COMO-ViT) block to describe images via a local-to-global hierarchical self-attention mechanism. Extensive experiments in later sections verify the stronger modelling capacity of our COMO-ViT for modelling long-range im-age dependencies than state-of-the-art Transformer blocks.
The main contributions of this paper are summarized as:
• We are the ﬁrst to propose utilizing learnable gamma correction to adaptively solve the insensitivity of existing models to extract effective illumination-recovering deep representation. To reach this goal, two illumination-aware gamma correction modules (GGCM and LGCM) are introduced to predict global and local adaptive correction factors. Moreover, Tay-lor Series are used to approximate exponential opera-tion to improve running efﬁciency.
• We propose to completely express images by explor-ing extensive dependencies of all pixels. A novel vi-sion Transformer (COMO-ViT) block is thus intro-duced to connect all pixels via a local-to-global hier-archical self-attention. COMO-ViT is veriﬁed to be able to extract highly-effective deep features and keep strong modelling capacities.
• We introduce an effective network architecture, i.e.,
IAGC, for low-light image enhancement by combining
GGCM, LGCM, and COMO-ViT block together. Ex-tensive experiments show that we obtain better quanti-tative evaluation and hue recovery than SOTA methods compared with ground truth. 2.