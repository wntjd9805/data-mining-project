Abstract
We introduce a novel superpoint-based transformer ar-chitecture for efficient semantic segmentation of large-scale 3D scenes. Our method incorporates a fast algorithm to par-tition point clouds into a hierarchical superpoint structure, which makes our preprocessing 7 times faster than existing superpoint-based approaches. Additionally, we leverage a self-attention mechanism to capture the relationships be-tween superpoints at multiple scales, leading to state-of-the-art performance on three challenging benchmark datasets:
S3DIS (76.0% mIoU 6-fold validation), KITTI-360 (63.5% on Val), and DALES (79.6%). With only 212k parameters, our approach is up to 200 times more compact than other state-of-the-art models while maintaining similar perfor-mance. Furthermore, our model can be trained on a single
GPU in 3 hours for a fold of the S3DIS dataset, which is 7× to 70× fewer GPU-hours than the best-performing meth-ods. Our code and models are accessible at github.com/ drprojects/superpoint_transformer. 1.

Introduction
As the expressivity of deep learning models increases rapidly, so do their complexity and resource requirements
[13]. In particular, vision transformers have demonstrated remarkable results for 3D point cloud semantic segmentation
[56, 37, 16, 23, 32], but their high computational require-ments make them challenging to train effectively. Addition-ally, these models rely on regular grids or point samplings, which do not adapt to the varying complexity of 3D data: the same computational effort is allocated everywhere, regard-less of the local geometry or radiometry of the point cloud.
This issue leads to needlessly high memory consumption, limits the number of points that can be processed simultane-ously, and hinders the modeling of long-range interactions.
Superpoint-based methods [26, 24, 21, 40] address the 75 70 d l o
F
-6
U o
I m
SPT
Stratified Trans.
L
Point Trans.
SPT-nano
RandLaNet
B
XL
Deep
View
Agg
KPConv
MinkowskiNet
S 100h
PointNeXt 25h 5h
Training time (GPU-h)
SPG 60 104 105
Model Size 107
Figure 1: Model Size vs. Performance. We visualize the performance of different methods on the S3DIS dataset (6-fold validation) in relation to their model size in log-scale.
The area of the markers indicates the GPU-time to train on a single fold. Our proposed method Superpoint Transformer (SPT) achieves state-of-the-art with a reduction of up to 200-fold in model size and 70-fold in training time (in GPU-h) compared to recent methods. The even smaller SPT-nano model achieves a fair performance with 26k parameters only. limitation of regular grids by partitioning large point clouds into sets of points— superpoints—which adapt to the local complexity. By directly learning the interaction between su-perpoints instead of individual points, these methods enable the analysis of large scenes with compact and parsimonious models that can be trained faster than standard approaches.
However, superpoint-based methods often require a costly preprocessing step, and their range and expressivity are lim-ited by their use of local graph-convolution schemes [46].
In this paper, we propose a novel superpoint-based trans-former architecture that overcomes the limitations of both approaches, see Figure 1. Our method starts by partition-ing a 3D point cloud into a hierarchical superpoint struc-ture that adapts to the local properties of the acquisition at multiple scales simultaneously. To compute this partition efficiently, we propose a new algorithm that is an order of magnitude faster than existing superpoint preprocessing al-gorithms. Next, we introduce the Superpoint Transformer (SPT) architecture, which uses a sparse self-attention scheme to learn relationships between superpoints at multiple scales.
By viewing the semantic segmentation of large point clouds as the classification of a small number of superpoints, our model can accurately classify millions of 3D points simulta-neously without relying on sliding windows. SPT achieves near state-of-the-art accuracy on various open benchmarks while being significantly more compact and able to train much quicker than common approaches. The main contribu-tions of this paper are as follows:
• Efficient Superpoint Computation: We propose a new method to compute a hierarchical superpoint structure for large point clouds, which is more than 7 times faster than existing superpoint-based methods. Our preprocessing time is also comparable or faster than standard approaches, ad-dressing a significant drawback of superpoint methods.
• State-of-the-Art Performance: Our model reaches per-formance at or close to the state-of-the-art for three open benchmarks with distinct settings: S3DIS for indoor scan-ning [3], KITTI-360 for outdoor mobile acquisitions [28], and DALES for city-scale aerial LiDAR [50].
• Resource-Efficient Models: SPT is particularly resource-efficient as it only has 212k parameters for S3DIS and
DALES, a 200-fold reduction compared to other state-of-the-art models such as PointNeXt [39] and takes 70 times fewer GPU-h to train than Stratified Transformer [23]. The even more compact SPT-nano reaches 70.8% 6-Fold mIoU on S3DIS with only 26k parameters, making it the smallest model to reach above 70% by a factor of almost 300. memory consumption, which hinders the processing of large scenes and the ability to leverage global context cues.
Partition-Based Methods. Partitioning images into super-pixels has been studied extensively to simplify image analy-sis, both before and after the widespread use of deep learning
[1, 49]. Similarly, superpoints are used for 3D point cloud segmentation [36, 29] and object detection [17, 10]. Super-PointGraph [26] proposed to learn the relationship between superpoints using graph convolutions [46] for semantic seg-mentation. While this method trains fast, its preprocessing is slow and its expressivity and range are limited, as it operates on a single partition. Recent works have proposed ways of learning the superpoints themselves [24, 21, 48], which yields improved results but at the cost of an extra training step or a large point-based backbone [22].
Hierarchical partitions are used for image processing
[2, 54, 55] and 3D analysis tasks such as point cloud com-pression [11] and object detection [7, 27]. Hierarchical ap-proaches for semantic segmentation use Octrees with fixed grids [35, 43]. On the contrary, SPT uses a multi-scale hi-erarchical structure that adapts to the local geometry of the data. This leads to partitions that conform more closely to semantic boundaries, enabling the network to model the interactions between objects or object parts.
Efficient 3D Learning. As 3D scans of real-world scenes can contain hundreds of millions of points, optimizing the ef-ficiency of 3D analysis is an essential area of research. Point-NeXt [39] proposes several effective techniques that allow simple and efficient methods [38] to achieve state-of-the-art performance. RandLANet [20] demonstrates that efficient sampling strategies can yield excellent results. Sparse [14] or hybrid [31] point cloud representations have also helped reduce memory usage. However, by leveraging the local sim-ilarity of dense point clouds, superpoint-based methods can achieve an input reduction of several orders of magnitude, resulting in unparalleled efficiency. 2.