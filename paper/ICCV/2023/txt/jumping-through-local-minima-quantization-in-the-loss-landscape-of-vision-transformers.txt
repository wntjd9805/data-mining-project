Abstract
Quantization scale and bit-width are the most im-portant parameters when considering how to quantize a neural network. Prior work focuses on optimizing quantization scales in a global manner through gradient methods (gradient descent & Hessian analysis). Yet, when applying perturbations to quantization scales, we observe a very jagged, highly non-smooth test loss land-scape.
In fact, small perturbations in quantization scale can greatly affect accuracy, yielding a 0.5 − 0.8% accuracy boost in 4-bit quantized vision transformers (ViTs). In this regime, gradient methods break down, since they cannot reliably reach local minima. In our work, dubbed Evol-Q, we use evolutionary search to ef-fectively traverse the non-smooth landscape. Addition-ally, we propose using an infoNCE loss, which not only helps combat overfitting on the small calibration dataset (1, 000 images) but also makes traversing such a highly non-smooth surface easier. Evol-Q improves the top-1 accuracy of a fully quantized ViT-Base by 10.30%, 0.78%, and 0.15% for 3-bit, 4-bit, and 8-bit weight quantization levels. Extensive experiments on a variety of CNN and ViT architectures further demonstrate its robustness in extreme quantization scenarios. Our code is available at https: // github. com/ enyac-group/ evol-q . 1.

Introduction
Quantization is a widespread technique for efficient neural network inference: reducing data precision from 32 bits to ≤ 8 bits is an effective approach to aggres-sively reduce model footprint and speed up computa-tion. Network quantization is an extremely important tool for deploying models in cloud [28] and edge set-tings [27], where we aim to maximize accuracy while reducing the computational burden. We consider the post-training quantization (PTQ) setting, where there (a) ResNet-18 Test Loss (b) DeiT-Tiny Test Loss
Figure 1: We perturb along two basis vectors of one layer/block’s quantization scales. The test loss land-scape during perturbation is smooth in the CNN case (a), and highly non-smooth in the ViT case (b). is access to a small (∼1, 000 image) calibration dataset but no access to the original training dataset. PTQ is an integral component of model deployment, when a carefully curated full-precision model is too expensive to retrain. Our work, Evol-Q, is a PTQ method for vision transformers which leverages four key observa-tions: 1. Small perturbations in quantization scale can lead to significant improvement in quantization accuracy. For example, small ad-justments in a self-attention block’s scale can in-duce a ± 1% change in accuracy. 2. As shown in Fig. 1, quantized vision trans-formers (ViTs) have an extremely non-smooth loss landscape, particularly with re-spect to the perturbation in quantization scales, making stochastic gradient descent a poor choice for optimization. We use evolutionary search to favor nearby local minima to significantly improve accuracy (∼ 0.5 − 1%). 3. In comparison to non-contrastive loss functions such as mean squared error, cosine similarity, and the KL divergence, contrastive losses tend to
smooth the loss landscape, as observed in our experiments and supported by recent work [10]. This finding inspires the use of con-trastive loss to further facilitate the quantization scale search process. Contrastive losses, specif-ically the infoNCE loss in this work, also helps in combating overfitting on the small calibration dataset by incorporating negative examples into the loss. 4. The Evol-Q framework generalizes to CNN quantization as well, since the infoNCE loss provides a smoother landscape than other losses.
Combining these observations, we devise a new op-timization scheme to adjust the quantization scales of low bit-width ViTs. Instead of using gradient descent to optimize all network parameters or estimating a noisy Hessian, we propose a series of cheap evolutionary search procedures to successively minimize the quanti-zation error. Evol-Q injects small perturbations into the quantization scale at one layer and uses a global infoNCE loss to evaluate them. In the next section, we will show how prior work does not properly address the non-smooth ViT loss landscape. 2.