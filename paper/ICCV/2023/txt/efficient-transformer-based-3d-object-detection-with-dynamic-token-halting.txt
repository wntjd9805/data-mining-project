Abstract
Balancing efﬁciency and accuracy is a long-standing problem for deploying deep learning models. The trade-off is even more important for real-time safety-critical sys-tems like autonomous vehicles. In this paper, we propose an effective approach for accelerating transformer-based 3D object detectors by dynamically halting tokens at different layers depending on their contribution to the detection task.
Although halting a token is a non-differentiable operation, our method allows for differentiable end-to-end learning by leveraging an equivalent differentiable forward-pass. Fur-thermore, our framework allows halted tokens to be reused to inform the model’s predictions through a straightfor-ward token recycling mechanism. Our method signiﬁcantly improves the Pareto frontier of efﬁciency versus accuracy when compared with the existing approaches. By halting tokens and increasing model capacity, we are able to im-prove the baseline model’s performance without increasing the model’s latency on the Waymo Open Dataset. 1.

Introduction
Recent progress has shown the potential of applying the transformer architecture, which has been widely used by the natural language processing community [72, 25], to computer vision tasks [11, 42]. Transformers already meet or exceed the performance of convolutional neural networks. However, transformer architectures often suf-fer from high latency, which is crucial for real-time safety-critical or edge-computing applications.
This paper explores how to speed-up transformer-based 3D object detection. Our method is inspired by network pruning, which increases efﬁciency by removing less im-portant parts of the model. However, instead of pruning neurons like in [41, 43, 84], our approach prunes or halts
*mao.ye@getcruise.com
†greg.meyer@getcruise.com tokens. We want to reduce superﬂuous tokens because the computational complexity of the transformer’s attention mechanism increases with the number of tokens. Unlike network pruning, deciding whether or not to halt a token needs to depend on the input, since the importance of a to-ken will depend on the particular example.
There are several recent works that attempt to dynami-cally halt tokens throughout a vision transformer [58, 87, 54, 16, 31, 70]. However, these works consider the task of image classiﬁcation, while we focus on 3D object detec-tion. Going from image classiﬁcation to 3D object detection has its challenges, but also its beneﬁts. A challenge that arises with object detection is that any token could contain an object; therefore, we require a method that can detect ob-jects from all tokens regardless of whether or not they were halted. This issue does not occur for image classiﬁcation with vision transformers. For image classiﬁcation, an ar-tiﬁcial token is often added to classify the image, and this token is prevented from being halted. Since the halting of a token is a non-differentiable operation, a new design of the computation graph is needed to deﬁne pseudo-gradients that enable the back-propagation during training. A beneﬁt with object detection is that the labels contain not only the object’s classiﬁcation but also its location, and our approach is able to leverage the localization of the objects to help the model learn which tokens are more important than others.
Our method is designed for 3D object detection for au-tonomous driving. For safety-critical tasks, it is often im-portant that the model is non-stochastic. Therefore, unlike
[58, 16], our halting module is designed to be deterministic.
Our contributions can be summarized as follows:
• We propose a deterministic module that progressively halts tokens throughout the transformer, and a simple but effective token recycling mechanism to reuse the information from the halted tokens.
• An equivalent differentiable forward-pass is proposed to overcome the non-differentiability of token halting, and a theoretical analysis is conducted to evaluate the accuracy of the pseudo-gradient.
• A non-uniform token sparsity loss is employed to im-prove the learning of the halting module by utilizing the ground-truth bounding boxes. tempts to downsample the background points, while our ap-proach tries to halt any unnecessary tokens. Due to the dif-ferent goals, the algorithms are considerably different. 2.