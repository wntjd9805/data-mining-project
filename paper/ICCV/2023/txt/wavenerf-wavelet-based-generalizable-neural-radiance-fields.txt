Abstract
Neural Radiance Field (NeRF) has shown impressive performance in novel view synthesis via implicit scene rep-resentation. However, it usually suffers from poor scala-bility as requiring densely sampled images for each new scene.
Several studies have attempted to mitigate this problem by integrating Multi-View Stereo (MVS) technique into NeRF while they still entail a cumbersome ﬁne-tuning process for new scenes. Notably, the rendering qual-ity will drop severely without this ﬁne-tuning process and the errors mainly appear around the high-frequency fea-tures. In the light of this observation, we design WaveN-eRF, which integrates wavelet frequency decomposition into
MVS and NeRF to achieve generalizable yet high-quality synthesis without any per-scene optimization.
To pre-serve high-frequency information when generating 3D fea-ture volumes, WaveNeRF builds Multi-View Stereo in the
Wavelet domain by integrating the discrete wavelet trans-form into the classical cascade MVS, which disentangles high-frequency information explicitly. With that, disentan-gled frequency features can be injected into classic NeRF via a novel hybrid neural renderer to yield faithful high-frequency details, and an intuitive frequency-guided sam-pling strategy can be designed to suppress artifacts around high-frequency regions. Extensive experiments over three widely studied benchmarks show that WaveNeRF achieves superior generalizable radiance ﬁeld modeling when only given three images as input. 1.

Introduction
Rendering novel views from a set of posed scene im-ages has been studied for years in the ﬁelds of computer vision and graphics. With the emergence of implicit neu-ral representation, neural radiance ﬁeld (NeRF)[25] and its variants[21, 23] have recently achieved very impressive per-formance in novel view synthesis with superb multi-view
*Shijian Lu is the corresponding author. consistency. However, most existing works fall short in model scalability by requiring a per-scene optimization pro-cess with densely sampled multi-view images for training.
To avoid the cumbersome process of training from scratch for new scenes, a popular line of generalizable
NeRF [3, 37, 32, 34, 13] introduces a pipeline that ﬁrst trains a base model on the training data and then conducts
ﬁne-tuning for each new scene, which improves the scal-ability and shortens the per-scene training process. Their base models often extract features from the source views and then inject the features into a neural radiance ﬁeld. Sev-eral previous studies [37, 32] directly use CNN networks to extract features while recent generalizable NeRF mod-els [3, 34, 13] resort to Multi-View Stereo (MVS) technique to warp 2D source feature maps into 3D features planes, yielding better performance than merely using CNN net-works. However, per-scene ﬁne-tuning still entails a fair number of posed training images that are often challenging to collect in various real-world tasks. On the other hand, removing the per-scene ﬁne-tuning will incur a signiﬁcant performance drop with undesired artifacts and poor detail.
Notably, we intriguingly observe that the rendering error mainly lies around image regions with rich high-frequency information as illustrated in Fig. 1. The phenomenon of los-ing high-frequency detail is largely attributed to the fact that most existing generalizable NeRFs conduct down-sampling operations at the feature extraction stage of their pipeline, i.e., the CNN networks adopted in [37, 32] or the MVS module adopted in [3, 34, 13]. the
In the light of aforementioned observation, we present Wavelets-based Neural Radiance Fields (WaveNeRF) which incorporates explicit high-frequency information into the training process and thus obviates the per-scene ﬁne-tuning under the generalizable and few-shot setting. Speciﬁcally, with MVS technique to construct 3D feature volumes which are converted to model NeRF in the spatial domain, we further design a Wavelet Multi-View Stereo (WMVS) to incorporate scene wavelet coef-ﬁcients into the MVS to achieve frequency-domain mod-eling. Distinct from other frequency transformations like
(a) Ground Truth (GT) (b) Rendered Result (c) Absolute Rendering Errors (d) High-Frequency Features of GT
Figure 1: The comparison between the absolute rendering errors (c) of GeoNeRF[13] and the high-frequency features of the ground truth (d). We can see that the errors mainly appear around the pixels with high-frequency features.
Fourier Transform, WaveNeRF employs Wavelet Trans-form which is coordinate invariant and preserves the relative spatial positions of pixels. This property is particularly ad-vantageous in the context of MVS as it allows multiple input views to be warped in the direction of a reference view to form sweeping planes in both the spatial domain and the fre-quency domain within the same coordinate system. Apart from MVS, this property also enables to build a frequency-based radiance ﬁeld so that a designed Hybrid Neural Ren-derer (HNR) can leverage the information in both the spatial and frequency domains to boost the rendering quality of the appearance, especially around the high-frequency regions.
In addition, WaveNeRF is also equipped with a Frequency-guided Sampling Strategy (FSS) which enables the model to focus on the regions with larger high-frequency coefﬁ-cients. The rendering quality can be improved clearly with
FSS by sampling denser points around object surfaces.
The contributions of this work can be summarized in three points.
• First, we design a WMVS module that preserves high-frequency information effectively by incorporat-ing wavelet frequency volumes while extracting geo-metric scene features.
• Second, we design a HNR module that can merge the features from both the spatial domain and the fre-quency domain, yielding faithful high-frequency de-tails in neural rendering.
• Third, we develop FSS that can guide the volume ren-dering to sample denser points around the object sur-faces so that it can infer the appearance and geometry with higher quality. 2.