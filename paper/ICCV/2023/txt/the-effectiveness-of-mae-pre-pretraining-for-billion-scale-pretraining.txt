Abstract
This paper revisits the standard pretrain-then-ﬁnetune paradigm used in computer vision for visual recognition tasks. Typically, state-of-the-art foundation models are pretrained using large scale (weakly) supervised datasets with billions of images. We introduce an additional pre-pretraining stage that is simple and uses the self-supervised
MAE technique to initialize the model. While MAE has only been shown to scale with the size of models, we ﬁnd that it scales with the size of the training dataset as well. Thus, our
MAE-based pre-pretraining scales with both model and data size making it applicable for training foundation models.
Pre-pretraining consistently improves both the model con-vergence and the downstream transfer performance across a range of model scales (millions to billions of parameters), and dataset sizes (millions to billions of images). We mea-sure the effectiveness of pre-pretraining on 10 different visual recognition tasks spanning image classiﬁcation, video recog-nition, object detection, low-shot classiﬁcation and zero-shot recognition. Our largest model achieves new state-of-the-art results on iNaturalist-18 (91.3%), 1-shot ImageNet-1k (62.1%), and zero-shot transfer on Food-101 (96.2%). Our study reveals that model initialization plays a signiﬁcant role, even for web-scale pretraining with billions of images. 1.

Introduction
The pretrain-then-ﬁnetune paradigm in visual recog-nition has enabled high performance visual recognition models across a range of tasks such as image classiﬁca-tion [52, 59, 70], video action recognition [25, 27, 28], object detection [11, 89], 3D etc. Typically, pretraining consists of training a model using a pretraining task on large scale data.
The resulting pretrained models learn general purpose visual representations that can be used for a range of target tasks, often with limited labeled data, by transfer learning.
In this paper, we show that an initial stage of pre-∗Equal technical contribution. †Project Lead.
SSv2
Accuracy
K400
Accuracy 75 86
COCO
APbox 58 71 82 89 iNat18
Accuracy
LVIS
APbox 50 46 54 62 50 42 58 67 78 71 71 81 81 78 85 82 85 89
IN1k
Accuracy 66 75 75 iNat18 5-Shot 86
IN1k
Linear 79 79
IN1k 5-Shot
IN1k
Zero Shot
MAE→WSP
WSP
MAE
Figure 1: MAE pre-pretraining improves performance. Trans-fer performance of a ViT-L architecture trained with self-supervised pretraining (MAE), weakly supervised pretraining on billions of im-ages (WSP), and our pre-pretraining (MAE→WSP) that initializes the model with MAE and then pretrains with WSP. Pre-pretraining consistently improves performance. pretraining before the standard pretraining task can im-prove vision models across a variety of different tasks. Our method combines two common pretraining tasks in vision: (1) weakly supervised pretraining that uses weak, often noisy, signals such as text or image hashtags as supervision, and (2) self-supervised pretraining that only uses the data with-out additional supervision. Both forms of pretraining start training with a randomly initialized model and have proven effective at learning general purpose vision models. While there have been attempts to combine both these forms of pretraining [54, 69], they are typically used independently in the pretrain-then-ﬁnetune two stage paradigm [23, 70, 84].
In this work we explore the combination of self- and weakly-supervised learning in a simple pre-pretraining framework, as follows. We ﬁrst begin with the Masked
Autoencoder (MAE) [33] self-supervised learning technique to pre-pretrain vision models without using any labels. After
initializing from the pre-pretrained model, we use standard weakly supervised pretraining on billions of images with noisy labels. We perform a large-scale empirical study to measure the effectiveness of pre-pretraining on 10 different visual recognition tasks spanning image classiﬁcation, video recognition, object detection, low-shot classiﬁcation and zero-shot recognition. Our study reveals that pre-pretrain initialization improves the performance for the weakly su-pervised models, and this improvement holds even at billion scale weakly labeled data, and across vision tasks (Figure 1).
It also improves the model convergence during pretraining, leading to an efﬁcient way of training large scale vision mod-els. Pre-pretrain further enjoys the computational efﬁciency of the MAE approach, making it simple and scalable. Finally, we show that by using pre-pretraining, both self-supervised learning and weakly supervised learning can be combined for improved model performance for billion-scale data.
Pre-pretrain is related to ‘intermediate ﬁnetuning’ [5, 50] which introduces a stage after pretraining to better align the pretrained features with the downstream task using labeled data. In contrast, pre-pretrain serves as a better way to ini-tialize a model before pretraining. Since we leverage MAE for pre-pretraining, we do not need additional information or labels for this stage and can re-use the pretraining data.
This makes pre-pretrain convenient and simple to use with existing pretraining datasets.
Our study on large-scale pre-pretraining reveals that model initialization plays a signiﬁcant role, even for web-scale pretraining, and pre-pretraining is a simple and promis-ing technique in that direction. In particular, we show that (i) MAE not only scales with model size as shown in [33], but also with the size of the training data (Figure 2). (ii)
Pre-pretraining improves both the model convergence and the ﬁnal downstream performance for different sized mod-els (millions to billions of parameters) trained on different sized datasets (millions to billions of labels). (iii) Using pre-pretraining combines the beneﬁts of both self-supervised learning and large scale weakly-supervised learning, and our models achieve excellent performance on a variety of differ-ent visual recognition tasks (Figure 1). Most prominently, our model sets new state-of-the-art results on iNaturalist-18 image classiﬁcation (91.3%), 1-shot ImageNet-1k image classiﬁcation (62.1%), and zero-shot transfer on Food-101 (96.2%). 2.