Abstract
Cross-scene generalizable NeRF models, which can di-rectly synthesize novel views of unseen scenes, have be-come a new spotlight of the NeRF field. Several existing attempts rely on increasingly end-to-end “neuralized” ar-chitectures, i.e., replacing scene representation and/or ren-dering modules with performant neural networks such as transformers, and turning novel view synthesis into a feed-forward inference pipeline. While those feedforward “neu-ralized” architectures still do not fit diverse scenes well out of the box, we propose to bridge them with the powerful
Mixture-of-Experts (MoE) idea from large language models (LLMs), which has demonstrated superior generalization ability by balancing between larger overall model capacity and flexible per-instance specialization. Starting from a re-cent generalizable NeRF architecture called GNT [52], we first demonstrate that MoE can be neatly plugged in to en-hance the model. We further customize a shared permanent expert and a geometry-aware consistency loss to enforce cross-scene consistency and spatial smoothness respec-tively, which are essential for generalizable view synthesis.
Our proposed model, dubbed GNT with Mixture-of-View-Experts (GNT-MOVE), has experimentally shown state-of-the-art results when transferring to unseen scenes, indicat-ing remarkably better cross-scene generalization in both zero-shot and few-shot settings. Our codes are available at https://github.com/VITA-Group/GNT-MOVE. 1.

Introduction
Given several images from different viewpoints, Neu-ral Radiance Field (NeRF) has achieved remarkable suc-cess on synthesizing novel views. Most existing methods
[34, 31, 40, 60, 14, 50, 3, 63, 64, 22] focus on overfit-ting one single scene by reconstructing its 3D radiance field
*Equal contribution. in a “backward” manner. Though capable of generating realistic and consistent novel views, the need for retrain-ing on each new scene limits their practical applications.
Recently, generalizable NeRF has settled a new trend: in place of the costly per-scene fitting, several pioneer works
[69, 57, 68, 52, 48] attempt to synthesize novel views of un-seen scenes in a “feedforward” fashion on the fly. Those models are first pre-trained by learning how to represent scenes and render novel views from captured images across different scenes, achieving high-quality “zero-shot” infer-ence results on new scenes. Among them, Generalizable
NeRF Transformer (GNT) [52] stands out by replacing the explicit scene modeling and rendering function via unified, data-driven, and scalable transformers, and automatically inducing multi-view consistent geometries and renderings via large-scale novel view synthesis pre-training.
However, those cross-scene NeRF models face the fun-damental dilemma between “generality” and “specializa-tion”. On the one hand, they need to broadly cover both diverse scene representations and/or rendering mechanisms due to different scene properties (e.g., color, materials) – hence larger overall model size is needed to guarantee suffi-cient expressiveness. On the other hand, since a single scene usually consists of specialized self-similar appearance pat-terns, those models must also be capable of per-scene spe-cialization to model the scene closely. Existing generaliz-able models still do not achieve a satisfactory balance be-tween both “generality” and “specialization”, as most of them [52, 48] do not fit diverse scenes well out of box, and some [57, 68] will need extra per-scene optimization step.
To fill the aforementioned gap, we propose to introduce and customize the powerful Mixture-of-Experts (MoE) idea
[46] into GNT framework, which is composed of a view transformer that aggregates multi-view image features and a ray transformer that decodes the point feature to synthe-size novel views. The inspiration is drawn from Large Lan-guage Models (LLMs) [27, 12], where MoE has become
the key knob to improve the generalization of these models, scaling up the total model size without exploding the per-inference cost, by encouraging different submodels (com-bination of activated experts) to be sparsely activated for different inputs and hence become “specialized”.
Specifically, to balance between cross-scene “general-ization” and per-scene “specialization”, we bake MoE into
GNT’s view transformer1, leading to a new GNT with
Mixture-of-View-Experts (GNT-MOVE). However, as we observed from experiments, naively plugging MoE into
NeRF fails to well balance between generality and special-ization, due to their intension with generalizable NeRF’s cross-scene consistency and spatial smoothness priors:
• Cross-scene consistency: similar appearance patterns or similar materials, from different scenes, should be treated consistently by choosing similar experts.
• Spatial smoothness: nearby views in the same scene should change continuously & smoothly, hereby mak-ing similar or smoothly transiting expert selection.
Those two “priors” are owing to the natural image ren-dering and multi-view geometry constraints. Yet, enforc-ing them risks causing the notorious representation collapse of MoEs [75], i.e., differently activated submodels may naively learn the same or similar functions and be unable to capture diverse specialized features. Such representational collapse has been addressed a lot in the general MoE litera-ture [76, 28, 43]. But it remains elusive whether those solu-tions will be at odds with the “consistency/smoothness”: a new challenge we must pay attention to.
In order to mitigate such gaps, we investigate two cus-tomized improvements of MoE for NeRF. Firstly, we aug-ment the MoE layer with a shared permanent expert, that will be selected in all cases. This shared expert enforces the commodity across scenes as an architectural regular-ization, and boosts cross-scene consistency. Secondly, a spatial smoothness objective is introduced for geometric-aware continuity, by encouraging two spatially close points to choose similar experts, and using the geometric distance between sampled points to re-weight their expert selections.
We empirically find the two consistency regularizations to work well with the typical expert diversity regularizer in
MoEs, together ensuring effectively large model capacity as well as meeting the consistency/smoothness demands. We have conducted comprehensive experiments on complex scene benchmarks. Remarkably, when trained on multiple scenes, GNT-MOVE attains state-of-the-art performance in two aspects: (1) often notably better zero-shot generaliza-tion to unseen scenes; and (2) consistently stronger perfor-mance on few-shot generalization to unseen scenes. 1In this paper, we mainly focus on the view transformer based on the hypothesis that the modular design of MoE could be naturally beneficial to multi-view feature aggregation. Introducing MoE into the ray transformer may be also promising and we leave it as future work.
Our main contributions can be summarized as follows:
• We present an LLM-inspired NeRF framework, GNT-MOVE, which significantly pushes the frontier of gen-eralizable novel view synthesis on complex scenes by introducing Mixture-of-Experts (MoE) transformers.
• To tailor MoE for generalizable NeRF, we introduce a shared permanent expert for cross-scene rendering consistency, and a geometry-aware spatial consistency objective for cross-view spatial smoothness.
• Experiments on complex scene benchmarks validate the effectiveness of GNT-MOVE on cross-scene gen-eralization with both zero-shot and few-shot settings. 2.