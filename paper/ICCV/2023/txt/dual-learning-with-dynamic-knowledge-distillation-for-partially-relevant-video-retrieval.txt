Abstract
Query: A man opens the door and enters the room.
Relevant moments nt
Relevan t moments
Almost all previous text-to-video retrieval works assume that videos are pre-trimmed with short durations. How-ever, in practice, videos are generally untrimmed contain-ing much background content. In this work, we investigate the more practical but challenging Partially Relevant Video
Retrieval (PRVR) task, which aims to retrieve partially rel-evant untrimmed videos with the query input. Particularly, we propose to address PRVR from a new perspective, i.e., distilling the generalization knowledge from the large-scale vision-language pre-trained model and transferring it to a task-speciﬁc PRVR network. To be speciﬁc, we introduce a
Dual Learning framework with Dynamic Knowledge Distil-lation (DL-DKD), which exploits the knowledge of a large vision-language model as the teacher to guide a student model. During the knowledge distillation, an inheritance student branch is devised to absorb the knowledge from the teacher model. Considering that the large model may be of mediocre performance due to the domain gaps, we further develop an exploration student branch to take the beneﬁts of task-speciﬁc information. In addition, a dynami-cal knowledge distillation strategy is further devised to ad-just the effect of each student branch learning during the training. Experiment results demonstrate that our proposed model achieves state-of-the-art performance on ActivityNet and TVR datasets for PRVR. 1.

Introduction
With the explosion of online videos, searching the videos of interest has been an indispensable activity in people’s daily lives. Meanwhile, text-to-video retrieval (T2VR), re-trieving videos w.r.t. a textual query from a large num-*Both authors contributed equally to this work.
†Corresponding author: Baolong Liu (liubaolongx@gmail.com)
Retrieval
..
.
.
.
.
.
.
.
Untrimmed Video Gallery (a) n o i t a r u
D e g a r e v
A 100 50 0
For mainstream T2VR
For PRVR
MSVD VATEX MSRVTT TVR ActivityNet (b)
....
...
.
.
...........
SOTA
CLIP
ActivityNet
TVR (c)
R m u
S 250 200 150 100 50 0
Figure 1. (a) An illustrative example of the PRVR task; (b)
Mean values of video duration of different datasets; (c) Perfor-mance comparisons between the state-of-the-art (SOTA) [8] and the vanilla CLIP-based [2] methods, where CLIP shows huge per-formance divergences for different PRVR datasets. ber of unlabeled videos, attracts growing attention re-cently [1, 6, 25, 35, 50, 59]. One basic assumption prereq-uisite for mainstream T2VR is that videos are pre-trimmed with short duration and supposed to be fully relevant to the query [29, 48, 61]. However, in practical applications, the majority of the existing videos are untrimmed. Besides, as queries are not known a priori, pre-trimmed video clips may not contain sufﬁcient content to fully meet the query. There-fore, there is a huge gap between the literature and the real world for the mainstream T2VR task [8].
To ﬁll the gap, a new text-to-video retrieval subtask, i.e.,
Partially Relevant Video Retrieval (PRVR), has been pro-posed recently in [8]. Different from previous T2VR, PRVR aims to retrieve the partially relevant untrimmed videos that contain at least one internal moment relevant to the given  
query (as exempliﬁed in Fig. 1(a)). Besides, videos used for
PRVR are much longer than that for T2VR (see Fig. 1(b)).
In this work, we target the PRVR task, considering it is more consistent with practical video retrieval scenarios.
Recently, we notice an increasing use of large-scale pre-trained vision and language models, e.g., Contrastive for various
Language-Image Pre-training (CLIP) [46], cross-modal tasks, recognition [49, 53], semantic segmen-tation [55,65] and person Re-Identiﬁcation [20,56], such as text-image retrieval [19, 47], visual question answer [4, 13], and achieving dominant performances. For text-to-video re-trieval task, current works [2, 15, 36, 39] mainly focus on the learning of temporal aggregation layers on top of CLIP features, this is because the videos are mainly composed of image sequences while CLIP is trained only on image-text pairs. Different from short videos in these works, the
PRVR task contains more complicated untrimmed videos with longer-duration moments of mixed query-relevant and query-irrelevant activities.
Therefore, directly treating
PRVR as mainstream text-to-video retrieval and aggregat-ing the CLIP features across all the frames may lead to huge performance divergences on PRVR datasets. As shown in
Fig. 1 (c), a vanilla CLIP performs superior on ActivityNet but depressing on TVR. Therefore, how to effectively trans-fer the knowledge of CLIP to PRVR models is still an open problem.
To this end, we propose a Dual Learning framework with
Dynamic Knowledge Distillation (DL-DKD) to purify the knowledge of CLIP into the PRVR. Speciﬁcally, we develop an effective teacher-student network where the CLIP model is adopted as the teacher and a dual-branch student model is devised to acquire the knowledge. The reason why we intro-duce two student branches is that CLIP may suffer from do-main gap issues due to complicated datasets. Therefore, one inheritance student branch is introduced to directly absorb the beneﬁcial knowledge of the teacher model on a speciﬁc domain, while another exploration student branch is uti-lized to only explore the task-speciﬁc property of the train-ing data. In addition, motivated by the fact that human be-ings ﬁrst learn from teachers and slowly carry out self-living evolutionary learning once they have formed their own pre-liminary cognition. Thus, a dynamic knowledge distillation strategy is devised, namely, the inheritance branch takes the prime position at the beginning and the exploration branch gradually becomes more prominent during the training pro-cess. In this manner, our DL-DKD is able to take the ad-vantage of both the powerful generalization-ability of CLIP and the beneﬁts of task-speciﬁc model convergence on the
PRVR data while alleviating their limitations, achieving more robust and effective retrieval. To sum up, the con-tributions of this work are threefold: propriate knowledge selectively for partially relevant video retrieval. Meanwhile, our framework supports single-teacher and multiple-teacher distillation.
• We explore how to take the advantage of the powerful generalization-ability of the large model and the ben-eﬁts of the task-speciﬁc model simultaneously while alleviating their limitations, and propose a dynamical knowledge distillation strategy.
• Extensive experiments demonstrate the effectiveness of the above contributions, and our proposed model achieves state-of-the-art performance on the challeng-ing PRVR task. 2.