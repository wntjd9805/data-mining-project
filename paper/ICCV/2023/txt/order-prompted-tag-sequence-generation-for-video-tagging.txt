Abstract
Video Tagging intends to infer multiple tags spanning relevant content for a given video. Typically, video tags are freely defined and uploaded by a variety of users, so they have two characteristics: abundant in quantity and disordered intra-video. It is difficult for the existing multi-label classification and generation methods to adapt di-rectly to this task. This paper proposes a novel gener-ative model, Order-Prompted Tag Sequence Generation (OP-TSG), according to the above characteristics.
It re-gards video tagging as a tag sequence generation problem guided by sample-dependent order prompts. These prompts are semantically aligned with tags and enable to decouple tag generation order, making the model focus on modeling the tag dependencies. Moreover, the word-based generation strategy enables the model to generate novel tags. To verify the effectiveness and generalization of the proposed method, a Chinese video tagging benchmark CREATE-tagging, and an English image tagging benchmark Pexel-tagging are es-tablished. Extensive results show that OP-TSG is signifi-cantly superior to other methods, especially the results on rare tags improve by 3.3% and 3% over SOTA methods on
CREATE-tagging and Pexel-tagging, and novel tags gener-ated on CREATE-tagging exhibit a tag gain of 7.04%. 1.

Introduction
Video tags are a series of discrete descriptive text in a free form, usually freely defined and uploaded by video platform users, to represent the specific content of the video.
Short video platforms have a large number of videos with no tags or low-quality tags. Exploring automatic video tagging technology can effectively serve practical industrial require-ments such as video recommendation, retrieval, and content review, and significantly reduce labor costs.
* Equal contribution. † Corresponding author.
Figure 1. Comparison of video categories, tags, and captions.
Video tags are more abundant in quantity than categories and are intra-video disordered compared to video captions.
Video tags have the nature of being abundant in quantity and disordered intra-video compared with video categories (labels)1 and video captions, respectively, as shown in Fig-ure 1. Compared to the fixed number of video categories strictly defined by experts, the abundance of user-defined video tags is primarily reflected in the following two as-pects: (1) Multiple perspectives for the same video, such as entities, attributes, scenes, or styles; (2) Distinct granular-ities for the same content, such as separate words or more expressive phrases. As a result, large collections of tags can easily reach tens of thousands or even hundreds of thou-sands of magnitude in a large-scale scenario, presenting an extreme long-tail distribution. Compared to video captions 1Note that video categories and labels are generally represented in the form of indexes, and the number is fixed and small. This paper does not distinguish between them.
that consider grammatical correctness and fluency, multiple video tags have no fixed order within the same video, al-though they are correlated with each other.
The above characteristics of video tags make it diffi-cult to directly apply current video multi-label classification models [4, 37, 6, 20] and generation models [24, 10, 34] to video tagging task. On the one hand, multi-label classifi-cation methods face a serious long-tail problem and also need to construct a classification head consistent with the pre-defined tag set, which will introduce a large number of parameters and cannot be replicated when the tag set changes. On the other hand, although the autoregressive-based generation methods can avoid bloated classification heads through word-by-word generation, the feature of tag disorder will plague the decoder of sequential generation and thus reduce the generation quality. Specifically: (1)
Rule-based tag orders (e.g., frequency-first) are subopti-mal; (2) Randomly ordering tags at each sampling without prompts puts the model in the dilemma of the same vision input with different tag sequences.
To this end, we propose a novel generative model, OP-TSG, which regards video tagging as a sequence generation problem guided by prompts and equips with a word-based generation strategy. OP-TSG includes a Video-Title Mul-timodal Hybrid Encoding module and an Order-prompted
Tag Sequence Decoding module. Specifically, the encod-ing module integrates visual and textual multimodal infor-mation into a unified video representation. The decoding module consists of two components: the prompt encoder and the order-aware tag decoder. The prompt encoder takes a fixed number of learnable quires as input and interacts with the video representation to generate a series of sample-dependent order prompts. These order prompts are then as-sociated with multiple annotated tags through the similarity measure function to form a similarity matrix. The Hungar-ian algorithm is introduced to acquire the bipartite matching between prompts and tags. In other words, the meaningful prompt is assigned to a specific tag, whereas the meaning-less prompt is assigned to a pre-defined [PAD] tag. Since each prompt gets a unique assigned tag, we feed prompts into the order-aware tag decoder whose generation target is a sentence composed of corresponding tags.
Our model has several advantages: (1) Improved tag de-pendency modeling: we decouple the tag generation order by introducing order prompts as a guide, i.e. the generation order of tags depends on the input order of prompts, making the model focus on modeling the tag dependencies and thus alleviating the long tail problem; (2) Allowing the genera-tion of novel tags: the model can infer novel tags by using the word-based generative model and pre-training; (3) Easy to extend: there is no need to pre-define a fixed number of tag sets and no need to modify the model for end-to-end training on new data.
We newly establish two benchmarks2: the Chinese video tagging benchmark CREATE-tagging and the En-glish image tagging benchmark Pexel-tagging, to validate both the effectiveness and generalization of the proposed method. CREATE-tagging is comprised of CREATE-210K and CREATE-3M, which contain about 210K and 3M videos respectively. The larger dataset is used to validate the extensibility in pre-training mode. The Pexel-tagging con-tains 162k images, which is used to verify the generaliza-tion of the model in different languages and visual modali-ties. Videos/Images and titles are provided for each sample.
The tags are separated into common high-frequency tags and rare low-frequency tags, followed by the introduction of label-based and example-based metrics to comprehensively evaluate the model’s performance at the tag and video lev-els, respectively. In addition, we define a novel metric, tag gain, to quantify the model’s ability to generate novel tags.
The contributions of this work are listed as follows: (1) Based on the practical application scenario, we ana-lyze the characteristics of the video tagging task and explain the differences with the traditional multi-label classification and video captioning tasks. (2) We propose the order-prompted tag sequence genera-tion approach to finish the video tagging task, which improves the modeling of tag relationships and allows the generation of novel tags. (3) We develop two benchmarks for evaluation, i.e.,
CREATE-tagging and Pexel-tagging.
The results of both benchmarks demonstrate that the proposed method is significantly superior to the others. 2.