Abstract
We present a physics-based humanoid controller that achieves high-fidelity motion imitation and fault-tolerant behavior in the presence of noisy input (e.g. pose estimates from video or generated from language) and unexpected falls. Our controller scales up to learning ten thousand mo-tion clips without using any external stabilizing forces and learns to naturally recover from fail-state. Given reference motion, our controller can perpetually control simulated avatars without requiring resets. At its core, we propose the progressive multiplicative control policy (PMCP), which dynamically allocates new network capacity to learn harder and harder motion sequences. PMCP allows efficient scal-ing for learning from large-scale motion databases and adding new tasks, such as fail-state recovery, without catas-trophic forgetting. We demonstrate the effectiveness of our controller by using it to imitate noisy poses from video-based pose estimators and language-based motion gener-ators in a live and real-time multi-person avatar use case. 1.

Introduction
Physics-based motion imitation has captured the imag-ination of vision and graphics communities due to its po-tential for creating realistic human motion, enabling plau-sible environmental interactions, and advancing virtual avatar technologies of the future. However, controlling high-degree-of-freedom (DOF) humanoids in simulation presents significant challenges, as they can fall, trip, or de-viate from their reference motions, and struggle to recover.
For example, controlling simulated humanoids using poses estimated from noisy video observations can often lead hu-manoids to fall to the ground[48, 49, 20, 22]. These lim-itations prevent the widespread adoption of physics-based methods, as current control policies cannot handle noisy ob-servations such as video or language.
In order to apply physically simulated humanoids for avatars, the first major challenge is learning a motion im-itator (controller) that can faithfully reproduce human-like motion with a high success rate. While reinforcement learn-ing (RL)-based imitation policies have shown promising re-sults, successfully imitating motion from a large dataset, such as AMASS (ten thousand clips, 40 hours of motion), with a single policy has yet to be achieved. Attempts to use larger or a mixture of expert policies have been met with some success [43, 45], although they have not yet scaled to the largest dataset. Therefore, researchers have resorted to using external forces to help stabilize the humanoid. Resid-ual force control (RFC) [50] has helped to create motion imitators that can mimic up to 97% of the AMASS dataset
[20], and has seen successful applications in human pose es-timation from video[52, 21, 11] and language-based motion generation [51]. However, the external force compromises physical realism by acting as a “hand of God” that puppets the humanoid, leading to artifacts such as flying and float-ing. One might argue that, with RFC, the realism of sim-ulation is compromised, as the model can freely apply a non-physical force on the humanoid.
Another important aspect of controlling simulated hu-manoids is how to handle noisy input and failure cases. In this work, we consider human poses estimated from video or language input. Especially with respect to video input, artifacts such as floating [51], foot sliding [55], and phys-ically impossible poses are prevalent in popular pose esti-mation methods due to occlusion, challenging view point and lighting, fast motions etc. To handle these cases, most physics-based methods resort to resetting the humanoid when a failure condition is triggered [22, 20, 49]. How-ever, resetting successfully requires a high-quality reference pose, which is often difficult to obtain due to the noisy na-ture of the pose estimates, leading to a vicious cycle of falling and resetting to unreliable poses. Thus, it is im-portant to have a controller that can gracefully handle un-expected falls and noisy input, naturally recover from fail-state, and resume imitation.
In this work, our aim is to create a humanoid controller specifically designed to control real-time virtual avatars, where video observations of a human user are used to con-trol the avatar. We design the Perpetual Humanoid Con-troller (PHC), a single policy that achieves a high success rate on motion imitation and can recover from fail-state nat-urally. We propose a progressive multiplicative control pol-icy (PMCP) to learn from motion sequences in the entire
AMASS dataset without suffering catastrophic forgetting.
By treating harder and harder motion sequences as a dif-ferent “task” and gradually allocating new network capac-ity to learn, PMCP retains its ability to imitate easier mo-tion clips when learning harder ones. PMCP also allows the controller to learn fail-state recovery tasks without compro-mising its motion imitation capabilities. Additionally, we adopt Adversarial Motion Prior (AMP)[33] throughout our pipeline and ensure natural and human-like behavior during fail-state recovery. Furthermore, while most motion imi-tation methods require both estimates of link position and rotation as input, we show that we can design controllers that require only the link positions. This input can be gen-erated more easily by vision-based 3D keypoint estimators or 3D pose estimates from VR controllers.
To summarize, our contributions are as follows: (1) we propose a Perpetual Humanoid Controller that can success-fully imitate 98.9% of the AMASS dataset without applying any external forces; (2) we propose the progressive multi-plicative control policy to learn from a large motion dataset without catastrophic forgetting and unlock additional capa-bilities such as fail-state recovery; (3) our controller is task-agnostic and is compatible with off-the-shelf video-based pose estimators as a drop-in solution. We demonstrate the capabilities of our controller by evaluating on both Motion
Capture (MoCap) and estimated motion from videos. We also show a live (30 fps) demo of driving perpetually simu-lated avatars using a webcam video as input. 2.