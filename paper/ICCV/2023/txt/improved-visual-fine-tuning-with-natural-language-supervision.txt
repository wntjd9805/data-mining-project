Abstract
Fine-tuning a visual pre-trained model can leverage the semantic information from large-scale pre-training data and mitigate the over-fitting problem on downstream vi-sion tasks with limited training examples. While the prob-lem of catastrophic forgetting in pre-trained backbone has been extensively studied for fine-tuning, its potential bias from the corresponding pre-training task and data, attracts less attention. In this work, we investigate this problem by demonstrating that the obtained classifier after fine-tuning will be close to that induced by the pre-trained model. To reduce the bias in the classifier effectively, we introduce a reference distribution obtained from a fixed text classi-fier, which can help regularize the learned vision classifier.
The proposed method, Text Supervised fine-tuning (TeS), is evaluated with diverse pre-trained vision models includ-ing ResNet and ViT, and text encoders including BERT and
CLIP, on 11 downstream tasks. The consistent improve-ment with a clear margin over distinct scenarios confirms the effectiveness of our proposal. Code is available at https://github.com/idstcv/TeS. 1.

Introduction
Fine-tuning a pre-trained visual deep model has become a prevalent paradigm for vision categorization [21]. By ini-tializing the model with parameters pre-trained on a large-scale data set, fine-tuning can effectively transfer the seman-tic information from the pre-training data to diverse down-stream tasks, which is essential to mitigate the over-fitting problem on data sets with limited training examples [19].
While supervised pre-training methods [21] require full
*Work done during internship at DAMO Academy, Alibaba Group.
†Corresponding author
Figure 1. Illustration of the proposed method, TeS. During fine-tuning, the vision classifier is regularized by reference distribu-tions defined with a fixed text classifier, which is obtained from class names.
In TeS, diverse reference distributions can be ob-tained for different examples even from the same class (e.g., cats on the right side). label information for the whole data set, the recent progress on self-supervised learning shows that an applicable pre-trained model can be obtained from unlabeled data [18].
Consequently, a lot of research efforts have been devoted to exploring unsupervised pre-training, which can elim-inate the labeling cost for a vast amount of examples.
Many effective self-supervised methods have been devel-oped, e.g., instance discrimination [7, 18], cluster discrimi-nation [5, 32], and masked image modeling [17]. Compared to the supervised counterparts, fine-tuning pre-trained mod-els from these unsupervised methods can achieve compara-ble or even better performance on downstream tasks.
With the success of pre-training, fine-tuning has thus at-tracted much attention to leverage the pre-trained model appropriately for downstream tasks. One main challenge for fine-tuning is the catastrophic forgetting in pre-trained backbone [9], that is, after sufficient learning with a large learning rate, the fine-tuned model becomes far away from
the pre-trained model, incurring the over-fitting problem.
The challenge has been extensively studied and can be tack-led by simply setting a small learning rate for backbone or constraining the distance from the fine-tuned model to the pre-trained model explicitly [24, 25].
The other challenge for fine-tuning is the potential bias existing in a pre-trained model. It should be noted that a pre-trained model is optimized by a specific pretext task using only the pre-training data. Therefore, if the data distribution of a target task is different from that of the pre-training data, the pre-trained model is biased to the pre-training data dis-tribution [30]. This bias problem becomes more challenging in the scenario together with catastrophic forgetting, where the bias cannot be eliminated sufficiently with a constraint making the fine-tuned model and pre-trained model close.
A recent work [26] proposes to add a subset of pre-training data that is related to the target task for fine-tuning. The op-timization with partial pre-training data can help preserve effective information in the pre-trained model to avoid over-fitting while reducing the bias for the target task. However, it requires the access of the pre-training data, which is in-feasible in many real applications.
Recently, it has been found that side information from other related modalities can help visual pre-training. For example, CLIP [33] pre-trains dual encoders by optimizing image-text pairs that align images with their corresponding text descriptions. The obtained model shows a strong zero-shot transfer performance, which can classify images us-ing proxies consisting of text representations extracted from class names. The observation implies that text information is capable of guiding visual representation learning. How-ever, the superior zero-shot performance highly depends on the paired vision and text encoders in pre-training, and thus is not applicable for an arbitrary vision encoder.
Inspired by vision-language pre-training [33], we intro-duce natural language supervision to fine-tuning, so as to mitigate the contradiction problem between the bias in pre-trained vision models and catastrophic forgetting in fine-tuning them. First, we show that without any side informa-tion, the classifier induced by the pre-trained vision model will be largely preserved after fine-tuning on the target data, which demonstrates the potential bias exiting in the conven-tional fine-tuning pipeline. To reduce the bias in the learned classifier, we propose to include text supervision as the ref-erence information. Concretely, with a fixed pre-trained text encoder, a text classifier for the target task can be obtained by extracting proxies with class names. Given the text clas-sifier, both class-level and instance-level reference distribu-tions can be obtained for the target vision task. Then, the pre-trained vision models can be fine-tuned with the appro-priate reference distributions for the vision classifier. Since the reference regularization is independent from the back-bone, our method can reduce the bias without catastrophic forgetting.
The proposed Text Supervised fine-tuning (TeS) method is illustrated in Fig. 1. It should be noted that reference dis-tributions are extracted from a fixed text encoder, and thus the overhead for the text supervision is negligible. The main contributions of this work can be summarized as follows.
• This work proposes to leverage text supervision from a fixed text encoder for fine-tuning an arbitrary pre-trained vision model. The text encoder can be pre-trained on a large corpus with rich context information, making it a good complement to vision tasks.
• With the classifier consisting of proxies from text en-coder, we investigate the class-level and instance-level distributions for regularizing the fine-tuning of the vi-sion classifier. By minimizing the derived cross en-tropy loss, the vision model can exploit the side in-formation from the text supervision to reduce the bias from the pre-trained models.
• Experiments on extensive downstream tasks demon-strate the effectiveness of text supervision for visual fine-tuning. In addition, the CLIP text encoder outper-forms the BERT encoder [11] and it implies that the text encoder pre-trained with vision-language pretext tasks is more appropriate for supervising visual tasks. 2.