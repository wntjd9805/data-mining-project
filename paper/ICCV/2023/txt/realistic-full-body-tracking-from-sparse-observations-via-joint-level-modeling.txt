Abstract
To bridge the physical and virtual worlds for rapidly developed VR/AR applications, the ability to realistically drive 3D full-body avatars is of great significance. Al-though real-time body tracking with only the head-mounted displays (HMDs) and hand controllers is heavily under-constrained, a carefully designed end-to-end neural net-work is of great potential to solve the problem by learn-ing from large-scale motion data. To this end, we pro-pose a two-stage framework that can obtain accurate and smooth full-body motions with the three tracking signals of head and hands only. Our framework explicitly models the joint-level features in the first stage and utilizes them as spatiotemporal tokens for alternating spatial and tem-poral transformer blocks to capture joint-level correlations in the second stage. Furthermore, we design a set of loss terms to constrain the task of a high degree of freedom, such that we can exploit the potential of our joint-level mod-eling. With extensive experiments on the AMASS motion dataset and real-captured data, we validate the effective-ness of our designs and show our proposed method can achieve more accurate and smooth motion compared to ex-isting approaches. 1.

Introduction
Driving human avatars in VR/AR can help to bridge the gap between the physical and virtual worlds, and create a more natural and immersive user experience. However, in a typical capture setting, only the head and hands are tracked with Head Mounted Displays (HMD) and hand con-trollers. With limited inputs, driving the full-body avatar is inherently an underconstrained problem. Considerable en-deavor has been dedicated to addressing the challenge of inferring full-body human pose exclusively through sparse
AR/VR signals, head and hands. Although recent studies
[5, 8, 9] have shown promising results, they are not suit-able for real-time applications like VR body tracking. With real-time performance in mind, Winkler et al. [39] use re-Project page: https://zxz267.github.io/AvatarJLM.
*Equal contribution.
†Corresponding author.
Figure 1. Our method accurately estimates full-body motion using only head and hand tracking signals. inforcement learning for training and outperform kinematic approaches with fewer artifacts. But their method requires future frames, which introduces latency to the system. Most recent work AvatarPoser [16] solves the problem in a more practical way by combining transformer-based architecture and inverse-kinetic optimization, setting the benchmark on large motion capture datasets (AMASS). Despite the suc-cess of AvatarPoser across a wide variety of motion classes, we argue that a learning-based end-to-end method provides more merits in simplicity, robustness, and generalization compared with a hybrid method.
Our key insight is that correlations between different body joints should be explicitly modeled for human pose es-timation as body movements are highly structured and coor-dinated. Especially for the problem of estimating full-body motion from sparse observations, joint-level modeling is es-sential as the position and rotation of each joint can affect each other, and the overall body pose. By taking into ac-count these correlations, we can derive more plausible full-body motion, even when observations are limited. There-fore, we design a two-stage joint-level modeling framework to capture these dependencies between body joints for more accurate and smoother human motion. In the first stage, we explicitly model the joint-level features. Then we utilize these features as spatiotemporal tokens in the second stage for a transformer-based network to capture the joint-level dependencies for recovering full-body motions.
In the first stage, we explicitly model the joint-level fea-tures as 1) joint-rotation features and 2) joint-position fea-tures. Joint-rotation offers higher compactness and com-putational efficiency, whereas joint-position enables more
Illustration of our two-stage joint-level modeling framework. In the first stage, we embed a sequence of sparse signals to
Figure 2. high-dimensional input features. Then, we utilize an MLP to obtain the initial full-body poses from the features. After that, we combine the initial full-body poses and the sparse input signals to generate initial joint-level features. In the second stage, we convert the initial joint-level features to joint-level tokens and then feed those tokens to a transformer-based network to capture the joint-level dependencies in spatial and temporal dimensions alternatively. In each spatial transformer block, we supplement an additional embedded input features token generated from the high-dimensional input features. Finally, we employ an SMPL regressor to transform the spatiotemporal modeled joint-level features into the 3D full-body pose sequence. precise control and is intuitively easier to comprehend. By combining the advantages of both features, we attain a re-silient and rational human motion with the improved ability to align endpoints more accurately.
In the second stage, we alternatively use spatial trans-former blocks and temporal transformer blocks to capture the joint dependencies. Given the ambiguity inherent in our problem, we also opt to utilize the features from the first stage of each individual frame as an embedded input fea-tures (EIF) token for every single spatial transformer block to reinforce the influence of the input sparse observations.
To capitalize on the advantages of our joint-level model-ing and mitigate the risk of overfitting in the highly under-constrained problem, we have incorporated a set of loss terms into our approach. These loss terms consist of hand alignment loss, motion smoothness loss, and physical loss, each meticulously designed to enhance the efficacy and gen-eralizability of our body-tracking system in real-world sce-narios, considering the intricate and uncertain problem na-ture.
Extensive experiments on the large-scale motion dataset
AMASS [26] have demonstrated the effectiveness of our proposed designs. We also collect real-data samples for fur-ther qualitative and quantitative evaluations. Specifically, we conduct a thorough comparison of our approach against existing methods using various protocols. The comparative results show that our approach significantly outperforms ex-isting methods in all protocols by a large margin. Moreover, our qualitative results demonstrate a significant improve-ment in accuracy and smoothness over the previous state-of-the-art approach, without the need for post-processing.
In summary, our contributions are the following:
• We propose a novel two-stage network that can effec-tively estimate full-body motion from the sparse head and hand tracking signals with high accuracy and tem-poral consistency. Note that our method does not need any post-processing and significantly outperforms ex-isting state-of-the-art approaches.
• We elaborately design our feature extractor that gen-erates joint-level rotational, positional, and embedded input features. These features are then utilized as spa-tiotemporal tokens and processed using a transformer-based network, which allows for better modeling of joint-level correlations.
• We introduce a set of losses that are tailored for the task of full-body motion estimation, and experimen-tally demonstrate the effectiveness of these losses in achieving high accuracy while avoiding undesirable ar-tifacts such as floating, penetration, and skating. 2.