Abstract
Federated Learning (FL) has recently emerged as a promising distributed machine learning framework to pre-serve clients’ privacy, by allowing multiple clients to up-load the gradients calculated from their local data to a cen-tral server. Recent studies ﬁnd that the exchanged gradi-ents also take the risk of privacy leakage, e.g., an attacker can invert the shared gradients and recover sensitive data against an FL system by leveraging pre-trained generative adversarial networks (GAN) as prior knowledge. However, performing gradient inversion attacks in the latent space of the GAN model limits their expression ability and general-izability. To tackle these challenges, we propose Gradient
Inversion over Feature Domains (GIFD), which disassem-bles the GAN model and searches the feature domains of the intermediate layers.
Instead of optimizing only over the initial latent code, we progressively change the opti-mized layer, from the initial latent space to intermediate layers closer to the output images. In addition, we design a regularizer to avoid unreal image generation by adding a small l1 ball constraint to the searching range. We also ex-tend GIFD to the out-of-distribution (OOD) setting, which weakens the assumption that the training sets of GANs and
FL tasks obey the same data distribution. Extensive experi-ments demonstrate that our method can achieve pixel-level reconstruction and is superior to the existing methods. No-tably, GIFD also shows great generalizability under differ-ent defense strategy settings and batch sizes. 1.

Introduction
Federated learning [21, 35] is an increasingly popular distributed machine learning framework, which has been
*Corresponding Author t e
N e g a m
I
Q
H
F
F
Dummy Input Latent Space
GIFD
Ground Truth
Figure 1: The reconstructed results of our proposed GIFD on ImageNet[6] and FFHQ[17]. The ﬁrst column contains the randomly initialized images generated by generators.
The next two columns show the reconstruction samples of the latent space search and our proposed GIFD, applied in many privacy-sensitive scenarios [19, 33], such as ﬁnancial services, medical analysis, and recommenda-tion systems.
It allows multiple clients to participate in collaborative learning under the coordination of the central server. The central server aggregates the uploaded gradients calculated from the local data by the end users, rather than the pri-vate data. This mechanism resolves the data silos problem and brings privacy beneﬁts to distributed learning. How-ever, a series of recent studies have shown that even the gradients uploaded in FL take the risk of privacy leakage.
Zhu et al. [40] ﬁrst formulate it as an optimization prob-lem and design an optimization-based algorithm that recon-structs private data by best matching the dummy gradients with the real gradients. Zhao et al. [38] further improve the attack with an extra label restoration step. Geiping et al.
[9] ﬁrst achieve ImageNet-level recovery through a well-designed loss function that adds a new regularization and
In order to improve the uses a different distance metric. performance on larger batch sizes, Yin et al. [34] propose a batch-level label extraction method and assume that certain side-information is available to regularize feature distribu-tions through batch normalization (BN) prior.
It is widely investigated and acknowledged that a pre-trained GAN learned from a public dataset generally cap-tures a wealth of prior knowledge. Recent studies [34, 16, 20] propose to leverage the manifold of GAN as prior infor-mation, which provides a good approximation of the natu-ral image space and enhances the attacks signiﬁcantly. The aforementioned works achieve impressive results in their own scenarios, but most of them rely on strong assumptions, e.g., known labels, BN statistics, and private data distribu-tion, which are actually impractical in the real FL scenario.
Therefore, it is hard for most existing methods to recover high-quality private data in a more realistic setting.
In this paper, we advocate a simple and effective solu-tion, Gradient Inversion over Feature Domain (GIFD), to address the challenges of expression ability and generaliz-ability of pre-trained GANs. Recently, it has been shown that rich semantic information is encoded in the interme-diate features and the latent space of GANs [2, 30, 26, 5].
Among them, the GAN-based intermediate layer optimiza-tion in solving compressed sensing problems achieves great performance [5]. Inspired by these works, We reformulate the GAN inversion as a novel intermediate layer optimiza-tion problem by minimizing the gradient matching loss by searching the intermediate features of the generative model.
Speciﬁcally, our ﬁrst step is to optimize the latent space and then we optimize the intermediate layers of the generative model successively. During the feature domain optimiza-tion stage, we only use part of the generator and the so-lution space becomes larger, which can easily lead to un-real image generation. To solve this problem, we iteratively project the optimizing features to a small l1 ball centered at the initial vector induced by the previous layer. Finally, we select output images from the layer with the corresponding least gradient matching loss as the ﬁnal results. The visual comparison in Figure 1 clearly demonstrates the necessity of optimizing the intermediate feature domains.
Another issue unsolved in GAN-based gradient attacks is the ﬂexibility of private data generation under more rig-orous and realistic settings. To relax these assumptions, we ﬁrst investigate an out-of-distribution (OOD) gradient attack scenario, where the private data distribution is signif-icantly different from that of the GAN’s training set. The signiﬁcant result improvement demonstrates the proposed method has excellent generalizability and achieves great performance on OOD datasets. Furthermore, we discuss several common defense strategies in protection form gra-dient sharing[36], including gradient sparsiﬁcation [28, 1], gradient clipping [10], differential privacy [10], and Sote-ria (i.e., perturbing the data representations) [29]. These frequently used privacy defense approaches have been con-ﬁrmed to achieve high resilience against existing attacks by degrading the privacy information carried by the share gradients. Extensive experiments and ablation studies have demonstrated the effectiveness of the GIFD attack.
Our main contributions are summarized as follows:
• We propose GIFD for exploiting pre-trained generative models as data prior to invert gradients by searching the latent space and the intermediate features of the generator successively with l1 ball constraint.
• We show that this optimization method can be used to generate private OOD data with different styles, demonstrating the impressive generalization ability of the proposed GIFD under a more practical situation.
• We systematically evaluate our proposed method com-pared with the state-of-the-art baselines with the gra-dient transformation technique under four considered defense strategies. 2.