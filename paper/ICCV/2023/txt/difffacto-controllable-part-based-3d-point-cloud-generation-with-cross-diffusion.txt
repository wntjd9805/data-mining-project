Abstract
While the community of 3D point cloud generation has witnessed a big growth in recent years, there still lacks an effective way to enable intuitive user control in the gen-eration process, hence limiting the general utility of such methods. Since an intuitive way of decomposing a shape is through its parts, we propose to tackle the task of con-trollable part-based point cloud generation. We introduce
DiffFacto, a novel probabilistic generative model that learns the distribution of shapes with part-level control. We pro-pose a factorization that models independent part style and part configuration distributions, and present a novel cross diffusion network that enables us to generate coherent and plausible shapes under our proposed factorization. Experi-ments show that our method is able to generate novel shapes with multiple axes of control. It achieves state-of-the-art part-level generation quality and generates plausible and coherent shape while enabling various downstream edit-ing applications such as shape interpolation, mixing, and transformation editing. Please visit our project webpage at https://difffacto.github.io/ 1.

Introduction 3D shape generation [45] is an important and popular task , where point clouds are one popular representation [46, 5, 52, 28] – due to their simple yet powerful expressivity as well as data availability, i.e. just a set of points and can directly be acquired by sensors. However, the generation of arbitrary plausible shapes is often of limited utility, as users often have a conceptual design idea and of what they want to generate.
A prerequisite of shape generation is to be able to learn a space of all possible shapes. To represent this space, one parsimonious way is to represent them as a combination of simpler atoms, known as parts. In this flavor, we propose the task of controllable part-based generation, which aims to generate plausible novel shapes with user control over indi-vidual parts. As mentioned before, a shape is a combination of parts, thus a ‘novel shape’ can be defined in three different ways: (1) novel configurations of existing parts, (2) existing configurations of novel parts, and (3) novel configurations of novel parts.
The first is explored in existing graphics literature such as part retrieval [41] and shape assembly [50], while the second can be tackled by existing generation methods [46, 5, 52] trained on parts. In contrast, we tackle the third, which is the more challenging case subsuming the first and second. A challenge arises because a shape is a combination of novel parts, leading to an exponential explosion of plausible shapes while having only limited training data. A further challenge stems from enabling control as this requires an approach that can vary individual parts and configurations while still generating plausible shapes.
To this end, we introduce a new method that tackles this task in a principled way by building a probabilistic gen-erative model that learns the distribution of shapes while enabling control on parts and configurations. Specifically, 1
we propose a factorization that decomposes the shape space into (i) the individual canonicalized (semantic) parts, and (ii) their transformations (position and size). These factors can be sampled or encoded independently, allowing for different modes of control in generation and intuitive editing.
Our approach learns independent latent spaces for each canonicalized (semantic) part through part stylizers. Then conditioned on the canonicalized parts, we also introduce learn a transformation sampler that learns a distribution of part configurations. Naive approaches can result in mode collapse since multiple parameter configurations can output a valid shape, if conditioned only on canonicalized parts.
We leverage on a sampling-based approach to learn a multi-modal distribution of part configurations through conditional
Implicit Maximum Likelihood [24] (cIMLE).
To generate plausible shapes through independently sam-pled factors, we also introduce our cross diffusion network that allows for the learning of a better shape distribution un-der our proposed factorization. Our cross-attention diffusion network, conditions on the proposed factors, i.e. indepen-dent part style and transformations, in the reverse diffusion process. Our design allows each generated point in the point cloud to be informed of both the global shape as well as the local part, resulting in more plausible and coherent output shapes while still enabling control. Moreover, we also in-troduce a generalized forward diffusion kernel that allows the explicit encoding of each part transformations, enabling better shape reconstruction and transformation extrapolation.
We dub our method DiffFacto, for factorized represen-tion with cross diffusion. To our best knowledge, we are the first to introduce a factorized representation that allows for control in both part styles and part configurations as we model independent part style distributions and transforma-tion distribution, enabling each to be independently sampled.
Experiments show that our approach achieves better intra-part and inter-part level scores compared to baselines. We also show that our approach generates novel and coherent shapes through a segmentation-based plausibility experiment and human study. Furthermore, we demonstrate that our approach also allows for controllable and localized shape editing on various applications such as part-level shape in-terpolation, shape mixing and transformation editing. 2.