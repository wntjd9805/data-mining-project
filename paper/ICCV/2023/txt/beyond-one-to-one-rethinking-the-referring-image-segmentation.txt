Abstract
Referring image segmentation aims to segment the tar-get object referred by a natural language expression. How-ever, previous methods rely on the strong assumption that one sentence must describe one target in the image, which is often not the case in real-world applications. As a re-sult, such methods fail when the expressions refer to either no objects or multiple objects.
In this paper, we address this issue from two perspectives. First, we propose a Dual
Multi-Modal Interaction (DMMI) Network, which contains two decoder branches and enables information flow in two directions. In the text-to-image decoder, text embedding is utilized to query the visual feature and localize the corre-sponding target. Meanwhile, the image-to-text decoder is implemented to reconstruct the erased entity-phrase condi-tioned on the visual feature.
In this way, visual features are encouraged to contain the critical semantic informa-tion about target entity, which supports the accurate seg-mentation in the text-to-image decoder in turn. Secondly, we collect a new challenging but realistic dataset called
Ref-ZOM, which includes image-text pairs under different settings. Extensive experiments demonstrate our method achieves state-of-the-art performance on different datasets, and the Ref-ZOM-trained model performs well on various types of text inputs. Codes and datasets are available at https://github.com/toggle1995/RIS-DMMI. 1.

Introduction
Referring image segmentation aims to segment the tar-get object described by a given natural language expres-sion. Compared to the traditional semantic segmentation task [32, 41, 3], referring image segmentation is no longer restricted by the predefined classes and could segment spe-cific individuals selectively according to the description of
*Equal contribution.
†Corresponding author.
[7]
[48]
Figure 1: (a) Taking the autonomous driving as an example, text expression may refer to varying number of targets, de-pending on the specific real-world scenario. (b) When the sentences refer to multiple or no targets, existing methods cannot realize accurate segmentation. text, which has large potential value for various applications such as human-robot interaction [43] and image editing [2].
Despite the recent progress, there are still several important challenges that need to be addressed in order to make this technology more applicable in real-world scenarios.
In referring image segmentation, most previous meth-ods only concentrate on the one-to-one setting, where each sentence only indicates one target in the image. However, as shown in Fig. 1(a), one-to-many and one-to-zero set-tings, where the sentence indicates many or no targets in the image, respectively, are also common and critical in the real-world applications. Unfortunately, previous methods tend to struggle when confronting one-to-many and one-to-zero samples. As illustrated in Fig. 1(b), the recent SOTA method, LAVT [48], only localizes one person in the image when given the description “Three persons playing base-ball”. As for one-to-zero input, previous methods still seg-ment one target even if it is completely irrelevant to the given text. Therefore, it is imperative to enable the model to adapt to various types of text inputs.
We attribute this problem to two main factors. First, al-though existing methods design various ingenious modules to align multi-modal features, most of them only supervise the pixel matching of the segmentation map, which cannot ensure the significant semantic clues from the text are fully incorporated into the visual stream. As a result, visual fea-tures lack the comprehensive understanding of the entity be-ing referred to in the expression, which limits the capacity when the model confronts various types of text inputs. Sec-ond, all popular datasets [21, 38, 36] for referring image segmentation are established under the one-to-one assump-tion. In the training, the model is enforced to localize one entity that is most related to the text. As a result, the model trained on these datasets is prone to overfitting and only re-members to segment the object with the largest response, which leads to the failure when segmenting one-to-many and one-to-zero samples.
To address the aforementioned issues, this paper pro-poses a Dual Multi-Modal Interaction Network (DMMI) to achieve robust segmentation when given various types of text expressions, and establishes a new comprehensive dataset Ref-ZOM (Zero/One/Many).
In the DMMI net-work, we address the referring segmentation task in a dual manner, which not only incorporates the text information into visual features but also enables the information flow from visual stream to the linguistic one. As illustrated in
Fig. 2, the whole framework contains two decoder branches.
On the one hand, in the text-to-image decoder, linguistic information is involved into the visual features to segment the corresponding target. On the other hand, we randomly erase the entity-phrase in the original sentence and extract the incomplete linguistic feature. Then, in the image-to-text decoder, given the incomplete text embedding, we uti-lize the Context Clue Recovery (CCR) module to recon-struct the missing information conditioned on the visual fea-tures. Meanwhile, multi-modal contrastive learning is also deployed to assist the reconstruction. By doing so, the vi-sual feature is encouraged to fully incorporate the semantic clues about target entity, which promotes the multi-modal feature interaction and leads to more accurate segmentation maps. Additionally, to facilitate the two decoder parts, we design a Multi-scale Bi-direction Attention (MBA) module to align the multi-modal information in the encoder. Be-yond the interaction between single-pixel and single-word
[48], the MBA module enables the multi-modal interaction in the local region with various sizes, leading to a more comprehensive understanding of multi-modal features.
In the Ref-ZOM, we establish a comprehensive and chal-lenging dataset to promote the referring image segmentation when given various types of text inputs. On the one hand, compared to the existing widely-used datasets [21, 38, 36], the text expressions are more complex in Ref-ZOM. It is not limited to the one-to-one assumption, and instead, the expression can refer to multiple or no targets within the im-age. Additionally, the language style in our Ref-ZOM is much more flowery than the short phrases found in [21].
On the other hand, Ref-ZOM also surpasses most main-stream datasets in terms of size, containing 55078 images and 74942 annotated objects.
We conduct extensive experiments on three popular datasets [21, 38, 36] and our DMMI achieves state-of-the-art results. Meanwhile, we reproduce some representative methods on our newly established Ref-ZOM dataset, where
DMMI network consistently outperforms existing methods and exhibits strong ability in handling one-to-zero and one-to-many text inputs. Moreover, the Ref-ZOM-trained net-work performs remarkable generalization capacity when being transferred to different datasets without fine-tuning, highlighting its potential for real-world applications.
The main contributions of this paper are summarized as follows:
• We find the deficiency of referring image segmenta-tion when meeting the one-to-many and one-to-zero text inputs, which strongly limits the application value in real-world scenarios.
• We propose a Dual Multi-Modal Interaction (DMMI)
Network to enable the information flow in two direc-tions. Besides the generation of segmentation map,
DMMI utilizes the image-to-text decoder to recon-struct the erased entity-phrase, which facilitates the comprehensive understanding of the text expression.
• We collect a new challenging dataset, termed as Ref-ZOM, in which the text inputs are not limited to the one-to-one setting. The proposed dataset provides a new perspective and benchmark for future research.
• Extensive experimental results show the proposed
DMMI network achieves new state-of-the-art results on three popular benchmarks, and exhibits superior ca-pacity in handling various types of text inputs on the newly collected Ref-ZOM. 2.