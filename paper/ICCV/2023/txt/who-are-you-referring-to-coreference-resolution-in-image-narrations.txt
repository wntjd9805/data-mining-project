Abstract
Coreference resolution aims to identify words and phrases which refer to the same entity in a text, a core task in natural language processing.
In this paper, we extend this task to resolving coreferences in long-form narrations of visual scenes. First, we introduce a new dataset with annotated coreference chains and their bounding boxes, as most existing image-text datasets only contain short sen-tences without coreferring expressions or labeled chains.
We propose a new technique that learns to identify coref-erence chains using weak supervision, only from image-text pairs and a regularization using prior linguistic knowl-edge. Our model yields large performance gains over sev-eral strong baselines in resolving coreferences. We also show that coreference resolution helps improve grounding narratives in images. 1.

Introduction
Consider the image paired with the long-form descrip-tion in Figure 1, an example from the Localized Narra-tives [47]. Can you tell whether the woman who is wear-ing spectacles refers to a person or another woman in the text? We are remarkably good at identifying referring expressions (or mentions) and determining which of them corefer to the same entity, a task that we regularly perform when we read text or engage in conversations. The text-only version of this problem is known as coreference resolution (CR) [29, 30, 57], a core task in natural language process-ing (NLP) with a large literature. While solving text-only
CR requires a very good understanding of the syntactic and semantic properties of the language, the visual version of
CR shown in the example also demands an understanding of the visual scene. In our example, a language model has to figure out that a person can be a woman, has hands, and correctly match it with her [hand] and the woman, but not with another woman. However, a language model alone cannot answer whether the woman refers to a person or the woman. This can only be disambiguated after visually in-specting which of the two is wearing spectacles.
Figure 1: Coreference resolution from an image and narra-tion pair. Each highlighted block of text is referred to as a mention. The mentions in the same color corefer to the same entity, and belong to the same coreference chain.
Text-only CR has been a crucial component for a range of NLP applications including question answering [28, 14], sentiment analysis [6, 43], summarization [19, 54] and ma-chine translation [40, 4, 63]. Most text-only CR methods are either rule-based [29, 49] using heuristics such as pronoun match or exact match based on part of speech tagging, or are learned on large annotated text datasets from domains such as news text or Wikipedia articles [5, 31, 30, 21]. State-of-the-art methods [30, 21] fail to resolve coreferences cor-rectly in image narrations for a few reasons. First, CR in image narrations often require image understanding (see
Fig. 1). Neural networks trained on text datasets [48, 9] suffer from poor transferability and a significant perfor-mance drop when applied to image narrations because of domain shift.
Image narrations are unstructured and can be noisy, unlike the well-edited text used during training (such as news or Wikipedia). Moreover, standard image-text datasets [34, 26, 8, 46] only contain short descriptions with very few or no cases of coreference, thus, are not suit-able for training text-only CR models.
Some prior work have looked at visual CR for specific tasks. [50] and [53] link character mentions in TV shows or movie descriptions to character occurrences in videos.
More recently, the Whoâ€™s Waldo dataset [13] links person names in the caption to their occurrence in the image. How-ever, these methods rely on a limited set of object categories and referring expression types (see Table 3 discussed be-low), require annotated training data and therefore cannot
be applied to long-form unconstrained image narrations that include open-world object categories and multiple types of referring expressions such as pronouns (she), common nouns (another woman), or proper nouns (Peter).
In this paper, we look at the problem of CR in image nar-rations, i.e., resolving the coreference of mentions in narra-tive text that is paired with an image. As the prior bench-marks in this domain are limited to either a small vocab-ulary of objects or specific referring expression types, we introduce a new dataset, Coreferenced Image Narratives ,
CIN which augments the rich long-form narrations in the existing Localized Narratives dataset [47]. We add corefer-ence chain annotations and ground each chain by linking it to a bounding box in the corresponding image.
Manually annotating the whole dataset [47] is expen-sive, hence these annotations are provided only for evalu-ation and are not available for training. To cope with this setting, we propose a weakly supervised CR method that learns to predict coreference chains from only paired image-text data. Our key idea is to learn the linking of the men-tions to image regions in a joint multi-modal embedding space and use the links to form coreference chains during training. To this end, we propose a multimodal pipeline that represents each modality (image regions, text mentions and also mouse traces, additionally provided by [47]) with a modality-specific encoder and then exploit the cross-modal correlations between them to resolve coreference. Finally, inspired by the rule-based CR [29], we incorporate linguis-tic rules into our learning formulation in a principled way.
We report extensive experiments on CIN and demonstrate that our method brings significant improvements in CR and in weakly supervised narrative grounding, a form of disam-biguation that has been underexplored in visual grounding1.
To summarize our contributions, we introduce (1) the new task of resolving coreferences in multimodal long-form textual descriptions (narrations), (2) a new dataset, CIN , that enables the evaluation of coreference chains in text and the localization of bounding boxes in images, which is pro-vided with multiple baselines and detailed analysis for fu-ture work, (3) a new method that learns to resolve corefer-ences while jointly grounding them from weak supervision and exploiting linguistic knowledge, (4) a rigorous exper-imental evaluation showing significant improvement over the prior work not only in CR but also in weakly supervised grounding of complex phrases in narrative text. 2.