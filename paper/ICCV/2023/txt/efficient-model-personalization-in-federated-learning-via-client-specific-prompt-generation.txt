Abstract
Federated learning (FL) emerges as a decentralized learning framework which trains models from multiple dis-tributed clients without sharing their data to preserve pri-vacy. Recently, large-scale pre-trained models (e.g., Vision
Transformer) have shown a strong capability of deriving robust representations. However, the data heterogeneity among clients, the limited computation resources, and the communication bandwidth restrict the deployment of large-scale models in FL frameworks. To leverage robust repre-sentations from large-scale models while enabling efficient model personalization for heterogeneous clients, we pro-pose a novel personalized FL framework of client-specific
Prompt Generation (pFedPG), which learns to deploy a personalized prompt generator at the server for producing client-specific visual prompts that efficiently adapts frozen backbones to local data distributions. Our proposed frame-work jointly optimizes the stages of personalized prompt adaptation locally and personalized prompt generation globally. The former aims to train visual prompts that adapt foundation models to each client, while the latter ob-serves local optimization directions to generate personal-ized prompts for all clients. Through extensive experiments on benchmark datasets, we show that our pFedPG is favor-able against state-of-the-art personalized FL methods un-der various types of data heterogeneity, allowing computa-tion and communication efficient model personalization. 1.

Introduction
With access to web-scale training data (e.g., LAION-5B [43]), deep learning has demonstrated remarkable achievements across computer vision [19, 18, 41] and nat-ural language understanding [12, 54, 2]. However, in real-world scenarios, user data is typically scattered across vari-ous domains, such as hospital sites or edge devices. Due to increasing risks of privacy breaches and stricter privacy pro-tection regulations [9], centralized learning schemes are not
Figure 1. Comparison between (a) FedAvg and (b) our approach.
Instead of updating and transporting entire models θ, our FL method learns to generate personalized prompts P by implicitly observing local optimization directions ∆P = (cid:101)P − P for efficient model personalization on top of frozen foundation models. preferable. With the aim of collaboratively training mod-els without exposing users’ private data, Federated learn-ing (FL) has emerged as a prominent distributed learn-ing framework and has garnered growing research interest.
This privacy-preserving learning paradigm has been widely adopted in applications like medical image diagnosis [6], face recognition [31], and person re-identification [57].
Without the need of data sharing among clients, the mainstream FL approach of FedAvg [34] learns a global model by averaging model parameters trained on clients’ private data. However, data distributed in each client might be heterogeneous in terms of domain discrepancy [29] or imbalanced class distribution [26]. Sharing a global model across heterogeneous data clients is prone to highly deviate from their local distribution, leading to severe performance degradation [44, 33]. Previous FL works [28, 26] propose types of constraints (e.g., L2 [28] or contrastive regular-ization [26]) to prevent the local training to be divergent from each other. To better handle the inevitable data het-erogeneity across clients, personalized federated learning (pFL) methods [44, 33, 4, 52, 45] are instead proposed to allow each client to train a personalized model that adapts to their own data distribution. For example, pFedHN [44] introduces a hypernetwork at the server to directly gener-ate model parameters for each client, whereas pFedLA [33] learns a layer-wise model aggregation policy to assign dif-ferent weights for personalized model aggregation. While the above pFL approaches are desirable for handling het-erogeneous data, they are typically restricted to small back-bone architectures (e.g., LeNet [24]) due to the high com-plexity of outputting model parameters [44] or aggregation weights [33] for large-scale models. Consequently, the ca-pability of derived features is limited, leading to a lack of performance improvement and training instability.
Recently, training from large foundation models [1] for downstream tasks has become a prominent paradigm in cen-tralized learning. To leverage the strong representations derived by foundation models for alleviating data hetero-geneity, ViT-FL [40] incorporates pre-trained Vision Trans-former (ViT) [13] into standard FL algorithms (e.g., Fe-dAvg [34]) and shows improved robustness and stability on heterogeneously distributed data. However, the use of large pre-trained models for all clients in existing FL algo-rithms can cause extensive computational and communica-tion burdens, as these methods require transporting entire model parameters between clients and the server. Addition-ally, overfitting issues might occur when large-scale models are trained with relatively limited client data.
For efficiently tuning large-scale models, prompt learn-ing [21, 55, 56] provides a flexible way to adapt pre-trained models to downstream tasks by solely training the addi-tional inserted trainable parameters (i.e., prompts). For in-stance, VPT [21] treats prompts as task-specific parame-ters and prepends them to the input tokens of a pre-trained
ViT. In this way, prompts could be optimized to capture task-specific information while instructing a frozen model to perform tasks of interest. However, a straightforward way to adopt prompt learning into FL, i.e., simply averaging prompts learned from all clients, cannot address data het-erogeneity among clients effectively and often leads to un-satisfactory performance (as evident in Tables 1-3). There-fore, there is a crucial challenge to develop new FL methods that can leverage prompt learning effectively while handling data heterogeneity among clients.
In this paper, we aim at achieving efficient model per-sonalization among clients with data heterogeneity. As de-picted in Fig. 1, different from conventional FL methods (e.g., FedAvg [34]) that updates and transports entire model parameters, we propose a novel personalized FL scheme of client-specific Prompt Generation (pFedPG) that exploits underlying client-specific characteristics to produce person-alized prompts for each client, which enables efficient adap-tation to local data distribution. To be more precise, each client trains the client-specific prompts to instruct a model to perform recognition tasks on the target client using its private data. As the local training is not required to up-date entire large models, the computation overload could be minimized while the possible overfitting issues are mit-igated accordingly. On the other hand, we employ a per-sonalized prompt generation module on the server side, which is learned to obtain the underlying optimization di-rections among clients. With such client characteristics im-plicitly observed, we are capable of producing personalized prompts to facilitate efficient adaptation for each client with heterogeneous data distribution. By iteratively training the above two stages in a mutually beneficial manner, we are capable of achieving effective yet efficient model person-alization on top of the robust representations derived from large-scale foundation models.
We now summarize the contributions of this work below:
• We propose a personalized FL framework of client-specific Prompt Generation (pFedPG), which alter-nates between personalized prompt generation and personalized prompt adaptation to enable efficient model personalization under heterogeneous data.
• We design a client-specific prompt generator at the server, which effectively exploits personalized optimization directions and produces client-specific prompts for updating each client model.
• Evaluations on several benchmark datasets in domain discrepancy and imbalanced class distribution verify that our method performs favorably against existing personalized FL approaches and exhibits sufficient training efficiency. 2.