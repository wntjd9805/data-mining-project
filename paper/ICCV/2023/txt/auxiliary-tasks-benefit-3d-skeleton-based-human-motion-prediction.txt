Abstract
Exploring spatial-temporal dependencies from observed motions is one of the core challenges of human motion pre-diction. Previous methods mainly focus on dedicated net-work structures to model the spatial and temporal dependen-cies. This paper considers a new direction by introducing a model learning framework with auxiliary tasks. In our auxiliary tasks, partial body joints’ coordinates are cor-rupted by either masking or adding noise and the goal is to recover corrupted coordinates depending on the rest coor-dinates. To work with auxiliary tasks, we propose a novel auxiliary-adapted transformer, which can handle incomplete, corrupted motion data and achieve coordinate recovery via capturing spatial-temporal dependencies. Through auxil-iary tasks, the auxiliary-adapted transformer is promoted to capture more comprehensive spatial-temporal dependencies among body joints’ coordinates, leading to better feature learning. Extensive experimental results have shown that our method outperforms state-of-the-art methods by remark-able margins of 7.2%, 3.7%, and 9.4% in terms of 3D mean per joint position error (MPJPE) on the Human3.6M, CMU
Mocap, and 3DPW datasets, respectively. We also demon-strate that our method is more robust under data missing cases and noisy data cases. Code is available at https:
//github.com/MediaBrain-SJTU/AuxFormer. 1.

Introduction 3D skeleton-based human motion prediction aims to fore-cast future human motions based on past observations, which has a wide range of applications, such as human-machine interaction [23, 21, 62] and autonomous driving [34, 9, 38].
One of the main challenges of this problem is extracting spatial-temporal dependencies among observed motions to make feature representations more informative. These depen-dencies arise due to the complex interactions between differ-ent joints and the temporal dynamics of motion. Therefore, developing effective methods to capture these dependencies
*Corresponding author.
Figure 1: Compared to previous methods with various operation designs, we propose a new direction: an auxiliary learning frame-work that incorporates auxiliary tasks, including denoising and masking prediction. These auxiliary tasks impose additional re-quirements through recovery, forcing the network to exploit spatial-temporal dependencies more comprehensively. is crucial for a more accurate human motion prediction.
Existing methods proposed dedicated network structures to model the spatial and temporal dependencies. A few meth-ods use RNN [48, 22] and TCN [11] structures to model temporal dependencies, but neglect the spatial ones. To learn spatial dependencies between joints, [47] proposes a GCN network with learnable weights where nodes are body joints.
Following the GCN design, DMGNN [37] and MSR-GCN
[12] further build multiscale body graphs to model local-global spatial features. PGBIG [43] additionally proposes temporal graph convolutions to extract spatial-temporal fea-tures. SPGSN [36] proposes a graph scattering network to further model temporal dependencies from multiple graph spectrum bands. Besides GCN-based methods, transformer architectures [1, 5] are also used to model pair-wise spatial-temporal dependencies. The structures of models to capture spatial-temporal dependencies have been extensively stud-ied. This raises a natural question: can we enhance spatial-temporal dependency modeling from other perspectives?
In this paper, besides network structures, we further con-sider an orthogonal approach: proposing a new auxiliary model learning framework by adding auxiliary tasks to pro-mote better learning of spatial-temporal dependencies. The proposed framework jointly learns the primary motion pre-1
diction task along with additional auxiliary tasks, with shar-ing the same dependency modeling network. The core idea of the auxiliary tasks is to corrupt partial observed coordi-nates and set a goal to recover corrupted coordinates us-ing correlated normal coordinates according to their spatial-temporal dependencies. The goals of the auxiliary tasks are highly correlated with the primary prediction task, as they both require the network to model spatial-temporal de-pendencies effectively. Therefore, through the additional requirements imposed by the auxiliary tasks, the depen-dency modeling network is forced to learn more effective and comprehensive spatial-temporal dependencies. Our learn-ing framework complements existing methods by further emphasizing the effective learning of the network structure.
To be specific, we introduce two kinds of auxiliary tasks: a denoising task and a masked feature prediction task. The denoising task randomly adds noise into joint coordinates at different timestamps in the input motion, and the aim is to recover the original input motion. The masked feature prediction task randomly masks joint coordinates at differ-ent timestamps, and the goal is to predict the masked joint positions. Comparing to previous popular methods based on masking/denoising autoencoding like masked autoencoder (MAE) [25] and denoising autoencoder (DAE) [56], which are mainly designed for model pre-training, we treat the de-noising and masking prediction as auxiliary tasks to aid the primary fully-supervised motion prediction task and jointly learn all the tasks together. Moreover, previous methods us-ing masking/denoising strategies mostly focus on data types of images [25, 45], videos [16, 53], languages [14, 50, 67] and point clouds [49, 59], but rarely on motion sequences, es-pecially human motions. Our work fills this gap and utilizes the strategies to promote more effective spatial-temporal dependencies learning in human motion prediction.
To cooperate with auxiliary tasks in the learning frame-work, the dependency modeling network structure faces two demands. First, the network has to learn spatial-temporal dependencies between the corrupted coordinate and the nor-mal coordinate on a coordinate-wise basis to enable recovery.
Second, the network has to be adaptive to incomplete mo-tions, caused by the masking prediction task. Thus, we specifically propose an auxiliary-adapted transformer net-work to meet both two demands. To model the coordinate-wise dependency, we consider each coordinate as one individ-ual feature in our network and use spatial-temporal attention that models spatial-temporal dependencies between two coor-dinates’ features. To be adaptive to incomplete data, we add tokens into the masked coordinates’ feature to inform the net-work that the data is missing, and incorporate a mask-aware design into spatial-temporal attention that enables arbitrary incomplete inputs. Integrating the above learning framework and network design, we name our method AuxFormer.
We conduct experiments on both short-term and long-term motion prediction on large-scale datasets: Human3.6M
[28], CMUMocap and 3DPW [57]. Our method significantly outperforms state-of-the-art methods in terms of mean per joint position error (MPJPE). We also show our method is more robust under data missing and noisy cases. The main contributions of our work are summarized here:
• We propose a new auxiliary learning framework for human motion prediction to jointly learn the prediction task with two auxiliary tasks: denoising and masking prediction.
Through auxiliary tasks, the model network is forced to learn more comprehensive spatial-temporal dependencies.
• We propose an auxiliary-adapted transformer to coop-erate with the learning framework. The auxiliary-adapted transformer models coordinate-wise spatial-temporal depen-dencies and is adaptive to incomplete motion data.
• We conduct experiments to verify that our method sig-nificantly outperforms existing works by 7.2%/3.7%/9.4%.
We also show our method is more robust under data missing cases and noisy data cases. 2.