Abstract
Real-world data tends to follow a long-tailed distribu-tion, where the class imbalance results in dominance of the head classes during training. In this paper, we propose a frustratingly simple but effective step-wise learning frame-work to gradually enhance the capability of the model in detecting all categories of long-tailed datasets. Specifically, we build smooth-tail data where the long-tailed distribu-tion of categories decays smoothly to correct the bias to-wards head classes. We pre-train a model on the whole long-tailed data to preserve discriminability between all cat-egories. We then fine-tune the class-agnostic modules of the pre-trained model on the head class dominant replay data to get a head class expert model with improved de-cision boundaries from all categories. Finally, we train a unified model on the tail class dominant replay data while transferring knowledge from the head class expert model to ensure accurate detection of all categories. Extensive experiments on long-tailed datasets LVIS v0.5 and LVIS v1.0 demonstrate the superior performance of our method, where we can improve the AP with ResNet-50 backbone from 27.0% to 30.3% AP, and especially for the rare categories from 15.5% to 24.9% AP. Our best model using ResNet-101 backbone can achieve 30.7% AP, which suppresses all exist-ing detectors using the same backbone. Our source code is available at https://github.com/dongnana777/
Long-tailed-object-detection. 1.

Introduction
The success of deep learning are seen in many computer vision tasks including object detection. Many deep learning-based approaches [5, 29, 4, 17, 23, 20, 18, 1, 39] are pro-posed and have shown impressive performance in localizing and classifying objects of interest in 2D images. However, it is important for these deep learning-based approaches to be trained on balanced and representative datasets. Un-*Work fully done while first author is a visiting PhD student at the
National University of Singapore.
Figure 1. LST [10] is more susceptible to catastrophic forgetting due to their incremental learning scheme with numerous data splits.
We alleviate the problem by building smooth-tail data that flattens long-tailed datasets and always maintains data from all categories. fortunately, most real-world datasets always follow a long-tailed distribution, where the head classes have a significantly larger number of instances than the tail classes. Training on such imbalanced datasets often leads to bias towards head classes and significant performance degeneration of the tail classes due to the extremely scarce samples.
To circumvent the long-tailed distribution problem of ob-ject detection task, many attempts exploit data re-sampling and loss re-weighting approaches. Data re-sampling meth-ods [6, 31] re-balance the distribution of the instance num-bers of each category. Loss re-weighting methods [28, 30, 15] adopt different re-weighting strategies to adjust the loss of different categories based on each categoryâ€™s statistics. As shown in Figure 2, Hu et al. [10] proposes LST which is a
"divide & conquer" strategy that leverages class-incremental few-shot learning to solve the long-tailed distribution prob-lem. The model is first trained with abundant labeled data of the head classes. The categories in the long-tailed training data is then sorted and divided according to the number of samples to get the corresponding subsets for incremental learning and merging of each part in N phases.
Despite the innovative adoption of class-incremental few-shot learning on the long-tailed distribution problem, we find that [10] catastrophically forgets the knowledge of the head classes and cannot sufficiently learn the tail classes in their incremental learning process. We postulate that this
the head class dominant and tail class dominant data. Sub-sequently, we fine-tune the pre-trained model on the head class dominant data to learn a head class expert model. Fi-nally, we learn a unified model on the tail class dominant data while preserving knowledge of the head classes with the head class expert model. Knowledge distillation at feature level with a head class focused mask is adopt to facilitate the learning of tail classes from the head class expert model. In addition, knowledge distillation at classification head is also adopted, where object query features from the head class expert model are shared to the unified model to align the predictions between them.
Our contributions can be summarized as follows: 1. We propose to build smooth-tail data, i.e., a head class dominant data and a tail class dominant data, to alleviate the extreme class imbalance of long-tail data and prevent catastrophic forgetting in our step-wise learning frame-work. 2. We design a novel step-wise learning framework that unifies fine-tuning and knowledge transfer for the long-tailed object detection task. 3. Our framework is frustratingly simple but effective.
We achieve state-of-the-art performances on long-tailed datasets LVIS v0.5 and LVIS v1.0 in both the overall accuracy, and especially the impressive accuracy of the rare categories. 2.