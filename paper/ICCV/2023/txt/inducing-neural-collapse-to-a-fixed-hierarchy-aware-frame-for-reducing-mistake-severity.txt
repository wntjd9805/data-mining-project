Abstract
There is a recently discovered and intriguing phe-nomenon called Neural Collapse: at the terminal phase of training a deep neural network for classification, the within-class penultimate feature means and the associated classifier vectors of all flat classes collapse to the vertices of a simplex Equiangular Tight Frame (ETF). Recent work has tried to exploit this phenomenon by fixing the related classifier weights to a pre-computed ETF to induce neural collapse and maximize the separation of the learned fea-In this work, tures when training with imbalanced data. we propose to fix the linear classifier of a deep neural net-work to a Hierarchy-Aware Frame (HAFrame), instead of an ETF, and use a cosine similarity-based auxiliary loss to learn hierarchy-aware penultimate features that collapse to the HAFrame. We demonstrate that our approach re-duces the mistake severity of the model’s predictions while maintaining its top-1 accuracy on several datasets of vary-ing scales with hierarchies of heights ranging from 3 to 12.
Code: https://github.com/ltong1130ztr/HAFrame. 1.

Introduction
A recent study [32] has unveiled a phenomenon termed neural collapse. It empirically revealed that the penultimate features of the same class tend to collapse to their within-class mean. The within-class means of all classes and their respective classifier weights tend to collapse to the vertices of a simplex Equiangular Tight Frame (ETF). A simplex
ETF is a geometric structure that maximally separates the pair-wise angles of the K vectors in Rd, d ≥ K, and the respective maximal pair-wise cosine similarity of these K vectors is −1
K−1 . As illustrated in Fig. 1(a), when K = 4, the simplex ETF reduces to a tetrahedron, and we can visualize this tetrahedron in 3D space via PCA projection since its geometry is 3D. One can view such ETF as an embedding of a hierarchy of four classes sharing the same root node (a) (b) (c) (d)
Figure 1. Illustration of a hierarchy-agnostic ETF (a) and a hierarchy-aware HAFrame (b) of four leaf classes and their hi-erarchies (c) and (d), respectively. All leaf classes in ETF have the same hierarchical distance. as their parent. This hierarchy is visualized in Fig. 1(c), where the four classes are equally separated regarding their hierarchical distance from each other.
Intuitively, the neural collapse phenomenon makes sense considering an ETF separates all classes equally and maxi-mally from each other. However, such a structure may not emerge when trained with imbalanced data. Features of mi-nor classes may collapse to the same vector (minority col-lapse) [13]. Therefore, some studies encourage the features to form an ETF structure by fixing the classifier weights at a pre-computed ETF [42] or employing additional regu-larizers to induce neural collapse [45, 29]. In the context of reducing mistake severity, this raises another concern: when the ETF classifier makes a mistake, it is mainly ran-dom due to its equiangular nature. Similarly, conventional neural networks are trained mainly with cross-entropy and one-hot labels, ignoring any underlying hierarchical label relationships. The associated performance evaluations also
focus on the top-1 accuracy of the predictions, treating all mistakes equally.
In real-world application scenarios, some classification mistakes would have a much worse im-pact than others, e.g., mistaking a human for a tree in au-tonomous driving. Hence, it is critical to incorporate mis-take severity into the performance evaluations and develop methods to reduce the mistake severity of the model pre-dictions. One off-the-shelf way to define the severity of mistakes is by leveraging the hierarchical label relationship between incorrect predictions and their ground truths.
To impose a preferred error structure, we propose to fix the classifier vectors to a Hierarchy-Aware Frame (HAFrame) instead of an ETF to ensure that the classifier vectors of certain classes are “closer” than others. Con-sequently, when a mistake occurs, it is more likely to fall onto a “closer” class in the HAFrame, resulting in less mistake severity. An example of such a HAFrame is shown in Fig. 1(b). Compared to the ETF for four classes, the
HAFrame captures the pair-wise hierarchical distances across the four classes from a hierarchy shown in Fig. 1(d), with class A closer to B, and A equally distant to C and D.
There are several contributions of our work:
• Our approach is easy to adapt to different hierarchies as we only require a minimal change of the classifier and no architectural change of the backbone network.
• Our approach provides an analytical solution to embed the hierarchical relationship of classes into the respec-tive classifier.
• Our approach offers a new route to reduce mistake severity and the average hierarchical distance of pre-dictions from the perspective of neural collapse. 2.