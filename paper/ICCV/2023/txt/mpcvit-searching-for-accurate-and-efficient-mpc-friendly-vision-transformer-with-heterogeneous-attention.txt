Abstract
Secure multi-party computation (MPC) enables compu-tation directly on encrypted data and protects both data and model privacy in deep learning inference. However, existing neural network architectures, including Vision Transform-ers (ViTs), are not designed or optimized for MPC and incur significant latency overhead. We observe Softmax accounts for the major latency bottleneck due to a high communi-cation complexity, but can be selectively replaced or lin-earized without compromising the model accuracy. Hence, in this paper, we propose an MPC-friendly ViT, dubbed
MPCViT, to enable accurate yet efficient ViT inference in
MPC. Based on a systematic latency and accuracy evalua-tion of the Softmax attention and other attention variants, we propose a heterogeneous attention optimization space.
We also develop a simple yet effective MPC-aware neu-ral architecture search algorithm for fast Pareto optimiza-tion. To further boost the inference efficiency, we propose
MPCViT+, to jointly optimize the Softmax attention and other network components, including GeLU, matrix multi-plication, etc. With extensive experiments, we demonstrate that MPCViT achieves 1.9%, 1.3% and 3.6% higher ac-curacy with 6.2×, 2.9× and 1.9× latency reduction com-pared with baseline ViT, MPCFormer and THE-X on the
Tiny-ImageNet dataset, respectively. MPCViT+ further achieves a better Pareto front compared with MPCViT. The code and models for evaluation are available at https:
//github.com/PKU-SEC-Lab/mpcvit. 1.

Introduction
As machine learning models are handling increasingly sensitive data and tasks, privacy has become one of the ma-jor concerns during the model deployment. Secure multi-party computation (MPC) [14] can protect the privacy of
*Corresponding author.
Figure 1. The latency breakdown of a Transformer block w.r.t (a)
# heads and (b) # tokens; and (c) the relative error of Softmax and
ReLU Softmax with different input variances. both data and deep neural network (DNN) models and has gained a lot of attention in recent years [42, 51, 60].
However, existing DNN architectures, especially the re-cently proposed Vision Transformers (ViTs) [2, 11, 64], are not designed or optimized for MPC (the high-level private
ViT inference framework in MPC is shown in Figure 2 and
Appendix A.1). Although ViTs have achieved superior per-formance for various vision tasks [2, 15, 16, 64], they face several realistic limitations when directly deployed in MPC: 1) communication overhead: in contrast to regular infer-ence on plaintext, operations like Softmax, GeLU, max, etc, require intensive communication in MPC, which usually dominates the total inference latency [42, 60]. For exam-ple, Softmax is usually very lightweight in plaintext infer-ence. However, as shown in Figure 1(a) and Figure 1(b), it accounts for the majority of the Transformer inference latency due to the high communication complexity; 2) ap-proximation error: operations like exponential, tanh, re-ciprocal, etc, cannot be computed directly and require itera-tive approximation, limiting the computation accuracy. For instance, as shown in Figure 1(c), the relative error of Soft-max significantly increases when the input variance is large due to its narrow dynamic range [60]. In contrast, replac-ing exponential in Softmax with ReLU reduces the relative error drastically (denoted as ReLU Softmax in §2.2).
Numerous efficient Transformer variants have been pro-Figure 2. An overall illustration of private Vision Transformer inference in the MPC framework. posed in recent years [5, 29, 49, 58]. Linear Transformers, including Linformer [58], cosFormer [49], Reformer [29], etc, reduce the quadratic computation complexity of the attentions and significantly accelerate model inference for long sequences. However, these Transformer optimizations have primarily focused on reducing the network computa-tion. Hence, they either retain complex non-linear functions or still require iterative approximation, both resulting in in-tensive communication overhead in MPC. Recently, MPC-Former [33] and THE-X [6] are proposed to improve the inference efficiency of BERT [28] in MPC. MPCFormer simplifies Softmax by replacing exponential with a more
MPC-friendly quadratic operation while THE-X approxi-mates Softmax with a small estimation network. However, their attention variants is not flexible and can still be too expensive for certain latency constraints. Moreover, the network-level optimization suffers from a large accuracy degradation when directly applied to vision tasks.
In this work, we first breakdown Softmax into more atomic operations and analyze their impact on the inference accuracy and efficiency, based on which we propose and compare a set of attention variants comprehensively. The comparison enables us to find MPC-friendly attention vari-ants, which are either highly accurate or highly efficient.
We further observe not all attentions are equally impor-tant within a ViT. Based on these observations, we propose the first MPC-friendly ViT architecture, dubbed MPCViT.
MPCViT features a heterogeneous attention design space to explore the trade-off between accuracy and efficiency, and an MPC-aware differentiable neural architecture search (NAS) algorithm for effective Pareto optimization. Our contributions can be summarized as follows:
• We breakdown Softmax and reveal the impact of atomic operations on the inference accuracy and effi-ciency, based on which a heterogeneous attention de-sign space is proposed.
• We propose an effective MPC-aware differentiable
NAS algorithm to explore the attention design space based on real latency measurements instead of prox-ies. To the best of our knowledge, we make the first attempt to introduce the MPC-aware NAS to optimize
ViT inference in MPC.
• Our MPC-friendly ViT model family, MPCViT, out-performs prior-art models in MPC in terms of both accuracy and efficiency. MPCViT achieves 1.9% and 1.3% higher accuracy with 6.2× and 2.9× latency re-duction compared with baseline ViT and MPCFormer on Tiny-ImageNet dataset, respectively.
• We further extend MPCViT to optimize other Trans-former components simultaneously with Softmax, named MPCViT+, achieving an even better Pareto front compared with MPCViT. 2. Preliminaries and