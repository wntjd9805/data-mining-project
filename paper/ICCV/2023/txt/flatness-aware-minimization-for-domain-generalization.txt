Abstract
Domain generalization (DG) seeks to learn robust mod-els that generalize well under unknown distribution shifts.
As a critical aspect of DG, optimizer selection has not been explored in depth. Currently, most DG methods follow the widely used benchmark, DomainBed, and utilize Adam as the default optimizer for all datasets. However, we re-veal that Adam is not necessarily the optimal choice for the majority of current DG methods and datasets. Based on the perspective of loss landscape flatness, we propose a novel approach, Flatness-Aware Minimization for Do-main Generalization (FAD), which can efficiently optimize both zeroth-order and first-order flatness simultaneously for
DG. We provide theoretical analyses of the FAD’s out-of-distribution (OOD) generalization error and convergence.
Our experimental results demonstrate the superiority of
FAD on various DG datasets. 1.

Introduction
Current neural networks are expected to generalize to unseen distributions in real-world applications, which can break the independent and identically distributional (I.I.D.) assumption of traditional machine learning algorithms [129, 99]. The distribution shift between training and test data may largely deteriorate most current approaches in practice.
Hence, instead of generalization within the training dis-tribution, the ability to generalize under distribution shift, namely domain generalization (DG) [73], has attracted in-creasing attention [99].
Various methods have been proposed to address the DG problem recently, many of which have shown promising performance. As a learning problem mainly in the com-puter vision field, the DG task relies highly on the chosen optimizer, yet the effect of the optimizer in DG has hardly been studied. Most current algorithms, following the well-known benchmark DomainBed [32], default to optimize
*Corresponding author
Figure 1. Test accuracy on PACS and DomainNet of different opti-mizers. Adam, the default optimizer in popular benchmarks shows no advantage compared with its counterparts, while the proposed
FAD achieve better OOD generalization performance compared with current optimizers. models with Adam [48] without considering other optimiz-ers. However, the in-distribution generalization ability of popular optimizers has been investigated from the perspec-tive of loss landscape recently [28, 29] and Adam is found to achieve inferior generalization compared with other op-timizers such as SGD [76] due to the sharp minima Adam selected. Thus, investigating the impact of optimizers in
DG and clarifying whether Adam should be considered as the default optimizer is of significance.
In this paper, we empirically compare the performance of current optimizers, including Adam, SGD, AdaBelief
[131], YOGI [117], AdaHessian [116], SAM [28], and
GAM[121]. Through extensive experiments on current DG benchmarks, we show that Adam can hardly surpass other optimizers on most DG datasets. The best choice of the optimizer for different DG methods varies across different datasets, and choosing the right optimizer can help exceed the current leaderboards’ limitations.
Recently, the connection between the flatness of the loss landscape and in-distribution generalization ability has been widely studied and verified both theoretically[130] and empirically[25]. Some works [13, 87] show that flat-ness also leads to superior OOD generalization. However, none of the previous works consider the optimization in
DG nor provide theoretical assurance of their approach. 1
Among flatness-aware optimization methods, sharpness-Aware Minimization (SAM) [28] and its variants [130, 53, 126, 47], which optimize the zeroth-order flatness, achieve
SOTA performance on various in-distribution image clas-sification tasks [53, 130]. Most recently, Gradient Norm
Aware Minimization (GAM) [121] shows that minimizing the first-order flatness provides a more substantial penalty on the sharpness of minima than zeroth-order flatness yet requires more computation overhead. Due to the first-order Taylor expansions adopted to optimize zeroth-order and first-order flatness, both SAM and GAM lose some of their penalty strength, and thus they can be mutually rein-forced by each other. To accelerate the optimization of first-order flatness and combine zero-order flatness, we propose a unified optimization framework, Flatness-Aware mini-mization for Domain generalization (FAD), which elimi-nates the considerable computation overhead for Hessian or Hessian-vector products. We theoretically show that the proposed FAD controls the dominant eigenvalue of Hessian of the training loss, which indicates the sharpness of the loss landscape along its most critical direction [42]. We present an OOD generalization bound to show that the proposed
FAD guarantees the generalization error on test data and convergence analysis. Through extensive experiments, we evaluate FAD and other optimizers on various DG bench-marks.
We summarize the main contribution as follows.
• We propose a unified optimization framework,
Flatness-Aware minimization for Domain generaliza-tion (FAD), to optimize zeroth-order and first-order flatness simultaneously. Without calculating Hessian and Hessian-vector products, FAD considerably re-duces the computation overhead of first-order flatness.
• We theoretically analyze the OOD generalization error and convergence of FAD. We show that FAD controls the dominant eigenvalue of Hessian and thus the flat-ness of learned minima.
• We empirically show that Adam, the default optimizer for most current DG benchmarks, can hardly be the optimal choice for most DG datasets. We present the superiority of FAD compared with current optimizers through extensive experiments.
• We empirically validate that FAD finds flatter optima with lower Hessian spectra compared with zeroth- and first-order flatness-aware optimization methods. 2.