Abstract
Training effective embodied AI agents often involves ex-pert imitation, specialized components such as maps, or leveraging additional sensors for depth and localization.
Another approach is to use neural architectures alongside self-supervised objectives which encourage better represen-tation learning. However, in practice, there are few guar-antees that these self-supervised objectives encode task-relevant information. We propose the Scene Graph Con-trastive (SGC) loss, which uses scene graphs as training-only supervisory signals. The SGC loss does away with explicit graph decoding and instead uses contrastive learn-ing to align an agent’s representation with a rich graphi-cal encoding of its environment. The SGC loss is simple to implement and encourages representations that encode ob-jects’ semantics, relationships, and history. By using the
SGC loss, we attain gains on three embodied tasks: Ob-ject Navigation, Multi-Object Navigation, and Arm Point
Navigation. Finally, we present studies and analyses which demonstrate the ability of our trained representation to en-code semantic cues about the environment. 1.

Introduction.
Researchers have pursued designing embodied agents with general neural architectures and training them via end-to-end reinforcement learning (RL) to flexibly complete
In practice, however, training a range of complex tasks. agents to perform long horizon tasks using only terminal re-wards has been ineffective and inefficient [37], particularly in complex visual environments with high-dimensional sen-sor inputs and large action spaces. This has led to the use of several common “tricks” to improve training, e.g. manu-ally engineered shaped rewards, use of off-the-shelf vision models to pre-process images, imitation learning with ex-pert trajectories, and the use of special purpose mapping architectures [9, 10, 69, 37, 22, 56, 40].
Looking to reduce the need for such “tricks”, one promising line of work has looked into training agents with RL and auxiliary losses to encourage the produc-tion of powerful and useful environment representations.
These include self-supervised losses like forward predic-tion [31], contrastive predictive coding [32], and inverse dynamics [55], that are task and environment independent.
The cost of this generality is that there are no guarantees that the resulting representations will encode task-relevant features like the semantic grounding of objects, which is often key to agent success. Moreover, in practice, these losses tend to work well in video game and gridworld en-vironments but are not effective in more complex visual worlds. On the other hand, supervised auxiliary losses such as disturbance avoidance [53], depth generation [52] are fre-quently designed to help with specific tasks but are not gen-erally useful for new tasks. 1
In this work, we propose the Scene Graph Contrastive (SGC) loss. SGC uses, as its supervisory signal, a non-parametric scene graph that develops and transforms itera-tively as the agent interacts with its environment. The agent, objects, and rooms are represented as nodes, agent-object (e.g. Sees and Touches) and object-object (e.g. Contains and
Above) relationships are edges and, category and spatial co-ordinates are represented as node attributes. SGC does not employ any graph decoders which tend to be complex, chal-lenging, and expensive to train [20, 49]. Instead, it uses a contrastive learning approach in which the agent must “pick out” the graph corresponding to the observations it has seen – a much simpler learning mechanism which still encour-ages the agent to develop a graph-aware belief state, see Fig-ure 1. Additionally, it prevents the need for a scene graph during evaluation. As we iteratively build the ground truth scene graph in an episode, we naturally generate hard neg-ative samples as graphs from nearby spatio-temporal states will only be subtly different from one another.
The SGC loss has several desirable characteristics.
Firstly, it encourages the belief representations to summa-rize object semantics, relationships, and history, informa-tion that can be intuitively useful for completing navigation-based embodied tasks. We present results that demon-strate the efficacy of these representations on multiple tasks.
Second, it requires the scene graph only at training time, which allows us to use any available supervisory data useful for learning powerful representations for embodied tasks.
Third, it is simple to implement. SGC does not require designing complex specialized decoders for predicting the scene graph and instead leverages well-studied, graph en-coder networks and contrastive losses.
We evaluate the SGC loss by training agents on three complex navigation-based tasks. These tasks include Ob-ject Navigation (ObjectNav) [18] (evaluated across four benchmarks), Multi-Object Navigation (MultiON) [68], and Arm Point Navigation (ArmPointNav) [23]. Across each of these tasks, SGC provides very significant absolute improvements of 10% in ObjectNav, 9% in MultiON and 3.5% in ArmPointNav over models trained with pure RL.
We find that these agents learn to represent many seman-tic cues about their environment, which we show via two studies. First, we demonstrate that SGC-trained agents can be quickly fine-tuned to novel goal object categories that were observed previously in their environments but never used as target objects for the task. Second, we present lin-ear probing experiments to study how the SGC loss impacts agents’ understanding of free-space and object semantics.
We find that the representations learned by the SGC-trained agent outperform those of an RL-only trained agent; sug-gesting that the SGC loss encourages agents to better repre-sent these concepts. use scene graph as a supervisory signal for training embod-ied agents, (2) the formulation of a Scene Graph Contrastive (SGC) loss which avoids the need to use complex graph de-coders, and (3) a suite of experimental results which demon-strate that the SGC loss leads to significant performance and sample efficiency gains across multiple embodied tasks. 2.