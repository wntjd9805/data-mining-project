Abstract
Creating relightable and animatable human characters from monocular video at a low cost is a critical task for dig-ital human modeling and virtual reality applications. This task is complex due to intricate articulation motion, a wide range of ambient lighting conditions, and pose-dependent clothing deformations. In this paper, we introduce a novel self-supervised framework that takes a monocular video of a moving human as input and generates a 3D neural rep-resentation capable of being rendered with novel poses un-der arbitrary lighting conditions. Our framework decom-poses dynamic humans under varying illumination into neu-ral fields in canonical space, taking into account geometry and spatially varying BRDF material properties. Addition-ally, we introduce pose-driven deformation fields, enabling bidirectional mapping between canonical space and obser-vation. Leveraging the proposed appearance decomposi-tion and deformation fields, our framework learns in a self-supervised manner. Ultimately, based on pose-driven defor-mation, recovered appearance, and physically-based ren-dering, the reconstructed human figure becomes relightable and can be explicitly driven by novel poses. We demonstrate significant performance improvements over previous works and provide compelling examples of relighting from monoc-ular videos of moving humans in challenging, uncontrolled capture scenarios. 1.

Introduction
Capturing the human appearance under varying poses, viewpoints, and environmental lighting is essential. This capability enables a range of applications from digital 3D human creation to immersive experiences for X-R expe-riences. Traditional pipelines, utilizing specialized equip-Figure 1. Given a monocular human motion video as input, our framework enables human relighting under novel illuminations and poses. ment for multi-view human scanning [12, 10, 16], rely on expensive hardware and are not feasible in uncontrolled en-vironments, making them unsuitable for individual users.
In contrast, recent neural rendering methods such as NeRF
[27] and its variants [51, 49, 14, 48, 28, 24, 24] have achieved significant progress in generating realistic human rendering effects. These methods are simple yet effective, offering a promising alternative to traditional pipelines. It has been demonstrated that the human body can be rep-resented as neural fields, enabling control and relighting, which expands the potential for creating more flexible and adaptive virtual human models.
Different from existing methods that rely on multi-view static scans [21, 53, 8], we focus on the problem of relight-ing dynamic humans using only a single monocular video.
As illustrated in Fig. 1, we model the dynamic human body by employing a neural human appearance field and a pose-driven deformation field. The former encodes the dynamic human, accounting for varying illumination, into a canon-ical volume, while the latter allows explicit control of the canonical model using a condition code (such as SMPL or
SMPL-X [23, 32]). Following the paradigm of recent work
[5], we utilize a set of multi-layer perceptron (MLP) net-works as an implicit representation to store the geometry and spatially varying BRDF (Bidirectional Reflectance Dis-tribution Function) of the human body within the canoni-cal volume. Geometry represents the human shape, encom-passing characteristics like density, color, and normal. The spatially varying BRDF is broken down into three compo-nents: base color, roughness, and metalness. In summary, our work has the following contributions:
We present a principled framework, which is the first to build a relightable and animatable human in complex mo-tion from a single video. we introduce a dense bidirectional mapping to modify rotation-related and rotation-unrelated parameters based on the deformation process.
We propose a progressive training strategy that enables learning BRDFs, surface normal, and ambient lighting in a self-supervised way under complex motions. Experiments show that our framework outperforms the state-of-the-art method in the human relighting task. 2.