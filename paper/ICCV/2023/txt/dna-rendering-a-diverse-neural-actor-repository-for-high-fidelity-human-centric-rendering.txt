Abstract
Realistic human-centric rendering plays a key role in both computer vision and computer graphics.
Rapid progress has been made in the algorithm aspect over the years, yet existing human-centric rendering datasets and benchmarks are rather impoverished in terms of diversity (e.g., outfit’s fabric/material, body’s interaction with ob-jects, and motion sequences), which are crucial for render-ing effect. Researchers are usually constrained to explore and evaluate a small set of rendering problems on cur-rent datasets, while real-world applications require meth-ods to be robust across different scenarios. In this work, we present DNA-Rendering, a large-scale, high-fidelity repos-itory of human performance data for neural actor render-*Joint-first authors with W. Cheng.
†Equal advising. ing. DNA-Rendering presents several appealing attributes.
First, our dataset contains over 1500 human subjects, 5000 motion sequences, and 67.5M frames’ data volume. Upon the massive collections, we provide human subjects with grand categories of pose actions, body shapes, clothing, accessories, hairdos, and object intersection, which ranges the geometry and appearance variances from everyday life to professional occasions. Second, we provide rich assets for each subject – 2D/3D human body keypoints, foreground masks, SMPLX models, cloth/accessory materials, multi-view images, and videos. These assets boost the current method’s accuracy on downstream rendering tasks. Third, we construct a professional multi-view system to capture data, which contains 60 synchronous cameras with max 4096 × 3000 resolution, 15 fps speed, and stern camera calibration steps, ensuring high-quality resources for task training and evaluation.
Dataset
Ethnicity Age
Attribute
Cloth Motion
Human3.6M [19]
CMU Panoptic [24]
ZJU-MoCap [44]
HUMBI [66]
AIST++ [56, 28]
THuman 4.0 [51]
HuMMan [5]
GeneBody [10]
DNA-Rendering (Ours)
✗
✓
✗
✓
✗
✗
✓
✓
✓
✗
✓
✗
✓
✗
✗
✓
✓
✓
✗
✗
✗
✓
✗
✓
✓
✓
✓
✓
✓
✓
✗
✗
✓
✓
✓
✓
Interactivity
✓
✓
✗
✗
✗
✗
✗
✓
✓
#ID × #Outfit 11 × 1 97 × 1 10 × 1 772 × 1 30 × 1 3 × 1 1000 × 1 50 × 2 500 × 3
Scale
#Motions 17 65 10
−
−
− 500 61 1187
#View 4 31 + 480∗ 24 107 9 24 10 48 60
#Frames 3.6M 15.3M 180K 26M 10.1M 10K 60M 2.95M 67.5M
Realism
HRes 1000P 1080P 1024P 1080P 1080P 1150P 1080P 2048P 4096P
Table 1: Dataset comparison on attributes and scales. We compare the proposed dataset with previous human-centric multiview datasets in terms of attribute coverage, scale, and realism. ‘Ethnicity’ denotes whether the dataset contains actors from multiple ethnic groups.
‘Age’ means if there is a wide age range containing elders or infants. ‘Cloth’ separates datasets with only daily costumes or with extra diverse clothing. ‘Attribute-Motion’ denotes whether it has human motion in different scenarios. ‘Interactivity’ tells whether there contains human-object interaction. We mark these attributes with ✓ and ✗. In scale, we list the number of key factors with compared dataset, Note that ‘Scale-#Motions’ means the number of motion categories, and superscript ∗ means low-resolution VGA cameras, we exclude them during ‘#View’ ranking and ‘#Frames’ calculation. We abbreviate resolution at height as ‘HRes’.
Along with the dataset, we provide a large-scale and quantitative benchmark in full-scale, with multiple tasks to evaluate the existing progress of novel view synthesis, novel pose animation synthesis, and novel identity render-ing methods.
In this manuscript, we describe our DNA-Rendering effort as a revealing of new observations, chal-lenges, and future directions to human-centric rendering.
The dataset, code, and benchmarks will be publicly avail-able at https://dna-rendering.github.io/. 1.

Introduction
Understanding humans is an everlasting problem in our research community, and extensive literature on perceiving and synthesizing humans shows great efforts toward this goal. Over the decades, many pioneers have constructed large-scale and diverse datasets, such as COCO [31] for hu-man pose estimation, and ActivityNet [4] for analyzing hu-man action. These datasets have been pivotal in advancing the development of human-centric perception algorithms.
Yet, when it comes to human-centric rendering, there is still a noticeable gap in comprehensive datasets. Capturing high-quality and massive 3D/4D human avatars is difficult due to the requirements of high-end equipment as well as an efficient data processing pipeline. Existing datasets [19, 23, 44, 51, 18] partially narrow the gaps but have significant limitations on sample diversity (e.g., clothing, motion, body shape, and human-object interaction), or have insufficient realism (e.g., camera resolution, and capture speed). These factors are crucial to rendering effects.
To drive advance in human-centric rendering, we con-tribute a large-scale multi-view human performance capture dataset, named DNA-Rendering, which includes the fac-tors that are important to rendering in great diversity and granularity. On the hardware side, we build a 360-degree indoor system equipped with 60 calibrated RGB cameras and 8 synchronized depth sensors. The captured videos are under the fidelity of up to 12MP (4096 × 3000) reso-lution and recorded at 15 fps. From the dataset’s footage design aspect, we intend to cover most attributes that could reflect the rendering differences with respect to texture, ma-terials, primary/secondary motion deformation, and cate-gory priors. In particular, we design over 1500 outfits and 1187 motion types to ensure the comprehensive coverage of real-world scenarios. We invite 500 actors to partici-pate in the data capture process. We record each person with three different outfits and at least nine unique motions.
The full dataset contains 5000 video sequences with over 67.5M frames. Compared with the existing human-centric dataset like CMU Panoptic [24], ZJU-MoCap [44], THU-man [51], and Human3.6M [19], DNA-Rendering com-prises the most multi-view body performance samples and reaches the highest image quality. The unfold comparisons between DNA-Rendering and the others are given in Tab. 1.
Meanwhile, we provide essential annotations attached to each frame to facilitate the application of downstream tasks.
We develop an automatic annotation pipeline encompassing camera calibration, color correction, image matting, 2D /3D landmark estimation, and SMPLX model fitting. To ensure the labeling quality, we developed a series of technical re-finements to the annotation toolchain. With these efforts, the automatic pipeline can generate faithful data annotations both effectively and efficiently.
The unprecedented richness of DNA-Rendering dataset provides fertile data soil for researchers to develop, and dis-sect their rendering methods in depth. To set up a kickoff example, we further construct benchmarks upon the dataset with extensive experiments. We evaluate the performances of several state-of-the-art full-body rendering and anima-tion approaches under three major tasks, i.e., novel view synthesis, novel pose animation, and novel identity ren-dering. To better analyze current methods in terms of the model capacity, module necessity, and methodology gener-ality, we set up multiple test set splits under different levels
of challenging aspects. For instance, we divide the easy, medium, and hard subsets w.r.t. the cloth looseness, the tex-ture complexity, the motion difficulty, and the human-object interactivity, respectively. We conclude a series of key ob-servations based on the benchmarks, such as how human prior influences the robustness of rendering, how sensitive the multi-view/frame relationship module design is to data volume/distribution, and how loss design affects the perfor-mance in terms of different rendering metrics.
In summary, the DNA-Rendering project fulfills the re-quirement of a high-fidelity human performance capture dataset for the research community. We establish by far the largest multi-view human body performance dataset for high-fidelity human-centric rendering research, with an emphasis on image quality and data attributes. The at-tached benchmarks provide baseline standards for three ma-jor tasks, with rigorous evaluations and dissections on mul-tiple state-of-the-art methods. We believe the dataset, the attached benchmarks, and the tools will boost a wide range of digital human applications and inspire future research. 2.