Abstract
Recently, large-scale diffusion models, e.g., Stable diffu-sion and DallE2, have shown remarkable results on image synthesis. On the other hand, large-scale cross-modal pre-trained models (e.g., CLIP, ALIGN, and FILIP) are com-petent for various downstream tasks by learning to align vision and language embeddings.
In this paper, we ex-plore the possibility of jointly modeling generation and dis-crimination. Specifically, we propose DiffDis to unify the cross-modal generative and discriminative pretraining into one single framework under the diffusion process. DiffDis first formulates the image-text discriminative problem as a generative diffusion process of the text embedding from the text encoder conditioned on the image. Then, we propose a novel dual-stream network architecture, which fuses the noisy text embedding with the knowledge of latent images from different scales for image-text discriminative learning.
Moreover, the generative and discriminative tasks can ef-ficiently share the image-branch network structure in the multi-modality model. Benefiting from diffusion-based uni-fied training, DiffDis achieves both better generation abil-ity and cross-modal semantic alignment in one architecture.
Experimental results show that DiffDis outperforms single-task models on both the image generation and the image-text discriminative tasks, e.g., 1.65% improvement on av-erage accuracy of zero-shot classification over 12 datasets and 2.42 improvement on FID of zero-shot image synthesis.
Figure 1. Comparison of our framework and single-task models. (b) The (a) The diffusion-based image generation-only model. image-text discrimination-only model. (c) Our DiffDis joints the discriminative and generative tasks under the diffusion processing into one framework. Better viewed in colors. 1.

Introduction
“What I cannot create, I do not understand.” by Richard
Feynman (a well-known theoretical physicist).
Recently, large-scale diffusion models (DM) [17, 45, 51] such as Stable Diffusion [42] and DallE2 [40] have shown impressive results in image synthesis and re-define the ca-pacity of state-of-the-art text-guided image synthesis. Typ-*Corresponding author: xdliang328@gmail.com ically speaking, these models contain more than one bil-lion parameters and have a large model capacity; thus have a good generalization and cover an extensive range of domains. Here, by rethinking the famous remarks from
Richard Feynman, we explore whether such powerful gen-erative models can learn the ability to further discriminate and understand cross-modal data.
On the other hand, recent large-scale Vision-Language
Pre-training (VLP) models [39, 20, 53, 26] like CLIP[39] and ALIGN [20] have demonstrated success in vari-ous downstream zero-shot image classification or retrieval tasks. Similar to large-scale diffusion models, these mod-els are pre-trained with millions of image-text pairs col-lected from the Internet. The critical idea of these works is to contrastively align the image and text embeddings into a joint feature space, thus gaining zero-shot discrimination capability, which is different from the diffusion models that consider the problem as the parameterized Markov chain.
In this paper, we focus on bridging the generative diffusion models with VLP models to empower the generative diffu-sion model with the cross-modal discrimination capability, in the spirit of similar principles via large-scale pretraining.
There exist some methods that have considered combin-ing generative models and discriminative models into a sin-gle framework. The famous generative adversarial networks (GAN) [14] introduce the discriminator to guide the adver-sarial learning of the generator. However, the implicit ad-versarial of GAN would lead to mode-collapse and training instabilities. Moreover, HybViT [52] tried to replace the
UNet structure in GLIDE [33] with a ViT [13] model and directly added a classification head to perform image gener-ation and classification jointly, while ignoring the powerful diffusion process for the discriminate task. On the other hand, recent unified general vision models such as Pix2Seq
[6], OFA [50], and Unified I/O [31] try to unify different vision-language tasks into an autoregressive sequence pre-diction framework. However, the autoregressive image gen-eration solutions such as DallE [41] and OFA [50] show inferior performance compared to the DM-based models, such as DallE2 [40] and Stable Diffusion [42], in terms of both generation quality and sampling efficiency.
In this paper, we present DiffDis, a unified vision-language diffusion model for both generative and discrim-inative tasks under the diffusion paradigm. Specifically,
DiffDis first formulates the image-text discriminative prob-lem as a generative diffusion process of the text embedding outputted by the text encoder conditioned on the input im-age. Therefore, the generation and discrimination tasks can share the same image-branch network (i.e., original U-Net) in the multi-modality diffusion model. During inference, zero-shot image classification is performed by calculating the cosine similarity between the generated text embedding and the downstream text embeddings. Secondly, we de-sign a dual-stream network architecture to better fuse the knowledge of latent images with different scales into the text query in image-text alignment. Finally, a unified train-ing paradigm is further proposed to alternatively feed the required inputs and the conditions when jointly performing diffusion-based generative and discriminative tasks. When training discriminative tasks, the image branch serves as an image encoder to feed conditional information into the re-verse text embedding diffusion process and vice versa.
Extensive experiments have shown that the proposed
DiffDis method can achieve better performance on both zero-shot classification and text-guided image generation tasks. Compared to the single-task baseline, our unified framework DiffDis can achieve 1.65% improvement on the average zero-shot classification accuracy on 12 datasets and a 2.42 improvement on FID of zero-shot image synthesis compared to the single-task model. This work is the first to unify the training of generative and discriminative tasks under the diffusion process. We hope that this research will serve as an early-stage exploration for future studies aiming to unify these two tasks under the diffusion process, thereby providing more choices for future multi-task multi-modal jointly-training frameworks.
Our contributions can be summarized as:
• We propose DiffDis to explore a unified vision-language diffusion model for both multi-modality gen-eration and discrimination tasks.
• DiffDis reformulates the image-text discriminative problem by utilizing a generative diffusion process of the text embeddings conditioned on input images.
• We propose a dual-stream network architecture and a diffusion-based unified training paradigm for jointly training the generative and discriminative tasks.
• Extensive experiments demonstrate that our DiffDis outperforms single-task models, achieving a 1.65% improvement on average zero-shot classification accu-racy across 12 datasets and a 2.42 improvement on FID of text-guided image generation. Additionally, DiffDis outperforms CLIP, with a 4.7% improvement on aver-age zero-shot classification accuracy across 12 datasets and a 14.5% improvement on average R@1 of image-text retrieval tasks on Flickr30k and MSCOCO. 2.