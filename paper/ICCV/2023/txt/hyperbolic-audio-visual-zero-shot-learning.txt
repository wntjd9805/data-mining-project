Abstract
Audio-visual zero-shot learning aims to classify samples consisting of a pair of corresponding audio and video se-quences from classes that are not present during training.
An analysis of the audio-visual data reveals a large de-gree of hyperbolicity, indicating the potential benefit of us-ing a hyperbolic transformation to achieve curvature-aware geometric learning, with the aim of exploring more com-plex hierarchical data structures for this task. The pro-posed approach employs a novel loss function that incor-porates cross-modality alignment between video and audio features in the hyperbolic space. Additionally, we explore the use of multiple adaptive curvatures for hyperbolic pro-jections. The experimental results on this very challenging task demonstrate that our proposed hyperbolic approach for zero-shot learning outperforms the SOTA method on three datasets: VGGSound-GZSL, UCF-GZSL, and ActivityNet-GZSL achieving a harmonic mean (HM) improvement of around 3.0%, 7.0%, and 5.3%, respectively. 1.

Introduction
Visual and audio signals frequently co-occur. For ex-ample, movies combine visual and auditory signals, pro-viding an immersive experience to viewers. Humans per-ceive multiple sensory inputs jointly and make decisions accordingly. When a vehicle approaches from behind and honks, the driver sees the vehicle in the rear-view mirror, hears the sound, and decides to give way. The integration of joint visual and audio signals benefits numerous applica-tions. For instance, acoustic features can help localize ob-jects that emit sound in videos [30, 58, 52, 45, 13, 46, 44].
Furthermore, the natural correlations between visual and audio signals provide strong supervision for learning video representations. As such, there has been growing inter-est in learning informative representations from audio sig-*Corresponding author nals for video classification [38, 39, 3, 2, 11, 29, 41]. The benefits of audio-visual multi-modality learning have also been demonstrated in other tasks, such as robotic naviga-tion [9, 18, 12], action recognition [27, 20], highlight detec-tion [54, 5], violence detection [51], aerial scene recogni-tion [26] and speech recognition [53, 1].
Collecting vast amounts of audio-visual data needed for training deep neural networks is time-intensive, expensive, and in some applications, impractical. Additionally, deep models may face difficulties in making accurate predictions when presented with objects from unseen classes in real-world scenarios. This is because they lack the necessary knowledge and context to make well-informed decisions about unfamiliar objects. Low-shot audio-visual tasks [42] have emerged to address the problem of insufficient data, with an aim to generalize models effectively from seen to unseen data. These tasks include one-shot learning [50], few-shot learning [34], and audio-visual zero-shot learning
[37, 36, 35, 42]. One-shot and few-shot audio-visual learn-ing deals with classification from a few examples of un-seen classes. However, audio-visual zero-shot classification poses a more challenging scenario as the model has no ac-cess to audio-visual data from unseen classes during train-ing [42].
In this paper, we focus on audio-visual zero-shot learn-ing. We investigate curvature-aware geometric learning for the audio-visual feature alignment. Our inspiration for us-ing hyperbolic geometry comes from the following obser-vations:
• Data hierarchy. Audio-visual datasets exhibit a hier-archy. For instance, in VGGSound [10], all 309 classes can be categorized into 9 parent classes (or super-classes): “people”, “animals”, “music”, “sports”, “na-ture”, “vehicle”, “home”, “tools” and “others”. Simi-larly, the dataset ActivityNet [6] provides a rich hier-archy with at least four levels. For example, the class
“Hand washing clothes” belongs to “Laundry” (the 2nd level), “Housework” (the 3rd level), and “House-hold Activities” (the 4th level). Most existing audio-Figure 1. We introduce the Hyperbolic Alignment Module, represented by the block with the blue line. The input data features from each modality are encoded by two consecutive networks (encoder and projector). A cross-attention module in between is used to explore the natural correspondence between visual and audio features. Before the cross-attention module processes the features, our Hyperbolic
Alignment Module computes a hyperbolic alignment loss, which aims to explore more hierarchy in audio-visual data. For example, the model may discriminate embeddings of “Playing piano” and “Walking the dog” when it finds that these embeddings belong to different superclasses: “Playing musical instruments” and “Walking/exercising/playing with animals”. visual works have not adequately leveraged the hierar-chical structure present in the audio-visual data.
• Hyperbolic geometric properties. Hyperbolic meth-ods have been shown to be effective in addressing low-shot visual problems [28, 31, 16, 14, 33]. The learned hyperbolic feature embeddings have the ability to cap-ture the hierarchical structure within the data. This is attributed to the tree-like nature of the underlying space, as shown in [24, 7]. One of the benefits of using a hyperbolic space is that the hyperbolic space facil-itates the distribution of embeddings in a tree-shaped structure since its volume expands exponentially.
Based on the observations above, we conjecture that the unique properties of hyperbolic spaces can be leveraged to capture the hierarchical structures in audio-visual data, leading to learning more discriminative embeddings for audio-visual samples. The current SOTA audio-visual zero-shot learning methods [37, 36, 35, 42, 57] operate in the non-curved Euclidean space without considering the data hierarchy. Therefore, there is a need for curvature-aware geometric solutions that can embed the data hierarchy to improve the performance of audio-visual zero-shot learn-ing. The contributions of this work can be summarized as:
• Our work provides a new perspective on using curved geometries for cross-modality, as shown in Figure 1.
We propose a hyperbolic alignment loss that learns features in curved space to improve audio-visual zero-shot learning. Specifically, we use alignment between visual and audio features in the hyperbolic space as an auxiliary learning method for feature fusion across modalities. To the best of our knowledge, we are the first to apply curvature-aware geometric solutions to this task.
• Furthermore, we introduce various frameworks for us-ing the hyperbolic embeddings: 1) Hyper-alignment, 2) Hyper-single, and 3) Hyper-multiple. The Hyper-alignment module maps audio and visual features from
Euclidean to hyperbolic space with a fixed negative curvature and compares them using intra-modal sim-ilarities. Based on Hyper-alignment, Hyper-single adapts the curvature to the model for flexible data structure exploration. Hyper-multiple generates a set of adaptive curvatures for alignments, enabling more generic embeddings.
• Extensive experiments demonstrate that, in most cases, the proposed modules outperform existing models.
Moreover, using the δrel metric [28, 17], we show that hyperbolic alignment enables the learned features to exhibit stronger hierarchical properties. Addition-ally, we observe from t-SNE [49] visualizations that the audio-visual feature embeddings from different su-perclasses become more distinct.
• Ablation studies are provided further to investigate the properties of our hyperbolic alignment module.
In addition to the curvature-negative hyperbolic projec-tion approach, we also test Euclidean and Spherical approaches, which have curvature-zero and curvature-positive properties, respectively.
2.