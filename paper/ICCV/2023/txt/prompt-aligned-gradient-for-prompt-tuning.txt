Abstract
Thanks to the large pre-trained vision-language mod-els (VLMs) like CLIP [37], we can craft a zero-shot clas-sifier by discrete prompt design, e.g., the confidence score of an image being “[CLASS]” can be obtained by using the VLM provided similarity between the image and the prompt sentence “a photo of a [CLASS]”. Further-more, prompting shows great potential for fast adaptation of
VLMs to downstream tasks if we fine-tune the soft prompts with few samples. However, we find a common failure that improper fine-tuning or learning with extremely few-shot samples may even under-perform the zero-shot prediction.
Existing methods still address this problem by using tradi-tional anti-overfitting techniques such as early stopping and data augmentation, which lack a principled solution spe-cific to prompting. In this paper, we present Prompt-aligned
Gradient, dubbed ProGrad to prevent prompt tuning from forgetting the general knowledge learned from VLMs. In particular, ProGrad only updates the prompt whose gradi-ent is aligned (or non-conflicting) to the general knowledge, which is represented as the optimization direction offered by the pre-defined prompt predictions. Extensive experiments under the few-shot learning, domain generalization, base-to-new generalization and cross-dataset transfer settings demonstrate the stronger few-shot generalization ability of
ProGrad over state-of-the-art prompt tuning methods. 1.

Introduction
After seeing and reading countless image-text association pairs, large and deep vision-language models (VLM) [37, 18] can memorize the general knowledge (a.k.a. encyclope-dic knowledge) about what visual patterns correspond to what textual sequence and vice versa. Thanks to the pow-erful language modeling of VLMs, we can establish a com-munication channel in human-readable natural language, i.e., prompt [25, 51, 19], to query the general knowledge.
*Corresponding author.
Prompting bridges the interface gap between the pre-trained and downstream tasks (e.g., regression vs. classification) without the need for additional fine-tuning adaptation. For example, we can craft a concrete prompt—“a photo of a [CLASS]”—for zero-shot image classification: by using the popular vision-language model CLIP [37], we input the image to the vision end and the prompt sentence to the lan-guage end, then obtain a vision-language similarity as the confidence score of classifying the image as “[CLASS]”.
In practice, the prompt-based zero-shot image classi-fication is not accurate because the hand-crafted prompt may not be the most machine-favorable (e.g., “this is a picture of” could be more grammatically prevailing in VLM training), or not specific to the downstream do-main (e.g., “a photo of a person doing” is better in action recognition) [37]. Recently, prompt tuning or prefix tuning [23, 26, 54, 55] has been proposed to replace the hand-crafted prompt with a set of tunable word embedding vectors, which do not have to be translatable back to human-readable words. Yet, prompt tuning is still as tricky as conventional fine-tuning: as the training continues, the generalization ability may decrease and even under-perform the zero-shot baseline. As shown in Figure 1(a&b), the prompt tuning method CoOp [54] achieves the best results via early stop-ping, and its accuracies heavily drop by at most 4% when the training continues. Besides, Figure 1(c&d) show that
CoOp underperforms zero-shot CLIP without augmentation or enough samples from downstream tasks. To the best of our knowledge, existing methods still rely on the conven-tional anti-overfitting techniques such as early stopping and data augmentation [54, 55, 11, 36], which lacks a principled solution to the nature of improper prompt tuning. Further-more, the Grad-CAM visualization results indicate that the fine-tuned prompt misleads the VLM to forget the general knowledge that the classification should at least focus on the foreground object but not the background. Comparing CoOp (Figure 2(b)) with zero-shot CLIP (Figure 2(c)), we find that the CoOp model distracts its attention to the background, while CLIP mainly focuses on the foreground object. These results demonstrate the over-fitting risk of existing prompt
Figure 1: Comparison of Zero-shot CLIP, CoOp, and our ProGrad on Stanford Cars and OxfordPets datasets. (a)&(b): Given 1 shot training sample, CoOp’s performance severely drops and under-performs zero-shot CLIP by large margins when the training continues. (c)&(d): CoOp may fail to improve CLIP without data augmentation or plenty of samples.
Figure 2: Comparisons of Grad-CAM [43] visualization for prompt tuning methods using different gradient strategies on
Stanford Cars Datasets. tuning strategies, especially when the number of training samples is extremely limited (e.g., 1 or 2).
To this end, we present a novel prompt tuning method called Prompt-aligned Gradient (ProGrad) to overcome the improperly biased tuning for CLIP. The principle of
ProGrad is to regularize each tuning step not to conflict with the general knowledge offered by the original prompt, e.g., the zero-shot CLIP predictions. Specifically, we mea-sure the general knowledge direction Gg using the gradient of Kullback–Leibler (KL) divergence between the predic-tions of the zero-shot prompted CLIP and the few-shot fine-tuned model, which we name as general direction. Similarly, we compute the domain-specific knowledge direction Gd using the gradient of cross-entropy between the ground-truth and the few-shot fine-tuned model, dubbed domain-specific direction. We decompose the domain-specific direction Gd into: 1) a vector G⊥ orthogonal to the general direction, which denotes the non-conflicting domain-specific knowl-edge; and 2) a vector G∥ parallel to the general direction, which denotes the general knowledge. Note that the first gra-dient component does NOT override the general direction as any two orthogonal vectors can be transformed into two non-conflicting base vectors. For the second component, it must be one of the two directions: 1) the same of the general direc-tion, which indicates that the update is aligned to the general knowledge, and 2) the opposite of general direction, indicat-ing a conflicting update that should be discarded to avoid forgetting. Overall, in each iteration, ProGrad only up-dates the parameters in the prompt-aligned direction that has an acute angle to the general direction. Compared to CoOp and CLIP, both Gg and G⊥ (Figure 2(d&e)) help to regu-larize the model to focus on the foreground, and ProGrad (Figure 2(f)) further improves the visual response.
Following CLIP, CoOp and CoCoOp [55], we evaluate
ProGrad under the few-shot learning, domain generaliza-tion, base-to-new generalization and cross-dataset transfer
settings over 15 image classification benchmarks, covering generic object classification, fine-grained image recognition, action classification. In summary, our ProGrad achieves: 1) clear improvement compared to CoOp over all of the 11 datasets; 2) clear improvement on the harmonic mean of base and new classes accuracies on all 11 datasets compared to CoOp and CoCoOp, and 3) clear improvement on both the source and target datasets of the domain generalization. 2.