Abstract
Text-driven localized editing of 3D objects is particu-larly difficult as locally mixing the original 3D object with the intended new object and style effects without distort-ing the object’s form is not a straightforward process. To address this issue, we propose a novel NeRF-based model,
Blending-NeRF, which consists of two NeRF networks: pre-trained NeRF and editable NeRF. Additionally, we intro-duce new blending operations that allow Blending-NeRF to properly edit target regions which are localized by text. By using a pretrained vision-language aligned model, CLIP, we guide Blending-NeRF to add new objects with varying colors and densities, modify textures, and remove parts of the original object. Our extensive experiments demonstrate that Blending-NeRF produces naturally and locally edited 3D objects from various text prompts. 1.

Introduction 3D image synthesis and related technologies are greatly impacting industries such as art, product design, and ani-mation. While recent 3D image synthesis techniques like
Neural Radiance Field (NeRF) [22] have opened up new applications for 3D content production [8, 14, 26] at scale, their ability to enable precise and localized editing of object shapes and colors remains a challenge for broader adoption.
Often time, a more localized and granular editing of 3D objects, especially attaching or removing certain objects of certain styles, is still difficult and costly in spite of several recent attempts at 3D object editing [4, 18, 21, 35, 40].
Previous attempts, such as EditNeRF [18] and NeRF-Editing [40], only offer limited and non-versatile editing options, while Text2Mesh [21] and TANGO [4] allow only simple texture and shallow shape transformations of entire 3D objects. CLIP-NeRF [35] propose a generative method with disentangled conditional NeRF for object editing but
*Equal contribution.
†Corresponding author. Partially conducted at LG Electronics. (a) bulldozer (original) (b) twinkle bulldozer (c) bulldozer in glass (d) bulldozer frame
Figure 1. Representative results of text-driven localized object editing using our method. (a) Bulldozer is the original object, and each editing is performed by (b) color change, (c) density addition, and (d) density removal operations. it requires a large volume of training data for the targeted editing category and is hard to edit only the desired part of objects locally. They present an additional approach, fine-tuning a single NeRF per scene with a CLIP-driven objec-tive, which can edit object appearance but not shape well.
To achieve effective and practical localized editing of 3D objects by any text prompts at scale, it is necessary to apply style changes to specific portions of the object, including selectively changing color and locally adding and remov-ing densities, as shown in Figure 1. In this study, we pro-pose a novel method for localized object editing that allows modification of 3D objects by text prompts, enabling full stylization including density-based localized editing. We believe that relying on the simple fine-tuning of a single
NeRF to generate new densities in the low initial density area or to alter existing densities through a CLIP-driven ob-jective is inadequate for achieving complete stylization of
shapes and colors. Instead, our approach involves parame-terizing specific regions in the implicit 3D volumetric rep-resentations and blending the original 3D object representa-tion with an editable NeRF architecture specifically trained to render the blended image naturally. We use a pretrained vision-language method like CLIPSeg [19] to specify the area to be modified in the text input workflow.
The proposed method is based on a novel layered NeRF architecture, called Blending-NeRF, which includes a pre-trained NeRF and an editable NeRF. There are some studies that employ multiple NeRFs and train them simultaneously to individually reconstruct the static and dynamic compo-nents of a dynamic scene [7, 33, 37, 39]. On the other hand, our approach introduces an additional NeRF to facilitate text-based modifications in specific regions of a pretrained static scene. These modifications encompass various edit-ing operations, including color changes, density addition, and density removal. By blending density and color from the two NeRFs, we can achieve fine-grained localized edit-ing of 3D objects. In summary, our contributions include:
• We propose the novel Blending-NeRF architecture that combines a pretrained NeRF with an editable NeRF using various objectives and training techniques. This approach allows to naturally edit the specific regions of 3D objects while preserving their original appearance.
• We introduce new blending operations that capture the degree of density addition, density removal, and color alteration. Thanks to these blending operations, our method allows for precisely targeting the specific re-gions for localized editing and constraining the degree of object editing.
• We conduct several experiments involving text-guided 3D object editing, such as editing of shape and color, and compare our approach to previous attempts and their simple extensions, showing that Blending-NeRF is both qualitatively and quantitatively superior. 2.