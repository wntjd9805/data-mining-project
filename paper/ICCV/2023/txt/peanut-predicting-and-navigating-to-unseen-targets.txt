Abstract
Efficient ObjectGoal navigation (ObjectNav) in novel environments requires an understanding of the spatial and semantic regularities in environment layouts. In this work, we present a straightforward method for learning these regularities by predicting the locations of unobserved ob-jects from incomplete semantic maps. Our method differs from previous prediction-based navigation methods, such as frontier potential prediction or egocentric map comple-tion, by directly predicting unseen targets while leveraging the global context from all previously explored areas. Our prediction model is lightweight and can be trained in a su-pervised manner using a relatively small amount of pas-sively collected data. Once trained, the model can be incor-porated into a modular pipeline for ObjectNav without the need for any reinforcement learning. We validate the effec-tiveness of our method on the HM3D and MP3D ObjectNav datasets. We find that it achieves the state-of-the-art on both datasets, despite not using any additional data for training.
Code is available at https:// ajzhai.github.io/ PEANUT. 1.

Introduction
Embodied visual navigation refers to a range of tasks that involve an agent navigating within the physical world based on visual sensory input [13]. One such task that has recently gained popularity is ObjectGoal navigation, also known as
ObjectNav [2]. Here, the agent is placed in an unknown environment and is tasked with navigating to a specific ob-ject category (e.g. “toilet”). ObjectNav is a fundamentally important task for embodied AI because it tests both scene understanding and long-term memory. It has immediate ap-plications for helping people with disabilities find objects in their homes. It is also a necessary precursor for many other semantic tasks, such as instruction following [1] and question answering [12].
In ObjectNav, the agent is placed in a random location in an unknown environment and is given access to RGB-D ob-servations, its own pose, and a target category. An episode is considered successful if the agent stops near an object
Figure 1. Explicit Prediction for ObjectNav. To find a semantic target in a novel environment, an agent may make explicit guesses about where the target might be. Humans excel at this type of reasoning and can generate a diverse set of reasonable guesses in-stantaneously. On the right is a top-down map representing the parts of the environment that have been observed. Yellow circles represent multiple human guesses as to where a bed might be. of the target category within a given time limit. In order to perform this task efficiently, the agent must leverage pri-ors about environment layouts while searching for the tar-get. For example, imagine that you observe the image in
Figure 1 and your target category is “bed”. The room on the right does not have any visible furniture but has a good chance to be a bedroom. On the other hand, the room on the left has what appears to be a mirror above a countertop, suggesting that it is a bathroom that probably does not con-tain a bed. The room directly in front could be a bedroom, but it is farther away than the other two. Thus, it would be a good idea to look inside the room on the right first and the room directly in front second. An ideal ObjectNav agent should be able to make such predictions about unexplored areas and reason about the uncertainty in its predictions to plan efficient search routes.
Existing methods for ObjectNav can be divided into two categories: end-to-end learning methods and modular meth-ods. End-to-end methods aim to learn a policy that directly maps sensor observations to actions. The policy is usually modeled as a recurrent neural network and learned either us-ing reinforcement learning [40, 45, 30, 26, 14] or imitation learning [38]. Such methods are flexible in the policies that they can produce, but they require large amounts of data, in-cur high computational costs (especially for RL), and lack
Figure 2. Overview of PEANUT. At each step, the agent’s RGB-D observation ot and pose observations pt are used to update the incomplete global semantic map mt. This map is then used to predict a target object probability map ht, which is used to select long-term goals gt. Finally, an analytical local planner is employed to calculate the low-level actions necessary to reach gt. interpretability. Modular methods combat these issues by decomposing the task into subproblems. Current modular methods for ObjectNav start by building a semantic map of the environment [6, 10], and then employ one module for high-level goal selection (“where to look”) and another module for low-level action planning (“how to get there”).
The “how to get there” subproblem has been studied in-dependently under the name of PointNav [13]. The key question for ObjectNav is how to solve the “where to look” subproblem. Recently, several works have proposed to tackle the “where to look” subproblem as an explicit pre-diction task by training a network to make predictions about what lies in the unexplored areas of the environment using supervised learning [28, 18, 36]. This explicit-prediction paradigm is especially attractive because it evades the sam-ple inefficiency and high computational cost of RL [18, 36].
Previous explicit-prediction methods for ObjectNav have tried to predict unseen targets from single-view obser-vations [18, 28] or estimate potential functions at the fron-tiers of the global semantic map [36]. We believe using the context provided by the global semantic map is crucial for making informative predictions. However, we choose to directly predict unseen targets instead of frontier poten-tial functions, which do not model uncertainty and may be difficult to predict accurately.
In this paper, we introduce PrEdicting And Navigating to Unseen Targets (PEANUT), a novel explicit-prediction method for ObjectNav that predicts target object probabil-ities in the unexplored areas of the environment based on the agent’s semantic map. Unlike previous map prediction methods [28, 18] that rely on single-view egocentric con-text, PEANUT performs prediction in a global, allocentric context. This approach allows the agent to leverage infor-mation from previous timesteps and is consistent with cog-nitive models of human and animal navigation [3, 17].
In order to select long-term goals, we take the tar-get probability prediction and apply a simple distance-weighting scheme that encourages the agent to search closer locations before moving on to faraway ones. This produces a value map, which we select long-term goals from by sim-ply taking the argmax. Combining this goal selection mod-ule with an analytical low-level planner yields a modular pipeline for ObjectNav that does not require any reinforce-ment learning and can be trained in less than one GPU day.
We evaluate PEANUT on the Habitat-Matterport3D (HM3D) dataset
[37] and the Matterport3D (MP3D) dataset, which are the settings for the 2022 and 2021
Habitat ObjectNav Challenges respectively. On HM3D,
PEANUT outperforms all previous published methods, in-cluding methods that utilize additional datasets for training.
On MP3D, PEANUT also outperforms all previous meth-ods, including recent modular methods.
In our ablation studies, we perform experiments to demonstrate the bene-fit of using global context for prediction. 2.