Abstract
Token-based masked generative models are gaining pop-ularity for their fast inference time with parallel decoding.
While recent token-based approaches achieve competitive performance to diffusion-based models, their generation performance is still suboptimal as they sample multiple to-kens simultaneously without considering the dependence among them. We empirically investigate this problem and propose a learnable sampling model, Text-Conditioned To-ken Selection (TCTS), to select optimal tokens via localized supervision with text information. TCTS improves not only the image quality but also the semantic alignment of the generated images with the given texts. To further improve the image quality, we introduce a cohesive sampling strategy,
Frequency Adaptive Sampling (FAS), to each group of tokens divided according to the self-attention maps. We validate the efficacy of TCTS combined with FAS with various generative tasks, demonstrating that it significantly outperforms the baselines in image-text alignment and image quality. Our text-conditioned sampling framework further reduces the original inference time by more than 50 % without modifying the original generative model. 1.

Introduction
In the flood of generative AI systems for vision do-mains, text-conditional image generation [26, 33, 42] is coming to the fore in recent years. Although many re-cent works have achieved success in synthesizing high-quality images [19, 34] with plausible class-alignment in class-conditional cases [4, 8], text-to-image (T2I) genera-tion is more challenging since generating visual outputs
*Equal contribution. that are semantically aligned with input texts is a nontrivial problem. We can roughly categorize the works on text-to-image generation into transformer-based autoregressive (AR) [12, 30, 33] and diffusion-based [18, 29] approaches.
Along with the advancement of language models, AR models using transformers have shown impressive performance in text-to-image generation. Despite their success, they suffer from the problem of unidirectional bias, which is undesirable for image generation, and crucially, the sampling process requires over 10 times as much time compared to existing models. Another line of work is diffusion-based methods that aim to generate images by iteratively denoising noisy samples. In particular, several continuous diffusion models
[10, 32] have achieved outstanding performance and reduced computational cost. Yet, they require excessive sampling steps to obtain high-quality images during the inference.
Recently, a new family of generative models, called token-based diffusion model [6, 23, 41], has emerged as an alter-native to tackle the problem of text-to-image generation.
Token-based diffusion models quantize the latent features into tokens and apply categorical corruption process in dis-crete state spaces [1], while conventional diffusion models use Gaussian noise in continuous space. Among various discrete diffusion methods, mask-based diffusion, similar to the absorbing state diffusion used in [1], is mostly used.
Compared to existing AR models, this token-based approach is advantageous for speeding up the generation process via simultaneously sampling multiple tokens. Despite the lim-itation on the reconstruction capacity, these token-based diffusion models significantly outperform the competitors in terms of FID scores, even with fewer sampling steps com-pared to the continuous diffusion models [5, 37].
However, sampling multiple tokens at once often leads
Figure 1: Generated samples on MS-COCO dataset and evaluation graph of various sampling methods showing their trade-off.
Uniform sampling is a fixed strategy with notably poor text alignment compared to other methods (FID-40K: 15.61, MID-L: 21.23). Random revoke sampling is a revocable strategy with improved text alignment (FID-40K: 16.81, MID-L: 26.98). Ours is TCTS combined with FAS, where both the image quality and the text alignment are significantly better compared to those of baselines (FID-40K: 13.6, MID-L: 29.5).
Metrics are measured on all 40K images with their corresponding single caption. The classifier-free guidance scale was fixed at 5 for all sampling methods. to inconsistency throughout a generated image [37]. For each location, the generator outputs a probability distribu-tion that are coherent with each other. However, there is no guarantee that every single token sampled from the distribu-tions will perfectly align with one another. In other words, it is still possible for the generator to sample incompatible tokens regardless of the generator’s capability, leading to potentially nonsensical outputs [24, 37]. This results in a trade-off between the sampling steps and generation quality.
Reducing the sampling steps leads to faster generation but results in performance degradation due to a large number of simultaneously sampled tokens. Especially, this problem fur-ther stands out in the text-to-image generation tasks since the distribution of text-aligned images is more restricted than the distribution of unconditioned or class-conditioned images.
To address these limitations, we propose a novel sampling approach that refines the images during the diffusion process based on the text condition, which we refer to as the Text
Conditioned Token Selection (TCTS). TCTS is a sampling strategy that can mask out and resample previously sampled tokens, which we refer to as revocable. To find the tokens that do not align with the given text condition, we propose to train a model that selects tokens to be masked out and re-generated, to be well-aligned with the given text. Com-bining this approach with the revocable sampling scheme,
TCTS can generate high-quality images with improved text-alignment in even fewer sampling steps compared to the naive generative model, as shown in Figure 1. We further introduce Frequency Adaptive Sampling (FAS) to solve the over-simplification problem that occurs when applying revo-cable methods for relatively longer steps. FAS leverages the self-attention map of the images and applies a mixed sam-pling method, which we call persistent sampling to prevent the issue. We summarize our contributions as follows:
• We experimentally show that the revocable sampling strategies are crucial for the trade-off between the text-alignment and the image quality and provide in-depth analysis compared to previous fixed sampling methods.
• We propose a novel revocable sampling method based on a learnable token selection model that significantly improves the text alignment as well as the image quality even with fewer sampling steps, without the need of retraining the generator model.
• Moreover, we propose a novel sampling method based on the self-attention map that can be combined with
TCTS to solve the over-simplification problem. 2.