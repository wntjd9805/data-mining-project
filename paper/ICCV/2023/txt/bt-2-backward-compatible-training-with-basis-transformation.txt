Abstract 1.

Introduction
Modern retrieval system often requires recomputing the representation of every piece of data in the gallery when updating to a better representation model. This process is known as backfilling and can be especially costly in the real world where the gallery often contains billions of samples.
Recently, researchers have proposed the idea of Backward
Compatible Training (BCT) where the new representation model can be trained with an auxiliary loss to make it back-ward compatible with the old representation. In this way, the new representation can be directly compared with the old rep-resentation, in principle avoiding the need for any backfill-ing. However, follow-up work shows that there is an inherent trade-off where a backward compatible representation model cannot simultaneously maintain the performance of the new model itself. This paper reports our “not-so-surprising” find-ing that adding extra dimensions to the representation can help here. However, we also found that naively increasing the dimension of the representation did not work. To deal with this, we propose Backward-compatible Training with a novel Basis Transformation (BT 2). A basis transforma-tion (BT) is basically a learnable set of parameters that applies an orthonormal transformation. Such a transforma-tion possesses an important property whereby the original information contained in its input is retained in its output.
We show in this paper how a BT can be utilized to add only the necessary amount of additional dimensions. We empiri-cally verify the advantage of BT 2 over other state-of-the-art methods in a wide range of settings. We then further extend
BT 2 to other challenging yet more practical settings, in-cluding significant changes in model architecture (CNN to
Transformers), modality change, and even a series of updates in the model architecture mimicking the evolution of deep learning models in the past decade. Our code is available at https://github.com/YifeiZhou02/BT-2.
*Equal contribution
Figure 1. Illustration of BT 2 The backbone produces a representa-tion (light green ovals) that is encouraged to match the new model’s representation, ϕ′ new, via a matching and classification loss. A subset of this then goes through a BT transformation, which retains the information (purple triangles) from the new representation. At the same time, the new representation is then projected into a layer (pink ovals) which is combined with part of the BT-transformed new representation (the three purple triangles). This layer then goes through a BT transformation that is then encouraged to match the old model’s representation, ϕold, in effect, resulting in a backward compatible representation as the BT transformations have to inher-ently retain information from both ϕ′ new and ϕold. This is akin to the BCT procedure. The two purple triangles (i.e., what we referred to as the additional dimensions) that are not part of this are used to capture extra information in the new representation that may not be compatible. The resulting ϕnew is then the representation used for all subsequent queries and new gallery samples. Refer to
Section 3.4 on the definitions of ϕ1,2,3,4,5.
Modern visual retrieval systems retrieve similar images from a pool of stored data (referred to as gallery) with a given image (referred to as query). This is often done by training a model to encode all the images in the gallery and storing the resulting representations. A given query is encoded with the same model and its representation is used to retrieve the im-ages with the most similar representations from the gallery.
As better representation model design becomes available, practitioners often desire to update the representations in the 1
gallery with the new model to achieve better performances.
The issue is that if the new model has been trained indepen-dently from the old model, the representations generated by the new model will not be compatible with those generated by the old model, which necessitates re-calculating the repre-sentations of the gallery set, a process known as “backfilling”
[31]. This process gets very costly or even impossible for real world galleries which often contain billions and billions of images.
Shen et al. [31] therefore proposes a framework to train the new model while being compatible with the old model, known as Backward Compatible Training (BCT), with the hope of removing the need for backfilling. They propose to add an “influence loss” to the training objective of the new model to heuristically induce a backward-compatible representation. However, as pointed out by [29], adding this influence loss can significantly hurt the performance of the new model when compared to its independently trained coun-terpart. To mitigate this issue, subsequent works [46, 24, 47] have proposed various more sophisticated influence losses, but these endeavors have achieved limited success. Indeed, as shall be detailed in Section 3.2, it may be impossible to find a new representation model that is at the same time backward compatible yet achieves the fullest potential of the new model. In view of this, another line of work in which researchers utilize a light-weight transformation of the old representation into the new representation [37, 29, 33] looks promising. However, despite their effort to make the trans-formation light-weight, it still requires a costly procedure of applying a neural network to update billions of images in the gallery.
In this paper, we present findings that the conflict between backward compatibility and new model performance can be mitigated by expanding the representation space to simul-taneously accommodate both the old model and the best independently trained new model. To motivate this, one can first consider an upper bound solution along this direction, where the representation of the old model is concatenated with that of an independently trained new model - being inde-pendently trained, the new model is no more limited by the backward compatibility requirement. Subsequently, queries and new samples added to the gallery are now encoded with the concatenated representations. During retrieval, since it is easy to distinguish between the gallery samples that are still of the old representations and those with the concate-nated representations due to the difference in size, we can simply truncate the new representation from the query when comparing with the old representations in the gallery. This upper bound solution is “perfectly” backward compatible but suffers from two critical drawbacks: (1) it significantly increases computations due to the additional number of for-ward passes when computing the query representation, and (2) it begets a significant dimension expansion as a result of the concatenation. In fact, both (1) and (2) can get especially severe after multiple model updates.
Nevertheless, such an upper bound solution provides us with the inspiration to consider adding dimensions to the representation as necessary while conducting BCT. We first tried naively adding dimensions (e.g., directly adding an extra 32 dimensions while training a BCT model) to the new representation, but found that this did not lead to a clear advantage as shown in Section 4. Instead, we conjecture and show that what would be more desirable is to add dimensions for the purpose of storing any information that is not com-patible between the old and new representation. Towards this end, we propose a novel Backward-compatible Training with Basis Transformation (BT 2) that exploits a series of learnable basis transformations (BT) to find the information in the new representation that is incompatible with the old representation. Because a BT is basically an orthonormal transformation, the output of a BT retains the entirety of the information stored in the input (see Lemma 3). With this in mind, we introduce some clever manipulation with BT that helps to exactly “force” incompatible information in the new representation into the additional dimensions, while keeping the compatible information in the BCT representation. Fig. 1 provides a conceptual explanation of our BT 2 design.
In summary, our contributions are three-fold:
• We show that the dilemma between backward compat-ibility and new model development can be reconciled with extra dimensions.
• We propose BT 2 that exploits a series of learnable changes of basis to effectively exploit the extra dimen-sions, and verify its empirical advantage over other state-of-the-art methods in a wide range of backward compatibility tasks.
• We extend BT 2 to more challenging and practical sce-narios that have not been considered by existing works to the best of our knowledge. These include significant changes in model architecture, compatibility between different modalities, and even a series of updates in the model architecture mimicking the history of deep learning in the past decade. 2.