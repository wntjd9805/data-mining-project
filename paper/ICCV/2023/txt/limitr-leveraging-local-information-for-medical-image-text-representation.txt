Abstract
Medical imaging analysis plays a critical role in the di-agnosis and treatment of various medical conditions. This paper focuses on chest X-ray images and their correspond-It presents a new model that ing radiological reports. learns a joint X-ray image & report representation. The model is based on a novel alignment scheme between the visual data and the text, which takes into account both lo-cal and global information. Furthermore, the model in-tegrates domain-specific information of two types—lateral images and the consistent visual structure of chest images.
Our representation is shown to benefit three types of re-text-image retrieval, class-based retrieval, trieval tasks: and phrase-grounding. Our code is publicly available 1. 1.

Introduction
X-ray imaging is one of the most common medical ex-aminations performed, where in the U.S. alone 70 million scans are performed every year [20]. During a post-scan routine, a medical report is written by a radiologist. This is a challenging task even for trained personnel, since the pathologies, which typically occupy a small portion of the image, might be characterized by subtle (sometimes dis-tributed) anatomical changes. Automating the analysis has the potential to aid experts and to speed-up the process.
The task of producing a joint representation of medi-cal image-report data inherently differs from that of nat-ural image-text, which has been extensively explored re-cently [6]. The available data is orders of magnitude smaller than that of natural images. Furthermore, the data is highly unbalanced, since most of the examples are normal. Even in abnormal examples, most of the image (/report) is normal.
Due to the importance of the task, the available medical data has been used for a variety of applications, including class-based retrieval (retrieving data of the same diagno-sis) [8, 27, 17, 25], pathology classification [23, 8, 3, 27, 1https://github.com/gefend/LIMITR
Figure 1: Image-report retrieval. Given the visual data, which contains both Lung Opacity & Lung Lesion diag-noses, our model retrieves the correct R1 report. Another report, R2, which corresponds to another image, contains the same pathologies. It differs in the subtle details: pathol-ogy description, localization, uncertainties, normalities and additional unlabelled pathologies.
In tasks such as class-based retrieval, R2 is considered a correct match. Our model aims also at tasks, such as image-text retrieval, which care about the subtleties. 17, 25], detection [8, 3, 23, 18], and phrase-grounding [3].
Often, these tasks do not require subtle details, such as the severity of the pathology, its exact location, or findings that are beyond the pre-defined pathologies.
We propose a novel model, which learns a joint X-ray image-report representation that is attentive to subtle de-tails, additional descriptions of the pathologies, uncertain-ties etc., as illustrated in Figure 1. It is based on three key ideas. First, our method learns to utilize both global align-ment (image-report pairs) that captures high-level informa-tion, such as the sheer existence of a disease, and local alignments (region-word pairs) that capture subtle details and abnormalities. Second, it benefits from lateral images, which are usually ignored. Third, it utilizes domain-specific localization information. We elaborate below.
Local alignment in medical imaging is challenging since the data is not annotated locally by bounding boxes and their labels. This is in contrast to natural image datasets, which provide localized ground-truth information [4, 9, 13, 14, 15, 16]. Furthermore, localization ambiguity is inher-ent in medical imaging, as report findings may correspond to multiple image regions. To this end, we propose a new aggregation method and a new loss, which synthesize both region-word alignments within a single pair and global and local alignments across pairs.
Second, we propose to use lateral views, if they exist, just like radiologists do. These views may provide addi-tional information, yet they are largely ignored by other representation learning works. We introduce an attention model that learns for each portion of the report when to con-sider both images, when only one, and when none.
Lastly, our model utilizes some basic domain-specific lo-calization information—the structure of the human body; for instance, the heart is always inbetween the lungs and is approximately in the same image position. We show that we can add global positional encoding for the whole dataset.
This encoding also allows the network to better learn local connections between the frontal and the lateral views.
To demonstrate the benefit of our model, we evaluate it for three different retrieval tasks: (1) Text-image retrieval:
Given a report, the goal is to find the most suitable image and vice versa. In this task, we expect the corresponding report (/image) to be retrieved, as illustrated in Figure 1, where all the information that appears in the text and in the image, are taken into account. This task demonstrates the ability of our method to accurately capture subtleties. (2) Phrase-grounding: Given an image and a correspond-ing phrase, the goal is to produce an attention map of the similarity between the phrase and each region. This task demonstrates the quality of the local alignment. (3) Class-based retrieval: Given a textual description, the goal is to retrieve images that belong to the same class of the descrip-tion. This is the most common retrieval task. We present
SoTA results for all these tasks.
Hence, our work makes the following contributions: 1. We propose a novel model for learning a joint X-ray image & report representation. It is based on a new local and global alignment scheme. This alignment is shown to boost performance. 2. Our model integrates additional domain-specific knowledge—lateral images and visual structure. This information further improves performance. 3. The benefit of our model is demonstrated on three re-trieval tasks, showing its ability to capture fine features in both images and reports. We demonstrate SoTA re-sults on commonly-used datasets. 2.