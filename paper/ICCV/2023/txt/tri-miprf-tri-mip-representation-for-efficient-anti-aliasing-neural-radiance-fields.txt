Abstract
Despite the tremendous progress in neural radiance fields (NeRF), we still face a dilemma of the trade-off between qual-ity and efficiency, e.g., MipNeRF [3] presents fine-detailed and anti-aliased renderings but takes days for training, while
Instant-ngp [36] can accomplish the reconstruction in a few minutes but suffers from blurring or aliasing when render-ing at various distances or resolutions due to ignoring the sampling area. To this end, we propose a novel Tri-Mip encoding (à la “mipmap”) that enables both instant recon-struction and anti-aliased high-fidelity rendering for neural radiance fields. The key is to factorize the pre-filtered 3D feature spaces in three orthogonal mipmaps. In this way, we can efficiently perform 3D area sampling by taking ad-vantage of 2D pre-filtered feature maps, which significantly elevates the rendering quality without sacrificing efficiency.
To cope with the novel Tri-Mip representation, we propose a cone-casting rendering technique to efficiently sample anti-aliased 3D features with the Tri-Mip encoding considering both pixel imaging and observing distance. Extensive experi-ments on both synthetic and real-world datasets demonstrate our method achieves state-of-the-art rendering quality and reconstruction speed while maintaining a compact represen-tation that reduces 25% model size compared against Instant-ngp. Code is available at the project webpage: https:
//wbhu.github.io/projects/Tri-MipRF 1.

Introduction
Neural radiance field (NeRF) [34], emerged as a ground-breaking implicit 3D representation, models the geometry and view-dependent appearance by a multi-layer perceptron (MLP) for rendering photo-realistic novel views. MipN-eRF [3] further pushes the boundaries of rendering qual-ity by integrated position encoding to formulate the pre-filtered radiance fields. Such impressive visual quality, how-ever, requires expensive computation in both reconstruction and rendering stages, e.g., MipNeRF [3] takes more than three days for the reconstruction and minutes for render-†Corresponding author.
Figure 1. Rendering quality vs. reconstruction time on the multi-scale Blender dataset [3]. Our Tri-MipRF achieves state-of-the-art rendering quality while can be reconstructed efficiently, compared with cutting-edge radiance fields methods, e.g., NeRF [34], MipN-eRF [3], Plenoxels [14], TensoRF [9], and Instant-ngp [36]. Equip-ping Instant-ngp with super-sampling (named Instant-ngp ↑5×) improves the rendering quality to a certain extent but significantly slows down the reconstruction. ing a single frame. On the other hand, recent works pro-posed explicit or hybrid representation for efficient render-ing [42, 17, 55, 19, 10, 6], and reconstruction [14, 46, 9, 36], e.g., the hash encoding [36] significantly reduces the recon-struction time from days to minutes and achieves real-time rendering. But all their rendering model is flawed in formu-lating the pixel as a single point without area, which would cause the renderings excessively blurred in close-up views and aliased in distant views. We face a dilemma of the trade-off between quality and efficiency due to the lacking of a representation to support efficient area sampling.
In this paper, we aim to design a radiance field represen-tation that supports both high-fidelity anti-aliased render-ings and efficient reconstruction. To address the aliasing and blurring issue, super-sampling and pre-filtering (a.k.a. area-sampling) are two popular streams of strategies in the offline and real-time rendering literature, respectively. But super-sampling each pixel by casting multiple rays through
its footprint significantly increases the computation cost, and directly pre-filtering the 3D volume is also memory-and computation-intensive, which conflicts with the goal of efficiency. Also, it is not trivial to pre-filter the radiance field represented with hash encoding, due to the hash col-lisions. We achieve this challenging goal with our novel
Tri-Mip radiance fields (Tri-MipRF). As shown in Fig. 1, our
Tri-MipRF achieves state-of-the-art rendering quality that presents high-fidelity details in close-up views and is free of aliasing in distant views. Meanwhile, it can be reconstructed super-fast, i.e., within five minutes on a single GPU, while the super-sampling variant of hash encoding, Instant-ngp
↑5×, takes around ten minutes for the reconstruction and has much lower rendering quality.
The key to achieving our goal is the proposed Tri-Mip encoding, i.e., featurizing the 3D space by three 2D mip (mul-tum in parvoto) maps. The Tri-Mip encoding first decom-poses the 3D space into three planes (planeXY , planeXZ, and planeY Z) inspired by the factorization for 3D content generation in [8], and then represent each plane by a mipmap.
It ingeniously models the pre-filtered 3D feature space by taking advantage of different levels of the 2D mipmaps. Our
Tri-MipRF belongs to the hybrid representation since it mod-els the radiance fields by Tri-Mip encoding and a tiny MLP, which makes it converge fast during the reconstruction. And the model size of our method is relatively compact since the MLP is very shallow and the Tri-Mip encoding only re-quires three 2D maps to store the base levels of the mipmaps.
To cope with the Tri-Mip encoding, we propose an efficient cone-casting rendering technique that formulates the pixel as a disc and emits a cone for each pixel. Different from
MipNeRF [3] that samples the cone with multivariate Gaus-sian, we adopt spheres that are inscribed with the cone. The spheres are further featurized by the Tri-Mip encoding ac-cording to their occupied area. The reason for doing so is that the features in mipmaps are pre-filtered isotropically.
The Tri-Mip encoding models the pre-filtered 3D feature space while the cone-casting is adaptive to the rendering distance and resolution, and they are effectively connected by the occupied area of the sampling sphere, which makes the renderings of our Tri-MipRF free of blurring in close-up views and aliasing in distant views. Besides, we also de-velop a hybrid volume-surface rendering strategy to enable real-time rendering on consumer-level GPUs.
We extensively evaluated our Tri-MipRF on both public benchmarks and images captured in the wild. Both quantita-tive and qualitative results demonstrate the effectiveness of our method for high-fidelity rendering and fast reconstruc-tion. Our contributions are summarized below.
• We propose a novel Tri-Mip encoding to model the pre-filtered 3D feature space by leveraging multi-level 2D mipmaps, which enables anti-aliased volume rendering with efficient area sampling.
• We propose a new cone-casting rendering technique that efficiently emits a cone for each pixel while gracefully sampling the cone with spheres on the Tri-Mip encoded 3D space.
• Our method achieves both state-of-the-art rendering quality and reconstruction speed (within five minutes on a single GPU), while still maintaining a compact rep-resentation (with a 25% smaller model size than Instant-ngp). Thanks to the hybrid volume-surface rendering strategy, our method also achieves real-time rendering when deploying on consumer-level devices. 2.