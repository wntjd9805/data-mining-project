Abstract
Foundation models are pre-trained on massive data and transferred to downstream tasks via fine-tuning. This work presents Vision Middleware (ViM), a new learning paradigm that targets unified transferring from a single foundation model to a variety of downstream tasks. ViM consists of a zoo of lightweight plug-in modules, each of which is independently learned on a midstream dataset with a shared frozen backbone. Downstream tasks can then benefit from an adequate aggregation of the module zoo thanks to the rich knowledge inherited from midstream tasks. There are three major advantages of such a design.
From the efficiency aspect, the upstream backbone can be trained only once and reused for all downstream tasks without tuning. From the scalability aspect, we can easily append additional modules to ViM with no influence on existing modules. From the performance aspect, ViM can include as many midstream tasks as possible, narrowing the task gap between upstream and downstream. Considering these benefits, we believe that ViM, which the community could maintain and develop together, would serve as a powerful tool to assist foundation models. 1.

Introduction
The pretrain-finetune paradigm has served as a gen-eral framework across various vision tasks, where models are pre-trained on large-scale datasets and fine-tuned on downstream tasks [64]. Recent efforts have been attracted to build up a foundation model [54, 73, 80] with large architecture and massive pre-trained data (e.g., in the scale of billions). Considering the trend of scaling up foundation models, it is costly to fine-tune model for different tasks separately, and worthy of solving tasks with single model.
When directly transferred to downstream tasks, foundation models still suffer from the task-gap problem. Downstream tasks may require different targets with the upstream, thus could not fully leverage the pre-learned knowledge. As shown in Figure 1, models are observed with preference
Figure 1. Task preference problem: (a) Single-task pre-trained models tend to perform better on tasks similar to their upstream. (b) Intermediate task fine-tuning further tunes the pre-trained model on specific task, showing similar imbalanced performance. to those tasks similar to their encountered ones. Task preference restricts unified transferring of single foundation model to multiple tasks with varying targets.
To bridge the task-gap, existing works address the problem on either upstream or downstream. Upstream works propose multi-task pre-training to simultaneously learn various types of tasks [18, 23, 28, 49, 72]. However, when extending to a new pre-training task, these works require to re-formulate the task I/O and re-train the model together with existing tasks, which is complicated and time-consuming. Downstream works introduce prompt [34, 46,
87] or adapter-based tuning [5, 8, 15, 31, 35] to adapt to downstream tasks with additional parameters. Since the downstream datasets for transferring are generally small (e.g., in the scale of thousands), the appended parameters might be insufficiently trained to master newly-encountered tasks, usually resulting in unsatisfying performance [21,74].
In this paper, we present a unified framework for sup-porting multiple downstream tasks with single foundation model. After the upstream pre-training, we introduce a collection of midstream tasks based on middle-scale datasets (e.g., in the scale of millions). For each midstream task, a lightweight plug-in module is inserted into the pre-trained backbone, and individually optimized to learn the current task. The foundation model is frozen without any parameter tuning in the above process. Throughout this way, we collect the Vision Middleware (ViM), a module zoo containing knowledge from diverse midstream tasks based on the single foundation model. To fully leverage the knowledge of ViM for downstream transferring, we develop practical strategies to adaptively aggregate ViM modules.
Modules correlated with the downstream task would be emphasized for better transferring. ViM is expected to cope with the task-gap problem with sufficient supervi-sion in midstream, and achieve balanced performance on multiple downstream tasks without preference. During the experiments, we build up a ViM consisting of modules trained from 47 midstream tasks in 12 types, which can be grouped into global recognition, local recognition, vision and language understanding and self-supervised learning.
We then evaluate with the averaged transferring perfor-mance on 30 downstream tasks in 4 types. The experimental results show satisfying performance of ViM with balanced improvements on multiple tasks.
ViM introduces a new paradigm of applying foundation models for unified transferring, which shows the following advantages: (i) For the efficiency, the foundation model is maintained frozen after upstream pre-training, thus without any cost of storing different model parameters for various downstream tasks. (ii) For the scalability, new ViM module can be individually trained and freely appended into the current ViM. The diverse middle-scale datasets from the community also enable us to easily expand ViM into great scale. (iii) For the performance, ViM addresses the task-gap issue via including ViM modules of various midstream tasks, thus can achieve balanced performance for unified downstream transferring. With the above advantages and verified performance, we would like to encourage the com-munity to maintain a public ViM to which all researchers can contribute with their own well-trained ViM modules. 2.