Abstract
Sparsely-gated Mixture of Expert (MoE), an emerging deep model architecture, has demonstrated a great promise to enable high-accuracy and ultra-efﬁcient model inference.
Despite the growing popularity of MoE, little work inves-tigated its potential to advance convolutional neural net-works (CNNs), especially in the plane of adversarial ro-bustness. Since the lack of robustness has become one of the main hurdles for CNNs, in this paper we ask: How to adversarially robustify a CNN-based MoE model? Can we robustly train it like an ordinary CNN model? Our pi-lot study shows that the conventional adversarial training (AT) mechanism (developed for vanilla CNNs) no longer remains effective to robustify an MoE-CNN. To better un-derstand this phenomenon, we dissect the robustness of an MoE-CNN into two dimensions: Robustness of routers (i.e., gating functions to select data-speciﬁc experts) and robustness of experts (i.e., the router-guided pathways de-ﬁned by the subnetworks of the backbone CNN). Our anal-yses show that routers and experts are hard to adapt to each other in the vanilla AT. Thus, we propose a new router-expert alternating Adversarial training framework for MoE, termed ADVMOE. The effectiveness of our pro-posal is justiﬁed across 4 commonly-used CNN model ar-chitectures over 4 benchmark datasets. We ﬁnd that ADV-MOE achieves 1% 4% adversarial robustness improve-ment over the original dense CNN, and enjoys the efﬁciency merit of sparsity-gated MoE, leading to more than 50% in-ference cost reduction. Codes are available at https:// github.com/OPTML-Group/Robust-MoE-CNN .
⇠ 1.

Introduction
Despite the state-of-the-art performance achieved by the outrageously large networks [1–5] in various deep learning
Correspondence to: Yihua Zhang<zhan1908@msu.edu> (DL) tasks, it still remains challenging to train and deploy such models cheaply. A major bottleneck is the lack of parameter efﬁciency [6]: A single data prediction only re-quires activating a small portion of the parameters of the full model. Towards efﬁcient DL, sparse Mixture of Ex-perts (MoE) [7–15] aims to divide and conquer the model parameters based on their optimal responses to speciﬁc in-puts so that inference costs can be reduced. A typical MoE structure is comprised of a set of ‘experts’ (i.e., sub-models extracted from the original backbone network) and ‘routers’ (i.e., additional small-scale gating networks to determine expert selection schemes across layers). During inference, sparse MoE only activates the most relevant experts and forms the expert-guided pathway for a given input data.
By doing so, sparse MoE can boost the inference efﬁciency (see ‘GFLOPS’ measurement in Fig. 1). Architecture-wise, sparse MoE has been used for both CNNs [8, 16] and vision transformers (ViTs) [7, 9–15, 17]. Yet, we will focus on the former since sparse MoE for CNNs is under-explored com-pared to non-sparse MoE for CNNs [18–20], and adversar-ial robustness (another key performance metric of our work) was extensively studied in the context of CNNs.
It is known that a main weakness of DL is the lack of adversarial robustness [21–23]. For example, CNNs can be easily fooled by adversarial attacks [21–23], in terms of tiny input perturbations generated to direct to erroneous predic-tions. Thus, adversarial training (AT) of CNNs has become a main research thrust [24–29]. However, when CNN meets sparse MoE, it remains elusive if the improved inference ef-ﬁciency brought by the sparse MoE comes at the cost of more complex adversarial training recipes. Thus, we ask: (Q) What will be the new insights into adversarial ro-bustness of sparse MoE-integrated CNNs? And what will be the suited AT mechanism?
To our best knowledge, problem (Q) remains open in the literature. The most relevant work to ours is [30], which investigated the adversarial robustness of MoE and lever-Models
Standard Training
Robust Training
Prior Art (AT)
Ours (AdvMoE)
Dense 95.2
ResNet18
S-Dense
MoE-CNN 93.7 94.3 50.1 48.0
" 44.9 51.8 !
Dense 96.2
WRN-28-10
S-Dense
MoE-CNN 94.7 95.3
Dense 93.6
VGG-16
S-Dense 92.2
MoE-CNN 92.9 51.8 49.1 55.7 !
" 46.2 46.2 43.7
" 41.4 49.8 !
Dense 93.1
DenseNet
S-Dense 90.7
MoE-CNN 92.3 44.5 38.1
" 31.4 39.7 !
Metrics
Standard Accuracy
Robust Accuracy (a) Illustration of CNN types considered in this work. (b) Performance overview on CIFAR-10.
Figure 1. (a) Model types (Dense, MoE-CNN, Sparse-CNN, and S(mall)-Dense) considered in this paper; see details in ‘Model setup’ of
Sec. 3. (b) Performance overview using the standard training and the robust training on model architectures in (a), where standard accuracy and robust accuracy are deﬁned by testing accuracy on the benign and adversarial test datasets, respectively. Compared to standard training (results in gray), the conventional AT [25] is no longer effective for MoE-CNN (see results in light purple). This is in contrast to AT for other CNN models (Dense and S-Dense). Different from AT, our proposed ADVMOE can effectively equip MoE-CNN with improved robustness, higher than Dense (see results in orange), without losing its inference efﬁciency (see “GFLOPS”). We refer readers to Sec. 5.1 for more experiment details. aged the ordinary AT recipe [24] to defend against adver-sarial attacks. However, it only focused on the ViT archi-tecture, making a vacancy for the research on robustiﬁca-tion for the sparse MoE-based CNN (termed MoE-CNN in this work). Most importantly, we ﬁnd that the vanilla AT
[24, 25] (widely used to robustify CNNs) is no longer ef-fective for MoE-CNN. Thus, new solutions are in demand.
To address (Q), we need to (1) make careful sanity checks for AT in MoE-CNN, (2) make an in-depth analysis of its failure cases, and (3) advance new AT principles that can effectively improve robustness without losing the gener-alization and efﬁciency from sparse MoE. Speciﬁcally, our contributions are unfolded below.
• We dissect the MoE robustness into two new dimen-sions (different from CNNs): routers’ robustness and experts’ robustness.
Such a robustness dissection brings novel insights into the (in)effectiveness of AT.
• Taking inspiration from the above robustness dissec-tion, we propose a new Adversarial training framework for MoE, termed ADVMOE, which enforces routers and experts to make a concerted effort to improve the overall robustness of MoE-CNN.
• We conduct extensive experiments to demonstrate the effectiveness of ADVMOE across 4 CNN architectures and 4 datasets. For example, ADVMOE outperforms
AT on the original dense CNN model (termed Dense) by a substantial margin: 1% 4% adversarial robust-ness improvement and over 50% reduction of inference overhead; see Fig. 1 for illustrations on different CNN types and highlighted performance achieved.
⇠ 2.