Abstract
Generative Adversarial Networks (GANs), especially the recent style-based generators (StyleGANs), have versatile semantics in the structured latent space. Latent semantics discovery methods emerge to move around the latent code such that only one factor varies during the traversal. Re-cently, an unsupervised method proposed a promising direc-tion to directly use the eigenvectors of the projection matrix that maps latent codes to features as the interpretable direc-tions. However, one overlooked fact is that the projection matrix is non-orthogonal and the number of eigenvectors is too large. The non-orthogonality would entangle semantic attributes in the top few eigenvectors, and the large dimen-sionality might result in meaningless variations among the directions even if the matrix is orthogonal. To avoid these issues, we propose Householder Projector, a flexible and general low-rank orthogonal matrix representation based on Householder transformations, to parameterize the pro-jection matrix. The orthogonality guarantees that the eigen-vectors correspond to disentangled interpretable semantics, while the low-rank property encourages that each identi-fied direction has meaningful variations. We integrate our projector into pre-trained StyleGAN2/StyleGAN3 and eval-uate the models on several benchmarks. Within only 1% of the original training steps for fine-tuning, our projector helps StyleGANs to discover more disentangled and pre-cise semantic attributes without sacrificing image fidelity.
Code is publicly available via https://github.com/
KingJamesSong/HouseholderGAN .
Figure 1: Motivation of our proposed Householder Projec-tor. Here “Projector” denotes the projection matrix that maps latent codes to features, i.e., the modulation weight of StyleGANs. (Top Left) The singular value imbalance of the non-orthogonal projector would entangle multiple se-mantics in the top interpretable directions. (Top Right) Due to the large dimensionality of the projector, directly en-forcing vanilla orthogonality would spread the data varia-tions among all the eigenvectors, leading to imperceptible and meaningless traversal. (Bottom) Our Householder Pro-jector equips the projection matrix with low-rank orthog-onal properties, which simultaneously disentangles seman-tics into multiple equally-important eigenvectors and guar-antees that each direction could correspond to semantically-meaningful variations. 1.

Introduction
Generative Adversarial Networks (GANs) [15] and the recent style-based generative models (StyleGANs) [27, 28,
The first two authors contribute equally; Wei Wang is the correspond-ing author. 26] in particular have become the leading paradigm of gen-erative modeling in the vision domain. The latent spaces of StyleGANs are known to embed rich and hierarchical semantics [14, 21]; moving the latent code in certain di-rections could trigger meaningful variations in the output images. Therefore, latent semantics discovery methods
emerge to identify such interpretable directions that each variation factor is disentangled and the generation process can be precisely controled [14, 47, 42, 61, 48, 1, 58, 37, 8].
Among the recent explorations of unsupervised inter-pretable semantics discovery methods [56, 48, 57, 41],
SeFa [48] pointed out a promising direction to discover se-mantically meaningful concepts by computing the eigen-vector of the projector. Here we refer to the projection matrix that maps latent codes to features as the projector.
The key observation is that using the eigenvectors of the projector for latent perturbation would maximize the data variations. Such identified eigenvectors/directions would correspond to meaningful semantic concepts. However, as shown in Fig. 1 top left, this is likely to cause semantics entangled in the top few eigenvectors. This phenomenon stems from the fact that the variation caused by an eigen-vector is actually determined by the associated eigenvalue.
Due to the imbalanced eigenvalue distribution, the discov-ered directions are not equally-important, and the top few ones would simultaneously manipulate multiple attributes.
This eigenvalue discrepancy can be mitigated by enforc-ing orthogonality constraint to the projector. Nonetheless, since standard orthogonal matrices have as many equally-important eigenvectors as the dimensionality, there would not be enough semantics to mine in practice when the method scales to large models such as StyleGANs whose projector dimension is 1024 or 512. Consequently, as an accompanying limitation, the data variations would be split among all the eigenvectors and none of them could produce meaningful output changes (see Fig. 1 top right).
To resolve the above issues, we propose Householder
Projector, a low-rank orthogonal matrix representation based on Householder transformations, to parameterize the projection matrix that maps latent codes to features. The projector is first decomposed to its SVD form (USVT ).
Next, the orthogonal singular vectors U and V are repre-sented by a series of Householder reflectors, respectively.
Thanks to the normalization of Householder reflections, the orthogonality is also preserved during backpropagation. For the singular value S, we explicitly set it as a low-rank iden-tity matrix (i.e., S=diag(1, . . . , 1, 0, . . . , 0)) whose rank defines exactly the number of semantic concepts. As shown in Fig. 1 bottom, the low-rank property guarantees that the identified directions would cause meaningful variations, while the orthogonality encourages that each semantic at-tribute is disentangled from the others. Moreover, a proper initialization scheme is proposed to leverage the statistics of pre-trained weights, and an acceleration technique is applied to speed up the computation. We also propose a metric dedicated to measuring the smoothness of latent space to interpretable directions based perturbations. Our
Householder Projector is integrated into pre-trained Style-GANs [28, 26] at multiple different layers to mine the di-verse and hierarchical semantics. Since our projector in-evitably changes the pre-trained parameters, the modified models incorporated with our projector are fine-tuned for limited steps to maintain the original image fidelity. Both quantitative and qualitative results on several widely used benchmarks [29, 63, 10, 25, 13] show that within marginal fine-tuning steps (1% of the training steps), our House-holder Projector improves the latent semantics discov-ery of StyleGANs to have more precise attribute control while not impairing the quality of generated images.
The key results and main contributions are as follows:
• We propose Householder Projector, a flexible and gen-eral low-rank orthogonal matrix representation based on
Householder transformations, to parameterize the projec-tor of generative models for latent semantics discovery.
• Our Householder Projector can be easily integrated into pre-trained GAN models. With the acceleration tech-nique, it can be efficiently fine-tuned for very limited ad-ditional steps, which paves the way for applying latent se-mantics discovery and orthogonality techniques to large-scale generative models such as StyleGANs.
• Extensive experiments on two popular backbones (Style-GAN2 [28] and StyleGAN3 [26]) and six benchmarks (FFHQ [29], LSUN Church and Cat [63], MetFaces [25],
AFHQ [10], and SHHQ [13]) demonstrate that within marginal extra fine-tuning steps (1% of the original train-ing steps), our method could both achieve precise at-tribute control and preserve the original image quality. 2.