Abstract
While discriminative correlation filters (DCF)-based trackers prevail in UAV tracking for their favorable effi-ciency, lightweight convolutional neural network (CNN)-based trackers using filter pruning have also demonstrated remarkable efficiency and precision. However, the use of pure vision transformer models (ViTs) for UAV tracking remains unexplored, which is a surprising finding given that ViTs have been shown to produce better performance and greater efficiency than CNNs in image classification.
In this paper, we propose an efficient ViT-based track-ing framework, Aba-ViTrack, for UAV tracking.
In our framework, feature learning and template-search coupling are integrated into an efficient one-stream ViT to avoid an extra heavy relation modeling module. The proposed
Aba-ViT exploits an adaptive and background-aware token computation method to reduce inference time. This ap-proach adaptively discards tokens based on learned halt-ing probabilities, which a priori are higher for background tokens than target ones. Extensive experiments on six
UAV tracking benchmarks demonstrate that the proposed
Aba-ViTrack achieves state-of-the-art performance in UAV tracking. Code is available at https://github.com/ xyyang317/Aba-ViTrack. 1.

Introduction
Unmanned aerial vehicles (UAVs) have been employed in various applications, and recently, UAV tracking has gained considerable attention in visual tracking [37, 4, 66, 67]. However, unlike general visual tracking, UAV track-ing poses unique challenges. Common issues such as ex-treme view angles, motion blur, and severe occlusion can degrade tracking precision. Moreover, the limited battery capacity, computing resources, and low power consumption
*Corresponding author.
Figure 1. Comparison on UAV123. Compared with DCF-based and CNN-based trackers, our efficient ViT-based tracker (Aba-Track) sets a new record with 0.864 precision and still runs ef-ficiently at around 180 f ps. requirements of UAVs impose stringent demands on effi-ciency [5, 66, 67, 34]. Therefore, a good UAV tracker must achieve high precision while remaining high efficiency.
As shown in Fig. 1, UAV tracking methods can be broadly divided into two categories: discriminative corre-lation filters (DCF)-based trackers and deep convolutional neural network (CNN)-based trackers. DCF-based track-ers are favored because of their high efficiency derived from operations in the Fourier domain, but they usually achieve low tracking precision [37, 32, 36, 28]. On the other hand, CNN-based trackers can easily obtain high pre-cision, but are not suitable for high-efficiency demands. To combat low efficiency, some lightweight CNN-based track-ers are proposed for UAV tracking [5, 66, 67] that em-ploy filter pruning to reduce the parameters of SiamFC++
[70] based on Fisher information [67] or rank informa-tion [66, 41], resulting in significant improvements in both precision and efficiency. Very recently, TCTrack [5] has been proposed to utilize temporal contexts to enhance UAV tracking. Different from existing CNN-based tracker, TC-Track is a hybrid deep learning architecture combining
CNN and transformer, where an online temporally adap-tive convolution enhances the spatial features with temporal information, and an adaptive temporal transformer refines similarity map. Despite the success in achieving high pre-cision and efficiency, the precision gain is not matched with consideration cost of speed and heavy temporal information use. An even more surprising finding is that exploring vi-sual transformers for UAV tracking remains unexplored.
In this paper, we make the first attempt to utilize effi-cient Vision Transformers (ViTs) for real-time unmanned aerial vehicle (UAV) tracking. Specifically, we investi-gate the use of efficient ViTs to enhance the feature learn-ing and template-search coupling processes, thereby mak-ing them suitable for real-time UAV tracking. While many lightweight ViTs have been proposed recently through low-rank methods [64], model compression [75, 52, 45], hy-brid design [8, 38], they are not well-suited for our pur-pose for the following reasons. For example, low-rank and quantization-based ViTs often compromise prediction accuracy. Pruning-based ViTs require a time-consuming decision-making process for pruning ratios and subsequent fine-tuning. Hybrid ViTs, which typically employ a CNN-based stem to downsample input images, are unsuitable for our unified framework because the template and search patches have different sizes.
Fortunately, we have efficient Vision Transformers (ViTs) based on conditional computation, such as those pro-posed in [49] and [73], which can dynamically reduce the number of tokens based on the input. Building on the recent work in A-ViT [73], which proposed an adaptive token re-duction mechanism that discards redundant spatial tokens according to dynamical halting probabilities, we present
Aba-ViT, an efficient ViT for UAV tracking. Our method incorporates adaptive and background-aware token compu-tation, which learns halting probabilities that are higher for background tokens than target ones through a more infor-mative loss generalized from A-ViT [73]. By taking into account prior knowledge, Aba-ViT is more effective than
A-ViT for UAV tracking. This is due to its ability to be aware of the background, which is typically filled with po-tential distractors and noise that can pose a significant chal-lenge to tracking algorithms. As background tokens are halted with higher probabilities in Aba-ViT without any ad-ditional computation burden, our method is expected to re-duce the overall compute requirements. As shown in Figure 1, our method sets a new record with a precision of 0.864 and runs efficiently at around 180 frames per second (fps), as compared to DCF- and CNN-based trackers. Extensive experiments on six benchmarks demonstrate that Aba-ViT achieves state-of-the-art performance.
Our contributions can be summarized as follows:
• We make the first attempt to explore using efficient
ViTs, particularly in a unified framework, for real-time
UAV tracking. The significant improvement in track-ing precision with favorable speeds indicates that our effort is very fruitful and worthwhile and may encour-age more work in this direction.
• We propose an efficient ViT, Aba-ViT, which incor-porates adaptive and background-aware token compu-tation. This allows Aba-ViT to learn halting proba-bilities that are a priori higher for background tokens than target ones. Using Aba-ViT as the backbone, we have developed a tracker named Aba-ViTrack, which has proven to be an efficient and effective tracker for real-time UAV tracking.
• Our Aba-ViT sets a new state-of-the-art record on six challenging benchmarks, namely UAV123@10fps
[48], VisDrone2018 [80], UAVDT [17], UAV123 [48],
DTB70 [35], and UAVTrack112 L [20]. 2.