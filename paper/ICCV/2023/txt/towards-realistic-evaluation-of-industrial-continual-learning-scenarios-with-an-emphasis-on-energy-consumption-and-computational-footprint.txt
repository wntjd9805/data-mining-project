Abstract
PODNet
POD-AANet
DER
Joint (CJT)
FOSTER
Incremental Learning (IL) aims to develop Machine
Learning (ML) models that can learn from continuous streams of data and mitigate catastrophic forgetting. We analyse the current state-of-the-art Class-IL implementa-tions and demonstrate why the current body of research tends to be one-dimensional, with an excessive focus on ac-curacy metrics. A realistic evaluation of Continual Learn-ing methods should also emphasise energy consumption and overall computational load for a comprehensive un-derstanding. This paper addresses research gaps between current IL research and industrial project environments, including varying incremental tasks and the introduction of Joint Training in tandem with IL. We introduce InVar-100 (Industrial Objects in Varied Contexts), a novel dataset meant to simulate the visual environments in industrial se-tups and perform various experiments for IL. Additionally, we incorporate explainability (using class activations) to interpret the model predictions. Our approach, RECIL (Real-World Scenarios and Energy Efficiency Considera-tions for Class Incremental Learning) provides meaningful insights about the applicability of IL approaches in practi-cal use cases. The overarching aim is to bring the Incre-mental Learning and Green AI fields together and encour-age the application of CIL methods in real-world scenarios.
Code and dataset are available. 1

Introduction
Advances in Machine Learning (ML) and Computer
Vision have demonstrated the capabilities of deep neural network-based (NN) models to learn from diverse data and
*Correspondence: vivek.chavan@ipk.fraunhofer.de
Dataset: http://dx.doi.org/10.24406/fordatis/266.2
DOI: 10.24406/fordatis/266.2
Code: https://github.com/Vivek9Chavan/RECIL 90 80 70 60 50 3 2 1 0 0 5 10 15 20 40 60 80 100
Top-1 Accuracy (%) vs. Total Classes
Energy Consumption(kWh) vs. Task number
Figure 1: Top-1 Accuracy and Task-wise Energy Consump-tion for ImageNet-Subset for different CIL approaches.
Task 0 introduces 10 classes, and all subsequent tasks add 5 classes. The total energy consumption of an approach is given by the area under the curve. Comparing the methods using only accuracy provides an incomplete understanding; computational footprint consideration is also important. perform a multitude of tasks with high accuracy [15,30,41].
The utilisation of ML in industrial applications is expected to increase substantially [6, 48, 52, 54]. A gradual ramp-up of raw materials, components, and related data occurs in industrial projects with long timelines (e.g. manufactur-ing [17, 29], reverse logistics [2, 57]). This necessitates the retraining of the ML model by sequentially learning from new data streams (tasks).
It is established that NN mod-els tend to forget the information learned from older data as they are retrained on new information; this phenomenon is known as Catastrophic forgetting [21, 27].
Iteratively retraining the model from scratch on the entire appended dataset is not a viable long-term solution, since it would re-sult in a compounding of training times and computational load. Such industrial applications present an opportunity for widespread adoption and implementation of Continual
Learning.
Metric
#Param(M) ↓
#PFLOPs ↓
Time (h) ↓
E (kWh) ↓
Accavg (%) ↑
POD DER 213.1 11.7 5.39 0.55 119.5 54.9 15.08 6.86 72.4 68.6
FOS 22.14 1.06 73.5 6.12 69.8
P-AA CJT 11.2 13.1 31.81 1.97 261.3 179.5 33.29 21.25 83.7 71.1
Table 1: Incremental Learning Results on ImageNet Subset with energy consumption and computational footprint. ↑ indicates higher value is better, ↓ indicates lower value is better.
Developing ML systems that can continuously learn and adapt to new data has been a broad topic of research in Artificial Intelligence (AI) for numerous applications
[10, 25, 26]. In recent years, several implementations have been proposed for combating catastrophic forgetting and incrementally training ML models on new tasks. Van de ven and Tolias [60] identified three scenarios for Incremen-tal Learning, viz. Task-, Domain- and Class-Incremental
Learning (CIL), the last being the most challenging of the three.
CIL can be generalised as an ML problem with a contin-uously growing dataset D, where new classes are introduced sequentially over training tasks 0, 1,...T, each containing new classes C0, C1...CT. The model must be able to classify the test images from all available classes (cid:80)t i=0 Ci at a given phase t of the project life cycle. CIL implementations gener-ally focus heavily on the Top-1 and Top-5 accuracy in stan-dardised benchmarking settings [5,47,60,73]. While this al-lows direct comparison between different implementations, it leads to an overemphasis on the established benchmarks, while neglecting diverse scenarios and other metrics. After a production ML model is trained (Task 0), the subsequent incoming data is likely to contain a significant variation in the number of object classes, amount of data and feature complexities from one task to the next [1, 2, 9, 57].
Additionally, original research works and review papers on IL generally do not expound on the training times, en-ergy consumption or computational complexity. Reduc-tion in training time and lower energy consumption are the key reasons for adopting a continual learning-based frame-work in practice. Theoretically, if the accuracy of predic-tion were the only metric of significance, then retraining the model on the whole dataset (Cumulative Joint Training:
CJT) would be preferred over incremental learning imple-mentations [5, 47, 72]. Thus, the current body of research in this field is lacking and one-dimensional. As shown in
Figure 1 and Table 1, comparing IL approaches only using accuracy metrics is not sufficient.
This study focuses on the gap between current AI re-search and its practical implementation in industry projects.
We advocate for IL research to be extended to practical scenarios with an emphasis on energy consumption and computational footprint. We look at performance metrics and considerations for comparing IL methods comprehen-sively. Industrial ML projects also require maintaining per-formance above a certain threshold, which depends on the requirements, complexity of the problem and available data
[6, 8, 54]. In a continual learning framework, this means in-troducing periodic Joint Training updates (JTupdate) in tan-dem with incremental training. We study the impact of such updates on different state-of-the-art CIL approaches. We also introduce a novel dataset of industrial objects in var-ied contexts, spanning different levels of intra-class visual complexities w.r.t. classification. We study Class Activation
Maps (CAMs) [50, 70] to interpret and understand the pre-diction patterns for the approaches. Our overarching aim is to bring the domains of Green AI and Incremental Learning together and provide the AI community with useful tools for the same. This work also aims to propose methodolo-gies for researchers and AI adopters to assess the suitability of continual learning frameworks for their own use cases. 2