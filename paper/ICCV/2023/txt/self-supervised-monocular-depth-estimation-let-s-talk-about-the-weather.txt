Abstract
Current, self-supervised depth estimation architectures rely on clear and sunny weather scenes to train deep neu-ral networks. However, in many locations, this assump-tion is too strong. For example in the UK (2021), 149 days consisted of rain. For these architectures to be ef-fective in real-world applications, we must create models that can generalise to all weather conditions, times of the day and image qualities. Using a combination of com-puter graphics and generative models, one can augment existing sunny-weather data in a variety of ways that sim-ulate adverse weather effects. While it is tempting to use such data augmentations for self-supervised depth, in the past this was shown to degrade performance instead of im-proving it.
In this paper, we put forward a method that uses augmentations to remedy this problem. By exploiting the correspondence between unaugmented and augmented data we introduce a pseudo-supervised loss for both depth and pose estimation. This brings back some of the bene-fits of supervised learning while still not requiring any la-bels. We also make a series of practical recommendations which collectively offer a reliable, efficient framework for weather-related augmentation of self-supervised depth from monocular video. We present extensive testing to show that our method, Robust-Depth, achieves SotA performance on the KITTI dataset while significantly surpassing SotA on challenging, adverse condition data such as DrivingStereo,
Foggy CityScape and NuScenes-Night. The project website can be found at https://kieran514.github.io/Robust-Depth-Project/. 1.

Introduction
Depth estimation has been a pillar of computer vision for decades and has many applications, such as self-driving, robotics, and scene reconstruction. While multiple-view ge-ometry is a well-understood computer vision problem, the advent of deep learning has enabled depth estimation from a single image. The first such methods used a supervised
Figure 1. Monodepth2 demonstrates impressive performance on sunny scenes but struggles in different weather scenarios. Our method is more robust to environmental changes. approach to estimate depth and required expensive ground truth data collected using LIDAR and Radar sensors. Re-cently, self-supervised monocular methods have been in-troduced, using photometric loss [57] to achieve view syn-thesis on consecutive images as a form of self-supervision.
These methods have received wide interest because of their low cost and ability to generalize to multiple scenarios [22].
However, despite its potential, self-supervised Monocular
Depth Estimation (MDE) has been hampered by adverse weather conditions and nighttime driving [46, 44]. The
KITTI dataset [15], used extensively for MDE, only con-tains daytime, dry, sunny images rather than varied real-istic conditions. Previous attempts have shown that train-ing these methods on different domains, like nighttime data, lead to worse performance [29]. Figure 1 illustrates Mon-odepth2 [17], trained on the KITTI dataset, estimating depth accurately on sunny data but struggles when shifting do-mains to different weather conditions. Our method Robust-Depth, trained with the KITTI dataset and augmentations, is more robust to such changes.
Unlike in other fields of computer vision [55, 35], dataset augmentation has led to worse performance for MDE (see
Table 1). One possible explanation is that augmentations lead to texture shifts, and we know that CNNs are poor at generalising to texture shifts [3]. It is known [10, 34, 46] that depth networks become reliant on vertical cues (e.g., the output for a pixel being dependent on its vertical po-sition in an image). Pixels at the bottom of the image are assumed to be close and pixels at the top are assumed to be far from the camera. While [46] suggests this is beneficial overall, these methods rely on potentially false cues. For ex-ample, a cliff-side image would pose difficulties to a system over-relying on vertical cues. In this work, we put forward a further observation that has to do with self-supervision itself: When performing data augmentation in supervised learning, we introduce some form of noise on the input im-age while maintaining a noise-free label. On the other hand, in self-supervised methods, the labels come from the data themselves. So augmenting a data point in a self-supervised method introduces noise in both data and target labels, lead-ing to a much harder machine learning problem.
To overcome this challenge we propose a different for-mulation of the self-supervised loss, which exploits the alignment between the unaugmented and augmented data.
Under that scheme, we are able to treat depth maps ob-tained from the unaugmented data as soft targets for the aug-mented depth estimations. Furthermore, with minimal extra effort, we can also treat depth maps of augmented images as labels for the unaugmented depth predictions leading to a fully symmetric bi-directional consistency constraint. We call this pseudo-supervision loss because it is an attempt to leverage the benefits of supervised learning (faster learn-ing rates) in the self-supervised domain. For more detail see section 3.2. The paper also puts forward a number of recommendations for creating a robust, data-augmentation framework for MDE, each of which helps overcome the re-liance on simplistic cues for depth. These include:
• Using the unaugmented images when warping the tar-get image with the current depth map (sec. 3.2 eq. (6))
• Training in (unaugmented, augmented) pairs (sec. 3.2 eq. (7))
• Applying a one-way pseudo-supervision loss for pose estimation (sec. 3.2 eq. (13))
Finally, we propose a wide-ranging set of weather-related data augmentations together with vertical cropping and till-ing, the effect of which is to move the focus of the network away from simplistic depth cues and towards deeper se-mantic cues. The paper contains an extensive experimental analysis, including an ablation study of the various algorith-mic components as well as a comparison to State-of-the-Art (SotA). The analysis shows that our method significantly surpasses SotA on adverse weather data and performs as well or better on sunny weather data. 2.