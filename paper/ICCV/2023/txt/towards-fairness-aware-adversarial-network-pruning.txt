Abstract
Network pruning aims to compress models while mini-mizing loss in accuracy. With the increasing focus on bias in AI systems, the bias inheriting or even magnification nature of traditional network pruning methods has raised a new perspective towards fairness-aware network prun-ing. Straightforward pruning plus debias methods and re-cent designs for monitoring disparities of demographic at-tributes during pruning have endeavored to enhance fair-ness in pruning. However, neither simple assembling of two tasks nor specifically designed pruning strategies could achieve the optimal trade-off among pruning ratio, accu-racy, and fairness. This paper proposes an end-to-end learnable framework for fairness-aware network pruning, which optimizes both pruning and debias tasks jointly by adversarial training against those final evaluation metrics like accuracy for pruning, and disparate impact (DI) and
In other words, our equalized odds (DEO) for fairness. fairness-aware adversarial pruning method would learn to prune without any handcraft rules. Therefore, our approach could flexibly adapt to variate network structures. Exhaus-tive experimentation demonstrates the generalization ca-pacity of our approach, as well as superior performance on pruning and debias simultaneously. To highlight, the proposed method could preserve the SOTA pruning per-formance while significantly improving fairness by around 50% as compared to traditional pruning methods. 1.

Introduction
With the massive growth of parameters in nowadays deep models, pruning techniques [10, 8, 7] have achieved appealing reductions in network memory footprint and time complexity. However, they tend to overlook the bias hid-‡Corresponding author.
Figure 1: Can existing pruning methods improve model fairness while maintaining accuracy? We use the area of a circle to represent the size of a model after pruning, i.e., the smaller the area, the more compact the model. Towards the bottom-right corner of the figure, it represents a more accurate and fair model. Please note that the SOTA net-work pruning method SGDP [28] (pink) achieves high ac-curacy while lagging far behind in fairness on CelebA [21] dataset. Even with enhanced fairness via Adversarial De-biasing [42] before pruning, existing pruning methods (yel-low) seriously degrade the fairness. Our method (green) sig-nificantly improves fairness and preserves a relatively high accuracy even with a high pruning ratio. den behind high-accuracy predictions [2, 27, 24]. Thus, it is critical to improving fairness in network pruning for broad and reliable applications of AI systems. Intuitively, fairness could be considered as postprocessing after pruning, but it would come to a suboptimal solution since disparate ob-jectives of these two tasks. Therefore, optimizing fairness during the pruning process would be a promising research direction, which motivates our work toward this track.
Various pruning techniques have been proposed to min-imize degradation in accuracy after network pruning [3, 7,
37] while seldom focusing on the improvement in model fairness. We have demonstrated in Figure 1 that existing pruning methods do not consider fairness, i.e., compressed models via SOTA pruning approach [28] (green) achieve high accuracy but present strong biases against sensitive at-tributes. Even if we enhance the model fairness in advance, the compressed models (purple) still suffer from unfair-ness with significant accuracy degradation. Current pruning techniques tend to pursue high accuracy with a high pruning ratio but ignore inherent unfairness in deep models.
Method
PR.
ACC↑
DEO↓
Normal training
FairGRAPE [19]
Ours 0% 81.63% 80% 80.04% 0.5155 (-0.0074) 80% 78.06% 0.3390 (-0.1839) 0.5229
Method
PR.
ACC↑
DI↑
Normal training
FairGRAPE [19]
Ours 0% 81.63% 80% 80.04% 0.2315 (-0.0829) 80% 79.96% 0.4560 (+0.1416) 0.3144
Table 1: Comparison of our method and FairGRAPE on ac-curacy and demographic fairness. PR. denotes the number of parameters to be pruned.
Some recent works have started to explore fairness en-hancement during pruning. Lin et al. [19] propose Fair-GRAPE to reduce the disparity in performance degrada-tion on different sub-groups caused by pruning while con-tributing less to demographic fairness as shown in Tab. 1.
Wu et al. [38] took pruning as a tool to improve model fair-ness. However, there is still space for improvement, since they targeted medical images that may require strong prior knowledge to design the method.
Above all, it is imperative to propose an effective to ame-liorate fairness and preserve the accuracy and efficiency of network pruning. Since it is difficult to train a small sub-network from scratch to achieve the same performance as its dense counterpart [34, 5, 22], the practical solution is to reduce a large-scale network with redundant and biased parameters to a compact and unbiased sub-network. The main challenge lies in searching for biased and redundant connections and improving model fairness while not hurt-ing the accuracy of the pruned model.
In this paper, we propose the fairness-aware pruning technique to improve fairness and preserve the accuracy and efficiency of compressed models. To achieve this, we guide the pruning process to decide which connections to prune in terms of parameter redundancy and model bias. The key idea is to formulate the pruning step as adversarial learning between fairness and performance. Specifically, we design a discriminator to distinguish predictions from one sensitive group against others. During the training process, the dis-criminator is trained to remove the correlation between pre-diction and sensitive attribute while the pruning step is to train the sub-network to deceive the discriminator, thus ac-complishing fairness-aware pruning in one shot. Exhaustive experimental evaluation demonstrates that our compressed networks simultaneously ameliorate fairness and maintain comparable accuracy and efficiency.
Recently, Ramanujan et al. [28] found the existence of hidden sub-networks with high benign accuracy within ran-domly initialized networks and Sehwag et al. [33] and Fu et al. [6] extend the finding to sub-networks with robust ac-curacy. Using our pruning technique, we further extend the finding to model fairness, where we uncover fair sub-networks within randomly initialized networks without any model training. This indicates that searching for the loca-tions of a subset of weights within a randomly initialized network might be potentially as effective as adversarially debiasing weight values in comparable model sizes, which opens up a new respective for understanding model fairness.
In summary, the main contributions are in three-folds:
• We propose the fairness-aware pruning technique, which designs a discriminator to distinguish the cor-relation between predictions and fairness-related at-tributes. The pruning step is trained adversarially with a discriminator. This design effectively ameliorates fairness, achieves efficiency, and preserves accuracy on par with comparable-sized models.
• Exhaustive experiment validates the superior perfor-mance of our method. The compressed networks out-perform the state-of-art pruning techniques on fairness and achieve comparable performance on accuracy.
• The proposed method can effectively search a fair sub-network from a randomly initialized network without any training. This finding would open up a new per-spective on model fairness. 2.