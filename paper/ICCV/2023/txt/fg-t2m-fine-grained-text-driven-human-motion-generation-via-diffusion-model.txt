Abstract
Text-driven human motion generation in computer vi-sion is both significant and challenging. However, cur-rent methods are limited to producing either deterministic or imprecise motion sequences, failing to effectively con-trol the temporal and spatial relationships required to con-form to a given text description. In this work, we propose a fine-grained method for generating high-quality, condi-tional human motion sequences supporting precise text de-scription. Our approach consists of two key components: 1) a linguistics-structure assisted module that constructs ac-curate and complete language feature to fully utilize text information; and 2) a context-aware progressive reasoning module that learns neighborhood and overall semantic lin-guistics features from shallow and deep graph neural net-works to achieve a multi-step inference. Experiments show that our approach outperforms text-driven motion genera-tion methods on HumanML3D and KIT test sets and gener-ates better visually confirmed motion to the text conditions. 1.

Introduction
Human motion generation is a crucial task in computer vision with various applications in animation production, gaming, robot control, and movie script visualization. Ob-taining human motion sequences through traditional soft-ware is a labor-intensive and tedious process, while motion capture is complex and expensive. Recently, with the ad-vancements in deep learning and computer vision, learning-based human motion generation has emerged as a solution to this problem, leading to the development of associated generation methods based on multimodal data. The input multimodal data include music [12, 15, 25, 27], motion cat-*Corresponding author.
Figure 1: Our approach generates human motion sequences that grasp fine-grained details. egories [4, 9, 20], text [2, 3, 5, 7, 8, 16, 21, 23, 28, 33], among others. Text-driven human motion generation has been a popular research topic, because of its convenience and human-friendliness.
In particular, natural language comprises nouns, verbs, adverbs, etc. The mutual connec-tions among different words in a sentence establish its se-mantics. Verbs define the action’s category, while adverbs control the fineness of the action. The interaction between words in syntax plays a vital role in determining the struc-ture and meaning of a sentence. Failure to fully incorporate these text features may result in inadequate text modeling, causing the generated motion sequence to deviate from the intended meaning of the original text.
Existing methods can be divided into two branches, in-cluding 1) cross-modal alignment of motion and text [2, 3, 5, 7, 8, 16, 21]; 2) conditional diffusion models [28, 33]. In the first methods, text sequences and motion sequences are mapped onto separate feature spaces and forcibly aligned, leading to a loss of original information from both domains.
In the second methods, the diffusion model incorporates
text information as a conditioning factor to learn the proba-bility mapping of human motions. However, the model in-teracts with only one text feature at each time step of the in-ference process, lacking a progressive approach. Moreover, text modeling only involves simple manipulation, which ignores the importance of certain fine-grained words and leads to incomplete semantic understanding, making it chal-lenging to learn focus points at each step. Overall, existing methods only use text information to a limited extent, which in turn affects the accuracy of motion generation based on the corresponding text content, especially for the motion in which texts contain fine-grained words. For instance, com-prehending the sentence “A man is walking forward while waving his right hand” can be a difficult task, and expecting the model to grasp the fine-grained meaning of the terms
“while” and “right hand” is even more demanding.
To tackle the aforementioned issues, we propose a fine-grained text-driven method for generating human motion sequences that precisely align with text prompts in Figure 1.
Typically, people initially read a sentence to gain an overall semantic understanding before focusing on the fine-grained details of individual words. To replicate this process, our method includes a linguistics-structure assisted module and a context-aware progressive reasoning module to fully uti-lize text information. Firstly, we utilize linguistics structure to facilitate information exchange between each text word.
We use dependency parsing [19] to analyze the relation-ships among words in each sentence and construct a depen-dency tree, allowing each node to effectively communicate based on its dependent nodes and relationships. Then, the dependency tree nodes are passed to multi-layer graph neu-ral networks to learn information aggregation. The multi-layer graph neural networks allow shallow network to learn neighborhood features as it can comprehend nearby details, and allow deep network to grasp overall semantic features because it is capable of aggregating information from en-tire nodes. Additionally, our GAT captures rich inter-word relationships while preserving the text linguistics structure by designing adaptive weights for each part-of-speech and dependency relation due to their distinctive role in the Text to Motion (T2M) task.
Secondly, achieving the purpose of fine-grained interac-tion, context-aware progressive reasoning module performs a multi-step inference process with the progressive fusion of global and local information between text and motion, which is unprecedented in the T2M task. This involves uti-lizing hierarchical semantic features to simulate the way hu-mans comprehend sentences. We adopt the diffusion model framework and stack the hierarchical semantic features ob-tained from deep to shallow networks at each step to cap-ture high-order relationships at different semantic levels.
We evaluate our method on HumanML3D dataset [7] and
KIT dataset [22]. Experiments show that our approach out-performs the state-of-the-art methods and generates better visual motion. Our main contributions include:
• To the best of our knowledge, we are the first to bring
NLP methods into T2M task. Utilizing the structured understanding of the natural language prompts to help
T2M models achieve better reasoning skills, which brings new ideas for the text-to-X community from a textual perspective.
• We propose the Linguistics-Structure Assisted Module (LSAM), which utilizes a dependency parsing tree and graph networks to facilitate effective information ex-change and data aggregation. It can obtain both neigh-borhood and overall semantic linguistic features.
• We propose a Context-Aware Progressive Reasoning
Module (CAPR) that implements a multi-step pro-gressive inference strategy within the diffusion model framework, mimicking the human reading process by moving from global to local relationships.
• Experimental results demonstrate that our proposed method outperforms previous methods, and achieves competitive performance on the HumanML3D and
KIT datasets. 2.