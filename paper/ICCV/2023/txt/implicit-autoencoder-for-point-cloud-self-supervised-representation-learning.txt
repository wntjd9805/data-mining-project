Abstract
This paper advocates the use of implicit surface repre-sentation in autoencoder-based self-supervised 3D repre-sentation learning. The most popular and accessible 3D representation, i.e., point clouds, involves discrete samples of the underlying continuous 3D surface. This discretization process introduces sampling variations on the 3D shape, making it challenging to develop transferable knowledge
In the standard autoencoding of the true 3D geometry. paradigm, the encoder is compelled to encode not only the 3D geometry but also information on the specific discrete sampling of the 3D shape into the latent code. This is because the point cloud reconstructed by the decoder is considered unacceptable unless there is a perfect mapping between the original and the reconstructed point clouds.
This paper introduces the Implicit AutoEncoder (IAE), a simple yet effective method that addresses the sampling variation issue by replacing the commonly-used point-cloud decoder with an implicit decoder. The implicit decoder reconstructs a continuous representation of the 3D shape, independent of the imperfections in the discrete samples.
Extensive experiments demonstrate that the proposed IAE achieves state-of-the-art performance across various self-supervised learning benchmarks. Our code is available at https://github.com/SimingYan/IAE. 1.

Introduction
The rapid advancement and growing accessibility of commodity scanning devices have made it easier to capture vast amounts of 3D data represented by point clouds. The success of point-based deep networks [46, 47] additionally enables many 3D vision applications to exploit this natural and flexible representation. Exciting results have emerged in recent years, ranging from object-level understanding, including shape classification [5] and part segmentation [55, 75, 77], to scene-level understanding, such as 3D object detection [11, 17, 56] and 3D semantic segmentation [3].
Annotating point-cloud datasets is a highly labor-Figure 1: The Sampling Variation Problem. Given a continuous 3D shape, there are infinitely many ways to sample a point cloud.
The proposed Implicit AutoEncoder (IAE) learns a latent represen-tation of the true 3D geometry independent of the specific discrete sampling process. By alleviating the sampling variation problem,
IAE improves existing point-cloud self-supervised representation learning methods in various downstream tasks. intensive task due to the challenges involved in designing 3D interfaces and visualizing point clouds. As a result, researchers are motivated to explore self-supervised rep-resentation learning paradigms. The fundamental concept is to pre-train deep neural networks on large unlabeled datasets and fine-tune them on smaller datasets annotated based on specific task requirements. A carefully designed self-supervised representation learning paradigm effectively initializes the network weights, enabling fine-tuning on downstream tasks (e.g., classification and segmentation) to avoid weak local minima and achieve improved stabil-ity [15].
Research in self-supervised representation learning pre-dates the history of 3D computer vision. Notably, significant effort has been dedicated to developing self-supervised learning methods for 2D images [6,12,31,41,69,82,83], with autoencoders being one of the most popular tools [4, 21, 41, 59,63]. An autoencoder-based self-supervised representation learning pipeline comprises an encoder that transforms the input into a latent code and a decoder that expands the
latent code to reconstruct the input. Since the latent code has a much lower dimension than the input, the encoder is encouraged to summarize the input by condensed latent features with the help of the reconstruction loss.
There is a growing interest in developing self-supervised representation learning methods for point clouds by drawing inspiration from image-based autoencoders. For example,
Yang et al. [76] propose a novel folding-based decoder, and
Wang et al. [66] develop a denoising autoencoder based on the standard point-cloud completion model. However, to the best of our knowledge, prior works in point-cloud self-supervised representation learning have relied on the same design principles as image-based methods, where both the encoder and decoder represent the 3D geometry in the same format (i.e., point clouds).
Point clouds are noisy, discretized, and unstructured representations of 3D geometry. As shown in Figure 1, a 3D shape can be represented by many different point clouds, all of which are valid representations of the same object. Different point cloud samples are subject to different noises, which are induced from various sources such as the intrinsic noises from the sensors and the interference from the environment. The unordered and noisy nature distinguishes point clouds from conventional structured data, such as pixel images defined on rectangular grids. When training a point-cloud autoencoder, the encoder is forced to capture sampling variations, limiting the modelâ€™s ability to extract valuable information about the true 3D geometry.
This paper, for the first time, formalizes the concept of sampling variations and proposes to combine the implicit surface representation with point-cloud self-supervised repre-sentation learning. Specifically, we introduce an asymmetric point-cloud autoencoder scheme, where the encoder takes a point cloud as input, and the decoder uses the implicit function as the output 3D representation. The rest of the paper refers to this design as the Implicit AutoEncoder (IAE). IAE enjoys many advantages over existing meth-ods. First, reconstruction under the implicit representation discourages latent space learning from being distracted by the imperfections brought by sampling variations and encourages the encoder to capture generalizable features from the true 3D geometry. Second, the reconstruction loss defined on implicit functions bypasses the expensive and unstable data association operation commonly used in point-based reconstruction losses such as the Earth Mover
Distance [52] and the Chamfer Distance [16]. The added efficiency allows IAE to process up to 40k input points with a single Tesla V100 GPU, making it possible to capture very fine geometric details when existing methods can only work with sparse data with approximately 1k points.
To demonstrate the effectiveness of IAE, we conduct experiments to verify that the learned representation from our pre-trained model can adapt to various downstream tasks, both at the object level and the scene level, including shape classification, linear evaluation, object detection, and indoor semantic segmentation. IAE consistently outperforms state-of-the-art methods in all settings. Specifically, under the best setting, IAE achieves 88.2% / 94.3% classification accuracy on ScanObjectNN [60] / ModelNet40 [68] respectively.
IAE is also the first to support the autoencoding paradigm in scene-level pre-training for the application to various scene-level downstream tasks. For example, compared to training from scratch, IAE achieves +0.7% and +1.1% absolute improvements in object detection quality evaluated by mAP@0.5 on ScanNet [11] and SUN RGB-D [56], respectively.
We summarize the contributions of our paper as follows:
- We propose an asymmetric point-cloud autoencoder called Implicit AutoEncoder (IAE). The IAE takes a point cloud as input and uses an implicit function as the output 3D representation. We combine the implicit surface representation with point-cloud self-supervised representation learning for the first time.
- We formalize the concept of sampling variations in point clouds and demonstrate that IAE is more effective at capturing generalizable features from true 3D geometry than standard point-cloud autoencoders.
- We conduct experiments to demonstrate the effectiveness of IAE on various downstream tasks, including shape classification, object detection, and indoor semantic segmentation. IAE consistently outperforms state-of-the-art methods in all settings. 2.