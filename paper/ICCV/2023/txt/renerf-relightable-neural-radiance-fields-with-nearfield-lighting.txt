Abstract
Recent work on radiance fields and volumetric inverse rendering (e.g., NeRFs) has provided excellent results in building data-driven models of real scenes for novel view synthesis with high photorealism. While full control over viewpoint is achieved, scene lighting is typically “baked” into the model and cannot be changed; other methods only capture limited variation in lighting or make restrictive as-sumptions about the captured scene. These limitations pre-vent the application on arbitrary materials and novel 3D
In this pa-environments with complex, distinct lighting. per, we target the application scenario of capturing high-fidelity assets for neural relighting in controlled studio con-ditions, but without requiring a dense light stage. Instead, we leverage a small number of area lights commonly used in photogrammetry. We propose ReNeRF, a relightable ra-diance field model based on the intuitive and powerful ap-proach of image-based relighting, which implicitly captures global light transport (for arbitrary objects) without com-plex, error-prone simulations. Thus, our new method is simple and provides full control over viewpoint and light-ing, without simplistic assumptions about how light inter-acts with the scene. In addition, ReNeRF does not rely on the usual assumption of distant lighting – during training, we explicitly account for the distance between 3D points in the volume and point samples on the light sources. Thus, at test time, we achieve better generalization to novel, contin-uous lighting directions, including nearfield lighting effects. 1.

Introduction
Neural 3D scene representations such as Neural Radi-ance Fields (NeRFs) and volumetric neural rendering [28, 29] have recently demonstrated the ability to model the real 3D world with impressive ease and photorealism, making this task as easy as capturing a short video using a smart
*Now at Google
Figure 1. We propose ReNeRF, a relightable neural radiance field trained on multi-view imagery captured under simple OLAT area lights (left). Once trained, our method allows re-rendering under novel viewpoints and illumination conditions, including both en-vironment maps and nearfield light sources (right). phone. NeRFs can synthesize novel images from controlled and arbitrary viewpoints, thus enabling virtual fly-throughs within captured scenes [24, 46, 47]. In addition, these tech-niques are highly valuable in many applications (e.g., fea-ture films, games, VR, AR, telepresence), whose virtual en-vironments can be populated with photoreal 3D assets sim-ply by photographing the real world. In this paper, we target the application scenario of capturing such high-fidelity ren-dering assets within controlled studio conditions, leveraging existing photogrammetry hardware.
NeRFs represent 3D objects and scenes using neural net-works (multilayer perceptrons, or MLPs) to model a contin-uous look-up function that encodes the density and color at any 3D point within the observed volume of space. As a key limitation, the scene lighting is typically fixed (“baked”) into the model’s colors and cannot be altered. As such, a model captured within a particular lighting environment (e.g., in studio) cannot be rendered photorealistically within another environment that has its own, distinct lighting con-ditions (e.g., in a feature film). Therefore, we require the ability to photorealistically relight these neural field mod-els in order to extend their usefulness to a larger variety of applications. Although there have already been attempts at making NeRFs relightable (see Section 2), the techniques proposed so far provide limited control over lighting, pho-torealism, generalization to different scene materials and il-lumination, or ease of capture and training. Most previous methods further break down in the presence of nearfield il-lumination changes, which pose additional challenges over the traditional assumption of distant illumination.
To address these limitations, we present a technique for building high-quality, relightable neural radiance fields (ReNeRFs) from images captured in studio conditions, that can achieve full control over camera viewpoint while re-lighting for any illumination, including nearfield lighting.
Our method is inspired by the simplicity and effectiveness of image-based relighting (IBRL) [35, 13], a data-driven ap-proach to model complex global illumination in scenes with arbitrary materials, without explicit light transport simula-tions.
IBRL exploits light superposition to photorealisti-cally perform relighting by linearly combining a discrete number of basis images, captured under one-light-at-a-time (OLAT) conditions. While IBRL is essentially 2D and as-sumes distant lighting and fixed viewpoints, our ReNeRF method is fully 3D and supports novel views while ex-plicitly accounting for the distance between 3D points in the scene and on the light sources, thus capturing nearfield lighting effects. In addition, ReNeRF learns to nonlinearly interpolate a continuous OLAT basis at variable distances, effectively performing OLAT super-resolution and general-izing to novel lighting conditions. We show how this model can be learned using only a few area light sources that are often employed for in-studio photogrammetry, therefore not requiring complex and dense light stage setups. 2.