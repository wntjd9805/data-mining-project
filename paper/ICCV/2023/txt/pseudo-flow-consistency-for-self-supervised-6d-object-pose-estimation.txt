Abstract
Most self-supervised 6D object pose estimation methods can only work with additional depth information or rely on the accurate annotation of 2D segmentation masks, limiting their application range. In this paper, we propose a 6D ob-ject pose estimation method that can be trained with pure
RGB images without any auxiliary information. We first ob-tain a rough pose initialization from networks trained on synthetic images rendered from the target’s 3D mesh. Then, we introduce a refinement strategy leveraging the geome-try constraint in synthetic-to-real image pairs from multi-ple different views. We formulate this geometry constraint as pixel-level flow consistency between the training images with dynamically generated pseudo labels. We evaluate our method on three challenging datasets and demonstrate that it outperforms state-of-the-art self-supervised methods sig-nificantly, with neither 2D annotations nor additional depth images. 1.

Introduction
The goal of 6D object pose estimation is to accurately estimate the 3D rotation and 3D translation of a rigid ob-ject with respect to the camera, which gives essential infor-mation about the world beyond classical 2D understanding and is a fundamental component in many applications, such as robotic manipulation [5], autonomous driving [36], and augmented reality [37].
Recent progress in this field has significantly improved the robustness and accuracy of the model [49, 60, 21, 56, 7, 29, 19, 18]. Most of these approaches, however, rely on a large number of real images with accurate 6D pose an-notations. But, compared to classical 2D annotation, these 6D annotations are either very hard to obtain [38, 34] or are prone to contain large labeling errors [16, 11, 58]. Some recent methods propose to use techniques based on image synthesis [15] or self-supervised learning [55, 54] to handle this problem. The main problem with synthetic images is the large domain gap to the real images, making the model’s generalization ability suffers in practice [42, 60]. On the other hand, most self-supervised methods rely on additional (a) Existing self-supervised 6D object pose methods (b) Our solution based on pseudo flow consistency
Figure 1. Comparison of self-supervised object pose methods. (a) Most existing self-supervised object pose methods rely on ei-ther the depth image [55, 54, 4, 31] or additional mask annota-tions [61, 46], limiting their application range. (b) By contrast, our method can be trained only with the guidance of flow consis-tency based on the intrinsic geometry constraint of multiple differ-ent views, and produces more accurate results than existing solu-tions, without relying on any auxiliary information. information. Some can only work with additional depth im-ages [55, 54, 31, 4] or others need pixel-level annotation of a segmentation mask [46, 61], which prevents the general applicability, as shown in Fig. 1.
In this work, we propose a self-supervised framework for 6D object pose estimation, which relies on neither depth nor additional 2D annotations. We first generate a synthetic dataset based on rendered images from the target’s 3D mesh and train networks only on this dataset to get a rough pose initialization. To close the domain gap between the syn-thetic and real data, we use a refinement strategy where we compare the rendered reference image according to the ini-tial pose and the real input based on pseudo labels [47, 62].
Pseudo labeling is widely used in many computer vision tasks [52, 59, 12, 8]. However, the two fundamental prob-lems of pseudo labeling are still open questions in 6D object pose estimation, including the generative strategy of creat-ing pseudo labels and the selection strategy of extracting high-quality labels from the noisy candidates, as shown in
Fig. 2.
(a) The standard strategy for self-supervised classification (b) The proposed strategy for self-supervised object pose estimation
Figure 2. Self-supervised strategies in different fields. (a) Teacher-student learning scheme is a classical framework for self-supervised classification [52]. The key is how to determine the quality of pseudo labels from the noisy prediction of the teacher network. For image classification, one can obtain the prediction quality by the output distribution after the softmax operation easily, which is usually implemented by checking if the probability of any class is above a threshold [47, 62]. (b) However, there is no such easy way to determine the quality of an object pose prediction without the ground truth. We propose to formulate pseudo object pose labels as pixel-level optical flow supervision signals, and then use the flow consistency between multiple views based on their underlying geometry constraint.
We propose to formulate the pseudo 6D pose labels as pixel-level flow supervision signals in a render-and-compare framework [16, 26, 29, 60, 21, 32, 9]. Unlike the common render-and-compare frameworks that need accu-rate pose annotations, we propose a geometry-guided learn-ing framework without any annotations. We render multi-ple images near the initial pose, and compare them with the real input with the guidance of flow consistency, based on the geometry constraints between these image pairs from different views. We choose high-quality flow labels based on the proposed consistency on the fly, and supervise the network training with these dynamically generated labels in every training step.
We evaluate our method on three challenging datasets
LINEMOD [14], Occluded-LINEMOD [24], and YCB-V [58], and show that it outperforms state-of-the-art self-supervised methods significantly, including those methods relied on depth image [55, 54] or auxiliary annotation infor-mation [61, 46].
Our contributions can be summarized as the following.
First, we investigate the problem of the standard teacher-student methods in selecting high-quality pseudo labels for self-supervised object pose estimation. Second, we propose a strategy based on flow consistency that embeds the ge-ometry constraint from multiple views. Finally, we demon-strate its effectiveness by significantly outperforming state-of-the-art self-supervised object pose methods, without re-lying on any auxiliary information. 2.