Abstract 1.

Introduction
We propose a novel, object-agnostic method for learn-ing a universal policy for dexterous object grasping from realistic point cloud observations and proprioceptive infor-mation under a table-top setting, namely UniDexGrasp++.
To address the challenge of learning the vision-based policy across thousands of object instances, we propose
Geometry-aware Curriculum Learning (GeoCurriculum) and Geometry-aware iterative Generalist-Specialist Learn-ing (GiGSL) which leverage the geometry feature of the task and significantly improve the generalizability. With our pro-posed techniques, our final policy shows universal dexter-ous grasping on thousands of object instances with 85.4% and 78.2% success rate on the train set and test set which outperforms the state-of-the-art baseline UniDexGrasp by 11.7% and 11.3%, respectively.
*Equal contribution.
†Corresponding author.
Robotic grasping is a fundamental and extensively stud-ied problem in robotics, and it has recently gained broader attention from the computer vision community. Recent works [61, 6, 17, 23, 65, 16, 12] have made significant progress in developing grasping algorithms for parallel grippers, using either reinforcement learning or motion planning. However, traditional parallel grippers have lim-ited flexibility, which hinders their ability to assist humans in daily life.
Consequently, dexterous grasping is becoming more im-portant, as it provides a more diverse range of grasping strategies and enables more advanced manipulation tech-niques. The high dimensionality of the action space (e.g., 24 to 30 degrees of freedom) of a dexterous hand is a key advantage that provides it with high versatility and, at the same time, the primary cause of the difficulty in executing a successful grasp. What’s more, the complex hand articula-tion significantly degrades motion planning-based grasping
methods, making RL the mainstream of dexterous grasping.
However, it is very challenging to directly train a vision-based universal dexterous grasping policy [36, 37, 39, 58].
First, vision-based policy learning is known to be difficult, since the policy gradients from RL are usually too noisy to update the vision backbone. Second, such policy learning is in nature a multi-task RL problem that carries huge varia-tions (e.g., different geometry and poses) and is known to be hard [39, 28, 58]. Despite recent advancements in reinforce-ment learning (RL) [4, 2, 36, 8, 9, 48, 40, 26, 37, 57, 67]that have shown promising results in complex dexterous manip-ulation, the trained policy cannot easily generalize to a large number of objects and the unseen. At the same time, most
[4, 2, 67, 9, 57, 48, 26] assume the robot knows works all oracle information such as object position and rotation, making them unrealistic in the real world.
A recent work, UniDexGrasp [68], shows promising re-sults in vision-based dexterous grasping on their benchmark that covers more than 3000 object instances. Their pol-icy only takes robot proprioceptive information and realis-tic point cloud observations as input. To ease policy learn-ing, UniDexGrasp proposes object curriculum learning that starts RL with one object and gradually incorporates similar objects from the same categories or similar categories into training to get a state-based teacher policy. After getting this teacher policy, they distill this policy to a vision-based
It finally achieves 73.7% and policy using DAgger [50]. 66.9% success rates on the train and test splits. One limi-tation of UniDexGrasp is that its state-based teacher policy can only reach 79.4% on the training set, which further con-strains the performance of the vision-based student policy.
Another limitation in the object curriculum is unawareness of object pose and reliance on category labels.
To overcome these limitations, we propose UniDex-Grasp++, a novel pipeline that significantly improves the performance of UniDexGrasp. First, to improve the per-formance of the state-based teacher policy, we first propose
Geometry-aware Task Curriculum Learning (GeoCurricu-lum) that measures the task similarity based on the ge-ometry feature of the scene point cloud. To further im-prove the generalizability of the policy, we adopt the idea of generalist-specialist learning [62, 38, 22, 28] where a group of specialists is trained on the subset of the task space then distill to one generalist. We further pro-pose Geometry-aware iterative Generalist-Specialist Learn-ing GiGSL where we use the geometry feature to de-cide which specialist handles which task and iteratively do distillation and fine-tuning. Our method yields the best-performing state-based policy, which achieves 87.9% and 83.7% success rate on the train set and test set. Then we distill the best-performing specialists to a vision-based gen-eralist and do GiGSL again on vision-based policies until it reaches performance saturation. With our full pipeline, our final vision-based policy shows universal dexterous grasp-ing on 3000+ object instances with 85.4% and 78.2% suc-cess rate on the train set and test set that remarkably outper-forms the state-of-the-art baseline UniDexGrasp by 11.7% and 11.3%, respectively. The additional experiment on
Meta-World [71] further demonstrates the effectiveness of our method which outperforms the previous SOTA multi-task RL methods. 2.