Abstract
Sign Language Translation (SLT) is a challenging task due to its cross-domain nature, involving the translation of visual-gestural language to text. Many previous meth-ods employ an intermediate representation, i.e., gloss se-quences, to facilitate SLT, thus transforming it into a two-stage task of sign language recognition (SLR) followed by sign language translation (SLT). However, the scarcity of gloss-annotated sign language data, combined with the information bottleneck in the mid-level gloss representa-tion, has hindered the further development of the SLT task. To address this challenge, we propose a novel Gloss-Free SLT based on Visual-Language Pretraining (GFSLT-VLP), which improves SLT by inheriting language-oriented prior knowledge from pre-trained models, without any gloss annotation assistance. Our approach involves two (i) integrating Contrastive Language-Image Pre-stages: training (CLIP) with masked self-supervised learning to create pre-tasks that bridge the semantic gap between vi-sual and textual representations and restore masked sen-tences, and (ii) constructing an end-to-end architecture with an encoder-decoder-like structure that inherits the param-eters of the pre-trained Visual Encoder and Text Decoder from the ﬁrst stage. The seamless combination of these novel designs forms a robust sign language representation and signiﬁcantly improves gloss-free sign language trans-lation. In particular, we have achieved unprecedented im-provements in terms of BLEU-4 score on the PHOENIX14T dataset (≥+5) and the CSL-Daily dataset (≥+3) com-pared to state-of-the-art gloss-free SLT methods. Further-more, our approach also achieves competitive results on the PHOENIX14T dataset when compared with most of the gloss-based methods1.
*Benjia Zhou and Zhigang Chen contributed equally to this paper.
†Corresponding author. 1https://github.com/zhoubenjia/GFSLT-VLP
Sign Language 
Video
Mid-level  representation: 
Gloss
NACH (after)
MITTAG  (midday)
STARK (strong)
WEHEN (blow)
Sign Language 
Sentence und dazu kommt nachmittags dieser starke südwind . (and then there is this strong southerly wind in the  afternoon.)  (a) Gloss-based approach.
SLR
SLT
Sign Language 
Video
Language-oriented visual representation
Sign Language 
Sentence und dazu kommt nachmittags dieser starke südwind . (and then there is this strong southerly wind in the  afternoon.) 
SLT (b) Gloss-free approach (ours).
Figure 1: Comparison of Two SLT Approaches: (a) incorporating gloss sequences as intermediates, e.g.,
Sign2Gloss2Text (directly) and Sign2Text (indirectly), and (b) excluding gloss information throughout the training and inference process. 1.

Introduction
Sign language is the main medium of communication among deaf people. To facilitate effective communica-tion with hard-of-hearing people, developing Sign Lan-guage Translation (SLT) techniques is a promising direc-tion. SLT refers to translating sign language into ﬂuent spoken language sentences, which is more challenging than traditional Natural Machine Translation (NMT) due to its
cross-domain translation nature and the scarcity of anno-tated data.
Recently, a growing body of literature [4, 41, 11, 6, 5] has promoted the SLT by directly or indirectly employ-ing the intermediate representations, namely sign glosses.
Gloss is a simpliﬁed representation of each sign language in continuous video as illustrated in Figure 1a. Although gloss-based methods have signiﬁcantly improved the SLT performance compared to end-to-end gloss-free approaches (as illustrated in Figure 1b), the former still suffers from the following problems: (i) annotating glosses is a labor-intensive task, which requires ﬁne-grained alignment and labeled by specialists, signiﬁcantly constraining the scala-bility of gloss-based SLT methods and (ii) the gloss-based approach introduces an information bottleneck in the mid-level gloss representation [4], which limits the network’s ability to understand sign language as the translation model can only be as good as the sign gloss annotations it was trained from.
Drawing inspiration from CLIP [29], which utilizes nat-ural language supervision for image representation learn-ing, we discovered that learning language-indicated visual representation from sign language videos is an effective pre-training task for SLT as it establishes a potential connection between visual signs and language context. However, di-rectly applying CLIP to SLT faces two challenges: (i) it lacks the ability to jointly pretrain the Visual Encoder and
Text Decoder for SLT, and (ii) sufﬁcient SLT data is neces-sary for this pretraining task. To address these obstacles, we must answer two key questions: (i) how can we efﬁciently perform joint pretraining using the limited SLT dataset? and (ii) how can we ensure that the pretrained model offers the most effective assistance for the downstream SLT task?
To overcome the ﬁrst challenge, we present a solution that operates at both the algorithmic and data levels. Al-gorithmically, we introduce a fresh pre-training approach known as VLP (Visual-Language Pretraining), shown in
Figure 2a, which combines masked self-supervised learning with CLIP. Speciﬁcally, we design a pretext task that aligns visual and textual representations in a joint multimodal se-mantic space, guiding the Visual Encoder to learn language-indicated visual representations. Meanwhile, we incorpo-rate masked self-supervised learning into the pre-training process to help the Text Decoder to capture the syntactic and semantic properties of sign language sentences. At the data level, we investigate a set of strong data augmentation techniques for sign videos to increase the diversity of visual data. This aspect has often been overlooked in previous SLT techniques.
To cope with the second aspect, as depicted in Figure 2b, we design an end-to-end Gloss-Free SLT architecture re-ferred to as GFSLT. This architecture takes the form of an encoder-decoder structure and inherits parameters from the pre-trained Visual Encoder and Text Decoder of the initial phase. GFSLT allows us to directly transform visual repre-sentations into spoken sentences without needing any inter-mediate steps or guidance. Furthermore, unlike alternative methods [4, 42, 5] that solely ﬁne-tune the spatial feature extractor (visual embedding module) in the Visual Encoder, we ﬁne-tune both the spatial feature extractor and the tem-poral relationship modeling network (Transformer encoder) as a uniﬁed whole.
In summary, the main contributions are listed:
• In this work, we have achieved unprecedented im-provements in the BLEU-4 score for SLT with-out using gloss annotations. Speciﬁcally, compared with state-of-the-art gloss-free SLT methods, our method has got ≥+5 and ≥+3 improvements on the
PHOENIX14T dataset and CSL-Daily dataset, respec-tively. We believe that these improvements represent a signiﬁcant breakthrough in the task of gloss-free SLT.
• To the best of our knowledge, this is the ﬁrst attempt to introduce the VLP strategy to align visual and textual representations in a joint semantic space in the gloss-free SLT task.
• We propose a novel pre-training paradigm that incor-porates masked self-supervised learning together with contrastive language-image pre-training to facilitate the gloss-free SLT task. This approach represents a signiﬁcant improvement over previous methods and has the potential to greatly enhance the accuracy and efﬁciency of SLT systems. 2.