Abstract
We generalize the class vectors found in neural networks to linear subspaces (i.e., points in the Grassmann manifold) and show that the Grassmann Class Representation (GCR) enables simultaneous improvement in accuracy and feature transferability. In GCR, each class is a subspace, and the logit is defined as the norm of the projection of a feature onto the class subspace. We integrate Riemannian SGD into deep learning frameworks such that class subspaces in a
Grassmannian are jointly optimized with the rest model pa-rameters. Compared to the vector form, the representative capability of subspaces is more powerful. We show that on
ImageNet-1K, the top-1 errors of ResNet50-D, ResNeXt50,
Swin-T, and Deit3-S are reduced by 5.6%, 4.5%, 3.0%, and 3.5%, respectively. Subspaces also provide freedom for fea-tures to vary, and we observed that the intra-class feature variability grows when the subspace dimension increases.
Consequently, we found the quality of GCR features is better for downstream tasks. For ResNet50-D, the average linear transfer accuracy across 6 datasets improves from 77.98% to 79.70% compared to the strong baseline of vanilla softmax.
For Swin-T, it improves from 81.5% to 83.4% and for Deit3, it improves from 73.8% to 81.4%. With these encouraging results, we believe that more applications could benefit from the Grassmann class representation. Code is released at https://github.com/innerlee/GCR. 1.

Introduction
The scheme deep feature→fully-connected
→softmax→cross-entropy loss has been the standard practice in deep classification networks. Columns of the weight parameter in the fully-connected layer are the class representative vectors and serve as the prototype for classes. The vector class representation has achieved huge
* Equal contribution. Work is done when Haoqi was at SenseTime.
† Corresponding author: Wayne Zhang. success, yet it is not without imperfections. In the study of transferable features, researchers noticed a dilemma that representations with higher classification accuracy lead to less transferable features for downstream tasks [19]. This is connected to the fact that they tend to collapse intra-class variability of features, resulting in loss of information in the logits about the resemblances between instances of different classes [29]. The neural collapse phenomenon [34] indicates that as training progresses, the intra-class variation becomes negligible, and features collapse to their class means. As such, this dilemma inherently originates from the practice of representing classes by a single vector. This motivates us to study representing classes by high-dimensional subspaces.
Representing classes as subspaces in machine learning can be dated back, at least, to 1973 [49]. This core idea is re-emerging recently in various contexts such as clustering [54], few-shot classification [12, 41] and out-of-distribution detec-tion [47], albeit in each case a different concrete instantiation was proposed. However, very few works study the subspace representation in large-scale classification, a fundamental computer vision task that benefits numerous downstream tasks. We propose the Grassmann Class Representation (GCR) to fill this gap and study its impact on classification and feature transferability via extensive experiments. To be specific, each class i is associated with a linear subspace Si, and for any feature vector x, the i-th logit li is defined as the norm of its projection onto the subspace Si, li := (cid:13) (cid:13)projSix(cid:13) (cid:13) . (1)
In the following, we answer the two critical questions, 1. How to effectively optimize the subspaces in training? 2. Is Grassmann class representation useful?
Several drawbacks and important differences in previous works make their methodologies hard to generalize to the large-scale classification problem. Firstly, their subspaces might be not learnable. In ViM [47], DSN [41] and the
SVD formulation of [54], subspaces are obtained post hoc
by PCA-like operation on feature matrices without explicit parametrization and learning. Secondly, for works with learnable subspaces, their learning procedure for subspaces might not apply. For example, in RegressionNet [12], the loss involves pairwise subspace orthogonalization, which does not scale when the number of classes is large because the computational cost will soon be infeasible. And thirdly, the objective of [54] is unsupervised subspace clustering, which needs substantial changes to adapt to classification.
It is well known that the set of k-dimensional linear sub-spaces form a Grassmann manifold, so finding the optimal subspace representation for classes is to optimize on the
Grassmannian. Therefore, a natural solution to Question 1 is to use geometric optimization [13], which optimizes the objective function under the constraint of a given manifold.
Points being optimized are moving along geodesics instead of following the direction of Euclidean gradients. We imple-mented an efficient Riemannian SGD for optimization in the
Grassmann manifold in Algorithm 1, which integrates the geometric optimization into deep learning frameworks so that the subspaces in Grassmannian and the model weights in Euclidean are jointly optimized.
The Grassmann class representation sheds light on the incompatibility issue between accuracy and transferability.
Features can vary in a high-dimensional subspace without harming the accuracy. We empirically verify this specula-tion in Section 5, which involves both CNNs (ResNet [16],
ResNet-D [17], ResNeXt [52], VGG13-BN [42]) and vision transformers (Swin [26] and Deit3 [45]). We found that with larger subspace dimensions k, the intra-class variation increase, and the feature transferability improve. The classi-fication performance of GCR is also superior to the vector form. For example, on ImageNet-1K, the top-1 error rates of
ResNet50-D, ResNeXt50, Swin-T and Deit3-S are reduced relatively by 5.6%, 4.5%, 3.0%, and 3.5%, respectively.
To summarize, our contributions are three folds. (1) We propose the Grassmann class representation and learn the subspaces jointly with other network parameters with the help of Riemannian SGD. (2) We showed its superior accu-racy on large-scale classification both for CNNs and vision transformers. (3) We showed that features learned by the
Grassmann class representation have better transferability. 2.