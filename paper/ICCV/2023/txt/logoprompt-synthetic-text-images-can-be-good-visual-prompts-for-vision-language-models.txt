Abstract base-to-new generalization, and domain generalization.
Prompt engineering is a powerful tool used to enhance the performance of pre-trained models on downstream tasks. For example, providing the prompt “Let’s think step by step” improved GPT-3’s reasoning accuracy to 63% on
MutiArith while prompting “a photo of” filled with a class name enables CLIP to achieve 80% zero-shot accuracy on
ImageNet. While previous research has explored prompt learning for the visual modality, analyzing what constitutes a good visual prompt specifically for image recognition is limited. In addition, existing visual prompt tuning methods’ generalization ability is worse than text-only prompting tuning. This paper explores our key insight: synthetic text images are good visual prompts for vision-language models! To achieve that, we propose our LoGoPrompt, which reformulates the classification objective to the visual prompt selection and addresses the chicken-and-egg challenge of first adding synthetic text images as class-wise visual prompts or predicting the class first. Without any trainable visual prompt parameters, experimental results on 16 datasets demonstrate that our method consistently outperforms state-of-the-art methods in few-shot learning, 1.

Introduction
Large-scale contrastive vision-language models (VLMs) like CLIP [34] and ALIGN [18], pretrained on large-scale image-text pairs via contrastive learning, encode general knowledge about the alignment of visual concepts and tex-tual sequences. VLMs with dual-encoder separately encode images and texts into vectors in joint embedding space, en-abling the transfer for downstream image classification by treating image vectors as “image features” and text vectors corresponding to classes as “classification weights”, respec-tively. The class-specific weights can encoded from hand-craft prompts [34, 18, 10, 24, 25], such as “a photo of a
[class]”, where the [class] token is filled with real class names. Thanks to exploring open-set visual concepts with high-capacity text encoder, VLMs with handcraft prompts show impressive potential in zero-shot transfer to image classification tasks.
Recently, CoOp [52] and CoCoOp [53] utilize prompt tuning [21, 27, 25] to learn a continuous text prompt with only a few shots of images and improve zero-shot VLMs’ performance. However, the text prompt tuning can only
change the “classification weights” but keeps the “image features” unchanged. It is a sub-optimal solution for adap-tion, making it hard to classify images with different classes but close image features. Therefore, some works [48, 1, 19] introduce visual prompts to VLMs for simultaneously ad-justing the “image features” and “classification weights”.
The visual prompts of VPT [19] and DPT [48] are specific to ViT architecture [8], and they adapt to downstream tasks by tuning additional image patch token embeddings. Con-sidering the generality to different families of visual back-bones, the recent work [1] proposes to learn image perturba-tion to perform data-space adaption. Although it explores a new visual prompting paradigm, the performance improve-ment is limited, especially in the same few-shot setting as the above-mentioned methods. Furthermore, these visual prompts [48, 1, 19] seem to affect VLMs’ generalization ability, whose base-to-new generalization performance is lower than zero-shot VLMs and approaches with text-only prompting [52, 53] (see Figure 1).
Therefore, we propose to explore visual prompting for
VLMs’ adaption, which can (1) work for different backbone families such as CNNs and Transformers, (2) effectively adapt to downstream classification tasks with few shots of images, and (3) preserve the VLMs’ generalization abil-ity. To achieve these goals, we follow [1] to perform data-space adaption (i.e., adapting on image pixels) and then ana-lyze how VLMs understand different images, especially the classification accuracy gaps between images with the same class. We are surprised to observe that the images with class name text have very high confidence to classify to the class, even only lower than the simplest cases, as shown in Fig-ure 1. The empirical study [12] about multimodal neurons of CLIP validates our observation. For a class, both images with the class name text and generic natural images of that class can activate the same neurons important for classify-ing that class.
The observation motivates our key insight: we can use synthetic images with class name text as the visual prompts for VLMs! However, it is not trivial to effectively utilize synthetic images because simply treating the synthetic im-ages as extra image data cannot achieve consistent perfor-mance improvement across different datasets and with dif-ferent image shots. In this paper, we propose to use syn-thetic images with class name text (i.e., the class-wise visual prompts) to modify images in the training set so that class-wise synthetic parts can help VLMs perceive class-relevant content in the original images for classification. In the train-ing phase, given a training image and its ground-truth class, we can easily transform it with its class-wise visual prompt, such as randomly replacing one of the original image’s pixel blocks with the synthetic one, as shown in Figure 1. How-ever, in the testing stage, since the test image’s class is un-known, it remains unclear which class-wise visual prompt should be used to benefit the classification prediction of the test image, which is a chicken-and-egg problem. Therefore, we reformulate the downstream classification objective as the visual prompt selection: select the synthetic images of the correct class for the original image as its visual prompts.
Specifically, for a training image, we transform it into mul-tiple images with different class-wise visual prompts and maximize the similarity of the image with the ground-truth visual prompt to the text features of that ground-truth class while minimizing the similarities of other images.
Furthermore, to develop the visual prompt selection to be effective and efficient, we propose a min-max con-trastive learning objective and introduce hard negative min-ing [37, 17]. The min-max contrastive learning first groups an original image and its transformed image with the class-wise visual prompt as a group. Then, it maximizes the mini-mal similarity to the text features in one ground-truth group while minimizing the maximal similarity in negative groups to preserve the ability to classify the original image.
Despite the simplicity, our novel insights of using syn-thetic images with class name text as visual prompts and vi-sual prompt selection learning strategy are particularly ef-fective. Without requiring any tuning of visual prompts, our method significantly outperforms state- of-the-art meth-ods [53] on base-to-new generalization, especially com-pared to previous visual prompting approaches [48, 1, 19], as shown in Figure 1. Notice that our visual prompts have no parameters to learn, but we still have text prompts for tuning following existing works [52, 48, 53]. To verify whether our method can benefit from visual prompt tun-ing, we extend a tuning version for visual prompts, further boosting the performance on few-shot classification. We name our proposed method LoGoPrompt, since we incor-porate synthetic text onto images in a manner akin to the application of logos onto visuals. We evaluate and compare
LoGoPrompt with state-of-the-art methods on 16 datasets that cover diverse image classification tasks. In summary, we make the following contributions:
• We are the first to propose using synthetic images with the text class name as visual prompts for VLMs. Our visual prompts can work for different backbone fami-lies, e.g., CNNs and Transformers.
• We reformulate the classification objective to visual prompt selection to address the chicken-and-egg chal-lenge of adding class-wise visual prompts and achieve the selection via min-max contrastive learning.
• Despite the simplicity, experiments on 16 datasets demonstrate our novel insight and learning strat-egy particularly effectively, consistently outperform-ing state-of-the-art methods in base-to-new generaliza-tion, few-shot learning, and domain generalization.
2.