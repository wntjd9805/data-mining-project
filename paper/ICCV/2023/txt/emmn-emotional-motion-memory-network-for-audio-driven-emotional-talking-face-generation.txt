Abstract
Synthesizing expression is essential to create realistic talking faces. Previous works consider expressions and mouth shapes as a whole and predict them solely from audio inputs. However, the limited information contained in au-dio, such as phonemes and coarse emotion embedding, may not be suitable as the source of elaborate expressions. Be-sides, since expressions are tightly coupled to lip motions, generating expression from other sources is tricky and al-ways neglects expression performed on mouth region, lead-*Corresponding author. ing to inconsistency between them. To tackle the issues, this paper proposes Emotional Motion Memory Net (EMMN) that synthesizes expression overall on the talking face via emotion embedding and lip motion instead of the sole au-dio. Specifically, we extract emotion embedding from au-dio and design Motion Reconstruction module to decom-pose ground truth videos into mouth features and expres-sion features before training, where the latter encode all facial factors about expression. During training, the emo-tion embedding and mouth features are used as keys, and the corresponding expression features are used as values to create key-value pairs stored in the proposed Motion Mem-ory Net. Hence, once the audio-relevant mouth features
and emotion embedding are individually predicted from au-dio at inference time, we treat them as a query to retrieve the best-matching expression features, performing expres-sion overall on the face and thus avoiding inconsistent re-sults. Extensive experiments demonstrate that our method can generate high-quality talking face videos with accurate lip movements and vivid expressions on unseen subjects. 1.

Introduction
High-fidelity audio-driven facial animation has various applications, including education, well-being, and enter-tainment [25]. Extensive efforts have been devoted to gen-erating not only lip motions [5, 34, 33] synchronized with the audio, but also rhythmic head movements [4, 36, 43, 40] to create more realistic talking heads. However, most ap-proaches rarely consider emotional expression, an essential element for delivering communicative information [10].
Facial emotion dynamics are typically expressed through the coordinated movements of multiple facial muscles in a global manner [12]. For instance, when surprisingly speak-ing, individuals tend to widen their eyes and open their mouths unconsciously larger than displaying other expres-sions. It suggests that expression and lip motions are intrin-sically interconnected and mouth shape is crucial for con-veying emotions [14, 20]. Recently, some works treat ex-pressions and mouth shapes as a whole and predict them from the audio [15, 11]. However, audio, which consists of phoneme-level contents and coarse-grained emotion em-beddings, is inadequate for controlling the generation of fine-grained global expressions. Therefore, our goal is to separately predict expression and mouth shape from sources in addition to audio and integrate the predictions globally. (1)
Nevertheless, there exist two formidable challenges:
Since expression and mouth shape are inherently coupled on the face, it is tough to achieve separate prediction and global integration. (2) Although emotional audio can pro-vide a rough emotion embedding that indicates the emotion category, it struggles to guide detailed facial emotion dy-namics effectively. Even though expression can be obtained from other plausible sources, it also requires compromis-ing the consistency between expression and mouth shape to express emotion overall on the face.
To deal with the challenges above, we propose a novel audio-driven emotional talking face generation framework, namely Emotional Motion Memory Network (EMMN). Ba-sically, we extract emotion embedding from audio follow-ing EVP [15] and leverage keypoint-based dense motion fields [29, 36] to represent facial dynamics. Our intuition is to completely disentangle the entire facial dynamics into mouth-related space and expression-related space. The for-mer merely contains the content of audio without expres-sion information, while the latter encodes all facial factors about expression to perform expression overall on the face.
To this end, we design Motion Reconstruction module that decouples and merges the facial dynamics. To simultane-ously train these two processes, the cross-reconstruction training strategy [1] is adopted. However, it brings the de-sire for paired videos with the exact same expression but different mouth shapes, which is almost impossible to real-ize in reality [35, 3] due to the strict requirement. To cre-ate the training pairs, we present a pseudo label generation strategy via a pre-trained Wav2Lip [26] model with satisfac-tory lip-sync generation performance. This enables the Mo-tion Reconstruction module to be trained and further em-ployed to decompose ground truth videos into mouth mo-tion features and expression motion features, which serve as pseudo labels for mouth shape and expression, facili-tating separate prediction setting. For instance, we design
Audio2Mouth module to predict solely the mouth motion features from audio.
To ensure the consistency between expression and lip motion, we hope to recall the expression motion features most relevant to emotion embedding and the mouth mo-tion features. Ideally, we suppose that they are disentangled from the same source. For this purpose, we resort to Mem-ory Network [39] which leverages external memory to store information. Specifically, we construct the Motion Mem-ory Net to store emo-mouth feature (emotion embedding and mouth motion feature) and the corresponding expres-sion motion feature as a key-value pair. During training, the memory aligns emo-mouth features with their correspond-ing expression motion features in the same address. Hence, at inference time, the best-matching expression motion fea-ture can be retrieved by querying with emo-mouth feature predicted via Audio2Mouth. Using the retrieved expression features and mouth motion features, Motion Reconstruction can globally generate the final representation of the entire emotional facial dynamics. Lastly, a flow estimator and an image generator from FOMM [29] are introduced to synthe-size photo-realistic results. Extensive experiments demon-strate the superiority of our method in terms of expression naturalness and emotion accuracy compared with state-of-the-art (SOTA) methods.
Our contributions are summarized as follows: (1) We present a system named Emotional Motion Memory Net-work (EMMN) to perform one-shot emotional talking face generation solely from a reference image and an emotional audio clip without additional emotion sources. Hence it is flexible in applying the system. (2) We propose Motion Re-construction that decomposes the face into expression and mouth feature to separately predict them and synthesize ex-pression overall on the face, including the mouth region. (3)
We design Motion Memory-Net to store aligned expression and emo-mouth features, which ensures the consistency of the final emotional face motion.
2.