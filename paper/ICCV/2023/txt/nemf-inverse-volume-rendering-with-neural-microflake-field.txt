Abstract 1.

Introduction
Recovering the physical attributes of an object’s appear-ance from its images captured under an unknown illumi-nation is challenging yet essential for photo-realistic ren-dering. Recent approaches adopt the emerging implicit scene representations and have shown impressive results.
However, they unanimously adopt a surface-based repre-sentation,and hence can not well handle scenes with very complex geometry, translucent object and etc. In this pa-per, we propose to conduct inverse volume rendering, in contrast to surface-based, by representing a scene using microflake volume, which assumes the space is filled with infinite small flakes and light reflects or scatters at each spatial location according to microflake distributions. We further adopt the coordinate networks to implicitly encode the microflake volume, and develop a differentiable mi-croflake volume renderer to train the network in an end-to-end way in principle. Our NeMF enables effective re-covery of appearance attributes for highly complex geom-etry and scattering object, enables high-quality relighting, material editing, and especially simulates volume rendering effects, such as scattering, which is infeasible for surface-based approaches. Our data and code are available at: https://github.com/YoujiaZhang/NeMF.
∗Corresponding author: Wei Yang (weiyangcs@hust.edu.cn).
Inverse rendering refers to the process of recovering an object’s physical attributes related to its appearances, in-cluding shape, reflectance and illumination, from its im-age observations. The above physical attributes play a vital role in graphic applications that require physically reason-able realism. The problem is highly ill-posed due to the complication of object geometries, material and varieties of illuminations. It becomes even more difficult when the images are captured under an unknown illumination condi-tion. The typical practice is to represent the object as sur-faces and then solve for the Spatially-Varying Bidirectional
Reflectance Distribution Functions (SVBRDF) at each ray-surface interaction. Traditional approaches rely on restric-tive assumptions [2, 8, 26, 27, 29] or sophisticated capture systems, such as light-stages [13], co-located flashlight and camera setup [59], and etc.
More recent works explore the implicit scene repre-sentations, e.g., radiance field and signed distance func-tions [55, 25, 41, 33, 65], and achieve promising results.
They exploit the geometry, reflectance, or visibility recov-ered by implicit models as initial estimates for solving the ill-posed inverse rendering problem. Notably, NeRD [7] adopts a two-stage estimation strategy by first predicting the sampling pattern and albedo, and then performing per-ray
SVBRDF decomposition. NeRFactor [63] applies hard sur-face approximation on NeRF geometry and recovers neu-ral fields of surface normals, light visibility, albedo and
SVBRDFs. Zhang et al. [64] represent the scene geome-try as a zero-level set and recover spatially-varying indirect illumination for more accurate inverse rendering. Never-theless, they either rely on a surface-based representation or need to extract geometry from a volume representation first for the subsequential reflectance estimation. This usu-ally leads to a multi-stage refinement framework, and the performance depends heavily on qualities of the recovered geometries.
Recall that the seminal work of NeRF [35] adopts a radi-ance volume representation and enables photorealistic ren-In essence, dering without explicit geometry modeling.
NeRF assumes the space is filled with infinite small par-ticles that emit radiation.
In this paper, we faithfully ex-tend the volumetric setup by replacing the particles with oriented flakes, which reflect or scatter light according to space occupancies and materials [23]. The interaction of light with a collection of microflakes in the volume is de-scribed by the microflake phase function, which is deter-mined by the ellipsoidal distribution of normals (NDF) and further parameterized by the microflake normal direction
ωm of and roughness τm, as shown in Fig. 1. Such repre-sentation can also simulate surface-like object with denser flakes inside the object, while sparser outside, as shown in
Fig. 2. With this Neural Microflake Field (NeMF) represen-tation, we propose to conduct inverse volume rendering, to tackle the overly dependency on geometry problem of ex-isting methods. We start from the vanilla NeRF model, add one additional MLP branch for estimating the mircoflake normal ωm. As for the microflake roughness τm, we use a
U-shaped MLP network to first encode the material to a fea-ture vector, apply sparsity constraint and then decode it back into albedo a and roughness τm. To estimate the density, albedo, and microflake distributions, we develop a differen-tiable microflake volume renderer and use the photometric loss between the rendered and groundtruth images for su-pervision. We evaluate the proposed method on both syn-thetic and real datasets. The experimental results show that our approach outperforms existing methods in terms of ren-dering quality, and is able to recover scenes with complex geometry and translucent objects. Moreover, our NeMF not only enables effective relighting and material editing but also allows for simulating volume scattering, as shown in
Fig. 1. 2.