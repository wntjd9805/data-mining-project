Abstract
Large scale Vision Language (VL) models have shown tremendous success in aligning representations between vi-sual and text modalities. This enables remarkable progress in zero-shot recognition, image generation & editing, and many other exciting tasks. However, VL models tend to over-represent objects while paying much less attention to verbs, and require additional tuning on video data for best zero-shot action recognition performance. While previous work relied on large-scale, fully-annotated data, in this work we propose an unsupervised approach. We adapt a
VL model for zero-shot and few-shot action recognition us-ing a collection of unlabeled videos and an unpaired ac-tion dictionary. Based on that, we leverage Large Lan-guage Models and VL models to build a text bag for each unlabeled video via matching, text expansion and caption-ing. We use those bags in a Multiple Instance Learning setup to adapt an image-text backbone to video data. Al-though finetuned on unlabeled video data, our resulting models demonstrate high transferability to numerous un-seen zero-shot downstream tasks, improving the base VL model performance by up to 14%, and even comparing fa-vorably to fully-supervised baselines in both zero-shot and few-shot video recognition transfer. The code is released at https://github.com/wlin-at/MAXI. 1.

Introduction
Figure 1: While previous work relied on full annotation of action datasets which is time-consuming and cost-intensive to collect, our approach MAXI finetunes the VL model with unlabeled video data. Specifically, we leverage a set of lan-guage sources (action dictionary, VL model and LLM) to construct a text bag for each unlabeled video, and employ the Multiple Instance Learning (MIL) objective for fine-tuning. MAXI demonstrates outstanding improvement of zero-shot and few-shot transfer on downstream novel action datasets.
Vision Language (VL) models [36, 23, 17] have met un-precedented success in unlocking many vision applications
[36] to work with potentially unlimited open vocabularies, through the promise of zero-shot transfer [55, 57, 58, 14, 39, 59, 22, 37]. This is empowered by the alignment be-† Correspondence: wlin2021at@gmail.com tween visual and language representation spaces, which is effectively attained by VL models leveraging huge amounts of paired image and text data. Incorporating a VL model as a source (base) model or as an architectural component has allowed scaling finetuning on relatively small datasets (e.g. limited in terms of the number of observed objects or
other visual concepts compared to the vast VL pretraining) towards zero-shot transfer at inference time. Such zero-shot transfer includes recognizing [55, 57, 58], detecting
[14, 39, 59], segmenting [22, 37], and even generating [40] objects unseen during the finetuning stage and only encoun-tered for the first time at the inference stage.
However, despite the progress in zero-shot image tasks,
VL models have been observed to underperform when ap-plied to zero-shot action recognition on video data without any finetuning [47, 33, 18, 50, 5, 38]. A possible reason, as extensively studied in several works [46, 56, 53, 15], is that VL models have a tendency to mostly represent objects (nouns) and not actions (verbs or verb phrases).
Therefore, to deal with these shortcomings of VL models w.r.t. zero-shot action recognition, previous works [47, 33, 18, 50, 5, 38] have used datasets with full annotation (e.g. the most popular
K400 [19]) to finetune VL models (e.g.
CLIP [36]) towards improved video zero-shot recognition performance. The potential downsides of this approach are: (i) reliance on full annotation of large-scale action datasets that is time-consuming and cost-intensive, and (ii) the ex-posure of the model to only the limited action vocabulary during the supervised finetuning (e.g. 400 actions of K400 vs. over 8K possible single verb actions and much more possible general actions in English language) limiting the performance of zero-shot transfer to unseen action cate-gories. In this context, we propose ‘MAtch, eXpand and
Improve’ (MAXI) – to allow finetuning on completely un-labeled video data (e.g. unlabeled K400 [19]) and a set of language sources, such as unpaired action dictionaries,
Large Language Models (LLM) (e.g. GPT-3 [3]), and VL models for matching (e.g. CLIP [36]) and captioning (e.g.
BLIP [23]). To this end, MAXI relies on individual bags of potential texts, collected and refined based on the different language sources, that correspond to each video in the unla-beled set. It then applies Multiple Instance Learning (MIL) for finetuning the VL model using those bags as illustrated in Figure 1. We extensively evaluate MAXI on seven down-stream zero-shot and few-shot transfer action recognition benchmarks completely unseen during training. We show that MAXI is effective in leveraging unlabeled video data, not only significantly (up to 14%) improving the source VL model performance on all of those tasks, but also favorably competing with state-of-the-art supervised methods trained on fully supervised counterparts of the same finetuning data, and even improving upon them in some zero-shot and few-shot action recognition transfer tasks.
Our contributions are as follows: (i) we propose MAXI, an approach that leverages an unlabeled video collection and a set of language sources to improve downstream zero-shot action recognition; (ii) we propose to match each un-labeled video with text bags of knowledge mined from the language sources, and employ Multiple Instance Learning for finetuning a VL model using these text bags; (iii) we extensively evaluate our approach on seven unseen action recognition benchmarks, and demonstrate up to 14% abso-lute zero-shot performance improvements over the source
VL model, and even outperform baseline models trained in a fully supervised manner on the same data. 2.