Abstract
Look-up table (LUT)-based methods have shown the great efficacy in single image super-resolution (SR) task.
However, previous methods ignore the essential reason of restricted receptive field (RF) size in LUT, which is caused by the interaction of space and channel features in vanilla convolution. They can only increase the RF at the cost of linearly increasing LUT size. To enlarge RF with con-tained LUT sizes, we propose a novel Reconstructed Con-volution (RC) module, which decouples channel-wise and spatial calculation. It can be formulated as n2 1D LUTs to maintain n × n receptive field, which is obviously smaller than n × nD LUT formulated before. The LUT generated by our RC module reaches less than 1/10000 storage com-pared with SR-LUT baseline. The proposed Reconstructed
Convolution module based LUT method, termed as RCLUT, can enlarge the RF size by 9 times than the state-of-the-art
LUT-based SR method and achieve superior performance on five popular benchmark dataset. Moreover, the efficient and robust RC module can be used as a plugin to improve other LUT-based SR methods. The code is available at https://github.com/liuguandu/RC-LUT. 1.

Introduction
Single image super-resolution (SR) aims to recover the high-resolution (HR) image from a low-resolution (LR) im-age and to bring back clearer edges, textures, and details while increasing the resolution of the input.
In the past decade, deep learning-based SR methods [43, 23, 18, 44, 35, 20, 13, 26, 27] have made remarkable improvements compared with traditional SR methods (e.g., interpolation based [17], sparse coding based [34, 33, 40, 5, 12]). How-*Corresponding Author
Figure 1. Comparison of PSNR and storage on Set5 benchmark dataset for ×4 SR task. We compare our method with prior LUT-based SR methods. Our method achieves the superior performance with relatively less LUT storage. ever, such kind of methods often possess huge amount of parameters, which has high computation cost and cannot be practically used on devices with limited computational re-sources. Exploring the pratical and real-time SR solutions have been a growing trend in the single image super resolu-tion (SISR) community.
Look-up table (LUT) based super-resolution (SR) meth-ods like SR-LUT [16] cache trained SR network results for every potential input in a LUT, simplifying inference by replacing runtime computation with faster indexing. How-ever, this strategy requires a limited receptive field (RF), as the input space size grows exponentially with an increase in input pixels. Specifically, SR-LUT has a 2 × 2 input size, yielding a 3 × 3 RF by rotation ensemble, and requires b2×2 × r2 bytes to store its LUT for upscaling by a factor of r (b = 255). For example, a 3 × 3 vanilla convolution gen-erates 2559 mappings, consuming 1.72 TB in LUT form.
A larger RF allows a model to capture more intricate se-mantics and structures within an image, playing a critical role in the training process. However, for SR-LUT, the ex-ponential growth of LUT size significantly limits RF im-provements. Attempts to solve this, such as MuLUT [22] and SPLUT [24], propose cascading multiple parallel LUTs to expand the RF to 9 × 9 and 6 × 6, respectively. These methods attempt to divide the entire LUT into several sub-LUTs, with a linear increase in LUT size. However, they don’t address the primary cause of restricted RF size in
LUT-based methods. Furthermore, the RF size of these
LUT-based methods falls short compared to DNN SR meth-ods, resulting in performance inferior to simple DNN mod-els, such as FSRCNN[9].
Vanilla convolution amalgamates features across both spatial and channel dimensions, necessitating that the LUT format traverses all potential combinations and permuta-tions of input pixels. Essentially, spatially dependent con-volution refers to other feature points within the spatial neighborhood to generate the input-output mapping of the current feature point. Given that the most significant fac-tor constraining the performance of LUT-based methods is spatially dependent convolution, we question the status quo. Specifically, we contemplate decoupling the spatial and channel calculations of the convolution process and propose storing the LUT with spatially independent con-volution. This could potentially address the inherent limita-tions of existing LUT-based methods, providing a new way forward for this class of algorithms.
In this paper, we propose a novel Reconstructed Con-volution (RC) method designed to decouple the spatial and channel calculations, thus effectively increasing the RF of the LUT while significantly reducing storage requirements.
This decoupled operation enables the network to circum-vent the constraints posed by the necessity of traversing all spatial pixel combinations. As a result, our method can employ n × n 1D LUTs to approximate the effect of an n × n convolution layer, cutting the LUT size from the ini-tial bn2 to b×n2.Building on our RC approach, we introduce a practical Reconstructed Convolution module-based LUT method (RCLUT) tailored for the SR task, allowing for an expansion of the RF with minimal storage consumption. As illustrated in Figure 1, our RCLUT attains competitive per-formance with minimal LUT size, demonstrating an optimal balance between performance and LUT storage.
Additionally, our RC method can be efficiently inte-grated as a plugin module. Although the RC module sac-rifices the interactive information between two-dimensional features, its minimal storage requirements and large recep-tive field successfully offset the shortcomings of previous
LUT-based methods. In our implementation, we integrate the RC module on top of SRLUT, resulting in a cost of only about 1/10,000 of the original storage for a 13-fold improve-ment in the RF size.
To conclude, the contribution of our work includes:
• We propose a novel Reconstructed Convolution (RC) method with large RF, which decouples the spatial and channel calculation of convolution. Based on the RC method, our RCLUT model achieves significant per-formance with less storage.
• Our RC method can be designed as a plugin module, which brings improvement to LUT-based SR methods with slight increasing size.
• Extensive results show that our method obtains supe-rior performance compared with SR methods based on
LUT. It is a new state-of-the-arts LUT method in SR task. 2.