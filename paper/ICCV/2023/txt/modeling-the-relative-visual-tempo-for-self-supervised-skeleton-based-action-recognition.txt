Abstract
Visual tempo characterizes the dynamics and the tem-poral evolution, which helps describe actions. Recent ap-proaches directly perform visual tempo prediction on skele-ton sequences, which may suffer from insufﬁcient feature
In this paper, we observe that rela-representation issue. tive visual tempo is more in line with human intuition, and thus providing more effective supervision signals. Based on this, we propose a novel Relative Visual Tempo Con-trastive Learning framework for skeleton action Represen-tation (RVTCLR). Speciﬁcally, we design a Relative Visual
Tempo Learning (RVTL) task to explore the motion informa-tion in intra-video clips, and an Appearance-Consistency (AC) task to learn appearance information simultaneous-ly, resulting in more representative spatiotemporal fea-tures. Furthermore, skeleton sequence data is much s-parser than RGB data, making the network learn short-cuts, and overﬁt to low-level information such as skeleton scales. To learn high-order semantics, we further design a new Distribution-Consistency (DC) branch, containing three components: Skeleton-speciﬁc Data Augmentation (S-DA), Fine-grained Skeleton Encoding Module (FSEM), and
Distribution-aware Diversity (DD) Loss. We term our entire method (RVTCLR with DC) as RVTCLR+. Extensive exper-iments on NTU RGB+D 60 and NTU RGB+D 120 datasets demonstrate that our RVTCLR+ can achieve competitive re-sults over the state-of-the-art methods. Code is available at https://github.com/Zhuysheng/RVTCLR.
∗Corresponding author. This work was supported in part by the New
Generation AI Major Project of Ministry of Science and Technology of
China under Grant 2018AAA0102501 and in part by the National Natural
Science Foundation of China (NSFC) under Grant U21B2027.
Figure 1. The basic visual tempo prediction is considered a classi-ﬁcation task, where the learned model is used to assign tempo la-bels to individual video clips. We introduce relative visual tempo learning and appearance-consistency based on contrastive learn-ing. It’s more human-intuitive for modeling actions. The top-1 accuracy on NTU-60 Xsub benchmark supports our claim. 1.

Introduction
As one of the most fundamental topics in video un-derstanding, human action recognition has been widely explored in many real-world scenarios, such as human-computer interaction [22], autonomous driving [30], and so on [34, 17]. Skeleton data provides more abstract and well-structured information with less computation and s-torage than raw RGB video. It is less susceptible to cam-era viewpoint changes and background distractions. Thus, skeleton-based action recognition has attracted extensive at-tention [39, 21, 28, 7, 35]. However, most of these methods rely heavily on full supervision. The collection of massive annotations is labor-intensive and time-consuming. Under this circumstance, learning action representations directly from the data in a self-supervised manner has attracted in-creasing attentions.
Several existing works [11, 38, 41] draw inspiration from video self-supervised learning and directly apply video pre-text tasks on skeleton sequences, such as jigsaw puzzle recognition [1, 9] and temporal order prediction [15, 37].
For sequence data, a commonly used technique to model spatiotemporal information is visual tempo prediction [42, 29]. Action visual tempo describes the speed at which ac-tions are performed, which is crucial for distinguishing ac-tions that exhibit similar temporal evolutions (e.g., walking and jogging). The basic idea is shown in the upper part of Figure 1. For each video, clips are sampled at differ-ent sampling frequencies to mimic different visual tempos (e.g., 1× and 2×), and predictions are made using the back-bone network. However, there are limitations in directly applying it to skeleton sequences. People perform action-s at their own tempo due to the inﬂuence of gender, age, etc. Even at the same tempo, the athlete’s walking speed is visibly faster than that of a child in a third-person perspec-tive. Therefore, it’s obviously inappropriate to treat tempo prediction as a classiﬁcation task. Furthermore, this basic strategy mainly focuses on motion information and cannot explicitly encourage the model to explore appearance in-formation that is equally important for recognizing actions.
Recently, contrastive learning has shown its great potential in extracting informative features [10, 6] in skeleton-based action recognition. Contrastive learning typically convert-s the naive classiﬁcation task to a matching problem and learns an embedding space in which augmentations of same skeleton sequence are kept closer together, while differen-t augmentations are far apart.
In this way, the above is-sues can be alleviated in an elegant way. However, skeleton sequence data is much sparser than RGB data, and apply-ing contrastive learning naively without explicit guidance may lead to model overﬁtting low-level information such as skeleton scales and angles, while failing to learn high-order semantics, resulting in insufﬁcient feature representa-tion capabilities.
To this end, we propose RVTCLR: a Relative Visual
Tempo Contrastive Learning framework for skeleton action
Representation. The basic idea is shown at the bottom of
Figure 1. First, we observe that it’s intuitively plausible to compare relative visual tempos within videos rather than to predict a speciﬁc visual tempo for each video. Speciﬁcally, for each video, we sample 3 clips with different visual tem-pos (e.g., 1×, 1×, and 2×) to construct the contrastive pairs and train a network to pull the anchor-positive pairs clos-er while repelling the anchor-negative pairs. Furthermore, to make the representations explicitly focus on appearance information, we design another Appearance-Consistency (AC) task. In this task, pairs from the same video are attract-ed no matter their visual tempos, while pairs from different videos are pushed away. In this way, the learned represen-tations are expected to focus on both skeleton motion and appearance information simultaneously.
In addition, in order to encourage the models to learn high-order semantics, we introduce a new Distribution-Consistency (DC) branch based on RVTCLR, which con-tains three components: Skeleton-speciﬁc Data Augmen-tation (SDA), Fine-grained Skeleton Encoding Module (F-SEM), and Distribution-aware Diversity (DD) Loss. We re-fer to this as RVTCLR+. First, we leverage SDA to generate more difﬁcult contrastive pairs by applying more skeleton-speciﬁc transformations (e.g., gaussian noise) at the input level, since augmentation plays a key role in learning better representations, as demonstrated by SimCLR [3] and Mo-Co [8]. However, too much strong augmentations may blur the joint connections compared to the normal augmented sequence (i.e., crop and shear), resulting in performance degradation. Beneﬁting from recent attention mechanism-s [5, 13, 44], we try to emphasize these connections by de-signing a novel module, FSEM, which contains an Intra-Inter-Part block (IIPB) for local spatial modeling and a
Non-local block (NLB) [32] for global spatiotemporal mod-eling. Finally, inspired by [33, 6], we introduce a DD loss to minimize the distributional divergence between the nor-mal augmented view and our DC branch. By combining these components, we hope that the DC branch can better learn local and global spatiotemporal features, which help to extract discriminative high-order semantics.
To sum up, the contributions of this work include: (1) We propose a novel contrastive representation learning frame-work named RVTCLR. The proposed RVTCLR leverages
Relative Visual Tempo Learning (RVTL) task to learn bet-ter skeleton motion information. By combining anoth-er Appearance-Consistency (AC) task, our model explic-itly learns to concentrate on skeleton appearance informa-tion simultaneously. (2) To encourage models to focus on high-order semantics, we propose RVTCLR+ by introduc-ing a new Distribution-Consistency (DC) branch. This DC branch contains three components: Skeleton-speciﬁc Da-ta Augmentation (SDA), Fine-grained Skeleton Encoding
Module (FSEM), and Distribution-aware Diversity (DD)
Loss. (3) These contrastive tasks are jointly trained using a two-branch structure such that the models can learn both spatiotemporal and high-order semantics simultaneously. 2.