Abstract
Weakly-supervised temporal action localization aims to localize action instances in videos with only video-level action labels. Existing methods mainly embrace a localization-by-classification pipeline that optimizes the snippet-level prediction with a video classification loss.
However, this formulation suffers from the discrepancy be-tween classification and detection, resulting in inaccurate separation of foreground and background (F&B) snippets.
To alleviate this problem, we propose to explore the un-derlying structure among the snippets by resorting to un-supervised snippet clustering, rather than heavily relying on the video classification loss. Specifically, we propose a novel clustering-based F&B separation algorithm. It com-prises two core components: a snippet clustering compo-nent that groups the snippets into multiple latent clusters and a cluster classification component that further classi-fies the cluster as foreground or background. As there are no ground-truth labels to train these two components, we introduce a unified self-labeling mechanism based on op-timal transport to produce high-quality pseudo-labels that match several plausible prior distributions. This ensures that the cluster assignments of the snippets can be accu-rately associated with their F&B labels, thereby boosting the F&B separation. We evaluate our method on three benchmarks: THUMOS14, ActivityNet v1.2 and v1.3. Our method achieves promising performance on all three bench-marks while being significantly more lightweight than pre-vious methods. Code is available at https://github. com/Qinying-Liu/CASE 1.

Introduction
Temporal action localization (TAL) [43] is a task to lo-calize the temporal boundaries of action instances and rec-ognize their categories in videos. In recent years, numerous
*Corresponding author works put effort into the fully supervised manner and gain great achievements. Albeit successful, these methods re-quire extensive manual frame-level annotations, which are expensive and time-consuming. Without the requirement of frame-level annotations, weakly-supervised TAL (WTAL) has received increasing attention, as it allows us to detect the action instances with only video-level action labels.
There has been a wide spectrum of WTAL methods de-veloped in the literature [48, 58, 37, 29]. With only video-level labels, mainstream methods employ a localization-by-classification pipeline, which formulates WTAL as a video action classification problem to learn a temporal class ac-tivation sequence (T-CAS). For this pipeline, foreground (i.e., action) and background separation remains an open question since video-level labels do not provide any cue for background class. There are two types of existing ap-proaches to solve it. The first type [48, 58] is based on the multiple instance learning (MIL), which uses the T-CAS to select the most confident snippets for each action class.
The second type [37, 29] introduces an attention mechanism to learn class-agnostic foreground weights that indicate the probabilities of the snippets belonging to foreground. De-spite recent progress, these methods typically rely on the video classification loss to guide the learning of the T-CAS or the attention weights. There is an inherent downside: the loss is easily minimized by the salient snippets [33] and fails to explore the distribution of the whole snippets, resulting in erroneous T-CAS or attention weights. This issue is rooted in the supervision gap between the classification and detec-tion tasks. Recent studies [39, 31] are devoted to producing snippet-level pseudo-labels to bridge the gap. However, the pseudo-labels are still derived from the unreliable T-CAS or attention weights.
Deep clustering [6], which automatically partitions the samples into different groups, has been proven to be capa-ble of revealing the intrinsic distribution of the samples in many label-scarce tasks [1, 4, 10, 27]. A natural issue arises: is it possible to adopt the clustering to capture the distribu-tion of snippets? Since clustering can be conducted in a
Figure 1: Conceptual illustration of our clustering-based F&B separation algorithm. In snippet clustering, we partition the snippets (or frames) into multiple clusters with explicit characteristics. In cluster classification, we classify the clusters as foreground or background. The above results are attained according to the predictions of our method. self-supervised manner, it is immune to the video classifi-cation loss. This suggests a great potential of clustering for
F&B separation in WTAL. A brute-force solution would be to group the snippets into two clusters, one for foreground and one for background. Whereas, we empirically find that it underperforms in practice (cf. Sec. 5.3). We argue that the reason is that snippets, regardless of foreground or back-ground, can differ dramatically in appearance (cf. Fig. 1 (a)). As a result, it may be difficult for a self-supervised model to group them accurately. Fortunately, in real-world videos, there are common characteristics (e.g., ”interview”,
”running”) shared by a group of snippets (cf. Fig. 1 (b)).
Compared to learning two clusters for F&B in the complex video content, it may be easier to explore the snippet clus-ters with clear and distinctive characteristics. This necessi-tates a clustering algorithm with multiple clusters. Further-more, it can be observed that the characteristics of clusters are sometimes indicative cues for F&B separation. For ex-ample, we can confidently classify the ”running” cluster to foreground and the ”interview” cluster to background ac-cording to the cluster-level characteristics. Consequently, it is promising to further leverage the cluster-level representa-tions to assist F&B separation.
In light of the above discussion, we propose a novel
Clustering-Assisted F&B SEparation (CASE) network. We begin by constructing a standard WTAL baseline that pro-vides a primary estimation of F&B snippets. We then introduce a clustering-based F&B separation algorithm (cf. Fig. 1) to refine the F&B separation. This algorithm is comprised of two main components: snippet clustering for dividing the snippets into multiple clusters, and clus-ter classification for classifying the clusters as foreground or background. Considering that no ground-truth labels are available to train the components, we propose a unified self-labeling mechanism to generate high-quality pseudo-labels for them. Specifically, we formulate the label assignment in both components as a unified optimal transport problem, which allows us to flexibly impose several customized con-straints on the distribution of pseudo-labels. After training these two components, we can transform the cluster assign-ments of the snippets to their F&B assignments, which can be used to refine the F&B separation of the baseline.
It is demonstrated that our method yields favorable per-formance while being much more lightweight compared to prior approaches. In summary, our contributions are three-fold. 1) We propose a clustering-based F&B separation al-gorithm for WTAL, which casts the problem of F&B sep-aration as a combination of snippet clustering and cluster classification. 2) We propose a unified self-labeling mecha-nism based on optimal transport to guide snippet clustering and cluster classification. 3) We conduct extensive exper-iments that demonstrate the effectiveness and efficiency of our method compared to existing approaches. 2.