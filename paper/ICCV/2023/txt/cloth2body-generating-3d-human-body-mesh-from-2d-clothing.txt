Abstract
In this paper, we define and study a new Cloth2Body problem which has a goal of generating 3d human body meshes from a 2D clothing image. Unlike the existing human mesh recovery problem, Cloth2Body needs to ad-dress new and emerging challenges raised by the partial observation of the input and the high diversity of the out-put. Indeed, there are three specific challenges. First, how to locate and pose human bodies into the clothes. Sec-ond, how to effectively estimate body shapes out of vari-ous clothing types. Finally, how to generate diverse and plausible results from a 2D clothing image. To this end, we propose an end-to-end framework that can accurately estimate 3D body mesh parameterized by pose and shape from a 2D clothing image. Along this line, we first utilize
Kinematics-aware Pose Estimation to estimate body pose parameters. 3D skeleton is employed as a proxy followed
*Work partially conducted during an internship at ZMO AI Inc.
â€ Corresponding author by an inverse kinematics module to boost the estimation accuracy. We additionally design an adaptive depth trick to align the re-projected 3D mesh better with 2D clothing image by disentangling the effects of object size and cam-era extrinsic. Next, we propose Physics-informed Shape
Estimation to estimate body shape parameters. 3D shape parameters are predicted based on partial body measure-ments estimated from RGB image, which not only improves pixel-wise human-cloth alignment, but also enables flexi-ble user editing. Finally, we design Evolution-based pose generation method, a skeleton transplanting method in-spired by genetic algorithms to generate diverse reasonable poses during inference. As shown by experimental results on both synthetic and real-world data, the proposed frame-work achieves state-of-the-art performance and can effec-tively recover natural and diverse 3D body meshes from 2D images that align well with clothing.
1.

Introduction 3D virtual human is widely used in contemporary indus-try, such as animation and game [19, 42], VR/AR applica-tions [12] and fashion design [18, 46, 44]. 3D human repre-sentation has advantage over 2D images due to its manipula-tion flexibility and generation robustness [5]. Indeed, many studies have focused on recovering 3D humans from 2D im-ages [30, 21, 47]. However, there are some application sce-narios where target human is not present in an image but we still want to imagine the person from certain context, such as generating human face from hair or dressing [57, 34] and generating posed human in a scene [49, 53, 37].
In this paper, we are interested in the scenario of gener-ating human body meshes from a 2D clothing image. Ex-isting methods can generate 2D human via deep generative models [16, 43, 34]. However, these methods lack robust-ness and explainability. The generated 2D humans are not amenable to interactive manipulation afterwards, thus not satisfying the need of many industrial scenarios. There-fore, we formulate a novel task called Cloth2Body which aims to generate 3D virtual human mesh that can fit into 2D clothing image. This task is non-trivial as it faces three challenges: 1) Partial observation: Human body pixels are absent from the 2D clothing image and the mesh should be inferred from its interaction with clothing. 2) Pixel-wise alignment: 3D human body should be well-aligned with 2D clothing in the pixel-wise level when re-projected onto the 2D plane. 3) Diverse outputs: The same 2D clothing is potentially suitable for multiple 3D human bodies of dif-ferent poses and shapes, so we need to model this outcome diversity.
To address these challenges, we propose an effective end-to-end framework. First, partial observation com-pels the model to effectively exploit contexts and priors for making accurate predictions. As a solution, our pipeline utilizes spatial information and pose database as priors for invisible joints and exploits RGB context to accurately lo-cate visible joints. In addition, we explicitly estimate body measurements for shape estimation which significantly im-proves the model performance. Here, body measurements can either be extracted from the image by cloth landmark detection [32] for convenience or from user input for in-teractive manipulation. The explicitly estimated measure-ments improve the model both in estimation accuracy and explainability. Second, pixel-wise alignment requires the model to accurately localize 3D body in camera space. We adopt an inverse projection method and introduce a novel adaptive-depth projection trick to improve the alignment be-tween the reconstructed joints and the inversely projected ones. Unlike camera parameter regression methods [21, 8], our mechanism can guarantee image space alignment be-tween the 2D clothing and the projected 3D body given accurate pose and shape. Finally, diverse output aims to diversify out-of-cloth poses while preserving human-cloth alignment. We propose an evolution-based pose generation method that uses skeleton crossover and mutation to gener-ate diverse reasonable 3D bodies. This training-free method is simple but effective for our pose diversifying purpose.
Our method demonstrates superior results over other alter-native methods adapted to this task.
To summarize our contributions: 1) We propose a novel task Cloth2Body aiming at generating 3D human body meshes from a 2D clothing image. Many fashion-related downstream applications can benefit from this task. 2) We design an end-to-end framework that can effectively handle the new challenges emerging from the Cloth2Body prob-lem setting. Specifically, our Landmark2Shape method and the adaptive-depth projection approach can also benefit other tasks such as human mesh recovery (HMR) towards better 2D alignment and explainability. 3) We introduce two new datasets based on existing 3D human datasets for
Cloth2Body training and evaluation. 2.