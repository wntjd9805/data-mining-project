Abstract
Humans describe the physical world using natural lan-guage to refer to specific 3D locations based on a vast range of properties: visual appearance, semantics, abstract asso-ciations, or actionable affordances. In this work we propose
Language Embedded Radiance Fields (LERFs), a method for grounding language embeddings from off-the-shelf mod-els like CLIP into NeRF, which enable these types of open-*Equal contribution, corresponding authors. ended language queries in 3D. LERF learns a dense, multi-scale language field inside NeRF by volume rendering CLIP embeddings along training rays, supervising these embed-dings across training views to provide multi-view consis-tency and smooth the underlying language field. After opti-mization, LERF can extract 3D relevancy maps for a broad range of language prompts interactively in real-time, which has potential use cases in robotics, understanding vision-language models, and interacting with 3D scenes. LERF enables pixel-aligned, zero-shot queries on the distilled 3D 1
CLIP embeddings without relying on region proposals or masks, supporting long-tail open-vocabulary queries hier-archically across the volume. See the project website at: https://lerf.io. 1.

Introduction
Neural Radiance Fields (NeRFs) [25] have emerged as a powerful technique for capturing photorealistic digital rep-resentations of intricate real-world 3D scenes. However, the immediate output of NeRFs is nothing but a colorful density field, devoid of meaning or context, which inhibits building interfaces for interacting with the resulting 3D scenes.
Natural language is an intuitive interface for interacting with a 3D scene. Consider the capture of a kitchen in Fig-ure 1. Imagine being able to navigate this kitchen by asking where the “utensils” are, or more specifically for a tool that you could use for “stirring”, and even for your favorite mug with a specific logo on it — all through the comfort and fa-miliarity of everyday conversation. This requires not only the capacity to handle natural language input queries but also the ability to incorporate semantics at multiple scales and relate to long-tail and abstract concepts.
In this work, we propose Language Embedded Radi-ance Fields (LERF), a novel approach that grounds lan-guage within NeRF by optimizing embeddings from an off-the-shelf vision-language model like CLIP into 3D scenes.
Notably, LERF utilizes CLIP directly without the need for finetuning through datasets like COCO or reliance on mask region proposals, which limits the ability to capture a wide range of semantics. Because LERF preserves the integrity of CLIP embeddings at multiple scales, it is able to handle a broad range of language queries, including visual prop-erties (“yellow”), abstract concepts (“electricity”), text (“boops”), and long-tail objects (“waldo”) as illustrated in
Figure 1.
We construct a LERF by optimizing a language field jointly with NeRF, which takes both position and physical scale as input and outputs a single CLIP vector. During training, the field is supervised using a multi-scale feature pyramid that contains CLIP embeddings generated from im-age crops of training views. This allows the CLIP encoder to capture different scales of image context, thus associating the same 3D location with distinct language embeddings at different scales (e.g. “utensils” vs. “wooden spoon”). The language field can be queried at arbitrary scales during test time to obtain 3D relevancy maps. To regularize the opti-mized language field, self-supervised DINO [5] features are also incorporated through a shared bottleneck.
LERF offers an added benefit: since we extract CLIP embeddings from multiple views over multiple scales, the relevancy maps of text queries obtained through our 3D
CLIP embedding are more localized compared to those ob-tained via 2D CLIP embeddings. By definition, they are also 3D consistent, enabling queries directly in the 3D fields without having to render to multiple views.
LERF can be trained without significantly slowing down the base NeRF implementation. Upon completion of the training process, LERF allows for the generation of 3D rel-evancy maps for a wide range of language prompts in real-time. We evaluate the capabilities of LERF on a set of hand-held captured in-the-wild scenes and find it can local-ize both fine-grained queries relating to highly specific parts of geometry (“fingers”), or abstract queries relating to mul-tiple objects (“cartoon”). LERF produces view-consistent relevancy maps in 3D across a wide range of queries and scenes, which are best viewed in videos on our website. We also provide quantitative evaluations against popular open-vocab detectors LSeg [22] and OWL-ViT [26], by distilling
LSeg features into 3D [21] and querying OWL-ViT from rendered novel views. Our results suggest that features in 3D from LERF can localize a wide variety of queries across in-the-wild scenes. The zero-shot capabilities of LERF leads to potential use cases in robotics, analyzing vision-language models, and interacting with 3D scenes. Code and data is available at https://lerf.io. 2.