Abstract
We present a cascaded diffusion model based on a part-level implicit 3D representation. Our model achieves state-of-the-art generation quality and also enables part-level shape editing and manipulation without any addi-tional training in conditional setup. Diffusion models have demonstrated impressive capabilities in data generation as well as zero-shot completion and editing via a guided re-verse process. Recent research on 3D diffusion models has focused on improving their generation capabilities with var-ious data representations, while the absence of structural information has limited their capability in completion and editing tasks. We thus propose our novel diffusion model us-ing a part-level implicit representation. To effectively learn diffusion with high-dimensional embedding vectors of parts, we propose a cascaded framework, learning diffusion first on a low-dimensional subspace encoding extrinsic param-eters of parts and then on the other high-dimensional sub-space encoding intrinsic attributes. In the experiments, we demonstrate the outperformance of our method compared with the previous ones both in generation and part-level completion and manipulation tasks. Our project page is https://salad3d.github.io. 1.

Introduction
The staggering rise of the recent image generative model such as DALL-E 2 [49], StableDiffusion [50], and Midjour-ney [34] has drawn great attention to the diffusion mod-*Equal contribution. els. With the state-of-the-art performance in generating data [12, 19, 50, 49, 34], diffusion models have quickly replaced existing generative models in many applications.
Besides the quality of the generated data, another key ad-vantage of the diffusion models is the zero-shot capability in completion and editing. Recent research [10, 30, 32] has shown that diffusion models trained without any conditions can be applied to completion and editing tasks by starting the reverse process from partial data and properly guiding the process.
Such capabilities of the diffusion models have prompted attempts to apply them to 3D generation [3, 31, 41, 65, 62, 27, 39, 22], although likewise the other neural 3D genera-tion and reconstruction work, the key challenge in applying diffusion to 3D is to find an appropriate representation of 3D data. Particularly, to take full advantage of the diffusion models, both producing realistic data and being leveraged to editing and manipulation, a careful design of the 3D data representation is needed. A naive adaption of the 2D image diffusion models to the 3D voxels is impractical due to the order of magnitude more computation time and memory.
Hence, the earlier attempt to apply diffusion or score-based models to 3D (which has also been continued until recently) was to use point clouds as 3D representation [3, 31, 41], al-though the fine details of shapes could not be reproduced since the training computation is still too heavy to increase the resolution — 2k points are used in training. Later, some hybrid representations have been explored, such as points and voxels [65], points and features [62], voxels and fea-tures [27], although these were still limited in being trained
Implicit representation has with low-resolution 3D data. been proven to be the best to capture fine details in 3D gen-eration and reconstruction [42, 7, 33]. Hence, concurrent work [27, 39] introduced latent diffusion methods gener-ating codes that can be decoded into implicit functions of 3D shapes. However, then the diffusion in a latent space cannot be used for the guided reverse process – e.g., fill-ing a missing part of a shape while preserving the others, and thus the model cannot be exploited for manipulation.
Neural wavelet [22] is a notable exception that improves ef-ficiency in training without a latent space but by learning diffusion in spectral wavelet space. While it succeeded in producing local details, it is still nontrivial to specify a local region to be modified in the spectral space, thus limiting the model to be used in the manipulation tasks.
As a 3D diffusion model feeding two birds with one seed, achieving high-quality generation and enabling ma-nipulation, we present our novel Shape PArt-Level LAtent
Diffusion Model, dubbed SALAD. Our work is inspired by recent work [15, 21, 28, 18] introducing disentangled im-plicit representations into parts. The advantages of the part-level disentangled representation are in the efficiency allo-cating the memory capacity of the latent code effectively to multiple parts, and also in the locality allowing each part to be edited independently, thus best fitted to our purpose. We specifically base our work on SPAGHETTI [18] that learns the part decomposition in a self-supervised way. Each part is described with an independent embedding vector describ-ing the extrinsics and intrinsics of the part as shown in Fig-ure 1, and thus the parts that need to be edited or replaced can be easily chosen. It is a crucial difference from latent diffusion where the latent codes do not explicitly express any spatial and structural information and voxel diffusion where the region to be modified can only be specified in the 3D space, not in the shape.
Our technical contribution is the diffusion neural net-work designed to properly handle the characteristics of the part-level implicit representation, which is a set of high-dimensional embedding vectors. To cope with the set data and achieve permutation invariance while allowing global communications across the parts, we employ Trans-former [57] and condition each self-attention block with the timestep in the diffusion process. The challenge is also in learning diffusion in the high-dimensional embedding space, which is known to be hard to train [61]. To get around the issue, we introduce a two-phase cascaded dif-fusion model. We leverage the fact that the part embed-ding vector is split into a small set of extrinsic parameters approximating the shape of a part and a high-dimensional intrinsic latent supplementing the detailed geometry infor-mation. Hence, our cascaded pipeline learns two diffusions, one generating extrinsic parameters first and the other pro-ducing an intrinsic latent conditioned on the extrinsics, ef-fectively improving the generation quality with the same computation resources.
Our quantitative and qualitative assessments on SALAD demonstrate its outperformance compared with SotA meth-ods in shape generation as shown in Section 5.1. We fur-ther demonstrate zero-shot manipulation capability of our
SALAD, trained solely for unconditional generation, by conducting extensive experiments on downstream tasks, in-cluding part completion (Section 5.2), part mixing and re-finement (Section 5.3). Last but not least, we showcase the versatility of SALAD in modeling multi-modal distribu-tions such as text-guided generation (Section 5.4) and com-pletion (Section 5.5). To summarize, our contributions are:
• We propose SALAD, a novel diffusion model capable of generating part-level 3D implicit representations.
• We propose a two-phase cascaded diffusion model, effective for handling high-dimensional latent spaces, that sets a new SotA in shape generation.
• We demonstrate the importance of orchestrating diffu-sion models and part-level implicit representation for the zero-shot capability of SALAD in shape editing.
• We further extend our SALAD to text-guided gener-ation and editing that can synergize with text-driven part segmentation network. 2.