Abstract
Vector-quantized image modeling has shown great po-tential in synthesizing high-quality images. However, gen-erating high-resolution images remains a challenging task due to the quadratic computational overhead of the self-attention process.
In this study, we seek to explore a more efficient two-stage framework for high-resolution im-age generation with improvements in the following three aspects. (1) Based on the observation that the first quan-tization stage has solid local property, we employ a local attention-based quantization model instead of the global at-tention mechanism used in previous methods, leading to bet-ter efficiency and reconstruction quality. (2) We emphasize the importance of multi-grained feature interaction during image generation and introduce an efficient attention mech-anism that combines global attention (long-range seman-tic consistency within the whole image) and local attention (fined-grained details). This approach results in faster gen-eration speed, higher generation fidelity, and improved res-olution. (3) We propose a new generation pipeline incorpo-rating autoencoding training and autoregressive generation strategy, demonstrating a better paradigm for image syn-thesis. Extensive experiments demonstrate the superiority of our approach in high-quality and high-resolution image reconstruction and generation. 1.

Introduction
High-fidelity image synthesis has achieved promising performance thanks to the progress of generative mod-els, such as generative adversarial networks (GANs) [12,
∗ This work was done during an internship at Machine Intelligence
Technology Lab, Alibaba Group.
† Corresponding author. 21, 22], diffusion models [15, 8] and autoregressive mod-els [11, 44]. Moreover, high-resolution image generation, a vital generation task with many practical applications, pro-vides better visual effects and user experience in the adver-tising and design industries. Some recent studies have at-tempted to achieve high-resolution image generation. Style-GAN [21, 22] leverages progressive growth to generate high-resolution images. However, GAN-based models of-ten suffer from training stability and poor mode cover-age [35, 48]. As diffusion models continue to evolve, recent studies [31, 34] have begun to explore the utilization of cas-caded diffusion models for generating high-resolution im-ages. This approach involves training multiple independent and enormous models to collectively accomplish a gener-ation task. On another note, some researchers [11, 44, 5] leverage a two-stage vector-quantized (VQ) framework for image generation, which first quantizes images into discrete latent codes and then model the data distribution over the discrete space in the second stage. Nonetheless, under the limited computational resources (e.g., memory and train-ing time), the architectures of the existing vector-quantized methods are inferior. In this paper, to solve the problems of existing models, we would like to explore a more efficient two-stage vector quantized framework for high-resolution image generation and make improvements from the follow-ing three aspects.
Firstly, prior methods [11, 44] claim the importance of the attention mechanism in the first quantization stage for better image understanding, and they leverage global atten-tion to capture long-range interactions between discrete to-kens. However, we find this global attention not necessary for image quantization based on the observation that the al-teration of several tokens will only influence their nearby tokens. Hence, local attention can yield satisfactory re-construction results and circumvent the computationally in-tensive nature of global attention, especially when gener-ating high-resolution images. Consequently, we propose
Efficient-VQGAN for image quantization adopting image feature extractor with local attention mechanism. This con-tributes to the acceleration of image reconstruction and ded-icates more computation to the local information, further improving the reconstruction quality.
Besides, for the second stage of the existing vector-quantized methods [11, 44, 5], it would be intractable to generate high-resolution images since the quadratic space and time complexity is respected to the discrete sequence length. Further, the global self-attention interaction could lead to the insufficient ability to capture fine details in local areas. Accordingly, the fined-grained local attention at a to-ken level for better local details capturing plays an essential role as coarse-grained global interaction for long-range con-text information capturing. We then utilize multi-grained at-tention, which implements different granularity of attention operations depending on the distance between tokens. As a result, it can support high-resolution image generation with a reduced length of the quantized image token sequence and reasonable computational cost.
Additionally, some recent studies related to text gener-ation [43, 2] in the field of natural language processing, which combine the merits of autoencoding pretraining and autoregressive generation, show great potential in gener-ating high-quality text sequence. Pretrained autoencoding models like BERT [23] can exploit bidirectional context to capture more information for reconstructing the masked in-put corpus, while autoregressive generation performing ex-plicit density estimation can ensure consistency of output token sequence.
Inspired by such combined training and inference strategy, we propose a similar pipeline for im-In the training stage, we utilize an age generation tasks. autoencoding-based masked visual token modeling strategy which is trained to recover the randomly masked image to-kens by attending to tokens from all directions, better cap-turing contextual information. In the inference stage, com-bined with our block-based multi-grained attention mech-anism, we autoregressively sample each image block in a fixed order and iteratively sample the tokens within the block in parallel, contributing to improved sampling speed and generation quality.
The contributions of this work can be summarized as fol-lows. (1) We propose a more efficient two-stage vector-quantized framework with several improvements in the first quantization stage and the second generative model-ing stage, yielding faster computational efficiency and bet-ter image quality. (2) We propose a new image genera-tion pipeline that combines the advantages of autoencoding training and autoregressive generation, further improving the synthesis quality. (3) The proposed two-stage vector-quantized model demonstrates the capability to generate higher-quality images at a faster speed on FFHQ and Im-ageNet datasets compared to previous methods. 2.