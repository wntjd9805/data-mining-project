Abstract
Recently emerged Vision-and-Language Navigation (VLN) tasks have drawn signiﬁcant attention in both com-puter vision and natural language processing communi-ties. Existing VLN tasks are built for agents that navigate on the ground, either indoors or outdoors. However, many tasks require intelligent agents to carry out in the sky, such as UAV-based goods delivery, trafﬁc/security
†These authors contribute equally to this work
∗Corresponding Author patrol, and scenery tour, to name a few. Navigating in the sky is more complicated than on the ground be-cause agents need to consider the ﬂying height and more complex spatial relationship reasoning. To ﬁll this gap and facilitate research in this ﬁeld, we pro-pose a new task named AerialVLN, which is UAV-based and towards outdoor environments. We develop a 3D simulator rendered by near-realistic pictures of 25 city-level scenarios. Our simulator supports continuous nav-igation, environment extension and conﬁguration. We also proposed an extended baseline model based on the widely-used cross-modal-alignment (CMA) naviga-tion methods. We ﬁnd that there is still a signiﬁcant gap between the baseline model and human perfor-mance, which suggests AerialVLN is a new challeng-ing task. Dataset and code is available at https:
//github.com/AirVLN/AirVLN . 1.

Introduction
Recently, a bunch of vision-and-language navigation tasks, such as R2R [2], RxR [20], REVERIE [28], Touch-Down [7], Alfred [33], iGibson [23, 32, 36], have drawn a large amount of attention from different research commu-nities like computer vision, natural language processing and robotics. These tasks as well as their datasets have greatly boosted the research of assembling the capabili-ties of vision and language understanding, cross-modality matching, path planning and reasoning [6, 8, 15, 18, 27].
However, all these VLN tasks are designed for ground-based agents, which means agents can only navigate indoors or outdoors on the ground. This overlooks an-other important application scenario: activities in the sky, which are becoming increasingly popular with the devel-opment of unmanned aerial vehicles (UAVs), especially multirotor. We can now use UAVs to enjoy spectacular scenes without going out of houses and they can be po-tentially utilized for goods delivery, trafﬁc surveillance, search/rescue and security patrol [10, 11, 12].
To release humans from manually operating UAVs and to ﬁll the research gap in the ﬁeld of navigation in the sky, we propose a city-level UAV-based vision-and-language navigation task, named AerialVLN, and a cor-responding dataset. Navigating in the sky is signiﬁcantly different from that on the ground in several aspects. First,
AerialVLN has a larger action space. Compared to con-ventional ground VLN [2, 7, 19, 20, 28], AerialVLN re-quires intelligent agents to additionally take actions such as “rise up” and “pan down” into consideration. More-over, multirotors can move left/right without turning its head. Second, the outdoor environments of AerialVLN are much bigger and more complex. AerialVLN covers a large variety of city-level scenes. An intelligent agent is required to distinguish referred buildings/objects by their spatial relationship from a bird-view as shown in Figure 1. Although the TouchDown [7] task is also devised for outdoor navigation, its environments are static, while ours are interactive and dynamic. For example, our agent can land on a building, and the weather and illumination conditions can dynamically change in the environment.
Third, to mimic multirotor ﬂying in real life, our Aeri-alVLN has a much longer path than ground VLNs. On average, our AerialVLN involves a path length of 661.8 units∗. There are about 9.7 referred objects in one instruc-tion on average, which is more than 2.6 times as many as in the R2R dataset [2]. Fourth, intelligent agents must learn to avoid getting stuck on objects in 3D space.
This is more challenging than avoiding obstacles when navigating on the ground as in VLN-CE [19] because agents have to estimate the 3D shapes of obstacles and the distance to obstacles. All these new characters render
AerialVLN a different and highly challenging task.
AerialVLN is implemented using Unreal Engine 4
[14] and Microsoft AirSim plugins [31], which enables continuous navigation and near-realistic rendering. In total, we have collected 25 different city-level environ-ments, covering a variety of scenes such as downtown cities, factories, parks, and villages, including more than 870 different kinds of objects. Our AerialVLN dataset consists of 8,446 ﬂying paths obtained by experienced human UAV pilots who hold the AOPA (Aircraft Own-ers and Pilots Association) certiﬁcate. We pair each path with 3 instructions annotated by AMT workers in the stan-dard dataset setting. Notably, we also align each sub-path to its sub-instruction, which enables ﬁne-grained cross-modality matching learning. On average, up to 83 words are in each instruction, involving a large vocabulary of 4,470 words. Finally, we evaluate ﬁve baselines, includ-ing two golden standard VLN models in VLN, Seq2Seq model and cross-modal matching (CMA) model, and our proposed model to serve as starting baselines on Aeri-alVLN. 2.