Abstract
Neural implicit fields are powerful for representing 3D scenes and generating high-quality novel views, but it re-mains challenging to use such implicit representations for creating a 3D human avatar with a specific identity and artistic style that can be easily animated. Our proposed method, AvatarCraft, addresses this challenge by using dif-fusion models to guide the learning of geometry and tex-ture for a neural avatar based on a single text prompt. We carefully design the optimization framework of neural im-plicit fields, including a coarse-to-fine multi-bounding box training strategy, shape regularization, and diffusion-based constraints, to produce high-quality geometry and texture.
Additionally, we make the human avatar animatable by de-forming the neural implicit field with an explicit warping field that maps the target human mesh to a template human mesh, both represented using parametric human models.
This simplifies animation and reshaping of the generated avatar by controlling pose and shape parameters. Extensive experiments on various text descriptions show that Avatar-Craft is effective and robust in creating human avatars and rendering novel views, poses, and shapes. Our project page is: https://avatar-craft.github.io/. 1.

Introduction
Creating human avatars is crucial for content generation in various immersive media, where users can alter the char-acter to a specific identity, apply an artistic style, or ani-*Corresponding Author. mate with simple motion control. While traditional manual authoring of digital characters often involves cumbersome and time-consuming efforts from skilled artists, the recent progress in human digitization has shown exciting potential towards more user-friendly solutions. Nevertheless, avatar generation still faces a set of tough challenges. First of all, intuitive control is highly coveted for the system to under-stand specific user needs in the most natural form. Second, the generated avatars should be immediately ready for ap-plications such as view synthesis, scene composition, and retargetable animation. Finally, the avatars should be of high quality, considering both the overall visual fidelity and preservation of target styles or identities in geometry and texture, especially when being manipulated or animated.
Significant efforts have been made in search of natu-ral user controls for avatars. One representative stream of works [7, 2, 11, 47, 52, 51, 50] takes reference images to stylize an avatar. Unfortunately, finding suitable refer-ences that perfectly match the desired shape and appear-ance is not always easy, substantially limiting real-world use. On the other hand, text prompts are attracting more attention as a more natural control for generating high-quality 3D avatars, with the recent advances in large-scale vision-language models.
In particular, text-to-3D avatar creation [16, 56, 31, 53] is explored by leveraging the zero-shot generation ability of Contrastive Language-Image Pre-Training (CLIP) [41]. Following this trend, our work also tackles the problem of text-guided avatar creation. In par-ticular, we aim at high-quality 3D avatar generation, which not only supports static view synthesis but also allows for controllable animation.
Text-driven avatar creation poses great challenges in pro-ducing high-quality geometry and texture while providing flexible animation capabilities. Existing methods [31, 56, 16] address these challenges by adopting cross-modal su-pervision to guide the generation and modeling avatars as explicit meshes to support skeleton-driven animation [16].
However, despite the powerful representational capabil-ity of CLIP, due to the semantic structures and complex deformations of human bodies, these methods oftentimes struggle to produce detailed and consistent appearances for avatars [56]. On a parallel thread, the pioneering text-conditional diffusion models [39, 27] demonstrate stronger text-to-3D generation ability compared to CLIP. However, these approaches focus on general static objects rather than animatble human avatars. To address these challenges, for the first time ever, we propose to tackle text-guided avatar creation leveraging a diffusion model for guidance, leading to improved results in terms of consistency and quality.
Additionally, instead of mesh-based representations [31, 56], we exploit neural implicit fields [33, 54] to represent the avatar, which allow for volume rendering and generate high-quality novel views, making them especially advan-tageous for complex topology reconstructions and photo-realistic renderings. The implicit representation also makes it straightforward to composite the avatar with any implicit 3D scene while preserving realistic occlusions. To animate the avatar, we use SMPL to directly deform the neural im-plicit field, which enables a flexible way to animate and re-shape the avatar by controlling the pose and shape parame-ters of SMPL, without requiring additional training.
In this paper, we propose AvatarCraft, an approach for transforming text into neural human avatars, which uses diffusion models to stylize the geometry and texture, with shape and pose controlled by parametric human models.
Our method starts with a bare neural human avatar as the template. Given a text prompt, we use diffusion models [43] to guide the creation of a human avatar by updating the template with geometry and texture that are consistent with the text. However, directly applying diffusion models [43] can lead to distorted geometry and textures as the diffusion loss [39] is sensitive to the input resolution and biased to-wards geometry modeling. To address this issue, we design a novel coarse-to-fine multi-bounding-box training strategy, where the diffusion model guides the creation at multiple scales to improve the global style consistency while pre-serving fine details. We also introduce a shape regular-ization method to penalize the accumulated ray opacity of the avatar, to stabilize the optimization process. Regard-ing avatar animation, unlike skeleton-driven mesh anima-tion in previous approaches [16], our method defines an ex-plicit warping field that maps the target human parameter-ization to the template human avatar and uses the warping field to deform the neural implicit field directly. Overall, our method enables parametrized shape and motion control of avatars and supports high-quality novel view synthesis, scene composition, and animation simultaneously. In sum-mary, our contributions are as follows:
• We present a text-guided method for creating high-quality 3D human avatars that outperform previous ap-proaches by using diffusion models as guidance.
• Our method enables easy animation and reshaping of neural avatar radiance fields using only the pose and shape parameters of the SMPL model, without requir-ing any additional training.
• We demonstrate the ability to composite our neu-ral avatar radiance fields with real neural scenes for occlusion-aware novel view synthesis. 2.