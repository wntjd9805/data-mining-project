Abstract
We propose a method for editing NeRF scenes with text-instructions. Given a NeRF of a scene and the collection of images used to reconstruct it, our method uses an image-conditioned diffusion model (InstructPix2Pix) to iteratively edit the input images while optimizing the underlying scene, resulting in an optimized 3D scene that respects the edit in-struction. We demonstrate that our proposed method is able to edit large-scale, real-world scenes, and is able to accom-plish more realistic, targeted edits than prior work. Result videos can be found on the project website: https://instruct-nerf2nerf.github.io. 1.

Introduction
With the emergence of efﬁcient neural 3D reconstruc-tion techniques, capturing a realistic digital representation of a real-world 3D scene has never been easier. The pro-cess is simple: capture a collection of images of a scene from varying viewpoints, reconstruct their camera param-eters, and use the posed images to optimize a Neural Ra-diance Field [26]. Due to its ease of use, we expect cap-tured 3D content to gradually replace the traditional pro-cesses of manually-generated assets. Unfortunately, while the pipelines for turning a real scene into a 3D representa-tion are relatively mature and accessible, many of the other necessary tools for the creation of 3D assets (e.g., those needed for editing 3D scenes) remain underdeveloped.
Traditional processes for editing 3D models involve spe-cialized tools and years of training in order to manually sculpt, extrude, and re-texture a given object. This pro-cess is made even more involved with the advent of neural representations, which often do not have explicit surfaces.
This further motivates the need for 3D editing approaches designed for the modern era of 3D representations, particu-larly approaches that are similarly as accessible as the cap-ture techniques themselves.
To this end, we propose Instruct-NeRF2NeRF, a method for editing 3D NeRF scenes that requires as input only a
Dataset Update
Original Dataset Image ructPix2Pix
InstructPix2Pix
Conditioning
Signal
Text Prompt
“Turn the bear into a grizzly bear”
Figure 2: Overview: Our method gradually updates a reconstructed NeRF scene by iteratively updating the dataset images while training the NeRF: (1) an image is rendered from the scene at a training viewpoint, (2) it is edited by InstructPix2Pix given a global text instruction, (3) the training dataset image is replaced with the edited image, and (4) the NeRF continues training as usual.
Current NeRF Render
Noise text instruction. Our approach operates on a pre-captured 3D scene and ensures that the resulting edits are reﬂected in a 3D-consistent manner. For example, given a 3D scene capture of a person shown in Figure 1 (left), we can enable a wide variety of edits using ﬂexible and expressive textual instructions such as “Give him a cowboy hat” or “Turn him into Albert Einstein.” Our approach makes 3D scene editing accessible and intuitive for everyday users.
Though there exist 3D generative models, the data-sources required for training these models at scale are still limited. Therefore, we instead choose to extract shape and appearance priors from a 2D diffusion model. Speciﬁcally, we employ a recent image-conditioned diffusion model,
InstructPix2Pix [2], which enables instruction-based 2D image editing. Unfortunately, applying this model on individual images rendered from a reconstructed NeRF produces inconsistent edits across viewpoints. As a solution to this, we devise a simple approach similar to recent 3D generation solutions like DreamFusion [33]. Our underly-ing method, which we refer to as Iterative Dataset Update (Iterative DU), alternates between editing the “dataset” of NeRF input images, and updating the underlying 3D representation to incorporate the edited images.
We evaluate our approach on a variety of captured NeRF scenes, validating our design choices by comparing with ab-lated variants of our method, as well as na¨ıve implementa-tions of the score distillation sampling (SDS) loss proposed in DreamFusion [33]. We also qualitatively compare our ap-proach to a concurrent text-based stylization approach [46].
We demonstrate that our method can accomplish a wide va-riety of edits on people, objects, and large-scale scenes. 2.