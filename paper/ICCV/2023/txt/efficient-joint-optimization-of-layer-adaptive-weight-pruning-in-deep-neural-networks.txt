Abstract
In this paper, we propose a novel layer-adaptive weight-pruning approach for Deep Neural Networks (DNNs) that addresses the challenge of optimizing the output distortion minimization while adhering to a target pruning ratio con-straint. Our approach takes into account the collective influence of all layers to design a layer-adaptive pruning scheme. We discover and utilize a very important additiv-ity property of output distortion caused by pruning weights on multiple layers. This property enables us to formulate the pruning as a combinatorial optimization problem and efficiently solve it through dynamic programming. By de-composing the problem into sub-problems, we achieve lin-ear time complexity, making our optimization algorithm fast and feasible to run on CPUs. Our extensive experi-ments demonstrate the superiority of our approach over ex-isting methods on the ImageNet and CIFAR-10 datasets. On
CIFAR-10, our method achieves remarkable improvements, outperforming others by up to 1.0% for ResNet-32, 0.5% for VGG-16, and 0.7% for DenseNet-121 in terms of top-1 accuracy. On ImageNet, we achieve up to 4.7% and 4.6% higher top-1 accuracy compared to other methods for VGG-16 and ResNet-50, respectively. These results highlight the effectiveness and practicality of our approach for enhanc-ing DNN performance through layer-adaptive weight prun-ing. Code will be available on https://github.com/
Akimoto-Cris/RD_VIT_PRUNE. 1.

Introduction
Deep Neural Networks (DNNs) [22, 34, 35, 17, 19] play a critical role in various computer vision tasks. However, to achieve high accuracy, DNNs typically require large num-ber of parameters, which makes it very energy-consuming and is difficult to be deployed on resource-limited mobile
*These authors contributed equally to this work devices [16, 15]. Pruning is one of the powerful ways to reduce the complexity of DNNs. By removing the redun-dant parameters, the operations can be significantly reduced (e.g., FLOPs), which leads to faster speed and less energy-consuming. Typically, pruning approaches can be divided into two categories: structured pruning [14, 1, 9, 31, 18, 30] and weight (unstructured) pruning [27, 32, 28, 39, 16, 15].
Structured pruning approaches consider a channel or a kernel as a basic pruning unit, while weight pruning ap-proaches consider a weight as a basic pruning unit. The former is more hardware-friendly and the latter is able to achieve higher pruning ratio.
In this paper, we focus on improving the weight prun-ing and propose a novel jointly-optimized layer-adaptive approach to achieve state-of-the-art results between FLOPs and accuracy. Recent discoveries [10, 13, 25] demonstrate that layer-adaptive sparsity is the superior pruning scheme.
However, one drawback in prior layer-adaptive approaches is that they only consider the impact of a single layer when deciding the pruning ratio of that layer. The mutual impact between different layers is ignored. Moreover, another chal-lenge is that the search space of the pruning ratio for each layer increases exponentially as the number of layers. In a deep neural network, the number of layers can be a hundred or even a thousand, which makes it very difficult to find the solution efficiently.
In our approach, we define a joint learning objective to learn the layer-adaptive pruning scheme. We aim to min-imize the output distortion of the network when pruning weights on all layers under the constraint of target pruning ratio. As the output distortion is highly related to accuracy, our approach is able to maintain accuracy even at high prun-ing ratios. We explore an important property of the output distortion and find that the additivity property [42, 41, 38] holds when we prune weights on multiple layers. In other words, the output distortion caused by pruning all layers’ weights equals to the sum of the output distortion due to the
(a) Sparsity=0.1. (b) Sparsity=0.2. (c) Sparsity=0.5. (d) Sparsity=0.8.
Figure 1: An example of additivity property collected on ResNet-32 on CIFAR-10. The vertical axis shows the output distortion when pruning only two consecutive layers. The horizontal axis shows the sum of the output distortion due to the pruning the involved two layers individually. Sub-figures display the situations when all layers in the model are assigned with the corresponding sparsity. pruning of each individual layer. We provide a mathemati-cal derivation for the additivity property by using the Taylor series expansion.
Moreover, utilizing the additivity property, we develop a very fast method to solve the optimization via dynamic programming, which has only linear time complexity. We rewrite the objective function as a combinatorial optimiza-tion problem. By defining the state function and the recur-sive equation between different states, we can decompose the whole problem into sub-problems and solve it via dy-namic programming. In practice, our approach is able to find the solution in a few minutes on CPUs for deep neural networks. Note that different from other approximation al-gorithms, dynamic programming is able to find the global optimal solution, which means that our approach provides optimal pruning scheme with minimal output distortion. We summarize the main contributions of our paper as follows:
• We propose a novel layer-adaptive pruning scheme that jointly minimizes the output distortion when pruning the weights in all layers. As the output distortion is highly related to the accuracy, our approach main-tains high accuracy even when most of the weights are pruned. We also explore an important additivity prop-erty for the output distortion based on Taylor series ex-pansion.
• We develop a fast algorithm to solve the optimiza-tion via dynamic programming. The key idea is to rewrite the objective function as a combinatorial op-timization problem and then relax the whole problem into tractable sub-problems. Our method can find the solution of a deep neural network in a few minutes.
• Our approach improves state-of-the-arts on various deep neural networks and datasets.
The rest of our paper is organized as follows. We discuss the related works in section 2. In section 3, we develop our approach in detail. We present the objective function, the optimization method, and the time complexity analysis of the algorithm. In the last section, we provide the compre-hensive experimental results. 2.