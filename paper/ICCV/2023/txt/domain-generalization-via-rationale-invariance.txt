Abstract
This paper offers a new perspective to ease the challenge of domain generalization, which involves maintaining robust results even in unseen environments. Our design focuses on the decision-making process in the final classifier layer.
Specifically, we propose treating the element-wise contri-butions to the final results as the rationale for making a decision and representing the rationale for each sample as a matrix. For a well-generalized model, we suggest the ra-tionale matrices for samples belonging to the same category should be similar, indicating the model relies on domain-invariant clues to make decisions, thereby ensuring robust results. To implement this idea, we introduce a rationale in-variance loss as a simple regularization technique, requiring only a few lines of code. Our experiments demonstrate that the proposed approach achieves competitive results across various datasets, despite its simplicity. Code is available at https://github.com/liangchen527/RIDG. 1.

Introduction
Most existing machine learning models implicitly assume the training and test data are drawn from a similar distribu-tion. While in practice, the real-world test samples often ex-hibit different characteristics due to distribution shift [35, 67], resulting in an unsatisfactory performance for the deployed model. This limitation hinders the further application of deep models in various tasks, such as autonomous driving or object recognition. Hence, it is crucial importance to develop effective domain generalization (DG) methods that can maintain robust results regardless of domain shift.
The seminal work[7] theoretically proves that well-generalized representations should remain consistent across different environments. Following this principle, various
DG methods have been proposed to identify invariant fea-tures that are stable across diverse domains. These efforts
*Corresponding authors. This work is done when L. Chen is an intern in Tencent AI Lab. (a) Feature invariance (b) Logit invariance (c) Rational invariance (ours)
Figure 1. Visualized versions of different invariance regularizations.
Here ∥ · ∥ are the l2 norm; left and right sides of figures (a) - (c) denote the different feature, logit, and rational elements from the sample and the corresponding mean value; W is the weight in the classifier; ⊙ is the element-wise product. In our setting, the rational matrix contains all element-wise contributions to the final results. Different from feature and logit invariance regularization, our rational invariance term considers both the feature and classifier weights involved in making a decision, providing a fine-grained characterization of the decision-making process. include explicit feature alignment[45, 25, 41, 30, 2], domain-adversarial training [24, 41, 65, 43], gradient regulariza-tion [1, 36, 55, 50], and meta-learning skills [39, 3, 19, 40], to name a few. Despite some notable achievements, however,
DG remains a formidable challenge and is far from being solved. In fact, a recent study [27] reveals that most current state-of-the-art methods perform inferior to the baseline em-pirical risk minimization (ERM) method[60] when applied with controlled model selection and restricted hyperparam-eter search. This finding highlights the need for innovative and effective models capable of maintaining robustness.
This work takes a different path toward achieving robust outputs by emphasizing the decision-making process in the classifier layer of a deep neural network, rather than focusing solely on the features. For most existing models, the final output logits are computed by multiplying the penultimate layer’s feature with the classifier’s weights1. Delving deep into the process, each logit value can be regarded as the sum-mation of the element-wise products between the feature and 1For simplicity, we omit the bias in the classifier.
the corresponding weight. Considering each product term as a contribution to the corresponding logit, we collect all these contributions for logits from all classes as a matrix and then reinterpret them as the rationale for making decisions regarding the input sample.
By introducing the new concept of rationale, we can refine and extend the invariance principle [7] from a new perspec-tive. We posit that to ensure robust results, a well-generalized model should make decisions based on clues that are stable across samples and domains. Building on this intuition, we propose a regularization term that enforces similarity be-tween the rationale matrix from each sample and the mean rationale matrix for the corresponding class. To implement our idea, we dynamically calculate the class-wise mean ra-tionale matrix through momentum updates during training, which can be implemented with just a few lines of code. As depicted in Figure 1, our approach differs from previous feature-based regularization [10] in that we also consider the influence of the classifier, preventing biased estimation of feature importance. Additionally, by providing a more fine-grained characterization of the decision-making process, our model overcomes the limitation of logit-based regular-ization [48], which fails to account for the varying impacts of each contribution to the final decision. Our experimental study also demonstrates that the proposed rationale invari-ance regularization strategy outperforms the feature and the logits invariance regularization schemes (see Figure 2).
By conducting extensive experiments on both the Do-mainBed [27] and Wilds [35] benchmarks, we show that the proposed method consistently improves upon the baseline method and achieves comparable performance against state-of-the-art models. These results highlight the effectiveness of our new idea, despite its simplicity.
The contributions of this work are three-fold:
• We introduce the concept of rationale in the decision-making process, which is new in the literature to the best of our knowledge, to improve DG.
• We propose a simple-but-effective strategy for utilizing the rationale concept toward the robust output objective, which is conducted by enforcing consistency between the rationale matrix from each sample and its corre-sponding mean value.
• We conduct extensive experiments on the existing benchmark with rigorous evaluation protocol [27] and demonstrate that the proposed rationale-based method can perform favorably against existing arts. 2.