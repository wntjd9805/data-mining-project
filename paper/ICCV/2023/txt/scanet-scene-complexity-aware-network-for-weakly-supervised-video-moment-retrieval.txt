Abstract
Video moment retrieval aims to localize moments in video corresponding to a given language query. To avoid the expensive cost of annotating the temporal moments, weakly-supervised VMR (wsVMR) systems have been stud-ied. For such systems, generating a number of proposals as moment candidates and then selecting the most appropriate proposal has been a popular approach. These proposals are assumed to contain many distinguishable scenes in a video as candidates. However, existing proposals of wsVMR sys-tems do not respect the varying numbers of scenes in each video, where the proposals are heuristically determined ir-respective of the video. We argue that the retrieval system should be able to counter the complexities caused by vary-ing numbers of scenes in each video. To this end, we present a novel concept of a retrieval system referred to as Scene
Complexity Aware Network (SCANet), which measures the
‘scene complexity’ of multiple scenes in each video and gen-erates adaptive proposals responding to variable complex-ities of scenes in each video. Experimental results on three retrieval benchmarks (i.e. Charades-STA, ActivityNet, TVR) achieve state-of-the-art performances and demonstrate the effectiveness of incorporating the scene complexity. 1.

Introduction
Video search has the core building block of recently growing video streaming services (e.g. YouTube, Netﬂix).
To enhance the capability of video search, video moment retrieval (VMR) aims to localize the start and end time of the moment pertinent to a given language query in an untrimmed video. The success of the VMR provides us with accurate video contextual information in less time and ef-fort. Until recently, these remarkable search performances have been dependent on the size and quality of labeled training datasets. However, these datasets cost a labor-intensive annotating process (i.e. Annotators should ﬁnd the start-end time of moments corresponding to query descrip-tions), and sometimes the annotated moments are ambigu-ous. To cope with this problem, many weakly-supervised
VMR (wsVMR) methods [21, 5, 42, 44] have been pro-posed by only utilizing the video-query pairs, which are less laborious to annotate.
To perform the weak supervision using video-query pairs, if one query is paired (i.e. annotated) with multiple videos, we can identify the common scene among these videos and determine the alignment between the query and the scene. To implement this, all videos are divided into multiple segments, and the retrieval system maximizes the similarity scores between each query and paired segments while suppressing the scores between the query and un-paired segments in other videos. During the inference, the system selects a segment with the highest score as a mo-ment prediction for a given query. For the wsVMR systems to accurately classify the best segment in a video, numer-ous video-language joint representation learning methods
[16, 24, 44, 32] have been proposed.
Recently, researchers also have another focus on a study of how to generate video segments to capture many scenes in a video [20, 44]. These segments are referred to as ‘can-didate moment proposals’, which is crucial, as they directly affect the retrieval performances by regulating the proposal quantities. Unfortunately, as supervision is not available in generating proposals, wsVMR systems [43, 44] use a ﬁxed number of proposals for all input videos under heuristic op-timization of a speciﬁc dataset, which is not reasonable to deal with varying numbers of scenes in a video. While some methods [20, 10] consider varying numbers of proposals for each video, they still rely on spurious correlations, such as generating proposals proportionally to the video length or using sliding window. Therefore, the current proposal gen-eration method could not accurately respond to the diverse number of scenes in each video. We refer to this situation as a ‘scene-proposal mismatch’. For instance, in Figure 1(a), the systems produce an unnecessarily large number of proposals by referring to the long length of the video, but the video only contains a single scene (i.e. scene of sit-ting still in a chair throughout the video), which should be
160 seconds (Long video) 17 seconds (Short video)
[ mIoU score distributions ]
* Experiments on Charades-STA
# of Scenes
Irregular scores mean IoU (mIoU)
Video
Scene 1 : sit in a chair
Proposal generation
Proposal
Proposal
Proposal
Proposal
Proposal
Scene 1 : runs down stairs
Scene 2 : eat bread
Scene 3 : open door
Proposal generation
Proposal
Proposal
Proposal
Proposal
Proposal
Proposal
Proposal
Proposal
Proposal
Proposal (many proposals, but few scenes) (few proposals, but many scenes) (a) Scene-proposal mismatch 4 3 2 1 0.5 0.45 0.4 0.35 0.3
Person sits in a chair.
[0.0 sec]
[7.0 sec]
Person begins to laugh.
[10.1 sec]
[15.5 sec]
Man is sitting at a chair.
[7.4 sec]
[0.0 sec] (*Remove redundancy)
Scene 1
Scene 2 6 7 8 9 10 11 12 13
# of Proposals
# of scenes  = 2 (b) Irregular retrieval quality  of wsVMR systems (c) Estimating the number of  scenes in a video
Figure 1: Scene-proposal mismatch in current wsVMR systems: (a) shows an unnecessary many proposals on a video containing few scenes and few proposals on a video containing many scenes, (b) shows mIoU scores of the current model’s predictions according to the number of scenes and the number of generated proposals and (c) shows a method for estimating the number of scenes, where redundant scenes are removed from the counts. handled by small amounts of proposals. They also show scene-proposal mismatch by producing a small number of proposals for the video containing many scenes, such that those scarce proposals seem not to work correctly.
Our experimental evidence in Figure 1(b) validates the current wsVMR systems’ incorrectness due to the scene-proposal mismatch. We plot performances (i.e. mean Inter-section over Union (mIoU) scores) over predictions along the number of scenes in videos and the number of propos-als generated, which shows irregularities in the scores. The scores are low for videos with many scenes but few pro-posals and also low for videos with few scenes but many proposals. To estimate the number of scenes in a video, as shown in Figure 1(c), we counted the number of paired queries for each video as a discrete approximation of the scene. Here, we found that some queries describing the same scene led to redundancy in the counting. Thus, we re-move the redundancy of those queries via calculating their
IoUs1 between temporal boundary annotations2. Our study further showed that the scene-proposal mismatch affects about 41% of videos in VMR benchmarks (i.e. Charades-STA [7], ActivityNet-Caption [13]).
Intrigued by the scene-proposal mismatch, this paper proposes a wsVMR system referred to as Scene Complex-ity Aware Network (SCANet), which allows the system to mitigate the scene-proposal mismatch problem and gen-erate proposals adaptive to the complexity of the scenes contained in the video. For a given input video, SCANet
ﬁrst deﬁnes the scene complexity with a scalar, meaning how difﬁcult for the system to ﬁnd (i.e. retrieve) a speciﬁc scene among multiple distinguishable scenes in the video, which can be effective prior knowledge of video by com-1We remove redundancy by scenes with IoU > 0.5. 2Temporal annotations are used only for identifying the proposal-scene mismatch problem and they are not involved in the wsVMR task plementing weak supervision of VMR. On top of the scene complexity, SCANet adaptively generates proposals and en-hances their representations. Therefore, SCANet incorpo-rates (1) Complexity-Adaptive Proposal Generation (CPG) that generates adaptive proposals by leveraging the quan-tities of proposals under consideration of the complexity and (2) Complexity-Adaptive Proposal Enhancement (CPE) that enhances the proposals’ representations corresponding to the scene complexity. Furthermore, motivated by recent success [42, 44] of contrastive learning for wsVMR system, we introduce technical contributions to mine hard negatives in the input video and further video corpus together under our designed framework. Our extensive experiments show the effectiveness of the proposed SCANet, and qualitative results validate enhanced interpretability. 2.