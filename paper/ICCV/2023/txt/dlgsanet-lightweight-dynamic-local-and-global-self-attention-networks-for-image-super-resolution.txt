Abstract
We propose an effective lightweight dynamic local and global self-attention network (DLGSANet) to solve image super-resolution. Our method explores the properties of
Transformers while having low computational costs. Moti-vated by the network designs of Transformers, we develop a simple yet effective multi-head dynamic local self-attention (MHDLSA) module to extract local features efficiently. In addition, we note that existing Transformers usually explore all similarities of the tokens between the queries and keys for the feature aggregation. However, using all the similar-ities does not effectively facilitate the high-resolution im-age reconstruction as not all the tokens from the queries are relevant to those in keys. To overcome this problem, we develop a sparse global self-attention (SparseGSA) module to select the most useful similarity values so that the most useful global features can be better utilized for image re-construction. We develop a hybrid dynamic-Transformer block (HDTB) that integrates the MHDLSA and SparseGSA for both local and global feature exploration. To ease the network training, we formulate the HDTBs into a residual hybrid dynamic-Transformer group (RHDTG). By embed-ding the RHDTGs into an end-to-end trainable network, we show that the proposed method has fewer network param-eters and lower computational costs while achieving com-petitive performance against state-of-the-art ones in terms of accuracy. More information is available at https:
//neonleexiang.github.io/DLGSANet/. 1.

Introduction
Single image super-resolution (SISR) aims to find a so-lution to the issue of reconstructing a high-resolution image from a low-resolution one so that the high-resolution image can be better displayed on high-definition devices. In order to produce high-resolution images, classical approaches, e.g., bicubic and bilinear, employ interpolation processes
*Corresponding author
Figure 1. Image super-resolution comparisons (×4) in terms of ac-curacy, network parameters, and floating point operations (FLOPs) from the Urban100 dataset. The area of each circle denotes the number of network parameters. Our model (DLGSANet) achieves comparable performance while having fewer network parameters (< 5M) and lower FLOPs. to complement the surrounding pixel values. Convolu-tional neural network (CNN)-based approaches such as
[7, 8, 15, 21, 33] tackle the image super-resolution chal-lenge, generating better super-resolved images than those of conventional approaches. These CNN-based approaches have greatly advanced the progress of SISR.
Furthermore, several follow-up studies, such as [26, 27, 11], progressively start to develop larger and deeper CNN models for better learning capacity. Although the quality of the super-resolved images is largely improved, the com-putational costs of those approaches is quite expensive due to the large number of network parameters and calculations (e.g., more than 60M in network parameters and 3000G in
FLOPs), which limits their real-world applications. Thus, there is a great need to develop a lightweight and efficient model to solve SISR.
As Vision Transformers (ViTs) [9] can model global contexts while having fewer network parameters, a recent method [4] applies them to SISR and achieves better results 1
in terms of accuracy and network parameters compared to the CNN-based ones. However, as the original ViTs are computationally expensive, the shifted window scheme has been adopted in [22]. Although the self-attention by the shifted window scheme is capable of extracting local fea-tures, discontinuous windows limit the ability to model lo-cal features within each window. Moreover, the window-based methods are unable to aggregate information outside of the window, which leads to limited ability for modeling global information.
To better explore global features while reducing the com-putational costs, several approaches, e.g., [30], develop transposed attentions that compute the self-attention along the number of features. We note that these transformer-based methods usually use all the similarity values in the self-attention for feature aggregation. However, as not all the tokens from the queries are relevant to those in keys, using all similarities does not effectively facilitate the high-resolution image reconstruction. Thus, it is of great inter-est to develop a method to explore the properties of Trans-formers for both better local and global feature exploration while reducing the computational costs for high-quality, high-resolution image reconstruction.
In this paper, we propose an effective lightweight dy-namic local and global self-attention network (DLGSANet) to solve SISR efficiently. To alleviate the problem caused by the discontinuous windows, we first develop a simple yet ef-fective multi-head dynamic local self-attention (MHDLSA) module. The MHDLSA is motivated by the network de-signs of Transformers and can dynamically explore the lo-cal self-attention based on a fully CNN model to better ex-tract local features. As not all the tokens from the queries are relevant to those in keys, using all similarities does not effectively facilitate the high-resolution image reconstruc-tion. To overcome this problem, we develop a sparse global self-attention (SparseGSA) module to select the most useful similarity values for feature aggregation. We propose a hy-brid dynamic-Transformer block (HDTB) that integrates the
MHDLSA and SparseGSA to explore both local and global features for high-resolution image reconstruction. We fur-ther develop a residual hybrid dynamic-Transformer group (RHDTG) that stacks the HDTB based on the residual learn-ing. We formulate the RHDTGs into an end-to-end train-able network, named DLGSANet, to solve SISR. Figure 1 shows that the proposed DLGSANet model achieves com-parable performance with fewer network parameters and lower computational costs.
The main contributions of this work are summarized as follows:
• We propose a lightweight SISR model, called DL-GSANet, to solve the SISR problem efficiently and ef-fectively. Our analysis shows that the proposed model has fewer network parameters (< 5M) and needs lower computational costs while generating competitive per-formance.
• We propose a simple yet effective multi-head dynamic local self-attention (MHDLSA) module to extract local features dynamically.
• We develop an effective sparse global self-attention module (SparseGSA) to generate better self-attention for global feature exploration. 2.