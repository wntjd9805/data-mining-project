Abstract
We carefully evaluate a number of algorithms for learn-ing in a federated environment, and test their utility for a variety of image classification tasks. We consider many issues that have not been adequately considered before: whether learning over data sets that do not have diverse sets of images affects the results; whether to use a pre-trained feature extraction “backbone”; how to evaluate learner performance (we argue that classification accuracy is not enough), among others. Overall, across a wide variety of settings, we find that vertically decomposing a neural net-work seems to give the best results, and outperforms more standard reconciliation-used methods. 1.

Introduction
There has been a recent influx of work aimed at devel-oping and evaluating new algorithms for Federated Learn-ing (FL) [2, 6, 20, 29], particularly for image classifica-tion [3, 14, 18, 27, 28, 31, 35, 50, 53]. Most papers are pri-marily concerned with the development of innovative al-gorithms, and less concerned with the design of appropri-ate benchmarks and especially, appropriate baselines to test against. This paper, in contrast, is concerned with bench-marking: Which existing methods for FL work best, and under what conditions do they work, or not work? As such, our primary contribution is a set of carefully-designed ex-periments, rather than the introduction of a new FL algo-rithm. Our benchmarks are designed to address the follow-ing concerns regarding the design of FL benchmarks and baselines. (1) Learning algorithms are typically evaluated on a few di-verse data sets, rather than a large variety of more focused data sets. The most common benchmark for evaluating fed-erated image classification is CIFAR-100 [25]. This dataset includes whales, chairs, dinosaurs, and so on. This is a very 1Equal contribution. diverse set of classes, and we are concerned that few FL tasks will involve differentiating dinosaurs from rabbits.
For example, imagine that the members of a bird watch-ing club taking pictures with their smartphones; club mem-bers label the pictures with the bird species, and FL is used to build a classifier. Or, a set of companies who are typi-cally competitors—and hence cannot exchange data—want to work together to classify pictures of industrial drill bits based on whether they are going to fail. Both of these deployments involve narrow domains, and the classifica-tion problems involve fine-grained differentiation [13, 48] among members of a narrow category. True, not all appli-cations of FL will be narrow, but they are likely to be far narrower than classifying whales versus chairs. We argue that, while evaluating on a broad data set—such as CIFAR-100—is useful, it is also necessary to evaluate FL algo-rithms on a variety of more narrow data sets. (2) Most evaluations focus on final accuracy or the number of communication rounds required for convergence. FL al-gorithms are often evaluated by reporting final accuracies (after convergence), or by plotting test accuracy as a func-tion of the number of epochs, or the number of communi-cation rounds. However, final accuracy, at least in isolation, is not really a useful metric. After all, the simplest “FL” al-gorithm is classical, data parallel learning [33, 41, 52]. That is, run distributed gradient descent; compute gradients lo-cally over mini-batches in a fully synchronous way, then do an all-reduce. As this is functionally equivalent to central-ized learning, it is invariant to data distribution, and is likely to be the most accurate method in terms of final accuracy.
However, it is inefficient in a federated environment.
Likewise, considering accuracy as a function of commu-nication rounds ignores the computation or communication cost of each round. Communication and computation costs can vary across methods, and are typically far more impor-tant than the number of rounds. A method that can quickly achieve high accuracy with a relatively large number of very inexpensive communication rounds—where a small frac-tion of the model is communicated at each round—is proba-bly preferred to one that uses few rounds, but must transmit a huge amount of data. Similarly, for computation, given similar accuracies, an algorithm that performs one forward and one backward gradient descent pass is preferred over one that performs two additional forward passes, as extra overhead [28], even though both operate in the same num-ber of communication rounds. (3) Pre-trained feature extractors need to be a standard baseline. Training the feature-extraction backbone that is used in most deep image processing networks—that is, the series of convolutional and pooling layers and without the final classification layer—is not easy. It is difficult to get the training process right, and training a good backbone re-quires a lot of computation.
Researchers have recently suggested using pre-trained backbones, especially for few-shot training [7, 16].
In a pre-trained backbone, a pre-trained feature extractor such as a convolutional neural network (CNN) [15, 19, 39] or vi-sion transformer [10] is used without modification to em-bed an image in a high-dimensional space. Learning is then performed on the resulting vectors, and not on the original images. The goal is to leverage an existing, well-trained model on the new learning problem. Given the resource constraints in many FL scenarios, it is unclear why attempt-ing to fully train such a backbone from scratch in a federated environment would be the first approach. Instead, we sug-gest simply taking a standard, pre-trained backbone (such as a ResNet [15], or a DenseNet [19], or both used together) and directly using that backbone as a feature extractor, with-out training.
The benefit is that each training image only needs to be pushed through the backbone one time to obtain a compact set of features, and then that set can be used during train-ing. As the backbone need not to be communicated or re-peatedly used to process the input images, both CPU/GPU cycles and communication are saved during federated train-ing. Furthermore, final accuracies might actually be better, as this approach sidesteps the difficulty of training the back-bone. (4) Off-the-shelf performance is really what matters, but it is often neglected in research studies. In a centralized en-vironment, it is feasible to train and re-train many times, as various parameters (learning rate, proximal weight, exact neural architecture, etc.) are tuned. In FL, this is much less feasible. Most FL scenarios imagine a resource-constrained environment (possibly involving edge devices), where one cannot ignore the cost of tuning. FL is typically happening on someone else’s hardware, and so running an algorithm many times during parameter tuning is likely to make par-ticipation in training far less palatable. drop out shortly after, never to be seen again. One cannot access a missing device’s data to perform validation, and clearly, one cannot retrain over data that cannot be accessed.
As such, we argue that “off-the-shelf” performance is more important than in centralized learning. That is, it be-comes important to settle on one set of universal parameters that tend to work in most deployments, such that the learn-ing algorithm can be universally deployed without further tuning. At the very least, this is an important use case that should be covered in most experiments.
Our Contributions. Our contributions are as follows.
• We suggest several rules-of-thumb governing benchmark and baseline development for FL over images, and use those rules to devise an extensive FL image classification benchmark, and use that benchmark.
• We show that using pre-trained features and other model reduction tools are necessary for practical FL.
• We highlight some surprising behaviors of current FL al-gorithms, including the effect of client population size, that to our knowledge are not sufficiently explored in cur-rent literature.
• Our source code https://github.com/huerdong/
FedVert-Experiments publicly is available at 2.