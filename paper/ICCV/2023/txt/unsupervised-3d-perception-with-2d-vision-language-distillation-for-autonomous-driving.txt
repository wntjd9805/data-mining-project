Abstract
Closed-set 3D perception models trained on only a pre-deﬁned set of object categories can be inadequate for safety critical applications such as autonomous driving where new object types can be encountered after deployment. In this paper, we present a multi-modal auto labeling pipeline ca-pable of generating amodal 3D bounding boxes and track-lets for training models on open-set categories without 3D human labels. Our pipeline exploits motion cues inherent in point cloud sequences in combination with the freely avail-able 2D image-text pairs to identify and track all trafﬁc par-ticipants. Compared to the recent studies in this domain, which can only provide class-agnostic auto labels limited to moving objects, our method can handle both static and moving objects in the unsupervised manner and is able to output open-vocabulary semantic labels thanks to the pro-posed vision-language knowledge distillation. Experiments on the Waymo Open Dataset show that our approach out-performs the prior work by signiﬁcant margins on various unsupervised 3D perception tasks. 1.

Introduction
In autonomous driving, most existing 3D detection mod-els [62, 22, 42] have been developed with the prior assump-tion that all possible categories of interest should be known and annotated during training. While signiﬁcant progress has been made in this supervised closed-set setting, these methods still struggle to fully address the safety concerns that arise in high-stakes applications. Speciﬁcally, in the dynamic real-world environment, it is unacceptable for au-tonomous vehicles to fail to handle a category that is not present in the training data. To address this safety concern, a recent development by Najibi et al. [35] proposed an unsu-pervised auto labeling pipeline that uses motion cues from point cloud sequences to localize 3D objects. However, by design, this method does not localize static objects which
∗Equal contribution
†Corresponding author
Figure 1. An illustration of three interesting urban scene examples of open-vocabulary perception. Left: our method can faithfully de-tect objects based on user-provided text queries during inference, without the need for 3D human supervision. Red points are points matched with the text queries. Right: camera images for readers’ reference. Note that the inference process solely relies on LiDAR points and does not require camera images. constitute a signiﬁcant portion of trafﬁc participants. More-over, it only models the problem in a class-agnostic way and fails to provide semantic labels for scene understand-ing. This is suboptimal as semantic information is essen-tial for downstream tasks such as motion planning, where category-speciﬁc safety protocols are deliberately added to navigate through various trafﬁc participants.
Recently, models trained with large-scale image-text datasets have demonstrated robust ﬂexibility and general-ization capabilities for open-vocabulary image-based classi-ﬁcation [38, 19, 33], detection [20, 12, 24, 59] and semantic segmentation [23, 11] tasks. Yet, open-vocabulary recogni-tion in the 3D domain [9, 15, 40] is in its early stages. In the context of autonomous driving it is even more underex-plored. In this work, we ﬁll this gap by leveraging a pre-trained vision-language model to realize open-vocabulary 3D perception in the wild.
We propose a novel paradigm of Unsupervised 3D
Perception with 2D Vision-Language distillation (UP-VL).
Speciﬁcally, by incorporating a pre-trained vision-language model, UP-VL can generate auto labels with substantially higher quality for objects in arbitrary motion states, com-pared to the latest work by Najibi et al. [35].
With our auto labels, we propose to co-train a 3D ob-ject detector with a knowledge distillation task, which can achieve two goals simultaneously, i.e. improving detection quality and transferring semantic features from 2D image pixels to 3D LiDAR points. The perception model there-fore is capable of detecting all trafﬁc participants and thanks to the distilled open-vocabulary features, we can ﬂexibly query the detector’s output embedding with text prompts, for preserving speciﬁc types of objects at inference time (see Figure 1 for some examples).
We summarize the contributions of UP-VL as follows:
• UP-VL achieves state-of-the-art performance on un-supervised 3D perception (detection and tracking) of moving objects for autonomous driving.
• UP-VL introduces semantic-aware unsupervised de-tection for objects in any motion state, a ﬁrst in the
ﬁeld of autonomous driving. This breakthrough elimi-nates the information bottleneck that has plagued pre-vious work [35], where class-agnostic auto labels were used, covering only moving objects with a speed above a predetermined threshold.
• UP-VL enables 3D open-vocabulary detection of novel objects in the wild, with queries speciﬁed by users at inference time, therefore removing the need to re-collect data or re-train models. 2.