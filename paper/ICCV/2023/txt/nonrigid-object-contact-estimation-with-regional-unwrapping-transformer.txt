Abstract
Acquiring contact patterns between hands and nonrigid objects is a common concern in the vision and robotics community. However, existing learning-based methods fo-cus more on contact with rigid ones from monocular im-ages. When adopting them for nonrigid contact, a major problem is that the existing contact representation is re-stricted by the geometry of the object. Consequently, con-tact neighborhoods are stored in an unordered manner and contact features are difﬁcult to align with image cues. At the core of our approach lies a novel hand-object contact representation called RUPs (Region Unwrapping Proﬁles), which unwrap the roughly estimated hand-object surfaces as multiple high-resolution 2D regional proﬁles. The re-gion grouping strategy is consistent with the hand kinematic bone division because they are the primitive initiators for a composite contact pattern. Based on this representation, our Regional Unwrapping Transformer (RUFormer) learns the correlation priors across regions from monocular inputs and predicts corresponding contact and deformed transfor-mations. Our experiments demonstrate that the proposed framework can robustly estimate the deformed degrees and deformed transformations, which makes it suitable for both nonrigid and rigid contact. 1.

Introduction
Perceptions of hand-object contact patterns are crucial to advance human-computer interaction and robotic imi-tation [44]. The interactive objects in these applications, from mouse/keyboard to bottle/doll, are mostly nonrigid.
Although impressive progress has been achieved towards monocular contact estimation between hands and 3D rigid objects [12, 41, 47, 35] or 2.5D cloth [37, 36, 1], it is still
*Corresponding author. E-mail: yangangwang@seu.edu.cn. All the authors from Southeast University are afﬁliated with the Key Laboratory of Measurement and Control of Complex Systems of Engineering, Min-istry of Education, Nanjing, China. This work was supported in part by the National Natural Science Foundation of China (No. 62076061), the
Natural Science Foundation of Jiangsu Province (No. BK20220127).
Figure 1: Contact patterns estimated from monocular
RGB images. Since the deformed degrees of the contact areas are considered by our framework, both contact with nonrigid (Row1, Row2, Row3) and rigid objects (Row4) can be plausibly estimated. difﬁcult to extend them to 3D nonrigid object. One impor-tant reason is that existing methods usually project the con-tact area of different objects onto their own surface (point cloud or mesh), which is represented by either unordered points or unregistered points and edges. As a result, it is challenging to store contact into a feature-aligned space.
To conquer this obstacle, our key idea is to ﬁrst rep-resent regional 3D surface where hand-object contact may occur as regional 2D unwrapping proﬁles, then pre-dict the nonrigid contact and deformation within/across regions according to monocular image cues through a
Vision Transformer. Considering that the mutual con-tact is caused by individual hand regions [17, 41, 47], our surface grouping is based on the 16 hand kinematic bones [30, 28, 41, 47] illustrated in Fig. 2(a). Each piece of object surface shown in Fig. 2(b) is divided into a cer-tain group when it can be directly intersected by a ray em-anating from the region center associated with this group.
Each subsurface is further mapped to the image plane ac-cording to the spherical unwrapping algorithm [46]. Con-sequently, the whole object surface is converted to 16 ob-ject regional unwrapping proﬁles (object-RUPs). Simi-larly, the hand surface is converted to 16 hand-RUPs, each of which records pixel-aligned ray intersections with the object-RUP in the same group. In contrast to object point clouds [25, 12, 35, 6], this novel representation preserves both the hand-object surface correlation and the contact point orderliness.
Numerous works [12, 35] only predicted plausible con-tact patterns according to data prior and ignore contact clues in the image. This may be applicable to rigid interaction.
However, when the deformed degree is considered, multi-ple nonrigid contact patterns can be yielded from the same hand-object spatial relationship. Therefore, our framework crops the image patches of the corresponding 16 hand bones as extra visual cues to estimate nonrigid contact. Alto-gether, our RUFormer is tamed to take those 16 groups of hand-RUPs, object-RUPs, and visual cues as the inputs.
It gradually estimates the contact and deformed features across RUPs, and ﬁnally predicts the deformed transforma-tions of the object. To our best knowledge, this is the ﬁrst framework that is applicable to reconstruct both rigid and nonrigid hand-object interaction from monocular images.
In summary, our main contributions are:
• A learning-based framework with the ambition to estimate the contact between hand and nonrigid objects from monoc-ular images;
• A hand-object interaction representation to record hand-object surfaces into multiple pixel-aligned and ﬁne-grained 2D proﬁles;
• A unwrapping-informed transformer to predict contact and deformation on the object according to both visual cues and data prior. 2.