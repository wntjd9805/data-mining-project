Abstract
Motion deblurring from a blurred image is a challeng-ing computer vision problem because frame-based cameras lose information during the blurring process. Several at-tempts have compensated for the loss of motion information by using event cameras, which are bio-inspired sensors with a high temporal resolution. Even though most studies have assumed that image and event data are pixel-wise aligned, this is only possible with low-quality active-pixel sensor (APS) images and synthetic datasets.
In real scenarios, obtaining per-pixel aligned event-RGB data is technically challenging since event and frame cameras have different optical axes. For the application of the event camera, we propose the first Non-coaxial Event-guided Image Deblur-ring (NEID) approach that utilizes the camera setup com-posed of a standard frame-based camera with a non-coaxial project: https://sites.google.com/view/neid2023 single event camera. To consider the per-pixel alignment between the image and event without additional devices, we propose the first NEID network that spatially aligns events to images while refining the image features from temporally dense event features. For training and evaluation of our network, we also present the first large-scale dataset, con-sisting of RGB frames with non-aligned events aimed at a breakthrough in motion deblurring with an event camera.
Extensive experiments on various datasets demonstrate that the proposed method achieves significantly better results than the prior works in terms of performance and speed, and it can be applied for practical uses of event cameras. 1.

Introduction
Motion blur occurs due to the motions during the ex-posure time since a frame-based camera records the scene during the exposure time and outputs the averaged signal. 1
The inverse problem, called deblurring, is restoring a sharp image given a blurry image [41, 35, 5, 13, 67, 71, 1, 22, 30, 39, 65]. Previous learning-based deblurring methods are de-signed to be sophisticated [6, 69] to improve the deblurred image quality, leading to enormous complexity.
With the advent of event cameras, event-based mo-tion deblurring methods have been proposed to overcome the loss of motion information in the frame-based camera
[43, 42, 21, 25, 31, 62, 66, 49, 72, 61]. The event camera asynchronously provides event data with low latency, which has no motion blur even in extreme motion and contains both texture and motion information over continuous time.
Despite these potentials of event data, the critical issue is that even the most recent research still exploits a strong assumption that events and images are pixel-wise aligned.
However, in real scenarios, it can be challenging to ob-tain per-pixel aligned event-RGB data unless the event cam-era provides active-pixel sensor (APS) images [3]; because event and frame cameras have different optical axes. Since
APS images are generally of low image quality [16, 2], some works use event simulators (e.g. ESIM [46]) on high-frame-rate video datasets (e.g. GoPro [37]) to generate syn-thetic event streams for restoring clean RGB images. How-ever, as mentioned in [19, 52, 44, 4], synthetic event data is far from the output of an actual event camera. A model that has been trained effectively on a synthetic dataset may experience significant performance degradation when eval-uated on a real-event dataset.
In the real world, two distinct types of cameras must be used together to acquire high-quality event stream and
RGB images simultaneously (see (a) of Fig. 1). Since the two cameras are different devices, they have different op-tical axes and field-of-views (FoV). It means they are not aligned pixel-wisely. For video interpolation, a task differ-ent from motion deblurring, existing works used a feature-based homography warping process [58] or an additional device (e.g. beamsplitter) [57] for pixel-wise alignment be-tween the two cameras. However, feature-based alignment between events and images [36] is too complicated due to the sequence of image reconstruction, feature matching, and global homography warping. Furthermore, this process only works when the camera is static; the object is front-paralleled in a limited range.
In non-coaxial camera se-tups, the pixel misalignment between two cameras varies with the 3D information of each scene. Therefore, the pixel alignment between two cameras obtained by the homogra-phy can not perfectly align all the pixels of both near and faraway objects, even calculated scene by scene as in [58].
In other words, feature-based homography cannot be a gen-eral solution to cover most situations. On the other hand, an additional device for pixel alignment could not achieve a compact design and larger FoV, leading to many limitations in real-world applications, such as mobile devices.
We aim to utilize events from non-coaxial event cameras for motion deblurring without homography/additional de-vices. As shown in (b) of Fig. 1, despite of close placement of the RGB camera and the event camera, the event and im-age are not aligned. Furthermore, the extent of unalignment is slightly different for each scene and even for each object in the same scene. Therefore, we introduce a Non-coaxial
Event-guided Image Deblurring (NEID) task that can un-lock the potential of the event camera in a real scenario.
NEID aims to spatially align the image and event at the fea-ture level during motion deblurring. To use pixel-wise mis-aligned events, we need a framework to align events with images, even if the images are blurry.
To this end, we propose a novel framework called Non-coaxial Event-guided Deblurring Network (NED-Net). As a key component for managing alignment between image and event features in the pipeline, we propose the Attention-based Deformable Align (ADA) module. The module lever-ages both structure-based coarse alignment and patch-based fine alignment. From these two cascade alignment pro-cesses, events and images can be aligned well at the fea-ture level despite the distinctive modality differences. Af-ter global spatial alignment, the proposed Local Score-based Aggregation (LSA) computes the similarity score with the surrounding image features to remove the unnec-essary event features and aggregate confident events to im-prove the effect of deblurring. Finally, we develop the
Cross-Channel Interaction (CCI) module for texture en-hancement. In addition, we first present the Non-Coaxial
Events and RGB (NCER) dataset consisting of 83 scenes collected with a sophisticated camera setup with tempo-rally synchronized frames and events. This dataset pro-vides multiple scenes with dynamic motion at varying dis-tances suitable for the NEID task. Unlike the existing event-guided motion deblurring dataset [28, 25, 52, 55], the
NCER dataset provides high-resolution event data and high-quality RGB images, not APS images. 2.