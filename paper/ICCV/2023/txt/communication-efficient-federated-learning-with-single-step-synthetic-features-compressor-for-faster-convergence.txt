Abstract
Reducing communication overhead in federated learn-ing (FL) is challenging but crucial for large-scale dis-tributed privacy-preserving machine learning. While meth-ods utilizing sparsification or other techniques can largely reduce the communication overhead, the convergence rate is also greatly compromised. In this paper, we propose a novel method named Single-Step Synthetic Features Com-pressor (3SFC) to achieve communication-efficient FL by directly constructing a tiny synthetic dataset containing synthetic features based on raw gradients. Therefore, 3SFC can achieve an extremely low compression rate when the constructed synthetic dataset contains only one data sam-ple. Additionally, the compressing phase of 3SFC utilizes a similarity-based objective function so that it can be op-timized with just one step, considerably improving its per-formance and robustness. To minimize the compressing er-ror, error feedback (EF) is also incorporated into 3SFC.
Experiments on multiple datasets and models suggest that 3SFC has significantly better convergence rates compared to competing methods with lower compression rates (i.e., up to 0.02%). Furthermore, ablation studies and visual-izations show that 3SFC can carry more information than competing methods for every communication round, further validating its effectiveness. 1.

Introduction
Until now, federated learning [22] (FL) is deemed as one of the most promising distributed techniques [8, 3] to tackle the isolated data island problem with privacy guarantees.
However, the training process of FL involves frequent ex-changing of model parameters between central servers and participating clients, which is becoming increasingly ex-pensive, especially considering the rapid growth of model
*Corresponding Author size today [6, 16, 10]. Moreover, participating clients of FL typically operate at unreliable and limited network connec-tion rates compared to data centers [15], further hindering the large-scale deployments of FL. Consequently, commu-nication is becoming the primary bottleneck for flexible FL at the scale [5].
To explore possible approaches for reducing communi-cation overhead in FL, various methods have been proposed targeting different objectives. The work in [30, 21] ap-plied top-k sparsification to the gradients so that only the most important information is transmitted at each epoch.
Moreover, Wangni [33] reported that using top-k sparsifi-cation with error feedback (EF), the communication over-head of ResNet-50 [12] trained on ImageNet [24] could be reduced by 99.6% while maintaining nearly the same model accuracy. On the other hand, The work in [2, 4] employed quantification to represent gradients by a lower precision data type with a considerably smaller size. Later
Karimireddy [17] introduced error feedback to quantifica-tion as well, substantially improving the rate of conver-gence. In [11], instead of gradients, several data samples distilled from the full training dataset were transmitted as they are much smaller than the gradients, and they can pro-duce similar gradients through back-propagation. More re-cently, Li [19] and Wu [34] proposed compressing and de-compressing communication data using compressed sens-ing and knowledge distillation, respectively.
While the methods mentioned above have been proven useful in reducing communication overhead reduction, em-pirical evidence indicates that they both suffer from de-graded model convergence rates. This means that the model is expected to converge much slower with a smaller com-pression rate, as demonstrated in Figure 1. Here, the com-pression rate is defined as Equation 1. In this paper, we pro-pose a single-step synthetic features compressor (3SFC), to boost the convergence rate during training and carry more information under a limited communication budget. Instead 1
Figure 1: Test accuracy of MLP (199,210 parameters) trained on non-i.i.d. MNIST dataset with 20 clients. The rate of convergence reduces as the compression rate de-creases. of transmitting raw gradients directly, 3SFC first constructs a tiny synthetic dataset for the FL model. Then a scaling coefficient is calculated to minimize the compression error.
Finally, the constructed synthetic dataset and the scale co-efficient are transmitted to the server. In addition, the error feedback [29] is also incorporated into 3SFC to further min-imize the overall compression error.
Comp. Rate =
Comp. Size
Uncomp. Size
= 1
Comp. Ratio
. (1)
Our contributions can be summarized as following: 1. Instead of transmitting gradients employed by most existing compression methods, 3SFC only transmits a tiny set of model inputs and labels, which is indepen-dent of the model architecture. Consequently, 3SFC can achieve an extremely low compression rate. 2. A similarity-based objective function is employed to construct synthetic inputs and labels, which drastically lowers the time and space complexity and improves the performance and robustness of 3SFC. Moreover, the error feedback is incorporated into 3SFC to minimize the overall compressing error and therefore boost the convergence rate. These design choices make 3SFC an effective solution for achieveing communication-efficient FL while maintaining model accuracy. 3. 3SFC can achieve a significantly better convergence rate compared to competing methods under the same and even lower communication budget (i.e., up to a compression ratio of 3600Ã—). Ablation study and other visualizations further validate the efficiency of 3SFC against other state-of-the-art works. The code is open-sourced for reproduction 1. 1https://github.com/Soptq/iccv23-3sfc
Figure 2: When trying to fit gradients obtained by 128 steps of SGD for 128 steps of simulation using the method in
[11], it should be perfectly fitted instead of collapsed. On the other hand, using 1 step of simulation (3SFC) requires less computation and storage but achieves significantly bet-ter fitting results. 2.