Abstract 1.

Introduction
We present a framework for generating appropriate facial responses from a listener in dyadic social interactions based on the speaker’s words. Given an input transcription of the speaker’s words with their timestamps, our approach autore-gressively predicts a response of a listener: a sequence of listener facial gestures, quantized using a VQ-VAE. Since gesture is a language component, we propose treating the quantized atomic motion elements as additional language token inputs to a transformer-based large language model.
Initializing our transformer with the weights of a language model pre-trained only on text results in significantly higher quality listener responses than training a transformer from scratch. We show that our generated listener motion is fluent and reflective of language semantics through quantitative metrics and a qualitative user study. In our evaluation, we analyze the model’s ability to utilize temporal and semantic aspects of spoken text.
*denotes equal contribution
Human face-to-face communication is multifaceted and multimodal [13, 28]. In particular, the flow and effectiveness of face-to-face dyadic interactions critically depend on non-verbal motions and responses from both participants in the conversation [28, 30, 10]. This paper focuses on the non-verbal facial feedback listeners provide to speakers during a dyadic conversation [34, 49, 18].
When listening to a speaker, we produce gestures in response to several multimodal communication channels, including speech, non-verbal gestures, and lexical seman-tics (the meaning of words) [28]. Previous studies have shown that speech and gesture are informative of listener responses [34, 49]. Here we ask how far we can get with lexical semantics alone. This question is significant given the abundant availability of textual dialogue data, in contrast to the limited availability of conversational motion datasets.
To study the transferability of large language models to the dyadic conversational motion domain, we propose the task depicted in Figure 1 of predicting listener motion from the raw text transcribed from the speaker’s words.
Since gesture is a conversational communication chan-Figure 2: Listener motion prediction model. The model takes as input text tokens (blue), along with their timestamps, and predicts tokens representing atomic listener motion elements (orange) that we discretized with a VQ-VAE. We feed in a fixed-size history window of text tokens before the listener response’s onset. Then we generate one discrete gesture token at a time while providing text tokens as the speaker speaks (i.e. according to word timestamps). t denotes the number of frames that have elapsed since the start of motion generation. Each discrete motion token represents 8 frames of continuous motion. nel, our central insight is to transfer knowledge from pre-trained large language models to the gesture generation task.
We propose to treat discrete atomic motion elements as novel language tokens. We first learn a data-driven dictio-nary of discrete atomic gesture elements by training a VQ-VAE [45] to capture the full spectrum of videotaped listener responses [34]. We then fine-tune a pretrained large language model to autoregressively predict these novel motion tokens given temporally-aligned speaker text (see Figure 2). We ensure that each motion token is only generated based on previously spoken words by interleaving the input speaker text tokens with the autoregressively predicted motion tokens.
Hence, our causal model can produce listener responses in real-time as it does not rely on future speaker words.
Our text-conditioned model outperforms baselines in both quantitative metrics and human evaluation. Notably, it per-forms competitively with prior work that uses audio and visual input for listener generation. The generated listener responses are consistently on-par with ground truth listener facial gestures, as our perceptual study demonstrates.
Given these results, we ask why a text-conditioned model performs well on an inherently multimodal task. We focus our analysis of the model on the two main qualities we expect from realistic listener non-verbal feedback: (1) tem-porally synchronous responses, such as head nods, and (2) semantic, emotionally meaningful responses, such as smiles or looks of puzzlement [9]. We find that a text transcrip-tion of a speaker’s utterance carries some temporal signal of when a response is in order. Punctuation, capitalization, and temporal breaks in word delivery are hints about sen-tence structure that embed this rhythmic information. We further demonstrate that lexical semantics is crucial for pro-ducing the correct emotional response, especially when the speaker’s facial expression does not reflect the emotional affect of their words. Finally, we note that, as expected, our model cannot capture responses for which the facial affect or motion of the speaker is crucial. 2.