Abstract
Generating both plausible and accurate full body avatar motion is the key to the quality of immersive experiences in mixed reality scenarios. Head-Mounted Devices (HMDs) typically only provide a few input signals, such as head and hands 6-DoF. Recently, different approaches achieved im-pressive performance in generating full body motion given only head and hands signal. However, to the best of our knowledge, all existing approaches rely on full hand visi-bility. While this is the case when, e.g., using motion con-trollers, a considerable proportion of mixed reality experi-ences do not involve motion controllers and instead rely on egocentric hand tracking. This introduces the challenge of partial hand visibility owing to the restricted field of view of the HMD. In this paper, we propose the first unified ap-proach, HMD-NeMo, that addresses plausible and accurate full body motion generation even when the hands may be only partially visible. HMD-NeMo is a lightweight neural network that predicts the full body motion in an online and real-time fashion. At the heart of HMD-NeMo is the spatio-temporal encoder with novel temporally adaptable mask to-kens that encourage plausible motion in the absence of hand observations. We perform extensive analysis of the impact of different components in HMD-NeMo and introduce a new state-of-the-art on AMASS dataset through our evaluation. 1.

Introduction
Mixed reality technology opens up new means of com-munication and interaction between people. With people at the heart of this technology, generating faithful and believ-able avatar motion is key to the quality of immersive expe-riences. Despite great advances in this area, generating full body avatar motion given HMD signal remains a challenge: in many current solutions, avatars only have upper bodies.
Prior works attempted to generate full body avatar mo-tion given sparse or partial observations, such as images [17, 4, 33], 2D joints/keypoints [21, 5], markers [35, 18, 34, 11], and IMUs [28, 14, 32, 31]. While such observations are
Figure 1. HMD-NeMo generates full body avatar motion given
HMD signals, i.e., head and hand 6-DoFs, from hand tracking sig-nal from the HMD as well as hand motion controllers. considered partial or sparse, they provide much richer input signal compared to a typical HMD’s head and hand 6-DoF.
More recently, great progress has been made to generate full body motion given only a HMD signal [1, 30, 22, 15], how-ever, they all rely on the availability and full visibility of both hands. While this is the case when, e.g., using motion controllers, many mixed reality experiences do not involve motion controllers and instead rely on hand tracking. This introduces the challenge of partial hand visibility owing to the restricted field of view (FoV) of the HMD sensors.
In this paper, we address this problem via HMD-NeMo (a neural motion model of human given HMD signal).
Within a unified framework, HMD-NeMo generates full body motion in real time, regardless of whether hands are fully or partially observed, or not observed at all. Our ap-proach is built upon recurrent neural networks to efficiently capture temporal information, and a transformer to cap-ture complex relations between different components of the input signal. At the heart of our approach is the TAMT (temporally adaptable mask token) module, allowing us to handle missing hand observations.
Our contributions are: (1) The first full body avatar mo-tion generation approach capable of generating accurate and plausible motions with full or partial hand visibility. This is a step forward for unlocking fully immersive experiences in mixed reality with fewer limitations on the hardware. (2)
Temporally adaptable Mask Tokens (TAMT), a simple yet
assumes a known root (pelvis joint) as an additional sig-nal. The assumption of known root joint also appears in [9], wherein, unlike [2, 8], the method generates full body motion given the HMD signal with Variational Au-toencoders [16]. Note that, while [9] generates temporally plausible motions, it works in an offline fashion, predicting the motion for an entire sequence only after observing the whole sequence of HMD signals.
Recently, different techniques have been proposed to generate full body avatar locomotion (in the world coordi-nates) given HMD signals. In this context, [1] proposes a matching algorithm, aiming to sample closest poses from a motion capture library at sparse time-steps and interpolate between poses. While this guarantees the selection of real-istic poses, the output is always limited to the utilized mo-tion capture library. In another work, [30] uses combination of an inverse kinematic (IK) solver and a recurrent neural network to generate upper body and lower body motions, respectively. Combination of different components to solve lower and upper body separately has also been explored in [22], wherein a neural network is trained to predict the root orientation given the HMD signal, which then is used as the feature vector for a motion matching algorithm [6, 13] to generate full body. More recently, fully learning-based approaches have shown promise in generating full body avatar motion [29, 15]. In this context, [29] simulates plau-sible and physically feasible motions within a reinforcement learning framework and [15] uses a transformer-based ap-proach to generate full body motion given HMD signals.
Although great progress has been made by recent ap-proaches, they all solve the problem of motion generation given head and both hands, typically captured with motion controllers. However many mixed reality experiences do not have motion controllers available and instead rely on hand tracking from HMD mounted sensors (e.g., cameras).
This introduces the challenge of hand tracking failures and partial hand visibility owing to the restricted field of views.
To the best of our knowledge, despite its usability, motion generation in the presence of partial hand observation has not been well-explored. In this paper, we propose a method capable of generating high fidelity and plausible full body motion even in presence of partially visible hands. 3. Proposed Method
In this section, we first define the problem, the scenar-ios we consider, as well as the input and the desired output representation. We then present our proposed method. 3.1. Problem Definition
Task. The task is to generate full-body 3D human locomo-tion (predicting both the instantaneous pose and the global trajectory of the human) given the sparse HMD signal in (a) Plane FoV (b) Motion Controllers (MC) (c) Hand Tracking (HT)
Scenario
Left hand visibility
Right hand visibility 100% 48.94%
Motion Controllers
Hand Tracking 100% 53.45%
Figure 2. Top: Definition of FoV for different scenarios for the same pose. (a) Planar FoV, as used in [8], wherein avatar’s left hand is visible while the right hand is not. (b) Fully visible, as in motion controller scenarios, wherein both hands are always visi-ble. (c) HMD’s hand tracking camera’s FoV, as in hand tracking scenarios, wherein avatar’s right hand is visible while the left hand is not. Bottom: Hand visibility statistics on AMASS test set. effective strategy for handling missing hand observations in a temporally coherent way. (3) Extensive experiments and ablations of our method as well as a new state-of-the-art performance on the challenging AMASS dataset. 2.