Abstract
Unsupervised Multi-View Stereo (MVS) methods have achieved promising progress recently. However, previous methods primarily depend on the photometric consistency assumption, which may suffer from two limitations: indis-tinguishable regions and view-dependent effects, e.g., low-textured areas and reflections. To address these issues, in this paper, we propose a new dual-level contrastive learn-ing approach, named CL-MVSNet. Specifically, our model integrates two contrastive branches into an unsupervised
MVS framework to construct additional supervisory sig-nals. On the one hand, we present an image-level con-trastive branch to guide the model to acquire more con-text awareness, thus leading to more complete depth esti-mation in indistinguishable regions. On the other hand, we exploit a scene-level contrastive branch to boost the rep-resentation ability, improving robustness to view-dependent effects. Moreover, to recover more accurate 3D geometry, we introduce an L0.5 photometric consistency loss, which encourages the model to focus more on accurate points while mitigating the gradient penalty of undesirable ones.
Extensive experiments on DTU and Tanks&Temples bench-marks demonstrate that our approach achieves state-of-the-art performance among all end-to-end unsupervised MVS frameworks and outperforms its supervised counterpart by a considerable margin without fine-tuning. 1.

Introduction
Multi-View Stereo (MVS) is a critical task in various applications, including robotics, self-driving, and VR/AR.
The goal of MVS is to estimate a dense 3D reconstruction from multiple images captured from different views. Tradi-tionally, it has been approached by computing dense corre-Figure 1. Qualitative comparison of reconstruction quality with the SOTA method [5] on scan29 of DTU [1]. Our method performs better on repetitive patterns. spondences between images, often based on hand-crafted similarity metrics and engineered regularizations [4, 33].
Recently, a surge of learning-based methods [7, 49–51] have been developed to advance the effectiveness of MVS, showing promising results in MVS benchmarks [1, 19].
However, most of them are supervised methods [3, 9, 27].
These methods heavily rely on large-scale ground-truth 3D training data, which are expensive to acquire.
To tackle this problem, unsupervised MVS methods
[5, 17, 41] have attempted to train MVS networks without annotations. Existing methods mainly depend on the hy-pothesis of photometric consistency, which states that the appearance of a point in 3D space is invariant across dif-ferent views. However, this hypothesis may be ineffective owing to indistinguishable regions and view-dependent ef-fects, e.g., low texture, repetitive patterns, and reflections.
Recently, the state-of-the-art (SOTA) method RC-MVSNet
[5] adopts a rendering consistency network to address the ambiguity caused by view-dependent photometric effects and occlusions. While achieving promising results, this ap-proach may suffer from significant performance degrada-tion in indistinguishable regions, as shown in Fig. 1. To ad-dress the absence of valid supervisory signals, we propose to leverage contrastive learning to boost the robustness and generalizability of unsupervised MVS in various scenarios.
Contrastive learning is a widely used paradigm in un-supervised learning to construct additional supervisory sig-nals. With the success of contrastive learning in images,
[41, 53] have extended its application to unsupervised MVS with color fluctuation augmentations or part segmentations.
However, there are still many remaining unresolved issues, e.g., occlusions and low-textured surfaces. And the poten-tial of incorporating contrastive learning into MVS remains largely under-explored. Moreover, recent studies [2, 55] showed that hard positive samples are of benefit to boost the contrastive learning. Inspired by this, we propose a dual-level contrastive learning approach, named CL-MVSNet, where image-level and scene-level contrastive learning are integrated into an unsupervised MVS framework.
To resolve ambiguity from indistinguishable regions, we introduce an image-level contrastive learning strategy to encourage the model to be more context-aware. Specifi-cally, for an image-level contrastive sample, all pixels in the source images are masked with independent and iden-tically distributed Bernoulli probability, simulating the case that local photometric consistency fails. Following that, we maximize the similarity between the depth estimations of the regular sample and the image-level contrastive sample.
The intuition is that the augmented images contain the same context information as the original ones, which can also be utilized to estimate complete depth maps as hard positive samples. In this way, the network is encouraged to exploit more contextual information instead of relying only on pho-tometric consistency over small regions.
In addition, we propose a scene-level contrastive learn-ing branch to alleviate the view-dependent photometric ef-fects. Due to severe occlusions, reflections, and illumina-tion changes, source images with few overlaps are often in-feasible to use in unsupervised MVS. However, from the perspective of contrastive learning, a scene-level contrastive sample containing randomly selected source images can be considered a natural hard positive sample. These hard sam-ples are expected to produce the same 3D representation as the regular samples, as they are captured from identi-cal scenes. Therefore, we enforce contrastive consistency between the scene-level contrastive branch and the regular branch, encouraging the model to learn more powerful 3D cost volume regularization for robust depth estimation.
Furthermore, we propose an L0.5 photometric consis-tency loss to support the training of the CL-MVSNet. In an
MVS system, after depth estimation, most points with un-desirable depth predictions will be filtered out before depth fusion. Besides, these points often locate in occluded ar-eas or useless backgrounds where photometric consistency enforcement may mislead the model. To this end, instead of using vanilla photometric consistency loss based on L1 or L2 norm, we propose to use the L0.5 norm, which has larger gradients with regard to smaller errors. In this way, the model increases the penalty of accurate points, resulting in more accurate survival points.
In conclusion, our main contributions are:
• We present an image-level contrastive consistency loss, which encourages the model to be more context-aware and recover more complete reconstruction in in-distinguishable regions.
• We propose a scene-level contrastive consistency loss, which boosts the representation ability to promote ro-bustness to view-dependent effects.
• We propose an L0.5 photometric consistency loss to further advance the contrastive learning framework, which enables the model to focus on accurate points, resulting in more accurate reconstruction.
• Experiments on DTU [1] and Tanks&Temples [19] benchmarks show that our method outperforms state-of-the-art end-to-end unsupervised models and sur-passes its supervised counterpart. 2.