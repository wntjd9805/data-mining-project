Abstract
Reconstructing interacting hands from a single RGB im-age is a very challenging task. On the one hand, severe mu-tual occlusion and similar local appearance between two hands confuse the extraction of visual features, resulting in the misalignment of estimated hand meshes and the image.
On the other hand, there are complex spatial relationship between interacting hands, which significantly increases the solution space of hand poses and increases the difficulty of network learning. In this paper, we propose a decoupled iterative refinement framework to achieve pixel-alignment hand reconstruction while efficiently modeling the spatial relationship between hands. Specifically, we define two fea-ture spaces with different characteristics, namely 2D visual feature space and 3D joint feature space. First, we obtain joint-wise features from the visual feature map and utilize a graph convolution network and a transformer to perform intra- and inter-hand information interaction in the 3D joint feature space, respectively. Then, we project the joint fea-tures with global information back into the 2D visual fea-ture space in an obfuscation-free manner and utilize the 2D convolution for pixel-wise enhancement. By performing multiple alternate enhancements in the two feature spaces, our method can achieve an accurate and robust reconstruc-tion of interacting hands. Our method outperforms all ex-isting two-hand reconstruction methods by a large margin on the InterHand2.6M dataset. 1.

Introduction 3D hand reconstruction plays an important role in many applications, such as virtual reality (VR), augmented re-*Corresponding authors
Figure 1. Decoupled Iterative Refinement. Our method extracts the visual feature map from the input RGB image, then iteratively performs visual feature refinement and joint feature interaction in a decoupled manner, and finally uses the enhanced joint feature for interacting hands reconstruction. ality (AR), robotics, etc. With the emergence of large-scale datasets and deep learning, single-hand pose estima-tion and reconstruction [50, 18, 53, 34, 52, 40, 63, 6, 36, 19, 5, 59, 64, 1, 22, 46, 2, 33, 32, 9, 7, 8] have made significant progress in the past few years. Furthermore, since two hands can express richer semantics and imple-ment more complex operations, interacting two-hand recon-struction has received a lot of attention recently. Previous work usually relies on depth cameras [3, 38, 23, 49, 37] or multi-camera systems [45], which greatly limits the appli-cation scenarios of these methods. In this paper, we focus on reconstructing interacting hands from the widely avail-able RGB image.
Compared to the single-hand reconstruction task, recon-structing the interacting hands from a single RGB image is more challenging and far from being solved. On the one hand, severe mutual occlusion between interacting hands re-sults in a large amount of hand area being unobservable. At the same time, the self-similar appearance brings severe am-biguity and confusion to the extraction of visual representa-tions. Thus, self-occlusion and self-similarity tend to cause misalignment between the estimated hand meshes and the input image. On the other hand, the interacting hands have complex spatial relationships, and the dramatic increase in the degrees of freedom of the pose solution brings difficul-ties to the optimization of the network.
Some methods attempt to alleviate the interference of self-occlusion and similar appearance by utilizing some vi-sual cues such as heatmaps [57], joint-visibility [20], and part-segmentation [13]. Nonetheless, these methods ignore the tight dependency between interacting hands. In order to better capture the spatial relationship between hands, Ham-[16] extract redundant node features from the pali et al. visual feature map and use a transformer to perform mes-sage passing between nodes of two hands. However, their method requires an additional association process to deter-mine the relationship between the redundant node and the joints, which increases the difficulty of network optimiza-tion. Li et al.
[24] propose to perform dense interaction between the hand vertices, so as to model the spatial rela-tionship between the hands. At the same time, they perform dense interaction between the hand vertices and pixel fea-tures, so as to achieve image-mesh alignment. However, attention-based dense interactions are computationally ex-pensive and suffer from the risk of overfitting.
In this paper, we decouple the difficult two-handed re-construction task into a spatial relationship modeling prob-lem and a pixel-level alignment problem, which can be han-dled in a simple but efficient way in specialized spaces, re-spectively. As shown in Fig 1, our method explicitly de-fines two feature spaces, 2D visual feature space and 3D joint feature space. In 3D joint feature space, we represent hand information by compact joint features. We adopt a
Graph Convolution Network (GCN) [60] and a transformer
[61] to model the intra- and inter-relationships of two hands, respectively. In 2D visual feature space, we adopt the vi-sual feature map to represent two hands information and enhance local visual features by fusing joint features in an obfuscation-free way. We communicate the two spaces through an unambiguous 2D-3D coordinate projection re-lationship. Performing long-range relational modeling in joint feature space is computationally friendly. It can take advantage of skeletal structure information, which reduces the difficulty of network optimization. At the same time, joint features with global information can provide strong disambiguation clues for local visual features, alleviating the information loss caused by self-occlusion and the ambi-guity caused by self-similarity.
Experiments show that our method significantly out-performs State-Of-The-Art (SOTA) methods on the In-terHand2.6M dataset [24]. At the same time, we also show qualitative images and videos (see supplementary material) of our method on multiple in-the-wild samples
[51, 54, 4, 44], from which we can observe that our method has a strong generalization ability. Code is available at: https://github.com/PengfeiRen96/DIR.
Our contributions can be summarized as follows:
• We propose a decoupled iterative refinement frame-work for reconstructing interacting hands. Our method achieves pixel-level mesh-image alignment while effi-ciently modeling the spatial relationship of the hands.
• We model the spatial relationship of two hands through compact and semantically explicit joint nodes, which is computationally friendly and can utilize the priors of hand bone structure.
• We propose an obfuscation-free way to project joint features into visual feature space, which alleviates the ambiguity caused by self-similarity and the absence of visual cues caused by self-occlusion.
• Our method outperforms recent SOTA methods by a large margin and shows a strong generalization ability for the in-the-wild images. 2.