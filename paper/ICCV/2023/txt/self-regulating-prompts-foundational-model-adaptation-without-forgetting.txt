Abstract
Prompt learning has emerged as an efficient alternative for fine-tuning foundational models, such as CLIP, for var-ious downstream tasks. Conventionally trained using the task-specific objective, i.e., cross-entropy loss, prompts tend to overfit downstream data distributions and find it chal-lenging to capture task-agnostic general features from the frozen CLIP. This leads to the loss of the model’s original generalization capability. To address this issue, our work introduces a self-regularization framework for prompting called PromptSRC (Prompting with Self-regulating Con-straints). PromptSRC guides the prompts to optimize for both task-specific and task-agnostic general representa-tions using a three-pronged approach by: (a) regulat-ing prompted representations via mutual agreement max-imization with the frozen model, (b) regulating with self-ensemble of prompts over the training trajectory to encode their complementary strengths, and (c) regulating with tex-tual diversity to mitigate sample diversity imbalance with this the visual branch. To the best of our knowledge, is the first regularization framework for prompt learning that avoids overfitting by jointly attending to pre-trained model features, the training trajectory during prompting, and the textual diversity. PromptSRC explicitly steers the prompts to learn a representation space that maxi-mizes performance on downstream tasks without compro-mising CLIP generalization. We perform extensive exper-iments on 4 benchmarks where PromptSRC overall per-forms favorably well compared to the existing methods.
Our code and pre-trained models are publicly available at: https://github.com/muzairkhattak/PromptSRC. 1.

Introduction
Vision-Language (VL) models, such as CLIP [35] and
ALIGN [20], have demonstrated remarkable generaliza-tion capabilities for downstream tasks. These VL models
*Joint first authors. (cid:0) uzair.khattak@mbzuai.ac.ae are trained on large-scale web data with a contrastive loss, which allows them to encode open-vocabulary concepts by aligning pairs of images and texts in a shared embedding space. The resulting model is suited for downstream tasks such as open-vocabulary image recognition [23], object de-tection [11], and image segmentation [29].
Prompt learning has emerged as a more efficient alter-native to fine-tuning large-scale models, as shown in re-cent studies [58, 59, 3, 17, 40, 28]. This approach intro-duces a few learnable prompt vectors to adapt models like
CLIP for downstream tasks while keeping the pre-trained model weights fixed. However, since the prompts are opti-mized with respect to the task-specific objective [59], such as the cross-entropy loss for ImageNet [6] classification, the prompted model tends to overfit to the task-specific data distribution as the training progresses. This can result in the prompted model losing the original generalization capa-bility of the frozen CLIP model towards new tasks. There-fore, learning prompts that can model both task-specific and task-agnostic representations remain a major challenge for adapting foundational VL models.
This work seeks to self-regulate prompts to address the issue of prompt overfitting. To this end, we propose a self-regularizing framework that guides the prompts to jointly optimize for both task-specific and task-agnostic general representations using a three-pronged approach. a) Reg-ulating via Mutual Agreement Maximization: We observe that generalizable zero-shot knowledge is preserved within frozen pre-trained VL model features but they lack task-specific knowledge.
In contrast, prompts achieve better adaptation to a given task but with reduced generalizabil-ity to new tasks. Therefore, we propose to regulate learned prompts by maximizing the agreement between prompted and frozen VL model features while adapting them to the downstream task. b) Regulating with the Self-ensemble:
In the early epochs, prompts act are not mature to capture contextual information. As the training progresses, prompts tend to become more task-specific. Therefore we deploy a weighted prompt aggregation technique to prompts during training to regulate them using their self-ensemble over the
Figure 1: (Left): Existing prompt learning approaches rely on task-specific objectives that restrict prompt learning to learn a feature space suitable only for downstream tasks and consequently lose the generalized knowledge of CLIP (shown in purple). Our self-regulating framework explicitly guides the training trajectory of prompts towards the closest point between two optimal solution manifolds (solid line) to learn task-specific representations while also retaining generalized CLIP knowledge (shown in green). (Middle): Averaged across 11 image recognition datasets, PromptSRC surpasses existing methods on the base-to-novel generalization setting. (Right): We evaluate our approach on four diverse image recognition benchmarks and it overall shows competitive results compared to the previous state-of-the-art. training phase. The weights are sampled from a Gaussian distribution which suitably aggregates the useful knowledge learned by prompts at different training epochs. c) Regu-lating with Textual Diversity: We note that unlike having multiple image samples per category for the vision encoder, there is only a single textual label available for each class.
Therefore, imposing the mutual agreement constraints on multi-modal features results in sub-optimal performance due to the lack of diversity in text-side labels for the text en-coder. We overcome this disparity and regulate the prompts through diverse text label templates for each class.
Overall, our approach explicitly steers prompts to learn a representation space that maximizes its performance on downstream tasks without compromising pre-trained CLIP generalization (Fig. 1: Left). We demonstrate the effective-ness of PromptSRC on four representative tasks. On the base-to-novel generalization benchmark across 11 datasets (Fig. 1: Middle), our method achieves average gains of
+1.42% in harmonic-mean over the state-of-the-art MaPLe
[22] and +8.26% over CLIP. Further, PromptSRC achieves competitive results in cross-dataset transfer, domain gener-alization, and few-shot image recognition (Fig. 1:Right).
In summary, our self-regulating prompt learning frame-work has the following main contributions:
• We address the inherent problem of prompt overfit-ting for adapting foundational models through self-regularization. Our framework explicitly guides the prompts to jointly acquire both task-specific knowl-edge and task-agnostic generalized knowledge by maximizing the mutual agreement between prompted and frozen VL model features. (§3.2.1)
• We suggest a weighted self-ensembling strategy for prompts that captures their complementary features learned at different epochs during training and en-hances their generalization performance. (§3.2.2)
• To overcome the significant diversity mismatch be-tween the text and visual domains, we propose text-side diversity which complements limited textual la-bels via multiple text augmentations and regularizes prompts to learn more generalized contexts. (§3.2.3) 2.