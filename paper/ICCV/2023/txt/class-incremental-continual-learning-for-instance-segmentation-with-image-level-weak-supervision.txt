Abstract
Instance segmentation requires labor-intensive manual labeling of the contours of complex objects in images for training. The labels can also be provided incrementally in practice to balance the human labor in different time steps. However, research on incremental learning for in-stance segmentation with only weak labels is still lacking. In this paper, we propose a continual-learning method to seg-ment object instances from image-level labels. Unlike most weakly-supervised instance segmentation (WSIS) which re-lies on traditional object proposals, we transfer the seman-tic knowledge from weakly-supervised semantic segmenta-tion (WSSS) to WSIS to generate instance cues. To ad-dress the background shift problem in continual learning, we employ the old class segmentation results generated by the previous model to provide more reliable semantic and peak hypotheses. To our knowledge, this is the first work on weakly-supervised continual learning for instance seg-mentation of images. Experimental results show that our method can achieve better performance on Pascal VOC and
COCO datasets under various incremental settings1. 1.

Introduction
Continual learning (CL) aims to continually learn from data provided in sequential sessions while avoiding catas-trophic forgetting [37, 19]. It has gained significant atten-tion since incrementally learning a model is useful in many applications. CL has two main scenarios. The first, task-incremental CL [33, 54], assumes that we know the task in-dices of the input data during inference. The second, class-incremental CL [5, 45, 44, 59], assumes that the task index is inaccessible for inference and we aim to classify the data labels of all seen tasks, which is more generally applicable.
*corresponding author. 1https://github.com/AI-Application-and-Integration-Lab/CL4WSIS
Figure 1. Our weakly-supervised incremental learning model. As-suming “horse” is the old class and “person” is the current during
CL, our model leverages semantic knowledge yielded by CL for
WSSS to produce synthetic center and offset labels for the current person class. Semantic-aware selective distillation is employed to preserve knowledge of the old horse class to achieve CL for WSIS.
Based on the image labels, previous CL works mainly devote to image classification of sequential tasks. In this paper, we take a step forward in class-incremental CL and learn instance segmentation (IS) models from the image labels only. To learn an IS model, previous works often need pixel-level boundary annotations of training objects.
Recently, methods that can learn instance segmenters in-crementally are developed in CL [23, 10]. However, they need expensive pixel-wise supervisions at each incremen-tal learning step. Our approach, on the other hand, requires only cheaper image-level labels that are easily available. To our knowledge, this is the first CL study using weakly su-pervised image labels for subsequent IS model learning.
On the other hand, IS has been studied for a long time and has made significant progress [18, 8, 26, 29, 38]. Many
Figure 2. An overview of our CL4WSIS framework. Our model employs an encoder-decoder structure. At the CL step t, the Decoder dt is responsible for generating Synthetic Pixel-wise Labels, i.e. semantic, center and offset maps, for the current classes to guide the Segmenter training. To learn with the Global Image Labels, our Decoder is combined with an Aggregator. Feature-level augmentation consistency (FLAC) and random dropout (Randrop) are further employed to enhance the reliability of WSSS generated from the Decoder. We leverage the instance cues from a Peak Generator (PG) in the Location Cue Extractor for synthetic center and offset maps for the current classes.
The knowledge is maintained by distilling the previous knowledge provided by the Previous-step Segmenter through a selective distillation and also through feature distillation. The learned Segmenter is used for the current-step inference and preserved for CL in the next step.
IS models are trained on existing benchmark datasets with pre-specified class labels. However, the learned model can only be used in limited cases of segmenting objects of pre-defined classes, but cannot handle new classes of objects.
The lack of class extensibility limits the use of models. A promising approach to this problem is to enable the model to continually learn from newly labeled images.
In the class-incremental setting, images collected in a new step can be used to train the model incrementally, where training data from previous steps cannot be used for learning in the new step. This setup has several advantages. For example, the training data in the previous steps may have to be pro-tected and cannot be used in the next step. Joint training data in all steps could also scale up learning and make com-putational resources unaffordable. However, fine-tuning from the previous-step model to the new-step model eas-ily leads to catastrophic forgetting. Due to the newly added classes, there is also a background shift problem where the background defined in the previous step is not consistent with the background in the new step images.
This paper aims to attain CL in a more effort-saving sce-nario for IS, where our model learns to predict instance-level segmentation using weakly supervised image labels.
We introduce an end-to-end incremental learning model. As semantic segmentation can be generally seen as the union of
IS for each class, we upgrade semantic segmentation to IS, as shown in Fig. 1. The model leverages a panoptic seg-mentation architecture whose decoder can generate seman-tic, instance center, and offset maps; the three maps can then yield our IS outcome.
To estimate the pixel-level semantic labels from only the image-level labels, it is observed that a single-round solu-tion derived directly from the attention map is often insuffi-cient for sophisticated boundary inference, and thus multi-round training is suggested [2, 34]. Our approach uses an attention mechanism leveraging the global image classifica-tion labels to extract the per-pixel location cues, which then helps synthesize the local labels for simultaneously train-ing our Segmenter, as shown in Fig. 2. When training is finished, only the Segmenter is used in the inference stage.
To continually update the model, the Segmenter learned in previous step serves as a teacher for model distillation.
Given images of the current step, in addition to training the Segmenter with the synthetic local labels of current classes, the Segmenter also distills from the teacher which provides the probability maps of old classes (Fig. 2). Hence, the model simultaneously learns from both fully supervised pixel-wise probability maps of the old classes and weakly supervised image labels of the new classes. Our method performs CL for WSSS at first and obtains a semantic map.
We then perform CL for WSIS (CL4WSIS) leveraging the semantic map later. To handle the CL for WSSS, our learn-ing mechanism addresses the background shift by an early occupation of the highly confident old-class objects found by the teacher model, and guides the seeking of new-class objects in the remaining regions. We also introduce the aug-mentation consistency and random dropout strategies to enhance the WSSS learning performance. We develop a peak generator to find more reliable location cues of the instances. To further tackle the background shift, we intro-duce a selective distillation strategy that learns the center and offset maps of old classes depending on the intermedi-ate semantic map. Characteristics of our method include:
• As far as we know, we have conducted the first study of
the CL4WSIS problem.
• Our method integrates CL and semantic knowledge trans-fer to IS. Not only can it outperform the previous incremen-tal WSSS method, but it can further achieve IS effectively. 2.