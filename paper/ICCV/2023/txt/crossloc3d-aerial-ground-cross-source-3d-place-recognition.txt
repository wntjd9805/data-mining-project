Abstract
We present CROSSLOC3D, a novel 3D place recognition method that solves a large-scale point matching problem in a cross-source setting. Cross-source point cloud data cor-responds to point sets captured by depth sensors with dif-ferent accuracies or from different distances and perspec-tives. We address the challenges in terms of developing 3D place recognition methods that account for the repre-sentation gap between points captured by different sources.
Our method handles cross-source data by utilizing multi-grained features and selecting convolution kernel sizes that correspond to most prominent features. Inspired by the dif-fusion models, our method uses a novel iterative refinement process that gradually shifts the embedding spaces from dif-ferent sources to a single canonical space for better metric learning. In addition, we present CS-CAMPUS3D, the first 3D aerial-ground cross-source dataset consisting of point cloud data from both aerial and ground LiDAR scans. The point clouds in CS-CAMPUS3D have representation gaps and other features like different views, point densities, and noise patterns. We show that our CROSSLOC3D algorithm can achieve an improvement of 4.74% - 15.37% in terms of the top 1 average recall on our CS-CAMPUS3D bench-mark and achieves performance comparable to state-of-the-art 3D place recognition method on the Oxford RobotCar.
The code and CS-CAMPUS3D benchmark will be available at github.com/rayguan97/crossloc3d . 1.

Introduction
Place recognition is an important problem in com-puter vision, with a wide range of applications including
SLAM [21], autonomous driving [6], and robot naviga-tion [25], especially in a GPS-denied region. Given a point cloud query, the place recognition task will predict an em-bedding such that the query will be close to the most struc-turally similar point clouds in the embedding space. The goal of place recognition is to compress information from a database with a location tag and find the closest data point from the database given a query, which is essentially an in-Figure 1: CROSSLOC3D: Our method processes cross-source point clouds into a better, shared embedding, which achieves a better retrieval outcome in a cross-source setting. formation retrieval task on a global scale.
In this paper, we deal with the problem of 3D cross-source place recognition, as illustrated in Fig. 1. The goal of 3D cross-source place recognition in the context of aerial views and ground views is extremely challenging and not well-studied in the field [13]. There are two aspects of cross-source data that result in additional challenges: cross-view and data consistency. First, the perspective differences between aerial and ground datasets would cause partial overlap of the scans, leading to a lack of point correspon-dences. Second, there may be no data consistency between sources, which will cause representation gaps. The chal-lenge of using multiple sources of data is primarily related to the different nature and fidelity of sensors. It is harder to match the 3D point clouds from different sources [13] than it is to perform similar matches between the 2D im-ages captured from the same locations, or even point clouds captured only by ground or aerial sensors. When differ-ent kinds of LiDAR sensors or the same kind of LiDAR at different distances, capture scans at the same location, the data is significantly different. 3D cross-source data is point
Figure 2: Representation gap between aerial and ground sources: We use the bounding box with the same color to focus on the same region and highlight the differences between aerial (left) and ground (right) LiDAR scans. Scopes (cyan): The aerial scans cover a large region, while ground scans cover only a local area. Coverages (green): The aerial scans cover the top of the buildings, while ground scans cover more details on the ground. Densities (blue): The distribution and density of the points are different because of various scan patterns, effective ranges, and fidelity of LiDARs. Noise Patterns (red): The aerial scans have larger noises, as we can see from a bird-eye view and top-down view of a corner of the building. cloud data captured by different depth sensors and differ significantly in terms of scope, coverage, point density, and noise distribution pattern, as shown in Fig. 2. We need a method that can better close the representation gap between the two sources by converting features in different embed-ding spaces to a common embedding space.
While the problems of localization and point-set registra-tions are well-studied, there is relatively less work on cross-source localization. Ge et al. [33] propose a localization method using point cloud data from both the air and the ground sensors. However, this method relies on semantic information for accurate 2D template matching; in addition, their data is private. To the best of our knowledge, there are no existing works that only utilize raw point information obtained from both ground and aerial LiDAR sensors and there are no known open datasets for such applications.
Main Results: In this paper, we propose a novel 3D place recognition method that works well on both single-source and cross-source point cloud data. We account for the rep-resentation gap by using multi-grained features and select-ing appropriate convolution kernel sizes. Our approach is inspired by the diffusion model [11, 23] and cold diffu-sion [4] and we propose a novel iterative process to refine multi-grained features from coarse to fine. We also propose a novel benchmark dataset that consists of point cloud data from both ground and aerial views. The novel aspects of our approach include: 1. We propose CROSSLOC3D, a novel place recognition method that utilizes multi-grained voxelization and multi-scale sparse convolution with a feature selection module to actively choose useful features and close the representation gap between different data sources. 2. We propose an iterative refinement process that shifts the feature distributions of various input sources to-ward a canonical latent space. We show that start-ing the refinement process from the coarsest features, which are most similar across different sources, toward the finer features, leads to better recall compared to doing it in the reverse order or simply concatenating features from different resolutions. 3. We present the first public 3D Aerial-Ground Cross-Source benchmark in a campus environment, CS-CAMPUS3D, which consists of both aerial and ground
LiDAR scans. We collect ground LiDAR data on mo-bile robots and process the aerial data from the state government into a suitable format to cross-reference the ground data. The dataset and code will be publicly released and made available for benchmark purposes.
We have evaluated CROSSLOC3D and other state-of-the-art 3D place recognition methods on the CS-Campus3D dataset and observe an improvement of 4.74% - 15.37% in terms of the top 1 average recall. Additionally, we observe that
CROSSLOC3D achieves close to 99% in terms of top 1% average recall on Oxford RobotCar [20] and performs com-parably, within a margin of 0.31%, to the SOTA methods on the traditional single-source 3D place recognition task. 2.