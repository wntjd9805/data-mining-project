Abstract
Sound can convey significant information for spatial rea-soning in our daily lives. To endow deep networks with such ability, we address the challenge of dense indoor prediction with sound in both 2D and 3D via cross-modal knowledge distillation. In this work, we propose a Spatial Alignment via Matching (SAM) distillation framework that elicits lo-cal correspondence between the two modalities in vision-to-audio knowledge transfer. SAM integrates audio fea-tures with visually coherent learnable spatial embeddings to resolve inconsistencies in multiple layers of a student model. Our approach does not rely on a specific input representation, allowing for flexibility in the input shapes or dimensions without performance degradation. With a newly curated benchmark named Dense Auditory Predic-tion of Surroundings (DAPS), we are the first to tackle dense indoor prediction of omnidirectional surroundings in both 2D and 3D with audio observations. Specifically, for audio-based depth estimation, semantic segmentation, and challenging 3D scene reconstruction, the proposed distil-lation framework consistently achieves state-of-the-art per-formance across various metrics and backbone architec-tures. 1.

Introduction
Humans can get a good grasp of various information about surroundings with hearing without seeing, like the size of a room or the location of an active alarm. A long line of research has analyzed such intriguing abilities of humans based on interaural differences [1, 2] or brain activation with respect to spatially aligned audio-visual inputs [3, 4], to list a few. Accordingly, there is an emerging interest in teaching neural network models for spatial reasoning without seeing.
Such models that spatially perceive the surroundings from sound can be utilized in various environments that are crit-ical for privacy preservation or visually ill-posed (e.g., low illumination or occlusion) [5, 6, 7, 8].
*Equal Contribution
Figure 1: key idea of our approach. (a) For vision-to-audio cross-modal distillation, instead of direct distillation between geometrically inconsistent modalities, we spatially align the latent feature maps of students with those of teach-ers. (b) Using auditory input only, we perform three dense predictions of surroundings: depth estimation, semantic segmentation, and 3D scene reconstruction.
Since predicting visual properties directly from audio is challenging, cross-modal knowledge distillation [9] is of-ten utilized, i.e., teaching audio models with the guidance of visual models. Visual models can make precise predic-tions about the image of the surroundings, like the location of objects or the depth of a scene. Thus, using visual mod-els as the teacher, audio models can learn how to predict visual properties in a scene from sound inputs. This cross-modal knowledge distillation has been successfully applied
to make audio models predict sparse attributes, e.g., vehicle tracking [5] or indoor navigation [7]. However, it remains challenging to make dense visual predictions about the sur-roundings from audio.
One of the core challenges behind the dense prediction with audio is to identify fine-grained attributions of the out-put. In other words, humans can intuitively make sense of the room layout by hearing, but have difficulty in explaining which bandwidths or timeframes are responsible for their perception. Unlike distilling an RGB image teacher for a thermal image student that is geometrically consistent up to the pixel level, there is no obvious one-to-one alignment between image and audio. Hence, it is not feasible to de-termine which part of the audio spectrogram corresponds to which region of the surrounding. While using multiple in-termediate features of a teacher model as a guide can still be beneficial [5, 8], it may not be possible to solve the underly-ing local correspondence problem between the two hetero-geneous modalities.
In this work, we are the first to address the dense indoor prediction of omnidirectional surroundings in both 2D and 3D with audio observations. To resolve the inconsistency problem, we propose a novel Spatial Alignment via Match-ing (SAM) distillation framework. SAM matches local cor-respondences between the two heterogeneous features by making use of learnable spatial embeddings in several lay-ers of the audio student model, combined with loose triplet-based learning objectives. We retain a set of learnable spa-tial embeddings to capture spatially varying information of each layer, which are pooled and integrated with initial au-dio features for alignment. This allows us to resolve incon-sistencies even when the shape of the audio input does not match that of the desired output, making it trivially extend-able to a challenging scenario like audio-to-3D distillation.
To comprehensively evaluate the performance of our method, we curate a new benchmark for audio-based dense prediction of surroundings based on Matterport3D [10] and SoundSpaces [7]. We collect 15.8K indoor scene multimodal observations with task-specific annotations for audio-based depth estimation, semantic segmentation, and 3D scene reconstruction. In dense auditory prediction tasks spanning from 2D to 3D, our framework consistently im-proves the performance by a wide margin, which is vali-dated on multiple architectures like U-Net [11], DPT [12], and ConvONet [13]. Also, qualitative results demonstrate that our approach can precisely predict the structure of the indoor environment with hearing without seeing. 2.