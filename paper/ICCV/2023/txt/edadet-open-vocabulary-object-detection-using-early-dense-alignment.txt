Abstract
Vision-language models such as CLIP have boosted the performance of open-vocabulary object detection, where the detector is trained on base categories but required to detect novel categories. Existing methods leverage CLIP’s strong zero-shot recognition ability to align object-level em-beddings with textual embeddings of categories. However, we observe that using CLIP for object-level alignment re-sults in overfitting to base categories, i.e., novel categories most similar to base categories have particularly poor per-formance as they are recognized as similar base categories.
In this paper, we first identify that the loss of critical fine-grained local image semantics hinders existing methods from attaining strong base-to-novel generalization. Then, we propose Early Dense Alignment (EDA) to bridge the gap between generalizable local semantics and object-level pre-diction. In EDA, we use object-level supervision to learn the dense-level rather than object-level alignment to main-tain the local fine-grained semantics. Extensive experi-ments demonstrate our superior performance to competing approaches under the same strict setting and without using external training resources, i.e., improving the +8.4% novel box AP50 on COCO and +3.9% rare mask AP on LVIS. 1.

Introduction
Open-vocabulary object detection aims to localize and recognize objects of both base categories and novel cate-gories when only the training labels on base categories are available. Beyond focusing on object detection on a closed set of categories [20, 3, 39, 30, 43], open-vocabulary detec-tion requires generalizing well from base to all novel cate-gories without annotations for each novel category.
One straightforward idea is to generate pseudo-proposals relevant to novel categories and train detectors with base and novel categories (see Figure 1b), adopted by [54, 28, 14, 38, 2]. They usually first extract concepts relevant to novel categories and then generate proposals of novel con-†Corresponding author
Figure 1: Different approaches to building an open-vocabulary detector: (a) their performance comparison. †: with self-training. (b) generate pseudo “novel” proposals from extra training re-sources and VLMs, or (c) generalize from VLMs cepts from extra training resources. Among them, some works [54, 14, 38] follow the weak open-vocabulary set-ting [48], where the class names of novel categories di-rectly corresponding to novel concepts are available in the training phase. Alternatively, some works [50, 28] extract novel concepts from captions or image-text pairs. Although these approaches have improved the performance of detect-ing novel categories, the need for additional training re-sources that heavily overlap with or are relevant to novel categories would limit them to practical applications.
Recently, contrastive pre-training of vision-language models (VLMs) like CLIP [34] and ALIGN [22] have shown strong open-vocabulary image recognition ability.
Some open-vocabulary detection works [16, 13, 50, 26, 47] explore utilizing VLMs to learn transferable object repre-sentations. Figure 2b shows a high-level abstraction of these open-vocabulary detection frameworks. Although they plug well-designed methods to close the gap between visual representation learning for objects and images, they
Figure 2: The comparison between Object-level Alignment and our Early Dense Alignment (Eda) on (b)-(c) architectures, (b1)-(c1) local image semantics and clustering results, and (a) box AP of novel categories similar to base categories. We list six novel categories most similar to base categories by calculating the average similarity between the randomly sampled thousands of novel objects’ visual features and base categories’ text embeddings. Our Eda: (1) successfully recognizes the fine-grained novel CD-player that is predicted to base speaker by object-level alignment; (2) better groups local image semantics into object regions compared with CLIP; (3) achieves a much higher novel box AP for predicting novel objects similar to base objects, showing that Eda can distinguish fine-grained details of similar novel and base categories. In contrast, object-level alignment overfits base categories. only achieve comparable or slightly better performance than
CLIP-RPN1 on novel categories (right part of Figure 1a).
We also follow the line of works that aim to general-ize VLMs for object detection without generating pseudo-proposals relevant to novel categories (see Figure 1c). And we explore how better to utilize VLMs for base-to-novel generalization in open-vocabulary object detection.
In this paper, we start by discovering and analyzing the respective advantages of VLMs and existing open-vocabulary detection frameworks for object detection. First, we observe that VLMs can predict local image semantics for novel categories while existing frameworks are easier to overfit base categories. As shown in the “Semantic” of
Figure 2 (CLIP), CLIP successfully recognizes the novel local regions of the “CD player”, while the existing frame-work classifies the novel “CD player” as the “speaker” in base categories. The reason is that VLMs may have seen fine-grained image-text pairs describing local seman-tics during training.
In contrast, existing frameworks di-rectly align the object representations to the classifier of base categories, which loses the fine-grained details that dis-tinguish novel objects from their similar base objects. With-out fine-grained details, the object-level representations of novel objects and their similar base objects are similar, re-1CLIP-RPN baseline simply utilizes CLIP to classify cropped propos-als generated by region proposal network (RPN) trained on base categories. sulting in them being classified into base categories. Fig-ure 2a (marked in grey) shows that the object-level align-ment’s prediction accuracy for novel objects similar to base objects is much lower. Therefore, we propose to avoid di-rect object-level alignment and fully utilize VLMs’ ability to distinguish fine-grained details for similar novel and base objects to preserve the recognition ability of novel cate-gories.
Second, we observe that the existing framework can bet-ter group local image semantics into object regions than
VLMs. The reason is that its object-level supervision for object representations generated from local semantics im-proves local semantic consistency to objects. As shown in the “K-means clustering” of Figure 2-b1 and 2 (CLIP), the existing framework groups the “‘keyboard” well (marked in yellow), while the corresponding two separate “keyboard” regions in CLIP’s clustering map are mixed and indistin-guishable. Therefore, we propose to adopt object-level su-pervision for the dense alignment of local image semantics.
Based on the above discoveries, we propose a simple but effective solution, named early dense2 alignment (Eda), to combine the strengths of VLMs and the existing frame-works. To avoid overfitting to base categories caused by 2“early” means the use of features in the early stages of the backbone, and “dense” means per-pixel alignment to text for obtaining object-level prediction (Note that only object-level annotations are used).
object-level alignment, Eda directly predicts object cate-gories from local image semantics to fully distinguish the fine-grained details of similar base objects and novel ob-jects. To maintain the local semantics consistent for better grouping and localization, we use object-level supervision to learn the dense-level alignment. As shown in Figure 2c,
Eda first aligns local image semantics to the CLIP’s seman-tic space early and then predicts object-level labels based on the dense probabilities to categories. Our Eda enables dense-level alignment for local image semantics, which is much more generalizable than late object-level alignment to novel categories. Meanwhile, it can better group local se-mantics to object regions (see Figure 2c1). Also, Figure 2a (mark in green) shows that our Eda significantly improves the prediction accuracy for novel categories similar to base categories.
Finally, we propose EdaDet, a simple open-vocabulary detection framework by leveraging our early dense align-ment (Eda). For object localization, we follow existing works [38, 54, 16, 26] to learn class-agnostic object pro-posals. To ensure an efficient end-to-end localization and recognition framework, we adopt a query-based proposal generation method like DETR [3] but revise it to be class-agnostic. For open-vocabulary recognition, we apply our generalizable Eda to predict the categories of class-agnostic
In addition, for better generalization, EdaDet proposals. deeply decouples the object localization and recognition by separating the open-vocabulary classification branch from the class-agnostic proposal generation branch at a more shallow layer of the decoder.
To evaluate the effectiveness of our EdaDet, we con-duct experiments on LVIS [17], COCO [29], and Ob-jects365 [42] benchmarks. In summary, our main contri-butions are as follows,
• We propose a novel and effective early dense align-ment (Eda) for base-to-novel generalization in object detection without knowing the class names of novel categories and using extra training resources.
• We propose an end-to-end EdaDet framework, which deeply decouples the object localization and recogni-tion by separating classification from the recognition at a more shallow layer of the decoder.
• Despite being simple, EdaDet achieves strong quan-titative results, outperforming state-of-the-art methods with the strict setting on COCO and LVIS by 5.8% box
AP50 and 2.0% mask AP on novel categories respec-tively. Moreover, EdaDet shows striking cross-dataset transferable capability.
• EdaDet shows impressive qualitative predictions on lo-cal image semantics and demonstrates efficient and effective performance improvement when scaling the model size thanks to our generalizable Eda and deeply decoupled detection framework. 2.