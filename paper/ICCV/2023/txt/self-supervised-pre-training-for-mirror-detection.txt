Abstract
Existing mirror detection methods require supervised
ImageNet pre-training to obtain good general-purpose im-age features. However, supervised ImageNet pre-training focuses on category-level discrimination and may not be suitable for downstream tasks like mirror detection, due to the overfitting upstream tasks (e.g., supervised image classi-fication). We observe that mirror reflection is crucial to how people perceive the presence of mirrors, and such mid-level features can be better transferred from self-supervised pre-trained models. Inspired by this observation, in this paper we aim to improve mirror detection methods by proposing a new self-supervised learning (SSL) pre-training frame-work for modeling the representation of mirror reflection progressively in the pre-training process. Our framework consists of three pre-training stages at different levels: 1) an image-level pre-training stage to globally incorporate mirror reflection features into the pre-trained model; 2) a patch-level pre-training stage to spatially simulate and learn local mirror reflection from image patches; and 3) a pixel-level pre-training stage to pixel-wisely capture mirror reflection via reconstructing corrupted mirror images based on the relationship between the inside and outside of mir-rors. Extensive experiments show that our SSL pre-training framework significantly outperforms previous state-of-the-art CNN-based SSL pre-training frameworks and even out-performs supervised ImageNet pre-training when trans-ferred to the mirror detection task. Code and models are available at https:// jiaying.link/ iccv2023-sslmirror/ 1.

Introduction
Mirrors are prevalent in our daily lives. As their appear-ances are largely determined by their surroundings, they generally lack a consistent appearance, making it difficult to separate them from their surroundings. This may affect many computer vision tasks such as object detection [2], vision-language navigation [1] and depth estimation [29].
*Corresponding authors. (a) Image (b) MirrorNet on ImageNet (c) VCNet on ImageNet (d) Ours (MirrorNet on our SSL) (e) GT
Figure 1. State-of-the-art mirror detection methods [37, 30] are based on costly supervised ImageNet pre-training. They may fail even in obvious cases, e.g., (a) when the mirror clearly reflects a real object outside of the mirror. (b) and (c) are MirrorNet [37] and VCNet [30], respectively, pre-trained on ImageNet with full supervision. (c) is MirrorNet with our proposed SSL framework and without supervised ImageNet pre-training. Our SSL scheme leverages the mirror reflection cue and avoids feature redundancy in the pre-training stage. It outperforms those pre-trained on Ima-geNet with full supervision (i.e., (b) and (c)).
It is therefore crucial to design high-performance mirror de-tection methods to facilitate computer vision applications.
Recently, a few methods [37, 23, 30] have been pro-posed for mirror detection, but they all require to be initial-ized with supervised ImageNet [8] pre-trained weights and then fine-tuned on mirror detection datasets. While such a transfer learning approach is common in general com-puter vision tasks (e.g., object detection [24] and seman-tic segmentation [7]) to utilize category-oriented features from large-scale image datasets, we challenge if this strat-egy is necessary for the mirror detection task for two rea-sons. First, the supervised ImageNet pre-training process involves high labelling costs, as it is very time-consuming and labor-intensive to label such a large-scale image dataset (∼1.3M images). Second, unlike general vision tasks, mir-ror detection does not require category-level object under-standing. Instead, it requires an understanding of the rela-tionship between inside and outside of mirrors (e.g., con-trast [37], similarity [23], and visual chirality [30]). Thus,
while pre-training on ImageNet requires expensive labels, it causes feature redundancy for the mirror detection task.
Figure 1 shows that existing models pre-trained on Ima-geNet tend to over-detect the mirror regions. Although the two SOTA methods [37, 30] can correctly locate the mirror regions, they are unable to distinguish between the mirrored objects inside the mirrors and the real objects outside, even though these methods contain well-designed modules to learn to address this problem via the fine-tuning stage. This motivates us to investigate whether the current pre-training approach may affect the final detection performance, and whether we may incorporate some intrinsic mirror proper-ties in the pre-training process to assist mirror detection.
An interesting observation is that humans do not need to be “trained” to recognize mirrors. Instead, we learn to rec-ognize them implicitly from young. This aligns with the key idea of self-supervised learning (SSL). Works from neu-roscience [31, 20] have shown that humans use mid-level visual cortex to recognize mirror reflection. According to
[44], mid-level features are hard to be transferred from su-pervised pre-training on ImageNet, and SSL is suitable for learning mid-level representations, which inspires us to de-velop an SSL pre-training framework for modeling mirror reflection.
In this paper, we propose a new SSL pre-training frame-work for mirror detection, which explicitly considers mirror reflection during the pre-training process. Our framework does not require human-annotated labels from large-scale image datasets. It consists of three stages to progressively pre-train the backbone network from global to local: 1) an image-level pre-training stage to obtain the representation of mirror reflection globally by recognizing the geometric transformation applied to the image; 2) a patch-level pre-training stage to mimic patch-wise mirror reflection and then learn the spatial correlation between the original ob-ject patches and the corresponding mirrored object patches; and 3) a pixel-level pre-training stage to extract the pixel-to-pixel relationship of mirror reflection by image recon-struction; Under such progressive pre-training scheme, our
SSL pre-training framework can learn the representation of mirror reflection, and then effectively transfer this knowl-edge to the subsequent mirror detection process for better detection performances. We conduct comprehensive ex-periments to demonstrate the effectiveness of our SSL pre-training framework. We show that it can significantly boost the performances of existing mirror detection methods, and outperform other CNN-based SSL pre-training frameworks on the mirror detection task.
To conclude, this paper makes three key contributions:
• To the best of our knowledge, we are the first to inves-tigate how existing SSL pre-training frameworks per-form on the mirror detection task, compared with su-pervised ImageNet pre-training.
• We propose a new SSL pre-training framework that consists of three stages at different levels to progres-sively learn the representation of mirror reflection.
Compared with the features from a supervised Ima-geNet pre-trained model, our representation is better due to the reduced gap between the pre-training task and the target downstream task (i.e., mirror detection).
• Extensive experiments show that our SSL pre-training framework performs the best among all state-of-the-art
SSL methods, and even better than models with super-vised ImageNet pre-training. 2.