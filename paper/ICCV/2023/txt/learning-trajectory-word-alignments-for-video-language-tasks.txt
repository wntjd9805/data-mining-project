Abstract
In a video, an object usually appears as the trajectory, i.e., it spans over a few spatial but longer temporal patches, that contains abundant spatiotemporal contexts. However, modern Video-Language BERTs (VDL-BERTs) neglect this trajectory characteristic that they usually follow image-language BERTs (IL-BERTs) to deploy the patch-to-word (P2W) attention that may over-exploit trivial spatial con-texts and neglect signiﬁcant temporal contexts. To amend this, we propose a novel TW-BERT to learn Trajectory-Word alignment by a newly designed trajectory-to-word (T2W) attention for solving video-language tasks. More-over, previous VDL-BERTs usually uniformly sample a few frames into the model while different trajectories have di-verse graininess, i.e., some trajectories span longer frames and some span shorter, and using a few frames will lose certain useful temporal contexts. However, simply sam-pling more frames will also make pre-training infeasible due to the largely increased training burdens. To alleviate the problem, during the ﬁne-tuning stage, we insert a novel
Hierarchical Frame-Selector (HFS) module into the video encoder. HFS gradually selects the suitable frames condi-tioned on the text context for the later cross-modal encoder to learn better trajectory-word alignments. By the proposed
T2W attention and HFS, our TW-BERT achieves SOTA per-formances on text-to-video retrieval tasks, and comparable performances on video question-answering tasks with some
VDL-BERTs trained on much more data. The code will be available in the supplementary material. 1.

Introduction
By witnessing the boom of BERT-like models in single domains, e.g., vision or language[52, 44, 21], researchers
*Corresponding authors. (a) RoI based IL-BERT (b) Patch based IL-BERT (c) Patch based VDL-BERT (d) TW-BERT Pretraining (e) TW-BERT Fine-Tuning
A boy is playing basketball.
Figure 1. The comparisons of ﬁve different ways to build object-word alignments. In (b), by patch-to-word (P2W) attention, im-plicit object-word alignment can be built, e.g., the “ball region” is aligned with basketball. While in (c), P2W attention may concen-trate the attention of an object on only one frame, e.g., basketball only attends over on the “ball region” of the ﬁrst frame. (d) When pre-training TW-BERT, the trajectory of an object is constructed, e.g., the “ball trajectory” through 4 frames built. Note that we only use 4 frames during pre-training, we show 8 frames for compar-ing with (e) and the transparent ones denote that they are not input into the model. (e) When ﬁne-tuning TW-BERT, we use HFS to select 4 frames from 8 and we can see that the selected frames are different from the uniformly sampled ones in (d). begin to build vision-language BERTs[3, 61, 10] for learn-ing robust cross-modal associations. Compared with the images, videos provide more details for building robust as-sociations, e.g., the spatiotemporal contexts in videos can 1
better describe the actions. However, directly addressing dynamic videos may cost more storage and computation resources. To circumvent such huge cost, researchers ﬁrst study image-language BERT (IL-BERT)[17, 25, 8, 27, 61] and then exploit the research fruits to build the challeng-ing, while more pragmatic, video-language BERT (VDL-BERT)[47, 2, 26, 36, 13, 35, 46, 37, 54, 51].
Pioneering techniques are directly inherited from IL-BERT into VDL-BERT, e.g., the image encoders are used to embed sampled video frames [23, 62]. However, since image encoders can hardly learn temporal contexts, the re-sultant VDL-BERTs will degrade to IL-BERT even if they are trained by video-text pairs. To ameliorate it, researchers apply the video encoders [32, 4, 12, 24] to embed spa-tiotemporal contexts. Though promising improvements are observed, these VDL-BERTs still neglect or ill-consider a signiﬁcant factor in IL-BERT: the object-word alignment, which helps learn robust associations.
To learn object-word alignments, as Figure 1(a) shows, researchers use an RoI-based extractor to embed an image into a series of RoI features [34, 27, 8]. However, this RoI-based extractor is ofﬂine trained by object detection with limited label inventory, which will weaken the IL-BERT since the extractor will not be updated during large-scale pre-training. To enable the end-to-end training, researchers substitute the RoI-based extractor with visual transformers whose outputs are a series of grid embeddings, which will be used to build the cross-modal connections. Although a single grid usually does not construct an integral object, fortunately, the widely applied patch-to-word (P2W) atten-tion of IL-BERT can softly seek the salient visual regions for a given query word. Then the object-word alignments can still be built between this softly detected object and the word, e.g., as shown in Figure 1(b), the object “basketball” can be implicitly attended by the corresponding word query.
Although the P2W attention remedies the loss of RoI-level features for learning object-word alignments in IL-BERT, its effectiveness is weakened in the video case. This is because the objects usually act as the Trajectories which span a few spatial while multiple temporal grids in the videos. Thus, directly applying the P2W attention may over-exploit the trivial spatial contexts while neglecting the signiﬁcant temporal contexts and then make the model at-tend to only one or two frames. Figure 1(c) shows this lim-itation that P2W attention only aligns the “ball” in the ﬁrst frame to the word ball. this
To address limitation, we propose to learn
Trajectory-to-Word alignments to solve video-language tasks and name this model as TW-BERT. Speciﬁcally, such alignment is learnt by a novel designed trajectory-to-word
T2W attention, which ﬁrst uses the word as the query to seek the salient parts of each frame and the sought parts are sequenced to form the trajectory. Then the query word at-tends over the trajectories again for capturing cross-modal associations.
In this way, the trivial spatial regions are weakened and the temporal contexts will be strengthened, e.g., as shown in Figure 1(d), the attention weights of the word will be concentrated on the object trajectory instead of only one frame as in (c). In the implementation, we fol-low most VDL-BERTs to set up the network: two single-modal encoders for the video and text and one cross-modal encoder, which is sketched in Figure 2. For the cross-modal encoder, since our T2W attention does not have the same structure as the word-to-patch (W2P) attention, our cross-modal encoder is asymmetric.
Moreover, previous VDL-BERTs usually uniformly sample a few frames into the model, which contains two drawbacks. Firstly, using a few frames may lose temporal context and secondly, uniform sampling can hardly capture the varying graininess of the trajectories, i.e., some trajec-tories span longer frames and some span shorter. However, simply sampling more frames will largely increase the pre-training burdens that are beyond the computation resources we own. To alleviate this problem, in the ﬁne-tuning stage, we sample more frames into the video encoder while only keeping the most relevant frames according to the cor-responding text by a novel designed Hierarchical Frame-Selector (HFS). For example, as shown in Figure 1(e), 4 frames are selected by HFS from the 8 uniformly sam-pled ones for learning trajectory-word alignment. Specif-ically, HFS inserts a few lightweight layers into the video encoder and these layers can gradually ﬁlter frames condi-tioned on the language context.
In this way, HFS learns the coarse-grained trajectory-word alignments, i.e., frame-word alignments, to help the later T2W attention learn more
ﬁne-grained trajectory-word alignments.
To sum up, our contributions are:
• We propose a novel perspective to consider the videos that are composed of moving object trajectories, which may inspire the researchers to build more advanced
VDL-BERTs.
• We propose a simple while effective T2W attention to learn Trajectory-to-Word alignments.
• We propose a novel hierarchical frame-selector (HFS) in TW-BERT during ﬁne-tuning to capture the varying graininess of the trajectory while not largely increasing the training burdens.
• We achieve SOTA performances compared with other
VDL-BERTs trained by the same amount of data. 2.