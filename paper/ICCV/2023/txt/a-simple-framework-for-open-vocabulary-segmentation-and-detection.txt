Abstract
We present OpenSeeD, a simple Open-vocabulary
Segmentation and Detection framework that jointly learns from different segmentation and detection datasets. To bridge the gap of vocabulary and annotation granularity, we first introduce a pre-trained text encoder to encode all the visual concepts in two tasks and learn a common se-mantic space for them. This gives us reasonably good re-sults compared with the counterparts trained on segmen-tation task only. To further reconcile them, we identify two discrepancies: i) task discrepancy – segmentation re-quires extracting masks for both foreground objects and background stuff, while detection merely cares about the former; ii) data discrepancy – box and mask annotations are with different spatial granularity, and thus not directly interchangeable. To address these issues, we propose a de-coupled decoding to reduce the interference between fore-ground/background and a conditioned mask decoding to assist in generating masks for given boxes. To this end, we develop a simple encoder-decoder model encompass-ing all three techniques and train it jointly on COCO and
Objects365. After pre-training, our model exhibits com-petitive or stronger zero-shot transferability for both seg-mentation and detection. Specifically, OpenSeeD beats the state-of-the-art method for open-vocabulary instance and panoptic segmentation across 5 datasets, and outperforms previous work for open-vocabulary detection on LVIS and
ODinW under similar settings. When transferred to specific tasks, our model achieves new SoTA for panoptic segmenta-tion on COCO and ADE20K, and instance segmentation on
ADE20K and Cityscapes (The bottom row in Fig. 1 shows a comparison of the performance of OpenSeeD and pre-vious SoTA methods). Finally, we note that OpenSeeD is the first to explore the potential of joint training on seg-mentation and detection, and hope it can be received as a strong baseline for developing a single model for both tasks in the open world. Code will be released at https:
//github.com/IDEA-Research/OpenSeeD.
∗Equal contribution. List in random.
†Equal advisory contribution.
This work is developed during an internship at IDEA.
1.

Introduction
Developing vision systems that can be transferable to novel concepts or domains has emerged as an important research topic in the community.
In the light of strong zero-shot transferability demonstrated in the seminal work
CLIP [40], a number of researchers have attempted to build advanced open-vocabulary models by leveraging large-scale image-text pairs for fine-grained vision tasks such as detection [10,12,22,30,57] and segmentation [7,17,48,59].
Arguably, core vision tasks like detection and segmenta-tion are fairly distinct in their vocabulary sizes and spatial granularities of supervision, as illustrated in Fig. 2 (a). For example, the commonly used public detection dataset Ob-jects365 [42] contains box annotations for 365 concepts in around 1.7M images, while mask annotations in COCO [33] cover merely 133 categories in 0.1M images. Previous works have explored different ways of leveraging a large amount of image-text data for open-vocabulary detection or segmentation, such as distilling the visual-semantic rep-resentations from multi-modal foundation models [12, 57], designing fine-grained or augmented contrastive learning methods [39] or utilizing pseudo-labeling techniques [30, 56]. To the best of our knowledge, most (if not all) of them focused on how to improve the performance for either detection or segmentation. Moreover, transferring weak image-level supervision to fine-grained tasks usually re-quires sophisticated designs to mitigate the huge granular-ity gap and is vulnerable to noises in image-text pairs. This leads to a natural question: can we bridge detection and seg-mentation that are cleaner and have a closer gap to attain a good open-vocabulary model for both?
Taking one step back, marrying detection and segmen-tation had been previously explored in two main ways. On one hand, Mask R-CNN [15] is one of the first works that proposed to jointly learn detection and instance segmenta-tion on COCO. On the other hand, it is shown that detec-tion models pre-trained on Objects365 can be feasibly trans-ferred for COCO panoptic segmentation [29]. However, as depicted in Fig. 2 (b), the former method requires the model to be trained on the same dataset containing aligned box and mask annotations, while the latter method follows pre-train-then-fine-tune protocol, leading to two separate closed-set models.
In this work, we are the first to pro-pose jointly learning from detection and segmentation data, and more importantly serving an open-vocabulary model for both tasks (Fig. 2 (b) bottom). Achieving this goal re-quires answering two critical questions: i) how to transfer the semantic knowledge across detection and segmentation data; ii) how to bridge the gap between box and mask super-vision. First, the vocabulary shares commons but also bear substantial differences between the two tasks. We need to accommodate the two vocabularies and further go beyond towards open vocabulary. Second, semantic and panoptic
Figure 2: (a) Semantic vocabulary sizes and spatial granularities com-parison across different vision tasks/datasets. “ITP” means image-text pairs; “OD” means object detection and “SG” means segmentation. Our
OpenSeeD is the first open-vocabulary model that jointly learn on segmen-tation and detection (gray region). (b) Different types of methods connect object detection and segmentation. segmentation tasks require segmenting not only foreground objects (things like “dog” and “cat”.) but also background concepts (stuff like “sky” and “building”), while detection task solely cares about foreground objects. Third, box su-pervision by nature is coarser than mask supervision. We can convert masks into boxes but hardly vice versa.
To the end, we propose OpenSeeD, a simple encoder-decoder framework to reconcile the two tasks by mitigat-ing the aforementioned problems. Concretely, we first ex-ploit a single text encoder to encode all concepts occurring in the data and train our model to align the visual tokens with the semantics in a common space. Second, we ex-plicitly divide the object queries in the decoder into two sub-types: foreground and background queries, where the first group is responsible for foreground objects from both segmentation and detection while the second group is only for background stuffs in segmentation. Third, we introduce conditioned mask decoding which learns to decode masks from ground-truth boxes from segmentation data and gen-erates the mask assistant for detection data. As a result, our OpenSeeD is able to learn from separate detection and segmentation data seamlessly and achieves outstanding or competitive zero-shot and transfer performance across vari-ous tasks/datasets. Fig 1 shows a visualization of our model on instance, panoptic and semantic segmentation tasks. It also shows the segmentation results on datasets that largely differ from our training data such as the SeginW datasets and demonstrates the conditioned segmentation ability of
OpenSeeD. Given the encouraging results, we hope our work can contribute as the first strong baseline for devel-oping a single open-vocabulary model for both tasks.
Contributions. To summarize, our main contributions are:
• We are the first to present a strong baseline model that can jointly learn from detection and segmentation data towards an open-vocabulary model for both tasks.
• We identify the discrepancies in two tasks/datasets and propose separate techniques including shared semantic space, decoupled decoding, and conditioned mask as-sistance to mitigate the issues.
• By jointly training our model on segmentation and de-tection data, we achieve new state-of-the-art perfor-mance for zero-shot task transfer across a variety of segmentation datasets, and competitive performance for zero-shot object detection. 2.