Abstract
Synthesizing realistic videos according to a given speech is still an open challenge. Previous works have been plagued by issues such as inaccurate lip shape generation and poor image quality. The key reason is that only motions and appearances on limited facial areas (e.g., lip area) are mainly driven by the input speech. Therefore, directly learn-ing a mapping function from speech to the entire head image is prone to ambiguity, particularly when using a short video for training. We thus propose a decomposition-synthesis-composition framework named Speech to Lip (Speech2Lip) that disentangles speech-sensitive and speech-insensitive motion/appearance to facilitate effective learning from lim-ited training data, resulting in the generation of natural-looking videos. First, given a fixed head pose (i.e., canoni-cal space), we present a speech-driven implicit model for lip image generation which concentrates on learning speech-sensitive motion and appearance. Next, to model the major speech-insensitive motion (i.e., head movement), we intro-duce a geometry-aware mutual explicit mapping (GAMEM) module that establishes geometric mappings between dif-ferent head poses. This allows us to paste generated lip images at the canonical space onto head images with arbi-trary poses and synthesize talking videos with natural head movements.
In addition, a Blend-Net and a contrastive sync loss are introduced to enhance the overall synthesis performance. Quantitative and qualitative results on three benchmarks demonstrate that our model can be trained by a video of just a few minutes in length and achieve state-of-the-art performance in both visual quality and speech-visual synchronization. Code: https://github.com/CVMI-Lab/Speech2Lip.
Figure 1. Given a speech as input, our model generates high-quality talking-head videos and supports pose-controllable synthe-sis. The decomposition and synthesis modules make learning from a short video more effective and the composition module enables us to synthesize high-fidelity videos. 1.

Introduction
Learning from a short video to generate personalized audio-synchronized talking videos driven by a speech is of great importance to various applications, for instance, digital human animation, video dubbing, and UGC video creation. However, synthesizing high-fidelity videos from speech for a desired speaker remains a challenging task.
The first challenge arises from complicated motion pat-terns. Although speech majorly influences lip areas (i.e., speech-sensitive areas), lip movements are often accom-panied by other motions, such as global head movements, which greatly impact lip shapes and appearances. Thus, di-rectly synthesizing a whole image from speech often leads to inaccurate lip synthesis. Second, existing methods still struggle to satisfy appearance fidelity requirements, which include both identity preservation (speaker-specific) and high-quality detail generation, such as clear details of teeth, tongue, and eye-blinking [9, 42, 26, 44, 7]. Third, itâ€™s dif-ficult to acquire videos longer than 10 hours for a speaker which is yet required by conventional methods [17, 31].
To tackle the aforementioned challenges , existing at-tempts can be coarsely categorized into two lines of re-search: speaker-independent and speaker-specific meth-ods. The first line often exploits GANs [15] that need to be trained on large-scale multi-person datasets. However,
GAN-based models [4, 7, 9, 26, 33, 38, 42, 43] usually syn-thesize low-resolution images and unnatural motions (i.e., background movements). They thus hardly meet the ap-pearance fidelity requirement in terms of sharpness and fine appearance details. Moreover, preserving the identity of the speaker remains a challenging task [7, 43].
For the consideration of high-fidelity and identity preser-vation, another strategy focuses on learning from a spe-cific speaker. Although attaining high-fidelity results, early works [17, 31] often require several hours of video footage from a speaker for training, hindering their practical appli-cability. Recently, NeRF [20] has emerged as a promis-ing approach for generating high-fidelity videos, which suc-ceeds in learning from a short video of just a few min-utes and having the potential to generate high-fidelity re-sults [16, 18, 28]. Nevertheless, the models still struggle with appearance and motion ambiguity issues because they model speech-sensitive motions/appearances together with other facial areas less correlated to the given speech. This issue becomes more severe when training data is limited since no extra information can be exploited to avoid inter-ference from signals that are not correlated with the speech.
As a result, they tend to generate lip sequences that do not synchronize well with the speech and produce blurry im-ages (see Figure 5 and Table 1). Therefore, reducing the complexity of modeling motions is critical to enable ef-fective learning from a short video and synthesizing high-quality videos for a specific speaker.
We thus design a preliminary experiment to identify that motion and appearance of lip areas have a strong correlation with speech, while head motion and other fa-cial areas are less related to speech (Figure ??). Moti-vated by the observation, we propose decomposing speech-insensitive motion/appearance from speech-sensitive one, and synthesizing them separately before composing them into a new talking video that aligns with the given speech (Figure 1). Toward this goal, we present a decomposition-synthesis-composition framework named Speech to Lip (Speech2Lip).
In the decomposition stage, we intro-duce a speech-driven implicit model that generates high-fidelity synced lip sequences in a fixed view (i.e., canonical view). To model 3D head motion effectively, we design a Geometry-Aware Mutual Explicit Mapping (GAMEM) module that estimates explicit geometric mappings be-tween an arbitrary observed view and the canonical view.
GAMEM also includes a jointly optimized canonical-view full-head depth map, which enables the model to be 3D-aware and supports controllable synthesis driven by new head poses. In the composition stage, GAMEM allows us to flexibly paste the synthesized canonical-view lips onto an arbitrary observed view to obtain natural synchronized talking videos. To improve the synthesis and synchroniza-tion qualities after composition, we incorporate a blending network (i.e., Blend-Net) to refine the results and a con-trastive sync loss to facilitate learning from a short video for generating synchronized talking videos.
Our major contributions are summarized as follows: 1) We introduce a novel framework that disentan-gles speech-sensitive and speech-insensitive mo-tion/appearance in high-fidelity video synthesis. By separating these components, the framework can ef-fectively learn from limited training data. 2) The proposed speech-driven implicit model synthe-sizes speech-sensitive contents and the GAMEM flex-ibly combines them with given speech-insensitive ar-eas to generate synchronized talking heads with natu-ral movements and support pose-controllable synthe-sis. 3) Both qualitative and quantitative experimental results demonstrate the superiority of our method over the state-of-the-art speaker-specific methods. 2.