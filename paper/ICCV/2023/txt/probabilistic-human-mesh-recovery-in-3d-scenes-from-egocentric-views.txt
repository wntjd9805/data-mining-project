Abstract 1.

Introduction
Automatic perception of human behaviors during so-cial interactions is crucial for AR/VR applications, and an essential component is estimation of plausible 3D human pose and shape of our social partners from the egocen-tric view. One of the biggest challenges of this task is se-vere body truncation due to close social distances in ego-centric scenarios, which brings large pose ambiguities for unseen body parts. To tackle this challenge, we propose a novel scene-conditioned diffusion method to model the body pose distribution. Conditioned on the 3D scene geometry, the diffusion model generates bodies in plausible human-scene interactions, with the sampling guided by a physics-based collision score to further resolve human-scene inter-penetrations. The classifier-free training enables flexible sampling with different conditions and enhanced diversity.
A visibility-aware graph convolution model guided by per-joint visibility serves as the diffusion denoiser to incor-porate inter-joint dependencies and per-body-part control.
Extensive evaluations show that our method generates bod-ies in plausible interactions with 3D scenes, achieving both superior accuracy for visible joints and diversity for in-visible body parts. The code is available at https:// sanweiliti.github.io/egohmr/egohmr.html.
With the rapid development of Augmented and Virtual
Reality (AR/VR) devices, understanding human actions, behaviors and interactions of our social partners (“inter-actee”) from the egocentric view is a vital component for head-mounted devices (HMDs) to truly become a virtual companion for humans. The first step towards the automatic perception of human behaviors and interactions for HMDs is to estimate 3D human pose and shape (i.e. human mesh recovery) of the interactee from egocentic view images.
The research community has extensively studied human mesh recovery (HMR) from a single RGB image (usu-ally captured with third-person view cameras) by predict-ing SMPL [39, 50] parameters from global image features
[8, 24, 27, 30, 45]. However, their performance degrade significantly in egocentric images [78]. A major challenge presented in egocentric scenarios is frequent body trunca-tion [38, 78], when individuals interact with each other within close proximity, while the HMD camera has a lim-ited field-of-view. Since human poses are highly reliant on the surrounding environment, 3D scene structures can po-tentially provide strong cues to infer invisible body parts, which is crucial for precise understanding of human behav-iors: sitting on a chair and standing on the ground may in-dicate distinct intentions and future behaviors. Following
PROX [15], we assume a rough 3D scene structure is avail-able, as such information nowadays can be easily obtained with commodity sensors. Furthermore, visual localization and mapping, one of the extensively studied topics in com-puter vision, is progressing rapidly for head-mounted de-vices [52]. Therefore, we make the assumption that a coarse 3D model of the scene and the localization of the egocen-tric camera are readily available (e.g. in HoloLens2 [1]).
In this paper, we focus on the challenging problem of es-timating 3D human bodies that are heavily truncated from egocentric views due to the proximity between people and the motion of embodied cameras.
Given an egocentric image and the corresponding 3D environment, what should an ideal human mesh recovery method achieve? In contrast to previous studies that mostly pursue full-body pose accuracy, we argue the following properties are desired in the egocentric scenario: (1) nat-ural and plausible human-scene interactions; (2) accurate body pose estimations consistent with image observations for visible joints; (3) a comprehensive conditional distri-bution to generate diverse and plausible poses for unob-served body parts. Several recent studies have attempted to tackle pose ambiguities caused by occlusions and body truncations. However, some methods can only produce a discrete number of body poses [3, 34, 48], ignoring the continuous nature of the pose manifold. Other approaches model the continuous body pose distribution via conditional variational autoencoder [53] or normalizing flows [32, 68], but with limited expressiveness. Furthermore, the 3D envi-ronment is often ignored although it provides strong cues for inferring missing body parts in the image. Existing works for human pose estimation [54, 55, 79] or genera-tion [14, 80–82] in 3D scenes typically cannot deal with truncated bodies in images.
To address these issues, we introduce a novel scene-conditioned probabilistic approach, the first method to re-cover human mesh in 3D scenes from the egocentric view
Inspired by the recent diffusion models that can image. generate high fidelity images [9, 17, 18, 59] and human motions [6, 25, 41, 63, 77] with flexible conditioning, our model is trained with a conditional diffusion framework, leveraging both the classifier-free guidance [18] and the classifier-guided diffusion sampling [9, 57, 60] for efficient scene conditioning. By training the model conditioning on a human-centric scene encoding, the diffusion denoiser can generate body poses with plausible human-scene interac-tions even with highly truncated bodies (see Fig. 1). The classifier-free training enables flexible sampling from mod-els with or without image conditions, achieving accurate estimations of visible body parts while generating diverse plausible results for unseen body parts. On top of that, the diffusion sampling process is guided by the gradients of a physics-based collision score guidance in the classifier-guided sampling manner, which further resolves human-scene inter-penetrations without requiring additional time-consuming postprocessing.
To facilitate the learning of the multimodal distributions of the invisible body parts, we introduce a visibility-aware graph convolution model as the diffusion denoiser net-work, to explicitly guide the network to learn more expres-sive distribution for invisible body parts. Unlike existing methods [24, 32, 36, 68] that simply condition the full body pose on a global image feature, we condition the pose dif-fusion of each body joint on the joint visibility mask and the 3D scene feature, in addition to the image feature. With such explicit visibility guidance, the model learns to esti-mate accurate body pose for visible joints while encourag-ing diversity for truncated body parts. We adopt a graph convolution network (GCN) [26, 75, 84] to better incorpo-rate local dependencies between highly-relevant body parts (e.g. knee-foot) according to the human kinematic tree.
In summary, our contributions are: 1) a novel scene-conditioned diffusion model for probabilistic human mesh recovery in the 3D environment from egocentric images; 2) a physics-based collision score that guides the diffu-sion sampling process to further resolve human-scene inter-penetrations; 3) a visibility-aware GCN architecture that in-corporates inter-joint dependencies for the pose diffusion, and enables per-body-part control via the per-joint visibil-ity conditioning. With extensive evaluations, the proposed method demonstrates superior accuracy and diversity of generated human bodies from egocentric images, in natu-ral and plausible interactions with the 3D environment. 2.