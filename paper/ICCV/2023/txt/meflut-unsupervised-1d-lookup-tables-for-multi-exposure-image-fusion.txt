Abstract
In this paper, we introduce a new approach for high-quality multi-exposure image fusion (MEF). We show that the fusion weights of an exposure can be encoded into a 1D lookup table (LUT), which takes pixel intensity value as in-put and produces fusion weight as output. We learn one 1D
LUT for each exposure, then all the pixels from different ex-posures can query 1D LUT of that exposure independently for high-quality and efficient fusion. Specifically, to learn these 1D LUTs, we involve attention mechanism in various dimensions including frame, channel and spatial ones into the MEF task so as to bring us significant quality improve-ment over the state-of-the-art (SOTA). In addition, we col-lect a new MEF dataset consisting of 960 samples, 155 of which are manually tuned by professionals as ground-truth for evaluation. Our network is trained by this dataset in an unsupervised manner. Extensive experiments are conducted to demonstrate the effectiveness of all the newly proposed components, and results show that our approach outper-forms the SOTA in our and another representative dataset
SICE, both qualitatively and quantitatively. Moreover, our 1D LUT approach takes less than 4ms to run a 4K image on a PC GPU. Given its high quality, efficiency and robustness, our method has been shipped into millions of Android mo-biles across multiple brands world-wide. Code is available at: https://github.com/Hedlen/MEFLUT. 1.

Introduction
Dynamic ranges of natural scenes are much wider than those captured by commercial imaging products, causing they are hard to be captured by most digital photography sensors. As a result, high dynamic range (HDR) [1, 2] imag-ing techniques have attracted considerable interest due to their capability to overcome such limitations. Among HDR solutions, multi-exposure image fusion (MEF) [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15] provides a cost-effective one, with which plausible images with vivid detail can be gener-ated. MEF has attracted wide attention and there have been many methods [16, 17, 18, 19, 20, 21, 22, 23, 24] available
*Corresponding author
Figure 1. (a) Inputs. (b) Our result. (c) Input patches. We show the results by various methods in patches. Our result hallucinates more accurate details over the saturated areas. to fuse images with faithful detail and color reproduction.
However, these MEF methods use hand-crafted features or transformations so that they usually suffer from robustness if being applied to modified conditions.
Inspired by the successes of the deep neural networks (DNNs) in many computer vision areas [25, 26, 27, 28], recently some deep learning-based approaches [5, 7] have been proposed to improve MEF. DeepFuse [5] uses DNNs for the first time to directly regress the Y channel of an YUV image as the target. As the core weight maps’ learning is ignored, its quality is limited despite of its acceptable effi-ciency due to the limited model size. To improve the quality,
MEFNet [7] alternatively learns the weight maps for blend-ing the input sequence. However, its speed is downgraded as the network becomes complex, hence it just works on low resolution input as a workaround to maintain the effi-ciency. Unfortunately, these methods do not take real-world deployment into consideration, their speed and quality can-not be well balanced, causing the difficulty of their wide
application such as into mobile platform. To tackle these issues, we propose a method named MEFLUT, which aims to achieve higher quality and efficiency simultaneously by taking advantages of deep learning techniques. Our method has no strict requirements on the running platform, it can run in PC and mobile CPU and GPU. As shown in Figs. 1 and 7, our method outperforms all the other methods in the image detail preservation and running speed.
MEFLUT consists of two parts. Firstly, we design a network based on multi-dimensional attention mechanism, which is trained via unsupervised methods. The attention mechanism works in frame, channel and spatial dimen-sion separately to fuse inter-frame and intra-frame features, which brings us quality gain in detail preservation. After the network converges, we simplify the model to multiple 1D LUTs, that encodes the fusion weight for a given in-put pixel value from a specific exposure image. In the test phase, the fusion weights corresponding to different expo-sures are directly queried from the LUTs. In order to further accelerate our method, the input is downsampled to obtain the fusion masks, which are then upsampled to the original resolution for the fusion, by guided filtering for upsampling (GFU) [7] to avoid boundary stratification. We verify that with or without GFU, our method always runs faster than our competitors, revealing the key effect of LUTs learned.
Besides, considering none of the existing MEF datasets is completely collected through mobile devices, we there-fore build a high-quality multi-exposure image sequence dataset. Specifically, we spent over one month to capture and filter out 960 multi-exposure images covering a diver-sity of scenes by different brands of mobile phones. Among them, 155 samples are produced with ground-truth (GT) for the purpose of quantitative evaluation, which are produced by first running image predictions by 14 algorithms, voted by 20 volunteers and then fine-tuned by an Image Quality (IQ) expert. Producing these GT samples totally cost at least 40 man-hours without counting the organization effort.
To sum up, our main contributions include:
• We propose MEFLUT that learns 1D LUTs for the task of MEF. We show that the fusion weights can be encoded into the LUTs successfully. Once learned,
MEFLUT can be easily deployed with high efficiency, so that a 4K image runs in less than 4ms on a PC GPU.
To the best of our knowledge, this is the first time to demonstrate the benefits of LUTs for MEF.
• We propose a new network structure with two attention modules in all dimensions that outperforms the state-of-the-art in quality especially detail preservation.
• We also release a new dataset of 960 multi-exposure image sequences collected by mobile phones in vari-ous brands and from diverse scenes. 155 samples of them are produced detailed image as ground-truth tar-get by professionals manually. 2.