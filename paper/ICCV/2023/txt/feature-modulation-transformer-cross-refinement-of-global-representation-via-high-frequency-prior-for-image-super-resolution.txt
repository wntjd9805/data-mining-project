Abstract
Transformer-based methods have exhibited remarkable potential in single image super-resolution (SISR) by effec-tively extracting long-range dependencies. However, most of the current research in this area has prioritized the de-sign of transformer blocks to capture global information, while overlooking the importance of incorporating high-frequency priors, which we believe could be beneﬁcial.
In our study, we conducted a series of experiments and found that transformer structures are more adept at cap-turing low-frequency information, but have limited capacity in constructing high-frequency representations when com-pared to their convolutional counterparts. Our proposed solution, the cross-reﬁnement adaptive feature modulation transformer (CRAFT), integrates the strengths of both con-volutional and transformer structures. It comprises three key components: the high-frequency enhancement resid-ual block (HFERB) for extracting high-frequency informa-tion, the shift rectangle window attention block (SRWAB) for capturing global information, and the hybrid fusion block (HFB) for reﬁning the global representation. Our experiments on multiple datasets demonstrate that CRAFT outperforms state-of-the-art methods by up to 0.29dB while using fewer parameters. The source code will be made available at: https://github.com/AVC2-UESTC/
CRAFT-SR.git. 1.

Introduction
Single image super-resolution (SISR) has garnered sig-niﬁcant attention in recent years, owing to its promising applications across diverse domains, such as surveillance video and medical image enhancement [31, 10], old im-age reconstruction [21, 17], and efﬁcient image transmis-sion [47]. Despite its practical value, SISR remains an ill-posed problem, given the existence of multiple solutions for a given low-resolution (LR) image. To tackle this challenge,
*Corresponding author. a multitude of classical approaches have been proposed, in-cluding A+ [36], SC [41], and ANR [35]. However, these methods exhibit limitations in their performance, primarily attributed to their constrained model capacities.
In recent years, deep learning has experienced signiﬁcant growth and demonstrated remarkable success in SISR [7, 20, 45, 22]. Prior research efforts have introduced resid-ual and dense connectives to facilitate the stacking of deep convolutional neural networks (CNNs) [16, 37], while oth-ers [46, 40, 29, 30] have leveraged attention mechanisms to enhance performance. Notably, the emergence of trans-former architectures has demonstrated their efﬁcacy in cap-turing long-range dependencies and attaining state-of-the-art performance [21, 6, 4, 18, 25]. Despite these advance-ments, these works have mainly focused on designing trans-former blocks to obtain global information and overlooked the potential of incorporating high-frequency priors [32, 8] to further bolster performance in SISR. Additionally, there is limited detailed analysis of the impact of frequency on performance.
In this paper, we investigate the inﬂuence of high-frequency information on the performance of CNN and transformer structures in SISR. We achieve this by discard-ing different ratios of high-frequency components from the input image and observing the corresponding performance changes. Our empirical ﬁndings reveal that transformers tend to prioritize low-frequency information and exhibit limited capability in constructing high-frequency represen-tations when compared to CNNs. To address this issue, we proposed a cross-reﬁnement adaptive feature modulation transformer (CRAFT) that integrates the strengths of both structures. Speciﬁcally, CRAFT comprises three key com-ponents, namely the high-frequency enhancement residual block (HFERB), the shift rectangle window attention block (SRWAB), and the hybrid fusion block (HFB), which work collaboratively to capture high-frequency details, extract long-range dependencies, and reﬁne the output for better representation. Experimental results show that CRAFT out-performs state-of-the-art performance with relatively fewer parameters. The main contributions of this paper are as fol-lows:
• We study the impact of CNN and transformer struc-tures on performance from a frequency perspective and observe that transformer is more effective in capturing low-frequency information while having limited ca-pacity for constructing high-frequency representations compared to CNN.
• Based on the observation, we design a parallel struc-ture to explore different frequency features. We utilize the HFERB branch to introduce high-frequency infor-mation, which is beneﬁcial to SISR, and the SRWAB branch to acquire global information.
• We propose a fuse strategy that integrates the strengths of CNN and transformer. Speciﬁcally, we treat the
HFERB branch as high-frequency prior and the output of SRWAB as key and value for inter-attention, result-ing in improved performance.
• Extensive experimental results on multiple datasets show that the proposed method performs on par with the existing state-of-the-art SISR methods while using fewer parameters. 2.