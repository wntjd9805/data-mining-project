Abstract
In video action recognition, shortcut static features can interfere with the learning of motion features, resulting in poor out-of-distribution (OOD) generalization. The video background is clearly a source of static bias, but the video foreground, such as the clothing of the actor, can also pro-vide static bias. In this paper, we empirically verify the ex-istence of foreground static bias by creating test videos with conflicting signals from the static and moving portions of the video. To tackle this issue, we propose a simple yet ef-fective technique, StillMix, to learn robust action represen-tations. Specifically, StillMix identifies bias-inducing video frames using a 2D reference network and mixes them with videos for training, serving as effective bias suppression even when we cannot explicitly extract the source of bias within each video frame or enumerate types of bias. Fi-nally, to precisely evaluate static bias, we synthesize two new benchmarks, SCUBA for static cues in the background, and SCUFO for static cues in the foreground. With exten-sive experiments, we demonstrate that StillMix mitigates both types of static bias and improves video representa-tions for downstream applications. Code is available at https://github.com/lihaoxin05/StillMix. 1.

Introduction
Traditional computer vision techniques perform well on independent and identically distributed (IID) test data, but often lack out-of-distribution (OOD) generalization [9, 32, 12]. This is intimately tied to the learning of shortcut fea-tures [27, 16, 17], which are easy to learn and correlate strongly with IID labels but cause poor OOD generalization
[53, 62, 49, 22]. In video action recognition, shortcut fea-tures often manifest as static cues. For example, a network may classify a video as golf swinging based on its back-ground, a golf course, even if the motion patterns indicate another action such as walking. While static cues can pro-vide valuable information [74, 11, 77], they often outcom-Figure 1: Evaluation of background and foreground static bias. (a) Testing on IID HMDB51 [36] test videos. (b) Test-ing on SCUBA videos, constructed by replacing the video (c) background with a synthetic sinusoidal stripe image.
Testing on videos with conflicting foreground cues, con-structed by inserting a random static foreground into the
SCUBA video. pete motion features [23, 40, 41, 52, 69] and result in low
OOD performance [41, 63, 26]. In contrast to the rich litera-ture on mitigating background static bias (e.g., golf courses for golf swinging) [5, 63, 73, 10, 6], foreground static bias has been underexplored. Examples of foreground bias in-clude swimsuits for swimming and guitars for guitar play-ing â€” people can swim without swimsuits or show guitars in the video without playing them.
The first question we ask is if foreground static bias exists and if it is captured by the representations learned by neural networks. Our investigation technique is to cre-ate test videos with conflicting action cues from the mov-ing part and the static part of the video.
In the first step, shown in Figure 1(b), we replace the backgrounds of IID HMDB51 [36] test videos by sinusoidal stripe im-ages. These videos have no meaningful backgrounds, so the action information must come from the foreground.
Therefore, models overly reliant on background static cues
Figure 2: An illustration of StillMix. We train a 2D reference network that classifies still frames into actions to capture static bias. With the reference network, we sample frames inducing static bias to construct a biased frame bank. We mix the frames from the bank with a given video to generate an augmented video, which is used to train a 3D main network to mitigate static bias. should perform poorly. A background debiasing technique,
FAME [10], coupled with a tiny Video Swin Transformer (Swin-T) [46], works relatively well on this test.
In the second step, shown in Figure 1(c), from a single frame of a random video, we extract its foreground (mainly human actors), and insert the static foreground into all the frames of the current SCUBA video. The resultant video contains only two action features: a static foreground that indicates one action label and a moving foreground that in-dicates another action label. Predictions made using the static foreground would be wrong. This design allows the quantification of foreground static bias. More details can be found in Sec. S1 of the Supplementary Material.
The results clearly show the existence of foreground static bias and its negative effects. On the second test set, both Swin-T and Swin-T+FAME suffer similar degradation and perform 5% worse than SCUBA videos. FAME works by procedurally isolating the foreground regions from each frame and use those for training. However, it is hard to sepa-rate the foreground motion from the static foreground (e.g., clothing, equipment, or other people attributes [40]) in the training videos, since both types of features are strongly tied to the human actors.
We propose StillMix, a technique that mitigates static bias in both the background and the foreground, without the need to explicitly isolate (or even enumerate [5]) the bias-inducing content within a frame. StillMix identifies bias-inducing frames using a reference network and mixes them with training videos without affecting motion features. The process is illustrated in Figure 2. Unlike FAME, StillMix could suppress static bias anywhere in a frame, including the background and the foreground. In Figure 1, StillMix outperforms FAME and suffers only 2% accuracy drop on the second benchmark, highlighting its resilience.
Evaluating OOD action recognition is challenging as test videos with OOD foregrounds, such as swimming without swimsuits or cycling while carrying a guitar, are rare. To pinpoint the static bias in either the background or the fore-ground, we create new synthetic sets of OOD benchmarks by altering the static features in IID test videos, as illus-trated in Figure 3. Specifically, we retain the foregrounds of actions and replace the backgrounds with diverse natu-ral and synthetic images. This procedure yields a test set that quantifies representation bias toward static cues in the background (SCUBA). Second, we create videos that re-peat a single random frame from SCUBA, producing a test set that quantifies representation bias toward static cues in the foreground (SCUFO). As these videos disassociate the backgrounds from the action and contain no motion, their actions can be recognized by only static foreground fea-tures. Thus, high accuracy on SCUFO indicates strong fore-ground static bias.
With the synthetic OOD benchmarks, we extensively evaluate several mainstream action recognition methods and make the following observations. First, all examined meth-ods exhibit static bias. Second, existing debiasing methods like ActorCutMix [78] and FAME [10] demonstrate resis-tance to background static bias, but remain vulnerable to foreground static bias. In contrast, the proposed StillMix consistently boosts performance of action recognition mod-asymmetry, continuity, and causality are designed to show the static bias in video representations [18]. (3) Mutual in-formation. [33] quantifies the static bias using mutual infor-mation between representations of different types of videos.
Although these works evaluate the static bias in the whole video, they do not specify the source of static bias. In this paper, we create new benchmarks to pinpoint the source of static bias as the background and the foreground.
Bias Mitigation. Prevalent techniques of mitigating bias in action representations can be broadly classified into four categories. (1) Attribute supervision. [5] uses scene pseudo-labels and human masks to discourage models from predict-ing scenes and recognizing actions without human, but it needs extra attribute labels. (2) Re-weighting. [40, 41] iden-tify videos containing bias and downweight them in train-ing, but [65] suggests merely weight adjustment is insuffi-cient. (3) Context separation. [66] learns to separate action and contexts by collecting samples with similar contexts but (4) Data augmentation. Similar to the different actions. proposed StillMix, a few works utilize augmented videos.
BE [63] mixes a frame from a video with other frames in the same video. ActorCutMix [78], FAME [10], ObjectMix
[31] and FreqAug [30] carefully carve out the foreground (human actors or regions of motion), and replace the back-ground with other images to create augmented training data.
SSVC [73] and MCL [38] focus the models to the dynamic regions. However, these methods have not addressed static cues in the foreground.
A particular advantage of StillMix is that it does not re-quire specially designed procedures to carve out the bias-inducing pixels within the frames like ActorCutMix [78] and FAME [10], or even to enumerate the source of bias like
[5]. Rather, it automatically identifies bias-inducing frames using a reference network. Consequently, StillMix can sup-press static bias in both the background and the foreground.
StillMix is also similar to two debiasing techniques de-signed for image recognition and text classification [48, 44], which use a reference network to identify bias-inducing data instances. However, StillMix exploits the special prop-erty of videos that they can be decomposed into individ-ual frames. StillMix identifies bias-inducing components (frames) using 2D networks rather than whole data points as in [48, 44].
Action Recognition. 3D convolution or decomposed 3D convolutions [28, 58, 4, 61, 59, 42] are popular choices for action recognition. Two-stream architectures employ two modalities to classify actions, such as both RGB frames and optical flow [54, 64], or videos with two different frame rates and resolutions [14]. Multi-scale temporal convolu-tions or feature fusion are designed for fine-grained actions with strong temporal structures [75, 24, 39, 67]. Trans-former networks are proposed to capture the long-range de-pendencies [1, 2, 46]. However, our understanding of the
Figure 3: An illustration of OOD benchmark construction.
To quantify static cues in the background, we reserve the foreground actions and replace the backgrounds with other images to synthesize SCUBA videos. To quantify static cues in the foreground, we randomly select one frame in the
SCUBA video and stack it into a single-frame video with-out motion, named SCUFO videos. els and compares favorably with the other debiasing tech-niques on both background and foreground static bias. In addition, StillMix improves the performance of transfer learning and downstream weakly supervised action local-ization.
The paper makes the following contributions:
â€¢ Through quantitative experiments, we highlight the importance to address foreground static bias in learn-ing robust action representations.
â€¢ We propose StillMix, a video data augmentation tech-nique to mitigate static bias in not only the background but also the foreground.
â€¢ We create new benchmarks to quantitatively evaluate static bias of action representations and pinpoint the source of static bias (backgrounds or foregrounds).
â€¢ We compare action recognition methods on the created benchmarks to reveal their characteristics and validate the effectiveness of StillMix. 2.