Abstract
Scene Text Image Super-resolution (STISR) aims to re-cover high-resolution (HR) scene text images with visu-ally pleasant and readable text content from the given low-resolution (LR) input. Most existing works focus on recov-ering English texts, which have relatively simple charac-ter structures, while little work has been done on the more challenging Chinese texts with diverse and complex char-In this paper, we propose a real-world acter structures.
Chinese-English benchmark dataset, namely Real-CE, for the task of STISR with the emphasis on restoring struc-turally complex Chinese characters. The benchmark pro-vides 1,935/783 real-world LR-HR text image pairs (con-tains 33,789 text lines in total) for training/testing in 2× and 4× zooming modes, complemented by detailed annota-tions, including detection boxes and text transcripts. More-over, we design an edge-aware learning method, which provides structural supervision in image and feature do-mains, to effectively reconstruct the dense structures of Chi-nese characters. We conduct experiments on the proposed
Real-CE benchmark and evaluate the existing STISR mod-els with and without our edge-aware loss. The bench-mark, is available at https://github.com/mjq11302010044/Real-CE. including data and source code, 1.

Introduction
Text images are different from natural images in that the main contents are composed of words and characters to ex-press different meanings and ideas. Due to limited sen-sor resolution and long photographing distance, the cap-tured text images often have degraded quality with blurry and noisy contents, impairing the readability of the text.
Therefore, scene text image super-resolution (STISR) is de-manded to reconstruct clear and legible text contents.
STISR has long been studied in the computer vision community [30, 40, 36, 31]. The traditional STISR methods investigate various priors on text restoration and hand-craft the text super-resolution process [1, 8]. Since the manu-ally designed priors cannot represent the complex text struc-tures and degradation process, the traditional methods have limited performance. Deep learning based STISR meth-ods train convolutional neural networks (CNNs) on datasets with low-resolution (LR) and high-resolution (HR) text im-age pairs, which can learn the complex text priors through data and reconstruct high-quality text images.
In deep learning based STISR [30, 40, 36, 31], datasets play an important role in model training and evaluation, be-cause the image pairs encode the text transformation from low to high resolution. In the early stage, synthetic datasets are widely used [30, 40, 31], in which high-quality text images are collected as HR ground truths, and the LR images are generated by imposing synthetic degradations (e.g., bicubic downsampling or blurring) on the HR images.
Since the real-world degradations are quite different from the synthetic ones, the STISR models trained on the syn-thetic datasets have limited performance on real-world LR text images. To alleviate this problem, Wang et al. [35] built a real-world text image dataset called TextZoom. The LR and HR text images in TextZoom are captured with differ-ent camera focal lengths and undergo the real-world degra-dation process. TextZoom provides a benchmark for the
STISR task, which allows standardized evaluation of STISR methods in terms of text recognition precision.
Though the TextZoom dataset has largely facilitated the research of real-world STISR [35, 4, 25, 6, 46, 26, 47], it has some limitations. First, TextZoom only contains En-glish texts composed of limited number of characters (i.e., 26 letters) with simple stroke structures. As a result, mod-els trained on TextZoom will produce inferior results on structurally complex characters like Chinese. Examples are shown in Figure 1(b). One can see that the model trained on TextZoom produces visually unpleasant artifacts on the reconstructed Chinese texts. This is because Chinese texts have a much larger number of characters, and many of them have complex structures. Thus, it is a more challenging task for performing STISR on Chinese texts. Moreover,
TextZoom focuses on small and fixed-size text images (i.e., 32 × 128), and thus the models trained on TextZoom can-not generalize to texts with various resolutions. Therefore, new dataset and benchmark are highly demanded for the re-Figure 1. Comparison of STISR results on Chinese text images by methods trained on TextZoom and Real-CE datasets. From left to right are (a) bicubic LR images, STISR outputs by RRDB model [38] trained on (b) TextZoom [35] and (c) our Real-CE, and (d) the ground-truth
HR text images. Please zoom in for more details. search of STISR on Chinese text images.
To tackle the above-mentioned problems, in this work we develop a novel real-world Chinese-English benchmark dataset, termed Real-CE, for the training and evaluation of
STISR models on both Chinese and English texts. The benchmark provides 1, 935 real-world LR-HR image pairs for training, and 783 for testing (261 and 522 pairs for 4× and 2× zooming modes, respectively). It contains 24, 666
Chinese text lines and 9, 123 English text lines in total with different sizes. Detailed annotations on the image pairs, in-cluding detection boxes and text transcripts, are also pro-vided to assist the training and evaluation. We also de-sign the evaluation process to adapt to different sizes of text lines, aiming to preserve the visual quality of SR text im-ages from resizing. Furthermore, we propose an edge-aware learning method for the reconstruction of Chinese texts with complex stroke structures. The text edge map is introduced as the network input as well as a structural loss in the train-ing process, enhancing the learning on text structural re-gions. Experimental results show that models trained on our
Real-CE data achieve superior performance over TextZoom on Chinese text super-resolution (as shown in Figure 1(c)) and the edge-aware learning can further promote the recon-struction quality on text regions.
The paper is organized as follows. Section 2 reviews the works on STISR research. Section 3 introduces the Real-CE benchmark in detail. Section 4 describes the edge-aware learning method. Section 5 shows the experimental results on the benchmark and Section 6 concludes the paper. 2.