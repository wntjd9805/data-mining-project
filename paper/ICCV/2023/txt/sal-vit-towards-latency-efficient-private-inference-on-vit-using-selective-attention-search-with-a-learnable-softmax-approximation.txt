Abstract
Recently, private inference (PI) has addressed the rising concern over data and model privacy in machine learning inference as a service. However, existing PI frameworks suffer from high computational and communication over-heads due to the expensive multi-party computation (MPC) protocols, particularly for large models such as vision transformers (ViT). The majority of this overhead is due to the encrypted softmax operation in each self-attention layer. In this work, we present SAL-ViT with two novel tech-niques to boost PI efficiency on ViTs. Our first technique is a learnable PI-efficient approximation to softmax, namely, learnable 2Quad (L2Q), that introduces learnable scal-ing and shifting parameters to the prior 2Quad softmax approximation, enabling improvement in accuracy. Then, given our observation that external attention (EA) presents lower PI latency than widely-adopted self-attention (SA) at the cost of accuracy, we present a selective attention search (SAS) method to integrate the strength of EA and
SA. Specifically, for a given lightweight EA ViT, we leverage a constrained optimization procedure to selectively search and replace EA modules with SA alternatives to maximize the accuracy. Our extensive experiments show that our
SAL-ViT can averagely achieve 1.28×, 1.28×, 1.14× lower
PI latency with 1.79%, 1.41%, and 2.08% higher accu-racy compared to the existing alternatives, on CIFAR-10,
CIFAR-100, and Tiny-ImageNet, respectively. 1.

Introduction
The past few years have seen the tremendous success of transformer-based models in natural language process-ing (NLP) [31], largely because of their self-attention (SA) modules’ ability to effectively capture long-range depen-*Authors contributed equally.
This work was supported in part by National Science Foundation (NSF) under Grant No. CCF-1763747.
Figure 1. Performance comparison between prior works (Baseline,
MPCFormer [21], EA ViT [9], MPCViT [35]) and ours on CIFAR-10. Note that the MPCFormer [21] was proposed on BERT that we have adapted to ViT. dencies. Recently, vision transformers (ViTs) extended this success to computer vision tasks, including image classi-fication [6, 11], object detection [2, 22], and semantic seg-mentation [22, 37, 34], by outperforming convolutional net-work architectures due to their lower inductive bias.
The success of ViT and other deep neural network mod-els have motivated emerging machine learning inference as a service (MLaaS), where a service provider trains the model and commercializes the inference service for various tasks including performing online diagnoses and financial product recommendations [18, 24]. However, growing pri-vacy concerns have impeded such commercialization.
In particular, clients may not wish to reveal their personal data to the service provider while the service providers wish to protect the details of their proprietary trained mod-els [18]. In general, neither party wants to send sensitive unencrypted information to the other party. To mitigate these rising concerns, various private inference (PI) meth-ods [26, 13, 25, 30, 12, 29] have been proposed that lever-age techniques such as Homomorphic encryption (HE) and secure multi-party computation (MPC) protocols to pre-serve both the privacy of the client’s data and the inference model’s intellectual property (IP).
While some existing works explored efficient PI on con-volutional neural networks (CNNs) [16, 20, 3] beyond only parameter reduction [17, 19], the study of PI for trans-formers has been less explored. A direct implementa-tion of existing PI methods on ViTs incurs dramatically higher latency and communication overhead than standard inference, creating a significant roadblock in their wide-range adaptation, especially in resource-constrained appli-cations [35, 21]. The high latency can be largely attributed to the softmax function, due to its high compute demand in PI [35, 21]. Interestingly, a recent work on BERT mod-els [21] addresses this challenge by replacing the softmax with its 2nd order polynomial approximation, 2Quad [4].
Also, for ViTs, [35] formulates a neural architecture search (NAS) algorithm to substitute the softmax with either the 2ReLU [26] or the scaling function [33].
However, these softmax approximations use a fixed con-stant to re-weight the attention maps, limiting the repre-sentation and thus costing accuracy. Because the heads at different layers aim at capturing diverse relations between patches, we hypothesize that a softmax approximation may need adaptable parameters to freely re-weight the attention map. With this motivation, we present a novel softmax ap-proximation, namely, learnable 2Quad (L2Q), that has two different types of learnable parameters, shifting and scaling, enabling a fine-grained approximation of the softmax.
More specifically, we provide three granularities of L2Q (global, head-wise, and element-wise) that differ in the de-gree of sharing of the learnable parameters across various instances of the approximation.
We further observe that, independent of the softmax ap-proximation, the architecture of attention also plays a key role in both PI latency and accuracy. We compare recent attention architectures [32, 27, 7, 23, 9] and find that exter-nal attention (EA) yields the lowest PI latency due to the reduced involvement of softmax but at the cost of a sig-nificant drop in accuracy compared to an all SA ViT base-line. With this motivation, we propose to use a judicious hybrid of EA and SA modules with our L2Q approximation to achieve high accuracy while keeping the PI latency low.
In particular, given a specified SA module budget B and an initial ViT Model incorporating both EA and SA at each layer, we introduce a selective attention search (SAS). This search determines which B layers should employ EA and subsequently assigns SA to the remaining layers, all while optimizing for maximum accuracy. Thus, SAS provides a PI-friendly ViT architecture with a configurable hybrid of SA and EA, where each attention variant uses the L2Q approximation. We refer to the result as SAL-ViT, a PI-friendly ViT obtained through a selective attention search with a learnable softmax approximation.
We summarize our contributions as follows.
• We present a novel softmax alternative L2Q with fine-grained learnability that presents higher accuracy than existing softmax approximations, and a quadratic form that presents low PI latency.
• We present a detailed analysis of the various attention methods and their impact on PI latency and show that, compared to baseline SA, EA [9] presents more than 1.95× lower PI latency at the cost of lower accuracy.
• We present a selective attention search (SAS) method to yield PI-friendly hybrid ViT models with a judicious mix of SA and EA, both leveraging our proposed L2Q.
Our experimental results show that our method outper-forms the SOTA scheme MPCViT [35] by generating mod-els with averagely 2.47%, 2.82% and 4.41% higher accu-racy on CIFAR-10, CIFAR-100, and Tiny-ImageNet, re-spectively, and with averagely 1.29×, 1.30×, 1.13× lower
PI latency on CIFAR-10, CIFAR-100, and Tiny-ImageNet, respectively. From Figure 1, our method produces ViTs with 1.14× lower PI latency compared to the lowest-PI-latency technique, i.e., EA ViT [9], and with 0.79% higher accuracy compared to the highest-accuracy PI ViT, i.e.,
MPCFormer [21] on CIFAR-10. 2.