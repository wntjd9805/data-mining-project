Abstract
Scene reconstruction in the presence of high-speed mo-tion and low illumination is important in many applica-tions such as augmented and virtual reality, drone naviga-tion, and autonomous robotics. Traditional motion estima-tion techniques fail in such conditions, suffering from too much blur in the presence of high-speed motion and strong noise in low-light conditions. Single-photon cameras have recently emerged as a promising technology capable of cap-turing hundreds of thousands of photon frames per second thanks to their high speed and extreme sensitivity. Unfortu-nately, traditional computer vision techniques are not well suited for dealing with the binary-valued photon data cap-tured by these cameras because these are corrupted by ex-treme Poisson noise. Here we present a method capable of estimating extreme scene motion under challenging con-ditions, such as low light or high dynamic range, from a sequence of high-speed image frames such as those cap-tured by a single-photon camera. Our method relies on it-eratively improving a motion estimate by grouping and ag-gregating frames after-the-fact, in a stratified manner. We demonstrate the creation of high-quality panoramas under fast motion and extremely low light, and super-resolution results using a custom single-photon camera prototype. For code and supplemental material see our project webpage. 1.

Introduction
Accurate recovery of motion from a sequence of images is one of the most fundamental tasks in computer vision, with numerous applications in robotics, augmented reality, user interfaces, and autonomous navigation. When success-fully estimated, motion information can be used to locate and track the camera or different objects in the scene [7], perform motion-aware video compression [19] or stabiliza-tion [22], relate multiple sensors, merge information across different viewpoints, and even reconstruct city-scale 3D models using only images from the web [1, 34, 28].
Image sequences can be used to estimate different kinds
‡This research was supported in part by an NSF CAREER award 1943149, NSF award CNS-2107060 and NSF ECCS-2138471. of motion ranging in complexity and degrees of freedom, from global motion models, such as simple translations, projective warps, or 3D (6-DoF) camera pose, to non-rigid, local motion models such as optical flow. However, re-gardless of the motion model, traditional methods cannot recover motion that is simply too fast for the camera to cap-ture. This is especially challenging when capturing scenes in low-light conditions—the camera will compensate by in-creasing the exposure, thereby introducing motion blur, as seen in Fig. 1(a), or increasing the gain (ISO), thereby in-troducing noise [14]. Fundamentally, the image degradation associated with faster motion or a darker scene causes tra-ditional motion estimation methods to fail.
One way to handle fast motion is by using specialized high-speed cameras. However, such cameras are not only bulky and costly but also suffer from extremely low signal-to-noise ratio due to both low signal values and high readout noise, at least an order of magnitude higher than conven-tional CMOS cameras1. This requires the scenes to be well illuminated, often in a controlled setting, further limiting their scope and widespread adoption.
Fortunately, there is an emerging class of sensors called single-photon cameras, which are capable of high-speed imaging in low-light conditions. Single-photon cameras based on single-photon avalanche diode (SPAD) technol-ogy [6] provide extreme sensitivity, are cheap to manufac-ture, and are increasingly becoming commonplace, recently getting deployed in consumer devices such as iPhones. The key benefit of SPADs is that they do not suffer from read-noise, enabling captures at hundreds of thousands of frames per second even in extremely low flux, while being limited only by the fundamental photon noise.
Although single-photon cameras can capture scene in-formation at unprecedented sensitivity and speed, each in-dividual captured frame is binary valued: a pixel is “on” if at least one photon is detected during the exposure time and
“off” otherwise. This binary imaging model presents unique challenges. Traditional image registration techniques rely on feature-based matching, or direct optimization using dif-ferences between pixel intensities, both of which rely on image gradients to converge to a solution. Individual binary 1For example, the Phantom v2640 has read noise up to 58e−.
Figure 1. Single photon panoramas: (a) Our single-photon camera prototype can capture binary frames at ∼ 100, 000 frames per second.
Conventional processing techniques that average the raw image frames struggle due to extreme motion blur. (b) Our proposed method recovers a high-quality scene reconstruction by iteratively refining a motion estimate and re-aggregating the raw photon data. (c) An example high-speed panorama reconstructed from single-photon frames in merely half a second total capture time. images suffer from severe noise and quantization (only hav-ing 1-bit worth of information per pixel), and are inherently non-differentiable, making it challenging, if not impossi-ble, to apply conventional image registration and motion estimation techniques directly on binary frames. Aggregat-ing sequences of binary frames over time increases signal (Fig. 1(a)) but comes at the cost of potentially severe mo-tion blur, creating a fundamental noise-vs-blur tradeoff.
We present a technique capable of estimating rapid mo-tion from a sequence of high-speed binary frames captured using a single-photon camera. Our key insight is that these binary frames can be aggregated in post-processing in a motion-aware manner so that more signal and bit-depth are collected, while simultaneously minimizing motion blur.
As seen in Fig. 1(b), our method iteratively improves the initial motion estimate, ultimately enabling scene recon-struction under rapid motion and low light and conditions.
Scope and Capabilities: We demonstrate the recovery of global projective motion (homography), enabling the cap-ture of high-speed panoramas with super-resolution and high dynamic range capabilities. As shown in Fig. 1(c), our algorithm can reconstruct a high-quality panorama, cap-tured in less than a second over a wide field-of-view, while simultaneously super-resolving details such as text from a long distance (∼ 1300 m). The ideas presented in this paper could also be used to enhance recent one-shot local motion compensation work [27, 35, 20] as they are complementary.
Limitations: Although single-photon camera technology is rapidly evolving, today’s SPAD arrays suffer from limita-tions such as low fill-factors, low spatial resolution, and lack of high-quality color filters. This limits the visual quality of the experimental results shown here. Fortunately, given the trend towards higher resolution SPAD arrays [31] and the increasing commercial availability of this technology [32], these are not fundamental limitations. 2.