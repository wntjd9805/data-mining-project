Abstract
Neural Radiance Fields (NeRF) achieves impressive ren-dering performance by learning volumetric 3D represen-tation from several images of different views. However, it is difficult to reconstruct a sharp NeRF from blurry input as often occurred in the wild. To solve this problem, we propose a novel Event-Enhanced NeRF (E2NeRF) by uti-lizing the combination data of a bio-inspired event cam-era and a standard RGB camera. To effectively introduce event stream into the learning process of neural volumet-ric representation, we propose a blur rendering loss and an event rendering loss, which guide the network via modelling real blur process and event generation process, respectively.
Moreover, a camera pose estimation framework for real-world data is built with the guidance of event stream to
In con-generalize the method to practical applications.
*Correspondence should be addressed to Lin Zhu and Jia Li. Website: https://cvteam.buaa.edu.cn trast to previous image-based or event-based NeRF, our framework effectively utilizes the internal relationship be-tween events and images. As a result, E2NeRF not only achieves image deblurring but also achieves high-quality novel view image generation. Extensive experiments on both synthetic data and real-world data demonstrate that
E2NeRF can effectively learn a sharp NeRF from blurry images, especially in complex and low-light scenes. Our code and datasets are publicly available at https:// github.com/iCVTEAM/E2NeRF. 1.

Introduction
With the proposal of Neural Radiance Fields (NeRF)
[27], significant progress has been made in neural 3D rep-resentation and novel view synthesis tasks in the past few years. NeRF takes 3D location and 2D view direction as input and uses multi-view images of objects or scenes as supervision to learn the neural volumetric representation,
which is parameterized as a multilayer perceptron (MLP).
To generate high-fidelity novel view images, NeRF uses volume rendering techniques with the output of the network (color and density) to render each pixel.
The premise that NeRF is capable of producing impres-sive results relies on the underlying assumption that the in-put image quality is of high standards, devoid of any blurs and has sufficient lighting. However, in many real-world settings, obtaining such high-quality images can be chal-lenging. For instance, capturing a handheld shot can cause motion blurs, especially in low-light conditions, which re-quire increasing the exposure time of the camera to collect enough luminance information of the scene, consequently leading to blurred images. Deblur-NeRF [24] solves this issue and proposes the deformable sparse kernel to model image blurring. Though this method attempts to mitigate the impact of motion blurs, it could fail in scenarios where the camera shakes in roughly the same direction across all views or the blur of the image is very severe.
In contrast to only relying on blurry images, combin-ing additional information to guide neural radiance fields learning process is promising. Event camera is a new bio-inspired vision sensor, which measures the brightness changes of each pixel asynchronously. Compared to tra-ditional frame-based cameras, event cameras can record high temporal resolution and high dynamic range infor-mation of the scene, which is important for modelling the blurring process. Therefore, event-based image deblurring has become a very attractive research topic in recent years
Inspired by this, we introduce event
[16, 20, 31, 35, 41]. stream into the learning process of neural volumetric rep-resentation to solve the problem caused by blurry input.
In this paper, we propose E2NeRF to learn sharp 3D volumetric representation with blurry images and the cor-responding event data. We introduce two novel losses into NeRF framework to enhance volumetric representation learning: Firstly, during the training process, we predict a blurry image with the poses and compare it to the input im-age to obtain our blur rendering loss. Additionally, the gen-eration process of events is simulated along with the change of camera pose to simulate the event data from predicted sharp images. With the actual event data as supervision, we develop a novel event rendering loss to refine the neu-ral 3D representation learning. To process real-world data efficiently, we design a camera pose estimation framework to guide the estimation of pose sequences of the blurry im-ages, making our method robust for severe blurry images.
Due to the augmentation of the network with event data, we can learn a sharp NeRF, which not only achieves deblurring of the input image but also achieves high-quality novel view generation when the quality of the input image is degraded.
We conduct experiments on both simulated data and real data and achieve satisfactory results.
To the best of our knowledge, this is the first work to reconstruct a sharp NeRF using both event data and RGB data. The event data can effectively enhance the robustness of NeRF to complex scenes such as motion blur. Our con-tributions can be summarized as follows: 1) We propose an Event-Enhanced Neural Radiance
Fields (E2NeRF), the first framework for reconstructing a sharp NeRF from blurry images and corresponding event data. Unlike previous image-based or event-based NeRF, our framework effectively exploits the internal relationship between events and images, which significantly enhances the performance and robustness of NeRF. 2) We develop a blur rendering loss and an event render-ing loss, which are effective in enhancing neural volumet-ric representation learning. Furthermore, an event-image-based pose estimation framework that can estimate the se-quence of camera poses corresponding to a blurry image is designed for real-world data with severe blur. 3) We build both synthetic and real-world datasets for training and testing our model. Experimental results demonstrate that our method outperforms existing meth-ods. Additionally, we propose a benchmark for future re-search on NeRF reconstruction from blurry images and event stream. 2.