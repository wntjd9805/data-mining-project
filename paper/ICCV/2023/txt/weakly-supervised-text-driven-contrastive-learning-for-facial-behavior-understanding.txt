Abstract
Contrastive learning has shown promising potential for learning robust representations by utilizing unlabeled data.
However, constructing effective positive-negative pairs for contrastive learning on facial behavior datasets remains challenging. This is because such pairs inevitably en-code the subject-ID information, and the randomly con-structed pairs may push similar facial images away due to the limited number of subjects in facial behavior datasets.
To address this issue, we propose to utilize activity de-scriptions, coarse-grained information provided in some datasets, which can provide high-level semantic informa-tion about the image sequences but is often neglected in pre-vious studies. More specifically, we introduce a two-stage
Contrastive Learning with Text-Embeded framework for
Facial behavior understanding (CLEF). The first stage is a weakly-supervised contrastive learning method that learns representations from positive-negative pairs constructed us-ing coarse-grained activity information. The second stage aims to train the recognition of facial expressions or facial action units by maximizing the similarity between the im-age and the corresponding text label names. The proposed
CLEF achieves state-of-the-art performance on three in-the-lab datasets for AU recognition and three in-the-wild datasets for facial expression recognition. 1.

Introduction
Facial expression is one of the most natural signals to an-alyze human emotion and behavior. Ekman [13] has indi-cated that facial expressions of emotion are universal across human cultures and categorized them, apart from neutral ex-pression, into six categories: anger, disgust, fear, happiness, sadness, and surprise. Then contempt was added as another basic emotion, according to the work [35]. Furthermore, fa-cial expressions are coded by specific facial muscle move-ments, called Action Units (AUs) in Facial Action Coding
System (FACS) [14]. Automatic Facial Expression Recog-nition (FER) and Action Unit recognition (AUR) have been (a) Self-supervised contrastive learning pairs (b) Activity-based weakly-supervised contrastive learning pairs
Figure 1: (a) shows the self-supervised contrastive learn-ing paring, where green represents positive pairs and red represents negative pairs. In a batch, the only positive sam-ples for an anchor are its augmentations, while all others are negative. Even if the last image is similar (same person and same expression) to the anchor, it will be pushed away from the anchor as a negative sample. (b) is the illustra-tion of the proposed weakly-supervised contrastive learn-ing method: samples from the same activity in a batch are selected as positive and the remaining are negative. The textual activity descriptions are used as coarse-grained in-formation to guide contrastive learning, for example, “talk to the experimenter and listen to a joke ...” core problems in facial analysis, attracting significant inter-est in the computer vision community.
Recently, many deep learning-based approaches [29, 54, 39, 42, 56, 18, 4] have been proposed and achieved state-of-the-art performance in FER and AUR. A variety of meth-ods [54, 52, 26, 62] aimed to disentangle the expression or
AU features from various disturbing factors, such as iden-tity, ethnic background, pose, etc. Along with the develop-ment of Self-Supervised Learning (SSL), unlabeled data is utilized for learning good representations to improve recog-nition performance. Chang et al. [4] proposed a rule that divides the face into eight regions, which are then fed in a contrastive learning component. Shu et al. [43] explored three core strategies in self-supervised contrastive learning to enforce expression-specific representations and minimize interference from other facial attributes. FaRL [64] pro-posed a vision-language pre-training model with a large number of facial image-text pairs to learn facial representa-tion. To build an appropriate self-supervised learning task, fine-grained auxiliary information, such as landmarks and image captions, is typically required, which in turn requires more data processing.
On the other hand, several works have investigated the different relations between AU pairs and their applications.
SRERL [22] was developed to learn the appearance repre-sentation of the semantic relationships between AUs by a graph convolutional network. Yang et al. [56] proposed a cross-modal attention module to enhance the image repre-sentations by including AU semantic descriptions. How-ever, due to the low consistency between the data structure of image and text, attention-based integration may not fully exploit the potential of textual data. Some works also mod-eled the AUs’ relationships with the expressions to improve the FER performance. Cui et al. [10] employed a Bayesian
Network(BN) to capture the generic knowledge on relation-ships among AUs and expression. In our work, we are in-terested in learning the direct relationships between expres-sions and between AUs in a simpler way. Moreover, pre-vious studies on relationship learning have rarely explored the representation of ground truth labels, instead focusing on fitting the model with numerical labels, thus sparking our interest in investigating label representation.
In order to overcome the above limitations, it is nec-essary to investigate the following two issues: i) whether there is any coarse-grained information, which can be eas-ily obtained and simple to use without compromising the performance; ii) whether there is any approach to enrich the relationship information of the label representation.
To address the above two issues, we propose a text-driven contrastive learning method, called CLEF, to utilize both the coarse-grained information and text-embedded la-bels. The proposed method comprises two stages, both us-ing a unified vision-text architecture known as CLIP [38].
In pre-training, for each anchor in a batch, we consider pos-itive samples from the same activity and negative samples from different activities. The activity descriptions are used as coarse-grained labels to guide the weakly-supervised
Table 1: Activity description samples in BP4D. See more descriptions in the Supplementary Material.
Activity Description
Talk to the experimenter and listen to a joke (Inter-view). The target emotion is happiness or amuse-ment
Watch and listen to a recorded documentary and discuss their reactions. The target emotion is sad-ness
Experience sudden, unexpected burst of sound.
The target emotion is surprise or startle
Play a game in which they improvise a silly song.
The target emotion is embarrassment
A1
A2
A3
A4 contrastive learning model that aims to minimize the intra-activity differences in representations. Table 1 shows some samples of activity descriptions of BP4D [58]. Figure 1b shows how we leverage the activity descriptions to create positive-negative pairs. Each activity contains multiple ex-pressions, but our pairing construction can increase the pos-sibility of grouping images with the same expression into positive ones. The distance between images belonging to different activities increases, even if the images have the same identities, which encourages the encoder to focus on the activity features rather than the identity features. Mean-while, the activity text description does not contain any identity information, allowing the text encoder to avoid en-coding identity features. Cross-modal contrastive learning is therefore designed to push image features close to such textual features. Performing on these pairs can enhance the learning of better representations, which in turn improves the performance of FER or AUR in downstream tasks.
In fine-tuning, we apply vision-text contrastive learning directly to classification tasks. Supervised contrastive learn-ing adapts the image representation to be close to its cor-responding label name feature, while self-supervised con-trastive loss encourages the feature of label names and de-scriptions to be similar, enriching the semantic information of the label representation. Therefore, we believe such label representation is more powerful than the numerical label.
The recognition prediction is based on finding the most sim-ilar label names of the testing image, following the method used in CLIP [38]. The main contributions of this paper are summarized in three aspects: 1. We proposed a weakly-supervised contrastive learning method that effectively leverages coarse-grained activ-ity information. It not only requires less data process-ing but also learns better representations. 2. We explore the use of text-driven contrastive learning on FER and AUR tasks, where the performance is im-proved by incorporating textual information.
3. Extensive experiments have been conducted on 3 in-the-lab datasets and 3 in-the-wild datasets. The pro-posed method achieves state-of-the-art performance in all 6 datasets, demonstrating the effectiveness of the proposed method. 2.