Abstract
{raise the arms, squat}
{walk forward, raise the arms}
Our goal is to synthesize 3D human motions given textual in-puts describing simultaneous actions, for example ‘waving hand’ while ‘walking’ at the same time. We refer to generating such simultaneous movements as performing spatial compositions.
In contrast to temporal compositions that seek to transition from one action to another, spatial compositing requires understand-ing which body parts are involved in which action, to be able to move them simultaneously. Motivated by the observation that the correspondence between actions and body parts is encoded in powerful language models, we extract this knowledge by prompting GPT-3 with text such as “what are the body parts involved in the action <action name>?”, while also providing the parts list and few-shot examples. Given this action-part mapping, we combine body parts from two motions together and establish the first automated method to spatially compose two actions. However, training data with compositional actions is always limited by the combinatorics. Hence, we further cre-ate synthetic data with this approach, and use it to train a new state-of-the-art text-to-motion generation model, called SINC (“SImultaneous actioN Compositions for 3D human motions”).
In our experiments, we find that training with such GPT-guided synthetic data improves spatial composition generation over baselines. Our code is publicly available at sinc.is.tue.mpg.de.  {wave with the left hand,  kick with the right foot}
{squat, wave with the right hand}
{put hands on the waist,  move torso left}
{put hands on the waist,  lean forwards}
{put hands on the waist,  move torso right}
{sit down,  drink from a cup}
{sit down,  eat with both hands}
{sit down,  stretch with both arms}
Figure 1. Goal: We demonstrate the task of spatial compositions in human motion synthesis. We generate 3D motions for a pair of actions, defined by a pair of textual descriptions. Here, we provide six sample input-output illustrations from our model. For example, we input the and generate
‘put hands on the waist’, ‘move torso left’ set of actions
}
{ one motion that simultaneously performs both. 1.

Introduction
Text-conditioned 3D human motion generation has recently attracted increasing interest in the research community
[4, 15, 44], where the task is to input natural language descriptions of actions and to output motion sequences that semantically correspond to the text. Such controlled motion synthesis has a variety of applications in fields that rely on motion capture data, such as special effects, games, and virtual reality. While there have been promising results in this direction,
*Equal contribution fine-grained descriptions remain out of reach. Consider the scenario in which a movie production needs a particular motion of someone jumping down from a building. One may generate an initial motion with one description, and then gradually
‘jumping refine it until the desired motion is obtained, e.g.,
{ down’, ‘with arms behind the back’, ‘while bending the knees’
.
}
State-of-the-art methods [9, 44] often fail to produce reasonable motions when conditioned on fine-grained text describing multiple actions. In this work, we take a step towards this goal by focusing on the spatial composition of motions. In other words, we aim to generate one motion depicting multiple simultaneous actions; see Figure 1. This paves the way for
further research on fine-grained human motion generation.
Previous work [2, 13, 33, 44] initially explored the text-conditioned motion synthesis problem on the small-scale
KIT Motion-Language dataset [46]. Recently, work [4, 15] has shifted to the large-scale motion capture collection AMASS [37], and its language labels from BABEL [47] or HumanML3D [15].
In particular, similar to this work, TEACH [4] focuses on fine-grained descriptions by addressing temporal compositionality, that is, generating a sequence of actions, one after the other.
We argue that composition in time is simpler for a model to learn since the main challenge is to smoothly transition between actions. This does not necessarily require action-specific knowl-edge, and a simple interpolation method such as Slerp [51] may provide a decent solution. On the other hand, there is no such trivial solution for compositions in space, since one needs to know action-specific body parts to combine two motions. If one knows that ‘waving’ involves the hand and ‘walking’ involves the legs, then compositing the two actions can be performed by cutting and pasting the hand motion into the walking motion.
This is often done manually in the animation industry.
To automate this process, we observe that pretrained language models such as GPT-3 [7] encode knowledge about which body parts are involved in different actions. This allows us to first establish a spatial composition baseline (analogous to the Slerp baseline for temporal compositions); i.e., indepen-dently generating actions then combining with heuristics. Not surprisingly, we find that this is suboptimal. Instead, we use the synthesized compositions of actions as additional training data for a text-to-motion network. This enriched dataset enables our model, called SINC (“SImultaneous actioN Compositions for 3D human motions”), to outperform the baseline. Our
GPT-based approach is similar in spirit to work that incorporates external linguistic knowledge into visual tasks [6, 60, 64].
⇠
While BABEL [47] and HumanML3D [15] have relatively large vocabularies of actions, they contain a limited number of simultaneous actions. A single temporal segment is rarely annotated with multiple texts. For example, BABEL contains only roughly 2.5K segments with simultaneous actions, while it has 25K segments with only one action. This highlights the difficulty of obtaining compositional data at scale. Moreover, for any reasonably large set of actions, it is impractical to collect data for all possible pairwise, or greater, combinations of actions such that there exists no unseen combination at test time [62, 64].
With existing datasets, it is easy to learn spurious correlations.
For example, if waving is only ever observed by someone stand-ing, a model will learn that waving involves moving the arm with straight legs. Thus generating waving and sitting would be highly unlikely. In our work, we address this challenge by artificially creating compositional data for training using GPT-3.
By introducing more variety, our generative model is better able to understand what is essential to an action like ‘waving’.
Our method, SINC, extends the generative text-to-motion model TEMOS [44] such that it becomes robust to input text describing more than one action, thanks to our synthetic training.
We intentionally build on an existing model to focus the analysis on our proposed synthetic data. Given a mix of real single actions, real pairs of actions, and synthetic pairs of actions, we train a probabilistic text-conditioned motion generation model.
We introduce several baselines to measure sensitivity to the model design, as well as to check whether our learned motion decoder outperforms a simpler compositing technique (i.e., simply using our GPT-guided data creation approach, along with a single-action generation model). We observe limited realism when compositing different body parts together, and need to incorporate several heuristics, for example when merging motions whose body parts overlap. While such synthetic data is imperfect, it helps the model disentangle the body parts that are relevant for an action and avoid learning spurious correlations.
Moreover, since our motion decoder has also access to real motions, it learns to generate realistic motions, eliminating the realism problem of the synthetic composition baseline.
Our contributions are the following: (i) We establish a new benchmark on the problem of spatial compositions for 3D hu-man motions, compare a number of baseline models on this new problem, and introduce a new evaluation metric that is based on a motion encoder that has been trained with text supervision. (ii)
To address the data scarcity problem, we propose a GPT-guided synthetic data generation scheme by combining action-relevant body parts from two motions. (iii) We provide an extensive set of experiments on the BABEL dataset, including ablations that demonstrate the advantages of our synthetic training, as well as an analysis quantifying the ability of GPT-3 to assign part labels to actions. Our code is available for research purposes. 2.