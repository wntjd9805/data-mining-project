Abstract 3D instance segmentation has recently garnered in-creased attention. Typical deep learning methods adopt point grouping schemes followed by hand-designed geo-metric clustering.
Inspired by the success of transform-ers for various 3D tasks, newer hybrid approaches have utilized transformer decoders coupled with convolutional backbones that operate on voxelized scenes. However, due to the nature of sparse feature backbones, the extracted fea-tures provided to the transformer decoder are lacking in spatial understanding. Thus, such approaches often pre-dict spatially separate objects as single instances. To this end, we introduce a novel approach for 3D point clouds in-stance segmentation that addresses the challenge of gener-ating distinct instance masks for objects that share similar appearances but are spatially separated. Our method lever-ages spatial and semantic supervision with query refine-ment to improve the performance of hybrid 3D instance seg-mentation models. Specifically, we provide the transformer block with spatial features to facilitate differentiation be-tween similar object queries and incorporate semantic su-pervision to enhance prediction accuracy based on object class. Our proposed approach outperforms existing meth-ods on the validation sets of ScanNet V2 and ScanNet200 datasets, establishing a new state-of-the-art for this task. 1.

Introduction
In recent years, remarkable advances have been made in 3D scene understanding, owing to the rapid development of 3D sensors (Kinect, RealSense, Velodyne laser scanner, among others) and the increase in the number of large-scale datasets. Data-driven deep learning models with a focus on either point or sparse voxel approaches have been widely explored. 3D instance segmentation on point clouds is the task of simultaneously localizing and recognizing 3D ob-jects from a set of 3D points. The desired output is a set of binary masks representing the objects with their corre-sponding semantic categories. This perception task serves
Input scene
Prediction w/ our method
Semantic instance ground truth
Prediction w/ Mask3D
Figure 1. Samples predictions of our approach on scenes from the
ScanNet200 [35] dataset. Our proposed approach utilizes both se-mantic and spatial supervision to generate distinct instance labels for objects in a given scene, by processing a 3D point cloud as in-put. This enables the model to generate instance masks for objects that are similar in appearance but located in different positions, resulting in highly accurate and comprehensive labeling. as the basis for a wide variety of applications, including au-tonomous driving, mixed and virtual reality, and robot nav-igation. 2D instance segmentation is a critical computer vision task that involves identifying and distinguishing individual objects or instances within an image and assigning semantic classes to them. Unlike semantic segmentation, which as-signs a label to each pixel in an image, instance segmenta-tion aims to accurately identify each object in an image and provide a unique mask or bounding box for each one. Thus, instance segmentation lies at the intersection of object de-tection and semantic segmentation. Numerous studies have been conducted in this area, with many works focusing on top-down approaches [4, 9, 6, 15], in which instance-level proposals are generated initially to predict instance masks that are later classified into one of the recognized classes.
One popular example of these approaches is BMask R-CNN
[9], which is an extension of Mask R-CNN. It was devel-oped to address the challenges associated with segmenting
objects with complex shapes and fine details. BMask R-CNN introduces a boundary-sensitive branch to the Mask
R-CNN architecture, which predicts object boundaries in addition to object masks.
In contrast to 2D instance segmentation, bottom-up pipelines dominate 3D instance segmentation, where, gen-erally speaking, point-level semantic labels are learned and then spatially close points of the same classes are grouped together into instances. Thus, there has been extensive work on developing grouping strategies for this purpose
[21, 7, 24, 43]. The remarkable results of transformers have motivated numerous researchers to explore their us-age in the instance segmentation task. To overcome the challenge of CNNs’ insufficient long-range dependencies, hybrid-based techniques used attention mechanisms along with CNN-based backbones for feature extraction. One such method is presented in [38] for 3D instance segmenta-tion. It relies on the successive and iterative refinement of queries to learn masks and semantic labels by attending to multi-scale features obtained from a CNN backbone. This approach has been proven to achieve state-of-the-art results, owing to CNNs’ proficiency in producing features for ob-jects of varying scales and the attention mechanism’s capac-ity to capture contextual information. Nevertheless, these approaches do not allow enough information exchange be-tween the encoder and the decoder because of the structural differences between the transformer blocks and the sparse convolutional backbone.
In this paper, we propose to improve the learned features for the modules of a hybrid-based instance segmentation technique that combines a sparse convolutional backbone with a transformer decoder for query refinement. Enhanced supervision that targets the encoder specifically is proposed to achieve this aim. Given the 3D geometry of a scene, the model labels all the geometry that belongs to a single ob-ject with a unique label and assigns a class to this object.
In particular, we propose a learning technique to regress per-voxel coordinates and learn per-voxel semantic labels in the encoder. Despite the benefits of using 3D point cloud voxelization to enable regular 2D convolution on 3D point clouds, the location and geometry information of 3D objects may be lost. This arises from the fact that the decoder ex-clusively relies on the encoder features, derived solely from the RGB color of the voxel. Moreover, the process of vox-elization can compound this issue by grouping small objects into a limited number of voxels. Consequently, these aggre-gated voxels fail to completely capture the geometry of the original objects. However, the utilization of the coordinates in voxel space after the sparse quantization step, which con-sists of the X, Y , and Z values, can aid in recovering the lost information.
To this end, our contributions are as follows:
• We explore various ways of improving information ex-change between the convolutional encoder and trans-former decoder of a hybrid 3D instance segmenta-tion technique (i) spatial and semantic supervision in the 3D encoder, (ii) appending raw coordinates to 3D backbone features before feeding them to the decoder.
• We enrich the highest-resolution features used for mask prediction with existing voxel positions to assist in the prediction of higher quality and more precise masks.
• We achieve state-of-the-art performance on ScanNet
V2 [12] (+1.3 mAP50) and ScanNet200 [35] (+2.7 mAP50). 2.