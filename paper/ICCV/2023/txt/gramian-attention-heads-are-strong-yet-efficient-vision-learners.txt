Abstract
We introduce a novel architecture design that enhances ex-pressiveness by incorporating multiple head classifiers (i.e., classification heads) instead of relying on channel expan-sion or additional building blocks. Our approach employs attention-based aggregation, utilizing pairwise feature simi-larity to enhance multiple lightweight heads with minimal resource overhead. We compute the Gramian matrices to reinforce class tokens in an attention layer for each head.
This enables the heads to learn more discriminative repre-sentations, enhancing their aggregation capabilities. Fur-thermore, we propose a learning algorithm that encourages heads to complement each other by reducing correlation for aggregation. Our models eventually surpass state-of-the-art CNNs and ViTs regarding the accuracy-throughput trade-off on ImageNet-1K and deliver remarkable perfor-mance across various downstream tasks, such as COCO object instance segmentation, ADE20k semantic segmenta-tion, and fine-grained visual classification datasets. The effectiveness of our framework is substantiated by practi-cal experimental results and further underpinned by gen-eralization error bound. We release the code publicly at: https://github.com/Lab-LVM/imagenet-models. 1.

Introduction
Supervised learning opened the door to the emergence of a plethora of milestone networks [52, 23, 59, 40, 11, 39] that achieved significant success on ImageNet [48]. Training a single network with the cross-entropy loss has been a simple standard for image classification; this also holds for training multiple networks or multiple features [43, 34, 74, 55, 51, 13]. The methods of extracting multiple features at different stages aim to aggregate diversified features from an archi-tectural perspective. Previous works [43, 34, 55, 13] expand their architectures by incorporating many trainable layers to refine features, relying on the architectural perspective.
⋆Equal contribution.
†This work was done when Jongwoo Lim was professor at Hanyang
University.
Their success is likely attributed to extra heavy layers that promote feature diversification. However, it remains uncer-tain whether the architectures effectively promote learning favorable less-correlated representations [53, 7, 50, 29, 28].
Additionally, their intentional design for high network ca-pacity with numerous trainable parameters increases compu-tational demands.
In this paper, we present a new design concept of deep neural networks that learns multiple less-correlated features at the same time. Since motivated by feature aggregation methods [43, 34, 74], we aim to avoid excessively over-parameterized networks and realize performance improve-ment through the learning of multiple less-correlated fea-tures. Our architecture consists of multiple shallow head classifiers on top of the backbone instead of increasing depth or width and without employing complicated decoder-like architectures. Therefore, it is evident that our architecture offers a speed advantage, but the potentially limited expres-siveness with lightweight heads is problematic. A question that naturally arises is how can we improve the network ca-pacity of shallow heads with limited trainable parameters?
Our answer centers on the idea of introducing the
Gramian matrix [17, 73] combined with the attention mod-ule [65]. The Gramian is identical to the bilinear pooling [36] that collects the feature correlations so that the attention can further leverage the information of the Gramian of fea-tures. Specifically, we compute the Gramian matrices of each output of heads before the final predictions and feed them into each attention as the query, which brings the pair-wise similarity of features. This design principle is naturally scalable to any backbones, including Convolutional Neural
Networks (CNNs) [23, 26, 59, 45, 40], Vision Transformers (ViTs) [11, 39], and hybrid architectures [9, 20, 75, 63].
We further introduce a learning algorithm that forces each head to learn different and less correlated representa-tions. The algorithm is based on the proposed decorrelation loss, which performs like an inverse knowledge distillation loss [25]. Our proposed framework compels lightweight heads to learn distinguished and enhanced representations.
It turns out that our trained models can replace complicated ones through empirical evaluations and effectively be gener-alized to other downstream tasks. 2.1. Motivation
We evaluate our models by training them on the CI-FAR100 [32] and ImageNet-1K [48] datasets to showcase the effectiveness of our proposed method. We systemati-cally compare the competing models with similar compu-tational budgets, including the throughput of a model; ours beat the state-of-the-art CNNs, ViTs, and hybrid architec-tures in the accuracy and throughput trade-off. We also show the transferability of our pretrained models to downstream tasks, including instance segmentation and semantic seg-mentation on COCO [35] and ADE20k [83], respectively, and fine-grained visual classification datasets1, including
CUB-200 [67], Food-101 [3], Stanford Cars [31], FGVC
Aircraft [42], and Oxford Flowers-102 [44].
Finally, we analyze the efficacy of our design elements, including hyper-parameters via Strength and Correlation the-ory [50, 4]. The theory manifests as Correlation is lowered while Strength increases, and classifiers learn a highly gen-eralizable representation. While Correlation and Strength are usually proportional, our framework has elements low-ering Correlation while increasing Strength, like evidence in Ryu et al. [50]. Therefore, this justifies that the ingredi-ents are well-proposed to leverage low generalization error.
We further support the theory through the analyses with the elements to showcase low validation errors in practice. We provide the following summary of our contributions: (i) We introduce a new network design principle to inten-sify a backbone by incorporating multiple lightweight heads instead of using a complicated head or expanding model in width and depth directions. (ii) We introduce a novel attention module that employs the Gramian of the penultimate features as a class to-ken within an attention layer, thereby strengthening lightweight classifiers based on pairwise feature sim-ilarity. We call Gramian attention, which enhances expressiveness without compromising model speed. (iii) We further propose a learning algorithm with a new loss that enforces multiple heads to yield less-correlated features to each other. Intriguingly, our learning method shows a faster convergence and yields strong precisions. (iv) We provide an analysis tool for diagnosing design ele-ments of a network and training methods to reveal the effectiveness of the proposed method based on Correla-tion and Strength with the generalization bound. 2. Method
This section first outlines the motivation for this work. We then present our network architecture and learning algorithm for less-correlated features. 1Experimental results of the fine-grained visual recognition tasks are found in Appendix.
Class tokens for class prediction. Learning class to-kens [11, 62, 72, 49] have gained popularity because of their effectiveness and simplicity in training ViTs. The class tokens are fed right after the patchification layer for long interactions with features following the original de-sign choice [65]. Additionally, it has been revealed that having a shorter interaction on only later layers improves performance [62]. Longer interactions may harm the dis-criminability of the class token due to the low-level features, while short interactions can effectively capture high-level information in the later class tokens. We adopt a short-interaction-like design for our network.
Second, using multiple class tokens [72, 49] has con-tributed to enhancing the interactions’ discriminability. They passively let the class token be learned upon a random initial-ization rather than actively using the class token. We notice that no studies have been conducted to strengthen the class token itself. We argue that the way of utilizing class tokens in previous literature might not fully exploit the maximum capability of the learned model. We thus further imbue the class token by computing the Gramian from the feature to assign it as the class token.
Employing multiple heads. Previous works [56, 74, 55, 51, 13, 38, 60, 49] guide us that aggregating multiple fea-tures give significant benefits over single-path models such as ResNet [23]. Motivated by the success, we design our network learning multiple heads on the top of the backbone, barely spending a high computational budget; each head takes advantage of the aforementioned design manner. The design manner is also supported by the literature [4, 50], which tells us that weak learners (i.e., classifiers) should be strongly trained individually while diversifying the learned features for generalization.
Furthermore, to learn stronger heads, we focus on the underexplored correlation among learned features [34, 60, 13, 33, 51]. We propose a so-called less-correlated learn-ing method to maximize feature diversity. In this light, we believe designing a network that branches lightweight head classifiers instead of a complicated network is an appropri-ate option for making a good combination of the proposed learning method and architecture. 2.2. Our Network Architecture
Gramian attention. We propose an attention-based mod-ule, dubbed Gram Attention, for aggregating visual tokens of a network more effectively. The primitive Transformer architecture with n-layers [65, 11] uses the C-dimensional class token Z ∈ RC to formalize the network output Y as: Y = f n (cid:0). . . f 1([Z; X])(cid:1), where [Z; X] denotes the concatenation of the N visual tokens X ∈ RN ×C and
Z; f 1 and f n stand for the patch extractor and final clas-sifier. This formulation indicates that an early concate-(a) ViT [11] (b) CaiT [62] (c) Ours
Figure 1: Contrast illustration of different heads. We concep-tually compare our head classifier design with class token-based classifier heads [11, 62]. Ours employs Gramian with an attention layer to enhance the capability of the class token. nation of class tokens with the input-side features subse-quently updates class tokens through long interactions with visual tokens (see Figure 1a). Figure 1b shows a varia-tion [62, 49] using the class token at later layers from an intermediate m-th layer, the output of which is formulated as Y = f n(. . . f m+1 (cid:0)[Z; f m (cid:0)...f 1 (X)(cid:1)](cid:1). Both of them are trained passively starting from the randomly initialized weights and may reach sub-optimal convergence points be-cause optimizing both X and Z may not guarantee optimum at the same time. Unlikely, we alternatively assign the class token using features from a network.
Figure 1c illustrates our class token assigned by the
Gramian computed with the penultimate features. This con-trasts with the previous methods, where class tokens are initialized randomly and updated indirectly via feature inter-actions. Our aim is to directly influence on entire trainable parameters in a network, so we compute the Gramian of the last-computed feature X n−1 ∈ RN ×HW ×C at the penulti-mate layer f n−1 as: (1)
Y = f n (cid:0)[X n−1; GX n−1 ](cid:1) , where GX denotes the Gramian matrix of X (i.e., an instance-wise computation GX = X T X ∈ RN ×C×C). We attribute the expressiveness of the Gramian to its computation of pairwise similarity. In practice, we compute G with the pro-jected feature VX = XWc, where Wc ∈ RC× ˜C. This is because the Gramian computation here has the complexity of O(HW C 2), and it becomes more computationally de-manding with a large C, so we reduce it by Wc to ˜C ≪ C for efficiency. Reducing the inner dimension also improves efficiency, but it could harm the encoded localization in-formation. We introduce a Gramian computation with the vectorized feature to compute pairwise similarity across all locations by the following formula:
GX = Vec(V T
X VX )Wg, (2) where Vec(·) denotes the instance-wise vectorization, and
Wg ∈ R ˜C2×C stands for another projection layer that re-stores the dimensionality to C, serving it as a class token for the subsequent attention layer.
Head classifier. Following Eq. (1), G in Eq. (2) is fed into f n after concatenated with the input feature. We employ the attention [65] as the final layer f n. We refer to this layer which computes the class embedding Y as the head classifier.
Note that the computed Gramian becomes the query, which is similar to [62]. Despite the shallow architecture, it has a large capacity standalone by the pairwise similarity com-puted by the Gramian. This associating operation is identical to the bilinear pooling [36], which has been revealed as learn-ing strong spatial representation [14, 54]. This operation is known to capture delicate spatial information across channel combinations, so it has been shown to improve the discrim-inative power of the object classification [6, 15, 16]. We leverage the expressiveness of the bilinear representation for the class tokens possessing a strong spatial representation.
Extending to multi-head architectures. Constructing mul-tiple branches on top of the backbone is a simple way to build multi-head classifiers. We do not rely simply on the final feature but instead, take the aggregated features from a backbone for the head classifiers. This is to take advantage of using diverse multi-level features similar to feature aggre-gation networks [34, 74]. Since we re-encode the aggregated features using lightweight heads, the multiple heads barely involve extra computational budgets. Therefore, our multi-head architecture can be regarded as an efficient alternative to heavy head architectures [55, 34, 38, 60, 12] or the way building complicated architectures [82, 59, 78]. 2.3. Training Multi-head Classifiers
On less-correlated multi-head classifiers. Here we intro-duce a novel less-correlated learning method to learn more expressive multi-head classifiers. Training multiple identi-cal network architectures or branches without considering feature diversity may not yield advantages. Since the mod-els are expected to converge to nearby local minima during training, the resulting models are likely to learn correlated representations [53, 7, 50, 29, 28] (see Figure 2a). We begin with the model averaging loss (i.e., equally weighting the outputs) with the i-th output of h heads as:
L = h (cid:88) i
CEi = − h (cid:88) i yT · log f n i (x), (3) where CEi denotes the cross-entropy loss with the ground-truth label y, and fi(x) denotes the output of i-th head for the input x. For simplicity, we abbreviate f n (i.e., n-th layer’s output) in previous notations to f .
Directly minimizing Eq. (3), the correlation among the predictions fi is likely to be high, so we propose a new
(a) Traditional learning method (b) Our learning method
Figure 2: Schematic illustrations. We compare our learning method with a traditional one. (a) Multiple features extracted from different models(or a complicated head) are trained under the task loss only, so they are likely to get close to the ground-truth labels as training progresses; (b) our method trains lightweight heads by ensuring the representations are not highly correlated with each other. decorrelation loss to avoid it:
L = − h (cid:88) i yT · log fi(ˆx) + λLdec, s.t. Ldec = h (cid:88) (cid:88) i j fj(ˆx)T n (cid:32)
· log fk(ˆx) n (cid:88) k (4) (cid:33)
− log fi(ˆx)
, where Ldec is coined by the decorrelation loss2 (see Fig-ure 2b), and λ is a tunable weighting parameter. Note that we use only negative λ in Eq. (4) to ensure the decorrelation loss functions in opposition to the cross-entropy loss.
Connection to knowledge distillation. One may speculate the proposed loss relates to Kullback Leibler Divergence used in the knowledge distillations [25, 47]. The canoni-cal knowledge distillation methods use a positive value for
λ; unlikely, our approach assigns a negative λ in Eq. (4), which lets the knowledge from the aggregated prediction be reversely transferred. Therefore, each prediction fi would deviate and be less correlated (see Figure 2b). We argue that training with knowledge distillation (i.e., using the KD loss [25]) may fail to let each head learn without high cor-relation. This result is obvious that a positive λ makes the distance between the aggregated prediction and each predic-tion get closer, so the predictions get similar, as shown in
Figure 2a. Our claim is addressed in the later discussion section providing both qualitative and quantitive results (see the visualization in Figure 6e and compare it with Figure 6d. 3. Experiment
This section begins with the empirical analyses of the components of our method. We then demonstrate the supe-riority of our models through ImageNet classifications and transfer them to downstream tasks. We coin a network using our Gramian attention-included heads as GA-network.
C 1 1 1 2 2 8 8 dim
Gram
Dec
FLOPs (G)
#Params (M)
Top-1 err (%) 32 32 64 64 128 128 128
-✓
✓
✓
✓
✓
✓
------✓ 0.26 0.26 0.35 0.26 0.35 0.22 0.22 2.5 2.6 4.0 2.6 4.1 2.0 2.0 21.8 21.2 19.8 20.1 18.4 18.9 16.9
Table 1: Factor analysis. The cardinality (C) and the reduced input channel (dim) of the head classifiers are studied. We mainly verify the impact of the proposed Gramian attention (Gram) and decorre-lation loss (Dec). We experiment with ResNet110 on CIFAR100.
A careful design significantly improves accuracy without added computational costs.
Net
Head
#heads
λ
#Params (M)
Top-1 acc (%) 0 5 t e
N s e
R
S
-T
V i
GAP-FC
CaiT
Gram
Gram
Gram
GAP-FC
ViT
CaiT
Gram
Gram
Gram 1 / 10 1 / 5 1 / 5 1 / 5 1 / 5 1 / 20 1 / 20 1 / 5 1 / 5 1 / 5 1 / 5
--0
-0.4
-0.8
---0
-0.4
-0.8 25.9 / 44.0 21.8 / 38.5 22.4 / 41.3 22.4 / 41.3 22.4 / 41.3 22.1 / 29.4 22.1 / 29.4 22.8 / 27.3 22.9 / 27.7 22.9 / 27.7 22.9 / 27.7 75.3 / 75.7 76.7 / 77.0 78.0 / 79.1 77.9 / 79.2 76.3 / 79.3 76.3 / 76.3 75.3 / 75.4 75.2 / 75.3 78.3 / 78.4 78.3 / 78.5 78.2 / 78.9
Table 2: Extended factor analysis. We extend the analysis to
ImageNet-1K, building upon learned insights from Tab. 1. We study the impact of head types (Head), the number of heads (#heads), and
λ in the decorrelation loss. We include the global average pooling with a fully-connected layer (GAP-FC), ViT, CaiT, and ours (Gram) shown in Fig. 1. We report the accuracy of both single and multiple heads adjusted to have similar parameters (single/multiple heads). 3.1. Preliminary Factor Analyses 2We use the term decorrelation here in the idiomatic context of reducing the relevance and correlation of output predictions.
First, we study how each design element of the proposed method works on the CIFAR dataset. Table 1 shows that
(a) ResNet-50 (b) ConvNext-T (c) DeiT-S
Figure 3: Empirical study on #heads and λ. We examine Ima-geNet accuracy versus the number of head classifiers across dif-ferent λ for the decorrelation loss. Single-head underperforms, whereas using more heads increases the performance across all backbones. Lower values of λ are more compatible with multiple heads; the best, with λ= − 0.8, is achieved with five heads.
Network
FLOPs (G)
#Params (M)
Throughput (img/sec)
Top-1 acc (%)
RSB-ResNet50 [68]
GA-ResNet50
RSB-ResNet152 [68]
ViT-S [11]
GA-ViT-S
GA-ViT-M
ViT-B [11]
ConvNeXt-T [40]
GA-ConvNeXt-T
ConvNeXt-S [40]
GA-ConvNeXt-S
ConvNeXt-B [40]
GA-ConvNeXt-B
ConvNeXt-L [40] 4.1 5.2 11.6 4.2 4.3 9.6 16.9 4.5 6.3 8.7 10.5 15.4 19.0 34.4 25.6 41.3 60.2 22.1 27.7 60.5 86.6 28.6 48.7 50.2 70.4 88.6 124.3 197.8 3409 2145 1463 2556 2289 1322 987 2098 1452 1282 967 903 668 507 79.8 82.5 81.8 79.8 80.9 82.6 81.8 82.1 83.2 83.1 83.9 83.8 84.3 84.3
Table 3: Our ImageNet-1K models. We apply our method to the popular architectures, including ResNet [23], ConvNeXt [40], and ViT [11, 61]; we dub our models GA-ResNet, GA-ConvNeXt, and GA-ViT, respectively. All our models improve the baselines by large margins and enjoy faster speeds than each counterpart having similar accuracy. using our proposed Gramian attention (Gram) and learning method with the decorrelation loss (Dec) boosts the accuracy significantly. The results also display a head classifier can be strengthened by increasing the aggregated dimension (dim) and cardinality (C) under similar computational demands.
Extending the analysis to the ImageNet-1K dataset, we in-vestigate the effectiveness of our multiple head architectures and the proposed learning method in Table 2. All experi-ments are performed with identical network configurations to the baselines models (ResNet50, ViT-S), such as the stage configuration and channel dimension. We report accura-cies training ResNet50s and ViTs for 50 and 100 epochs, respectively.
As shown in Table 2, we confirm the models with pro-posed multiple heads significantly outperform baseline net-works trained with the naive global average pooling (GAP-Network
RSB-ResNet50 [68]
RSB-ResNet152 [68]
ResNetY-8G [45]
ViT-S [11, 61]
Swin-S [39]
PoolFormer-M36 [75]
CoatNet-0 [9]
CSwin-T [10]
ConvNeXt-S [40]
GA-ResNet50
GA-ConvNeXt-T
ResNetY-16G [45]
ViT-B [11, 61]
Swin-B [39]
PoolFormer-M48 [75]
CoatNet-1 [9]
InceptionNeXt-S [76]
CSwin-S [10]
MaxViT-T [63]
SLaK-S [37]
ConvNeXt-B [40]
GA-CSwin-T
GA-ConvNeXt-S
ResNetY-32G [45]
InceptionNeXt-B [76]
SLaK-B [37]
CoatNet-2 [9]
CSwin-B [10]
ConvNeXt-L [40]
MaxViT-S [63]
CoatNet-3 [9]
GA-ConvNeXt-B
GA-ConvNeXt-B†
GA-CSwin-S
FLOPs (G)
#Params (M)
Throughput (img/sec)
Top-1 acc (%) 4.1 11.6 8.0 4.2 8.5 8.8 4.2 4.3 8.7 5.2 6.3 15.9 16.9 15.1 11.6 7.6 8.4 6.9 5.6 9.8 15.4 6.1 10.5 32.3 14.9 17.1 14.5 15.0 34.4 11.7 32.5 19.0 26.1 8.7 25.6 60.2 39.2 22.1 49.6 56.2 27.4 23.0 50.2 41.3 48.7 83.6 86.6 87.8 73.5 41.7 49 35.0 30.9 55 88.6 42.0 70.4 145.1 87 95 73.9 78.0 197.8 68.9 165.2 124.3 124.3 54.3 3409 1463 827 2556 1024 796 1781 1498 1282 2145 1452 632 987 731 601 985
-933 976
-903 1001 967 378
--629 549 507 636 360 668 524 671 79.8 81.8 82.1 79.8 83.0 82.1 81.6 82.7 83.1 82.5 83.2 82.2 81.8 83.5 82.5 83.3 83.5 83.6 83.6 83.8 83.8 84.1 83.9 82.4 84.0 84.0 84.1 84.2 84.3 84.5 84.5 84.3 84.5 84.7
Table 4: ImageNet-1K results. Our models are compared with the state-of-the-art networks, including CNN, Transformer, and hybrid architectures on ImageNet-1K. We group the networks according to the computational budgets. All accuracies are borrowed from the original paper; RegNet accuracy is taken from [68]. We report the throughputs measured by ourselves, running on an RTX 3090 GPU.
Our networks perform well over competitors with manageable resources and faster speed. We also provide the memory usage in the supplementary material. † uses 272 × 272 image size. GA extremely improves CSwin family; we presume the lower channel dimension of CSwin architectures is an underlying reason.
FC). Our Gramian attention remarkably outperforms existing
ViT- and CaiT-like class token methods again. The pro-posed learning method with decorrelation loss (Dec) also contributes to performance, and this contribution is more significant with multiple heads and lowered λ across all ar-chitectures. Figure 3 gives more information on the accuracy variation of the models with multiple heads concerning λ in the decorrelation loss. It verifies that the decorrelation loss can diversify learned features so that a higher λ (λ= − 0.8) performs better than other lower λ cases (λ=0 and λ=−0.4). 3.2. ImageNet Classification
Implementation details. We employ ResNet [23], Con-vNeXt [40], CSwin [10], and ViT [61] as our baseline net-works, with each backbone branching out five heads. For
ResNet50, we build our GA-network with some popular tweaks; we reduce the channel dimension of the last three residual blocks to 1024 and exploit SE [27], and design tweaks introduced in the previous work [24]. For ConvNeXt and ViT, we use the original architectures. For ViT, we en-compass ViT-M having an intermediate model size between
ViT-S and ViT-B, which has 576 channels with nine attention heads. For ConvNeXt and CSwin, due to the lower channel dimension compared to ResNet, we utilize a larger feature scale with minimal overhead. Note that we do not delve into investigating more compatible backbones for our method architecturally. Instead, our focus is to showcase the effec-tiveness of our method through performance improvements on popular and straightforward network architectures under minimal resources.
Comparison with state-of-the-arts. We compare the per-formance of GA-networks with the contemporary state-of-the-art network architectures regarding the accuracy and computational complexities. GA-networks competes with the recently proposed network architectures, including the
CNN architectures of RSB-ResNet [68], RegNet [45], Con-vNeXt [40], and SLaK [37]; the ViT [11]-related architec-tures, including ViT [61], Swin Transformer [39], and CSwin
Transformer [10]; the hybrid architectures PoolFormer [75],
CoatNet [9], MaxViT [63], and InceptionNeXt [76]. We systematically compare GA-networks, including scaled-up models shown in Table 3 with the competing models grouped by computational budgets, mainly focusing on throughput.
Furthermore, we perform comprehensive comparisons with the popular contemporary models in Table 4, and it shows our models have clear advantages in throughput over their counterparts and outperform the competing networks, in-cluding the state-of-the-art CNN, ViT, and hybrid models. 3.3. Downstream tasks
We investigate the applicability of the proposed method to two downstream tasks, including instance segmentation and semantic segmentation. Compared with the previous state-of-the-art models, we train pretrained GA-networks on ImageNet-1k in Table 4. Following the setups in liter-ature [39, 75, 49], we attach detection and segmentation networks to ours. As in the literature [34, 49], where the rear layers of the network are connected to the frontal layers, we
Network
AP (box)
AP (mask)
#Params (M)
RegNetX-12G
Swin-T
Poolformer-S36
GA-R50
X101-64
Swin-S
ConvNeXt-S
GA-ConvNeXt-S 42.2 42.7 41.0 42.8 48.3 51.9 51.9 52.3 38.0 39.3 37.7 39.3 41.7 45.0 45.0 45.3 64.1 47.8 31.6 42.5 140 107 108 108
Table 5: COCO instance segmentation results. Our models
ResNet50 (R50) and ConvNeXt-S outperform competing back-bones using identical segmentation heads, respectively. attach dense prediction layers on our backbones. We train our model with the widely-used MMDetection and MM-Segmentation libraries3, and we report the performance of previous methods from the same training epochs or itera-tions.
Object instance segmentation. We train the object instance segmentation model on COCO 2017 [35]. We exploit Mask
R-CNN [22] for ResNet-50 and Cascade Mask R-CNN [5] for ConvNeXt-S as the baseline model. As shown in Table 5, ours outperform the models based on RegNet [45], Swin
Transformer [39], and PoolFormer [75].
Semeantic segmentation. We train our models on the
ADE20k semantic segmentation [83]. We employ two widely used heads: FPN [34] and UperNet [70] for the seg-mentation head in our model. As shown in Table 6, our networks exhibit competitive performance relative to mod-els employing PoolFormer [75] and Swin Transformer [39] using each head. 3.4. Training Setups
ImageNet-1K. Recent state-of-the-art networks [80, 46, 68, 1, 21, 61] exploit training regimes with strong data augmenta-tions, mostly based on timm library4 [69]. We adopt a similar training regime, which employs Mixup [79], CutMix [77], and RandAugment [8] for data augmentation and use the cosine learning rate scheduling [41] with 300 epochs5.
Downstream task. For fair comparisons, we follow the same training setup of the competing backbones. We exploit 1× training schedule with 12 epochs in COCO. On ADE20k, we follow the same training setup of competitors again to train our segmentation model with iterations of 40k. We use 32 batch size for the 40k-iterations setup to compare ours with PoolFormer [75] and use the 120k-iterations setup in 3https://github.com/open-mmlab 4https://github.com/rwightman/pytorch-image-models/ 5In our ablation study in Table 2, we primarily train networks with
ResNet-based and ConvNeXt-based models for 50 epochs and exceptionally train ViT-based models for 100 epochs due to its late convergence.
Head
Network
Iter. mIOU #Params (M)
FPN
PoolFormer-S36
GA-R50
UperNet
Swin-T
GA-R50 40k 40k 160k 160k 41.6 41.8 44.4 45.2 34.6 26.6 59.9 67.3
Table 6: ADE20k semantic segmentation results. Our models out-perform competing backbones with identical segmentation heads.
Figure 4: Generalization error bound. We visualize Correlation (ρ), Strength (s), and the upper bound of the generalization error (˜γ). We plot the metrics versus the architectural elements and different λ values in the decorrelation loss. The left tick of the
Gramian attention on the x-axis shows that architecture elements contribute to lowering the generalization error bound, and λ in our less-correlated feature learning drops the bound on the right side.
Swin Transformer [39] with 16 batch size.
CIFAR100. We follow the standard 300-epochs training protocol with SGD [19, 77] with the initial learning rate of 1e−3 decaying by 0.1 at 150 and 225 epochs. We use 64 batch size for training using two GPUs. 4. Discussions
In this section, we investigate our method through the generalization bound analysis and the visualization method. 4.1. Analyzing Our Method
Here we justify our proposed design principle and learn-ing method based on the foundation theory [4, 50] that in-vestigates the generalization capability of a model with mul-tiple classifiers like ours. The theory is to compute the de-gree of Strength and Correlation for the generalization error bound [4, 50], and the magnitude of the metrics indicates how well the model generalizes [50].
Strength and Correlation. Strength s is firstly defined as the expectation of the margin between model prediction and the ground truth labels. The margin function is formulated as f (Yϕ, ˆY ) = P (Yϕ = ˆY ) − maxj̸= ˆY P (Yϕ = j), where
Yϕ and ˆY denote the output labels of a head classifier ϕ and the ground-truth labels of the data points, respectively. The (a) Error vs. λ (b) Error vs. #heads
Figure 5: Validation error trend w.r.t λ and #heads. (a) top-1 error versus λ in the decorrelation loss; (b) top-1 error versus the number of heads. We observe that training with multiple heads with λ < 0 significantly reduce the top-1 error. last term maxj̸= ˆY P (Yϕ = j) stands for a set of labels with the largest probability amongst wrong answers.
Correlation ρ is computed with the raw margin function
ψ, which is defined as ψ(Yϕ, ˆY ) = I(Yϕ = ˆY ) − I(Yϕ = maxj̸= ˆY P (Yϕ = j)), where I(·) is the indicator function.
ρ is then computed by averaging the Pearson Correlation coefficient of ψ between all combinations of heads (ϕi, ϕj).
Generalization error bound. The upper bound of general-ization error ˜γ is compute from Strength S and Correlation
ρ, which is γ ≤ ρ(1 − s2)/s2. This implies Correlation and Strength are opposite to each other to achieve a low generalization error; however, importantly, the previous lit-erature [50] showed there could exist a method that trains a model to decrease correlation while increasing Strength.
Based on the evidence, we conjecture that an appropriate design of the head may also achieve it again. We confirm this by measuring the metric – the generalization error bound – to be reduced for particular architectural or training-related elements. Figure 4 shows that the proposed architectural design elements and learning method significantly reduce the upper bound of the generalization error.
We further visualize Correlation and Strength metrics to-gether and observe Correlation gets consistently lowered as appending the architectural elements and adjusting the de-gree of the correlation (adjusted by λ) in our learning method.
This result indicates that our network architecture with multi-ple heads trained with our proposed learning method pushes the model to learn less-correlated and diversified features to contribute to the model’s generalization capability. We fur-ther claim that the generalization bound is actually connected to performance in practice. We train models and visualize their validation errors in Figure 5a and Figure 5b. Along with Table 2 reporting the error decreases as architecture advances, the figures show a consistent trend with Figure 4.
(a) λ = 0 (b) λ = −0.3 (c) λ = −0.5 (d) λ = −0.8 (e) λ = 0.7
Figure 6: t-SNE plots of the features extracted from learned head classifers. We visualize how much the proposed learning method scatters the output features of each head. We extract the features from the images in the validation set and distinguish them from different head classifiers by color. We use features of a ResNet110 for (a) to (e). Specifically, (a) eight head classifiers without the decorrelation loss (λ = 0); (b), (c), (d), and (e) different weighting parameters λ, respectively; We observe that 1) accuracy is aligned with the feature correlation; 2) our proposed learning method (i.e. λ < 0) works to increase the feature diversity with lowered correlation; 3) learned features with (λ > 0) do not guarantee both low correlation and error (see (e).) 4.2. Visualizing Learned Features
We investigate the impact of the decorrelation loss in
Eq. (4) with different λ by visualizing the output features with t-SNE [64]. Figure 6 shows the clear trend when using
λ < 0; larger (to the negative direction) λ let the model learn less-correlated features; the performance follows the trend.
All with negative λ outperforms the case of λ = 0 that does not use the decorrelation loss.
The performance with different λ gets clearer with Fig-ure 5a, we achieve the best performance when the λ is near
-0.8, and when λ > 0 the performance gets poorer than the model with λ = 0. Additionally, a comprehensive visual-ization both with the number of heads and different λ in
Figure 5b reveals some interesting aspects. We observe that when λ reaches -0.7, the performance improves significantly as the number of heads increases. Performance gets saturated trained only with three heads when λ ≥ 0, while negativeλ lets the model avoid saturation. 5.