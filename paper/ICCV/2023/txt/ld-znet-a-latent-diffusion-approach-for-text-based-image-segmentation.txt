Abstract
Large-scale pre-training tasks like image classiﬁcation, captioning, or self-supervised techniques do not incentivize learning the semantic boundaries of objects. However, re-cent generative foundation models built using text-based la-tent diffusion techniques may learn semantic boundaries.
This is because they have to synthesize intricate details about all objects in an image based on a text description.
Therefore, we present a technique for segmenting real and
AI-generated images using latent diffusion models (LDMs) trained on internet-scale datasets. First, we show that the latent space of LDMs (z-space) is a better input representa-tion compared to other feature representations like RGB im-ages or CLIP encodings for text-based image segmentation.
By training the segmentation models on the latent z-space, which creates a compressed representation across several domains like different forms of art, cartoons, illustrations, and photographs, we are also able to bridge the domain gap between real and AI-generated images. We show that the internal features of LDMs contain rich semantic infor-mation and present a technique in the form of LD-ZNet to further boost the performance of text-based segmenta-tion. Overall, we show up to 6% improvement over standard baselines for text-to-image segmentation on natural images.
For AI-generated imagery, we show close to 20% improve-ment compared to state-of-the-art techniques. The project is available at https://koutilya-pnvr.github.io/LD-ZNet/. 1.

Introduction
Teaching neural networks to accurately ﬁnd the bound-aries of objects is hard and annotation of boundaries at internet scale is impractical. Also, most self-supervised or weakly supervised problems do not incentivize learning boundaries. For example, training on classiﬁcation or cap-tioning allows models to learn the most discriminative parts of the image without focusing on boundaries [42, 60]. Our insight is that Latent Diffusion Models (LDMs) [38], which
*This work was done when Koutilya and Bharat were at Amazon.
A picture of an astronaut, 
A picture of the Stonehenge t=400
Latent Diffusion
Model
Latent Diffusion
Model t=400
NULL
Figure 1: Coarse segmentation results from an LDM for two dis-tinct images, demonstrating the encoding of ﬁne-grained object-level semantic information within the model’s internal features. can be trained without object level supervision at internet scale, must attend to object boundaries, and so we hypothe-size that they can learn features which would be useful for open world image segmentation. We support this hypothe-sis by showing that LDMs can improve performance on this task by up to 6%, compared to standard baselines and these gains are further ampliﬁed when LDM based segmentation models are applied on AI generated images.
To test the aforementioned hypothesis about the pres-ence of object-level semantic information inside a pre-trained LDM, we conduct a simple experiment. We com-pute the pixel-wise norm between the unconditional and text-conditional noise estimates from a pretrained LDM as part of the reverse diffusion process. This computation identiﬁes the spatial locations that need to be modiﬁed for the noised input to align better with the corresponding text condition. Hence, the magnitude of the pixel-wise norm depicts regions that identify the text prompt. As shown in the Figure 1, the pixel-wise norm represents a coarse seg-mentation of the subject although the LDM is not trained on this task. This clearly demonstrates that these large scale
LDMs can not only generate visually pleasing images, but their internal representations encode ﬁne-grained semantic information, that can be useful for tasks like segmentation. text-based image segmentation has gained
Recently,
A picture of an astronaut
CLIP Text
Encoder
LD-ZNet
Timestep t
Second-stage of LDM 
Denoising UNet
Encoder/Decoder Block
Spatial-Attention module
Residual layer f l e
S n o i t n e t t a s s o r
C n o i t n e t t a
Timestep t
CLIP  
Text features
Latent Diffusion 
Features
E z
Attention Pool
First-stage of LDM 
VQGAN encoder
Pretrained, Frozen
Forward Diffusion
ZNet
Figure 2: Overview of the proposed ZNet and LD-ZNet architectures. We propose to use the compressed latent representation z as input for our segmentation network ZNet. Next, we propose LD-ZNet, which incorporates the latent diffusion features at various intermediate blocks from the LDM’s denoising UNet, into ZNet. traction for creating and editing AI generated content (like
AI art, illustrations, cartoons etc.) in image inpainting workﬂows 1 as it provides a conversational interface. Since the latent space z [11], extracted by a VQGAN is trained on several domains like art, cartoons, illustrations and real photographs, we posit that it is a more robust input represen-tation for text-based segmentation on AI-generated images.
Furthermore, the internal layers of the LDM are responsible for generating the structure of the image and hence contain rich semantic information about objects. Soft masks from these layers have also been used as a latent input in recent work on image editing [15, 2]. Since this information is already present while generating the image, we propose an architecture in the form of LD-ZNet (shown in Figure 2) to decode it for obtaining the semantic boundaries of ob-jects generated in the scene. Not only does our architecture beneﬁt segmentation of objects in AI generated images, but it also improves performance over natural images. Overall our contributions are as follows:
• We propose a text-based segmentation architecture,
ZNet that operates on the compressed latent space of the LDM (z).
• Next, we study the internal representations at differ-ent stages of pretrained LDMs and show that they are useful for text-based image segmentation.
• Finally, we propose a novel approach named LD-ZNet to incorporate the visual-linguistic latent diffusion fea-tures from a pretrained LDM and show improvements 1https://github.com/brycedrennan/imaginAIry, https://github.com/AUTOMATIC1111/stable-diffusion-webui across several metrics and domains for text-based im-age segmentation. 2.