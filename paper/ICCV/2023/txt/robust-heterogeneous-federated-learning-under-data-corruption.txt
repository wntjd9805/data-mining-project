Abstract
Model heterogeneous federated learning is a realistic and challenging problem. However, due to the limitations of data collection, storage, and transmission conditions, as well as the existence of free-rider participants, the clients may suffer from data corruption. This paper starts the first attempt to investigate the problem of data corruption in the model heterogeneous federated learning framework. We de-sign a novel method named Augmented Heterogeneous Fed-erated Learning (AugHFL), which consists of two stages: 1)
In the local update stage, a corruption-robust data augmen-tation strategy is adopted to minimize the adverse effects of local corruption while enabling the models to learn rich lo-cal knowledge. 2) In the collaborative update stage, we de-sign a robust re-weighted communication approach, which implements communication between heterogeneous models while mitigating corrupted knowledge transfer from others.
Extensive experiments demonstrate the effectiveness of our method in coping with various corruption patterns in the model heterogeneous federated learning setting. 1.

Introduction
Modern society contains a large number of edge devices, such as smartphones, mobile networks, IoT devices, etc., which can be regarded as local clients with limited private data. The direct collection of private data from clients se-riously compromises their privacy and security. Federated learning [48, 37, 7] is a distributed machine learning frame-work with secure encryption techniques. It is intended to enable multiple institutions to collaboratively train machine learning models while following the data-stay-local policy.
In addition to the attention of academia [63, 68, 36, 45, 32, 59, 59, 56], federated learning has also been extensively ex-plored in industrial fields such as healthcare [10, 64, 27, 15], finance [4], and data security [61], etc. Despite its remark-*Corresponding Author: Mang Ye (yemang@whu.edu.cn)
Figure 1. Illustration of heterogeneous federated learning with data corruption, where the clients may possess different model struc-tures and corrupted private datasets. able success, the most prevalent federated learning algo-rithms [16, 51, 49] rely heavily on the assumption that all clients share an identical local model structure. For exam-ple, FedAvg [48], FedProx [35], and Per-FedAvg [13] en-able communication between clients by taking the weighted average of the local model parameters.
In practical application scenarios, clients expect to inde-pendently design their own models to meet different tasks and specifications, which inevitably leads to the challenge of model heterogeneity [69, 46, 66, 72] in federated learn-ing, as shown in Fig. 1. However, clients are often reluc-tant to share the details of their local model designs in con-sideration of privacy protection. Additionally, hardware, software, and communication capabilities [34] often differ between clients, which can further exacerbate model het-erogeneity. Consequently, designing heterogeneous feder-ated learning strategies to achieve collaborative communi-1
cation between heterogeneous models has become the focus of significant research. FedMD [31] performs communi-cation between heterogeneous clients based on the average class score output by the local models. FedDF [41] lever-ages unlabeled data or artificially generated examples to ex-tract knowledge from the clients. RHFL [14] learns the knowledge distribution from other clients by aligning the feedback outputs of client models on public irrelevant data.
However, the aforementioned methods all assume that each client has a private dataset with clean images, which is dif-ficult to satisfy in practical applications.
The widely used heterogeneous federated learning algo-rithms show exceptional performance under ideal circum-stances where the image samples are clean. However, clean image samples require substantial costs in terms of data col-lection, storage, and transmission. In real-world scenarios, the quality of the collected data can be influenced by several factors such as weather conditions, collection means, stor-age methods, etc. Consequently, the algorithms inevitably encounter corrupted data. Furthermore, there may be free-riding participants [40] in the federated learning system, who pretend to participate in the federated learning process, benefiting from the information contributed by other clients without actually providing any data. This phenomenon re-sults in honest clients being reluctant to share their real information, raising concerns about data privacy and user fairness. As a means of safeguarding their privacy, hon-est clients may intentionally offer corrupted data. This data corruption strategy maintains the fairness of the federated learning system at the expense of model performance.
Data corruption is inevitable in data collection, storage, and transmission. Worse still, free-rider concerns exacer-bate the potential for data corruption. As a consequence of data corruption, clients iteratively learn and share wrong knowledge, causing local models to update in incorrect di-rections. Ultimately, the performance of the federated learn-ing system suffers a severe degradation, hindering the prac-tical deployment of the models. In single-model machine learning, existing methods to mitigate the negative impact of data corruption during model training can be classified into three categories: data augmentation [2, 44, 53], noise injection [38, 42, 43], and pre-training [23, 50, 3]. The multi-model scenario of federated learning is more com-plex, thereby rendering overcoming data corruption more challenging. We expect to fully learn local knowledge and resist multiple corruptions under the federated learn-ing framework. Thus, how to mitigate model performance degradation caused by data corruption inside the client dur-ing the local update phase is an important challenge.
In addition, data corruption in model heterogeneous sce-narios encounters a new problem, i.e., how to avoid learning the corrupted knowledge from others while realizing hetero-geneous models communication. In heterogeneous feder-Figure 2. Evaluation of each client model on the clean and cor-rupted test data. We observe that clean data are more susceptible to correct prediction than corrupted data. Compared to vanilla mod-els, AugHFL achieves significantly higher accuracy on corrupted data and maintains performance on clean data. ated learning scenarios, the local models of clients may have different model architectures, thereby presenting inconsis-tent decision boundaries.
It is noteworthy that corrupted samples are generally harder to classify correctly than clean samples, as illustrated in Fig. 2. For the same corrupted im-age, different model predictions have significant variance, as depicted in Fig. 1. Thus, during the collaborative up-date phase, the clients may learn erroneous predictions from other models. However, existing centralized methods for corruption can only handle internal corruption, but they can-not prevent external corruption from low-robustness models in the collaborative learning phase. Moreover, existing ho-mogeneous FL methods for corruption are not suitable for model heterogeneous scenarios, as they rely on the local model training loss to measure the model reliability. The model training loss can be affected by many factors, and it is not fair or reliable to compare the training loss of hetero-geneous models directly. Therefore, how to minimize cor-rupted feedback from unreliable clients while implementing heterogeneous model communication is a crucial challenge.
In this paper, we propose AugHFL to address the data corruption problem in heterogeneous federated learning, which consists of two stages: 1) For the negative impact of local data corruption, we perform multiple random data augmentation operations and then mix the augmented im-ages. Simultaneously, a consistency constraint for diverse augmentations of the same image is imposed on the clas-sifiers. This technique improves the classification robust-ness of the local models against corrupted data in the lo-cal update phase. 2) For corrupted knowledge learned from others, we design an adaptive re-weighted commu-nication strategy called Public data Augmentation (Pub-Aug). The main idea is to dynamically adjust the contri-butions of clients according to their reliability in the collab-orative update phase. We measure the predicted distribu-tion consistency of the local model on the corrupted public datasets and the original public dataset. Then, the robust-ness of models to uncertain corruption patterns is quantified by the computed distribution consistency. The local models that can effectively against multiple corruptions are consid-ered reliable and thus their weights are increased, while the weights of unreliable models decrease synchronously. The main contributions of this work are as follows:
• We introduce AugHFL for a new and challenging model heterogeneous federated learning problem with data corruption. It simultaneously handles intra-client and inter-client corruption, enhancing the robustness against diverse corruptions.
• We propose PubAug, an adaptive re-weighted commu-nication method. It adaptively adjusts the contribution of clients based on their ability to cope with data cor-ruption while enabling communication between het-erogeneous models.
• Extensive experiments demonstrate that AugHFL out-performs State-Of-The-Art (SOTA) methods in terms of both performance and robustness, especially on datasets with severe data corruption. 2.