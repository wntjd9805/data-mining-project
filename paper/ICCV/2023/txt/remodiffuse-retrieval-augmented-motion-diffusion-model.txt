Abstract github.io/projects/ReMoDiffuse.html 3D human motion generation is crucial for creative indus-try. Recent advances rely on generative models with domain knowledge for text-driven motion generation, leading to substantial progress in capturing common motions. How-ever, the performance on more diverse motions remains unsatisfactory. In this work, we propose ReMoDiffuse, a diffusion-model-based motion generation framework that integrates a retrieval mechanism to refine the denoising process. ReMoDiffuse enhances the generalizability and diversity of text-driven motion generation with three key designs: 1) Hybrid Retrieval finds appropriate references from the database in terms of both semantic and kine-matic similarities. 2) Semantic-Modulated Transformer selectively absorbs retrieval knowledge, adapting to the difference between retrieved samples and the target motion sequence. 3) Condition Mixture better utilizes the retrieval database during inference, overcoming the scale sensi-tivity in classifier-free guidance. Extensive experiments demonstrate that ReMoDiffuse outperforms state-of-the-art methods by balancing both text-motion consistency and motion quality, especially for more diverse motion gener-ation. Project page: https://mingyuan-zhang. (cid:12) Corresponding author. 1.

Introduction
Human motion generation has numerous practical appli-cations in fields such as game production, film, and virtual reality. This has led to a growing interest in generating manipulable, plausible, diverse, and realistic human mo-tion sequences. Traditional modeling processes are time-consuming and require specialized equipment and a signif-icant amount of domain knowledge. To address these chal-lenges, generic human motion generation models have been developed to enable the description, generation, and modi-fication of motion sequences. Among all forms of human-computer interaction, natural language, in the form of text, provides rich semantic details and is a commonly used con-ditional signal in human motion generation.
Previous research has explored various generative mod-els for text-driven motion generation. TEMOS uses a
Variational-Auto-Encoder (VAE) to synthesize detailed mo-tions, utilizing the KIT Motion-Language dataset [17]. Guo et al. [7] propose a two-stage auto-regressive approach for generating motion sequences. More recently, diffusion models have been applied to human motion generation due to their strength and flexibility. MotionDiffuse [27] gener-ates realistic and diverse actions while allowing for multi-level motion manipulation in both spatial and temporal di-mensions. MDM [24] uses geometric losses as training constraints to make predictions of the sample itself. While these methods have achieved impressive results, they are not versatile enough for uncommon condition signals.
Some recent works on text-to-image generation utilize retrieval methods to complement the model framework, providing an retrieval-augmented pipeline to tackle the above issue [22, 4, 3]. However, simply transferring these methods into text-driven motion generation fields is imprac-tical due to three new challenges. Firstly, the similarity between the target motion sequence and the elements in database is complicated. We need to evaluate both seman-tic and kinematic similarities to find out related knowledge.
Secondly, a single motion sequence usually contains several atomic actions. It is necessary to learn from the retrieved samples selectively. In this procedure, the model should be aware of the semantic difference between the given prompt and retrieved samples. Lastly, motion diffusion models are sensitive to the scale in classifier-free guidance, especially when we supply another condition, retrieved samples.
In this paper, we propose a new text-driven motion gen-eration pipeline, ReMoDiffuse, which addresses the above-mentioned challenges and thoroughly benefits from the re-trieval techniques to generate diverse and high-quality mo-tion sequences. ReMoDiffuse includes two stages: retrieval stage and refinement stage. In the retrieval stage, we ex-pect to acquire the most informative samples to provide useful guidance for the denoising process. Here we con-sider both semantic and kinematic similarities and suggest a Hybrid Retrieval technique to achieve this objective. In the refinement stage, we design a Semantics-Modulated
Transformer to leverage knowledge retrieved from an ex-tra multi-modal database and generate semantic-consistent motion sequences. During inference, Condition Mixture technique enables our model to generate high-fidelity and description-consistent motion sequences. We evaluate our proposed ReMoDiffuse on two standard text-to-motion gen-eration benchmarks, HumanML3D [7] and KIT-ML [17].
Extensive quantitative results demonstrate that ReMoDif-fuse outperforms other existing motion generation pipelines by a significant margin. Additionally, we propose several new metrics for quantitative comparisons on uncommon samples. We find that ReMoDiffuse significantly improves the generation quality on rare samples, demonstrating its superior generalizability.
To summarize, our contributions are threefold: 1) We carefully design a retrieval-augmented motion diffusion model which efficiently and effectively explores the knowl-edge from retrieved samples; 2) We suggest new metrics to evaluate the modelâ€™s generalizability under different scenar-ios comprehensively; 3) Extensive qualitative and quantita-tive experiments show that our generated motion sequences achieve higher generalizability on both common and un-common prompts. 2.