Abstract
In this paper, we introduce a novel approach to novel object captioning which employs relative contrastive learn-ing to learn visual and semantic alignment. Our approach maximizes compatibility between regions and object tags in a contrastive manner. To set up a proper contrastive learn-ing objective, for each image, we augment tags by lever-aging the relative nature of positive and negative pairs ob-tained from foundation models such as CLIP. We then use the rank of each augmented tag in a list as a relative rel-evance label to contrast each top-ranked tag with a set of lower-ranked tags. This learning objective encourages the top-ranked tags to be more compatible with their image and text context than lower-ranked tags, thus improving the dis-criminative ability of the learned multi-modality represen-tation. We evaluate our approach on two datasets and show that our proposed RCA-NOC approach outperforms state-of-the-art methods by a large margin, demonstrating its ef-fectiveness in improving vision-language representation for novel object captioning. 1.

Introduction
Describing novel objects unseen in training data is a highly desired capability for a real-world image captioning model. Conventional image captioning models [4, 20, 28] often fail to describe novel objects because they only cover limited visual concepts and generalize poorly to images in the wild [26]. To overcome this limitation, approaches that rely on object detection as the external resource [7,8,13,18, 20, 25, 27, 33, 35] have been widely explored and demon-strated breakthroughs in vision-language (VL) understand-ing. (e.g., object
Although detection models
Faster
RCNN [23]) have been improved to recognize a wide range of objects including novel ones like zero-shot object detection [14], using object detection in novel captioning models brings a new challenge. VIVO [13] leverages extra object tags to pre-train a visual vocabulary and help image captioning generalize to new categories. NOC-REK [29]
Figure 1. An illustrative example of our method to lever-age relative semantic relevance to achieve modality alignment.
Our method could give accurate captions "A cute red panda sitting in a tree branch" conditioned on the objects "red panda" while VINVL+VIVO generates a wrong caption "A cute raccoon sitting in a tree branch". This inference result shows that our method can differentiate some confusing objects and generate accurate captions for novel objects when the object detection is well aligned with other modalities. tries to augment object tags in the training stage based on the similarity between region and word. Such methods sim-ply concatenate regions, object tags, and caption features as input to a Transformer-based model and use masked token reconstruction to implicitly learn an alignment between vision and language. Although such an alignment can help bring regions and words of the same concept closer, it lacks an effective mechanism to push away irrelevant concepts.
As a result, it still makes mistakes on some confusing concepts, as shown in Figure 1.
Different from previous methods, this study aims to learn visual and semantic alignment in a contrastive man-ner. Gupta et al. [10] showed that modality alignment could be achieved by maximizing the information lower bound between an image and its object tags. That is, given pairs of image and object tags, we can maximize the compatibility between tags and their attention-weighted region represen-tations, compared to regions and non-corresponding tags.
However, there are two critical problems that need to be further addressed: 1) how to effectively generate contrastive tags (augmented tags) that are closely relevant to an image, and 2) how to design a proper contrastive learning objective that allows the model to effectively leverage the contrastive
tags to align vision and semantics.
To tackle the first problem, we utilize CLIP [22] to create a list of contrastive tags which are closely linked to an im-age and contain global structural information and high-level concepts describing scenes. This approach aids in the dis-covery of useful tags that are essential for contrastive learn-ing. To address the second problem, a proper contrastive learning objective needs to be explored. The major chal-lenge here is that the augmented contrastive tags are inaccu-rate and inevitably noisy. We cannot simply treat one tag as positive and others as negative to perform contrastive learn-ing, because the augmented tags might be highly correlated or similar to each other. To tackle this problem, we leverage the relative relevance, rather than the absolute relevance, of the augmented tags, which is more robust to data noise.
Specifically, given an image, for each of its labeled ob-ject tags, we generate a ranked list of contrastive tags using
CLIP. We regard the rank of each augmented tag as its rel-ative semantic relevance with the image.
In general, the top-ranked tags are assumed to be more relevant to the im-age than the lower-ranked tags. We divide the list into two parts: the (relatively) relevant part with higher rank and the (relatively) irrelevant part with lower rank (Figure 1 (a)).
In our proposed objective, we treat each tag in the relevant part as positive and all tags in the irrelevant part as nega-tive to perform contrastive learning. Note that we do not let tags both in the relevant part contrast with each other as they might be highly correlated concepts. In this way, our approach weakens the strict assumption of contrastive learning in previous works and exploits the relative ranking in a loose form to achieve modality alignment.
We conduct experiments on the Nocaps and Held-Out
COCO datasets to demonstrate the effectiveness of RCA-NOC. Our contributions can be summarized as follows:
• We propose Relative Contrastive Alignment (RCA) to learn the relative semantic relevance in a loose form by maximizing the compatibility between regions and their relevant tags compared with regions and irrelevant tags to achieve vision and language alignment and improve the dis-criminative ability of the multi-modality representation.
• A method called Uncertainty-Aware Selection and
Reweighting (UASR) is proposed to estimate and exploit the uncertainty of each contrastive sample to mitigate the negative effect brought by noisy tags. UASR can effectively prioritize highly reliable samples and demote false positives and false negatives.
• We validate the proposed method on the Nocaps and
Held-Out COCO benchmarks, which outperforms other state-of-the-art methods by a large margin. 2.