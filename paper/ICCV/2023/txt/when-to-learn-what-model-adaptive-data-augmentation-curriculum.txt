Abstract
Data augmentation (DA) is widely used to improve the generalization of neural networks by enforcing the invari-ances and symmetries to pre-defined transformations ap-plied to input data. However, a fixed augmentation pol-icy may have different effects on each sample in differ-ent training stages but existing approaches cannot adjust the policy to be adaptive to each sample and the training model. In this paper, we propose “Model-Adaptive Data
Augmentation (MADAug)” that jointly trains an augmen-tation policy network to teach the model “when to learn what”. Unlike previous work, MADAug selects augmenta-tion operators for each input image by a model-adaptive policy varying between training stages, producing a data augmentation curriculum optimized for better generaliza-tion.
In MADAug, we train the policy through a bi-level optimization scheme, which aims to minimize a validation-set loss of a model trained using the policy-produced data augmentations. We conduct an extensive evaluation of
MADAug on multiple image classification tasks and net-work architectures with thorough comparisons to existing
DA approaches. MADAug outperforms or is on par with other baselines and exhibits better fairness: it brings im-provement to all classes and more to the difficult ones.
Moreover, MADAug learned policy shows better perfor-mance when transferred to fine-grained datasets. In addi-tion, the auto-optimized policy in MADAug gradually in-troduces increasing perturbations and naturally forms an easy-to-hard curriculum. Our code is available at https:
//github.com/JackHck/MADAug. 1.

Introduction
Data augmentation is a widely used strategy to increase the diversity of training data, which improves the model generalization, especially in image recognition tasks [21, 35, 17]. Unlike previous works that apply manually-designed augmentation operations [6, 44, 47, 23, 4, 24], re-*Corresponding author cent researchers have resorted to searching a data augmen-tation policy for a target dataset/samples. Despite the suc-cess of these learnable and dataset-dependent augmentation policies, they are fixed once learned and thus non-adaptive to either different samples or models at different training stages, resulting in biases across different data regions [2] or inefficient training.
In this paper, we study two fundamental problems to-wards developing a data-and-model-adaptive data augmen-tation policy that determines a curriculum of “when to learn what” to train a model: (1) when to apply data augmen-tation in training? (2) what data augmentations should be applied to each training sample at different training stages?
First, applying data augmentation does not always bring improvement over the whole course of training. For exam-ple, we observed that a model tends to learn faster during earlier training stages without using data augmentation. We hypothesize that models at the early stage of training even have no capability to recognize the original images so ex-cessively augmented images are not conducive to the con-vergence of the models. Motivated by this observation, we first design a strategy called monotonic curriculum to pro-gressively introduce more augmented data to the training.
In particular, we gradually increase the probability of apply-ing data augmentation to each sample by following the Tanh function (see Figure 1), so the model can be quickly im-proved in earlier stages without distractions from augmen-tations while reaching a better performance in later stages through learning from augmented data.
Secondly, a fixed augmentation policy is not optimal for learning every sample or different training stages. Al-though the monotonic curriculum gradually increases the augmented data as the model improves, it does not deter-mine which augmentations applied to each sample can bring
Intuitively, the most improvement to the model training. the model can learn more from diverse data augmentations.
Moreover, the difficulty of augmented data also has a great impact on the training and it depends on both the augmen-tations and the sample they are applied to. For example,
“simple” augmentation is preferred in the early stages to accelerate model convergence but more challenging aug-Figure 1: MADAug applies a monotonic curriculum to gradually introduce more data augmentations to the task model training and uses a policy network to choose augmentations for each training sample. MADAug trains the policy to minimize the validation loss of the task model, so the augmentations are model-adaptive and optimized for different training stages. mented data provide additional information for learning more robust features for better generalization in the later stage. One plausible strategy is leveraging expert knowl-edge and advice to adjust the augmentation operation and
In this paper, instead of their strengths [29, 47, 34, 14]. relying on human experts, we regard the evaluation of the current model on a validation set as an expert to guide the optimization of augmentation policies applied to each sam-ple in different training stages. As illustrated in Figure 1, we utilize a policy network to produce the augmentations for each sample (i.e., data-adaptive) used to train the task model, while the training objective of the policy network is to minimize the validation loss of the task model (i.e., model-adaptive). This is a challenging bi-level optimiza-tion [5]. To address it, we train the task model on adaptive augmentations of training data and update the policy net-work to minimize the validation loss in an online manner.
Thereby, the policy network is dynamically adapted to dif-ferent training stages of the task model and generates cus-tomized augmentations for each sample. This results in a curriculum of data augmentations optimized for improving the generalization performance of the task model.
Our main contributions can be summarized as follows: (a) A monotonic curriculum gradually introducing more data augmentation to the training process. (b) MADAug that trains a data augmentation policy net-work on the fly with the task model training. The pol-icy automatically selects augmentations for each train-ing sample and for different training stages. (c) Experiments on CIFAR-10/100, SVHN, and ImageNet demonstrate that MADAug consistently brings greater improvement to task models than existing data aug-mentation methods in terms of test-set performance. (d) The augmentation policy network learned by is transferable to un-MADAug on one dataset seen datasets and downstream tasks, producing better models than other baselines. 2.