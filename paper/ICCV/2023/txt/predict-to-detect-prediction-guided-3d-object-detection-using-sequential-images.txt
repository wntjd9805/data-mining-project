Abstract
Recent camera-based 3D object detection methods have introduced sequential frames to improve the detection per-formance hoping that multiple frames would mitigate the large depth estimation error. Despite improved detection performance, prior works rely on naive fusion methods (e.g., concatenation) or are limited to static scenes (e.g., temporal stereo), neglecting the importance of the motion cue of objects. These approaches do not fully exploit the potential of sequential images and show limited perfor-mance improvements. To address this limitation, we pro-pose a novel 3D object detection model, P2D (Predict to
Detect), that integrates a prediction scheme into a detection framework to explicitly extract and leverage motion fea-tures. P2D predicts object information in the current frame using solely past frames to learn temporal motion features.
We then introduce a novel temporal feature aggregation method that attentively exploits Bird’s-Eye-View (BEV) fea-tures based on predicted object information, resulting in ac-curate 3D object detection. Experimental results demon-strate that P2D improves mAP and NDS by 3.0% and 3.7% compared to the sequential image-based baseline, proving that incorporating a prediction scheme can significantly im-prove detection accuracy. 1.

Introduction 3D object detection is an essential task for building a re-liable self-driving system. In recent years, camera-based 3D object detection [29, 33, 35, 38] has gained widespread at-tention due to the cost-effectiveness of a camera sensor and its high-resolution characteristic. However, camera-based 3D object detection still has limited performance due to the scale ambiguity caused by projecting 3D space onto a 2D image and the absence of motion cues that are difficult to capture in a single image.
Recent works have mitigated these drawbacks by lever-aging multiple frames from history. Multi-frame ap-proaches incorporate temporal information into the space domain to provide richer information. Moreover, sequence images are often readily available in real-world applications such as autonomous driving, making the use of sequence images an attractive option for performance improvements.
Previous works [12, 14, 18, 23] have used temporal im-ages in feature-level aggregation by concatenating sequen-tial features to merge them. On the other hand, another line of work [16, 30, 36, 41] has adopted a temporal stereo [42] to enhance depth estimation using the multi-view stereo (MVS) [9]. Although these methods have proved the effec-tiveness of sequence frames over a single frame, they did not thoroughly investigate into the motion cue of objects, from which the object detection would benefit by using se-quence images.
Temporal images have rich motion information, which can provide critical motion features for accurate object de-tection. To further demonstrate the importance of motion cues in detection, we conducted experiments and evaluated the performance of the prediction-only results, which rely on motion prediction from previous frames, without using the current frame. Our findings from Table 3 indicate that the prediction-only results (P) can achieve comparable per-formance to the final results (P+D), reaching up to 76% and 89% in terms of mAP and NDS, respectively. The final results (P+D) denote detection results that incorporate all temporal frames, including the current frame. This experi-ment highlights the potential of using motion features from previous frames, which has been overlooked in prior works.
To this end, we propose a novel sequential image-based 3D object detection model that learns motion cues to im-prove detection accuracy. Our approach, P2D (Predict to
Detect), introduces a prediction scheme into the detection task to fully exploit multi-frame image data. Specifically,
P2D conducts motion prediction using previous frames to output the predicted objects’ information for the current frame. In the feature aggregation module, we employ a de-formable attention [46] to make a spatio-temporal feature on the basis of prediction results that contains motion fea-tures. Finally, the 3D detection head takes aggregated the
Figure 1. Comparison of temporal image-based methods. All methods align features by warping previous frames to the current frame. (a) Feature-level aggregation methods naively concatenate sequential features before inputting them to the detection head. (b) Depth enhancement methods facilitate depth estimation using Multi-View Stereo (MVS). (c) Our proposed approach combines prediction and detection to leverage motion features. We predict object information from previous frames and use it to detect objects in the current frame. spatio-temporal feature and outputs the final detection re-sults. In this way, our proposed method can fully benefit from multi-frame inputs by predicting objects’ motion and utilizing it explicitly, providing a more accurate and reliable 3D object detection system for autonomous driving.
In summary, our contributions are as follows:
• We identify the motion feature as a key factor when handling sequential images for 3D object detection. A prediction mechanism is introduced to fully exploit the motion feature of multi-frame image data.
• We propose a novel 3D object detection model using sequential images. Our model includes a Prediction
Head to predict object information and a Prediction-guided Feature Aggregation to integrate temporal fea-tures using motion features.
• Our approach achieves improved performance com-pared to prior state-of-the-art methods. Extensive ex-perimentation confirms the effectiveness of our ap-proach in adapting to moving objects and accurately estimating their velocities. 2.