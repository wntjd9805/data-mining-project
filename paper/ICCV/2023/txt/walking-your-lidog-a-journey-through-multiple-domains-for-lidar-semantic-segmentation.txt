Abstract
The ability to deploy robots that can operate safely in diverse environments is crucial for developing embodied intelligent agents. As a community, we have made tremen-dous progress in within-domain LiDAR semantic segmenta-tion. However, do these methods generalize across domains?
To answer this question, we design the first experimental setup for studying domain generalization (DG) for LiDAR semantic segmentation (DG-LSS). Our results confirm a sig-nificant gap between methods, evaluated in a cross-domain setting: for example, a model trained on the source dataset (SemanticKITTI) obtains 26.53 mIoU on the target data, compared to 48.49 mIoU obtained by the model trained on the target domain (nuScenes). To tackle this gap, we propose the first method specifically designed for DG-LSS, which obtains 34.88 mIoU on the target domain, outperforming all baselines. Our method augments a sparse-convolutional encoder-decoder 3D segmentation network with an addi-tional, dense 2D convolutional decoder that learns to clas-sify a birds-eye view of the point cloud. This simple auxiliary task encourages the 3D network to learn features that are robust to sensor placement shifts and resolution, and are transferable across domains. With this work, we aim to in-spire the community to develop and evaluate future models in such cross-domain conditions. 1.

Introduction
We address the challenge of achieving accurate and ro-bust semantic segmentation of LiDAR point clouds. LiDAR semantic segmentation (LSS) is one of the most fundamental perception problems in mobile robot navigation, with appli-cations ranging from mapping [28], localization [32], and online dynamic situational awareness [45].1
Status Quo. State-of-the-art LSS methods [8, 16, 44, 68] perform well when trained and evaluated using the same sen-sory setup and environment (i.e., source domain, Fig. 1, left).
However, their performance degrades significantly in the presence of domain shifts (Fig. 1, right), commonly caused by differences in sensory settings, e.g., a new type of sensor, or recording environments, e.g., geographic regions with different road layouts or types of vehicles. One way to miti-gate this is to collect multi-domain datasets for pre-training, 1Semantic point classifier was part of the perception stack in early autonomous vehicles, such as Stanley, that won the DARPA challenge in 2005. 1
similar to what has been done in the image domain using inexpensive cameras that are widely available [9, 30]. How-ever, building a crowd-sourced collection of multi-domain
LiDAR datasets is currently not feasible.
Stirring the pot. As a first step towards LiDAR seg-mentation models that are robust to domain shifts, we present the first-of-its-kind experimental test-bed for study-ing Domain Generalization (DG) in the context of LiDAR
Semantic Segmentation (LSS). In our DG-LSS setup we train and evaluate models on different domains, includ-ing two synthetic [39] and two real-world densely labeled datasets [3, 13], recorded in different geographic regions with different sensors. This evaluation setup reveals a signif-icant gap in terms of mean intersection-over-union between models trained on the source and target domains: for exam-ple, a model transferred from SemanticKITTI to nuScenes dataset obtains 26.53 mIoU compared 48.49 mIoU obtained by the model fully trained on the target domain. Could this gap be alleviated with DG techniques?
Insights. To address this challenge, we propose LiDOG (LiDAR DOmain Generalization) as a simple yet effective method specifically designed for DG-LSS. In addition to reasoning about the scene semantics in 3D space, LiDOG projects features from the sparse 3D decoder onto the 2D birdâ€™s-eye-view (BEV) plane along the vertical axis and learns to estimate a dense 2D semantic layout of the scene.
In this way, LiDOG encourages the 3D network to learn features that are robust to variations in, e.g., type of sensor or geo locations, and thereby can be transferred across dif-ferent domains. This directly leads to increased robustness toward domain shifts and yields +8.35 mIoU improvement on the target domain, confirming the efficacy of our ap-proach. Our experimental evaluation confirms this approach is consistently more effective compared to prior efforts in data augmentations [61, 34, 38], domain adaptation tech-niques [49, 22], and image-based DG techniques [7, 35], applied to the LiDAR semantic segmentation.
Contributions. We make the following key contributions.
We (i) present the first study on domain generalization in the context of LiDAR semantic segmentation. To this end, we (ii) carefully construct a test-bed for studying DG-LSS using two synthetic and two densely-labeled real-world datasets recorded in different cities with different sensors. This al-lows us to (iii) rigorously study how prior efforts proposed in related domains can be used to tackle domain shift and (iv) propose LiDOG, a simple yet strong baseline that learns robust, generalizable, and domain-invariant features by learn-ing semantic priors in the 2D birds-eye view. Despite its simplicity, we achieve state-of-the art performance in all the generalization directions. 2 2Our code is available at https://saltoricristiano.github.io/lidog/. 2.