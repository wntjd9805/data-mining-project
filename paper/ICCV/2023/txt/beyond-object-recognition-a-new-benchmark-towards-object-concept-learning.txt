Abstract
Understanding objects is a central building block of AI, especially for embodied AI. Even though object recogni-tion excels with deep learning, current machines struggle to learn higher-level knowledge, e.g., what attributes an object has, and what we can do with it. Here, we propose a chal-lenging Object Concept Learning (OCL) task to push the envelope of object understanding. It requires machines to reason out affordances and simultaneously give the reason: what attributes make an object possess these affordances.
To support OCL, we build a densely annotated knowledge base including extensive annotations for three levels of ob-ject concept (category, attribute, affordance), and the clear causal relations of three levels. By analyzing the causal structure of OCL, we present a baseline, Object Concept
Reasoning Network (OCRN). It leverages concept instanti-ation and causal intervention to infer the three levels. In experiments, OCRN effectively infers the object knowledge while following the causalities well. Our data and code are available at https://mvig-rhos.com/ocl. 1.

Introduction
Object understanding is essential for intelligent robots.
Recently, benefiting from deep learning and large-scale
*Corresponding author. datasets [10, 36], category recognition [30, 55] has made tremendous progress. But to close the gap between human and machine perception, machines need to pursue deeper understanding, e.g., recognizing higher-level attributes [27] and affordances [16], which may help it establish object concept [41] when interacting with contexts.
Category apple is a symbol indicating its referent (real apples).
In line with symbol grounding [22], ma-chines should learn knowledge beyond category to ap-proach concept understanding. According to cognition studies [58, 41], attribute depicting objects from the physi-cal/visual side plays an important role in object understand-ing. Thus, many works [32, 68, 13] studied to ground ob-jects with attributes, e.g., a hammer consists of a long handle and a heavy head. Moreover, attributes can depict object states [27]. An elegant characteristic of attributes is cross-category: objects of the same category can have vari-ous states (big or fresh apple), whilst various objects can have the same state (sliced orange or apple). If the category is the first level of object concept, the attribute can be seen as the second level closer to the physical fact.
However, recognizing attributes is still far away from concept understanding. Given a hammer, we should know it can be held to hit nails, i.e., requiring machines to infer affordance [16] indicating what actions humans can perform with objects. Thus, we refer to affordance as the third level, which is closely related to common sense and causal inference [16]. Though affordance has been
studied in robotics [12, 24] and vision [8, 73] commu-nities for decades, it is still challenging. First, previous works [45, 15] often focus on recognizing affordance solely.
But we usually infer affordance based on attribute observa-tion. If we need to knock in a nail without a hammer at hand, we may find other hard or heavy objects instead, e.g., a thick book. This profoundly reveals the causality between attribute and affordance. Second, previous works are designed for scale/scene-limited tasks, e.g., in [73], 40 objects and 14 affordances are included; Hermans et al. [24] collect 375 indoor images of 6 objects, 21 attributes, and 7 affordances; a recent dataset [45] contains 10 indoor objects and 9 affordances. Thus, they cannot afford general affor-dance reasoning for large-scale applications.
To reshape object learning, we believe it is essential to look at the above three levels in a unified and causal way based on an extensive knowledge base. Hence, we move a step forward to propose the object concept learn-ing (OCL) task: given an object, machines need to infer its category, attributes, and further answer “what can we do upon it and why”, as shown in Fig. 1. In a nutshell, ma-chines need to reason affordance based on object appear-ance, category, and attributes. To this end, we build a large-scale and dense dataset consisting of 381 categories, 114 attributes, and 170 affordances. It contains 80,463 images of diverse scenes and 185,941 instances in different states.
Different from previous works [6, 24, 73], OCL offers a more subtle angle. It includes: (1) category-level attribute (A) and affordance (B) labels; (2) instance-level attribute (α) and affordance (β) labels. Besides, we annotate the causal relations between three levels to evaluate the reason-ing ability of models and keep the follow-up methods from fitting data only. Accordingly, based on the causal struc-ture of OCL, we propose a neuro-causal method, Object
Concept Reasoning Network (OCRN), as the future base-line. It leverages concept instantiation (from category-level to instance-level) and causal intervention [50] to infer at-tributes and affordances. OCRN outperforms a host of base-lines and shows impressive performance while following the causal relations well.
In summary, our contributions are threefold: (1) Introducing the object concept learning task poses challenges and opportunities for object understanding and knowledge-based reasoning. (2) Building a benchmark consisting of diverse objects, together with elaborate attributes, and affordances, their clear causal relations. (3) An object concept reasoning network is introduced to reason three levels with concept instantiation perform-ing well on OCL. 2.