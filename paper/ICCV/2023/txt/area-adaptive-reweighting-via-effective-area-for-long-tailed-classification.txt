Abstract
Large-scale data from the real-world usually follow a long-tailed distribution (i.e., a few majority classes occupy plentiful training data, while most minority classes have few samples), making the hyperplanes heavily skewed to the minority classes. Traditionally, reweighting is adopted to make the hyperplanes fairly split the feature space, where the weights are designed according to the number of sam-ples. However, we find that the number of samples in a class can not accurately measure the size of its spanned space, especially for the majority class, where the size of its spanned space is usually larger than the samples’ num-ber because of the high diversity. Therefore, weights de-signed based on the samples’ number will still compress the space of minority classes. In this paper, we reconsider reweighting from a totally new perspective of analyzing the spanned space of each class. We argue that, besides statisti-cal numbers, relations between samples are also significant for sufficiently depicting the spanned space. Consequently, we estimate the size of the spanned space for each category, namely effective area, by detailedly analyzing its samples’ distribution. By treating samples of a class as identically distributed random variables and analyzing their correla-tions, a simple and non-parametric formula is derived to estimate the effective area. Then, the weight simply cal-culated inversely proportional to the effective area of each class is adopted to achieve fairer training. Note that our weights are more flexible as they can be adaptively adjusted along with the optimizing features during training. Exper-iments on four long-tailed datasets show that the proposed weights outperform the state-of-the-art reweighting meth-ods. Moreover, our method can also achieve better results on statistically balanced CIFAR-10/100. Code is available
*Corresponding author
Figure 1. The effective area and statistical number on CIFAR-100-LT under the imbalance ratio of 100. We can see that since sam-ples in “Beaver” are distributed dispersedly with different back-grounds, viewpoints, and poses, their effective area is larger than the samples’ number. Whereas, the effective area of “Clock” is smaller than its number of samples, as samples are highly over-lapped with limited backgrounds and viewpoints. at https://github.com/xiaohua-chen/AREA. 1.

Introduction
Recently, deep neural networks have achieved great im-provements on many tasks [16, 23, 55]. Their dramatic per-formance significantly depends on high-quality annotated and balanced distributed datasets, such as ImageNet [46] and COCO [29]. However, data are usually long-tailed in the real world, where many minority classes occupy a small number of samples and most samples belong to a few ma-jority classes. Obviously, majority classes will dominate the
training, making the hyperplanes heavily skewed to minor-ity classes [43]. Therefore, it is a serious challenge to train a classifier with a severely unbalanced dataset [35, 19, 36, 1].
Rebalancing is an intuitive approach for fair training. To achieve a balanced training set, we can under-sample the majority classes by randomly discarding some samples, or over-sample the minority classes by duplicating their sam-ples [15, 2, 3]. But under-sampling may degrade the per-formance of majority classes, and over-sampling makes mi-nority classes easily over-fitted [53, 4, 9]. Although data augmentation may intuitively diversify minority classes, the enlarged dataset will tremendously slow down the training procedure [57, 25, 5]. Besides, the reasonability of the aug-mented data to a target category requires further discussion.
Compared to rebalancing, reweighting is much more simple and lightweight, which assigns higher costs to the
In reweighting, weights are im-loss of minority classes. portant for good performance. Thus, complex re-weighting methods are designed with sophisticated hyper-parameters, held-out validation sets, or expensive training procedures
[28, 48, 59]. Usually, weights are devised based on the num-ber of samples. However, the spanned space of a category is essentially far more than the statistical number, where the relation of samples is also significant. Class-Balanced Loss
[9] is the preliminary attempt that considers the samples’ distribution in reweighting. It estimates the size of a cat-egory with the assumption that each sample is either en-tirely inside or outside the set of previous data. However, this strong assumption is difficult to be satisfied, where in-stances are naturally partially overlapped. For example, the shape of “Apple” is round, but the color of an instance can be red, green, or yellow, which leads to partial overlap (i.e., sharing the same shape but having different colors).
In contrast to existing work, we consider the size of the spanned space of a category by detailedly analyzing the samples’ distribution. We call this size the effective area, which can be larger than, smaller than, or equal to the num-If samples are distributed far away from ber of samples. each other, then the effective area should be larger than the statistical number. Inversely, when samples are highly over-lapped, the effective area is smaller. Figure 1 shows the effective areas on CIFAR-100-LT under the imbalance ra-tio of 100. We can see that since samples in “Beaver” are distributed dispersedly with different backgrounds, view-points, and poses, the effective area (452.43) of “Beaver” is larger than the samples’ number (415). Whereas, the ef-fective area (173.10) of “Clock” is less than the number of samples (179) as samples are highly overlapped because of the limited backgrounds and viewpoints. While the effec-tive area of the “Pine tree” is equal to the statistical num-ber. Therefore, weights designed by inversing the statistical number will still lead to a biased allocation of the feature space. For example, the weight ratio of “Beaver” to the
“Pine tree” in the statistical number-based line is (1:12.96).
While it can be (1:14.14) under the effective area. However, estimating the effective area is unexplored.
In this paper, we propose a simple, elegant, and non-parametric formula to calculate the effective area for each category. Thus, our method Adaptive Reweighting based on the Effective Area (AREA) can achieve better weights for reweighting. It consists of two stages. In the first stage, we train a basic feature extractor with the standard cross-entropy loss. In the second stage, the trained feature extrac-tor is used to extract features for the training data. Then, for a category, the correlation coefficient between any two sam-ples can be calculated, making up its correlation coefficient matrix R. Subsequently, we compute its effective area with (cid:1) ∈ the derived formula 1/aTRa, where a = (cid:0) 1
RN ×1, N is its statistical number. Finally, the loss for each category is reweighted by the inversely proportional to its effective area. Notably, the weights in AREA can be adap-tively updated during training, since R is calculated with the optimizing features. Extensive experiments demonstrate that the proposed AREA can not only greatly improve the performance on long-tailed datasets, but also promote the accuracy on balanced datasets.
N , ..., 1
N , 1
N
Our key contributions can be summarized as follows:
• To the best of our knowledge, we are the first to pro-pose using the effective area instead of the statistical sample number for reweighting.
• To quantify the effective area of a category, we derive a simple and non-parametric formula 1/aTRa accord-ing to the correlation between all its samples.
• By directly assigning the inversely proportional effec-tive area as the weight for each category, the simply improved cross-entropy loss can substantially boost the performance of long-tailed classification.
• The proposed AREA outperforms current state-of-the-art reweighting methods on four long-tailed datasets.
In addition, it also achieves good improvements on sta-tistically balanced CIFAR-10/100. 2.