Abstract
Recently, the open-vocabulary semantic segmentation problem has attracted increasing attention and the best per-forming methods are based on two-stream networks: one stream for proposal mask generation and the other for segment classification using a pre-trained visual-language model. However, existing two-stream methods require pass-ing a great number of (up to a hundred) image crops into the visual-language model, which is highly inefficient. To address the problem, we propose a network that only needs a single pass through the visual-language model for each input image. Specifically, we first propose a novel network adaptation approach, termed patch severance, to restrict the harmful interference between the patch embeddings in the pre-trained visual encoder. We then propose classifica-tion anchor learning to encourage the network to spatially focus on more discriminative features for classification. Ex-tensive experiments demonstrate that the proposed method achieves outstanding performance, surpassing state-of-the-art methods while being 4 to 7 times faster at inference.
Code: https://github.com/CongHan0808/DeOP.git 1.

Introduction
Semantic segmentation is a critical computer vision task that entails grouping image pixels into semantically signifi-cant regions and predicting their class labels. Previously, se-mantic segmentation networks have concentrated on a pre-defined set of semantic classes based on the dataset. Re-cently, more attention has been devoted to open-vocabulary (also known as zero-shot) semantic segmentation, thanks to the rise of large-scale pre-trained visual-language models (VLMs) like CLIP [32] and ALIGN [19].
OpenSeg [15] was among the first methods for the open-vocabulary semantic segmentation task, which proposes to adopt class-agnostic masks for possible semantic regions and then classify them using text embeddings extracted from a pre-trained VLM (ALIGN [19] in its case). It is ef-*Equal contribution.
†Corresponding authors. (a) (b) (c)
Figure 1: Comparisons between three macro-architectures for open-vocabulary semantic segmentation: (a) coupled network, e.g. OpenSeg [15]; (b) decoupled multi-pass net-work, e.g. SimBaseline [38]; (c) decoupled one-pass net-work (our baseline). The decoupled structure can maintain the generality of the pre-trained VLM, while the one-pass mechanism brings high computational efficiency. ficient since it only requires passing the image through the visual encoder once for segmentation. However, the single shared visual encoder that is updated during training inevitably breaks the original visual-language alignment of the pre-trained VLM. Therefore, it requires training on numerous semantic concepts to ensure its zero-shot capa-bility. For instance, training on Localized Narratives [31] is necessary. We refer to OpenSeg as the coupled one-pass method. Another line of work, including ZegFormer [10],
SimBaseline [38] and OVSeg [26], decouples the two sub-tasks and designs a two-stream architecture. On this note, the class-agnostic mask proposal stream and the mask clas-sification stream use two separate backbones, preserving the generality of the VLM’s visual encoder. However, the downside of such approaches is their high computa-tional overhead since image crops obtained by proposal masks are fed to the visual encoder individually. We re-fer to methods like these as decoupled multi-pass methods.
In this work, we attempt to answer a question: can we develop a framework that can maintain the zero-shot abil-ity of the VLM while being computationally efficient? To explore this possibility, we first design a baseline architec-ture based on two key design principles - decoupled and one-pass. As depicted in Figure 1(c), our baseline approach features two decoupled visual backbones, while the visual encoder of the pre-trained VLM (i.e. CLIP in this work) remains fixed to retain its generalization ability. Addition-ally, to achieve high efficiency, only one pass is required for the image in the classification stream. However, we empiri-cally find that the performance of this baseline architecture is far from ideal. We further apply prompt learning to adapt both the CLIP visual encoder and text encoder to the seg-mentation task at hand, but the performance gap between the baseline model and previous decoupled methods is still significant.
After conducting a thorough analysis, we discover that the main reason for poor classification performance lies in the segment classification rather than the quality/recall of the masks. Specifically, we have identified two main problems that contribute to this issue: (1) The patch em-beddings that belong to different segments interact too much with each other in the original CLIP visual back-bone. In contrast, in multi-pass decoupled methods, patch embeddings in different masks have no interaction at all when passing through the VLM. Although patches belong-ing to different masks can provide some context information for classification, excessive interaction between different segment embeddings can actually harm the classification performance. (2) The final segment embedding, which is obtained by mask-based pooling of patch embeddings, is not optimal for classification. This is because the mask proposal network is trained in a class-agnostic manner, and the weightings in the mask only indicate how likely each patch embedding belongs to a particular segment, regard-less of the category. Therefore, pooling the patch embed-dings based on these masks is sub-optimal for classification.
Therefore, by addressing these issues, we believe that it is possible to improve the classification performance of one-pass segmentation models while maintaining their compu-tational efficiency.
To this end, we propose a two-stream framework, called
Decoupled One-Pass network (DeOP), with the Generalized
Patch Severance (GPS) and Classification Anchor Learning (CAL) to alleviate the above two problems, respectively.
Generalized Patch Severance can be seen as a ‘severance’ operation on the patch tokens/embeddings in the CLIP vi-sual encoder (e.g. ViT-Base). It aims to reduce the harm-ful interference between patch tokens in the encoder, while maintaining the embedding space of CLIP. Classification
Anchor Learning is designed to find patches that are more suitable for segment classification, which we term classi-fication anchors. It is achieved by appending a module at the end of the CLIP visual encoder, and the module learns to generate a heatmap for each mask proposal, indicating which patch embeddings should be focused (i.e. the an-chors) in the following spatial pooling for classification.
We extensively experiment on public benchmarks and show that DeOP consistently outperforms previous meth-ods in both intra- and cross-dataset evaluation, while being significantly more efficient than other multi-pass methods (e.g., SimBaseline), validating the effectiveness of our pro-posed GPS and CAL. 2.