Abstract
Detecting out-of-distribution inputs for visual recogni-tion models has become critical in safe deep learning. This paper proposes a novel hierarchical visual category mod-eling scheme to separate out-of-distribution data from in-distribution data through joint representation learning and statistical modeling. We learn a mixture of Gaussian models for each in-distribution category. There are many Gaussian mixture models to model different visual categories. With these Gaussian models, we design an in-distribution score function by aggregating multiple Mahalanobis-based met-rics. We don’t use any auxiliary outlier data as training samples, which may hurt the generalization ability of out-of-distribution detection algorithms. We split the ImageNet-1k dataset into ten folds randomly. We use one fold as the in-distribution dataset and the others as out-of-distribution datasets to evaluate the proposed method. We also con-duct experiments on seven popular benchmarks, including
CIFAR, iNaturalist, SUN, Places, Textures, ImageNet-O, and OpenImage-O. Extensive experiments indicate that the proposed method outperforms state-of-the-art algorithms clearly. Meanwhile, we ﬁnd that our visual representation has a competitive performance when compared with fea-tures learned by classical methods. These results demon-strate that the proposed method hasn’t weakened the dis-criminative ability of visual recognition models and keeps high efﬁciency in detecting out-of-distribution samples. 1.

Introduction
Modern deep neural networks have shown strong gen-eralization ability when training and test data are from the
†indicates corresponding authors. same distribution [44, 18, 51, 10, 33]. However, encoun-tering unexpected scenarios is inevitable in real-world ap-plications. Thus assuring that training and test data share the same distribution becomes problematic. In applications like autonomous driving [3, 4] and medical image anal-ysis [55, 14, 43], it is critical for models to identify in-puts beyond their recognition capacity – known as out-of-distribution (OOD) detection. OOD detection algorithms can enable the system to warn humans promptly in many safety-related scenarios. Moreover, it has become an im-portant research topic in the research community of safe ar-tiﬁcial intelligence [21, 29, 23, 32, 56].
Many popular OOD detection methods aim to build probability models to describe training distributions [29, 56, 46, 24, 23]. With these probability models, they built a score function that can calculate in-distribution scores for test samples. These in-distribution scores reﬂect whether these samples fall into a given distribution. Then the test sample can be evaluated by the score function to decide whether it is an OOD sample or not. Thus modeling fea-tures of in-distribution data become extremely important.
Previous works [29, 16, 5, 12, 38] build a distribution over the whole training data. Since training images may come from various visual categories, the decision boundary be-tween In-Distribution (InD) and OOD data becomes ex-tremely complex. To solve this problem, subsequent stud-ies [24, 7, 15, 56] decomposed the whole dataset into sev-eral subgroups to simplify the decision boundary. Although representative algorithms like MOS [24], have gotten im-pressive performance in identifying OOD samples, they failed to detect near OOD samples. Because when different visual categories are grouped together, the OOD decision boundary will become even more uncertain.
A typical framework for out-of-distribution detection in-volves two key steps: 1) learning a compact feature rep-resentation that can ﬁt probability models easily; 2) mod-eling features of in-distribution data in complex distribu-tions accurately. The above two problems are mutually con-nected because more compact features will make modeling the data distribution easier, and stronger probability model-ing techniques will exploit fewer restrictions on represen-tation learning. However, achieving the above goals is dif-ﬁcult, even when there are a lot of breakthroughs in deep learning. Because if training samples from the same cate-gory are too close in the feature space, it will usually lead to overﬁtting. Meanwhile, in-distribution samples may come from different visual categories that have large variations in appearance and semantic information, which makes model-ing complex training distributions become challenging even for excellent statisticians.
In this paper, we propose a new out-of-distribution detec-tion framework, called hierarchical visual category model-ing, to solve the above two issues simultaneously. We hold an assumption that given a training set that contains multi-ple visual categories, we can learn a probability model for every category independently. The out-of-distribution de-tection problem can be solved easily by aggregating proba-bility models of known categories. Our motivation is that decomposing the whole dataset into subsets and model-ing each category independently can avoid ﬁnding com-mon characteristics shared by different categories. How-ever, modeling an individual visual category is still chal-lenging since classical supervised learning won’t lead to compact feature representation. Thus, for each input sam-ple, we need to force its feature representation to match the corresponding statistical model. That means we need to jointly conduct density estimation and representation learn-ing. If we can jointly learn visual representations and op-timize statistical models end-to-end, we can get good fea-ture representations falling into distributions of the corre-sponding visual categories. Besides, we exploit knowledge distillation as done in [6] to learn robust feature represen-tation. In this way, we can describe the complex training distribution with multiple Gaussian mixture models while not impairing the generalization ability of visual features.
In practice, to learn visual concepts that are in com-plex distributions, we build a Gaussian mixture model (GMM [39]) for each visual category. Given input samples, we extract their deep features and project these features into a high-dimensional attribute space. Different from classi-cal Gaussian mixture models that send the same input into
K different Gaussian models, we divide the attribute space into multiple groups and build a Gaussian model in each group independently. This strategy can give every attribute group a clear learning target and lead to better convergence.
Experimental results indicate that this strategy works quite well. After the visual representation learning and statisti-cal model parameters optimization, we can directly aggre-gate these statistical models to judge whether a test sample comes from the training distribution or not. To evaluate our
OOD detector, we split ImageNet into ten folds randomly and select one of these splits as the training set and all other splits as the OOD dataset to conduct extensive tests. Exper-iments indicate that the proposed method has a strong abil-ity to identify OOD samples. We also evaluate our method on seven popular OOD benchmarks. Experimental results demonstrate that the proposed method not only can identify
OOD samples efﬁciently but also improves the discrimina-tive ability of learned visual representations.
The contributions of this paper are summarized as fol-lows:
• We introduce a new out-of-distribution detection scheme, called hierarchical visual category modeling to conduct joint representation learning and density estimation. It provides a new perspective for out-of-distribution detection to learn visual representation and probability models end-to-end.
• We exploit multiple Gaussian mixture models to model visual concepts in complex distributions. Visual at-tributes are divided into subgroups and modeled by different Gaussian components, which makes param-eter learning much more efﬁcient.
• We conducted comprehensive experiments and ab-lation studies on popular benchmarks to investigate the effectiveness of the proposed method. Experi-ments demonstrated that our out-of-distribution detec-tion models achieve better performance clearly when compared with previous methods. 2.