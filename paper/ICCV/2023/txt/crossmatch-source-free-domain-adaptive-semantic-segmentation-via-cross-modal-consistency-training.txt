Abstract
Source-free domain adaptive semantic segmentation has gained increasing attention recently. It eases the require-ment of full access to the source domain by transferring knowledge only from a well-trained source model. How-ever, reducing the uncertainty of the target pseudo labels becomes inevitably more challenging without the supervi-sion of the labeled source data. In this work, we propose a novel asymmetric two-stream architecture that learns more robustly from noisy pseudo labels. Our approach simul-taneously conducts dual-head pseudo label denoising and cross-modal consistency regularization. Towards the for-mer, we introduce a multimodal auxiliary network during training (and discard it during inference), which effectively enhances the pseudo labels’ correctness by leveraging the guidance from the depth information. Towards the latter, we enforce a new cross-modal pixel-wise consistency be-tween the predictions of the two streams, encouraging our model to behave smoothly for both modality variance and image perturbations. It serves as an effective regularization to further reduce the impact of the inaccurate pseudo la-bels in source-free unsupervised domain adaptation. Exper-iments on GTA5 → Cityscapes and SYNTHIA → Cityscapes benchmarks demonstrate the superiority of our proposed method, obtaining the new state-of-the-art mIoU of 57.7% and 57.5%, respectively. 1.

Introduction
Semantic segmentation predicts pixel-level category la-bels to given scenes. Although deep neural networks have been widely adopted, attaining state-of-the-art performance relies mainly on the assumption that the training and test-ing data follow the same distribution [62, 32, 33]. This as-sumption is impractical as target scenarios often exhibit a
*The corresponding authors.
Figure 1. Comparison of our proposed framework with existing depth-aware semantic segmentation models. (a) Prior art mostly adopts a multitask learning framework by adding depth estimation as an auxiliary task. (b) We introduce a multimodal auxiliary net-work that takes depth modality as an additional input for effective pseudo label denoising and consistency regularization. distribution shift, e.g., street scenes collected under a cross-city [11] or cross-weather [44] environment. Unsupervised domain adaptation (UDA) techniques have been proposed to address the domain shift problem, which aim at transfer-ring the knowledge learned from a labeled source domain to an unlabeled target domain [48, 50, 69, 67]. However, one major limitation of such UDA approaches lies in the re-quirement for full access to the source dataset. In practice, the source data may be restricted from being shared due to proprietary, privacy, or profit related concerns [26].
To cope with data sharing restrictions, recent efforts have investigated source-free domain adaptation, which trans-fers knowledge from a well-trained source model (rather than from the source data itself) to an unlabeled target do-main [39, 31]. Early solutions introduce a generator to es-timate the source domain based on the pre-trained source model [31], which can be used to generate fake source sam-ples for supervision as in typical UDA. However, due to the
lack of supervision from the real source domain, advanced techniques designed for typical UDA, such as depth-aware semantic segmentation and pseudo label denoising meth-ods, may work less satisfactorily in a source-free setting.
With the above insights, we propose a novel two-stream segmentation network for source-free UDA. As shown in
Figure 1 (a), existing depth-aware semantic segmentation for typical UDA mainly adopts a multitask learning frame-work where depth estimation is modeled as an auxiliary task [51, 53]. However, we observe through experiments that the regularization induced by the auxiliary task is quite limited for source-free UDA due to the lack of ground-truth semantic labels. It cannot effectively prevent the main seg-mentation network from overfitting to the incorrect over-confident pseudo labels of the target images. To solve this problem, we alternatively propose a multimodal auxiliary network, as shown in Figure 1 (b), which takes the depth information and the intermediate representations generated by the main stream image encoder as the input. We train both the main and the auxiliary streams on the segmentation task via self-training, and formulate an explicit cross-modal consistency loss between the output of the two streams for effective regularization. The benefits of our proposed seg-mentation network are threefold:
First, our inference-stage model consists of the main stream only, which is a unimodal model that infers from
RGB images the same way as existing models. Second, the asymmetric design of our neural network introduces modality variance in addition to the typical input pertur-bations produced by data augmentation, dropouts, etc. On one hand, the auxiliary network better rectifies the pseudo labels with multimodal knowledge expansion [61]. On the other hand, the cross-modal consistency effectively trans-fers the knowledge learned from the multimodal auxiliary network to the unimodal main network. Third, our pro-posed framework has better feasibility compared to exist-ing depth-aware UDA as ours only requires the depth in-formation in the target domain. Without annotation cost, the depth information can be easily learned from video se-quences or stereo images based on self-supervised depth es-timation models [17, 71, 53]. Here we summarize our con-tributions as follows:
• We propose a novel source-free UDA framework by introducing a multimodal auxiliary network. It models the correlations between depth and semantics, and can be discarded completely at inference time.
• We enforce a cross-modal consistency between the predictions of the main and auxiliary streams with dual-head pseudo label denoising, to reduce the impact of inaccurate pseudo labels in source-free UDA.
• Our proposed method outperforms the prior art by a significant margin, obtaining an mIoU of 57.7% and 57.5% on the Cityscapes dataset when adapting from the GTA5 and SYNTHIA benchmarks, respectively. 2.