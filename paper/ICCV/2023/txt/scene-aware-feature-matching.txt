Abstract
Current feature matching methods focus on point-level matching, pursuing better representation learning of indi-vidual features, but lacking further understanding of the scene. This results in signiﬁcant performance degradation when handling challenging scenes such as scenes with large viewpoint and illumination changes. To tackle this prob-lem, we propose a novel model named SAM, which applies attentional grouping to guide Scene-Aware feature Match-ing. SAM handles multi-level features, i.e., image tokens and group tokens, with attention layers, and groups the im-age tokens with the proposed token grouping module. Our model can be trained by ground-truth matches only and pro-duce reasonable grouping results. With the sense-aware grouping guidance, SAM is not only more accurate and ro-bust but also more interpretable than conventional feature matching models. Sufﬁcient experiments on various appli-cations, including homography estimation, pose estimation, and image matching, demonstrate that our model achieves state-of-the-art performance. 1.

Introduction
Feature matching, which refers to ﬁnding the correct cor-respondence between two sets of features, is a fundamen-tal problem for many computer vision tasks, such as object recognition [17], structure from motion (SFM) [28], and simultaneous localization and mapping (SLAM) [9]. But with illumination changes, viewpoint changes, motion blur and occlusion, it is challenging to ﬁnd invariance and obtain robust matches from two images.
The classic image matching pipeline generally consists of four parts: (1) feature detection (2) feature description (3) feature matching (4) outlier ﬁltering. For feature de-tection, keypoints that have distinguishable features to fa-cilitate matching are detected. For feature description, de-scriptors are extracted based on keypoints and their neigh-borhoods. The keypoint positions and the corresponding
*Corresponding author. group-level  correspondence guiding point-level  correspondence
Figure 1. An illustration of the grouping (top) and matching (bot-tom) result of our proposed method. Points in corresponding re-gions in the two images are correctly assigned to the same group, and the grouping information provides scene-aware guidance for point-level feature matching. descriptors are employed as features of the image. Then the feature matching algorithm is applied to ﬁnd the correct cor-respondence in two sets of extracted features. Finally, the outlier ﬁltering algorithm is applied to identify and reject outlier matches based on the obtained matches.
The current dominant approaches for image matching are learning-based descriptors with attention-based feature matching models. Learning-based descriptors extract local features with better representation capabilities by convo-lutional neural networks (CNN). Attention-based networks can enhance local features by perceiving global informa-tion and modeling contextual relationships between local features. While the above feature matching pipeline is the dominant method, the model performance still degrades sig-niﬁcantly when dealing with extreme cases, such as scenes with large viewpoint changes and illumination changes. Be-cause current methods only ﬁnd correspondences at the low level, i.e., point-level textures, and do not incorporate scene-aware information, such as grouping information, semantic information, etc. Therefore, intra- and inter-image grouping is introduced to SAM to guide point-level attention-based matching.
In this work, we take point-level descriptors as image
tokens and additionally introduce the concept of group to-kens, which are selected from image tokens by the proposed group token selection module. Each group token represents a group of features shared by two images, and we intend to assign the corresponding points in both images to the same group, while the points that do not correspond to each other are assigned to different groups. We apply Transformer to model the relationship between image tokens and group to-kens for intra- and inter-images, and propose a token group-ing module to assign image tokens to different groups based on similarity. A novel multi-level score strategy is pro-posed to utilize the scene-aware grouping information to give guidance on point-level features, and to obtain reason-able grouping results relying only on ground-truth match supervision.
In summary, the contributions of this paper include:
• We propose a novel feature matching model SAM, which allows feature matching to rely on more than point-level textures by introducing group tokens to construct scene-aware features.
• The multi-level feature attention encoder and token grouping module are proposed to enable image tokens and group tokens to perceive global context informa-tion and assign image tokens to different groups.
• We are the ﬁrst to utilize only ground-truth match su-pervision to enable the feature matching model to per-form the scene-aware grouping and matching through the proposed multi-level score.
• SAM achieves state-of-the-art performance in multiple experiments while demonstrating outstanding robust-ness and interpretability. 2.