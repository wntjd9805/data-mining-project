Abstract
Human driver can easily describe the complex traffic scene by visual system. Such an ability of precise percep-tion is essential for driver’s planning. To achieve this, a geometry-aware representation that quantizes the physical 3D scene into structured grid map with semantic labels per cell, termed as 3D Occupancy, would be desirable. Com-pared to the form of bounding box, a key insight behind oc-cupancy is that it could capture the fine-grained details of critical obstacles in the scene, and thereby facilitate subse-quent tasks. Prior or concurrent literature mainly concen-trate on a single scene completion task, where we might argue that the potential of this occupancy representation might obsess broader impact. In this paper, we propose Oc-cNet, a multi-view vision-centric pipeline with a cascade and temporal voxel decoder to reconstruct 3D occupancy.
At the core of OccNet is a general occupancy embedding to represent 3D physical world. Such a descriptor could be applied towards a wide span of driving tasks, including detection, segmentation and planning. To validate the effec-tiveness of this new representation and our proposed algo-rithm, we propose OpenOcc, the first dense high-quality 3D occupancy benchmark built on top of nuScenes. Empirical experiments show that there are evident performance gain across multiple tasks, e.g., motion planning could witness a collision rate reduction by 15%-58%, demonstrating the superiority of our method.
Figure 1. Scene as Occupancy. Representing objects as ViDAR (a) or 3D occupancy (b) has been endorsed by industry [1, 2]. due to the fact that conventional 3D bounding box cannot describe in detail irregular vehicles in daily driving scenes, e.g., protruding tail in (a) or (c). Defining the 3D world as Occupancy in (d) serves better to represent obstacles and avoid collision. In this paper, we envision Occupancy as a general Scene Descriptor as in (e) for a wide span of driving tasks beyond detection, such as planning, and witness performance gain compared to previous alternatives. 1.

Introduction
When you are driving on the road, how would you de-scribe the scene in 3D space through your eyes? Human driver can easily describe the environment by “There is a
Benz on the left side of my car in around 5 inches”, “There is a truck carrying huge protruding gas pipe on the rear, in around 50 meters ahead” and so on. Having the ability to describe the real world in a “There is” form is essential for making safe autonomous driving (AD) a reality. This is non-trivial for vision-centric AD systems due to the diverse range of entities present in the Scene, including vehicles such as cars, SUVs, and construction trucks, as well as static barriers, pedestrians, background buildings and vegetation.
Quantizing the 3D scene into structured cells with semantic labels attached, termed as 3D Occupancy, is an intuitive so-lution, and this form is also advocated in the industry com-munities such as Mobileye [1] and Tesla [2] . Compared to the 3D box that oversimplifies the shape of objects, 3D oc-cupancy is geometry-aware, depicting different objects and background shapes via the 3D cube collections with differ-ent geometric structure. As illustrated in Figure 1(c-d), 3D box can only describe the main body of the construction vehicle, while 3D occupancy can preserve the detail of its crane arm. Other conventional alternatives, such as point cloud segmentation and bird’s-eye-view (BEV) segmenta-tion, while being widely deployed in the context of AD, have their limitations in cost and granularity, respectively.
A detailed comparison can be referred in Table 1. Such ev-ident advantages of 3D occupancy encourage an investiga-tion into its potential for augmenting conventional percep-tion tasks and downstream planning.
Similar works have discussed 3D occupancy at an initial stage. Occupancy grid map, a similar concept in Robotics, is a typical representation in mobile navigation [29] but only serves as the search space of planning. 3D semantic scene completion (SSC) [33] can be regarded as a perception task to evaluate the idea of 3D occupancy. Exploiting temporal information as geometric prior is intuitive for the vision-centric models to reconstruct the geometry-aware 3D occu-pancy, yet previous attempts [19, 21, 5, 26] have failed to address this. A coarse-to-fine approach is also favorable in improving 3D geometric representation at affordable cost,
In while it is ignored by one-stage methods [19, 26, 5]. addition, the community still seeks a practical approach to evaluate 3D occupancy in a full-stack autonomous driving spirit as vision-centric solutions [16, 8] prevail.
Towards these issues aforementioned, we propose Oc-cNet, a multi-view vision-centric pipeline with a cascade voxel decoder to reconstruct 3D occupancy with the aid of temporal clues, and task-specific heads supporting a wide range of driving tasks. The core of OccNet is a compact and representative 3D occupancy embedding to describe the 3D scene. To achieve this, unlike straightforward voxel fea-ture generation from image features or sole use of BEV fea-ture as in previous literature [22, 7, 37], OccNet employs a cascade fashion to decode 3D occupancy feature from
BEV feature. The decoder adopts a progressive scheme to recover the height information with voxel-based temporal self-attention and spatial cross-attention, bundled alongside a deformable 3D attention module for efficiency. Equipped with such a 3D occupancy descriptor, OccNet simultane-ously supports general 3D perception tasks and facilitates downstream planning task, i.e., 3D occupancy prediction, 3D detection, BEV segmentation, and motion planning. For fair comparison across methods, we build OpenOcc, a 3D occupancy benchmark with dense and high-quality annota-Representation
Output space
Foreground object