Abstract
Core-Set
ReDAL
We propose SeedAL, a method to seed active learning for efficient annotation of 3D point clouds for semantic seg-mentation. Active Learning (AL) iteratively selects rele-vant data fractions to annotate within a given budget, but requires a first fraction of the dataset (a ’seed’) to be al-ready annotated to estimate the benefit of annotating other data fractions. We first show that the choice of the seed can significantly affect the performance of many AL meth-ods. We then propose a method for automatically con-structing a seed that will ensure good performance for AL.
Assuming that images of the point clouds are available, which is common, our method relies on powerful unsuper-vised image features to measure the diversity of the point clouds. It selects the point clouds for the seed by optimiz-ing the diversity under an annotation budget, which can be done by solving a linear optimization problem. Our exper-iments demonstrate the effectiveness of our approach com-pared to random seeding and existing methods on both the
S3DIS and SemanticKitti datasets. Code is available at https://github.com/nerminsamet/seedal. 1.

Introduction
We are interested in the efficient annotation of sparse 3D point clouds (as captured indoors by depth cameras or out-doors by automotive lidars) for semantic segmentation.
Modern AI systems require training on large annotated datasets to reach a high performance on such a task. As annotating is costly (more than capturing the data itself), several approaches have been proposed to achieve a more frugal learning, such as semi-supervision [71], weak super-vision [88], few-shot [52] and zero-shot learning [56], self-supervision [37, 1] and, as studied here, active learning [57].
Active learning (AL) methods iteratively select relevant fractions of a dataset to be annotated within a given budget so that, after a few iterations, the model learned on the an-notated fractions reaches a performance close to the perfor-mance of the model learned on the fully-annotated dataset,
)
% (
U o
I m
)
% (
U o
I m 40 30 20 40 30 20 50 40 30 20 random random avg.
SeedAL random random avg.
SeedAL 3 5 7 9 3 5 7 9
MC Dropout
Segment Entropy 40 30 20 random random avg.
SeedAL random random avg.
SeedAL 3 5 7 9 3 5 7 9
% of labeled points
% of labeled points
Figure 1: Impact of active learning seed on performance.
We show the variability of results obtained with 20 different random seeds (blue dashed lines), within an initial annota-tion budget of 3% of the dataset, when using various active learning methods for 3D semantic segmentation of S3DIS.
We compare it to the result obtained with our seed selection strategy (solid red line), named SeedAL, which performs better or on par with the best (lucky) random seeds among 20, and “protects” from very bad (unlucky) random seeds. although at a much lower annotation cost. This AL selec-tion typically targets the most uncertain [38] or most di-verse [62] data, that are assumed to have the most positive impact when annotated and used for training.
The criteria used in AL methods for selecting data to an-notate generally require a preliminary fraction of the dataset to be already annotated. Assuming this fraction is represen-tative enough of the rest of the dataset, it can be used to esti-mate the benefit of annotating other data fractions. But such a preliminary fraction is not available at the first AL itera-tion, when confronted to a completely unannotated dataset.
There is thus a cold start problem, which is to determine a first fraction of the dataset to annotate — the seed.
Initial set. Nevertheless, most AL publications pay little attention to the choice of this seed before iterating their AL method. They just pick a random fraction of the dataset. Al-though results are sometimes presented averaged over a few runs, it can be insufficient given the high variance level. As shown in Figure 1, it is particularly true in our 3D semantic segmentation context. Besides, nothing prevents in practice from drawing an “unlucky” seed, leading to significant un-derperformance or annotation overheads.
To address this issue, our method, named SeedAL, au-tomatically selects extremely good AL seeds, and thus also protects from drawing unlucky seeds, as illustrated on Fig-ure 1. Only few approaches have recently been proposed to construct such good seeds to get AL on track right away
[84, 54, 40, 29, 48, 81, 13, 50], and they mostly regard text or image classification. To the best of our knowledge, no such AL seeding approach exists for 3D point clouds.
Self-supervised features, for text or images [20, 31, 12], are the key to most of these approaches. It thus seems nat-ural to use self-supervised 3D features in our case. But existing pre-trained 3D backbones are not as versatile as their 2D counterpart, and provide lower quality features (see
Sect. 3.3). Our approach to seed AL for 3D point cloud is to leverage high-quality pretrained features [12] for images that are views of the scanned scenes.
As we rely on image features like [54, 40, 29, 81, 13], it seems natural as well to reuse these AL seeding methods for 3D scenes. But a direct transposition to 3D is not sensi-ble because, in our case, the initial budget for the seed is not given in terms of number of images but in the size of scenes (number of 3D points): selecting a scene just because it con-tains a single image of interest would be suboptimal.
Our approach integrates scenes and views to make a better use of the initial annotation budget. While 2D ap-proaches use feature clustering to guide seed selection, we show it is suboptimal in our context (2D views of 3D scenes) and propose a better formulation based on binary linear programming. As our algorithm has a higher com-plexity than clustering, we also develop heuristics to scale to large datasets by first extracting a small-enough pool of good candidates to select from.
Last, we observe that usual pretrained features, generally created from object-centric datasets such as ImageNet, fail to convey the diversity of complex multi-object scenes, by giving too much importance to big or repeated objects. We address this issue by analyzing images at patch level.
Contributions. To the best of our knowledge, we are the first to address AL seeding for 3D point clouds:
• We study the sensitivity of AL seeding for the 3D se-mantic segmentation of point clouds and the relevance of 2D images features to select 3D scenes.
• Leveraging these studies, we propose a general ap-proach to seed any AL method for point clouds origi-nating from scenes with available image views.
• We present experimental evidence of the effectiveness of our AL seeding method, which consistently im-proves over random seeds and 2D-inspired baselines, requiring less iterations (thus less annotations) and/or reaching higher segmentation performance. 2.