Abstract
Data-free knowledge distillation (DFKD) is a promis-ing approach for addressing issues related to model com-pression, security privacy, and transmission restrictions.
Although the existing methods exploiting DFKD have achieved inspiring achievements in coarse-grained clas-siﬁcation, in practical applications involving ﬁne-grained classiﬁcation tasks that require more detailed distinctions between similar categories, sub-optimal results are ob-tained. To address this issue, we propose an approach called DFKD-FGVC that extends DFKD to ﬁne-grained vi-sual categorization (FGVC) tasks. Our approach utilizes an adversarial distillation framework with attention gen-erator, mixed high-order attention distillation, and seman-tic feature contrast learning. Speciﬁcally, we introduce a spatial-wise attention mechanism to the generator to syn-thesize ﬁne-grained images with more details of discrimi-native parts. We also utilize the mixed high-order attention mechanism to capture complex interactions among parts and the subtle differences among discriminative features of the ﬁne-grained categories, paying attention to both lo-cal features and semantic context relationships. Moreover, we leverage the teacher and student models of the distilla-tion framework to contrast high-level semantic feature maps in the hyperspace, comparing variances of different cat-egories. We evaluate our approach on three widely-used
FGVC benchmarks (Aircraft, Cars196, and CUB200) and demonstrate its superior performance. Code is available at https://github.com/RoryShao/DFKD-FGVC.git 1.

Introduction
Fine-grained visual categorization (FGVC) aims at dis-tinguishing subcategories from father categories, e.g., sub-categories of birds [43], aircraft [29], and cars [23]. It has
*Corresponding authors. This work was supported in part by National
Natural Science Foundation of China under Grant (No. 62072182, No. 92270119, and No. 62172261) and Key Laboratory of Advanced Theory and Application in Statistics and Data Science, Ministry of Education. long been considered a more challenging issue than tradi-tional image classiﬁcation due to the subtle inter-class and large intra-class variations [42]. To distinguish subtle diver-sities, the current approaches commonly exploit deeper neu-ral networks with elaborate designs [50, 26, 53] to excavate the discriminative features effectively. Inevitably, the net-work becomes more and more complex, which leads to an-other problem, i.e., complicated networks are not easily de-ployed on embedded or mobile devices. Besides, the train-ing data of released pre-trained models are often unavailable due to transmission, privacy, or legal issues. For example, pre-trained models commonly need a large amount of data such as ImageNet [24]. If the data is transmitted directly, a large amount of bandwidth is consumed. Moreover, some sensitive data such as e-commerce items or medical data are usually not directly accessible to the public due to in-tellectual property rights or privacy protection considera-tions. To obtain a lightweight model, recent research has made signiﬁcant progress, including pruning [25], quanti-zation [49, 27], and knowledge distillation [16]. Among them, knowledge distillation (KD) is a popular and effec-tive paradigm for model compression and knowledge trans-fer [16]. It works by transferring knowledge from a cum-bersome teacher network to a lightweight student network.
Thanks to this separable architecture, it can also be used to solve privacy protection in data-free scenarios, which is called data-free knowledge distillation (DFKD) [5] or zero-shot knowledge distillation (ZSKD) [33].
Fortunately, a series of DFKD methods have been pro-posed [5, 33, 30, 11, 47, 12]. The existing approaches can be divided into two paradigms. The ﬁrst paradigm is based on the category distribution, which exploits the out distribu-tion of teacher and student to optimize the student and gen-erator, e.g., DFAL [5], ZSKT [30], DFAD [11], ZSKD [33].
Such a paradigm commonly fails to generate realistic sam-ples due to the lack of semantic-related information, es-pecially when it comes to complex samples. The second paradigm is based on prior distribution, which exploits the prior information (i.e., BatchNorm) to optimize synthetic images for distillation, e.g., MAD [10], CMI [12], DFQ [8],
ADI [47]. This paradigm can produce realistic features and, therefore, gives the student a noticeable improvement.
Although the existing methods have achieved inspiring achievements in coarse-grained classiﬁcation, in practical applications, sub-optimal results are achieved due to the subtle variations widely found in different scenarios. The main reasons for this situation are as follows: Firstly, for
FGVC tasks, the variances of the same category are more prominent than that of coarse-grained classiﬁcation due to different factors, such as viewing angles, lighting, back-grounds, occlusion, etc. Secondly, compared to coarse-grained categories, the feature discrepancies of different categories in FGVC are not obvious. Besides, in the data-free scenario, the model can not access the raw data directly.
For synthesized images, it is difﬁcult for the teacher model to capture the subtle variances of discriminative features. To our best knowledge, there are still no specialized data-free studies on ﬁne-grained DFKD. Therefore, this inspires us to explore this issue and tackle this task in a data-free scenario.
In this paper, we tackle this issue by extending DFKD to ﬁne-grained visual classiﬁcation (FGVC) tasks and propose an approach named DFKD-FGVC, which is achieved by exploiting the adversarial distillation frame-work with attention generator, mixed high-order attention distillation (MHAD) and semantic feature contrast learn-ing (SFCL). Concretely, as shown in Fig. 1, to promote the generator to synthesize more ﬁne-grained images, we ex-ploit the generator with spatial-wise attention, which can help the generator synthesize the images with more details of discriminative parts. Then, to fully mine the knowledge of discriminative features for student, we exploit the mixed high-order attention mechanism to capture complex inter-actions among parts and the subtle differences among dis-criminative features of the ﬁne-grained categories, paying attention to both local features and semantic context rela-tionships. Besides, to compare variances of different cate-gories, we skillfully exploit the teacher and student model of distillation framework to contrast semantic feature maps in the hyperspace. To verify our approach, massive ex-periments are conducted on three ﬁne-grained benchmarks, such as Aircraft, Cars196, and CUB200 to evaluate the ef-fectiveness of our approach.
In a nutshell, our contributions are four-fold: 1) We are the ﬁrst to propose an approach for FGVC in the data-free distillation scenario, which aims to optimize the en-tire generation and distillation stages to focus on discrim-inative features. 2) To synthesize more ﬁne-grained im-ages for adversarial distillation, we employ the generator with spatial-wise attention, which motivates the generator to synthesize the images with more details of discrimina-tive features. 3) Particularly, to effectively mine the po-tential semantic features and contextual relationships of the
ﬁne-grained categories, we provide two strategies, namely,
MHAD and SFCL, both of which can promote the perfor-mance of DFKD from different dimensions. 4) Extensive experiments demonstrate the effectiveness of our approach in the data-free setting, which achieves state-of-the-art per-formance on the mainstream FGVC benchmark datasets. 2.