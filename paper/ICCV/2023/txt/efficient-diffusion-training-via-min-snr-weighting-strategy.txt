Abstract
Denoising diffusion models have been a mainstream approach for image generation, however, training these models often suffers from slow convergence.
In this pa-per, we discovered that the slow convergence is partly due to conﬂicting optimization directions between timesteps.
To address this issue, we treat the diffusion training as a multi-task learning problem, and introduce a simple yet effective approach referred to as Min-SNR-γ. This method adapts loss weights of timesteps based on clamped signal-to-noise ratios, which effectively balances the con-ﬂicts among timesteps. Our results demonstrate a signif-icant improvement in converging speed, 3.4× faster than previous weighting strategies.
It is also more effective, achieving a new record FID score of 2.06 on the Ima-geNet 256 × 256 benchmark using smaller architectures than that employed in previous state-of-the-art. The code is available at https://github.com/TiankaiHang/Min-SNR-Diffusion-Training. 1.

Introduction
In recent years, denoising diffusion models [48, 19, 57, 36] have emerged as a promising new class of deep gener-ative models due to their remarkable ability to model com-plicated distributions. Compared to prior Generative Ad-versarial Networks (GANs), diffusion models have demon-strated superior performance across a range of generation tasks in various modalities, including text-to-image gener-ation [40, 43, 41, 17], image manipulation [26, 34, 4, 56], video synthesis [18, 47, 22], text generation [28, 16, 59], 3D avatar synthesis [39, 53], etc. A key limitation of present denoising diffusion models is their slow conver-gence rate, requiring substantial amounts of GPU hours for
*Corresponding authors. 30 20 10
D
I
F baseline ours 3.4× speedup 200 400 600 800 1000
Training Iterations (K)
Figure 1: By leveraging a non-conﬂicting weighting strat-egy, our method can converge 3.4 times faster than baseline, resulting in superior performance. training [41, 40]. This constitutes a considerable challenge for researchers seeking to effectively experiment with these models.
In this paper, we ﬁrst conducted a thorough examina-tion of this issue, revealing that the slow convergence rate likely arises from conﬂicting optimization directions for dif-ferent timesteps during training.
In fact, we ﬁnd that by dedicatedly optimizing the denoising function for a speciﬁc noise level can even harm the reconstruction performance for other noise levels, as shown in Figure 2. This indi-cates that the optimal weight gradients for different noise levels are in conﬂict with one another. Given that cur-rent denoising diffusion models [19, 12, 36, 41] employ shared model weights for various noise levels, the conﬂict-ing weight gradients will impede the overall convergence rate, if without careful consideration on the balance of these noise timesteps.
To tackle this problem, we propose the Min-SNR-γ loss weighting strategy. This strategy treats the denoising pro-cess of each timestep as an individual task, thus diffusion
training can be considered as a multi-task learning problem.
To balance various tasks, we assign loss weights for each task according to their difﬁculty. Speciﬁcally, we adopt a clamped signal-to-noise ratio (SNR) as loss weight to alle-viate the conﬂicting gradients issue. By organizing various timesteps using this new weighting strategy, the diffusion training process can converge much faster than previous ap-proaches, as illustrated in Figure 1.
Generic multi-task learning methods usually seek to mit-igate conﬂicts between tasks by adjusting the loss weight of each task based on their gradients. One classical ap-proach [11, 46], Pareto optimization, aims to seek a gradient descent direction to improve all the tasks. However, these approaches differ from our Min-SNR-γ weighting strategy in three aspects: 1) Sparsity. Most previous studies in the generic multi-task learning ﬁeld have focused on scenarios with a small number of tasks, which differs from the diffu-sion training where the number of tasks can be up to thou-sands. As in our experiments, Pareto optimal solutions in diffusion training tend to set loss weights of most timesteps as 0. In this way, many timesteps will be left without any learning, and thus harm the entire denoising process. 2)
Instability. The gradients computed for each timestep in each iteration are often noisy, owing to a limited number of samples for each timestep. This hampers the accurate com-putation of Pareto optimal solutions. 3) Inefﬁciency. The calculation of Pareto optimal solutions is time-consuming, signiﬁcantly slowing down the overall training.
Our proposed Min-SNR-γ strategy is a predeﬁned global step-wise loss weighting setting, instead of run-time adap-tive loss weights for each iteration as in the original Pareto optimization, thus avoiding the sparsity issue. Moreover, the global loss weighting strategy eliminates the need for noisy computation of gradients and the time-consuming
Pareto optimization process, making it more efﬁcient and stable. Though suboptimal, the global strategy can be also almost as effective: Firstly, the optimization dynamics of each denoising task are largely shaped by the task’s noise level, without the need to account for individual samples too much. Secondly, after a moderate number of iterations, the gradients of the majority subsequent training process become more stable, thus it can be approximated by a sta-tionery weighting strategy.
To validate the effectiveness of the Min-SNR-γ weight-ing strategy, we ﬁrst compute its Pareto objective value and compare it with the optimal step-wise loss weights obtained by directly solving the Pareto problem. Together, we also compare it with several conventional loss weighting strate-gies, including constant weighting, SNR weighting, and
SNR with an lower bound. Figure 4 shows that our Min-SNR-γ weighting strategy produces Pareto objective val-ues almost as low as the optimal one, signiﬁcantly better than other existing works, indicating a signiﬁcant allevia-tion of the gradient conﬂicting issue. As a result, the pro-posed weighting strategy not only converges much faster than previous approaches, but is also effective and general for various generation scenarios. It achieves a new record of FID score 2.06 on the ImageNet 256×256 benchmark, and proves to also improve models using other prediction targets and network architectures.
Our contributions are summarized as follows:
• We have uncovered a compelling explanation for the slow convergence issue in diffusion training: a conﬂict in gradients across various timesteps.
• We have proposed a new loss weighting strategy for diffusion model training, which greatly mitigates the conﬂicting gradients across timesteps and results in a marked acceleration of convergence speed.
• We have established a new FID score record on the
ImageNet 256 × 256 image generation benchmark. 2.