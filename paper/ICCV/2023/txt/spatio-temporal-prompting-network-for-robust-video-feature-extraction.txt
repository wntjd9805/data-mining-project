Abstract
Frame quality deterioration is one of the main chal-lenges in the field of video understanding. To compen-sate for the information loss caused by deteriorated frames, recent approaches exploit transformer-based integration modules to obtain spatio-temporal information. However, these integration modules are heavy and complex. Fur-thermore, each integration module is specifically tailored for its target task, making it difficult to generalise to mul-tiple tasks.
In this paper, we present a neat and uni-fied framework, called Spatio-Temporal Prompting Net-work (STPN). It can efficiently extract robust and accu-rate video features by dynamically adjusting the input fea-tures in the backbone network. Specifically, STPN predicts several video prompts containing spatio-temporal informa-tion of neighbour frames. Then, these video prompts are prepended to the patch embeddings of the current frame as the updated input for video feature extraction. More-over, STPN is easy to generalise to various video tasks be-cause it does not contain task-specific modules. Without bells and whistles, STPN achieves state-of-the-art perfor-mance on three widely-used datasets for different video un-derstanding tasks, i.e., ImageNetVID for video object de-tection, YouTubeVIS for video instance segmentation, and
GOT-10k for visual object tracking. Codes are available at https://github.com/guanxiongsun/STPN 1.

Introduction
Video understanding is a fundamental research direction in the field of computer vision. It plays an important role in many real-world applications, such as autonomous driving
[64, 32], video surveillance [34, 5], and sports analysis [56, 25]. However, a significant challenge in this field is the deterioration of video frames due to motion blur, occlusion, and deformation, which makes it difficult to extract relevant information.
Figure 1. Comparisons between pipelines of (a) existing methods and (b) the proposed Spatio-temporal Prompting Network (STPN).
Existing methods introduce complex and task-specific integration modules (
). In contrast,
STPN is a unified framework for multiple tasks. A lightweight dy-) generates a set of DVPs namic video prompt (DVP) predictor (
). Best viewed in to adjust input before backbone networks ( colour.
) after backbone networks (
To overcome this challenge, inspired by the great suc-cess of transformers in many computer vision tasks [49, 54, 27, 19, 4], researchers explore various transformer-based in-tegration modules to alleviate the information loss on the deteriorated video frames. For example, in video object detection, transformer-based feature aggregation methods
[13, 7, 57, 43] are investigated to enhance the feature of pro-posals [40] in the detection head. In video instance segmen-tation, deteriorated frames usually cause wrong instance as-sociations in a sequence, so 3D mask decoders [55, 22, 8] are introduced to learn associations of instance masks in an end-to-end manner. For visual object tracking, traditional correlation filters [26, 47] meet their limitations in severely degraded frames. Therefore, transformer-based integration modules [11, 62, 6, 10] are proposed to better capture com-plicated correlations between the template region and the search region. To conclude, the summarised pipeline of re-cent methods is shown in Figure 1 (a). A backbone network extracts spatially-only features from the current frame and
the support frames in the same video. Then, different in-tegration modules integrate spatial-only features on multi-ple frames to obtain spatio-temporal features. Finally, the spatio-temporal features are used for the targeted video un-derstanding task.
While the current pipeline achieves good performance, there are still two major limitations of transformer-based integration modules. Firstly, these integration modules are involved as an extra component after the backbone network, leading to increased complexity and additional computa-tional costs. Secondly, each integration module is tailored specifically for the target task and thus cannot be gener-alised to multiple video tasks. Hence, we put forward the following question: can we remove the complex integration modules and directly obtain spatio-temporal information in backbone networks?
To answer this question, inspired by recent prompting techniques [24, 23, 28], in this paper, we present a neat and unified framework, called Spatio-Temporal Prompting Net-work (STPN). Instead of using complex integration mod-ules, STPN simplifies the current pipeline for video under-standing by introducing spatio-temporal information into the backbone network, as shown in Figure 1 (b). Specif-ically, given a video frame, we propose a dynamic video prompt (DVP) predictor to generate several video prompts according to support frames. Then, the predicted DVPs are prepended to the patch embeddings of the current frame as the updated input. Finally, a vision transformer backbone network extracts video features using the updated input for future video understanding tasks. It is worth noting that the
DVP predictor is a lightweight structure and introduces only a small number of extra parameters, e.g., 0.11M. Moreover,
STPN can be easily adapted to different video understand-ing tasks, since it does not contain task-specific modules, and all modifications happen before the backbone network.
In summary, our key contributions are: (1) We present the Spatio-Temporal Prompting Network (STPN) that can extract robust video features on deteriorated video frames.
STPN simplifies the current pipeline for video understand-ing and is easy to generalise to different video understand-ing tasks. (2) To the best of our knowledge, we are the first to explore promoting techniques for robust video fea-ture extraction on the task of video object detection (VOD), video instance segmentation (VIS), and visual object track-ing (VOT). (3) Without bells and whistles, STPN achieves state-of-the-art performance on three widely-used video un-derstanding benchmarks, i.e., ImageNet-VID [41] for VOD,
YouTube-VIS [63] for VIS, and GOT-10k [21] for VOT. 2.