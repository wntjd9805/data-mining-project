Abstract 1.

Introduction
Implicit neural representations (INRs) aim to learn a continuous function (i.e., a neural network) to represent an image, where the input and output of the function are pixel coordinates and RGB/Gray values, respectively. However, images tend to consist of many objects whose colors are not perfectly consistent, resulting in the challenge that image is actually a discontinuous piecewise function and cannot be well estimated by a continuous function. In this paper, we empirically investigate that if a neural network is en-forced to fit a discontinuous piecewise function to reach a fixed small error, the time costs will increase exponentially with respect to the boundaries in the spatial domain of the target signal. We name this phenomenon the exponential-increase hypothesis. Under the exponential-increase hy-pothesis, learning INRs for images with many objects will converge very slowly. To address this issue, we first prove that partitioning a complex signal into several sub-regions and utilizing piecewise INRs to fit that signal can signifi-cantly speed up the convergence. Based on this fact, we introduce a simple partition mechanism to boost the per-formance of two INR methods for image reconstruction: one for learning INRs, and the other for learning-to-learn
INRs. In both cases, we partition an image into different sub-regions and dedicate smaller networks for each part.
In addition, we further propose two partition rules based on regular grids and semantic segmentation maps, respec-tively. Extensive experiments validate the effectiveness of the proposed partitioning methods in terms of learning INR for a single image (ordinary learning framework) and the learning-to-learn framework. Code is released here.
*Corresponding author: Haishuai Wang (haishuai.wang@zju.edu.cn)
Recently, an innovative model for data/signal repre-sentation called implicit neural representations (INRs) has aroused researchers’ great attention, due to their remarkable visual performance in computer vision tasks, including im-age generation [29, 9, 31, 4] and novel views synthesis [12].
To fit such an implicit neural representation for a 2D im-age, we usually learn a continuous function formalized by a neural network, which takes space coordinates x ∈ R2 as input and outputs the color values at the queried coordinate (y ∈ R3 if RGB and y ∈ R if gray).
However, in-the-wild images are actually discontinuous piecewise functions. They consist of discrete objects with not perfectly consistent colors (as shown in Figure 1(a)).
Large gradients exist on the boundaries between two dis-continuous parts, preventing the neural network from con-verging to a small error when fitting images. To study the above issue, related research called “spectral bias” [19, 34] has proved that neural networks prioritize learning the low-frequency components. Yet, they only describe this phe-nomenon from the view of the implicit frequency domain and do not propose a quantitative relation between the con-vergence rate and the attribute of the target signal.
In this paper, we first re-examine the above phenomenon from the explicit spatial domain and empirically investigate a quantitative relation: the time complexity of fitting a dis-continuous piecewise function with a neural network would increase exponentially with respect to the number of bound-aries. For example, in Figure 1(b) and 1(c), we use SIREN
MLPs [29] to fit 1D synthetic signals and 2D synthetic sig-nals where N boundaries exist in their spatial domain. We then explore the relation between the required convergence step n and the number of boundaries N , and find that the relation curves align with the exponential function. We
Figure 1. (a) Discontinuous parts exist obviously in regions with red boxes, which motivates us to use piecewise functions to represent the complex signals. (b) & (c) We dedicate a single neural network to fit the 1D and 2D synthetic signals with N boundaries. The results show that the relation between the convergence step n and the number of boundaries N align with the exponential function n ∝ O(pN ), where p = 1.0656 for 1D synthetic signals and p = 1.00815 for 2D synthetic signals. The detailed experiment is presented in Appendix A. call this phenomenon the exponential-increase hypothesis.
Under this hypothesis, the optimization process of fitting a high-resolution in-the-wild image with a single continuous
INR will converge at a slow rate.
Based on the exponential-increase hypothesis, we math-ematically prove that partitioning images into several parts and learning INRs within each part can reduce the exponen-tial complexity to linear complexity and significantly de-crease the convergence time. In light of this fact, we pro-pose partition-based INR methods and utilize partition in two INR frameworks: one for learning INRs, and the other for learning-to-learn INRs. Specifically, in both frame-works, we partition an image into different sub-regions based on particular rules and dedicate smaller networks for each sub-region. We also propose two partition rules: one is based on regular grids, and the other is based on semantic segmentation maps. Both of them can speed up the conver-gence of learning INRs as well as learning-to-learn INRs.
In summary, the contributions of this work are as follows.
• From the view of spatial domains, we investigate the exponential relation between the network convergence rate and the number of boundaries in the target signals, namely the exponential-increase hypothesis.
• Based on the exponential-increase hypothesis, we mathematically prove that partition reduces the expo-nential complexity of fitting all boundaries to the linear complexity of fitting separate regions.
• We propose partition-based learning and learning-to-learn INRs frameworks for image reconstruction task.
We also propose two partition rules that are based on regular grids or semantic segmentation maps.
• Extensive experiments on image reconstruction show that (i) partition boosts learning INRs framework to faster convergence, (ii) partition boosts learning-to-learn INRs framework to better reconstruction perfor-mance with fixed optimization steps. 2.