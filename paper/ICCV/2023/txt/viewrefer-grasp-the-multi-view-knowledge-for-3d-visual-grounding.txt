Abstract
Understanding 3D scenes from multi-view inputs has been proven to alleviate the view discrepancy issue in 3D visual grounding. However, existing methods normally ne-glect the view cues embedded in the text modality and fail to weigh the relative importance of different views.
In this paper, we propose ViewRefer, a multi-view framework for 3D visual grounding exploring how to grasp the view knowledge from both text and 3D modalities. For the text branch, ViewRefer leverages the diverse linguistic knowl-edge of large-scale language models, e.g., GPT, to expand a single grounding text to multiple geometry-consistent de-scriptions. Meanwhile, in the 3D modality, a transformer fusion module with inter-view attention is introduced to boost the interaction of objects across views. On top of that, we further present a set of learnable multi-view pro-totypes, which memorize scene-agnostic knowledge for dif-ferent views, and enhance the framework from two perspec-tives: a view-guided attention module for more robust text features, and a view-guided scoring strategy during the fi-nal prediction. With our designed paradigm, ViewRefer achieves superior performance on three benchmarks and surpasses the second-best by +2.8%, +1.5%, and +1.35% on Sr3D, Nr3D, and ScanRefer. 1.

Introduction
The aim of visual grounding is to ascertain the precise location of an object from an image or a 3D scene accord-ing to given query texts. The field of 2D visual ground-ing [24, 44, 28] has undergone significant advances recently.
Meanwhile, the developments in embodied agent [36, 9], vision-language navigation [62, 40], and autonomous driv-* Equal Contribution. (cid:66) Corresponding authors
Figure 1: The Paradigm of ViewRefer. With our proposed LLM-expanded texts and multi-view prototypes, ViewRefer adopts a fu-sion transformer to effectively grasp view knowledge from multi-modal data, which achieves superior 3D grounding performance. ing [14] have also stimulated increasing attention on 3D vi-sual grounding [1, 6].
Inspired by the strategies in 2D counterparts, most 3D vi-sual grounding methods adopt a two-stage pipeline, which first detect all object proposals in the scene and then ground the target ones. Unlike 2D images with fixed object positions, the large-scale 3D scenes consist of irregular-distributed point clouds with intricate spatial information, which is view-invariant and causes more challenges. As dis-cussed in previous works [35, 22], one urging issue is view discrepancy, caused by the uncertain perspective between the intelligent agent (model) and the commander (ground-ing text giver). Given relative positions between objects, the text descriptions are supposed to change according to different viewpoints, e.g., when turning the view from “fac-ing” into “back to”, the “right” chair should be rectified as a
“left” one. Unfortunately, the public available datasets [1, 6] for 3D visual grounding only provide one text query corre-sponding to point clouds of uncertain viewpoints.
To alleviate such potential misalignment, existing meth-ods either manually align the 3D scenes to the paired texts [35], or simultaneously feed multiple views into the network for better view robustness [22]. However, these methods have two major limitations. Firstly, they only focus on solving the view dependence issue from the 3D modal-ity, while neglecting the lack of view cues within text in-put. Secondly, for the multi-view input, they introduce no specifically designed modules to capture the view knowl-edge, which is yet significant to discriminate the relative importance of each view. Therefore, we ask: Can we ex-plicitly grasp the view knowledge from both text and 3D modalities to further boost the 3D grounding performance?
To this end, we propose ViewRefer, a multi-view frame-work for 3D visual grounding, which captures sufficient view cues from both text and 3D modalities to understand the spatial inter-object relation. Our overall paradigm is shown in Figure 1. For the text modality, we leverage large-scale language models (LLMs) to expand the input grounding text with view-related descriptions. Such LLM-expanded texts can capture sufficient multi-view semantics inherited from LLMs’ linguistic knowledge and perform better grounding performance for the target objects. For the 3D modality, we take as input the multi-view 3D scenes and adopt a fusion transformer for 3D-text feature interactions.
In each block, we apply different attention mechanisms to exchange information across modalities, views, and objects, contributing to thorough multi-modal fusion. On top of that, we further introduce a set of learnable multi-view proto-types, which aims to capture the inter-view knowledge dur-ing training. The guidance of prototypes lies in two as-pects. The first complements input text features with adap-tive multi-view semantics, the second refines the final out-put by weighing the importance of different views. Both of them provide high-level guidance for multi-view visual grounding in ViewRefer.
To demonstrate the effectiveness of our approach, we evaluate its performance on three commonly used bench-marks, i.e., Sr3D [1], Nr3D [1] and ScanRefer [6], where
ViewRefer consistently achieves superior performance, sur-passing the second-best by +2.8%, +1.5%, +1.35%, re-spectively. The main contributions of our paper are sum-marized as follows:
• We propose ViewRefer, a multi-view framework for 3D visual grounding, which grasps view knowledge to alleviate the challenging view discrepancy issue.
• For the text and 3D modalities, we respectively in-troduce LLM-expanded grounding texts and a fusion transformer for capturing multi-view information.
• We present multi-view prototypes to provide high-level guidance to our framework, which contributes to superior 3D grounding performance. 2.