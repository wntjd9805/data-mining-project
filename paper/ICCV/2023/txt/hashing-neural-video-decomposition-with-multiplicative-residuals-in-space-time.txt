Abstract
We present a video decomposition method that facilitates layer-based editing of videos with spatiotemporally varying lighting and motion effects. Our neural model decomposes an input video into multiple layered representations, each comprising a 2D texture map, a mask for the original video, and a multiplicative residual characterizing the spatiotempo-ral variations in lighting conditions. A single edit on the tex-ture maps can be propagated to the corresponding locations in the entire video frames while preserving other contents’ consistencies. Our method efﬁciently learns the layer-based neural representations of a 1080p video in 25s per frame via coordinate hashing and allows real-time rendering of the edited result at 71 fps on a single GPU. Qualitatively, we run our method on various videos to show its effectiveness in generating high-quality editing effects. Quantitatively, we propose to adopt feature-tracking evaluation metrics for ob-jectively assessing the consistency of video editing. Project page: https:// lightbulb12294.github.io/ hashing-nvd/ 1.

Introduction
Unlike image editing, video editing involves modeling the frame-to-frame relationships and addressing temporal variations such as motion and illumination changes. The task of video editing becomes challenging with the added dimension of time for a user who attempts to apply edits to a video while ensuring consistency across all frames. It is more convenient if we can handle the spatiotemporally varying components and reduce video editing to image editing—The user thus only needs to do edits on images with ease, and the editing results can seamlessly propagate to the entire video.
To achieve this goal, we may consider incorporating ef-fective representations that can model and reconstruct the static and dynamic information in videos. Furthermore, for practical concerns, the algorithm must be efﬁcient enough in modeling and rendering to support interactive editing. Re-cent work on video decomposition has proposed to employ neural-based representations, such as layered neural atlases
[16] and deformable sprites [39], to enable the conversion between the space-time video domain and 2D texture do-main for editing. Despite their successes in showcasing impressive video editing effects, we notice that they often require longer training time or restrict to limited frame res-olution. For example, it takes more than ten hours to de-rive the layered neural atlases [16] from a 100-frame 480p video. Deformable sprites [39] are relatively fast to derive but require more computation resources (over 24GB of GPU memory for a 480p video) and thus are unsuitable for editing
Figure 2: The procedure of layer-based video editing. Our model allows users to apply edits to the extracted texture for rendering the edited video. high-resolution videos. We address the computation issue by incorporating hash grid encoding [24] into our framework and achieve fast training and rendering for high-resolution videos. Moreover, we introduce the multiplicative resid-ual estimator to model the lighting variations across video frames, which can improve the reconstruction quality and allow illumination-aware editing unachievable by prior work, as shown in Fig. 1.
We summarize the contributions of this work as follows: 1. This work is the ﬁrst to consider spatiotemporally vary-ing lighting effects for layered-based video editing. The proposed multiplicative-residual estimator effectively decomposes the lighting conditions from the video with-out supervision. Our method can produce better-quality videos by fusing the edits with expected illuminations. 2. Our approach is efﬁcient in both training and rendering.
Compared with prior work, the proposed method im-proves the training time with fewer resources and thus enables training on higher-resolution or longer videos.
The trained model can achieve fast video rendering via hashing-based coordinate inference. It takes about 40 minutes to train with a 1080p video of 100 frames on a single 3090 Ti GPU. The inference speed for rendering an edited video is 71 fps for 1080p resolution, allowing real-time video editing. 3. The experimental results demonstrate appealing video edits in various challenging contexts, such as modifying the texture of moving objects, handling occlusion, and manipulating camera motion, where all can be fused with vivid lighting and shading for better effects that are not easy to achieve by prior work. shown to be effective in estimating layered representations for video segmentation and video editing [3, 14, 16, 38, 39].
In this work, we also adopt neural networks to derive lay-ered representations from videos. Further, inspired by the pioneering work in visual tracking for handling illumina-tion changes [10], we incorporate a new multiplicative residual representation to model the lighting variations for illumination-aware video editing.
Video editing. Layered representations can beneﬁt video editing in various ways. For example, the layered repre-sentations can serve as a visual proxy for intuitive video editing [16] or can be used to create the retiming effects [21].
Editing can be more easily done on a uniﬁed texture map built from the video’s background, such as background mo-saics [7], tapestries [4], and layered neural atlases [16].
Building a uniﬁed texture map for the foreground object is also useful, e.g., ﬂexible sprites [15], unwrap mosaics [28], and deformable sprites [39]. Another type of decomposi-tion is to derive temporally-consistent intrinsic components from videos [27, 20] so that the edits can be performed on the reﬂectance for recoloring or texture transfer. Recent deep-learning-based methods mainly address a single task of video editing, e.g., video style transfer [34, 19, 18, 12, 29]. or category-speciﬁc GAN-based video editing [2, 25, 32, 37].
Regarding achieving consistent video editing, it is crucial to know the temporal correspondences [13, 5] so typical animation techniques like Rotoscoping [1] can be applied. 3. Approach 2.