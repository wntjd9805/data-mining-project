Abstract
Due to their highly structured characteristics, faces are easier to recover than natural scenes for blind image super-resolution. Therefore, we can extract the degrada-tion representation of an image from the low-quality and recovered face pairs. Using the degradation representa-tion, realistic low-quality images can then be synthesized to ﬁne-tune the super-resolution model for the real-world low-quality image. However, such a procedure is time-consuming and laborious, and the gaps between recovered faces and the ground-truths further increase the optimiza-tion uncertainty. To facilitate efﬁcient model adaptation to-wards image-speciﬁc degradations, we propose a method dubbed MetaF2N, which leverages the contained Faces to ﬁne-tune model parameters for adapting to the whole
Natural image in a Meta-learning framework. The degra-dation extraction and low-quality image synthesis steps are thus circumvented in our MetaF2N, and it requires only one ﬁne-tuning step to get decent performance. Consid-ering the gaps between the recovered faces and ground-truths, we further deploy a MaskNet for adaptively predict-ing loss weights at different positions to reduce the impact of low-conﬁdence areas. To evaluate our proposed MetaF2N, we have collected a real-world low-quality dataset with one or multiple faces in each image, and our MetaF2N achieves superior performance on both synthetic and real-world datasets.
Source code, pre-trained models, and collected datasets are available at https://github. com/yinzhicun/MetaF2N . 1.

Introduction
With the development of dataset construction, net-work design, and many other relevant methods, blind im-age super-resolution (SR) [9, 33] has acquired enormous progress in recent years. To construct pairwise low-/high-quality samples for training the image SR models, typically one can capture low- and high-quality pairs by adjusting the focal length of cameras [5, 55, 61] and shooting dis-tance [8], or synthesize low-quality images via degradation modeling [26, 51, 59]. However, these methods can only cover a limited and biased range of degradations, which is insufﬁcient for real-world applications. Besides, most ex-isting blind image SR methods [51, 59] train a static model for all testing scenarios, greatly limiting their ﬂexibility and generalization ability.
In order to break the restriction of limited training sets, self-supervised learning has been introduced to train a model for each low-quality image, without requiring pair-wise ground-truths [10, 42, 47]. Although these methods exhibit a great deal of ﬂexibility, they largely rely on cer-tain priors or assumptions, showing inferior image SR per-formance. Recently, Li et al. [26] reached a better com-promise on the requirement of ground-truths and proposed
ReDegNet by leveraging the faces contained in natural im-ages.
In speciﬁc, the face regions in a real-world low-quality natural image are processed via blind face SR meth-ods [36, 50, 57, 63], which have achieved appealing re-sults thanks to the highly structured characteristics of faces.
Then, the low-quality and recovered face pairs can be uti-lized to model the degradations in the image. Finally, more low-quality images are synthesized with the degradation representations for ﬁne-tuning the SR model. With the above design, ReDegNet [26] is ﬂexible to process a sin-gle image or a batch of images with diverse degradations.
Although ReDegNet [26] achieves superior SR perfor-mance, especially in speciﬁc scenarios (i.e., ﬁne-tuning with faces within the test image), there are still several limitations. On the one hand, the training procedure is time-consuming and computationally intensive, where the modules for degradation representation extraction and low-quality image synthesis are jointly optimized. Furthermore, when the model is intended to deal with a single low-quality image, it is difﬁcult to determine when to terminate the training process for avoiding under-ﬁtting or over-ﬁtting.
On the other hand, even though the faces processed by the blind face restoration methods show appealing quality, there inevitably exist gaps between the recovered faces and the real ground-truths. Such gaps may affect the degradation representation accuracy and further hamper the training of the SR model. These problems have a large impact on the
SR performance, especially when the model is ﬁne-tuned for image-speciﬁc super-resolution.
To remedy the aforementioned problems regarding train-ing efforts and data gaps, we propose an efﬁcient and effec-tive method dubbed MetaF2N. In speciﬁc, the SR model directly ﬁne-tunes the parameters from the low-quality and recovered face pairs, then uses the ﬁne-tuned parameters to process the whole natural image. As such, the cumber-some degradation extraction and low-quality image synthe-sis steps are circumvented in our method, which avoids the interference of degradation modeling errors.
In order to stabilize and accelerate the ﬁne-tuning process during in-ference, we adopt the model-agnostic meta-learning [11] framework in the training phase, resulting in a much more practical model adaptation procedure that can be ﬁnished in a single ﬁne-tuning step.
Additionally, considering the gaps between low-quality faces and that processed by blind face restoration methods, we argue that treating all pixels equally will magnify the ef-fect of the errors. Therefore, we further deploy a MaskNet, which adaptively predicts loss weights for different posi-tions, to fulﬁll that the recovered face regions more similar to the ground-truths are assigned with higher loss weights.
Ideally, the weight map should be extracted from the re-covered faces and the ground-truths. Unfortunately, the ground-truths are unavailable during inference. Consider-ing that predicting the loss weight map is similar to the im-age quality assessment (IQA) task, following the idea of degraded-reference IQA methods [2, 62], the MaskNet in-stead takes the low-quality and recovered faces as input.
According to our observation, typically the areas closer to the ground-truth are assigned with larger weights and vice versa, which is consistent with the intuitions.
For evaluating the proposed MetaF2N, we have con-structed several synthetic datasets based on FFHQ [21] and CelebA [34]. To further show the effectiveness under real-world scenarios, we have also collected a real-world low-quality image dataset from the Internet and existing datasets, namely RealFaces200 (RF200), which contains a single face or multiple faces in each image. Extensive ex-periments show that our MetaF2N achieves superior perfor-mance on both synthetic and real-world datasets.
In summary, the contribution of this paper includes,
• We propose an efﬁcient and effective method dubbed
MetaF2N for blind image super-resolution, which takes advances of blind face restoration models and learns model adaptation from the face regions with only one ﬁne-tuning step.
• Considering that the gaps between faces recovered by blind face restoration methods and the ground-truth ones may result in an inaccurate degradation model-ing, a MaskNet is introduced for conﬁdence prediction to mitigate the effect of the gaps.
• A real-world low-quality image dataset with one or multiple faces in each image is collected, which will be helpful for face-guided blind image super-resolution. 2.