Abstract
Video Temporal Grounding (VTG), which aims to ground target clips from videos (such as consecutive intervals or disjoint shots) according to custom language queries (e.g., sentences or words), is key for video browsing on so-cial media. Most methods in this direction develop task-specific models that are trained with type-specific labels, such as moment retrieval (time interval) and highlight de-tection (worthiness curve), which limits their abilities to generalize to various VTG tasks and labels.
In this pa-per, we propose to Unify the diverse VTG labels and tasks, dubbed UniVTG, along three directions: Firstly, we revisit a wide range of VTG labels and tasks and define a uni-fied formulation. Based on this, we develop data annota-tion schemes to create scalable pseudo supervision. Sec-ondly, we develop an effective and flexible grounding model capable of addressing each task and making full use of each label. Lastly, thanks to the unified framework, we are able to unlock temporal grounding pretraining from large-scale diverse labels and develop stronger grounding abilities e.g., zero-shot grounding. Extensive experiments on three tasks (moment retrieval, highlight detection and video summarization) across seven datasets (QVHighlights,
Charades-STA, TACoS, Ego4D, YouTube Highlights, TV-Sum, and QFVS) demonstrate the effectiveness and flexi-bility of our proposed framework. The codes are available at https://github.com/showlab/UniVTG. 1.

Introduction
With the increasing interest in sharing daily lives, video has emerged as the most informative yet diverse visual form on social media. These videos are collected in a variety of settings, including untrimmed instructional videos [28], and well-edited vlogs [18]. With massive scales and di-verse video forms, automatically identifying relevant mo-ments based on user queries has become a critical capability in the industry for efficient video browsing.
This significant demand has given rise to a number of video understanding tasks, including moment retrieval [66, 63, 30], highlight detection [52, 16, 56], and video sum-marization [14, 45, 42]. As depicted in Fig. 1, mo-(cid:66): Corresponding Author.
Figure 1: Given a video and a specific user query, UniVTG serves as a general video browsing helper that assists users by returning different scale target clips to support various VTG tasks. ment retrieval tends to localize consecutive temporal win-dows (interval-level) by giving natural sentences; highlight detection aims to pick out the key segment with highest worthiness (curve-level) that best reflects the video gist; video summarization collects a set of disjoint shots (point-level) to summarize the video, with general or user-specific queries. Despite task-specific datasets [10, 5, 46, 45] and models [66, 63, 56] have been developed, these tasks are typically studied separately. In general, these tasks share a common objective of grounding various scale clips based on customized user queries, which we refer to as Video Tem-poral Grounding (VTG).
Though these tasks are closely related, their relationship has not been explicitly studied until recently.
[20] intro-duces the first unified benchmark QVHighlights for mo-ment retrieval and highlight detection, and presents the first model Moment-DETR for jointly learning. On this basis,
UMT [26] expands audio inputs, and QD-DETR [29] devel-ops negative-pairs and saliency tokens. Nevertheless, these studies solely focus on designing models that intersect two subtasks and learn grounding capabilities rely on specific labels. This means that they lack the ability to general-ize the VTG across diverse temporal labels, such as unique point-level narrations in Ego4D [13]. Furthermore, we have witnessed promising progress in Vision-Language Pretrain-ing (VLP). One notable work is GLIP [23, 64], which de-velops a unified model via joint utilizing large-scale diverse image annotations such as image captions and bounding boxes for spatial grounding. However, we do not observe similar progress in video-language pretraining. Most works in this area are designed for video-level tasks such as video-text retrieval [54, 47] rather than temporal grounding. This
is largely due to the manual cost of fine-grained tempo-ral annotations is expensive, making it challenging to ob-tain open-source, scalable yet diverse annotations to support grounding pretraining along the temporal axis in videos.
Therefore, we see a clear motivation to pursue a Uni-fied VTG framework and propose our UniVTG, which aims to unify diversity in VTG along three directions: (i) From the label and task aspect, we first define a formulation for
VTG where each video is decomposed as a clip sequence that each clip is assigned three basic query-conditional el-ements. Such a formulation enables us to unify various
VTG labels and tasks under the same framework. More-over, to address the limitation of temporal labels, we pro-pose a data annotation scheme based on CLIP [36] to pro-(ii) From the duce scalable fine-grained pseudo labels. model aspect, we develop a flexible yet effective grounding model that inherits the principles of our formulation. Our model devises single-stream and dual-stream pathways for modality fusion and modality alignment respectively, and is equipped with three heads to decode three key elements.
This favorable design is capable of addressing each task and utilizing each label. (iii) Lastly, thanks to the unified frame-work and the availability of pseudo labels, we can perform large-scale temporal grounding pretraining across var-ious labels to enhance our grounding abilities. This em-powers us to address various VTG downstream tasks across multiple domains, including zero-shot inference.
To validate the effectiveness of our proposed frame-work, we conduct experiments not only on joint mo-ment retrieval and highlight detection benchmark (QVHigh-lights [20]), but also on three individual tasks for moment retrieval (Ego4D [13], Charades-STA [10], TACoS [38]), highlight detection (YouTube Highlights [46], TVSum [45]) and video summarization (QFVS [42]). Our UniVTG, one unified model with 4.2M samples for temporal grounding pretraining, has achieved remarkable results, outperform-ing state-of-the-art methods that are specifically tailored for each task. Overall, our contributions are four folds:
• To the best of our knowledge, our UniVTG is the first video temporal grounding pretraining that across var-ied domains and tasks, including moment retrieval, highlight detection and video summarization.
• We introduce a unified VTG framework that can fully leverage rich supervision from open-source, scalable yet diverse temporal annotations, such as point-level, interval-level, and curve-level labels.
• To address the limitations of pretraining corpus, we de-velop an efficient annotation method that uses CLIP as a teacher to produce scalable pseudo temporal labels.
• We demonstrate the effectiveness and flexibility of the proposed framework across four settings and seven datasets. Detailed ablation studies validate the supe-riority of the proposed components.
Figure 2: Diverse VTG labels can be divided into three types, each mainly associated with specific benchmarks: (a) point-level labels for video summarization [42] and timestamp narrations [13]; (b) interval-level labels for moment retrieval [13, 10, 20]; (c) curve-level labels for highlight detection [45, 20]. (d) UniVTG uni-fies diverse labels and tasks within one framework, enabling large-scale pretraining with diverse labels (dotted gray line) that can be transferred to various downstream tasks (solid green line). 2.