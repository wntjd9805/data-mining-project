Abstract
In this paper, we realize automatic visual recognition and direction estimation of pointing. We introduce the ﬁrst neural pointing understanding method based on two key contributions. The ﬁrst is the introduction of a ﬁrst-of-its-kind large-scale dataset for pointing recognition and di-rection estimation, which we refer to as the DP Dataset.
DP Dataset consists of more than 2 million frames of 33 people pointing in various styles annotated for each frame with pointing timings and 3D directions. The second is
DeePoint, a novel deep network model for joint recogni-tion and 3D direction estimation of pointing. DeePoint is a Transformer-based network which fully leverages the spatio-temporal coordination of the body parts, not just the hands. Through extensive experiments, we demonstrate the accuracy and efﬁciency of DeePoint. We believe DP Dataset and DeePoint will serve as a sound foundation for visual human intention understanding. 1.

Introduction
Gauging a person’s intent from passive visual observa-tions is one of the key goals of computer vision research.
Successful visual intent understanding would be essential for a wide range of applications including personal assis-tance, elderly care, and surveillance. Visual recognition of the gesticulations of a person is essential for this as they di-rectly express those intents. Pointing, the act of extending one’s (usually index) ﬁnger towards something in the per-son’s view to call attention to it, is particularly important as it conveys explicit information about the person’s inter-actions with the environment including conversations with others.
Despite the broad interest in gesture recognition, re-search on visual understanding of pointing has been surpris-ingly limited. Visual pointing interpretation requires both recognition (is the person pointing) and direction estima-tion (which direction is the person pointing). Past works have relied on special cameras, such as RGB-D sensors, or required the person to point in a speciﬁc way. For in-the-wild natural pointing understanding, we must be able to recognize and estimate their directions in 3D from regular
RGB cameras. A typical scenario we consider is a person in a room pointing at various things around her while freely moving around, which is observed by cameras ﬁxed to room corners.
Pointing recognition and direction estimation from ﬁxed-view cameras is particularly challenging. The person is usu-ally small in the view and the ﬁngers can hardly be dis-cerned. The hand can even be completely occluded by the person’s body. The pointing gesture would also typically span only about half a second, which makes its recognition in the video hard. Estimating the direction becomes even more challenging. In a full HD video frame captured with a
ﬁxed corner camera in a typical living room, the index ﬁn-ger would span only about 30 pixels. Analytical modeling such as line regression to such observations would be futile.
Even if that were possible, due to intra- and inter-personal variations of pointing, such estimations would be prone to error. Accounting for those variations would naturally ne-cessitate a learning-based approach that directly regresses to
the intended directions. This is also, however, not straight-forward, as the task is inherently spatio-temporal and, most important, large-scale data of pointing is difﬁcult to collect and currently devoid in the community.
In this paper, as illustrated in Fig. 1, we make two key contributions to realize automatic visual recognition and di-rection estimation of pointing. The ﬁrst is the introduction of a comprehensive dataset for pointing recognition and di-rection estimation. We refer to this as the DP Dataset. It consists of 2,800,000 frames of 33 people pointing at var-ious directions in different styles captured in 2 different rooms. Each of these frames is annotated with whether the person is pointing or not, and, when pointing, the 3D di-rection intended by the pointing person. This ﬁrst of its kind large-scale collection and annotation of natural point-ing gestures is achieved semi-automatically with a combi-nation of multi-view geometry and audio processing.
The second key contribution is DeePoint, a novel deep network model for joint recognition and 3D direction esti-mation of pointing. To overcome the challenges stemming from the ﬁxed-view observations from a distance, our key idea is to leverage the whole-body appearance and motion to detect and estimate 3D pointing. For this, we introduce a Transformer model, inspired by the STLT [28], that fully leverages the spatio-temporal coordination of the body in-cluding the head and joints in addition to the hand. By in-corporating the appearance of these as tokens and through cascaded attention transforms in space and time, we show that pointing gestures can be detected in time and their 3D directions can be estimated accurately.
We conduct extensive experiments to evaluate the ef-fectiveness of DeePoint. We ﬁrst evaluate the accuracy of recognition and direction estimation on the DP Dataset. We then evaluate the generalizability of DeePoint across differ-ent people and scenes. Through ablation studies, we also show that the spatio-temporal modeling of the body appear-ance and movements are essential for the task. We also con-duct comparative studies with related works, including eval-uation on the PKU-MMD dataset [5]. The experimental re-sults collectively demonstrate the accuracy and efﬁciency of
DeePoint. Our future work includes incorporating environ-mental cues and audio including spoken words to enhance the accuracy of pointing direction estimation, the challenge of which lies in realizing this without overﬁtting to the par-ticular context. We believe DeePoint provides a sound foun-dation for these further studies. 2.