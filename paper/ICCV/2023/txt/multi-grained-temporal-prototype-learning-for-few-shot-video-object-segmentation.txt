Abstract
Few-Shot Video Object Segmentation (FSVOS) aims to segment objects in a query video with the same category de-fined by a few annotated support images. However, this task was seldom explored. In this work, based on IPMT, a state-of-the-art few-shot image segmentation method that com-bines external support guidance information with adaptive query guidance cues, we propose to leverage multi-grained temporal guidance information for handling the temporal correlation nature of video data. We decompose the query video information into a clip prototype and a memory pro-totype for capturing local and long-term internal temporal guidance, respectively. Frame prototypes are further used for each frame independently to handle fine-grained adap-tive guidance and enable bidirectional clip-frame prototype communication. To reduce the influence of noisy memory, we propose to leverage the structural similarity relation among different predicted regions and the support for selecting reli-able memory frames. Furthermore, a new segmentation loss is also proposed to enhance the category discriminability of the learned prototypes. Experimental results demonstrate that our proposed video IPMT model significantly outper-forms previous models on two benchmark datasets. Code is available at https://github.com/nankepan/VIPMT. 1.

Introduction
To mitigate the data-hungry issue of modern deep seman-tic segmentation models [18, 2, 4], few-shot semantic seg-mentation emerges by only requiring a few support samples with annotated masks for segmenting the objects of the same class in new images. Many recent works [25, 32, 13, 16] have shown very promising results on image data using the meta-learning scheme. They simulate the inference process and partition the training set into numerous episodes, in each
†Corresponding author: yaoxiwen517@gmail.com.
Figure 1. Given a few annotated support frames, FSVOS aims to segment the objects with the same category from a query video (a).
Simply considering single-frame information leads to inconsecu-tive segmentation results (b), while our method generates accurate results by using multi-grained temporal prototypes (c). of which, the model samples a few support images and learns to guide the segmentation on the query images.
Inspired by the classic Few-Shot Image Semantic Seg-mentation (FSISS) task, the Few-Shot Video Object Segmen-tation (FSVOS) task was introduced by [1, 24]. For FSISS, many works adopt the prototype-based methods, in which a prototype vector is extracted from the support to encode the category guidance information, and then a segmentation head learns to match the prototype with the feature at each query pixel for performing query segmentation. However, the intra-class diversity can cause the matching gap between the support-induced prototype guidance and the query features.
The IPMT model [16] solved this problem by learning an intermediate prototype that integrates both support-induced external category guidance knowledge and query-induced adaptive guidance information. As for FSVOS, video data additionally show temporal correlation, from which we can also induce internal temporal guidance. If we ignore this prior knowledge and simply consider single-frame informa-tion, the learned prototypes may vary significantly among different frames, leading to inconsecutive segmentation re-sults, as shown in Figure 1 (b).
In this work, we extend the IPMT model [16] for video data by tackling the temporal correlation nature. Based on the intermediate prototype mechanism of IPMT, we propose to decompose the query video information into the clip-level, frame-level, and memory-level prototypes, consisting of a multi-grained temporal structure, which was NEVER ex-plored by existing models [1, 24]. Among them, the clip prototype encodes local temporal object guidance informa-tion in each consecutive clip, while the memory prototype introduces long-term historical guidance cues. Combing them can effectively handle the temporal correlation problem.
However, such a design may ignore fine-grained per-frame adaptive guidance information, hence may fail to handle large scene changes and object transformation. To this end, we further learn an adaptive frame prototype by indepen-dently encoding per-frame object features. Additionally, we enable bidirectional clip-frame prototype communication by making the clip-level and frame-level prototypes initialize each other, hence promoting intra-clip temporal correlation.
To better leverage the historical memory guidance, we also follow a Video Object Segmentation (VOS) method
[17] and train an IoU regression network to select reliable memory frames for reducing the negative influence brought by noisy memory. However, different from [17], we ex-plicitly consider the nature of the FSVOS task and propose to leverage the structural similarity relation among differ-ent predicted regions and the support information for pre-dicting more accurate IoU scores. To make the learned prototype more category discriminative, we also propose a
Cross-Category Discriminative Segmentation (CCDS) loss by leveraging negative batch samples. Extensive experimen-tal results have verified the effectiveness of our proposed
Video IPMT (VIPMT) model and showed its significant performance improvement over state-of-the-art models.
In conclusion, our contributions can be summarized as follows:
• For the very first time, we propose to learn multi-grained temporal prototypes for FSVOS, by extending the IPMT [16] model. Clip and memory prototypes are learned for internal temporal guidance. Frame proto-types are used for fine-grained adaptive guidance and also enable prototype communication.
• We propose to leverage the structural similarity relation among different predicted regions and the support for selecting reliable memory information. We also present a CCDS loss using the negative samples within each batch for promoting category discriminability of the learned prototypes.
• Experimental results have demonstrated the significant effectiveness of our proposed model, which improves state-of-the-art results by more than 4% and 3%, on two benchmark datasets, respectively. 2.