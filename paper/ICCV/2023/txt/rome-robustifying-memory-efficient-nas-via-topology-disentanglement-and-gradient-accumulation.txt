Abstract
Albeit being a prevalent architecture searching ap-proach, differentiable architecture search (DARTS) is largely hindered by its substantial memory cost since the entire supernet resides in the memory. This is where the single-path DARTS comes in, which only chooses a single-path submodel at each step. While being memory-friendly, it also comes with low computational costs. Nonetheless, we discover a critical issue of single-path DARTS that has not been primarily noticed. Namely, it also suffers from severe performance collapse since too many parameter-free operations like skip connections are derived, just like
DARTS does. In this paper, we propose a new algorithm called RObustifying Memory-Efficient NAS (ROME) to give a cure. First, we disentangle the topology search from the operation search to make searching and evaluation consis-tent. We then adopt Gumbel-Top2 reparameterization and gradient accumulation to robustify the unwieldy bi-level op-timization. We verify ROME extensively across 15 bench-marks to demonstrate its effectiveness and robustness. 1.

Introduction
Despite the fast development of neural architecture search (NAS) [52] to aid network design in vision tasks like classification [32, 6, 41, 42], object detection [11, 36], and segmentation [22], there has been an urging demand for faster searching algorithms. Early methods based on the evaluation of a huge number of candidate models
[52, 31, 16] require unaffordable costs (typically 2k GPU days).
In the light of weight-sharing mechanism intro-duced in SMASH [2], a variety of low-cost approaches have emerged [1, 27, 24]. DARTS [24] has taken the dominance with a myriad of follow-up works [38, 3, 39, 10, 5, 47]. In this paper, we investigate a single-path based variation of
DARTS, typically GDAS [10], for its fast speed and low
GPU memory.
Unlike many DARTS variants that have to perform all candidate operations, single-path methods [30, 10, 39], also termed as memory-efficient NAS1, are developed to sample and activate only a subset of operations in the supernet. For differentiable search, Gumbel-Softmax reparameterization tricks [17, 25] are generally employed [38, 10, 39].
In this paper, we show that single-path methods also suf-fer from severe performance collapse where many param-eterless operations accumulate, akin to that of DARTS as broadly discussed in [48, 4, 39, 8, 20]. We propose a robust algorithm called ROME to resolve this issue.
We observe that single-path methods usually intertwine topology search with operation search, which creates incon-sistency between searching and evaluation. We first disen-tangle the two from each other. Specifically, in addition to the existing architectural parameters (α) that represent the significance of each operation, we involve topology pa-rameters (β) to denote the relative importance of edges. A single-path architecture can then be induced by both α and
β. Moreover, to further robustify the searching process, we propose a gradient accumulation strategy during the bi-level optimization. We sketch the framework of ROME in
Fig. 1. In a nutshell, our contributions are, 1) Revealing performance collapse in single-path dif-ferentiable NAS. Similar to the performance collapse prob-lem in DARTS, the architectures searched by single-path based methods can also be dominated by parameterless op-erations, especially skip connections. However, this issue hasn’t been deeply explored, which motivates us to propose a new robust, lower memory cost and search cost method. 2) Consistent search and evaluation by disentangling topology search from operation selection. We introduce
*Equal contribution, † Correspondence author. 1We interchangeably use the term ‘single-path’ and ‘memory-efficient’.
Figure 1: ROME (v2): Gumbel-Top2 is devised to sample edges to satisfy the restriction that each node has in-degree 2.
Subnetworks apply forward and backward independently. Gradients are accumulated to update the supernet weights at once. independent topology parameters to unwind topology from operations, which avoids structure inconsistency between the search and evaluation stage. We further devise Gumbel-Top2 re-parameterization to make our method differentiable and provide its theoretical validity. To our best knowledge, this is the first work to achieve consistent search and evalu-ation for single-path differentiable NAS. 3) Robustifying bi-level optimization via gradient ac-cumulation. We devise two gradient accumulation tech-niques to address the aforementioned issue. One helps fair training for each candidate operations. The other instead reduces the estimation variance on architectural weights. 4) Strong performance while maintaining low mem-ory cost. Tested on the popular settings2 for NAS [48, 21, 6], our approach achieves state-of-the-art on various search spaces and datasets across 15 benchmarks. Our approach is fast, robust, and memory efficient. Compared with GDAS and PC-DARTS, our approach costs 26% and 38% lower memory in the standard search space of DARTS respec-tively. The source code will be made publicly available. 2.