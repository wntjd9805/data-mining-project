Abstract
Multi-class cell nuclei detection is a fundamental pre-It is critical requisite in the diagnosis of histopathology. to efficiently locate and identify cells with diverse morphol-ogy and distributions in digital pathological images. Most existing methods take complex intermediate representations as learning targets and rely on inflexible post-refinements while paying less attention to various cell density and fields of view. In this paper, we propose a novel Affine-Consistent
Transformer (AC-Former), which directly yields a sequence of nucleus positions and is trained collaboratively through two sub-networks, a global and a local network. The lo-cal branch learns to infer distorted input images of smaller scales while the global network outputs the large-scale pre-dictions as extra supervision signals. We further introduce an Adaptive Affine Transformer (AAT) module, which can automatically learn the key spatial transformations to warp original images for local network training. The AAT module works by learning to capture the transformed image regions that are more valuable for training the model. Experimental results demonstrate that the proposed method significantly outperforms existing state-of-the-art algorithms on various benchmarks. 1.

Introduction
A major task of pathologists is to make a diagnosis with digital pathological images, which are obtained by scan-ning tissue slides with a whole-slide scanner [33, 42]. In this process, a pathologist is required to provide the grad-ing of tumors and to classify benign and malignant dis-eases [31, 15], by locating and identifying certain histo-logical structures such as lymphocytes, cancer nuclei, and glands. In some applications, instead of locating pixels on each nucleus boundary, it could be useful to quantify the
†Junjia Huang and Haofeng Li contribute equally to this work.
∗Guanbin Li is the corresponding author.
Figure 1. The visual comparison of predictions and training targets between existing methods and ours. Different types of nuclei are marked by red, green and blue boxes or centroids. Our method can predict a sequence of position coordinates and categorical labels of nuclei directly from an input pathological image. different categories of cells. For example, the counts of tu-mor cells and lymphocytes have been utilized as an effective prognostic marker [12]. Thus, in this paper, we do not focus on predicting the nucleus sizes or boundaries, but only aim at inferring the types and rough locations of cell nuclei in digital slide images, following the previous work [1].
In the early stage, automatic nucleus detection and clas-sification have been achieved by handcrafted features based methods[35, 3, 49, 4]. These methods lack sufficient accu-racy and generalization, while deep learning (DL) models can tackle these issues via learning robust representations.
For nuclei detection, existing DL methods can be divided into three groups, according to the different forms of pre-diction targets. As Figure 1 shows, the first group is to outline the contour or to locate the region of each single nucleus via pixel-wise prediction [14, 9, 16, 41, 29, 30].
These methods rely on the high-quality boundary annota-tion of nuclei that are expensive and time-consuming. The second group [17, 1, 38] is to pixel-wisely predict centroids or dilated centroids (‘Dots Map’ in Figure 1), by convert-ing the detection into a binary segmentation task. Due to the diversity of cell density, the boundaries between adja-cent nuclei are often confused, which makes these methods fail to segment adherent nuclei and results in missed de-tection. The third group is to predict the bounding boxes of nuclei [39, 23, 56] based on the anchors of pixels, but the performance of these methods are affected by anchor parameters and post-processing. Adjacent nuclei with un-clear boundaries increase the difficulty of detecting bound-ing boxes. Thus, we propose to convert the nuclei detection into a task of directly predicting a set of cell positions and categories.
Besides, the diversity of image scale and nuclei den-sity causes more difficulties in the detection and classifi-cation tasks. Higher magnification levels or scaling factors could lead to a smaller field of view and more sparse dis-tribution of nuclei. We claim that it is essential to develop a robust model for different image scales. Some existing works [53, 50] employ multi-scale deep learning architec-tures or simply unify the scale by dividing patches, which does not take the prediction consistency among multiple scales into consideration.
To avoid the synthesis of indirect learning targets, we consider formulating the nucleus localization and classifica-tion problem as a sequence generation task. A transformer-based framework is adopted to decode a list of position co-ordinates and category labels of nuclei in a direct manner.
To adapt to diverse scales, we further split the transformer framework into two network branches, a local network and a global one, which aim to infer global-scale images and their local-scale views, respectively. The local network is not only supervised by the ground-truth annotations but also guided by the global network that captures broader con-textual information from the large-scale input. Thus, the well-trained networks could accommodate to diverse fields of view. To compute the training losses, a matching al-gorithm is utilized to assign each target nucleus to a nu-cleus proposal in the predicted sequence. Importantly, we claim that not all local image regions are equal for training a scale-consistent nuclei detection model. Therefore, we propose a novel adaptive affine transformer that predicts a series of affine transformation parameters to harvest the key local-scale inputs for improving the global-local training.
Since our proposed framework is trained to deal with var-ious fields of view and distributions of cells, it has the po-tential to well separate the densely distributed nuclei from each other and to reduce the missed detection rate.
In short, our major contributions are summarized as three folds:
• We introduce a novel Adaptive Affine Transformer that automatically learns to augment effective multi-scale samples for training.
• We propose an Affine-Consistent Transformer frame-work for nuclei detection.
Its local branch learns to output a set of nucleus-level predictions with small-scale inputs, guided by the global branch with a large-scale input.
• We conduct extensive experiments and demonstrate the state-of-the-art performance of our method on three widely-used benchmarks. 2.