Abstract
Knowledge Distillation (KD) uses the teacher’s logits as soft labels to guide the student, while self-KD does not need a real teacher to require the soft labels. This work uni-fies the formulations of the two tasks by decomposing and reorganizing the generic KD loss into a Normalized KD (NKD) loss and customized soft labels for both target class (image’s category) and non-target classes named Univer-sal Self-KD (USKD). We decompose the KD loss and find the non-target loss from it forces the student’s non-target logits to match the teacher’s, but the sum of the two non-target logits is different, preventing them from being identi-cal. NKD normalizes the non-target logits to equalize their sum. It can be generally used for KD and self-KD to better use the soft labels for distillation. USKD generates cus-tomized soft labels for both target and non-target classes without a teacher. It smooths the target logit of the student as the soft target label and uses the rank of the intermedi-ate feature to generate the soft non-target labels with Zipf’s law. For KD with teachers, NKD achieves state-of-the-art performance on CIFAR-100 and ImageNet, boosting the Im-ageNet Top-1 accuracy of Res-18 from 69.90% to 71.96% with a Res-34 teacher. For self-KD without teachers, USKD is the first method that can be effectively applied to both
CNN and ViT models with negligible additional time and memory cost, resulting in new state-of-the-art results, such as 1.17% and 0.55% accuracy gains on ImageNet for Mo-bileNet and DeiT-Tiny, respectively. Code is available at https://github.com/yzd-v/cls_KD. 1.

Introduction
Deep convolutional neural networks (CNNs) have sig-nificantly advanced the performance in many tasks [8, 9,
*This work was done when Zhendong was an intern at IDEA.
†Corresponding authors.
Figure 1. Illustration of the proposed NKD and USKD for distilla-tion loss calculations. NKD normalizes the non-target logits, using the soft labels more effectively, and achieves better performance.
Meanwhile, USKD sets customized soft labels for both target and non-target classes, and can be applied to both CNNs and ViTs. 27, 29]. In general, a larger model performing better needs more computing resources. On the other hand, smaller models have lower computation complexity but are less competitive than larger models. To bridge this gap and improve the performance of smaller models, knowledge distillation (KD) has been proposed [11]. The core idea of KD is to employ the teacher’s prediction logits as soft labels to guide the student. Self-knowledge distillation (self-KD) [33, 47] is inspired by the knowledge distillation method, but it does not require an actual teacher. Instead, it designs soft labels through auxiliary branches or special distribution. The similarity between KD and self-KD is that they utilize soft labels for distillation loss, while the key difference is in how they obtain the soft labels. This paper aims to 1) improve the utilization of soft labels for distilla-tion loss and 2) propose a general and effective method to obtain customized soft labels for self-KD. The targets make us obtain the soft labels with a teacher and use our modified distillation loss for better performance. Alternatively, when we lack a teacher, we can use the proposed self-KD method
to obtain the soft labels and then calculate the loss.
The original cross-entropy (CE) loss for classification calculates the loss on the target class (the image’s category).
While the soft labels from the teacher include target and non-target class, thus the KD loss also includes both target and non-target loss. The decoupled method has been proven effective for KD in DKD [49]. Unlike DKD’s method of ad-justing hyper-parameters on target and non-target loss, we present a simple yet effective way to decompose the KD loss. We decompose the KD loss into a combination of the target loss (like the original CE loss) and the non-target loss in CE form. The non-target loss transforms the inter-nal distribution of the student’s non-target logits to match the teacher’s distribution. However, we find that the sum of the student’s and teacher’s non-target logits is changing and different, which hinders the alignment of their distributions.
To address this issue, we normalize the non-target logits to equalize their sum, transferring teacher’s non-target knowl-edge. With this slight modification, we introduce our Nor-malized Knowledge Distillation (NKD) loss, as depicted in
Fig.1, significantly enhancing KD’s performance.
Our proposed NKD utilizes the teacher’s target logit and normalized non-target logits to guide the student, resulting in state-of-the-art performance. This demonstrates the ef-fectiveness of NKD loss formulation. Also, it can be gen-erally used for self-KD to calculate the distillation loss, but how to generate the soft labels without a real teacher gener-ally and efficiently is also important.
Various self-KD methods have explored using manually designed soft labels to enhance students with less time than
KD. These methods [14, 38, 44, 47] typically obtain the la-bels from auxiliary branches or contrastive learning, as de-picted in Fig. 2 (a), (b), and (c). However, despite requir-ing less time than KD, they still involve significant over-head compared to training the model directly. Recently, state-of-the-art Zipf’s LS [17], as shown in Fig. 2 (d), in-troduced soft non-target labels based on a special distri-bution that can significantly reduce resource and time re-quirements. It classifies the student’s feature in the spatial dimension and determines the rank of the non-target class using Zipf’s law [25]. However, it requires the pixel-level features before average pooling, making it unsuitable for
ViT-like models [6] with patch-wise tokenization.
To address the limitations of existing methods, we pro-pose a general and effective way to obtain soft labels. We design customized soft labels available for both CNN and
ViT models. Following the NKD loss formulation, our cus-tomized soft labels comprise soft target label and soft non-target labels for corresponding loss. For the soft target la-bel, we replace the teacher’s target logit with the smoother label value obtained from the student’s prediction. Since the student’s predictions vary drastically during training, especially in the beginning, we smooth the student’s tar-get output within each training batch to stabilize the label values. For the soft non-target labels, we need their rank and distribution. First, for the rank, we get it from the in-termediate feature, making our method available for both
CNN and ViT models. We take weak supervision on the intermediate feature to get weak logit. Then, we normal-ize and combine it with the final logit and sort for the rank, as shown in Fig. 2 (f). The soft non-target labels’ distribu-tion follows Zipf’s Law [25]. With the soft target and non-target labels, we set our customized soft labels and propose
Universal Self-Knowledge Distillation (USKD) as shown in
Fig. 1. Besides, USKD only needs an extra linear layer for weak supervision. So it just takes a few more computing re-sources and time than training the model directly. USKD is a simple and effective method that achieves state-of-the-art performance on both CNN and ViT models.
As described above, we normalize KD’s non-target logits and propose NKD, using the soft labels better and improv-ing KD’s performance significantly. For the generation of soft labels without a real teacher, we set soft target and non-target labels, proposing USKD for self-KD. In a nutshell, the contributions of this paper are:
• We normalize the non-target logits in the classical KD, making it better to optimize the cross-entropy loss.
With this minor change, we propose Normalized KD (NKD) loss, using teacher’s soft labels better and im-proving KD’s performance significantly.
• We propose a novel way to set customized soft labels without a real teacher, including target and non-target classes for self-KD. We utilize the weak logit to obtain soft non-target labels. Besides, we enlarge the differ-ence between student’s target logit for different images and soften them for soft target labels.
• We propose a simple and effective self-KD method
USKD with our customized soft labels, which applies to both CNN and ViT models. Importantly, USKD re-quires only almost negligible additional time and re-sources compared to training the model directly.
• We conduct extensive experiments on CIFAR-100 and
ImageNet to verify the effectiveness of NKD and
USKD, achieving state-of-the-art performance. Addi-tionally, we demonstrate the efficacy of models trained with our self-KD method on COCO for detection. 2.