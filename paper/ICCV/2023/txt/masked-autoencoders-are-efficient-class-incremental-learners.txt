Abstract
Class Incremental Learning (CIL) aims to sequentially learn new classes while avoiding catastrophic forgetting of previous knowledge. We propose to use Masked Au-toencoders (MAEs) as efﬁcient learners for CIL. MAEs were originally designed to learn useful representations through reconstructive unsupervised learning, and they can be easily integrated with a supervised loss for classiﬁca-tion. Moreover, MAEs can reliably reconstruct original in-put images from randomly selected patches, which we use to store exemplars from past tasks more efﬁciently for CIL.
We also propose a bilateral MAE framework to learn from image-level and embedding-level fusion, which produces better-quality reconstructed images and more stable rep-resentations. Our experiments conﬁrm that our approach performs better than the state-of-the-art on CIFAR-100,
ImageNet-Subset, and ImageNet-Full. The code is avail-able at https://github.com/scok30/MAE-CIL. 1.

Introduction
Deep learning has had a broad and deep impact on most computer vision tasks over the last ten years. Given the way humans learn continually in their lifespan, it is natu-ral to expect models also to be able to accumulate knowl-edge and build on past experiences to adapt to new tasks incrementally. The real world is very dynamic, leading to varying data distributions over time, while deep models tend to catastrophically forget old tasks when learning new ones [26].
Class Incremental Learning (CIL) aims to learn new classiﬁcation tasks sequentially while avoiding catastrophic forgetting [2, 25]. CIL approaches can be roughly di-vided into three categories [5], i.e., Rehearsal-based meth-ods [13, 29, 31], Regularization-based methods [15, 17], and Architecture-based methods [1, 23, 24]. Among them,
Rehearsal-based methods achieve state-of-the-art perfor-mance by storing exemplars from past tasks or generating synthetic samples for replay.
*Corresponding author (xialei@nankai.edu.cn)
Current Task 
Data
Main 
Encoder
Main 
Decoder
Original Output
Reconstruction 
Replay Buffer
Embedding 
Fusion
Classification
Image 
Fusion
Detailed 
Encoder
Detailed 
Decoder
Residual Detail
Figure 1. Our proposed bilateral MAE for efﬁcient CIL. The Re-play Buffer contains random patches selected from past task im-ages, which is more efﬁcient than storing whole images. Combin-ing these with masked input data from the current task, the MAE simultaneously learns to classify and reconstruct images from the masked input. To further improve the reconstructed image quality and learned representations, embedding-level and image-level fu-sion is used to learn more stable representations and more detailed reconstructions for CIL.
Normally, only a ﬁxed size of memory is allowed during incremental learning. Therefore, it limits the stored exem-plars from past tasks. Other works exploit generative net-works [35, 38, 43] (e.g., GANs) to synthesize samples from old tasks for replay. Although they can generate replay data to mitigate forgetting, a typical drawback is the quality of generated images, and that forgetting can also happen in generative models. In this work, we introduce Masked Au-toencoders (MAEs) [11] as a base model to replay. It al-lows efﬁcient exemplar storage by only requiring a small subset of patches to reconstruct whole images. Therefore, we can store more exemplars with the same amount of lim-ited memory as other exemplar-based approaches. Com-pared to previous generative methods, replay by MAE is more stable because it uses partial cues to infer global in-formation, which is task-agnostic and suffers less forgetting across tasks. This relieves the unstable generation effect of
GANs across tasks with stationary image patches.
Masked Autoencoders (MAEs) [11] were initially pro-feature representations in self-posed to learn better supervised learning scenarios.
In this work, we see it as efﬁcient incremental learners and propose a novel bilat-eral transformer architecture for efﬁcient exemplar replay in CIL. Our main idea is simple: by randomly masking patches of input images and training models to reconstruct the masked pixels, MAEs can provide a new form of self-supervised representation learning for CIL and thus learn more generalizable representations essential for CIL. In ad-dition, leveraging a supervised objective with classiﬁcation labels beneﬁts unsupervised MAE in training efﬁciency and model robustness [18]. Masked inputs can also serve as strong classiﬁcation regularization by only providing a ran-dom subset of data.
When learning new tasks, the MAE can coarsely recon-struct images from sparsely sampled patches from exem-plars. This process enables the framework to generate re-constructed replay data, but two problems remain: (i) the generated images tend to have less detailed and less real-istic textures, which reduces data diversity for replay; and (ii) on the embedding-level, the linear classiﬁer lacks infor-mation from low-level features. Therefore, we introduce a bilateral MAE framework with image-level and embedding-level fusion for CIL (see Fig. 1 for a schematic overview).
Fusing a complementary detailed and reconstructed image alleviates catastrophic forgetting by enriching the insufﬁ-cient replay data with detailed, high-quality data distribu-tions. Embedding-level fusion from the two branches also maintains stable and diverse embeddings, and our frame-work can thus achieve a better trade-off between plasticity and stability.
The main contributions of our bilateral MAE framework are threefold:
• We introduce an MAE framework for efﬁcient incre-mental learning that incorporates beneﬁts from both self-supervised reconstruction and data generation for replay.
• To further boost the quality of reconstructed images and learning efﬁciency, we design a novel bilateral
MAE with two complementary branches for better-reconstructed images and regularized representations.
• Our approach perfor-mance under different CIL settings on CIFAR-100,
ImageNet-Subset, and ImageNet-Full. state-of-the-art achieves 2.