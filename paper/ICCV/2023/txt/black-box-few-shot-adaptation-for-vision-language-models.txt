Abstract
Vision-Language (V-L) models trained with contrastive learning to align the visual and language modalities have been shown to be strong few-shot learners. Soft prompt learning is the method of choice for few-shot downstream adaption aiming to bridge the modality gap caused by the distribution shift induced by the new domain. While parameter-efficient, prompt learning still requires access to the model weights and can be computationally infeasi-ble for large models with billions of parameters. To ad-dress these shortcomings, in this work, we describe a black-box method for V-L few-shot adaptation that (a) operates on pre-computed image and text features and hence works without access to the model’s weights, (b) it is orders of magnitude faster at training time, (c) it is amenable to both supervised and unsupervised training, and (d) it can be even used to align image and text features computed from uni-modal models. To achieve this, we propose Lin-ear Feature Alignment (LFA), a simple linear approach for
V-L re-alignment in the target domain. LFA is initialized from a closed-form solution to a least-squares problem and then it is iteratively updated by minimizing a re-ranking loss. Despite its simplicity, our approach can even surpass soft-prompt learning methods as shown by extensive exper-iments on 11 image and 2 video datasets.
Code available at: https://github.com/saic-fi/LFA 1.

Introduction
Large-scale Vision-Language (V-L) models [60] trained with contrastive learning currently represent the de-facto approach for few-shot visual adaptation. Their unprece-dented success lies in part in the strength of the joint V-L embedding space learned by aligning the image and text modalities. However, when a V-L model is applied to a new domain, the domain shift exacerbates the V-L modality gap [48], and some sort of adaptation is required to obtain high accuracy (see Fig. 1(a)). The question that we want to address in this paper is: “can we effectively adapt a V-L model to a new domain by having access to pre-computed features only?” We call this black-box adaptation.
Similar to their NLP counterparts [60, 44], soft prompt learning has emerged as the preferred technique for adapt-ing a V&L to new tasks.
Specifically, a number of works [86, 85, 11, 87, 15, 67, 51, 36] have proposed to replace the manually designed prompts of [60] (e.g., a photo of a {cls name}), with a sequence of learn-able vectors, coined soft prompts. These are passed as input to the text encoder jointly with the class name cls name to create the new prototypes effectively reducing the modality gap. The prompts are learned in a supervised manner using a standard cross-entropy loss given a set of labeled images. learning approaches demonstrate promising results on various downstream tasks [86, 85, 11], they suffer from two limitations: (1) They require access to the model’s weights, and (2) the training cost can be prohibitive, especially on commodity hardware and low-power devices, as computing the gradients and updating the prompts for thousands of iterations [86] is required. As the model’s size continues to grow (e.g., billion-parameter models such as CoCa [80]), and the industry transitions to models as a service (e.g., via API), the existing methods can be rendered either inapplicable or impractical.
While soft-prompt
In this work, we seek to address these limitations by bridging the modality gap directly in the feature space with-out prompting or access to the model’s weights. We first empirically show that a simple linear transformation can ap-proximate the alignment effect of prompt learning (e.g., see
Fig. 1 and Sec. 3). Importantly, this shows that it is possible to derive a black-box method that manipulates the CLIP fea-tures directly for downstream adaptation. Then, motivated by this observation, we propose Linear Feature Alignment (LFA), a black-box method that learns a linear mapping W, obtained by solving a simple optimization problem, which effectively aligns the image features X with their text class prototypes Y, i.e., X W−−→ Y. Specifically, our contribu-tions are:
• We propose the very first black-box method for the few-shot adaptation of V-L models.
• To this end, and motivated by the observation that prompting can be successfully approximated by a lin-Figure 1: Effect of Linear Feature Alignment (LFA): We use 16-shot (per class) training data for two fine-grained image
In (1), we show the training set modality gap between paired image classification datasets: DTD and FGVC Aircraft. embeddings and class prototypes following the same procedure as in [48], and the obtained test set accuracy. The embeddings are visualized in 2D using PCA. (a) With CLIP features, we observe a big modality gap, resulting in low test accuracy. (b)
After learning a set of soft-prompts, we obtain a better alignment and improved results. However, the modality gap is still not sufficiently reduced. (c) A simple linear transformation W that maps the original class prototypes (obtained using only the class names) to the ones obtained with soft-prompt learning induces a similar modality gap. (d) Motivated by (c) we propose
LFA, which aligns the image embeddings with their class prototypes via linear mapping W, obtained by solving a simple optimization problem. LFA results in better alignment and improved accuracy. In (2), we show that with LFA, the test image features are closely aligned with their corresponding class prototypes, resulting in higher cosine similarity scores compared to the ones obtained with soft prompts. ear transformation, we propose Linear Feature Align-ment (LFA), an efficient and effective adaptation method for reducing the modality gap between the im-age and text modalities of a V-L model. LFA is initial-ized by β-Procrustes, a regularized version of orthog-onal Procrustes, and then minimizes a simple adaptive reranking loss adapted for V-L models.
• We propose both supervised and unsupervised formu-lations for LFA and moreover, a variant that works for the case of base-to-new (i.e., zero-shot) generalization.
• We demonstrate that LFA can achieve better align-ment (e.g., see Fig. 1 (1d) and (2)) and improved ac-curacy compared to prompt learning methods while being more efficient (i.e., training takes few minutes) and practical (i.e., not requiring access to the model’s weights). Finally, we show that it can even align image and text features computed from uni-modal models.
Table 1: Training Time: train time for CoOp [86] and for the proposed LFA on ImageNet (16-shot) using ViT-B/16 as the visual encoder on a single V100 GPU.
Method
CoOp
LFA (Feature Extraction)
LFA (Procrustes Initialisation)
LFA (Refinement)
LFA (Total)
Training Time Test Acc. 3h 22min 71.92 2min 37s 4s 28s 3min 9s 72.61 2.