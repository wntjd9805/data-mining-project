Abstract 1.

Introduction
Despite advancements in user-guided video segmenta-tion, extracting complex objects consistently for highly com-plex scenes is still a labor-intensive task, especially for production. It is not uncommon that a majority of frames need to be annotated. We introduce a novel semi-supervised video object segmentation (SSVOS) model, XMem++, that improves existing memory-based models, with a perma-nent memory module. Most existing methods focus on sin-gle frame annotations, while our approach can effectively handle multiple user-selected frames with varying appear-ances of the same object or region. Our method can extract highly consistent results while keeping the required number of frame annotations low. We further introduce an iterative and attention-based frame suggestion mechanism, which computes the next best frame for annotation. Our method is real-time and does not require retraining after each user input. We also introduce a new dataset, PUMaVOS, which covers new challenging use cases not found in previous benchmarks. We demonstrate SOTA performance on chal-lenging (partial and multi-class) segmentation scenarios as well as long videos, while ensuring significantly fewer frame annotations than any existing method. Project page: https://max810.github.io/xmem2-project-page/
*These authors contributed equally to the work
Video Object Segmentation (VOS) [48] is a widely per-formed vision task with applications ranging from object recognition, scene understanding, medical imaging, to filter effects in video chats. While fully automated approaches based on pre-trained models for object segmentation are of-ten desired, interactive user guidance is commonly prac-ticed to annotate new training data or when precise roto-scoping is required for highly complex footages such as those found in visual effects. This is particularly the case when the videos have challenging lighting conditions and dynamic scenes, or when partial region segmentation is re-quired. While automatic VOS methods are designed to seg-ment complete objects with clear semantic outlines, interac-tive video object segmentation (IVOS) and semi-supervised video object segmentation (SSVOS) techniques [48] are more flexible, and typically use a scribble or contour draw-ing interface for manual refinement such as those found in commercial software solutions such as Adobe After Ef-fects and Nuke. Despite advancements in IVOS and SSVOS techniques, rotoscoping in film production is still a highly labor-intensive task, and often requires nearly every frame of a shot to be annotated and refined [36].
State-of-the-art
IVOS and SSVOS techniques use memory-based models [27] and have shown impressive segmentation results on complex scenes based on user-provided mask annotations, but they are often designed to
improve single annotation performances [27, 43, 44, 6, 4] and are still not suitable for production use cases. In par-ticular, they tend to over-segment known semantic outlines (person, hair, faces, entire objects) and fail on partial re-gions (e.g., half of a person’s face, a dog’s tail), harsh light-ing conditions such as shadows, and extreme object poses.
As a result, inconsistent segmentations are obtained when only a single annotated frame is provided, due to the inher-ent ambiguity of the object’s appearance, Especially when it varies too much due to large viewing angles and com-plex lighting conditions, which limits the scalability of these techniques, and it is often unclear which frame annotations to prioritize, especially for long sequences.
We propose a new SSVOS framework, XMem++, which uses a permanent memory module that stores all annotated frames and makes them available as references for all the frames in the video. While most SSVOS methods focus on single-frame mask annotations, our approach is designed to handle multiple frames that can be updated by the user it-eratively with optimal frames being recommended by our system. While we adopt the cutting-edge architecture of
XMem [4] as backbone, we show that our important modi-fication enables accurate segmentation of challenging video objects (including partial regions) in complex scenes with significantly fewer annotated frames than existing methods.
Our modification does not require any re-training or calibra-tion and additionally shows improved temporal coherence in challenging scenarios (Fig. 1, 6, 8). Our attention-based frame suggestion method predicts the next candidate frame for annotation based on previous labels while maximizing the diversity of frames being selected. Our system supports both sparse (scribbles) and dense (full masks) annotations and yields better quality scaling with more annotations pro-vided than existing methods. The video segmentation per-forms in real-time, and frame annotations are instantly taken into account with a pre-trained network.
We further introduce a new dataset, PUMaVOS for benchmarking purposes, which includes new challenging scenes and use cases, including occlusion, partial segmen-tation, and object parts segmentation, where the annotation mask boundaries may not correspond to visual cues.
We evaluate the performance of our algorithm both qual-itatively and quantitatively on a wide range of complex video footages as well as existing datasets, and demon-strate SOTA segmentation results on complex scenes.
In particular, we show examples where our method achieves higher accuracy and temporal coherence than existing meth-ods with up to 5× fewer frame annotations and on 2× twice less on existing benchmarks (Section 5). We further demon-strate the effectiveness of our frame annotation candidate selection method by showing that it selects semantically meaningful frames, similar to those chosen by expert users.
We make the following contributions:
• We have introduced a new VOS model, XMem++, that uses a permanent memory module that effec-tively utilizes multiple frame annotations and produces temporally-smooth segmentation results without over-fitting to common object cues.
• We further propose the use of an attention-based simi-larity scoring algorithm that can take into account pre-viously predicted frame annotations to suggest the next best frame for annotation.
• We present a new dataset, PUMaVOS, which con-tains long video sequences of complex scenes and non object-level segmentation cues, which cannot be found in existing datasets.
• We achieve SOTA performance on major benchmarks, with significantly fewer annotations, and showcase successful examples of complex multi-class and partial region segmentations that fail for existing techniques. 2.