Abstract
NICE-SLAM [79]
Point-SLAM (ours)
We propose a dense neural simultaneous localization and mapping (SLAM) approach for monocular RGBD in-put which anchors the features of a neural scene repre-sentation in a point cloud that is iteratively generated in an input-dependent data-driven manner. We demonstrate that both tracking and mapping can be performed with the same point-based neural scene representation by minimiz-ing an RGBD-based re-rendering loss. In contrast to recent dense neural SLAM methods which anchor the scene fea-tures in a sparse grid, our point-based approach allows dy-namically adapting the anchor point density to the informa-tion density of the input. This strategy reduces runtime and memory usage in regions with fewer details and dedicates higher point density to resolve fine details. Our approach performs either better or competitive to existing dense neu-ral RGBD SLAM methods in tracking, mapping and ren-dering accuracy on the Replica, TUM-RGBD and Scan-Net datasets. The source code is available at https:// github.com/eriksandstroem/Point-SLAM . 1.

Introduction
Dense visual simultaneous localization and mapping (SLAM) is a long-standing problem in computer vision where dense maps have widespread applications in aug-mented and virtual reality (AR, VR), robot navigation and planning tasks [17], collision detection [7], detailed occlu-sion reasoning [46], and interpretation [72] of scene content which is vital for scene understanding and perception.
To estimate a dense map via SLAM, tracking and map-ping steps have traditionally been employed with different scene representations which creates undesirable data redun-dancy and independence since the tracking is then often per-formed independently of the estimated dense map. Cam-era tracking is frequently done with sparse point clouds or depth maps, e.g. via frame-to-model tracking [36, 66, 6, 38, 21] and with incorporated loop closures [15, 77, 5].
For dense mapping the most common scene representations
*Equal contribution. s t n i o
P r o h c n
A g n i r e d n e
R h t u r
T d n u o r
G
Figure 1: Point-SLAM Benefits. Due to the spatially adap-tive anchoring of neural features, Point-SLAM can encode high-frequency details more effectively than NICE-SLAM which leads to superior performance in rendering, recon-struction and tracking accuracy while attaining competitive runtime and memory usage. The first row shows the fea-ture anchor points. For NICE-SLAM we show the centers of non-empty voxels located on a regular grid, while the density of anchor points for Point-SLAM depends on depth and image gradients. The row below depicts resulting ren-derings showing substantial differences on areas with high-frequency textures like the vase, blinds, floor or blanket. are voxel grids [36, 37], voxel hashing [38, 15, 21, 20], oc-trees [16, 49, 29], or point/surfel clouds [77, 5, 48]. The introduction of learned scene representations [42, 30, 8, 32] has led to rapid progress for learning-based online map-ping methods [63, 64, 31, 18, 24, 41] and offline meth-ods [43, 1, 57, 73]. However, most of these methods re-quire ground truth depth or 3D for model training and may not generalize to unseen real-world scenarios at test time.
To eliminate the potential domain gap between train and test time, recent SLAM methods rely on test time optimiza-tion via volume rendering [53, 69, 79]. Compared to tradi-tional approaches, neural scene representations have attrac-tive properties for mapping like improved noise and out-lier handling [64], better hole filling and inpainting capa-bilities for unobserved scene parts [69, 79], and data com-pression [42, 58]. Like DTAM [37] or BAD-SLAM [48] recent neural SLAM methods [79, 69, 53] only use a sin-gle scene representation for both tracking and mapping but they rely either on a regular grid structure [79, 69] or a single MLP [53].
Inspired by BAD-SLAM [48], NICE-SLAM [79] and Point-NeRF [67], the research question we tackle in this work is:
Can point-based neural scene representations be used for tracking and mapping for real-time capable SLAM?
To this end, we introduce Point-SLAM, a point-based so-lution to dense RGBD SLAM, which allows for a data-adaptive scene encoding. The key ideas of our method are as follows: Instead of anchoring the feature points on a reg-ular grid, our approach populates points adaptively depend-ing on information density in the input data which allows for a better memory vs. accuracy trade-off. For render-ing, we depart from the classical splatting technique used for surfels and instead aggregate neural point features in a ray-marching fashion. MLP decoders translate these fea-tures into scene geometry and color estimates. Tracking and mapping are performed alternatingly by minimizing an
RGBD-based re-rendering loss. Different from grid-based approaches, we do not model free space and encode only little information around the surface. We evaluate our pro-posed method on a selection of indoor RGBD datasets and demonstrate state-of-the-art performance on dense neural
RGBD SLAM in terms of tracking, rendering, and map-ping - see Fig. 1 for exemplary results. In summary, our contributions include:
• We present Point-SLAM, a real-time capable dense
RGBD SLAM approach which anchors neural features in a point cloud that grows iteratively in a data-driven manner during scene exploration. We demonstrate that the proposed neural point-based scene representation can be effectively used for both mapping and tracking.
• We propose a dynamic point density strategy which al-lows for computational and memory efficiency gains and trade reconstruction accuracy against speed and memory.
• Our approach shows clear benefits on a variety of datasets in terms of tracking, rendering and mapping accuracy. 2.