Abstract
Liquid perception is critical for robotic pouring tasks. It usually requires the robust visual detection of flowing liq-uid. However, while recent works have shown promising re-sults in liquid perception, they typically require labeled data for model training, a process that is both time-consuming and reliant on human labor. To this end, this paper pro-poses a simple yet effective framework PourIt!, to serve as a tool for robotic pouring tasks. We design a simple data col-lection pipeline that only needs image-level labels to reduce the reliance on tedious pixel-wise annotations. Then, a bi-nary classification model is trained to generate Class Acti-vation Map (CAM) that focuses on the visual difference be-tween these two kinds of collected data, i.e., the existence of liquid drop or not. We also devise a feature contrast strategy to improve the quality of the CAM, thus entirely and tightly covering the actual liquid regions. Then, the container pose is further utilized to facilitate the 3D point cloud recovery of the detected liquid region. Finally, the liquid-to-container distance is calculated for visual closed-loop control of the physical robot. To validate the effectiveness of our proposed method, we also contribute a novel dataset for our task and name it PourIt! dataset. Extensive results on this dataset and physical Franka robot have shown the utility and effec-tiveness of our method in the robotic pouring tasks. Our dataset, code and pre-trained models will be available on the project page 1. 1.

Introduction
Recent advances have seen the great progress in the ca-pabilities of grasping and manipulating the rigid objects.
Haitao Lin is with Academy for Engineering and Technology, and En-gineering Research Center of AI and Robotics, Fudan University.
Yanwei Fu and Xiangyang Xue are corresponding authors. Yanwei
Fu is with the School of Data Science, Fudan University, and Fudan
ISTBI—ZJNU Algorithm Centre for Brain-inspired Intelligence, Zhejiang
Normal University, Jinhua, China. 1Project page. https://hetolin.github.io/PourIt
Figure 1. Our visual closed-loop robotic pouring. Unlike [17], our approach recovers a 3D point cloud of the liquid using the source container’s pose and 2D liquid perception data. This allows the robot to pour accurately based on visual feedback, even without the depth measurement of the liquid.
However, the manipulation of non-rigid objects such as liq-uids, cloth, and rope remains a formidable challenge due to the absence of fixed patterns and geometrical shapes associ-ated with flexible properties. Perceiving images of liquid is particularly challenging due to their reliance on refraction of light as the primary visual cue and the absence of depth measurement as shown in Fig. 1 (top left). Thus, further research to enhance liquid perception would be highly ben-eficial, especially for robots involved in real-world service
tasks like cooking, drink serving, and plant watering.
Pouring water is a highly relevant task in the field of liquid manipulation, yet it poses a number of significant challenges. These challenges include (1) the need for large amounts of pixel-wise annotated data to facilitate effective training, (2) the lack of salient visual cues within images, and (3) the unavailability of a reliable depth measurement system for accurate liquid pouring. Addressing these chal-lenges is crucial for advancing the state of liquid manipu-lation technology and enabling practical applications in do-mains such as robotics and industrial automation.
To address the aforementioned challenges, researchers have explored the use of extra sensors to generate ground-truth labels for robotics pouring. For example, Schenck et al. [26] skillfully utilize the thermal camera and heating wa-ter to obtain the ground-truth annotations. However, this approach is time-consuming and relies on additional equip-ment. Other methods [15, 34, 36, 9, 18, 32] rely on the audio but not visual signals to help robotics pouring, which limits their efficacy in noisy environments. Lin et al. [17] simply utilize the estimated container’s pose and size to cal-culate the initial pouring point without any liquid percep-tion (Fig. 1), thus cannot guarantee whether the liquid is poured into target container in a closed-loop manner. More recently, a self-supervised method [20] has been proposed to transfer colored liquid into transparent liquid without the need for manual annotations. However, this approach’s re-liance on colored liquid and a statically placed transparent container limits its applicability in more general environ-ments. Therefore, the challenge of robustly perceiving dy-namically out-flowing liquid during pouring operations re-mains an open question that requires further research.
This paper presents a simple yet effective framework
PourIt! that solves challenges in robotic pouring tasks, par-ticularly during the pouring stage. Our open-source library is positioned similarly to existing tools such as MoveIt! [7] for motion planning and GraspIt! [19] for grasping plan-ning. We address three specific challenges, including lim-ited annotated real-world data for training, non-salient vi-sual cues of liquid, and unavailable depth measurement of the liquid. (1) To address the first challenge, we design an semi-automatic pipeline that will collect images with two-class labels, indicating whether the liquid flows from the bottleneck of the source container. This pipeline enables the robot to collect more data, thereby improving the perfor-mance of the network. (2) To tackle the second challenge, we classify the images by distinguishing between the two types of collected data (existence of out-flowing liquid or not). However, such a classification task may not fully fo-cus on the entire liquid region. To overcome this, we use a feature contrast strategy to pull foreground local features close and separate foreground-background local features to identify the liquid region. (3) To resolve the third challenge, we make the gravitational assumption that the liquid stream aligns along the body of container. This enables us to ap-proximate the liquid’s 3D shape with estimated container pose, providing valuable visual feedback for robot control.
Technically, we first collect two types of real-world data by robot or human executing pouring action with using empty or full container, and turning on or off the taps as shown in Fig. 2. The data is divided into positive samples if there is flowing liquid, otherwise negative samples. Then we train a classification network to distinguish the positive and negative samples, which drives the network to focus on the difference of the two types of data, i.e., existence of liq-uid or not. Thus the derived Class Activation Map (CAM) will coarsely focus on the liquid regions. We further uti-lize this initial CAM to separate the feature maps into fore-ground and background local features. Then, we bridge the distance between foreground-foreground pairs while widen-ing the distance between foreground-background pairs.
This generates a better CAM which aligns with low-level boundaries and completely covers the target liquid region.
Then, we use the off-the-shelf category-level pose estima-tion, e.g., SAR-Net [17] to recover the pose of the source container for calculating the plane equation aligned with gravity and orientation of container’s neck. Finally, the 3D point cloud of liquid is approximated by calculating the line-plane intersections, where the lines are the rays back-projected from estimated 2D liquid region.
In summary, the main contributions of this paper are: (1) We first propose a novel weakly-supervised pipeline to transfer the 2D liquid perception problem into a classifica-tion task. We also propose a feature contrast strategy to improve quality of CAM. (2) We first propose the method of approximate 3D shape of liquid by utilizing the 6-DoF pose of source containers and estimated liquid mask for robotic pouring-related task. To the best of our knowledge, we are the first work to perceive and model the 3D liq-uid from a single image without temporal information. (3)
We deploy our real-time framework (10Hz) on the physi-cal Franka robot to serve as visual feedback to pour liquid more accurately into target container. (4) We also propose the PourIt! dataset, which could be a test bed benchmark of the self/weakly-supervised liquid segmentation task for computer vision and robotics community. We believe that
PourIt! framework should endow the ability for robots in continuous self-supervised learning. For example, extend-ing the tasks in Fig. 1 to continuously make the robot mutu-ally pour liquid from two containers to collect data in a self-supervised manner, thus fine-tuning the model using more data for better liquid perception. 2.