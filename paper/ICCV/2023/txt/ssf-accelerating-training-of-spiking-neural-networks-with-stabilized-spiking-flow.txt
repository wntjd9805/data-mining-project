Abstract
Surrogate gradient (SG) is one of the most effective approaches for training spiking neural networks (SNNs).
While assisting SNNs to achieve classiﬁcation performance comparable to artiﬁcial neural networks, SG suffers from the problem of time-consuming training, preventing it from
In this paper, we formally analyze the efﬁcient learning. backward process of classic SG and ﬁnd that the membrane accumulation through time leads to exponential growth of training time. With this discovery, we propose Stabilized
Spiking Flow (SSF), a simple yet effective approach to ac-celerate training of SG-based SNNs. For each spiking neu-ron, SSF averages its input and output activations over time to yield stabilized input and output, respectively. Then, in-stead of back propagating all errors that are related to cur-rent neuron and inherently entangled in time domain, the auxiliary gradient is directly propagated from the stabilized output to input through a devised relationship mapping. Ad-ditionally, SSF method is suitable to different neuron mod-els. Extensive experiments on both static and neuromorphic datasets demonstrate that SNNs trained with SSF approach can achieve performance comparable to the original coun-terparts, while reducing the training time signiﬁcantly. In particular, SSF speeds up the training process of state-of-the-art SNN models up to 10× when time steps equal to 80. 1.

Introduction
Beneﬁting from huge amount of data and advances in computing hardwares, deep artiﬁcial neural networks (ANNs) have achieved rapid development in past decades
∗Corresponding authors.
Figure 1. Training time and backward time cost in one epoch with different time steps. Classic SG method [7] experiences an ex-ponential growth of training time when number of time steps in-creases. By contrast, SSF method signiﬁcantly decreases training time cost, especially when training SNNs with long time steps. and even outperform human beings in areas like image classiﬁcation [14], semantic segmentation [38] and object detection [25]. However, the promising performance of
ANNs builds on the expense of substantial computational resources and power consumption during both training and inference, limiting its application in edge devices.
Known as the third generation of neural networks, Spik-ing Neural Network (SNN) simulates biological neuron’s action potential process which accumulates inputs as mem-brane potential and generates a binary spike when exceed-ing threshold. This simulation brings SNN with special neural dynamics, strong biological plausibility, and promis-ing high energy efﬁciency on neuromorphic hardwares [26].
These advantages make SNN potential in solving many problems ANN encountered, including limited energy sup-plies in edge devices mentioned before. Hence, SNN has drawn a lot of attention of researchers in recent years.
Yet still training SNN is a big challenge since the dis-crete nature of binary spikes hampers the effective usage of gradient-based backpropagation (BP) methods on the train-ing of SNN [31]. Among all the studies, one effective idea
is using some auxiliary variable approximating the non-differentiable part during back propagation. Based on this idea comes surrogate gradient (SG) method, which trains
SNNs in the same way of training recurrent neural net-works (RNNs) with the help of continuous relaxation of the real gradients [36]. With several years’ development, SG method enables SNNs to obtain high accuracy and be pro-moted to very deep neural networks [7]. However, the com-putation cost for SG is unacceptably high. As illustrated in
Fig. 1, when SNNs’ time steps grow longer, time needed in training process increases exponentially. This creates an awkward situation that bigger number of time steps usu-ally leads to better performance as longer spike trains can represent more complex information [7, 12, 41] (shown in
Fig. 4). Therefore, it is desired to speed up the training of SG-based SNNs, especially with long time steps, while maintaining competitive performance.
In this paper we propose an acceleration method, dubbed
Stabilized Spiking Flow (SSF), for efﬁciently training
SNNs. We ﬁrst formalize the backward process of clas-sic SG method and identify that the main contributor to the exponential growth of the training time is SNNs’ mem-brane accumulation process which has to calculate gradi-ents sequentially to back-propagation in time domain. To decrease the temporal correlation between time steps, SSF approximates output and input spike trains in each layer with stabilized inputs and outputs. Through the built map-ping between stabilized inputs and outputs, SSF skips the membrane accumulation process and thus greatly speeds up the training process. As shown in Fig. 1, the effect of our method becomes more pronounced when time steps grow longer. Our main contributions are summarized as follows: 1. We systematically study the backward process of clas-sic SG method and ﬁnd that the exponential growth of training cost is mainly caused by SNNs’ membrane accumulation process. 2. To deal with the problem, we propose SSF method.
SSF approximates each layers’ input and output spike trains with stabilized spiking ﬂows and skips the cum-bersome backpropagation in time domain by building relationship between inputs and outputs directly. 3. The sufﬁcient experiments on both static and neuro-morphic datasets prove the effectiveness of SSF. Par-ticularly, SSF can speed up the backward process of state-of-the-art models up to 10× while achieving nearly the same classiﬁcation accuracy. 2.