Abstract
Image manipulation detection (IMD) is of vital impor-tance as faking images and spreading misinformation can
IMD is the core be malicious and harm our daily life. technique to solve these issues and poses challenges in two main aspects: (1) Data Uncertainty, i.e., the manip-ulated artifacts are often hard for humans to discern and lead to noisy labels, which may disturb model training; (2)
Model Uncertainty, i.e., the same object may hold differ-ent categories (tampered or not) due to manipulation oper-ations, which could potentially confuse the model training and result in unreliable outcomes. Previous works mainly focus on solving the model uncertainty issue by designing meticulous features and networks, however, the data uncer-tainty problem is rarely considered. In this paper, we ad-dress both problems by introducing an uncertainty-guided learning framework, which measures data and model uncer-tainties by a novel Uncertainty Estimation Network (UEN).
UEN is trained under dynamic supervision, and outputs es-timated uncertainty maps to refine manipulation detection results, which significantly alleviates the learning difficul-ties. To our knowledge, this is the first work to embed uncer-tainty modeling into IMD. Extensive experiments on various datasets demonstrate state-of-the-art performance, validat-ing the effectiveness and generalizability of our method. 1.

Introduction
Seeing is no longer believing! With the development of modern image editing tools, such as Photoshop, Snapseed etc., editing and sharing images have become easier and more popular. However, the abuse of manipulated im-ages can also cause various problems that affect our daily life negatively. In addition, manipulated images are often realistic-looking, making them difficult and laborious for humans to discern. Thus, it is critical to develop effective methods for image manipulation detection (IMD).
*Corresponding author: bobblair.wj@antgroup.com
Figure 1: Data uncertainty refers to noisy labels as shown in the red bounding box of the first row where the child is removed from the original position but is labeled as untam-pered. Such data uncertainty also shows in the red mask boundary in the second row where the blurring operation makes it hard to label. Besides, model uncertainty is caused by the fact that the same object may have different labels, confusing the model training. In the first row, the same child in the original and manipulated images are labeled differ-ently.
The main challenge of the IMD task is twofold: data un-certainty and model uncertainty. The former is induced by the fact that the manipulated artifacts are often hard to dis-cern, posing a great challenge to annotation and resulting in noisy labels. Humans can effortlessly distinguish ma-nipulated content by comparing original and manipulated images, e.g. comparing (a) and (b) in Figure 1, the differ-ent regions between them are manipulated areas. However, during the real-word labeling process, original images are usually unavailable, making annotations prone to errors. As shown in the first row, the red bounding box indicates the child has been removed, which is easily overlooked by an-notators and marked as untampered. In the second row, we draw the mask boundary in red on the manipulated image.
The blurring operation makes the boundary difficult to iden-tify, resulting in an inaccurate boundary. Such data uncer-tainty problem poses great challenges to model learning.
As for model uncertainty, it’s caused by the fact that la-bels of the same visual content may be inconsistent in dif-ferent images. As shown in the top row of Figure 1, for con-ventional semantic segmentation tasks, the child is always labeled as the same category across different images, which is semantically consistent. While in this task, the same child can be labeled as both tampered and untampered, depending on whether the object is modified, which potentially con-fuses model learning and leads to unreliable outputs.
For the IMD research community, the primary focus of many methods is to design meticulous network structures and features that can mitigate model uncertainty. Various techniques have been proposed to detect digital image ma-nipulations, such as those based on artifacts from resam-pling [5], color filter arrays [29], and SRM [43]. MVSS-Net [11] takes this a step further by suppressing image content through the learning of manipulated region edges.
However, these approaches tend to address model uncer-tainty implicitly, and some manipulation artifacts are too subtle to capture, which can lead to over-segmentation, under-segmentation, and phantom-segmentation during the model prediction process. Furthermore, few works focus on data uncertainty. Conventional methods [44, 9, 2] and deep learning based methods [41, 35] are able to generate manip-ulated data with accurate labels, and may alleviate the data uncertainty issue to some extent. Nevertheless, the gener-ated data is not identically distributed with the read-world manipulation data. So these methods fail to confront the data uncertainty problem in practical cases.
To reveal the two kinds of uncertainties, a straightfor-ward solution is to estimate them via uncertainty estima-tion techniques. In these techniques, the data uncertainty and model uncertainty are also called aleatoric uncertainty and epistemic uncertainty respectively [21, 19]. The former refers to the noise inherent in the observations, which can not be reduced even if more data were to be collected. The latter accounts for uncertainty in the model and captures the lack of representativeness of the model, which can be ex-plained away with increasing training data. Plenty of works have been proposed to model the two types of uncertain-ties. They usually utilize Bayesian Neural Network (BNN) to learn mappings from input data to data uncertainty and compose these together with model uncertainty approxima-tions [21]. The main issue of BNN for uncertainty estima-tion is the intractable posterior inference, thus most existing uncertainty estimation techniques focus on designing ap-proximate posterior inference [13, 21, 14, 17, 4]. [13] uti-lizes dropout variational inference as a practical approach for approximate inference in large and complex models.
[21] uses Monte Carlo Sampling to approximate posterior inference. Following [21], we adopt the Monte Carlo Sam-pling for uncertainty approximation.
In our work, we propose an uncertainty-guided learn-ing framework that incorporates a novel Uncertainty Esti-mation Network (UEN) to capture data and model uncer-tainty. UEN is comprised of two key components, namely
Dynamic Uncertainty Supervision (DUS) and Uncertainty
Prediction Refinement (UPR). Specifically, we derive the difference between the predicted results and ground truth as the dynamic uncertainty supervision for UEN. Thanks to the delicate design of DUS, the data uncertainty and model uncertainty maps are precisely estimated and are further integrated to refine the manipulation predictions in UPR.
With the above designs, our method is capable of identify-ing over-segmentation, under-segmentation, and phantom-segmentation regions by assigning high uncertainty to mis-classified areas and assigning low uncertainty to correctly classified regions. Extensive experiments demonstrate the effectiveness and generalizability of our method.
The main contributions of this paper are summarized as:
• We develop a new learning framework for image ma-nipulation detection, which is applicable to existing segmentation-based manipulation detection methods.
As far as we know, we are the first to introduce un-certainty modeling into image manipulation detection to resolve the challenges.
• We address data uncertainty and model uncertainty by explicitly modeling them. With dynamic supervision, our method is able to output accurate uncertainty es-timation results, which are further integrated with the
UPR module to improve the manipulation predictions.
• We achieve state-of-the-art results on four benchmarks for image manipulation detection. Extensive experi-ments on multiple benchmarks demonstrate the supe-riority and the generalizability of our method. 2.