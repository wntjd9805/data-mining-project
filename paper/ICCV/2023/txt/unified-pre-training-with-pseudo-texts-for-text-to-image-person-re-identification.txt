Abstract
The pre-training task is indispensable for the text-to-image person re-identification (T2I-ReID) task. However, there are two underlying inconsistencies between these two tasks that may impact the performance: i) Data inconsis-tency. A large domain gap exists between the generic im-ages/texts used in public pre-trained models and the specific person data in the T2I-ReID task. This gap is especially se-vere for texts, as general textual data are usually unable to describe specific people in fine-grained detail. ii) Train-ing inconsistency. The processes of pre-training of images and texts are independent, despite cross-modality learning being critical to T2I-ReID. To address the above issues, we present a new unified pre-training pipeline (UniPT) de-signed specifically for the T2I-ReID task. We first build a large-scale text-labeled person dataset “LUPerson-T”, in which pseudo-textual descriptions of images are auto-matically generated by the CLIP paradigm using a divide-conquer-combine strategy. Benefiting from this dataset, we then utilize a simple vision-and-language pre-training framework to explicitly align the feature space of the im-age and text modalities during pre-training. In this way, the pre-training task and the T2I-ReID task are made consis-tent with each other on both data and training levels. With-out the need for any bells and whistles, our UniPT achieves competitive Rank-1 accuracy of, i.e., 68.50%, 60.09%, and 51.85% on CUHK-PEDES, ICFG-PEDES and RSTPReid, respectively. Both the LUPerson-T dataset and code are available at https://github.com/ZhiyinShao-H/UniPT. 1.

Introduction
Text-to-image person re-identification [23] (T2I-ReID) is a retrieval task that aims to search specific person im-*Equal contribution. †Corresponding author.
Figure 1. Pipeline comparison on (a) general pre-training in pre-vious works, (b) T2I-ReID tasks, and (c) our unified pre-training (UniPT). Our UniPT shares a similar format with the T2I-ReID pipeline at both the data and training levels. ages based on natural language descriptions [23]. Com-pared with large-scale datasets in general image-text re-trieval tasks [34, 24, 51], existing T2I-ReID datasets suffer from limited scale and diversity. Therefore, the prior knowl-edge learned through large-scale pre-training is critical if good performance on the T2I-ReID task is to be achieved.
Previous works [60, 56, 44, 25, 30] simply utilize pub-licly released pre-trained models that have learned from
In more detail, the visual encoders [14, 6] generic data. are usually pre-trained on ImageNet [33], while the tex-tual encoders [16, 4] learn on hundreds of millions of words [65, 4]. These models naturally serve as the initial parameters of visual and textual encoders in T2I-ReID. The existing literature [48, 35, 8, 57, 42, 5] thus pays more at-tention to designing various cross-modality modules to mit-igate the modality gap [48, 35, 8] and the part-alignment problems [57, 42, 5] instead. No extant research has yet investigated how the pre-training task affects the following
T2I-ReID task.
In this paper, we make the first attempt to explicitly re-veal the inconsistencies between the existing pre-training task and the T2I-ReID task. As shown in Figure 1, there are two main inconsistencies: i.e., data and training. Regarding the data inconsistency, we observe that the pre-training data are obtained from various objects and scenes, while the T2I-ReID data are specific to pedestrians: a large domain gap exists between these two kinds of datasets. A similar ob-servation can be made with respect to image-based person
ReID [9, 28, 10]. Accordingly, [9] proposes a large-scale person dataset, i.e., “LUPerson”. Despite the possibility of initializing the visual encoder with LUPerson pre-trained models, this approach is not optimal due to the remaining textual gap. In fact, the generic text data fail to describe the person in the kind of fine-grained detail that is crucial to capturing an individual person’s characteristics. This high-lights the requirement of a large-scale image-text person dataset for the pre-training. Moreover, regarding training inconsistency, it is worth noting that the initial visual and textual models in previous methods are pre-trained indepen-dently. This approach might not be suitable for T2I-ReID, since the interaction between images and texts key to nar-rowing down the modality gap. We therefore consider how to directly apply the modality interaction during the pre-training to match the process in T2I-ReID.
To this end, we present a novel unified pre-training pipeline, UniPT, which is specifically designed for the T2I-ReID task. Enabled by LUPerson [9], we create a new text-labeled variant, i.e., LUPerson-T, for the model pre-training. Removing the labor and cost associated with man-ual annotation, the pseudo-text descriptions are derived au-tomatically using a novel divide-conquer-combine strategy based on CLIP [32]. Specifically, we first divide the person characteristics into attribute phrases, e.g., “a blue striped shirt” and “long hair” (divide stage). Next, we convert these phrases to prompts, and input them into the frozen CLIP model to obtain the most matched attributes for specific per-son images (conquer stage). Subsequently, we insert the matched attributes into pre-defined templates to obtain com-plete pseudo-texts. Moreover, we adopt synonym substitu-tion to enrich the diversity of descriptions (combine stage).
Despite the noise contained in the pseudo-texts, LUPerson-T focuses describing individual persons in detail, effectively reducing the data domain gap between the pre-training and the T2I-ReID tasks.
Based on the LUPerson-T dataset, we simply use the vision-and-language pre-training framework in the pre-Inspired by CLIP [32], we employ the training process. contrastive loss on pairs of person images and their pseudo-texts. Meanwhile, we apply the masked language model objective [4] to prevent the underlying over-fitting. In this way, the features of images and texts can be made to align with each other during the pre-training, ensuring that the training processes of the pre-training and T2I-ReID tasks
Task
Dataset
Person Image
Size
Only
Pre-training
T2I-ReID
×
×
✓
✓ 1.3M
ImageNet [33]
-Text data in [4] 4M
LUPerson [9]
LUPerson-T (Ours) 1.3M
CUHK-PEDES [23] ✓ 40,206
✓ 54,522
ICFG-PEDES [5]
✓ 20,505
RESPReid [63]
Text
Size
-Source
-3.3B Words Collected
-1.3M 80,412 54,522 40,110
-Generated
Labeled
Labeled
Labeled
Table 1. The statistics comparison on the pre-training datasets and the T2I-ReID datasets. are consistent. follows:
In summary, our key contributions are as
• We first reveal the data and training inconsistencies be-tween the pre-training and T2I-ReID tasks.
• We build a text-labeled dataset LUPerson-T, in which the pseudo text descriptions are automatically gener-ated by the proposed divide-conquer-combine strategy.
• We propose a unified pre-training pipeline, namely
UniPT, on LUPerson-T to make the pre-training and the T2I-ReID task consistent at both the data and train-ing levels.
• We conduct comprehensive experiments and analyses to show the effectiveness of our UniPT. Our method outperforms current state-of-the-art methods on three benchmarks. 2.