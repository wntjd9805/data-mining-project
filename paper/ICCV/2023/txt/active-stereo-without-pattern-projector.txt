Abstract
This paper proposes a novel framework integrating the principles of active stereo in standard passive camera sys-tems without a physical pattern projector. We virtually project a pattern over the left and right images according to the sparse measurements obtained from a depth sensor.
Any such devices can be seamlessly plugged into our frame-work, allowing for the deployment of a virtual active stereo setup in any possible environment, overcoming the limita-tion of pattern projectors, such as limited working range or environmental conditions. Experiments on indoor/outdoor datasets, featuring both long and close-range, support the seamless effectiveness of our approach, boosting the accu-racy of both stereo algorithms and deep networks. 1.

Introduction
Depth perception is crucial in several computer vision tasks, including autonomous driving, 3D reconstruction, robotics, and augmented reality. Inferring depth from stan-dard cameras, according to different setups and strategies, is one of the most widely deployed techniques due to its low cost and potentially unbounded image resolution. At the core of these approaches, using multiple cameras or a moving one, there is the problem of determining visual correspondence. However, matching points across frames is inherently ambiguous in the presence of textureless re-gions, repetitive patterns, and non-Lambertian materials.
This task is even more challenging when performed densely for each pixel in the input images. Although deep learning has achieved excellent results, as witnessed by the recent literature [53], it is also prone to the well-known domain shift issue absent in conventional, less accurate hand-crafted methods. Specifically, since learning-based methods rely on training data, they suffer severe drops in accuracy when fac-ing different data distributions [52]. A different approach to depth perception relies on active sensing technologies, such as LiDAR (Light Detection and Ranging), ToF (Time of
Flight), and Radar (Radio Detection and Ranging). How-ever, each technology has limitations. LiDAR technology is reliable but features a density much lower than the res-olution of modern cameras, making it extremely expensive as density increases. ToF suffers from the same limitation and is also unreliable under sunlight and at longer distances.
Radar allows for a more extended depth range sensing, but it is sparser, noisier, and with a narrower vertical field of
view [39]. Finally, active systems that estimate depth from images also exist, relying on structured [23] or unstructured
[31] pattern projection results more accurate than passive imaging techniques and at higher resolution with respect to the aforementioned devices. However, these systems are bounded by the need for the projected pattern to be clearly visible in images, and thus cannot work beyond very close distances (i.e., a few meters), are unsuited for outdoor use with sunlight, and the presence of multiple projectors might make them interfere.
Due to their complementary strengths and limita-tions, setups made of active and passive technologies are widespread for several application fields, ranging from au-tonomous driving, where almost all prototypes have het-erogeneous sensor suites, to augmented reality with smart-phones and tablets equipped with cameras and active depth sensors. Consequently, different solutions exist in the lit-erature to exploit the synergy between active and passive depth sensing [51, 13, 73]. The common key trait of most of these sensor-fusion methods consists in modifying the in-ternal behavior of the camera-based stereo matcher or con-catenating the sparse points with the color images. In con-trast, this paper proposes a novel paradigm to leverage the synergy between active and passive sensing. It works by coherently hallucinating the vanilla stereo pair acquired by a standard camera simplifying the visual correspondence task performed by any stereo network/algorithm as if a vir-tual pattern projector were present in the scene. Such vir-tual coherent pattern projection is feasible by exploiting the stereo geometry and a registered active depth sensor provid-ing sparse yet accurate measurements, like in [51, 13, 73].
Our proposal shares the same motivations of active methods based on unstructured pattern projection. However, unlike these strategies, it does not rely on a specific physical pat-tern projector with all the limitations outlined previously.
Instead, by selecting the depth sensor that is better suited for the specific scenario, our approach can work in any en-vironment and is agnostic to moving objects and camera ego-motion, as shown in Fig. 1. Experimental results on standard stereo datasets support the following claims:
• Even with meager amounts of sparse depth seeds (e.g., 1% of the whole image), our approach outperforms by a large margin state-of-the-art sensor fusion methods based on handcrafted algorithms and deep networks.
• When dealing with deep networks trained on synthetic data, it dramatically improves accuracy and shows a compelling ability to tackle domain shift issues, even without additional training or fine-tuning.
• By neglecting a physical pattern projector, our solution works under sunlight, both indoors and outdoors, at long and close ranges with no additional processing cost for the original stereo matcher.
We believe that our proposal, dubbed Virtual Pattern
Projection (VPP), has the potential to become a standard component for depth perception and pave the way to excit-ing future developments in the field. 2.