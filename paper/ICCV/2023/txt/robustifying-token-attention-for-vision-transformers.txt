Abstract
Despite the success of vision transformers (ViTs), they still suffer from significant drops in accuracy in the presence of common corruptions, such as noise or blur. Interestingly, we observe that the attention mechanism of ViTs tends to rely on few important tokens, a phenomenon we call token overfocusing. More critically, these tokens are not robust to corruptions, often leading to highly diverging attention patterns. In this paper, we intend to alleviate this overfo-cusing issue and make attention more stable through two general techniques: First, our Token-aware Average Pool-ing (TAP) module encourages the local neighborhood of each token to take part in the attention mechanism. Specif-ically, TAP learns average pooling schemes for each token such that the information of potentially important tokens in the neighborhood can adaptively be taken into account.
Second, we force the output tokens to aggregate informa-tion from a diverse set of input tokens rather than focusing on just a few by using our Attention Diversification Loss (ADL). We achieve this by penalizing high cosine similarity between the attention vectors of different tokens. In experi-ments, we apply our methods to a wide range of transformer architectures and improve robustness significantly. For ex-ample, we improve corruption robustness on ImageNet-C by 2.4% while improving accuracy by 0.4% based on state-of-the-art robust architecture FAN. Also, when fine-tuning on semantic segmentation tasks, we improve robustness on
CityScapes-C by 2.4% and ACDC by 3.0%. Our code is available at https://github.com/guoyongcs/TAPADL. 1.

Introduction
Despite the success of vision transformers (ViTs), their performance still drops significantly on common image cor-ruptions such as ImageNet-C [24, 52, 19], adversarial ex-amples [18, 16, 40, 56], and out-of-distribution examples as benchmarked in ImageNet-A/R/P [64, 24]. In this paper, we examine a key component of ViTs, i.e., the self-attention mechanism, to understand these performance drops. Inter-estingly, we discover a phenomenon we call token overfo-Figure 1. Stability against image corruptions in terms of attention visualization (left, a matrix of 196×196) and cosine similarity of attention between clean and corrupted examples (right). Left: We average the attention maps across different heads for visualization and show the results of the last layer. We observe that ViTs put too much focus on very few tokens, a phenomenon we call token overfocusing. More critically, the attention of the baseline FAN model [16] is fragile to image corruptions, e.g., with Gaussian noise. Our approach, in contrast, alleviates token overfocusing and thereby improves stability of the attention against corruptions.
Right: On ImageNet, we plot the distribution of cosine similarities across all layers (without averaging heads) between clean and cor-rupted examples. We show that our model yields a significantly higher similarity score than the baseline model. cusing, where only few important tokens are relied upon by the attention mechanism across all heads and layers. We hypothesize that this overfocusing is particularly fragile to the corruptions on input images and greatly hampers the ro-bustness of the attention mechanism.
Starting from the state-of-the-art robust architecture
FAN [67], we exemplarily investigate the last attention layer (see attention of other layers in supplementary). Specifi-cally, Figure 1 shows a clean and a corrupted input image as well as the corresponding attention maps. These are ma-trices of N × N , with N being the number of input/output tokens. Here, the i-th row indicates which input tokens (columns) the i-th output token “attends” to – darker red indicates higher attention scores. Token overfocusing can then, informally, be defined by observing pronounced verti-cal lines in the attention map: First, each output token (row) focuses on only few important input tokens, ignoring most
of the other information. Second, all output tokens seem to focus on the same input tokens, leading to a very low diver-sity among the attention vectors in different rows. We high-light that the overfocusing issue is present throughout the entire ImageNet dataset, and also across diverse architec-tures (see more examples in Figure 3 and supplementary).
Moreover, this overfocusing issue has also been observed in existing works. For example, Figure 5 of [10] observes very few important tokens (in deep red color) in layers 3∼6;
Figure 1 of [16] also reports very few important tokens (in yellow color). More critically, we find that these important tokens are extremely fragile in the presence of common cor-ruptions. To be specific, when applying Gaussian noise on the input image, the tokens recognized as important change entirely, see Figure 1 (left, second column). Quantitatively, this can be captured by computing the cosine similarity be-tween the clean and corrupted attention maps. Unsurpris-ingly, as shown by the blue box in Figure 1 (right), the co-sine similarity is indeed extremely low, confirming our ini-tial hypothesis. This motivates us to robustify the attention by alleviating the token overfocusing issue.
To this end, we encourage the attention module to focus on diverse input tokens. This can be achieved by changing the patterns in both columns and rows of an attention map, which motivates the two key components of our method.
First, when comparing attention columns in Figure 1, very few tokens are important, leading to a fragile attention. To address this, we encourage output tokens to not only focus on individual input tokens but take into account the local neighborhood around these tokens, in order to make more tokens (columns) contribute meaningful information. Intu-itively, an individual token itself may not be important but can be enhanced by aggregating the information from po-tentially important tokens located within its neighborhood.
We achieve this using a learnable average pooling mecha-nism applied to each input token to aggregate information before computing self-attention. Second, when comparing attention rows in Figure 1, most output tokens focus on the same input tokens. Once these tokens are distorted, the whole attention may fail. To address this issue, we seek to diversify the set of input tokens (columns) that the out-put tokens (rows) rely on. We achieve this using a loss that explicitly penalizes high cosine similarity across rows. In
Figure 1, the combination of these techniques leads to more balanced attention across columns and more diverse atten-tion across rows. More critically, these attention maps are more stable in the light of image corruptions. Again, we quantitatively confirm this on ImageNet using the cosine similarity which is significantly higher.
Overall, we make three key contributions: 1) we pro-pose a Token-aware Average Pooling (TAP) module that encourages the local neighborhood of tokens to participate in the self-attention mechanism. To be specific, we conduct averaging pooling to aggregate information and control it by adjusting the pooling area for each token. 2) We further de-velop an Attention Diversification Loss (ADL) to improve the diversity of attention received by different tokens, i.e., rows in the attention map. To this end, we compute the attention similarity between rows in each layer and mini-mize it during training. 3) We highlight that both TAP and
ADL can be applied on top of diverse transformer archi-tectures. In the experiments, the proposed methods consis-tently improve the model robustness on out-of-distribution benchmarks by a large margin while preserving a compet-itive improvement of clean accuracy at the same time. For example, we improve robustness against corruptions and distribution shifts on ImageNet-A/C/P/R by at least 1.5%, as shown in Table 2. In addition, the improvement also gen-eralizes well to other downstream tasks, e.g., semantic seg-mentation, with an improvement of 2.4% in Table 4. 2.