Abstract
Interactions between humans are diverse and context-dependent, but previous works have treated them as cate-gorical, disregarding the heavy tail of possible interactions.
We propose a new paradigm of learning human-human in-teractions as free text from a single still image, allowing for flexibility in modeling the unlimited space of situations and relationships between people. To overcome the absence of data labelled specifically for this task, we use knowledge distillation applied to synthetic caption data produced by a large language model without explicit supervision. We show that the pseudo-labels produced by this procedure can be used to train a captioning model to effectively understand human-human interactions in images, as measured by a va-riety of metrics that measure textual and semantic faith-fulness and factual groundedness of our predictions. We further show that our approach outperforms SOTA image captioning and situation recognition models on this task.
We will release1 our code and pseudo-labels along with
Waldo and Wenda, a manually-curated test set for still im-age human-human interaction understanding. 1.

Introduction
“No man is an island entire of itself.”
-John Donne
Humans are social beings. As such, interactions among people are ubiquitous and diverse, affected by various fac-tors including social context and cultural norms. Reason-ing about these interactions is crucial for gaining a holistic understanding of visual scenes depicting people. However, in spite of significant progress in analyzing isolated human actions [29, 73, 79] and relationships between entities and objects [27, 83], far less attention has been devoted towards an automatic understanding of human-human interactions (HHI). This is despite the importance of this task for appli-cations such as interactive robotics, social behaviour under-standing, and captioning systems for the visually impaired.
There are a number of factors that make the analysis of
Figure 1. How would you describe the interactions depicted in these images? There are unlimited possible interactions between people which cannot be easily described by a fixed set of cate-gories or actions. Context plays a crucial role, as in the left image where the clothing and cake in the background help to interpret the depicted interaction. Moreover, interactions may be involve participants at a physical distance as in the image on the right. To model the heavy tail of possible interactions, we propose to learn
HHI as free text (see below2 for predictions using our method).
HHI difficult. The space of possible interactions between people is vast and requires understanding social context and physically non-local relationships, as illustrated in Figure 1.
In addition, images depicting HHI may have multiple inter-pretations, some of which may be simultaneously correct.
For example, the image on the left might depict “celebrat-ing a wedding” as well as “dancing”. Contextual cues such as the cake in the background of the image provide addi-tional information that hints at the depicted HHI.
Prior works targeting HHI understanding focus on a small fixed number of interactions; representative works in-clude [63, 65, 43, 30], all of whose models are trained to recognize no more than ten interaction classes. In this work, we are interested in modeling the heavy tail of possible HHI to better understand the rich variety of ways in which people interact. To this aim, we propose to model HHI understand-ing as free text generation; since HHI are not confined to a fixed set of categories or even to a syntactic class such as verbs, HHI as free text enables the expression of an infinite variety of possible interactions. Furthermore, in contrast to previous works that frequently rely on extra context such as video data [60], we use a single image with no addi-tional information (during inference), making our method more widely applicable. We focus on what Stergiou and 1via our project page https://learning-interactions. 2A model fine-tuned on our pseudo-labels yields “dancing” and “hav-github.io ing a picnic”.
Poppe [60] term dyadic interactions—pairwise interactions between two people. Our goal is to identify the most salient dyadic interaction given an image of two or more people interacting.
One of the primary challenges to modeling HHI is a scarcity of labelled data for this particular task. There are only a handful of relatively small datasets specific to HHI, and larger video datasets for action recognition are lack-ing in coverage of interactions (see Table 1). To better model the heavy tail of possible HHI, we leverage the abun-dance of high-quality images of people and associated tex-tual captions available on the Internet. In particular, we use the Who’s Waldo dataset [13] that contains 270K image-caption pairs from Wikimedia Commons depicting people captured in a broad range of situations. Unlike many other image captioning datasets, Who’s Waldo focuses on human-centric situations which are described using real-world cap-tioned Internet data, and thus is more relevant to HHI un-derstanding. However, it is extremely challenging to learn
HHI from raw Internet captions directly, due to significant noise introduced by clutter and irrelevant details. To over-come this, we infer interactions from the original captions by applying knowledge distillation to synthetic data gener-ated by a large language model, without explicit supervi-sion. This approach allows for creating accurate pseudo-labels that provide textual descriptions of the HHI depicted in the images. We will release these pseudo-labels along with a manually annotated test set containing 1K image-interaction pairs from diverse Internet images which we name Waldo and Wenda, a new benchmark for our paradigm of HHI understanding as free text on still images, capturing the heavy tail of human-human interactions.
We demonstrate the utility of these pseudo-labels for learning HHI from images by training captioning models and using them as targets for a language modelling objec-tive. We provide qualitative and quantitative analysis on the Waldo and Wenda test set; in addition, we evaluate this method on a larger scale by applying it to verb prediction on the imSitu situation recognition dataset [75], which we filter to select for images relevant to HHI.
Because we predict HHI as free text rather than categor-ically as in previous works, we propose a set of evaluation metrics chosen to measure important aspects of predicted
HHI quality, namely textual similarity, factual grounded-ness, and verb similarity. Our evaluation shows that our
HHI pseudo-labels allow for generating meaningful HHI free text descriptions from images, as measured by these metrics. We also show that learning on these pseudo-labels captures HHI substantially more effectively than either us-ing existing SOTA image captioning models as-is or than training on interactions extracted with naive syntactic pars-ing. Explicitly stated, our key contributions are:
• A new paradigm and benchmark for HHI understand-ing from images—i.e., predicting interactions as free text—allowing to better understand the vast variety of ways in which people interact.
• A method for isolating HHI from noisy Internet cap-tions using knowledge distillation applied to a large language model, and a set of pseudo-labels generated by this method.
• An evaluation framework with metrics that capture
HHI understanding, and results demonstrating that training image captioning models on these pseudo-labels can allow for modeling the heavy tail of possible
HHI across various situations and configurations more effectively than SOTA image captioning and situation recognition models. 2.