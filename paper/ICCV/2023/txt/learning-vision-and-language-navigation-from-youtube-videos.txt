Abstract
Vision-and-language navigation (VLN) requires an em-bodied agent to navigate in realistic 3D environments using natural language instructions. Existing VLN methods suffer from training on small-scale environments or unreasonable path-instruction datasets, limiting the generalization to un-seen environments. There are massive house tour videos on
YouTube, providing abundant real navigation experiences and layout information. However, these videos have not been explored for VLN before. In this paper, we propose to learn an agent from these videos by creating a large-scale dataset which comprises reasonable path-instruction pairs from house tour videos and pre-training the agent on it. To achieve this, we have to tackle the challenges of auto-matically constructing path-instruction pairs and exploiting real layout knowledge from raw and unlabeled videos. To address these, we ﬁrst leverage an entropy-based method to construct the nodes of a path trajectory. Then, we propose an action-aware generator for generating instruc-tions from unlabeled trajectories. Last, we devise a trajec-tory judgment pretext task to encourage the agent to mine the layout knowledge. Experimental results show that our method achieves state-of-the-art performance on two popu-lar benchmarks (R2R and REVERIE). Code is available at https://github.com/JeremyLinky/YouTube-VLN 1.

Introduction
An important goal of embodied artiﬁcial intelligence is to develop agents that can interact with humans in natural language to carry out real-world tasks. Toward this goal, vision-and-language navigation (VLN) [1] is a rudimentary artiﬁcial intelligence task, requiring an indoor agent to nav-igate in unseen environments following natural instructions.
VLN has attracted widespread attention in the ﬁelds of com-puter vision and robotics due to its promising applications such as in-home robots [54] and warehouse assistants [29].
One of the key challenges of VLN is the generaliza-*Equal contribution. Email: {imkunyanglin, phchencs}@gmail.com
†Corresponding author. Email: mingkuitan@scut.edu.cn tion ability of agents to unseen environments. Existing
VLN methods attempt to cope with this challenge via self-supervised pre-training on vision-and-language datasets.
As shown in Figure 1 (a), some previous works [30, 19, 36, 7] learn the agents on simulated navigation environments and manual-labeled data. The other works [14, 13, 42] seek to construct path-instruction pairs by using web image data, which is shown in Figure 1 (b). Despite their promising performance, existing agents still suffer from the follow-ing limitations. 1) Training on simulated datasets is limited to a restricted number of environments. 2) Constructing a trajectory by simply concatenating web images leads to un-reasonable room layouts, which hamper the agent to learn layout reasoning ability. As a result, VLN agents trained on such data are brittle to adapt to unseen environments.
Fortunately, there are massive house tour videos on
YouTube, providing real navigation experiences and layout information but are still under-explored. We can be natu-rally inspired to let an agent learn VLN ability from such videos, thereby addressing the limitations of existing meth-ods. An intuitive way is to model the navigation experi-ences as path-instruction pairs to train the agent. Motivated by this, we propose a “Lily” agent who Learns Vision-and-Language Navigation from YouTube Videos. Speciﬁcally, we ﬁrst develop an in-domain pre-training dataset from house tour YouTube videos, namely YouTube-VLN, which comprises VLN-like path-instruction pairs. Our YouTube-VLN dataset has the advantages of diverse environments, real layouts, and native actions1, reducing the domain gap with VLN datasets, as illustrated in Figure 1 (c). Then, we pre-train the agent using these path-instruction pairs. Bene-ﬁting from in-domain pre-training on our proposed dataset, our agent thus generalizes well to unseen environments.
Constructing and utilizing such a dataset, however, is still far from trivial work and remains an open problem due to the following challenges. 1) As the nodes in a trajec-tory are expected to be diverse and informative, it is hard to determine the locations of trajectory nodes from massive video frames and represent the visual content in a node. 1The execution actions that objectively exist
• Restricted Number of Environments
• Confused Layouts
• Ambiguous Actions
•• D
• Diverse Environments s •• R
• Real Layouts s •• N
• Native Actions (a) Dataset from Simulation Environments (b) Dataset from Web Images Data (c) Dataset from YouTube Videos (Ours)
Figure 1: Comparison of different datasets for pre-training. Existing datasets are built from either simulation environ-ments (a) or web images data (b). The former only covers limited environments, and the latter contains confusing layouts and ambiguous actions. Our dataset built from YouTube videos (c) is able to provide diverse environments, real layouts and native actions simultaneously. 2) Real VLN instructions include various action descrip-tions, but obtaining corresponding instructions from navi-gation clips is challenging due to the actions being implicit in videos. Thus it is nontrivial to acquire matching instruc-tion on a trajectory. 3) Layout knowledge from real naviga-tion experience is hard to mine and model, which impedes the agent of learning layout reasoning ability.
In this paper, we address the above challenges as follows.
To conquer challenge 1), we propose an entropy-based tra-jectory generation method. Speciﬁcally, we ﬁrst envisage that the nodes of a trajectory should contain as many types of rooms as possible to diversify trajectories. Accordingly, we group the frames with the same room types in videos and consider each group as a node in the trajectory. Then, inspired by that low classiﬁcation entropy image is reliable and contains rich information relevant to a speciﬁc class (room type in our case) [39], the frame with the lowest clas-siﬁcation entropy in a group is chosen to represent the visual content in a node. To tackle challenge 2), we introduce an action-aware instruction generation method. Speciﬁcally, we adopt an action inverse model to pseudo-label the action along trajectories and ﬁll them in the instructions via hand-designed rules. To grapple with challenge 3), we devise a self-supervised pretext task. As we all know, humans often judge whether a navigation trajectory is reasonable based on the layout of the environment. Therefore, it is believed that an agent equipped with layout reasoning ability should be able to make similar judgment. To this end, we propose trajectory judgment pretext task to ask the agent to iden-tify reasonable navigation trajectories, which further equips the model with the ability to reason environment layouts.
We empirically show that the diverse entropy-based trajectory generation method and action-aware instruction generator allow us to harvest high-quality path-instruction pairs from YouTube house tour videos, resulting in the
YouTube-VLN dataset. By integrating the self-supervised trajectory judgment task in pre-training a VLN agent, our
Lily agent presents state-of-the-art performance on two ma-ture and solid benchmarks (R2R [1], REVERIE [41]). The proposed Lily agent reaches the ﬁrst place on the R2R leaderboard in terms of success rate and outperforms the
SOTA method under discriminative setting and generative setting by 2% and 3% w.r.t. success rate, respectively.
Our main contributions are as follows:
• We unleash the huge potential of house tour videos for VLN. By leveraging these videos, we introduce a large-scale dataset containing real navigation path-instruction pairs for promoting VLN pre-training and a self-supervised pretext task for the learning of layout reasoning.
• Our diverse trajectory generation method, together with the action-aware instruction generator, creates infor-mative and diverse trajectory nodes and produces matching instructions, both of which make the path-instruction pairs authentic and of high quality for training a VLN agent.
• The proposed trajectory judgment pretext task allows the model to build up an awareness of learning and reason-ing the layout knowledge, which is crucial in the VLN task of indoor environments. We also empirically substantiate that the agent indeed learns the layout learning ability. 2.