Abstract
Existing visual tracking methods typically take an im-age patch as the reference of the target to perform tracking.
However, a single image patch cannot provide a complete and precise concept of the target object as images are lim-ited in their ability to abstract and can be ambiguous, which makes it difficult to track targets with drastic variations.
In this paper, we propose the CiteTracker to enhance tar-get modeling and inference in visual tracking by connecting images and text. Specifically, we develop a text generation module to convert the target image patch into a descriptive text containing its class and attribute information, provid-ing a comprehensive reference point for the target. In ad-dition, a dynamic description module is designed to adapt to target variations for more effective target representation.
We then associate the target description and the search im-age using an attention-based correlation module to gener-ate the correlated features for target state reference. Ex-tensive experiments on five diverse datasets are conducted to evaluate the proposed algorithm and the favorable per-formance against the state-of-the-art methods demonstrates the effectiveness of the proposed tracking method. The source code and trained models will be released at https:
//github.com/NorahGreen/CiteTracker. 1.

Introduction
Visual object tracking aims to estimate the state (loca-tion and extent) of an arbitrary target in a video sequence based on a specified region of the target in the initial frame as a reference point. It is challenging to locate a target that undergoes drastic appearance variations (e.g., changes in pose, illumination, or occlusions) using only one target im-age sample since the target appearances can be significantly different. To successfully track the target with appearance changes, it is crucial to acquire a comprehensive represen-tation of the target for establishing associations between the target exemplar and the target in test frames.
† Equal contribution, ∗ corresponding author
Figure 1. Comparison of the proposed algorithm and existing tracking methods in terms of target modeling and association. The left and right parts depict the typical visual tracking framework and the proposed one, respectively. Our approach first generates a text description of the target object and then takes the feature of the text to estimate the target state in the test image, enabling a more comprehensive target modeling and association.
Most existing deep trackers [2, 19, 36, 7, 39] learn an embedded feature space, where target samples with differ-ent appearances are still close to each other, to generate a robust representation to target variations. To build a more comprehensive target representation and better associate the target exemplar with the test target, several recent track-ers [6, 40, 23] perform interaction of the target template and search region in every block of their feature extraction backbone, achieving state-of-the-art tracking performance.
However, these methods do not perform well when the tar-get changes drastically or the given target exemplars are of low-quality (i.e., the appearance of the target exemplar is uncommon or the exemplar contains background noise).
The following issues arise when using an image patch as the target reference for tracking. First, the visual repre-sentation of a target is insufficient to provide a comprehen-sive understanding for recognizing the target with appear-ance variations, since images are limited in their ability to abstract. An image patch of a target only captures its ap-pearance from a particular angle, but its shape, texture, and surface features can vary significantly when viewed from different angles, resulting in a completely different appear-ance that makes it difficult to track the target. Second, as images can be ambiguous and open to interpretation, a ran-dom target image patch can mislead tracking models by causing them to overemphasize certain unstable appearance features and ignore the more essential and stable features of the target, resulting in drifting to the background and track-ing failures. For example, when tracking a circular object, the target patch may include a lot of the background, which causes the tracker to drift to the background.
We note that the human-created language signal provides a more abstract and precise concept of an object compared to the image signal, which has the potential to solve the aforementioned issues. In addition, the study [30] on con-necting language and images shows that text and image fea-tures can be well-aligned and transferred to each other, al-lowing for using the advantages of both language and image signals for visual tracking. Motivated by these insights, we study correlating text and images for visual tracking.
In this paper, we propose a new tracking framework that uses an adaptive text description of the target as the refer-ence point and correlates it with test image features to per-form tracking, named as CiteTracker. Specifically, we first develop a text generation model via prompt learning with a pre-defined open vocabulary including class and attribute labels, enabling generating the text description of the target based on a target image patch. The generation model is built using the CLIP model as the baseline, which already con-nects text with rich image features. To adapt to target vari-ations over time, we develop a dynamic text feature model that generates adaptive text features along the change of the target. Finally, we associate the features of the target text description with test image features to generate the corre-lated features for further target state estimation. We con-duct extensive experiments on a variety of public datasets including GOT-10K [18], LaSOT, TrackingNet, OTB100, and TNL2K to evaluate the proposed algorithm. The fa-vorable performance against the state-of-the-art methods on all the datasets demonstrates the effectiveness of correlating images and text for visual tracking.
We make the following contributions in this paper:
• We propose a text-image correlation based tracking framework. We use a text description to provide a more comprehensive and precise concept of the target and correlate the text with the test image for inferring the target location, enabling a more powerful ability to handle the target variation issue.
• We develop an adaptive feature model for target de-scriptions to better adapt to target variations in test videos, contributing to more precise target features and more accurate tracking performance.
• We achieve state-of-the-art performance on numerous tracking datasets. We conduct extensive experiments including ablation studies to demonstrate the effective-ness of the proposed method and the effect of every component. 2.