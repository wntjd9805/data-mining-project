Abstract
Reconstructing interacting hands from monocular im-ages is indispensable in AR/VR applications. Most ex-isting solutions rely on the accurate localization of each skeleton joint. However, these methods tend to be unre-liable due to the severe occlusion and confusing similar-ity among adjacent hand parts. This also defies human perception because humans can quickly imitate an inter-action pattern without localizing all joints. Our key idea is to first construct a two-hand interaction prior and re-cast the interaction reconstruction task as the conditional sampling from the prior. To expand more interaction states, a large-scale multimodal dataset with physical plausibility is proposed. Then a VAE is trained to further condense these interaction patterns as latent codes in a prior distri-bution. When looking for image cues that contribute to in-teraction prior sampling, we propose the interaction adja-cency heatmap (IAH). Compared with a joint-wise heatmap for localization, IAH assigns denser visible features to those invisible joints. Compared with an all-in-one visi-ble heatmap, it provides more fine-grained local interac-tion information in each interaction region. Finally, the correlations between the extracted features and correspond-ing interaction codes are linked by the ViT module. Com-prehensive evaluations on benchmark datasets have ver-ified the effectiveness of this framework. The code and dataset are publicly available at https://github. com/binghui-z/InterPrior_pytorch. 1.

Introduction
Reconstruction of interacting hands is significant for en-hancing the behavioral realism of digital avatars in com-*Corresponding author. E-mail: yangangwang@seu.edu.cn. All the authors from Southeast University are affiliated with the Key Laboratory of
Measurement and Control of Complex Systems of Engineering, Ministry of Education, Nanjing, China. This work was supported in part by the
National Natural Science Foundation of China (No. 62076061), in part by the Natural Science Foundation of Jiangsu Province (No. BK20220127).
Figure 1. Illustration of reconstructing interacting hands from monocular images by our framework. Left: Using a ViT-based fusion network, we map the extracted features from inputs to the learned latent space. Right: We sample reasonable reconstructions from the pre-built interaction prior. munication, thinking and working. With the advent of the
RGB dataset [43] recording two-hand interactions, numer-ous attempts have been implemented to reconstruct inter-Inspired by acting hands from monocular RGB images. the existing single-hand frameworks [23, 69, 7], pioneer works [64, 12] localize and identify all two-hand joints as the interacting clues. Unfortunately, this process can be seriously misguided by the regional occlusion and lo-cal similarity between hands. Subsequent improvements in-clude optimizing re-projection errors [51], localizing mesh vertices from coarse to fine [34], and querying all-in-one visible heatmap [27, 16]. Nevertheless, they still rely on more accurate joint 2D estimators, more diverse marker-less training data, or more computational complexity.
To overcome this hurdle, our key idea is to first con-struct a comprehensive interaction prior with multi-modal datasets and then sample this pre-built prior ac-cording to the interaction cues extracted from a monoc-ular image.
It is noted that existing frameworks are al-ways trained with paired data of calibrated images and mesh annotations. This may lead to difficulty in general-izing since the well-known benchmark [43] contains sim-ple backgrounds and only around 8.5K interaction patterns.
We break this images-paired manner and construct an in-teraction prior with multimodal datasets, including marker-based data, marker-less data and hands-object data. To do this, a dataset with 500K two-hand patterns is proposed, which contains physically plausible 3D hand joints and
MANO parameters. This dataset is used for the unsuper-vised training of a prior container, which can be formulated by a VAE [29]. As a result, each two-hand interaction pat-tern is mapped to an interaction code in the prior space.
Since the correlation between the two hands is considered, this representation is more compact than doubling the hand joint/vertex positions or MANO parameters [49].
We argue that accurate joint localization is challeng-ing for the monocular reconstruction of interacting hands.
As an alternative, we sample the above pre-built interac-tion prior according to the interaction adjacency heatmap (IAH). This heatmap is defined as the mixture coordinate distribution of this joint and other two-hand joints within its coordinate neighborhood. Compared with the 2.5D joint heatmap [23, 64], our IAH abandons the pseudo depth and concatenates more on spatial correlations of the target joint.
This heatmap formulation is easier to regress because even for an invisible joint, humans can determine its identity and location according to its spatial neighborhood. Considering that the Gaussian distribution has a more ambiguous bound-ary, the Laplacian distribution [2] is selected as the kernel function of each joint. This effectively reduces the alias-ing of interacting adjacency information. These IAHs are further converted to be the corresponding interaction codes through the ViT [10] module and then are regarded as condi-tions to sample reasonable interaction from the latent space.
In summary, our main contributions are:
• A powerful interaction reconstruction framework that compactly represents two-hand patterns as latent codes, which are learned from multimodal datasets in an unsuper-vised manner.
• An effective feature extraction strategy that utilizes inter-action adjacency as clues to identify each joint, which is inspired by human perception and is more friendly for net-work learning.
• A large-scale multimodal dataset that records 500K pat-terns of closely interacting hands, which is more conducive to the construction of our latent prior space. 2.