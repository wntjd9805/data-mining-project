Abstract 1.

Introduction
We explore a new class of diffusion models based on the transformer architecture. We train latent diffusion models of images, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches. We an-alyze the scalability of our Diffusion Transformers (DiTs) through the lens of forward pass complexity as measured by
Gﬂops. We ﬁnd that DiTs with higher Gﬂops—through in-creased transformer depth/width or increased number of in-put tokens—consistently have lower FID. In addition to pos-sessing good scalability properties, our largest DiT-XL/2 models outperform all prior diffusion models on the class-conditional ImageNet 512 256 benchmarks,
⇥ achieving a state-of-the-art FID of 2.27 on the latter. 512 and 256
⇥
Machine learning is experiencing a renaissance pow-ered by transformers. Over the past ﬁve years, neural architectures for natural language processing [42, 8], vi-sion [10] and several other domains have been subsumed by transformers [60]. Many classes of image-level gener-ative models remain holdouts to the trend, though—while transformers see widespread use in autoregressive mod-els [43, 3, 6, 47], they have seen less adoption in other gen-erative modeling frameworks. For example, diffusion mod-els have been at the forefront of recent advances in image generation [9, 46]; yet, they all adopt a convolutional U-Net architecture as the de-facto choice of backbone.
* Work done during an internship at Meta AI, FAIR Team.
Code and project page available here.
Figure 2: ImageNet generation with Diffusion Transformers (DiTs). Bubble area indicates the ﬂops of the diffusion model. Left: FID-50K (lower is better) of our DiT models at 400K training iterations. Performance steadily improves in
FID as model ﬂops increase. Right: Our best model, DiT-XL/2, is compute-efﬁcient and outperforms all prior U-Net-based diffusion models, like ADM and LDM.
The seminal work of Ho et al. [19] ﬁrst introduced the
U-Net backbone for diffusion models. Having initially seen success within pixel-level autoregressive models and con-ditional GANs [23], the U-Net was inherited from Pixel-CNN++ [52, 58] with a few changes. The model is con-volutional, comprised primarily of ResNet [15] blocks. In contrast to the standard U-Net [49], additional spatial self-attention blocks, which are essential components in trans-formers, are interspersed at lower resolutions. Dhariwal and
Nichol [9] ablated several architecture choices for the U-Net, such as the use of adaptive normalization layers [40] to inject conditional information and channel counts for con-volutional layers. However, the high-level design of the U-Net from Ho et al. has largely remained intact.
With this work, we aim to demystify the signiﬁcance of architectural choices in diffusion models and offer empiri-cal baselines for future generative modeling research. We show that the U-Net inductive bias is not crucial to the per-formance of diffusion models, and they can be readily re-placed with standard designs such as transformers. As a result, diffusion models are well-poised to beneﬁt from the recent trend of architecture uniﬁcation—e.g., by inheriting best practices and training recipes from other domains, as well as retaining favorable properties like scalability, ro-bustness and efﬁciency. A standardized architecture would also open up new possibilities for cross-domain research.
In this paper, we focus on a new class of diffusion models based on transformers. We call them Diffusion Transform-ers, or DiTs for short. DiTs adhere to the best practices of
Vision Transformers (ViTs) [10], which have been shown to scale more effectively for visual recognition than traditional convolutional networks (e.g., ResNet [15]).
More speciﬁcally, we study the scaling behavior of trans-formers with respect to network complexity vs. sample quality. We show that by constructing and benchmark-ing the DiT design space under the Latent Diffusion Mod-els (LDMs) [48] framework, where diffusion models are trained within a VAE’s latent space, we can successfully replace the U-Net backbone with a transformer. We further show that DiTs are scalable architectures for diffusion mod-els: there is a strong correlation between the network com-plexity (measured by Gﬂops) vs. sample quality (measured by FID). By simply scaling-up DiT and training an LDM with a high-capacity backbone (118.6 Gﬂops), we are able to achieve a state-of-the-art result of 2.27 FID on the class-conditional 256 256 ImageNet generation benchmark.
⇥ 2.