Abstract
Perception systems in modern autonomous driving vehi-cles typically take inputs from complementary multi-modal sensors, e.g., LiDAR and cameras. However, in real-world applications, sensor corruptions and failures lead to inferior performances, thus compromising autonomous safety. In this paper, we propose a robust framework, called
MetaBEV, to address extreme real-world environments, in-volving overall six sensor corruptions and two extreme sensor-missing situations. In MetaBEV, signals from multi-ple sensors are first processed by modal-specific encoders.
Subsequently, a set of dense BEV queries are initialized, termed meta-BEV. These queries are then processed iter-atively by a BEV-Evolving decoder, which selectively ag-gregates deep features from either LiDAR, cameras, or both 0∗ Equal contribution. 0† Corresponding authors. modalities. The updated BEV representations are further leveraged for multiple 3D prediction tasks. Additionally, we introduce a new M2oE structure to alleviate the perfor-mance drop on distinct tasks in multi-task joint learning. Fi-nally, MetaBEV is evaluated on the nuScenes dataset with 3D object detection and BEV map segmentation tasks. Ex-periments show MetaBEV outperforms prior arts by a large margin on both full and corrupted modalities. For instance, when the LiDAR signal is missing, MetaBEV improves 35.5% detection NDS and 17.7% segmentation mIoU upon the vanilla BEVFusion [25] model; and when the camera signal is absent, MetaBEV still achieves 69.2% NDS and 53.7% mIoU, which is even higher than previous works that perform on full-modalities. Moreover, MetaBEV performs moderately against previous methods in both canonical per-ception and multi-task learning settings, refreshing state-of-the-art nuScenes BEV map segmentation with 70.4% mIoU.
1.

Introduction
Perceiving the surrounding environment is a fundamen-tal capability of autonomous driving systems.
In pursuit of higher perceptual accuracy, prior works make signifi-cant efforts in designing stronger task-specific modules [41, 16, 21], cultivating effective training paradigms [5], lever-aging multi-modalities [25, 22, 7], etc. Among all these, the multi-sensors fusion strategy exhibits significant advan-tages in achieving stronger perception abilities [31, 39, 25, 1], thus being widely explored in both academia and indus-try. While the majority of works focus on achieving opti-mal performance on ideal multi-modal inputs for a single specific task, they unintentionally neglect how the designed models perform with sensor failures, which are commonly encountered and inevitable in real-world applications.
To alleviate performance drop on sensor failures, pre-vious works encounter two challenges as follows. 1) Fea-tures misalignment: Existing fusion methods typically uti-lize CNNs and features concatenation for fusion [25, 22].
The pixel-level position correlation is consistently imposed, giving rise to multi-modal features misalignment, espe-cially when geometric-related noises are introduced. This issue could be attributed to the intrinsic characteristics of
CNNs, which exhibit limitations in long-range perception and adaptive attention to input features. 2) Heavily reliant on complete modalities: Prior arts generate the fused BEV features using either query-indexing or channel-wise fusion manners. Query-indexing methods [29, 31, 32, 14] typically rely on LiDAR and 2D Camera features for mutual query-ing, while channel-wise fusion approaches [25, 22] are in-evitable to involve element-wise operations (e.g. element-sum) for feature merging. Both fusion strategies are heavily dependent on complete modality inputs and lead to inferior perception performance encountering extreme sensor fail-ures such as LiDAR-missing or Cameras-missing, thus be-ing limited in the practical applications.
In this research, we propose MetaBEV to tackle the above features misalignment and full-modality dependence problems through modality-arbitrary and task-agnostic learning in the unified bird’s-eye view (BEV) representation space [18]. We identify the major bottleneck in modality-dependent methods is the lack of designs that enable inde-pendent fusion of different modalities by the fusion module.
Therefore, we present a modality-arbitrary BEV-Evolving decoder, which leverages cross-modal attention to correlate the learnable meta-BEV queries with either a single camera-BEV feature, LiDAR-BEV feature, or both to eliminate the bottleneck. Finally, we apply a few task-specific heads to support different 3D perception predictions.
Except for the canonical perception (on no corrupted sensors), we also evaluate MetaBEV on overall six sensor corruptions (Limited Field (LF), Beams Reduction (BR),
Missing Objects (MO), View Drop (VD), View Noise (VN),
Figure 2. MetaBEV shows stronger robustness on missing sen-sors. We perform representative methods with full/missing modal-ities on both map segmentation (the left part) and 3D detection (the right part). Results show MetaBEV can mitigate the performance drop on input absence. Quantitatively, when facing missing cam-eras, MetaBEV still achieves 53.7% mIoU, which outperforms the representative method (i.e., BEVFusion [25]), by +49.6% mIoU, and is even better than MVP [39] performed on full-modalities.
The superior performance could also be found on 3D detection tasks.
Obstacle Occlusion (OO)) and two sensor-missing scenar-ios (Missing LiDAR (ML) and Missing Camera (MC)).
Compared with prior works, MetaBEV performs more ro-bustly as depicted in Fig. 1. For example, it achieves 69.2%
NDS and 42.6% NDS on detection when totally missing the cameras or LiDAR, respectively. For map segmentation, when total cameras absence occurs, MetaBEV could still achieve better performance compared to the work trained on multi-modalities (i.e., 54.7% v.s. MVP’s 49.0% mIoU [39]).
Besides, the attention-based MetaBEV exhibits inherent ro-bustness against multiple heavy corruptions with zero-shot and in-domain tests. For instance, as shown in Tab. 3, even missing 66.6% of LiDAR points, MetaBEV still achieves 55% NDS, outperforming the competitor by +11.7%.
Moreover, considering the limited computational re-source in practice, using a single framework with shared parameters for different tasks is more efficient than using separate frameworks for multiple tasks. However, the task conflicts in the joint learning of detection and segmenta-tion often lead to severe performance drop [36, 21, 25], and existing methods rarely analyze and design for multi-task learning (MTL). We incorporate MetaBEV with a flexible module based on Multi-Task Mixture of Experts (M2oE) to demonstrate one possible solution for MTL and hope to stimulate further research in this area.
The appealing advantages of MetaBEV are concluded: 1. MetaBEV is a novel BEV perception framework for 3D object detection and BEV map segmentation, which can maintain resilient performance under arbi-trary sensor input. Plenty of real-world sensor corrup-tions are formulated as well as methodically experi-mented with and analyzed to verify its robustness.
2. MetaBEV involves the M2oE structures to alleviate tasks conflict when performing 3D detection and seg-mentation tasks with the same trained weights. 3. MetaBEV achieves state-of-the-art performance on
It’s the first method designed nuScenes dataset [3]. for both sensor failures and tasks conflict. We hope
MetaBEV will facilitate future research. 2.