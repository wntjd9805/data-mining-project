Abstract
Ego4D Detection mAP
Charades-Ego Recognition mAP
We present Ego-Only, the first approach that enables state-of-the-art action detection on egocentric (first-person) videos without any form of exocentric (third-person) trans-ferring. Despite the content and appearance gap separating the two domains, large-scale exocentric transferring has been the default choice for egocentric action detection. This is because prior works found that egocentric models are dif-ficult to train from scratch and that transferring from exo-centric representations leads to improved accuracy. How-ever, in this paper, we revisit this common belief. Mo-tivated by the large gap separating the two domains, we propose a strategy that enables effective training of ego-centric models without exocentric transferring. Our Ego-Only approach is simple. It trains the video representation with a masked autoencoder finetuned for temporal segmen-tation. The learned features are then fed to an off-the-shelf temporal action localization method to detect actions. We find that this renders exocentric transferring unnecessary by showing remarkably strong results achieved by this simple
Ego-Only approach on three established egocentric video datasets: Ego4D, EPIC-Kitchens-100, and Charades-Ego.
On both action detection and action recognition, Ego-Only outperforms previous best exocentric transferring methods that use orders of magnitude more labels. Ego-Only sets new state-of-the-art results on these datasets and bench-marks without exocentric data. 1.

Introduction
In this paper we consider the problem of action detec-tion from egocentric videos [30, 21, 19] captured by head-mounted devices. While action detection in third-person videos [6, 36] has been the topic of extended and active re-search by the computer vision community, the formulation of this task in the first-person setting is underexplored.
One major challenge of egocentric action detection is i.e. insufficient amount of egocentric the lack of data, videos to train large-capacity models to competitive results.
For example, existing methods such as Ego-Exo [43] and
Charades-Ego [56], attempted to train egocentric models
Figure 1. Our Ego-Only approach achieves state-of-the-art results on Ego4D [30] action detection and Charades-Ego [56] action recognition without any extra data or labels (Section 4). Compared with exocentric transferring, Ego-Only uses orders of magnitude fewer labels, simplifies the pipeline, and improves the results.
Egocentric Videos (length: 480 seconds)
Exocentric Videos (length: 10 seconds)
Figure 2. Domain gap between egocentric videos (Ego4D [30]) and exocentric videos (Kinetics-400 [37]). Exocentric videos are typically in the form of short trimmed clips, which show the actors as well as the contextual scene. Egocentric videos are dramatically longer, capture close-up object interactions but only the hands of the actor. These differences make it challenging to transfer models from exocentric action recognition to egocentric action detection. from scratch using egocentric data only, but failed to obtain satisfactory results. Therefore, current egocentric action de-tection methods rely on out-of-domain large-scale exocen-tric (third-person) videos [37] or even images [22], under the assumption that the large-scale pretraining with proper transferring techniques can mitigate the negative effect of the domain gap between egocentric and exocentric videos.
This hope is reinforced by the observation that deep neu-ral networks exhibit invariance to object viewpoints [55],
as evidenced by the effective transfers from large-scale
ImageNet pretraining to various still-image [50, 35, 62] and video understanding tasks [37, 5, 2]. Prior video ap-proaches [43, 56] also demonstrated empirical benefits of transferring from exocentric representations over simply learning egocentric representations from scratch. As a re-sult, this line of research focuses mainly on improving the transferring techniques that minimize the domain gap, or simply scaling exocentric data to a huge amount [72, 29].
However, we argue that the dramatically different view-point of first-person videos poses challenges that may not be addressed simply by scaling exocentric data or design-ing better transferring techniques, as illustrated in Figure 2: (1) No actor in view. In egocentric videos, the subject is behind the camera and is never visible, except for their hands. Conversely, third-person videos usually capture the actors as well as informative spatial context around them. (2) Domain shift. Egocentric videos entail daily life activi-ties such as cooking, playing, performing household chores, which are poorly represented in third-person datasets. (3)
Class granularity. First-person vision requires fine-grained recognition of actions within the same daily life category, such as “wipe oil metallic item”, “wipe kitchen counter”,
“wipe kitchen appliance”, and “wipe other surface or ob-ject” [30]. (4) Object interaction. Egocentric videos capture a lot of human-object interactions as a result of the first-person viewpoint. The scales and views of the objects are dramatically different than in exocentric videos. (5) Long-form. Egocentric videos are typically much longer than ex-ocentric videos and thus require long-term reasoning of the human-object interactions rather than single frame classifi-cation. (6) Long-tail. Real-world long-tail distribution is often observed in egocentric datasets, as they are uncurated and thus reflect the in-the-wild true distribution of activities, which is far from uniform. (7) Localization. Egocentric ac-tion detection requires temporally sensitive representations which are difficult to obtain from third-person video classi-fication on short and trimmed clips.
We argue that these challenges impede effective transfer from the exocentric to the egocentric domain and may ac-tually cause detrimental biases when adapting third-person models to the first-person setting (as shown in Section 4).
Therefore, instead of following the common transferring as-sumption, we revisit the old good idea of training with in-domain egocentric data only, but this time in light of the development of recent data-efficient training methods, such as masked autoencoders [32, 59, 27] as well as the scale growth of egocentric data collections (e.g., the recently in-troduced Ego4D dataset [30]). bootstraps the backbone representation, (2) a simple fine-tuning stage that performs temporal semantic segmentation of egocentric actions, and (3) a final detection stage using an off-the-shelf temporal action detector, such as Action-Former [73], without any modification. This approach en-ables us to train an egocentric action detector from random initialization without any exocentric videos or images.
Empirically, we evaluate Ego-Only on the three largest egocentric datasets, Ego4D [30], EPIC-Kitchens-100 [21],
Charades-Ego [56], and two tasks, action detection and ac-tion recognition. Surprisingly, Ego-Only outperforms all previous results based on exocentric transferring, setting new state-of-the-art results, obtained for the first time with-out additional data. Specifically, Ego-Only advances the state-of-the-art results on Ego4D Moments Queries detec-tion (+6.5% average mAP), EPIC-Kitchens-100 Action De-tection (+5.5% on verbs and +6.2% on nouns), Charades-Ego action recognition (+3.1% mAP), and EPIC-Kitchens-100 action recognition (+1.1% top-1 accuracy on verbs).
In addition to the state-of-the-art comparison, we also noticed a few critical factors (as shown in Section 4) for the effectiveness of an Ego-Only approach: (1) dramatic per-formance deterioration when skipping either MAE pretrain-ing or temporal segmentation finetuning; (2) importance of
MAE pretraining on egocentric (as opposed to exocentric) data to learn the in-domain distribution; (3) criticality of long-term modeling for good accuracy; (4) the sensitivity to amount of unsupervised data; (5) surprising lack of per-formance gains by joint ego-exo pretraining or finetuning.
In summary, our contributions are four-fold:
• We propose the first Ego-Only method that trains ego-centric action representations effectively without any form of exocentric data or transferring.
• We demonstrate that exocentric transferring is not nec-essary for state-of-the-art egocentric action detection.
• Ego-Only advances state-of-the-art results on both ac-tion detection and action recognition, evaluated on three large-scale egocentric datasets.
• Our empirical evaluation reveals several critical factors for the effectiveness of an Ego-Only approach. 2.