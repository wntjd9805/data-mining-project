Abstract
The exact 3D dynamics of the human body provides cru-cial evidence to analyze the consequences of the physical interaction between the body and the environment, which can eventually assist everyday activities in a wide range of applications. However, optimizing for 3D configurations from image observation requires a significant amount of computation, whereas real-world 3D measurements often suffer from noisy observation or complex occlusion. We re-solve the challenge by learning a latent distribution rep-resenting strong temporal priors. We use a conditional variational autoencoder (CVAE) architecture with a trans-former to train the motion priors with a large-scale motion dataset. Then our feature follower effectively aligns the fea-ture spaces of noisy, partial observation with the necessary input for pre-trained motion priors, and quickly recovers a complete mesh sequence of motion. We demonstrate that the transformer-based autoencoder can collect necessary spatio-temporal correlations robust to various adversaries, such as missing temporal frames, or noisy observation un-der severe occlusion. Our framework is general and can be applied to recover the full 3D dynamics of other subjects with parametric representations. 1.

Introduction
Understanding human motion is important for many real-world applications to assist humans. It has been a con-sistent interest of research, and has witnessed remarkable progress. Many previous works attempted to detect human poses by locating predefined joints from 2D images and fit-ting template meshes. 3D information has to be inferred as post-processing with either multi-view observations or by incorporating prior knowledge about the size of the body or objects. A large volume of works also find 3D poses from 3D data of marker-based motion captures, or gyro-scopes, which require additional hardware attached to the body parts. On the other hand, point clouds are easy to
Figure 1. Overview of our approach. Our model firstly learns to recover mesh sequence from the complete point cloud sequence.
Using this kinematics prior, we train our model in various other scenarios and recover the mesh sequence. Our model can also be trained on other input modality to generate its mesh sequence. obtain using a commodity depth camera or a LiDAR sensor observing the scene. By directly measuring the 3D locations of the parts, we can easily reason about body positions rel-ative to surroundings and have advantages in inferring the consequences of human-object interaction or human-human interaction.
We propose a pipeline to obtain full 3D dynamic mesh from noisy, partial point cloud sequences. Real-world ob-servations of motion are highly complex and suffer from oc-clusion by other objects or noises. Nonetheless, humans can easily infer the motion context of other humans. Not only does the physical connectivity of the skeleton structure de-fine the range of possible motions for human body parts, but the motion semantics result in a rich correlation be-tween temporal frames. To this end, we observe a sequence of point cloud measurements, instead of individual frames, and utilize strong kinematics information obtained from a large-scale motion data. Note that the overall pipeline is not bounded to human mesh, but can also be extended to other
subjects with kinematic structures.
We gain robustness against complex real-world chal-lenges with a generative prior obtained from a large-scale motion sequences and a transformer to focus on reliable ev-idence. Given noisy partial measurements, there are many possible motions to explain the observations. Instead of re-gressing for a single deterministic pose of a given time step, we embrace the uncertainty by maintaining the distribution of latent space of motions with conditional variational auto-encoder (CVAE). After the kinematic priors are obtained as a latent distribution, we can sample a latent vector and generate a plausible mesh output. We also employ the trans-formers to apply attention to meaningful observations while robust to unknown missing data. We demonstrate the supe-rior performance of our full 3D motion recovery compared to other approaches focusing on single-time steps or deter-ministic methods.
Our contributions are summarized as follows:
• We demonstrate reusable 3D kinematic priors from large-scale motion datasets can provide strong struc-tural semantics to recover dynamic 3D mesh in various scenarios.
• We show multi-modal motion prediction using varia-tional frameworks and transformers, and demonstrate that both are critical to maintaining stable performance in challenging inputs.
• Our framework achieves superior performance over other existing methods, and can be generally applied for a diverse set of motions.
We expect the proposed method to provide an essential tool to capture and understand human motion in real-world sce-narios and extend to practical applications to assist humans. 2.