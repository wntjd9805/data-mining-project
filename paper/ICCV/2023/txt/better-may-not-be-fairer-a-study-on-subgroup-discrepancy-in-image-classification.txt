Abstract
In this paper, we provide 20,000 non-trivial human an-notations on popular datasets as a first step to bridge gap to studying how natural semantic spurious features affect im-age classification, as prior works often study datasets mix-ing low-level features due to limitations in accessing real-istic datasets. We investigate how natural background col-ors play a role as spurious features by annotating the test sets of CIFAR10 and CIFAR100 into subgroups based on the background color of each image. We name our datasets
CIFAR10-B and CIFAR100-B1 and integrate them with
CIFAR-Cs.
We find that overall human-level accuracy does not guarantee consistent subgroup performances, and the phe-nomenon remains even on models pre-trained on ImageNet or after data augmentation (DA). To alleviate this issue, we propose FlowAug, a semantic DA that leverages decoupled semantic representations captured by a pre-trained genera-tive flow. Experimental results show that FlowAug achieves more consistent subgroup results than other types of DA methods on CIFAR10/100 and on CIFAR10/100-C. Addi-tionally, it shows better generalization performance.
Furthermore, we propose a generic metric, MacroStd, for studying model robustness to spurious correlations, where we take a macro average on the weighted standard deviations across different classes. We show MacroStd be-ing more predictive of better performances; per our met-ric, FlowAug demonstrates improvements on subgroup dis-crepancy. Although this metric is proposed to study our curated datasets, it applies to all datasets that have sub-groups or subclasses. Lastly, we also show superior out-of-distribution results on CIFAR10.1. 1.

Introduction
Deep neural networks (DNNs, e.g., [25, 19]), properly trained via empirical risk minimization (ERM), have been 1Dataset is released at https://github.com/charismaticchiu/CIFAR-B demonstrated to significantly improve benchmark perfor-mances in a wide range of application domains. However, minimizing empirical risk over finite or biased datasets of-ten results in models latching on to spurious correlations that do not show a robust relationship between the input data and output labels. Moreover, benchmark evaluations based solely on average accuracy may overlook these critical is-sues. For instance, Fig. 1 shows that on CIFAR10, even though a standard ERM model reaches human-level test ac-curacy (red line), if we dive deeper into each class and com-pute their respective worst test accuracy stratified by back-ground colors, they are inconsistent across the ten classes and the degradation from total accuracy is huge (black line) for some. Such inconsistency and discrepancy have huge real-world implications, suggesting DNN models may make biased decisions against or in favor of specific spurious fac-tors, such as certain background colors.
Researchers have been working in different directions to understand the effect of spurious correlations, including model over-parameterization [35], causality [1] and infor-mation theory [27, 53]. Various techniques have emerged over the years to address this challenge, among which
DA [41] has stood out for its simplicity and effectiveness.
DA shows better generalization results in various machine learning tasks than other approaches [52, 50, 47, 18, 46, 39].
These augmentation methods, however, are often based on heuristic and coarse image processing techniques such as flipping, rotating, blurring, or manipulating images by mix-ing attributes from other inputs [52, 50, 11, 21] (Fig. 2); therefore, they can only address limited aspects of spurious correlations, for which we will show an example in ยง 2. To address this limitation, instead of mixing low-level features, we seek to augment the training set by learning semantic deep representations and then using them to generate new images.
In this paper, as the very first step towards comprehen-sive evaluation of subgroup performance against semanti-cally meaningful and realistic spurious correlations in im-age classification, we conduct a case study experiment to
Figure 1: FlowAug reduces subgroup discrepancy. CIFAR10-B enables us to observe the worst test time subgroup accu-racy in each class. Standard ERM shows subgroup discrepancy, uneven subgroup performances across all classes, and a huge gap between total accuracy (red line) and the worst subgroup accuracy (black line). This issue persists even after common
DAs are used (top). Our proposed FlowAug mitigates this issue (bottom) and also reports improved overall performance.
Figure 2: Examples of different augmentation methods. Row 2 & 3 are generated by our methods. investigate background colors as spurious features (ยง2), for their commonality in image classification and immediate implications for trustworthiness [34]. To directly quantify the results, we annotated the test data of CIFAR10 and
CIFAR100 into subgroups based on natural image back-ground colors (see Fig. 3), yielding CIFAR10-