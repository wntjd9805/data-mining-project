Abstract
Multi-task visual learning is a critical aspect of com-puter vision. Current research, however, predominantly concentrates on the multi-task dense prediction setting, which overlooks the intrinsic 3D world and its multi-view consistent structures, and lacks the capability for versatile imagination. In response to these limitations, we present a novel problem setting – multi-task view synthesis (MTVS), which reinterprets multi-task prediction as a set of novel-view synthesis tasks for multiple scene properties, includ-ing RGB. To tackle the MTVS problem, we propose Mu-vieNeRF, a framework that incorporates both multi-task and cross-view knowledge to simultaneously synthesize multi-ple scene properties. MuvieNeRF integrates two key mod-ules, the Cross-Task Attention (CTA) and Cross-View Atten-tion (CVA) modules, enabling the efficient use of informa-tion across multiple views and tasks. Extensive evaluation on both synthetic and realistic benchmarks demonstrates that MuvieNeRF is capable of simultaneously synthesiz-ing different scene properties with promising visual qual-ity, even outperforming conventional discriminative mod-els in various settings. Notably, we show that MuvieNeRF exhibits universal applicability across a range of NeRF backbones. Our code is available at https://github. com/zsh2000/MuvieNeRF. 1.

Introduction
When observing a given scene, human minds exhibit a remarkable capability to mentally simulate the objects within it from a novel viewpoint in a versatile manner [40].
It not only includes imagination of the colors of objects, but also extends to numerous associated scene properties such as surface orientation, semantic segmentation, and edge pat-terns. Prompted by this, a burgeoning interest has emerged, seeking to equip modern robotic systems with similar ca-pabilities for handling multiple tasks. Nevertheless, con-temporary research [33, 65, 66] has primarily centered on
*Equal contribution
Figure 1. Comparison between (a) the conventional multi-task learning scheme and (b) our multi-task view synthesis setting. The conventional “discriminative” multi-task learning makes predic-tions for single images while multi-task view synthesis aims to render visualizations for multiple scene properties at novel views. the multi-task dense prediction setting, which employs a conventional discriminative model to simultaneously pre-dict multiple pixel-level scene properties using given RGB images (refer to Figure 1(a)). Yet, the methodologies arising from this context often demonstrate practical limitations, primarily due to their tendency to treat each image as a sep-arate entity, without constructing an explicit 3D model that adheres to the principle of multi-view consistency. Even more critically, they lack the ability to “imagine” – they are incapable of inferring scene properties from an unseen viewpoint, as these models invariably require RGB images.
To circumvent these constraints, we propose a novel ap-proach that revisits multi-task learning (MTL) [5] from a synthesis perspective. This leads to a more flexible prob-lem setting that reinterprets multi-task visual learning as a collection of novel-view synthesis problems, which we re-fer to as multi-task view synthesis (MTVS) (refer to Fig-ure 1(b)). As an illustration, the task of predicting surface
normals for a given image could be reframed as visualiz-ing a three-channel “image” with the given pose and cam-era parameters. With the achievements of Neural Radiance
Fields (NeRF) [32], the implicit scene representation of-fers an effective solution to synthesize scene properties be-yond RGB [68]. Importantly, this scene representation takes multi-view geometry into account, which consequently en-hances the performance of all tasks.
Extending from [68], we make the exploration that min-ing multi-task knowledge can simultaneously enhance the learning of different tasks, extending beyond discriminative models [45, 66] to include synthesis models as well. Fur-thermore, we argue that the alignment of features across multiple reference views and the target view can rein-force cross-view consistency, thereby bolstering the implicit scene representation learning. Informed by this insight, we propose MuvieNeRF, a unified framework for the MTVS task, which incorporates Multi-task and cross-view knowl-edge, thus enabling the simultaneous synthesis of multiple scene properties through a shared implicit scene representa-tion. MuvieNeRF can be applied to an arbitrary conditional
NeRF architecture and features a unified decoder with two key modules: Cross-Task Attention (CTA) module, which investigates relationships among different scene properties, and Cross-View Attention (CVA) module, which aligns fea-tures across multiple views. The integration of these two modules within MuvieNeRF facilitates the efficient utiliza-tion of information from multiple views and tasks, leading to better performance across all tasks.
To demonstrate the effectiveness of our approach, we first instantiate our MuvieNeRF with GeoNeRF [22], a state-of-the-art conditional NeRF model, and conduct com-prehensive evaluations on both synthetic and real-world benchmarks. The results illustrate that MuvieNeRF is ca-pable of solving multi-task learning in a synthesis man-ner, even outperforming several competitive discriminative models in different settings. Moreover, we ablate the choice of conditional NeRF backbones to illustrate the broad appli-cability of our framework. We further validate the individ-ual contributions of the CVA and CTA modules by building and comparing different variants of MuvieNeRF. Finally, we demonstrate the broader applications and analysis of
MuvieNeRF, such as generalization on out-of-distribution datasets.
In summary, our contributions are three-fold: (1) We pioneer a novel problem definition, multi-task view synthe-sis (MTVS), which reconsiders multi-task visual learning as a set of view synthesis tasks. The introduction of MTVS paves the way for robots to emulate human-like mental sim-ulation capabilities by utilizing the implicit scene repre-sentation offered by Neural Radiance Fields (NeRF). (2)
We present MuvieNeRF, a unified framework that em-ploys Cross-Task Attention (CTA) and Cross-View Atten-tion (CVA) modules to leverage cross-view and cross-task information for the MTVS problem. (3) Comprehensive ex-perimental evaluations demonstrate that MuvieNeRF shows promising results for MTVS, and greatly outperforms con-ventional discriminative models across diverse settings. 2.