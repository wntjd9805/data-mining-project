Abstract
Video temporal grounding aims to pinpoint a video seg-ment that matches the query description. Despite the recent advance in short-form videos (e.g., in minutes), temporal grounding in long videos (e.g., in hours) is still at its early stage. To address this challenge, a common practice is to employ a sliding window, yet can be inefficient and inflexi-ble due to the limited number of frames within the window.
In this work, we propose an end-to-end framework for fast temporal grounding, which is able to model an hours-long video with one-time network execution. Our pipeline is formulated in a coarse-to-fine manner, where we first extract context knowledge from non-overlapped video clips (i.e., anchors), and then supplement the anchors that highly response to the query with detailed content knowledge.
Besides the remarkably high pipeline efficiency, another advantage of our approach is the capability of capturing long-range temporal correlation, thanks to modeling the entire video as a whole, and hence facilitates more accurate grounding. Experimental results suggest that, on the long-form video datasets MAD and Ego4d, our method signifi-cantly outperforms state-of-the-arts, and achieves 14.6× / 102.8× higher efficiency respectively. Project can be found at https://github.com/afcedf/SOONet.git. 1.

Introduction
Video temporal grounding [5, 6, 13, 18, 19, 29, 31, 32], which aims to localize a specific moment in the video corresponding to a natural language description, has found its applications in many real-world scenarios, such as video retrieval [11, 22], video highlight detection [23, 24], and video question answering [9, 26].
Despite the rapid advance in recent years, existing methods for temporal grounding usually target short-form
*Corresponding authors.
Figure 1. Pipeline comparison between sliding window-based
It methods (top) [19, 29, 31, 32] and our SOONet (bottom). is noteworthy that the sliding window pipeline requires repeated inference on overlapped clips and the final result aggregation, while ours can deliver the result with one-time network execution.
Detailed discussion can be found in Sec. 4.5. videos (e.g., in minutes) and characterize the input video with a small number of frames (e.g., 128) [19, 27, 29, 30, 31, 32]. When it comes to the case of long-form video temporal grounding (LVTG) [6, 18], however, temporally downsampling a video (e.g., in hours) to so few frames could cause severe information loss and further result in drastic performance degradation [6].
A straightforward solution is to reorganize a long video to a sequence of short videos using a sliding window and perform temporal grounding within each window [6, 8, 18].
However, such a solution as shown in the top half of
Fig. 1 has three main drawbacks. (1) Inference inefficiency:
The overlap between adjacent windows brings redundant computations. the large amounts of highly overlapped predictions cause post-processing (e.g., non-maximum suppression) time-consuming. It is noteworthy by saying efficiency, we mean pipeline efficiency instead of
Besides,
model efficiency, which considers the total execution time from data input to final results output, including data pre-processing, model forward running and post-processing.1 (2) Training insufficiency: The network with a sliding window can only scan the video contents within a local time range at one time, yet ignore the long-range temporal correlation. (3) Prediction inflexibility: The prediction is restricted inside a single window, making it hard to generalize to segments with long duration.
In this work, we propose an anchor-based end-to-end framework, termed as SOONet, which facilitates efficient and accurate LVTG by Scanning a long-form video Only
Once. As shown in the bottom half of Fig. 1, SOONet follows a pipeline of pre-ranking, re-ranking, regression, via leveraging both the inter-anchor context knowledge and the intra-anchor content knowledge.
Specifically, we first produce non-overlapped anchor sequence via anchor partition layer, then three procedures are implemented to obtain final predictions: (1) Multi-scale context-based anchor features are acquired by modeling inter-anchor context knowledge via cascaded temporal swin transformer blocks [12]. Meanwhile, a coarse anchor rank is obtained via sorting the context-based matching scores with respect to query. (2) Content-based anchor features and a content-enhanced anchor rank can be obtained by supplementing anchors with detailed intra-anchor content knowledge. We pick out the top-m anchors that highly corresponds to query from each scale to form an anchor subset, then implement re-ranking within subset to reduce (3) Boundary regression the computational complexity. is adopted to achieve flexible predictions, leveraging both inter-anchor and intra-anchor knowledge. To take full ad-vantage of the abundant cross-modal semantic relationship in long videos, we sample one video with a batch of queries grounded in this video at one training step, then optimize the full-length anchor rank and query rank simultaneously with the help of proposed dual-form approximate rank loss, which achieves superior cross-modal alignment.
Extensive experiments are conducted on two long-form video datasets, i.e., MAD [18] and Ego4d [6]. Our method significantly outperforms state-of-the-arts, and achieves 14.6× / 102.8× higher pipeline efficiency, which demon-strate the effectiveness. 2.