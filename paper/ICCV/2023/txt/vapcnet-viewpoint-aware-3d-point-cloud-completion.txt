Abstract
Most existing learning-based 3D point cloud comple-tion methods ignore the fact that the completion process is highly coupled with the viewpoint of a partial scan.
However, the various viewpoints of incompletely scanned objects in real-world applications are normally unknown and directly estimating the viewpoint of each incomplete object is usually time-consuming and leads to huge an-notation cost.
In this paper, we thus propose an un-supervised viewpoint representation learning scheme for 3D point cloud completion without explicit viewpoint es-timation. To be specific, we learn abstract representa-tions of partial scans to distinguish various viewpoints in the representation space rather than the explicit estima-tion in the 3D space. We also introduce a Viewpoint-Aware Point cloud Completion Network (VAPCNet) with flexible adaption to various viewpoints based on the learned representations. The proposed viewpoint representation learning scheme can extract discriminative representations to obtain accurate viewpoint information. Reported ex-periments on two popular public datasets show that our
VAPCNet achieves state-of-the-art performance for the point cloud completion task. Source code is available at https://github.com/FZH92128/VAPCNet. 1.

Introduction
Incomplete shapes in 3D scans resulting from occlusion (both self-occlusion and occlusion by other objects) and low resolution of sensors often make them unsuitable for direct use in practical applications such as object grasping and Vir-tual Reality (VR) [16, 9, 25, 10]. To remedy this, shape completion aims to recover complete 3D shapes from par-tial 3D scans.
Current learning-based shape completion methods can be classified into two categories: volumetric representation
The former category transforms a point cloud into 3D occu-pancy grids and uses 3D convolution operations to predict
Figure 1. Example of an unsupervised viewpoint representation learning. In MVP dataset [30], all objects have been aligned, and missing parts indicate various incomplete objects scanned from
In this context, incomplete objects marked varying viewpoints. with orange and red boxes are scanned from the same viewpoint, while those marked with black boxes have been scanned from dif-ferent viewpoints. the complete shapes of objects. However, these volumet-ric representations often come with high memory expenses and restricted shape accuracy. The second category, on the other hand, operates directly on point clouds, providing a more memory-efficient way to represent 3D data. However, these point cloud based methods are difficult to recover de-tailed structures of objects due to the irregular and disorder nature of point clouds.
To recover the full shape of partial point cloud ob-jects, most existing methods adopt a coarse-to-fine ap-proach to firstly predict coarse full shapes and then re-cover detailed complete shapes using complex refinement modules [54, 41, 52, 29, 30, 57]. More specifically, skip-connection is used to connect the detailed visible point-wise features with the missing point-wise features to preserve the detailed local patterns of objects [41]. In addition, at-tention mechanism is used to explore correlation between (non-)local patterns to recover detailed structures [52]. Al-though existing methods have made a certain progress in recovering the details of objects, they ignore the fact that the completion process is highly coupled with the scanning viewpoint.
Commonly, complete objects can be composed of vis-ible parts and missing parts. For each object, the visible parts are closely related to corresponding viewpoints and and can vary a lot when obtained from different viewpoints.
This means that the viewpoint information of each incom-plete object can provide additional cues to 3D point cloud completion. However, explicit viewpoint estimation is usu-ally time-consuming and leads to huge annotation cost.
Motivated by the recent advances in contrastive learning
[2, 7, 17, 20, 35], we propose to use unsupervised view-point representation learning by contrasting positive pairs with negative pairs within the latent space (as shown in
Fig. 1), thereby generating a distinct latent representation for each incomplete object. The benefits of the proposed viewpoint representation learning are twofold: First, it is more practicable to learn abstract representations to distin-guish between different viewpoints than extracting full rep-resentations for estimating the viewpoints of incomplete ob-jects. Consequently, we can obtain a discriminative view-point representation of incomplete objects to provide aux-iliary viewpoint information. Second, the unsupervised viewpoint representation learning does not require supervi-sion with ground-truth viewpoints, making it more suitable for real-world applications with unknown viewpoints.
In this paper, we propose to complete partial point cloud objects under the guidance of viewpoint representation.
More specifically, we first encode incomplete point clouds into viewpoint representations using contrastive learning.
Then, we propose a Viewpoint-Aware Point cloud Comple-tion Network (VAPCNet) with flexible adaptation to differ-ent viewpoints based on the learned representations. The proposed VAPCNet incorporates viewpoint information to perform feature adaptation by predicting convolutional ker-nels and modulated self-attention for local information ag-gregation from the viewpoint representation. Experimental results show that our network can handle various viewpoints and produce state-of-the-art results on both MVP [30] and
PCN datasets [54]. Formally, our contributions include:
• A novel formulation of point cloud completion with unsupervised viewpoint representation learning, which reveals the usefulness of viewpoint information;
• An effective viewpoint-aware module that perform feature adaptation by predicting convolutional kernels and modulated self-attention for local information ag-gregation for accurate point cloud completion;
• State-of-the-art completion results on both MVP and
PCN datasets. 2.