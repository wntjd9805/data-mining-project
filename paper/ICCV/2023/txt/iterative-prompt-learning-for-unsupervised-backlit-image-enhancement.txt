Abstract
We propose a novel unsupervised backlit image enhance-ment method, abbreviated as CLIP-LIT, by exploring the po-tential of Contrastive Language-Image Pre-Training (CLIP) for pixel-level image enhancement. We show that the open-world CLIP prior not only aids in distinguishing between backlit and well-lit images, but also in perceiving hetero-geneous regions with different luminance, facilitating the optimization of the enhancement network. Unlike high-level and image manipulation tasks, directly applying CLIP to enhancement tasks is non-trivial, owing to the difficulty in finding accurate prompts. To solve this issue, we devise a prompt learning framework that first learns an initial prompt pair by constraining the text-image similarity between the prompt (negative/positive sample) and the corresponding im-age (backlit image/well-lit image) in the CLIP latent space.
Then, we train the enhancement network based on the text-image similarity between the enhanced result and the ini-tial prompt pair. To further improve the accuracy of the initial prompt pair, we iteratively fine-tune the prompt learn-ing framework to reduce the distribution gaps between the backlit images, enhanced results, and well-lit images via rank learning, boosting the enhancement performance. Our method alternates between updating the prompt learning framework and enhancement network until visually pleasing results are achieved. Extensive experiments demonstrate that our method outperforms state-of-the-art methods in terms of visual quality and generalization ability, without requiring any paired data. 1.

Introduction
Backlit images are captured when the primary light source is behind some objects. The images often suffer from highly imbalanced illuminance distribution, which affects the visual quality or accuracy of subsequent perception algorithms.
Correcting backlit images manually is a laborious task
given the intricate challenge of preserving the well-lit re-gions while enhancing underexposed regions. One could apply an automatic light enhancement approach but will find that existing approaches could not cope well with backlit images [15]. For instance, many existing supervised light enhancement methods [27, 28, 34] cannot precisely perceive the bright and dark areas, and thus process these regions us-ing the same pipeline, causing over-enhancement in well-lit areas or under-enhancement in low-light areas. Unsuper-vised light enhancement methods, on the other hand, either rely on ideal assumptions such as average luminance and a gray world model [10, 16] or directly learn the distribu-tion of reference images via adversarial training [11]. The robustness and generalization capability of these methods are limited. As for conventional exposure correction meth-ods [1, 30], they struggle in coping with real-world backlit images due to the diverse backlit scenes and luminance in-tensities. The problem cannot be well resolved by collecting backlit images that consist of ground truth images that are retouched by photographers [21], since these images can never match the true distribution of real backlit photos.
In this work, we propose an unsupervised method for backlit image enhancement. Different from previous unsu-pervised methods that learn curves or functions based on some physical hypothesis or learn the distribution of well-lit images via adversarial training that relies on task-specific data, we explore the rich visual-language prior encapsulated in a Contrastive Language-Image Pre-Training (CLIP) [3] model for pixel-level image enhancement. While CLIP can serve as an indicator to distinguish well-lit and backlit im-ages to a certain extent, using it directly for training a backlit image enhancement network is still non-trivial. For example, for a well-lit image (Fig. 2 top left), replacing similar con-cepts “normal light” with “well-lit” brings a huge increase in
CLIP score. In the opposite case (Fig. 2 top right), “normal light” becomes the correct prompt. This indicates the opti-mal prompts could vary on a case-by-case basis due to the complex illuminations in the scene. In addition, it is barely possible to find accurate ‘word’ prompts to describe the precise luminance conditions. Prompt engineering is labor-intensive and time-consuming to annotate each image in the dataset. Moreover, the CLIP embedding is often interfered by high-level semantic information in an image. Thus, it is unlikely to achieve optimal performance with fixed prompts or prompt engineering.
To overcome the problems, we present a new pipeline to tailor the CLIP model for our task. It consists of the follow-ing components: 1) Prompt Initialization. We first encode the backlit and well-lit images along with a learnable prompt pair (positive and negative samples) into the latent space using the pre-trained CLIP’s image and text encoder. By nar-rowing the distance between the images and text in the latent space, we obtain an initial prompt pair that can effectively distinguish between backlit and well-lit images. 2) CLIP-Figure 2: Motivation. CLIP scores of proper prompts demon-strate alignment with human annotations (e.g., well-lit im-ages), suggesting that CLIP can serve as an indicator to differentiate between well-lit and backlit images. However, the best wordings could differ on a case-by-case basis due to complex illumination.
In contrast, the learnable posi-tive/negative prompts are more robust and consistent with the labels. aware Enhancement Training. With the initialized prompt, we train an enhancement network using the text-image simi-larity constraints in the CLIP embedding space. 3) Prompt
Refinement. We introduce a prompt fine-tuning mechanism, in which we update the prompt by further distinguishing the distribution gaps among backlit images, enhanced results, and well-lit images via rank learning. We iteratively update the enhancement network and prompt learning framework until achieving visually pleasing results.
Our method stands apart from existing backlit image en-hancement techniques as we leverage the intrinsic perceptual capability of CLIP. Rather than solely utilizing CLIP as a loss objective [8, 36], we incorporate prompt refinement as an essential component of the optimization process to further enhance performance. Our method is the first work to utilize prompt learning and the CLIP prior into the low-level vision task. Our approach surpasses state-of-the-art methods in both qualitative and quantitative metrics, without requiring any paired training data. We demonstrate the generalization capability and robustness of our method through the preview of our results shown in Fig. 1, and we compare our results with existing methods in Fig. 3. 2.