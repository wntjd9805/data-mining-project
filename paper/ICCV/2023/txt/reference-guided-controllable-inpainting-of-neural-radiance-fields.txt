Abstract
The popularity of Neural Radiance Fields (NeRFs) for view synthesis has led to a desire for NeRF editing tools.
Here, we focus on inpainting regions in a view-consistent and controllable manner. In addition to the typical NeRF inputs and masks delineating the unwanted region in each view, we require only a single inpainted view of the scene, i.e., a reference view. We use monocular depth estima-tors to back-project the inpainted view to the correct 3D positions. Then, via a novel rendering technique, a bilat-eral solver can construct view-dependent effects in non-reference views, making the inpainted region appear consis-tent from any view. For non-reference disoccluded regions, which cannot be supervised by the single reference view, we devise a method based on image inpainters to guide both the geometry and appearance. Our approach shows superior performance to NeRF inpainting baselines, with the addi-tional advantage that a user can control the output via a single inpainted image. Please visit our project page. 1.

Introduction
There has long been intense interest in manipulating im-ages, due to the broad range of content creation use cases.
Object removal and insertion, corresponding to the image inpainting task, is among the most studied manipulations.
Current inpainting models are capable of generating per-ceptually realistic content that conforms to the surrounding image. Yet, these models are limited to single 2D image inputs; our goal is to continue progress in applying such models to the manipulation of full 3D scenes.
The advent of Neural Radiance Fields (NeRFs) has made transforming real 2D photos into realistic 3D representa-tions more accessible. As algorithmic improvements con-tinue and computational requirements lessen, such 3D rep-resentations may become ubiquitous. We are thus interested in enabling the same manipulations of 3D NeRFs that are
∗ Authors contributed equally.
Input Views 
+ Camera Parameters
Input Masks
Shared Inputs
A red fence
A rubber duck
A ﬂower pot
Inpainted NeRFs
Outputs (Novel Views)
Figure 1: Visualization of our 3D inpainting approach.
Starting from (i) a set of posed images (i.e., standard structure-from-motion outputs), (ii) a multiview mask set associated to (i), and (iii) a single inpainted reference image from among (i), we produce a complete inpainted 3D scene, via a novel NeRF ﬁtting algorithm. By merely providing a different reference image, which can be as simple as chang-ing the text input, T, for a single-image text-conditioned inpainter (e.g., [55]), a user can controllably generate 3D scenes with the novel desired content. available for images, particularly inpainting (see Fig. 1).
Inpainting in 3D is non-trivial for a number of reasons, such as the paucity of 3D data and the need to account for 3D geometry as well as appearance. Using NeRFs as a scene representation comes with additional challenges.
First, the “black box” nature of implicit neural represen-tations makes it infeasible to simply edit the underlying data structure based on geometric understanding. Second, because NeRFs are trained from images, special consider-ations are required for maintaining multiview consistency.
Simply independently inpainting the constituent images us-ing powerful 2D inpainters yields viewpoint-inconsistent imagery (see Fig. 2), leading to visually unrealistic outputs.
One approach is to attempt to resolve these inconsisten-cies post hoc. For example, NeRF-In [34] simply combines views via a pixelwise loss. More recently, SPIn-NeRF [42] improved on this strategy by employing a perceptual loss
Figure 2: Sample independent inpaintings [55] for four dif-ferent views of a scene in the SPIn-NeRF dataset [42], us-ing the same prompt. The inpaintings are highly diverse, including some semantic differences, not just textural ones.
[84] instead. Yet, this fails when the inpainted views are perceptually different (i.e., the textures are far apart, even in the perceptual metric space). This limits applicability in the case of complex appearances or novel object inser-tion. For instance, recent diffusion-based inpainters (e.g.,
[55, 76]) can controllably hallucinate novel objects in 2D inpaintings – utilizing this capability is currently impossible in the post hoc framework. In addition, this approach im-pedes the preservation of speciﬁc desired details (i.e., inter-image conﬂicts prevent conservation of exact textures).
In contrast, others have considered single-reference in-painting (e.g., [34]): using only one inpainted view pre-cludes inconsistencies by construction. However, this lack of 3D information introduces a different set of challenges, including (a) poor visual quality in views far from the refer-ence, in part due to a lack of geometric supervision, (b) lack of view-dependent effects (VDEs), and (c) disocclusions.
In this work, we utilize a single inpainted reference, thus immediately avoiding view inconsistencies, and present a novel algorithm for handling challenges (a-c). First, to geometrically supervise the inpainted area, we utilize an optimization-based formulation with monocular depth es-timation. Second, we show how to simulate VDEs of non-reference views from the reference viewpoint. This enables a guided inpainting approach, propagating non-reference colours (with VDEs) into the mask. Finally, we inpaint dis-occluded appearance and geometry in a consistent manner.
We enumerate our contributions as follows: (i) a single-reference 3D inpainting algorithm (depicted in Fig. 1), which avoids visual quality deterioration at views far from the reference; (ii) a uniﬁed method for constructing super-vision for masked and disoccluded areas; (iii) a novel ap-proach to generating VDEs in non-reference views, without multiview appearance information; (iv) signiﬁcant empiri-cal improvements over prior work, not only in the unprece-dented sharpness of novel inpainted views, but also in terms of controllability, enabling users to insert novel objects into 3D scenes by simply providing a single inpainted 2D view. 2.