Abstract
Scene Graph Generation (SGG) aims to extract <sub-ject, predicate, object> relationships in images for vision understanding. Although recent works have made steady progress on SGG, they still suffer long-tail distribution is-sues that tail-predicates are more costly to train and hard to distinguish due to a small amount of annotated data com-pared to frequent predicates. Existing re-balancing strate-gies try to handle it via prior rules but are still confined to pre-defined conditions, which are not scalable for various
In this paper, we propose a Cross-models and datasets. modal prediCate boosting (CaCao) framework, where a visually-prompted language model is learned to generate diverse fine-grained predicates in a low-resource way. The proposed CaCao can be applied in a plug-and-play fash-ion and automatically strengthen existing SGG to tackle the long-tailed problem. Based on that, we further in-troduce a novel Entangled cross-modal prompt approach for open-world predicate scene graph generation (Epic), where models can generalize to unseen predicates in a zero-shot manner. Comprehensive experiments on three bench-mark datasets show that CaCao consistently boosts the per-formance of multiple scene graph generation models in a model-agnostic way. Moreover, our Epic achieves compet-itive performance on open-world predicate prediction. The data and code for this paper are publicly available.1 1.

Introduction
Scene graph generation (SGG) aims to detect visual re-lationships in real-world images, which consist of the sub-ject, predicate, and object (i.e., subject: flag, predicate: dis-played on, object: screen in Figure 1 (a)). Since scene graphs bridge the gap between raw pixels and high-level vi-sual semantics, SGG has been widely used in a variety of vi-†Corresponding Authors. 1https://github.com/Yuqifan1117/CaCao (a) Detected Image (b) Base SGG (c) Enhanced SGG (e) PredCls of different predicates (d) Long-tail predicate distribution
Figure 1. Illustration of handling long-tail distribution prob-lem by cross-modal predicate boosting in Visual Genome. (b) and (c) show scene graphs enhanced by visual knowledge gen-erating more informative predicates in long-tail distribution. (d) indicates the imbalance of predicates due to the long-tailed distri-bution in the training set. (e) For prediction of scene graph rela-tionships (PredCls), our CaCao framework can obtain consistent improvement on both head predicates and tail predicates. sual scene analysis and understanding tasks [3, 29, 31, 30], such as visual question answering [20, 15], image caption-ing [63, 54], and 3D scene understanding [14, 61].
Recently, various methods [4, 59, 58, 48, 55, 53, 7] have been proposed to improve the SGG performance, but still tend to predict frequent but uninformative predicates due to the long-tailed distribution of predicates in SGG datasets [52, 33, 56]. In a way, those approaches degenerate into a trivial solution, which undermines the application of
SGG. As shown in Figure 1 (d), in the Visual Genome [52], the top 20% of predicate categories account for almost 90% of samples, while other tail fine-grained predicates lack suf-ficient training data. Accordingly, the PredCls recalls of
SGG models on those tail predicates are remarkably lower than head predicates, as demonstrated in Figure 1 (e).
Prior works have been proposed in recent years to al-leviate the bias caused by the long-tail distribution based on causal rules [47, 32], reweighting [48, 51, 56] and resampling strategy [2, 53, 33] gradually. Nevertheless, these methods still require careful tuning of additional hyper-parameters, such as sampling frequency and category weight. They are sensitive to different architectures and data distributions, which are not flexible for real-world sit-uations. Another alternative way is to increase the number of tail predicates in training. IETrans [60] uses internal re-lation correlation to enhance the existing dataset. However, these methods rely on the prior distribution of source data and only work in specific pre-defined conditions. Such a manner based on hand-designed rules covers only limited categories, which is time-consuming and unscalable.
In this paper, we propose a Cross-modal prediCate boosting (CaCao) framework, which leverages the exten-sive knowledge from the pre-trained language models to enrich the tail predicates of scene graphs in a low-cost and easily scalable way. Our fundamental intuition is that lan-guage models gain extensive knowledge about informative relationships from massive text corpus during general sen-tence pre-training (i.e. Large silver airplane parked outside an airport with a pilot sitting in it that has come back from a mission, while the pilot gets some rest.) [44, 46]. While the pre-trained language models contain diverse relational knowledge, it is non-trivial to elicit this knowledge from them to scene graph generation. First, there is a significant modality gap in migrating extensive linguistic knowledge into scene graph predicate prediction since such large-scale language models are ‘blind’ to visual regions. An alterna-tive way is to use vision-language pre-training (VLP) mod-els. However, VLP models are mainly trained by image-text contrastive learning, lacking the delicate language ability to generate fine-grained predicate category words. Second, a predicate type might correspond to many different linguistic expressions (e.g., he “walks through” / “is passing through”
/ “passed by” a street may correspond to the same predi-cate). Without considering such semantic co-reference phe-nomenon, the adapted language model for predicate gener-ation can easily collapse to monotonic predictions.
To address the above challenges, we first introduce a novel cross-modal prompt tuning approach, which enables the language model to subtly capture visual context and predict informative predicates as masked language model-ing, called the visually-prompted language model. As for semantic co-reference, we further present an adaptive se-mantic cluster loss for prompt tuning, which models the se-mantic structures of diverse predicate expressions and adap-tively adjusts the distribution to inhibit excessive enhance-ment of specific predicates during boosting process, thus rendering a diverse and balanced distribution. Moreover, we introduce a fine-grained predicate-boosting strategy to extend the existing dataset with the informative predicates generated by our visually prompted language model. From the comprehensive view of Figure 1 (e), our CaCao can greatly improve the SOTA models’ performance in a plug-and-play way, where PredCls of most predicates are con-sistently increased by 30% in the purple bar than the blue.
From a more general perspective, our CaCao can not only effectively alleviate the long-tail distribution problem even in large-scale SGG but also generalize to open-world predicates by leveraging the generalizability of human lan-guage.
Inspired by the impressive zero-shot performance of vision-language pre-training models [42, 26, 22], which utilize the generalizability of human language for zero-shot transfer, we replace the traditional fixed predicate classifi-cation layer with category-name embedding and use the di-verse predicates generated by our CaCao to learn general and transferable predicate embeddings. Specifically, we propose a novel Entangled cross-modal prompt approach for open-world predicate scene graph generation (Epic), where the entangled cross-modal prompt alternately tinkers with the predicate representation, making the scene graph model aware of the abstract interactive semantics.
Surprisingly, without using any ground-truth annotations and only with the informative relations generated by our Ca-Cao framework, our Epic achieves competitive performance on the open-world predicate learning problem.
Our main contributions are summarized as follows:
• We propose a novel Cross-modal prediCate boost-ing (CaCao) framework, where a visually-prompted language model is learned to enrich the existing dataset with fine-grained predicates in a low-resource and scalable way.
• Our CaCao can be applied to SOTA models in a plug-and-play fashion. Experiments over three datasets show steady improvement in standard SGG tasks, demonstrating a promising direction to automatically boosting data by large-scale pre-trained language mod-els rather than time-consuming manual annotation.
• In addition, we introduce Entangled cross-modal prompt approach for open-world predicate scene graph generation (Epic) to explore the expansibility of Ca-Cao for unseen predicates, and validate its effective-ness with comprehensive experiments. 2.