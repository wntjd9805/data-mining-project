Abstract
Current closed-set instance segmentation models rely on pre-defined class labels for each mask during train-ing and evaluation, largely limiting their ability to detect novel objects. Open-world instance segmentation (OWIS) models address this challenge by detecting unknown ob-jects in a class-agnostic manner. However, previous OWIS approaches completely erase category information during training to keep the model’s ability to generalize to un-known objects.
In this work, we propose a novel train-ing mechanism termed SegPrompt that uses category infor-mation to improve the model’s class-agnostic segmentation ability for both known and unknown categories. In addition, the previous OWIS training setting exposes the unknown classes to the training set and brings information leakage, which is unreasonable in the real world. Therefore, we pro-vide a new open-world benchmark closer to a real-world scenario by dividing the dataset classes into known-seen-unseen parts. For the first time, we focus on the model’s ability to discover objects that never appear in the training set images.
Experiments show that SegPrompt can improve the over-all and unseen detection performance by 5.6% and 6.1% in AR on our new benchmark without affecting the infer-ence efficiency. We further demonstrate the effectiveness of our method on existing cross-dataset transfer and strongly supervised settings, leading to 5.5% and 12.3% relative im-provement. Code and data are released at: https:// github.com/ aim-uofa/ SegPrompt 1.

Introduction
Datasets are one of the most important driving force for deep learning. For general foundation models, massive paired text-image data harvested from the internet is a con-venient and valuable resource. However, for instance-level
*HC is the corresponding author. WM was visiting Zhejiang Univer-sity. perception tasks such as detection and segmentation, it is still very challenging to collect datasets at similar scales be-cause every instance requires pixel-level annotations.
In-creasing the number of categories will introduce ambiguity and cause unrealistic manual labor. Category-level anno-tations prevail in these tasks, and an object can be discov-ered only if it is classified into one of the known classes in the training set. One promising solution is to decouple detection and classification tasks to increase the semantic complexity. For example, in open-vocabulary segmentation
[3, 8, 28], a class-agnostic segmentation network is often adopted to locate objects, and the classification part relies on a vision language model. Under this framework, the ability to discover novel objects becomes crucial. In other words, we need to figure out how to discover more novel objects given the appropriate annotations. This task fits into the definition of open-world instance segmentation (OWIS)
[23], where the model is required to not only segment the known categories but also the unknown categories.
Recent works [18] have shown that class-agnostic train-ing encourages cross-dataset generalization.
Previous works on OWIS also have agreed that class-aware train-ing harms the model’s ability to generalize to unknown ob-jects. Thus, they all completely discard category informa-tion in the training phase. Consistent with this, our prelim-inary experimental results indicate that a stronger closed-world model generalizes better to unseen classes and class-agnostic training further improves the results. Specifically,
Mask2Former [1] surpasses Mask R-CNN by 3.9% AR and class-agnostic training leads to another 2.5% AR improve-ment. All phenomena show that the direct inclusion of cat-egory information damages the generalization ability of the model. is
In this work, we explore an interesting question: the category information really useless? To make use of the category information while maintaining the generaliza-tion ability, we propose a prompt learning mechanism—
SegPrompt—which serves as a training time auxiliary su-pervision to improve class-agnostic segmentation quality in
and re-organizing the train-validation split for COCO and
LVIS datasets. See Figure 1. Specifically, the ‘seen’ repre-sents objects that appear in the training set but are not anno-tated, and the ‘unseen’ represents real-world long-tail cate-gories that never appear in the training images. It is worth mentioning that the existing approaches [17, 22, 29] often rely on pseudo-labels, which only improve the seen cate-gories, and no work in literature has explored whether these methods are indeed effective on unseen categories. We are the first to emphasize the model’s ability to detect unseen objects that never appear in training images.
Experiments show that our approach SegPrompt can im-prove the overall/unseen detection by 5.6% and 6.1% in
AR. On existing cross-dataset transfer and strongly super-vised setting, it yields 5.5% and 12.3% relative improve-ment. We show that these prompts can be generated from word embeddings and example masks, extending our meth-ods to open vocabulary and few-shot segmentation tasks.
Our main contributions can be summarised as follows: 1. An auxiliary training mechanism, SegPrompt, which effectively uses category information to improve class-agnostic segmentation on various benchmarks includ-ing open-world, cross-dataset transfer, and strongly su-pervised segmentation. 2. A new benchmark, LVIS-OW, for open-world segmen-tation which explicitly separates “known”, “seen” and
“unseen” categories and aligns with real-world long-tail object discovery challenge. For the first time, we focus on the “unseen” categories that never appear in training images. 3. Experiments demonstrate that category-level prompts do have the ability to encode corresponding cate-gory appearance representations and control mask gen-eration, and have the potential to extend to open-vocabulary and few-shot segmentation. 2.