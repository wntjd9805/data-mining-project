Abstract
Deep Neural Networks (DNNs) for 3D point cloud recog-nition are vulnerable to adversarial examples, threaten-ing their practical deployment. Despite the many research endeavors have been made to tackle this issue in recent years, the diversity of adversarial examples on 3D point clouds makes them more challenging to defend against than those on 2D images. For examples, attackers can generate adversarial examples by adding, shifting, or re-moving points. Consequently, existing defense strategies are hard to counter unseen point cloud adversarial exam-ples.
In this paper, we first establish a comprehensive, and rigorous point cloud adversarial robustness bench-mark to evaluate adversarial robustness, which can pro-vide a detailed understanding of the effects of the defense and attack methods. We then collect existing defense tricks in point cloud adversarial defenses and then perform ex-tensive and systematic experiments to identify an effec-tive combination of these tricks. Furthermore, we pro-pose a hybrid training augmentation methods that con-sider various types of point cloud adversarial examples to adversarial training, significantly improving the adversar-ial robustness. By combining these tricks, we construct a more robust defense framework achieving an average accuracy of 83.45% against various attacks, demonstrat-ing its capability to enabling robust learners. Our code-base are open-sourced on: https://github.com/ qiufan319/benchmark_pc_attack.git. 1.

Introduction
As an prominent form of 3D data representation, point clouds are extensively employed in various real-world sensing applications, such as autonomous driving [36], robotics [14], and healthcare [1]. To achieve precise per-Figure 1. Point Cloud defense’s adversarial robustness to various attacks in a radar chart. We evaluate the defense under 9 at-tack methods, including PGD [9], SIA [7], L3A [23], Drop [38],
AOF [8], KNN [26], GeoA3 [30], AdvPC [5], Add [33], IFGM [9],
Perturb [33]. Our method achieve good adversarial robustness against all attacks. ceive 3D objects, prior studies [15, 16, 29] have investigated the development of deep neural networks (DNNs) capable of detecting, segmenting, and identifying objects from point cloud data. While these DNN-based methods have exhib-ited notable success, recent studies have exposed their sus-ceptibility to adversarial examples [33, 38, 30]. Specifi-cally, the addition, removal, or shifting of a small propor-tion of 3D points from an object can significantly degrade the performance of the DNNs.
*L. Wang and L. Sun are the corresponding authors.
To mitigate the risk of adversarial examples, several de-fense strategies have been proposed to enhance the robust-ness of point cloud DNNs [32, 39, 9]. For example, pre-processing techniques are applied to remove the points per-turbed by adversarial examples [39, 32]. In addition, adver-sarial training [9, 27, 11], which incorporates adversarial examples in the model training process, is designed to im-prove the adversarial robustness.
Despite the initial success of investigating the adversarial robustness of point cloud DNNs, there are three obvious limitations for existing attacks and defenses:
L1: Unrealistic attack and defense scenarios. The current state-of-the-art (SOTA) adversarial learning has pri-marily focused on wihite-box attacks and defenses [33, 30, 39], where the attacker has complete knowledge of the model architecture and paramteres. While these scenarios are useful for testing the limits of existing methods and un-derstanding their vulnerabilities, they do not reflect the real-world security threat landscape. In many security-critical applications, such as autonomous driving and financial sys-tems, attackers may not access to the model parameters.
L2: Lack of a unified and comprehensive adversarial robustness benchmark. While several studies [18, 28, 25, 19] have been proposed to evaluate the robustness of point cloud DNNs, they have are all focused on benchmark un-der diverse types of corruptions. However, existing bench-marks research for studying adversarial robustness remains unexplored. Compared with the corruption-oriented attack methods, adversarial examples are difficult to be detected by both humans and machines. Moreover, perturbation gen-erated using gradient descent are more effective than ran-dom corruptions, resulting in higher error rate and better imperceptibility. Despite recent studies exploring adversar-ial examples and defense on point cloud DNNs [33, 26], most of them have employ substantially different evaluation settings such as datasets, attacker’s capability, perturbation budget, and evaluation metrics. The lack of a unified evalu-ation framework makes it challenging to fairly quantify the adversarial robustness. Additionally, current adversarial ro-bustness evaluations only focus on one or a few attacks, de-fenses, and victim models, limiting the generalization and comparability of the results. For instance, the effectiveness of point cloud attack methods [33, 30] is typically evalu-ated under a limited set of defenses and models. Similarly, defense strategies are often evaluated against only a few early attacks, making it difficult to capture their strengths and weaknesses based on incomplete evaluations.
L3: Poor generalization of defense strategies. Differ 2D image attacks that modify the pixel value in a fixed data size, the adversarial example on point cloud offer a wider at-tack space and arbitrary data size. For instance, an attacker can generate adversarial example by adding, shifting, or re-moving points on the original point cloud. Unfortunately, most of existing defense strategies only consider one or two types, which can not handle unseen adversarial examples.
In this paper, we propose the first comprehensive and systematic point cloud adversarial robustness benchmark.
Our benchmark provides a unified adversarial setting and comprehensive evaluation metrics that enable a fair com-parison for both attacks and defenses. By analyzing the quantitative results, we propose a hybrid training strategy and construct a more robust defense framework by com-bining effective defense tricks. Our main contributions are summarized below: 1) Pratical Scenario. To evaluate the real-world perfor-mance of attacks and defenses, we refine the capability of both the attacker and defender, For example, we limited the maximum number of points added and the knowledge level of the attacker and defender. In our benchmark, all attackers are processed under the black-box setting, where the attacker does not have any additional knowledge about the model parameters, model structure, and training dataset. 2) Unified Evaluation Pipeline. Our benchmarks pro-vide a comprehensive and standardized evaluation method-ology, enabling fair comparison and reproducibility of the proposed methods. For example, we evaluate the attack from attack success rate, transferability, and imperceptible, which are essential metrics for assessing the effectiveness, imperceptibility, and generalization of the attacks. 3) Bag-of-tricks for Defending Adversarial Examples.
Based on our adversarial robustness analyses with our benchmark, we proposed a hybrid training approach that jointly consider different types of adversarial examples, in-cluding adding, shifting, and removing points, to perform adversarial training. Through analysis of experiment result, we further construct a more robust defense framework by combining the effective defense tricks. As shown in Fig-ure 1, our framework achieve the SOTA adversarial robust-ness under various attacks. 2.