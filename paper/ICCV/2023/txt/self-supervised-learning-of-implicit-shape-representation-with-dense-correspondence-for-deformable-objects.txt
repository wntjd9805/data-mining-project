Abstract
Learning 3D shape representation with dense correspon-dence for deformable objects is a fundamental problem in computer vision. Existing approaches often need addi-tional annotations of specific semantic domain, e.g., skele-ton poses for human bodies or animals, which require extra annotation effort and suffer from error accumulation, and they are limited to specific domain. In this paper, we pro-pose a novel self-supervised approach to learn neural im-plicit shape representation for deformable objects, which can represent shapes with a template shape and dense cor-respondence in 3D. Our method does not require the pri-ors of skeleton and skinning weight, and only requires a collection of shapes represented in signed distance fields.
To handle the large deformation, we constrain the learned template shape in the same latent space with the train-ing shapes, design a new formulation of local rigid con-straint that enforces rigid transformation in local region and addresses local reflection issue, and present a new hi-erarchical rigid constraint to reduce the ambiguity due to the joint learning of template shape and correspondences.
Extensive experiments show that our model can represent shapes with large deformations. We also show that our shape representation can support two typical applications, such as texture transfer and shape editing, with competi-tive performance. The code and models are available at https://iscas3dv.github.io/deformshape. 1.

Introduction
Shape representation with dense correspondence is a fundamental problem in computer vision. It plays a key role in many applications such as shape reconstruction [28, 34, 24], texture mapping [32, 10], and shape editing [10, 9, 36].
Early works often need additional semantic prior or anno-tations to learn such representation, e.g., SMPL [22] and
*indicates corresponding author
Figure 1: We present a self-supervised method to learn neu-ral implicit representation for deformable objects with a col-lection of shapes. Our method can generate shapes by de-forming a learned template and get dense correspondence.
SMAL [42] use registered meshes in certain semantic cate-gories and LEAP [27] and NASA [9] require annotations of skeletons and skinning weights, which limits the use cases and scalability of these representations. On the other hand, with the emerging implicit representation, more 3D assets are encoded in implicit sign distance function (SDF), and a shape representation for deformable objects in SDF is still largely missing in the community.
In this paper, we aim to design a neural representation for deformable objects (Figure 1). Given a target deformable object represented as a set of sign distance field under var-ious deformations, our method learns an implicit represen-tation that is able to reconstruct the 3D shapes, interpolate between the given examples, and provide dense correspon-dence across shapes, in a fully self-supervised manner with-out any additional annotation or semantic prior. Following the common idea [10, 31], we formulate the deformable shape as a static shape in a canonical (or template) space, plus the mappings from any target deformation space to the template space for arbitrary locations in the 3D space. How-ever, in the existence of large deformation, such as humans
and animals, we empirically found the above-mentioned ap-proaches tend to be unstable and can easily get stuck in local optima if learned self-supervised due to unique challenges:
Highly Under-constrained Optimization. The free-form template shape and deformation need optimizing jointly but highly under-constrained. Under the case with large defor-mation across training examples, the per-location mapping is error-prone, which in return affects the template shape, and there lacks a good constraint to push both back to the correct case. To mitigate the issue, we learn a generative model for the training shapes, where each shape is repre-sented as a code in a latent space. We then enforce a valid shape for the template shape by sampling from this latent space. We found this helps constrain the template shape and benefits the learning of dense correspondence.
Incomplete Local Rigid Constraint. As-rigid-as-possible (ARAP) constraint [1, 36] has been extensively used for dis-cretized surface such as meshes to penalize irregular surface deformation in many previous work [1, 36, 15]. However, defining ARAP equivalent constraint on continuous SDF is non-trivial. Existing works conduct a few attempts but all have their drawbacks. For example, Deng et al. [10] use smooth constraints to avoid predicted deformation be-ing large. However, this work cannot model the shapes with large deformation. Park et al. [31] propose a local rigid constraint however does not penalize flip mapping in a lo-cal region, and as a result a point on the left hand might be incorrectly mapped to the right hand.
In contrast, we propose a novel formulation of local rigid constraint that enforces rigid transformation in local region and addresses local flip mapping issue, as illustrated in reflection issue of
Figure 3(a). We give theoretical analysis and show that our local rigid constraint is, to the best of our knowledge, the first ARAP equivalent constraint define with implicit repre-sentation in infinitesimal scopes.
Insufficient Large-scale Deformation Prior. We found the learned shape representations with only local rigid con-straint at each point [31, 10] still suffer from local optima.
This is because shapes with large deformation often have rigid deformation in large scope, such as small neighbor-hood rigid regions (Figure 3(b)) and large rigid regions such as the limb on human body (Figure 3(c)), and the local rigid constraint does not have enough spatial context to take effect. Several rigid constraints on meshes [41] or point clouds [19] have been designed and proved to be effec-tive for shape representation by utilizing connections be-tween surface points. However, it is not straight-forward to extend these rigid constraints to implicit shape representa-tion due to the lack of explicit connections between points.
Therefore, previous implicit representation methods neglect large-scale deformation prior. To this end, we design hierar-chical rigid constraints for implicit representation to utilize spatial context of shapes at rigid part level and neighbor-hood level to reduce the ambiguity to learn shape and cor-respondence, which effectively constrains rigid motion in large scale and stabilizes the learning of the representation.
We perform extensive experiments to verify that our neural representation, learned with three above mentioned contributions, has superior capability in shape reconstruc-tion, deformation interpolation, and building dense corre-spondence. We also show that high quality results can be achieved in various applications, including texture transfer and shape editing, using our learned representation. 2.