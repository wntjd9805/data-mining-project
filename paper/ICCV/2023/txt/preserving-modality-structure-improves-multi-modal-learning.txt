Abstract
Self-supervised learning on large-scale multi-modal datasets allows learning semantically meaningful embed-dings in a joint multi-modal representation space without relying on human annotations. These joint embeddings en-able zero-shot cross-modal tasks like retrieval and classifi-cation. However, these methods often struggle to general-ize well on out-of-domain data as they ignore the semantic structure present in modality-specific embeddings. In this context, we propose a novel Semantic-Structure-Preserving
Consistency approach to improve generalizability by pre-serving the modality-specific relationships in the joint em-bedding space. To capture modality-specific semantic re-lationships between samples, we propose to learn multi-ple anchors and represent the multifaceted relationship be-tween samples with respect to their relationship with these anchors. To assign multiple anchors to each sample, we propose a novel Multi-Assignment Sinkhorn-Knopp algo-rithm. Our experimentation demonstrates that our pro-posed approach learns semantically meaningful anchors in a self-supervised manner. Furthermore, our evaluation on
MSR-VTT and YouCook2 datasets demonstrates that our proposed multi-anchor assignment based solution achieves state-of-the-art performance and generalizes to both in-and out-of-domain datasets. Code: https://github. com/Swetha5/Multi_Sinkhorn_Knopp 1.

Introduction
Humans often rely on multiple sensory inputs to have a better understanding of everyday events. Most commonly, we utilize vision, audio, and language to perceive an event as they provide complementary information for robust rea-soning. The closest approximation of this setup is video data as it provides both visual and audio information along with a text description as a caption. Recently, researchers have started to explore learning meaningful representations by leveraging multiple modalities to train efficient mod-els at scale [5, 11, 42]. Such systems focus on repre-sentation learning that either improves features for each modality separately [5] or learns a joint multi-modal em-bedding [11, 42] space that enables various zero-shot tasks like retrieval or classification. However, given the inher-ent differences across the modalities, it is challenging to learn effective joint embeddings. Furthermore, the real-world data presents additional challenges like misalignment between modalities, leading to weak supervision.
Current pre-training approaches in this area usually em-ploy a contrastive objective [33] to learn the joint embed-dings that pulls the cross-modal embeddings of a sample from the same temporal instance closer and pushes embed-dings of other samples farther. Despite promising perfor-mances, these methods struggle with generalizability. This is particularly evident in previous approaches trained on
HT100M [11, 42], which do well on the closely related downstream dataset YouCook2 but struggle to improve on the MSR-VTT dataset, which exhibits a relatively larger do-main shift with respect to HT100M [42]. This is due to the contrastive objectiveâ€™s emphasis on strict alignment be-tween modalities in the joint embedding space while ignor-ing the inherent weak alignment between different modali-ties [45], as well as the underlying semantic structure across samples [43, 50]. Recent works have tackled these issues, either by using joint multi-modal clustering [11] to pre-serve the semantic structure in the joint embedding space or by incorporating a reconstruction objective [11, 25] to re-tain modality-specific features in the joint embedding space, allowing for weak multi-modal alignment. However, the usual reconstruction objective trivially tries to retain most modality-specific features in the joint space, thus prevent-ing the learning of optimal features for cross-modal tasks.
And the multi-modal clustering approaches perform hard-clustering making it less flexible. Therefore, the limitations of the contrastive objective cannot be adequately addressed even after combining these independent objectives.
To address this, we propose a semantic-structure-preserving consistency loss (SSPC) to only retain infor-mation that is beneficial for both cross-modal embedding learning and retaining modality-specific semantic structure.
In particular, for SSPC loss we consider each sample (e.g., a video clip) to be composed of multiple concepts: scene or objects involved in the downstream task. Therefore, the relationship between samples is multifaceted, representing both shared and unique concepts across samples. To cap-ture this multifaceted relationship in a flexible manner, we propose to learn anchors (latent codes) and model the re-lationship between samples with respect to their relation-ships with these anchors. Therefore, these anchors act as a proxy to represent the modality-specific relationships be-tween samples (semantic structure) which can be preserved using the proposed SSPC loss. Since we have no supervision to learn these anchors, we formulate this anchor learning problem as a many-to-many assignment problem, as mod-eling this multifaceted relationship simultaneously involves assigning multiple anchors to one sample and multiple sam-ples to one anchor. Although there is a vast literature on solving the many-to-one assignment problem [9, 5, 38, 48], there is no efficient way to solve this many-to-many assign-ment problem.
To this end, we propose a novel Multi-Assignment
Sinkhorn-Knopp (Multi-SK) algorithm that iteratively op-timizes the many-to-many anchor assignments for both the modality-specific embeddings (in input space) and modality-agnostic multi-modal embeddings (in joint em-bedding space). To allow for weak alignment between modalities, we select the dominant anchors for each sample to represent the relationship between different samples. Our proposed SSPC loss enforces consistency between the dom-inant anchor assignments at the input and joint embedding spaces to preserve the modality-specific semantic structure.
To demonstrate the effectiveness of our proposed solution, we train our model on HT100M dataset and test on 6 zero-shot tasks on multiple downstream datasets and observe that our approach leads to state-of-the-art results in all settings.
In summary, we make the following contributions: (i)
We propose a flexible modality-specific semantic-structure-preserving approach to improve the generalizability of cross-modal features. (ii) We introduce Multi-Assignment
Sinkhorn-Knopp, a novel algorithm to enable multiple as-signments for flexible sample relationship modeling. (iii)
Our proposed method outperforms the current state-of-the-art for multi-modal self-supervised representation learning on both in- and out-of domain datasets. 2.