Abstract
The goal of continual learning is to improve the perfor-mance of recognition models in learning sequentially ar-rived data. Although most existing works are established on the premise of learning from scratch, growing efforts have been devoted to incorporating the benefits of pre-training.
However, how to adaptively exploit the pre-trained knowl-edge for each incremental task while maintaining its gen-eralizability remains an open question.
In this work, we present an extensive analysis for continual learning on a pre-trained model (CLPM), and attribute the key challenge to a progressive overfitting problem. Observing that selec-tively reducing the learning rate can almost resolve this is-sue in the representation layer, we propose a simple but ex-tremely effective approach named Slow Learner with Clas-sifier Alignment (SLCA), which further improves the clas-sification layer by modeling the class-wise distributions and aligning the classification layers in a post-hoc fash-ion. Across a variety of scenarios, our proposal provides substantial improvements for CLPM (e.g., up to 49.76%, 50.05%, 44.69% and 40.16% on Split CIFAR-100, Split
ImageNet-R, Split CUB-200 and Split Cars-196, respec-tively), and thus outperforms state-of-the-art approaches by a large margin. Based on such a strong baseline, critical factors and promising directions are analyzed in-depth to facilitate subsequent research. Code has been made avail-able at: https://github.com/GengDavid/SLCA. 1.

Introduction
Continual learning aims to learn effectively from se-quentially arrived data, behaving as if they were observed
*Equal contribution.
â€ Corresponding author. simultaneously. Current efforts are mainly based on the premise of learning from scratch, attempting to mitigate catastrophic forgetting [27] of previously-learned knowl-edge when adapting to each incremental task. However, the success of large-scale pre-training has revolutionized the training paradigm of deep neural networks. The pre-training stage brings both strong knowledge transfer and robustness to catastrophic forgetting for downstream con-tinual learning [42], which tends to be more significant as the scale of pre-training increases [30, 28]. Therefore, con-tinual learning on a pre-trained model (CLPM) turns out to be an emerging direction and receives growing attention.
For CLPM, the pre-trained knowledge is usually ex-pressed by the representation layer, adapted to a sequence of incremental tasks. There are two major strategies to lever-age the pre-trained knowledge [42]: (1) fine-tune the repre-sentations, or (2) keep the representations fixed while learn-ing a few additional parameters (e.g., adaptor [16], prompt
[24], instruction [8], etc.). Although (2) is becoming domi-nant in natural language processing (NLP) [18], the choice of (1) and (2) remains an open question for continual learn-ing in computer vision (CV). The recently proposed prompt-based approaches, such as L2P [45] and DualPrompt [44], followed the second strategy and reported to be far supe-rior to the traditional continual learning baselines of fine-tuning the representation layer. On the other hand, since the large amount of pre-training data are typically unla-beled and may also arrive incrementally, it seems more rea-sonable to use self-supervised pre-training than supervised pre-training [5, 42], also regarding that (upstream) contin-ual learning in a self-supervised manner is generally more robust to catastrophic forgetting [17, 26, 9].
In this work, however, we present an extensive analy-sis to reconsider the current progress and technical route for CLPM in CV. Specifically, CLPM poses a critical chal-lenge for continual learning that the pre-trained knowledge
der supervised or self-supervised pre-training, our proposal provides substantial improvements for CLPM in CV and dramatically fills the gap of current progress from the upper bound performance. For example, the naive Seq FT is im-proved by 49.76%, 50.05%, 44.69% and 40.16% on Split
CIFAR-100, Split ImageNet-R, Split CUB-200 and Split
Cars-196, respectively, thus outperforming the SOTA ap-proaches by a large margin (summarized in Fig. 1). On Split
CIFAR-100 and Split ImageNet-R, the performance gap is only less than 2% for supervised pre-training and less than 4% for self-supervised pre-training.
Our contributions include three aspects: (1) We present an extensive analysis of continual learning on a pre-trained model (CLPM), and demonstrate that the progressive over-fitting problem is the key challenge for traditional continual learning baselines. (2) We propose a simple but effective approach to address this problem, which provides substan-tial improvements for CLPM and clearly outperforms the state-of-the-art approaches, serving as a strong baseline to re-evaluate the current progress and technical route. (3) Our results further identify critical factors and promising direc-tions for CLPM, such as pre-training paradigm and down-stream granularity, so as to facilitate subsequent research. 2.