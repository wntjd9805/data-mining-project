Abstract
Referring video object segmentation (RVOS), as a su-pervised learning task, relies on sufficient annotated data for a given scene. However, in more realistic scenarios, only minimal annotations are available for a new scene, which poses significant challenges to existing RVOS meth-ods. With this in mind, we propose a simple yet effective model with a newly designed cross-modal affinity (CMA) module based on a Transformer architecture. The CMA module builds multimodal affinity with a few samples, thus quickly learning new semantic information, and enabling the model to adapt to different scenarios. Since the pro-posed method targets limited samples for new scenes, we generalize the problem as - few-shot referring video object segmentation (FS-RVOS). To foster research in this direc-tion, we build up a new FS-RVOS benchmark based on cur-rently available datasets. The benchmark covers a wide range and includes multiple situations, which can maxi-mally simulate real-world scenarios. Extensive experiments show that our model adapts well to different scenarios with only a few samples, reaching state-of-the-art performance on the benchmark. On Mini-Ref-YouTube-VOS, our model achieves an average performance of 53.1 J and 54.8 F, which are 10% better than the baselines. Furthermore, we show impressive results of 77.7 J and 74.8 F on Mini-Ref-SAIL-VOS, which are significantly better than the baselines.
Code is publicly available at https://github.com/ hengliusky/Few_shot_RVOS. 1.

Introduction
Referring video object segmentation (RVOS) aims to segment target objects described in natural language in
*Equal contribution. This work was done when G. Li visited to Feng
Zheng Lab in Southern University of Science and Technology.
†Corresponding author.
Figure 1: Comparison of Few-shot RVOS and RVOS and the setting of Few-shot RVOS. (a) The training and testing sets overlap in the RVOS. (b) Disjoint training and testing sets in the Few-shot RVOS. Different shapes represent dif-ferent classes. (c) Few-shot RVOS segments the referred object of the same class as the support set in the video. videos. In real-world scenarios, it has a wide range of ap-plications, such as video editing [9] and human-computer interaction, so RVOS has attracted much attention from the research community. Unlike traditional semi-supervised video object segmentation [10], RVOS is more challeng-ing because it not only lacks the ground-truth mask of the first frame of the video but also needs to interact with mul-timodal information of vision and language.
The great success of various tasks based on deep learn-ing benefits from sufficient labeled data. Detailed annotated masks and language descriptions in real-world RVOS tasks are relatively scarce. The researchers have to annotate each frame in the video in detail and provide a referring expres-sion for the segmentation object. Therefore, obtaining high-quality labeled data requires a high cost. With the popular-ity of movies, YouTube videos, TikTok streaming videos,
etc., video data in various fields has shown explosive growth in this media age. The demand for processing diverse data has brought significant challenges. In order to handle di-verse data, existing RVOS methods must rely on massive and diverse labeled data for training. But due to fixed and limited training classes, existing RVOS methods [1, 33] are essentially constrained to adapt to the highly dynamic and highly diverse data in the real world. On the other hand, if ones fine-tune existing RVOS methods on a few samples to adapt to real-world data, high-quality results are hard to be achieved because the labelled data is insufficient to support the model for learning the new semantics. Therefore, how to make the RVOS methods applicable to real-world diverse data with a lower cost is an urgent problem.
To address this problem, we propose the cross-modal affinity (CMA) module to build multimodal relationships in a few samples and learn new semantic information for di-versified data. Specifically, given only a few annotated sam-ples (language expressions and the referred object masks), we hierarchically fuse visual and text features in a cross-attention manner to obtain robust feature representations for a specific category. In this way, the model can handle enor-mous data in the same category more efficiently.
Essentially, the proposed method targets limited sam-ples. Therefore, we generalize the problem as Few-Shot Re-ferring Video Object Segmentation (FS-RVOS). We show the setting of FS-RVOS and the difference from existing
RVOS in Figure 1. Unlike RVOS, the training and testing sets’ categories disjoint in the FS-RVOS. Given a few sup-port video clips together with corresponding language de-scriptions and object masks, FS-RVOS aims at segmenting videos in the query set, as shown in Figure 1(c).
The key to FS-RVOS lies in the support set utilization and understanding of vision-language information. To bet-ter leverage the information in the support set, two meth-ods have been proposed based on the prototype and atten-tion mechanisms. The prototype-based [21, 35] methods compress the features belonging to different classes to ob-tain prototypes. However, noise is easily generated during the process. In addition, the spatial structures are ignored, resulting in different degree of information loss. Another methods [37, 38, 39] employ the attention mechanism to en-code foreground pixels from support features and aggregate them with query features. Although these methods achieve high-quality results in image and video domains, they are still under-explored in vision-language tasks.
To better utilize vision-language inputs, we propose the cross-modal affinity module to build the multimodal rela-tionships between samples in the support and query sets.
Specifically, multimodal features within the support set and query set are first fused separately. The information among them is then aggregated, which effectively prevents query features from being biased by irrelevant features.
Since this is the first work exploring Few-shot RVOS, the existing datasets are not directly applicable. Therefore, we build up a new FS-RVOS benchmark based on Ref-YouTube-VOS [22], named Mini-Ref-YouTube-VOS. The new benchmark covers a wide range with a balanced num-ber of high-quality videos in each category. To measure the model’s generalization ability, we also build a dataset different from natural scenes based on a synthetic dataset
SAIL-VOS [13], named Mini-Ref-SAIL-VOS. Since only videos and detailed annotated masks exist in the SAIL-VOS dataset, we add natural language descriptions correspond-ing to the segmentation targets for the dataset.
The main contributions of this work are as follows.
• For real-world limited samples, we propose a Cross-Modal Affinity (CMA) for building multimodal informa-tion affinity for referring video object segmentation.
• We explore a novel Few-shot RVOS problem, which learns new semantic information with limited samples and can adapt to diverse scenarios.
• We build up the first FS-RVOS benchmark, where we conduct comprehensive comparisons with existing meth-ods, showing the superiority of the proposed model. 2.