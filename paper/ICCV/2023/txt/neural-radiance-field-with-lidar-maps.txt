Abstract 1.

Introduction
We address outdoor Neural Radiance Fields (NeRF) [23] with real-world camera views and LiDAR maps. Existing methods usually require densely-sampled source views and do not perform well with the open source camera-LiDAR datasets.
In this paper, our design leverages 1) LiDAR sensors for strong 3D geometry priors that significantly improve the ray sampling locality, and 2) Conditional
Adversarial Networks (cGANs) [15] to recover image details since aggregating embeddings from imperfect LiDAR maps causes artifacts. Our experiments show that while
NeRF baselines produce either noisy or blurry results on
Argoverse 2 [42], our system not only outperforms baselines in image quality metrics under both clean and noisy conditions, but also obtains closer Detectron2 [43] results to the ground truth images. Furthermore, this system can be used in data augmentation for training a pose regression network [3] and multi-season view synthesis. We hope this work to serve as a new LiDAR-based NeRF baseline that pushes this research direction forward (released here).
Despite the fact that recent works have made massive improvements in novel view synthesis (NVS) for small scenes [2, 10, 17, 21, 23, 25, 28], large-scale outdoor scenes – such as street views and parks – are still challenging. Improv-ing the NeRF results on large-scale outdoor scenes would greatly benefit multiple applications, such as realistic simu-lators for robot navigation [1], localization [20], active map-ping and planning [46], and novel view augmentation [24].
The transition from indoor to outdoor presents a non-trivial challenge. Typically, the training of NeRF models de-mands densely sampled views to achieve good accuracy [7].
However, collecting densely sampled training views in out-door scenarios requires much labor and storage space, and the camera trajectories in outdoor settings are typically bi-ased (straight and along the lane). Many parts of the scene are often only observed by a limited number of views and range of view angles. This lack of data coverage issue of common outdoor datasets [9, 11, 42] is also addressed by previous works [30, 38], where specially collected datasets
instead of common public outdoor datasets were used. Fur-thermore, previous methods using simple MLPs to repre-sent large blocks tend to generate blurry results (Fig. 1 (b)).
On the other hand, it has been demonstrated that the de-mand of dense training views can be effectively reduced by geometry priors [7, 41], and modern outdoor robots – such as autonomous vehicles – are often equipped with Li-DAR sensors in addition to cameras. In contrast to previous
LiDAR-assisted works [5,30] that used LiDAR scans only as supervision or as a guidance for ray sampling, our approach treats the LiDAR map as sparse samples of the environment and directly distributes localized embeddings on it.
Using localized embeddings instead of global represen-tation can ease the burden of memorizing the whole scene with a single MLP in NeRF, leading to better embedding locality and convergence speed [13, 19, 44]. PointNeRF [44] embedded per-point features to 3D point clouds (from CNN prediction or COLMAP [34, 35]) and aggregated these point cloud embeddings along ray samples for volume rendering.
However, the method in [44] is not directly applicable to the common camera-LiDAR datasets we target. The real-world
LiDAR maps are prone to noise due to imperfect conditions such as bad weather. Simply using the point-based NeRF on outdoor LiDAR maps leads to noisy and unsatisfactory image quality, as in Fig. 1 (c). The 3D point cloud refine-ment techniques proposed in [44] requires the photometric constraints from the texture of dense training views, and thus are not suitable for the outdoor datasets where the views are sparser and the scenes are more complex. Instead of 3D point cloud refinement as [44], we propose to leverage strong 2D image refinement in this work.
The proposed pipeline takes a neural 3D point cloud (i.e. the LiDAR map with embeddings) as input and aggregates the LiDAR embeddings to perform volume rendering. In addition, we propose a tight sampling strategy in contrast to the naive radius-based counterpart in [44] to make samples better align with the LiDAR geometry prior. Finally, we refine the quality of synthesized views in 2D with a condi-tional GAN (cGAN) [15]. The proposed cGAN module can be trained end-to-end without additional data, and largely improves the final image quality. Besides common image metrics, we also show that the Detectron2 [43] detection results from our rendered images are closer to the results from ground truth images than the baselines. Last but not least, the proposed system can serve several interesting ap-plications, including data augmentation for training a pose regression network [3] and seasonal appearance rendering.
In summary, our pipeline amalgamates the strengths of neural radiance field (for deep implicit 3D representation),
LiDAR maps (for geometric priors), and cGAN (for deep realistic appearance rendering). The demonstrated applica-tions also show foreseeable potential of our system to benefit tasks that require a richer neural map representation. 2.