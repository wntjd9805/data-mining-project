Abstract
Generative Adversarial Networks (GANs) can pro-duce photo-realistic results using an unconditional image-generation pipeline. However, the images generated by
GANs (e.g., StyleGAN) are entangled in feature spaces, which makes it difficult to interpret and control the con-tents of images.
In this paper, we present an encoder-decoder model that decomposes the entangled GAN space into a conceptual and hierarchical latent space in a self-supervised manner. The outputs of 3D morphable face mod-els are leveraged to independently control image synthesis parameters like pose, expression, and illumination. For this purpose, a novel latent space decomposition pipeline is in-troduced using transformer networks and generative mod-els. Later, this new space is used to optimize a transformer-based GAN space controller for face editing. In this work, a StyleGAN2 model for faces is utilized. Since our method manipulates only GAN features, the photo-realism of Style-GAN2 is fully preserved. The results demonstrate that our method qualitatively and quantitatively outperforms base-lines in terms of identity preservation and editing precision. 1.

Introduction
Generative Adversarial Networks (GANs) [15, 20, 12] are formulated as a two-step learning procedure via gen-erator and discriminator models. This pipeline can pro-duce photo-realistic images that are hard to distinguish. Es-pecially, generative models such as StyleGAN2 [20] have become one of the most effective image synthesis tools.
They are capable of generating high-resolution images us-ing nonlinear features learned from low-dimensional fea-ture spaces. In the end, coarse and fine details of synthe-sized images are simply derived from these spaces. How-ever, existing GAN models do not offer intuitive control, i.e., not human understandable parameterization, for image generation. Recent works [16, 39, 32, 36] have shown that the outputs of GAN models can be edited by disentangling their features spaces without the need of employing full su-pervision or updating pre-trained model parameters.
Figure 1: Rather than finding a direct mapping between a
GAN space X of learned features (⋆), and a space P of pa-rameters of face concepts (▲) [39], our method estimates an intermediate latent space Q whose latent codes (•) are con-ceptual and hierarchical. After all, a GAN space controller trained on this space can edit each concept independently while preserving the details at different abstraction levels of
GANs for face editing.
There have been multiple efforts in the literature to achieve controllable content manipulation of features of
GANs with supervised and unsupervised learning methods.
In [17, 7], they use labelled data to expand user control onto a GAN space of features (depicted by X in Fig. 1).
These labels are used to decouple the entangled represen-tations with learnable projections. However, this control needs costly manual annotations that usually involve gath-ering many images and labels with clear definitions. Hence, performance is significantly impacted by the total number of annotations and the quality of labels. In contrast to the supervised methods, unsupervised approaches [34, 16, 28] achieve disentanglement by finding principal directions in feature spaces without relying on labeled data. In the end, each direction alters a different visual content so that a semi-automatic control is provided for the existing GAN models.
Other studies [36, 22, 37] have suggested that manipulating the GAN spaces may be limited by biases present in pre-trained models. Hence, their goal is to train a new generator model from scratch using generative priors for controllable image synthesis. Although the models have robust control
over the feature space, they are difficult and costly to train.
A setting [39] for face editing is to use concepts such as pose, light, expression, and likeness, derived from 3D Mor-phable face models [24, 30], as pseudo-labels. Each concept has a unique parameter set, and these parameters are used as pseudo-labels to disentangle a pre-trained GAN space.
For this purpose, the authors propose a model (called Dif-ferentiable Face Reconstruction (DFR)) and fix the original
GAN parameters. This model directly maps a GAN space
X to a face parameter space P (i.e., morphable faces) as il-lustrated in Fig. 1. Later, with a differentiable face renderer, the DFR model is used to decouple the GAN space X . To be specific, a face renderer is adapted to generate face im-ages from single-dimensional parameters estimated by the
DFR model. Thus, multi-resolution GAN features must be projected onto a concept parameter set. However, the draw-back is that a face parameter space P with no hierarchical information is utilized to edit a GAN space X . Thereby, the level of details in the GAN space is degraded and con-ceptual information in multi-resolution features may not be truly captured. Ultimately, the model fails to associate fa-cial concepts with a multi-resolution GAN space.
Our paper proposes a novel method to manipulate GAN spaces for face editing. For this purpose, we propose an encoder-decoder model that estimates an intermediate la-tent space (Q in Fig. 1) while learning a mapping between a
GAN space X and a face parameter space P. Compared to baseline, employment of this new space is crucial, since it derives hierarchical and conceptual latent codes for further
In other words, face concepts are independently usages. represented while hierarchical details at different abstrac-tion levels of GANs are maintained. Later, these latent codes are used to learn controlling a GAN space for face editing. Pseudo-labels estimated by 3D morphable models are used, and our pipeline does not require to update the
GAN models. Our approach has two main components: (i) A transformer-based latent code decomposer that computes conceptual and hierarchical latent codes from a
GAN space X of features, and the parameters of face con-cepts from P. A novel encoder-decoder model is pro-posed to compute intermediate latent codes, with an en-coder model based on transformer networks. These inter-mediate latent codes are then reprojected to the face pa-rameter space P using a multi-resolution generative model.
The proposed pipeline performs the decomposition in a self-supervised manner during the space mapping, relying solely on the reconstruction error at the output of our model. (ii) A GAN space controller that manipulates GAN space
X of features with face control parameters. To optimize the model, we enforce the consistency between the projected representations of the original and manipulated GAN fea-tures on the intermediate latent space Q. The model is based on a transformer network that uses face control parameters to manipulate the GAN space X .
Our contributions can be summarized as follows:
• We propose a novel encoder-decoder model that de-composes a multi-resolution GAN space X into an in-termediate latent space Q in a self-supervised man-ner. The decomposition is performed by our novel NN-architecture, relying solely on the reconstruction error during the space mapping.
• The intermediate latent space Q is used to train a GAN space controller for face editing. To this end, we in-troduce a transformer-based network inspired by [18] where user-specific control inputs are first encoded to multi-resolution representations. These representa-tions are then used to alter the GAN features.
• In the analyses, our method outperforms baselines in terms of identity preservation (achieving relative accu-racy improvement by 33%) and editing precision under extreme pose, expression and illumination manipula-tion, both qualitatively and quantitatively. 2.