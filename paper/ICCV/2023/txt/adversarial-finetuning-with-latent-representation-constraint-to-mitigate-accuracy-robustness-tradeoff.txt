Abstract
This paper addresses the tradeoff between standard ac-curacy on clean examples and robustness against adversar-ial examples in deep neural networks (DNNs). Although adversarial training (AT) improves robustness, it degrades the standard accuracy, thus yielding the tradeoff. To mit-igate this tradeoff, we propose a novel AT method called
ARREST, which comprises three components: (i) adversar-ial ﬁnetuning (AFT), (ii) representation-guided knowledge distillation (RGKD), and (iii) noisy replay (NR). AFT trains a DNN on adversarial examples by initializing its parame-ters with a DNN that is standardly pretrained on clean ex-amples. RGKD and NR respectively entail a regularization term and an algorithm to preserve latent representations of clean examples during AFT. RGKD penalizes the distance between the representations of the standardly pretrained and AFT DNNs. NR switches input adversarial examples to nonadversarial ones when the representation changes signiﬁcantly during AFT. By combining these components,
ARREST achieves both high standard accuracy and robust-ness. Experimental results demonstrate that ARREST mit-igates the tradeoff more effectively than previous AT-based methods do. 1.

Introduction
Deep neural networks (DNNs) have demonstrated im-pressive performance for various computer vision tasks [18, 28, 32, 45, 51, 55]. However, standardly trained DNNs can easily be deceived by adversarial examples [15,56], causing incorrect predictions. Such adversarial examples are images with maliciously designed, human-imperceptible perturba-tions to deceive a DNN. As DNNs penetrate almost every corner of our daily life (e.g., autonomous driving), defense
)
% ( y c a r u c c a k c a t t
A o t u
A 4 5 2 5 0 5 8 4 6 4 4 4 2 4 0 4 8 3 6 3
LBGAT
●
Ours w/ AWP
● φ =45o
AWP
●ST
●
●
●
S2 O
●
LAS−AT
TRADES
IAD
●
BS
●
SAT
●
●
AGKD−BML*
LAT
●
● TLA
● DAT
Adversarial training
●
●
φ =37.5o
● φ =30o
● φ =45o
φ =37.5o
●
● φ =30o
ARREST (Ours)
FAT
●
FS*
●
● AIT* 84 86 88
Standard accuracy (%) 90
Figure 1. Relationship between the standard and AutoAttack ac-curacies of various existing methods (see Appendix A) and our proposed method (ARREST) on CIFAR-10. * indicates a result obtained with WideResNet-28-10 [66]; the other results were ob-tained with WideResNet-34-10. We also evaluated our method by integrating it with the state-of-the-art AWP method [63], as de-noted by orange points. The red dashed line is an approximated curve of the accuracy-robustness tradeoff. techniques against adversarial examples are becoming in-creasingly important.    
The many defense techniques include feature squeez-ing [65], input denoising [17, 49], adversarial detection [30, 33], gradient regularization [41], and adversarial train-ing [15, 37]. Among these, adversarial training (AT) has attracted much attention as a promising defense method. AT attempts to build a robust DNN through training on adver-sarial examples that are generated online to maximally de-ceive the on-training DNN [37]. Since AT’s effectiveness was demonstrated by M ˛adry et al., a remarkable number of improvements have been proposed [5, 8, 12, 23, 24, 26, 29, 31, 34, 40, 52, 58, 59, 61–63, 68–72].
Although AT is the de facto standard method to build it
DNNs that are robust against adversarial examples, has the disadvantage of degrading the classiﬁcation ac-curacy on clean examples (i.e., the standard accuracy).
This implies a tradeoff between the adversarial robustness and standard accuracy [20, 57], which we refer to as the accuracy-robustness tradeoff. Figure 1 shows the accuracy-robustness tradeoff on CIFAR-10 [27] for various exist-ing methods evaluated by AutoAttack [11]. One partic-ular state-of-the-art method, AWP [63], achieves high ro-bustness, but its standard accuracy is 85.57 %, which is de-graded from 95.37 % with a standardly trained DNN. This tradeoff limits the practical applications of AT, as many real-world DNN applications require high standard accu-racy and cannot afford much degradation.
Several studies [12, 52, 70, 71] have attempted to miti-gate the tradeoff; however, the standard accuracy is still de-graded from the original accuracy of a standardly trained
DNN. One possible reason for this degradation is the dis-tribution mismatch [54, 64], which indicates that clean and adversarial examples have different underlying distribu-tions [54, 64]. This mismatch suggests that if we train a robust DNN from scratch, like in the above studies, the la-tent representation will signiﬁcantly diverge from that of a standardly trained DNN on clean examples (see Table 3).
Hence, there is room for improvement in terms of obtaining suitable latent representations of both clean and adversarial examples.
In this paper, we propose a novel method to mitigate the accuracy-robustness tradeoff in AT, called AdversaR-ial ﬁnetuning with REpresentation conSTraint (ARREST).
The idea behind our method is to obtain suitable represen-tations of adversarial examples while preserving suitable representations of clean examples from standardly trained
DNNs. To this end, ARREST comprises three key compo-nents: (i) adversarial ﬁnetuning (AFT), (ii) representation-guided knowledge distillation (RGKD), and (iii) noisy re-play (NR). ARREST uses a two-step training process for robust DNNs, with standard pretraining of DNNs on clean examples followed by ﬁnetuning on adversarial examples to increase robustness. We especially refer to the sec-ond step as AFT. AFT encourages a DNN to obtain suit-able representations of both clean and adversarial exam-ples through ﬁnetuning with a standardly pretrained DNN, in contrast to previous studies that trained the DNN from scratch [12, 52, 70, 71]. We also propose RGKD and NR to preserve representations of clean examples from the pre-trained DNN by alleviating the distribution mismatch is-sue [54, 64] during AFT. Inspired by knowledge distilla-tion [19, 48], RGKD penalizes the distance between the on-training DNN’s representation and that of the pretrained
DNN. While RGKD modiﬁes the objective function of training, NR modiﬁes the perturbation of inputs in AFT.
When the on-training DNN’s representation of a certain clean example signiﬁcantly diverges from that of the pre-trained DNN, NR switches the input from an adversarial example to a noisy one, which is a clean example with added uniform random noise. NR thus serves to “remind” the DNN of the standard pretraining and encourage repre-sentations of clean examples to be close to the pretrained
DNN’s original representations.
We experimentally demonstrate that ARREST achieves an impressive performance. For example, Fig. 1 shows its qualitative effectiveness in mitigating the accuracy-robustness tradeoff, as the results for ARREST are clus-tered on the upper-right side. Furthermore, we quantita-tively evaluate the degree of tradeoff mitigation with a new metric inspired by the BD-Rate [3, 53] utilized in the ﬁeld of video compression research. Speciﬁcally, our metric cal-culates the distance from the tradeoff by approximating a curve to represent it (red dashed line in Fig. 1). In terms of this metric, ARREST achieves a state-of-the-art perfor-mance, thus conﬁrming both its qualitative and quantitative effectiveness.
Our main contributions are threefold: 1. We propose a novel adversarial training method, AR-REST, to mitigate the accuracy-robustness tradeoff.
ARREST comprises three components that work com-plementarily to obtain suitable representations of both clean and adversarial examples. 2. We conduct a wide range of experiments to demon-strate ARREST’s effectiveness. Overall, the exper-imental results provide insights into the strengths of
ARREST and the properties of its components. 3. We propose a novel quantitative evaluation metric in-spired by the BD-Rate, and we show that ARREST achieves state-of-the-art performance in terms of this metric. 2.