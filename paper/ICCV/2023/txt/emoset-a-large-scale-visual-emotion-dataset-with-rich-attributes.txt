Abstract
Visual Emotion Analysis (VEA) aims at predicting peo-ple’s emotional responses to visual stimuli.
This is a task in affective computing, promising, yet challenging, which has drawn increasing attention in recent years. Most of the existing work in this area focuses on feature design, while little attention has been paid to dataset construction.
In this work, we introduce EmoSet, the ﬁrst large-scale vi-sual emotion dataset annotated with rich attributes, which is superior to existing datasets in four aspects: scale, anno-tation richness, diversity, and data balance. EmoSet com-prises 3.3 million images in total, with 118,102 of these im-ages carefully labeled by human annotators, making it ﬁve times larger than the largest existing dataset. EmoSet in-cludes images from social networks, as well as artistic im-ages, and it is well balanced between different emotion cat-egories. Motivated by psychological studies, in addition to emotion category, each image is also annotated with a set of describable emotion attributes: brightness, colorfulness, scene type, object class, facial expression, and human ac-tion, which can help understand visual emotions in a precise and interpretable way. The relevance of these emotion at-tributes is validated by analyzing the correlations between them and visual emotion, as well as by designing an at-tribute module to help visual emotion recognition. We be-*Corresponding author. lieve EmoSet will bring some key insights and encourage further research in visual emotion analysis and understand-ing. Project page: https://vcc.tech/EmoSet. 1.

Introduction
Emotions are different ways to think that our mind uses to increase our intelligence [31]. Much of the research in Artiﬁcial Intelligence (AI) has focused on designing human-like machines, while neglecting emotional intelli-gence. Since emotions are innate to human beings, AI sys-tems should aim to better understand emotions, in order to succeed in mimicking human behavior. Affective comput-ing [35] is an emerging ﬁeld that aims to identify, under-stand, and respond to human emotions. This ﬁeld has seen signiﬁcant progress in recent years, and has potential ap-plications in areas such as education [49], healthcare [56], advertising [42], and safety [9].
Visual Emotion Analysis (VEA) is a promising, yet chal-lenging, task in affective computing, aiming to predict emo-tional responses to visual stimuli. For instance, when view-ing the images in Figure 1, one not only recognizes the vi-sual elements therein, but also may experience emotional reactions. Furthermore, even though emotions are subjec-tive, people tend to share similar reactions to the same ex-ternal stimuli. With the prevalence of social networks, users often choose to convey feelings via images shared on the in-ternet. Thus, VEA is an increasingly popular research topic within the computer vision ﬁeld [51, 63]. Advances in VEA may beneﬁt high-level vision tasks (e.g., image aesthetic assessment [26], stylized image captioning [12], and im-age understanding [47]), as well as human-centered appli-cations (e.g., opinion mining [27], mental health [46], smart advertisement [41], and hate detection [1]).
Most of the work in VEA focused on feature design, cov-ering hand-crafted features [28, 60, 3] and, more recently, learned ones [37, 53, 51]. Based on art and psychologi-cal theories, hand-crafted features fail to cover all important factors in human emotions. Although deep learning meth-ods boost recognition performance signiﬁcantly, the results are still unsatisfying. In particular, while supervised deep learning methods often require large-scale labeled datasets, little attention has been paid to dataset construction. Ex-isting VEA datasets are usually unlabeled on a large scale or labeled on a relatively small scale [24, 58, 34]. Besides, only emotion labels are provided in most datasets. Since emotions are abstract, a key problem is how to bridge the affective gap [13] between images and emotions with aux-iliary information. We believe that a new and rich dataset is needed for further research and improvement in VEA.
To tackle the above issues, we introduce EmoSet, a large-scale visual emotion dataset, annotated with rich attributes.
EmoSet is superior to existing datasets in four aspects: scale, annotation richness, diversity, and data balance. The full (EmoSet-3.3M) dataset comprises 3.3 million machine retrieved and annotated images, among which there are 118,102 human-annotated ones (EmoSet-118K). The latter is ﬁve times larger than the widely-used FI dataset [58], as reported in Table 1. Apart from emotions, our dataset is an-notated with emotion attributes. Inspired by psychological studies [22, 4, 7], we propose a set of describable visual at-tributes to facilitate understanding why an image evokes a certain emotion. Considering the complexity of emotions, the attributes are designed to cover different levels of visual information, including brightness, colorfulness, scene type, object class, facial expression, and human action. With these rich attribute annotations, we hope EmoSet will im-prove not only the recognition of visual emotions, but also their understanding.
By querying 810 emotion keywords based on Mikels model [29], we collect 3.3 million candidate images from four different sources to form the EmoSet-3.3M dataset. A subset of EmoSet-3.3M is then labeled by human annota-tors, yielding the EmoSet-118K dataset. Compared with existing datasets, EmoSet contains diverse images covering both social and artistic types. Furthermore, EmoSet-118K is well balanced between the eight emotion categories, each of which is represented with 10,660 to 19,828 images, as reported in Table 2. We further analyze the correlations be-tween our attributes and emotion categories, and demon-strate that some attributes are indeed strongly relevant to emotions. In addition, to mine the emotion-related infor-mation from each attribute, we design an attribute module for visual emotion recognition, and validate it using several
CNN backbones.
In summary, our contributions are:
• EmoSet, the ﬁrst large-scale visual emotion dataset with rich attributes, exceeding existing VEA datasets in terms of scale, annotation richness, diversity and data balance.
• A set of describable emotion attributes motivated by psychological studies, which help understand visual emotional stimuli in a precise and interpretable way.
• A series of in-depth analyses on EmoSet, to explore how emotion attributes advance emotion understand-ing. Statistical analysis shows that correlations do oc-cur between emotions and attributes, which is consis-tent with human cognition.
• An attribute module to facilitate visual emotion recog-nition. Experimental results and visualizations further validate the relevance of emotion attributes and show their potential in understanding visual emotions. 2.