Abstract
Text-to-video retrieval systems have recently made sig-nificant progress by utilizing pre-trained models trained on large-scale image-text pairs. However, most of the latest methods primarily focus on the video modality while disre-garding the audio signal for this task. Nevertheless, a recent advancement by ECLIPSE has improved long-range text-to-video retrieval by developing an audiovisual video repre-sentation. Nonetheless, the objective of the text-to-video retrieval task is to capture the complementary audio and video information that is pertinent to the text query rather than simply achieving better audio and video alignment. To address this issue, we introduce TEFAL, a TExt-conditioned
Feature ALignment method that produces both audio and video representations conditioned on the text query. Instead of using only an audiovisual attention block, which could suppress the audio information relevant to the text query, our approach employs two independent cross-modal atten-tion blocks that enable the text to attend to the audio and video representations separately. Our proposed method’s efficacy is demonstrated on four benchmark datasets that include audio: MSR-VTT, LSMDC, VATEX, and Charades, and achieves better than state-of-the-art performance con-sistently across the four datasets. This is attributed to the additional text-query-conditioned audio representation and the complementary information it adds to the text-query-conditioned video representation. 1.

Introduction
The emergence of online streaming services has led to an enormous and rapidly growing collection of multimedia assets comprising video and audio. Retrieving semantically similar content in these assets is crucial for finding infor-mation of interest, making it an important aspect of major
*This work was done while Sarah Ibrahimi and Mohamed Omar were at Amazon Prime Video.
Figure 1. Comparison of our method with the current methods, text-to-video retrieval with (A) video only, (B) audio-video fu-sion and (C) proposed text-conditioned audio-video alignment (TEFAL). streaming platforms. Text-to-video retrieval is a common approach to achieve this goal, whereby video content that best matches a textual description is searched for by learn-ing a joint latent space for text and video representations.
This space allows the text modality input to be matched with the video modality, enabling the closest videos to be found through a distance metric.
The rise of large-scale transformer models of vision and language has led to the development of many multimodal transformer-based architectures that are commonly evalu-ated on the text-to-video retrieval task. These architectures can either be pre-trained on large-scale multimodal datasets from scratch or use existing pre-trained models, such as
CLIP [29], as starting points or frozen backbones. One of the earliest CLIP-based model architectures, CLIP4Clip
[26], shows a significant improvement in performance com-pared to previous state-of-the-art methods [3, 19, 21] on common text-to-video retrieval benchmarks. By utilizing only a few video frames per video and a simple tech-nique to aggregate the frame embeddings for each video,
CLIP4Clip demonstrated the utility of a 2D vision trans-former to outperform model architectures using 3D videos as input. Since then, many other works have been built upon this baseline approach with novel ways of cross-attention
[16], pretext tasks [12], prompting [20], and other archi-tectural modifications. Recently, significant improvements have been obtained by incorporating post-processing tech-ure 1.
We use the CLIP [29] model as the video and text feature extractor and the AST [15] model as the audio feature ex-tractor in our approach. The text feature serves as a crucial link between the video and audio representations, acting as the query in the calculation for cross-attention. Meanwhile, the video and audio features are used for key and value com-putations. To simplify matters, the two aligned feature types are combined to produce the final audio-enhanced video representation. We conducted extensive experiments on various datasets that include audio, such as MSR-VTT [41],
LSMDC [32], VATEX [39], and Charades [34]. Our pro-posed audio-enhanced text-conditioned feature alignment method consistently outperforms existing methods. Specif-ically, our method improves the Recall@1 by over 4% com-pared to ECLIPSE on the MSR-VTT dataset.
Our key contributions are summarised as follows:
• We propose a text-conditioned feature alignment ap-proach for audio-enhanced text-to-video retrieval. We are the first to do so and we explain why this approach is more suitable for this task than audiovisual align-ment. To achieve this, we utilise two independent cross-modal attention blocks for the text to attend to the audio and video representations.
• We conducted extensive experiments on several bench-mark datasets that include audio, namely MSR-VTT,
LSMDC, VATEX, and Charades. Our results demon-strate state-of-the-art performance in text-to-video re-trieval when compared with the best previously pub-lished results. 2.