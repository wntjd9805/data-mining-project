Abstract
Real-time rendering for video games has become in-creasingly challenging due to the need for higher resolu-tions, framerates and photorealism. Supersampling has emerged as an effective solution to address this challenge.
Our work introduces a novel neural algorithm for super-sampling rendered content that is 4× more efficient than existing methods while maintaining the same level of accu-racy. Additionally, we introduce a new dataset which pro-vides auxiliary modalities such as motion vectors and depth generated using graphics rendering features like viewport jittering and mipmap biasing at different resolutions. We believe that this dataset fills a gap in the current dataset landscape and can serve as a valuable resource to help measure progress in the field and advance the state-of-the-art in super-resolution techniques for gaming content. 1.

Introduction
Real-time rendering has become increasingly difficult for video games due to the demand for higher resolutions, framerates and photorealism. One solution that has recently emerged to address this challenge consists in rendering at lower resolution and then use an upscaling technique to achieve the desired resolution. However, developing effi-cient upscaling solutions that balance speed and accuracy remains a challenge. Recently, several commercial solu-tions have been developed for gaming super-resolution, in-cluding those that are based on deep learning (DL) such as
Nvidia’s DLSS [36] or Intel’s XeSS [11], as well as solu-tions that do not rely on machine learning, such as AMD’s
FSR [18, 19]. Despite the availability of these commercial solutions, there has been relatively little published research on the application of DL-based super-resolution for gaming.
We believe that one of the reasons why DL-based super-resolution for gaming has received little attention compared to super-resolution of natural content is that there is cur-rently no standard, publicly available dataset for develop-*Qualcomm AI Research is an initiative of Qualcomm Technologies,
Inc. and/or its subsidiaries.
Figure 1. Example images produced by our solution using neural networks of different sizes. These models produce 1080p outputs in respectively 1.08 ms, 1.53 ms, and 3.52 ms on an RTX 3090, which is 4× to 12× faster than previous work by Xiao et al. [59].
Figure 2. Example of data modalities available in the QRISP dataset. First row, from left to right: Native 270p, Negative 2 mipmap biased 270p, Negative 1.58 mipmap biased 360p, Nega-tive 1 mipmap biased 540p. Second row, from left to right: 540p depth, 540p motion vectors, Native 1080p, Enhanced 1080p ing gaming-specific super-resolution solutions. Researchers and developers who want to study or improve upon exist-ing methods must create their own datasets, which can be a time-consuming and resource-intensive process.
Our work makes the following contributions:
• we release a dataset specifically designed for the re-search and development of gaming super-resolution al-gorithms. We show that models trained on this dataset 1
Figure 3. Example of color, depth and motion vector images from the QRISP dataset. In total, our dataset contains 8760 frames (7260 for training, 1500 for testing) from 13 distinct scenes, rendered at different resolutions ranging from 270p to 1080p. More details can be found in the supplementary materials. can compete and outperform the quality levels ob-tained by commercial solutions such as DLSS [36].
• we propose an efficient gaming super-resolution ar-chitecture which leverages auxiliary modalities (sub-pixel accurate motion vectors, depth) and graphics ren-dering features (viewport jittering, mipmap biasing) commonly-used for temporal anti-aliasing. Our solu-tion is 4× more efficient than previous published work
[59] for the same level of accuracy.
Overall, we believe that this work provides a new re-source to measure progress in the field and help advance the state-of-the-art in gaming super-resolution. 2.