Abstract
In daily life, humans utilize hands to manipulate objects.
Modeling the shape of objects that are manipulated by the hand is essential for AI to comprehend daily tasks and to learn manipulation skills. However, previous approaches have encountered difficulties in reconstructing the precise shapes of hand-held objects, primarily owing to a deficiency in prior shape knowledge and inadequate data for train-ing. As illustrated, given a particular type of tool, such as a mug, despite its infinite variations in shape and appear-ance, humans have a limited number of ‘effective’ modes and poses for its manipulation. This can be attributed to the fact that humans have mastered the shape prior of the
‘mug’ category, and can quickly establish the correspond-ing relations between different mug instances and the prior, such as where the rim and handle are located. In light of this, we propose a new method, CHORD, for Category-level
⋆These authors contributed equally.
†Cewu Lu is the corresponding author. He is the member of Qing Yuan
Research Institute and MoE Key Lab of Artificial Intelligence, AI Institute,
Shanghai Jiao Tong University, China, and Shanghai Qi Zhi institute.
Hand-held Object Reconstruction via shape Deformation.
CHORD deforms a categorical shape prior for reconstruct-ing the intra-class objects. To ensure accurate reconstruc-tion, we empower CHORD with three types of awareness: appearance, shape, and interacting pose. In addition, we have constructed a new dataset, COMIC, of category-level hand-object interaction. COMIC contains a rich array of object instances, materials, hand interactions, and viewing directions. Extensive evaluation shows that CHORD outper-forms state-of-the-art approaches in both quantitative and qualitative measures. Code, model, and datasets are avail-able at https://kailinli.github.io/CHORD 1.

Introduction
In daily life, we perform complex tasks by continu-ally manipulating a limited number of simple objects with our hands. Understanding the physical nature of hand-object interaction is crucial for AI to comprehend human activities. This requires us to pursue a geometric rep-resentation (reconstruction) of the hand-held object, es-pecially for manipulable tools. Many significant efforts
[26, 31, 56, 54, 57, 11, 5] have been made for reconstructing
both the hand and hand-held objects. These works, where the hand is represented by a kinematic prior, such as MANO
[43], have achieved high qualities on hand reconstruction via keypoints estimation. However, producing the object shape in high quality is more challenging, primarily due to the mutual occlusion, lack of geometrical prior, and insuffi-cient data of variant shapes and appearances.
Most previous works thus resorted to reconstructing ob-jects with known template shape [47, 36, 25, 54, 24]. This setting is commonly referred to as “object pose estimation”.
However, it would fail on the unseen object instances. To address this limitation, some works [26, 31, 57, 11] have at-tempted to directly regress object-agnostic (and also pose-agnostic) surfaces from images using large-scale synthetic data. However, their results are not robust to varied ob-ject shapes and appearances, and often produce “bubble-like” shapes and broken geometries. These issues can be attributed to either a lack of shape basis [31, 57, 11] or the use of an underlying shape basis with fixed topology and resolution [26, 20, 29].
In this paper, we aim to leverage the best of both worlds: utilizing shape information from a known tem-plate while also generalizing well to unseen instances. With this in mind, we propose to first estimate the object pose at category-level, and then resort to a categorical object shape prior (later we call it object-prior) to reconstruct the surface of unseen objects. We design CHORD, stand for
Category-level Hand-held Object Reconstruction via shape
Deformation. CHORD is a deep-learning model that learns to “deform” an implicit surface from an explicit object-prior. Given the estimated pose of the object-prior, CHORD takes two steps. It first deforms the 2D surface-aware fea-ture maps (i.e. normal and depth map) of the object-prior to those of the actual object instance. From these feature maps,
CHORD then deforms the 3D shape of the object-prior to the actual object instance.
Inspired by [45, 53, 39], the first step is achieved by leveraging the design of an image-to-image translation net-work (GN, Sec. 3.2), Specifically, we rendered the normal and depth map of the object-prior in the estimated poses, along with the image, as inputs for GN. Additionally, we use the rendered depth and normal maps of the estimated
MANO hand mesh as extra inputs to help with decoupling the surfaces of the hand and object.
To perform the second step, we use a point-wise im-plicit function (IF, denoted as GS) [40] to regress the signed distance of query points to the surface of object instance.
The final reconstruction is then obtained as the zero-level set of these query points. The use of IF enables us to re-construct objects with fine-grained geometry and arbitrary topology. We argue that for robustly and accurately recon-structing the hand-held object, it is necessary for CHORD to possess three different types of awareness: (1) the appear-ance awareness, (2) the shape awareness of the categorical object-prior, and (3) the pose awareness of the interacting hand. To integrate these awareness, we develop three types of local features (Sec. 3.3-A,B,C). Consequently, GS’s pre-diction of signed distance is conditioned on these features.
The final issue is that the current datasets for hand-object interaction (HOI) are not suitable for reconstruction in category-level. These datasets commonly lack diverse samples within the same category [23, 3, 17, 2] or lack real-world human interacting behaviors [26]. To address this limitation, we propose a new dataset, named COMIC, which is built from high-fidelity rendering and targets on category-level hand-object reconstruction (Sec. 3.5). This dataset contains a large number of images that depict the interaction between the hand and categorical objects with diverse shapes and appearances (see Figs. 1 and 5). Unlike the simulated grasping poses as in GraspIt [38], which do not reflect the intention of human behaviors, the interacting poses in COMIC are based on real-world human demonstra-tions captured in [55].
We conduct an extensive evaluation of CHORD utiliz-ing the COMIC dataset, including qualitative outcomes on several additional HOI datasets (not incorporated for train-ing) as well as unseen, in-the-wild images. Our quantitative comparisons verify that CHORD exceeds the state-of-the-art (SOTA) in performance. Furthermore, our qualitative re-sults illustrate that CHORD generalizes more effectively to unseen, in-the-wild instances. We summarize our contribu-tion in three-folds:
• We propose the reconstruction of hand-held objects at category-level via our novel model, CHORD. The model explicitly encapsulates shape from an object-prior, facil-itating the deformation of the implicit surface to actual object instances.
• Within CHORD, we incorporate three types of awareness
- appearance, shape, and interacting pose - to ensure the accuracy of reconstruction. An extensive ablation study and gain analysis substantiate the theory that performance enhancement corresponds with increased awareness.
• We introduce a new dataset for category-level hand-object reconstruction, known as COMIC. This dataset comprises a wealth of high-fidelity images featuring a vast variety of objects and interacting hand poses. 2.