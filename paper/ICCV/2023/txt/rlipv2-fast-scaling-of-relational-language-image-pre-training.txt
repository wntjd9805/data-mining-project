Abstract
Relational Language-Image Pre-training (RLIP) aims to align vision representations with relational texts, thereby advancing the capability of relational reasoning in com-puter vision tasks. However, hindered by the slow conver-gence of RLIPv11 architecture and the limited availability of existing scene graph data, scaling RLIPv1 is challenging.
In this paper, we propose RLIPv2, a fast converging model that enables the scaling of relational pre-training to large-scale pseudo-labelled scene graph data. To enable fast scal-ing, RLIPv2 introduces Asymmetric Language-Image Fu-sion (ALIF), a mechanism that facilitates earlier and deeper gated cross-modal fusion with sparsified language encoding layers. ALIF leads to comparable or better performance than RLIPv1 in a fraction of the time for pre-training and fine-tuning. To obtain scene graph data at scale, we extend object detection datasets with free-form relation labels by introducing a captioner (e.g., BLIP) and a designed Rela-tion Tagger. The Relation Tagger assigns BLIP-generated relation texts to region pairs, thus enabling larger-scale re-lational pre-training. Through extensive experiments con-ducted on Human-Object Interaction Detection and Scene
Graph Generation, RLIPv2 shows state-of-the-art perfor-mance on three benchmarks under fully-finetuning, few-shot and zero-shot settings. Notably, the largest RLIPv2 achieves 23.29mAP on HICO-DET without any fine-tuning, yields 32.22mAP with just 1% data and yields 45.09mAP with 100% data. Code and models are publicly available at https://github.com/JacobYuan7/RLIPv2. 1.

Introduction
The pretraining-finetuning paradigm has achieved ma-jor breakthroughs in vision and language domains [58, 13,
*Work conducted during their research internships at DAMO Academy.
†Corresponding author. 1RLIPv1 refers to the model presented in [86], and RLIPv2 refers to the model presented in this paper.
Figure 1: Left: Fine-tuning comparison on HICO-DET. Right:
Pre-training epoch and zero-shot (NF) comparison on HICO-DET.
Except where stated, RLIPv2-ParSeDA architecture is adopted. 23, 103, 65, 77]. In this context, a number of particularly notable results have been obtained through aligned Vision-Language Pre-training (VLP) [65, 42, 41, 78, 95, 91]. These research efforts have typically employed a robust base model [14, 25, 12, 74] that is trained on language-image paired data to produce foundation models.
RLIPv1 [86] presents the first attempt to specifically align vision representations and relational texts using VLP.
By pre-training on open-vocabulary scene graph data like
Visual Genome (VG) [38], RLIPv1 demonstrates its use-fulness in zero-shot, few-shot and fully-finetuned Human-Object Interaction (HOI) Detection. Although RLIPv1 is proven effective, we find it challenging to scale for the fol-lowing reasons: (i) Model perspective: RLIPv1 converges slowly, as exemplified by DETR [3]-based RLIPv1, which requires 150/90 epochs to converge during pre-training/fine-tuning.
Even when building on Deformable DETR (DDETR) [102] and DAB-DDETR [56], 50/60 epochs are still required. (ii) Data perspective: as observed by the authors of
RLIPv1, data with relation triplet annotations is scarce, im-peding RLIPv1’s scaling. Annotating triplets in the form of
⟨subject, relations, object⟩ is both time- and cost-intensive.
To resolve the aforementioned challenges, we introduce
RLIPv2, a fast converging model that enables relational pre-training on larger-scale pseudo-labelled scene graph data.
From both the model and data perspectives, we summarize the contributions of RLIPv2 as follows.
From the model perspective, we observe that the slow convergence of DDETR can be attributed to the late fusion after decoding. language-image fusion strategy:
Prior works [15, 44] have demonstrated an earlier and deeper fusion mechanism facilitates cross-modal alignment.
In light of this, we propose Asymmetric Language-Image
Fusion (ALIF) in RLIPv2 that encourages fusion in the en-coding stage with sparsified language layers. Without sac-rificing inference speed thanks to sparsification, RLIPv2 re-quires only 20 epochs to pre-train and fine-tune based on
DDETR family models [102, 56] as shown in Fig. 1, while performing better than or comparably to RLIPv1.
From the data perspective, we leverage well-established object detection datasets [53, 69, 39]. Specifically, we ex-tend these datasets with relational annotations by pseudo-labelling. To perform pseudo-labelling, we must tackle two challenges: (i) sorting out the relations contained in the image and (ii) tagging relation texts to region pairs. Re-garding the first challenge, we employ external captioners (e.g. BLIP [42]) that generate captions containing rela-tion descriptions. Regarding the second challenge, we reuse
RLIPv2 model as a Relation Tagger (R-Tagger) that enables assigning the generated open-vocabulary relation texts to region pairs. Equipped with such a pipeline, we investi-gate the scaling behavior of both the model and the data for
RLIPv2, which demonstrates improved zero-shot, few-shot and fine-tuning performance.
Furthermore, we introduce Scene Graph Generation (SGG), an analogously defined task to HOI detection for evaluating RLIPv2. RLIPv2 achieves state-of-the-art per-formance on Open Images v6 [39] for SGG, which under-scores its robustness and efficacy in tackling relational rea-soning tasks. 2.