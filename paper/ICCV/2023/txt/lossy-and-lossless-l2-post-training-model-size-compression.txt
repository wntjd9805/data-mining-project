Abstract
Deep neural networks have delivered remarkable perfor-mance and have been widely used in various visual tasks.
However, their huge sizes cause significant inconvenience for transmission and storage. Many previous studies have explored model size compression. However, these stud-ies often approach various lossy and lossless compression methods in isolation, leading to challenges in achieving high compression ratios efficiently. This work proposes a post-training model size compression method that combines lossy and lossless compression in a unified way. We first propose a unified parametric weight transformation, which ensures different lossy compression methods can be per-formed jointly in a post-training manner. Then, a dedicated differentiable counter is introduced to guide the optimiza-tion of lossy compression to arrive at a more suitable point for later lossless compression. Additionally, our method can easily control a desired global compression ratio and allocate adaptive ratios for different layers. Finally, our method can achieve a stable 10× compression ratio with-out sacrificing accuracy and a 20× compression ratio with minor accuracy loss in a short time. Our code is available at https://github.com/ModelTC/L2 Compression. 1.

Introduction
In recent years, deep neural networks (DNNs), espe-cially convolutional neural networks (CNNs) [1, 2, 3, 4], have achieved attractive performance in various computer vision tasks such as image classification, detection, and seg-mentation. However, as their performance improves, their parameter counts also significantly increase, which is very storage-consuming. Therefore, despite their excellent per-formance, it is difficult to deploy models with a large num-ber of parameters, particularly on mobile or edge devices with limited storage resources.
Model compression [5, 6, 7, 8, 9, 10] is a common solu-tion to reduce the model size, including lossless and lossy
*Corresponding Author
Figure 1. Comparison between previous compression methods and our unified post-training compression. compression. Common lossless compression methods such as Huffman coding and Range coding are both entropy cod-ing methods. They can leverage redundant information in data to achieve distortion-free compression. However, for data with weak spatiotemporal coherence, high compres-sion ratios are often difficult to achieve. Compared with lossless methods, lossy methods such as pruning [11] and quantization [12, 13] have attracted more attention recently.
Pruning reduces the model size by removing extraneous weights, and quantization replaces weights in a low-bit for-mat. Both pruning and quantization are trade-offs between model distortion and compression ratio. Previous studies have primarily focused on individual compression methods or have merely combined different compression techniques without considering the interaction between them, resulting in multiple isolated trade-offs in successive stages. Hence, they can hardly achieve a higher compression ratio with a small amount of data and little training time.
To address this issue, as shown in Figure 1, this paper proposes to mix different compression methods and opti-mize them under the recent popular post-training setting, which slightly adjusts weights for better performance. We build an optimization objective that introduces an entropy regularization term, making the global compression ratio controllable. Based on it, a unified parametric weight trans-formation is first designed to integrate different lossy com-pression techniques together, allowing us to jointly explore various compression strategies and determine unique ones for each layer. Second, we devise a novel differentiable counter to make our entropy regularization term differen-tiable by leveraging kernel functions. This differentiable way imposes constraints on the distribution of compressed weights during optimization, contributing to more compati-ble optimized weights with later lossless compression. Con-sequently, our method combining lossy and lossless com-pression can achieve a consistently superior compression ratio with satisfying accuracy performance in an efficient manner.
To the best of our knowledge, this is the first work that presents a unified modeling approach for various lossy com-pression methods while leveraging the characteristics of lossless compression to optimize the process of lossy com-pression. Extensive experiments on various networks verify the efficacy of our method (e.g., stable 10× compression ra-tio without sacrificing accuracy and up to 20× compression ratio with minor accuracy loss).
Our main contributions can be summarized as follows:
• We propose a pioneering post-training model size compression method that combines lossy and lossless compression with a new optimization objective.
• We design a unified parametric weight transformation approach for lossy compression methods, integrating techniques such as pruning and quantization into a sin-gle stage and determining each layer’s unique com-pression scheme.
• We introduce a dedicated differentiable counter to esti-mate the entropy of compressed weights. This counter ensures that the distribution of the optimized weights is more amenable for lossless compression.
• Extensive experiments conducted on various archi-tectures, including classification and object detection tasks, demonstrate that our method achieves high com-pression ratios with negligible accuracy drops. 2.