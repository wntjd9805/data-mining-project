Abstract
Learning discriminative task-specific features simulta-neously for multiple distinct tasks is a fundamental prob-lem in multi-task learning. Recent state-of-the-art mod-els consider directly decoding task-specific features from one shared task-generic feature (e.g., feature from a back-bone layer), and utilize carefully designed decoders to pro-duce multi-task features. However, as the input feature is fully shared and each task decoder also shares decod-ing parameters for different input samples, it leads to a static feature decoding process, producing less discrimi-native task-specific representations. To tackle this limita-tion, we propose TaskExpert, a novel multi-task mixture-of-experts model that enables learning multiple representa-tive task-generic feature spaces and decoding task-specific features in a dynamic manner. Specifically, TaskExpert in-troduces a set of expert networks to decompose the back-bone feature into several representative task-generic fea-tures. Then, the task-specific features are decoded by us-ing dynamic task-specific gating networks operating on the decomposed task-generic features. Furthermore, to estab-lish long-range modeling of the task-specific representa-tions from different layers of TaskExpert, we design a multi-task feature memory that updates at each layer and acts as an additional feature expert for dynamic task-specific fea-ture decoding. Extensive experiments demonstrate that our
TaskExpert clearly outperforms previous best-performing methods on all 9 metrics of two competitive multi-task learning benchmarks for visual scene understanding (i.e.,
PASCAL-Context and NYUD-v2). Codes and models will be made publicly available. 1.

Introduction
With the rapid development of deep learning models, many computer vision tasks can be effectively handled by deep networks, which strongly motivates researchers to de-Figure 1: Motivative illustration of TaskExpert for dynami-cal decoding discriminative task-specific representations. A set of K expert networks decompose the task-generic back-bone feature into K representative feature spaces. Then, the feature activations of an input from the multiple expert networks are dynamically assembled by the Task Gating network, to produce task-specific features for the T tasks.
TaskExpert further performs long-range modeling of task-specific representations by designing a multi-task feature memory.
It is updated at the current layer based on the assembled task-specific representations, and is used as an additional feature expert in the next layer. velop unified multi-task networks to perform joint learn-ing and inference on multiple distinct tasks [1, 2, 3, 4].
Multi-Task Learning (MTL) can avoid repetitive training of single-task models and is able to simultaneously generate predictions for different tasks with only one-time inference.
On the other hand, as different computer vision tasks usu-ally share a basic common understanding of input images, learning multiple tasks concurrently can help improve the representation power of each task and boost the overall per-formance of multi-task models [5, 6].
Learning discriminative task-specific features is a funda-mental problem for MTL models. To achieve this objective, recent state-of-the-art works [7, 2, 8, 1] consider a decoder-focused paradigm, in which a shared backbone model with pretrained parameters is applied to learn generic image rep-resentations, and then separate task decoders are carefully designed to generate task-specific features. Although this paradigm is straightforward to achieve task-specific feature decoding, and produces promising multi-task prediction re-sults, these models have two aspects of potential limitations: (i) The input task-generic feature from a backbone layer is fully shared for the different task decoders, which requires a careful design of the decoders (e.g. capacity and structure) for the different tasks to achieve task-discriminative decod-ing. (ii) The parameters of each task decoder are typically shared for different input samples, resulting in a static de-coding of task-specific features, while a sample-dependant decoder is highly beneficial for learning more discrimina-tive task features, as the input samples can be diverse, and sample-related context information is particularly impor-tant for multi-task learning and inference. All these aspects hamper the previous models from learning effective task-specific features and predictions.
To tackle the above-mentioned issues, this paper pro-poses a novel multi-task mixture-of-experts framework, coined as “TaskExpert”, which dynamically performs the task-feature decoding for different inputs and tasks as de-picted in Figure 1.
Instead of using one shared back-bone feature to decode different task features, the proposed
TaskExpert introduces a set of task-generic expert networks that learn to decompose the backbone feature into a set of representative task-generic features. The feature decompo-sition enables the decoders to interact with finer-granularity task-generic feature spaces to produce more discriminative task features, as each expert can be responsible for model-ing one representative feature space from the training data.
Then, we dynamically assemble feature activations from different experts to produce task-specific representations, based on sample-dependent and task-specific gating scores predicted from a designed gating network. On the other hand, visual understanding tasks greatly benefit from repre-sentations of different network levels [9]. Thus, we further devise a “multi-task feature memory” to aggregate the dy-namically decoded task-specific features produced at differ-ent network layers, and propose a “Memorial Mixture-of-Experts (MMoE)” module. For each task, MMoE utilizes its corresponding task feature from the multi-task feature memory as an additional feature expert, to improve task-specific feature decoding. In this way, MMoE enables long-range dynamic modeling of task-specific representation for each task throughout the entire backbone.
In summary, the main contribution is three-fold:
• We propose a multi-task mixture-of-experts model that allows effective decomposition of a task-generic fea-ture from any backbone network layer and enables dy-namically assembling discriminative multi-task repre-sentations by a task gating mechanism.
• We a design novel Memorial Mixture-of-introducing
Experts (MMoE) module by further a multi-task feature memory, which can effectively interact (read and write) with the mixture of experts at different network layers to achieve long-range dy-namic decoding of task-specific features by utilizing different levels of network representations.
• The proposed TaskExpert is extensively validated on two challenging multi-task visual scene understanding benchmarks (i.e. Pascal-Context and NYUD-v2). The results clearly show the effectiveness of TaskExpert for dynamic multi-task representation learning, and also establish new state-of-the-art performances on all 9 metrics on the benchmarks, regarding the compari-son with previous best-performing methods using both transformer- and CNN-based backbones. 2.