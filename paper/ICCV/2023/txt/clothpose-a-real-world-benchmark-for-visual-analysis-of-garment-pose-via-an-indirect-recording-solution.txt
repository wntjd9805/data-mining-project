Abstract
Garments are important and pervasive in daily life.
However, visual analysis on them for pose estimation is challenging because it requires recovering the complete configurations of garments, which is difficult, if not impos-sible, to annotate in the real world. In this work, we pro-pose a recording system, GarmentTwin, which can track garment poses in dynamic settings such as manipulation.
GarmentTwin first collects garment models and RGB-D ma-nipulation videos from the real world and then replays the manipulation process using physics-based animation. This way, we can obtain deformed garments with poses coarsely aligned with real-world observations. Finally, we adopt an optimization-based approach to fit the pose with real-world observations. We verify the fitting results quantita-tively and qualitatively. With GarmentTwin, we construct a large-scale dataset named ClothPose, which consists of 30K RGB-D frames from 2400 video clips on 600 garments of 10 categories. We benchmark two tasks on the proposed
ClothPose: non-rigid reconstruction and pose estimation.
The experiments show that previous baseline methods strug-gle with highly large non-rigid deformation of manipulated garments. Therefore, we hope that the recording system and the dataset can facilitate research on pose estimation tasks on non-rigid objects. Datasets, models, and codes will be made publicly available. 1.

Introduction
We manipulate garments every day: we fold our T-shirts, flatten our suits, and perform other tasks that involve inter-acting with garments. A vision system to reconstruct the complete configuration of the garment can be beneficial for
* indicates equal contributions.
§ Cewu Lu is the corresponding author, the member of Qing Yuan
Research Institute and MoE Key Lab of Artificial Intelligence, AI Institute,
Shanghai Jiao Tong University, China and Shanghai Qi Zhi institute. downstream tasks such as object understanding, VR/AR, and robotic manipulation. The problem of reconstructing the garment configuration is defined as garment pose esti-mation in GarmentNets [9]. However, problems arise when dealing with real-world data. Given that a garment has a near-infinite degree of freedom, it is extremely hard to record its pose in the real world. Real-world garments can easily undergo large non-rigid deformation, making them hard to annotate. As a result, previous works are limited to annotate key points [6, 46] or feature lines [4] when using real-world data. Such simplifications hinder the develop-ment of research on visual garment understanding. A ques-tion still haunts the community: Is it really impossible to measure the garment pose in the real world?
Taking the lessons from capturing poses of rigid or artic-ulated objects in the real world [18, 33], passive sensors such as QR-like fiducials or reflective markers are com-monly used. These markers are particularly effective for rigid object parts, as they share the same pose transforma-tion for all the vertices. With such rigid constraints, the pose of markers in occluded regions can be inferred from visible regions in observation. However, for deformable ob-jects such as garments, these rigid constraints do not apply, making it challenging to infer the pose from visible regions.
Thus, the passive sensors cannot be applied to garments.
As for active sensors, their accuracy is often insufficient to meet the requirement of having an error range smaller than the thickness of a garment. A typical garment has a thickness of 0.05 ∼ 1cm. Therefore, an active measuring sensor should have an error of at least 1cm to prevent one side of the garment from penetrating the other. However, even magnetic sensors, one of the most accurate localiza-tion techniques available, only have an accuracy of 2.6cm within a 30cm × 30cm × 30cm region [7]. For a more in-depth discussion of active sensors, please refer to Sec. 2.
As the sensor technology seems to need a long time to catch up with the necessary accuracy, we turn to an indi-rect method to measure garment poses. Instead of directly measuring the garment pose from a static state, our method 1
measures it based on its dynamic movements. We draw in-spiration from continuum mechanics that the deformation is influenced only by external forces applied on the object from a given configuration. Therefore, knowing the initial pose of garments and the operation to deform it allows us to determine the pose during the operation. In this way, we can estimate the garment pose even when it is in large de-formation, as long as it is tracked and starts from a simple configuration. Based on the observation, we propose a new recording setup for garments, named GarmentTwin.
The workflow of GarmentTwin consists of five major procedures: (1) Model collection: We prepare 3D mod-(2) Manip-els of the real-world garments by scanning. ulation recording: In the real world, volunteers manipu-late the garments from a given initial pose, and the point cloud sequences of garment movements and the hand poses are recorded with a multi-view RGB-D camera setup. (3)
Coarse alignment: We replay the manipulation process with scanned garment models and recorded hand poses in a simulator [15] with physics-based animation.
In this way, the garment poses can be coarsely aligned with real-world observations. (4) Fine pose fitting: Finally, we use an optimization-based approach to fit the coarsely aligned shape onto the corresponding point cloud sequence from the real world. (5) Verification: After the process, we verify the annotation by checking the accuracy at predefined key points, which is called Grid Layout, and visually inspect whether the fitted pose is satisfactory. The GarmentTwin pipeline can effectively fit the garment pose with good ac-curacy (∼ 0.2cm) and without penetration.
With GarmentTwin, we can collect real-world garment pose flow during manipulation, which enables us to con-struct a large-scale real-world garment pose dataset, Cloth-Pose. ClothPose is built upon a real-world garment reposi-tory, Garment3D, which includes 600 garments of 10 cat-egories. We ask volunteers to manipulate the garments with predefined operations, i.e. folding and randomization. As a result, we have 30K point cloud sequences with annotated poses for every frame.
Although we annotate the complete pose and are primar-ily interested in non-rigid pose estimation tasks (NRPE), the annotation can also support incomplete tasks such as non-rigid reconstruction (NRR). Therefore, we benchmark
NRR with different baseline approaches to attract a broader audience. Specifically, we adopt DynamicFusion [34] and
DeepDeform [6] for NRR task, and GarmentNets [9] for
NRPE task.
In summary, our contributions are as follows:
• To the best of our knowledge, we are the first to ac-complish garment pose recording with complex pose configurations in natural manipulation tasks in the real world. To achieve this, we propose a novel recording pipeline GarmentTwin.
• With GarmentTwin, we propose a large-scale real-world dataset, ClothPose, which includes an asset dataset, Garment3D, with 600 garments of 10 cate-gories, and a task dataset with RGB-D sequences of garments in manipulation. This dataset allows re-searchers to conduct garment pose research directly in the real world for the first time.
• We benchmark two relevant non-rigid tasks namely reconstruction and pose estimation on our ClothPose dataset with different baselines. We are especially in-terested in facilitating the research in non-rigid pose estimation tasks, which are less explored as no proper benchmark exists before. 2.