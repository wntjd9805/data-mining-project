Abstract
We propose a novel progressive parameter-sharing strat-egy (MPPS) in this paper for effectively training multitask learning models on diverse computer vision tasks simulta-neously. Specifically, we propose to parameterize distribu-tions for different tasks to control the sharings, based on the concept of Exclusive Capacity that we introduce. A schedul-ing mechanism following the concept of curriculum learn-ing is also designed to progressively change the sharing strategy to increase the level of sharing during the learning process. We further propose a novel loss function to reg-ularize the optimization of network parameters as well as the sharing probabilities of each neuron for each task. Our approach can be combined with many state-of-the-art mul-titask learning solutions to achieve better joint task perfor-mance. Comprehensive experiments show that it has com-petitive performance on three challenging datasets (Multi-CIFAR100, NYUv2, and Cityscapes) using various convolu-tion neural network architectures. 1.

Introduction
Performing multiple tasks at the same time is a fun-damental ability for many intelligent agents. While the remarkable success of deep neural networks (DNNs) has been achieved in various computer vision applications such as image classification [10], semantic segmentation [38], depth estimation [31], surface normal estimation [69] and image generation [15, 21], learning and performing similar but distinctive tasks simultaneously are still a challenge for them. To address this issue, researchers proposed the Multi-Task Learning (MTL) paradigm, which usually trains one model to act as multiple distinct models. Each task in this scheme can benefit from reusing knowledge learned from
†This work was done when the author worked as a research assistant at
Nanyang Technological University, Singapore.
Figure 1. An illustrative diagram of proposed dynamic optimiza-tion in MPPS for MTL. others to improve its performance and reduce the model’s learning time. Generally, the DNNs architecture for MTL is composed of shared parameters and task-specific param-eters, which can strike a balance between shared and exclu-sive knowledge among different tasks [4].
There are two problems that need to be addressed in de-signing effective MTL algorithms. The first one is the con-struction of the parameter-sharing scheme. The most com-mon approach is hard-parameter sharing, which builds a shared feature extractor to map input data from all tasks into dense features in a common hidden representation space and then uses these features to generate different outputs via task-specific functions. Past works have demonstrated the effectiveness of this strategy in considerably reducing the model size and enhancing its overall performance across di-verse settings [51]. However, it still suffers from two issues. 1) some task combinations may result in a notable perfor-mance reduction, which is known as the negative transfer
[50, 61]; 2) an optimal design of the MTL models based on hard-parameter sharing still requires a high level of hu-man expertise and rich domain-specific knowledge. To mit-igate these issues, various solutions have been introduced to complement the shared architecture [40, 37, 49]. Given the increasing complexity of DNNs (in terms of model size, design choices, and available search spaces), how to find the optimal scheme for MTL resource sharing and resource allocation is still an open problem.
The second problem in MTL is how to control the opti-mization process to achieve the best joint task performance.
Recent works focus on balancing the task training speed via re-weighting the loss coefficients [7, 25, 36, 67], avoiding the conflict between gradients [78, 35, 8], and finding better local minima on the optimal Pareto front [65, 54]. These al-gorithms often clearly utilize the intuitive understanding of the training process to dynamically balance various tasks.
However, some previous works [30, 73] showed that partic-ular fixed, precisely searched loss weights can achieve the same or even better performance. This motivates us to re-think the relationship between the hard parameter-sharing design and the training dynamics optimization methods ag-nostic to DNNs architectures.
In this paper, we propose Multitask Learning with
Progressive Parameter Sharing (MPPS), a novel progressive parameter-sharing strategy that incorporates the design of a parameter-sharing scheme with the optimization of train-ing dynamics, aiming to achieve adaptive knowledge shar-ing among different tasks at different training stages. Fine-grained network architecture control is essential in this con-text to ensure an adequate neural network capacity for each task and support the complex mapping for all tasks with various relationships. To achieve that, we propose a dy-namic resource allocation strategy at the neuron level by parameterizing the task-specific sharing probability of each neuron of the neural network, conditioning different tasks to be learned simultaneously. This design allows distinct distributions to be learned across different tasks across all neurons, potentially significantly enhancing the flexibility of deep MTL model parameter sharing to accommodate a wide range of task combinations.
However, the optimization is challenging due to the in-creased search space.
Inspired by [1], we design a loss function to regularize the learning following an exclusive capacity scheduler, which progressively changes the shar-ing strategy from the highest exclusive capacity to a fully shared one during the entire training process. Figure 1 il-lustrates an example of our dynamic optimization. This de-sign is inspired by the development of biological brains and dynamic functional brain connectivity. The structural mod-ular segregation increases from 0 to 6 ages and the integra-tion increases after then [71]. For the functional brain net-work, the integration increases along with higher cognitive workloads [45, 2, 56]. The biological analog and the MTL in practice also suggest that optimizing multiple single-task models may be easier than a multi-task model with shared parameters. This motivates us to design an exclusive capac-ity scheduler with the curriculum learning concept.
We perform extensive experiments over popular multi-task benchmarks to validate the superiority of MPPS. Eval-uations show that our approach can bring substantial task performance improvement. We also perform detailed abla-tion studies to disclose the effectiveness and efficiency of our dynamic capacity controlling and curriculum scheduler.
We summarize our contributions as follows:
• We propose a novel MTL dynamic resource allocation strategy at the neuron level by parameterizing the task-specific sharing probability conditioning on tasks.
• We propose the “Exclusive Capacity” concept and de-sign an Exclusive Capacity scheduler from the curricu-lum learning intuition.
• Our approach shows competitive results on image clas-sification and dense prediction tasks. 2.