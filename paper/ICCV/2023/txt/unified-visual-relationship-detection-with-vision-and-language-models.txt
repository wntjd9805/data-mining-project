Abstract
This work focuses on training a single visual relation-ship detector predicting over the union of label spaces from multiple datasets. Merging labels spanning differ-ent datasets could be challenging due to inconsistent tax-onomies. The issue is exacerbated in visual relationship de-tection when second-order visual semantics are introduced between pairs of objects. To address this challenge, we propose UniVRD, a novel bottom-up method for Uniﬁed
Visual Relationship Detection by leveraging vision and lan-guage models (VLMs). VLMs provide well-aligned image and text embeddings, where similar relationships are op-timized to be close to each other for semantic uniﬁcation.
Our bottom-up design enables the model to enjoy the ben-eﬁt of training with both object detection and visual rela-tionship datasets. Empirical results on both human-object interaction detection and scene-graph generation demon-strate the competitive performance of our model. UniVRD achieves 38.07 mAP on HICO-DET, outperforming the cur-rent best bottom-up HOI detector by 14.26 mAP. More im-portantly, we show that our uniﬁed detector performs as well as dataset-speciﬁc models in mAP, and achieves fur-ther improvements when we scale up the model. Our code will be made publicly available on GitHub1. 1.

Introduction
Visual relationship detection (VRD) is a fundamental problem in computer vision, where visual relationships are typically deﬁned over pairs of localized objects, connected with a predicate. Despite the availability of a diverse set of data with rich pair-wise object annotations [6, 21, 25, 32], existing VRD models, however, are typically focusing on training from a single data source. The resultant models are therefore restricted in both image domains and text vo-cabularies, limiting their generalization and scalability. Can 1https://github.com/google-research/scenic/ tree/main/scenic/projects/univrd
Figure 1. Different VRD datasets provide different sets of unary object classes and binary relationships. We train a single visual relationship detector to unify their label spaces that generalizes across datasets. For each visual relationship, we highlight its sub-ject in red, predicate in green, and object in blue. we train a single visual relationship detector that uniﬁes di-verse datasets with heterogeneous label spaces?
Consider Figure 1, labels for objects and relations across datasets are non-disjoint and therefore could be synony-mous (e.g., ‘read’ in HICO-DET [6] vs. ‘look at’ in V-COCO [21]), subsidiary (e.g., ‘person’ in COCO [43] vs.
‘woman’ / ‘man’ in Visual Genome [32]), or overlapping (e.g., ‘wine glass’ in Objects365 [59] vs. ‘glass’ in Visual
Genome). Furthermore, VRD models need to infer rela-tionships (i.e., predicates) of second-order visual semantics between objects. The combinatorial complexity elevates the challenge to a new level. Depends on context, the same ob-ject or predicate might appear in different tenses or forms (e.g., ‘man’ vs. ‘men’ or ‘wears’ vs. ‘wearing’ in Visual
Genome) and their meanings may vary (e.g., ‘eating a sand-wich’ vs. ‘eating (with) a fork’ in V-COCO). Therefore, manually curating a merged label space spanning different datasets for training a uniﬁed VRD model is difﬁcult.
On the other hand, recent breakthroughs in vision and language models (VLMs) that are jointly trained on web-scale image-text pairs (e.g., CLIP [54] and ALIGN [27]) provide an alternative direction to approach our challenge.
Intuitively, beneﬁting from the large language encoders [14, 55] and contrastive image-text co-training, a pre-trained
VLM should be able to encode “similar visual relation-ships” close to each other in the embedding space. These relationships contain similar action, subject, and object la-bels in semantics, e.g., “a person watching a television” vs.
“a man looking at a TV”. They are commonly measured by distances between semantic words or language embed-dings [48], which motivates us to use large language models for uniﬁcation. Speciﬁcally, the learnt text embeddings of
VLMs can be used to reconcile heterogeneous label spaces across VRD datasets of similar visual relationships, while their jointly trained image encoders ensure the alignment with the visual content.
In light of this, we propose UniVRD (Uniﬁed Visual
Relationship Detection), a bottom-up framework consisting of an object detector and pair-wise relationship decoder in a cascaded manner. To ﬁne-tune VLMs for object detec-tion, we adopt an encoder-only architecture [50] and attach a minimal set of heads to each Transformer output token so that the learnt knowledge from the image-level pre-training can be preserved. A lightweight Transformer decoder [2] is then appended to the object detector for decoding pair-wise relationships from the predicted objects by formulating the optimization target as a set prediction problem [5]. Fur-ther, we use natural languages in place of categorical inte-gers to deﬁne and unify the label space. Our bottom-up de-sign and language-deﬁned label space enable the model to enjoy various existing object detection and visual relation-ship detection datasets for training, yielding strong scalabil-ity and substantial performance improvements over existing bottom-up detection approaches.
We evaluate our approach on two VRD tasks: human-object interaction (HOI) detection and scene graph genera-tion (SGG). Crucially, we demonstrate competitive perfor-mances on both tasks — our model achieves the state of the art on HICO-DET (38.07 mAP), a substantial improvement of 14.26 mAP over the current best-performing bottom-up
HOI detector. For the ﬁrst time, we show that a uniﬁed model can perform as well as dataset-speciﬁc ones, and obtain notable improvements in mAP on long-tailed VRD datasets when the model is scaled up.
In summary, this paper makes the following main con-tributions: (1) a novel VRD framework that uniﬁes multi-ple datasets which cannot be done by previous work without
VLMs; (2) an effective and strong model training recipe, including improvements on models, losses, augmentations, training paradigms, etc.; (3) state-of-the-art results showing strong scalability and generalization of our model. Our de-sign is simple, interoperable, and can easily leverage new advances in VLMs. We believe our work is ﬁrst-of-its-kind that brings new insights to the community and as a ﬂexible starting point for future research on tasks requiring visual relationship understanding. 2.