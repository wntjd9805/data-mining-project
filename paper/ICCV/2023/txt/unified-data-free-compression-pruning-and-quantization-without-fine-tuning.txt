Abstract
Structured pruning and quantization are promising ap-proaches for reducing the inference time and memory foot-print of neural networks. However, most existing methods require the original training dataset to fine-tune the model.
This not only brings heavy resource consumption but also is not possible for applications with sensitive or proprietary data due to privacy and security concerns. Therefore, a few data-free methods are proposed to address this problem, but they perform data-free pruning and quantization separately, which does not explore the complementarity of pruning and quantization. In this paper, we propose a novel framework named Unified Data-Free Compression(UDFC), which per-forms pruning and quantization simultaneously without any data and fine-tuning process. Specifically, UDFC starts with the assumption that the partial information of a dam-aged(e.g., pruned or quantized) channel can be preserved by a linear combination of other channels, and then de-rives the reconstruction form from the assumption to restore the information loss due to compression. Finally, we for-mulate the reconstruction error between the original net-work and its compressed network, and theoretically deduce the closed-form solution. We evaluate the UDFC on the large-scale image classification task and obtain significant improvements over various network architectures and com-pression methods. For example, we achieve a 20.54% accu-racy improvement on ImageNet dataset compared to SOTA method with 30% pruning ratio and 6-bit quantization on
ResNet-34. Code will be available at here. 1.

Introduction
Model compression is the most common way to reduce the memory footprint and computational costs of the model, and it mainly includes two methods: pruning[22, 28, 4] and quantization[8, 17, 43, 5, 3]. Among the pruning domain, structured pruning[41, 36] is more actively studied than unstructured pruning[20, 34] since it eliminates the whole
*Equal contribution.
†Corresponding author. channel or even the layer of the model while not requiring any special hardware or libraries for acceleration. Under such conditions, we also focus our attention on structured pruning in this paper. Quantization methods attempt to re-duce the precision of the parameters and/or activations from 32-bit floating point to low-precision representations. Thus the storage requirement for the model can be diminished substantially, as well as the power consumption.
Although the existing compression methods achieve a satisfactory compression ratio with a reasonable final per-formance, most of them require the original training data for a tedious fine-tuning process. The fine-tuning process is not only data-dependent but also computationally expen-sive, while users may not have access to sufficient or com-plete data in many real-world applications, such as medi-cal data and user data. Therefore, data-free compression methods are proposed, which don’t require any real data and fine-tuning process. For instance, Data-free parameter pruning[38] first introduces the data-independent technique to remove the redundant neurons, and Neuron Merging[18] extends the data-free method from fully connected lay-ers to convolutional layers. Meanwhile, there exist some methods using the synthetic samples to perform the fine-tuning process, such as Dream[44].
In the field of quan-tization, recent works propose post-training quantization methods[32, 2, 45, 24, 6, 46] that use the synthetic data to replace the real data for quantization and achieve the SOTA results. For instance, ZeroQ [2] uses the distilled data that matches the statistics of batch normalization layers to per-form post-training quantization. DSG[45] proposes a novel
Diverse Sample Generation scheme to enhance the diversity of synthetic samples, resulting in better performance.
However, some problems still hinder the deployment of data-free compression. On the one hand, the latest data-free quantization approaches focus on improving the quality of synthetic samples rather than releasing the quantization from its dependence on data. In this case, generating the synthetic samples introduces extra computational costs. On the other hand, current approaches perform data-free prun-ing and quantization separately, which does not explore the complementarity of weight pruning and quantization.
Figure 1. The general overview of UDFC, which performs the pruning and quantization simultaneously. S is the scale factor and S ̸= S′
. See Section 3 for details of S. After the output channels of l-th layer are pruned or quantized, our goal is to maintain the feature map
Z (l+1) of (l + 1)-th layer. We first deduce the reconstruction form based on our assumption and then reconstruct the input channels of (l + 1)-th layer to restore the information loss caused by compression of l-th layer. Finally, we formulate the reconstruction error between the feature map Z (l+1) and ˆZ (l+1)/ ˜Z (l+1).
In this paper, we propose a novel joint compression framework named Unified Data-Free Compression(UDFC), which overcomes abovementioned issues without any origi-nal/synthetic data and fine-tuning process, as shown in Fig-ure 1. Our contributions can be summarized as follows:
• We propose the assumption that the partial information of a damaged(e.g., pruned or quantized) channel can be preserved by a linear combination of other channels.
Based on this assumption, we derive that the informa-tion loss caused by pruning and quantization of the l-th layer can be restored by reconstructing the channels of the (l+1)-th layer. The assumption and reconstruction form are described in Section 3.2.
• Based on the reconstruction form, we formulate the re-construction error between the original network and its compressed network. The reconstruction error is de-scribed in Section 3.3.
• Based on the reconstruction error, we prove that re-construction error can be minimized and theoreti-cally deduce the closed form solution in Section 4.
Furthermore, extensive experiments on CIFAR-10[19] and ImageNet[35] with various popular architectures demonstrate the effectiveness and generality of UDFC.
For example, UDFC on VGG-16 yields around 70%
FLOPS reduction and 28× memory footprint reduc-tion, with only a 0.4% drop in accuracy compared to the uncompressed baseline on CIFAR-10 dataset. 2.