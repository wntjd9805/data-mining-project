Abstract
Distilling knowledge from convolutional neural net-works (CNNs) is a double-edged sword for vision trans-formers (ViTs). It boosts the performance since the image-friendly local-inductive bias of CNN helps ViT learn faster and better, but leading to two problems: (1) Network de-signs of CNN and ViT are completely different, which leads to different semantic levels of intermediate features, mak-ing spatial-wise knowledge transfer methods (e.g., feature mimicking) inefﬁcient. (2) Distilling knowledge from CNN limits the network convergence in the later training period since ViT’s capability of integrating global information is suppressed by CNN’s local-inductive-bias supervision.
To this end, we present Cumulative Spatial Knowledge
Distillation (CSKD). CSKD distills spatial-wise knowledge to all patch tokens of ViT from the corresponding spatial re-sponses of CNN, without introducing intermediate features.
Furthermore, CSKD exploits a Cumulative Knowledge Fu-sion (CKF) module, which introduces the global response of CNN and increasingly emphasizes its importance dur-ing the training. Applying CKF leverages CNN’s local in-ductive bias in the early training period and gives full play to ViT’s global capability in the later one. Extensive ex-periments and analysis on ImageNet-1k and downstream datasets demonstrate the superiority of our CSKD. Code: https://github.com/Zzzzz1/CSKD 1.

Introduction
Recently, Vision Transformers (ViT) attract lots of atten-tion in the computer vision community. Due to the powerful network capacity brought by the self-attention module, ViT achieves great performance on many vision tasks (e.g., im-age recognition[7, 22] and object detection[20, 3]). Knowl-edge distillation (KD) is a widely-used technique, which improves the performance of a lightweight model (the stu-dent) by transferring knowledge from a heavy one (the teacher). DeiT[34] ﬁrst applies this technique to ViT, which proposes to distill knowledge from the convolutional neural networks (CNNs) for better training efﬁciency. The net-work design of CNN is image-friendly since the convolu-tion operation is translation invariant, i.e., CNN has a local inductive bias which beneﬁts the vision tasks. Thus, ap-plying DeiT could help ViT converge faster and better for vision tasks. However, distilling knowledge from CNN is a double-edged sword. It introduces two problems, which are explained as follows.
Firstly, the network designs of CNN and ViT are differ-ent in three aspects: (1) The ways to extend the receptive
ﬁeld, (2) the ways to stack blocks, and (3) the types of nor-malization layers.
It makes intermediate features hard to align in terms of semantics. Thus, feature-based distillation methods could be inefﬁcient due to the misalignment. The inefﬁciency is worthy of attention because the features can provide spatial-wise supervision with abundant knowledge.
Secondly, CNN’s local inductive bias could hinder the full power of long-range dependencies and positional en-coding of ViTs at convergence, although it helps ViT con-verge faster in the earlier stage (which is also observed in
[4]). This phenomenon can be explained as follows: ViT’s network capability is stronger than CNN’s due to its power-ful self-attention module. It could predict some hard train-ing samples correctly that CNN can not handle. However, distilling knowledge from CNN’s predictions provides neg-ative supervision, leading ViT to predict wrong labels. With the progress of training, this problem is going to get worse.
Previous works make efforts to solve the problems above. (1) To alleviate the ﬁrst one, DearKD[4] presents the MHCA module which modiﬁes ViT’s network architec-ture to align the intermediate features better. Despite the improved performance, extra module design is needed and
ViT’s capability of modeling the global relation is weak-ened. (2) As for the second one, DearKD [4] presents a two-stage learning strategy, and the distillation loss is disabled in the second stage to bypass the problem.
In CviT[27], multiple networks with various inductive biases are used as teachers to provide more abundant knowledge, so that
ViT’s capability could be less suppressed. Both works boost the distillation performance. However, the former degrades into a baseline training scheme in the later period because it doesn’t apply knowledge distillation, and the latter makes
CNN Teacher
,12341 fc pool + fc
,-(./0(
ℒ$%&'%(( distill token patch tokens
ViT Student
,A4B3C
… r e d o c n
E
… r e d o c n
E reshape
… class token
ℒ"#
ℒ = 1 2
ℒ"# + 1 2
ℒ$%&'%(( + ℒ")*+ (a) CSKD framework
Cumulative 
Knowledge 
Fusion
ℒ")*+
Cumulative Knowledge Fusion
,12341 9
,<12=41
: − 9 9 argmax 1 0 0 0.5 training epoch 1 hard label
,>0'?@
Cross Entropy
Loss
ℒ")*+ (b) Cumulative Knowledge Fusion 
Figure 1. Illustration of our Cumulative Spatial Knowledge Distillation (CSKD). the training process more computationally intensive.
In this paper, we present Cumulative Spatial Knowledge
Distillation (CSKD) to alleviate the aforementioned prob-lems in a simple but effective way. (1) CSKD generates spatial classiﬁcation predictions from CNN’s last features and uses them as the supervision signals of the correspond-ing patch tokens of ViT (as shown in Figure 1 (a)). In this way, spatial-wise knowledge is transferred without intro-ducing intermediate features. Thus, the ﬁrst problem can be alleviated successfully. (2) CSKD exploits a Cumulative
Knowledge Fusion (CKF) module which fuses the local and the global responses and increasingly emphasizes the im-portance of the global ones with the progress of training. (as shown in Figure 1 (b)). In this way, CKF leverages the lo-cal inductive bias for fast convergence in the early period and gives full play to ViT’s network capability in the later one. Experimental results on ImageNet-1k and downstream datasets demonstrate the effectiveness of our CSKD. We further provide visualizations to validate that CSKD lever-ages ViT’s network capability better.
Our contributions are summarized as follows.
• Two obstacles to distilling knowledge from CNN to (1) the spatial-wise knowledge
ViT are discussed: transfer is inefﬁcient and (2) the network convergence is limited in the later training period.
• We present Cumulative Spatial Knowledge Distilla-tion (CSKD) to alleviate the two problems in a sim-ple yet effective way. CSKD transfers spatial-wise knowledge without introducing intermediate features and increasingly emphasizes the importance of global responses for better supervision.
• Extensive experiments validate the effectiveness of our
CSKD. We further provide analysis and visualizations, demonstrating that CSKD gives full play to ViT’s pow-erful network capability. 2.