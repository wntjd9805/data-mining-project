Abstract
One critical challenge in 6D object pose estimation from a single RGBD image is efficient integration of two different modalities, i.e., color and depth. In this work, we tackle this problem by a novel Deep Fusion Transformer (DFTr) block that can aggregate cross-modality features for improving pose estimation. Unlike existing fusion methods, the pro-posed DFTr can better model cross-modality semantic cor-relation by leveraging their semantic similarity, such that globally enhanced features from different modalities can be better integrated for improved information extraction.
Moreover, to further improve robustness and efficiency, we introduce a novel weighted vector-wise voting algorithm that employs a non-iterative global optimization strategy for precise 3D keypoint localization while achieving near real-time inference. Extensive experiments show the effec-tiveness and strong generalization capability of our pro-posed 3D keypoint voting algorithm. Results on four widely used benchmarks also demonstrate that our method outper-forms the state-of-the-art methods by large margins. Code is available at https://github.com/junzastar/DFTr Voting. 1.

Introduction 6D object pose estimation aims to recognize the 3D po-sition and orientation of objects in the camera coordinate system.
It is a widely studied task in both computer vi-sion and robotics for its critical importance to many real-world applications, such as robotic grasping and manipu-lation [16, 37], augmented reality [1, 52] and autonomous navigation [14, 63]. Although significant progress has been made in recent years, critical challenges remain due to many factors such as varying illuminations, sensor noise, heavy occlusion, and the highly reflective surface of objects. Re-cently, along with the dramatic growth of RGB-D sensors, methods based on RGB-D data has attracted more atten-The * indicates equal contribution.
Figure 1. An illustration of typical challenges for object pose estimation, e.g. texture-less material, surface reflection, and inter-object occlusion. tion, due to the fact that extra geometry information in the depth channel is complimentary to the color informa-tion for better alleviating difficulties in 6D pose estimation
[24, 26, 59, 63].
However, so far, how to efficiently integrate these two modalities, i.e., color and depth, for better 6D pose estima-tion remains an open question. Prior works [9, 36, 61, 63] use the depth information as an additional clue to refine the final pose or concatenate the extracted RGB and geometric features directly for pose estimation. However, such meth-ods do not make full use of the 3D geometry information and are sensitive to severe occlusions, and also ignore the global feature representation. Current methods are still lim-ited, whether a simple point-wise fusion encoder [20, 58] or a k-Nearest Neighbor (k-NN) based feature query tactic
[19], where inter-modality global semantic correlations are not considered. Such fusion strategies are more likely to be disturbed severely when the object has highly reflective sur-faces also without any texture cues, e.g., metal objects, as shown in Fig. 1, because (1) the lack of one modality data will directly cause the failure of CNN or PCN (point cloud network) feature extraction, (2) the occlusion between ob-jects results in data loss, and (3) the k-NN based feature clustering approach is sensitive to noise since the integrated features are likely absent on the query object.
Furthermore, given the fused representative RGB-D fea-tures, how to robustly estimate the object pose parameters
in challenging scenarios is another open problem. Works like [8, 42, 45, 61, 63] propose to regress the final pose parameters directly using the MLP-likes prediction mod-ule. However, such methods often need a highly cus-tomized post-procedure for pose refinement. Conversely, the correspondence-based methods [3, 5, 7, 10] estimate pose by optimizing the pre-established correspondence, so they can achieve robust performance without the post-refinement procedure. Such methods can be subdivided into dense and sparse keypoint-based correspondence. Due to less computation cost and less hypothesis verification, sparse keypoint-based approaches have been widely used
[19, 20]. However, most existing approaches, which output the point-wise translation offsets pointing to the keypoint directly without any scale constraint, is not conducive to network learning since the offset will change in scale due to the object’s size, thus severely degrading the keypoint local-ization accuracy. Besides, the keypoints voting method de-ployed in prior works, like MeanShift [15], is highly time-consuming for the iterative steps, which also limits perfor-mances in real-time applications.
In this work, we propose a Deep Fusion Transformer network for effective RGB-D fusion to estimate object 6D pose. The core of our network is to design a novel cross-modality fusion block named Deep Fusion Transformer
It implicitly aggregates distinguished features of (DFTr). two modality data by reasoning about the global seman-tic similarity between appearance and geometry informa-tion. Given two modality features from encoding or decod-ing layers of the network, our DFTr constructs a long-term dependence between them to extract cross-modality corre-lation for global semantic similarity modeling by using a transformer-based structure. We argue that the global se-mantic similarity modeling can alleviate perturbations in feature space caused by missing modality data and noises.
Subsequently, with the learned fused RGB-D features, we adopt the keypoint-based workflow [19, 20] for 6D pose es-timation, for their robustness to occlusion. Different from existing 3D keypoints voting methods, we propose to learn the 3D point-wise unit vector field and introduce an ef-fective and non-iterative weighted vector-wise voting algo-rithm for 3D keypoints localization. In this way, the offsets with length constrained are easier for the network to learn and the inference speed is greatly improved while keeping comparable even superior location accuracy.
In summary, the main contributions of this work are:
• We propose an effective cross-modality feature aggre-gation network for 6DoF object pose estimation, in which a novel Deep Fusion Transformer (DFTr) block is designed and employed on a multi-scale level for ro-bust representation learning.
• We propose an effective weighted vector-wise voting algorithm, in which a global optimization scheme is deployed for 3D keypoint localization. We replace the original clustering method with the proposed algo-rithm in PVN3D [20] and FFB6D [19] framework, our approach is 1.7x faster than PVN3D and 2.7x faster than FFB6D when keeps a comparable even superior performance on the YCB dataset.
• We conduct extensive experiments on MP6D [11],
YCB-Video [4], LineMOD [21], and Occlusion
LineMOD [2] public benchmarks.
Our method achieves dramatic performance improvements over other state-of-the-art methods without any post-refinement procedures. 2.