Abstract
Large-scale text-to-image diffusion models have signif-icantly improved the state of the art in generative image modeling and allow for an intuitive and powerful user inter-face to drive the image generation process. Expressing spa-tial constraints, e.g. to position specific objects in particular locations, is cumbersome using text; and current text-based image generation models are not able to accurately follow such instructions. In this paper we consider image gener-ation from text associated with segments on the image can-vas, which combines an intuitive natural language interface with precise spatial control over the generated content. We propose ZestGuide, a “zero-shot” segmentation guidance approach that can be plugged into pre-trained text-to-image diffusion models, and does not require any additional train-ing. It leverages implicit segmentation maps that can be ex-tracted from cross-attention layers, and uses them to align the generation with input masks. Our experimental results combine high image quality with accurate alignment of gen-erated content with input segmentations, and improve over prior work both quantitatively and qualitatively, including methods that require training on images with corresponding segmentations. Compared to Paint with Words, the previous state-of-the art in image generation with zero-shot segmen-tation conditioning, we improve by 5 to 10 mIoU points on the COCO dataset with similar FID scores. 1.

Introduction
The ability of diffusion models to generate high-quality images has garnered widespread attention from the research community as well as the general public. Text-to-image models, in particular, have demonstrated astonishing capa-bilities when trained on vast web-scale datasets [16, 33, 35,
*These authors contributed equally to this work.
Figure 1. In ZestGuide the image generation is guided by the gra-dient of a loss computed between the input segmentation and a segmentation recovered from attention in a text-to-image diffusion model. The approach does not require any additional training of the pretrained text-to-image diffusion model to solve this task. 37]. This has led to the development of numerous image editing tools that facilitate content creation and aid creative media design [17, 25, 36]. Textual description is an intuitive and powerful manner to condition image generation. With a simple text prompt, even non-expert users can accurately describe their desired image and easily obtain correspond-ing results. A single text prompt can effectively convey in-formation about the objects in the scene, their interactions, and the overall style of the image. Despite their versatility, text prompts may not be the optimal choice for achieving fine-grained spatial control. Accurately describing the pose, position, and shape of each object in a complex scene with words can be a cumbersome task. Moreover, recent works have shown the limitation of diffusion models to follow spa-tial guidance expressed in natural language [1, 7].
On the contrary, semantic image synthesis is a condi-tional image generation task that allows for detailed spatial control, by providing a semantic map to indicate the desired class label for each pixel. Both adversarial [29, 38] and diffusion-based [43, 44] approaches have been explored to generate high-quality and diverse images. However, these approaches rely heavily on large datasets with tens to hun-dreds of thousands of images annotated with pixel-precise
“A cat wearing a dress.”
“ A dog looking at the sunrise behind the fuji.”
“Astronauts on the street with rainbow in outer space ”
Figure 2. ZestGuide generates images conditioned on segmenta-tion maps with corresponding free-form textual descriptions. label maps, which are expensive to acquire and inherently limited in the number of class labels.
Addressing this issue, Balaji et al. [2] showed that se-mantic image synthesis can be achieved using a pretrained text-to-image diffusion model in a zero-shot manner. Their training-free approach modifies the attention maps in the cross-attention layers of the diffusion model, allowing both spatial control and natural language conditioning. Users can input a text prompt along with a segmentation map that indi-cates the spatial location corresponding to parts of the cap-tion. Despite their remarkable quality, the generated images tend to only roughly align with the input segmentation map.
To overcome this limitation, we propose a novel ap-proach called ZestGuide, short for ZEro-shot SegmenTation
GUIDancE, which empowers a pretrained text-to-image diffusion model to enable image generation conditioned on segmentation maps with corresponding free-form textual descriptions, see examples presented in Fig. 2. ZestGuide is designed to produce images which more accurately ad-here to the conditioning semantic map. Our zero-shot ap-proach builds upon classifier-guidance techniques that al-low for conditional generation from a pretrained uncondi-tional diffusion model [13]. These techniques utilize an ex-ternal classifier to steer the iterative denoising process of diffusion models toward the generation of an image cor-responding to the condition. While these approaches have been successfully applied to various forms of conditioning, such as class labels [13] and semantic maps [3], they still rely on pretrained recognition models. In the case of seman-tic image synthesis, this means that an image-segmentation network must be trained, which (i) violates our zero-shot objective, and (ii) allows each segment only to be condi-tioned on a single class label. To circumvent the need for an external classifier, our approach takes advantage of the spatial information embedded in the cross-attention layers of the diffusion model to achieve zero-shot image segmen-tation. Guidance is then achieved by comparing a segmen-tation extracted from the attention layers with the condition-ing map, eliminating the need for an external segmentation network. In particular, ZestGuide computes a loss between the inferred segmentation and the input segmentation, and uses the gradient of this loss to guide the noise estimation process, allowing conditioning on free-form text rather than just class labels. Our approach does not require any training or fine-tuning on top of the text-to-image model.
We conduct extensive experiments and compare our
ZestGuide to various approaches introduced in the recent literature. Our results demonstrate state-of-the-art perfor-mance, improving both quantitatively and qualitatively over prior approaches. Compared to Paint with Words, the previ-ous state-of-the art in image generation with zero-shot seg-mentation conditioning, we improve by 5 to 10 mIoU points on the COCO dataset with similar FID scores.
In summary, our contributions are the following:
• We introduce ZestGuide, a zero-shot method for im-age generation from segments with text, designed to achieve high accuracy with respect to the condition-ing map. We employ the attention maps of the cross-attention layer to perform zero-shot segmentation al-lowing classifier-guidance without the use of an exter-nal classifier.
• We obtain excellent experimental results, improving over existing both zero-shot and training-based ap-proaches both quantitatively and qualitatively. 2.