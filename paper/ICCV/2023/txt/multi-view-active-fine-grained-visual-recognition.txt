Abstract
Despite the remarkable progress of Fine-grained visual classification (FGVC) with years of history, it is still limited to recognizing 2D images. Recognizing objects in the phys-ical world (i.e., 3D environment) poses a unique challenge – discriminative information is not only present in visible local regions but also in other unseen views. Therefore, in addition to finding the distinguishable part from the cur-rent view, efficient and accurate recognition requires infer-ring the critical perspective with minimal glances. E.g., a person might recognize a “Ford sedan” with a glance at its side and then know that looking at the front can help tell which model it is.
In this paper, towards FGVC in the real physical world, we put forward the problem of multi-view active fine-grained visual recognition (MAFR) and complete this study in three steps: (i) a multi-view, fine-grained vehicle dataset is collected as the testbed, (ii) a pilot experiment is designed to validate the need and re-search value of MAFR, (iii) a policy-gradient-based frame-work along with a dynamic exiting strategy is proposed to achieve efficient recognition with active view selection.
Our comprehensive experiments demonstrate that the pro-posed method outperforms previous multi-view recognition works and can extend existing state-of-the-art FGVC meth-ods and advanced neural networks to become “FGVC ex-perts” in the 3D environment. Our code is available at https://github.com/PRIS-CV/MAFR. 1.

Introduction
In the past two decades, fine-grained visual classifica-tion (FGVC) has made significant progress in recognizing sub-categories of objects belonging to the same class. This progress has been demonstrated in various domains, such as recognizing cars [32, 57], aircraft [36], birds [50, 48], and foods [39], with extensive outstanding works surpass-ing human experts in many application scenarios [34, 56,
*indicates the corresponding author.
Figure 1. Conventional FGVC versus Multi-view Active FGVC.
Conventional FGVC takes a single static image for inference, which may omit the discriminative information. In contrast, Multi-view FGVC takes a step further by predicting the potential dis-criminative view and fusing a sequence of visual information for the final decision. 19, 53, 13, 4, 5, 16, 14]. However, the previous efforts on FGVC have remained mainly limited to a single-view-based paradigm, where only the visual content within a sin-gle static image is considered. This paradigm may be suffi-cient for coarse-grained classification where inter-class dif-ferences are easily captured, such as distinguishing a coupe from other vehicles by its streamlined body, seductive en-gine, or headlamps. However, fine-grained classification presents a different challenge where discriminative clues are rare and often found in subtle structural differences that are not easily captured by a single static view. For instance, to distinguish between different Ford sedans, one can only rely on subtle differences in the design of car headlights. Pre-dictably, for single-view-based approaches, an image/view without discriminative clues is completely indistinguishable at the fine-grained level, fundamentally limiting the model’s theoretical performance.
Factually, visual recognition is never limited to observ-ing 2D environments and processing static images. Vi-sion algorithms equipped by portal devices (e.g., smart-phone, smart glasses, etc.) or embodied AI agents [18] (e.g., intelligent robots) play the core roles during machine-environment interaction and have become one of the fo-cuses of computer vision research. Therefore, to embrace the new trend, a natural extension of ordinary FGVC fol-lows – in addition to locating discriminative parts within an image, we aim to infer the unseen distinguishable perspec-tive within the physical world (i.e., 3D environment). Fig-ure 1 shows an example where, with a single glance from the front, the algorithm may be uncertain about the model of the Ford sedan, but can infer that looking at its front will help to resolve the ambiguity.
Specifically, we re-propose the concept of active vi-sion [1] in the context of FGVC termed multi-view active fine-grained visual recognition (MAFR). MAFR is based on two essential hypotheses. Firstly, we hypothesize that dis-criminative information for different fine-grained categories is distributed across various object views, making discrim-inative perspective inference a non-trivial and worth study-ing. Secondly, we hypothesize that even views that appear indistinguishable at first glance can contain visual clues that can help infer the discriminative perspective, making the problem solvable.
To start with, due to the absence of qualified datasets, we first collect a fine-grained, multi-view vehicle dataset named Multi-view Cars (MvCars) as the testbed. MvCars comprises 233 car models from 35 brands, covering 5 dif-ferent car types. Each car in the dataset has 8 aligned views, and the dataset contains more than 26, 000 images. In Sec-tion 3.2, we conducted a simple pilot experiment and ver-ified our first hypothesis, which suggests that MvCars is well-suited for the problem at hand.
Secondly, our next contribution is an efficient multi-view fine-grained recognition framework building upon ac-tive next-view selection and dynamic exiting. In particular, following the general idea of view-based 3D object under-standing [47], an extraction-aggregation architecture is de-signed as the feature encoder, where a convolutional neu-ral network (e.g., ResNet50 [25]) is first applied to extract single-view features independently. Then a recurrent neural network (e.g., GRU [11]) is adopted to aggregate multi-view features and form global descriptions. Afterwards, we for-mulate the next-view selection as a sequential decision pro-cess, where the model is demanded to decide the next dis-criminative view (action) according to previously observed views (state). Therefore, we implement a proximal policy optimization (PPO) [46] based multi-stage training strat-egy.
In addition, considering that the difficulty of recog-nizing different samples varies and the resulting number of perspectives required differs, we design a dynamic exit in-ference strategy for better effort allocation – inspired by the idea of budgeted batch classification, a series of exciting confidence thresholds are estimated under a given effort ex-pectation. Note that the proposed framework does not rely on specific neural network architectures. It can extend any visual recognition network to an FGVC experts in the 3D environment.
Finally, several carefully designed baselines are re-produced on MvCars as benchmark results, including pre-vious multi-view recognition works, advanced neural net-works, and popular FGVC methods. The experimental re-sults demonstrate that the proposed method delivers a better performance-efficient trade-off than all competitors. After that, analysis of the upper bound and the selected trajecto-ries reveals the inherent characteristics of MAFR. In addi-tion, comprehensive ablation studies are carried out to ver-ify the necessities of each model component. 2.