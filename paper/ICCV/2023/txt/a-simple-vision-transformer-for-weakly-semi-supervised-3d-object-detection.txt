Abstract
Advanced 3D object detection methods usually rely on large-scale, elaborately labeled datasets to achieve good performance. However, labeling the bounding boxes for the 3D objects is difficult and expensive. Although semi-supervised (SS3D) and weakly-supervised 3D object detec-tion (WS3D) methods can effectively reduce the annotation cost, they suffer from two limitations: 1) their performance is far inferior to the fully-supervised counterparts; 2) they are difficult to adapt to different detectors or scenes (e.g, indoor or outdoor). In this paper, we study weakly semi-supervised 3D object detection (WSS3D) with point anno-tations, where the dataset comprises a small number of fully labeled and massive weakly labeled data with a single point annotated for each 3D object. To fully exploit the point an-notations, we employ the plain and non-hierarchical vision transformer to form a point-to-box converter, termed ViT-WSS3D. By modeling global interactions between LiDAR points and corresponding weak labels, our ViT-WSS3D can generate high-quality pseudo-bounding boxes, which are then used to train any 3D detectors without exhaustive tun-ing. Extensive experiments on indoor and outdoor datasets (SUN RGBD and KITTI) show the effectiveness of our method. In particular, when only using 10% fully labeled and the rest as point labeled data, our ViT-WSS3D can en-able most detectors to achieve similar performance with the oracle model using 100% fully labeled data. 1.

Introduction 3D object detection is one of the fundamental tasks in computer vision and has a wide range of real-world applica-tions, such as self-driving and navigation. It aims to regress the 3D bounding boxes and corresponding category labels of objects for a given scene. Due to the inherent limitation
*Equal contribution. †Corresponding author.
Work done when Dingkang Liang was an intern at Baidu.
Figure 1. (a) Different types of supervision for 3D object detection. (b) Various annotation formats. The cost of point-level annotations is significantly lower than the box-level annotation. (c) The com-parative performance of using the fully-supervised and our weakly semi-supervised settings. of LiDAR sensors, point clouds are usually disordered and sparse, making 3D object detection a challenging task.
To accurately locate the objects, elaborately annotated large-scale data is inevitable for the existing methods, while labeling 3D bounding boxes is tedious and time-consuming.
Recently, some methods [16, 35, 27, 1] have been pro-posed to reduce the expensive cost of labeling. There are two typical settings: semi-supervised 3D object detection (SS3D) [35, 47], where only a small amount of precisely annotated scenes are available; weakly-supervised 3D ob-ject detection (WS3D) [27, 37], where coarse annotations (e.g., labeling a point for an object) are used to train the 3D detector instead of precisely annotated 3D bounding boxes.
Although SS3D and WS3D methods can effectively re-duce annotation costs, they still have obvious limitations.
On the one hand, their performance is still far inferior
In specific, the to their fully-supervised counterparts.
SS3D methods [35, 47] usually transfer knowledge from labeled data to unlabeled data in a teacher-student frame-work. However, knowledge transfer may be ineffective when the domain gap between labeled and unlabeled data is vast (e.g., labeled and unlabeled data belong to sunny and rain, respectively). For the WS3D methods [27, 37], the supervision information provided by the weak annota-tion is hard to reflect characters (e.g., the geometric struc-ture) of 3D objects, leading to poor performance. On the other hand, current SS3D and WS3D methods are usu-ally designed for specific frameworks or scenes (e.g., in-door or outdoor), which are hard to transform into other frameworks or scenes. For example, a representative semi-supervised method 3DIoUMatch [35], initially designed for
PVRCNN [29], can bring a 4.6% improvement compared with the supervised counterpart under 2% full label setting.
However, we empirically find that it can not work well in
PointRCNN [31], with only 1.5% improvement achieved.
Considering these issues, training 3D object detec-tors with considerably low annotation cost by a gen-eral paradigm while achieving comparable performance with the fully-supervised counterpart is worth exploring.
To achieve this goal, a cheap yet effective annotation format is needed. Among various weak formats (e.g., point-level [37], scene-level [27]), point-level annotation is simple to annotate, convenient to store and use, and localization-aware, which provides a stronger prior of ob-ject location. According to the method in [37], a box an-notation takes 110 seconds1, while a point annotation only takes 5 seconds, as shown in Fig. 1 (b).
Nevertheless, only adopting point-level annotations is not enough. A natural way to achieve a good trade-off between detection performance and annotation costs is to combine a small number of fully annotated data, where we treat such a setting as the weakly semi-supervised paradigm.
Recently, some methods [3, 44, 8] have demonstrated the potential of weakly semi-supervised paradigm in 2D object detection. These methods help students obtain favorable re-sults and save tremendous resource consumption. Regard-ing 3D object detection, there is no doubt that replacing 3D bounding boxes with point labels in 3D point clouds is nec-essary since annotating 3D objects is more time-consuming and labor-intensive than 2D objects. Whereas, how to adopt weakly semi-supervised learning with points to 3D scenes, especially point cloud, has not been explored yet.
In this paper, we aim to explore the weakly semi-supervised 3D object detection (WSS3D) with points, as shown in Fig. 1 (a). To fully utilize the limited box-level annotations and abundant points, we propose a simple yet 1Note that some modern softwares [50] may improve labeling process-ing, but it also accelerates point labeling processing, and the cost gap be-tween full and weak labels still remains. effective WSS3D pipeline: 1) Train a point-to-box con-verter with a small number of fully-labeled data. 2) The trained converter transforms massive point annotations into pseudo-bounding boxes. 3) Finally, train any 3D object detector with fully-labeled and pseudo-labeled scenes in a fully-supervised setting.
The core of such a pipeline is to build a robust point-to-box converter. Recently, vision transformers [4, 38] have shown great potential in feature interaction. Inspired by YOLOS [7] that directly encode the image token as a sequence for object detection, we propose a simple vi-sion transformer-based converter for WSS3D, termed ViT-WSS3D. Specifically, the ViT-WSS3D adopts the plain and non-hierarchical ViT [5] to extract features from point clouds and point annotations. Despite the simple de-signs, ViT-WSS3D can generate high-quality pseudo boxes through point annotations.
The benefits of ViT-WSS3D are from three aspects: 1)
Thanks to vision transformers’ strong feature representation ability, our ViT-WSS3D can be extremely simple, which en-joys a plain and non-hierarchical encoder structure without specific domain knowledge for design. 2) The simple and compact ViT-style architecture makes it easy to scale up the model and take advantage of pre-trained technologies (e.g., MAE [11]) proposed in advances of 2D vision. 3) Our method is out-of-the-box, which can be adapted to any 3D object detector without exhaustive tuning and modification.
To demonstrate the effectiveness of our method, we con-duct extensive experiments on outdoor KITTI [9] and in-door SUN RGB-D [32] datasets. In particular, with only 10% fully-annotated scenes on both datasets, our ViT-WSS3D can help existing detectors perform closely com-pared to the 100% fully-supervised counterparts. 2.