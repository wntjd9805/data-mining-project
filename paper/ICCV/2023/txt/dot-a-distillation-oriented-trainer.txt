Abstract
Knowledge distillation transfers knowledge from a large model to a small one via task and distillation losses. In this paper, we observe a trade-off between task and distillation losses, i.e., introducing distillation loss limits the conver-gence of task loss. We believe that the trade-off results from the insufﬁcient optimization of distillation loss. The reason is: The teacher has a lower task loss than the student, and a lower distillation loss drives the student more similar to the teacher, then a better-converged task loss could be obtained.
To break the trade-off, we propose the Distillation-Oriented
Trainer (DOT). DOT separately considers gradients of task and distillation losses, then applies a larger momentum to distillation loss to accelerate its optimization. We empiri-cally prove that DOT breaks the trade-off, i.e., both losses are sufﬁciently optimized. Extensive experiments validate the superiority of DOT. Notably, DOT achieves a +2.59% accuracy improvement on ImageNet-1k for the ResNet50-MobileNetV1 pair. Conclusively, DOT greatly beneﬁts the student’s optimization properties in terms of loss conver-gence and model generalization. https://github.com/megvii-research/mdistiller. 1.

Introduction
Knowledge distillation [17, 44, 12, 2, 21, 49, 24] has been proved to be an effective manner to transfer knowl-edge from a heavy (teacher) model to a light (student) one in a wide range of deep learning tasks [40, 14, 35, 7, 4].
Novel learning algorithms have been proposed to achieve better distillation performance [36, 46, 15, 43]. The work-ing mechanism of knowledge distillation also attracts re-search attention [33, 28, 20, 30, 42, 39, 41, 45]. Yet, the op-timization property of knowledge distillation has not been widely investigated, which is also an important perspective to understand KD.
As shown in Figure 1 (left), the typical optimization ob-jective of knowledge distillation is composed of two parts, 1 a task loss (e.g., the cross-entropy loss) and a distillation loss (e.g., the KL-Divergence [17]). We mainly study how the incremental distillation loss inﬂuences the optimization of task loss. Concretely, for an image classiﬁcation task, we visualize (1) the task loss landscapes and (2) task and distillation loss dynamics during the optimization. As Fig-ure 1 (middle) illustrates, we observe that the distillation loss helps the student converge to ﬂat minima, where the student tends to generalize better due to the robustness of
ﬂatter minima [25, 22, 8]. However, as illustrated in Fig-ure 1 (right), introducing distillation loss brings about a trade-off. The task loss is not converged as sufﬁciently as the cross-entropy baseline, although the student’s logits be-come similar to the teacher’s.
We suppose that the trade-off is somehow counter-intuitive. The reason is presented below: the teacher always yields a lower task loss than the student due to the larger model capacity. If the distillation loss is sufﬁciently opti-mized, the task loss would also be decreased since the stu-dent becomes more similar to the better-performing teacher.
We ask: why is there a trade-off and how to break it? We attempt to answer this question from the following perspec-tive. The task and distillation loss terms are combined with a simple summation in practical implementations of popular distillation methods [17, 36, 46, 43, 15]. It could make the optimization manner degrade to multi-task learning, where the network attempts to ﬁnd a balance between the two tasks. As aforementioned, if the distillation loss is sufﬁ-ciently optimized, both losses would be decreased. Thus, we believe that sufﬁciently optimizing the distillation loss is the key to breaking the trade-off.
To this end, we present the Distillation-Oriented
Trainer (DOT) which enables the distillation loss to dom-inate the optimization. It separately considers the gradients provided by the task and the distillation losses, then adjusts the optimization orientation by weighing different momen-tums. A larger momentum is applied to distillation loss, while a smaller one is to task loss. It ensures the optimiza-tion is dominated by gradients of distillation loss since a larger momentum accumulates larger and more consistent
training task loss test task loss task loss distillation loss teacher distillation loss student task loss improvement of KD improvement of DOT
KD baseline
DOT trade-off trade-off training training baseline
KD
DOT
Figure 1: Left: the framework of knowledge distillation (KD). KD introduces an extra distillation loss, transferring knowledge from the teacher model. Middle: a conceptual sketch of ﬂat and sharp local minima [22, 13]. The Y-axis denotes the loss value, and the X-axis the network parameters. The considerable sensitivity of the training loss at sharp minima damages the generalization on test data. In this paper, we discover that knowledge distillation (KD) beneﬁts the student baseline (CE) with ﬂatter minima but unexpectedly limits the convergence.
See Figure 2 and 3. Right: the task and distillation loss dynamics. It suggests that introducing KD brings about a trade-off between the task and distillation losses. See Figure 3. To address this trade-off issue and achieve better performance, we propose Distillation-Oriented
Trainer (DOT). Our DOT breaks the trade-off and leads the student to ideal minima of both great ﬂatness and convergence. gradients than a smaller one. In this way, DOT ensures a sufﬁcient optimization of distillation loss. We validate the effectiveness of DOT from three perspectives. (1) As illus-trated in Figure 1 (right), we prove that DOT successfully breaks the trade-off between task and distillation losses. (2)
As illustrated in Figure 1 (middle), our DOT achieves more generalizable and ﬂatter minima, empirically supporting the beneﬁts of DOT. (3) DOT improves performances of popu-lar distillation methods without bells and whistles, achiev-ing new SOTA results.
More importantly, our research brings new insights into the knowledge distillation community. We show great po-tential for a better optimization manner of knowledge distil-lation. To the best of our knowledge, we provide the ﬁrst at-tempt to understand the working mechanism of knowledge distillation from the optimization perspective. 2.