Abstract
Video-Language Pre-training (VLP) has become one of the most popular research topics in deep learning. How-ever, compared to image-language pre-training, VLP has lagged far behind due to the lack of large amounts of video-text pairs. In this work, we train a VLP model with a hy-brid of image-text and video-text pairs, which significantly outperforms pre-training with only the video-text pairs. Be-sides, existing methods usually model the cross-modal in-teraction using cross-attention between single-scale visual tokens and textual tokens. These visual features are either of low resolutions lacking fine-grained information, or of high resolutions without high-level semantics. To address the is-sue, we propose Hierarchical interactive Video-Language
Pre-training (HiVLP) that efficiently uses a hierarchical vi-sual feature group for multi-modal cross-attention during pre-training. In the hierarchical framework, low-resolution features are learned with focus on more global high-level semantic information, while high-resolution features carry fine-grained details. As a result, HiVLP has the ability to effectively learn both the global and fine-grained represen-tations to achieve better alignment between video and text inputs. Furthermore, we design a hierarchical multi-scale vision contrastive loss for self-supervised learning to boost the interaction between them. Experimental results show that HiVLP establishes new state-of-the-art results in three downstream tasks, text-video retrieval, video-text retrieval, and video captioning. 1.

Introduction
Recently, the framework of pre-training with large-scale uncurated data and then fine-tuning on some specific down-stream tasks has attracted much attention. It firstly emerges in the field of Natural Language Processing (NLP), such as BERT [10], GPT [41] and T5 [42], which are pre-trained on a large corpus of web-scraped dataset and then fine-tuned on a wide variety of NLP downstream tasks.
Figure 1. (a) Coarse visual feature injecting to CA blocks. (b)
Fine-grained visual feature injecting to CA blocks. (c) Hierarchi-cal visual feature injecting to CA blocks. A CA block consists of a casual/bi-directional self-attention layer, a cross-attention layer, and a feed forward layer. It is with bi-directional self-attention for vision-language understanding and with casual self-attention layer for vision-language generation.
Hereafter, it is transferred rapidly to the computer vision area. For examples, CLIP [40], ALIGN [16], Florence
[58] and BLIP [20] all use more than 100 million open-domain image-text pairs in Image-Language Pre-training (ILP). However, most of the existing Video-Language Pre-training (VLP) works [47, 22, 51, 64, 30] use either a small-scale dataset (e.g., YouCookII [63] with 14K video-text pairs) or a large-scale dataset with less diversity (e.g.,
Howto100M [34] sourced from 1.22M videos). To solve this problem, we use a larger-scale dataset with 114M image-text pairs and a dataset with 2.5M video-text pairs to pre-train our model. We show that diversity is more im-portant than the total amount of training pairs, and a small set of image-text pairs can achieve much better performance than using millions of video-text pairs. We believe this is a significant way to enhance VLP models and alleviate the
cost of the collection of video-text pairs.
In VLP and ILP, existing works [47, 22, 64, 21, 56] of-ten use cross-attention to model the cross-modal interaction between visual features and text features. However, they usually adopt only the single-scale and low-resolution vi-sual features (i.e., 1 16 scale of the input) for cross-attention (CA) blocks, as shown in Figure 1(a). This scheme fails to obtain fine-grained interaction with text features and lim-its the performance of the pre-training model. For finer-grained interactions, [33] injects the high-resolution visual features (i.e., 1 4 scale of the input) to CA blocks as shown in Figure 1(b), but it does not have high-level semantics.
To overcome these limitations, we propose Hierarchical in-teractive Video-Language Pre-training (HiVLP) that effi-ciently uses a Hierarchical Visual Feature Group (HVFG) for multi-modal cross-attention. As shown in Figure 1(c),
HVFG includes different scales of visual features, where the low-resolution ones with high-level semantics are bene-ficial for global representation and the high-resolution ones with detailed information are useful for fine-grained inter-action. Especially, HVFG is able to achieve much better accuracy because of such a multi-scale.
Many works [47, 64, 51] use self-supervised learning to assist the video-language pre-training by reconstruct-ing the masked frame tokens. However, it may introduce noise to interactions between visual and textual features for the masked frame tokens [32]. In this paper, we pro-pose a Multi-level Vision Contrastive (MVC) loss for our
HiVLP by applying a global-to-local contrast learning to every scale in HVFG. The MVC loss does not damage the visual tokens and helps the multi-level alignment between visual and textual features.
Our contributions can be summarized as follows:
• To the best of our knowledge, our HiVLP is the first work that uses a hierarchical interaction for video-language pre-training.
It is able to effectively learn both the global and fine-grained representations for better alignment between visual and textual features.
• We design a multi-level vision contrastive (MVC) loss for self-supervised learning that can sufficiently mine multi-level visual information to help video-language pre-training.
• We reveal that diversity is more important than the amount of training pairs, and using more diverse image-text pairs benefits a lot for VLP.
• Our HiVLP unifies video-language understanding and generation. It achieves state-of-the-art results in text-video retrieval, video-text retrieval, and video caption-ing. 2.