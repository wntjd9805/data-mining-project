Abstract
High-fidelity virtual human avatar applications create a need for photorealistic video face synthesis with control-lable semantic editing over facial features. While recent generative neural methods have shown significant progress in portrait video synthesis, intuitive facial control, e.g., of mouth interior and gaze at different levels of details, remains a challenge. In this work, we present a novel face editing framework that combines a 3D face model with StyleGAN vector-quantization to learn multi-level semantic facial con-trol. We show that vector quantization of StyleGAN features unveils richer semantic facial representations, e.g., teeth and pupils, which are difficult to model with 3D tracking priors.
Such representations along with 3D tracking can be used as self-supervision to train a generator with control over coarse expressions and finer facial attributes. Learned represen-tations can be combined with user-defined masks to create semantic segmentations that act as custom detail handles for semantic-aware video editing. Our formulation allows video face manipulation with precise local control over facial at-tributes, such as eyes and teeth, opening up a number of face reenactment and visual expression articulation applications. 1.

Introduction
High-resolution editing and rendering of photorealistic facial portrait videos are in high demand due to virtual hu-man applications, such as actor performance editing, multi-language telepresence, and video conferencing.
In the area of high-resolution video face editing, state-of-the-art methods [41, 36, 56, 1, 42, 57] use pre-trained generative neural networks, such as StyleGAN [25, 27] that, when trained on large face datasets [25, 32], leads to learning a face prior that can be exploited for video editing. In the literature, various formulations leverage such a pre-trained
GAN as input in an attempt to disentangle style spaces for semantic face image editing [44, 8]. Here, editing is lim-ited by the training dataset distribution as well as attributes such as pose range, expression, ethnicity, and gender. The edits are often not explicit and can be unintuitive since it involves finding directions or discriminating features in a high-dimensional feature space to achieve the desired ef-fects [44].
Several generative neural face synthesis methods have been proposed [51]. They take 2D videos frames and a tracked 3D face, often parameterized via a 3D morphable model (3DMM) [11], as supervised inputs and learn to neu-rally render the video. Once the neural network is learned, faces can be edited by modifying the tracked 3D face (or the 3DMM parameters) and rendering new subject videos. The tracked 3D face can be altered using audio input [53, 67], tar-get reenactment videos [55], or artist-based modifications of the 3D face morphable model [49, 50]. Such 3DMM-aware
methods allow global 3D head pose control and mouth shape articulation. Photorealism is handled implicitly by genera-tive neural networks that act as a black-box renderer [31, 54].
However, these approaches lack local control, e.g., of mouth interior or gaze. The main reason is that such local details can not trivially be tracked via standard 3D tracking methods.
On the other hand, 3DMM-free methods, where no 3D face tracking is given, learn to transfer the mouth and facial motion of a driving actor video to that of a tar-get video via deep learning and 2D computer vision tech-niques [46, 39, 10]. Such methods warp the input (original) face frames into the target space while preserving the target face appearance and semantics. Unfortunately, their success is somewhat limited since topological changes, e.g., mouth opening, can not be explained by simple 2D image warping transformations [67].
Thus, we desire an approach that learns conditional (rig-like) control automatically for intuitive video editing appli-cations. To this end, we use vector quantization (VQ) to discover meaningful segmentations in (pre-trained) Style-GAN feature-space. VQ has been used for generative model-ing [40] and recently adopted in GAN frameworks and vision transformers [13]. Unlike existing methods, we use VQ to learn spatial segmentations automatically from a pre-trained
StyleGAN for a given input video. It provides not only dense input to the generator, but also an effective mechanism to manipulate local facial details. Our approach is divided into two stages, as shown in Fig. 1. Stage-I establishes a map between an input (tracked) video sequence and a pretrained
StyleGAN. Stage-II utilizes the learned VQ-representations, represented as a semantic segmentation mask, from Stage-I as input to a generator network that acts as the final renderer.
Our method provides a configurable interface for seman-tic edits thanks to the dense segmentations produced in Stage-I. Given the segmentation masks discovered in Stage-I, artists can label the spatial layout to add human interpretability and combine the segmented regions to create editing priors that act as artist handles for semantic-aware editing. As a result, several applications are feasible, such as semantic editing – gaze, nose, mouth interior, edits such as teeth removal, blink-ing, and nose and eyebrow shape change. To the best of our knowledge, we are the first to adopt VQ for unsupervised segmentation of a pre-trained StyleGAN given a video and use the learned segmentation features for localized semantic video editing. Our key novelties include: 1. A novel formulation that uses vector quantization of a pre-trained StyleGAN for automatic unsupervised video segmentation. 2. A VQ-aware decoder that converts automatically dis-covered VQ segmentation into decoder’s SPADE fea-ture blocks for high-resolution image synthesis. 3. User-guided semantic segmentation masks that act as editing handles for intuitive facial video editing. 4. Video editing applications that enable users to control semantic facial attributes not seen before (see Teaser). 2.