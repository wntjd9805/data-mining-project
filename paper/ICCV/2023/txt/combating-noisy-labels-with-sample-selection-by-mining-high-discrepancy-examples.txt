Abstract
The sample selection approach is popular in learning with noisy labels. The state-of-the-art methods train two deep networks simultaneously for sample selection, which aims to employ their different learning abilities. To prevent two networks from converging to a consensus, their diver-gence should be maintained. Prior work presents that the divergence can be kept by locating the disagreement data on which the prediction labels of the two networks are differ-ent. However, this procedure is sample-inefficient for gener-alization, which means that only a few clean examples can be utilized in training. In this paper, to address the issue, we propose a simple yet effective method called CoDis. In particular, we select possibly clean data that simultaneously have high-discrepancy prediction probabilities between two networks. As selected data have high discrepancies in prob-abilities, the divergence of two networks can be maintained by training on such data.
In addition, the condition of high discrepancies is milder than disagreement, which al-lows more data to be considered for training, and makes our method more sample-efficient. Moreover, we show that the proposed method enables to mine hard clean examples to help generalization. Empirical results show that CoDis is superior to multiple baselines in the robustness of trained models. 1.

Introduction
Learning with noisy labels can be dated back to more than three decades ago [1], and still is one of the hottest problems in weakly supervised learning. The reason is that, in our daily life, noisy labels are unavoidable such as crowd sourcing [67, 32] and web queries [39, 58]. However, the combination of noisy labels and deep networks is rather pessimistic, since deep networks have strong learning ca-pacities and can fully memorize given noisy labels, leading
*Corresponding author (harryjun@ustc.edu.cn). to poor generalization [88, 60, 9, 24, 35, 89, 11, 65, 78, 71, 33]. General-purpose regularization such as dropout and weight decay cannot address this issue well [80].
Fortunately, even though deep networks can fit anything given for training eventually, they learn patterns first [2]: this suggests that deep networks can gradually memorize the data, moving from clean data to mislabeled data. The sample selection approach therefore was proposed to handle noisy labels [21, 16, 47, 76], which is also our focus in this paper. The works on sample selection try to select possibly clean data out of noisy ones, and then use them to update the deep networks. Intuitively, if the training data can become less noisy, better generalization can be achieved.
As the idea of self-teaching sample selection is argued to have the inferiority of accumulated errors caused by the sample-selection bias [16], some advanced algorithms were proposed, which maintain two deep networks, working in a cooperative manner [76, 40, 31, 63]. The key component making the cooperative sample selection works better than the self-teaching one, is that two different networks have different learning abilities and can filter different types of errors introduced by noisy labels. That is to say, when each network selects clean data for its peer network for updates, the error flows coming from the biased selection, can be reduced by peer networks mutually [16].
To keep the different learning abilities of two networks, prior work [79] utilizes a simple strategy called “Update by Disagreement”. In more detail, two networks feed for-ward and predict all data first, and only keep prediction disagreement data, i.e., the data with different prediction labels from two networks. Then, each network selects its clean data from such disagreement data to the peer net-work. At first glance, this method can use less noisy data and meanwhile maintain the different learning abilities of two networks. However, its sample selection procedure is sample-inefficient for network weight updates. It is because the condition of disagreement is somewhat strong in sample selection, which makes that the sample size of prediction disagreement data is often small, especially when the label
noise rate is large [63]. When we tend to select clean data out of them, the sample size of available data for network weight updates will be further reduced. The issue causes that a few clean examples can be utilized in training, which impairs generalization severely [63].
In this paper, to handle the above problem, a robust learn-ing paradigm called CoDis is proposed. Specifically, we inherit the property that deep networks learn patterns first for sample selection, as did in [21, 16, 40, 69]. Mean-while, the training examples with high discrepancies be-tween two networks are encouraged to be involved in train-ing. The network divergence can be maintained by training on such examples.
In this work, for a training example, we measure the discrepancy by using the distance of pre-diction probabilities between two networks, which is con-tinuously valued. As the measurement of whether an ex-ample can be clean (e.g., the cross-entropy loss), is also continuous, it is convenient to make a great trade-off that considers the examples which are likely to be clean (with small cross-entropy losses) and simultaneously can main-tain the two networks diverged (with high discrepancies).
Additionally, the condition of high discrepancies in sam-ple selection is milder than the condition of disagreement.
In other words, the prediction disagreement data must have high-discrepancy prediction probabilities, but the data with high discrepancies can have different prediction probabil-ities but the same prediction labels from two networks.
The milder condition allows us to consider more data for training. Therefore, compared with the prior mentioned procedure of sample selection [79], our procedure is more sample-efficient, which improves generalization.
Furthermore, the examples with high discrepancies in training are probable to be hard examples [12], which play an important role in shaping the decision boundary. Shared with a similar philosophy, the proposed method emphasizes high-discrepancy examples and enables to mine hard clean examples that are critical for generalization. Benefiting from maintaining two networks simultaneously, the discrep-ancy measurement in our work can be conducted on-the-fly, and without the need to carefully determine that useful in-formation on how many training iterations is introduced.
The main contributions of this paper are summarized as three aspects: (1). We provide a simple but effective method to tackle noisy labels, which is more sample-efficient to help generalization. (2). The proposed method can main-tain the network divergence meanwhile enable to mine hard clean examples that are significant for generalization. We also provide theoretical insights into the divergence applied in sample selection. (3). We conduct a series of exper-iments on both simulated noisy datasets including class-balanced and imbalanced noisy datasets, and real-world noisy datasets. Extensive results demonstrate that the ro-bustness of deep models trained by CoDis can well com-bat noisy labels. Particularly, on class-imbalanced noisy datasets, our method can outperform comparison methods by more than 5% of test accuracy. 2.