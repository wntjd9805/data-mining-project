Abstract
Although previous works on traffic scene understanding have achieved great success, most of them stop at a low-level perception stage, such as road segmentation and lane detection, and few concern high-level understanding.
In this paper, we present Visual Traffic Knowledge Graph Gen-eration (VTKGG), a new task for in-depth traffic scene un-derstanding that tries to extract multiple kinds of informa-tion and integrate them into a knowledge graph. To achieve this goal, we first introduce a large dataset named CASIA-Tencent Road Scene dataset (RS10K) with comprehensive annotations to support related research. Secondly, we pro-pose a novel traffic scene parsing architecture containing a Hierarchical Graph ATtention network (HGAT) to ana-lyze the heterogeneous elements and their complicated re-lations in traffic scene images. By hierarchizing the hetero-geneous graph and equipping it with cross-level links, our approach exploits the correlation among various elements completely and acquires accurate relations. The experimen-tal results show that our method can effectively generate vi-sual traffic knowledge graphs and achieve state-of-the-art performance. The dataset RS10K is available at http:
//www.nlpr.ia.ac.cn/pal/RS10K.html. 1.

Introduction
Traffic scene understanding [26, 3, 5, 34] plays an impor-tant role in auto drive systems. Some of its subtasks, such as road segmentation [11, 2], lane detection [43, 36], and traf-fic sign detection [32, 22], have made significant progress in the recent years. However, such low-level output with lim-ited information is far from sufficient for auto drive. In this work, we introduce Visual Traffic Knowledge Graph Gen-eration (VTKGG), a high-level traffic scene understanding task to provide comprehensive and well-formatted traffic scene information for not only auto drive, but also position-Figure 1. Traffic knowledge graphs for scene images in RS10K.
Roads, lanes, and signs are noted with “R”, “L”, and “S”, respec-tively. The current roads are masked by red and right roads by green. For better visualization, some annotations are simplified. ing assistance and map correction.
VTKGG aims to extract traffic information from traffic scene images and organize the information into a knowl-edge graph. Considering that the relevant techniques are quite developed, VTKGG is carried out on the position-known traffic signs, roads, and lanes. As shown in Fig-ure 1, the traffic knowledge graph is a graph connecting roads, lanes, locations, warnings, etc. To accomplish this, we must parse the content of traffic signs and identify re-lations between sign components (i.e., texts, symbols, and arrowheads on signs) and their correspondence to traffic el-ements (i.e., roads and lanes), which is complicated, how-ever. First, relations exist between traffic signs (S-S rela-tions), which indicate one sign is a supplement to another, e.g., “S1” and “S2” in Figure 1(a) and Figure 1(c). Second, relations exist between sign components (C-C relations), which has been demonstrated in [12]. Third, relations exist
between components and traffic elements (C-T relations), since different components in the same sign, e.g., the three arrows of “S0” in Figure 1(a), may correspond to different traffic elements. All these relations are not independent.
For example, C-C relations may span two related signs to connect their components and some C-T relations can be in-ferred from other C-C and C-T relations. Therefore, we can summarize two characteristics of VTKGG: multiple hetero-geneous elements and complicated entangled relations.
Due to the aforementioned factors, pertinent studies are limited. Guo et al. [12] proposed traffic sign understanding, a task seeking to decipher traffic signs using sign compo-nents and their relations. However, it only concerns cropped distortion-free traffic panels without traffic elements. Greer et al.
[10] tried to detect salient traffic signs pertaining to the current road, which is only a coarse-grained corre-spondence, unfortunately. To take consider all elements and obtain fine-grained relations, a straightforward approach is creating a fully connected graph that treats all the elements equally. However, this approach is inefficient for compli-cated scene images by neglecting the hierarchy of elements and relations. The S-S relations are high-level relations de-pending on high-level features. They further affect C-C relations since C-C relations may span two related signs.
The C-C relations support the reasoning of C-T relations by helping the model to learn the layout of the sign. Based on the analysis above, we suggest a novel architecture using a Hierarchical Graph ATtention network (HGAT) to inte-grate the reasoning of S-S, C-C, and A-T relations consid-ering their interactions, where “A” denotes the representa-tive arrow elements selected from all components. Along with this architecture, a large dataset CASIA-Tencent Road
Scene dataset (RS10K) is collected and annotated.
The overall framework proposed is shown in Figure 3.
Concretely, given an input image, a sign component detec-tor is applied to detect components on signs. Then node features are gathered from bounding boxes or masks of el-ements to build the input graph. After that, our HGAT per-forms feature refining and relation classification to get all the element relations. After all sign texts are recognized by a text recognizer and an attribute recognizer, the traf-fic knowledge graph is constructed from the above results.
Our HGAT adopts a top-down manner to organize its lay-ers. Three levels in each layer process subgraphs for S-S relations, C-C relations, and A-T relations respectively. To make the best of the correlation between different levels, cross-level links are created between adjacent levels to al-low for cross-level interaction. Additionally, the affiliation and the relative position between traffic elements are em-bedded as edges between traffic elements, which we call
T-T links. By deploying these techniques, HGAT can lever-age the correlation between relations completely and obtain accurate relations for traffic knowledge graph generation.
RS10K has 10066 high-resolution images of various traffic scenes. Over all these images, there are 42923 traffic signs of different types. To facilitate VTKGG, comprehen-sive annotations, as well as knowledge graphs, are provided, including road mask, road direction, lane mask, lane type, components on signs, and all kinds of relations. As shown in Table 1, RS10K is a large versatile dataset that can also be utilized in the future for tasks like road and lane segmen-tation, traffic sign detection and understanding, etc.
Consequently, our contributions are as follows: 1. We propose a new valuable task VTKGG, and intro-duce a new large dataset RS10K with rich annotations to support it and other relevant research. 2. We propose a framework for VTKGG with HGAT to reason all relations hierarchically while leveraging their correlation effectively. 3. Experimental results on RS10K show that our frame-work is an effective solution for VTKGG, and achieves state-of-the-art performance compared with other rela-tion reasoning methods. 2.