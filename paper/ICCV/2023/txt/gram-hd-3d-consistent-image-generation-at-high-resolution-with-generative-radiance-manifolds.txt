Abstract
Recent works have shown that 3D-aware GANs trained on unstructured single image collections can generate mul-tiview images of novel instances. The key underpinnings to achieve this are a 3D radiance field generator and a volume rendering process. However, existing methods ei-ther cannot generate high-resolution images (e.g., up to 256×256) due to the high computation cost of neural vol-ume rendering, or rely on 2D CNNs for image-space upsam-pling which jeopardizes the 3D consistency across different views. This paper proposes a novel 3D-aware GAN that can generate high resolution images (up to 1024×1024) while keeping strict 3D consistency as in volume render-ing. Our motivation is to achieve super-resolution directly in the 3D space to preserve 3D consistency. We avoid the otherwise prohibitively-expensive computation cost by ap-plying 2D convolutions on a set of 2D radiance manifolds
*Work done when JX and YD were interns at MSRA. 1Project page: https://jeffreyxiang.github.io/GRAM-HD/ defined in the recent generative radiance manifold (GRAM) approach, and apply dedicated loss functions for effective
GAN training at high resolution. Experiments on FFHQ and AFHQv2 datasets show that our method can produce high-quality 3D-consistent results that significantly outper-form existing methods. It makes a significant step towards closing the gap between traditional 2D image generation and 3D-consistent free-view generation. 1 1.

Introduction
While generative modeling of 2D images have achieved tremendous success [5, 22–24] with generative adversarial networks (GANs) [17], 3D-aware GANs that aims to gen-erate photorealistic multiview images begun to emerge in recent years [8, 12, 41, 47, 54, 67]. Despite both being trained on unstructured 2D image collections, the latter is capable of synthesizing the images of an object at different 3D viewpoints. The key to achieve this is to generate an un-derlying 3D representation, for which neural radiance field (NeRF) [37] has been the cornerstone for recent methods.
With volumetric rendering, NeRF can produce realistic im-ages while enforcing strong 3D consistency across views.
However, the high computational cost of neural volu-metric rendering greatly limits the affordable image res-olution for GAN training.
It also introduces hurdles in fine detail generation due to insufficient point sampling. A workaround is only judging whether a patch of the gener-ated image is real to not during training [47, 54] rather than a whole image. But using a patch discriminator may lack global perception of the images and lead to inferior image generation quality. The recent generative radiance manifold (GRAM) method [12] significantly improved the generation quality by sampling points on a set of learned surface mani-folds. Still, it can only be trained on images up to 256×256 resolution on modern GPUs, which is in sheer contrast to state-of-the-art 2D GANs that can easily model 1024×1024 images with moderate computing cost.
Along a different axis, many methods resort to 2D con-volutions to tackle the dilemma [7, 18, 41, 44, 64]. A straightforward idea shared by these methods is to render low-resolution images or feature maps and apply 2D CNNs to increase the resolution. With this strategy, they have demonstrated higher-resolution generation (e.g., 512×512 for [7, 64] and 1024×1024 for [18, 44]). Unfortunately, image-space upsampling with 2D CNNs inevitably incurs 3D inconsistency among the generated multiview images.
As such, these methods can be used in user-interactive im-age generation and manipulation but are not suitable for video synthesis and animation.
We propose GRAM-HD, a GAN method that can synthe-size strongly 3D-consistent images at high resolution. Our motivation is to do upsampling or super-resolution in the 3D space and keep the volume rendering paradigm to retain strict 3D consistency. But how to achieve this efficiently is not straightforward (e.g., upsampling a discretized low-resolution volume using 3D convolutions quickly becomes untractable for high-resolution output).
In this paper, we leverage the GRAM [12] method, which defines a set of sur-face manifolds, to handle high resolution generation. Our key insight is that the surface manifolds can be upsampled using 2D CNNs for efficient super-resolution. We flatten and sample each learned surface to regular 2D image grids and apply a shared 2D CNN for upsampling and feature-to-radiance translation. This way, our method not only ensures multiview consistency by generating a high-resolution 3D representation for rendering, but also enjoys the computa-tional efficiency of 2D CNNs. In essence, we tackle a 3D super-resolution task with object-space 2D CNN.
We evaluate our method on the FFHQ [23] and
AFHQv2-CATS [11] datasets. We show that GRAM-HD can generate photorealistic images that are both of high resolution (up to 1024×1024) and strongly multiview-consistent, which cannot be achieved by any previous method. It also outperforms GRAM in terms of both gen-eration quality and speed at the same image resolution by upsampling low-resolution manifolds.
The contributions of this work are summarized below:
• We present a novel 3D-aware image generation ap-proach that can generate high-resolution images (up to 1024×1024) with strong multiview-consistency and highly-realistic geometry details.
• We introduce a method for 3D space super-resolution using efficient 2D CNN under the radiance manifold representation.
• We significantly reduced the computation cost of the radiance manifold based 3D-aware generation method while obtaining higher quality images (e.g., 76%↓ memory cost, 58%↓ inference time, 21%↓ FID on
FFHQ-2562; 95%↓ inference time for 10242) 2.