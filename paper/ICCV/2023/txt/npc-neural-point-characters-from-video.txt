Abstract 1.

Introduction
High-fidelity human 3D models can now be learned di-rectly from videos, typically by combining a template-based surface model with neural representations. However, ob-taining a template surface requires expensive multi-view capture systems, laser scans, or strictly controlled condi-tions. Previous methods avoid using a template but rely on a costly or ill-posed mapping from observation to canon-ical space. We propose a hybrid point-based represen-tation for reconstructing animatable characters that does not require an explicit surface model, while being gener-alizable to novel poses. For a given video, our method automatically produces an explicit set of 3D points repre-senting approximate canonical geometry, and learns an ar-ticulated deformation model that produces pose-dependent point transformations. The points serve both as a scaffold for high-frequency neural features and an anchor for effi-ciently mapping between observation and canonical space.
We demonstrate on established benchmarks that our repre-sentation overcomes limitations of prior work operating in either canonical or in observation space. Moreover, our au-tomatic point extraction approach enables learning models of human and animal characters alike, matching the per-formance of the methods using rigged surface templates despite being more general. Project website: https:
//lemonatsu.github.io/npc/.
It is now possible to reconstruct photo-realistic charac-ters from monocular videos, but reaching high-fidelity re-constructions still requires controlled conditions and dedi-cated capture hardware that prevents large-scale use. While static scenes can be reconstructed from multiple views recorded with a single moving camera, capturing dynamic human motion demands controlled studio conditions [2, 22, 23, 39], usually with a large number of synchronized cam-eras. One way to tackle the monocular case is to exploit the intuition that movement of the camera with respect to a body part is roughly equivalent to moving a body part with respect to the camera [43, 44]. However, the uncon-strained setting remains difficult, as it requires establishing correspondences across frames and through dynamic defor-mation.
A prominent line of work follows the traditional ani-mation pipelineâ€”rigging a surface mesh to an underlying skeleton and equipping the mesh either with a neural tex-ture [2, 22] or learnable vertex features [16, 34, 35]. This approach is very efficient, as the forward mapping using for-ward kinematics provides a robust estimate of underlying geometry in closed form, and it also allows for high-quality reconstructions, as neural textures are capable of represent-ing high-frequency details [22, 46]. However, it does re-quire highly accurate 3D pose and body shape, which are
typically obtained from expensive laser scans [50] or by of-fline fitting personalized parametric body models to multi-view captures [2, 55]. Moreover, most of the existing para-metric models rely on linear blend skinning [18], which is prone to artefacts. In this work, we aim at building an an-imatable full-body model from a single monocular video, without relying on a pre-defined template or complex cap-ture systems.
Another line of work reduces the problem to building a body model in canonical space, e.g., learning a neural radi-ance field of the person in T-pose [14, 19, 50]. In practice, this family of approaches requires finding the backward mapping from the observation space to the canonical space, which is either learned from high-fidelity scans [49], e.g. by learning a deformation on top of rigid motion [31, 52], or through root finding [19, 50]. These methods are typi-cally limited in generalization, require high-quality training data, and are computationally heavy at test time.
Our goal is to attain the generality of surface-free canon-ical models and the speed of forward mappings without a need for a pre-defined template that restricts applicability.
To this end, we rely on an implicit, surface-free body rep-resentation that does not depend on a precise 3D pose nor a rig [43, 44]. Namely, we use A-NeRF [44] alongside off-the-shelf pose estimators [15] to get a reasonable 3D pose estimate and DANBO [43] to learn an initial surface esti-mate in the canonical space.
Given such an initial estimate, our method, Neural Point
Characters (NPC), reconstructs a high-quality neural char-acter that can be animated with new poses and rendered in novel views (Figure 1). The difficulty is that the ini-tial shape is very approximate and noisy, and is insufficient to model high-quality geometry. Central to NPC is thus a novel point representation that is designed to improve noisy shape estimates, and subsequently learns to represent fine texture details and pose-dependent deformations. Keys are our two main contributions: First, we propose to find corre-spondences between the canonical and observation space by inverting the forward mapping via nearest-neighbor lookup, including the non-linear pose-dependent deformation field modeled with a graph neural network (GNN). The surface points serve as anchors for inferring the backward map-ping of a query point to the neural field in canonical space through an efficient amortized lookup. Our approach is more efficient than root finding and more precise than mod-els assuming piece-wise rigidity. Second, we propose to use the non-linearly deformed points as a scaffold in obser-vation space and represent high-frequency pose-dependent details. Point features and neural fields in canonical space are further combined with bone-relative geometry encod-ings to encourage better generalization. We provide a com-prehensive evaluation of our method, and demonstrate state-of-the-art results on the established benchmarks in monoc-ular and multi-view human reconstruction. We additionally demonstrate versatility by reconstructing human and animal characters alike. 2.