Abstract
Text-to-motion generation is a formidable task, aiming to produce human motions that align with the input text while also adhering to human capabilities and physical laws.
While there have been advancements in diffusion models, their application in discrete spaces remains underexplored.
Current methods often overlook the varying significance of different motions, treating them uniformly. It is essential to recognize that not all motions hold the same relevance to a particular textual description. Some motions, being more salient and informative, should be given precedence during generation. In response, we introduce a Priority-Centric Motion Discrete Diffusion Model (M2DM), which utilizes a Transformer-based VQ-VAE to derive a concise, discrete motion representation, incorporating a global self-attention mechanism and a regularization term to counteract code collapse. We also present a motion discrete diffusion model that employs an innovative noise schedule, deter-mined by the significance of each motion token within the entire motion sequence. This approach retains the most salient motions during the reverse diffusion process, leading to more semantically rich and varied motions. Addition-ally, we formulate two strategies to gauge the importance of motion tokens, drawing from both textual and visual indica-tors. Comprehensive experiments on the HumanML3D and
KIT-ML datasets confirm that our model surpasses existing techniques in fidelity and diversity, particularly for intricate textual descriptions. 1.

Introduction
Generating realistic and diverse 3D human motions un-der various conditions e.g., action labels [34, 18], natural language descriptions [55, 17, 5, 56], and musical cues
[27, 28, 29], presents a significant challenge across mul-tiple domains, including gaming, filmmaking, and robotic animation. Notably, motion generation based on language de-scriptions has garnered substantial interest, given its promise
*Corresponding author: xinchao@nus.edu.sg for enhancing realism and broadening practical applications.
However, generating a high-quality motion is not trivial due to the inherent modality gap and the complex mapping between text and motion modalities. Previous works [35, 43] align the latent feature space between text and motion modal-ities. For instance, TEMOS [35] aligns the text and mo-tion feature by learning text and motion encoders. Motion-Clip [43] aligns the human motion manifold to CLIP [37] space for infusing semantic knowledge of CLIP into the motion extractor. Despite these advancements, they might encounter performance degradation when dealing with com-plex textual descriptions.
To process these complex textual descriptions, some text-to-motion methods are proposed. State-of-the-art approaches such as TM2T [17] and T2M-GPT [55] use vector-quantized autoencoder (VQ-VAE) [46] to learn a discrete and compact motion representation, followed by a translation model [38, 47] to map the text modality to the motion modality. With the popularity and the superior performance in the generation tasks of diffusion models [20], MDM [44] and MLD [5] are proposed to learn conditioned diffusion models on the raw motion representation space and the latent feature space, respectively.
While there have been promising advancements, two pri-mary issues remain unresolved: i) The aforementioned dif-fusion methods, namely MDM [44] and MLD [5], predomi-nantly address the latent feature within a continuous space.
Although VQ-VAE-inspired architectures have made consid-erable strides in motion generation [55, 17], particularly with the support of discrete and compact motion representations, the integration of the diffusion model into a discrete space remains inadequately explored. ii) Discrete diffusion models employed in prior studies [15, 14, 22] treat all tokens uni-formly. This approach presupposes that every token conveys an equivalent amount of information, neglecting the inherent disparities among tokens within a sequence. A more intuitive generative approach for humans would involve a progres-sive hierarchy, commencing with overarching concepts and gradually delving into finer details.
To address the aforementioned challenges, we introduce a priority-centric motion discrete diffusion model (M2DM)
Figure 1: Our Priority-Centric Motion Discrete Diffusion Model (M2DM) generates diverse and precise human motions given complex textual descriptions. The color of human mesh goes from light to dark over time. designed to generate motion sequences from textual descrip-tions, progressing in a primary to secondary manner. Initially, we employ a Transformer-based VQ-VAE, which is adept at learning a concise, discrete motion representation through the global self-attention mechanism. To circumvent code collapse and guarantee the optimal utilization of each motion token within the codebook, a regularization term is incor-porated during the VQ-VAE training phase. Furthermore, we craft a noise schedule wherein individual tokens within a sequence are allocated varying corruption probabilities, contingent on their respective priorities during the forward process. Specifically, tokens of higher priority are slated for corruption towards the latter stages of the forward process.
This ensures that the subsequent learnable reverse process ad-heres to a primary-to-secondary sequence, with top-priority tokens being reinstated foremost.
To discern the significance of motion tokens within a se-quence, we introduce two evaluative strategies: static assess-ment and dynamic assessment. 1) Static assessment: Draw-ing inspiration from the neural language domain, where the significance of individual words is gauged by their entropy across datasets, we calculate the entropy of each motion to-ken across the entire motion dataset. 2) Dynamic assessment:
We cultivate an agent specifically to dynamically gauge the importance of tokens within a sequence. Given a discrete motion token sequence, the agent masks one token at each iteration. These masked token sequences are then fed into the VQ decoder for continuous motion reconstruction. The agent’s objective is to curtail the cumulative reconstruction discrepancy between the original and the reconstructed mo-tions at every phase. This is achieved using a reinforcement learning [23] (RL) strategy, facilitating the agent’s identifi-cation of motion tokens that convey minimal information within a sequence—a process we term dynamic analysis.
Our priority-centric discrete diffusion model has showcased commendable generative prowess, especially when dealing with intricate textual descriptions. Through rigorous test-ing, our method has proven its mettle, consistently matching or surpassing the performance of prevailing text-to-motion generation techniques on the HumanML3D and KIT-ML datasets.
We summarize our contributions as follows:
• To capture long-range dependencies in the motion se-quences, we apply Transformer as the architecture of
VQ-VAE. Besides, a regularization term is applied to increase the usage of tokens in the codebook.
• We design a priority-centric scheme for human motion generation in the discrete latent space. To enhance the performance given complex descriptions, we design a novel priority-centric scheme for the discrete diffusion model.
• Our proposed priority-centric M2DM achieves state-of-the-art performance on the HumanML3D [16] and
KIT-ML [36] datasets. 2.