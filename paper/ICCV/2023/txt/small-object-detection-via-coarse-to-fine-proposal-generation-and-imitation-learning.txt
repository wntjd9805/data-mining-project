Abstract
The past few years have witnessed the immense success of object detection, while current excellent detectors strug-gle on tackling size-limited instances. Concretely, the well-known challenge of low overlaps between the priors and object regions leads to a constrained sample pool for opti-mization, and the paucity of discriminative information fur-ther aggravates the recognition. To alleviate the aforemen-tioned issues, we propose CFINet, a two-stage framework tailored for small object detection based on the Coarse-to-ﬁne pipeline and Feature Imitation learning. Firstly, we introduce Coarse-to-ﬁne RPN (CRPN) to ensure sufﬁ-cient and high-quality proposals for small objects through the dynamic anchor selection strategy and cascade regres-sion. Then, we equip the conventional detection head with a Feature Imitation (FI) branch to facilitate the re-gion representations of size-limited instances that perplex the model in an imitation manner. Moreover, an auxil-iary imitation loss following supervised contrastive learn-ing paradigm is devised to optimize this branch. When integrated with Faster RCNN, CFINet achieves state-of-the-art performance on the large-scale small object detec-tion benchmarks, SODA-D and SODA-A, underscoring its superiority over baseline detector and other mainstream detection approaches. Code is available at https:// github.com/shaunyuan22/CFINet. 1.

Introduction
Small object detection (SOD) 1 aims to classify and lo-calize the instances with limited regions, which plays an im-portant role in a wide range of scenarios, such as pedestrian detection, autonomous driving, and intelligent surveillance understanding, to name a few [42, 28, 22, 19, 36, 2, 20].
Compared to the generic object detection which has been
*Corresponding author: gcheng@nwpu.edu.cn 1This paper focuses on the detection of ”pure” small objects, where the scales of all the objects are distributed within a relatively tight range
[45, 34, 9], which is distinct from the known small objects in multi-scale object detection [24].
Figure 1. Distribution of maximum IoU of anchors matched to each ground-truth instance in SODA-D [9] train-set, where extremely Small (eS), relatively Small (rS), and generally Small (gS) correspond to three area subsets in SODA [9] with the ranges (0, 144], (144, 400] and (400, 1024]. The smaller the objects are, the lower IoU the matched anchors have, hence the commonly used positive IoU threshold (0.7) is too rigorous for small objects. extensively studied, SOD task receives relatively little at-tention and good solutions are still scarce so far. Moreover, generic detectors [29, 6, 25, 31, 7, 3, 26, 5] usually struggle on handling small objects due to two inherent challenges: the insufﬁcient and low-quality samples for training and the uncertain prediction of RoIs (Region of Interests).
First, current prevailing detectors exploit either overlap-based [29, 26] or distance-based [31] strategies to select the positive priors of objects for training. However, small instances usually occupy an extremely limited area, there-fore the region overlaps between densely arranged anchors and ground truth boxes are signiﬁcantly small and far from the commonly used positive IoU (Intersection-over-Union) threshold, as in Figure 1. In other words, the existing pos-itive sample criterion is overly stringent when applied to small/tiny objects, resulting in a restricted number of sam-ples available for optimization. An intuitive approach in-volves reducing the threshold for deﬁning a positive sam-ple [47]. However, while this can lead to an increase in the number of positive samples, it often at the expense of overall sample quality, in which low-quality samples disrupt optimization and incur a trivial regression solution. Worse still, this is actually contradictory to the purpose of proposal
ctr/ign ratio AP
Baseline 0.2/0.5 0.5/0.8 0.8/1.0 28.9 29.1 29.5 27.5
AP50 AP75 APeS APrS APgS APN 59.4 43.0 44.7 56.5 45.0 57.8 42.5 54.1 13.8 12.5 13.5 11.2 34.5 35.4 35.8 33.7 25.7 25.5 26.2 23.8 24.1 25.9 26.0 24.3
Table 1. The performances of Cascade RPN [32] compared to the baseline (a vanilla Faster RCNN [29]). The ctr/ign ratio de-notes the sampling region in ﬁrst regression stage of Cascade RPN.
The results are tested on the SODA-D [9] test-set and with a
ResNet-50 [17] as the backbone. network, i.e., guaranteeing the recall and ease the burden of subsequent work.
To sum up, current prevailing priors-to-proposals paradigms that heavily depend on the overlap or distance metric have inherent limitations in detecting small objects, and nowadays devised assignment or sampling schemes contribute minimally to this problem [9, 40]. Since the pro-posals play such a crucial role in two-stage detectors, so how about the improved Region Proposal Network (RPN) variants [13, 32, 33] meet small object detection? Follow-ing this line, we take Cascade RPN [32], one of the most superior proposal network for generic object detection, to perform preliminary experiments and the results are shown in Table 1. While the auxiliary regression phase provides reﬁned priors with better initialization for subsequent re-gression, the ﬁnal results remain somewhat unsatisfactory.
Speciﬁcally, the improvement mainly comes from the larger objects and the APeS as well as APrS actually decrease sig-niﬁcantly instead, indicating that the region-based sampling strategy is inclined to large instances which further domi-nate the proposal network. Meanwhile, enlarging the sam-ple region contributes little (even negative) to this condition (see the bottom row in Table 1). Therefore, the coarse-to-ﬁne pipeline has the potential to surmount the barrier of conventional prior-to-proposal paradigm, but the crux lies in dedicating sufﬁcient attention to small instances.
Second, small objects usually lack discriminative in-formation and distorted structures, leading the inclination of model to give ambiguous even incorrect predictions
[1, 11]. Meanwhile, there are a certain amount of large instances embodying clear visual cues and better discrim-ination. Building upon this observation, several works pro-posed to bridge the representation gap between small ob-jects and large ones, and most of them [2, 22, 28, 1] rely on Generative Adversarial Network (GAN) [14] or simi-larity learning [19, 36] to super-resolve/restore the features of size-limited instances under the guidance of large ones that are deemed to be visually authentic. However, these approaches overlook the fact: high quality (cid:54)= large size meanwhile small size (cid:54)= low quality. In other words, the criterion for humans and the model to decide whether a sample is competent to be a good example is distinct. For the latter, it is dynamic and should be adjusted according to the current optimization of the detector. Moreover, efforts in this line have to resort to sophisticated training strategies or additional models, which is time-consuming and break the conventional end-to-end paradigm.
Putting the above parts together, we propose a two-stage small object detector CFINet based on the coarse-to-ﬁne pipeline and feature imitation learning. Concretely, enlight-ened by the multi-stage proposal generation scheme in Cas-cade RPN, we devise Coarse-to-ﬁne RPN (CRPN). It ﬁrstly employs an dynamic anchor selection strategy to mine po-tential priors to conduct coarse regression, and henceforth, these reﬁned anchors will be classiﬁed and regressed by the region proposal network. In addition, we extend the conven-tional classiﬁcation-and-regression setting with an auxiliary
Feature Imitation (FI) branch, which can leverage the re-gional features of high-quality instances to guide the learn-ing of those objects with uncertain/mistaken predictions, and a loss function based on the Supervised Contrastive
Learning (SCL) [18] is designed to optimize the whole pro-cess. The main contributions of this paper are summarized as follows:
• A coarse-to-ﬁne proposal generation pipeline named
CRPN was built to perform anchor-to-proposal proce-dure, where an area-based anchor mining strategy and cascade regression empower the high-quality propos-als for small instances.
• An auxiliary Feature Imitation (FI) branch was intro-duced to enrich the representations of low-quality in-stances perplexing the model under the supervision of high-quality instances, and this novel branch is opti-mized by a tailored loss function based on SCL.
• The experiment results on the SODA-D and SODA-A datasets exhibit the superiority of our CFINet to detect these instances with extremely limited sizes. 2.