Abstract
Many existing FL methods assume clients with fully-labeled data, while in realistic settings, clients have limited labels due to the expensive and laborious process of label-ing. Limited labeled local data of the clients often leads to their local model having poor generalization abilities to their larger unlabeled local data, such as having class-distribution mismatch with the unlabeled data. As a result, clients may instead look to beneﬁt from the global model trained across clients to leverage their unlabeled data, but this also becomes difﬁcult due to data heterogeneity across clients. In our work, we propose FEDLABEL where clients selectively choose the local or global model to pseudo-label their unlabeled data depending on which is more of an ex-pert of the data. We further utilize both the local and global models’ knowledge via global-local consistency regulariza-tion which minimizes the divergence between the two models’ outputs when they have identical pseudo-labels for the unla-beled data. Unlike other semi-supervised FL baselines, our method does not require additional experts other than the local or global model, nor require additional parameters to be communicated. We also do not assume any server-labeled data or fully labeled clients. For both cross-device and cross-silo settings, we show that FEDLABEL outperforms other semi-supervised FL baselines by 8-24%, and even outper-forms standard fully supervised FL baselines (100% labeled data) with only 5-20% of labeled data. 1.

Introduction
Federated learning (FL) [27] enables collaborative learn-ing across clients without explicit disclosure of their local data [18, 38]. In FL, a server updates its global model by aggregating the local gradients obtained from clients’ train-ing on their datasets. These clients can be a number of edge-devices such as cell-phones (cross-device) [44] or a handful of hospitals, for example, willing to train a model for disease prediction without sharing patients’ private data
†Work done while at Microsot Research. (a) CIFAR10 (Cross-Device) (b) OrganAMNIST (Cross-Silo)
Figure 1: Test accuracy of the global model for varying amount of labeled local data in each client. The fewer the labeled data, the lower the test accuracy for standard FL algorithms (FedAvg,
FedProx) while our proposed FEDLABEL performs signiﬁcantly higher. For CIFAR10, FEDLABEL achieves an even higher test accuracy than when 100% of labels are used. (cross-silo) [32]. While FL can indeed allow clients to train a single global model without private data sharing, a crucial yet often overlooked limitation found in realistic FL scenarios is that labels can be scarce [13, 9, 2, 21, 34]. In cross-device settings, owners of edge-devices rarely go through the effort of labeling all of their local data such as photos, resulting in only a few labeled samples and a large volume of unlabeled data. Similarly in cross-silo settings, such as hospitals collab-orating for predicting diseases, labeling is often a laborious process where healthcare experts are required to process vol-umes of patients’ data [15, 12]. Such scarcity of labels can lead to severe performance degadation as shown in Fig. 1.
A naïve approach to tackle label scarcity in clients is using standard semi-supervised learning (SSL) methods de-vised for general machine learning (ML) applications at each client, using its own local data. For example, consistency regularization methods [41] or pseudo-labeling methods [35] can be directly used with each client’s local data with its lo-cal model. However, with limited labels, the feature-label pair distribution of the labeled data can be different from that of the unlabeled data as shown in Fig. 2 and previous work [47, 3]. We call this difference between the two distri-butions as class distribution mismatch. As such, the limited
High Data
Heterogeneity 20% Labeled 50% Labeled
Data
Data 32.57 (
Only Local 37.51 (
Only Global
±
Global+Local (ours) 44.51 (
± 2.20) 41.21 ( 1.80) 38.43 (
± 1.85) 50.53 (
± 1.80) 0.94) 1.74)
±
±
Figure 2: Class distribution mismatch (CIFAR10) is shown just by limiting the number of labeled data to 20% without artiﬁcially biasing the labeled data distribution. labeled data can poorly generalize to a large number of un-labeled local data. This is also shown in Table 1, where the ‘Only Local’ performance largely degrades for a smaller number of labeled data. Due to these scenarios, leveraging only the local knowledge of a client may not be enough to fully utilize its unlabeled data. Thus, in FL, a client can seek to utilize the global knowledge shared across clients through the aggregation of the local updates to the global model. The problem, however, with only using the global knowledge for leveraging the unlabeled data at the clients is that the data distribution amongst clients’ local data is heterogeneous [33, 29, 11, 26, 20, 39]. Due to this data het-erogeneity, for clients whose local data distribution differs from the overall global data distribution, the global model may also not be useful in assisting the clients to leverage their unlabeled data as shown in Table 1, ‘Only Global’ case in which the test accuracy is lower than selectively leveraging both global and local models.
Based on the observations above, either the local or global model, or both, can be useful for clients to leverage their unlabeled data depending on the labeled data’s generaliz-ability to the unlabeled data and clients’ data heterogene-ity. Therefore, to utilize the setting of FL where clients have access to both the knowledge from their local data and the global model, we propose a selective knowledge assimilation method named FEDLABEL where each client chooses between its local and global model to pseudo-label its unlabeled data based on each model’s conﬁdence score.
Moreover, with our proposed global-local consistency regu-larization, we fully utilize both the local and global models when both have useful knowledge of the unlabeled data.
Most relevant line of recent work to FEDLABEL has pro-posed the server to identify multiple experts for each client.
The experts can be other clients with similar data distribu-tions [17], or the local, global, and the mixture of the local and global models by model splicing [2]. However, the additional computational and communication overhead for the server to ﬁnd and send the appropriate experts for each client can become exponentially costly with the increasing participating clients [40]. Moreover, these works treat all experts equally, taking an average of their knowledge for leveraging each client’s unlabeled data. Due to this, we show
Table 1: Test acc. (CIFAR10) when only the local or global model is used for pseudo-labeling which largely underperforms the case when both models are selectively used by our proposed FEDLABEL. that these methods’ performance degrades signiﬁcantly (see
Section 5.2) when the number of labels decreases and the data heterogeneity gets higher. In our work, we use only the natural two experts that are available in FL, local and global, and show that this is enough to fully utilize the unlabeled data at the clients when executed properly for both high and lower label scarcity and data heterogeneity cases. Other related work proposes methods with restrictive assumptions such as the server having labeled data that is similar to the data distribution of the clients [8] or several clients having fully labeled data [23].
In FL, the server does not have access to client data and labels are scarce, making these assumptions practically improbable.
As summarized in Table 2, previous work in SSFL: i) as-sumes restrictive settings such as the server or several clients having good-enough fully labeled data, ii) imposes addi-tional computational/communication burden at the server to
ﬁnd and send more experts other than the naturally occurring local and global models, and iii) does not consider the poor generalization of limited data to the unlabeled data such as class distribution mismatch. Improving on these drawbacks, we propose our novel method FEDLABEL, which:
• Is robust to both the limited generalizability of the labeled data such as class distribution mismatch and data hetero-geneity by using just two experts, global and local, to leverage a large number of unlabeled data (80-95%) with just a few labeled data (5-20%).
• Does not require the server having any labeled data or a few clients to have fully labeled data. It also does not re-quire additional experts to be computed or communicated other than the local and global model used in standard FL algorithms [27, 33].
• Leverages unlabeled data by adaptively choosing either the local or global model based on the conﬁdence of the model’s prediction for pseudo-labeling with our pro-posed global-local consistency regularization that mini-mizes the divergence between the models’ outputs when their pseudo-labels are identical.
• Achieves 8-24% test accuracy improvement compared to the other SSFL baselines, and even achieves a higher test accuracy than fully supervised scenario (100% labeled data) with only using 5-20% of labels for extensive experi-ments (3 tasks for cross-device and 2 tasks for cross-silo).
Method
SemiFL [8]
Rscfed [23]
FedTriNet [2]
FedMatch [17]
FedLabel (ours)
Requires Server
Labeled Data
Requires Fully
Labeled Clients
Requires Additionally
Computed Expert(s)
Requires Additional
Comm.- of Params.
Robust to
Class-Dist. Mismatch
Yes
No
No
No
No
No
Yes
No
No
No
No
No
Yes
Yes
No
No
No
Yes
Yes
No
No
No
No
No
Yes
Table 2: Comparison of related work with FEDLABEL. 2.