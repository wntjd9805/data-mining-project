Abstract
Sample selection is a prevalent method in learning with noisy labels, where small-loss data are typically consid-ered as correctly labeled data. However, this method may not effectively identify clean hard examples with large losses, which are critical for achieving the model’s close-to-optimal generalization performance.
In this paper, we propose a new framework, Late Stopping, which leverages the intrinsic robust learning ability of DNNs through a pro-longed training process. Specifically, Late Stopping gradu-ally shrinks the noisy dataset by removing high-probability mislabeled examples while retaining the majority of clean hard examples in the training set throughout the learning process. We empirically observe that mislabeled and clean examples exhibit differences in the number of epochs re-quired for them to be consistently and correctly classified, and thus high-probability mislabeled examples can be re-moved. Experimental results on benchmark-simulated and real-world noisy datasets demonstrate that the proposed method outperforms state-of-the-art counterparts. 1.

Introduction
Deep Neural Networks (DNNs) have achieved outstand-ing success in various tasks, while the success largely re-lies on data with high-quality annotations [13, 55, 41, 19].
In many real-world scenarios, it would be quite difficult to collect large-scale accurately labeled data, which could in-evitably contain noisy labels. Unfortunately, previous stud-ies [1, 56] showed that DNNs can easily overfit random la-bels, resulting in poor generalization performance. There-fore, an increasing number of methods have been proposed for Learning with Noisy Labels (LNL).
One mainstream solution in existing methods of LNL is to train the classifier with confident examples [20, 14, 46, 34, 36], which is based on the memorization effect of DNNs, i.e., DNNs learn example with dominant pat-terns first and then overfit rare ones [1]. Given only noisy data, to exploit the memorization effect, the typical strat-egy starts from a small confident clean dataset and then
*Corresponding authors. gradually expands the dataset, which prevents DNNs from over-fitting noisy data. In general, there are two primary approaches to exploit confident examples in the learning process. The first approach involves identifying examples with high-probability clean labels and training the classi-fier based on these examples, which is commonly referred to as the “small-loss trick” [20, 14, 12, 28, 52]. The sec-ond approach involves controlling the learning process of the classifier to primarily learn high-probability clean ex-amples in noisy datasets, which is commonly referred to as
“early stopping” [37, 42, 17, 25, 3].
Despite providing satisfactory performance, these ap-proaches present an unintended consequence in their meth-ods to prevent over-fitting noise. Specifically, to reduce the impact of mislabeled examples, these approaches limit the capability of the model to effectively learn clean hard ex-amples (CHEs) in noisy datasets, where CHEs are defined as clean examples that are close to the decision boundary, and a significant proportion of CHEs are non-dominated
Identifying CHEs in noisy sub-population examples [9]. data is quite challenging [2, 22], as both the CHEs and the mislabeled examples are often characterized by large losses
[52, 11, 6], causing them to become entangled. To main-tain the purity of the dataset of confident examples, existing
LNL methods [34, 2] normally try to eliminate potential ex-amples that are likely to be mislabeled, which inevitably contain many CHEs. However, it is necessary to consider the positive impact of memorization effects on underrep-resented sub-populations [10] (i.e., CHEs) when learning from natural datasets, as this is crucial for achieving close-to-optimal generalization performance.
In this work, our goal is to enable the classifier to learn as many useful non-dominated sub-population examples in the training set as possible during the process of learning with noisy labels. This entails selecting as many clean examples as possible, particularly the clean hard examples, during the sample selection process. In relation to this context, we in-troduce a novel concept termed First-time k-epoch Learning (FkL), which is defined as the index of the epoch during the training procedure where an example has been predicted to its given label for consecutive k epochs for the first time, as shown in Figure 1(a).
Figure 1. (a) We propose the First-time k-epoch Learning (FkL) metric, which determines the minimum index of the training epoch until which an example has been predicted as its given label for consecutive k epochs. (b) The normalised histogram of CIFAR-10 examples with 40% symmetric label noise w.r.t. the sequence they meet the FkL metric during training procedure. The horizontal axis represents the sequential order in which training examples meet the FkL metric. The vertical axis represents the normalised histogram of examples. (c) Rather than the methods that start from a small yet clean training set (‘Early’ Stopping), our proposed framework starts from a large training set (‘Late’ Stopping) that retains as many clean examples as possible.
Using First-time k-epoch Learning (FkL) as a metric, we sequence the examples in the noisy dataset according to the order they meet the FkL metric during the training proce-dure, as shown in Figure 1(b). As shown in the same figure, most mislabeled examples can only be classified into their given labels for consecutive k epochs in the later stages of training (i.e., larger in the sequence), which implies a rel-atively large FkL value. This observation suggests that the examples with large FkL values (i.e., those examples that are classified to their given labels for consecutive k epochs only in the late training stage) are predominantly those with incorrect labels. Therefore, the FkL values of different training examples can be used to distinguish whether an ex-ample is mislabeled or not.
Motivated by the above observations, we propose a novel method based on reverse thinking of the conventional con-fident example selection strategy. Our proposed method, called Late Stopping, employs an iterative sample selec-tion process that gradually reduces the noise rate of the dataset, leading to a positive feedback loop. Instead of se-lecting high-probability clean examples in the early stage, our method focuses on selecting high-probability misla-beled examples in the late stage to retain as many CHEs as possible in the training set throughout the learning pro-cess, even though this method may lead to the retention of an acceptable level of mislabeled examples, as shown in 1(c). Building on this, we introduce a novel sample se-lection criterion, FkL, which works effectively in select-ing mislabeled examples under the Late Stopping frame-work, and empirical results show that the FkL is more ef-fective than the loss criterion. We evaluate our method on benchmark-simulated and real-world noisy datasets. Em-pirical results demonstrate that the Late Stopping method achieves superior performance compared with state-of-the-art counterparts of learning with noisy labels. 2.