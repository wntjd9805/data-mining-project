Abstract
Traditional and deep Structure-from-Motion (SfM) meth-ods typically operate under the assumption that the scene is rigid, i.e., the environment is static or consists of a sin-gle moving object. Few multi-body SfM approaches ad-dress the reconstruction of multiple rigid bodies in a scene but suffer from the inherent scale ambiguity of SfM, such that objects are reconstructed at inconsistent scales. We propose a depth and camera pose estimation framework to resolve the scale ambiguity in multi-body scenes. Specifi-cally, starting from disorganized images, we present a novel multi-view scale estimator that resolves the camera pose ambiguity and a multi-body plane sweep network that gen-eralizes depth estimation to dynamic scenes. Experiments demonstrate the advantages of our method over state-of-the-art SfM frameworks in multi-body scenes and show that it achieves comparable results in static scenes. The code and dataset are available at https://github.com/ andreadalcin/MultiBodySfM . 1.

Introduction
Real-world environments often contain independently moving objects, and their reconstruction is fundamental for safe robot navigation and augmented reality in complex dy-namic environments. Unfortunately, traditional and deep learning Structure-from-Motion (SfM) algorithms assume that the scene is static and treat moving objects as outliers, missing important information and producing suboptimal results when the scene is dynamic. In Fig. 1-left, the depth reconstructed by the state-of-the-art DeepSfM [36] shows artifacts in the image region of a moving object (in red).
In this work, we go one step further and enable consis-tent 3D reconstruction (Fig. 1-right) of a scene with multi-ple rigidly moving objects. This task, also known as Multi-Body Structure-from-Motion (MBSfM), is still open and far from being solved in practice. Traditional MBSfM at-tempts [1, 26] segment rigid motions to obtain partial re-constructions of individually moving objects in the scene.
However, these methods are hampered by the relative scale problem, namely that the 3D structure of each indepen-Figure 1: Two-views of a multi-body scene (top-row). Depth esti-mation networks (e.g. [36]) yield inconsistent depths (bottom-left) for moving objects. Our method explicitly accounts for moving objects to produce geometrically consistent depths (bottom-right). dently moving object is estimated up to a similarity, so that each object is reconstructed in its own scale. There-fore, a unified reconstruction of all objects up to a common global scale factor is impossible without additional infor-mation. Similarly, deep methods for SfM that leverage ex-plicit multi-view constraints [34, 36, 13, 29, 4] are affected by the relative scale problem and cannot regress a unified depth map of a multi-body scene since their underlying ar-chitecture cannot handle multiple motions.
Recent monocular depth estimators [9, 2] use learned priors to regress dense depth maps from images without exploiting multi-view constraints. Thus, monocular meth-ods implicitly avoid the relative scale problem by regress-ing consistent depth maps across the entire image. How-ever, monocular depth estimation is inherently ill-posed and these models do not provide the same generalization capa-bilities of multi-view methods when scenes are static. Fur-thermore, the 3D structures inferred from different images of the same scene are not guaranteed to be consistent. Thus, this inconsistency eventually manifests in alignment prob-lems that are not trivial to solve.
In this work, we present a novel deep learning frame-work for depth and camera pose estimation designed specif-ically for multi-body scenes. Following consolidated MB-SfM pipelines, we perform motion segmentation to recover a set of relative camera poses up to a scale factor and the corresponding sparse 3D reconstructions of each moving object. Then, we depart from existing MBSfM approaches by directly solving the relative scale problem and regressing geometrically consistent depth maps and refined poses.
Specifically, our main contributions are twofold: (1) a robust scale estimation to fix the scale ambiguity, (2) a multi-body plane sweep network to regress refined camera poses and depth maps in multi-body scenes.
Our scale estimator uses monocular depth maps to unify the 3D structures and camera poses under a global scale fac-tor. This is based on a robust voting scheme that mitigates the impact of inaccuracies in monocular depth estimates.
We adopt a deep neural network for multi-body depth estimation and pose refinement. The network adopts our novel multi-body plane sweep algorithm, which uses all the rigid motions in the scene, now devoid of scale ambigu-ity, to compute dense depth maps and refined camera poses.
Notably, the estimated depth maps are geometrically con-sistent even in image regions covered by moving objects.
Plane sweep has never been extended to support multiple rigid motions to the best of our knowledge. Our network also includes a pose estimation module that refines camera poses based on current depth estimates.
We demonstrate the effectiveness of our approach on both static and multi-body scenes. On static scenes, our method performs on par with traditional and deep SfM methods. On multi-body scenes, it significantly outper-forms SOTA deep learning SfM methods in the depth es-timation task and achieves comparable camera pose results. 2. Problem Formulation
Multi-body depth and camera pose estimation is framed as follows. The input is a set of n unstructured images
{Ii}n i=1 captured by a moving camera and depicting a multi-body scene in which µ objects B = {βi}µ i=1 independently move according to their rigid motions. Without loss of generality, we consider the object β1 to be static w.r.t. to the camera motion. Our goal is to estimate depth maps
{Di}n i=1 for each image Ii and the absolute camera poses
{Ri, ti}n i=1 in the reference frame of β1. We assume that: i) an upper bound M on the maximum number µ of moving objects in the scene is given, ii) the camera is perspective and its intrinsic parameters K are known.
Consider that our problem is different from non-rigid
SfM [16, 14]. Non-rigid SfM handles a wider class of object deformations but does not assume unstructured input im-ages, typically exploiting dense temporal information, and is restricted to orthographic cameras. 3.