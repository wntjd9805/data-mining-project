Abstract
In this paper, we study the problem of semi-supervised 3D object detection, which is of great importance consider-ing the high annotation cost for cluttered 3D indoor scenes.
We resort to the robust and principled framework of self-teaching, which has triggered notable progress for semi-supervised learning recently. While this paradigm is nat-ural for image-level or pixel-level prediction, adapting it to the detection problem is challenged by the issue of pro-posal matching. Prior methods are based upon two-stage pipelines, matching heuristically selected proposals gen-erated in the first stage and resulting in spatially sparse training signals.
In contrast, we propose the first semi-supervised 3D detection algorithm that works in the single-stage manner and allows spatially dense training signals. A fundamental issue of this new design is the quantization er-ror caused by point-to-voxel discretization, which inevitably leads to misalignment between two transformed views in the voxel domain. To this end, we derive and implement closed-form rules that compensate this misalignment on-the-fly. Our results are significant, e.g., promoting Scan-Net mAP@0.5 from 35.2% to 48.5% using 20% annotation.
Codes and data are publicly available1. 1.

Introduction 3D object detection (and reconstruction/tracking) [3, 25, 28, 37, 42, 62] is a fundamental problem in 3D scene under-standing [17, 26, 51, 57, 59, 63], but its progress still lags behind 2D detection due to a high annotation cost. As such, semi-supervised 3D object detection [48, 56, 60] has recently attracted much attention as it holds the promise to improve accuracy using enormous unlabeled data. These semi-supervised 3D detectors are trained with a widely rec-ognized framework called mean teachers (MT) [44]. While semi-supervised image classification [52] and semantic seg-mentation [2] using MT boil down to pairing predictions at 1Code: https://github.com/AIR-DISCOVER/DQS3D
Figure 1: This figure demonstrates the average count of box pairs in representative proposal matching methods SESS
[60], 3DIoUMatch [48] and Proficient Teachers [56]. Our dense matching formulation (DQS3D) allows significantly more box pairs and spatially dense training signals. The x-axis is distorted according to a squared root mapping. the image or pixel level, how to pair predictions between two sets of 3D boxes remains an open question.
This open question is not yet well answered by prior methods [48, 56, 60], as demonstrated by the analysis in
Fig. 1. Shown by the upper three bars, they only exploit a very limited number of box pairs for MT training and we attribute this limitation to the two-stage architecture (i.e.,
VoteNet [37]) they are built upon. VoteNet makes final box predictions using seed proposals extracted by the first stage and only a limited number of proposal pairs are aligned.
Being densely-matched. The emergence of fully convo-lutional 3D detection [39] inspires us to address the afore-mentioned issue using densely matched boxes, and it turns out fruitful. As shown by the lower two bars in Fig. 1, our method allows much more box pairs for MT training even after label filtering. This change leads to spatially dense training signals that translate to notable performance im-In one word, our method predicts provement (Table. 1). one 3D box for each voxel, getting rid of the intermediate proposal generation stage. Thus pairing teacher and stu-dent predictions in a voxel-wise manner becomes a natural choice and this directly leads to dense training signals.
Being quantization-aware. During the development of our densely matched paradigm, we identify a fundamental issue specific to 3D detection: point-to-voxel quantization.
It is widely known that the power of MT is unleashed only with diverse data augmentation [2,14,52] and random trans-formation is a typical augmentation strategy [10, 18, 45] for 3D point cloud. Unfortunately, applying random transfor-mation inevitably leads to a different point-to-voxel map-ping due to the existence of quantization error and a mis-match between teacher and student predictions on each voxel. To this end, we derive a closed-form compensation rule and implement it on-the-fly, which leads to consistent performance gains in various settings.
Highlighting our two technical contributions mentioned above, we name our method DQS3D, which is short for densely-matched quantization-aware semi-supervised 3D detection. Our contributions are summarized as follows:
• We shed light on the superiority of dense matching over proposal matching in semi-supervised 3D object detection, which could not only harvest more pseudo labels but also improve the pseudo-label quality.
• We propose the first framework for densely-matched quantization-aware semi-supervised 3D object detec-tion, where we point out the problem of quantization error and come up with an on-the-fly fix to it.
• We conduct extensive experiments on public datasets and achieve significant results. For example, DQS3D scores 48.5% mAP@0.5 on ScanNet using 20% data while the best published result is 35.2%. 2.