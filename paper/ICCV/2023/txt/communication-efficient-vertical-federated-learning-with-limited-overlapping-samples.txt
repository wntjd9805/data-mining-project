Abstract
Federated learning is a popular collaborative learning approach that enables clients to train a global model without sharing their local data. Vertical federated learning (VFL) deals with scenarios in which the data on clients have dif-ferent feature spaces but share some overlapping samples.
Existing VFL approaches suffer from high communication costs and cannot deal efficiently with limited overlapping samples commonly seen in the real world. We propose a prac-tical VFL framework called one-shot VFL that can solve the communication bottleneck and the problem of limited over-lapping samples simultaneously based on semi-supervised learning. We also propose few-shot VFL to improve the accuracy further with just one more communication round between the server and the clients. In our proposed frame-work, the clients only need to communicate with the server once or only a few times. We evaluate the proposed VFL framework on both image and tabular datasets. Our methods can improve the accuracy by more than 46.5% and reduce the communication cost by more than 330× compared with state-of-the-art VFL methods when evaluated on CIFAR-10.
Our code is available at https://nvidia.github. io/NVFlare/research/one-shot-vfl. 1.

Introduction
Federated Learning (FL) is a distributed learning method that enables multiple parties to collaboratively train a model without centralizing their raw data. Therefore, the clients can retain control over their own data assets. FL has received significant attention and has become a major research topic due to its capability to build real-world applications where datasets are isolated across different organizations/devices while preserving data governance and privacy [18, 14].
Existing approaches primarily focus on horizontal feder-Figure 1. An example of data splitting in a two-client VFL setting. ated learning (HFL), where the data from different clients share the same feature space but have different samples [32].
One application of HFL is that smartphone users collabo-ratively train a next-word prediction model for the smart keyboard [10]. In HFL, the clients are expected to learn common knowledge from heterogeneous data distributions and produce a global model by aggregating the updates of local models. Hence, the main challenge of HFL is data distribution heterogeneity and under cross-device scenarios, limited computation resources.
Vertical federated learning (VFL), on the other hand, focuses on scenarios in which the data on clients have different feature spaces but share some overlapping sam-ples [32]. In addition, the true labels can reside on a third-party server [25] as shown in Fig. 1. For example, a credit bureau collaborates with an e-commerce company and a bank to train a model to estimate a user’s credit score. In this case, only the credit bureau has the credit score of the users which will not be shared with the e-commerce com-pany and the bank. VFL is mostly deployed in cross-silo scenarios, and the computation power is usually not a major concern [14]. However, VFL faces two unique challenges.
First, VFL requires the clients to communicate with the server for each iteration (rather than after several epochs under HFL) of training, which introduces extremely high
communication costs. It is also notable that iterative commu-nications require reliable communication channels between the server and the clients, which is usually expensive. In addition to the high communication cost, the other major challenge of VFL is that the number of overlapping sam-ples may be limited. For example, two hospitals in different countries are not expected to have a large number of overlap-ping patients. The model trained with limited overlapping samples likely cannot achieve reliable performance.
Furthermore, VFL is currently not as well explored as
HFL. Some existing works can reduce the communication cost by reducing the communication frequency or compress-ing the communicated data [20]. However, most methods only achieve limited reduction from one local update to mul-tiple while still requiring heavy iterative communications.
Other works focus on improving the performance with lim-ited overlapping samples [15, 30]. Notably, both challenges are bottlenecks of applying VFL in realistic scenarios and leaving either one unsolved hinders the deployment of VFL in the real world. To the best of our knowledge, there is no work aiming at solving these two challenges simultaneously.
In this paper, we propose one-shot VFL, which is a communication-efficient VFL algorithm that can achieve high performance with minimal overlapping samples. In one-shot VFL, the clients are guided to conduct local semi-supervised learning (SSL) using both the overlapping sam-ples and the unaligned samples to train well-performing feature extractors. Under one-shot setting, the clients only need to conduct two upload operations and one download operation for the training session, which drastically reduces the communication cost and frequency. We further propose few-shot VFL as an extension of one-shot VFL. Few-shot
VFL expands the supervised dataset on clients to improve the performance of the local feature extractors. Compared with one-shot VFL, clients in few-shot VFL conduct one more time of uploading and downloading but can achieve better performance, especially when the number of overlapping samples is small.
Our key contributions are summarized as follows:
• We propose a communication-efficient VFL algorithm called one-shot VFL. To the best of our knowledge, one-shot VFL is the first algorithm that can simultaneously address the challenges of high communication cost and limited overlapping samples.
• We propose few-shot VFL that can improve the perfor-mance further under settings with minimal overlapping samples. 46.5% and reduce the communication cost by more than 330× compared with the state-of-the-art (SOTA) VFL methods on CIFAR-10. 2.