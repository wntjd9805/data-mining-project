Abstract
Listener head generation centers on generating non-verbal behaviors (e.g., smile) of a listener in reference to the information delivered by a speaker. A significant challenge when generating such responses is the non-deterministic na-ture of fine-grained facial expressions during a conversation, which varies depending on the emotions and attitudes of both the speaker and the listener. To tackle this problem, we propose the Emotional Listener Portrait (ELP), which treats each fine-grained facial motion as a composition of several discrete motion-codewords and explicitly models the proba-bility distribution of the motions under different emotion in conversation. Benefiting from the “explicit” and “discrete” design, our ELP model can not only automatically generate natural and diverse responses toward a given speaker via sampling from the learned distribution but also generate controllable responses with a predetermined attitude. Under several quantitative metrics, our ELP exhibits significant improvements compared to previous methods. 1.

Introduction
Listener Head Generation (LHG) technology aims to syn-thesize the motion of the listener in response to the speaker.
In contrast to speaker head generation (SHG) [10, 44, 21, 13, 35, 47, 9, 53, 45], which focuses on generating lip-speech synchronized portrait videos, LHG analyzes the talk-ing semantics of the speaker automatically, without explicit guidance, to synthesize corresponding interactive motions of the listener. As shown in Figure 1, the listener reacts positively when the speaker shares happy, and vice versa.
LHG can be employed in many applications, e.g. human-computer interaction [57, 25, 60], virtual reality [22, 24], metaverse [8, 7, 46] and media forensics [42, 37, 43, 17] etc.
The distinct nature of LHG, which necessitates a compre-hensive modeling of the speaker’s motion [32], presents a significant hurdle in yielding realistic listener head. In the absence of audio-to-mouth matching evaluation, audiences are more inclined to discern subtle changes in facial expres-sions and head movements. However, the existing meth-ods Responsive Listening Head Generation (RLHG) [61] and Learning2Listen [31] have ignored these key compo-nents. Specifically, RLHG [61] has replicated the regres-sion experience from SHG [10], which weakens the non-deterministic properties and smoothes the listener motion.
Meanwhile, although the motion categories in codebook pro-posed by Learning2Listen [31] alleviate this problem, the one-dimensional codebook from VQ-VAE [29] limits the
diversity of motion and emotional representation. Conse-quently, different emotional states are intricately intertwined within a single codebook, and the generated listener emotion tends to be the biased emotion in the training set. More-over, neither method can simulate fine-grained facial motion under different emotions, such as minute alterations in the motion surrounding the eyes and movements of the mouth.
To explore the solution toward superior LHG results, we focus on the two unresolved hurdles: (1) how to simulate finer-grained listener movements, including the head motion and expression details of the face, and (2) how to explicitly model emotions in the discrete space.
In this paper, we propose a novel method called
Emotional Listener Portrait (ELP) for vivid listener head video generation. The visual and audio information from the speaker is combined together for the listener motion synthe-sis (the right part of Figure 1). (1) To overcome the limita-tion imposed by one codeword search, we have expanded the classification dimensions to facilitate the mapping of listener motion onto a higher-dimensional discrete space. The fine-grained listener movements correspond in high-dimensional discrete space, which offers greater capacity for the precise depiction of the listener’s facial expression and head pose than a single codeword. (2) Despite the expanded latent space on the codeword, explicit emotional representation remains unattainable. As such, building upon the increased space, we leverage emotion priors to split and rearrange the discrete space. More specifically, different emotions are rear-ranged into corresponding spaces, with the distance between these spaces being determined by the value range of code-word, shown in the middle of Figure 1 (it takes the ternary emotion as an example). The listener features (blink, facial and head motion) from the spaces with emotion are decoded into different emotional listeners, shown in the right part of
Figure 1. It indicates that the distance between listeners with different emotions is widened, such as listeners in positive respone by smiling, while listeners in negative tend to frown.
There are two modules in the ELP, the Adaptive Space
Encoder (ASE) and Mesh-to-Video Renderer. In ASE, the discrete latent space obtained by the one-hot vectors argu-ment maximum is concatenated and weighted according to the position with the prior emotion. Employing this approach leads to a further enlarge in the probability distribution dis-tance, and the listener motion coefficients are learned from that. The Mesh-to-Video Renderer renders the photorealistic face from the mesh corresponding to the predicted coeffi-cients with only a single portrait image of the listener.
We demonstrate the ability of our method through quan-titative and qualitative experiments on two popular con-versation portraits datasets, the ViCo [61] and the large-scale in the wild conversation videos collected by the Learn-ing2Listen [31]. Our contributions are summarized: – We propose a novel framework called ELP for emo-tional listener head generation in dynamic conversion, which can improve the fidelity of the fine-grained gen-erated listener with facial expression, head pose and blink etc. – We introduce the Adaptive Space Encoder (ASE) to rearrange the latent space based on emotional priors to obtain more explicit emotional representation. – Extensive experiments demonstrate that our method outperforms most existing methods in quantitative and qualitative results. 2.