Abstract
Continual learning aims to emulate the human abil-ity to continually accumulate knowledge over sequential tasks. The main challenge is to maintain performance on previously learned tasks after learning new tasks, i.e., to avoid catastrophic forgetting. We propose a Channel-wise
Lightweight Reprogramming (CLR) approach that helps convolutional neural networks (CNNs) overcome catas-trophic forgetting during continual learning. We show that a CNN model trained on an old task (or self-supervised proxy task) could be “reprogrammed” to solve a new task by using our proposed lightweight (very cheap) reprogram-ming parameter. With the help of CLR, we have a bet-ter stability-plasticity trade-off to solve continual learning problems: To maintain stability and retain previous task ability, we use a common task-agnostic immutable part as the shared “anchor” parameter set. We then add task-specific lightweight reprogramming parameters to reinter-pret the outputs of the immutable parts, to enable plas-ticity and integrate new knowledge. To learn sequential tasks, we only train the lightweight reprogramming param-eters to learn each new task. Reprogramming parameters are task-specific and exclusive to each task, which makes our method immune to catastrophic forgetting. To mini-mize the parameter requirement of reprogramming to learn new tasks, we make reprogramming lightweight by only ad-justing essential kernels and learning channel-wise linear mappings from anchor parameters to task-specific domain knowledge. We show that, for general CNNs, the CLR pa-rameter increase is less than 0.6% for any new task. Our method outperforms 13 state-of-the-art continual learning baselines on a new challenging sequence of 53 image clas-sification datasets. Code and data are in the top link. 1.

Introduction
Continual Learning (CL) focuses on the problem of learning from a stream of data, where agents continually extend their acquired knowledge by sequentially learning new tasks or skills, while avoiding forgetting of previous
Figure 1. Equipped with a task-agnostic immutable CNN model, our approach ”reprogram” the CNN layers to each new task with lightweight task-specific parameters (less than 0.6% of the original model) to learn sequences of disjoint tasks, assuming data from previous tasks is no longer available while learning new tasks. tasks [40]. In the literature, CL is also referred to as lifelong learning [10, 2, 40] and sequential learning [4]. This differs from standard train-and-deploy approaches, which cannot incrementally learn without catastrophic interference across tasks [21]. How to avoid catastrophic forgetting is the main challenge of continual learning, which requires that the per-formance on previous learned tasks should not degrade sig-nificantly over time when new tasks are learned. This is also related to a general problem in neural network design, the stability-plasticity trade-off [23], where plasticity repre-sents the ability to integrate new knowledge, and stability refers to the ability to retain previous knowledge [13].
Dynamic Network methods have been shown to be among the most successful ones to solve continual learn-ing, which usually shows great stability on previous tasks and alleviates the influence of catastrophic forgetting by dy-namically modify the network to solve new tasks, usually by network expansion [40, 56, 12, 57]. For stability, a de-sirable approach would be to fix the backbone and learn ex-tra task-specific parameters on top of it, which will have no catastrophic forgetting. However, the number of parameters in such methods can quickly become very large. How to re-duce the amount of required extra parameters is still a chal-lenging problem. To solve the above issues, our approach is based on three motivations: (1) Reuse instead of re-learn. Adversarial Reprogram-ming [16] is a method to “reprogram” an already trained and frozen network from its original task to solve new tasks by perturbing the input space without re-learning the net-work parameters.
It computes a single noise pattern for each new task. This pattern is then added to inputs for the new task and fed through the original network. The origi-nal network processes the combined input + noise and gen-erates an output, which is then remapped onto the desired new task output domain. For example, one pattern may be computed to reprogram a network originally trained on Im-ageNet [46] to now solve MNIST [14]. The same pattern, when added to an image of an MNIST digit, would trigger different outputs from the ImageNet-trained network for the different MNIST classes (e.g., digit 0 + pattern may yield
“elephant”; digit 1 + pattern “giraffe”, etc). These outputs can then be re-interpreted according to the new task (ele-phant means digit 0, etc). Although the computation cost is prohibitively high compared to baseline lifelong learning approaches, here we borrow the reprogramming idea; but we conduct more lightweight yet also more powerful re-programming in the parameter space of the original model, instead of in the input space. (2) Channel-wise transformations may link two different kernels. GhostNet [24] could generate more feature maps from cheap operations applied to existing feature maps, thereby allowing embedded devices with small memory to run effectively larger networks. This approach is motivated by near redundancy in standard networks: after training, several learned features are quite similar. As such, Han et al.
[24] generate some features as linear transformations of other features. Inspired by this, our approach augments a network with new, linearly transformed feature maps, which can cheaply be tailored to individual new tasks. (3) Lightweight parameters could shift model distribu-tion. BPN [56] adds beneficial perturbation biases in the fully connected layers to shift the network parameter dis-tribution from one task to another, which is helpful to solve continual learning. This is cheaper than fine-tuning all the weights in the network for each task, instead tun-ing only one bias per neuron. Yet, this approach provided good lifelong learning results. However, the method could only handle fully connected layers and the performance is bounded by the limited ability of the bias parameters to change the network (only 1 scalar bias for each neuron).
Our method instead designs more powerful reprogramming patterns (kernels) for the CNN layers, which could lead to better performance on each new task.
Drawing from these three ideas, we propose channel-wise lightweight reprogramming (CLR). We start with task-agnostic immutable parameters of a CNN model pretrained on a relatively diverse dataset (e.g., ImageNet-1k, Pascal
VOC, ...) if possible, or on a self-supervised proxy task, which requires no semantic labels. We then adaptively “re-program” the immutable task-agnostic CNN layers to adapt and solve new tasks by adding lightweight channel-wise lin-ear transformation on each channel of a selected subset of
Convolutional layers (Fig 2). The added reprogramming pa-rameters are 3x3 2D convolutional kernels, each working on separate channels of feature maps after the original convo-lutional layer. CLR is very cheap but still powerful, with the intuition that different kernel filters could be reprogrammed with a task-dependent linear transformation.
The main contributions of this work are:
• We propose a novel continual learning solution for
CNNs, which involves reprogramming the CNN lay-ers trained on old tasks to solve new tasks by adding lightweight task-specific reprogramming parameters.
This allows a single network to learn potentially un-limited input-to-output mappings, and to switch on the fly between them at runtime.
• Out method achieves better stability-plasticity balance compared to other dynamic network continual learn-ing methods: it does not suffer from catastrophic for-getting problems and requires limited extra parameters during continual learning, which is less than 0.6% of the original parameter size for each new task.
• Our method achieves state-of-the-art performance on task incremental continual learning on a new challeng-ing sequence of 53 image classification datasets. 2.