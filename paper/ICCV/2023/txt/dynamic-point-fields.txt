Abstract
Recent years have witnessed significant progress in the field of neural surface reconstruction. While extensive focus was put on volumetric and implicit approaches, a number of works have shown that explicit graphics primitives, such as point clouds, can significantly reduce computational com-plexity without sacrificing the reconstructed surface quality.
However, less emphasis has been put on modeling dynamic surfaces with point primitives. In this work, we present a dy-namic point field model that combines the representational benefits of explicit point-based graphics with implicit de-formation networks to allow efficient modeling of non-rigid 3D surfaces. Using explicit surface primitives also allows us to easily incorporate well-established constraints such as isometric-as-possible regularization. While learning this deformation model is prone to local optima when trained in a fully unsupervised manner, we propose to also leverage semantic information, such as keypoint correspondence, to guide the deformation learning. We demonstrate how this approach can be used for creating an expressive animat-able human avatar from a collection of 3D scans. Here, previous methods mostly rely on variants of the linear blend skinning paradigm, which fundamentally limits the expres-sivity of such models when dealing with complex cloth ap-pearances, such as long skirts. We show the advantages of our dynamic point field framework in terms of its rep-resentational power, learning efficiency, and robustness to out-of-distribution novel poses. The code for the project is publicly available 1. 1.

Introduction
Neural surface reconstruction and rendering have been subject to giant leaps in the last couple of years [81]. While original works have suffered from the representational limi-tations stemming from using a single global neural network to represent a surface as implicit signed distance [4, 52, 73] or occupancy field [43], recent models pushed the level of 1sergeyprokudin.github.io/dpf
Figure 1. Dynamic point field. We propose to model dynamic surfaces with a point-based model, where the motion of a point pi over time is represented by an implicit deformation field g(t)
Î¸ .
Working directly with points rather than SDFs allows us to eas-ily incorporate various well-known deformation constraints, e.g. as-isometric-as-possible [23]. We showcase the usefulness of this approach for creating animatable avatars in complex clothing. reconstruction quality by introducing auxiliary data struc-tures such as octrees [80] or hashmaps [46]. While pro-viding high level of accuracy, these lines of work rely on the ability to perform large amounts of queries to the un-derlying multi-layer perceptrons (MLPs) to reconstruct the surface.
In the field of volumetric neural rendering, this restric-tion has motivated development of compact and computa-tionally attractive models that can train quickly and ren-der at interactive rates [6, 11, 36, 67, 91]. Among them, point-based methods for neural rendering [1, 26, 66, 68, 86, 87, 93] proved to be scalable alternatives to purely implicit or volumetric approaches. In the context of differentiable neural surface modeling, points have also been explored for static scenes [57, 90] and dynamic human modeling [42].
This work aims to achieve three goals. First, we show-case the advantage of a point-based surface representation compared to the most recent implicit models [46]. We do this by carefully analyzing the behavior of the existing frameworks when representing complex 3D meshes with
Figure 2. Overview of the main results. (a): static surface reconstruction with optimised point clouds. We propose a point cloud optimisation scheme that efficiently utilises available off-the-shelf point renderers [20, 86] and surpasses the performance of more sophisticated methods
[46, 80], with an explicit surface representation which requires zero inference time. (b-e): we extend the approach to allow modeling dynamic surfaces of varying topology and complexity, and propose a guided learning for the case of complex deformations. Our formulation also allows us to interpolate easily between canonical and target surfaces. perfect ground truth available. Here, point clouds as primi-tives demonstrate direct benefits over their implicit alterna-tives in all the comparison dimensions: reconstructed ge-ometry quality, training time, runtime, and model size.
Second, we extend the point-based surface model to support non-rigidly deforming surfaces. A number of ap-proaches has been proposed to tackle this problem, both for generic 3D scenes [19, 49] and human-specific mod-els [42, 92]. However, these models often tend to constrain the space of deformations via restrictions to specific alge-braic operations, commonly linear blend skinning functions
[42, 89] or a mixture of affine warps [19, 37]. Instead, we rely on a deformation field represented by a neural network
[51], similar to the approach taken in dynamic neural ra-diance fields [53, 64]. Again, we show that learning such deformation networks on point sets is easier and more ro-bust compared to SDFs. More importantly, it also allows us to easily incorporate various well-known regularisation techniques, such as as-isometric-as-possible [17, 23], en-forcing the preservation of distances between points in the canonical and deformed space.
Further, we propose a guided learning regime to improve the robustness of the dynamic point fields and allow 3D shape manipulation with our framework. Our key idea is the following: undesirable local optima can be avoided by ex-ploiting sparse correspondences in space-time as additional constraints, further improving optimisation speed and sta-bility. For instance, to reconstruct a highly dynamic and complex deformation of a long skirt (Figure 5), we super-vise the learning of the deformation field with the body vertices correspondences between frames from the read-ily available underlying unclothed body model registrations
[42]. As demonstrated in the experiments section, the cor-respondence constraints on the minimally-clothed bodies provide strong generalization capabilities on the deforma-tion of a long skirt largely deviating from the body surface.
Based on this observation, we propose a method for zero-shot avatar reposing based on the introduced deformation framework. Our method compares favorably to other point-based [40, 42] and implicit methods [70] which are funda-mentally based on the linear blend skinning paradigm and fail to correctly represent challenging cloth types.
To summarise, our main contributions are as follows. We first introduce dynamic point fields, a simple and compu-tationally attractive model combining the compactness and efficiency of point primitives with the flexibility and accu-racy of neural networks to model deformations. Advantages of the proposed approach are demonstrated by comparing it with the state-of-the-art in various surface reconstruc-tion and deformation learning tasks. Second, we propose a
guided deformation field learning and show how additional constraints on available keypoint correspondences can be used to efficiently guide the learning of surface dynamics, without sacrificing the expressivity of a model. 2.