Abstract
To enable progress towards egocentric agents capable of understanding everyday tasks speciﬁed in natural language, we propose a benchmark and a synthetic dataset called
Egocentric Task Veriﬁcation (EgoTV). The goal in EgoTV is to verify the execution of tasks from egocentric videos based on the natural language description of these tasks. EgoTV contains pairs of videos and their task descriptions for multi-step tasks – these tasks contain multiple sub-task de-compositions, state changes, object interactions, and sub-task ordering constraints. In addition, EgoTV also provides abstracted task descriptions that contain only partial details about ways to accomplish a task. Consequently, EgoTV requires causal, temporal, and compositional reasoning of video and language modalities, which is missing in existing datasets. We also ﬁnd that existing vision-language models struggle at such all round reasoning needed for task veriﬁ-cation in EgoTV. Inspired by the needs of EgoTV, we pro-pose a novel Neuro-Symbolic Grounding (NSG) approach that leverages symbolic representations to capture the com-positional and temporal structure of tasks. We demonstrate
NSG’s capability towards task tracking and veriﬁcation on our EgoTV dataset and a real-world dataset derived from
CrossTask [82] (CTV). We open-source the EgoTV and CTV datasets and the NSG model for future research on egocen-tric assistive agents. 1.

Introduction
Inspired by recent progress in visual systems [40, 65], we consider an assistive egocentric agent capable of rea-soning about daily activities. When invoked via natural lan-guage commands, for e.g., while baking a cake, the agent understands the steps involved in baking, tracks progress through the various stages of the task, detects and proac-tively prevents mistakes by making suggestions. Such a vir-*Work done while interning at Meta. tual agent [11] would empower users to learn new skills and accomplish tasks efﬁciently.
Developing this egocentric agent capable of tracking and verifying everyday tasks based on their natural language speciﬁcation is challenging for multiple reasons. First, such an agent must reason about various ways of doing a multi-step task speciﬁed in natural language. This entails de-composing the task into relevant actions, state changes, ob-ject interactions as well as any necessary causal and tem-poral relationships between these entities. Secondly, the agent must ground these entities in egocentric observations to track progress and detect mistakes. Lastly, to truly be useful, such an agent must support tracking and veriﬁca-tion for a combination of tasks and, ideally, even unseen tasks. These three challenges – causal and temporal rea-soning about task structure from natural language, visual grounding of sub-tasks, and compositional generalization – form the core goals of our work.
As our ﬁrst contribution, we propose a benchmark –
) – and a corre-Egocentric Task Veriﬁcation (EgoTV sponding dataset in the AI2-THOR [29] simulator. Given a natural language (NL) task description and a correspond-ing egocentric video of an agent, the goal of EgoTV is to verify whether the task was successfully completed in the video or not. EgoTV contains multi-step tasks with or-dering constraints on the steps and abstracted NL task de-scriptions with omitted low-level task details inspired by the needs of real-world assistants. We also provide splits of the dataset focused on different generalization aspects, e.g., un-seen visual contexts, compositions of steps, and tasks (see
Figure 1). Consequently, EgoTV dataset provides the ﬁne-grained control necessary for rigorous testing and reﬁne-ment of task reasoning models, which is often missing in real-world datasets [17, 10]. Yet, EgoTV mirrors the real world by leveraging visual photo-realism and task diversity.
Our second contribution is a novel approach for order-aware visual grounding – Neuro-Symbolic Grounding (NSG), capable of compositional reasoning and general-izing to unseen tasks owing to its ability to leverage ab-heat_then_clean (apple) heat_then_slice (apple) microwave sinkbasin sinkbasin apple apple stoveburner apple knife apple apple fridge fridge apple slice
NL Description
Task Verified apple is positive heated 
, then cleaned in a sinkbasin
NL Description
Task Verified apple is cooled in a fridge after negative slicing
Novel Tasks
Novel Steps
Novel Scenes train: clean(apple), cool(apple) test: clean_and_cool(apple) train: clean(apple), cool(egg) test: clean(egg), cool(apple)      train: Scenes [1, 25]      test: Scenes [26, 30]
Abstraction           train: apple is heated, cleaned in a                  sinkbasin, and placed in a plate. test: hot, clean apple is placed
Figure 1. EgoTV benchmark. A positive example [Left] and a negative example [Right] from the train set along with illustrative examples from the test splits [Bottom] of EgoTV are shown. The test splits are focused on generalization to novel compositions of tasks, unseen sub-tasks or steps and scenes, and abstraction in NL task descriptions. The bounding boxes are solely for demonstration purposes and are not used during training/inference. stract NL descriptions along with compositional and tem-poral structure of tasks (task decomposition, ordering). In contrast, state-of-the-art vision-language models [76, 50, 36, 3] struggle to ground NL descriptions in egocentric videos, and do not generalize to unseen tasks. NSG out-performs these models by 33.8% on compositional gener-alization and 32.8% on abstractly described task veriﬁca-tion. Finally, to evaluate NSG on real-world data, we in-stantiate EgoTV on the CrossTask [82] instructional video dataset. We ﬁnd that it also outperforms state-of-the-art models at task veriﬁcation on CrossTask. We hope that the
EgoTV benchmark and dataset will enable future research on egocentric agents capable of aiding in everyday tasks. 2.