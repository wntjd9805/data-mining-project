Abstract
Generating photorealistic images with controllable cam-era pose and scene contents is essential for many applica-tions including AR/VR and simulation. Despite the fact that rapid progress has been made in 3D-aware generative mod-els, most existing methods focus on object-centric images and are not applicable to generating urban scenes for free camera viewpoint control and scene editing. To address this challenging task, we propose UrbanGIRAFFE, which uses a coarse 3D panoptic prior, including the layout distribu-tion of uncountable stuff and countable objects, to guide a 3D-aware generative model. Our model is compositional and controllable as it breaks down the scene into stuff, ob-jects, and sky. Using stuff prior in the form of semantic voxel grids, we build a conditioned stuff generator that ef-fectively incorporates the coarse semantic and geometry in-formation. The object layout prior further allows us to learn an object generator from cluttered scenes. With proper loss functions, our approach facilitates photorealistic 3D-aware image synthesis with diverse controllability, including large camera movement, stuff editing, and object manipulation.
We validate the effectiveness of our model on both synthetic and real-world datasets, including the challenging KITTI-360 dataset. 1.

Introduction
Generating photorealistic urban scenes has many appli-cations in simulation, gaming and virtual reality. Unfortu-nately, designing diverse urban scenes with novel 3D visual content is typically expensive and time-consuming as it re-quires the expertise of professional artists.
Recent advances in generative models have demon-strated a promising direction to reduce the cost via learning to generate images from data. Ideally, the generated scenes should be controllable in terms of camera pose and 3D con-*Corresponding author.
Figure 1: Illustration. UrbanGIRAFFE generates a photo-realistic image given a sampled panoptic prior in the form of a semantic voxel grid and object layout. Our method enables diverse controllability regarding camera pose, in-stance, and stuff. tent. For example, the camera should be able to move freely in the scene with six degrees of freedom. The poses of in-stantiated objects (e.g., cars) should be able to be manipu-lated independently. Furthermore, the layout of the scene should be controllable.
There are many attempts to generate photorealistic ur-ban images. Several methods study semantic image synthe-sis to transfer a 2D semantic segmentation map to an RGB urban scene image [25, 53, 57]. However, when chang-ing the camera poses, the generated images across multi-ple frames may not be consistent using such 2D generative models. Recently, 3D-aware generative models have wit-nessed rapid progress by lifting the generation process to the 3D space. Despite achieving multi-view consistency, most existing 3D-aware generative models are limited to object-centric images, e.g., faces and cars [58, 10, 9]. There are a few attempts to generate scene images in a compositonal manner [36, 49, 47, 69]. However, all these methods strug-gle to learn a good geometry of the background and hence
do not support large camera movement, e.g., moving the camera along the road. Another line of work enables cam-era movement but ignores the compositional nature of the scene, thus lacking controllability of the 3D content [14, 2].
In this paper, we propose UrbanGIRAFFE to address the challenging task of compositional and controllable 3D-aware image synthesis of urban scenes, see Fig. 1. Our key idea is to leverage scene-level but coarse 3D panoptic prior, simplifying the task of learning complex geometry through 2D supervision and incorporating semantic information for scene editing. The panoptic prior, including semantic voxel grids of uncountable stuff and bounding boxes of countable objects, can be obtained from existing datasets [37] or in-ferred from pre-trained models [8]. Specifically, our model represents the scene as compositional neural feature fields consisting of stuff, objects, and sky. We propose a semantic voxel-conditioned stuff generator, effectively preserving the semantic and geometry information provided by the prior.
In terms of objects, we follow GIRAFFE [49] to generate objects in canonical space by leveraging the object layout prior. We further model the sky and far regions using a sky generator. With all three generators, we render a compos-ited feature map via volume rendering and upsample it to the target image using a neural renderer. For the compli-cated urban scenes, we observe that training with an adver-sarial loss on the full image alone is insufficient. We addi-tionally employ an adversarial loss applied to objects and a reconstruction loss to the stuff image regions to improve the image fidelity.
Our contributions are as follows. i) We propose to study a novel yet challenging task of 3d-aware urban generative models with diverse controllability in terms of large cam-era movement, object manipulation and stuff editing. ii) We leverage coarse 3D panoptic prior to address this challeng-ing task and design compositional generative radiance fields that leverage the prior information effectively. iii) With our carefully design the training objectives, our method demon-strates state-of-the-art performance compared to existing methods on both synthetic and real-world datasets, includ-ing the challenging KITTI-360 dataset. 2.