Abstract
We present CO-Net, a cohesive framework that optimizes multiple point cloud tasks collectively across heterogeneous dataset domains. CO-Net maintains the characteristics of high storage efficiency since models with the preponderance of shared parameters can be assembled into a single model.
Specifically, we leverage residual MLP (Res-MLP) block for effective feature extraction and scale it gracefully along the depth and width of the network to meet the demands of dif-ferent tasks. Based on the block, we propose a novel nested layer-wise processing policy, which identifies the optimal architecture for each task while provides partial sharing pa-rameters and partial non-sharing parameters inside each layer of the block. Such policy tackles the inherent chal-lenges of multi-task learning on point cloud, e.g., diverse model topologies resulting from task skew and conflicting gradients induced by heterogeneous dataset domains. Fi-nally, we propose a sign-based gradient surgery to pro-mote the training of CO-Net, thereby emphasizing the usage of task-shared parameters and guaranteeing that each task can be thoroughly optimized. Experimental results reveal that models optimized by CO-Net jointly for all point cloud tasks maintain much fewer computation cost and overall storage cost yet outpace prior methods by a significant mar-gin. We also demonstrate that CO-Net allows incremental learning and prevents catastrophic amnesia when adapting to a new point cloud task. 1.

Introduction
With the substantial breakthroughs in deep neural net-works, modern architectures deliver significant enhance-ments in the realm of point cloud analysis [27, 28, 37, 42, 61], such as 3D point classification, 3D point segmentation,
*Corresponding author. and 3D point detection, etc. Nonetheless, these methods are inefficient when conducting multiple tasks since they are generally designed to execute singular task. While paral-lel computing can alleviate this dilemma, it may introduce additional overheads, such as memory volumes and storage expenses that increase proportionately with the quantity of tasks, which is prohibitively expensive for cutting-edge de-vices with limited resources (e.g., mobile devices).
Multi-task learning (MTL) [3, 14, 47, 15] provides a
In visual tasks, MTL methods remedy for this problem. have primarily been introduced to jointly accomplish depth estimate task, surface normal estimate task, and semantic segmentation task from a single RGB image [16, 19, 59].
An MTL model is capable of delivering advantages in terms of complexity, inference time, and learning efficiency due to the fact that a major portion for the network can be shared across all tasks. Nonetheless, training multiple tasks con-currently for point cloud presents two critical obstacles: i) As opposed to typical visual tasks, in which a back-bone that executes admirably for image classification task can be effortlessly fine-tuned to other vision tasks, using the same backbone to jointly optimize all point cloud tasks may result in suboptimal solution for some tasks. Thus, it is preferable to find an optimal backbone for each point cloud task under resource constraint. ii) We endeavour to operate multiple point cloud tasks concurrently by taking heterogeneous dataset domains as input rather than a regular multi-task dataset. Consequently, the gradients of different tasks will arise substantial discrep-ancies in the directions under multi-task learning settings, a phenomenon known as negative transfer [35].
To tackle the first challenge, we leverage residual MLP (Res-MLP) block, a basic point feature extraction block that can accommodate the requirements of various tasks in terms of the depth and width of the model architecture.
Based on Res-MLP, inspired by slimmable neural networks
[55, 54, 56, 4], we introduce a novel nested layer-wise pro-cessing policy that progressively handles the weight of each layer of the network, that is, using NAS technique to find the optimal architecture for all tasks in terms of model struc-ture and offering fine-grained parameter sharing adaptively within the model. Compared with recent Poly-PC [53] that enables various tasks to share their common parts for a cer-tain layer while sacrificing the flexibility of sharing, the central idea of our proposed nested layer-wise processing policy is that: (1) entangling the weights of multiple tasks within the same layer, empowering us to find optimal archi-tectures for all tasks and achieve parameter sharing inside each layer, (2) transforming the parameter sharing of the backbone into a learnable problem so that deciding which parameters of the backbone to be shared or not can be done after training.
For negative transfer, we primarily consider conflicting directions of the gradients across various tasks, which pro-duces a more significant impact than differences in gradient magnitude, typically for point cloud tasks, as illustrated in
Table 8. Specifically, due to the manner in which gradients of various tasks are added together, gradients of multiple tasks can wipe each other out once they point to opposing directions of the parameter space, resulting in a crappy up-date direction for a subset or all tasks. Furthermore, CO-Net is developed for jointly streamlining multiple 3D point tasks over heterogeneous dataset domains, in which the diverse dataset domains could exacerbate such conflicting. Only very recently a few of works begin to offer ways for mitigat-ing the conflicting gradients problem, such as eliminating conflicting portions of the gradients [57] or randomly ‘drop-ping’ pieces of the gradient vector [6].
In this work, we propose a sign-based gradient surgery that homogenizes the gradient direction of the task-shared parameters by leverag-ing a sign-mask manner. In this way, our proposed gradient surgery emphasizes the usage of task-shared parameters and guarantees that each task can be fully trained.
Over the well-optimized CO-Net, we perform an evolu-tionary searching under resource constraints to identify op-timal architectures for diverse 3D point tasks. Experimental results indicate that the searched CO-Net for different tasks outperform a number of baselines and can be comparable with current state-of-the-art works optimized individually for specific tasks, as illustrated in Table 1, Table 2, and Ta-ble 3. Besides, we demonstrate that CO-Net permits in-cremental learning and prevents catastrophic amnesia when adapting to a new point cloud task, as shown in Table 10.
Hence, CO-Net is designed to be parameter-efficient and can scale more smoothly as task numbers grow.
To summarize, the contributions of this work are as fol-lows: 1) We propose CO-Net, a unified framework that op-timizes multiple point cloud tasks collectively under various dataset domains. 2) We propose a nested layer-wise pro-cessing policy that employs NAS technique to identify the optimal architecture of different tasks while automatically determining, rather than manually, whether the parameters of the backbone are shared or not. 3) We introduce a novel sign-based gradient surgery that utilizes a sign-mask way to eliminate conflicting gradients. 4) We demonstrate that once the training for CO-Net is done, CO-Net allows in-cremental learning with fewer task-specific parameters by freezing the task-shared parameters. 2.