Abstract
While recent 3D-aware generative models have shown photo-realistic image synthesis with multi-view consistency, the synthesized image quality degrades depending on the camera pose (e.g., a face with a blurry and noisy boundary at a side viewpoint). Such degradation is mainly caused by the difficulty of learning both pose consistency and photo-realism simultaneously from a dataset with heavily imbal-anced poses. In this paper, we propose SideGAN, a novel 3D GAN training method to generate photo-realistic im-∗Both authors contributed equally to this research. Also, this work was done during an internship at Kakao Brain. ages irrespective of the camera pose, especially for faces of side-view angles. To ease the challenging problem of learn-ing photo-realistic and pose-consistent image synthesis, we split the problem into two subproblems, each of which can be solved more easily. Specifically, we formulate the prob-lem as a combination of two simple discrimination prob-lems, one of which learns to discriminate whether a syn-thesized image looks real or not, and the other learns to discriminate whether a synthesized image agrees with the camera pose. Based on this, we propose a dual-branched discriminator with two discrimination branches. We also propose a pose-matching loss to learn the pose consistency of 3D GANs. In addition, we present a pose sampling strat-egy to increase learning opportunities for steep angles in a pose-imbalanced dataset. With extensive validation, we demonstrate that our approach enables 3D GANs to gener-ate high-quality geometries and photo-realistic images ir-respective of the camera pose. 1.

Introduction
Generative Adversarial Networks (GANs)
[9] have shown remarkable success in photo-realistic image genera-tion [13, 14] by learning the distributions of high-resolution image datasets. Recent studies have taken this success one step further by extending GANs to pose-controllable image generation based on the guidance of a 3DMM prior [25, 5] or a differentiable renderer [28]. However, they produce in-consistent results across different poses and also suffer from limited pose controllability as they learn to generate 2D im-ages for different poses independently without considering the 3D face structure.
Therefore, 3D-aware GANs have emerged to achieve multi-view consistent image generation. Recent studies [19, 2, 10, 27, 1, 23, 17] have tackled this problem by modeling the 3D structure of a face using neural radiance fields [16], enabling explicit view control. Combining volumetric fea-ture projection with convolutional neural networks (CNNs) enables 3D GANs to generate photo-realistic face images in high resolution [10, 18, 1]. Albeit their ability to synthesize photo-realistic images with explicit view control, their re-sults do not have a stable quality depending on the camera pose (Fig. 1). To be specific, side-view facial images gener-ated by such methods show degraded qualities compared to photo-realistic images of frontal viewpoints (e.g., a blurry and a noisy facial boundary).
This unstable image quality is caused by the chal-lenge for 3D-aware GANs to simultaneously learn to generate pose-consistent and photo-realistic images from a pose-imbalanced dataset (Fig. 2) such as the FFHQ dataset [13] where most images are frontal-view images.
Specifically, EG3D [1], the state-of-the-art 3D GAN ap-proach, formulates the problem as a learning problem of a pose-conditional distribution of real images. Unfortunately, learning the distribution of real images for each pose can be extremely challenging, especially for poses with only a small number of real images. GRAM [6] casts the problem as a combination of the learning of real/fake image discrim-ination and pose estimation. Nevertheless, pose estimation from degraded side-view images is not trivial to learn either.
As a result, images generated by the existing 3D GANs are blurry or have noisy boundaries in the face region at steep pose angles (Fig. 1).
To tackle this problem, we propose SideGAN, a novel 3D GAN training method to generate photo-realistic im-ages irrespective of the viewing angle. Our key idea is as follows. To ease the challenging problem of learning photo-Figure 2: Real-world face datasets generally have an imbal-anced pose distribution, which is mainly concentrated on the frontal viewpoint. realistic and multi-view consistent image synthesis, we split the problem into two subproblems, each of which can be solved more easily. Specifically, we formulate the problem as a combination of two simple discrimination problems, one of which learns to discriminate whether a synthesized image looks real or not, and the other learns to discriminate whether a synthesized image agrees with the camera pose.
Unlike the formulations of the previous methods, which try to learn the real image distribution for each pose, or to learn pose estimation, our subproblems are much easier as each of them is analogous to a basic binary classification problem.
Based on this key idea, we propose a dual-branched discriminator, which has two branches for learning photo-realism and pose consistency, respectively. As these branches are supervised explicitly for their respective pur-poses, high-quality images with pose consistency can be produced at each viewing angle, and consequently, the gen-erator creates high-quality images and shapes. In addition, we propose a pose-matching loss to give supervision to the discriminator for the pose consistency, by considering a positive pose (i.e., rendering pose or ground truth pose) and a negative pose (i.e., irrelevant pose) for a given image.
For example, the frontal viewpoint is one of the irrelevant poses for a side-view image. As reported in the experiments, this loss helps improve image and shape quality. Com-pared to the previous pose estimation strategy [6], our pose-matching loss provides a more effective way to learn pose-consistent image generation, as the pose-matching loss casts the learning of pose-consistent image generation as the learning of simple binary classification that is much easier than the learning of accurate pose regression.
Additionally, we suggest a simple but effective train-ing strategy to alleviate the degradation caused by in-sufficient semantic knowledge at steep poses in a pose-imbalanced dataset. As shown in Fig. 2, most in-the-wild face datasets [13, 12, 3] usually have pose distributions concentrated on the frontal angle, causing the degradation of generated images at steep poses. While we may con-struct a pose-balanced dataset in a controlled environment, it requires a significant amount of effort, and is also hard to guarantee the diversity like in the in-the-wild datasets.
Instead, we present an additional uniform pose sampling (AUPS) strategy that draws camera poses from both a uni-form distribution and the actual camera pose distribution to enhance learning opportunities for steep angles during training. Our experiments show that this simple pose sam-pling strategy substantially improves the generation quality for side-view images.
Our contributions are summarized as follows:
• We split the problem of learning of 3D GANs into two easier subproblems: real/fake image discrimina-tion and pose-consistency discrimination.
• We propose a dual-branched discriminator and a pose-matching loss to effectively learn the pose consistency by considering both positive and negative poses of a given image.
• We also present a simple but effective pose sampling strategy to compensate for the insufficient amount of side-view images in pose-imbalanced in-the-wild datasets.
• With extensive evaluations, SideGAN shows the state-of-the-art image and shape quality irrespective of the camera pose, especially at steep view angles. 2.