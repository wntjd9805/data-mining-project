Abstract
Multilingual text recognition (MLTR) systems typically focus on a fixed set of languages, which makes it diffi-cult to handle newly added languages or adapt to ever-changing data distribution. In this paper, we propose the
Incremental MLTR (IMLTR) task in the context of incre-mental learning (IL), where different languages are intro-duced in batches. IMLTR is particularly challenging due to rehearsal-imbalance, which refers to the uneven distri-bution of sample characters in the rehearsal set, used to retain a small amount of old data as past memories. To ad-dress this issue, we propose a Multiplexed Routing Network (MRN). MRN trains a recognizer for each language that is currently seen. Subsequently, a language domain predictor is learned based on the rehearsal set to weigh the recog-nizers. Since the recognizers are derived from the original data, MRN effectively reduces the reliance on older data and better fights against catastrophic forgetting, the core issue in IL. We extensively evaluate MRN on MLT17 and
MLT19 datasets. It outperforms existing general-purpose
IL methods by large margins, with average accuracy im-provements ranging from 10.3% to 35.8% under different settings. Code is available at https://github.com/ simplify23/MRN . 1.

Introduction
Scene text recognition (STR) is a task aiming to read text in natural scenes. Recent advances in deep learning have significantly improved the accuracy of STR, allowing it to recognize text in the presence of font variations, distortions, and noise interference [39, 40, 43, 38, 19, 53]. As countries and cultures are more interconnected, the task of simulta-neously recognizing multiple languages, i.e., multilingual text recognition (MLTR), has also become more important.
*Corresponding Author.
Figure 1.
Incremental multilingual text recognition (IMLTR) fo-cuses on the practical scenario where different languages are intro-duced sequentially. The goal is to accurately recognize the newly introduced language while maintaining high recognition accuracy for previously seen languages. IMLTR introduces a task focusing on text recognition that faces rehearsal-imbalance challenges.
Existing methods typically address this challenge by train-ing on mixed multilingual data [8, 4, 34] or designing in-dependent language blocks [29, 22, 24]. However, when each time a new language is added, the above methods need retraining on a dataset mixing the old and new languages.
This increases the training cost [37, 46] and also may lead to an imbalance [7, 14] between old and new data.
Incremental learning (IL) is designed for scenarios where new data is continuously learned and typically, the old samples are maintained by a small ratio. The collection of old samples is referred to as the rehearsal set [51, 27], which serves as limited past memories. IL aims to learn the new data well while minimizing forgetting the past learned knowledge. Most existing studies [37, 7, 52, 28] conduct experiments on balanced datasets and maintain a constant number of classes at each learning step. However, in real-world scenarios, the number of classes and samples may differ across steps, leading to imbalanced datasets. To ad-dress these issues, IL2M [7] alleviated class-imbalance by storing statistics of old classes rather than samples. Delange et al. De Lange et al. [14] surveyed typical IL methods on datasets and solutions with different data imbalances. De-spite progress made, research on data and class imbalance is still in its infancy stage. Moreover, as illustrated in Fig. 1, there is currently no research introducing IL to STR.
We rewrite MLTR in the context of IL. Languages are treated as tasks and characters are their classes. During training, the model only observes the newly arrived lan-guage data and a small amount of data from old languages.
The recognition model is expected to maintain the ability to recognize characters of all languages that it has encountered before, regardless of whether their data are still available or discarded. We term this problem incremental multilingual text recognition (IMLTR).
IMLTR poses significant challenges to IL approaches due to its unbalanced features. 1) At the dataset level, it is difficult to collect sufficient training data for minority languages such as Bangla compared to popular languages such as English and Chinese, which affects the quality of recognition models. 2) At the language level, the size of character sets varies from tens to thousands across different languages, which leads to data imbalance. 3) At the char-acter level, the occurrence frequency of characters follows a long-tailed distribution, leading to class imbalance. In ad-dition, IMLTR faces the problem of variable length recog-nition, where text instances are the recognizing unit instead of character classes. Therefore, IL methods cannot sample characters as evenly as required in the context of IMLTR, resulting in a significant fraction of characters not being in-cluded in the rehearsal data, as shown in Fig. 2. This phe-nomenon is summarized as rehearsal-imbalance in Fig. 1.
Rehearsal-imbalance leads to catastrophic forgetting, where forgotten characters cannot be recognized. Therefore, there is an urgent need to develop new methods to overcome it.
Although the rehearsal set does not ensure full cover-age of all interlingual character classes, it is still adequate for training a language domain predictor to identify the lan-guages. Motivated by this observation, we propose a novel
Multiplexed Routing Network (MRN) for IMLTR. MRN in-volves training a new text recognition model at each learn-ing step and utilizing it and previously trained models for parallel feature extraction. A domain MLP router is de-signed to receive these features and predict the probability over the languages. Meanwhile, these features are used for character recognition in their own domain by feeding them to the multi-lingual modeling module. Finally, we fuse the results obtained at both the language domain and character levels to decode the recognized character sequence.
Our contributions can be summarized as follows. First, we introduce the IMLTR task, the first effort to adapt IL to text recognition. It contributes to the exploration of other
Figure 2. The showcase of rehearsal-imbalance. Data-imbalance (top) and class-imbalance (bottom) are severely aggravated from the full dataset to the rehearsal set, while the character classes to be read remain the same, making IMLTR particularly challenging. practical scenarios for text recognition. Second, we de-velop MRN to address the rehearsal-imbalance problem in
ILMTR. It is a dynamic and scalable architecture that is compatible with various IL methods and recognition mod-els. Third, experiments on two benchmarks show that MRN significantly outperforms existing general-purpose IL meth-ods, achieving accuracy improvements ranging from 10.3% to 27.4% under different settings. 2.