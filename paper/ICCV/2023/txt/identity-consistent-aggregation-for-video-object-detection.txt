Abstract
In Video Object Detection (VID), a common practice is to leverage the rich temporal contexts from the video to en-hance the object representations in each frame. Existing methods treat the temporal contexts obtained from different objects indiscriminately and ignore their different identities.
While intuitively, aggregating local views of the same object in different frames may facilitate a better understanding of the object. Thus, in this paper, we aim to enable the model to focus on the identity-consistent temporal contexts of each object to obtain more comprehensive object representations and handle the rapid object appearance variations such as occlusion, motion blur, etc. However, realizing this goal on top of existing VID models faces low-efficiency prob-lems due to their redundant region proposals and nonpar-allel frame-wise prediction manner. To aid this, we propose
ClipVID, a VID model equipped with Identity-Consistent
Aggregation (ICA) layers specifically designed for mining fine-grained and identity-consistent temporal contexts.
It effectively reduces the redundancies through the set pre-diction strategy, making the ICA layers very efficient and further allowing us to design an architecture that makes parallel clip-wise predictions for the whole video clip. Ex-tensive experimental results demonstrate the superiority of our method: a state-of-the-art (SOTA) performance (84.7% mAP) on the ImageNet VID dataset while running at a speed about 7× faster (39.3 fps) than previous SOTAs. 1.

Introduction
Video Object Detection (VID) aims to recognize and lo-calize the objects in all frames given a video clip. It is a challenging task as it must handle the complex appearance variations of video objects, caused by motion blur, occlu-sion, rotation, unusual poses, and deformable shapes, etc.
To tackle these issues, prior works [16, 26, 27, 53] utilize a set of support frames (e.g., neighboring frames of the target frame), which provide rich temporal contexts, to guide the
∗Corresponds to qi.wu01@adelaide.edu.au. Code is available at https://github.com/bladewaltz1/clipvid.
Figure 1. Illustration of the temporal context aggregation in a typ-ical VID method [45]. The region proposals from the support frames (dashed boxes) are treated indiscriminately regardless of their object identities. However, to detect the cat in the target frame, the region proposals with red dashed boxes should provide more relevant information, as they are obtained from the same cat. object detection in the target frame. For example, [52, 43, 3] build grid-level relations between the feature maps of the support frames and the target frame to propagate temporal contexts. More recent SOTA methods [45, 8, 19, 10, 28] adopt object-level relation modules [24] to leverage the re-gion proposals extracted from long-range support frames to enhance the object representations in the target frame.
However, as shown in Figure 1, the temporal contexts from the support frames usually contain irrelevant and noisy information that may negatively affect the object represen-tations. E.g., when detecting the cat in the target frame, the yellow boxes in the support frames only provide informa-tion from different objects and even from the background.
On the other hand, the red boxes are different local views of the same cat, showing it from various perspectives. Intu-itively, incorporating these local views into a unified repre-sentation could lead to a more comprehensive understand-ing of the object, and further facilitate the model to deal with the rapid variations of the object appearance. Unfortu-nately, existing methods make no distinction between these two kinds of temporal contexts. In light of this, we propose
an Identity-Consistent temporal context Aggregation (ICA) approach, which aims to discover and utilize the local views of each object to learn its global view to guide the detection.
To achieve this, a prerequisite is to ensure that the re-gion proposals extracted from support frames have a high recall rate for the video objects, so that each object in the target frame can find its identity-consistent temporal con-texts from them. This requires existing methods to extract a large number of region proposals (e.g., 300) from each sup-port frame due to the redundant predictions made by their base detectors [11, 35]. However, the computation com-plexity of the object relation module is usually quadratic to the total number of region proposals in the video. There-fore, existing methods have to lower the number of region proposals extracted from the support frames to make the computation feasible, decreasing the recall rate of the video objects and hampering the ICA process. Worse still, the low recall rate also leads to a low detection performance on the support frames. Thus, given an input video clip, existing methods only make predictions for one target frame, while the support frames are merely used as guidance. This non-parallel behavior further hampers the model efficiency.
For these reasons, we build our VID model based on the
DETR [5] framework. Specifically, instead of generating a large number of redundant object candidates, we represent each object in a video frame with a learnable embedding, a.k.a. “object query”, and iteratively incorporate its object-related visual content from the frame representation and its identity-consistent temporal contexts from the video into the object query. Hungarian algorithm is used to perform one-to-one bipartite matching between the object queries and the ground-truth objects. In this way, the total number of object queries can be typically less than 100 per frame, which is an order of magnitude smaller than the number of region proposals in previous methods. Thus, the whole de-tection process can be achieved efficiently, further allowing our model to perform parallel clip-wise predictions.
The proposed model, termed ClipVID, adopts a clean backbone + Transformer decoder architecture. It first ex-tracts features from each frame separately using a CNN-based [22] backbone. Then, the object queries for all input frames are adaptively generated and are fed into a Trans-former decoder jointly to propagate the temporal contexts.
To perform identity-consistent temporal context aggrega-tion, we assign each object query with an object iden-tity, and additionally predict an identity embedding for it, which is then adopted to select the object queries from other frames that are close in the embedding space. These se-lected object queries are considered to have the same object identity and are fed into an ICA module to maintain the identity consistency of the video objects. Finally, the ob-ject queries from all frames are fed into the detection head jointly to obtain their predictions in parallel.
When evaluated on the ImageNet VID dataset [37],
ClipVID achieves a significant performance improvement in fast-moving objects, which are the type of objects that suffers mostly from the appearance variations in a video, e.g., motion blur, occlusion, and deformation. This further leads to a state-of-the-art overall performance (84.7% mAP) without the need for post-processing. Moreover, our model is able to run at 39.3 fps, which is about 7× faster than re-cent SOTAs. In summary, our contributions are three-fold: 1. We propose the ClipVID model which is able to lever-age the identity-consistent temporal contexts to obtain comprehensive representations for the video objects, leading to a SOTA performance on the VID task. 2. The proposed ClipVID makes clip-wise predictions for the VID task, i.e., detects the objects on all in-put frames simultaneously, which is significantly faster than previous frame-wise prediction methods. 3. We conduct extensive experiments to analyze the per-formance of the proposed ClipVID model. 2.