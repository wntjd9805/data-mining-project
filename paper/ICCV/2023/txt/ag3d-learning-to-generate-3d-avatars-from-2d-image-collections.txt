Abstract 1.

Introduction
While progress in 2D generative models of human ap-pearance has been rapid, many applications require 3D avatars that can be animated and rendered. Unfortunately, most existing methods for learning generative models of 3D humans with diverse shape and appearance require 3D training data, which is limited and expensive to acquire.
The key to progress is hence to learn generative models of 3D avatars from abundant unstructured 2D image col-lections. However, learning realistic and complete 3D ap-pearance and geometry in this under-constrained setting re-mains challenging, especially in the presence of loose cloth-ing such as dresses. In this paper, we propose a new adver-sarial generative model of realistic 3D people learned from 2D images. Our method captures shape and deformation of the body and loose clothing by adopting a holistic 3D gener-ator and integrating an efﬁcient, ﬂexible, articulation mod-ule. To improve realism, we train our model using multiple discriminators while also integrating geometric cues in the form of predicted 2D normal maps. We experimentally ﬁnd that our method outperforms previous 3D- and articulation-aware methods in terms of geometry and appearance. We validate the effectiveness of our model and the importance of each component via systematic ablation studies.
*Equal contribution
Generative models, like GANs [19], can be trained from large image collections, to produce photo-realistic images of objects [5, 29–31] and even clothed humans [2, 18, 20, 33, 34, 55]. The output, however, is only a 2D image and many applications require diverse, high-quality, virtual 3D avatars, with the ability to control poses and camera view-points, while ensuring 3D consistency. To enable the gener-ation of 3D avatars, the research community has been study-ing generative models that can automatically produce 3D shapes of humans and/or clothing based on input parame-ters such as body pose and shape [9, 11, 38, 50]. Despite rapid progress, most existing methods do not yet consider texture and require accurate and clean 3D scans of humans for training, which are expensive to acquire and hence lim-ited in quantity and diversity. In this paper, we develop a method that learns a generative model of 3D humans with texture from only a set of unstructured 2D images of vari-ous people in different poses wearing diverse clothing; that is, we learn a generative 3D human model from data that is ubiquitous on the Internet. See Fig. 1.
Learning to generate 3D shapes and textures of articu-lated humans from such unstructured image data is a highly under-constrained problem, as each training instance has a different shape and appearance and is observed only once from a particular viewpoint and in a particular pose. Recent
progress in 3D-aware GANs [6, 22, 48] shows impressive results in learning 3D geometry and appearance of rigid ob-jects from 2D image collections. However, since humans are highly articulated and have more degrees of freedom to model, such methods struggle to generate realistic humans.
By modeling articulation, recent work [4, 47] demonstrates the feasibility of learning articulated humans from image collections, allowing the generation of human shapes and images in desired poses, but only in limited quality and res-olution. Recently, EVA3D [23] achieves higher resolution by representing humans as a composition of multiple parts, each of which is generated by a small network. However, there is still a noticeable gap between the generated and real humans in terms of appearance and, in particular, geometry.
Additionally, the compositional design precludes modeling loose clothing that is not associated with a single body part, such as dresses shown in Fig. 5c.
In this paper, we contribute a new method for learning 3D human generation from 2D image collections, which yields state-of-the-art image and geometric quality and nat-urally models loose clothing. Instead of representing hu-mans with separate body parts as in EVA3D [23], we adopt a simple monolithic approach that is able to model the hu-man body as well as loose clothing, while adding multiple discriminators that increase the ﬁdelity of perceptually im-portant regions and improve geometric details.
Holistic 3D Generation and Deformation: To achieve the goal of high image quality while ﬂexibly handling loose clothing, we propose a novel generator design. We model 3D humans holistically in a canonical space using a mono-lithic 3D generator and an efﬁcient tri-plane representa-tion [6]. To attain high-quality images it is critically im-portant to enable fast volume rendering. To this end, we adapt the efﬁcient articulation module, Fast-SNARF [8], to our generative setting and further accelerate rendering via empty-space skipping, informed by a coarse human body prior. Our articulation module is more ﬂexible than prior methods that base deformations of the clothed body on
SMPL [38], enabling it to faithfully model deformations for points that are far away from the body.
Modular 2D Discriminators: We further propose multiple discriminators to improve geometric detail as well as the perceptually-important face region as we found that a single adversarial loss on rendered images is insufﬁcient to recover meaningful 3D geometry in such a highly under-constrained setting. Motivated by the recent success of methods [25, 69] that exploit monocular normal cues [54, 65] for the task of 3D reconstruction, we explore the utility of normal infor-mation for guiding 3D geometry in the generative setting.
More speciﬁcally, we discriminate normal maps rendered from our generative 3D model against 2D normal maps ob-tained from off-the-shelf monocular estimators [54] applied to 2D images of human subjects. We demonstrate that this additional normal supervision serves as useful and comple-mentary guidance, signiﬁcantly improving the quality of the generated 3D shapes. Furthermore, we apply separate face discriminators on both the image and normal branch to en-courage more realistic face generation.
We experimentally ﬁnd that our method outperforms pre-vious 3D- and articulation-aware methods by a large mar-gin in terms of both geometry and texture quality, quanti-tatively (Table 1), qualitatively (Fig. 5) and through a per-ceptual study (Fig. 4). In summary, we contribute (i) a gen-erative model of articulated 3D humans with SotA appear-ance and geometry, (ii) a new generator that is efﬁcient and can generate and deform loose clothing, and (iii) several, specialized discriminators that signiﬁcantly improve visual and geometric ﬁdelity. Code and models are available at https://zj-dong.github.io/AG3D/. 2.