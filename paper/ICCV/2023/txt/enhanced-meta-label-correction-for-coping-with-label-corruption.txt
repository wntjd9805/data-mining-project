Abstract
Traditional methods for learning with the presence of noisy labels have successfully handled datasets with ar-tificially injected noise but still fall short of adequately handling real-world noise. With the increasing use of meta-learning in the diverse fields of machine learning, re-searchers leveraged auxiliary small clean datasets to meta-correct the training labels. Nonetheless, existing meta-label correction approaches are not fully exploiting their poten-tial.
In this study, we propose an Enhanced Meta Label
Correction approach abbreviated as EMLC for the learning with noisy labels (LNL) problem. We re-examine the meta-learning process and introduce faster and more accurate meta-gradient derivations. We propose a novel teacher ar-chitecture tailored explicitly to the LNL problem, equipped with novel training objectives. EMLC outperforms prior approaches and achieves state-of-the-art results in all stan-dard benchmarks. Notably, EMLC enhances the previous art on the noisy real-world dataset Clothing1M by 1.52% while requiring ×0.5 the time per epoch and with much faster convergence of the meta-objective when compared to the baseline approach. 1 1.

Introduction
The remarkable success of Deep Neural Networks (DNNs) for visual classification tasks is predominantly due to the availability of massive labeled datasets. In many prac-tical applications, obtaining the required amounts of reli-able labeled data is intractable. As a result, numerous works over the past decade have sought ways to reduce the amount of labeled data required for classification tasks. Notably, semi-supervised learning exploits unlabeled data [32, 23], transfer learning exploits prior knowledge obtained from different tasks [7, 38], and self-supervised learning exploits data augmentations for label agnostic representation learn-ing [3, 11]. Nonetheless, for many applications, the quality of the labeled data can be sacrificed for the sake of its quan-1Project emlc-paper. page: https://sites.google.com/view/
Figure 1: Comparison of the training labels recovery (solid line) and the wrong labels recovery (dashed line) over train-ing epochs on the CIFAR-10 dataset with 90% symmetric noise. EMLC (ours) is comapred to MLC [40] and to MLC
[40] with self-supervised pretraining. tity. A prominent example is the process of crawling search engines and online websites as demonstrated by [17, 31].
The crawling process is often easy to automate however re-sults in significant amounts of label noise with complicated patterns. A fundamental problem with such data, however, is that classical learning methodologies tend to fail when significant label noise is present [35]. Therefore, designing learning frameworks that are able to cope with label corrup-tion is a task of great importance.
While traditional methods for learning with noisy labels (LNL) are capable of handling data with immense artificial, injected noise, their ability to handle real-world noise re-mains highly limited. Thus, noisy labeled datasets alone limit the capability of learning from real-world noisy la-beled datasets. Luckily, in many real-world applications, while obtaining large amounts of labeled data is infeasible, obtaining small amounts of labeled data is usually attain-able. Thus, the paramount objective is to find methods that can incorporate both large amounts of noisy labeled data and small amounts of clean data. A natural choice is to adopt a meta-learning framework, a popular design regu-larly used for solving various tasks. Consequently, recent
trends in LNL leverage meta-learning using auxiliary small clean datasets. Prominent examples include meta-sample weighting [25, 37], meta-robustification to artificially in-jected noise [16], meta-label correction [40], and meta-soft label correction [30].
In this work, we propose EMLC – an enhanced meta-label correction approach for learning from label-corrupted data. We first revise the bi-level optimization process by deriving a more accurate meta gradient used to optimize the teacher. In particular, we derive an exact form of the meta-gradient for the one-step look-ahead approximation and suggest an improved meta-gradient approximation for the multi-step look-ahead approximation. We further pro-vide an algorithm for efficiently computing our derivations using a modern hardware accelerator. We empirically val-idate our derivations, demonstrating a significant improve-ment in convergence speed and training time. We further propose a dedicated teacher architecture that employs a fea-ture extraction to generate initial predictions and incorpo-rates them with the noisy label signal to generate refined soft labels for the student. Our teacher architecture is com-pletely independent of the student, avoiding the confirma-tion bias problem. In addition, we propose a novel auxil-iary adversarial training objective for enhancing the effec-tiveness of the teacher’s label correction mechanism. As demonstrated in fig. 1, our teacher proves to have a superior ability of purifying the training labels.
Our contributions can be summarized as follows:
• We derive fast and more accurate procedures for com-puting the meta-gradient used to optimize the teacher.
• We design a unique teacher architecture in conjunction with a novel training objective toward an improved la-bel correction process.
• We combine these two components into a single yet effective framework termed EMLC. We demonstrate the effectiveness of EMLC on both synthetic and real-world label-corrupted standard benchmarks. 2.