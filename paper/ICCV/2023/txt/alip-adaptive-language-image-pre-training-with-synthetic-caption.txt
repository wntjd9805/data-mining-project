Abstract
Contrastive Language-Image Pre-training (CLIP) has significantly boosted the performance of various vision-language tasks by scaling up the dataset with image-text pairs collected from the web. However, the presence of in-trinsic noise and unmatched image-text pairs in web data can potentially affect the performance of representation learning. To address this issue, we first utilize the OFA model to generate synthetic captions that focus on the im-age content. The generated captions contain complemen-tary information that is beneficial for pre-training. Then, we propose an Adaptive Language-Image Pre-training (ALIP), a bi-path model that integrates supervision from both raw text and synthetic caption. As the core components of ALIP, the Language Consistency Gate (LCG) and Description
Consistency Gate (DCG) dynamically adjust the weights of samples and image-text/caption pairs during the train-ing process. Meanwhile, the adaptive contrastive loss can effectively reduce the impact of noise data and en-hances the efficiency of pre-training data. We validate
ALIP with experiments on different scales of models and pre-training datasets. Experiments results show that ALIP achieves state-of-the-art performance on multiple down-stream tasks including zero-shot image-text retrieval and linear probe. To facilitate future research, the code and pre-trained models are released at https://github.com/ deepglint/ALIP. 1.

Introduction
With the development of mobile networks and social platforms, there has been an explosion in the production of image-text pairs on a massive scale [3, 13]. This un-precedented abundance of data has laid a solid foundation for vision-language pre-training [32, 15]. Through image-text alignment on large-scale data, the Contrastive Lan-* corresponding author.
Figure 1. Examples from the YFCC15M dataset to illustrate the mismatched (left) and matched (right) image-text pairs. The syn-thetic caption is generated from the OFA [37] model. The raw text description “Leisure Sunday” is less aligned with the raw image in the left sample, while the synthetic caption “A woman sitting on a step reading a book” is a more accurate representation. guage–Image Pre-training (CLIP) method [32] has demon-strated huge success in multi-modal learning. Specifically,
CLIP learns two separate unimodal encoders for image and text using a contrastive loss, one of the most effective losses for representation learning [36, 14, 5, 7]. Nevertheless, the negative impact of the noise in the web-crawled data has been largely overlooked, shadowed by the performance gain achieved from scaling up the training data [31, 1].
To facilitate research on large-scale multi-modal models,
LAION400M [34] and LAION5B [33] were released, com-prising 400 million and 5 billion image-text pairs respec-tively, which were filtered using the CLIP model. How-ever, the current offline filtering approach results in a sub-stantial loss of training data, as the original dataset con-tains 5 billion image-text pairs. this ap-proach may introduce biases due to the limited represen-tation power of the pre-trained model used for filtering.
Despite efforts to curate data for high-quality image-text pairs (e.g., LAION [34, 33] and YFCC100M [35]), noisy and poorly-aligned pairs still exist in existing image-text datasets, which can potentially impact the performance of representation learning.
Furthermore,
In Fig. 1, we present two samples from YFCC15M. The raw text of the right sample (“Drink beer”) is correct and
matches the content in the image, while the raw text of the left sample (“Leisure Sunday”) is too abstract and does not exactly match the concrete visual signals of the image.
To alleviate the influence of the noisy and poorly-aligned image-text pairs, BLIP [19] bootstraps the captions with an online captioner generating synthetic captions and an online filter removing the noisy ones, while momentum-based methods (e.g., ALBEF [20] and PSD [2]) employs soft labels computed using embeddings from a moving average momentum model. However, these filtering and momentum-based methods require additional computation costs and memory consumption.
In this paper, we first employ the OFA [37] model to generate synthetic captions that are consistent with the im-age content. Specifically, the OFA [37] model is guided by the prompt “What does the image describe” to generate synthetic captions. Compared with the raw texts in Fig. 1, the synthetic captions “A woman sitting on a step reading a book” and “A bunch of green cans of beer parked in front of a building” provide additional as well as reliable descrip-tions, such as object information (book, cans), attribute in-formation (green), action information (sitting, parked), and spatial relationship (in front of), which can be used to en-hance the representation learning.
Given the normalized embedding features of the im-age, raw text, and synthetic caption, we propose an Adap-tive Language-Image Pre-training (ALIP) method, a bi-path model that integrates raw text supervision and synthetic caption supervision. As the core components of ALIP, the Language Consistency Gate (LCG) and the Descrip-tion Consistency Gate (DCG) are designed to dynamically adjust the weights of samples and image-text/caption pairs during the training process. The LCG considers the con-sistency between raw text and synthetic caption to identify the high-quality sample, while the DCG considers the con-sistency of image-text or image-caption to adjust the con-trastive pair loss. The adaptive contrastive loss influenced by the above weights substantially reduces the impact of noise data and enhances the efficiency of pre-training data.
Extensive experiments show that ALIP achieves state-of-the-art performance on multiple downstream tasks includ-ing zero-shot image-text retrieval and linear probe. Ex-periment results on different model sizes and pre-training datasets also prove the strong robustness of the ALIP. The main contributions of this paper are summarized as follows:
• We propose a bi-path model that integrates raw text supervision and synthetic caption supervision. Based on the similarity triplet between image, text, and cap-tion, the proposed method can dynamically adjust the weights of samples and image-text/caption pairs through the language consistency gate and description consistency gate.
• Based on the adaptive weights, we design the adap-tive contrastive loss, which can effectively reduce the impact of noise data and improve the pre-training data efficiency.
• We conduct extensive experiments and prove that
ALIP achieves state-of-the-art performance on multi-ple downstream tasks including zero-shot image-text retrieval and linear probe task. 2.