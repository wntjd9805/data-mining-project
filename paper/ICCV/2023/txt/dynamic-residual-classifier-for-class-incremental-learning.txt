Abstract
···
Task t-1
Task t
···
The rehearsal strategy is widely used to alleviate the catastrophic forgetting problem in class incremental learn-ing (CIL) by preserving limited exemplars from previous tasks. With imbalanced sample numbers between old and new classes, the classiﬁer learning can be biased. Existing
CIL methods exploit the long-tailed (LT) recognition tech-niques, e.g., the adjusted losses and the data re-sampling methods, to handle the data imbalance issue within each increment task. In this work, the dynamic nature of data imbalance in CIL is shown and a novel Dynamic Resid-ual Classiﬁer (DRC) is proposed to handle this challenging scenario. Speciﬁcally, DRC is built upon a recent advance residual classiﬁer with the branch layer merging to handle the model-growing problem. Moreover, DRC is compati-ble with different CIL pipelines and substantially improves them. Combining DRC with the model adaptation and fu-sion (MAF) pipeline, this method achieves state-of-the-art results on both the conventional CIL and the LT-CIL bench-marks. Extensive experiments are also conducted for a de-tailed analysis. The code is publicly available1. 1.

Introduction
Deep models are prone to forgetting previously learned knowledge when sequentially ﬁne-tuned on different tasks.
Severe performance degradation on the old tasks can be ob-served. It is also known as catastrophic forgetting [17, 18].
Class incremental learning (CIL) methods [9, 35] aim to handle this issue and equip deep models with the capacity to continuously learn new categories without forgetting the old ones. The rehearsal strategy [37, 49, 38, 42, 44] has been widely used to achieve this goal. Speciﬁcally, a lim-ited amount of exemplars from previous tasks are stored in a memory buffer and replayed when learning new tasks.
*indicates corresponding author. 1https://github.com/chen-xw/DRC-CIL
# Instances 1t-D
# Instances tD 1t-M
··· tM
···
Categories 1t-C 1t-C tC
Categories
Figure 1. Data imbalance of CIL. With the exemplars of previous tasks buffered, the training data within each task is imbalanced.
As the task increment proceeds, more categories appear in a ﬁxed-size memory. Such imbalance becomes more severe.
Due to the relatively small size of the memory buffer, the training samples of a new class are far more than the old ones. Therefore, adopting the rehearsal strategy can intro-duce the data imbalance problem to CIL. Two kinds of long-tailed recognition techniques, the adjusted losses [36, 8] and the data re-sampling [28], are exploited by many CIL methods [25, 42, 3, 44] to learn the classiﬁer with less bias.
These methods alleviate the data imbalance within each in-crement task independently.
However, the data imbalance in CIL is dynamic and be-comes more extreme as the task increment proceeds, as illustrated in Fig. 1. A novel dynamic residual classiﬁer (DRC) is proposed in this work to handle this challenging scenario. Inspired by the recent advance residual classiﬁer (RC) [7], a lightweight branch layer is inserted before the classiﬁer to encode the task-speciﬁc knowledge. This new architecture enables the residual fusion of classiﬁer out-puts to alleviate the data imbalance effectively. However, directly applying RC for CIL leads to the model-growing problem, i.e., the growing overhead from the additional branch layers assigned to the new tasks. The proposed DRC handles this dynamic increment issue via the simple yet ef-fective branch layer merging.
DRC is directly applicable to different CIL pipelines
by simply replacing the fully connected (fc) classiﬁers.
Three typical CIL pipelines, i.e., the Model Direct Trans-fer (MDT) [37, 15], the Model Expansion and Compres-sion (MEC) [42, 44] and the Model Adaptation and Fusion (MAF) [24, 5], as shown in Fig. 2, are chosen to be com-bined with DRC. They can consistently beneﬁt from such combinations, with clear improvements observed. The de-tails and comparisons on such combinations are given in
Sec. 3.3. More importantly, DRC is most compatible with
MAF among the three pipelines. The resulting MAFDRC method achieves state-of-the-art performance under both conventional CIL and long-tailed CIL (LT-CIL) settings.
Extensive analyzes are conducted to provide insights into each part. The main contributions are three-fold:
• We show the data imbalance issue in CIL rehearsal is dynamic across tasks rather than static within each task. The proposed dynamic residual classiﬁer (DRC) aims to handle this challenging scenario from the per-spective of classiﬁer architecture, which is comple-mentary to existing efforts in CIL;
• The branch layer architecture and residual fu-sion mechanism from a recent long-tailed classiﬁer (RC) [7] are adopted by DRC to alleviate the nega-tive impact of data imbalance on CIL for the ﬁrst time.
More importantly, the model-growing problem of the vanilla RC under the CIL setting is handled with the simple yet effective branch layer merging in DRC;
• The proposed DRC is generalizable. On the one hand, incorporating DRC brings clear improvements to dif-ferent CIL pipelines. On the other hand, the effective-ness of DRC is demonstrated in both the CIL and the
LT-CIL settings. 2.