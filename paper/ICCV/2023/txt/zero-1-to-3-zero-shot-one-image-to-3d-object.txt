Abstract 1.

Introduction
We introduce Zero-1-to-3, a framework for changing the camera viewpoint of an object given just a single RGB image. To perform novel view synthesis in this under-constrained setting, we capitalize on the geometric priors that large-scale diffusion models learn about natural im-ages. Our conditional diffusion model uses a synthetic dataset to learn controls of the relative camera viewpoint, which allow new images to be generated of the same ob-ject under a speciﬁed camera transformation. Even though it is trained on a synthetic dataset, our model retains a strong zero-shot generalization ability to out-of-distribution datasets as well as in-the-wild images, including impres-sionist paintings. Our viewpoint-conditioned diffusion ap-proach can further be used for the task of 3D reconstruction from a single image. Qualitative and quantitative experi-ments show that our method signiﬁcantly outperforms state-of-the-art single-view 3D reconstruction and novel view synthesis models by leveraging Internet-scale pre-training.
From just a single camera view, humans are often able to imagine an object’s 3D shape and appearance. This ability is important for everyday tasks, such as object manipula-tion [18] and navigation in complex environments [7], but is also key for visual creativity, such as painting [33]. While this ability can be partially explained by reliance on geomet-ric priors like symmetry, we seem to be able to generalize to much more challenging objects that break physical and geometric constraints with ease. In fact, we can predict the 3D shape of objects that do not (or even cannot) exist in the physical world (see third column in Figure 1). To achieve this degree of generalization, humans rely on prior knowl-edge accumulated through a lifetime of visual exploration.
In contrast, most existing approaches for 3D image re-construction operate in a closed-world setting due to their reliance on expensive 3D annotations (e.g. CAD models) or category-speciﬁc priors [37, 23, 36, 67, 68, 66, 27, 26, 2].
Very recently, several methods have made major strides
in the direction of open-world 3D reconstruction by pre-training on large-scale, diverse datasets such as CO3D
[44, 31, 36, 16]. However, these approaches often still require geometry-related information for training, such as stereo views or camera poses. As a result, the scale and di-versity of the data they use remain insigniﬁcant compared to the recent Internet-scale text-image collections [48] that enable the success of large diffusion models [46, 45, 34].
It has been shown that Internet-scale pre-training endows these models with rich semantic priors, but the extent to which they capture geometric information remains largely unexplored.
In this paper, we demonstrate that we are able to learn control mechanisms that manipulate the camera viewpoint in large-scale diffusion models, such as Stable Diffusion
[45], in order to perform zero-shot novel view synthesis and 3D shape reconstruction. Given a single RGB image, both of these tasks are severely under-constrained. However, due to the scale of training data available to modern generative models (over 5 billion images), diffusion models are state-of-the-art representations for the natural image distribution, with support that covers a vast number of objects from many viewpoints. Although they are trained on 2D monocular im-ages without any camera correspondences, we can ﬁne-tune the model to learn controls for relative camera rotation and translation during the generation process. These controls allow us to encode arbitrary images that are decoded to a different camera viewpoint of our choosing. Figure 1 shows several examples of our results.
The primary contribution of this paper is to demonstrate that large diffusion models have learned rich 3D priors about the visual world, even though they are only trained on 2D images. We also demonstrate state-of-the-art results for novel view synthesis and state-of-the-art results for zero-shot 3D reconstruction of objects, both from a single RGB image. We begin by brieﬂy reviewing related work in Sec-tion 2.
In Section 3, we describe our approach to learn controls for camera extrinsics by ﬁne-tuning large diffusion models. Finally, in Section 4, we present several quantita-tive and qualitative experiments to evaluate zero-shot view synthesis and 3D reconstruction of geometry and appear-ance from a single image. We will release all code and models as well as an online demo. 2.