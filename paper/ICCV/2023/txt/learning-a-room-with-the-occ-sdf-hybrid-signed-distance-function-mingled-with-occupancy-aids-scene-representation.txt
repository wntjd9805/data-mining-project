Abstract
Implicit neural rendering, using signed distance function (SDF) representation with geometric priors like depth or surface normal, has made impressive strides in the surface reconstruction of large-scale scenes. However, applying this method to reconstruct a room-level scene from images may miss structures in low-intensity areas and/or small, thin objects. We have conducted experiments on three datasets to identify limitations of the original color rendering loss and priors-embedded SDF scene representation.
Our ﬁndings show that the color rendering loss creates an optimization bias against low-intensity areas, resulting in gradient vanishing and leaving these areas unoptimized.
To address this issue, we propose a feature-based color ren-dering loss that utilizes non-zero feature values to bring back optimization signals. Additionally, the SDF represen-tation can be inﬂuenced by objects along a ray path, dis-rupting the monotonic change of SDF values when a single object is present. Accordingly, we explore using the occu-pancy representation, which encodes each point separately and is unaffected by objects along a querying ray. Our ex-perimental results demonstrate that the joint forces of the feature-based rendering loss and Occ-SDF hybrid repre-sentation scheme can provide high-quality reconstruction results, especially in challenging room-level scenarios. The code is available at https://github.com/shawLyu/Occ-SDF-Hybrid 1.

Introduction
Reconstructing a 3D scene from a series of multi-view images is a crucial problem in the realm of computer vi-sion. This process has widespread applications in various
ﬁelds such as animation, gaming, and virtual/augmented re-ality (VR/AR). The recent trend is to represent a 3D scene as an implicit function parameterized by a neural network
[13, 19, 26, 21], whose optimization is supervised by ex-1
plicit 3D data like point cloud or real SDF value. Recent advancements in neural radiance ﬁeld (NeRF) [14] further enable learning an implicit 3D representation from purely sparse posed images [35, 15].
However, when it comes to producing high-quality novel-view synthesis, these methods frequently utilize vol-ume density [14] to represent the 3D geometry. Unfor-tunately, this approach does not adequately constrain the 3D geometry in the presence of ambiguities [18], ulti-mately leading to poor surface reconstructions (as depicted in Fig. 1: Volume Density).
Accordingly, research efforts have been made to exploit geometry-friendly representations, including signed dis-tance function (SDF) [32, 34, 19] or occupancy [18], whose zero-level set can be extracted to become the concerned 3D surface. Albeit improving quality, they consider the recon-struction only of a single object, thereby, the performance degrades dramatically when applied to scene-level surface reconstruction, i.e., representing a room (Fig. 1: SDF). An attribute is that reconstructing texture-less areas often suf-fers from ambiguous visual cues with only RGB loss as the regularization. To address this problem, recent research has attempted to incorporate semantic [8] or geometric pri-ors (depth/normal [36, 31] constraints) to further regularize scene-level reconstruction. With SDF-based representation and geometric priors [36], the reconstruction quality has been greatly improved (Fig. 1: SDF + Geometry Priors), es-pecially concerning large ﬂat areas and objects. However, it still cannot faithfully reconstruct the 3D scene with missing structures in low-intensity dark areas and small/thin objects (Fig. 1: SDF + Geometry Priors).
The above observation motivates us to dive into bridg-ing the remaining missing blocks of existing neural surface representation methods. Notably, we focus on the SDF-based representation as it achieves state-of-the-art perfor-mance and has been widely adopted. Our analysis suggests that both the RGB color rendering formulation and SDF representation have clear limitations preventing existing so-lutions from fully unleashing the potential of implicit neural surface representation for large-scale room-level scenes.
First, the color itself can show a signiﬁcant impact on the optimization of geometric representation relying on the original RGB-based rendering formula [14], namely color bias. In particular, dark pixels with small intensity values will make the partial derivation of the loss with respect to the corresponding SDF value become zero, corrupting the optimization and resulting in missing structures in dark ar-eas (see for example in Fig. 1: Low Intensities). Accord-ingly, herein instead of directly calculating the weighted color, we ﬁrst compute weighted features and then use a learnable multi-layer perceptron (MLP) to decode the ﬁnal rendering color. In such a way, we would still be able to effectively optimize the corresponding geometry represen-tation as long as the feature vector contains non-zero values.
Second, the vanilla SDF-based neural rendering only considers a single ray directly passing through the object surface from the empty space and ignores objects along the ray [32, 34]. This conﬁguration violates scene-level geom-etry where the existence of multiple objects clearly affects the distributions of SDF (Fig. 5(a)). Meanwhile, the op-timization of thin structures and small objects, which nat-urally has small sampling probability, will be greatly de-graded by this violation even with correct geometry prior and the structure will be erased to minimize the global ge-ometry loss (Fig. 1: Detailed Structures).
In addition, although occupancy-based representations are likely to generate unwanted structure and cannot war-rant a smooth surface reconstruction (Fig. 1), they are often sufﬁciently robust to objects along the ray and free from object interference in scene-level data. Therefore, during optimization, we propose to describe the room-level scene using occupancy in conjunction with signed distance func-tions (SDFs) to compensate for each other’s defects.
The technical contributions are as follows:
• We explore an improved feature rendering scheme to overcome the problem of vanishing gradients in neu-ral implicit reconstruction brought by the vanilla color space rendering formula.
• We carefully investigate insights and limitations in ex-isting SDF and occupancy representations, and accord-ingly propose a hybrid representation mingling SDF with occupancy, dubbed Occ-SDF Hybrid, to resolve surfaces with thin structures and small objects.
• We conduct a large body of qualitative and quantitative experiments against state-of-the-art, indicating that our
Occ-SDF hybrid formula can yield a higher-ﬁdelity room-level scene representation, particularly with suc-cessfully resolving small and dark objects. 2.