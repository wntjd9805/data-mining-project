Abstract
We propose an Explicit Conditional Multimodal Varia-tional Auto-Encoder (ECMVAE) for audio-visual segmen-tation (AVS), aiming to segment sound sources in the video sequence. Existing AVS methods focus on implicit feature fusion strategies, where models are trained to fit the dis-crete samples in the dataset. With a limited and less diverse dataset, the resulting performance is usually unsatisfactory.
In contrast, we address this problem from an effective repre-sentation learning perspective, aiming to model the contri-bution of each modality explicitly. Specifically, we find that audio contains critical category information of the sound producers, and visual data provides candidate sound pro-ducer(s). Their shared information corresponds to the tar-get sound producer(s) shown in the visual data. In this case, cross-modal shared representation learning is especially important for AVS. To achieve this, our ECMVAE factorizes the representations of each modality with a modality-shared representation and a modality-specific representation. An orthogonality constraint is applied between the shared and specific representations to maintain the exclusive attribute of the factorized latent code. Further, a mutual information maximization regularizer is introduced to achieve extensive exploration of each modality. Quantitative and qualitative evaluations on the AVSBench demonstrate the effectiveness of our approach, leading to a new state-of-the-art for AVS, with a 3.84 mIOU performance leap on the challenging
MS3 subset for multiple sound source segmentation. 1.

Introduction
Audio-visual data can work collaboratively towards a better perception of the scene. The audio-visual segmen-tation (AVS) [1] task aims to segment the objects from the video sequence that producing the sound in the audio. On
† Corresponding author (daiyuchao@gmail.com).
This work was done when Yuxin Mao was an intern at Shanghai AI
Laboratory. the one hand, the audio data provides category information for the localization of the object in the video. On the other hand, the visual data provides a sound producer pool with precise structure information of the foreground (sound pro-ducer(s)). Different from conventional multimodal settings, where each modality can be used individually for the target task, audio in AVS task serves as “command” to localize and segment the sound producer(s) from the visual data. In this case, the contribution of audio should be extensively explored for accurate foreground segmentation.
The baseline model [1] focuses on implicit feature fu-sion via audio-visual cross attention.
It purely relies on fitting the discrete samples in the dataset. Although rea-sonable performance is obtained, there are no constraints to guarantee the contribution of each modality, making it hard to decide if the audio data is effectively used, as the model can directly regress the final segmentation maps by only taking the video as input. Fortunately, we discover that each modality for AVS contains both shared and specific in-formation. For example, the audio data includes both the information from the sound producers and the background noise, while the visual data shows the appearance of the entire scene, where the sound producers only take a small portion of it. In our specific task setting, we find modality factorization is suitable to model both the modality-shared representation, i.e. information of the sound producers, and the modality-specific representation toward a better under-standing of the contribution of each modality.
The straightforward solution to learn the feature repre-sentation of the input data is through an auto-encoder (AE) framework. However, AE is mainly used for data com-pression, as the learned feature space is not continuous, which cannot provide a rich semantic correlation of the data. Differently, with latent space regularization, e.g. the latent space is assumed to be Gaussian in variational auto-encoder (VAE) [2], VAE obtains semantic meaningful latent space, which is continuous, and it is also the basic require-ment for reliable latent space factorization.
To learn the semantic correlated feature representation of the AVS data, we propose an Explicit Conditional Mul-timodal Variational Auto-Encoder (ECMVAE) for audio-visual segmentation to learn both the shared and the spe-cific representation in the latent space of each modality.
Our model is built upon a multimodal variational auto-encoder [3, 4], with the Jensen-Shannon divergence to achieve a trade-off between sampling efficiency and sam-ple quality. Based on the latent space factorization, we im-pose constraints for the shared and specific representations to explicitly maximize the contribution of each modality.
Specifically, we first assume that one latent code of the factorized representation should contain independent infor-mation compared to others. Furthermore, for the fused rep-resentation, we further claim that it should be more infor-mative for the target task compared with each modality.
To achieve the former, we propose an information orthog-onality constraint between the factorized representations of each modality to ensure that the modality-shared and modality-specific representations capture different aspects of the audio-visual input. For the latter, we fuse the factor-ized representations of each modality to construct a fused space. Then we introduce a mutual information maximiza-tion regularizer between the fused representations of each modality to extensively explore the contribution of each modality. Extensive experimental results demonstrate that our ECMVAE achieves state-of-the-art AVS performance.
Our pipeline achieves a 3.84 mIOU improvement for the challenging multiple sound source segmentation.
We summarize our main contributions as:
• An explicit semantic correlated feature representation learning framework for audio-visual segmentation is proposed with latent space factorization to capture both the modality-shared and specific representations.
• Based on the latent space factorization, we intro-duce a unimodal orthogonality constraint between the shared and specific representations and the cross-modal mutual-information maximization regularizer to extensively explore the contribution of each modality.
• State-of-the-art segmentation performance is achieved, showing both the effectiveness of each module and the contribution of each modality. 2.