Abstract
Multi-turn textual feedback-based fashion image re-trieval focuses on a real-world setting, where users can it-eratively provide information to refine retrieval results un-til they find an item that fits all their requirements. In this work, we present a novel memory-based method, called
FashionNTM, for such a multi-turn system. Our frame-work incorporates a new Cascaded Memory Neural Turing
Machine (CM-NTM) approach for implicit state manage-ment, thereby learning to integrate information across all past turns to retrieve new images, for a given turn. Un-like vanilla Neural Turing Machine (NTM), our CM-NTM operates on multiple inputs, which interact with their re-spective memories via individual read and write heads, to learn complex relationships. Extensive evaluation results show that our proposed method outperforms the previous state-of-the-art algorithm by 50.5%, on Multi-turn Fash-ionIQ [60] – the only existing multi-turn fashion dataset currently, in addition to having a relative improvement of 12.6% on Multi-turn Shoes – an extension of the single-turn Shoes dataset [5] that we created in this work. Fur-ther analysis of the model in a real-world interactive set-ting demonstrates two important capabilities of our model – memory retention across turns, and agnosticity to turn order for non-contradictory feedback. Finally, user study results show that images retrieved by FashionNTM were fa-vored by 83.1% over other multi-turn models. 1.

Introduction
Image retrieval has been extensively studied in the com-puter vision community, both using classical approaches
[10, 52, 25] and recently, using learning-based techniques
[2, 15, 41, 43, 23]. Existing works can be grouped based on input queries considered – from image-only queries, com-monly known as Content-Based Image Retrieval (CBIR)
*Work primarily done during internship at Amazon. Additional details are avail-able at https://sites.google.com/eng.ucsd.edu/fashionntm.
Figure 1: Illustration of multi-turn fashion image retrieval. Ini-tially (Turn 1), the system receives an initial query image, and a textual feedback mentioning the user’s desired changes. The model then retrieves a ranked list of closest matching images. Sub-sequently, the user keeps refining their choice by providing more feedback, while the model retrieves newer images by considering both current and past feedback. This continues until the multi-turn system has successfully obtained the final retrieved image (Turn
N) with all the desired properties mentioned across every past turn.
[35, 39, 48], to attributes [18], sketches [44], and natural language [32, 61]. However, most of these methods do not incorporate interactive user feedback, which is necessary for a personalized task such as fashion retrieval.
Textual feedback-based fashion image retrieval allows users to refine online shopping search results by providing information about how the results differ from their desired product (e.g., “a dress like this but darker in color”). Sev-eral approaches for implementing such a system have been proposed recently [14, 7, 31, 56, 17, 60, 63], which involve learning a joint representation via multi-modal information fusion across the query (reference) image and the associ-ated feedback, and using it to retrieve the closest matching image in the database (product catalog) as the target.
Popular methods for fashion retrieval task [14, 31, 7,
4, 23] involve single-turn exchange of information, where users provide feedback exactly once to update the search results. However, this is not characteristic of the real-world setting as online shopping customers typically start with a general idea of what they want and iteratively update the re-quirements until they find something that matches their de-sired features. This usually involves providing additional at-tributes, or modifying previously specified features in each turn to refine the search results. The ideal feedback-based fashion image retrieval system is, hence, inherently multi-turn, as illustrated in Figure 1.
There are two major challenges associated with multi-turn image retrieval. First, there is a lack of sufficient training and evaluation datasets – despite the abundance of single-turn fashion retrieval datasets, to the best of our knowledge, there is only one publicly available multi-turn fashion image retrieval dataset [60] currently. This is be-cause labeling a sequence of images while ensuring conti-nuity, consistency, and uni-directional information flow is a difficult problem. Thus, to facilitate research in this domain, we created a new fashion image retrieval dataset to allow for further benchmarking. Second, generalizing performance to real-world dynamic user interactive cases is non-trivial – as this is still a relatively new research domain, most exist-ing algorithms do not generalize beyond the training dataset to consider multiple turns of interactive feedback. In this work, we propose a novel memory-based framework to ex-plicitly consider sequential feedback from users across mul-tiple turns to retrieve desired items, both for the static image datasets, as well as real-world dynamic users.
Sequential modeling is a relatively mature field of re-search. However, a majority of the existing approaches
[46, 27, 49, 20, 9] do not maintain an explicit memory, and therefore cannot learn long and complex information.
Vanilla memory network-based methods, which explicitly maintain an external memory, could be used for retaining past information, but they do not provide a robust mecha-nism to iteratively update their memory [55, 51]. In con-trast, Neural Turing Machines (NTMs) [16] provide a fully differentiable model with sophisticated read and write op-erations to extract and update historical information in its explicit memory via an attention mechanism. Therefore, in this work, we build on NTMs to develop a novel framework for the multi-turn retrieval task. We further propose a novel
Cascaded Memory Neural Turing Machine (CM-NTM) that allows us to encode multiple relationships from the features of a particular turn and store them over time across multi-ple memories in a multi-turn setting. This is similar to how multi-head attention (MHA) operates for transformers [53].
To ensure that the individual memories effectively utilize each other’s information, we link them together in a cas-caded fashion. Evaluation results demonstrate that our pro-posed approach improves the retrieval performance as com-pared to the previous state-of-the-art by 50.5% on Multi-turn FashionIQ, and by 12.6% on Multi-turn Shoes.
In summary, we make the following contributions. First, we propose a state-of-the-art memory-based framework, called FashionNTM, for multi-turn feedback-based fashion image retrieval, that uses an external memory to learn com-plex long-term relationships. Second, we develop a novel
Cascaded Memory Neural Turing Machine (CM-NTM), that extends NTM to learn relationships across multiple in-puts via additional controllers and read/write heads in a cascaded fashion. Third, we conduct experiments to show that the proposed approach outperforms existing state-of-the-art retrieval models by 50.5% on Multi-turn FashionIQ
[60], and around 12.6% on the multi-turn version of Shoes dataset [5] respectively. Additionally, by performing an in-teractive analysis, we demonstrated two important capabil-ities of our multi-turn system – memory retention across turns, and agnosticity to turn order for non-contradictory feedback. Finally, a user study result shows that on an aver-age, the images retrieved by our model are preferred 83.1% more than those from other multi-turn methods. 2.