Abstract
Continual learning (CL) can help pre-trained vision-language models efficiently adapt to new or under-trained data distributions without re-training. Nevertheless, dur-ing the continual training of the Contrastive Language-Image Pre-training (CLIP) model, we observe that the model’s zero-shot transfer ability significantly degrades due to catastrophic forgetting. Existing CL methods can miti-gate forgetting by replaying previous data. However, since the CLIP dataset is private, replay methods cannot access the pre-training dataset. In addition, replaying data of pre-viously learned downstream tasks can enhance their per-formance but comes at the cost of sacrificing zero-shot per-formance. To address this challenge, we propose a novel method ZSCL to prevent zero-shot transfer degradation in the continual learning of vision-language models in both feature and parameter space. In the feature space, a ref-erence dataset is introduced for distillation between the current and initial models. The reference dataset should have semantic diversity but no need to be labeled, seen in pre-training, or matched image-text pairs.
In parame-ter space, we prevent a large parameter shift by averag-ing weights during the training. We propose a more chal-lenging Multi-domain Task Incremental Learning (MTIL) benchmark to evaluate different methods, where tasks are from various domains instead of class-separated in a sin-gle dataset. Our method outperforms other methods in the traditional class-incremental learning setting and the
MTIL by 9.7% average score. Our code locates at https:
//github.com/Thunderbeee/ZSCL. 1.

Introduction
Most deep learning models can access all the data during training [27, 16, 12]. If we want to expand a model’s knowl-edge, such as learning a newly found animal species [41], we can re-train the model by adding new classes to the training dataset. However, re-training a large model is
Figure 1. a) Conventional CL learns distinct task-specific heads, while CL with vision-language models can predict both learned tasks and out-of-distribution tasks. b) Accuracy (%) changes dur-ing CL of four datasets on 11 datasets. Our method is superior to others in preventing the forgetting of both zero-shot transfer abil-ity and new knowledge. costly. crementally learns task one after another.
In contrast, continual learning (CL) [33, 47] in-It can reduce
this cost by only learning the new data, thereby present-ing itself as an efficient alternative to conventional learn-ing methods [43, 34, 5]. Nonetheless, a model tends to forget previous information catastrophically when learning new tasks [47, 33, 59]. The “catastrophic forgetting” phe-nomenon is a great challenge for CL.
Recently, vision-language models have shown power-ful zero-shot transfer ability [44, 23, 32]. They can give zero-shot predictions without any training examples of a task. However, the performance on some tasks is poor due to insufficient relevant image-text pairs in the pre-training datasets. For example, it is difficult for CLIP [44] to distinguish among digital numbers, with an accuracy on
MNIST [8] below 60% much lower than a naively trained
CNN [29]. If we want to widen the knowledge in the vision-language model by re-training, the computational cost is too large (e.g., CLIP is pretrained on 400 million image-text pairs). Fine-tuning downstream tasks achieves high per-formance, but one model for a task takes much memory, and the model is not reusable. Prompt learning [64, 63] keeps the backbone parameters unchanged. However, it is only effective with limited training data due to a lim-ited prompt length [64, 24].
In contrast, continual learn-ing makes learning new knowledge a lifelong process for the vision-language model. The continually learned model can handle any image-text input and can be further used for downstream tasks [11, 53].
We find that existing CL methods hardly prevent the for-getting phenomenon for zero-shot transfer ability in con-tinual learning of a pre-trained vision-language model. As shown in Fig. 1 (a), the CL with a pre-trained vision-language model differs from the traditional one. Besides forgetting previously learned task knowledge, the CLIP-based CL suffers from forgetting pre-training knowledge, namely a degradation of zero-shot transfer ability. For the replay-based CL methods [47, 50, 36, 21, 28, 42], the dataset during pre-training may be private and inaccessi-ble during fine-tuning. For distillation-based CL meth-ods [33, 10, 13, 11], they do not lay enough emphasis on the pre-trained model. On the one hand, a large model state change hinders tasks thereafter from using high-quality fea-ture representations. On the other hand, it significantly de-grades zero-shot performance on unseen datasets.
Our method ZSCL protects the Zero-Shot transfer abil-ity during Continual Learning. We view the knowledge stored in the pre-training model from two perspectives: a well-learned feature space and a good value in the parame-ter space. In feature space, we re-design previous distilla-tion loss [18, 47] with different loss styles, teacher models, and data sources. We find the original CLIP model, as op-posed to the newly acquired model, is the best option for the teacher model. Instead of using data collected from pre-vious tasks [47] or current task [18], we find a reference dataset with diverse semantics (e.g., images sampled from
ImageNet) is a good option for distillation loss. The refer-ence images need not be labeled or matched with the text.
Preserving the relative similarity between reference images and texts makes the feature space deviate little from the original. In the parameter space, WiSE-FT [58] proposes in-terpolating the initial and fine-tuned model for better perfor-mance. Inspired by this, we ensemble the weights through-out continuous training to prevent a significant shift from the initial CLIP, which can be seen as interpolating mod-els of different zero-shot transfer and downstream task per-formance tradeoffs. The weight ensemble method is more stable and not sensitive to hyper-parameters.
To better evaluate our method, we propose a new bench-mark Multi-domain Task Incremental Learning (MTIL).
Previous CL tasks are crafted by separating classes in one dataset [14, 60, 65], where the images and classes are within a single domain. In contrast, MTIL consists of data from different sources requiring different expert knowledge. It comprises 11 tasks ranging from animal species to aircraft series recognition. As displayed in Fig. 1 (b), when sequen-tially training CLIP on 11 datasets, the drop in the perfor-mance of task i after training task i is the traditional for-getting phenomenon. The degradation in the accuracy com-pared to the original zero-shot one before training task i represents the forgetting in zero-shot transfer ability. Our method better protects the zero-shot transfer ability and preserves the learned knowledge. We outperform previ-ous methods in both conventional class-incremental learn-ing and MTIL settings. In Fig. 1 (b),
To summarize, our contributions are as follows:
• We investigate continual training with the vision-language model and demonstrate the importance of preserving zero-shot transfer ability. A more challeng-ing benchmark MTIL is proposed to evaluate CL meth-ods where the tasks come from distinct domains.
• We propose a novel method ZSCL to mitigate the catastrophic forgetting problem in continual learning of the vision-language model by distillation in the fea-ture space and weight ensemble in the parameter space.
• The proposed ZSCL outperforms all state-of-the-art methods across multiple benchmark datasets. On 10 steps CL of CIFAR100 and TinyImageNet, our method outperforms the best of previous ones by 7.7% and 6.0% for the Last accuracy. On MTCL, ZSCL outper-forms others by 10.9% on Transfer and 9.7% on Avg. scores. 2.