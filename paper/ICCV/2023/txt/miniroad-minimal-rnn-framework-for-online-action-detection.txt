Abstract
Online Action Detection (OAD) is the task of identifying actions in streaming videos without access to future frames.
Much effort has been devoted to effectively capturing long-range dependencies, with transformers receiving the spot-light for their ability to capture long-range temporal struc-tures. In contrast, RNNs have received less attention lately, due to their lower performance compared to recent methods that utilize transformers. In this paper, we investigate the underlying reasons for the inferior performance of RNNs compared to transformer-based algorithms. Our findings indicate that the discrepancy between training and infer-ence is the primary hindrance to the effective training of
RNNs. To address this, we propose applying non-uniform weights to the loss computed at each time step, which al-lows the RNN model to learn from the predictions made in an environment that better resembles the inference stage.
Extensive experiments on three benchmark datasets, THU-MOS, TVSeries, and FineAction demonstrate that a minimal
RNN-based model trained with the proposed methodology performs equally or better than the existing best methods with a significant increase in efficiency. The code is avail-able at https://github.com/jbistanbul/MiniROAD. 1.

Introduction
Online Action Detection (OAD) is a challenging task that involves identifying actions taking place in a stream-ing video without access to future frames. Since its intro-duction [7], OAD has gained great attention in research cir-cles, as it has numerous real-world applications, such as au-tonomous driving [20], smart surveillance systems [26, 27], and anomaly detection [31]. These applications require the ability to process streaming videos in real-time, making
OAD an essential component of many advanced systems.
As a result, there has been a growing interest in developing more accurate and efficient methods for performing OAD.
To address the challenges of OAD, recent efforts have focused on developing novel models for analyzing stream-ing videos in real-time. The conventional process of OAD consists of two stages. In the first stage, frames are con-Figure 1: Performance of recent OAD methods with respect to GFLOPs and the model’s size. verted into a sequence of features by a feature extractor.
In the second stage, the extracted features are processed to capture the temporal information required for perform-ing OAD. Much attention on OAD research has been paid to this second stage where the two mainstream models are
RNNs [11, 30, 18, 5] and Transformers [9, 10, 33].
Each model has its strengths and weaknesses. RNNs can capture temporal transitions effectively by modeling se-quences recurrently but are known for their training diffi-culties [23, 30]. Transformers can capture long-term de-pendencies through self-attention, however, attention com-putation is computationally expensive. Although recent re-search has shifted towards using Transformer models, our paper revisits RNN, which has been overlooked in recent studies. temporal
In this work, we show that RNN’s intrinsic inductive bias—prioritizing the current input while preserving mean-information from the histories—makes ingful them well-suited for OAD. Unlike in natural language pro-cessing, where a distant word can significantly affect the next sequence prediction, we observe that in OAD, the cur-rent and its neighboring frames have a dominant influence on the current prediction.
In addition, RNN is efficient, as it requires a small computational overhead compared to
the transformer models, making them well-suited for online tasks that require running in real-time.
Despite its architecture being well-suited for the OAD task, RNN has been recently overlooked due to the promis-ing results shown by transformer-based models. The pre-vailing reasoning behind the lower performance of RNN based method was the difficulty of learning long-range dependencies [37, 23].
In this paper, we find that it is the training-inference discrepancy that hinders the effective training of RNNs.
During inference, the video is processed frame-by-frame in a streaming fashion. During training, however, the video is divided into multiple clips, which are then fed to the model, as illustrated in Figure 2. This creates a significant discrepancy between the two phases since videos are usu-ally much longer than the training clips. Consequently, the clips provide limited temporal information, leading to a re-stricted view of the video during training. This limitation is not as problematic for recent OAD models using trans-formers as it is for models using RNNs. This is because recent transformer models utilize the temporal information between clips using explicit long-term memory even during training. At the same time, RNNs do not since they receive a non-informative hidden state initialized as zero at the be-ginning of the clip. As a result, RNNs make most predic-tions during training with the hidden state that lacks tempo-ral information, whereas it makes most predictions during inference with the hidden state accumulated with sufficient temporal information.
We demonstrate that addressing the training and infer-ence discrepancy in RNNs alone can significantly improve their performance without requiring any changes to their architecture. Specifically, we find that introducing non-uniform weights to losses at different time steps can al-leviate this discrepancy, which is a primary factor con-tributing to RNN’s suboptimal performance. This ap-proach differs from the conventional assumption of aver-aging the loss over the joint prediction, as we demonstrate that non-uniform weights to the loss are essential for effec-tively training the model. We demonstrate the effectiveness of our methodology using Minimal Recurrent neural net-work for Online Action Detection (MiniROAD), which is a lightweight GRU based model. Figure 1 shows the compar-ison of our model with other recent models in terms of the model’s performance, GFLOPs, and size. It shows that our method with only ∼ 0.4% computational power needed for the previous state-of-the-art, is able to achieve better perfor-mance.
Our model is evaluated on conventional OAD bench-mark datasets, including THUMOS’14 and TVSeries, as well as on the recently introduced FineAction dataset. As conventional datasets used for benchmarking OAD tasks are relatively small, the much larger FineAction dataset offers a more challenging testbed that better depicts real-life sce-narios. Larger datasets like FineAction open up more pos-sibilities for future work on OAD and hence our results can serve as a strong baseline. Our experiments show that our proposed methodologies achieve on-par or better results on the benchmarked dataset with significant improvements in efficiency.
The main contributions of this work are:
• We revisit the potential of RNNs and identify the training-inference discrepancy as a primary cause of their lower performance. To address this issue, we pro-pose a simple yet effective method that makes RNN both effective and efficient in capturing the essential temporal information for online action detection.
• Extensive experimental results show that the pro-posed method performs favorably against the state-of-the-art with high efficiency; our model with only
∼ 0.4% computational overhead compared to the ex-isting methods, achieves on par or better performance on a challenging benchmark dataset, with ∼17% per-formance improvement over the state-of-the-art in a large-scale dataset.
• We expand our methodology and carry out perfor-mance evaluation on Action Anticipation and show the applicability of our method on other online tasks. 2.