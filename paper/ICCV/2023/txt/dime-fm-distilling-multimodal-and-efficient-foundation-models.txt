Abstract
Large Vision-Language Foundation Models (VLFM), such as CLIP, ALIGN and Florence, are trained on large-scale datasets of image-caption pairs and achieve superior transferability and robustness on downstream tasks, but they are difficult to use in many practical applications due to their large size, high latency and fixed architectures. Unfortu-nately, recent work shows training a small custom VLFM for resource-limited applications is currently very difficult using public and smaller-scale data. In this paper, we intro-duce a new distillation mechanism (DIME-FM) that allows us to transfer the knowledge contained in large VLFMs to smaller, customized foundation models using a relatively small amount of inexpensive, unpaired images and sentences.
We transfer the knowledge from the pre-trained CLIP-ViT-L/14 model to a ViT-B/32 model, with only 40M public im-ages and 28.4M unpaired public sentences. The resulting model “Distill-ViT-B/32” rivals the CLIP-ViT-B/32 model pre-trained on its private WiT dataset (400M image-text pairs): Distill-ViT-B/32 achieves similar results in terms of zero-shot and linear-probing performance on both Ima-geNet and the ELEVATER (20 image classification tasks) benchmarks. It also displays comparable robustness when evaluated on five datasets with natural distribution shifts from ImageNet. Please refer to our project page for code and more details. 1.

Introduction
In contrast to neural networks learnt to solve a single target vision task (i.e. task-specific models) [26, 48, 70, 13, 68, 45], CLIP [55] and other Vision-Language “Foundation
Models” (VLFMs) [41, 84] achieve superior accuracy on diverse novel downstream tasks and improved robustness to natural domain shifts during inference. At the same time, small and customizable VLFMs are in high demand for many applications that have limited computational resources (AV,
AR/VR and other edge devices). Unfortunately, only a few labs in the world can afford the large-scale vision-language datasets (e.g. WiT [55] with 400M image-text pairs) and
†Work done when interning at Meta AI.
Figure 1: Conceptual Figure of our Vision-Language Knowl-edge Distillation DIME-FM. We distill the knowledge from a large
VLFM “CLIP-ViT-L/14’ pretrained on 400M private image-text paired dataset. We only use public unpaired image and text cor-pora as inputs. Our Distill-ViT-B/32 rivals CLIP-ViT-B/32 in both transferability and robustness. ZS: Zero-Shot, LP: Linear Probing. the immense computing resources required to train VLFMs.
Efforts to re-create VLFMs on public data [82, 64, 15]) either fall short on accuracy or require even more expensive training on huge datasets of images paired with captions (e.g. over 5B pairs [61]).
Instead of pretraining, model distillation used to offer a convenient way to obtain a smaller custom model. Re-cent work distills CLIP specifically for one or a few target tasks (i.e. task-specific distillation). For example, some works [78, 79, 87, 54] distill the CLIP’s image feature maps for better visual feature representations. BeamCLIP [36] distills CLIP logits for a single target image classification task, e.g. ImageNet-1K [16]. More recently, CLIP-TD [76] distills CLIP to solve three specific vision-language tasks.
Even though these task-specific distillation works achieve good performance for the specialized downstream task, they are not scalable to solve new downstream tasks by zero-shot transferring. There is no approach for distilling VLFMs to another foundation model which preserves transferability to novel tasks and robustness to domain shifts. Due to the un-affordable large-scale pretraining and the lack of foundation-model distillation mechanism, practitioners must rely on the few labs to release smaller VLFMs, and cannot easily customize their size or architecture.
In this work, we successfully distill smaller custom
VLFMs using only smaller-scale public data, but achiev-ing comparable transferability and robustness as if they were pre-trained on the large-scale data. Specifically , we trans-fer the knowledge from the released CLIP-ViT-L/14 [55] to our small VLFM “Distill-ViT-B/32”. During the distil-lation, we adopt only 40M images from public datasets and 28.4M unpaired sentences. Remarkably, with less than one-tenth of CLIP’s pretraining dataset WiT, Distill-ViT-B/32 achieves comparable transferability and robustness to CLIP-ViT-B/32 [55] (see Fig. 1).
To accomplish this, we propose a novel distillation mecha-nism to DIstill Multimodal and Efficient Foundation Models (DIME-FM) from CLIP. In standard distillation of image classification models with the fixed categories (i.e. fixed-vocabulary models), the class scores (logits) are matched between the teacher and student models [31, 37, 5, 52, 9].
However, since VLFMs do not have fixed-vocabulary logits, we instead match similarity of images to sentences (i.e. open-vocabulary logits) to retain the transferability (especially zero-shot ability) and robustness of VLFMs. We perform a careful ablation study of how “vocabulary”, determined by training sentences, affects the student model’s perfor-mance and find that it is crucial to perform distillation with a visually-related vocabulary rather than a random vocabulary.
To construct a visually-related distillation text corpus, we propose an efficient algorithm that selects visually-grounded sentences (i.e. sentences which describe the visual world) from an NLP corpus rather than require the expensive human-annotated image captions or use noisy web-crawled image-related text. On top of text selection algorithm, we design two distillation losses to augment open-vocabulary logits in VLFM and empirically show that our novel distillation losses benefit vision-language (VL) knowledge distillation.
To summarize, we make three contributions in this paper: 1. We propose a vision-language knowledge distillation mechanism DIME-FM to transfer knowledge of pre-trained huge VLFMs to small foundation models with smaller-scale public images and unpaired sentences. 2. We distill the pre-trained CLIP-ViT-L/14 to Distill-ViT-B/32, with only unpaired 40M public images and 28.4M sentences. Notably, our Distill-ViT-B/32 rivals the CLIP-ViT-B/32 that was pre-trained on private 400M image-text paired data in both transferability and robustness. 3. Our proposed DIME-FM consists of an efficient algo-rithm to construct a visually-grounded text corpus from an NLP corpus and two specific distillation losses to aug-ment open-vocabulary logits in VL distillation. 2.