Abstract
We present Text2Room1, a method for generating room-scale textured 3D meshes from a given text prompt as input.
To this end, we leverage pre-trained 2D text-to-image mod-els to synthesize a sequence of images from different poses.
In order to lift these outputs into a consistent 3D scene rep-resentation, we combine monocular depth estimation with a text-conditioned inpainting model. The core idea of our approach is a tailored viewpoint selection such that the con-tent of each image can be fused into a seamless, textured 3D mesh. More specifically, we propose a continuous align-ment strategy that iteratively fuses scene frames with the existing geometry to create a seamless mesh. Unlike exist-ing works that focus on generating single objects [56, 41] or zoom-out trajectories [18] from text, our method generates complete 3D scenes with multiple objects and explicit 3D geometry. We evaluate our approach using qualitative and quantitative metrics, demonstrating it as the first method to generate room-scale 3D geometry with compelling textures from only text as input.
∗ joint first authorship 1https://lukashoel.github.io/text-to-room 1.

Introduction
Mesh representations of 3D scenes are a crucial com-ponent for many applications, from AR/VR asset creation to computer graphics, yet creating these 3D assets remains a painstaking process that requires considerable expertise.
In the 2D domain, recent works have successfully cre-ated high-quality images from text using generative mod-els, such as diffusion models [65, 58, 67]. These methods significantly reduce the barriers to creating images that con-tain a user’s desired content, effectively helping towards the democratization of content creation. An emerging line of work has sought to apply similar methods to create 3D mod-els from text [9, 56, 29, 41, 38], yet existing approaches come with a number of significant limitations and lack the generality of 2D text-to-image models.
One of the core challenges of generating 3D models is coping with the lack of available 3D training data, as 3D datasets are vastly smaller than those available in many other applications, such as 2D image synthesis. For ex-ample, methods that directly use 3D supervision, such as
Chen et al. [9], are often limited to datasets of simple shapes, such as ShapeNet [8]. To address these data lim-itations, recent methods [56, 29, 41, 38, 88] lift the expres-sive power of 2D text-to-image models into 3D by formu-lating 3D generation as an iterative optimization problem in the image domain. This allows them to generate 3D ob-jects stored in a radiance field representation, demonstrating the ability to generate arbitrary (neural) shapes from text.
However, these methods cannot easily be extended to cre-ate room-scale 3D structure and texture. The challenge of generating large scenes is ensuring that the generated out-put is dense and coherent across outward-facing viewpoints, and that these views contain all of the required structures, such as walls, floors, and furniture. Additionally, a mesh remains a desired representation for many end-user tasks, such as rendering on commodity hardware (which requires an additional conversion step as presented in Lin et al. [41]).
To address these shortcomings, we propose a method that extracts scene-scale 3D meshes from off-the-shelf 2D text-to-image models. Our method iteratively generates a scene through inpainting and monocular depth estimation.
We produce an initial mesh by generating an image from text, and backproject it into 3D using a depth estimation model. Then, we iteratively render the mesh from novel viewpoints. From each one, we fill in holes in the rendered images via inpainting, then fuse the generated content into the mesh (Fig. 1a).
Our iterative generation scheme has two important de-sign considerations: how we choose the viewpoints, and how we merge generated scene content with the existing mesh. We first select viewpoints from predefined trajec-tories that will cover large amounts of scene content, then adaptively select viewpoints that close remaining holes.
When merging generated content with the mesh, we align the two depth maps to create smooth transitions, and remove parts of the mesh that contain distorted textures. Together, these decisions lead to large, scene-scale 3D meshes with compelling textures and consistent geometry (Fig. 1b), that can represent a wide range of rooms.
To summarize, our contributions are:
• Generating 3D meshes of room-scale indoor scenes with compelling textures and geometry from any text input.
• A method that leverages 2D text-to-image models and monocular depth estimation to lift frames into 3D in an iterative scene generation. Our proposed depth alignment and mesh fusion steps, enable us to create seamless and undistorted geometry and textures.
• A two-stage tailored viewpoint selection that samples camera poses from optimal positions to first create the room layout and furniture and then close any remaining holes, creating a watertight mesh. 2.