Abstract
Referring Video Object Segmentation (RVOS) is to segment the object instance from a given video, according to the tex-tual description of this object. However, in the open world, the object descriptions are often diversified in contents and flexible in lengths. This leads to the key difficulty in RVOS, i.e., various descriptions of different objects are corre-sponding to different temporal scales in the video, which is ignored by most existing approaches with single stride of frame sampling. To tackle this problem, we propose a con-cise Hybrid Temporal-scale Multimodal Learning (HTML) framework, which can effectively align lingual and visual features to discover core object semantics in the video, by learning multimodal interaction hierarchically from different temporal scales. More specifically, we introduce a novel inter-scale multimodal perception module, where the language queries dynamically interact with visual features across temporal scales. It can effectively reduce complex object confusion by passing video context among different scales. Finally, we conduct extensive experiments on the widely used benchmarks, including Ref-Youtube-VOS,
Ref-DAVIS17, A2D-Sentences and JHMDB-Sentences, where our HTML achieves state-of-the-art performance on all these datasets. 1.

Introduction
Referring Video Object Segmentation (RVOS) has wit-nessed the growing interest, due to its wide applications in visual editing, virtual reality, human-robotic interaction and so on. Different from the traditional vision-only VOS,
RVOS aims to segment the object instance from an input video, according to an open-world description about the re-† Corresponding author.
Figure 1: Referring descriptions in different lengths. (a) The description is simple containing only the category name. (b) The description is complicated with movement and position of the object. Single-scale baseline (e.g., four frames in (a) and two frames in (b)) fails to segment the referred object, while our hybrid-scale HTML succeeds.
More discussion can be found in introduction. ferred object. In this case, the model has to learn both visual and textual contents comprehensively, in order to discover the underlying object by multimodal interaction.
Recent studies [4, 12, 28, 29] have shown that, cross-modal attention is an effective way to bridge the gap be-tween vision and language in RVOS. However, these ap-proaches perform vision-language interactions with video frames sampled from a single temporal scale, which may limit their power to infer the referred object with accurate segmentation. The main reason is that, the open-world de-scriptions vary in length and contain rich semantics about the referred object, e.g., where it is, how it moves, which objects it interact with. Apparently, such diversified texts are corresponding to various temporal-scale snippets.
For example, the language query in Fig. 1 (a) is a ten-nis ball. Such a short description is corresponding to the ball located at a small region in the middle two frames. If the single-scale baseline samples four frames as input, it will fail to segment the referred object. This is because it overlooks the dog in the center place among all these four frames, while lacking the detailed understanding in the mid-dle two frames. Alternatively, the language query in Fig. 1 (b) is a sheep top second right moves down and comes out of the circle. Such a long description is corresponding to the particular sheep in the group, which moves across frames.
If the single-scale baseline samples two frames as input, it will fail to segment the referred object. This is because it is misled by the subtle movement of sheep group in only two frames, without understanding how each sheep moves from the adjacent frames.
To tackle this difficulty, we propose a concise Hy-brid Temporal-scale Multimodal Learning (HTML) Frame-work for RVOS, which can alleviate object confusion by language-vision interactions across different temporal scales. Specifically, we sample video frames according to different temporal scales. For each temporal scale, we intro-duce an intra-scale multimodal perception module, which can effectively exploit core visual semantics within the frames at this temporal scale, by mutual enhancement be-tween textual and visual embeddings. Then, we design an inter-scale multimodal perception module, where linguis-tic embeddings dynamically interact with visual features across temporal scales.
In this case, we can hierarchi-cally leverage object context from all the temporal scales to boost RVOS. Finally, we evaluate our HTML on a num-ber of benchmarks, including Ref-Youtube-VOS [22], Ref-DAVIS17 [9], A2D-Sentences and JHMDB-Sentences [7].
The extensive experiments have shown that, our HTML achieves the state-of-the-art performance on all of them.
Overall we make three contributions in this paper:
• Concise and unified learning framework: our Hybrid
Temporal-scale Multimodal Learning (HTML) frame-work hierarchically constructs multimodal interactions via different strides of frame sampling, which can mu-tually enhance embeddings from both modalities for ac-curate segmentation.
• Effective multimodal perception module: our Cross-scale
Multimodal Perception (CMP) module can effectively reduce complex object confusions with intra-scale and inter-scale multimodal perceptions, where linguistic and visual features interact across temporal scales.
• State-of-the-art performance on the widely-used bench-marks, which shows the superiority of our framework.
Specifically, on Ref-Youtube-VOS [22], our method with
ResNet-50 achieves 57.8 in L&F, outperforming the recent SOTA method [29] with ResNet-101. 2.