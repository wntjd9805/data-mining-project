Abstract
Recent vision transformers, large-kernel CNNs and
MLPs have attained remarkable successes in broad vision tasks thanks to their effective information fusion in the global scope. However, their efficient deployments, espe-cially on mobile devices, still suffer from noteworthy chal-lenges due to the heavy computational costs of self-attention mechanisms, large kernels, or fully connected layers.
In this work, we apply conventional convolution theorem to deep learning for addressing this and reveal that adap-tive frequency filters can serve as efficient global token mixers. With this insight, we propose Adaptive Frequency
Filtering (AFF) token mixer. This neural operator trans-fers a latent representation to the frequency domain via a Fourier transform and performs semantic-adaptive fre-quency filtering via an elementwise multiplication, which mathematically equals to a token mixing operation in the original latent space with a dynamic convolution kernel as large as the spatial resolution of this latent representation.
We take AFF token mixers as primary neural operators to build a lightweight neural network, dubbed AFFNet. Exten-sive experiments demonstrate the effectiveness of our pro-posed AFF token mixer and show that AFFNet achieve su-perior accuracy and efficiency trade-offs compared to other lightweight network designs on broad visual tasks, includ-ing visual recognition and dense prediction tasks. Code is available at https://github.com/microsoft/
TokenMixers. 1.

Introduction
Remarkable progress has been made in ever-changing vi-sion network designs to date, wherein effective token mix-ing in the global scope is constantly highlighted. Three ex-isting dominant network families, i.e., Transformers, CNNs and MLPs achieve global token mixing with their respective ways. Transformers [16, 63, 40, 12, 80, 74] mix tokens with self-attention mechanisms where pairwise correlations be-tween query-key pairs are taken as mixing weights. CNNs
*This work was done when Zhipeng Huang was an intern at MSRA.
†Correspondence to zhizzhang@microsoft.com.
Figure 1. Comparison of Top-1 accuracy on ImageNet-1K [55] be-tween our proposed AFFNet to some state-of-the-art lightweight networks that have global token mixing. The bubble size corre-sponds to FLOPs. achieve competitive performance with transformers by scal-ing up their kernel sizes [51, 15, 38, 9]. MLPs [60, 25, 35] provide another powerful paradigm via fully connections across all tokens. All of them are effective but computation-ally expensive, imposing remarkable challenges in practical deployments, especially on edge devices.
Recently, there is increased attention on improving the efficiency of token mixing in transformers. Some works
[30, 40, 12, 22, 45, 50, 31, 75] squeeze the scope of to-ken mixing in different ways to compromise the represen-tation capacities of neural networks for their efficiencies.
Other works reduce the complexity of the matrix operations in self-attention by making use of the associativity property of matrix products [29] or low-rank approximation methods
[20, 76]. These methods all sacrifice the expressiveness of neural networks and lead to unsatisfactory performance of efficient network designs. A general-purpose global token mixing for lightweight networks is still less explored. Better trade-off between accuracy and efficiency for global-scope token mixing is worthy of further study.
In this work, we reveal that adaptive frequency filters can serve as efficient global token mixers, inspired by the con-volution theorem [44, 53, 48] widely used in conventional signal processing. This theorem states that a convolution in one domain mathematically equals the Hadamard prod-uct (also known as elementwise product) in its correspond-ing Fourier domain. This equivalence allows us to frame
global token mixing as a large-kernel convolution in the la-tent space and efficiently implement this convolution with a Hadamard product operation in the frequency domain by performing Fourier transforms on tokens in the latent space.
Besides large scopes, the adaptability to semantics also matters for token mixing as studied in [13, 8, 70, 1, 72].
This means that the weights for token mixing should be instance-adaptive. Moreover, different semantic attributes of the learned latent representations distribute in different channels [1, 73]. This property poses requirements for channel-specific token mixing wherein the weights of token mixing vary across different channels. From the perspective of framing global adaptive token mixing as a convolution, the kernel of this convolution operation should be not only large but also spatially dynamic. However, it is well known that dynamic convolutions are computationally expensive in common. Large-kernel dynamic convolutions seem ex-tremely prohibitive for efficient/lightweight network de-signs. In this paper, we propose to adopt frequency filter-ing in the Fourier domain with learned instance-adaptive masks as a mathematical equivalent of token mixing us-ing large-kernel dynamic convolutions by making use of the aforementioned convolution theorem. This equivalent could reduce the complexity of token mixing from O(N 2) to O(N log N ) thanks to adopting Fast Fourier Transforms (FFT), which is more computationally efficient.
With the key insight above, we propose Adaptive Fre-quency Filtering (AFF) token mixer. In this neural operator, the latent representations (i.e., a set of tokens) are trans-ferred from its original latent space to a frequency space via a 2D discrete Fourier transform applied spatially. In this way, we get the frequency representations whose spatial po-sitions correspond to different frequency components. We adopt an extremely lightweight network to learn instance-adaptive masks from these frequency representations, and then calculate the Hadamard product between the learned masks and the frequency representations for adaptive fre-quency filtering. The filtered representations are transferred back to the original latent space via an inverse Fourier trans-form. The features after this inverse transform could be viewed as the results of token mixing with depthwise convo-lution kernels whose spatial dimensions are as large as those of latent representations (i.e., the token set). According to the convolution theorem [44], our proposed operation math-ematically equals to taking the tensors of applying an in-verse Fourier transform to the learned masks in the Fourier domain as the corresponding kernel weights and perform convolution with this kernel in the original domain. De-tailed introduction, demonstration and analysis are given in subsequent sections.
Furthermore, we take the proposed AFF token mixer as the primary neural operator and assemble it into an
AFF block together with a plain channel mixer. AFF blocks serve as the basic units for constructing efficient vision backbone, dubbed AFFNet. We evaluate the effectiveness and efficiency of our proposed AFF token mixer by con-ducting extensive ablation study and comparison across di-verse vision tasks and model scales.
Our contributions can be summarized in the following:
• We reveal that adaptive frequency filtering in the latent space can serve as efficient global token mixing with large dynamic kernels, and propose Adaptive Frequency Filter-ing (AFF) token mixer.
• We conduct theoretical analysis and empirical study to compare our proposed AFF token mixer with other re-lated frequency-domain neural operators from the per-spective of information fusion for figuring out what really matters for the effects of token mixing.
• We take AFF token mixer as the primary neural op-erator to build a lightweight vision backbone AFFNet.
AFFNet achieves the state-of-the-art accuracy and effi-ciency trade-offs compared to other lightweight network designs across a broad range of vision tasks. An experi-mental evidence is provided in Fig.1. 2.