Abstract
This paper presents IP-SLT, a simple yet effective frame-work for sign language translation (SLT). Our IP-SLT adopts a recurrent structure and enhances the semantic rep-resentation (prototype) of the input sign language video via an iterative refinement manner. Our idea mimics the be-havior of human reading, where a sentence can be digested repeatedly, till reaching accurate understanding. Techni-cally, IP-SLT consists of feature extraction, prototype ini-tialization, and iterative prototype refinement. The initial-ization module generates the initial prototype based on the visual feature extracted by the feature extraction module.
Then, the iterative refinement module leverages the cross-attention mechanism to polish the previous prototype by ag-gregating it with the original video feature. Through re-peated refinement, the prototype finally converges to a more stable and accurate state, leading to a fluent and appro-priate translation. In addition, to leverage the sequential dependence of prototypes, we further propose an iterative distillation loss to compress the knowledge of the final itera-tion into previous ones. As the autoregressive decoding pro-cess is executed only once in inference, our IP-SLT is ready to improve various SLT systems with acceptable overhead.
Extensive experiments are conducted on public benchmarks to demonstrate the effectiveness of the IP-SLT. 1.

Introduction
Sign language translation (SLT) aims to automatically generate spoken language translations based on sign lan-guage videos, which holds both social significance and aca-demic value. On the one hand, a high-quality SLT sys-tem can greatly facilitate communication between deaf-mute and hearing individuals [4, 11, 12, 41]. On the other hand, SLT as an interdisciplinary research topic necessitates a comprehensive understanding of computer vision [31, 13, 36, 19] and natural language processing [43, 40], given its
*Corresponding authors: Wengang Zhou and Houqiang Li
Figure 1. Illustration of the pipeline of the previous works and our
IP-SLT. (a) The previous studies rely on a one-pass forward pro-cess to generate the final translation. (b) To mitigate the vision-text gap, we introduce the iterative refinement module into the original
SLT system. The refinement module updates the current prototype conditioned on the sign language video, which can be run itera-tively to obtain a better representation of the semantic meaning of the sign language video. involvement with vision and text modalities. As a result,
SLT has emerged as a vital research topic, garnering in-creasing attention [5, 49, 31, 2, 21, 20, 42].
SLT is a challenging task, which faces a tough domain gap between the input video and output text, as well as a limited dataset scale due to costly data collection and an-notation [49, 7, 44, 21]. Since SLT is typically viewed as a sequence-to-sequence mapping problem, the existing SLT systems [23, 5, 7] commonly adopt the one-pass forward pipeline based on encoder-decoder architecture [40, 30] (as shown in Fig. 1 (a)).
In such a framework, the encoder transforms the sign video into its semantic representation (prototype), which is then fed into the decoder to obtain the final translation. However, due to the inherent gap between vision and text, it may be hard to conduct such mapping within the vanilla one-pass architecture.
In this study, we present IP-SLT with the iterative proto-type to boost sign language translation (as shown in Fig. 1 (b)), which is inspired by the human reading process. Dur-ing this process, we note that repeatedly digging into the
source materials is necessary for accurate understanding.
Similarly, when we are trying to translate a sign language video into a sentence, we commonly do not directly write it down. Instead, we would recall and go back to the orig-inal sign video to check our answers. To implement the above idea, our IP-SLT adopts a recurrent structure that en-hances the semantic representation (prototype) of the input sign language video via an iterative refinement process. IP-SLT generally contains three main components, including feature extraction, prototype initialization, and iterative pro-totype refinement. Given a sign video to be translated, we first extract its visual representation, which is then used for generating an initial raw prototype. Subsequently, we iter-atively leverage the attention mechanism [43] to update the prototype toward the semantic meaning of the sign video.
At each iteration, we refine the previous prototype by ag-gregating it with the original visual representation. In this way, the network repeatedly digs the semantic context from the sign video to polish the prototype. Through iterative refinement, the prototype finally converges to a stable and accurate state, producing a high-quality translation.
In addition, our IP-SLT introduces a novel design dis-cussed next. Firstly, to leverage the sequential dependence between different iterations, we further propose the iterative distillation loss which allows the previous prototypes to ob-tain supervision from the final one. Since the final prototype converges to a more stable and accurate state, it is possible for IP-SLT to achieve better performance. Secondly, during training, all predicted prototypes are transformed into their corresponding translations to provide guidance for each it-eration. Our inference process is neat since only the fi-nal prototype is used for the autoregressive decoding pro-cess. Thirdly, our IP-SLT can easily work with different vi-sual backbones. Through end-to-end optimization, our IP-SLT achieves significant performance improvements over the baselines.
In summary, our contributions are three-fold:
• We propose IP-SLT, a novel framework to amelio-rate sign language translation, which iteratively refines the prototypes by aggregating the previous translation progress and the original visual representation.
• We propose an iterative distillation loss to enhance the basic supervision, by leveraging the sequential depen-dence between the outputs at each iteration.
• We conduct extensive experiments to validate the pro-posed method, and show encouraging improved results on the two prevalent benchmarks, i.e., CSL-Daily [49] and PHOENIX-2014T [5]. 2.