Abstract
Most lip-to-speech (LTS) synthesis models are trained and evaluated with the assumption that the audio-video pairs in the dataset are well synchronized. In this work, we demonstrate that commonly used audiovisual datasets such as GRID, TCD-TIMIT, and Lip2Wav can, however, have the data asynchrony issue, which will lead to inaccurate eval-uation with conventional time alignment-sensitive metrics such as STOI, ESTOI, and MCD. Moreover, training an LTS model with such datasets can result in model asynchrony, meaning that the generated speech and input video are out of sync. To address these problems, we first provide a time-alignment frontend for the commonly used metrics to ensure accurate evaluation. Then, we propose a synchronized lip-to-speech (SLTS) model with an automatic synchronization mechanism (ASM) that corrects data asynchrony and penal-izes model asynchrony during training. We evaluated the effectiveness of our approach on both artificial and popu-lar audiovisual datasets. Our proposed method outperforms existing SOTA models in a variety of evaluation metrics. 1.

Introduction
Lip-to-speech (LTS) refers to the task of reconstructing spoken audio from a speaker’s lip movements in a video that lacks sound. It is especially useful in circumstances where audio is missing due to various reasons, such as inadequate recording devices, ambient noise, transmission failures, etc.
Deep learning has significantly advanced this field, with the development of various data-driven deep networks aimed at solving the LTS task.
During the training and evaluation of LTS models, it is often assumed that there is little or no time offset be-tween the corresponding audio-video data pair, as audiovi-sual datasets are usually believed to be well synchronized.
However, as shown in Fig. 2 and studies by others [26], commonly used datasets for training and evaluating LTS models have varying time offsets within their audio-video pairs, which we refer to as data asynchrony. While some
Figure 1: The impact of offsets on alignment-sensitive met-rics such as STOI, ESTOI, and MCD can be significant. A 40ms offset, which is equivalent to a single video frame at 25 fps, can lead to severe degradation in the scores of the original versions of these metrics (i.e. Vanilla). PESQ is less affected due to its alignment mechanism. Our proposed so-lution, which is a time alignment frontend applied to each of the metrics (i.e. Aligned), ensures consistent scores regard-less of the offsets. The results were obtained by computing the scores between the ground truth audio as the reference and its offset versions as the test audios on the test set of
GRID-4S (as described in Section 4.1). datasets such as GRID [4] and TCD-TIMIT [7] have small offsets within ±1 video frame (i.e., ±40ms), others like
Lip2Wav [20] can have larger offsets of multiple video frames.
Data asynchrony can significantly impact the evaluation of LTS models. Even a slight misalignment between the reference and the test audio can have a major impact on the vanilla STOI [24], ESTOI [11] and MCD [16] scores during evaluation. To address this issue, we propose a time-alignment frontend that precedes the computation of alignment-sensitive metrics. This frontend effectively miti-(a) GRID-4S. (b) TCD-TIMIT-LS. (c) Lip2Wav.
Figure 2: AV offsets produced by SyncNet on various datasets with the SyncNet confidence scores noted beneath the speaker
ID. We only consider offsets with confidence scores greater than 3.0. Most of the samples in the GRID-4S and TCD-TIMIT-LS datasets exhibit zero or slight offsets of ±40 ms. In contrast, the Lip2Wav dataset shows larger offsets, with the chess speaker exhibiting -200 to -250 ms and other speakers showing offsets around -80 ms. gates the impact of synchronization errors on conventional time-alignment sensitive metrics, thus ensuring consistent scoring despite data asynchrony, as demonstrated in Fig. 1.
When a model is trained on a dataset with synchroniza-tion errors, it can generate offset audio against the input video during inference. We refer to this issue as model asyn-chrony. To train LTS models that can handle asynchrony in datasets and produce synchronized output, we propose the synchronized lip-to-speech (SLTS) model, which incor-porates an automatic synchronization mechanism (ASM) to ensure synchronization from both the data and model perspectives during training. The ASM overcomes data asynchrony with a data synchronization module (DSM) and prevents model asynchrony using the self-synchronization module (SSM).
In our experiment section, we first validated the robust-ness of the proposed SLTS model on a small-scale artificial dataset with severe data asynchrony issue called GRID-4S-Async, which we created from GRID [4] by adding artificial audio-video offsets uniformly sampled from −150 to 150 ms. Then we tested the SLTS model on popular audiovi-sual datasets, including GRID-4S [4], TCD-TIMIT-LS [7], and Lip2Wav [20]. Our findings demonstrate that SLTS is effective on datasets with either obvious asynchrony that can be seen by the human eye (e.g. GRID-4S-Async and
Lip2Wav [20]), or slight synchronization errors (e.g. single frame or sub-frame offsets, such as in GRID-4S and TCD-TIMIT-LS [7]).
In summary, this paper offers several contributions to the field of lip-to-speech (LTS) synthesis. We begin by identifying two types of asynchrony issues that arise dur-ing the development of LTS models: data asynchrony and model asynchrony. Next, we propose a time alignment fron-tend to enable consistent evaluation regardless of the time offsets in the audiovisual dataset. Following this, we in-troduce a novel synchronized lip-to-speech (SLTS) model, which incorporates an automatic synchronization mecha-nism (ASM) that actively learns audiovisual time offsets during training, aiding in the rectification of data asyn-chrony and alleviating model asynchrony. The SLTS model shows competitive and, in many cases, superior perfor-mance on various datasets, leading to high-quality and syn-chronized audio reconstruction. 2.