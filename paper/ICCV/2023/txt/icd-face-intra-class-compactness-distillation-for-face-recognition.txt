Abstract
Knowledge distillation is an effective model compression method to improve the performance of a lightweight student model by transferring the knowledge of a well-performed teacher model, which has been widely adopted in many computer vision tasks, including face recognition (FR).
The current FR distillation methods usually utilize the
Feature Consistency Distillation (FCD) (e.g., L2 distance) on the learned embeddings extracted by the teacher and student models. However, after using FCD, we observe that the intra-class similarities of the student model are lower than the intra-class similarities of the teacher model a lot.
Therefore, we propose an effective FR distillation method called ICD-Face by introducing intra-class compactness distillation into the existing distillation framework. Specif-ically, in ICD-Face, we first propose to calculate the similarity distributions of the teacher and student models, where the feature banks are introduced to construct suffi-cient and high-quality positive pairs. Then, we estimate the probability distributions of the teacher and student models and introduce the Similarity Distribution Consistency (SDC) loss to improve the intra-class compactness of the student model. Extensive experimental results on multiple benchmark datasets demonstrate the effectiveness of our proposed ICD-Face for face recognition. 1.

Introduction
Face recognition (FR) has been well-investigated for many years. Most of the progress is credited to large-scale training datasets [64, 22], resource-intensive networks with millions of parameters [15, 43] and effective loss functions [7, 52]. Although larger FR models usually ex-hibit better recognition performance, the requirements for huge computational resources are usually prohibitive on mobile and embedded devices. Therefore, how to de-velop lightweight and effective FR models in resource-* Equal contribution. (cid:0) Corresponding author: Jiaheng Liu (liujiaheng@buaa.edu.cn).
Figure 1: (a) The similarity distributions of positive pairs based on different methods. “ArcFace (T)” and “ArcFace (S)” mean teacher and student models are trained by Arc-Face [7]. “FCD (S)” denotes the student model is trained by FCD. (b) The similarity distributions of negative pairs based on different methods. (c) True Accept Rate (TAR) and threshold results of the teacher and student models us-ing different methods. limited scenarios has become an emergency problem in re-cent years. Knowledge distillation [16] is a popular strategy for compressing models, which transfers the “knowledge” from the teacher model to the lightweight student model.
Most existing knowledge distillation methods usually aim to guide the student to mimic the teacher’s behav-ior by introducing probability constraints (e.g., KL diver-gence [16]) between the teacher’s predictions and student models. However, for FR, the performance is usually eval-uated in an open-set setting, where the identities of the test-ing set are disjoint from the training set. Meanwhile, at the testing phase, the similarities of feature embedding are employed for FR instead of the probabilities of the clas-sifier used in classical close-set classification. Therefore, it is more important to improve the discriminative ability of the feature embedding of the student model for FR. A
straightforward and effective FR distillation method is to di-rectly minimize the L2 distance of the feature embeddings extracted by the teacher and student models [54, 42, 26], which is called Feature Consistency Distillation (FCD).
FCD enables the student model to share the same embed-ding space with the teacher model for similarity compari-son, and FCD has been widely used in practice to improve the performance of the lightweight neural networks for FR.
However, we observe that it is unfeasible for student models with low capacities to align the feature space with the teacher model well. As shown in Fig. 1, we use the
ResNet-50 and MobileNetV2 as teacher and student mod-els, respectively, and report the performance results of these models using different losses on the IJB-C [56] dataset.
Specifically, after using FCD, as shown in Fig. 1(a), when compared with the similarity distribution of the positive pairs using the teacher model, we observe that the similari-ties of positive pairs using the student model decrease a lot, which means that FCD reduces the intra-class compactness a lot. Meanwhile, in Fig. 1(b), the similarity distributions of the negative pairs between “FCD (S)” and “ArcFace (T)” are very close, which indicates that the student model can main-tain inter-class discrepancy well after using FCD. Besides, in Fig. 1(c), we observe that the widely-used FR evaluation metric True Accept Rate (TAR) of the student model will be significantly improved after using FCD, and the similar-ity threshold value under the False Accept Rate (FAR) of the student model decreases a lot and is close to the thresh-old calculated by the teacher model, which also represents that the similarities of the negative pairs are effectively de-creased to a similar degree with the teacher model and the
FCD method can effectively help students to learn the inter-class distribution of the teacher model.
Therefore, FCD cannot preserve the intra-class compact-ness of the student model well, and it is critical to align the similarity distributions of the positive pairs between the teacher and student models.
Motivated by the aforementioned analysis, we propose a new FR distillation framework (ICD-Face), which includes
FCD and Intra-class Compactness Distillation (ICD). The
ICD aims to improve the similarity distribution consistency between the teacher and student models. Specifically, we first pre-train the teacher model on the large-scale train-ing dataset. Then, in FCD, we calculate the L2 distance of the embeddings extracted by the teacher and student mod-els to calculate the FCD loss for aligning the embedding spaces of the teacher and student models. In ICD, as it is important to generate sufficient positive pairs to estimate accurate similarity distribution for FR models, inspired by
MoCo [14], we propose to construct the teacher and student feature banks and generate the positive pairs using the fea-tures from the feature banks and the features from the cur-rent batch in our Similarity Distribution Generation mod-ule. After that, we estimate the probability distributions of the teacher and student models, and introduce the Simi-larity Distribution Consistency (SDC) loss to directly align the similarity distributions of the positive pairs between the teacher and student models in the training process.
The contributions of our ICD-Face are as follows:
• In our work, we first investigate the gap of the intra-class similarity distributions between the teacher and student models, and propose a new FR distillation method called as ICD-Face, which additionally intro-duces Intra-class Compactness Distillation (ICD) into the existing FR distillation method.
• In ICD, we first propose to generate sufficient positive pairs for estimating intra-class similarity distributions of the teacher and student models, and then utilize the similarity distribution consistency loss to align the intra-class similarity distributions between FR models.
• Extensive experiments on multiple benchmark datasets demonstrate the effectiveness and generalization abil-ity of our proposed ICD-Face method. 2.