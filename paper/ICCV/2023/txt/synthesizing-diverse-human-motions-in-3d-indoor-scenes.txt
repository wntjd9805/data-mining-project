Abstract
We present a novel method for populating 3D indoor scenes with virtual humans that can navigate in the environ-ment and interact with objects in a realistic manner. Exist-ing approaches rely on high-quality training sequences that contain captured human motions and the 3D scenes they in-teract with. However, such interaction data are costly, diffi-cult to capture, and can hardly cover the full range of plau-sible human-scene interactions in complex indoor environ-ments. To address these challenges, we propose a reinforce-ment learning-based approach that enables virtual humans to navigate in 3D scenes and interact with objects realisti-cally and autonomously, driven by learned motion control policies. The motion control policies employ latent motion action spaces, which correspond to realistic motion primi-tives and are learned from large-scale motion capture data using a powerful generative motion model. For naviga-tion in a 3D environment, we propose a scene-aware policy 1 with novel state and reward designs for collision avoidance.
Combined with navigation mesh-based path-finding algo-rithms to generate intermediate waypoints, our approach enables the synthesis of diverse human motions navigat-ing in 3D indoor scenes and avoiding obstacles. To gener-ate fine-grained human-object interactions, we carefully cu-rate interaction goal guidance using a marker-based body representation and leverage features based on the signed distance field (SDF) to encode human-scene proximity re-lations. Our method can synthesize realistic and diverse human-object interactions (e.g., sitting on a chair and then getting up) even for out-of-distribution test scenarios with different object shapes, orientations, starting body posi-tions, and poses. Experimental results demonstrate that our approach outperforms state-of-the-art human-scene inter-action synthesis methods in terms of both motion natural-ness and diversity. Code, models, and demonstrative video results are available at: https://zkf1997.github.io/DIMOS.
1.

Introduction
Simulating how humans interact with environments plays an essential role in many applications, such as gen-erating training data for machine learning algorithms, and simulating autonomous agents in AR/VR and computer games. Although this task is highly related to character an-imation in computer graphics, most existing character an-imation methods (e.g. [4, 5, 16]) focus on improving the realism and controllability of character movements. With the traditional character animation workflows, one can pro-duce high-quality animations but can hardly generate au-tonomous and spontaneous natural human motions inter-acting with the surroundings in diverse plausible ways as real humans. Previous learning-based interaction synthesis methods [12, 42, 57] require simultaneously capturing hu-man motion and scenes for supervision. However, captur-ing such training data is costly and challenging, resulting in a notably limited spectrum of human-scene interaction mo-tions and difficulties in handling unseen interaction scenar-ios. This restriction also results in inferior motion quality of synthesized virtual humans.
To address this problem, we leverage reinforcement learning (RL) [45] to solve our task. By formulating goals as rewards, perception as states, and latent variables of deep generative models as actions, we can synthesize continu-ous, stochastic, plausible, and spontaneous motions of vir-tual humans to inhabit the digital world. Although existing
RL-based motion synthesis approaches (e.g. [26, 33, 60]) can effectively generate natural motions to achieve goals, their generated virtual humans can only interact with simple scenes, rather than complex environments with functional furniture and diverse objects. For example, GAMMA [60] employs generative motion primitives and a policy network that are generalizable across diverse human body shapes, but it can only synthesize waypoint-reaching locomotions.
The trained digital humans are not aware of how to perform actions like sitting on a chair or lying on a sofa, and fre-quently inter-penetrate with the scene geometry.
To overcome these limitations, we propose a novel framework to learn both scene and interaction-aware mo-tion control policies for synthesizing realistic and diverse human-scene interaction motions. First, in order to improve the physical plausibility of the synthesized human motions, we design a new scene-aware policy to help virtual humans avoid collisions with scene objects. Specifically, we use a 2D occupancy-based local walkability map to incorpo-rate scene information into the locomotion policy. In ad-dition, we add features derived from the signed distance from body markers to the object surface and the gradient direction of the signed distance to encode the proximity be-tween humans and objects for object interaction policies.
Second, in order to achieve controllable object interactions, we provide fine-grained guidances based on surface mark-ers [58] of a human body performing the target interac-tions. Specifically, we use COINS [61] to generate human bodies interacting with scene objects given the interaction semantics, and then use the body markers as the interac-tion guidance for motion synthesis. Combined with navi-gation mesh-based path-finding algorithms to generate in-termediate waypoints in 3D scenes, virtual humans can au-tonomously reach target locations in complex environments and mimic target poses in a variety of plausible ways.
We train the policy networks in synthetic scenes con-sisting of randomized objects to learn generalizable scene-aware locomotion and fine-grained object interactions.
With this framework, we investigate how to synthesize di-verse in-scene motions consisting of locomotion, sitting, and lying. We empirically evaluate the motion realism and expressiveness of our proposed method, and compare it with state-of-the-art methods. The results show that our approach consistently outperforms the baselines in terms of diversity, physical plausibility, and perceptual scores.
In summary, we aim to let virtual humans inhabit virtual environments, and present these contributions: 1. We propose a reinforcement learning-based frame-work to generate realistic and diverse motions of vir-tual humans in complex indoor scenes. 2. We propose to use body surface markers as detailed interaction goals for fine-grained human-object inter-action synthesis and leverage COINS [61] to generate articulated 3D human bodies based on interaction se-mantics to make virtual humans controllable via inter-action semantics and fine-grained body poses. 3. We design scene and interaction-aware policies to en-able virtual humans to navigate in 3D scenes while to interact with scene objects, avoiding collisions, and to continuously perform sequences of activities in complex scenes. 2.