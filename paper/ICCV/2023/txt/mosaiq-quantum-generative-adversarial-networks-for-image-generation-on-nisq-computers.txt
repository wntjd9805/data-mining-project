Abstract
Opportunity Gap for Quantum GANs.
Quantum machine learning and vision have come to the fore recently, with hardware advances enabling rapid ad-vancement in the capabilities of quantum machines. Re-cently, quantum image generation has been explored with many potential advantages over non-quantum techniques; however, previous techniques have suffered from poor qual-ity and robustness. To address these problems, we introduce
MosaiQ a high-quality quantum image generation GAN framework that can be executed on today’s Near-term In-termediate Scale Quantum (NISQ) computers. 1.

Introduction
Generative Adversarial Networks, or GANs, are a type of neural network architecture used in machine learning and computer vision for generative modeling [8, 22, 2, 9]. A classical GAN consists of two neural networks, a genera-tor, and a discriminator, that are trained simultaneously in a competitive process. The generator generates fake data samples, while the discriminator tries to distinguish them from real ones found in the training set, hence serving as an
“adversarial entity”. Classical GANs have received signif-icant attention for generating high-quality images, among other purposes including text generation, data augmenta-tion, and anomaly detection [20, 1, 5].
Naturally, this has spawned interest in the quantum infor-mation science community to develop corresponding quan-tum GANs that run on quantum computers. While recent efforts toward developing quantum GANs have been instru-mental and early results have been encouraging, we discov-ered that existing approaches have severe scalability bottle-necks and have significant room for improvement. This is especially true in the context of generating high-quality im-age generation on real-system quantum computers.
A recent work by Huang et al. [11], referred to as QG-Patch in this paper, is the state-of-the-art demonstration of
QuantumGANs on real quantum computers. As our pa-per also demonstrates, QGPatch can learn different shapes and produce recognizable images in some cases, but can of-ten yield low-quality images. It suffers from the scalability challenge because it breaks the image into “patches” and performs a pixel-by-pixel learning. Second, it is not effec-tive at generating a variety of images within the same class – this problem is known as “mode collapse” [7]. It is non-trivial to achieve high-quality image generation while also maintaining variety. Motivated by these limitations, Mo-saiQ’s design pushes the state of the art by achieving higher scalability, image quality, and variety.
Contributions of MosaiQ
I. MosaiQ introduces the design and implementation of a novel quantum generative adversarial network for image generation on quantum computers. MosaiQ’s approach is a hybrid classical-quantum generation network where a net-work of low-circuit-depth variational quantum circuits are leveraged to learn and train the model. Upon acceptance,
MosaiQ will be available as an open-source contribution.
II. MosaiQ’s design demonstrates how the extraction of principal components of images enables us to learn and gen-erate higher-quality images, compared to the state-of-the-art approach which is limited in its scalability due to pixel-by-pixel learning [11]. However, exploiting information in principal components to its full potential is non-trivial, and
MosaiQ proposes to mitigate those challenges using fea-ture redistribution. Furthermore, MosaiQ introduces a novel adaptive input noise generation technique to improve both the quality and variety of generated images – mitigating the common risk of mode collapse in generative networks.
Figure 1. Classical generative adversarial network (GAN) and MosaiQ’s hybrid quantum-classical GAN architecture.
III. Our evaluation demonstrates that MosaiQ significantly outperforms the state-of-the-art methods [11] on both sim-ulation and real quantum computers with a hardware error.
MosaiQ is evaluated on the MNIST [6] and Fashion MNIST datasets [21] – widely-used for QML evaluation on Near-term Intermediate Scale Quantum (NISQ) machines [15, 11]. MosaiQ outperforms the state-of-the-art methods [11] both visually and quantitatively – for example, over 100 points improvement in image quality generated on IBM
Jakarta quantum computer using the FID (Fr´echet Incep-tion Distance) score [10], which is popularly used for com-paring image quality. MosaiQ is open-sourced at https:
//github.com/SilverEngineered/MosaiQ. 2. MosaiQ: Challenges and Solution
In this section, we present the design and implementation of MosaiQ, a quantum image generative network. To pro-vide better context for understanding, first, we briefly de-scribe the classical generative adversarial networks (GAN) (depicted in Fig. 1(a)), describe the limitations of the state-of-the-art quantum GANs, and then, provide the details of
MosaiQ’s quantum GAN design (depicted in Fig. 1(b)). 2.1. Generative Adversarial Networks (GANs)
Generative Adversarial Networks, or GANs, are a type of neural network architecture used in machine learning for generative modeling [8]. The basic architecture and workflow of a classical (non quantum) GAN are shown in
Fig. 1 (a). A classical GAN consists of two neural net-works, a generator, and a discriminator, that are trained simultaneously in a competitive process. The generator generates fake data samples, while the discriminator tries to distinguish them from real ones found in the training set, hence serving as the “adversary”. Through training, the generator learns to create increasingly realistic samples that mimic the distribution of the real data in order to bet-ter fool an increasingly effective discriminator, eventually reaching an attempt to attain equilibrium with the value function expressed as minG maxD Ex∼qdata(x)[log D(x)]+
Ez∼p(z)[log(1 − D(G(z)))]. This value function has two 2
Figure 2. State-of-the-art quantum image generator, QG-Patch [11], generates low-quality images for different datasets. However, its quality is still similar to a classi-cal GAN with 25× more parameters and iterations. main identifiable components that correspond to the respec-tive optimization objectives of the generator and the dis-criminator. The use of log provides more numerical stabil-ity because it converts the multiplication of multiple small probabilities into addition, and it also allows us to calculate the derivatives more easily during the optimization process.
Once fully trained, a generator component of the GAN is capable of converting random noise into new data sam-ples (e.g., boots) that conform to the original distribution of the training data. Essentially, the generator, without the use of a discriminator, can be inferred to obtain new data samples. GANs have been shown to be useful in a variety of applications such as image and text generation, data aug-mentation, and anomaly detection [20, 1, 5]. Naturally, this has spawned interest in the quantum information science community to develop corresponding quantum GANs.
Quantum GANs follow a similar generator and discrim-inator structure as classical GANs but use quantum princi-ples to train and infer from the model. Typically, the dis-criminator functions on classical resources, and the genera-tor is trained on quantum resources. This is expected, as this structure allows the Quantum GANs to leverage quantum resources for generation tasks – the component that persists beyond training; recall that the discriminator is essentially a quality inspector that can be decoupled after training and is not required during inference. While this structure is in-tuitive and has been demonstrated to be somewhat promis-ing [11], there are multiple open challenges that need to be overcome to achieve higher quality. 2.2. Limitations of Existing Quantum GANs
A recent work by Huang et al. [11], referred to as QG-Patch, is the most complete, state-of-the-art demonstration of QuantumGANs on real quantum computers. QGPatch follows the Quantum GAN architecture described earlier but achieves limited effectiveness – QGPatch can learn dif-ferent shapes and produce recognizable images in some cases, but often suffers from low quality. Fig. 2 shows that digit ‘0’ is a reasonable approximation of the ground truth, but the generated boot image is far less recognizable. De-spite the imperfect generations, we can see in Fig. 2 how a classical GAN with 25× the parameters of QGPatch and
trained for 25× as many iterations is worse or similar to the quantum technique – supporting the potential of quantum machine-learning for image generation tasks as compared to the classical approach with much higher resources and complexity. This independently provides experimental evi-dence for why the quantum information science community is motivated to accelerate the progress of Quantum GANs, despite its current limitations.
The reasons for the limited effectiveness of Quatum
GANs are multi-fold. The quantum generator component runs on the quantum hardware and requires many qubits to produce high-quality images from random input noise (a qubit is the fundamental unit of computation and informa-tion storage on quantum computers). QGPatch addresses this challenge by breaking the image into “patches” and employing a generator for different patches. While this is reasonable for smaller resolution images, the “patch-based” approach suffers from scalability due to its fundamental na-ture of learning pixel-by-pixel. For example, a total of 245 qubits are required for images in the full-resolution 784-pixel MNIST dataset.
The second challenge is performing efficient learning from random input noise – it is critical for the quantum gen-erator to effectively utilize the random input noise to gen-erate a variety of images for the same class. Inability to generate a variety of images within the same class – which even classical GANs suffer from – is popularly known as the
“mode collapse” problem [7]. Mode collapse is a side-effect of the generator learning to generate only produce one type of image for a given class because the generator has learned to “fool” the discriminator for this image and saturates in its learning.
It is non-trivial to achieve high-quality gen-erating while also maintaining variety. Motivated by these limitations, MosaiQ’s design pushes the state of the art. 2.3. Overview of MosaiQ and Key Ideas
Hybrid Quantum-Classical Architecture. MosaiQ uses a hybrid quantum-classical architecture for image generation, as summarized in Fig. 1(b). The generator is quantum in na-ture and trained using quantum simulation, and the discrim-inator is classical – similar to the construction in [15, 11].
However, there are two key novel architectural changes: (1) to address the scalability and quantum resource bottle-necks for generating high-quality images, MosaiQ applies a transformation on the input image dataset, and (2) MosaiQ employs quantum-style random noise as input to enhance the “variety or mode-collapse” challenge. These two key features are described after a brief description of MosaiQ’s generator and discriminator.
MosaiQ’s Quantum Generator Network. MosaiQ’s quantum generator component is a network of multiple sub-generators. Each sub-generator is a variational quantum circuit that is iteratively optimized to train a model. Vari-Figure 3. Circuit ansatz design of MosaiQ’s generators.
The RY gates in each layer are optimized during training. ational quantum circuits are specific types of quantum cir-cuits where some components of the circuits are tunable pa-rameters, which are iteratively optimized. As a background, a quantum circuit is essentially a sequence of quantum gates applied to qubits. The quantum gates are fundamental op-erations that manipulate the qubit state, and any gate can be represented as a unitary matrix. One-qubit gates, such as the Pauli gates (σx, σy, σz), apply a rotation to just one qubit and are categorized by the axis around which the rota-tion takes place, and by how much. Multi-qubit gates, such as the CNOT gate, allow the creation of entanglement.
Variational quantum circuits in MosaiQ are comprised of two types of sections. The first type, X, has fixed gates that entangle the qubits. The second type, θ, has tunable
U3 gates that are optimized via training. The overall cir-cuit V is therefore an optimization function built of unitary transformations such that V = U (X, θ). One key feature of variational quantum circuits is the ability to leverage many off-the-shelf classical deep-learning techniques for training and optimization. This includes learning through common loss functions such as L2 loss and leveraging existing opti-mizers such as Adam [13] that MosaiQ leverages.
All sub-generator circuits in MosaiQ have identical ar-chitectures, composed of a five-qubit circuit. Fig. 3 shows an example of the sub-generator circuit. The circuit consists of encoding the input noise as angles using Rx gates and Ry gates. Following the embedding of the noise, the parame-terized weights are encoded on each quantum layer along-side CZ gates used to entangle the qubits at each layer.
These weights contain the portion of the circuit that is op-timized. Following these repeated layers, the P auliX ex-pected value is taken for each qubit in measurement.
We note the simplistic design of MosaiQ’s generator cir-cuits is intentional; limiting the number of tunable param-eters and depth of the circuits enables MosaiQ to mitigate hardware error impact on real quantum machines and main-tain high quality, as confirmed by our evaluation (Sec. 3).
Scalable Learning by Extracting Principal Components.
Unlike previous approaches, which learn to construct the pixels of an image directly and consequently, suffer from the scaling bottleneck, MosaiQ demonstrates that extract-ing and learning the rich features is effective. The principal component analysis (PCA) method enables efficient learn-3
sub-generators in training to form rich connections for im-age generation, when some sub-generators do not contain much useful information in any feature, the entire generator does not pose much utility.
Therefore, while effective in achieving scalability, only performing PCA may not achieve the full potential of Mo-saiQ’s quantum image generation as this leads to unbal-anced training, likely to quickly discover gradients that ig-nore many sub-generators and plateaus in training. To mit-igate this challenge, we propose a new PCA feature distri-bution mechanism to counteract the unbalanced nature of assigning principal components to sub-generators (Fig. 4).
We begin by assigning the first principal component to the first sub-generator, the second principal component to the second, and so on until each sub-generator has one top principal component. We follow this up by assign-ing the last n-1 principal components to the first genera-tor, where n is the size of each sub-generator. This is fol-lowed by assigning the second to last n-1 features to the second sub-generator and repeated on the third generator and so on until all features have been assigned. This creates a much more balanced distribution, which enables us to en-sure all sub-generators hold utility during the training pro-cess and the learning is not skewed. As an important side-note, MosaiQ’s distribution also mitigates the pathologi-cal side-effects of hardware errors on NISQ quantum ma-chines where the majority of critical principal component features could be concentrated on a single sub-generator.
This sub-generator could be mapped to qubits with higher hardware error rates, and this can potentially make the train-ing less effective. Our evaluation confirms the effective-ness of MosaiQ’s principal component feature distribution among learners. Finally, we describe how MosaiQ utilizes the adaptive noise during image generation to increase the variety of images generated to avoid mode collapse.
Adaptive Input Noise to Improve Variety of Generated
Images. Recall that it is critical for the quantum generator to effectively utilize the random input noise to generate a variety of images for the same class. The inability to gen-erate a variety of images within the same class leads to the
“mode collapse” problem. Therefore, while MosaiQ’s pre-vious methods help us achieve high quality, it is critical to achieving high-quality, while also maintaining variety.
To address this challenge, MosaiQ introduces an adap-tive noise range based on the ratio of training loss between the quantum generator and the classical discriminator. The noise range is determined by the current progress of the gen-erator discriminator mini-max game, instead of the tradi-tional fixed range of [0, π 2 ] as employed by QGPatch. For-mally, the adaptive noise (Noiseadaptive) is defined below in terms of the Generator loss GL and Discriminator loss DL and the Ratio GL0 is the Discriminator/Generator loss ratio
DL0
Figure 4. MosaiQ divides the PCA features among the eight generator circuits in a manner that the total explained vari-ance is close to equal across all generators. ing by focusing on the important features that compromise unique images, as opposed to having to learn to distinguish the important features from the redundant features in an im-age via the pixel-by-pixel method. PCA maximizes the en-tropy for a given number of components, concentrating the information of a dataset efficiently.
The first step in learning principal components is nor-malizing the input data. Once the images are normalized,
MosaiQ decomposes the images into principal components and scales these components between [0, 1] so that the quan-tum sub-generators can generate throughout the entire space of inputs. As quantum machines rely on only unitary trans-formations, it is not possible to obtain an output with an ab-solute value greater than one in measurement. These scaled components are fed into the discriminator as the “real la-bels” and the generator begins to learn how to mimic the distribution throughout the training process. After learning, the outputs are scaled back to the original principal compo-nent range, then are inverse transformed to form a unique image that is within the distribution of the original images.
This method helps us achieve higher quality in a scalable way. However, experimentally we found that this alone is not sufficient. MosaiQ performs intelligent distribution of features to make the learning more effective and robust.
Feature Distribution Among Learners.
In the conven-tional implementation of the previously proposed idea, all generated features are distributed among several sub-generators before being concatenated. Recall that the Mo-saiQ’s quantum generator is an ensemble of multiple sub-generators, where each sub-generator is a variation quantum circuit. In the default case where PCA features are aligned one after another to the sub-generators, an unbalanced dis-tribution of explained variance may emerge, where some sub-generators are responsible for significantly more impor-tant generations than others. This is because, by definition, the explained variance of PCA features is heavily concen-trated on the first few features, and there is often a signif-icant drop-off in explained variance from one PCA feature to the next. Recall that PCA features are entangled within 4
Figure 6. MosaiQ’s inference process to generate new im-ages via running the generators on quantum machines. tent training as the generator can focus on quality when it is doing relatively worse compared to the classical discrim-inator and expand to generating more variety in high qual-ity as it begins to fool the discriminator more on an easier objective. Overall, the adaptive noise mechanism helps us increase the variety of images for a given class while ensur-ing that the image quality also remains high. This is further confirmed by our evaluation (Sec. 3).
MosaiQ’s Classical Discriminator. MosaiQ ’s discrimina-tor is a classical deep-learning network, designed to aid in the training of the quantum generator. This network is es-sentially training wheels to guide a difficult-to-train quan-tum GAN, which can be discarded after training. The dis-criminator is much larger than the quantum generator, as there is only a single discriminator that must compete in the adversarial game with multiple quantum generators. Typi-cal to its classical counterpart, MosaiQ’s discriminator has multiple linear layers with ReLU activation (e.g., 64 lay-ers). MosaiQ has a terminal layer with Sigmoid activa-tion which introduces more non-linearity into the network, which enables it to learn more complex structures. The sin-gle value produced at the end of the discriminator network allows it to act as a classifier to distinguish between real and generated data.
Putting It All Together. The overall workflow for train-ing and inference for MosaiQ are visually summarized in
Fig. 5 and 6, respectively. Fig. 5 depicts the continuous feedback-driven process where the discriminator and gen-erator participate in a non-cooperative game to improve the overall quality of MosaiQ. Depending upon the relative loss of the discriminator and generator networks, the random in-put noise is automatically adjusted to help MosaiQ genera-tors avoid the problem of mode collapse problem and enable it to generate variety. During the training process, the input images are transformed using PCA to encode critical and more information in a compact manner for resource-limited
NISQ quantum computers, instead of learning over the in-put images pixel by pixel. Finally, the five-step inference procedure shown in Fig. 6 highlights that the PCA transfor-mation is inverted to generate new images when running the fully-trained MosaiQ generators on real quantum computers for inference to generate new images.
Figure 5. MosaiQ’s process for training and optimizing the quantum generator circuits using PCA transformation, feature distribution, and adaptive noise generation. observed after the first epoch during training.
Noiseadaptive =
π 8
+ 5π 8
ReLU(Tanh(cid:0) DL
GL
−
GL0
DL0 (cid:1)) (1)
As the above formula indicates, this adaptive bound en-forces a minimum noise range of π 8 and a maximum range of 3π 4 for the noise. While the bounds are chosen to be a specific constant, the noise is always distributed within this range and the upper bound automatically adjusts itself based on the relative effectiveness of the generator and the dis-criminator. MosaiQ does not require tuning the thresholds for the range for different classes, it automatically adjusts itself to adapt to different conditions during training.
MosaiQ embeds the adaptive noise using angle embed-ding on the quantum circuit. The T anh function is used to scale the adaptive noise to asymptotically approach the range [−1, 1]. MosaiQ chose T anh because it is a widely-used activation function in deep learning applications which transforms unbounded data into data bound by [−1, 1].
T anh is defined as T anh(x) = e2x−1
In practice, we e2x+1 . observed that it is not effective to have a noise range lower than π 8 , we leverage ReLU to force negative values to be zero. ReLU is also a commonly used activation function, however, ReLU is used to provide a non-linear transform that makes all negative values 0 (to ensure that the noise is never less than π 8 ), and keeps all positive values the same.
While the non-linearity of T anh and ReLU are impor-tant when used typically as activation functions, the non-linearity is not the main function in this application.
If the generator is not able to catch up with the discrimina-tor for some iterations at this small range, it should learn this smaller range instead of getting even smaller. We set the minimum as π 8 as we discovered that a lower value than
π 8 results in mode collapse, with almost no distinction be-tween images generated for a given class.
Leveraging adaptive noise for input ensures more consis-5
3. Experimental Methodology
Datasets. MosaiQ is evaluated on the MNIST [6] and Fash-ion MNIST datasets [21] as they have been widely-used for
QML evaluation on NISQ-era quantum machines [15, 11].
MNIST consists of 28x28 gray-scale images of handwritten digits from 0 to 9. Fashion MNIST consists of 28x28 gray-scale images of clothing and accessories. Fashion MNIST provides more challenges for image generation and is cho-sen to explore more domains of quantum image generation.
For both datasets, we split the dataset by image label and train individual models for each data type. This technique has been used in [11] and allows us to interpret the genera-tion process and difficulties isolated for each class of data.
Experimental Framework and Training Details. The environment for MosaiQ consists of PyTorch [16] acting as a wrapper for Pennylane [3]. MosaiQ is trained on
IBM’s quantum simulator for speed, but the inference is performed on both the quantum simulator and real quantum machine. For real quantum machine experiments, Penny-lane compiles the circuits into a backend compatible with
IBM-Qiskit [18]. All real machine runs were performed on the IBM QX Jakarta machine [18]. Images used in training are selected from the training set and decomposed to prin-cipal components of size 40. These components are divided across eight five-qubit sub-generators. The discriminator learns to differentiate at the level of principal components and does not need to utilize a full image. Our final FID metrics reported are at the end of the training after 500 iter-ations (where all methods achieve near-final stability).
MosaiQ uses PyTorch’s Standard Gradient Descent Op-timizer for both the generator and discriminator and Binary
Cross Entropy Loss for the shared loss of the generator and discriminator. The Adaptive Noise range is simple to calcu-late based on the generator and discriminator loss and Mo-saiQ automatically guides it to adjust itself every training iteration. The generator learning rate is 0.3 and the discrim-inator learning rate to .05 with a batch size of 8.
Framework for Competing Techniques. We set up QG-Patch, the state-of-the-art technique [11], using the popu-lar Pennylane framework and choose the parameters used in the original paper [11]. Our QGPatch training is per-formed using the batch size of eight and it trains until the quality stabilizes (500 iterations). We use the same network size that is mentioned in the original paper, using 4 sub-generators with 5 features each. Evaluating the probabili-ties in measurement yields a 64-pixel (8x8) image. As Mo-saiQ’s results are based on the original size of the datasets of 784 pixels, we upscale the results of QGPatch using Bilin-ear Interpolation to allow for direct comparison. We rigor-ously explored multiple interpolation techniques to provide
QGPatch as much as an advantage as possible and experi-mentally determined that Bilinear interpolation yielded the
Figure 7. MosaiQ produces visually higher-quality images compared to the state-of-the-art technique, QGPatch, for different classes of MNIST and Fashion-MNIST datasets. highest quality upscaling. For QGPatch, similar to MosaiQ, all metrics scores shown are calculated based on 500 im-ages generated at the end of training compared to the entire distribution of the respective image category.
We additionally set up other classical experiments to act as ablations for the efficacy of MosaiQ. We introduce two techniques: (1) PCAInverse (using random inputs and PCA inverse transformation to generate images), and (2) Classi-calPCA (a purely-classical GAN using the same number of parameters as MosaiQ, but uses MosaiQ’s PCA technique for feature compression). PCAInverse applies the same in-verse PCA transformation as MosaiQ, to random noise of size 40. This technique is designed to explore the isolated effects of the inverse PCA transformation procedure since it does not perform any learning. ClassicalPCA trains a purely-classical GAN with an identical number of param-eters as MosaiQ and apply the same inverse PCA transfor-mation workflow.
Figures of Merit. To capture the quality of image gener-ation, our evaluation figures of merit are both qualitative and quantitative. Our primary quantitative figure of merit is
FID (Fr´echet inception distance) score [10]. FID evaluates the distance between two distributions, as defined below for a Gaussian with mean (m, C) and a second Gaussian with mean (mw, Cw): FID = ∥m − mw∥2 2 + T r(C + Cw − 2(CCw)1/2). A lower FID score between two distributions indicates higher similarity – and hence, lower FID scores imply higher quality. The FID score has been shown to pro-vide a more meaningful comparison over alternative metrics (e.g., Inception Score) for image GANs [10].
We also evaluate the variance of our images, by evaluat-ing the variance of the pixel values relative to the mean.
As defined below, the variance metric gives insight into the distinctness of the images generated for each method.
For each pixel in an image row r and column c in an im-age, we sum the squared difference from the mean value for that respective pixel to achieve our variance score V .
We then take the cumulative density function (CDF) of these variance scores for each image generated image G(z) given uniformly distributed noise z from [0, π/2]: V = (cid:80) c(µrc −G(z)rc)2. A higher variance indicates higher r variety in generated images for a given class – which is de-sirable, however, it is also critical that a method should at-tain a lower FID score before demonstrating higher variety. (cid:80) 6
Figure 8. MosaiQ consistently produces lower FID score images (higher quality images) compared to QGPatch across different classes of the MNIST dataset.
Figure 11. MosaiQ produces higher quality images on ev-ery class of the MNIST dataset as compared to HQCGAN
[19] in addition to the classical methods tested.
Figure 9. MosaiQ consistently produces lower FID score images (higher quality images) compared to QGPatch across different classes of the Fashion-MNIST dataset.
Figure 10. MosaiQ produces higher quality images on the
MNIST and Fashion MNIST dataset as compared to HQC-GAN [19]. MosaiQ produces higher quality images than the classical methods: PCAInverse and ClassicalPCA. 4. Evaluation and Analysis
In this section, we go over the results of MosaiQ and provide an analysis of the key elements of the design.
MosaiQ: Key Results and SOTA Comparison. Mo-saiQ yields significantly better image quality both visually and quantitatively compared to the state-of-the-art (SOTA) method, QGPatch [11]. As discussed earlier, MosaiQ is evaluated on two different datasets to cover diversity across datasets and within a dataset with different shapes and styles (digits and clothing). First, we highlight that MosaiQ provides a significant visual enhancement in the images produces across different classes compared to the SOTA method. In fact, Fig. 7 shows that the SOTA-produced im-ages are almost unrecognizable for sophisticated shapes (‘0’ 7
Figure 12. MosaiQ produces higher quality images on ev-ery class of the Fashion MNIST dataset as compared to
HQCGAN [19] in addition to the classical methods tested. vs. ‘5’ and different types of shoes). In comparison, Mo-saiQ is effective across different class types – this is further substantiated by lower FID scores of the images generated by MosaiQ (Fig. 8 and 9).
As expected, the FID score obtained by MosaiQ varies across different classes for both datasets; this is because of varying degrees of difficulty and shapes of different images.
However, the most notable observation is that MosaiQ con-sistently outperforms QGPatch across all class types by a significant margin (by more than 10 points in many cases).
In fact, MNIST digit ‘7’ is quite challenging for QGPatch (FID score is 60), and MosaiQ improves the FID score by approximately 20 points. Similarly, the most challenging class in the Fashion-MNIST dataset (Boot) receives a 45-point improvement by MosaiQ. The key reason MosaiQ outperforms QGPatch is the efficiency of principal compo-nents in capturing information. Instead of having to learn pixels one by one, MosaiQ scales more easily by learning a distribution of features that better organize redundancy.
For example, most of the background of MNIST and Fash-ion MNIST images are black, in the case of QGPatch, each pixel must be synthesized, where MosaiQ may be able to build these features with only a few principal components.
We also compare MosaiQ to other methods, including the PCAInverse technique, the ClassicalPCA technique, and a recent work on hybrid quantum GANs which we re-fer to as HQCGAN [19]. We sample generated images in
Fig. 10 and find the images produced by all methods tested
Figure 13. MosaiQ’s PCA feature distribution among learners improves the quality of generated images. The fig-ure shows a distribution of 200 FID scores (comparing 8 images each to the entire data distribution) for the case with and without PCA feature redistribution. are lower quality than MosaiQ, producing images which are far less human-recognizable. We show the correspond-ing FID scores for MNIST in Fig 11 and Fashion MNIST in Fig. 12. We find that MosaiQ produces higher quality images and significantly improved FID scores in all cases tested compared to the classical methods (PCAInverse and
ClassicalPCA), and HQCGAN. Importantly, MosaiQ out-performs ClassicalPCA, demonstrating the power of quan-tum networks in image generation when compared to equal-sized classical networks. Our results show that while HQC-GAN is promising and useful, the final quality may not be as high as MosaiQ – this is because HQCGAN requires sig-nificantly more parameters, more training resources, lacks noise-mitigation, and learns noise in a fixed range ([0, 1) in-stead of MosaiQ’s adaptive noise range technique). Next, we dissect the key reasons behind MosaiQ’s effectiveness in more detail.
Why Does MosaiQ Work Effectively? Effect of careful
PCA feature distribution among sub-generators. Recall that
MosaiQ employs an intelligent PCA feature distribution among sub-generators in the generation phase. The goal is to equalize the explainable PCA feature variance across learners – in an effort to make learners equally capable in-stead of weaker learners not effectively contributing toward the overall quality. To better understand and demonstrate the effectiveness of this mechanism, Fig. 13 shows the FID score over multiple training iterations with and without this mechanism while keeping all other design elements intact.
For easier interpretation, Fig. 13 shows this for class digit
‘5’ for the MNIST dataset. We observe that PCA feature distribution allows MosaiQ to achieve lower (and hence, better) FID score over training compared to without em-ploying this mechanism. A side benefit of this mechanism is that MosaiQ can also mitigate hardware errors on real quantum computers, as discussed later.
Effect of adaptive noise generation during training. Fig. 14 shows the effect of adaptive noise generation that is used by the MosaiQ generators instead of using a constant
Figure 14. Adaptive noise during generation helps MosaiQ achieve variety (higher variance across images for the same class to mitigate mode-collapse challenge), while achieving lower FID score at the same time, as shown earlier.
Figure 15. Visual image quality of images produced by Mo-saiQ on real IBM quantum machine (Jakarta) for six repre-sentative classes for MNIST and Fashion-MNIST datasets. noise threshold range (i.e., [0, π 2 ]) used by the QGPatch method. Recall that GANs often suffer from intra-class mode-collapse challenges where they can produce a high-quality image for a given class, but there is a possibility that all generated images for a given class appear similar. There-fore, it is critical to ensure that the generated images for a given class have sufficient variety. Fig. 14 confirms that adaptive noise improves variety (class digit ‘5’ used as an example) compared to fixed noise ranges. This is because adaptive noise enables the generators to learn over differ-ent distributions more effectively and generate variations.
Having an adaptive range allows the generator to improve variety when it is doing comparatively well and focus on stability when it is performing poorly with a smaller range of inputs. Taking advantage of this allows us to have high stability in training and high variety over time.
MosaiQ on NISQ Quantum Machines. Our experimental campaign on real superconducting-based IBM NISQ quan-tum computers confirms MosaiQ produces higher quality images across different datasets with diverse classes of im-ages. Fig. 15 and Table 1 summarize the image quality results for QGPatch and MosaiQ. While MosaiQ’s qual-ity remains consistent with simulations, QGPatch’s qual-ity degrades considerably on real computers. In fact, our results revealed QGPatch produces lower quality images and sometimes unrecognizable images because QGPatch is more prone to side-effects of prevalent quantum gate errors on real machines, while MosaiQ’s simplistic design suc-cessfully mitigates those effects as discussed next.
First, we observe that MosaiQ produces competitive 8
Table 1. QGPatch and MosaiQ’s FID scores on a real quantum computer for MNIST & Fashion-MNIST datasets. (a) QGPatch
Environment
Digit 0
Digit 5
Digit 9
T Shirt
Pants
Shoes
Quantum Simulator
IBM Jakarta Machine 52 145 52 134 48 131 45 125 42 144 82 124 (b) MosaiQ
Environment
Digit 0
Digit 5
Digit 9
T Shirt
Pants
Shoes
Quantum Simulator
IBM Jakarta Machine 42 45 42 44 33 35 34 38 22 23 38 38 quality images despite hardware errors on real quantum computers. Due to high queue wait time and limited avail-ability of quantum resources, we are presenting results only for selected classes (three from each dataset) which cover a diverse range of intricacies in terms of shape and informa-tion. In fact, via visual inspection for different images pro-duced for a given class, we observe that the variety in the produced images by MosaiQ is also high on the quantum computer - effectively mitigating the mode-collapse pitfall in GANs. This is because of MosaiQ’s adaptive noise mech-anism during the generation phase that improves image va-riety and avoids mode collapse.
Second, we observe that the FID scores of MosaiQ on a real quantum computer are similar to error-free simula-tion results, across different classes. This result demon-strates that MosaiQ’s design is effective even when hard-ware errors are present. In fact, the IBM Jakarta computer has a considerably low quantum volume (higher is better) with a quantum volume of 16. Quantum volume is a use-ful metric in determining the error rate and capabilities of a quantum computer. For perspective, at the time of writ-ing, IBM has computers available with quantum volumes of 128. We chose a relatively less capable quantum computer for our experiments to demonstrate the portability and ro-bustness of MosaiQ. The reason for MosaiQ’s effectiveness is twofold: (1) its simplistic design for generator learners, which avoids a large number of two-qubit gates, and cir-cuits with high depths. which are more prone to hardware noise, and (2) its PCA feature distribution mapping among the ensemble of learners in the generator – this careful re-distribution ensures that most critical PCA features are not concentrated on a few qubits, which in the pathological case could be most severely impacted by hardware-errors. Mo-saiQ’s PCA redistribution ensures that the learners do not need to know the error rate of individual qubits on the com-puter or make assumptions about different error types. 5.