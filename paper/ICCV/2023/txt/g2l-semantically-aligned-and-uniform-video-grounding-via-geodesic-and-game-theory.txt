Abstract
Query (cid:1869) : The person sharpens the blade on the second belt.
The recent video grounding works attempt to introduce vanilla contrastive learning into video grounding. However, we claim that this naive solution is suboptimal. Contrastive learning requires two key properties: (1) alignment of fea-tures of similar samples, and (2) uniformity of the induced distribution of the normalized features on the hypersphere.
Due to two annoying issues in video grounding: (1) the co-existence of some visual entities in both ground truth and other moments, i.e. semantic overlapping; (2) only a few moments in the video are annotated, i.e. sparse annotation dilemma, vanilla contrastive learning is unable to model the correlations between temporally distant moments and learned inconsistent video representations. Both character-istics lead to vanilla contrastive learning being unsuitable for video grounding. In this paper, we introduce Geodesic and Game Localization (G2L), a semantically aligned and uniform video grounding framework via geodesic and game theory. We quantify the correlations among moments lever-aging the geodesic distance that guides the model to learn the correct cross-modal representations. Furthermore, from the novel perspective of game theory, we propose semantic
Shapley interaction based on geodesic distance sampling to learn ﬁne-grained semantic alignment in similar moments.
Experiments on three benchmarks demonstrate the effec-tiveness of our method. 1.

Introduction
Video grounding [18, 59, 36, 45, 17, 75, 8, 69, 72, 67, 6, 25] aims to identify the timestamps semantically cor-responding to a given query within an untrimmed video, which is a challenging multimedia retrieval task due to ﬂex-ibility and complexity of the query and video content. The video grounding model needs well to model the complex cross-modal correlations and semantic information.
† Corresponding author. (cid:1865)(cid:2869) (cid:1865)(cid:2870) (cid:1865)(cid:2871)
Video (a) (cid:1865)(cid:2871) (cid:1865)(cid:2870) (cid:1865)(cid:2874) (cid:1865)(cid:2873) (cid:1865)(cid:2871)(cid:2871) (cid:1865)(cid:2870)(cid:2870) (cid:1865)(cid:2869) (cid:1865)(cid:2872) (cid:1869) (b) (cid:1865)(cid:2872)
GT (cid:1865)(cid:2873) (cid:1865)(cid:2874) (cid:1865)(cid:2873) (cid:1865)(cid:2874)
Moment
Query
Positive
Negative
Pull close
Push away
Geodesic
Shapley Interaction (cid:1865)(cid:2869) (cid:1865)(cid:2872) (cid:1869) (c)
Figure 1. (a) Illustration of video grounding. ‘GT’ indicates the ground truth. A comparison of (b) existing contrastive learning-based methods and (c) our proposed G2L method. G2L makes semantically similar video moments closer in representation space while exploring nuances among similar moments.
Contrastive learning [10, 23, 43] is proposed to learn rep-resentations by contrasting positive pairs against negative pairs. With the popularity of contrastive learning in vision-language tasks [34, 13, 46, 7, 5, 64, 62], several works also apply it to video grounding. Nan et al. [45] propose a dual contrast learning to learn more informative feature repre-sentations by maximizing the mutual information between the query and the corresponding video clips. This naive so-lution, however, achieves sub-optimum performance.
Generally, contrastive learning requires two key proper-ties [58]: alignment and uniformity. Alignment favors en-coders that assign similar features to similar samples. Uni-formity prefers a feature distribution that preserves maximal information, i.e., the uniform distribution on the unit hyper-sphere. We argue that these two issues are not satisﬁed in current video grounding works with contrastive learning.
Firstly, the semantic overlapping issue is widespread,
i.e., the co-existence of some visual entities in both ground truth and other moments. As shown in Figure 1(a), the entity of ‘person’, ‘blade’ and ‘belt’ appear in both ground-truth moment m4 and others. Since there exist no classiﬁcation labels in video grounding, previous meth-ods distinguish positive and negative samples only based on the annotated moments. This strict scheme, however, ignores the semantic overlapping among video moments, which leads to the contradiction in feature representations.
As shown in Figure 1(a)(b), the moment m1 of “sharpen the blade on the first belt” only differs from target moment m4 in the ‘second’ order. They share sim-ilar semantic meanings but are forced to be pushed away in feature space. It is not consistent with the alignment princi-ple of the ideal contrastive learning.
Another issue lies in the sparse annotation dilemma [67, 31]. Due to the costly labeling process, only a few mo-ments are annotated regardless of the thousands of frames contained. Such severe data imbalance leads to signiﬁcant learning bias for vanilla contrastive learning, i.e., unanno-tated moments are pushed away by different queries, re-gardless of the semantic relationships. Consequently, their representations are close, although they don’t necessarily have strong semantic similarities. This undermines the uni-formity requirement of contrastive learning. For example in
Figure 1(b), wrong results always exist when encountering these unannotated moments (e.g., m1).
To address the issues mentioned above, we propose a novel Geodesic and Game Localization (G2L), a semanti-cally aligned and uniform video grounding framework as shown in Figure 1(c). We propose to measure the similarity according to the geodesic distance [29] between two video moments along the manifold. In Figure 1(c), the geodesic distance between m4 to m6 is the length of the shortest path as the moment graph, i.e. m4 → m2 → m3 → m5 → m6.
In contrast to previous methods, we construct positive and negative pairs based on the geodesic distance rather than the temporal moment to relax the strict positional principle.
The geodesics from the target moment to other moments are used to guide the maximizing mutual information. In this manner, the distance between video moments correctly reﬂects semantic relevance.
Unfortunately, the relaxed contrastive objective with geodesic leads to one side-effect, i.e., the model may con-fuse similar video moments. As shown in Figure 1(c), the model may falsely map m1 to be as close to the query q since m1 shares a similar appearance with the ground truth m4. To further prevent the model from confusing similar video moments, we formulate video moments and queries as multiple players into a cooperative game and quantify their game-theoretic interactions (i.e., Shapley in-teractions [49, 20]). Through this, we evaluate the marginal contributions of each ﬁne-grained component, which leads to a more accurate division. However, computing the ex-act Shapley interaction for all players is an NP-hard prob-lem [41] and is difﬁcult to achieve solutions in the video grounding setting. Therefore, we further propose a seman-tic Shapley interaction module, which samples similar intra-video moments by geodesic distance with a focus on their nuances.
In sum, our contributions are summarized as follows:
• We present G2L which introduces geodesic and game theory to learn the semantic alignment and uniformity between video and query for video grounding.
• We propose a novel geodesic-guided contrastive learn-ing scheme that considers the correct semantics of all moments in the video.
• We introduce an effective semantic Shapley interaction strategy based on geodesic distance.
• Extensive experiments on three public datasets demon-strate the effectiveness of our G2L. 2.