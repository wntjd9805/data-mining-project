Abstract
In medical vision, different imaging modalities provide complementary information. However, in practice, not all modalities may be available during inference or even train-ing. Previous approaches, e.g., knowledge distillation or image synthesis, often assume the availability of full modal-ities for all subjects during training; this is unrealistic and impractical due to the variability in data collection across sites. We propose a novel approach to learn enhanced modality-agnostic representations by employing a meta-learning strategy in training, even when only limited full modality samples are available. Meta-learning enhances partial modality representations to full modality represen-tations by meta-training on partial modality data and meta-testing on limited full modality samples. Additionally, we co-supervise this feature enrichment by introducing an aux-iliary adversarial learning branch. More specifically, a missing modality detector is used as a discriminator to mimic the full modality setting. Our segmentation frame-work significantly outperforms state-of-the-art brain tumor segmentation techniques in missing modality scenarios. 1.

Introduction
Multiple medical imaging modalities/protocols are re-quired to provide complementary diagnostic cues to clin-icians. For instance, multiple Magnetic Resonance Imag-ing (MRI) sequences (henceforth referred to as modalities), namely native T1, post-contrast T1 (T1c), T2-weighted (T2), and Fluid Attenuated Inversion Recovery (FLAIR) are used together to understand the underlying spatial complex-ity of brain tumors and their surroundings [3, 5]. Deep learning approaches [21, 36, 10, 58, 51, 49] have found great success in multimodal brain tumor segmentation and treatment response assessment. These conventional brain tumor segmentation methods perform well only when all
Figure 1: Comparison of the paradigms generally adopted by existing missing modality approaches (left) vs. ours (right) for brain tumor segmentation. N and n refer to the number of subjects (patients) with partial and full modal-ities, respectively.
Previous methods either utilize full modality data Df for all subjects or simulate partial modal-ity data Dm from Df . On the contrary, our approach works in a limited full modality setting, i.e., |Df | ≤ |Dm|. four acquisition modalities are available as input (i.e., in the full modality setting). However, in clinical practice, of-ten only a subset of modalities are available due to issues including image degradation, motion artifacts [18], erro-neous acquisition settings, and brief scan times. Hence it is crucial to develop robust modality-agnostic methods which can achieve state-of-the-art performance in missing modal-ity settings, i.e., when different modalities are unavailable during inference or even training.
Recently, a plethora of works has been proposed to ad-dress missing modality scenarios for brain tumor segmen-tation. Two major categories include: 1) Knowledge distil-lation: These methods [43, 23, 52, 50, 2] learn privileged information from a teacher network trained on full modal-ity data, i.e., data with all modalities available. 2) Image synthesis: Several works [42, 54, 59, 25, 55] train gener-ative models to synthesize images of the missing modali-ties. The synthesized “full modality” images are used for segmentation. One major issue is that both categories of methods require full modality data for all subjects in the training set (see Fig. 1a), either to train the teacher or the
generator. This can be very unrealistic; in real-world appli-cations, most studies only have very limited full modality data, far from sufficient for training. In this paper, we focus on a more realistic setting: most training data is only partial modality data, i.e., having a few modalities missing. We ask the following question: How do we efficiently learn from a large amount of partial modality data and a small amount of full modality data (see Fig. 1b)?
Another category recently rising in popularity is Shared
Latent Space modeling [22, 14, 28, 7, 13, 56, 57, 53, 20, 30, 60, 8]. These methods learn a shared latent represen-tation from partial modality data. However, the quality of the learned representation can be limited by the heterogene-ity of available modalities. The learned representation will be biased towards the most frequently available modalities and essentially overlook minority modalities (i.e., modali-ties that appear less frequently in training). This will in-evitably lead to sub-optimal performance on test data, es-pecially with minority modalities. To compensate for this undesirable bias, these methods often resort to segmenting all modalities individually from the shared representation, ultimately requiring full modality for all cases during train-ing.
These observations, further summarized in Tab. 1, moti-vate us to design a modality-agnostic method that can fully utilize partial modality data. Through the usage of the meta-learning strategy, our method learns enriched shared rep-resentations that are generalizable and not biased towards more frequent modalities, even with limited full modality data.
Category
KD [23, 52, 50, 2]
GAN [42, 54, 59, 25, 55]
Shared (others) [8, 13, 56]
SMIL [31]
Shared (Ours)
Can handle limited FM? Learns Unbiased mapping?
N
N
N
Y
Y
Y
Y
N
N
Y
Table 1: Advantages of our approach over existing frame-works. We are able to train in a limited full modality (FM) setting (with ≤ 50% FM samples), and learn an unbiased mapping that is unaffected by the proportion of any given modality in training.
Our core idea is based on the meta-learning technique
[16]. Meta-learning provides an effective framework to learn to perform multiple tasks in a mutually beneficial manner. We consider segmentation with each partial modal-ity input combination as a different task, yielding 2M − 1 meta-tasks for M modalities. By learning all meta-tasks in parallel, meta-learning ensures the network gen-erates modality-agnostic representations. Thanks to meta-learning, tasks depending on rare modalities can be signifi-cantly improved even with limited training data. This max-imally mitigates the bias against rare modalities. Mean-while, we propose using a small amount of full modality
Figure 2: Framework overview. Dm (partial modality) and
Df (full modality) are used as inputs for encoder-decoder networks in the meta-train and meta-test phase, respec-tively. Partial modality representations are adapted to the full modality domain via: 1) meta-optimization of gradients in both data, and 2) adversarial learning based on predic-tions by a modality absence classifier. data only during meta-testing. Meta-testing is introduced as an intermediate step in meta-learning to boost the gener-alization performance of the model across different tasks.
Using full modality data, albeit limited, in meta-test can maximally leverage such data to enhance the representation quality of the model. This innovative meta-learning design ensures we learn with a large amount of partial modality data and only a small amount of full modality data, with negligible partial modality bias.
Recently a meta-learning approach [31] performed clas-sification with missing modalities. They predict the prior weights of modalities via a feature reconstruction network, the quality of which is indirectly dependent on the num-ber of full modality samples. This method is unsuitable for our segmentation framework since conventional approaches (PCA [40], K-Means [33]) cannot be used to cluster the pri-ors in a high dimensional latent space. Moreover, [31] deals with only two input modalities and considers them individu-ally as meta-tasks, while we construct a heterogeneous task distribution with different combinations of inputs respect-ing the heterogeneity settings of real-world data.
We also employ a novel adversarial learning technique that further enhances the quality of the generated shared latent space representation.
Previous GAN-based ap-proaches [42] reconstruct the missing modalities in im-age space; this leads to the impractical requirement of full modality as ground truth for training. Our task is achieved in latent space by designing the discriminator as a multi-label classifier. The discriminator predicts the pres-ence/absence of modalities from the fused latent represen-tation performing a binary classification for each modality.
Our ultimate goal is to hallucinate the full modality repre-sentation from the hetero-modal feature space. Note that due to the hetero-modal nature of the data, the number of available modalities can vary dramatically across subjects.
To address this, we utilized a channel-attention weighted
fusion module that can accept a varying number of repre-sentations as input but generates a single fused output.
Overall, our contributions can be summarized as follows:
• We propose a meta-learning paradigm to train with hy-brid data (partial and full modalities) and also enhance the learned partial modality representations to mimic a full modality representation. This is accomplished by meta-training on partial modality data while finetun-ing on limited full modality data during the meta-test.
Such a training strategy overcomes the over-reliance on full modality data, as well as succeeds in learning an unbiased representation for all missing situations.
• We introduce a novel adversarial learning strategy to further enrich the shared representations in the latent space. It differs from other generative approaches that synthesize missing images and demand full modality ground truths for training. Our approach does not ne-cessitate reconstructing missing modality images. 2.