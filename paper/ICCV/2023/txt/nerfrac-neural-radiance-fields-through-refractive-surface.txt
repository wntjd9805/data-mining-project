Abstract
Neural Radiance Fields (NeRF) is a popular neural rep-resentation for novel view synthesis. By querying spatial points and view directions, a multilayer perceptron (MLP) can be trained to output the volume density and radiance along a ray, which lets us render novel views of the scene.
The original NeRF and its recent variants, however, are limited to opaque scenes dominated with diffuse reflec-tion surfaces and cannot handle complex refractive sur-faces well. We introduce NeRFrac to realize neural novel view synthesis of scenes captured through refractive sur-faces, typically water surfaces. For each queried ray, an
MLP-based Refractive Field is trained to estimate the dis-tance from the ray origin to the refractive surface. A re-fracted ray at each intersection point is then computed by
Snell’s Law, given the input ray and the approximated lo-cal normal. Points of the scene are sampled along the re-fracted ray and are sent to a Radiance Field for further ra-diance estimation. We show that from a sparse set of im-ages, our model achieves accurate novel view synthesis of the scene underneath the refractive surface and simultane-ously reconstructs the refractive surface. We evaluate the effectiveness of our method with synthetic and real scenes seen through water surfaces. Experimental results demon-strate the accuracy of NeRFrac for modeling scenes seen through wavy refractive surfaces. Github page: https:
//github.com/Yifever20002/NeRFrac. 1.

Introduction
Neural view synthesis has become one of the most pop-ular topics in computer vision and graphics, thanks to the breakthrough brought by NeRF[25]. Many follow-up works have extended NeRF in various ways. Parameterization of the viewing direction, however, has basically stayed the same, which fundamentally limits their application to opaque and mostly Lambertian surfaces which is easier to interpolate with a neural representation. Significant depar-tures from this fundamental requirement implicitly imposed by the viewing parameterization can cause dramatic accu-(a) (b)
Figure 1: (a) Refraction causes significant erroneous defor-mations to NeRF as it fundamentally relies on straight-line sampling. It produces artifacts when dealing with scenes behind refractive surface. (b) Novel view synthesis of an underwater scene. Our method is physically-based and renders scenes through the refractive surface accurately in comparison to past state-of-the-art NeRFs. racy drop. To handle specular reflection, for instance, a non-trivial reparameterization becomes essential [45].
Another limitation of NeRF is that it samples points on straight rays. This is a stronger assumption than one may think, as it implies that, under any circumstances, light travels along a straight line from the scene surfaces to the
viewer (human eyes or cameras). A typical physical phe-nomenon we encounter in everyday life that violates this assumption is refraction. Consider looking at an underwa-ter scene from outside of the water, for instance, fish swim-ming in a pond. When viewed from different angles, this re-fracted underwater scene no longer conforms to the straight ray sampling assumption of NeRF. It would cause unwanted deformations to the scene.
Some NeRFs model light bending with an additional de-form layer[31, 33, 13], which outputs an offset of each 3D point to smooth the rendering results. This offset, however, is not physically grounded and cannot model light refrac-tion. We also experimentally find that in underwater scenes, images rendered with a deform layer do maintain visual co-herence, but severely deviate from the ground truth.
In this work, we propose NeRFrac, a NeRF that models deformation caused by refraction from its first principles.
Instead of simply deforming points in 3D space, we first estimate the depth of the refractive surface and then bend rays instead of points according to the Snell’s Law. Our
MLP-based neural Refractive Field allows direct inference of intersections between rays and the refractive surface and can implicitly learn the multi-view consistency. Once the intersection is estimated, we can proceed to calculate each refracted ray by Snell’s law. Then 3D points are sampled along the refracted ray which are input to our underwater
Radiance Field.
Water surfaces are the most commonly encountered re-fractive surfaces in daily life. Thus, we evaluate the effec-tiveness of our method on modeling underwater scenes. We build a 3 × 3 camera array to obtain a novel dataset of real underwater scenes. Objects are placed under the water sur-face, while the camera array captures images from outside the water surface. To evaluate the accuracy of the simul-taneously recovered water surface itself, we also build a novel synthetic dataset using ray tracing [20] to create vari-ous complex water surfaces. The experimental results show that our NeRFrac outperforms the related NeRF methods.
We also show that by removing the learned Refractive Field, we can view the underwater scene as if it were in air (or the viewer were in the water), just with images captured out-side from the water surface. To summarize, we make the following main contributions in this paper: 1) NeRFrac, an end-to-end method for novel view syn-thesis of scenes captured through refractive surfaces trained with only sparse images; 2) A novel Refractive Field as part of NeRFrac, which explicitly recovers the 3D complex refractive surfaces, whose removal realizes elimination of the refractive surface (e.g., in-water viewing); 2.