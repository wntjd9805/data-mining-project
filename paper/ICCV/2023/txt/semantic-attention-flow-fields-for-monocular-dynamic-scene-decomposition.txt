Abstract
From video, we reconstruct a neural volume that cap-tures time-varying color, density, scene flow, semantics, and attention information. The semantics and attention let us identify salient foreground objects separately from the back-ground across spacetime. To mitigate low resolution seman-tic and attention features, we compute pyramids that trade detail with whole-image context. After optimization, we per-form a saliency-aware clustering to decompose the scene. To evaluate real-world scenes, we annotate object masks in the
NVIDIA Dynamic Scene and DyCheck datasets. We demon-strate that this method can decompose dynamic scenes in an unsupervised way with competitive performance to a super-vised method, and that it improves foreground/background segmentation over recent static/dynamic split methods.
Project webpage: https://visual.cs.brown.edu/saff 1.

Introduction
Given a casually-captured monocular RGB video of a dynamic scene, decomposing it into foreground objects and background is an important task in computer vision, with downstream applications in segmentation and video editing.
An ideal method would also reconstruct the geometry and appearance over time including frame correspondences.
Previous methods have made great progress but there are many limitations. Some works assume that there is no object motion in the scene [32, 46, 31, 16, 4], take input from multi-view cameras [16, 46], or do not explicitly reconstruct the underlying 3D structure of the scene [4, 7].
For objects, some works rely on masks or user input to aid segmentation [42, 15, 7], or use task-specific training datasets [7]. Sometimes, works assume the number of foreground objects [4, 20]. Given the challenges, many works train and test on synthetic data [14, 38].
We present Semantic Attention Flow Fields (SAFF): A method to overcome these limitations by integrating low-level reconstruction cues with high-level pretrained information— both bottom-up and top-down—into a neural volume. With this, we demonstrate that embedded semantic and saliency (attention) information is useful for unsupervised dynamic scene decomposition. SAFF builds upon neural scene flow fields [18], an approach that reconstructs appearance, geom-etry, and motion. This uses frame interpolation rather than explicit canonicalization [35] or a latent hyperspace [26],
Figure 1: We decompose a dynamic 3D scene from monocular input into time-varying color, density, scene flow, semantics, and attention information. This could be used to provide precise editing of objects or effects focused around objects. which lets it more easily apply to casual videos. For opti-mization, we supervise two network heads with pretrained
DINO-ViT [6] semantic features and attention. Naively super-vising high-resolution SAFF with low-resolution DINO-ViT output reduces reconstruction quality. To mitigate the mis-match, we build a semantic attention pyramid that trades detail with whole-image context. Having optimized a SAFF repre-sentation for a dynamic scene, we perform a saliency-aware clustering both in 3D and on rendered feature images to de-scribe objects and their background. Given the volume recon-struction, the clustering generalizes to novel spacetime views.
To evaluate SAFF’s dynamic scene decomposition capacity, we expand the NVIDIA Dynamic Scene [45] and DyCheck [10] datasets by manually annotating object masks across input and hold-out views. We demonstrate that SAFF outperforms 2D DINO-ViT baselines and is comparable to a state-of-the-art video segmentation method
ProposeReduce[19] on our data. Existing monocular video dynamic volume reconstruction methods typically separate static and dynamic parts, but these often do not represent meaningful foreground. We show improved foreground segmentation over NSFF and the current D2NeRF [40] method for downstream tasks like editing. 1
2.