Abstract 1.2. Answer Grounding
Answer grounding is the task of locating relevant visual evidence for the Visual Question Answering task. While a wide variety of attention methods have been introduced for this task, they suffer from the following three prob-lems: designs that do not allow the usage of pre-trained networks and do not benefit from large data pre-training, custom designs that are not based on well-grounded pre-vious designs, therefore limiting the learning power of the network, or complicated designs that make it challeng-ing to re-implement or improve them.
In this paper, we propose a novel architectural block, which we term Sen-tence Attention Block, to solve these problems. The pro-posed block re-calibrates channel-wise image feature-maps by explicitly modeling inter-dependencies between the im-age feature-maps and sentence embedding. We visually demonstrate how this block filters out irrelevant feature-maps channels based on sentence embedding. We start our design with a well-known attention method, and by mak-ing minor modifications, we improve the results to achieve state-of-the-art accuracy. The flexibility of our method makes it easy to use different pre-trained backbone net-works, and its simplicity makes it easy to understand and be re-implemented. We demonstrate the effectiveness of our method on the TextVQA-X, VQS, VQA-X, and VizWiz-VQA-Grounding datasets. We perform multiple ablation studies to show the effectiveness of our design choices. 1.

Introduction 1.1. Visual Question Answering
Visual Question Answering (VQA) systems try to accu-rately answer natural language questions regarding an input image [1]. This topic aims at developing systems that can communicate effectively about an image in natural language and comprehend the contents of images similar to humans.
The answer grounding task is defined as detecting the pixels that can provide evidence for the answer to a given question regarding an image [3]. In other words, the task is to return the image regions used to arrive at the answer for a given visual question (question-image pair) with an answer. Although the VQA community has made signifi-cant progress, the best-performing systems are complicated black-box models, raising concerns about whether their an-swer reasoning is based on correct visual evidence. By understanding the reasoning mechanism of the model, we can evaluate the quality of answers, improve model perfor-mance, and provide explanations for end-users. To address this problem, answer grounding has been introduced into
VQA systems, which requires the model to locate relevant image regions as well as answer visual questions.
By providing answer groundings in response to visual questions, numerous applications become possible. First, they allow for evaluating whether a VQA model is reason-ing correctly based on visual evidence. This is useful as an explanation as well as for assisting developers with model debugging. Second, answer grounding makes it possible to separate important regions from irrelevant background regions. Given that non-professional users can mistakenly have private information in the background of their pictures, answer grounding is a useful tool to obfuscate the back-ground for privacy preserving. Third, suppose a service can magnify the relevant visual evidence. In that case, users will be able to discover the needed information in less time. This is useful in part because VQA answers can be insufficient sometimes. 1.3. Multimodal Deep Learning
By definition, the VQA and answer grounding tasks are multimodal tasks since a method for these tasks should be able to process and correlate two different modalities.
Multimodal Joint-Embedding Models: These models merge and learn representations of multiple modalities in a joint feature space. Joint-embeddings underpin the building of a lot of cross-modal methods as they can bridge the gap between different modalities. In this joint space, the dis-tance of different points is equivalent to the semantic dis-tance between their corresponding original inputs.
Multimodal Attention-based Models: The main objec-tive of the attention mechanism is to design systems that use local features of image or text, extract features from different regions, and assign priorities to them. The atten-tion portion in an image determines salient regions. Then the language generation component focuses more on those salient regions for additional processing [35, 2, 12, 16, 26]. 1.4. Attention Mechanism
In deep learning, attention is a mechanism that mimics cognitive attention. The goal is to enhance the essential fea-tures of the input data and vanish out the rest. Attention methods can be classified into two classes based on their inputs: self-attention and cross-attention. Self-attention (also known as intra-attention) is a type of attention that quantifies the interdependence within the elements of a sin-gle input. At the same time, cross-attention (also known as inter-attention) finds the interdependence across two or more inputs [40, 18, 17]. Usually, cross-attention meth-ods are used for multimodal inputs [43]. Cross-attention models first process individual modalities using modality-specific encoders, then the encoded features are fed into cross-attention modules.
The Squeeze-and-Excitation method [13] is a channel-wise self-attention mechanism widely used in classification networks [37, 38]. It consists of a global average pooling of the input, followed by two linear layers with an interleaved non-linearity and a sigmoid function. Concretely, the output of this method is:
σ(F C(RELU (F C(g avg pool(X))))) × X (1) where X is the feature-maps. This module aims to dynami-cally focus on more important channels, essentially increas-ing the importance of specific channels over others. This is accomplished by scaling the more important channels by a higher value. While many feature descriptors exist to re-duce the spatial dimensions of the feature maps to a sin-gular value, this module uses average pooling to keep the required computation low. The next part of the module maps the scaling weights using a Multi-Layer Perceptron (MLP) with a bottleneck structure. The result values are scaled to a range of 0-1 by passing them through a sigmoid activation layer. Afterward, using a common broadcasted element-wise multiplication, the output is applied directly to the input. This multiplication scales each input feature-maps channel with its corresponding weight learned from the MLP in the Excitation module.
The following is the summary of our contributions:
• We present a novel attention module based on the Squeeze-and-Excitation method for the answer grounding task.
• We evaluate our method on common datasets and show that it achieves new state-of-the-art results.
• We compare our design with the top-performing net-works.
• We perform multiple ablation studies to learn more about this network. 2.