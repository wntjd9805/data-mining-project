Abstract
Live commerce is the act of selling products online through live streaming. The customer’s diverse demands for online products introduce more challenges to Livestream-ing Product Recognition. Previous works have primarily focused on fashion clothing data or utilize single-modal in-put, which does not reflect the real-world scenario where multimodal data from various categories are present.
In this paper, we present LPR4M, a large-scale multimodal dataset that covers 34 categories, comprises 3 modalities (image, video, and text), and is 50× larger than the largest publicly available dataset. LPR4M contains diverse videos and noise modality pairs while exhibiting a long-tailed dis-tribution, resembling real-world problems. Moreover, a cRoss-vIew semantiC alignmEnt (RICE) model is proposed to learn discriminative instance features from the image and video views of the products. This is achieved through instance-level contrastive learning and cross-view patch-level feature propagation. A novel Patch Feature Recon-struction loss is proposed to penalize the semantic mis-alignment between cross-view patches. Extensive experi-ments demonstrate the effectiveness of RICE and provide insights into the importance of dataset diversity and ex-pressivity. The dataset and code are available at https:
//github.com/adxcreative/RICE. 1.

Introduction
Livestreaming Product Recognition (LPR) [3, 9, 11] is one of the significant machine learning application in the e-commerce industry. Its goal is to recognize products a sales-person presents in a live commerce clip through content-based video-to-image retrieval. The real-time and accurate recognition of livestreaming products can facilitate the on-line product recommendation, and thereby improve the pur-chasing efficiency of consumers.
The task of LPR involves two fundamental processes: multimodal-based intended product identification and shop
*Equal contribution
†Corresponding author
Figure 1. The pipeline of LPR. A livestreaming consists of many clips introducing different products. We show two clip examples with ASR text in (a) and (d). In (b), the intended product refers to the product the salesperson is introducing, and the other products on the screen are indicated as the distracted background products. (c) presents a shop with hundreds of images, some with subtle vi-sual differences called small inter-class variations. The LPR aims to identify the clip’s intended product using the ASR text prompt, then retrieve the ground-truth product from the shop images. product retrieval. This task poses significant challenges in real-world scenarios, including (1) the need to distinguish intended products from the cluttered background products in a livestreaming frame, exemplified in Fig. 1 (b), (2) the requirement for models to capture sufficient fine-grained features to match the ground-truth (GT) image accurately in the shop, where there are many images with subtle visual nuances, (3) the heterogeneous video-to-image and cross-domain livestream-to-shop problem, and (4) the appear-ance changes of products in the livestreaming domain due to articulated deformations, occlusions, diverse background clutters, and significant illumination variations, making it a highly intricate task to match the clip to the GT image in the shop. Various datasets have emerged in the computer vi-sion community to study this task, including AsymNet [3],
WAB1, and MovingFashion [9]. However, AsymNet and
MovingFashion lack crucial text modal, which provides es-sential auxiliary information for identifying intended prod-ucts. Furthermore, the data scale of WAB is relatively small, with only 70K pairs, and only provides fashion clothing data, diverging from the real-world scenario.
In order to narrow the gap between existing datasets 1https://tianchi.aliyun.com/competition/entrance/231772/information
Figure 2. BAR CHART: Number of clips per category for LPR4M, with a long-tailed distribution resembling most real-world problems.
TABLE: Comparison of LPR4M against other LPR datasets in terms of modal, content, and scale. LPR4M offers significantly broader coverage of live commerce product categories and several orders of magnitude larger data scales. and the real-world scenario and advance research in this challenging task, we present LPR4M, a large-scale multi-modal live commerce dataset that includes extensive cate-gories, diverse data modalities of clip, image, and text, as well as heterogeneous and cross-domain correspondences of ⟨clip, image⟩ pairs. This dataset offers several signif-icant advantages. (1) Large-Scale: LPR4M contains over (2) Ex-4M pairs, significantly exceeding its precedents. pressivity: LPR4M draws data pairs from 34 commonly used live commerce categories rather than relying solely on clothing data. Additionally, LPR4M offers auxiliary clip ASR text and image title modalities, which are criti-cal for intended product identification and product feature representation. (3) Diversity: LPR4M promotes clip diver-sity while preserving the real-world data distribution, with a focus on three components: product scale, visible dura-tion, and the number of products in the clip, as depicted in
Fig. 3. To the best of our knowledge, LPR4M is currently the largest dataset created explicitly for real-world multi-modal LPR scenarios.
Our work based on LPR4M tackles a realistic problem: how to achieve fine-grained LPR using large-scale multi-modal pairwise data? Given image and clip views, we first utilize Instance-level Contrastive Learning (ICL) to align global features. However, since instance features of these two views are extracted independently from the visual en-coder, it can be challenging to differentiate between prod-ucts with subtle visual differences without cross-view inter-actions. Consequently, we propose a patch-level semantic alignment approach to enable cross-view patch information propagation. We suggest measuring similarity via a cross-attention based Pairwise Matching Decoder (PMD), which treats image patches as Query and video patches as both Key and Value. In addition, we propose a novel Patch Feature
Reconstruction (PFR) loss to provide patch-level supervi-sion for pairwise matching, expecting to reconstruct each feature of an image patch from its paired video patches.
The main contributions of this paper can be summa-rized as follows. (1) A large-scale live commerce dataset is collected, offering a significantly broader coverage of categories and diverse modalities such as video, image, and text. This dataset is the most extensive one known to date, tailored explicitly for real-world multimodal LPR scenarios. (2) The RICE model is introduced to integrate instance-level contrastive representation learning and patch-level pairwise matching into a framework. (3) A novel
Patch Feature Reconstruction loss is proposed to penalize the semantic misalignment between patches of video and image. (4) The benchmark dataset and evaluation proto-cols are carefully defined for LPR. Extensive experiments demonstrate the effectiveness of LPR4M and RICE. 2.