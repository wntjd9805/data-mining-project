Abstract
Video amodal segmentation is a particularly challeng-ing task in computer vision, which requires to deduce the full shape of an object from the visible parts of it. Recently, some studies have achieved promising performance by us-ing motion flow to integrate information across frames un-der a self-supervised setting. However, motion flow has a clear limitation by the two factors of moving cameras and object deformation. This paper presents a rethink-ing to previous works. We particularly leverage the su-pervised signals with object-centric representation in real-world scenarios. The underlying idea is the supervision signal of the specific object and the features from different views can mutually benefit the deduction of the full mask in any specific frame. We thus propose an Efficient object-centric Representation amodal Segmentation (EoRaS). Spe-cially, beyond solely relying on supervision signals, we de-sign a translation module to project image features into the Bird’s-Eye View (BEV), which introduces 3D informa-tion to improve current feature quality. Furthermore, we propose a multi-view fusion layer based temporal module which is equipped with a set of object slots and interacts with features from different views by attention mechanism to fulfill sufficient object representation completion. As a result, the full mask of the object can be decoded from image features updated by object slots. Extensive experi-ments on both real-world and synthetic benchmarks demon-strate the superiority of our proposed method, achieving state-of-the-art performance. Our code will be released at https://github.com/kfan21/EoRaS. 1.

Introduction
Deep learning has demonstrated remarkable success in various computer vision tasks. Nevertheless, neural net-† co-first authors; ∗ corresponding authors.
Figure 1: Illustrations of the difference between view prior, shape prior, and our model. While SaVos [35] draws sup-port from the optical flow to realize the view prior, image-level amodal segmentation algorithms typically just utilize the shape prior brought in by the supervision signals. Con-sequently, they are limited by camera motion and compli-cated object types, respectively. Unlike the previous meth-ods, beyond the mergence of those two priors, EoRaS uti-lizes view prior by object-centric learning and further intro-duces the BEV space where obstruction doesn’t exist, which enables our EoRaS to easily handle complex scenarios. works are limited to learning visible patterns in the data, and are typically challenged in reasoning about the broader and unseen components. Currently, most researches in object detection and segmentation tasks concentrate on enhancing the visible part’s performance, leaving few studies on in-ferring occluded information. Conversely, humans possess an innate ability to imagine and extrapolate, enabling us to easily complete an occluded part of an image based on prior knowledge. This critical capacity is instrumental in ad-vanced deep learning models for real-world scenarios, such as medical diagnosis and autonomous driving. Thereby, the central issue addressed in this paper is the video amodal segmentation task, which aims to deduce an object’s com-plete mask, whether it is partially obscured or not.
Prior studies on image amodal segmentation [22, 30, 32]
are over-reliance on prior knowledge, which actually ham-pers the model’s generalization abilities, resulting in lim-ited improvements under complex circumstances. For video amodal, Yao et al. [35] proposed that the occluded part of the current frame may appear in other frames, and therefore, information from all frames should be collected to fill in the occluded regions of any specific frame. While this method achieves promising results under the self-supervised setting, it fails when camera motion exists, as 2D warping is used to make connections within different frames, leading to dis-torted signals.
This paper aims to propose a better approach for video amodal segmentation by rethinking the importance of using supervised signals with object-centric representation. Such object-centric representations reflect the compositional na-ture of our world, and potentially facilitate supporting more complex tasks like reasoning about relations between ob-jects. While signals such as motion flow and shape priors have shown promising results, they are limited by moving cameras and complicated object types respectively. In con-trast, recent advances [6, 13, 21] in video object segmen-tation produce highly accurate object masks that are less sensitive to moving cameras, making them better suited as supervision signals. Surprisingly, such video object masks have not been fully exploited before.
To this end, we propose a novel approach that learns video amodal segmentation not only from observed ob-ject supervision signals in the current frame (shape prior) but also from integrated information of object features under different views (view prior). Our motivation is clearly shown in Fig. 1. By using visual patterns of other views to explain away occluded object parts in the current frame [31], our approach gets rid of optical flow and elim-inates the shortcomings of mere reliance on shape priors.
Our model is highly effective, even in complex scenarios.
In this paper, we propose a novel supervised method for the video amodal segmentation task that leverages a multi-view fusion layer based temporal module and a Bird’s-Eye
View (BEV) feature translation network. Rather than rely-ing on warping the amodal prediction into the next frame us-ing optical flow or using shape priors alone, we enhance the current frame features by incorporating feature information from different viewpoints and leveraging the supervision signals simultaneously. Specifically, we first extract front-view features from the videos using FPN50 [18]. Then, we employ a translation network to transform these front-view features into bird’s-eye view features, which bring in 3D in-formation through the usage of the intrinsic matrix. In con-trast to some related work [28] extracting object-centric 3D representation by object reconstruction, the acquisition of
BEV feature is simpler, faster, and easier to train. As each frame is equivalent to a unique view, features from both dif-ferent frames and the BEV space, which carry shape infor-mation about the occluded part, are further utilized. We repurpose the vanilla object-centric representations [19] – object slots to integrate those information, which is accom-plished by our novel multi-view fusion layer. Finally, we refine the front-view features using the updated object slots containing object information from multiple views and de-code the full mask. Compared to previous methods [35], our model can handle scenarios with 3D viewing angle changes or complex object shapes better by leveraging shape knowl-edge and integrating information across multiple views si-multaneously. To evaluate our method, we conduct exten-sive experiments on real-world and synthetic amodal bench-marks. The results demonstrate that our model achieves outstanding performance compared to comparable models and effectively demonstrates the efficacy of our architec-ture.
In summary, our main contributions are listed below. (1)
Our contribution lies in formulating the video amodal seg-mentation task using supervised signals for the first time.
Our model efficiently learns the shape and view priors, en-abling it to handle complex scenarios with ease. (2) We propose a novel approach to learning object-centric repre-sentations through a multi-view fusion layer based temporal module equipped with a set of object slots, which achieves significant improvement in the correlation of information from different views. (3) We introduce the novel concept of bird’s-eye view features in our amodal task, which provides front-view features with 3D information, resulting in con-sistent benefits. (4) By utilizing the bird’s-eye view genera-tor and multi-view fusion layer based temporal module, our algorithm achieves remarkable improvement on both real-world and synthetic amodal benchmarks, highlighting the novelty of our approach. 2.