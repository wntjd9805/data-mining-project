Abstract
This work aims for transferring a Transformer-based im-age compression codec from human perception to machine perception without fine-tuning the codec. We propose a transferable Transformer-based image compression frame-work, termed TransTIC. Inspired by visual prompt tuning,
TransTIC adopts an instance-specific prompt generator to inject instance-specific prompts to the encoder and task-specific prompts to the decoder. Extensive experiments show that our proposed method is capable of transferring the base codec to various machine tasks and outperforms the competing methods significantly. To our best knowl-edge, this work is the first attempt to utilize prompting on the low-level image compression task. 1.

Introduction
End-to-end learned image compression systems [8, 12, 42] have recently attracted lots of attention due to their competitive compression performance to traditional im-age coding methods, such as intra coding in VVC [4] and HEVC [39]. Among them, transformer-based autoen-coders [45, 31, 30, 40] emerge as attractive alternatives to convolutional neural networks (CNN)-based solutions be-cause of their high content adaptivity. Some even feature lower computational cost than CNN-based autoencoders. In common, most learned image compression systems are de-signed primarily for human perception.
Recently, image coding for machine perception becomes an active research area due to the rising demands for trans-mitting visual data across devices for high-level recognition tasks. Coding techniques in this area mainly include ap-proaches that produce multi-task or single-task bitstreams.
The methods with the multi-task bitstream feature one sin-gle compressed bitstream that is able to serve multiple downstream tasks, such as human perception and machine (a) Visual Prompt Tuning (VPT) [17]
Figure 1. Comparison of VPT [17] and our proposed TransTIC. (b) Ours (TransTIC) perception. Most of them [6, 11, 9, 27, 44] aim to learn a robust, general image representation via multi-task or con-trastive learning. However, a general bitstream can hardly be rate-distortion optimal from the perspective of each indi-vidual task.
The methods with the single-task bitstream allow the im-age codec to be tailored for individual downstream tasks, thereby generating multiple task-specific bitstreams. One straightforward approach is to optimize a codec end-to-end for each task [5]. However, given the sheer amount of machine tasks and their recognition networks, along with the ongoing developments of new machine tasks and models, customizing a neural codec, particularly hardware-based, for every one-off machine application would be prohibitively expensive even if not impossible. Region-of-interest (ROI)-based [38] and transferring-based meth-ods [26] are two preferred solutions. The former performs spatially adaptive coding of images according to an im-portance map, which can be optimized for different down-stream tasks. The latter aims to transfer a pre-trained base codec to a new task without changing the base codec. How-ever, how to transfer efficiently a given codec without re-training is a largely under-explored topic.
In this work, we aim to transfer a well-trained
Transformer-based image codec from human perception to machine perception without fine-tuning the codec.
In-spired by Visual Prompt Tuning (VPT) [17], we propose a plug-in mechanism, which injects additional learnable inputs, known as prompts, to the fixed base codec. As shown in Fig. 1 (a), VPT [17] targets re-using a large-scale, pre-trained Transformer-based feature extractor on differ-ent recognition tasks. This is achieved by injecting a small amount of task-specific learnable parameters, prompts, to the Transformer-based feature extractor and learning a task-specific recognition head. Different from VPT [17], which considers only the performance of the downstream recog-nition task, our task focuses on image compression, which needs to strike a balance between the downstream task per-formance and the transmission cost (i.e. the bitrate needed to signal the bistream). Fig. 1 (b) sketches the high-level de-sign of our proposed method. As shown, the Transformer-based encoder and decoder are initially optimized for hu-man perception while the recognition model is an off-the-shelf recognition network. To transfer the codec from hu-man perception to machine perception, we inject prompts to both the encoder and decoder. On the encoder side, we introduce an instance-specific prompt generator to generate instance-specific prompts by observing the input image. On the decoder side, the input image is not accessible. We thus introduce task-specific prompts to the decoder.
Our main contributions are four-fold:
• Without fine-tuning the codec, we transfer a well-trained Transformer-based image codec from hu-man perception to machine perception by injecting instance-specific prompts to the encoder and task-specific prompts to the decoder.
• To the best of our knowledge, this work is the first at-tempt to utilize prompting techniques on the low-level image compression task.
• The plug-in nature of our method makes it easy to in-tegrate any other Transformer-based image codec.
• Our proposed method is capable of transferring the codec to various machine tasks. Extensive experiments show that our method achieves better rate-accuracy performance than the other transferring-based methods on complex machine tasks. 2.