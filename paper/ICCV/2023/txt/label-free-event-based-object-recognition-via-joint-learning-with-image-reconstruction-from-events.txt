Abstract
Input
Output
Images
Recognizing objects from sparse and noisy events be-comes extremely difﬁcult when paired images and cate-gory labels do not exist.
In this paper, we study label-free event-based object recognition where category labels and paired images are not available. To this end, we pro-pose a joint formulation of object recognition and image reconstruction in a complementary manner. Our method
ﬁrst reconstructs images from events and performs ob-ject recognition through Contrastive Language-Image Pre-training (CLIP), enabling better recognition through a rich context of images. Since the category information is es-sential in reconstructing images, we propose category-guided attraction loss and category-agnostic repulsion loss to bridge the textual features of predicted categories and the visual features of reconstructed images using CLIP.
Moreover, we introduce a reliable data sampling strategy and local-global reconstruction consistency to boost joint learning of two tasks. To enhance the accuracy of pre-diction and quality of reconstruction, we also propose a prototype-based approach using unpaired images. Exten-sive experiments demonstrate the superiority of our method and its extensibility for zero-shot object recognition. Our project code is available at https://github.com/
Chohoonhee/Ev-LaFOR. 1.

Introduction
Event cameras are neuromorphic vision sensors that asynchronously perceive per-pixel brightness changes with high temporal resolution. They show advantages over con-ventional frame-based cameras, such as high dynamic range (HDR) and low latency. However, despite the superior-ity of cameras, algorithms using events are still in their infancy. Existing event-based object recognition meth-ods [1, 4, 22, 24, 25, 26, 30, 32, 34, 36, 40, 53] have been shown that object recognition is a crucial task for event fea-ture learning [12, 29, 38] and endeavors have successfully
*The ﬁrst two authors contributed equally. In alphabetical order.
Image
Reconstruction
Object
Recognition
Optional  for
Visual
Quality
Events
Category Labels
Paired Images
Dragonfly
Airplanes
Dog
Scissors
Dragonfly
Joshua Tree
Category Labels dog, dragonfly,  (cid:1710)
Category Set 
Crawling Images from Web
Figure 1: We tackle label-free event-based object recog-nition where category labels and paired images are not available. Our approach can simultaneously perform ob-ject recognition and image reconstruction through proposed joint learning framework. Optionally, our method can lever-age unpaired images to enhance object recognition perfor-mance and reconstructed image quality. classiﬁed the object with events. However, they rely on su-pervision with category labels to achieve high performance.
Furthermore, the sparsity of events poses a challenge for users to label them accurately when there are no paired im-ages. Several unsupervised methods [25, 36] have been pro-posed for object recognition, however, their performances remain subpar. Hence, there is a natural need for robust and effective label-free event-based object recognition methods.
In this paper, we study label-free event-based object recognition where category labels and paired images are not available as in Fig. 1. To tackle this task, we focus on two following facts. Firstly, as demonstrated in [45, 60], per-forming object recognition on reconstructed images with spatially dense information and rich context shows better performance than solely using events. Secondly, during re-constructing images from events, taking the categories into
account can improve the quality of the reconstructed im-ages. From these observations, we propose a joint learning framework of two closely-linked tasks: event-based object recognition and event-to-image reconstruction.
In our joint learning framework, we ﬁrst reconstruct im-ages from events and perform object recognition on the re-constructed images using Contrastive Language-Image Pre-training (CLIP) [42]. CLIP is an image-text embedding model that has been trained on a large number of image and language pairs sourced from the web. We utilize the textual features encoded from CLIP since they are well-aligned with visual features, which is advantageous for our tasks that lack labels and images. To this end, we propose the category-guided attraction loss that aligns the visual fea-tures from reconstructed images close to the texture features of the predicted category from CLIP.
Although the above framework enables the joint learn-ing of event-based object recognition and image recon-struction, using the predicted categories brings two issues. (1) Collapsing to the trivial solution, resulting in all out-puts being assigned to a single category and (2) Interrupt-ing the learning process due to unreliable (wrong) predic-tions. To alleviate these problems, we propose (1) category-agnostic repulsion loss to increase the distances between visual features for preventing collapse and (2) a reliable data sampling (RDS) method to reduce the effects of un-reliable predictions. For RDS, we devise two reliable indi-cators named posterior probability indicator (PPI) and tem-porally reversed consistency indicator (TRCI). The PPI se-lects reliable samples based on the probability of prediction.
For additional sampling, TRCI utilizes the characteristics of events that temporally reversed ones should be catego-rized into the same category as the original events. This reliable data sampling enables stable learning even with the predicted categories, leading to a substantial performance improvement in object recognition. Furthermore, to restore the local details, we introduce the local-global reconstruc-tion consistency. Finally, to enhance the accuracy of recog-nition and visual quality of reconstructed images, we ex-pand our method by employing the existing unpaired im-ages. Extensive experiments demonstrate that our method shows outstanding results on the subset of the N-ImageNet dataset over unsupervised and image reconstruction-based approaches and even surpasses the supervised methods on the N-Caltech101 dataset without using labels and images.
In addition, we show that our framework can be extended to event-based zero-shot object recognition.
Our work presents four main contributions: (I) We pro-pose a novel joint learning framework for event-based ob-ject recognition and image reconstruction, without requir-ing paired images and labels. (II) We introduce a reliable data sampling strategy and local-global reconstruction con-(III) We sistency that enhances label-free joint training. further introduce employing unpaired images in our frame-work for boosting performance. (IV) We conduct exten-sive experiments of our method along with zero-shot object recognition and superset categories learning, and demon-strate our method’s superior performance and effectiveness. 2.