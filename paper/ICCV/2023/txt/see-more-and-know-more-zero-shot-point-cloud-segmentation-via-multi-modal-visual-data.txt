Abstract
Zero-shot point cloud segmentation aims to make deep models capable of recognizing novel objects in point cloud that are unseen in the training phase. Recent trends favor the pipeline which transfers knowledge from seen classes with labels to unseen classes without labels. They typically align visual features with semantic features obtained from word embedding by the supervision of seen classes’ anno-tations. However, point cloud contains limited information to fully match with semantic features. In fact, the rich ap-pearance information of images is a natural complement to the textureless point cloud, which is not well explored in previous literature. Motivated by this, we propose a novel multi-modal zero-shot learning method to better utilize the complementary information of point clouds and images for more accurate visual-semantic alignment. Extensive exper-iments are performed in two popular benchmarks, i.e., Se-manticKITTI and nuScenes, and our method outperforms current SOTA methods with 52% and 49% improvement on average for unseen class mIoU, respectively. 1.

Introduction
Point cloud segmentation is a critical task for 3D scene understanding, which promotes the development of au-tonomous driving, assistive robots, digital urban, AR/VR,
Fully supervised methods [76, 66, 29, 36] have etc. achieved impressive performance. However, there exist tremendous categories of objects in the real world, espe-cially in large-scale outdoor scenes, bringing challenges for such methods to generalize to novel objects without labels in training data. Furthermore, manual annotations for 3D point clouds are extremely time-consuming and ex-pensive. Zero-shot learning can recognize unseen objects by utilizing side information, especially the word embed-ding, to transfer the knowledge of seen categories to unseen
*Equal contribution. † Corresponding author.
Semantic features of objects obtained by word em-Figure 1. bedding contain rich and diverse information, including appear-ance characteristics existing in images(i.e., color, light), geometry and location information contained in LiDAR point clouds(i.e., scale, shape), and some other non-visual properties(i.e., smell, weight). Previous image-based or point cloud-based zero-shot learning only considers the alignment between uni-modal visual features and semantic features, where the former can just match a small subset of the latter. We propose a more effective solution for zero-shot 3D segmentation by using multi-modal visual features. ones, which is important for the point cloud segmentation in large-scale scenes.
Zero-shot semantic segmentation on 2D images has made promising progress in the past few years [63, 3, 8, 40, 17, 27, 31, 32]. There are two main streams of methods, including generative methods and projection-based meth-ods, which inspire the following research works on 3D point clouds. For generative methods [47, 44], they usu-ally train a fake feature generator supervised by seen classes and fine-tune a classifier for recognizing real seen-class fea-tures and synthesized unseen-class features. However, 3D features are more difficult to generate than 2D features due to higher dimensional information, making such strategies
perform unsatisfactorily on 3D point clouds. Moreover, these methods require additional training efforts when new unseen categories appear, which limits the generalization capability on real-world applications. For projection-based approaches [15], they target to align visual features to cor-responding semantic features by the seen-class supervision, so that unseen class can be recognized by leveraging the similarity between its visual features and semantic features.
Such methods can be easily generalized to novel classes without retraining. However, visual features extracted from the point cloud can only match a subset of semantic features and yield limited performance, as shown in Fig. 1.
In fact, current autonomous vehicles and robots are usu-ally equipped with multiple sensors, where LiDAR and camera are the most common ones [9, 6]. Since point cloud contain accurate location and geometric information and images provide rich color and texture characteristics, many researchers focus on exploring sensor-fusion meth-ods [16, 43, 61] for achieving more precise perception.
Considering that see more and know more, we aim to make these two uni-modal visual data complement each other and generate more comprehensive visual features to better align with semantic features for more effective zero-shot learn-ing. To our knowledge, we are the first to explore zero-shot learning based on multi-modal visual data.
In this paper, we focus on transductive generalized zero-shot learning for point cloud-based semantic segmentation, where both seen and unseen classes will appear in one scene but only objects of seen classes have labels during training.
Based on the input of the synchronized point cloud and im-age, we propose a novel zero-shot point cloud segmenta-tion method. Specifically, we propose an effective multi-modal feature fusion approach, termed Semantic-Guided
Visual Feature Fusion (SGVF), to obtain a more compre-hensive visual feature representation, where valuable infor-mation from two uni-modal visual features are adaptively selected under the guidance of semantic features. As op-posed to previous sensor-fusion methods, our strategy is more flexible and applicable for zero-shot learning by in-troducing semantic features to play an active role in the vi-sual feature fusion stage. In this condition, exactly valid in-formation can be utilized for the following semantic-visual feature alignment. Then, the knowledge of seen classes can be effectively transferred to unseen classes. Furthermore, to reduce the semantic-visual domain gap in advance, we pro-pose Semantic-Visual Feature Enhancement (SVFE) to enhance both semantic features and visual features by trans-ferring the domain knowledge, such as relationships among classes, to each other, which definitely benefits the follow-ing SGVF and the final semantic-visual alignment process.
Actually, our method can be easily extended to more visual modalities.
We conduct extensive comparisons with current 2D and 3D zero-shot segmentation methods and our method outper-forms others significantly on different datasets and settings.
The effectiveness of each module of our method is also ver-ified by ablation studies. In summary, our contributions are summarized as follows:
• We propose a novel multi-modal zero-shot approach for point cloud semantic segmentation.
• We design an effective feature-fusion method with semantic-visual feature enhancement, which can better align visual features with semantic features to benefit the recognition of unseen classes.
• Our method achieves state-of-the-art performance on
SemanticKITTI and nuScenes datasets. 2.