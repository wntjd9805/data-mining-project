Abstract
Inspired by recent advances in diffusion models, which are reminiscent of denoising autoencoders, we investigate whether they can acquire discriminative representations for classification via generative pre-training. This paper shows that the networks in diffusion models, namely denoising dif-fusion autoencoders (DDAE), are unified self-supervised learners: by pre-training on unconditional image genera-tion, DDAE has already learned strongly linear-separable representations within its intermediate layers without auxil-iary encoders, thus making diffusion pre-training emerge as a general approach for generative-and-discriminative dual learning. To validate this, we conduct linear probe and fine-tuning evaluations. Our diffusion-based approach achieves 95.9% and 50.0% linear evaluation accuracies on CIFAR-10 and Tiny-ImageNet, respectively, and is comparable to contrastive learning and masked autoencoders for the first time. Transfer learning from ImageNet also confirms the suitability of DDAE for Vision Transformers, suggesting the potential to scale DDAEs as unified foundation models.
Code is available at github.com/FutureXiang/ddae. (a) Denoising networks in pixel-space and latent-space diffusion models. (b) Evaluating DDAEs as self-supervised representation learners.
Figure 1. Denoising Diffusion Autoencoders (DDAE). Top: Dif-fusion networks are essentially equivalent to level-conditional de-noising autoencoders (DAE). The networks are named as DDAEs due to this similarity. Bottom: By linear probe evaluations, we confirm that DDAE can produce strong representations at some in-termediate layers. Truncating and fine-tuning DDAE as vision en-coders further leads to superior image classification performance. 1.

Introduction
Understanding data with limited human supervision is a crucial challenge in machine learning. To cope with mas-sive amounts of data with scarce annotations, deep learning paradigms are shifting from supervised to self-supervised pre-training. Regarding natural language processing (NLP), self-supervised models such as BERT [31], GPTs [43, 44, 8] and T5 [45] have achieved outstanding performance across diverse tasks, and large language models like ChatGPT [40] are showing a profound impact beyond the machine learn-ing community. Among these, BERT uses masked language modeling (MLM) as a pretext task to train encoders while which cannot generate full text samples. In contrast, GPTs
*Corresponding author. and T5 have shown capabilities in generating long para-graphs autoregressively (AR). Moreover, they prove that decoder-only or encoder-decoder models can acquire deep language understandings via generative pre-training, with-out the need of training an encoder intentionally. With the rise of AI-Generated Content (AIGC), GPTs and T5 have been garnering more attention compared to pure encoders, which unify the generative (e.g. translation, summarization) and discriminative (e.g. classification) tasks [44, 45].
In computer vision, self-supervised learners have not yet achieved similar feats as GPTs in bridging the gap between generation and recognition. While generative adversarial networks (GAN) [20, 30, 7] and autoregressive Transform-ers [55, 19, 47] can synthesize high-fidelity images, they do not offer significant benefits for discriminative tasks. For
recognition, contrastive methods [11, 12, 21, 9] build fea-tures through pretext tasks on augmented images. Masked autoencoders [23, 3, 60] introduce BERT-like masked im-age modeling (MIM) pre-training for Vision Transformers
[16], but they seem not natural and practical for convolu-tional networks. Note that although MIM-based methods can recover masked image tokens, they are problematic in synthesizing full images, mainly because the complete data distribution is not directly modeled. These methods are not referred to as unified generative-and-discriminative models.
Instead, we denote them as “semi-” generative for their par-tial similarities to full generative models (Table 1).
Theoretically, it is more practical to extend generative models for discriminative purposes and gain the benefit of both ways. Recently, we have witnessed the flourish of AI-generated visual content due to the emergence of diffusion models [26, 53], with state-of-the-art results reported in im-age synthesis [13, 29, 41], image editing [24] and text-to-image synthesis [46, 48, 50]. Considering such capability, versatility, and scalability in generative modeling, we ask: whether diffusion models can replicate the success of GPTs and T5, in becoming unified generative-and-discriminative learners? We regard them as very promising alternatives in the visual domain, based on the following observations: (i) It has been demonstrated that discriminative image or text encodings can be learned through end-to-end genera-tive pre-training [10, 36, 38], i.e. “analysis-by-synthesis”.
Intuitively, the full generation task should contain, and be more challenging than the semi-generative masked model-ing, suggesting that image (or language) generation is com-patible with visual (or language) understanding, but not vice versa. The generative pre-training paradigm supports dif-fusion as a meaningful discriminative learning method. (ii) Diffusion networks are trained as multi-level denois-ing autoencoders (DAE, Figure 1(a)). The idea of denoising autoencoding has been widely applied to discriminative visual representation learning [56, 57, 6]. More recently, masked autoencoders (MAE) [23] have further highlighted the effectiveness of denoising pre-training, which can also be inherited by diffusion networks — likewise, recovering images with large, multi-scale noise is nontrivial and may also require a high-level understanding of visual concepts. (iii) The benefits of diffusion-based representation learn-ing are evidenced. DDPM-seg [4] confirms that diffusion can capture pixel-wise semantic information, indicating the feasibility. Besides, previous attempts in GANs and Trans-formers, i.e. BigBiGAN [15] and iGPT [10], find that better image generation capability can translate to improved fea-ture quality, suggesting that diffusion is even more capable of representation learning as the state-of-the-art generative model. Diffusion-based representation learners can also be facilitated by large AIGC projects if the learned knowledge can be conveniently transferred from pre-trained models.
Model
Pre-training target and method (i) Generative pre-training (ii) Denoising autoencoding
Encoder-only, MLM Semi-Decoder-only, AR
Full
Enc-dec, MLM+AR Full
Natural Language Processing
BERT [31]
GPT [43]
T5 [45]
Computer Vision
MAE [23] iGPT [10]
Encoder-only, MIM Semi-Decoder-only, AR
Full
DDAE (ours) Enc-dec, Diffusion
Full
Masked –
Masked
Masked –
Multi-level
Gaussian
Table 1. A summary of self-supervised learners. DDAE takes full advantage of generative pre-training and denoising autoencoding.
Driven by this analysis, we investigate whether diffusion models, which incorporate the best practices of generative pre-training and denoising autoencoding (as summarized in
Table 1), can learn effective representations for image clas-sification. Our approach is straightforward: we evaluate dif-fusion pre-trained networks, namely denoising diffusion au-toencoders (DDAE), as feature extractors by measuring the linear probe and fine-tuning accuracies of intermediate ac-tivations (Figure 1(b)). For linear probing, we pass noised images with specific scales (or timesteps) to DDAE and ex-amine the activations at different layers. For fine-tuning, we truncate DDAE at the best representation layer as an image encoder and fine-tune it without additional noising.
We confirm that via end-to-end diffusion pre-training,
DDAEs do learn strongly linear-separable features, which lie in the middle of up-sampling and can be extracted when images are perturbed with noises. Moreover, we validate the correlation between generative and discriminative perfor-mance of DDAEs through ablation studies on noise config-urations, training steps, and the mathematical model. Eval-uations on CIFAR-10 [33] and Tiny-ImageNet [34] show that the diffusion-based approach is comparable to super-vised WideResNet [63], contrastive SimCLRs [11, 12] and
MAE [23] for the first time. The transfer ability has also been verified on ImageNet [49] pre-trained models, includ-ing the ones constructed by pixel-space UNets and latent-space Vision Transformers such as DiT [41].
Our study highlights the underlying nature of diffusion models as unified vision foundation models. The revealed duality of DDAEs as state-of-the-art generative models and competitive recognition models may inspire improvements to vision pre-training and applications in both domains.
With the insightful elucidation and observations presented in this paper, it is highly likely to transfer powerful discrim-inative knowledge from large-scale pre-trained AIGC mod-els like Stable Diffusion [48] in the near future. 2.