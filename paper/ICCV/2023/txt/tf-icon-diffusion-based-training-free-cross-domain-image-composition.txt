Abstract
Text-driven diffusion models have exhibited impres-sive generative capabilities, enabling various image edit-ing tasks. In this paper, we propose TF-ICON, a novel
Training-Free Image COmpositioN framework that har-nesses the power of text-driven diffusion models for cross-domain image-guided composition. This task aims to seam-lessly integrate user-provided objects into a speciﬁc vi-sual context. Current diffusion-based methods often involve costly instance-based optimization or ﬁnetuning of pre-trained models on customized datasets, which can poten-tially undermine their rich prior. In contrast, TF-ICON can leverage off-the-shelf diffusion models to perform cross-domain image-guided composition without requiring addi-tional training, ﬁnetuning, or optimization. Moreover, we introduce the exceptional prompt, which contains no in-formation, to facilitate text-driven diffusion models in ac-curately inverting real images into latent representations, forming the basis for compositing. Our experiments show that equipping Stable Diffusion with the exceptional prompt outperforms state-of-the-art inversion methods on various datasets (CelebA-HQ, COCO, and ImageNet), and that TF-ICON surpasses prior baselines in versatile visual domains.
Code is available at https://github.com/Shilin-LU/TF-ICON 1.

Introduction
Image composition task involves incorporating unique objects from different photos to create a harmonious image within a speciﬁc visual context, a.k.a. image-guided com-position. For instance, consider the scenario where one de-sires to incorporate a beloved panda into one’s favorite art-work, e.g., oil or sketchy painting. The objective is to create a new image where the panda blends seamlessly into the scene without altering the appearance of the panda and the background, just as an artist meticulously crafted this panda
for that artwork (See Figure 1). This task is inherently chal-lenging, as it requires maintaining illumination consistency and preserving identifying features. The challenge is further compounded when the photos come from various domains.
While recently large-scale text-to-image models [10, 19, 51, 56, 59, 61, 78] have achieved remarkable success in text-driven image generation, the ambiguity inherent in natural language presents challenges in conveying precise and nu-anced visual details, even with highly detailed text prompts.
Although this challenge is effectively addressed by enabling personalized concept learning [24, 25, 34, 37, 60], these methods require costly instance-based optimization and are limited in generating concepts with speciﬁed backgrounds.
Recent studies [67, 77] have shown that diffusion models can achieve image-guided composition by explicitly incor-porating additional guiding images. However, these models are retrained from the pretrained diffusion model on tailored datasets, which can damage the rich prior of the model. As a result, these models have limited compositional abilities be-yond their training domain and still require signiﬁcant com-putational resources.
Given the wealth of large text-to-image models that have been trained on extensive language-image datasets, we pose a question: how could these models be leveraged for image-guided composition without incurring costly training or
ﬁnetuning, thereby avoiding damaging the diverse prior?
To answer it, we propose the Training-Free Image COm-positioN (TF-ICON) framework, which equips attention-based text-to-image diffusion models with the capability to perform image-guided composition without requiring ad-ditional training, ﬁne-tuning, extra data, or optimization.
To the best of our knowledge, this is the ﬁrst training-free framework developed for image-guided composition. The framework is compatible with various diffusion model sam-plers, enabling completion within 20 steps, and harnesses rich semantic knowledge to facilitate image-guided compo-sitions across diverse domains (see Figure 1).
Our approach constitutes an image-guided composition interface through denoising from a reliable starting latent code with the injection of composite self-attention maps.
Finding the latent code that allows for reconstructing an input image while maintaining its editability, a.k.a. image inversion, is a challenging yet crucial step for state-of-the-art (SOTA) image editing frameworks involving real images
[15, 26, 35, 38, 48, 53, 54, 70]. For diffusion models, while denoising diffusion implicit models (DDIM) inversion [65] has been effective for unconditional diffusion models, it falls short for text-driven diffusion models [26, 50, 70, 71].
To circumvent this, we introduce the exceptional prompt to accurately invert real images into latent codes upon pre-trained text-to-image models to serve for further compo-sition generation. The accurate latent codes are composed as the starting noise for the diffusion process. Through the gradual injection of composite self-attention maps that are speciﬁcally designed to reﬂect the relations between guid-ing images, we are able to infuse contextual information from the background into the incorporated objects, which results in harmonious image-guided compositions.
To summarize, we make the following key contributions: 1. We demonstrate the superior performance of high-order diffusion ODE solvers compared to commonly used DDIM inversion for real image inversion. 2. We present an exceptional prompt that allows text-driven models to achieve accurate invertibility, laying a solid groundwork for subsequent editing. Experimen-tal results show that it surpasses SOTA inversion meth-ods on three vision datasets. 3. We propose the ﬁrst training-free framework that enables cross-domain image-guided composition for attention-based diffusion models. 4. We demonstrate quantitatively and qualitatively that our framework outperforms prior baselines for image-guided composition. 2.