Abstract
Despite excellent average-case performance of many im-age classifiers, their performance can substantially deteri-orate on semantically coherent subgroups of the data that were under-represented in the training data. These system-atic errors can impact both fairness for demographic mi-nority groups as well as robustness and safety under do-main shift. A major challenge is to identify such subgroups with subpar performance when the subgroups are not anno-tated and their occurrence is very rare. We leverage recent advances in text-to-image models and search in the space of textual descriptions of subgroups (“prompts”) for sub-groups where the target model has low performance on the prompt-conditioned synthesized data. To tackle the expo-nentially growing number of subgroups, we employ combi-natorial testing. We denote this procedure as PROMPTAT-TACK as it can be interpreted as an adversarial attack in a prompt space. We study subgroup coverage and identi-fiability with PROMPTATTACK in a controlled setting and find that it identifies systematic errors with high accuracy.
Thereupon, we apply PROMPTATTACK to ImageNet classi-fiers and identify novel systematic errors on rare subgroups. 1.

Introduction
Deep learning based approaches have revolutionized many fields of computer vision [31, 37] and are increasingly applied in safety-critical applications such as automated driving [20]. An important prerequisite for deployment of learned models in such safety-critical domains is that they need to work reasonably well for all subgroups from an op-erational design domain [10] and strong requirements are imposed on assuring safety of systems [8]. That is: there must not be catastrophic but avoidable failure cases on any subgroup, regardless of how rare the subgroup might be.
Unfortunately, spurious correlations in the training data can often result in classifiers that utilize shortcut decision mak-ing [18, 50, 30, 29] - a phenomenon long known from ani-mal and human psychology [40]. Such shortcuts can work
Figure 1. Samples along with histograms over two models’ class prediction rates (shown in left and right inlays, based on 400 sam-ples) for 4 different subgroups. The baseline subgroup (top left) is classified mostly as minivan by all models, while the misclas-sification rates to a snowplow (top right), to pickup (bottom left), and to police van (bottom right) are significantly increased on the shown subgroups for a VGG16, ViT-L/32, ResNet50 respectively.
We refer to Section 5 and D for more details and samples. well on subgroups that occur frequently in-distribution, that is: on data that follows the same distribution as the training data. However, they can easily fail after a domain shift to out-of-distribution data since very rare subgroups become suddenly much more frequent [57]. For instance, Beery et al. [6] demonstrate that a shift in background can largely af-fect an image classifier, resulting in misclassifying, e.g., a cow on the beach.
Accordingly, a crucial aspect of model auditing [2] is to separately evaluate the behaviour of a classifier on every subgroup from a large set of subgroups. If the performance of a classifier on certain subgroups is considerably worse than on the totality of the domain’s data, then we denote this subgroup as a systematic error of a classifier [13, 26, 53], a case of hidden stratification [48]. More specifically, a systematic error refers to a subgroup of inputs on which a pretrained classifier has a high probability of misclassi-fication (“error”) and at the same time a large percentage of elements in the subgroup share a human-interpretable con-cept: the group appears semantically coherent to a human (“systematic”). Applying methods for identifying such sys-tematic errors could become a prerequisite for deployment in many domains while at the same time, systematic error are actionable: their exemplars can be used for finetuning a model and improving its robustness, reliability, and fairness
[17, 28].
Some prior work [13, 26] require the availability of a la-belled hold-out set covering data from rare subgroups for identification of systematic errors on these subgroups. Un-fortunately, it is often expensive to acquire (labelled) data for certain subgroups that are very rare in the domain pre-deployment, even though subgroups could become much more frequent after some domain shift. This is problem-atic because systematic errors are much more likely to occur on subgroups that are rare in the training data. Other prior work is based on large unlabelled hold-out data but requires a human-in-the-loop [17], which increases the costs of sys-tematic error identification and thus limits applicability.
Another line of work (including ours) focuses on audit-ing models on synthetically generated subgroup data. Re-cent progress of text-to-image models [42, 43, 45, 9] in terms of compositionality can allow synthesizing data of rare subgroups that have not been part of the training data.
Wiles et al. [53] focused on an open-ended approach that synthesizes data according to the distribution induced by a fixed prompt that encodes the class but no subgroup infor-mation. Concurrently to our work, Vendrow et al. [51] gen-erated text-conditioned counterfactual examples to study the robustness to single semantic shifts.
We propose PROMPTATTACK (see Figure 2), which leverages text-to-image models for synthesizing images of subgroups by encoding subgroup information directly in the prompt. To deal with large operational design domains and the resulting combinatorial explosion of subgroups,
PROMPTATTACK builds upon combinatorial testing [38, 4] that allows a near-equable coverage of the operational de-sign domain while keeping the number of explored sub-groups relatively small. In contrast to the open-ended ap-proach by Wiles et al. [53], PROMPTATTACK is targeted and reliably explores subgroups from a prespecified oper-ational design domain (see Section 4.1). Moreover, it does not require any pretrained models or heuristics components for failure case clustering and captioning.
In contrast to
Vendrow et al. [51], PROMPTATTACK can identify system-atic errors on subgroups that require the concurrence of sev-eral semantic shifts (see Figure 7).
Overall, our main contributions are the following:
• In Section 3, we introduce PROMPTATTACK, a novel procedure for identifying systematic errors based on synthetic data from a text-to-image model, conditioned on a prompt encoding subgroup and class information.
PROMPTATTACK explores a large subset of subgroups from an operational design domain using combinato-rial testing, achieving near-equable coverage of sub-groups (Section 4.1).
• We propose a benchmark for testing and comparing methods for systematic error identification (Section 4.2).
In contrast to prior work [13], this benchmark does not train multiple classifiers with training-time in-terventions but is based purely on inference-time inter-ventions on zero-shot classifiers such as CLIP [41].
• PROMPTATTACK identifies classifier-specific and tar-geted systematic misclassifications on rare subgroups of ImageNet classifiers (see Figure 1 and Section 5). 2.