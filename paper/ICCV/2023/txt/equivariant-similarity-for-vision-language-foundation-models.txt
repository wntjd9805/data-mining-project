Abstract (VLMs),
This study explores the concept of equivariance in vision-language foundation models focusing specifically on the multimodal similarity function that is not only the major training objective but also the core delivery to support downstream tasks. Unlike the existing image-text similarity objective which only categorizes matched pairs as similar and unmatched pairs as dissimilar, equivariance also requires similarity to vary faithfully according to the semantic changes. This allows VLMs to generalize better to nuanced and unseen multimodal compositions. However, modeling equivariance is challenging as the ground truth of semantic change is difficult to collect. For example, given an image-text pair about a dog, it is unclear to what extent the similarity changes when the pixel is changed from dog to cat? To this end, we propose EQSIM, a regularization loss that can be efficiently calculated from any two matched training pairs and easily pluggable into existing image-text retrieval fine-tuning. Meanwhile, to further diagnose the equivariance of VLMs, we present a new challenging bench-mark EQBEN. Compared to the existing evaluation sets,
EQBEN is the first to focus on “visual-minimal change”.
Extensive experiments show the lack of equivariance in cur-rent VLMs1 and validate the effectiveness of EQSIM2. 1.

Introduction
Vision-language (VL) training is all about learning the fea-“good” features for each modality, such that tures should faithfully represent the underlying semantics.
Thanks to the large-scale image-text pairs on the Web, we have abundant multimodal supervision for the two features with the same semantic meaning [60, 35, 48, 27]—each matched image-text pair should have “similar” visual and textual features, and each unmatched pair should have “dis-similar” ones. Thus, the image-text similarity plays a cru-cial role to define the feature quality in training VL founda-1We also include results of Multimodal LLM in Appendix A.4. 2Code is available at https://github.com/Wangt-CN/EqBen
Figure 1: (a) Comparison between oracle and the latest SoTA
VLM FIBER [13] similarity measure by ranking the candidate im-ages with the given query texts. Check Appendix for full ranking results. (b) Measuring the similarity score change (number in ■) of FIBER [13] and our proposed EQSIM by applying a slight text change (“2”↔“3”). Darker color indicates larger similarity. tion models (VLMs) [68, 74, 35, 48, 27, 14, 13, 34].
Has the prevailing “matched vs. unmatched” similarity fulfilled its duty? Yes and no. On the one hand, recent
VLMs [51, 68, 13, 48, 74, 49] have demonstrated impres-sive results in various downstream VL tasks such as image-text retrieval. However, on the other hand, it is acknowl-edged by the community that the VLMs still fall short in nu-anced and complex semantic compositions [49, 9, 44, 62].
In this regard, we present a text-to-image retrieval exam-ple on LAION400M [54] with the most recent SOTA VLM
FIBER [13]. As shown in Figure 1(a), given the query text
“the house on the right side of the road”, we first invite 5 graduate students to rank 25 candidate images from most similar to least similar. The continuously decreasing rank-) is served as the oracle semantic ing from human judges ( similarity measure. We then compared this ranking with the
). Although FIBER correctly re-ones from FIBER [13] ( trieved the top-1 image (image#1, ranks 1), some semanti-cally incorrect images (e.g., image#25, ranks 17) are falsely ranked higher than the correct ones (e.g., image#2, ranks 20). Furthermore, when modifying the query text with a slight semantic change (“right” → “left”), the rankings re-where µ(I) (µ(T )) denotes the measure [52] in image (text) space, i.e., an infinitesimal unit of visual (textual) change. Based on Definition 1, we formally derive EQSIM, an equivariance loss for a hybrid learning strategy on both semantically close and distant training pairs (Section 3).
Specifically, EQSIM directly enforces s11 −s12 = s22 −s21 and s11 − s21 = s22 − s12 for semantically close samples; while for semantically distant samples, we derive a simpli-fied formulation of s12 = s21. We show that adding EQSIM as a regularization term improves existing similarity train-ing objectives significantly on challenging datasets (e.g., over 4% on Winoground [62]) and tricky tasks (e.g., around 30% on VALSE [44]). EQSIM can also retain or even im-prove retrieval performance on Flickr30K [46] dataset.
Equivariance Benchmark. To further facilitate the proper evaluation of equivariance in VL community, we present a novel evaluation benchmark dubbed EQBEN (Section 4).
Motivated by the examples in Figure 1(b), EQBEN fea-tures “slightly” mis-matched pairs with a minimal semantic drift from the matched pairs, as opposed to “very different” matched and unmatched pairs that are easily distinguish-able by both non-equivariant and equivariant similarities.
Unlike recent efforts [44, 62] focusing on minimal seman-tic changes in captions, EQBEN pivots on diverse visual-minimal changes, automatically curated from time-varying visual contents in natural videos and synthetic engines with more precise control. We benchmark a full spectrum of
VLMs on EQBEN, and reveal that the non-equivariant sim-ilarity in existing VLMs fails easily. On this new test bed,
EQSIM can serve as a remedy and bring a large performance gain of ∼3% on average.
Our contributions are summarized as follows: (1) We comprehensively study the problem of similarity equivari-ance in VLMs. We propose EQSIM for equivariant train-ing and EQBEN for diagnostic evaluation; (2) EQSIM is not only theoretically grounded but also simple, effective and easily pluggable; and (3) EQBEN clearly diagnoses that conventional evaluation is not responsive to equivariance.
Furthermore, EQSIM can significantly improve VLMs on
EQBEN, as well as other challenging benchmarks. 2.