Abstract
Semi-Supervised Semantic Segmentation (S4) aims to train a segmentation model with limited labeled images and a substantial volume of unlabeled images. To improve the robustness of representations, powerful methods introduce a pixel-wise contrastive learning approach in latent space (i.e., representation space) that aggregates the representa-tions to their prototypes in a fully supervised manner. How-ever, previous contrastive-based S4 methods merely rely on the supervision from the model’s output (logits) in logit space during unlabeled training. In contrast, we utilize the outputs in both logit space and representation space to ob-tain supervision in a collaborative way. The supervision from two spaces plays two roles: 1) reduces the risk of over-fitting to incorrect semantic information in logits with the help of representations; 2) enhances the knowledge ex-change between the two spaces. Furthermore, unlike pre-vious approaches, we use the similarity between represen-tations and prototypes as a new indicator to tilt training those under-performing representations and achieve a more efficient contrastive learning process. Results on two pub-lic benchmarks demonstrate the competitive performance of our method compared with state-of-the-art methods. 1.

Introduction
Semantic segmentation is a fundamental task in com-puter vision, aiming to classify each pixel in an image. Sig-nificant progress [22, 4] has been made in training on high-quality labeled images using segmentation models com-posed of an encoder and a segmentation head. However, annotating images is expensive and time-consuming. Semi-*equal contribution
†corresponding author
Figure 1. We enhance the knowledge exchange between the logit and representation spaces. Orange and blue represent different classes. Top: Existing contrastive-based S4 methods overlook the semantic information in representation space. Bottom: Our method uses dual-space collaborative supervision. supervised Semantic Segmentation (S4) alleviates the thirst for annotation by leveraging unlabeled images to train seg-mentation models.
Most existing works learn from unlabeled images via self-training [44, 37, 15] or consistency regularization [40, 38, 13] strategies, both of which retrain the model with its predictions on unlabeled images. Recently, great suc-cess has been achieved by introducing pixel-wise con-trastive learning to semantic segmentation, which endows the model with a stronger features-extracting ability by ac-cessing a more discriminative representation space. Spe-cially, these methods [45, 23] project each pixel to repre-sentation space as a representation and regularize it in a fully supervised manner, i.e., aggregating the representa-tions with the same class and separating them with differ-ent classes. In semi-supervised settings, due to limited la-bels, most methods [1, 32, 46] obtain supervision from the model’s output logits in logit space during the unlabeled training process. However, recent contrastive-based seman-tic segmentation methods [1, 32, 46, 45] mainly focus on the learning process in logit space while only taking that in representation space as an auxiliary task. The unidirectional supervision makes training dominated by the predicted log-its, leading to the neglect of information in the representa-tion space. We argue that this kind of single-space super-vision may incorrectly provide semantic guidance to rep-resentation learning and fails to facilitate knowledge ex-change between the two spaces (see Sec. 5.1).
In this work, we extend the single-space supervision to a dual-space supervision for contrastive-based S4 and pro-pose Collaborative Space Supervision (CSS). Our key in-sight is to: i) utilize the semantic information in represen-tations to obtain more reliable guidance during unlabeled training, and to enhance knowledge exchange between two spaces; ii) provide a more accurate reference for the model’s performance on predicting each representation to tilt train-ing those under-performing representations. To achieve ob-jective i), we obtain dense semantic predictions by retriev-ing the nearest class prototype for each representation in the representation space and then engage with predictions from the logits for collaborative supervision of the model.
For objective ii), we measure the similarity between the representations and prototypes and use the similarity after a normalization operation as the indicator for guiding the learning process in the representation space. Unlike previ-ous works that utilize confidence as the indicator to involve representation learning, the similarity directly reflects the confusion level between representations and prototypes, re-sulting in more efficient representation learning.
To summarize, our main contributions are three-fold: 1)
The dual-space collaboration for contrastive-based S4, en-hances the knowledge exchange between the logit and rep-resentation spaces. 2) Utilizing similarity to provide a more accurate reference for the model’s performance in represen-tation learning. 3) Extensive experiments on two S4 bench-marks demonstrate the effectiveness of our method. 2.