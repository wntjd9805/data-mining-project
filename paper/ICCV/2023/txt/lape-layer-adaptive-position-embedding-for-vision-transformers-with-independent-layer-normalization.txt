Abstract
Position information is critical for Vision Transformers (VTs) due to the permutation-invariance of self-attention operations. A typical way to introduce position information is adding the absolute Position Embedding (PE) to patch embedding before entering VTs. However, this approach operates the same Layer Normalization (LN) to token em-bedding and PE, and delivers the same PE to each layer.
This results in restricted and monotonic PE across layers, as the shared LN affine parameters are not dedicated to
PE, and the PE cannot be adjusted on a per-layer basis.
To overcome these limitations, we propose using two inde-pendent LNs for token embeddings and PE in each layer, and progressively delivering PE across layers. By imple-menting this approach, VTs will receive layer-adaptive and hierarchical PE. We name our method as Layer-adaptive
Position Embedding, abbreviated as LaPE, which is sim-ple, effective, and robust. Extensive experiments on image classification, object detection, and semantic segmentation demonstrate that LaPE significantly outperforms the default
PE method. For example, LaPE improves +1.06% for CCT on CIFAR100, +1.57% for DeiT-Ti on ImageNet-1K, +0.7 box AP and +0.5 mask AP for ViT-Adapter-Ti on COCO, and +1.37 mIoU for tiny Segmenter on ADE20K. This is remarkable considering LaPE only increases negligible pa-rameters, memory, and computational cost.
Figure 1. A brief illustration of the default PE joining method and our proposed LaPE. We take T2T-ViT-7 with 1-D sinusoidal
PE as an example, and we visualize the position correlation of first 5 layers to explain the emphasis and advantages of our method. (a) By default, token embedding and PE are coupled together and treated with the same Layer Normalization (LN) in each layer.
This yields monotonic and limited position correlations. (b) We argue that each layer’s token embedding and PE need independent
LNs (LNT, LNP).
In this way, the expressiveness of PE is en-hanced and the position correlations are adjusted into hierarchical and layer-adaptive. 1.

Introduction
Vision Transformer (VT) has become one of the most popular research topics due to its superior performance on
∗Equal Contribution. (cid:66)Corresponding author.
Project page: https://github.com/Ingrid725/LaPE various computer vision tasks, such as image classifica-tion, object detection, and semantic segmentation. ViT
[10] is the first pure transformer model for image classi-fication, which outperforms CNNs when applied to large training data. Since then, many works based on ViT [10] have sprung up. Lots of work improves the tokenization
[14, 44], self-attention mechanism [23, 45, 35, 9], archite-cuture [33, 29, 43, 37, 24], and position embedding (PE)
[6, 38, 27, 12].
Due to the permutation-invariance of self-attention oper-ation, it is critical to provide position information for VTs.
The solution can be roughly divided into two categories: (1) PE-based methods, including absolute and relative PE; (2) PE-free methods, typically designing modules with in-ductive bias (e.g., convolution) to include implicit position information. Most of the VTs use the absolute PE, and add it directly to the patch embedding before entering Trans-former Encoders. But seldom do they notice the defect of joining PE in this way.
In this paper, we analyze the input and output of each encoder layer in VTs using reparameterization and visu-alization, and find that the default PE joining method has inherent drawbacks, which limit the performance of VTs.
Most of the VTs deliver the same PE to each layer through shortcuts, and operate the same Layer Normalization (LN)
[1] to PE and token embedding in each layer. However, PE and token embedding represent different information and have different distributions, so the affine parameters in LN have to trade off between them, which limits the expressive-ness of PE and hence constrains the performance of VTs.
Fig. 1 (a) provides an illustration.
To overcome this limitation with minimum cost, we pro-pose to use two independent LN for token embeddings and
PE in each layer (see Fig. 1 (b)), and deliver the PE se-rially across layers (see Fig. 2 (c)). By doing so, VTs receive layer-adaptive and hierarchical PE. We name this new PE joining method Layer-adaptive Position Embed-ding (LaPE), which yields significant improvement versus the default absolute PE joining method. Moreover, LaPE achieves better performance than relative PE and can fur-ther improve it (see Table 1 and Table 7), and LaPE can be used in parallel with PE-free methods to further improve the performance of VTs. Upon analysis, we find that LaPE can significantly enhance the expressiveness of PE, e.g., trans-forming a sinusoidal PE with 1-D correlation into 2-D cor-relation (Fig. 1 and Fig. 3), or generating hierarchical PEs that change from local to global as the layer goes deeper (Fig. 1 and Fig. 4).
Extensive experiments on image classification and downstream tasks demonstrate that LaPE can be a general method for Vision Transformers. It is effective and robust to various VTs on multiple tasks and datasets. On image classification, LaPE improves 0.84% accuracy for ViT-Lite
[14] on Cifar10 [19], 1.06% for CCT [14] on Cifar100 [19], and 1.57% for DeiT-Ti [29] on ImageNet-1K [8]. On ob-ject detection, LaPE gains 0.7% APbox and 0.5% APmask for ViT-Adapter-Ti[5]. On semantic segmentation, LaPE improves 1.37% and 0.43% for tiny and small Segmenter
[28], respectively. Besides, LaPE can also make VTs robust to PE types. Original DeiT-Ti [29] shows a performance gap of 3.84% between sinusoidal PE and learnable PE, while
LaPE shrinks the gap to 0.59%. These results are remark-able, as the overhead introduced by LaPE (parameters, time and memory) is negligible compared to the improvement brought by it (Table 5 and 6).
To conclude, our contribution includes: 1 We provide theoretical analysis on the default use of absolute PE in common VTs and reveal its limitations. 2 We propose the LaPE, a new PE joining method, which is easy to implement and deploy. We reveal that LaPE can improve the expressiveness of PE and elevate the model performance. 3 Through extensive experiments, we verify that LaPE is a general and effective method for VTs on multiple tasks, including image classification, object detection, and semantic segmentation. 2.