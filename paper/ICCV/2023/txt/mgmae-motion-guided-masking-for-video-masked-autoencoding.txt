Abstract
Masked autoencoding has shown excellent performance on self-supervised video representation learning. Temporal redundancy has led to a high masking ratio and customized masking strategy in VideoMAE. In this paper, we aim to further improve the performance of video masked autoencod-ing by introducing a motion guided masking strategy. Our key insight is that motion is a general and unique prior in video, which should be taken into account during masked pre-training. Our motion guided masking explicitly incorporates motion information to build temporal consistent masking volume. Based on this masking volume, we can track the unmasked tokens in time and sample a set of temporal con-sistent cubes from videos. These temporal aligned unmasked tokens will further relieve the information leakage issue in time and encourage the MGMAE to learn more useful struc-ture information. We implement our MGMAE with an online efficient optical flow estimator and backward masking map warping strategy. We perform experiments on the datasets of
Something-Something V2 and Kinetics-400, demonstrating the superior performance of our MGMAE to the original
VideoMAE. In addition, we provide the visualization analysis to illustrate that our MGMAE can sample temporal consis-tent cubes in a motion-adaptive manner for more effective video pre-training. 1.

Introduction
Attention-based Transformer [38] has witnessed great success in computer vision since the introduction of Vision
Transformer (ViT) [12]. It has been applied for a variety of vision tasks and obtains state-of-the-art performance, such as image classification [36, 59, 52], object detection [23, 46], semantic segmentation [49], and object tracking [8].
Thanks to this high performance, ViT models have been also applied to the video domain for action recognition [5, 1] and detection [33, 54]. However, the high capacity of
Transformer often demands pre-training on a large-scale dataset to reduce the over-fitting risk of subsequent fine-(cid:66): Corresponding author (lmwang@nju.edu.cn). (a) original video clip (b) tube masking (c) random masking (d) motion guided masking
Figure 1: Comparison of different masking strategies.
Masked autoencoding [11, 17] has been explored in video domain for self-supervised pre-training by employing dif-ferent masking strategies: random masking [14] and tube masking [35]. We propose to track masking maps under the guidance of motion information (termed as motion guided masking). Our resulting MGMAE can build a more chal-lenging and meaningful task for video pre-training. tuning. Therefore, an effective pretraining strategy of ViT is particularly important for obtaining excellent performance in the video domain due to the smaller video dataset.
The early video transformers [1, 5] often rely on the pre-training of image-based transformer derived from the large-scale image dataset [10]. This pre-training scheme makes the learnt video model to be naturally biased by image-based ViTs. Recently, masked autoencoding (MAE) [35, 14, 41] has been explored for pre-training video transformer on the video dataset due to its simplicity and promising result in image domain [17]. However, unlike the image, video data is equipped with an extra time dimension and exhibits the unique property of temporal redundancy and correlation. This property requires some customized designs on video masked autoencoder compared with image-based
MAE. For example, VideoMAE and MAE-ST both propose to use an extremely high masking ratio in video masked autoencoder pre-training to improve its performance. In addition, VideoMAE devises a tube masking strategy of dropping tokens at the same position across frames to further relieve the information leakage in time. This tube masking
approach, though straightforward, makes the assumption of no or small motion occurring between adjacent frames. Such an assumption might be not true for some scenarios with high-speed motion.
Based on the above analysis, in this paper, we aim to propose a new masking strategy for improving video masked autoencoder pre-training, by explicitly using motion infor-mation to reduce information leakage in time. Specifically, we devise the Motion Guided Masking in the video masked encoder processing and the resulted masked autoencoder is termed as MGMAE. Motion is general prior information contained by video. The optical flow representation explic-itly encodes the movement of each pixel from the current frame to the next one. We propose to use this optical flow to align masking maps between adjacent frames to build consis-tent masking volumes across time. The consistent masking volumes enable to build a more challenging reconstruction task by enforcing only a small set of cube tracks visible to the encoder. Hopefully, this motion guided masking can further relieve the risk of information leakage in time and encourage learning more meaningful visual representations.
More specifically, we use an online and lightweight opti-cal flow estimator (RAFT [34]) to capture motion informa-tion, which could be seamlessly integrated into the existing
VideoMAE framework. To build the temporally consistent masking volume, we first randomly generate an initial mask-ing map at the base frame. Then, we use the estimated optical flow to warp the initial masking map to adjacent frames. With multiple warping operations, we build the tem-poral consistent masking volume for all frames in the video.
Finally, based on this masking volume, we sample a set of visible tokens to MAE encoders with top-k selection based on a frame-wise manner. The same autoencoding process with the original VideoMAE is applied to these sampled tokens for video pretraining. With this simple motion guided masking, we are able to further increase the difficulty of video pre-training task and thus lead to a better pre-trained model for subsequent fine-tuning.
We mainly verify the effectiveness of the proposed MG-MAE on the datasets of Something-Something V2 [16] and
Kinetics-400 [20] by comparing them with the original tube masking in VideoMAE. The results demonstrate that MG-MAE pre-training can result in more powerful video founda-tion models with higher fine-tuning accuracy on the down-stream tasks. In particular, on the motion-centric benchmark of Something-Something, the improvement of MGMAE is more evident, implying that our motion guided masking is adaptive to motion variations and can better capture tem-poral structure information for pre-training. We hope our findings can inspire some specific and unique designs in video masked autoencoding with respect to image counter-parts. 2.