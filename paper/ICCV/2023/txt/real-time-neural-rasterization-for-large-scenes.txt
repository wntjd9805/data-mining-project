Abstract 1.

Introduction
We propose a new method for realistic real-time novel-view synthesis (NVS) of large scenes. Existing neural ren-dering methods generate realistic results, but primarily work for small scale scenes (< 50m2) and have difﬁculty at large scale (> 10000m2). Traditional graphics-based rasterization rendering is fast for large scenes but lacks re-alism and requires expensive manually created assets. Our approach combines the best of both worlds by taking a moderate-quality scaffold mesh as input and learning a neu-ral texture ﬁeld and shader to model view-dependant effects to enhance realism, while still using the standard graphics pipeline for real-time rendering. Our method outperforms existing neural rendering methods, providing at least 30× faster rendering with comparable or better realism for large self-driving and drone scenes. Our work is the ﬁrst to en-able real-time rendering of large real-world scenes.
*Indicates equal contribution. †Work done while an intern at Waabi.
Synthesizing and rendering images for large-scale scenes, such as city blocks, holds signiﬁcant value in ﬁelds such as robotics simulation and virtual reality (VR). In these
ﬁelds, achieving a high level of realism and speed is of utmost importance. VR requires photorealistic renderings at interactive frame rates for an immersive and seamless user experience. Similarly, robot simulation development requires high-ﬁdelity image quality for real world transfer and high frame rates for evaluation and training at scale, especially for closed-loop sensor simulation [51].
Achieving both speed and realism in large-scale scene synthesis has been a long-standing challenge. Recently, a variety of neural rendering approaches [34, 44, 45] have shown impressive realism results in novel view synthesis (NVS). These methods fall into two primary paradigms: implicit and explicit-based approaches. Implicit-based ap-proaches [34, 6, 37, 65] represent scene geometry and ap-pearance with multi-layer-perceptrons (MLPs) and render novel views by evaluating these MLPs hundreds of thou-sands of times via volume rendering. Explicit-based ap-proaches [44, 45] reconstruct a geometry scaffold (e.g., mesh, point cloud), and then learn image features that are blended and reﬁned with neural networks (NN) to render a novel view. Both implicit and explicit methods require large amounts of NN computation to perform NVS. As a consequence, these approaches have primarily focused on reconstructing objects or small-scale scenes (< 50m2), and typically render at non-interactive frame rates (< 5 FPS).
Several recent methods have enabled rendering at higher frame rates (> 20 FPS) while maintaining realism through several strategies such as sub-dividing the scene [35, 69, 50, 54, 41], caching mechanisms [17, 21, 69, 22], and op-timized sampling [60, 36, 25]. However, these approaches still focus primarily on single objects or small scenes. They either do not work on large scenes (> 10000m2) or far-away regions due to memory limitations when learning and rendering such a large volume, or have difﬁculty maintain-ing both photorealism and high speed. One area where large scenes are rendered at high speeds is in computer graph-ics through rasterization-based game engines [23]. How-ever, to render with high quality, these engines typically re-quire accurate speciﬁcations of the exact geometry, light-ing, and material properties of the scene, along with well-crafted shaders to model physics. This comes at the cost of tedious and time-consuming manual efforts from ex-pert 3D artists and expensive and complex data collec-tion setups for dense capture [15, 19]. While several re-cent methods have leveraged rasterization-based rendering in NVS [9, 13, 3, 40, 31, 24], they have only been demon-strated on small scenes.
In this paper, we introduce NeuRas, a novel neural raster-ization approach that combines rasterization-based graph-ics and neural texture representations for realistic real-time rendering of large-scale scenes. Given a sequence of sen-sor data (images and optionally LiDAR), our key idea is to ﬁrst build a moderate quality geometry mesh of the scene, easily generated with existing 3D reconstruction methods [64, 46, 70, 35]. Subsequently, we perform ras-terization with learned feature maps and Multi-Layer Per-ceptrons (MLPs) shaders to model view-dependent effects.
Compared to computationally expensive neural volume ren-dering, leveraging an approximate mesh enables high-speed rasterization, which scales well for large scenes. Compared to existing explicit-based geometry methods that use large neural networks to perform blending and image feature re-ﬁnement, we use light-weight MLPs that can be directly ex-ported as fragment shaders in OpenGL for real-time render-ing. We also design our neural rasterization method with several enhancements to better handle large scenes. First, inspired by multi-plane and multi-sphere image representa-tions [73, 4], we model far-away regions with multiple neu-ral skyboxes to enable rendering of distant buildings and sky. Additionally, most NVS methods focus on rendering at target views that are close to the source training views.
But for simulation or VR, we need NVS to generalize to novel viewpoints that deviate from the source views. To ensure our approach works well at novel viewpoints, we utilize vector quantization [20, 55] to make neural texture maps more robust and to store them efﬁciently.
Experiments on large-scale self-driving scenes and drone footage demonstrate that NeuRas achieves the best trade-off between speed and realism compared to existing SoTA.
Notably, NeuRas can achieve comparable performance to
NeRF-based methods while being at least 30× faster. To the best of our knowledge, NeuRas is the ﬁrst method of its kind that is capable of realistically rendering large scenes at a resolution of 1920 × 1080 in real-time, enabling more scalable and realistic rendering for robotics and VR. 2.