Abstract
Offline vs Online-based Tracking Evaluation
Visual object tracking is essential to intelligent robots.
Most existing approaches have ignored the online latency that can cause severe performance degradation during real-world processing. Especially for unmanned aerial vehicles (UAVs), where robust tracking is more challenging and on-board computation is limited, the latency issue can be fatal.
In this work, we present a simple framework for end-to-end latency-aware tracking, i.e., end-to-end predictive visual tracking (PVT++). Unlike existing solutions that naively append Kalman Filters after trackers, PVT++ can be jointly optimized, so that it takes not only motion information but can also leverage the rich visual knowledge in most pre-trained tracker models for robust prediction. Besides, to bridge the training-evaluation domain gap, we propose a relative motion factor, empowering PVT++ to generalize to the challenging and complex UAV tracking scenes. These careful designs have made the small-capacity lightweight
PVT++ a widely effective solution. Additionally, this work presents an extended latency-aware evaluation benchmark for assessing an any-speed tracker in the online setting.
Empirical results on a robotic platform from the aerial per-spective show that PVT++ can achieve significant perfor-mance gain on various trackers and exhibit higher accu-racy than prior solutions, largely mitigating the degrada-tion brought by latency. Our code is public at https:
//github.com/Jaraxxus-Me/PVT_pp.git. 1.

Introduction
Visual object tracking1 is fundamental for many robotic applications like navigation [49], cinematography [5], and multi-agent cooperation [9]. Most existing trackers are de-veloped and evaluated under an offline setting [38, 29, 34,
*These authors contributed equally to this work.
†Corresponding author 1We focus on single object tracking in this work. 0.78 0.73 0.68 0.63 0.58 0.53 0.48  0 10 20
Real-time 40 30
Distance Precision | Speed (FPS) 0.61 0.56 0.51 0.46 0.41 0.36 0.31 10  0
Success Rate | Speed (FPS) 30 20
Real-time 40
PVT++
M
SiamRPN++
M
PVT++
Mask
SiamMask
PVT++
R
SiamRPN++
R
DiMP18
SiamGAT
PrDiMP18
SiamRPN
DaSiamRPN
SiamAPN++
Figure 1. Distance precision and success rate of the trackers on
UAVDT dataset [16]. Compared with offline evaluation, the track-ers suffer a lot from their onboard latency in the online setting (30 frames/s (FPS)). Coupled with PVT++, the predictive trackers achieve significant performance gain with very little extra latency, obtaining on par or better results than the offline setting. 33, 6, 8], where the trackers are assumed to have zero pro-cessing time. However, in real-world deployment, the on-line latency caused by the trackers’ processing time can-not be ignored, since the world would have already changed when the trackers finish processing the captured frame. In particular, with limited onboard computation, this issue is more critical in the challenging unmanned aerial vehicle (UAV) tracking scenes [21, 38, 20]. As shown in Fig. 1, compared with offline setting (gray markers), the latency can cause severe performance degradation during online processing (colored markers). If not handled well, this can easily lead to the failure of robotic applications such as UAV obstacle avoidance [1] and self-localization [61].
To be more specific, the latency hurts online tracking due to: (1) The tracker outputs are always outdated, so there will be mismatch between the tracker result and world state. (2) The trackers can only process the latest frame, so that the non-real-time ones may skip some frames, which makes object motion much larger (see Fig. 2(a) right).
Input timestamp
Output timestamp latency
Tracker output (boxes)
Predictor output (boxes)
Visual feature from trackers
Tracker
Tracker
Predictor
Realtime
Non-realtime
Realtime
Non-realtime rf 0 1 2 3 4 r0 r1 r2 r3 0 1 2 3 r0 r1 0 1 2 3 4
̂b1
̂b2
̂b3
̂b4 0 1 2 3
̂bf+1
. . .
. . .
̂b2
, 
̂b3
̂b4,
̂b5
. . . rf−1 rf
Object box rf−1 rf
Visual feature
Kalman filter
PVT++
Motion predictor
Visual predictor xf−1 xf
Joint predictor
̂bf+1
̂bf+1 (a) Standard Tracker (b) Latency-Aware Tracker (c) Comparison between [32,36] and PVT++
Figure 2. (a) Standard tracker suffers from onboard latency (height of the red boxes). Hence, its result lags behind the world, i.e., rf is always obtained after f on the timestamp. (b) Latency-aware trackers introduce predictors to compensate for the latency, which predict the word state, ˆbf +1, when finishing the processed frame. (c) Compared with prior KF-based solutions [32, 36], our end-to-end framework for latency-aware tracking PVT++ leverages both motion and visual feature for prediction.
I
The existence of the latency in real-world applications calls for trackers with prediction capabilities, i.e., predic-tive trackers. While a standard tracker yields the objects’ location in the input frame (i.e., when it starts processing the input frame, as in Fig. 2(a)), a predictive tracker pre-dicts where the objects could be when it finishes processing the input frame, as illustrated in Fig 2(b).
Existing solutions [32, 36] directly append a Kalman fil-ter (KF) [30] after trackers to estimate the potential object’s location based on its motion model (see Fig 2(c)). How-ever, the rich and readily available visual knowledge from trackers is primarily overlooked, including the object’s ap-pearance and the surrounding environments, which can be naturally exploited to predict the objects’ future paths [51].
To this end, we present a simple framework PVT++ for end-to-end predictive visual tracking. Composed of a tracker and a predictor, PVT++ is able to convert most off-the-shelf trackers into effective predictive trackers. Specif-ically, to avoid extra latency brought by the predictor, we first design a lightweight network architecture, consisting of a feature encoder, temporal interaction module, and predic-tive decoder, that leverage both historical motion informa-tion and visual cues. By virtue of joint optimization, such a small-capacity network can directly learn from the visual representation provided by most pre-trained trackers for an efficient and accurate motion prediction, as in Fig. 2(c).
However, learning this framework is non-trivial due to the training-evaluation domain gap in terms of motion scales.
To solve this, we develop a relative motion factor as train-ing objective, so that our framework is independent of the motion scales in training data and can generalize well to the challenging aerial tracking scenes. The integration of lightweight structure and training strategy yields an effec-tive, efficient, and versatile solution.
Beyond methodology, we found that the existing latency-aware evaluation benchmark (LAE) [32] is unable to pro-vide an effective latency-aware comparison for real-time trackers, since it evaluates the result for each frame as soon as it is given. In this case, the latency for any real-time trackers is one frame. Hence, we present an extended latency-aware evaluation benchmark (e-LAE) for any-speed trackers. Evaluated with various latency thresholds, real-time trackers with different speeds can be distinguished.
Empirically, we provide a more general, comprehensive, and practical aerial tracking evaluation for state-of-the-art trackers using our new e-LAE. Converting them into pre-dictive trackers, PVT++ achieves up to 60% improvement under the online setting. As shown in Fig. 1, powered by PVT++, the predictive trackers can achieve comparable or better results than the offline setting. Extensive experi-ments on multiple tracking models [33, 57, 22] and datasets
[47, 16, 37] show that PVT++ works generally for latency-aware tracking, which, to the best of our knowledge, is also the first end-to-end framework for online visual tracking. 2.