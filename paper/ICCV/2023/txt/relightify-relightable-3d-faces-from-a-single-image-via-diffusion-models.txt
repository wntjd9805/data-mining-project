Abstract 1.

Introduction
Following the remarkable success of diffusion models on image generation, recent works have also demonstrated their impressive ability to address a number of inverse prob-lems in an unsupervised way, by properly constraining the sampling process based on a conditioning input. Motivated by this, in this paper, we present the first approach to use diffusion models as a prior for highly accurate 3D facial
BRDF reconstruction from a single image. We start by leveraging a high-quality UV dataset of facial reflectance (diffuse and specular albedo and normals), which we render under varying illumination settings to simulate natural RGB textures and, then, train an unconditional diffusion model on concatenated pairs of rendered textures and reflectance components. At test time, we fit a 3D morphable model to the given image and unwrap the face in a partial UV texture.
By sampling from the diffusion model, while retaining the observed texture part intact, the model inpaints not only the self-occluded areas but also the unknown reflectance com-ponents, in a single sequence of denoising steps. In con-trast to existing methods, we directly acquire the observed texture from the input image, thus, resulting in more faithful and consistent reflectance estimation. Through a series of qualitative and quantitative comparisons, we demonstrate superior performance in both texture completion as well as reflectance reconstruction tasks.
Creating digital avatars of real people is of paramount importance for a range of applications, including VR, AR or the film industry. Human faces have been studied exten-sively over the years, attracting attention at the intersection of Computer Vision, Graphics and Machine Learning re-search. Although vast literature exists around the estima-tion of the 3D shape and reflectance of a face from un-constrained inputs such as “in-the-wild” RGB images, it still remains a challenging problem in the field. In partic-ular, the recent breakthrough in image synthesis using dif-fusion generative models creates a new perspective towards photo-realistic 3D face reconstruction, which has not been explored so far and stems from the state-of-the-art perfor-mance of these models in solving inverse problems without supervised training.
Facial reflectance capture typically requires a control-lable illumination system equipped with multiple cameras, first introduced as a Light Stage [12]. Polarized illumination and gradient patterns can be employed for diffuse-specular separation [48, 26], using which, spatially varying facial reflectance maps can be acquired, that describe BRDF pa-rameters, including the diffuse and specular albedo and nor-mals. Although recent works attempt to simplify the captur-ing apparatus and process using inverse rendering [28, 55] or commodity devices [38], such methods still require a la-borious capturing process and expensive equipment.
Since their introduction by Blanz and Vetter [3], 3D Mor-phable Models (3DMMs) [54, 11, 44, 5, 4] have been estab-lished as a robust methodology for monocular 3D face re-construction [18, 69] by regularizing the otherwise ill-posed optimization problem towards a known statistical prior of the facial geometry, which is usually defined by the lin-ear space of a PCA model.
In addition to the coarse ge-ometry estimation, 3DMMs have been used in conjunction with powerful CNN-based texture models, leading to im-pressively detailed avatar reconstructions even from low-resolution images [57, 23, 24]. Furthermore, another line of research [6, 32, 68, 2, 17, 16, 39, 41] revolves around the reconstruction of rendering assets such as reflectance com-ponents (diffuse and specular albedo) and high-frequency normals of the facial surface. As a result, the recovered 3D faces can be realistically rendered in arbitrary illumination environments. However, prior work either contains scene il-lumination inhibiting relighting [13, 21, 23] or is restricted by the models’ generalization, lowering the identity simi-larity [23, 39, 47]. Our work shares the same objective in that we couple a 3DMM with high-quality UV reflectance maps, but attempts to solve both of these issues, by preserv-ing the observed texture details from the input image and jointly inferring the facial reflectance.
In fact, the visible pixels of the facial texture by the given camera pose are directly recoverable from the input image via inverse rasterization of the fitted 3D mesh. Therefore, we cast the 3D face reconstruction problem as an image in-painting task in the UV space; i.e. the goal is to fill in the missing pixels in a consistent manner with respect to some statistical prior. In particular, we propose to use a diffusion model as the generative backbone of our method. Diffu-sion models [61] are naturally associated with guided image synthesis since they treat image generation as a sequence of denoising steps in the form of a learnable Markov process.
This allows to directly interfere with the sampling process, given that samples at each part of the chain are distorted ver-sions of real images with known noise variances. Thus, by properly modifying the sampling process, a single uncon-ditional diffusion model can be used for different inverse problems, such as image editing [50], inpainting [46, 10], restoration [36] or super-resolution [9, 8], without problem-specific training.
In this paper, we build a high-quality statistical model of facial texture and reflectance by means of a diffusion model and adopt an inpainting approach to complete the partially reconstructed UV texture produced by a 3DMM fitting step.
We further extend the sampling process to recover the miss-ing reflectance components by enforcing consistency with the input texture. As a result, our method, dubbed Relight-ify, generates accurate and render-ready 3D faces from un-constrained images, as shown in Fig. 1.
In summary, we make the following contributions:
• We present the first, to the best of our knowledge, diffusion-based approach for relightable 3D face re-construction from images. By training on a pseudo-ground-truth dataset of facial reflectance, while di-rectly recovering texture parts from the input, we achieve high-quality rendering assets that preserve im-portant details of the input face (e.g. wrinkles, moles).
• We propose an efficient way of predicting different modalities in a consistent way by learning a generative model on concatenated reflectance maps and casting the reconstruction as an inpainting problem, spatially, but also channel-wise.
• We qualitatively and quantitatively demonstrate the su-periority of our approach against previous methods re-garding both the completed textures as well as the re-covered reflectance maps. 2.