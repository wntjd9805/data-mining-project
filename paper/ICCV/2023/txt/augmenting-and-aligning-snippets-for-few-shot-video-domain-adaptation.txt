Abstract
For video models to be transferred and applied seam-lessly across video tasks in varied environments, Video Un-supervised Domain Adaptation (VUDA) has been intro-duced to improve the robustness and transferability of video models. However, current VUDA methods rely on a vast amount of high-quality unlabeled target data, which may not be available in real-world cases. We thus consider a more realistic Few-Shot Video-based Domain Adapta-tion (FSVDA) scenario where we adapt video models with only a few target video samples. While a few methods have touched upon Few-Shot Domain Adaptation (FSDA) in images and in FSVDA, they rely primarily on spatial augmentation for target domain expansion with alignment performed statistically at the instance level. However, videos contain more knowledge in terms of rich tempo-ral and semantic information, which should be fully con-sidered while augmenting target domains and perform-ing alignment in FSVDA. We propose a novel SSA2lign to address FSVDA at the snippet level, where the target domain is expanded through a simple snippet-level aug-mentation followed by the attentive alignment of snippets both semantically and statistically, where semantic align-ment of snippets is conducted through multiple perspec-tives. Empirical results demonstrate state-of-the-art per-formance of SSA2lign across multiple cross-domain ac-tion recognition benchmarks. Code will be provided at: https://github.com/xuyu0010/SSA2lign. 1.

Introduction
Video Unsupervised Domain Adaptation (VUDA) [4, 7, 51, 46, 53] aims to improve the generalizability and robust-ness of video models by transferring knowledge to new do-mains, and is widely applied in scenarios where massive labeled videos are unavailable. Current VUDA methods assume that sufficient target data are accessible which en-ables domain alignment by minimizing cross-domain distri-*Equal contribution.
†Corresponding author. bution discrepancies and obtaining domain invariant repre-sentations [4, 7, 54]. However, this assumption may not be feasible in real-world applications such as in smart hospitals and security surveillance where video models are leveraged for anomaly behavior recognition [35, 32], and are expected to be functional at all times even across different environ-ments. It is more practical to obtain a few labeled videos during the early stage of model deployment to improve the transferred models’ performances in the new (target) envi-ronment. A Few-Shot Video Domain Adaptation (FSVDA) task is hence formulated to enable knowledge learned from labeled source video to be transferred to the target video domain given only very limited labeled target videos.
With only several target domain samples, FSVDA is more challenging than VUDA, since aligning distributions with limited samples is harder. A few research have touched on the image-based Few-Shot Domain Adaptation (FSDA) [27, 45, 49, 11] by domain alignment, e.g., mo-ment matching or adversarial training, between a spatial-augmented target domain and a filtered target-similar source domain. More recently, there have been a few early research on FSVDA [12, 13] which extends the above strategies to videos by viewing each video sample as a whole and ob-taining frame-based video features.
However, there are two major shortcomings when the image-based FSDA is applied to video domains. Firstly, ap-plying frame-level spatial augmentation towards individual video frames ignores and undermines temporal correlation across sequential frames, and we find that such augmenta-tion would result in only minor or even negative effects on
FSVDA performance. Secondly, the effectiveness of do-main alignment methods is built upon sufficient source do-main and target domain data that depicts the distribution discrepancy, which is not available in FSVDA. Even worse, statistical estimation of video data distribution is less accu-rate due to the complicated temporal structure of video data.
In this paper, we aim to overcome these two challenges by designing more effective target domain augmentation and semantic alignment in the spatial-temporal domain.
To this end, we propose to address the FSVDA task by a Snippet-attentive Semantic-statistical Alignment with
Stochastic Sampling Augmentation (SSA2lign). Instead of aligning features of whole video samples at the video level or frame level [12, 13], we align source and target video fea-tures at the snippet level. Snippets are formed from a lim-ited series of adjacent sequential frames, thus they contain both spatial and short-term temporal information. Lever-aging snippet features for FSVDA brings two unique ad-vantages: i) a larger amount of target domain samples could be obtained via spatial-temporal augmentations on snippets, obtaining more diverse features across the temporal dimen-sion; ii) additional alignment of the diverse but highly cor-related snippet features of each video could further improve the discriminability of the corresponding videos, which has been proven to benefit the effectiveness of video do-main adaptation [6, 59, 21, 53]. SSA2lign is therefore pro-posed. It firstly augments the source and target domain data by a simple yet effective stochastic sampling process that makes full use of the abundance of snippet information and then performs semantic alignment from three perspectives: alignment based on semantic information within each snip-pet, cross-snippets of each video, and across snippet-level data distribution. Our method is demonstrated to be very effective for the FSVDA problem, surpassing state-of-the-art methods by large margins on two VUDA benchmarks.
In summary, our contributions are threefold. (i) We pro-pose a novel SSA2lign to address FSVDA at the snippet level by both statistical and semantic alignments that are achieved from three perspectives. (ii) We propose to aug-ment target domain data and the snippet-level alignments by a simple yet effective stochastic sampling of snippets for more robust video domain alignment. (iii) Extensive exper-iments show the efficacy of SSA2lign, achieving a remark-able average improvement of 13.1% and 4.2% over current state-of-the-art FSDA/FSVDA methods on two large-scale cross-domain action recognition benchmarks. 2.