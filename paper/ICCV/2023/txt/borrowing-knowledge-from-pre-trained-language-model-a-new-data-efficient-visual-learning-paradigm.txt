Abstract
The development of vision models for real-world ap-plications is hindered by the challenge of annotated data scarcity, which has necessitated the adoption of data-efficient visual learning techniques such as semi-supervised learning. Unfortunately, the prevalent cross-entropy super-vision is limited by its focus on category discrimination while disregarding the semantic connection between con-cepts, which ultimately results in the suboptimal exploita-tion of scarce labeled data. To address this issue, this paper presents a novel approach that seeks to leverage linguis-tic knowledge for data-efficient visual learning. The pro-posed approach, BorLan, Borrows knowledge from off-the-shelf pretrained Language models that are already endowed with rich semantics extracted from large corpora, to com-pensate the semantic deficiency due to limited annotation in visual training. Specifically, we design a distribution align-ment objective, which guides the vision model to learn both semantic-aware and domain-agnostic representations for the task through linguistic knowledge. One significant ad-vantage of this paradigm is its flexibility in combining vari-ous visual and linguistic models. Extensive experiments on semi-supervised learning, single domain generalization and few-shot learning validate its effectiveness. Code is avail-able at https://github.com/BIT-DA/BorLan. 1.

Introduction
The tremendous accomplishment of deep learning in computer vision is mostly supported by large-scale labeled datasets [13, 46]. Nevertheless, in real-world scenarios, the acquisition of extensive labeled data through manual anno-tation for each specific task is a time-consuming and labor-exhaustive endeavor [11, 70]. As such, the development of data-efficient learning methods has become an imperative
‡Corresponding author.
Figure 1: Illustration of BorLan. In both domains of lan-guage and vision, we can easily have access to various off-the-shelf models that are pretrained on large datasets in their respective modalities. This paper proposes a data-efficient visual learning paradigm (black arrows), aiming to im-prove various vision models on challenging data-scarce vi-sion tasks by borrowing linguistic knowledge from frozen pretrained language models. In this way, we successfully leverage the rich semantics embedded in language modality to enhance data-efficiency in visual learning. research direction aimed at enhancing the feasibility and practicality of deep neural networks [53, 61].
To mitigate the requirement for labeled data, techniques leveraging supplementary visual knowledge have been ex-tensively investigated in vision community. For instance, transfer learning [63, 60, 27, 64] employs models pre-trained on a large image dataset as the initialization, semi-supervised learning [26, 3, 44] exploits unlabeled data via self-training, and out-of-domain generalization [68, 58, 55] incorporates visual prior knowledge in the training using methods such as data augmentation [12]. However, their commonly adopted cross-entropy supervision mainly em-phasizes category discrimination, while overlooking the se-mantic relevance between visual concepts. As a result, the learned image feature space may become distorted [25], and the inter-class relationships inferred by the model can be-come ambiguous, as shown in Fig 5. This observation mo-tivates us to explore an additional form of supervision that can capture semantic information from image annotations
prior to their conversion into one-hot labels.
In this paper, we propose a novel approach to address the challenge of annotated data scarcity in vision tasks by lever-aging off-the-shelf pretrained language models (PLMs), such as BERT [21] and GPT [41], to provide explicit se-mantic guidance that is generalizable to various data-scarce scenarios. PLMs are known to possess semanti-cally rich embedding spaces, since they are pretrained on large corpora. Therefore, we borrow the general linguistic knowledge embedded within these models to enhance the data-efficiency of visual learning.
Particularly, vision models will benefit from two merits that the text embedding space of PLM possesses: (1) the semantic relationship between concepts could be reflected through text embedding similarities, i.e., concept “cat” is more similar to “tiger” than “airplane”; (2) the concepts expressed in language are more domain-agnostic, which means they are less affected by styles of varying visual do-mains, i.e., description “a photo of a cat” can be applied indiscriminately to cats in different kinds of environments.
Therefore, by aligning image feature space towards the text embedding space, vision model can learn semantic relation-ships between concepts and domain-invariant knowledge for the given task.
More specifically, we combine a set of predetermined prompts with task-specific concepts to create the input sen-tences, and obtain text embeddings through the PLM. To capture all possible variants of each concept, we estimate the text embedding distribution of each concept using the generated embeddings. Finally, a distribution-aware knowl-edge transfer objective is optimized in its upper bound form to guide the vision model align its image representations with the text distribution. The framework is shown in Fig. 1.
Recently, motivated by the strong feature transferability and open-set recognition ability of the pretrained vision-language models (VLM) like CLIP [40] and ALIGN [20], a series of subsequent works adopt VLM to improve few-shot learning performance on data-scarce tasks [72, 71, 33, 15, 69, 19]. These VLM-based tuning methods such as
CoOp [72] and Tip-Adapter [69] inherit and leverage the vision-language semantic connection established through joint pre-training on massive image-text pairs to efficiently adapt the model to specific tasks with few labeled samples.
Different from them, our framework is designed to be more flexible, enabling knowledge transfer between various inde-pendently pretrained vision and language models and is also applicable on jointly pre-trained vision-language models. learning scenarios:
We evaluate our method in three representative data-efficient semi-supervised learning (SSL), single domain generalization (SDG), and few-shot learning (FSL). All scenarios pose serious challenges to vi-sion models as they need to capture the high-level seman-tics within the training data instead of merely memorizing them. We empirically validate that our method consistently improves the performances of data-efficient training on a variety of benchmarks for these tasks, and we demonstrate that our method can promote vision models of different ar-chitectures and sizes, ranging from ResNet-50 [17] to Swin-Base [30], with the guidance knowledge obtained from a various choice of PLMs like BERT [21] and GPT [41].
We summarize our contributions in this work as follows:
• We present a novel data-efficient visual learning paradigm, named BorLan, that borrows lingnguistic knowledge from PLMs for explicit semantic guidance and as a complement to scarce visual data.
• We propose text embedding distribution-aware objec-tive, enabling flexible combination of various indepen-dently or jointly pretrained vision and language mod-els, and full parameter fine-tuning on specific visual tasks for better adaptation performance.
• Extensive experiments on three scenarios and various benchmarks are conducted to thoroughly validate our method and gain empirical insights. 2.