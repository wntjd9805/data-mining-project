Abstract
Data mixing strategies (e.g., CutMix) have shown the ability to greatly improve the performance of convolutional neural networks (CNNs). They mix two images as inputs for training and assign them with a mixed label with the same ratio. While they are shown effective for vision transform-ers (ViTs), we identify a token fluctuation phenomenon that has suppressed the potential of data mixing strategies. We empirically observe that the contributions of input tokens fluctuate as forward propagating, which might induce a dif-ferent mixing ratio in the output tokens. The training target computed by the original data mixing strategy can thus be inaccurate, resulting in less effective training. To address this, we propose a token-label alignment (TL-Align) method to trace the correspondence between transformed tokens and the original tokens to maintain a label for each to-ken. We reuse the computed attention at each layer for effi-cient token-label alignment, introducing only negligible ad-ditional training costs. Extensive experiments demonstrate that our method improves the performance of ViTs on image classification, semantic segmentation, objective detection, and transfer learning tasks. Code is available at: https:
//github.com/Euphoria16/TL-Align. 1.

Introduction
The recent developments of vision transformers (ViTs) have revolutionized the computer vision field and set new state-of-the-arts in various tasks, such as image classifica-tion [16, 43, 31, 10], object detection [5, 56, 12, 13], and semantic segmentation [28, 39, 54, 9]. The successful struc-ture of alternative spatial mixing and channel mixing in
ViTs also motivates the arising of high-performance MLP-like deep architectures [40, 41, 42] and promotes the evo-lution of better CNNs [15, 32, 17]. In addition to architec-*Equal contribution.
â€ Corresponding author. ture designs, an improved training strategy can also greatly boost the performance of a trained deep model [23, 44, 8, 7].
The training of modern deep architecture almost all adopts data mixing strategies for data augmentation [48, 45, 24, 47, 52, 53], which have been proven to consistently im-prove the generalization performance. They randomly mix two images and their labels with the same mixing ratio to produce mixed data. As the most commonly used data mix-ing strategy, CutMix [52] performs a copy-and-paste oper-ation on the spatial domain to produce spatially mixed im-ages. While data mixing strategies have been widely studied for CNNs [48, 45, 24], few works have explored their com-patibilities with ViTs [7]. We find that self-attention in ViTs causes a fluctuation of the original spatial structure. Unlike the translation equivalence that ensures a global label con-sistency for CNNs, self-attention in ViTs undermines this global consistency and causes a misalignment between the token and label. This misalignment induces a different mix-ing ratio in the output tokens. The training targets computed by the original data mixing strategies can then be inaccu-rate, resulting in less effective training.
To address this, we propose a token-label alignment (TL-Align) method for ViTs to obtain a more accurate target for training. We present an overview of our method in Figure 1.
We first assign a label to each input token in the mixed im-age according to the source of the token. We then trace the correspondence between the input tokens and the trans-formed tokens and align the labels accordingly. We assume that only the spatial self-attention and residual connection operation alter the presence of input tokens since channel
MLP and layer normalization process each token indepen-dently. We reuse the computed attentions to linearly mix the labels of input tokens to obtain those of transformed tokens.
The token-label alignment is performed iteratively to obtain a label for each output token. For class-token-based classi-fication (e.g., ViT [16] and DeiT [43]), we directly use the aligned label for the output class token as the training target.
For global-pooling-based classification (e.g., Swin [31]), we similarly average the labels of output tokens as the train-Figure 1. An overview of the proposed TL-Align. (a) CutMix-like methods [52] are widely used in model training, which spatially mix the tokens and their labels in the input space. (b) They are originally designed for CNNs and assume the processed tokens are spatially aligned with the input tokens. We show that it does not hold true for ViTs due to the global receptive field and adaptive weights. (c) Compared with existing methods, our method can effectively and efficiently align the tokens and labels without requiring a pretrained teacher network. ing target. The proposed TL-Align is only used for train-ing to improve performance and introduces no additional workload for inference. We apply the proposed TL-Align to various ViT variants with CutMix including plain ViTs (DeiT [43]) and hierarchical ViTs (Swin [31]). We observe a consistent performance boost across different models on
ImageNet-1K [14]. Specifically, our TL-Align improves
DeiT-S by 0.8% using the same training recipe. We evalu-ate the ImageNet-pretrained models on various downstream tasks including semantic segmentation, objection detection, and transfer learning. Experimental results also verify the robustness and generalization ability of our method. 2.