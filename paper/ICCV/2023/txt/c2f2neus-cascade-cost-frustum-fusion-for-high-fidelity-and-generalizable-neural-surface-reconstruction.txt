Abstract
There is an emerging effort to combine the two popular 3D frameworks using Multi-View Stereo (MVS) and Neural
Implicit Surfaces (NIS) with a specific focus on the few-shot
/ sparse view setting. In this paper, we introduce a novel in-tegration scheme that combines the multi-view stereo with neural signed distance function representations, which po-tentially overcomes the limitations of both methods. MVS uses per-view depth estimation and cross-view fusion to generate accurate surfaces, while NIS relies on a common coordinate volume. Based on this strategy, we propose to construct per-view cost frustum for finer geometry estima-tion, and then fuse cross-view frustums and estimate the im-plicit signed distance functions to tackle artifacts that are due to noise and holes in the produced surface reconstruc-tion. We further apply a cascade frustum fusion strategy to effectively captures global-local information and struc-tural consistency. Finally, we apply cascade sampling and a pseudo-geometric loss to foster stronger integration be-tween the two architectures. Extensive experiments demon-strate that our method reconstructs robust surfaces and out-performs existing state-of-the-art methods. 1.

Introduction
Reconstructing 3D structures from a set of images is a fundamental task in computer vision, with widespread applications in fields such as architectural preservation, virtual/augmented reality, and digital twins. Multi-view stereo (MVS) is a widely-used technique for addressing this task, exemplified by MVSNet [52] and its succes-sors [51, 42, 43, 38, 49]. These methods construct 3D cost volumes based on the camera frustum, rather than regular euclidean space, to achieve precise depth map estimation.
However, these methods typically require post-processing steps, such as depth map filtering, fusion, and mesh recon-*Corresponding author. Contact him at yuesongwang@hust.edu.cn
Figure 1. A regular volume doesn’t simultaneously fit all cameras well, and can easily run into a dilemma when choosing the resolu-tion. In a low-resolution volume, a single voxel may cover multi-ple pixels of an image, resulting in blurred volume features, such as the blue pixel in view B. In a high-resolution volume, multiple voxels may cover only a single pixel of an image, causing con-fusion of different voxels, such as the pink-purple pixels in view
A. Instead, frustum volume is view-dependent and extracts pixel-level image features. Hence, we build the cost frustum for each view and adopt the fusion of per-view frustum so as to fit each view. struction, to reconstruct the 3D surface of the scene, and can not well handle noises, textureless regions, and holes.
The implicit scene representation approaches, e.g., Neu-ral Radiance Fields (NeRF) [27] and its peer Neural Signed
Distance Function [36, 35, 39], achieves remarkable results in view synthesis and scene reconstruction. The implicit surface reconstruction approaches typically employ Multi-layer Perceptrons (MLPs) to implicitly fit a volume field.
We then can extract scene geometry and render views from the implicit volume field. These approaches usually require a large number of images from different viewpoints and adopt a per-scene optimization strategy, which means they are not generalizable to unknown scenes.
There is an emerging effort [4, 3, 59, 16] to merge the two technical paths. MVSNeRF [4] combines NeRF [27] with MVSNet [52] for generalizable view synthesis. RC-MVSNet [3] utilizes NeRF’s neural volume rendering to
handle view-dependent effects and occlusions. The most related to our approach is the SparseNeUS [23] for gen-eralizable surface reconstruction method for sparse views.
It builds a regular euclidean volume (i.e., cube) to encode geometric information by aggregating 2D feature maps of multiple images. The features sampled from it and the cor-responding positions are used to estimate the signed dis-tance function (SDF). However, a regular volume doesn’t fit a camera’s view naturally, which can be better modeled as view frustum. More specifically, as illustrated in Fig. 1, a volume with a higher resolution costs more memory and collect redundant image features, while a coarser volume causes quality degradation. Instead, we propose to build the cost frustum for each view and this strategy has been proven to be effective on MVSNet [52] and its successors.
In this paper, we propose a novel integration scheme that combines MVS with neural implicit surface reconstruction.
To encode the global and local geometric information of the scene, we adopt the cascade architecture of CasMVS-Net [13], which is a volume pyramid. Specifically, we first construct a volume on the camera frustum and then convert it into a cascade geometric frustum. As shown in Fig. 1, to fit each camera’s view well, we build a cascade frustum for every view and then fuse them using a proposed cross-view and cross-level fusion strategy that effectively cap-tures global-local information and structural consistency.
By combining the 3D position, fused feature, and view di-rection, we estimate the SDF and render colors using vol-ume rendering [39]. Moreover, we utilize the intermediate information output by MVS part to apply cascade sampling and a pseudo-geometric loss, which further improves the quality of the reconstructed surface. Our experiments on the DTU [1] and BlendedMVS [53] datasets demonstrate the effectiveness and generalization ability of our proposed method, surpassing existing state-of-the-art generalization surface reconstruction techniques.
Our approach makes the following contributions:
• We introduce a novel exploration approach that in-tegrates MVS and implicit surface reconstruction ar-chitectures for end-to-end generalizable surface recon-struction from sparse views.
• We propose a cross-view and cross-level fusion strat-egy to effectively fuse features from multiple views and levels.
• We further utilize information from the MVS part to apply cascade sampling and a pseudo-geometric loss to the neural surface part, promoting better integration between the two architectures. 2.