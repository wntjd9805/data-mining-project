Abstract
Vision Transformers (ViTs) have achieved state-of-the-art performance on various computer vision applications.
However, these models have considerable storage and com-putational overheads, making their deployment and efficient inference on edge devices challenging. Quantization is a promising approach to reducing model complexity, and the dyadic arithmetic pipeline can allow the quantized models to perform efficient integer-only inference. Unfortunately, dyadic arithmetic is based on the homogeneity condition in convolutional neural networks, which is not applicable to the non-linear components in ViTs, making integer-only in-ference of ViTs an open issue. In this paper, we propose I-ViT, an integer-only quantization scheme for ViTs, to enable
ViTs to perform the entire computational graph of inference with integer arithmetic and bit-shifting, and without any floating-point arithmetic. In I-ViT, linear operations (e.g.,
MatMul and Dense) follow the integer-only pipeline with dyadic arithmetic, and non-linear operations (e.g., Softmax,
GELU, and LayerNorm) are approximated by the proposed light-weight integer-only arithmetic methods. More specif-ically, I-ViT applies the proposed Shiftmax and ShiftGELU, which are designed to use integer bit-shifting to approxi-mate the corresponding floating-point operations. We eval-uate I-ViT on various benchmark models and the results show that integer-only INT8 quantization achieves compa-rable (or even slightly higher) accuracy to the full-precision (FP) baseline. Furthermore, we utilize TVM for practi-cal hardware deployment on the GPU’s integer arithmetic units, achieving 3.72∼4.11× inference speedup compared to the FP model. Code of both Pytorch and TVM is released at https://github.com/zkkli/I-ViT. 1.

Introduction
Vision Transformers (ViTs) have recently achieved great success on a variety of computer vision tasks [13, 10, 4].
*Corresponding author. (a) FasterTransformer [34] (b) I-ViT (ours)
Figure 1. Computation flows of Softmax, GELU, and LayerNorm in FasterTransformer [34] and our proposed I-ViT. I-ViT realizes the entire computational graph with integer-only arithmetic, which is more promising and practical for low-cost model deployment and efficient inference.
Nevertheless, as compared to convolutional neural net-works (CNNs), ViTs suffer from higher memory footprints, computational overheads, and power consumption, hinder-ing their deployment and real-time inference on resource-constrained edge devices [25, 17, 15, 37]. Thus, compres-sion approaches for ViTs are being widely researched.
Model quantization, which reduces the representation precision of weight/activation parameters, is an effective and hardware-friendly way to improve model efficiency
[12, 20, 7, 35]. With the quantized low-precision param-eters, previous work [18] presents the dyadic arithmetic pipeline to realize integer-only inference, where the quan-tization scaling factors are collapsed into the integer multi-plication and bit-shifting in the requantization process. This can enable the quantized models to fully benefit from the fast and efficient low-precision integer arithmetic units and thus provides promising speedup effects [41, 44]. For in-stance, the edge processor core in ARM Cortex-M family only support the deployment of the integer-only kernels; the recent Turing Tensor Cores in GPU server class also add support for integer logical units, and their high through-put capability enables notably lower latency compared to floating-point arithmetic.
However, the above integer-only pipeline is designed for
CNNs and works under the homogeneity condition, mak-ing it only applicable to linear (e.g., Dense) or piecewise linear (e.g., ReLU) operations [18, 44]. Therefore, the non-linear operations (e.g., Softmax, GELU, and LayerNorm) in ViTs cannot naively follow it. To cope with this prob-lem, a brute-force scheme is to simply leave the non-linear operations as dequantized floating-point arithmetic, such as FasterTransformer [34] shown in Figure 1(a). Unfor-tunately, this scheme makes them tolerate the inefficiency of floating-point arithmetic units, and the cut of the com-putational graph also introduces communication costs be-tween integer and floating-point units, which severely lim-its the speedup of inference. In addition, low-cost integer-only hardware cannot meet mixed-precision computing re-quirements, hence one has to design heterogeneous chips by adding floating-point arithmetic units, which definitely increases the budget for model deployment.
Consequently, integer-only arithmetic for non-linear op-erations is significant for low-cost deployment and efficient inference. To this end, several works have attempted on language Transformer models. Fully-8bit [29] employs L1
LayerNorm to replace the non-linear arithmetic of standard deviation, and I-BERT [19] proposes integer polynomial ap-proximations for the non-linear operations. However, such approaches are inefficient and fail to fully exploit the ben-efits of hardware logic. Moreover, they are developed for language models, making it infeasible to properly transfer to ViTs due to differences in data distribution. For ViTs,
FQ-ViT [30] preliminarily explores the feasibility of inte-ger arithmetic for part of the operations (e.g., Softmax), but it is simply built on I-BERT [19] and ignores the notable
GELU operation, leaving a huge gap between it and integer-only inference. As a result, how to accurately perform the non-linear operations of ViTs with efficient integer-only arithmetic remains an open issue.
In this paper, we propose I-ViT, which quantizes the en-tire computational graph to fill the research gap of integer-only quantization for ViTs. Specifically, linear operations follow the dyadic arithmetic pipeline; and non-linear op-erations are approximated without accuracy drop by novel light-weight integer-only arithmetic methods, where Shift-max and ShiftGELU perform most arithmetic with bit-shifting that can be efficiently executed with simple shifters in hardware logic [39], and I-LayerNorm calculates the square root with integer iterations instead.
The main contributions are summarized as follows:
• We propose I-ViT, which fully quantizes the compu-tational graph of ViTs and allows performing the en-tire inference with integer arithmetic and bit-shifting, without any floating-point operations. To the best of our knowledge, this is the first work on integer-only quantization for ViTs.
Figure 2. Accuracy-speed curves of I-ViT and the FP baseline on
DeiT [38] and Swin [31]. Accuracy is evaluated on ImageNet dataset, and speed is obtained from the latency on an RTX 2080Ti
GPU (batch=8). As we can see, I-ViT provides significant accel-erations (3.72∼4.11×) while achieving similar (or even slightly higher) accuracy.
• We propose novel light-weight integer approximations for non-linear operations (as shown in Figure 1(b)), in particular, Shiftmax and ShiftGELU use integer bit-shifting to accomplish most arithmetic, which fully benefit from the efficient hardware logic.
• I-ViT is evaluated on various models for the large-scale classification task, achieving compression with similar (or even slightly higher) accuracy. Moreover, we de-ploy I-ViT on an RTX 2080Ti GPU using TVM1 [6], which accelerates the integer-only inference of ViTs with Turing Tensor Cores, achieving a 3.72∼4.11× speedup over the FP model (as shown in Figure 2). 2.