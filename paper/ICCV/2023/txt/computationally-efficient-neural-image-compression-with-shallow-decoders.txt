Abstract
Neural image compression methods have seen increas-ingly strong performance in recent years. However, they suffer orders of magnitude higher computational complex-ity compared to traditional codecs, which hinders their real-world deployment. This paper takes a step forward in closing this gap in decoding complexity by adopting shallow or even linear decoding transforms. To compen-sate for the resulting drop in compression performance, we exploit the often asymmetrical computation budget be-tween encoding and decoding, by adopting more power-ful encoder networks and iterative encoding. We theoret-ically formalize the intuition behind, and our experimen-tal results establish a new frontier in the trade-off between rate-distortion and decoding complexity for neural image compression. Specifically, we achieve rate-distortion per-formance competitive with the established mean-scale hy-perprior architecture of Minnen et al. (2018) at less than 50K decoding FLOPs/pixel, reducing the baseline’s overall decoding complexity by 80%, or over 90% for the synthe-sis transform alone. Our code can be found at https:
//github.com/mandt-lab/shallow-ntc. 1.

Introduction
Deep-learning-based methods for data compression [51] have achieved increasingly strong performance on visual data compression, increasingly exceeding classical codecs in rate-distortion performance. However, their enormous computational complexity compared to classical codecs, es-pecially required for decoding, is a roadblock towards their wider adoption [34, 37]. In this work, inspired by the parallel between nonlinear transform coding and traditional trans-form coding [17], we replace deep convolutional decoders with extremely lightweight and shallow (and even linear) decoding transforms, and establish the R-D (rate-distortion) performance of neural image compression when operating at the lower limit of decoding complexity.
More concretely, our contributions are as follows:
Figure 1. R-D performance on Kodak v.s. decoding computation complexity as measured in KMACs (thousand multiply-accumulate operations) per pixel. The circle radius corresponds to the parame-ter count of the synthesis transform in each method (see Table. 1)
• We offer new insight into the image manifold param-eterized by learned synthesis transforms in nonlinear transform coding. Our results suggest that the learned manifold is relatively flat and preserves linear com-binations in the latent space, in contrast to its highly nonlinear counterpart in generative modeling [11].
• Inspired by the parallel between neural image compres-sion and traditional transform coding, we study the effect of linear synthesis transform within a hyperprior architecture. We show that, perhaps surprisingly, a
JPEG-like synthesis can perform similarly to a deep lin-ear CNN, and we shed light on the role of nonlinearity in the perceptual quality of neural image compression.
• We give a theoretical analysis of the R-D cost of neu-ral lossy compression in an asymptotic setting, which quantifies the performance implications of varying the complexity of encoding and decoding procedures.
• We equip our JPEG-like synthesis with powerful en-coding methods, and augment it with a single hidden layer. This simple approach yields a new state-of-the-art result in the trade-off between R-D performance and decoding complexity for nonlinear transform coding, in the regime of sub-50K FLOPs per pixel believed to be dominated by classical codecs. 2.