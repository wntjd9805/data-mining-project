Abstract
Generative models such as StyleGAN2 and Stable Dif-fusion have achieved state-of-the-art performance in com-puter vision tasks such as image synthesis, inpainting, and de-noising. However, current generative models for face inpainting often fail to preserve fine facial details and the identity of the person, despite creating aesthetically convinc-ing image structures and textures. In this work, we propose
Person Aware Tuning (PAT) of Mask-Aware Transformer (MAT) for face inpainting, which addresses this issue. Our proposed method, PATMAT 1, effectively preserves identity by incorporating reference images of a subject and fine-tuning a MAT architecture trained on faces. By using ∼ 40 reference images, PATMAT creates anchor points in MAT’s style module, and tunes the model using the fixed anchors to 1All experiments and data processing activities were conducted at
Carnegie Mellon University. adapt the model to a new face identity. Moreover, PATMAT’s use of multiple images per anchor during training allows the model to use fewer reference images than competing methods. We demonstrate that PATMAT outperforms state-of-the-art models in terms of image quality, the preservation of person-specific details, and the identity of the subject. Our results suggest that PATMAT can be a promising approach for improving the quality of personalized face inpainting 2. 1.

Introduction
The objective of image inpainting is to generate plausi-ble content to complete missing regions within an image.
Preserving contextual integrity of the inpainted image is a crucial factor, where the reconstructed regions must conform 2The code will humansensinglab/PATMAT. out to sam(dot)motamed(at)insait(dot)ai. at https://github.com/
For any questions, please reach available be
to reasonable structure and texture based on local and non-local priors in the image. This task becomes increasingly challenging when addressing larger and irregular missing regions in the image. In particular, facial inpainting poses a significant challenge as it requires maintaining fine facial details and the subject’s identity, which are essential for var-ious applications including security (e.g., inpainting a face behind a mask or sunglasses), entertainment (e.g., seeing a person while they are wearing a virtual reality headset), or photo restoration. In each of these tasks, it is essential to keep the subject’s identity and fine facial details intact.
However, recent inpainting methods have largely focused on generating high-quality images with little attention given to preserving the identity of the subject. Therefore, we explore the feasibility of inpainting techniques that aim to maintain the subject’s identity and facial characteristics while generat-ing high-quality, photo-realistic images.
Recent works have pushed the boundaries of large-hole image inpainting [17, 36, 35, 23]. Mask-Aware Transformer (MAT) [17] used vision transformers, with a multi-head con-textual attention mechanism with shifting windows [20] to build long-range dependency priors and achieved great in-painting results compared to competing methods such as
CoModGAN [43], ICT [36] and LaMa [35]. While these models are able to synthesize high quality inpainted images, the reconstruction result sometimes fails to recover the de-tails of the ground truth. Particularly, in the case of face inpainting, existing models (e.g., MAT, CoModGAN, Re-Paint) cannot recover the same subtle facial details as the ground truth (e.g., brows shape, eye look, beard shape,...).
As a result, they do not preserve the subject’s identity af-ter inpainting (see Fig. 1). For tasks such as image editing and restoration, especially in the domain of faces, identity preservation is a requirement, yet the problem of identity preserving image inpainting is relatively unexplored.
This work proposes PATMAT for personalized face in-painting. Given a few reference images of a person, PAT-MAT integrates this information into a pre-trained MAT model by tuning the network parameters, conditioned on the style vectors within MAT. We compare two style condi-tioning methods and propose a regularization loss to prevent over-fitting to the reference images during the tuning pro-cess. To evaluate our method, we curated images of seven public figures due to the lack of diverse and sufficiently large datasets (no public datasets have sufficient number of im-ages per identity such as CelebA-HQ [12] and VFHQ [39]).
Our qualitative and quantitative experiments demonstrate that PATMAT outperforms several contemporary state-of-the-art inpainting models in terms of quality and identity-preservation.
Our contributions are: 1. We propose PATMAT, a tuning method based on creat-ing anchors in MAT’s style space that allows for high quality personalized inpainting of the face. 2. We propose a regularization method that controls over-fitting during tuning and further improves the quality of personalized inpainting. 3. In order to make our personalized method more practi-cal, our effort reduces the number of required reference images from ∼ 100 − 200 in previous works [25] to
∼ 40 images. 2.