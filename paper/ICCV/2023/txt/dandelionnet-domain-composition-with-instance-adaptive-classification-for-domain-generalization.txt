Abstract
Domain generalization (DG) attempts to learn a model on source domains that can well generalize to unseen but different domains. The multiple source domains are innately different in distribution but intrinsically related to each other, e.g., from the same label space. To achieve a generalizable feature, most existing methods attempt to reduce the domain discrepancy by either learning domain-invariant feature, or additionally mining domain-specific feature. In the space of these features, the multiple source domains are either tightly aligned or not aligned at all, which both cannot fully take the advantage of complementary information from multiple domains. In order to preserve more complementary informa-tion from multiple domains at the meantime of reducing their domain gap, we propose that the multiple domains should not be tightly aligned but composite together, where all do-mains are pulled closer but still preserve their individuality respectively. This is achieved by using instance-adaptive classifier specified for each instanceâ€™s classification, where the instance-adaptive classifier is slightly deviated from a universal classifier shared by samples from all domains. This adaptive classifier deviation allows all instances from the same category but different domains to be dispersed around the class center rather than squeezed tightly, leading to bet-ter generalization for unseen domain samples. In result, the multiple domains are harmoniously composite centered on a universal core, like a dandelion, so this work is referred to as DandelionNet. Experiments on multiple DG benchmarks demonstrate that the proposed method can learn a model with better generalization and experiments on source free domain adaption also indicate the versatility. 1.

Introduction
In recent decades, the deep learning based methods achieves significant progress in a few fields, including object
Figure 1: The importance of the design of the proposed method. Conventional common feature constraint DG meth-ods may result in feature space twist as in (a). While an ideal well generalizable feature space should make different domains pulled closer but still preserve their individuality respectively, i.e., slightly deviated from the universal center (the green star) as illustrated in (b). detection, face recognition, natural language process, etc.
These methods mostly base on independent and identically distributed (i.i.d.) assumption. However, in real world, those unseen samples or domains usually lie in different distribu-tions from that of the training set. In such situation, the learnt model would perform poorly on those unseen samples.
The technique of domain generalization (DG) aims to deal with the fore-mentioned problem by learning a robust model that can generalize to unseen domains. In domain generalization, the source domains and unseen domains are assumed to share the same categories and domain labels are known, while the unseen domains are unavailable during training stage.
Previous DG methods mainly do domain invariant fea-ture learning between multiple domains with domain align-ment constraints [46, 18, 21, 45, 41, 40, 16, 24], where the
domain invariant features are expected to be shared among source domains as well as unseen domains. However, these methods only emphasize the common information between domains, but neglect those domain specific information, which may restrict their discriminative ability. Besides, the domain invariant feature cannot be perfectly obtained by just domain alignment constraint, so the domains would be squeezed together and even twisted severely, which leads to poor generalization on unseen samples, as shown in Fig-ure 1. To recall more beneficial features, domain-specific feature exploration methods as in [11, 47, 7, 42] are pro-posed to preserve those domain specific feature by addi-tionally disentangling it from the domain invariant feature.
These approaches obtain more informative features leading to better performance. However, in these methods, the two parts of features are just considered in two different man-ners, i.e., the domain invariant features are tightly aligned as in those domain-invariant feature learning methods, while those domain specific features are free of any consideration of domain commonality, which is similar with freely learning domain specific models and may be with obvious domain dis-crepancy. But over-introduction of domain specific features may also lead to incorrect classification of unseen samples especially those differing largely from the source domains.
To get a better generalizable mode that can sufficiently exploit the complementary information among domains, in this work, we propose a method that neither tightly aligns the domains which would result in twisted mapping and loss of information, nor naturally considers them separately to preserve the favorable domain specific feature, but organizes the multiple domains together where all domains are pulled closer but still preserve their individuality respectively. To ensure that all domains are composite together rather than squeezed, each sample is classified via an instance adaptive classifier, which is slightly deviated from the universal clas-sifier. This adaptive classifier deviation allows all instances of the same category but with different variations including intra-class and domain-related ones to be dispersed around the class centers rather than roughly squeezed together, lead-ing to better generalization for unseen samples.
Briefly, the main contributions of this work lie in: 1. We propose a new method that integrates rather than tightly aligns the multiple domains for domain general-ization. Specifically, the source domains are composite together by allowing each instance being classified by instance-adaptive classifier. The instance-adaptive clas-sifier is slightly deviated from the universal classifier, in order to pull all domains closer but still preserve their individuality respectively. 2. To prevent the learnt instance-adaptive classifier devia-tion from being too large which has negative influence on correct classification, the deviation norm scaling hyper-parameter is introduced to help control the shift-ing degree. 3. The experimental analyses verify the effectiveness of this method. Besides, they also demonstrate that our predicted classifier deviation keeps category semantic and domain relation information, showing the rational-ity of the method and results. 2.