Abstract
Network calibration aims to accurately estimate the level of confidences, which is particularly important for employ-ing deep neural networks in real-world systems. Recent ap-proaches leverage mixup to calibrate the network’s predic-tions during training. However, they do not consider the problem that mixtures of labels in mixup may not accurately represent the actual distribution of augmented samples. In this paper, we present RankMixup, a novel mixup-based framework alleviating the problem of the mixture of labels for network calibration. To this end, we propose to use an ordinal ranking relationship between raw and mixup-augmented samples as an alternative supervisory signal to the label mixtures for network calibration. We hypothesize that the network should estimate a higher level of confi-dence for the raw samples than the augmented ones (Fig.1).
To implement this idea, we introduce a mixup-based rank-ing loss (MRL) that encourages lower confidences for aug-mented samples compared to raw ones, maintaining the ranking relationship. We also propose to leverage the rank-ing relationship among multiple mixup-augmented samples to further improve the calibration capability. Augmented samples with larger mixing coefficients are expected to have higher confidences and vice versa (Fig.1). That is, the order of confidences should be aligned with that of mixing coeffi-cients. To this end, we introduce a novel loss, M-NDCG, in order to reduce the number of misaligned pairs of the coef-ficients and confidences. Extensive experimental results on standard benchmarks for network calibration demonstrate the effectiveness of RankMixup. 1.

Introduction
Deep neural networks (DNNs) have achieved remarkable advances in numerous computer vision tasks, including im-age classification, object detection, and semantic segmen-tation. Despite the impressive performance, DNNs trained using a softmax cross-entropy (CE) loss and one-hot en-*Corresponding author.
Figure 1: Motivation of RankMixup. We show raw samples of a cat and dog in red boxes, and mixup-augmented samples in blue boxes with dashed lines. The mixup-augmented samples are gen-erated by linearly interpolating the raw samples with a mixing co-efficient λ. We expect that confidences of the raw samples to be higher than that of the augmented sample (top). The augmented samples with larger coefficients λ would have higher confidences than the samples with smaller coefficients (bottom). Best viewed in color. coded labels often output overconfident and/or undercon-fident probabilities, which do not reflect the actual likeli-hood [10, 32, 41]. This suggests that the networks are not able to estimate the level of confidence, leading to inac-curate and unreliable predictions, which restricts practical applications in real-world systems, particularly for safety-sensitive domains, such as medical diagnosis [18] and au-tonomous driving [9].
To improve the reliability of DNNs, many methods pro-pose to calibrate the output distribution after or during train-ing. Post-hoc approaches adjust miscalibrated probabilities directly from pretrained networks [7, 10, 12, 21, 27]. Al-though these approaches are straightforward to apply, cali-bration performance depends heavily on datasets and net-works used for training [25], and they are not robust to distribution shifts between training and test samples [33].
Training-time approaches [4, 8, 14, 25, 28, 29, 30, 41, 51], on the other hand, exploit an additional regularization term along with the CE loss to calibrate the output probabilities
during training. Early works [29, 30, 41] have explored the effects of regularization techniques, including label smooth-ing (LS) [40], focal loss (FL) [24], and mixup [49], on net-work calibration, and shown that the networks trained with these techniques are better calibrated than the ones trained with the CE loss alone. Recently, the works of [8, 25, 29] highlight the limitations of LS and FL in terms of network calibration, and introduce calibration-aware LS and FL.
On the contrary, few studies have explored integrating the mixup technique into network calibration. Current mixup-based methods [35, 41, 51] simply interpolate input images and training labels with a mixing coefficient to soften out-put probabilities, while not considering network calibration itself. Since the interpolated labels may not represent ac-curately the distribution of the label mixture in augmented images [3, 19, 39, 43], training networks with such uncer-tain labels degrades the calibration performance.
In this work, we present a mixup-based approach to net-work calibration, dubbed RankMixup, that addresses the problem of directly using the mixture of labels for cali-brating network predictions [35, 41, 51]. To this end, we conjecture that networks should give higher confidences for the raw sample, not augmented by the mixup tech-nique, than the augmented one (Fig. 1). To implement this idea, we leverage an ordinal ranking relationship between raw and augmented samples (Fig. 1) and use it as a su-pervisory signal for network calibration, instead of using the label mixtures.
In particular, we introduce a mixup-based ranking loss (MRL) that encourages the confidences of augmented samples to be lower than those of raw sam-ples by a margin, in order to preserve the ranking relation-ship. To further strengthen the capability of network cali-bration, we also propose to exploit the ranking relationship for multiple mixup-augmented samples. We expect that higher confidences are favorable for augmented samples with larger mixing coefficients λ, and vice versa (Fig. 1), such that the orders of confidences and mixing coefficients are aligned to each other. To achieve this, we introduce a novel loss, M-NDCG, based on normalized discounted cu-mulative gain (NDCG) in information retrieval [17], which penalizes the number of misaligned pairs of the coefficients and confidences. Extensive experimental results on stan-dard benchmarks demonstrate the effectiveness of our ap-proach to leveraging the ordinal ranking relationships on network calibration. The main contributions of our work can be summarized as follows:
• We introduce a novel mixup-based framework for net-work calibration, RankMixup, that alleviates the problem of exploiting label mixtures for calibrating network pre-dictions. To the best of our knowledge, we are the first to handle this issue in the context of confidence calibration.
• We propose to exploit ordinal ranking relationships be-tween confidences from raw and augmented samples, as well as those among confidences from multiple aug-mented samples, and use them as supervisory signals, in-stead of the mixture of labels. To implement this idea, we introduce MRL and M-NDCG that facilitate aligning the confidences and mixing coefficients to preserve the rela-tionships.
• We demonstrate that our approach provides better re-sults in terms of expected calibration error (ECE), com-pared to other mixup-based methods. We also achieve competitive performances on standard benchmarks on network calibration, including CIFAR10/100 [20], Tiny-ImageNet [23], and ImageNet [6] with various network architectures. 2.