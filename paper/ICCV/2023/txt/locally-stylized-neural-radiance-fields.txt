Abstract
In recent years, there has been increasing interest in ap-plying stylization on 3D scenes from a reference style image, in particular onto neural radiance fields (NeRF). While per-forming stylization directly on NeRF guarantees appearance consistency over arbitrary novel views, it is a challenging problem to guide the transfer of patterns from the style image onto different parts of the NeRF scene. In this work, we pro-pose a stylization framework for NeRF based on local style transfer. In particular, we use a hash-grid encoding to learn the embedding of the appearance and geometry components, and show that the mapping defined by the hash table allows us to control the stylization to a certain extent. Stylization is then achieved by optimizing the appearance branch while keeping the geometry branch fixed. To support local style transfer, we propose a new loss function that utilizes a seg-mentation network and bipartite matching to establish region correspondences between the style image and the content images obtained from volume rendering. Our experiments show that our method yields plausible stylization results with novel view synthesis while having flexible controllability via manipulating and customizing the region correspondences. 1.

Introduction
Stylizing a visual world is an increasingly popular and demanding task in games, movies, or extended reality ap-plications. Imagine that one can navigate in an artistic vir-tual world that resembles the painting styles by different renowned artists. This problem is generally known as 3D style transfer.
Traditionally, 3D style transfer can be achieved via post-processing. For example, in the well-known traditional com-puter graphics pipeline, it typically involves a programmable shading stage to post-process the appearance of the rendered geometry or screen images. Neural radiance field [19] is a recent advance in 3D deep learning that aims to represent a 3D scene implicitly by using a neural network trained with multi-view images and differentiable volume render-Figure 1. We propose a stylization method for NeRF, aware of correspondences between different style patterns and local regions within the rendered image. ing. Since this pioneer work, significant milestones have been made to greatly improve the performance of neural radiance fields in practice, including improved spatial repre-sentation [16], training convergence [20], explicit geometry representation [30]. It is therefore promising to revisit the 3D style transfer problem by stylizing a 3D scene implicitly represented by a neural radiance field.
In this work, our goal is to transfer the appearance from a reference style image to a neural radiance field and keep the style transfer consistent across novel views rendered from the radiance field. We are inspired by works in image [2, 6] and video style transfer methods that have received great attention since the introduction of modern neural networks.
While previous works [21, 31] have proposed adapting the style transfer problem to neural radiance fields as well, the stylization results lack diversity and controllability.
To address these limitations, we devise a new style trans-fer method that considers the local transfer between a ref-erence style image and the radiance field rendering. In par-ticular, our method treats style transfer as a post-processing step after the original geometry and appearance of the neural radiance field is learned and therefore aim to perform style transfer while keeping the geometry implicitly represented
by the neural radiance field unchanged. We propose a new backbone suitable for stylizing neural radiance fields that has a dual-branch architecture to learn the density and the ap-pearance field, respectively. We devise a hash-grid encoding scheme with an extended hash function to support storing multiple styles in a single parametric embedding of the ap-pearance field. Given the same reference style image, it is possible to diversify the stylization results by customizing the hash function used for positional encoding.
We further propose a new segmentation-based stylization loss which subdivides both the 3D scene and style image into subregions; different regions in the scene are matched with a region in the style image and stylized accordingly.
The matching between scene and style image regions are for-mulated as a bipartite matching problem and solved by the
Hungarian algorithm. We show that our method automati-cally generates plausible stylization results with high-quality geometry and appearance, reflecting a diverse range of local styles found in the style image. In addition, the generated matching can also be manually edited by the user, making the stylization process controllable.
To summarize, our contributions are:
• A reference-guided style transfer method for neural radiance fields. Our architecture for learning a neural radiance field is a dual-branch network that aims at optimizing the appearance while keeping the geometry fixed during stylization;
• An extended hash-encoding scheme for stylization. We provide an analysis of hash-encoding and its influence on the style transfer on radiance fields and multiple style support;
• A new style loss that adopts optimal assignment from bipartite matching on segmented regions between the reference style image and the radiance field rendering for style transfer. 2.