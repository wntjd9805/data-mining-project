Abstract
Video deblurring aims to restore the latent video frames from their blurred counterparts. Despite the remarkable progress, most promising video deblurring methods only investigate the temporal priors in the spatial domain and rarely explore their its potential in the frequency domain.
In this paper, we revisit the blurred sequence in the Fourier space and figure out some intrinsic frequency-temporal pri-ors that imply the temporal blur degradation can be accessi-bly decoupled in the potential frequency domain. Based on these priors, we propose a novel Fourier-based frequency-temporal video deblurring solution, where the core de-sign accommodates the temporal spectrum to a popular video deblurring pipeline of feature extraction, alignment, aggregation, and optimization. Specifically, we design a
Spectrum Prior-guided Alignment module by leveraging en-larged blur information in the potential spectrum to miti-gate the blur effects on the alignment. Then, Temporal En-ergy prior-driven Aggregation is implemented to replenish the original local features by estimating the temporal spec-trum energy as the global sharpness guidance. In addition, the customized frequency loss is devised to optimize the pro-posed method for decent spectral distribution. Extensive ex-periments demonstrate that our model performs favorably against other state-of-the-art methods, thus confirming the effectiveness of frequency-temporal prior modeling. 1.

Introduction
Video deblurring, as a fundamental vision task, aims to recover the latent frame from the blurred sequence by lever-aging intrinsic temporal information. Therefore, many re-search efforts have been advocated to explore the potential of hidden temporal information in blurred sequences, which can be categorized into two groups: traditional optimization and deep learning-based methods.
*Both authors contributed equally to this research.
†Corresponding author.
Figure 1. (a) Clustering blurry and sharp frames in the spatial domain and frequency domain via t-SNE, respectively. The blurry and sharp frames in the frequency domain are separated while they are tangly in the spatial domain. (b) Unfolding the sets of the fre-quency domain in (a) along the temporal dimension. The blurred video appears to have greater temporal fluctuation than the sharp one in terms of the L2 norm of spectrums.
Traditional optimization methods often highlight the as-sumptions over the blur degradation process and apply some hand-crafted temporal priors to alleviate the video deblur-ring, e.g., temporal sharpness prior [2], motion-blurred prior
[1], and temporal coherence prior [6]. However, these pri-ors are difficult to design and the methods are also difficult to optimize, limiting their practical usage.
In recent years, we have witnessed explosive deep
(a) Blur sequence (b) Frequency spectrums of (a) (c) Sharp sequence (d) Frequency spectrum of (c)
Figure 2. Visualization comparison of several consecutive blurry frames (green) and sharp frames (blue). (a) and (c) are in the spa-tial domain, while (b) and (d) are in the frequency domain. learning-based video deblurring approaches for addressing the above challenges. Most of them follow the common pipeline: feature extraction, alignment, fusion, and opti-mization. Specifically, the pioneering EDVR [26] formu-lates an implicit alignment via the redesigned deformable convolution [42]. Instead, RTA [37] employs a gradual re-finement scheme to execute the motion compensation for more accurate temporal modeling. Despite the remarkable progress, the above strategies only investigate the temporal information from the perspective of spatial domain and have not fully explored its potential. We thus wonder “Can we provide a new solution to effectively model the temporal prior information?”.
To answer this question, we first revisit the differences between blurry-sharp pairs in the spatial and frequency do-mains respectively (see Figure 1(a)), and then unfold the frequency information of the blurry-sharp sets along the time dimension (see Figure 1(b)).
In addition, we crop a video clip and perform the Discrete Fourier Transform (DFT) to visually illustrate our motivation (see Figure 2).
On the basis of the above analyses, we can infer the follow-ing rules to encourage us to effectively explore and exploit the temporal prior information:
• (1) In terms of Figure 1(a), the feature distributions of the blurry frames and their sharp counterparts are intertwined in the spatial domain. Instead, their fre-quency feature distributions are distinguished by em-ploying DFT. Therefore, the blurred degradation can be better modeled in the frequency domain.
• (2) In terms of Figure 1(b), due to the addition of mo-tion blur, the spectral energy calculated by the L2 norm in the blurry video fluctuates more intensely than in the sharp ones temporally. Furthermore, for qualita-tive comparison, Figure 2(b) and (d) shows that the frequency spectrum of the blurry video has more obvi-ous temporal differences than the spectrum of the sharp video. In short, the frequency spectrum can enlarge the unpredictable change of temporal motion blur.
• (3) In the vertical comparison of Figure 1(b), the spec-trum norm of the sharp video is larger than the blur one. Furthermore, the high-frequency information is lost due to blur degradation, as demonstrated in Fig-ure 2(b) and (d). This phenomenon implies that the blur degradation may decay the energy spectrum of the sharp videos.
Based on the above observations, we propose a novel
Fourier-based frequency-temporal solution for video de-blurring. The key insight is to transform the blurred feature sequences into the frequency domain applying DFT, then explore and exploit the temporal sharp cues over the above pipeline of video deblurring as follows:
• Feature extraction. Based on observation (1), the blur degradation can be effectively modeled in the fre-quency domain. Therefore, we design the Spatial-frequency Feature Extraction (SFE) block by employ-ing the Fourier transform and pure convolution unit.
• Alignment. Temporal alignment aims to reduce the context difference between adjacent frames. How-ever, it often becomes not so effective over the un-predictable blur case where high-frequency details are lost severely. Based on observation (2), we propose the Spectrum Prior-guided Alignment (SPA) module by excavating the enlarged blur degradation informa-tion in the frequency spectrum to relieve the negative impact caused by motion blur in the alignment.
• Fusion. Temporal fusion is responsible for aggregat-ing clear patches from multiple frames. However, most existing methods only focus on local spatial features, which are limited by the preceding alignment results.
To address this issue, based on observations (2) and (3), we propose the Temporal Energy Attention (TEA) module, which equips the temporal spectrum energy as the global sharpness guidance to achieve complemen-tary effects with previous local spatial manners.
• Optimization. Frequency spectrum can be regarded as an indicator of global blurriness. Based on obser-vations (1) and (3), we devise the frequency spectrum loss and energy loss functions to better optimize the proposed solution in the frequency domain.
Extensive experiments are performed over multiple video deblurring tasks and validate the superiority of our proposed method. Specifically, in Figure 1(b), our solution is capable of restoring a closer spectrum distribution with ground-truth frames. 2.