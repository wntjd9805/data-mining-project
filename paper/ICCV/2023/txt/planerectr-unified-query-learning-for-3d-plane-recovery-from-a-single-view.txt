Abstract 3D plane recovery from a single image can usually be divided into several subtasks of plane detection, segmen-tation, parameter estimation and possibly depth estima-tion. Previous works tend to solve it by either extend-ing the RCNN-based segmentation network or the dense pixel embedding-based clustering framework. However, none of them tried to integrate above related subtasks into a unified framework but treated them separately and se-quentially, which we suspect is potentially a main source of performance limitation for existing approaches. Moti-vated by this finding and the success of query-based learn-ing in enriching reasoning among semantic entities, in this paper, we propose PlaneRecTR, a Transformer-based ar-chitecture, which for the first time unifies all subtasks re-lated to single-view plane recovery with a single compact model. Extensive quantitative and qualitative experiments demonstrate that our proposed unified learning achieves mutual benefits across subtasks, obtaining a new state-of-the-art performance on public ScanNet and NYUv2-Plane datasets. Codes are available at https://github. com/SJingjia/PlaneRecTR. 1.

Introduction
Immersing in a virtual 3D world involves efficient rea-soning about surrounding scenes, whose properties need to be frequently updated. Though tremendous efforts have been paid to create authentic 3D geometry from multi-view image observations or even a single image, a trade-off ex-ists between the reconstruction quality and efficiency, de-pending on the underlying scene representations [7, 21, 37, 38, 39]. 3D maps composed of sparse primitives such as point clouds are light-weight to maintain but lack topo-logical structures, while dense geometry like volumetric grids and meshes are computational intensive to acquire and maintain. In this spectrum, planar representation has been proven to be a reliable alternative, which is compact, effi-cient, expressive and generalizable enough to be deployed ubiquitously as well. Therefore, in practice it would always 1Shuaifeng Zhi and Kai Xu are corresponding authors.
Illustration of PlaneRecTR. Our core idea is to use
Figure 1. unified query learning to jointly model all subtasks of single-view plane recovery and output plane-level predictions, which achieves mutual benefits among tasks and enables higher performance. be ideal to infer planar information purely from a video se-quence, or even a single image. Being a challenging and yet fundamentally ill-posed computer vision problem, sin-gle view plane recovery/reconstruction has been extensively researched and focused so far. There have been early at-tempts using image processing to extract low-level primi-tives, such as line segments and vanishing points, to extract planar structures from the input image [8, 15].
As Convolutional Neural Networks (CNNs) have be-come the major approach to tackle vision problems in the past few years, their excelling in performance gradually spreads to the plane estimation task. Pioneering works such as PlaneNet [17] and PlaneRCNN [16] propose CNN-based efficient solutions to piece-wise planar structure recovery from a single image in a top-down manner. There have also been bottom-up solutions such as PlaneAE [35] and
PlaneTR [23] which obtain plane-level masks by a post-clustering procedure on top of learned deep pixel-wise em-bedding space. Recently, Transformers [25], as another ma-jor type of fundamental vision models, have made great progress on various vision tasks [26, 9, 33, 24, 28]. The success of vision Transformers does not merely comes from its realization of global/long-range interaction across im-ages via attention mechanisms, another important aspect is the design of query-based set predictions, initially proposed in Detection Transformer (DETR) [3] to enable reasoning between detected instances and global context. The intro-duction of query learning technique to vision Transformers
has been further proven to be effective in several high-level vision tasks including instance and semantic segmentation
[5, 4, 32], video panoptic segmentation [29, 36], etc. Plan-eTR [23] is an early attempt of using such ideas in single-view plane reconstruction. Motivated by the structured-guided learning, PlaneTR integrate additional geometric cues like line segments into its training process, leading to state-of-the-art performance on the ScanNet dataset and un-seen NYUv2-Plane dataset.
However, all of above mentioned single-view plane re-covery methods somewhat dis-entangle the prediction of principle components required for plane reconstruction.
Specifically, PlaneRCNN [16] learns to predict plane off-sets from a monocular depth prediction branch while other attributes such as plane masks and normals are estimated separately from colour images. PlaneTR [23] also learns to predict a monocular depth map apart from the Transformer module, which is later used for acquiring plane segmenta-tion masks via associate embedding clustering [35]. Un-der such cases, these closely related prediction subtasks are usually interleaved and we conjecture this could be one per-formance bottleneck for existing data-driven approaches.
Motivated by this finding, we seek to borrow recent ad-vance in query-based learning and aim to design a single, compact and unified model to jointly learn all plane-related tasks. We expect such design would achieve a mutual ben-efits among tasks and improve the existing performance of single view plane reconstruction. Extensive experimental results on two public benchmark datasets show that with-out using neighbouring views [16] nor extra structural cues
[23] during training, our unified querying learning model, named PlaneRecTR, achieves new state-of-the-art perfor-mance with a concise structure. Additionally, we have also found such framework can well take the advancements in fundamental vision models and is able to further increase its performance given stronger backbones.
To summarize, our contribution are as follows:
• We proposed a first single unified framework to ad-dress the single view plane reconstruction task, where all related subtasks including plane detection, plane parameter estimation, plane segmentation and depth prediction are jointly inferred in a multi-task manner motivated by query-based learning.
• Extensive numerical evaluation and visual compar-isons demonstrate the benefits of our proposed uni-fied query learning, which has been proven to take full advantage of plane semantic and geometric cues to achieve mutual benefits.
• We have achieved state-of-the-art single image plane recovery performance on the public ScanNet and
NYUv2-Plane datasets, surpassing existing CNN-based and Transformer-based baselines.
In addition, we have shown that our model could keep benefiting from stronger backbone vision models. 2.