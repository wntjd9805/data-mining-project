Abstract image, point cloud, natural
Backdoor attacks against deep neural network (DNN) models have been widely studied. Various attack techniques have been proposed for different domains and paradigms, language processing, e.g., transfer learning, etc. The most widely-used way to em-bed a backdoor into a DNN model is to poison the training data. They usually randomly select samples from the benign training set for poisoning, without considering the distinct contribution of each sample to the backdoor effectiveness, making the attack less optimal.
It theoretical proofing.
A recent work [40] proposed to use the forgetting score to measure the importance of each poisoned sam-ple and then filter out redundant data for effective back-door training. However, this method is empirically de-signed without is also very time-consuming as it needs to go through several train-ing stages for data selection.
To address such lim-itations, we propose a novel confidence-based scoring methodology, which can efficiently measure the contri-bution of each poisoning sample based on the distance posteriors. We further introduce a greedy search algo-rithm to find the most informative samples for backdoor injection more promptly.
Experimental evaluations on both 2D image and 3D point cloud classification tasks show that our approach can achieve comparable perfor-mance or even surpass the forgetting score-based search-ing method while requiring only several extra epochsâ€™ com-putation of a standard training process. Our code can be found at https://github.com/WU-YU-TONG/ computational_efficient_backdoor 1.

Introduction
The diversity of security threats [1, 23] to deep neural networks (DNNs) hinder their commercialization processes in many computer vision (CV) tasks. One of the most se-vere and wide-known threats is the backdoor attack [23].
*Corresponding Author
The adversary can inject a stealthy backdoor into the tar-get model corresponding to a unique trigger during train-ing [43, 28, 25, 17, 5, 10, 13]. During inference, the com-promised model performs well on the benign samples but can misbehave over the malicious samples with the trigger.
The most common approach to achieve backdoor attacks is data poisoning, where the adversary compromises certain training samples to embed the backdoor. A critical criterion for a successful backdoor attack is the ratio of poisoning samples. From the adversarial perspective, we hope to poi-son as few samples as possible while maintaining the same attack success rate (ASR) for two reasons: (1) a smaller poi-soning ratio can enhance the attack feasibility. The adver-sary only needs to compromise a small portion of training data, so the attack requirement is relaxed. (2) The backdoor attack with fewer poisoned samples will be more stealthy.
Numerous works proposed backdoor detection solutions via investigating the training samples [7, 15, 29, 35, 38, 34, 18].
A smaller poisoning ratio will significantly increase the de-tection difficulty and reduce the defense effectiveness.
A majority of backdoor attacks randomly select a fixed ratio of training samples for poisoning, and the ratio is heuristically determined [12, 16, 21, 41, 24]. This strategy is less optimal as it ignores the distinction of different sam-ples in terms of contributions to the backdoor. To overcome this limitation, a recent work [40] proposed a new method, forgetting score, to optimize the process of poisoning sam-ple selection. It uses the frequency of forgetting events per sample during model training as the score to represent the importance of each sample to the backdoor injection. Then, samples with higher scores are filtered out via a filtering-and-updating strategy (FUS) in N full training circles. For-getting score can effectively reduce the poisoning ratio by 25-50% compared with the random selection strategy while achieving a similar ASR. It also confirms the diverse im-pacts of different samples on the attack performance.
Forgetting score can achieve data efficiency but at the cost of computation inefficiency. Specifically, it empirically counts the number of times each sample is forgotten during training to locate the critical samples that contribute more to poisoning. Calculation of the forgetting scores requires
multiple full training circles (N = 10 for ImageNet-10 in [40]) to reduce the number of poisoning samples, which introduces significant efforts and is impractical for attacking large-scale datasets. As such, an intriguing question arises: is it possible to achieve both data efficiency and computa-tion efficiency in backdoor poisoning, i.e., effortlessly and precisely identifying the critical poisoning samples?
In this paper, we propose a novel attack methodology to confirm the above question. Our contributions are two-fold.
First, we propose the representation distance (RD) score, a new trigger-agnostic and structure-free metric to identify the poisoning samples that are more crucial to the success of backdoor attacks. Specifically, our goal is to locate those poisoning samples that have a larger distance to the target class since they will contribute more to reshaping the deci-sion boundary formation during training (backdoor embed-ding). By adopting this RD score, we can filter out the poi-soning samples that are insensitive to the backdoor infection to reduce the poisoning ratio. Second, the RD score can be used at a very early stage of model training (i.e. only a few epochs after training starts) with a greedy search scheme to select the poisoning samples. This significantly reduces the computation compared to the forgetting score.
We perform extensive experiments on five state-of-the-art models and 4 common datasets in the 2D image and 3D point cloud classification tasks. Evaluations show that our solution can effectively reduce the amount of samples to poison for a successful backdoor attack. Most importantly, compared to the forgetting score-based method, our solu-tion can figure out the important poisoned sample using the scoring models that only requires several epochs of training, which is a tiny cost in comparison to the forgetting score-based method. As the latter needs to go through the whole training processes to achieve the same goal. 2.