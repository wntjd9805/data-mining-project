Abstract
Temporal sentence grounding (TSG) aims to locate a specific moment from an untrimmed video with a given nat-ural language query. Recently, weakly supervised methods still have a large performance gap compared to fully super-vised ones, while the latter requires laborious timestamp annotations. In this study, we aim to reduce the annotation cost yet keep competitive performance for TSG task com-pared to fully supervised ones. To achieve this goal, we investigate a recently proposed glance-supervised temporal sentence grounding task, which requires only single frame annotation (referred to as glance annotation) for each query. Under this setup, we propose a Dynamic Gaussian prior based Grounding framework with Glance annotation (D3G), which consists of a Semantic Alignment Group Con-trastive Learning module (SA-GCL) and a Dynamic Gaus-sian prior Adjustment module (DGA). Specifically, SA-GCL samples reliable positive moments from a 2D temporal map via jointly leveraging Gaussian prior and semantic consis-tency, which contributes to aligning the positive sentence-moment pairs in the joint embedding space. Moreover, to al-leviate the annotation bias resulting from glance annotation and model complex queries consisting of multiple events, we propose the DGA module, which adjusts the distribu-tion dynamically to approximate the ground truth of tar-get moments. Extensive experiments on three challenging benchmarks verify the effectiveness of the proposed D3G.
It outperforms the state-of-the-art weakly supervised meth-ods by a large margin and narrows the performance gap compared to fully supervised methods. Code is available at https://github.com/solicucu/D3G. 1.

Introduction
Temporal sentence grounding is a fundamental prob-lem in computer vision and receives an increasing atten-*Corresponding author
Figure 1. Illustration of glance annotation g (red dashed line) and simple comparison between ViGA and D3G. The red rectangle indicates the boundary of target moment. tion in recent years. Given the query sentence and an untrimmed video, the goal of TSG is to localize the start and end timestamps of specific moment that semantically corre-sponds to the query. In recent years, full supervised tem-poral sentence grounding (FS-TSG) has achieved tremen-dous achievements [9, 1, 41, 43, 34, 29, 33, 42]. However, obtaining accurate timestamps for each sentence is labor-intensive and subjective, which prevents it from scaling to large-scale video-sentence pairs and practical applications.
Weakly supervised temporal sentence grounding (WS-TSG), which requires only the video and query pairs, re-ceives an increasing attention recently. Although great ad-vances [19, 32, 12, 45, 43, 44] have been achieved in recent years, there still remains a huge performance gap between
WS-TSG and FS-TSG. WS-TSG suffers from severe local-ization issues due to the large discrepancy between video-level annotations and clip-level task.
Recently, Cui et al. [6] propose a new annotating paradigm called glance annotation for TSG, requiring the timestamp of only random single frame within the tempo-ral boundary of the target moment.
It is noted that such annotation only increases trivial annotating cost compared to WS-TSG. Figure 1 illustrates the details of glance an-notation. With glance annotation, Cui et al. propose the
ViGA based on contrastive learning. ViGA first cuts the in-put video into clips of fixed length, which are assigned with
Gaussian weights generated according to the glance anno-tation, and contrasts clips with queries. There are two ob-vious disadvantages in this way. First, moments of interest usually have various durations. Therefore, these clips can-not cover a wide range of target moments, which inevitably aligns the sentence with incomplete moment and obtains sub-optimal performance. Second, ViGA utilizes a fixed scale Gaussian distribution centered at the glance frame to describe the span of each annotated moment. However, the glance annotations are not guaranteed at the center of target moments, which results in annotation bias as shown in Fig-ure 2. Besides, since some complex query sentences consist of multiple events, a single Gaussian distribution is hard to cover all events at the same time as shown in Figure 3. To address the aforementioned defects and fully unleash the potential of Gaussian prior knowledge with the low-cost glance annotation, we propose a Dynamic Gaussian prior based Grounding framework with Glance annotation (D3G) as shown in Figure 4.
We first generate a wide range of candidate moments fol-lowing 2D-TAN [43]. Afterwards, we propose a Semantic
Alignment Group Contrastive Learning module (SA-GCL) to align the positive sentence-moment pairs in the joint em-bedding space. Specifically, for each query sentence, we sample a group of positive moments according to calibrated
Gausssian prior and minimize the distances between these moments and the query sentence. In this way, it tends to gradually mine the moments which have increasing over-lap with the ground truth. Moreover, we propose a Dy-namic Gaussian prior Adjustment module (DGA), which further alleviates annotation bias and approximates the span of complex moments consisting of multiple events. Specif-ically, we adopt multiple Gaussian distributions to describe the weight distributions of moments. Therefore, the weight distributions for various moments can be flexibly adjusted and gradually approach to the ground truth. Our contribu-tions are summarized as follows:
• We propose a Dynamic Gaussian prior based Ground-ing framework with Glance annotation (D3G), which temporal sentence facilitates the development of grounding with lower annotated cost.
• We propose a Semantic Alignment Group Contrastive
Learning module to align the features of the positive sentence-moment pairs and a Dynamic Gaussian prior
Adjustment module to ease the annotation bias and model the distributions of complex moments.
• Extensive experiments demonstrate that D3G obtains consistent and significant gains compared to method under the same annotating paradigm and outperforms weakly supervised methods by a large margin.
Figure 2. Illustration of annotation bias and Gaussian prior after dynamic adjustment. Top: the target moment is assigned with a low weight wm due to the bias of glance annotation according to
ViGA, which we call annotation bias. Bottom: a reasonable Gaus-sian distribution is obtained via DGA described in Section 3.3. 2.