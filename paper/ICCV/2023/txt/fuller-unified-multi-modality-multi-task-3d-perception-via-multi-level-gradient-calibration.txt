Abstract
Multi-modality fusion and multi-task learning are be-coming trendy in 3D autonomous driving scenario, consid-ering robust prediction and computation budget. However, naively extending the existing framework to the domain of multi-modality multi-task learning remains ineffective and even poisonous due to the notorious modality bias and task conflict. Previous works manually coordinate the learning framework with empirical knowledge, which may lead to sub-optima. To mitigate the issue, we propose a novel yet simple multi-level gradient calibration learning framework across tasks and modalities during optimization. Specifi-cally, the gradients, produced by the task heads and used to update the shared backbone, will be calibrated at the back-bone’s last layer to alleviate the task conflict. Before the calibrated gradients are further propagated to the modal-ity branches of the backbone, their magnitudes will be cal-ibrated again to the same level, ensuring the downstream tasks pay balanced attention to different modalities. Ex-periments on large-scale benchmark nuScenes demonstrate the effectiveness of the proposed method, e.g., an absolute 14.4% mIoU improvement on map segmentation and 1.4% mAP improvement on 3D detection, advancing the appli-cation of 3D autonomous driving in the domain of multi-modality fusion and multi-task learning. We also discuss the links between modalities and tasks. 1.

Introduction 3D perception task plays an important role in au-tonomous driving. Previous works are mainly developed on single modality [44, 20, 16, 43, 7, 35, 21, 41, 27, 28] and different perception tasks are separated into individual mod-*Equal contribution.
†Corresponding author.
Figure 1. Comparison of paradigms on 3D perception. (a)
BEVFormer [21] focuses on multi-task learning, which could save the computation burden and thus facilitate the depolyment of real-world application. (b) Transfusion [2] is proposed for multi-modality fusion for robust prediction since point cloud and images are complementary. (c) Our proposed Fuller is a unified frame-work that integrates these ingredients organically by solving the notorious problems of modality bias and task conflict. els [2, 19, 18, 45, 5]. Often we wish to leverage complemen-tary modalities to produce robust prediction and integrate multiple tasks within a model for the sake of computation budget. For instance, with the development of hardware, it is affordable to deploy both LiDAR and camera on a car, which are responsible to provide spatial information and se-Integrating semantic-complementary mantic information. vision tasks within a framework would greatly facilitate the deployment of real-world application [3].
Recent advances have stayed tuned for multi-modality fusion [30, 23] and multi-task learning [47, 21] in the appli-cations of 3D autonomous driving scenario. Meanwhile, it is of great interest to unify multi-modality fusion and multi-task learning within a framework. In fact, it is unlikely to expect that dumping all the individual components into one framework and they would function organically. We build up a competitive baseline based on BEVFusion [30], which
takes as input both the point cloud and image, and serves for two complementary vision tasks: 3D detection (foreground) and map segmentation (background). However, we observe the severe issues of modality bias and task conflict: a) dif-ferent tasks prefer specific modality, e.g., 3D detection re-lies on spatial information provided by LiDAR sensor while segmentation task relies more on image inputs. b) adding a new task will degrade both tasks: -3.0 % mAP for detection and -18.3% mIoU for map segmentation.
From the perspective of optimization, we investigate the potential gradient imbalance that occurs during end-to-end training in a hierarchical view. First, we study the gradients which are produced by different task heads and are applied to update the parameters of the shared backbone. We ob-serve that simply summing up these raw gradients to update the shared backbone would damage the performance of both tasks, suggesting an imbalance between them. Empirical findings prove that there is a great discrepancy between the gradient magnitudes w.r.t. the task objectives. Second, we inspect the gradients produced in the intra-gradient layer, which is to be separated into successive modality branches.
Given a trained baseline, we visualize the gradient distri-butions of different modality branches and find their mag-nitudes imbalanced greatly. We further calculate the task accuracy by dropping one of the modalities to measure the modality bias. Our findings catch up with the theoretical analysis of [40], which suggests that the point cloud and image branches are suffering from the imbalanced conver-gence rate w.r.t. the downstream tasks.
We motivate our method by noting the findings discussed above, which is proposed to unify multi-modality multi-task 3D perception via multi-level gradient calibration, dubbed as Fuller. Specifically, we devise the multi-level gradient calibration, comprised of inter-gradient and intra-gradient calibration, to address the associated issues.
In terms of the task conflict, we find that the task with lower gradient magnitude would be overwhelmed by another task with higher gradient magnitude. Thus, we propose to cali-brate the gradients of different task losses at the backbone.
Since the gradient would be manipulated at the layer level, this technique is referred to as inter-gradient calibration.
Regarding modality bias, we expect the different modalities can update and converge at the same pace. Hence, before the gradients are separated into the modality branches, we calibrate their magnitudes to the same level, which is per-formed in the intra-gradient layer internally and thus called intra-gradient calibration.
On top of the gradient calibration, we introduce two lightweight heads for our tasks. These two heads are both transformer-based. With our specially designed initializa-tion methods, they can generate fine-grained results with just a one-layer decoder, allowing to save much more pa-rameters than dense heads.
We thoroughly evaluate the Fuller on the popular bench-mark nuScenes [3]. Regarding the sensory input, we adopt the point cloud to provide accurate spatial information and use the image to compensate for the lack of visual seman-tics. In terms of perception tasks, we select two representa-tive and complementary tasks: 3D detection and map seg-mentation, which are responsible for dynamic foreground objects and static road elements understanding. Note that
BEVFusion [30] only organizes these ingredients empiri-cally without mentioning the problems discussed above. To summarize, our contributions are:
• We propose the Fuller which organically integrates multi-modality fusion and multi-task learning for 3D perception via multi-level gradient calibration during end-to-end optimization.
• We introduce the new architecture design for task heads, which outperforms or is comparable with the previous head design while saving ∼40% parameters.
• Extensive experiments demonstrate that Fuller can pre-vent the notorious problems of modality bias and task conflict, e.g., an absolute 14.4% mIoU improvement on map segmentation and 1.4% mAP improvement on 3D detection. 2.