Abstract
Vision Transformer (ViT) based Vision-Language Pre-training (VLP) models have demonstrated impressive per-formance in various tasks. However, the lengthy visual to-ken sequences fed into ViT can lead to training inefficiency and ineffectiveness. Existing efforts address the challenge by either bottom-level patch extraction in the ViT backbone or top-level patch abstraction outside, not balancing train-ing efficiency and effectiveness well. Inspired by text sum-marization in natural language processing, we propose a
Bottom-Up Patch Summarization approach named BUS , coordinating bottom-level extraction and top-level abstrac-tion to learn a concise summary of lengthy visual token sequences efficiently. Specifically, We incorporate a Text-Semantics-Aware Patch Selector (TSPS) into the ViT back-bone to perform a coarse-grained visual token extraction and then attach a flexible Transformer-based Patch Abstrac-tion Decoder (PAD) upon the backbone for top-level vi-sual abstraction. This bottom-up collaboration enables our
BUS to yield high training efficiency while maintaining or even improving effectiveness. We evaluate our approach on various visual-language understanding and generation tasks and show competitive downstream task performance while boosting the training efficiency by 50%. Addition-ally, our model achieves state-of-the-art performance on many downstream tasks by increasing input image resolu-tion without increasing computational costs over baselines. 1.

Introduction
Large-scale pre-training of vision-language models has recently received tremendous success on a wide range of cross-modal tasks [45, 7, 14, 28, 54, 26, 49]. Such vision-language models learn cross-modal representations from a
* Corresponding Author.
Figure 1: Subfigure (a) shows the examples of selected text-relevant patch in the VQA scenario. Subfigure (b) shows the overview of our proposed bottom-up patch summarization. large number of image-text pairs by aligning the visual and linguistic modalities.
Most recent works [14, 49, 26, 20, 11] adopt ViT as the visual encoder or cross-modal fusion encoder, due to its excellent ability to model the fine-grained long visual se-quences from the image grids or patches.
Despite the impressive progress of ViT-based VLP mod-els, they still face challenges of training inefficiency and ineffectiveness caused by lengthy visual token sequences.
Firstly, long visual sequences will bring heavy self-attention calculation for visual representation modeling and cross-modal fusion, leading to time-consuming training. Sec-ondly, long visual sequences contain many redundant patches irrelevant to the text semantics. For instance, as illustrated in Figure 1 (a), during the VQA task, when an-swering the question “What is the color of the buildings in the far left of the photo?”, about 80% of the image patches may be irrelevant with the question. On the one hand, those 1
text-irrelevant patches (e.g., the yellow building in the im-age) will hinder the fine-grained alignment between the tex-tual and visual modalities. On the other hand, they will lead to the overshadowing of brief linguistic signals (e.g., of short image captions) by complex visual ones during the cross-modal fusion, namely the “vanishing information” problem of textual information [23].
The limitations above underscore the importance of re-ducing visual token sequences. Recent related efforts can be categorized into two lines.
• Top-level Abstraction. The first line tackles the is-sue outside the ViT-based visual backbones from a top-level perspective [24, 3]. Specifically, these works use a fixed number of learnable latent query vectors to query the long visual sequences output, obtaining the final fixed-length visual sequence representations, in an abstractive way. An obvious bottleneck is that they can not optimize the costly and potentially un-necessary self-attention calculation in the visual back-bones. Meanwhile, this visual representation abstrac-tion process only considers the semantics of the vi-sual modality, ignoring the textual guidance and con-sequently leading to representation deficiency.
• Bottom-level Extraction. The second line focuses on reducing patch tokens in the ViT-backbone from the bottom-level perspective, usually in an extractive man-ner [39, 29, 16] . The problem here lies in that overly reducing the visual sequence by extracting critical to-kens in the backbone, while accelerating the attention calculation, may deconstruct images’ structural infor-mation. Therefore, balancing efficiency and effective-ness remains a bottleneck.
To achieve a better trade-off between the efficiency and effectiveness of VLP, we propose integrating the merits of top-level abstraction and bottom-level extraction. Inspired by bottom-up text summarization[4, 17], which first select key phrases and then abstractively generate the final text summaries, we design a bottom-up summarization process for visual tokens. We first exploit coarse-grained key patch extraction in the ViT backbone, with regulation from text modality, to identify text-relevant tokens and remove po-tentially redundant ones, reducing the computational cost in the ViT backbone. Then fine-grained text-guided patch abstraction is performed upon the output sequence of the
ViT backbone to obtain a further condensed visual repre-sentation sequence.
Specifically, we incorporate a Text Semantic-aware
Patch Selector (TSPS) module into the ViT-based back-bone for bottom-level extraction. We transform object/re-gion annotations to patch-level annotations to train an effec-tive extractor with a novel auxiliary pre-training task named
Patch-Text Matching (PTM), which facilitates patch extrac-tion and fine-grained patch-text alignment. Next, we in-troduce a lightweight Transformer-based Patch Abstraction
Decoder (PAD) for top-level abstraction. It takes the top-K text-relevant patch tokens from the output sequence of ViT-backbone as the input and the overall visual sequence as the encoder hidden states to generate the final visual patch summary.
We evaluate BUS on various representative VL under-standing and generation tasks, including visual question an-swering, cross-modal retrieval, and image captioning. We find that by reducing the length of the patch sequence to 20% of its original length, we can not only get compet-itive or better downstream task performance but also en-joy a significant increase in efficiency over previous similar
VLP models. For instance, BUS reduces about 51% of the inference time (see Table 7) and even improves by about 0.3 on the VQA test-dev with the same experimental set-tings. Furthermore, by increasing the input image resolu-tion, BUS achieves state-of-the-art downstream task perfor-mance (e.g., 78.28 on VQA test-dev) which benefits from processing more image tokens without increasing computa-tional costs. 2.