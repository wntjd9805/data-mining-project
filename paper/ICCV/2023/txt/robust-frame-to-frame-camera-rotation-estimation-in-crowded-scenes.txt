Abstract
We present an approach to estimating camera rotation in crowded, real-world scenes from handheld monocular video. While camera rotation estimation is a well-studied problem, no previous methods exhibit both high accuracy and acceptable speed in this setting. Because the setting is not addressed well by other datasets, we provide a new dataset and benchmark, with high-accuracy, rigorously ver-ified ground truth, on 17 video sequences. Methods de-veloped for wide baseline stereo (e.g., 5-point methods) perform poorly on monocular video. On the other hand, methods used in autonomous driving (e.g., SLAM) lever-age specific sensor setups, specific motion models, or lo-cal optimization strategies (lagging batch processing) and do not generalize well to handheld video. Finally, for dy-namic scenes, commonly used robustification techniques like RANSAC require large numbers of iterations, and be-come prohibitively slow. We introduce a novel generaliza-tion of the Hough transform on SO(3) to efficiently and ro-bustly find the camera rotation most compatible with op-tical flow. Among comparably fast methods, ours reduces error by almost 50% over the next best, and is more ac-curate than any method, irrespective of speed. This repre-sents a strong new performance point for crowded scenes, an important setting for computer vision. The code and the dataset are available at https://fabiendelattre.com/robust-rotation-estimation.
Figure 1. Left. A frame from our BUSS dataset of crowded scenes. The red vectors show optical flows compatible with the winning rotation estimate R∗, indicating the rotation of the cam-era. Gray vectors show optical flows not explained purely by R∗.
Right. The three axes show the space of rotations in 3D. Each line shows the one-dimensional set of rotations that are compati-ble with a single optical flow vector. The red lines (correspond-ing to the red flow vectors in the top figure) intersect in a single small bin, indicating that their optical flows are compatible with the same rotation. The gray lines, which are affected by other motion effects, are scattered in an unstructured manner, and corre-spond to the gray optical flows above. Our algorithm finds the set of lines with greatest coherence in SO(3), revealing the rotation
R∗ of the camera. 1

Introduction
The estimation of camera motion through a scene is a fundamental problem in computer vision that is highly related to a number of vision tasks such as motion seg-mentation [5], video stabilization [44], 3D reconstruc-tion [9], visual odometry [56], Simultaneous Localisa-tion and Mapping (SLAM) [53], Structure-from-Motion (SfM) [65], human-computer interaction [54], autonomous navigation [69], and many more. Hence, developing a method that can accurately predict the camera’s movement through a scene is critical in solving these problems.
As the camera moves through the scene, the motion field depends not only on the camera’s motion but also on the scene’s geometry and objects’ motion in the environ-ment. Given a sufficiently crowded location with many moving objects (e.g., pedestrians and vehicles), estimating the camera motion requires the difficult task of distinguish-ing between static and moving objects. This paper pro-poses a novel, robust method of estimating camera rotation in crowded scenes such as the one shown in Fig. 1.
It is important to clarify the difference between frame-to-frame camera motion estimation and relative pose esti-mation. Specifically, camera motion estimation is a con-strained version of relative pose estimation where only two views are used, constrained to be (a) spatially close, (b) tem-porally close, and (c) taken from the same camera, which matches the case of adjacent frames in a moving-camera video.
Nowadays, many authors focus on relative pose estima-tion using point correspondences. Most of these methods focus on estimating the essential matrix [45, 56], which works best in the presence of large parallax [47, Remark 5.2] (large baselines). Therefore, correspondence-based methods are primarily used for offline localization and map-ping strategies such as SfM and 3D reconstruction, or on-line pipelines with local optimization like SLAM. In con-trast, optical flow-based methods are better suited for small motions, which is the domain of interest in this paper.
As in state-of-the-art correspondence-based relative pose problems [34], the best optical flow-based methods for frame-to-frame camera motion estimation focus on decou-pling the transformation into rotation- and translation-only estimation [5,7]. While there are fast and accurate solutions to motion estimation, they are highly sensitive to moving objects in the scene–they frequently break down with signif-icant numbers of moving objects in the scene. Similarly to correspondence-based techniques, optical flow-based meth-ods are often used within RANSAC [14] to handle locally wrong optical flow and moving objects, and thereby in-crease robustness. In this paper, we focus on rotation es-timation since flow-based translation estimation given rota-tion estimates can be easily computed as shown in [5, 7].
We propose a new method to estimate the camera rota-tion based on optical flow. Our approach can be used for highly dynamic scenes, from the assumption that optical flow from faraway points is less sensitive to dynamic ob-jects in the scene. The proposed technique uses a com-patible rotation voting mechanism and does not require
RANSAC (see Fig. 1).
In addition, since public datasets only contain static scenes or have minor dynamic objects (a large portion of the frames contain static environments), we acquire a new and challenging dataset of 17 sequences in (anonymized) crowded environments. The dataset will be made available. To summarize, our contributions are as follows: – A novel robust frame-to-frame camera rotation estima-tion algorithm based on optical flow that finds compat-ible rotations using a voting mechanism based on the
Hough transform in the space of 3D rotations; – We show that our algorithm significantly outperforms the discrete and continual baselines in highly dynamic scenes and performs comparably in static scenes; and called BUsy Street Scenes (BUSS) that comes with rigorously verified ground truth rotation. 2