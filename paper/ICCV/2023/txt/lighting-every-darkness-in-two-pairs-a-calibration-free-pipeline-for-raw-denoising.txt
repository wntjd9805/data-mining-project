Abstract
Calibration-based methods have dominated RAW image denoising under extremely low-light environments. How-ever, these methods suffer from several main deficiencies: 1) the calibration procedure is laborious and time-consuming, 2) denoisers for different cameras are difficult to transfer, and 3) the discrepancy between synthetic noise and real noise is enlarged by high digital gain. To overcome the above shortcomings, we propose a calibration-free pipeline for Lighting Every Drakness (LED), regardless of the dig-ital gain or camera sensor.
Instead of calibrating the noise parameters and training repeatedly, our method could adapt to a target camera only with few-shot paired data and fine-tuning. In addition, well-designed structural modifica-tion during both stages alleviates the domain gap between synthetic and real noise without any extra computational cost. With 2 pairs for each additional digital gain (in total 6 pairs) and 0.5% iterations, our method achieves superior performance over other calibration-based methods. 1.

Introduction
Noise, an unescapable topic for image capturing, has been systematically investigated in recent years [5, 66, 51, 41, 2, 8, 57]. Compared with standard RGB im-ages [56, 21, 54, 34, 33, 32], RAW images enjoys two great tractable, primitive noise potentials for image denoising: distribution [57] and higher bit depth for differentiating sig-nal from noise. Learning-based methods have achieved sig-nificant progress on RAW image denoising with paired real datasets [67, 22, 64, 32, 33]. However, it is unfeasible to collect a large-scale real RAW image dataset for each sin-gle camera model. Therefore, increasing attention has been drawn from deploying learning-based methods on synthetic dataset [1, 61, 31, 57, 68, 44, 40].
*Equal contribution.
†C. L. Guo is the corresponding author.
[57]
[37]
Figure 1. LED achieves state-of-the-art performance in every darkness situation (different digital gain and camera sensor) com-pared with calibration-based or transfer learning-based methods.
Also, a minimum cost is required for applying to a new camera model by the proposed pipeline. Details can be found in Sec. 4.
Calibration-based noise synthesis with physics-based models has proved its effectiveness in fitting real noise [53,
In general, these methods conduct 57, 68, 45, 69, 17]. the following steps. First, they build a well-designed noise model depending on the electronic imaging pipeline. Then, they select a specific target camera and carefully calibrate the parameters of the predefined noise model. Finally, they generate synthetic paired data for training a denoising net-work. Additionally, some methods resort to Deep Neural
Network (DNN)-based generative models for noise param-eter calibration [45, 69].
Though great performance has been achieved, these methods are limited by three main deficiencies, as shown in Fig. 2 (a). 1) The calibration-specialized data collec-tion requires a stable illumination environment and elab-orated post-processing, leading to a time-consuming and labor-intensive procedure. 2) denoising network trained for the specific camera is difficult to transfer to another cam-era. This leads to a strong connection between the network and the camera, resulting in repeated calibration and train-ing for different target cameras. 3) Certain noise distribu-tions might not be included in the noise model, denoted as
(a) Calibration-based methods (b) Our proposed pipeline
Figure 2. The thumbnail of calibration-based methods and our proposed LED. The “→” denotes the problems of the calibration-based methods, and the “→” highlights our solutions for the above problems. Calib. represents the calibration operations, including predefining a noise model, collecting calibration-specialized data, post-processing, and calculating the noise parameters. In LED, the collection procedure only captures few-shot paired data, alleviat-ing the deployment cost. out-of-model noise [57, 68, 17]. In other words, the domain gap between Synthetic Noise (SN) and Real Noise (RN) still remains. Although recent work [69] mainly focuses on al-leviating the cost of calibration by DNN-based calibration.
The coupling issue and the out-of-model noise still increase the training expense and limit their performance.
To work on the above three problems of the calibration-based methods, we propose a calibration-free pipeline for lighting every darkness (LED). As shown in Fig. 2 (b), our framework does not need any data or operations for cal-ibration. Furthermore, for decoupling the strong connec-tion between the denoising network and the specific target camera, we propose a pre-training and fine-tuning frame-work. As for the gap between virtual1 and target cameras, as well as the influence of the out-of-model noise, we propose a Reparameterized Noise Removal (RepNR) block. Dur-ing pre-training, the RepNR block is equipped with several camera-specific alignments (CSA). Each CSA is responsi-ble for learning the camera-specific information of a vir-tual camera and aligning features to a shared space. Then, the common knowledge of in-model (components that have been assumed as part of the noise model) noise is learned by the denoising convolution. In fine-tuning, we average all the CSAs of virtual cameras as initialization of the target camera. In addition, a parallel convolution branch is added for the out-of-model noise removal (OMNR). Only 2 pairs for each ratio (additional digital gain) captured by the target camera, in a total 6 raw image pairs, are used for learn-ing to remove real noise of it (discussion on why 2 pairs for each ratio can be found in Sec. 5). During deploy-ment, all the RepNR blocks can be structurally reparame-terized [15, 16, 10] into a simple 3 × 3 convolution without any extra computational cost, yielding a plain UNet [49]. 1“Virtual” cameras do not correspond to any real camera models, but with reasonable noise parameters of the predefined noise model.
Our main contributions are summarized as follows:
• We propose a calibration-free pipeline for lighting ev-ery darkness, which avoids all extra costs for calibrat-ing the noise parameters.
• Designed CSA loosens the coupling between the de-noising network and camera model, while OMNR en-ables few-shot transfer by learning the out-of-model noise of different sensors.
• Only 2 raw image pairs for each ratio and 0.5% itera-tions are required compared with SOTA methods. 2.