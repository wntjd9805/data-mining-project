Abstract
Vision-language pre-training (VLP) models have shown vulnerability to adversarial examples in multimodal tasks.
Furthermore, malicious adversaries can be deliberately transferred to attack other black-box models. However, ex-isting work has mainly focused on investigating white-box attacks. In this paper, we present the first study to investigate the adversarial transferability of recent VLP models. We ob-serve that existing methods exhibit much lower transferabil-ity, compared to the strong attack performance in white-box settings. The transferability degradation is partly caused by the under-utilization of cross-modal interactions. Particu-larly, unlike unimodal learning, VLP models rely heavily on cross-modal interactions and the multimodal alignments are many-to-many, e.g., an image can be described in various natural languages. To this end, we propose a highly trans-ferable Set-level Guidance Attack (SGA) that thoroughly leverages modality interactions and incorporates alignment-preserving augmentation with cross-modal guidance. Ex-perimental results demonstrate that SGA could generate adversarial examples that can strongly transfer across dif-ferent VLP models on multiple downstream vision-language tasks. On image-text retrieval, SGA significantly enhances the attack success rate for transfer attacks from ALBEF to
TCL by a large margin (at least 9.78% and up to 30.21%), compared to the state-of-the-art. Our code is available at https://github.com/Zoky-2020/SGA.
Figure 1: Comparison of attack success rates (ASR) using five different attacks on image-text retrieval. Adversar-ial examples are crafted on the source model (ALBEF) to attack the target white-box model or black-box models. The first three columns refer to the image-only PGD attack [24], text-only BERT-Attack [19] (BA), and the combined sepa-rate unimodal attack (SA), which all belong to the methods without cross-modal interactions. The fourth column is the state-of-the-art multimodal Co-Attack [41] (CA) that em-ploys single-pair cross-modal interactions. The last column is the proposed Set-level Guidance Attack (SGA), which leverages multiple set-level cross-modal interactions, suc-cessfully attacking the white-box model and transferring to attack all black-box models with the highest ASR. More discussions are in Section 3. 1.

Introduction
Recent work has shown that vision-language pre-training (VLP) models are still vulnerable to adversarial examples
[41], even though they have achieved remarkable perfor-mance on a wide range of multimodal tasks [30, 15, 16].
Existing work mainly focuses on white-box attacks, where
* Equal contribution. â€  Corresponding author. information about the victim model is accessible. How-ever, the transferability of adversarial examples across VLP models has not been investigated, which is a more practical setting. It is still unknown whether the adversarial data gen-erated on the source model can successfully attack another model, which poses a serious security risk to the deployment of VLP models in real-world applications.
This paper makes the first step to investigate the trans-(Figure 2). Specifically, we introduce alignment-preserving augmentation which enriches image-text pairs while keeping their alignments intact. The image augmentation is based on the scale-invariant property of deep learning models [22], thus we can construct multi-scale images to increase the diversity. For text augmentation, we select the most match-ing caption pairs from the dataset. More importantly, SGA generates adversarial examples on multimodal augmented input data with carefully designed cross-modal guidance.
In detail, SGA iteratively pushes supplemental information away between two modalities with another modality as su-pervision to disrupt the interactions for better harmonious perturbations. Note that resultant adversarial samples could perceive the gradients originated from multiple guidance.
We conduct experiments on two well-established mul-timodal datasets, Flickr30K [27] and MSCOCO [23], to evaluate the performance of our proposed SGA across var-ious Vision-and-Language (V+L) downstream tasks. The experimental results demonstrate the high effectiveness of
SGA in generating adversarial examples that can be strongly transferred across VLP models, surpassing the current state-of-the-art attack methods in multimodal learning. In par-ticular, SGA achieves notable improvements in image-text retrieval under black-box settings and also exhibits superior performance in white-box attack settings. Moreover, SGA also outperforms the state-of-the-art methods in image cap-tioning and yields higher fooling rates on visual grounding.
We summarize our contributions as follows. 1) We make the first attempt to explore the transferability of adversarial examples on popular VLP models with a systematical evalu-ation; 2) We provide SGA, a novel transferable multimodal attack that enhances adversarial transferability through the effective use of set-level alignment-preserving augmenta-tions and well-designed cross-modal guidance; 3) Extensive experiments show that SGA consistently boosts adversarial transferability across different VLP models than the state-of-the-art methods. 2.