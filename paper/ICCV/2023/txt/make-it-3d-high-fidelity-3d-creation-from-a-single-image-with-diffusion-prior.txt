Abstract 1.

Introduction
In this work, we investigate the problem of creating high-fidelity 3D content from only a single image. This is inherently challenging: it essentially involves estimat-ing the underlying 3D geometry while simultaneously hal-lucinating unseen textures. To address this challenge, we leverage prior knowledge from a well-trained 2D diffusion model to act as 3D-aware supervision for 3D creation. Our approach, Make-It-3D, employs a two-stage optimization pipeline: the first stage optimizes a neural radiance field by incorporating constraints from the reference image at the frontal view and diffusion prior at novel views; the second stage transforms the coarse model into textured point clouds and further elevates the realism with diffusion prior while leveraging the high-quality textures from the reference im-age. Extensive experiments demonstrate that our method outperforms prior works by a large margin, resulting in faithful reconstructions and impressive visual quality. Our method presents the first attempt to achieve high-quality 3D creation from a single image for general objects and en-ables various applications such as text-to-3D creation and texture editing.
†Work is done during the internship at Microsoft Research.
‡Corresponding authors.
Given a single image as in Figure 1, how would the ob-ject portrayed in the image look like from a different per-spective? Humans possess an innate ability to effortlessly imagine 3D geometry and hallucinate the appearance of novel views with a glance at the picture based on their prior knowledge about the world. In this work, we aim to achieve a similar goal: creating high-fidelity 3D content from a real or artificially generated single image. This will open up new avenues for artistic expression and creativity, such as bring-ing 3D effects to the fantasy images given by the cutting-edge 2D generative models like Stable Diffusion [38]. By offering a more accessible and automated way to create vi-sually stunning 3D content, we hope to engage a broader audience with the world of 3D modeling with ease.
The creation of 3D objects from a single image presents a significant challenge due to the limited information that can be inferred from a single viewpoint. One categories of works aim to produce 3D photo effect [28, 41, 11, 47] in the manner of image-based rendering or single-view 3D recon-struction with neural rendering [54, 56, 37]. However, these methods often struggle with reconstructing fine geometry and fall short of rendering in large views. Another line of research [26, 51, 59, 55] projects the input image into the la-tent space of the pretrained 3D-aware generative networks. 1
Despite their impressive performance, the existing 3D gen-erative networks mainly model objects from a specific class and are therefore incapable of handling general 3D objects.
In our case, we aim for general 3D creation from an arbi-trary image, yet constructing a sufficiently large and diverse dataset for estimating the novel views or building a power-ful 3D foundation model for general objects remains insur-mountable.
Unlike the scarcity of 3D models, images are much more readily available, and recent advancements in diffu-sion models have sparked a revolution in 2D image genera-tion [34, 39, 38, 52, 4]. Interestingly, we observed that well-trained image diffusion models can generate images under various views, which implies that they have already incor-porated 3D knowledge. This has motivated us to explore the possibility of cultivating prior knowledge in a 2D diffu-sion model to reconstruct 3D objects. With diffusion prior, we propose Make-It-3D, a two-stage 3D content creation method that can generate a high-fidelity 3D object with su-perior quality from only one image.
In the first stage, we leverage diffusion prior to optimize a neural radiance field (NeRF) [23] by applying score dis-tillation sampling (SDS) [32], and constrain this optimiza-tion with reference-view supervision. Different from prior text-to-3D works [32, 18, 22], we focus on image-based 3D creation so that we need to prioritize the faithfulness to the reference image. However, we observed that while 3D mod-els generated with SDS match text prompts well, they often fail to align faithfully with reference images since textual descriptions do not capture all object details. To address this issue, we go beyond SDS by simultaneously maximiz-ing the image-level similarity between the reference and the novel view rendering denoised by a diffusion model. Also, as images inherently capture more geometry-related infor-mation than textual descriptions, we can thus incorporate the depth of the reference image as an extra geometry prior to alleviate the shape ambiguity of NeRF optimization.
While the first stage generates a coarse model with plau-sible geometry, its appearance often deviates from the qual-ity of the reference, exhibiting over-smooth textures and saturated colors [32]. This has limited its overall real-ism, and it is imperative to further bridge the gap between coarse model and reference image. As texture is more crit-ical than geometry for human perception in the context of high-quality rendering, we choose to prioritize texture en-hancement in the second stage, while inheriting the geome-try from the first stage. We refine the model by leveraging the availability of ground-truth textures for regions that are observable in the reference image. To achieve this, we ex-port the coarse NeRF model to textured point clouds and project reference textures onto their corresponding areas in the point clouds. We then utilize diffusion prior to enhance the texture of the remaining points by jointly optimizing the point feature and a point cloud renderer, resulting in a clearly improved texture of the generated 3D model.
With diffusion prior as multi-view supervision, our ap-proach can be applied to general objects without being lim-ited to specific categories. To evaluate the method, we cre-ate a benchmark consisting of 400 images including both real images and generated images from 2D diffusion. We evaluate the proposed method on public DTU dataset [1] and our benchmark, and extensive experiments show a clear improvement over previous works. Furthermore, our method enables a range of applications beyond image-to-3D creation such as texture editing and high-quality text-to-3D creation. Our main contributions are summarized as:
• We propose Make-It-3D, a framework to create a high-fidelity 3D object from a single image, using a 2D dif-fusion model as 3D-aware prior.
It does not require multi-view images for training and can be applied to any input image, whether it is real or generated.
• With a two-stage creation scheme, Make-It-3D repre-sents the first work to achieve high-fidelity 3D creation for general objects. The resulting 3D models exhibit detailed geometry and realistic textures that accurately conform to the reference images.
• Beyond image-to-3D creation, our method enables multiple applications such as high-quality text-to-3D creation and texture editing. 2.