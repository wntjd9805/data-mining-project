Abstract
Neural Radiance Field (NeRF) has been a mainstream in novel view synthesis with its remarkable quality of rendered images and simple architecture. Although NeRF has been developed in various directions improving continuously its performance, the necessity of a dense set of multi-view im-ages still exists as a stumbling block to progress for practi-cal application. In this work, we propose FlipNeRF, a novel regularization method for few-shot novel view synthesis by utilizing our proposed flipped reflection rays. The flipped reflection rays are explicitly derived from the input ray di-rections and estimated normal vectors, and play a role of effective additional training rays while enabling to estimate more accurate surface normals and learn the 3D geometry effectively. Since the surface normal and the scene depth are both derived from the estimated densities along a ray, the accurate surface normal leads to more exact depth estima-tion, which is a key factor for few-shot novel view synthesis.
Furthermore, with our proposed Uncertainty-aware Empti-ness Loss and Bottleneck Feature Consistency Loss, FlipN-eRF is able to estimate more reliable outputs with reducing floating artifacts effectively across the different scene struc-tures, and enhance the feature-level consistency between the pair of the rays cast toward the photo-consistent pixels without any additional feature extractor, respectively. Our
FlipNeRF achieves the SOTA performance on the multiple benchmarks across all the scenarios. 1.

Introduction
Neural Radiance Field (NeRF) [24] has achieved great success in rendering photo-realistic images from novel viewpoints. However, the necessity of a dense set of train-ing images remains as a practical bottleneck since it suf-fers from significant performance degradation when trained with sparse views.
There are two mainstreams for few-shot novel view syn-thesis: pre-training and regularization methods, both of which focus on learning the 3D geometry efficiently from sparse inputs. The pre-training methods [43, 5, 6, 39, 20, (a) Sparse training views mip-NeRF [1]
Ref-NeRF [36]
MixNeRF [31]
FlipNeRF (Ours) (b) Synthesized unseen views
Figure 1: Synthesis results from sparse inputs. Our Flip-NeRF significantly improves rendering quality compared to other baselines. Compared to the vanilla mip-NeRF [1] and
MixNeRF [31], which is the state-of-the-art regularization method, ours reduces the noises and floating artifacts no-ticeably with superior surface normal estimation. Although
Ref-NeRF [36] estimates smooth normal vectors, it shows much inferior rendering results to ours with a large chunk of noise under the few-shot setting. 12, 19, 29, 35, 14] require large-scale datasets consisting of different scenes with multi-view images for injecting prior knowledge during the pre-training, while the regularization methods [25, 31, 15, 11, 30, 8, 17] are optimized per scene, exploiting additional training resources, e.g. unseen view-points [25, 15, 17], depth map generation [8, 30], off-the-shelf models [11, 25], and so on, for an effective regulariza-tion to alleviate overfitting. Although the prior arts achieved promising results in novel view synthesis from sparse in-puts, there still exist hurdles to overcome. The large-scale datasets, which are used for pre-training methods, are ex-pensive to collect and the NeRF model is prone to perfor-mance degradation for the out-of-distribution dataset. On the other hand, the regularization methods heavily rely on additional training resources which might not always be available and require many heuristic factors, e.g. the choice of off-the-shelf models, the hyperparameters for sampling unseen viewpoints, and so on.
In this paper, we propose FlipNeRF, which is an effec-tive regularization method exploiting the flipped reflection rays1 as additional training resources with filtering the inef-fective newly generated rays. We derive a batch of flipped reflection rays from the original ray directions and esti-mated surface normals so that they are cast toward the same target pixels of the original input ray. Compared to the existing regularization methods which have mainly fo-cused on the accurate depth estimation from limited input views [31, 25, 15, 30, 8], our FlipNeRF is trained to recon-struct surface normals accurately by learning to generate effective reflection rays to be used in training. Since both estimated surface normals and depths are derived from the volume densities representing underlying 3D geometry, ac-curately estimating the surface normals of an object natu-rally leads to more accurate depth maps.
Furthermore, we propose an effective regularization loss,
Uncertainty-aware Emptiness Loss (UE Loss), to reduce the floating artifacts effectively while considering the un-certainty of the model’s outputs by using the estimated scale parameters for mixture models. Since our FlipNeRF is built upon MixNeRF [31], which is a regularization method achieving promising results by modeling input rays with mixture density models [2], we are able to apply our pro-posed loss without any modification of the architecture by using the estimated scale parameters of each sample along a ray, which stand for the uncertainty of the samples’ esti-mated probability density distributions.
Additionally, inspired by [6, 11, 15] which address the feature-level consistency of targets under the sparse input setting, we encourage the consistency for the pairs of bot-tleneck features between the original input rays and flipped reflection rays. We leverage a Jensen-Shannon Divergence, which is based on the similarity between the probability dis-tributions, to make the pairs of bottleneck feature distribu-tions of original and flipped reflection rays more similar to each other improving feature consistency.
We demonstrate the effectiveness of our proposed Flip-NeRF through the experiments on the multiple bench-1The term ‘flipped’ is used because the reflected ray has an opposite direction (from an object to a camera). marks, e.g. Realistic Synthetic 360◦ [24], DTU [13], and
LLFF [23]. Our method achieves state-of-the-art (SOTA) performances compared to other baselines. Especially, ours outperforms other baselines by a large margin with more ac-curate surface normals under the extremely sparse settings such as 3/4-view setting which are the most challenging ones. Our contributions are summarized as follows:
• We propose an effective training framework for NeRF with sparse training views, called FlipNeRF. It leverages flipped reflection rays to provide additional training re-sources, resulting in more precise surface normals with our proposed masking strategy to filter the ineffective rays.
• We also propose an effective regularization loss,
Uncertainty-aware Emptiness Loss (UE Loss), which re-duces floating artifacts with considering the uncertainty of outputs, leading to more reliable estimation.
• We enhance the consistency of bottleneck features be-tween the original input rays and flipped reflection rays by
Jensen-Shannon Divergence, coined as Bottleneck Fea-ture Consistency Loss (BFC Loss), improving the robust-ness for rendering from unseen viewpoints.
• Our FlipNeRF achieves SOTA performance over the multiple benchmarks. Especially, ours outperforms other baselines by a large margin in more challenging scenar-ios, e.g. 3/4-view. 2.