Abstract
Domain generalization person re-identification (DG-ReID) aims to train a model on source domains and gen-eralize well on unseen domains. Vision Transformer usu-ally yields better generalization ability than common CNN networks under distribution shifts. However, Transformer-based ReID models inevitably over-fit to domain-specific bi-ases due to the supervised learning strategy on the source domain. We observe that while the global images of differ-ent IDs should have different features, their similar local parts (e.g., black backpack) are not bounded by this con-straint. Motivated by this, we propose a pure Transformer model (termed Part-aware Transformer) for DG-ReID by designing a proxy task, named Cross-ID Similarity Learn-ing (CSL), to mine local visual information shared by differ-ent IDs. This proxy task allows the model to learn generic features because it only cares about the visual similarity of the parts regardless of the ID labels, thus alleviating the side effect of domain-specific biases. Based on the local similarity obtained in CSL, a Part-guided Self-Distillation (PSD) is proposed to further improve the generalization of global features. Our method achieves state-of-the-art performance under most DG ReID settings. The code is available at https://github.com/liyuke65535/
Part-Aware-Transformer.
Figure 1. (a) We applied different Transformers to DG ReID. Mod-els are trained on Market and tested on MSMT. Results show Vi-sion transformers (blue bars) are better than CNNs (orange bars) even with fewer parameters. (b) Visualization of attention maps of
“class token” on source domain (MSMT) and target domain (Mar-ket). We use ViT [4] as the backbone and fuse the attention results of the shallow layers. However, the attention to discriminative in-formation is still limited on target domain. 1.

Introduction
Person Re-Identification (ReID) [35, 2, 38, 37] aims to find persons with the same identity from multiple disjoint cameras. Thanks to the great success of Convolutional Neu-ral Network (CNN) in the field of computer vision [11, 24], supervised, unsupervised person ReID has made significant progress. However, a more challenging task, domain gen-eralization (DG) ReID [31] which trains a model on source domains yet generalizes well on unseen target domains, still lags far behind the performance of the supervised ReID.
*Jingkuan Song is the corresponding author.
Thus, many DG methods are proposed to learn generic features. These methods explore the generalization of CNN based on disentanglement [10, 19] or meta-learning [3, 18].
Recently, Transformer has gained increasing attention in computer vision.
It is a neural network based on atten-tion mechanisms [26]. Vision Transformer usually yields better generalization ability than common CNN networks under distribution shift[32, 17]. However, existing pure transformer-based ReID models are only used in supervised and pre-trained ReID [16, 8]. The generalization of Trans-former is still unknown in DG ReID.
To investigate the performance of Transformer in DG
cares about the visual similarity of the parts regardless of the
ID labels, thus alleviating the side effect of domain-specific biases.
Part-guided Self-Distillation (PSD) is proposed to fur-ther improve the generalization of the global representa-tion. Self-distillation has been proven effective in DG im-age classification [27, 33]. It can learn visual similarities beyond hard labels and make the model converges easier to the flat minima. However, we experimentally find that the traditional self-distillation method would reduce the gener-alization in DG ReID. The reason is that ReID is a fine-grained retrieval task, and the difference between different categories is not significant.
It is difficult to mine useful information from the classification results. Therefore, PSD uses the results of CSL to construct soft labels for global representation. In general, the motivation of self-distillation is similar to CSL, which is to learn generic features by data themselves regardless of the ID labels.
Extensive experiments have proved that CSL and PSD can improve the generalization of the model.
Specifi-cally, our method achieves state-of-the-art performance un-der most DG ReID settings, especially when using small source datasets. Under the Market→CUHK-NP setting, our method exceeds state-of-the-art by 3.2% and 4.6% in Rank1 and mAP, respectively. The contributions of this work are three-fold: a) We propose a pure Transformer-based framework for
DG ReID for the first time. Specifically, we design a proxy task, named Cross-ID Similarity Learning module (CSL), to learn generic features. b) We design part-guided self-distillation (PSD) for DG
ReID, which learns visual similarities beyond hard labels to further improve the generalization. c) Extensive experiments have proved that our Part-aware Transformer achieves state-of-the-art of DG ReID. 2.