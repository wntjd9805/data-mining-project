Abstract regular video benchmarks.
In this paper, we show that recent advances in video rep-resentation learning and pre-trained vision-language mod-els allow for substantial improvements in self-supervised video object localization. We propose a method that first localizes objects in videos via a slot attention approach and then assigns text to the obtained slots. The latter is achieved by an unsupervised way to read localized semantic informa-tion from the pre-trained CLIP model. The resulting video object localization is entirely unsupervised apart from the implicit annotation contained in CLIP, and it is effectively the first unsupervised approach that yields good results on
∗ Equal contribution. Ke Fan is the first intern author, Zechen Bai is the first FTE author, they contributed equally. Work down during Ke Fan’s internship in AWS Shanghai AI Lab
† Corresponding authors. Yanwei Fu and Ke Fan are with the School of Data Science, Fudan University. 1.

Introduction
Deep learning has demonstrated remarkable perfor-mance in object-detection and -recognition, both on images and videos. Most models are trained via supervised learn-ing, which requires expensive manual annotation of train-ing data. Such approaches also run the risk of overfitting to specific distributional characteristics of the training data, and may not generalize well beyond that. Self-supervised learning methods are a promising approach to overcome this problem. They can learn from vast amounts of unlabeled vi-sual data and unlock the full potential of deep learning, for example in video understanding.
Recently, a lot of progress was made in training seg-mentation models without supervision. Caron et al. [2]
demonstrated that vision transformers, combined with self-supervised learning, can create a feature space in which pat-terns of the same object class cluster. This feature space was used in a series of papers to learn unsupervised seg-mentation models [14, 26, 19]. In another line of research,
Locatello et al. [13] proposed an approach, where so-called slots localize individual objects in the scene and represent their visual patterns and properties. This type of approach belongs to the field of object-centric learning (OCL). So far, much of the OCL literature has focused on static images.
Only recently it became possible to obtain good results not only on synthetic but also on real-world datasets [19]. Few papers have tried to apply a slot-based OCL approach to video data [11, 6]. These works scale to real-world data by adding some form of weak annotation.
In contrast, this paper proposes an OCL pipeline for real-world video data with two main goals: (1) partitioning the videos into spatially, temporally, and semantically meaning-ful groups without using any labeled training data; and (2) labeling those groups using an off-the-shelf vision-language model, such as CLIP [17]. Since CLIP was trained to align text with global image features, it is initially unable to align text with local features from individual objects. However,
CLIP can be fine-tuned on image-text data to make it align the local patterns to text [15]. In this paper, we show that paired image-text data is not necessary to allow for such alignment. We propose a self-supervised objective to fine-tune the last layer of the CLIP vision transformer, which trains on image data only.
Overview. The overall framework is composed of three parts, as depicted in Figure 2. The individual process-ing stages are visualized in Figure 1. The first part yields bottom-up object-centric video representations (slots) that localize the regions in the video with a spatio-temporal grouping model (Section 3). The second part adapts the off-the-shelf image-text alignment model CLIP [17] to assign a text to the video slots (Section 4). Finally, a merging pro-cedure leverages overlapping information from the text and the image to improve both localization and labeling (Sec-tion 5). Our contributions are twofold:
• We provide the first approach that localizes objects with spatio-temporal consistency in real-world videos, without labelled training data.
• We assign text labels to video slots using a pre-trained
CLIP model without additional supervised fine-tuning. 2.