Abstract
Knowledge distillation (KD) has been widely explored in unsupervised anomaly detection (AD). The student is as-sumed to constantly produce representations of typical pat-terns within trained data, named “normality”, and the rep-resentation discrepancy between the teacher and student model is identified as anomalies. However, it suffers from the “normality forgetting” issue. Trained on anomaly-free data, the student still well reconstructs anomalous repre-sentations for anomalies and is sensitive to fine patterns in normal data, which also appear in training. To mitigate this issue, we introduce a novel Memory-guided Knowledge-Distillation (MemKD) framework that adaptively modu-lates the normality of student features in detecting anoma-lies. Specifically, we first propose a normality recall mem-ory (NR Memory) to strengthen the normality of student-generated features by recalling the stored normal informa-tion. In this sense, representations will not present anoma-lies and fine patterns will be well described. Subsequently, we employ a normality embedding learning strategy to pro-mote information learning for the NR Memory. It constructs a normal exemplar set so that the NR Memory can memorize prior knowledge in anomaly-free data and later recall them from the query feature. Consequently, comprehensive ex-periments demonstrate that the proposed MemKD achieves promising results on five benchmarks. 1.

Introduction
Anomaly detection (AD) has attracted increasing atten-tion in recent years for its wide applications, to name a few, defect detection [23], medical diagnosis [34], and video surveillance [10]. The lack of anomalous samples makes it more challenging and it is usually formulated as an unsu-pervised learning problem, only relying on normal data.
*Equal contribution. Work done when Zhihao Gu is an intern at CATL. (cid:0) : ellery-holmes@sjtu.edu.cn.
†Corresponding author.
Figure 1. Left: Current knowledge distillation based paradigms.
Right: Our method adopts the memory to strengthen the normality of student-generated features and learns the normal information via the normality embedding learning strategy (dotted lines).
Since anomalous data are unavailable in the training phase, a straightforward choice is to compare the normal data with the given target. To achieve this, memory bank based techniques [6, 24, 32] are proposed. They exploit the memory bank to store normal representations extracted by the ImageNet [9] pre-trained network. These features are then used to measure the normality distribution and out-liers are considered to be anomalous. However, to compute anomaly maps, they need to search the entire memory bank through complex formulations, which increases the compu-tational complexity as the size of the training set grows.
Reconstruction based methods [17, 11, 19] are conceptu-ally simple and have been extensively explored for the task.
It is expected that the reconstruction error of normal sam-ples is lower than that of abnormal samples and anomalies can be detected by thresholding the difference between the input and the retrieved one. Nevertheless, they may fail to reconstruct subtle details from latent representations, result-ing in large reconstruction errors for anomaly-free data.
Recent efforts tend to explore the Knowledge Distilla-tion (KD) [13] and instead detect the anomaly at the feature level (left part in Fig. 1) for unsupervised AD. The student is assumed to constantly produce presentations of typical pat-egy enables the NR memory to memorize the normality and further integrate it into the query feature by dealing with relevant information. Comprehensive experiments and vi-sualization results validate the effectiveness of the proposed
MemKD. In summary, the main contributions are threefold:
• We identify the “normality forgetting” issue of the stu-dent in knowledge distillation based anomaly detec-tors, and propose a novel Memory-guided Knowledge
Distillation framework to address it.
• We design the NR Memory to recall normal informa-tion for strengthening the feature normality in the stu-dent network. Besides, we also devise a normality em-bedding learning strategy to promote the memorization of normal information from anomaly-free data,
• The proposed method outperforms its state-of-the-art competitors on five widely used benchmarks, and ex-tensive experiments further validate its effectiveness. 2.