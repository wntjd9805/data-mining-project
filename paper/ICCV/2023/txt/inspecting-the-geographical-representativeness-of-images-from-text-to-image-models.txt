Abstract
Recent progress in generative models has resulted in models that produce both realistic as well as relevant im-ages for most textual inputs. These models are being used to generate millions of images everyday, and hold the po-tential to drastically impact areas such as generative art, digital marketing and data augmentation. Given their out-sized impact, it is important to ensure that the generated content reflects the artifacts and surroundings across the globe, rather than over-representing certain parts of the world. In this paper, we measure the geographical repre-sentativeness of common nouns (e.g., a house) generated through DALL·E 2 and Stable Diffusion models using a crowdsourced study comprising 540 participants across 27 countries. For deliberately underspecified inputs without country names, the generated images most reflect the sur-roundings of the United States followed by India, and the top generations rarely reflect surroundings from all other countries (average score less than 3 out of 5). Specifying the country names in the input increases the representativeness by 1.44 points on average on a 5 − point Likert scale for
DALL·E 2 and 0.75 for Stable Diffusion, however, the over-all scores for many countries still remain low, highlighting the need for future models to be more geographically inclu-sive. Lastly, we examine the feasibility of quantifying the geographical representativeness of generated images with-out conducting user studies.1 1.

Introduction
Over the last year, the quality of text-to-image genera-tion systems has remarkably improved [32, 61, 35, 37]. The generated images are more realistic and relevant to the tex-tual input. This progress in text-to-image synthesis is partly fueled by the sheer scale of models and datasets used to train 1The generated images and human ratings for each country are available at https://github.com/val-iisc/Geographical_
Representativeness.
Figure 1. An illustrative question from our study, where a partic-ipant (in this case, from India) is presented with an image of a common noun (a wedding), generated from the Stable Diffusion model. The participant is asked to rate the generated image on how well it reflects the weddings in their surroundings. them, and partly by the architectural advancements includ-ing Transformers [54] and Diffusion models [20]. Given the impressive generation capabilities that these models dis-play, such models have captured the interest of researchers and general public alike. For instance, DALL·E 2 is being used by over 1.5 million users to generate more than 2 mil-lion images per day for applications including art creation, image editing, digital marketing and data augmentation [1].
Despite the broad appeal of text-to-image models, there are looming concerns about how these models may exhibit and amplify existing societal biases. These concerns stem
from the fact that image generation models are trained on large swaths of image-caption pairs mined from the inter-net, which is known to be rife with toxic, stereotyping, and biased content. Further, internet access itself is unequally distributed, leading to underrepresentation and exclusion of voices from developing and poor nations [6, 2].
There exists a wide body of work demonstrating biases in large language and vision models [19, 57, 36, 48], and some recent work investigates text-to-image models for bi-ases related to representation of race, gender and occupa-tion [8, 4]. Another important—and often overlooked— aspect of inclusive representation is geographical represen-tation. For such systems to be geographically representa-tive, they should generate images that represent the objects and surroundings of different nations in the world, and re-frain from overrepresenting certain nations and contribut-ing to their hegemony. For instance, a typical house in the
United States looks different from one in Japan. Often the input descriptions to text-to-image models are underspec-ified, leaving the models to fill in the missing details. In such underspecified descriptions, there is an increasing risk that models overrepresent certain demographics [21]. In ad-dition to representational harms, biased image generation systems can also cause allocational harms as such systems are used to augment datasets, which run the risk of further propagating existing biases. Further, the experience of us-ing systems that underrepresent certain areas would likely be unpleasant for the residents of those regions.
In this paper, we measure the degree to which the text-to-image-generation systems produce images that reflect the artifacts and surroundings of participants from different parts of the world (§2). To answer this question, we con-duct a user study involving 540 participants from 27 differ-ent countries. We present each user 80 images of common nouns generated from DALL·E 2 [32] and Stable Diffusion (v 1.4) [35] models. Half of the presented images are gener-ated by specifying the country of the participant in the input, and the remaining images are deliberately underspecified to examine the default generations. The users evaluate the presented images based on a 5-point Likert scale indicating how well do the generated images reflect the given entity in their physical surroundings (see Figure 1). We also ask respondents to score generated images on (i) how realistic they look, and (ii) how the realism impacted their scores about geographical representativeness.
Overall, we find that the geographical representativeness of images for many countries is considerably low (§3). In the unspecified case, i.e., without any country name in the input, we find that the generated images most reflect ar-tifacts from the United States (average geographical rep-resentativeness score of 3.35 out of 5), followed by India (score of 3.23) and Canada (score of 2.82), and least re-flect the nouns from Greece, Japan and New Zealand (with scores less than or around 2.0). Out of 27 countries, 25 countries have a score of less than 3 for both DALL·E 2 and Stable Diffusion models. When we specify the coun-try name in the input prompt, the average score over all the studied countries increases to 3.49 (from 2.39 in the unspec-ified case). However, these scores suggest that there is room for future text-to-image models to produce more geographi-cally representative content. Between DALL·E 2 and Stable
Diffusion, we find DALL·E 2 to be better at generating geo-graphically representative content when we specify country names, but we observe no statistically significant difference in the underspecified case.2 We find that the participants’ ratings about the realism of the images are correlated with their scores about the geographical representativeness.
Finally, we examine the feasibility of automating the process of quantifying the geographical representativeness of text-to-image generation models through two different ways (§4). First, we consider the similarity of a country-specific textual prompt and the test image using CLIP, a pre-trained text-image alignment model [29]. Second, we eval-uate the viability of using user annotations for DALL·E 2 as a means for estimating the geographical representativeness for images generated through Stable Diffusion. We find both these approaches to be inadequate in accurately eval-uating the geographical representativeness of the images, emphasizing the need for a user study. We conclude with a discussion on limitations of our work, and suggestions for future research in this area (§5). 2. Approach
Geographical Representativeness. We present crowd-workers from different countries with several model-generated images of common nouns, and for each image, we ask them to rate on a scale of 1-5 about how well do the generated images reflect their surroundings. Geograph-ical representativeness (GR) of the model m for country c, is then defined as the average rating participants from that country provide to the model generated images of common nouns (N ), using a corresponding set of input prompts (P).
Similarly, we define the realism, R(c, m, p), as the average of realism ratings given by participants from country c to images generated by model m using a prompt p.
Research Questions. Using the above notions of geo-graphical representativeness and realism, we ask:
• RQ1: Are the images generated using DALL·E 2 and
Stable Diffusion geographically representative? Do they over-represent rich or populous nations? 2Note that the scope and focus of our study is solely on measuring the extent of geographical representativeness for both country-specified and unspecified prompts, rather than finding better ways to prompt the model, or improve the model to produce more geographically inclusive content.
• RQ2: To what extent does specifying the country name in the input improve the representativeness?
• RQ3: Does the realism of images impact participants’ ratings about the geographical representativeness?
• RQ4: How feasible is it to automatically assess the geographical representativeness of generated images?
Selected Countries. We reach out to residents of 88 coun-tries using Amazon Mechanical Turk (AMT)3 and Prolific4 crowdsourcing platforms. However, a large majority of crowdworkers belong to only a few countries, and we even-tually end up with sufficient responses only from 27 coun-tries. We sample the 88 countries using weighted random sampling where each nation was weighted by its population.
The final set of 27 countries (denoted by C) includes: the
United States of America, Canada, Mexico, Brazil, Chile, the United Kingdom, Italy, Spain, Greece, Japan, Korea, In-dia, Israel, Australia, South Africa, Belgium, Poland, Portu-gal, Germany, France, Latvia, Hungary, the Czech Repub-lic, Estonia, New Zealand, Finland, and Slovenia.
Chosen Artifacts. To curate a list of diverse but common artifacts, we extract the most common nouns from the popu-lar Conceptual Captions dataset [43] which contains image-caption pairs, used for training various vision+language systems [23, 50, 22, 33]. We use a POS-tagger from the
NLTK library to extract the nouns, and sort them by de-creasing order of their frequency. We choose the 10 most common nouns after manually excluding nouns that are uni-versal in nature (e.g., sky, sun). The final list of 10 common nouns, denoted by N , includes city, beach, house, festival, road, dress, flag, park, wedding, and kitchen.
Input Prompts. As mentioned earlier, we use two types of queries for image synthesis. For half of the queries, we include the country name, and for the remaining half, we do not specify any country name (to assess the default genera-tions). When specifying the country name, we modify the query to “high definition image of a typical [artifact] in [country]”, where we include the word typical to gen-erate the most common form of the concept in the specified country. We denote such queries by pc, where c refers to the country name in question. For the underspecified case, our query is “high definition image of a [artifact]”, which we denote by p. We use the same prompt for both
DALL·E 2 and Stable Diffusion models. 3https://www.mturk.com 4https://www.prolific.co
Figure 2. Agreement among participants. We plot the percent-age of participants from each country that choose the most com-mon option (for that country). We see that there is a considerable agreement among respondents, as about half the participants in many countries agree on one out of five options.
Questionnaire Details. For each of the 10 nouns, we gen-erate 8 images, 4 using DALL·E 2 and 4 from Stable Dif-fusion. Overall for a given country, our survey comprises 80 images. Participants are not privy to the details of the models, and do not know which images were generated from which model. For each image, we ask each partici-pant: “How well does the automatically generated image of this [artifact] reflect the [artifact] in your sur-roundings in [country]?”. (See Figure 1). For each question, participants mark their responses using a 5-point
Likert scale, where where 1 indicates “not at all”, and 5 rep-resents “to a great extent”. After the 80 questions, we ask the users to rate the photo-realism of the generated images on a scale of 1-5, and how it impacted their scores about geographical representativeness. We pay AMT participants based on the estimated hourly income of crowdworkers in their respective countries. For participants from Prolific, we pay them a platform-set minimum of 6.91 USD per hour.
Validating Responses. To verify if the participants an-swered the questions earnestly, we include 4 trick questions which are presented in the same format. Two of these trick questions inquire about apples and milk, whereas the cor-responding images are of mangoes and water. Therefore, we expect participants to mark a low score for these two questions. For the other two trick questions, we ask about
a pen and sun, and include images of the same, and expect the users to mark a high score. We discard the responses from participants who do not pass these checks. While the crowdsourcing platforms allow us to target users from a given country, we re-confirm with participants if they in-deed reside (or have lived) in the specified countries.
Inter-rater Agreement. We compute (for each country) the percentage of participants who opted for the most se-lected option. We observe a high agreement among partic-ipants; for 19 out of the 27 studied countries we see that the most common option is picked by over 50% of the re-spondents (Figure 2). The agreement would be (on an av-erage) 20% if participants marked options arbitrarily. The percentages in Figure 2 demonstrate some degree of con-sensus among participants. Further, we observe the highest agreement for images of flags (81%) and the least agree-ment for kitchens (41%). 3. Results
In this section, we share the findings of our study. First, we discuss the metrics of interest, and then answer the four research questions posed in Section 2. 3.1. Metrics
Below, we define a few notations that we use for eval-uating the user ratings. Remember from Section 2 that we defined GR(c, m, n, p) as the geographical representative-ness score assigned by participants from country c to images generated for noun n from model m using prompt p.
• GR(c, m, ·, pc): Average ratings that participants of a country c assign for geographical representativeness of images generated by model m across all nouns in N .
Here, we use a country-specific prompt (pc).
• GR(c, m, ·, p): Average ratings that participants of a country c assign for geographical representativeness of images generated by model m across all nouns. The prompt p does not specify the country name.
• GR(·, m, n, pc): Average ratings that participants for all countries in C assign for geographical representa-tiveness (GR) of images of noun n, generated by model m. Here, we use a country-specific prompt (pc).
• GR(·, m, n, p): Average ratings that participants from all countries in C assign for geographical representa-tiveness of images of noun n, generated by model m.
The prompt p does not specify the country name.
Analogously, we define R(c, m, n, pc) and R(c, m, n, p) as the average realism score for generated images using country specific (pc) and unspecific prompt (p) respectively. 3.2. Geographical Representativeness
Here, we elaborate on the extent to which the generated artifacts are geographically representative (RQ1 in Section 2). We compute the geographical representativeness scores for each country, averaged over the two models for the im-ages generated by prompts that do not specify the country name, i.e., GR(c, ·, ·, p). We present these results in Table 1.
From the table, we can see that out of the 27 countries, 25 have a score lower than 3 (on a scale of 1 to 5), indicating that participants from most of the studied countries do not feel that the generated images reflect their surroundings to a large extent. The only countries to obtain scores higher than 3 are the United States (3.35) and India (3.23).
In-terestingly, for DALL·E 2, India obtains the highest score (3.44) followed by the United States (3.24). The overall least scores are assigned by participants from Greece (1.94),
Japan (1.95) and Finland (2.03). The average score across the studied 27 countries is 2.39.
To answer the follow up questions posed in the RQ1, about whether the artifacts generated are more representa-tive of richer and populous nations: 1. We find no correlation between the degree of geo-graphical representativeness of the generated images for the studied countries and their per-capita GDP. The
Pearson correlation coefficient, ρ, is −0.03. More-over, after separating the country pool into the “Rich
West” countries5 and others, we evaluate if average
GR scores of the two groups are different, but we find no statistically significant difference. We acknowledge and speculate that we may observe different trends if the study included participants from many other devel-oping countries. However, significantly improving the coverage of the study is challenging (see Section 5). 2. We observe that the geographical representativeness scores of the 27 countries is positively correlated with their population (ρ = 0.64). This may suggest that the datasets used to pre-train the chosen models contain many images from residents of populous countries. 3.3. Effect of Country-specific Prompts
In this subsection, we analyse the geographical repre-sentativeness of images generated by including the country name (RQ2 in Section 2). From Table 1, we observe that for each nation, mentioning its name in the prompt increases the average GR score for that country as compared to the under-specified case. We conduct a paired sample t-test to confirm this, and find that indeed there is a statistically sig-nificant increase with p-value < 0.05. Specifically, adding the country name in the textual query increases the average geographical representativeness score by over 1.44 points 5As defined per: https://worldpopulationreview.com/ country-rankings/western-countries
Table 1. Geographic Representativeness. We tabulate the geographical representativeness scores for DALL·E 2 (D2), Stable Diffusion (SD) and combination of both the models (Overall) for different countries, both for the case when the model is prompted using the country name and without it. In the unspecified case, we observe that the scores is highest for United States, followed by India (scores greater than 3.2 out of 5), but low for many other countries. We observe a consistent improvement in the scores when we include the country names.
Overall
DALL·E 2
Stable Diffusion
Countries
US
India
Canada
South Africa
Brazil
UK
Mexico
Spain
Portugal
Italy
Belgium
France
Poland
Germany
Australia
Czech Republic
Hungary
New Zealand
Estonia
Slovenia
Chile
Israel
South Korea
Latvia
Finland
Japan
Greece
Average w/ country
GR(c, ·, ·, p) GR(c, ·, ·, pc) GR(c, D2, ·, p) GR(c, D2, ·, pc) GR(c, SD, ·, p) GR(c, SD, ·, pc)
Unspecified
Unspecified
Unspecified w/ country w/ country 3.54 ±0.23 3.74 ±0.26 3.62 ±0.40 3.25 ±0.30 3.70 ±0.26 3.82 ±0.38 3.83 ±0.26 3.44 ±0.29 3.73 ±0.29 3.58 ±0.47 3.49 ±0.43 3.32 ±0.34 3.62 ±0.30 3.64 ±0.35 3.35 ±0.45 3.43 ±0.48 3.41 ±0.49 3.10 ±0.44 3.36 ±0.25 3.29 ±0.46 3.12 ±0.40 3.14 ±0.39 3.49 ±0.24 3.52 ±0.34 3.62 ±0.30 3.55 ±0.32 3.43 ±0.46 3.49 ±0.06 3.35 ±0.18 3.24 ±0.41 2.82 ±0.51 2.74 ±0.40 2.69 ±0.55 2.65 ±0.49 2.59 ±0.56 2.46 ±0.44 2.46 ±0.47 2.40 ±0.49 2.40 ±0.52 2.34 ±0.44 2.29 ±0.44 2.26 ±0.45 2.26 ±0.46 2.25 ±0.52 2.24 ±0.55 2.23 ±0.36 2.22 ±0.33 2.21 ±0.43 2.15 ±0.42 2.15 ±0.49 2.10 ±0.39 2.10 ±0.49 2.03 ±0.34 1.95 ±0.40 1.94 ±0.49 2.39 ±0.08 3.56 ±0.29 4.00 ±0.22 3.78 ±0.55 3.49 ±0.57 4.00 ±0.38 4.14 ±0.53 4.18 ±0.30 3.62 ±0.38 4.02 ±0.40 3.66 ±0.66 3.76 ±0.71 3.54 ±0.67 4.14 ±0.39 4.03 ±0.30 3.55 ±0.74 3.68 ±0.50 3.65 ±0.59 3.10 ±0.76 3.89 ±0.58 3.48 ±0.49 3.62 ±0.64 3.62 ±0.67 3.92 ±0.45 4.11 ±0.44 3.93 ±0.54 3.97 ±0.37 3.65 ±0.48 3.78 ±0.09 3.24 ±0.27 3.44 ±0.49 2.73 ±0.59 2.70 ±0.44 2.65 ±0.78 2.41 ±0.61 2.74 ±0.72 2.29 ±0.65 2.47 ±0.73 2.40 ±0.70 2.28 ±0.57 2.38 ±0.70 2.23 ±0.59 2.04 ±0.46 2.10 ±0.49 2.18 ±0.64 2.06 ±0.52 2.24 ±0.70 2.18 ±0.51 2.19 ±0.45 2.26 ±0.69 2.10 ±0.67 2.24 ±0.66 1.87 ±0.52 1.95 ±0.44 1.97 ±0.49 1.92 ±0.66 2.34 ±0.10 3.51 ±0.25 3.48 ±0.49 3.47 ±0.52 3.02 ±0.52 3.40 ±0.23 3.48 ±0.56 3.49 ±0.57 3.26 ±0.53 3.44 ±0.61 3.50 ±0.66 3.21 ±0.61 3.09 ±0.47 3.10 ±0.66 3.26 ±0.70 3.15 ±0.66 3.18 ±0.79 3.18 ±0.74 3.11 ±0.49 2.84 ±0.49 3.10 ±0.69 2.62 ±0.57 2.66 ±0.59 3.06 ±0.47 2.93 ±0.46 3.30 ±0.59 3.13 ±0.59 3.22 ±0.69 3.19 ±0.10 3.46 ±0.27 3.03 ±0.41 2.91 ±0.66 2.78 ±0.58 2.72 ±0.56 2.88 ±0.80 2.45 ±0.64 2.63 ±0.66 2.45 ±0.54 2.39 ±0.63 2.52 ±0.80 2.30 ±0.52 2.35 ±0.70 2.49 ±0.78 2.41 ±0.65 2.31 ±0.66 2.42 ±0.76 2.22 ±0.39 2.26 ±0.49 2.23 ±0.65 2.04 ±0.58 2.19 ±0.64 1.96 ±0.44 2.32 ±0.67 2.10 ±0.48 1.94 ±0.48 1.97 ±0.48 2.44 ±0.10 for DALL·E 2 and 0.75 for Stable Diffusion. Overall, for 14 out of 27 countries (despite the increase upon includ-ing country names), the geographical representation scores were between 3 to 3.5, indicating a considerable headroom for future models to generate more representative artifacts.
We show illustrative examples of images generated by the unspecified and country-specific prompts in Figure 3.
Specifically, we show images for 5 countries: Brazil, Mex-ico, Italy, Japan and South Korea, and 4 nouns: house, city, flag and wedding. The images generated by DALL·E 2 are surrounded by green boxes, whereas those generated by Stable Diffusion are surrounded by yellow boxes. For each of the nouns, we show images generated by the under-specified prompts first, followed by the ones generated through country specific prompts.
In the supplementary material, we show images generated separately by both
DALL·E 2 [32] and Stable Diffusion [35] for all the 10 nouns, whereas we choose one country from each continent:
US, Chile, UK, Japan, South Africa, and Australia. 3.4. Photo-realism of Generated Images
We seek to answer if, and to what degree, does the photo-realism of images impact participants’ perceptions of geographical representativeness of a given artifact (RQ3 in Section 2). We believe that there may be an effect, as unrealistic-looking images might be perceived less ge-ographically appropriate (in the extreme case, unrealistic-looking photos might be hard to even interpret). To an-swer this question, we ask participants to rate the realism of images generated by DALL·E 2 and Stable Diffusion re-spectively (for both the under-specified and country-specific prompts) on a Likert-scale of 1 to 5. Additionally, in the exit
Figure 3. Qualitative examples of images of four common nouns generated by DALL·E 2 (images surrounded by green boxes) and Stable
Diffusion models (images surrounded by yellow boxes). Through these examples and others, we see that the default generations often reflect artifacts from US and Canada. For example, the average score (in unspecified case) for the images of houses generated through
DALL·E 2 is 3.95 for US and Canada, and 2.09 for the remaining countries. survey, we ask participants to self assess the impact that the realism of images had on the scores they assigned for geo-graphical representativeness of images.
First, we find that geographical representativeness and realism scores are correlated, with a Pearson correlation of 0.62 for Stable Diffusion (unspecified case), and 0.47 for the case with country names. For DALL·E 2 the correlation is not as large (0.21 and 0.57 for unspecified and country-specific prompts respectively). This is also concordant with the self-evaluation provided by participants, where we note that participants, on average, indicate that the realism in-fluenced their ratings on geographical representativeness to a moderate extent (average score of 3.5 on a scale of 1-5).
Interestingly, we find that that the average realism score as-signed by participants is lower (averaged over all countries) when the prompt excludes the country name (this differ-ence is statistically significant with p value < 0.05). Albeit, we do see that for some countries, e.g., the United States and Brazil, the realism scores decreases upon including the country names in the prompt. More details and countrywise statistics on the realism values (for all 27 countries) can be found in the supplementary material. 3.5. Comparison of DALL·E 2 and Stable Diffusion
We compare DALL·E 2 vs Stable Diffusion models to see which model produces more geographically represen-tative images (Figure 4). We find that (i) for country-specific prompts, the geographical representativeness of im-Figure 4. DALL-E 2 vs Stable Diffusion: Average geographical representativeness scores for images generated by DALL·E 2 and
Stable Diffusion, with and without country-specific prompts. ages generated through DALL·E 2 are higher than those from Stable Diffusion by about 0.6 points (and this differ-ence is statistically significant as per a paired t-test with a p-value < 0.05); and (ii) for country agnostic prompts, the differences are not statistically significant (see Figure 4). 4. Feasibility of Automating the Evaluation
Evaluating geographical representativeness of text-to-image models through user studies is labor intensive, expen-sive and not easily reusable (for future models). It would be ideal to automatically quantify the geographical representa-tiveness of unseen test images. In this section, we analyse
the feasibility of such automatic evaluation (RQ4 in Sec-tion 2). Particularly, we explore automatically estimating the geographical representativeness using two different ap-proaches: (i) using CLIP (a text-image alignment model) to obtain the similarity between the country-specific textual prompt and the test image; and (b) using the similarity of the test image to already annotated images, i.e., via a k-nearest neighbor model. We elaborate these schemes below: 4.1. CLIP-based Similarity
One of the common techniques used to automatically quantify biases in the text-to-image models is to use CLIP-based similarity as a proxy [29]. For instance, CLIP sim-ilarity scores have been previously used to evaluate gen-der, racial, ethnic and cultural biases in text-to-image mod-els [8, 3, 49]. Further, it has also been used to evaluate cross-lingual coverage of a concept in text-to-image mod-els [39]. To assess if the CLIP model could be a useful tool for automatically estimating the geographical represen-tativeness scores for a given country-noun pair, we use it to obtain the un-normalized similarity score between the im-age and a query of the form “high definition image of a typ-ical [noun] in [country]”, and compare it to the geo-graphical representativeness score assigned by participants from our study. We evaluate if we could reach the same findings (as in §3) by using the CLIP similarity scores.
Results.
Overall, we find that the images generated through country-specific prompts have higher CLIP-based similarity scores than those generated by country-agnostic prompts (p-value < 0.001), for both DALL·E 2 and Stable
Diffusion. Of all the cases where DALL·E 2 images gen-erated using country-specific prompts have a higher score than images generated without country names, 98.7% of the times the CLIP similarity scores are also higher. For the Stable Diffusion model, the corresponding percentage is 96.4%. These high-level findings are consistent with the user study. However, when we compare the scores of
DALL·E 2 and Stable Diffusion models, CLIP-based sim-ilarity suggests that there is no statistically significant dif-ference in the geographical representativeness of images generated with country name, which contradicts the re-sults from the participants (they find images generated from
DALL·E 2 with country-specific prompts to be more geo-graphically representative than ones from Stable Diffusion).
Moreover, for images generated without the country name, the CLIP similarity scores are higher for Stable Diffusion than DALL·E 2 unlike the human ratings, for which there is no statistically significant difference.
Next, we study if we could obtain finer-grained findings similar to what we observe through a human study. We first compute the Pearson’s correlation coefficient, ρ, be-tween country-wise geographical representativeness scores and CLIP similarity scores. We find no correlation across all nouns for images generated with country names (ρ = 0.01), and weak correlation for images with country-agnostic prompts (ρ = 0.34). Further, we curate a benchmark com-prising pairs of images, and evaluate how often do human preferences (about which of the two images is more ge-ographically representative) match with the one selected through CLIP-based similarity. We note that the agreement is merely 52.4% (random chance agreement is 50%). These results indicate that the CLIP-based similarity is an inade-quate proxy for the geographical representativeness. 4.2. Estimation using Nearest Neighbors
We further explore the viability of estimating the geo-graphical representativeness of a given test image (possibly generated by a future text-to-image generation model) us-ing the existing ratings collected for images from DALL·E 2 and Stable Diffusion. For a test image X T n of a given noun n, we define X c n as the set of images of n annotated by participants of country c. Since a given image may be re-flective of surroundings in multiple countries, we attempt to estimate the GR scores corresponding to all the studied countries. For X T n , we find its k nearest neighbors by ex-n and the images in X c tracting the feature vectors of X T n from the vision model used by CLIP, and then computing the cosine similarities between the corresponding features.
The predicted GR score of X T n for country c is the average of the human ratings corresponding to the obtained near-est neighbors. Specifically, we use the participant ratings of
DALL·E 2 as the training data and those of Stable Diffusion for testing. Therefore, for noun n and country c, |X c n| = 4, as we have 4 annotated images per noun for a given coun-try, 2 generated with country-specific prompts, the other 2 generated without the country-specific prompts. For ex-ample, to evaluate the GR score of an image of a house in India generated by Stable Diffusion, we find its k near-est neighbors among the images that are generated through
DALL·E 2 and annotated by Indians. The estimated score is then compared to the true ratings of Indian participants.
Results. Given that |X c n| = 4, we set k = 1 for all our ex-periments. We find that the average correlation coefficient, the correlation between the human marked scores and the estimated scores is moderate (ρ = 0.46) over all the coun-tries in the unspecified case, however, we find no correlation (ρ = 0.01) in the case of country-specific prompts. Fur-ther, the mean squared error (MSE) between the human and estimated scores is 1.39 for images with country-agnostic prompts and 1.56 for images with country-specific prompts.
As a reference, we also check the MSE for a baseline value of 3.0 for all the test images across all countries (as 3 falls in the middle of 1-5 scale). For this reference, the MSE is 1.18 for unspecified case and 0.83 for country specific case— both these error values are lower than the corresponding val-ues obtained using the estimates from the k nearest neighbor
Table 2. Evaluating the estimated geographical representativeness using k-nearest neighbor approach. We find the the Mean Squred
Errors (MSE) for all the feature extractors are too high to be useful.
Approach w/o country w/ country
Reference (= 3.0)
Feature extractors:
VGG16 [46]
ResNet18 [18]
ResNet50 [18]
ViT [12]
CLIPVision [29] 1.18 1.55 1.67 2.04 1.81 1.38 0.83 1.52 1.77 1.62 1.51 1.56 model. These values point to the infeasibility of using this approach for automatically estimating the geographical rep-resentativeness, at least in the current form. We believe that this is partly due to the fact that we only have a few anno-tated images in the training corpus to match with. We also speculate that the image feature extractors (used for simi-larity computation) may not extract features that differenti-ate images along the geographical lines. We further present the MSE scores of the nearest neighbor method by varying the underlying pretrained feature extractor in Table 2. We note that for both the country unspecified and the country specific cases, the MSE values for the predicted GR scores with respect to all the feature extractors are higher than that of values obtained using the baseline score of 3.0. This fur-ther underscores that automatically estimating geographical representativeness of images is challenging.
Both the investigated approaches for estimating geo-graphical representativeness turn out to be inadequate. We are able to reach similar high-level conclusions using CLIP-based similarity, but the similarity scores contradict finer-grained findings. Overall, it is fundamentally challenging to automatically estimate the representativeness of images. 5. Limitations & Future Directions
There are several important limitations of our work. De-spite our efforts to reach out to participants from 88 coun-tries, we received sufficient responses from users only in 27 countries, and hence our study is limited to only 27 coun-tries. We received less than 5 responses from participants in Nepal (1), Bangladesh (2), Malaysia (2), Turkey (5), Sin-gapore (2), Argentina (1), Kenya (3), Venezuela (1), Pak-istan (1), Indonesia (2), Nigeria (2), Romania (2), Colom-bia (3), Namibia (1), and zero responses from Laos, Ar-menia, Yemen, Thailand, Vietnam, Sri Lanka, Kazakhstan,
Ukraine, Sierra Leone, Burkina Faso, Morocco, Senegal,
Philippines, Egypt, Peru, Ethiopia, Mozambique, Kyrgyz
Republic, Tanzania, Mali, Ecuador, Myanmar, Cambodia,
Russia, Andorra, Finland, Tunisia, Gabon, Angola, Alge-ria, Libya, Botswana, and Seychelles. As past surveys note, internet is not uniformly accessible across the globe [6, 2].
The lack of access disproportionately impacts marginalized and poor nations, which further limits the voice residents of marginalized countries have on the internet. Systems trained on the internet data run the risk of excluding such communities. Perhaps due to internet access issues, crowd-sourcing platforms have few (or no) participants from many developing countries, which further exacerbates inclusive development and evaluation of machine learning models (country-wise details can be found in Figure 5).
Figure 5. The number of participants available for research studies on Prolific are heavily skewed, and have few (or no) participants from many poor and developing nations. Such disparity is a seri-ous challenge for inclusive model development and evaluation.
Another weakness of our work is that we evaluate gen-erated images for only 10 common nouns. As we evaluate two different models with two different kinds of prompts and use multiple images per noun, we end up with a survey comprising 80 images per participant. Including additional nouns would have resulted in longer (or more) surveys and likely lower participation. However, we will open-source the code and required tools for future work to reproduce and extend similar studies. An interesting future direction is to examine techniques to aggregate images (for a given noun and a country) to speed and scale up the evaluation.
To improve the models, and the geographical represen-tativeness of the generated images, we believe that more work is required to better document the sources of image-text pairs in the training data so as to understand the dis-tributions of different objects and countries. For exam-ple, an interesting future work could be studying the train-ing data of Stable Diffusion, i.e. the open-source LAION-5B [40] to understand how the image-caption pairs depict-ing some objects co-occur with the different demographic regions of the world. Further, we need to collect and augment more data from the under-represented countries— there have been some past attempts at scraping more diverse image data [31]. Lastly, we call for improving the participa-tion from under-represented countries in development and evaluation of machine learning models. 6.