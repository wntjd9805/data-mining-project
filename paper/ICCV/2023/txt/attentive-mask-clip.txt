Abstract
In vision-language modeling, image token removal is an efﬁcient augmentation technique to reduce the cost of en-coding image features. The CLIP-style models, however, have been found to be negatively impacted by this tech-nique. We hypothesize that removing a large portion of image tokens may inadvertently destroy the semantic in-formation associated to a given text description, resulting in misaligned paired data in CLIP training. To address this issue, we propose an attentive token removal approach, which retains a small number of tokens that have a strong semantic correlation to the corresponding text description.
The correlation scores are dynamically evaluated through an EMA-updated vision encoder. Our method, termed at-tentive mask CLIP, outperforms original CLIP and CLIP variant with random token removal while saving the train-ing time. In addition, our approach also enables efﬁcient multi-view contrastive learning. Experimentally, by train-ing ViT-B on YFCC-15M dataset, our approach achieves 43.9% top-1 accuracy on ImageNet-1K zero-shot classi-ﬁcation, 62.7/42.1 and 38.0/23.2 I2T/T2I retrieval accu-racy on Flickr30K and MS COCO, outperforming SLIP by
+1.1%, +5.5/+0.9, and +4.4/+1.3, respectively, while be-ing 2.30× faster. An efﬁcient version of our approach runs 1.16× faster than the plain CLIP model, while achieving signiﬁcant gains of +5.3%, +11.3/+8.0, and +9.5/+4.9 on these benchmarks, respectively. Code will be release in https://github.com/microsoft/A-CLIP. 1.

Introduction
Large-scale vision-language pre-training models, such as CLIP [28] and ALIGN [16], have demonstrated remark-able capabilities in zero-shot image classiﬁcation and multi-modal retrieval. However, these models typically require a large amount of training data, which raises the training
*Equal contribution. †This work was done during internship in MSRA.
Methods
CLIP
SLIP
MaskCLIP
A-CLIP
A-CLIP-eff
Training
Time 1.00× 2.67× 1.56× 1.16× 0.86×
GPU
Memory 14G 30G 16G 14G 13G
IN 1K Flickr30K MS COCO 0-shot 37.6 42.8 42.7 43.9 42.9
I2T/T2I 27.9/17.6 33.6/21.9 34.1/21.2 38.0/23.2 37.4/22.5
I2T/T2I 51.4/32.6 57.2/41.2 60.0/38.8 62.7/42.1 62.7/40.6 1 The full training wall clock time and GPU memory footprint are mea-sured on the same device. We report the training cost relative to the original CLIP [28].
Table 1: We compare our attentive mask CLIP (A-CLIP) with CLIP [28], SLIP [27] and MaskCLIP [7]. A-CLIP out-performs CLIP by +6.3%, +11.3/+9.5 and +10.1/+5.6 on
Imagenet-1K [30] zero-shot classiﬁcation, Flickr30K [39] and MS COCO [23] I2T/T2I retrieval. An efﬁcient variant termed A-CLIP-eff outperforms CLIP by +5.3%,
+11.3/+8.0, and +9.5/+4.9 on these benchmarks, while re-ducing the training time to 0.86×.
Contrastive Loss
Contrastive Loss
Image 
Encoder
Text 
Encoder
Image 
Encoder
Text 
Encoder
“Ferrari 312T at Goodwood
Festival of Speed  2011”
“Ferrari 312T at Goodwood
Festival of Speed  2011”
Attentively Masked Image
Randomly Masked Image
Figure 1: Attentive mask vs. random mask. Left part is the attentive mask applied CLIP training process, and right part is random mask applied. Here we use ViT-B16 model from
A-CLIP’s visual encoder to generate above masked images with patch size of 32 × 32 and 25% mask ratio. The image and alt-text are sampled from YFCC-100M [32].
cost. For instance, CLIP is trained on 400 million image-text pairs, and ALIGN uses more than 1 billion paired data.
This raises the need for more efﬁcient language-image pre-training methods.
This paper aims to improve the efﬁciency of CLIP train-ing by introducing an efﬁcient image augmentation ap-proach called “image token removal”. This approach drops a large portion of image tokens, thereby reducing the com-putation in the image encoder. It has been shown to be ef-fective in masked image modeling [13, 35, 26, 15, 31] when combined with vision Transformer architectures [8]. Thus, we seek to introduce this approach into CLIP training to improve its efﬁciency.
However, previous work has shown that dropping tokens randomly can harm CLIP performance [9, 19]. This is also evidenced in our own experiments (see Table 2). Speciﬁ-cally, we observe a -2.6% top-1 accuracy drop on ImageNet zero-shot classiﬁcation when we randomly remove 50% to-kens in each image. The underlying reason for this issue is that the process of removing tokens may mistakenly elimi-nate semantic content that is pertinent to the alt-text, leading to inaccurate image-text pairs for CLIP training.
To mitigate this issue, we propose an attentive token re-moval strategy, as shown in Figure 1. The fundamental idea is to retain a small set of image tokens that are more closely related to the corresponding text description while discard-ing those that are irrelevant. Concretely, we encode all im-age tokens into a latent space and then calculate the cor-relation scores between each image-token feature and the text feature extracted by the CLIP’s text encoder. We in-vestigate several strategies for selecting image tokens using correlation scores and conclude that retaining these image tokens that are most semantically related to the text descrip-tion yields the best performance. The correlation scores are computed by using the exponential moving average (EMA) version of the vision encoder. Speciﬁcally, we use the atten-tion weights of the [CLS] token of the visual encoder as the correlation scores. We also ﬁnd that using the averaged at-tention weights of all layers performs better. Figure 4 shows the selected tokens, which correlate well with the associated text semantics.
The efﬁciency of the proposed token removal approach allows us to construct multiple masked views from an im-age while keeping the training as efﬁcient as the original
CLIP, e.g., 2 masked views with a token removal ratio of 50%. The random cropping strategy adds certain stochas-tic effects to different masked views, enabling the applica-tion of an auxiliary contrastive loss between the augmented views in addition to the plain CLIP loss. In fact, as has been shown in SLIP [27, 7], the auxiliary task facilitates the CLIP training. We consider both SimCLR [3] and SimSiam [4] methods for the auxiliary contrastive learning task.
It is worth noting that the EMA branch can be naturally treated as another view, and we also apply an online-to-EMA con-trastive or consistency loss and use the formulation from
BYOL [12] for this purpose.
The proposed approach is called A-CLIP, which intro-duces only 16% computational overhead in comparison to the plain CLIP. It is also 2.30× and 1.34× faster than pre-vious CLIP improvements that also include multiple views and additional self-supervised losses, while being more ef-fective. Using ViT-B and the YFCC-15M [32, 28] dataset, the A-CLIP framework achieves 43.9% top-1 accuracy on
ImageNet-1K [30] zero-shot classiﬁcation(see Table 1).
Additionally, it achieves 62.7/42.1 and 38.0/23.2 I2T/T2I retrieval accuracy on Flickr30K [39] and MS COCO [23], respectively, which is +1.1%, +5.5/+0.9, and +4.4/+1.3 higher than the SLIP method, and +1.2%, +2.7/+3.3, and
+3.9/+2.0 higher than the MaskCLIP method.
Also note the training cost of our approach can be fur-ther reduced by using a lower resolution input for the EMA network, i.e., 2x lower reduced resolution. This has lit-tle affect on the accuracy of correlation score computing, while marginally reduce the efﬁcacy of online-to-EMA con-trastive loss. This strategy will reduce the EMA computa-tion by more than 4×, resulting in an efﬁcient variant that is even 1.16× faster than the plain CLIP model, and is signiﬁ-cantly more accurate. We refer to this more efﬁcient variant as A-CLIP-eff. 2.