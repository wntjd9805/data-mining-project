Abstract
Most existing forecasting systems are memory-based meth-ods, which attempt to mimic human forecasting ability by employing various memory mechanisms and have progressed in temporal modeling for memory dependency.
Nevertheless, an obvious weakness of this paradigm is that it can only model limited historical dependence and can not transcend the past.
In this paper, we rethink the temporal dependence of event evolution and pro-pose a novel memory-anticipation-based paradigm to model an entire temporal structure, including the past, present, and future.
Based on this idea, we present
Memory-and-Anticipation Transformer (MAT), a memory-anticipation-based approach, to address the online action detection and anticipation tasks.
In addition, owing to the inherent superiority of MAT, it can process online action detection and anticipation tasks in a unified manner.
The proposed MAT model is tested on four challenging benchmarks TVSeries, THUMOS’14, HDD, and EPIC-Kitchens-100, for online action detection and anticipation tasks, and it significantly outperforms all existing methods.
Code is available at https://github.com/Echo0125/
Memory-and-Anticipation-Transformer. 1.

Introduction
Online anticipation [32] or detection [10] in computer vision attempt to perceive future or present states from a historical perspective. They are the crucial factors of
AI systems that engage with complex real environments and interact with other agents, e.g. wearable devices [46], human-robot interaction systems [33], and autonomous ve-hicles [62].
Human beings frequently imagine future events based on past experiences. Similarly, anticipating future events inherently requires modeling past actions or the progres-sion of events to predict what will happen next. To bridge the gap with human forecasting ability, current systems
* Equal contribution, (cid:66) Corresponding author (lutong@nju.edu.cn)
Figure 1: Examples for the forward influence of memory and backward constraint of anticipation. (a) An athlete prepares to start, starts to run, and then jumps to form the result of completing a high jump; (b) The goals of a camera wearer for cutting a tomato dictate the sequence of actions he will perform: hold the knife, get ready to cut, etc.; (c) Compared with the memory-based method, the memory-anticipation-based method is able to establish circular dependencies between memory and future. strive to mimic this cognitive ability by employing differ-ent memory mechanisms [58, 26, 7, 67, 44, 17]. Previous research [14, 44] demonstrated that modeling long temporal context is crucial for accurate anticipation. LSTR [58] fur-ther decomposes the memory encoder into long and short-term stages for online action detection and anticipation, al-lowing for the extraction of more representative memory features. Similarly, research efforts such as [67, 7, 27] have also illustrated improvements based on this principle.
However, despite various memory-based approaches, memory is not the only driver of present or future action.
The synchronization between shot transitions and the evo-lution of actor behavior appears to cohere with the beliefs and wills of the event weavers, be they actors or camera-In (a) and (b) of Fig 1, we use the examples wearers. of high jumping and cooking to show the forward influ-ence of memory and backward constraint of anticipation.
Upon closer examination of both examples, it has been observed that future-oriented thoughts may impact action and memory, which seems modulated by the encoding of new information [43, 25]. Additionally, anticipation may change as ongoing behavior progresses and memory is up-dated [41, 20]. This indicates that there exists a circular interdependence between anticipation and memory, con-straining the evolution of behavior or events.
Based on the preceding analysis, we reevaluate the tem-poral dependencies inherent in memory-based methods. In their preoccupation with the impact of memory on antici-pation, these memory-based methods tend to overlook the inverse direction of impact, i.e., anticipation on memory.
Consequently, historical representations are not adequately corrected, thereby risking impeding any attempts to tran-scend the past. Against this backdrop, we assert that a com-prehensive temporal structure seamlessly integrates mem-In other words, a ory and anticipation is indispensable. memory-anticipation-based method building circular de-pendencies between memory and anticipation, as shown in
Fig 1(c), holds tremendous promise for enhancing cogni-tive inference capabilities and advanced understanding of
AI systems about the present and future.
To this end, we propose Memory-and-Anticipation
Transformer (MAT), a novel memory-anticipation-based approach that fully models the complete temporal context, including history, present, and future. A Progressive Mem-ory Encoder is designed to provide a more precise history summary by compressing long- and short-term memory in a segment-based fashion. Meanwhile, we propose our key idea of modeling circular dependencies between memory and future, implemented as Memory-Anticipation Circular
Decoder. It first learns latent future features in a supervised manner, then updates iteratively the enhanced short-term memory and the latent future features by performing Con-ditional circular Interaction between them. Among them, multiple interaction processes capture the circular depen-dency and supervise the output to maintain stable features with real semantics.
Remarkably, owing to the inherent superiority of our model design, we are able to adapt both tasks, i.e., online action detection and anticipation, in a unified manner, span-ning the training and online inference stages. For any given dataset, the MAT model obviates the need for separate train-ing or testing for each task. Rather, a single training process is enough, and during inference, the corresponding token for each task can be extracted effortlessly.
In summary, our contributions are 1). We rethink the temporal dependence of event evolution and propose a memory-anticipation-based paradigm for the circular inter-action of memory and anticipation, introducing the concept of memory and future circular dependence. 2). We propose a unified architecture Memory-Anticipation Transformer (MAT) that simultaneously processes online action detec-tion and anticipation, showing effective performance. 3).
MAT significantly outperforms all existing methods on four challenging benchmarks for online action detection and an-ticipation tasks, i.e., TVSeries [10], THUMOS’14 [30],
HDD [42] and EPIC-Kitchens-100 [9]. 2.