Abstract
Accurate intrinsic calibration is essential for camera-based 3D perception, yet, it typically requires targets of well-known geometry. Here, we propose a camera self-calibration approach that infers camera intrinsics during application, from monocular videos in the wild. We pro-pose to explicitly model projection functions and multi-view geometry, while leveraging the capabilities of deep neural networks for feature extraction and matching. To achieve this, we build upon recent research on integrating bundle adjustment into deep learning models, and intro-duce a self-calibrating bundle adjustment layer. The self-calibrating bundle adjustment layer optimizes camera in-trinsics through classical Gauß-Newton steps and can be adapted to different camera models without re-training. As a specific realization, we implemented this layer within the deep visual SLAM system DROID-SLAM, and show that the resulting model, DroidCalib, yields state-of-the-art cal-ibration accuracy across multiple public datasets. Our re-sults suggest that the model generalizes to unseen environ-ments and different camera models, including significant lens distortion. Thereby, the approach enables perform-ing 3D perception tasks without prior knowledge about the camera. Code is available at https://github.com/ boschresearch/droidcalib. 1.

Introduction
Inferring 3D structure and camera motion from a se-quence of images typically requires knowledge about the camera. Specifically, the camera’s mapping of 3D points to the 2D image, characterized by the intrinsic camera param-eters, must be known (Fig. 1). Intrinsic camera parameters are commonly obtained in a calibration process in which well-known calibration targets are moved in front of the camera [53, 46]. As the 3D structure of these calibration targets is precisely known, images of the target give 3D-2D correspondences that can be used to infer intrinsic camera parameters. However, this is a time-consuming process and does not allow for continuous re-calibration.
Figure 1. The proposed self-calibration infers the intrinsic cam-era parameters θ that define a camera’s projection function (top) without relying on calibration targets. A deep neural network N predicts weighted correspondences (flow fij, confidence weights wij), while the self-calibrating bundle adjustment (SC-BA) layer estimates the intrinsics through differentiable Gauß Newton steps (bottom).
Camera self-calibration aims at inferring camera intrin-sics based on arbitrary images or image sequences, without the need for a calibration target [14]. Yet, achieving ac-curacy and robustness comparable to target-based calibra-tion remains challenging. Single-image self-calibration ap-proaches (e.g. [52, 51, 22, 19]) have proven effective for image undistortion and for cases in which only a single im-age is available, however, they have to rely on known or learned properties of the environment (e.g. the existence of straight lines) which limits their ability to generalize, and they are typically limited to a subset of the intrinsics.
Multi-view approaches, on the other hand, use a se-quence of images and exploit the consistency of the scene structure over time to estimate camera intrinsics (e.g. [24, 33, 25]). The most common approach is to use a classi-cal Structure-from-Motion (SfM) pipeline, and to refine in-trinsics jointly with camera motion and 3D structure [33].
While classical SfM pipelines generalize well to unseen environments, they typically rely on handcrafted features and thus do not leverage the capabilities of learned fea-tures which have proven effective across a variety of 3D computer vision tasks [43, 40, 48, 32]. Deep learning ap-proaches, on the other hand, replace the SfM pipeline by a
deep learning pipeline and either implement the intrinsics as learned parameters of the model [9, 47, 8], or they regress the intrinsics from input sequences at inference time [9, 5].
While learning the intrinsics can achieve comparatively ac-curate results, it requires training a model specifically for every camera [9, 47, 8]. Regressing the intrinsics, on the other hand, can potentially generalize to different cameras, but the accuracy of existing models was shown to be lim-ited [9] and generalization to different cameras will require a balanced training dataset that contains sequences taken by a large variety of cameras.
We argue that camera self-calibration should leverage the capabilities of deep neural networks, but to enable gen-eralization across cameras, we propose to complement the learned part with explicit modeling of projection functions and multi-view geometry. Thereby, the model does not have to learn the underlying multi-view geometry from scratch, and it is not tied to one specific projection model, such as pinhole or fisheye model. To achieve this, we build upon recent research on integrating bundle adjustment into deep learning models [39, 42, 44], and introduce a self-calibrating bundle adjustment layer to optimize cam-era intrinsics classically within an end-to-end deep learning model (Fig. 1). After introducing the general idea, we pro-pose to integrate this layer within the deep visual SLAM system DROID-SLAM [44], resulting in a system that in-fers camera intrinsics from monocular video during appli-cation. Our contributions are the following: 1. We propose an intrinsic camera self-calibration ap-proach that leverages deep learning while explicitly modeling projections and multi-view geometry. 2. To this end, we introduce a differentiable self-calibrating bundle adjustment (SC-BA) layer that en-ables estimating camera intrinsics within a deep learn-ing model. 3. We integrate the SC-BA layer into the DROID-SLAM [44] architecture, giving a system that infers camera intrinsics from monocular video, and show that it yields consistently higher calibration accuracy than baseline methods on three public datasets. 2.