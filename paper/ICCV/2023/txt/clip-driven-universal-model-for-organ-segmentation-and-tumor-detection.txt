Abstract
An increasing number of public datasets have shown a marked impact on automated organ segmentation and tu-mor detection. However, due to the small size and partially labeled problem of each dataset, as well as a limited inves-tigation of diverse types of tumors, the resulting models are often limited to segmenting speciﬁc organs/tumors and ig-nore the semantics of anatomical structures, nor can they be extended to novel domains. To address these issues, we pro-pose the CLIP-Driven Universal Model, which incorporates text embedding learned from Contrastive Language-Image
Pre-training (CLIP) to segmentation models. This CLIP-based label encoding captures anatomical relationships, enabling the model to learn a structured feature embedding and segment 25 organs and 6 types of tumors. The proposed model is developed from an assembly of 14 datasets, using a total of 3,410 CT scans for training and then evaluated on 6,162 external CT scans from 3 additional datasets. We rank ﬁrst on the Medical Segmentation Decathlon (MSD) public leaderboard and achieve state-of-the-art results on
Beyond The Cranial Vault (BTCV). Additionally, the Uni-versal Model is computationally more efﬁcient (6 faster) compared with dataset-speciﬁc models, generalized better to CT scans from varying sites, and shows stronger transfer learning performance on novel tasks.
⇥ 1.

Introduction
Enormous advances in medical imaging beneﬁt from the ever-growing number of annotated datasets [43, 1, 42, 31, 76]. Although a total of around 5,000 annotated abdom-inal CT scans are publicly available, it is still commonly perceived that medical imaging datasets are too small to de-velop robust AI models [94, 72, 54, 66, 95, 13]. One reason for this impression is the high cost of detailed per-voxel seg-*Corresponding authors: Yucheng Tang (yuchengt@nvidia.com) and
Zongwei Zhou (zzhou82@jh.edu)
Figure 1. Cosine similarity between CLIP embeddings. The
CLIP embedding reveals the intrinsic semantics of the anatomical structures by mapping similar concepts close to each other in the embedding space. For example, “Liver” has a large similarity with
“Liver Tumor” and “Hepatic Vessel” (the hepatic vessel returns low-oxygen blood from the liver to the heart, which has a high anatomical relationship with the liver). mentation annotations, which can take nearly one hour per organ for an expert annotator. Since each institute has time, monetary, and clinical constraints, the number of CT scans in each dataset is limited, and the types of annotated organs vary signiﬁcantly from institute to institute. Moreover, only a small proportion (hundreds) of public CT scans contain tumor annotation performed by experts [3, 24, 1].
The partially labeled problem [32, 88, 37] can impose signiﬁcant limitations on the performance of models trained on existing public datasets, ultimately hindering their effec-tiveness for multi-organ segmentation and tumor detection.
However, despite this challenge, the potential of AI models in these areas remains promising and largely unexplored.
This has motivated us to exploit the public datasets with par-tial labels, and demonstrate the clinical impact of AI frame-work, including model expansibility (i.e., adaptable to var-ious network backbone), generalizability (i.e., robust to CT scans from various hospitals) [43] and transferability (i.e., generic image representation that is transferable to multiple
downstream tasks) [97]. Speciﬁcally, we have assembled 14 publicly available datasets, including 3,410 CT scans with 25 partially annotated organs and 6 tumors.
Formidable challenges exist in assembling partially an-notated datasets. First, label inconsistency, in ﬁve aspects. (i) Index inconsistency. The same organ can be labeled as different indexes. For example, the stomach is labeled ‘7’ in BTCV, but ‘5’ in WORD. (ii) Name inconsistency. Nam-ing can be confusing if multiple labels refer to the same anatomical structure. For example, “postcava” in AMOS22 and “inferior vena cava” in BTCV. (iii)