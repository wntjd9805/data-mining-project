Abstract
Continual Learning (CL) involves training a machine learning model in a sequential manner to learn new infor-mation while retaining previously learned tasks without the presence of previous training data. Although there has been significant interest in CL, most recent CL approaches in computer vision have focused on convolutional architectures only. However, with the recent success of vision transformers, there is a need to explore their potential for CL. Although there have been some recent CL approaches for vision trans-formers, they either store training instances of previous tasks or require a task identifier during test time, which can be lim-iting. This paper proposes a new exemplar-free approach for class/task incremental learning called ConTraCon, which does not require task-id to be explicitly present during in-ference and avoids the need for storing previous training instances. The proposed approach leverages the transformer architecture and involves re-weighting the key, query, and value weights of the multi-head self-attention layers of a transformer trained on a similar task. The re-weighting is done using convolution, which enables the approach to maintain low parameter requirements per task. Additionally, an image augmentation-based entropic task identification approach is used to predict tasks without requiring task-ids during inference. Experiments on four benchmark datasets demonstrate that the proposed approach outperforms several competitive approaches while requiring fewer parameters.1 1.

Introduction
Humans excel at solving newer tasks without forgetting previous knowledge. Deep neural networks, however, face the challenge of forgetting old knowledge when trained for a novel task. This problem, known as Catastrophic Forgetting, is a fundamental challenge if a model needs to learn when data arrives in a sequence of non-overlapping tasks. Con-†Work started before joining Amazon 1Project contracon
Page: https://cvir.github.io/projects/
Figure 1: The proposed ConTraCon architecture. T1 represents a transformer model trained on a previous task which is adapted to a new task using only a few learnable parameters and temporal skip-gating to a new transformer Tt on the Right. tinual Learning [1, 21, 26, 37, 40, 41] aims to handle such problems arising from non-stationary data to retain previ-ously learned knowledge as well as acquire knowledge from new data with limited or no access to previous data.
Although continual learning in computer vision has wit-nessed remarkable progress, most of the methods are tailored for CNNs. Recently, vision transformers have shown promis-ing results in big data regime [8, 15, 32]. However, data hungry vision transformers and data-scarce continual learn-ing do not seem to go hand in hand, and thus continual learning in vision transformers has received relatively little attention. Early continual learning approaches on ConvNets relied on exemplar rehearsal which re-trains newer models on previous data instances stored in a fixed size memory buffer [2, 7, 9, 23, 33, 38, 40, 41]. Dytox [16] and LVT [52] are contemporary works addressing continual learning on vision transformers using such previously stored data. How-ever, storing task samples in raw format may not always be feasible, especially for tasks where long-term storage of data is not permitted owing to privacy or data use legisla-tion [24]. Dynamic architecture based methods, on the other hand, grows an initial model dynamically or rearranges its internal structure, [19, 44, 46, 58] with the arrival of new tasks without the need for storing previous data.
The main focus of this paper is in modeling the new set of parameters for new tasks with as low overhead as possible. 1
We conjecture that better representation learning capability of a transformer keeps it ready for easily adapting to new tasks by manipulating the already learned weights. Convolu-tion offers a cheap way to manipulate transformer weights and the weight matrices are natural fits to convolution as input and output. In this paper, we propose a dynamically ex-pandable architecture with novel convolutional adaption on previously learnt transformer weights to obtain new weights for new tasks. In addition to this, we employ a learnable skip-gating which learns to convexly combine the old and the convoluted weights. This helps significantly in maintaining the stability-plasticity tradeoff by balancing between how much to retain and how much to forget. The resulting pro-posed approach – Continual Transformer with Convolutions (ConTraCon) (ref. Fig. 1) – not only leverages a vision transformer’s learning capability but also does it with sig-nificantly low memory overhead per task as is shown in our experiments.
Specifically, for each new task, we reweigh the key, query, and value weights of the previously learned transformer by convolving them with small filters. Transformers are known for their ability to capture long-range dependencies between patches. Convolution, on the other hand, exploits a local neighborhood only. Such local dependency not only restricts large changes in the weights for the new task from the old tasks, but also allows us to achieve this with very little in-crease in the model size. As the convolution weights are separate for different tasks, during inference, this will require the information of which task an image belongs to. However, in class incremental continual learning, images may come arbitrarily without associated task information. To tackle the challenging scenario, we propose a novel entropy-based criterion to infer the task before calling for the corresponding task-specific convolution weights In particular, our proposed approach creates multiple augmented views of a test im-age and evaluates the agreement of their predictions among different task-specific models. The task-id of the image is determined by identifying the task giving the lowest entropy value of the average predictions from the various augmented views. The task-specific model with the most consistent and confident predictions across different augmentations corre-sponds to the correct task for the image.
Extensive experiments on four benchmark datasets, con-sidering both the availability and unavailability of task-ids at test time, demonstrate the superiority of our proposed method over several state-of-the-art methods including the extension of popular approaches on CNNs to transformers.
Despite being exemplar-free, our approach outperforms state-of-the-art exemplar-based continual learning approaches that use transformers as backbone architectures (Dytox [16] and
LVT [52]) with an average improvement of 5%. Moreover, our approach accomplishes this with only about ∼ 60% pa-rameters used by above models. We also perform ablation studies highlighting the contribution of each component in our approach. To summarize, our key contributions include:
• We propose ConTraCon, a dynamic architecture for CL on transformers. We use convolution on the Multi-head self-attention (MHSA) layers to learn new tasks thereby achieving better performance with significantly low mem-ory overhead. We also apply a temporal skip-gating func-tion to tradeoff between stability and plasticity.
• Our entropy based approach adds the flexibility of not having to know the task information during inference.
• We performed extensive experimentation and ablation of the proposed approach thereby validating the superiority of our model and helping to disentangle the significance of different components. 2.