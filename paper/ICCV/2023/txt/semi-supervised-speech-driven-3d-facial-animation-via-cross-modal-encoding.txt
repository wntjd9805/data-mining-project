Abstract
Existing Speech-driven 3D facial animation methods typically follow the supervised paradigm, involving regres-sion from speech to 3D facial animation. This paradigm faces two major challenges: the high cost of supervision acquisition, and the ambiguity in mapping between speech and lip movements. To address these challenges, this study proposes a novel cross-modal semi-supervised framework, comprising a Speech-to-Image Transcoder and a Face-to-Geometry Regressor. The former jointly learns a com-mon representation space from speech and image domains, enabling the transformation of speech into semantically-consistent facial images. The latter is responsible for re-constructing 3D facial meshes from the transformed im-ages. Both modules require minimal effort to acquire the necessary training data, thereby obviating the dependence on costly supervised data. Furthermore, the joint learn-ing scheme enables the fusion of intricate visual features into speech encoding, thereby facilitating the transforma-tion of subtle speech variations into nuanced lip movements, ultimately enhancing the ﬁdelity of 3D face reconstruc-tions. Consequently, the ambiguity of the direct mapping of speech-to-animation is signiﬁcantly reduced, leading to coherent and high-ﬁdelity generation of lip motion. Exten-sive experiments demonstrate that our approach produces competitive results compared to supervised methods. 1.

Introduction
Speech-driven 3D facial animation aims to automatically animate vivid facial expressions of realistic 3D avatars from the speech signal.
It has become increasingly popular in many ﬁelds, such as games, virtual reality, ﬁlm production, and online communication, as it enables the generation of lifelike facial expressions with minimal effort.
Recently proposed approaches, including VOCA[7],
MeshTalk[28], and FaceFormer[10], have shown promis-ing results in recovering 3D facial meshes from speech signals through the utilization of regressive networks or transformer-based autoregressive models. However, these methods, which fall under the supervised paradigm, en-counter two primary challenges. Firstly, obtaining paired supervision of speech and high-ﬁdelity 3D facial animation requires the utilization of a professional and expensive fa-cial motion capture system, leading to considerable costs.
Secondly, the mapping of low-dimensional speech signals to high-dimensional 3D facial meshes with signiﬁcant vari-ability may result in ambiguity issues, leading to suboptimal outcomes.
This study for the ﬁrst time proposes a novel cross-modal semi-supervised framework to address the above is-sues, which consists of a Speech-to-Image Transcoder and a Face-to-Geometry Regressor. The former is designed to transform speech into semantically-consistent facial im-ages, while the latter is responsible for reconstructing 3D facial meshes from the transformed images. The design of the Speech-to-Image Transcoder is inspired by the recent success of unsupervised image-to-image translation [26], which allows for transformation between different input im-age domains. Our transcoder extends this mechanism to fa-cilitate cross-modal conversion between speech and image domains. To achieve this capability, we learn a common representation space using data from three domains: paired speech and real facial images, as well as synthetic facial im-ages. We train the translation between the real and synthetic image domains in an unsupervised manner, and simultane-ously leverage the paired relationship between real images and speech to project speech features into the image repre-sentation space. Through this joint training, we construct a common space where the representations of speech and images are tied together. With the learned common repre-sentation, we can convert speech into synthetic facial im-ages using our domain-dependent image decoder. Further-more, we leverage the synthetic facial data, which is paired with rendered facial images and 3D face meshes, to train the Face-to-Geometry Regressor. This allows us to infer the corresponding 3D face meshes from the synthetic faces generated from speech.
*These authors contributed equally to this work.
Our pipeline is capable of effortlessly acquiring the nec-essary data, which includes synchronized speech and real facial images extracted from readily available video clips, as well as synthetic facial images generated through the defor-mation of a polygonal face mesh using a rendering engine.
Notably, our approach is not bound by any prerequisites for paired speech and 3D face animation, thereby effectively addressing the issue of supervision data scarcity.
Moreover, our joint training scheme integrates the speech-to-image regression and image-to-image recon-struction into a cohesive framework, which enables the in-corporation of detailed visual features into speech encoding.
Leveraging the domain-dependent image decoders, which store intricate facial visual priors, we are able to convert nuanced speech variations into micro-expressions that are rich in detail, thereby facilitating the subsequent generation of high-ﬁdelity 3D face reconstructions. Consequently, the uncertainties associated with the direct mapping of speech-to-animation in the supervised paradigm are considerably reduced, leading to the generation of lip motion with ﬁner granularity.
In summary, the main contributions of our research are:
• We make the ﬁrst attempt to build a semi-supervised framework for speech-driven 3D facial animation, which eliminates the need for paired speech and 3D animation.
• Our proposed joint training scheme leverages cross-modal translation to convert subtle speech variations into images that are rich in detail, resulting in more high-ﬁdelity lip motion generation.
• Our extensive experiments and user studies demon-strate that our approach produces competitive results compared to supervised methods. 2.