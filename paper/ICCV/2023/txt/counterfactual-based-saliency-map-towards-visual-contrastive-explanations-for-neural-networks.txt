Abstract
Explaining deep models in a human-understandable way has been explored by many works that mostly explain why an input causes a corresponding prediction (i.e., Why P?).
However, seldom they could handle those more complex causal questions like “Why P rather than Q?” and “Why one is P, while another is Q?”, which would better help humans understand the behavior of deep models. Consid-ering the insufﬁcient study on such complex causal ques-tions, we make the ﬁrst attempt to explain different causal questions by contrastive explanations in a uniﬁed frame-work, i.e., Counterfactual Contrastive Explanation (CCE), which visually and intuitively explains the aforementioned questions via a novel positive-negative saliency-based ex-planation scheme. More speciﬁcally, we propose a content-aware counterfactual perturbing algorithm to stimulate contrastive examples, from which a pair of positive and negative saliency maps could be derived to contrastively explain why P (positive class) rather than Q (negative class). Beyond existing works, our counterfactual perturba-tion meets the principles of validity, sparsity, and data dis-tribution closeness at the same time. In addition, by slightly adjusting the objective of perturbation, our framework can adapt to different causal questions. Extensive experimen-tal evaluation demonstrates the effectiveness and superior performance of the proposed CCE on different benchmark metrics for interpretability, including Sanity Check, Class
Deviation Score and Insertion-Deletion tests. A user study is conducted and the results show that user conﬁdence is in-creasing signiﬁcantly when presented with CCE compared to standard saliency map baselines. 1.

Introduction
Deep neural networks (DNNs) have achieved break-through performance in various computer vision tasks, e.g.,
∗Zhibo Wang is the corresponding author. image classiﬁcation [19, 34], object detection [28, 29], se-mantic segmentation [23], etc. However, the “black-box” nature of DNNs, i.e., massive unexplainable parameters and lack of intuitive understanding in prediction, has been draw-ing chaos in the human-understandable analysis of their be-havior. Since human-understandable explanations of DNNs would signiﬁcantly facilitate the causal analysis, e.g., what causes misclassiﬁcation or bias, it is rising increasing de-mand for related research. One of the popular approaches to interpreting deep learning models is to display visual ex-planations in the form of saliency maps [6, 32, 33, 36].
Most saliency-based approaches highlight the most infor-mative areas by assigning weights to image regions con-cerning their contributions to the ﬁnal prediction, revealing their causal relationship.
However, in the ﬁeld of social science, Miller [26] sys-tematically surveyed explanation methods and pointed out that most explanations are contrastive. That is, people not only want to know “Why P?”, where P refers to the given conclusion, but also have great interest in “Why P, rather than Q?”, in which Q is often implicit from the context.
Van Bouwel and Weber [40] deﬁned the latter question as
P-contrast referring to differences that occur on properties within an object, and formally state it as “Why does object a have property P, rather than property Q?” Saliency maps ef-ﬁciently answer the question “Why P?” by highlighting the most informative regions, but it does not take the P-contrast question into account. For example, when an application for a loan is denied by the bank, the applicant would be left wondering not only why was the loan denied, but also how to pass the application. The saliency maps might only point out that the income is the main reason for denial but fail to state the difference between approval and denial.
For structured data such as tabular data, counterfactual explanation [42, 7] is a popular method to answer P-contrast question. A counterfactual explanation modiﬁes the input
Why hummingbird? Why nightingale?
Why hummingbird, rather than nightingale?
GradCAM
Counterfactual
Contrastive 
Explanation
Heatmap for hummingbird Heatmap for nightingale
CCE Positive Heatmap CCE Negative Heatmap
Because those highlight regions are important when the picture is classified as a hummingbird (nightingale).
Because when classified as a hummingbird the positive regions are in dominant, but when classified as a nightingale the negative regions are in dominant.
Figure 1: Comparison between GradCAM and our proposed Counterfactual Contrastive Explanation (CCE), i.e., non-contrastive vs. contrastive explanations. appropriately to change the output of the model from P to
Q. For the previous example, a potential counterfactual ex-planation could be: “The loan was denied as your income was $20,000, you would have been offered a loan if your in-come had increased to $25,000.” These approaches empha-size the counterfactual example itself as the explanation and ignore the importance of considering the model’s decision changes in a contrasting perspective. For users, “increas-ing income by $500” is a more accurate explanation, which can be easily converted in tabular data. Several previous studies [4, 5, 12, 44, 16] have proposed methods to answer
P-contrast question, they use GAN to generate counterfac-tuals as explanations directly or ﬁnd additional images as counterfactuals to show contrast. However, for unstructured data such as images, the difference between counterfactual and original sample is not intuitive, and the correlation be-tween counterfactual and model decision is lower than that of tabular data.
Besides P-contrast question, Van Bouwel and Weber [40] also deﬁned O-contrast question, that is “Why does object a have property P, while another object b has property Q?”, which note differences occur on properties between objects themselves.
In deep learning, this question is highly rel-evant to the adversarial example [39], that carefully con-structed perturbations are added to input and aimed to maxi-mize the change of the model’s decision. Typically, saliency maps explain an adversarial example and its benign coun-terpart by highlighting different regions. However, Zhang et al. [47] proposed an attack method that changes the model decision while keeping the saliency maps unchanged. This study reveals that the existing explanation approaches are not fully aligned with the model decisions and cannot pro-vide convincing explanations for the phenomenon of ad-versarial examples. We argue that to interpret the adver-sarial example phenomenon, it is essential to answer the
O-contrast question “Why is the original input assigned to class P, while the adversarial example is considered to be class Q?”, which is not considered in existing explanation approaches. i.e., Counterfactual Contrastive Explanation method, (CCE), which gives the ﬁrst attempt to answer “Why P?”,
P-contrast, and O-contrast questions in a uniﬁed framework for image classiﬁcation task. As illustrated in Fig. 1, our method contrastively explains why the image is classiﬁed as a hummingbird rather than a nightingale, while the tra-ditional way na¨ıvely explains the two categories in sepa-rate and overlapped saliency maps that are difﬁcult to tackle contrastive questions. The core of contrastive explanation is to identify suitable contrastive objects, such that we could contrastively reason what results in different predictions.
Correspondingly, we propose to counterfactually synthesize such contrastive objects. Ideally, a counterfactual example should minimally change the features but maximally dif-fer the predictions [42]. Towards this end, we generate sparse and content-aware counterfactual perturbations that are overlayed onto the original image to change the model conﬁdence in the target label while preserving the conﬁ-dence in the rest classes. Intuitively, comparing such coun-terfactual examples with their original inputs is supposed to generate visual contrastive explanations. To better visual-ize dominant features in a contrastive manner, we propose gradient-aware feature maps that utilize class-speciﬁc gra-dients to achieve decoupled saliency maps based on origi-nal and counterfactual examples. More speciﬁcally, we dis-entangle feature representations of DNNs into positive and negative saliency maps, which contrastively visualize what features dominate corresponding predictions. We summa-rize our contributions as below:
• To the best of our knowledge, we give the ﬁrst attempt to explain three types of causal questions “Why P?”,
“Why P rather than Q?”, and “Why a is P while b is
Q?” in a uniﬁed framework, which drastically boosts generalization capacity as compared to existing works.
• We propose a positive-negative saliency scheme to provide visual contrastive explanations, which can in-tuitively answer the aforementioned causal questions by comparing original and counterfactual examples.
Therefore, we propose a novel visual explanation
• We propose a counterfactual perturbing algorithm for
images to generate counterfactual examples that si-multaneously satisfy the principles of validity, spar-sity, and data distribution closeness by constraining the changes of logits.
• Extensive experimental results demonstrate that the proposed counterfactual contrastive explanations (CCE), outperform existing works in multiple inter-pretability metrics including Sanity Check [3], Class
Deviation Score and Insertion-Deletion tests [30]. Fur-ther, we conducted a user study demonstrating the ef-fectiveness of CCE in helping users gain a deeper un-derstanding of model’s prediction. 2.