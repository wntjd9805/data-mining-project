Abstract
Diffusion models generate images by iterative denoising.
Recent work has shown that by making the denoising pro-cess deterministic, one can encode real images into latent codes of the same size, which can be used for image edit-ing. This paper explores the possibility of deﬁning a latent space even when the denoising process remains stochastic.
Recall that, in stochastic diffusion models, Gaussian noises are added in each denoising step, and we can concatenate all the noises to form a latent code. This results in a latent space of much higher dimensionality than the original image.
We demonstrate that this latent space of stochastic diffusion models can be used in the same way as that of determinis-tic diffusion models in two applications. First, we propose
CycleDiffusion, a method for zero-shot and unpaired image editing using stochastic diffusion models, which improves the performance over its deterministic counterpart. Second, we demonstrate uniﬁed, plug-and-play guidance in the latent spaces of deterministic and stochastic diffusion models.1 1The code is available at humansensinglab/cycle-diffusion. 1
1.

Introduction
Diffusion probabilistic models (DPMs) [30, 9] have achieved unprecedented results in generative modeling and are instrumental to text-to-image models such as DALL
E-2
·
[24]. In DPMs, images are generated by iterative denoising.
In the original formulation, the denoising process is stochas-tic, where Gaussian noises are added in each denoising step.
This stochastic formulation makes DPMs different from pre-vious models such as VAEs [15], GANs [6], and normalizing
ﬂows [14], for which the generation can be inverted to obtain a latent code from a real image.
Different from the stochastic formulation, prior works [31, 29] showed that every stochastic DPM has an ODE-based, deterministic counterpart with the same output distribution.
An important advantage of ODE-based, deterministic DPMs is that the denoising process can be inverted. That is, given an image, one can obtain a latent code (of the same size as the image) that can be denoised to reconstruct the image. This property makes deterministic DPMs a promising candidate for image editing [32], where one can encode an image into a latent code with one model (or condition) and decode it with another model (or condition) to obtain a new image.
In this paper, we show that (1) stochastic DPMs can also have a latent space, (2) real images can be encoded into this latent space, and (3) the latent space can be used in the same way as the latent space of deterministic DPMs.
To deﬁne the latent space, recall that in stochastic DPMs,
Gaussian noises are added in each denoising step, and we concatenate all the noises to form a latent code. This results in a latent space of much higher dimensionality than the original image. For encoding, we propose DPM-Encoder, an algorithm for encoding real images into the latent space of stochastic DPMs. DPM-Encoder is based on the fact that the process of adding noises to the real image (i.e., the forward process) is pre-deﬁned. Therefore, we can sample consecu-tive noisy images from the forward process and compute the noise used in denoising by deﬁnition.
We will show two cases in which this latent space of stochastic DPMs can be used in the same way as the latent space of deterministic DPMs. First, previous works [32, 7] have shown that deterministic DPMs can be used for image editing. Given two deterministic DPMs trained with the same noise schedule on two domains, one can encode a source image into a latent code with the source-domain model and decode it with the target-domain model to obtain a target image [32]. Given a deterministic text-to-image diffusion model, one can encode an image into a latent code conditioned on the source text and decode conditioned on the target text to obtain a new image [7]. Built upon these works, we propose CycleDiffusion, an extension of the same idea from deterministic DPMs to stochastic DPMs. Our experiments show that CycleDiffusion outperforms previous methods for both unpaired image editing (Section 4.1) and zero-shot image editing (Section 4.2).
Second, we show that both deterministic and stochas-tic DPMs can be guided in a plug-and-play manner using the energy-based model [18, 20, 35]. Such plug-and-play guidance was previously proposed to sample controlled dis-tributions from pre-trained GANs. Notably, different from classiﬁer guidance [5], the plug-and-play guidance does not require ﬁnetuning the guidance model on noisy images. Our experiments demonstrate that plug-and-play guidance can sample controlled and diverse images from both determinis-tic and stochastic DPMs (Section 4.3). 2.