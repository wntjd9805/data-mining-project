Abstract
Previous work on adversarial examples typically involves a ﬁxed norm perturbation budget, which fails to capture the way humans perceive perturbations. Recent work has shifted towards natural unrestricted adversarial examples (UAEs) that breaks `p perturbation bounds but nonetheless remain semantically plausible. Current methods use GAN or VAE to generate UAEs by perturbing latent codes. How-ever, this leads to loss of high-level information, resulting in low-quality and unnatural UAEs. In light of this, we propose
AdvDiffuser, a new method for synthesizing natural UAEs using diffusion models. It can generate UAEs from scratch or conditionally based on reference images. To generate nat-ural UAEs, we perturb predicted images to steer their latent code towards the adversarial sample space of a particular classiﬁer. We also propose adversarial inpainting based on class activation mapping to retain the salient regions of the image while perturbing less important areas. On CIFAR-10,
CelebA and ImageNet, we demonstrate that it can defeat the most robust models on the RobustBench leaderboard with near 100% success rates. Furthermore, The synthesized
UAEs are not only more natural but also stronger compared to the current state-of-the-art attacks. Speciﬁcally, com-pared with GA-attack, the UAEs generated with AdvDiffuser exhibit 6 smaller
FID scores and 0.28 higher in SSIM metrics, making them perceptually stealthier. Finally, adversarial training with
AdvDiffuser further improves the model robustness against attacks with unseen threat models.1 smaller LPIPS perturbations, 2 3
⇥
⇠
⇥ 1.

Introduction
Deep neural networks (DNNs) have achieved unprece-dented success in various visual recognition tasks. Despite
*Equal contribution. Correspondence to Xitong Gao. 1AdvDiffuser is open source and available at https://github. com/lafeat/advdiffuser. 1 their remarkable success, DNNs are susceptible to adver-sarial examples [23], i.e. DNN predictions can be fooled by adding a tiny and difﬁcult to perceive perturbation to a natural image. Furthermore, deep models face greater threats in real-world scenarios from unrestricted adversarial examples (UAEs) [2]. UAEs can make extensive changes to images without signiﬁcantly affecting human perception of their meanings and faithfulness, and have thus emerged as a prominent direction in the study of adversarial attacks over the past few years.
Gradient-based unrestricted adversarial attacks perturbs original images within predeﬁned perturbation bounds.
Geometry-aware attacks [20] uses proxy models to mini-mize the `p budget required, and it won the 1st place in a
CVPR Competition on unrestricted adversarial attacks [4].
On the other hand, perceptual attacks [19, 51] optimize per-turbations using bounds on perceptual distances, such as
LPIPS [49] and structural similarity [42]. Others consider image recolorization [36, 37]. However, selecting proxy models and distance metrics require subjective prior knowl-edge to generate adversarial examples that appear realistic.
Generative models such as generative adversarial net-works (GANs) have the ability to learn and sample from the data distribution effectively. This is why [38, 50] use them to generate adversarial examples. These approaches search for perturbations in the latent space that can cause the targeted model to misclassify the images after decoding, in order to
ﬁnd adversarial examples. Nevertheless, perturbing latent codes alters the high-level semantics of generated images in a way that is perceptually salient to humans [17]. Such perturbations can introduce ambiguity in certain image at-tributes, and visibly distort the original concept, resulting in
UAEs that are often semantically vague and of poor quality.
These UAEs could thus be perceptually very different from the original examples.
In order to address these issues, we propose AdvDif-fuser, a novel generative unrestricted adversarial attack based on diffusion models [13]. Diffusion models draw inspira-tion from non-equilibrium thermodynamics, which deﬁne a
Markov process of noise-adding image diffusion steps, and then learns to reverse the diffusion process to generate data samples from noisy images. This enables trained diffusion models to sample the data distribution with high ﬁdelity and diversity. In Section 3.1, we utilize and modify the backward denoising process of pre-trained diffusion models, and inject small adversarial perturbations that can attack the defending model successfully. Diffusion models are trained with a de-noising objective, and therefore, they can effectively remove conspicuous adversarial noise while retaining the ability to attack, resulting in naturally appearing UAEs. To achieve more realistic outcomes, we introduce adversarial inpainting, which leverages masks derived from gradient-based class activation mapping (GradCAM) [35]. It tunes the denoising strength of each pixel based on object saliency, ensuring that regions containing important objects undergo smaller modi-ﬁcations. As AdvDiffuser perturbs images at the pixel level, it produces perceptual perturbations that are considerably smaller when compared to those generated by GAN-based methods. The ﬁnal UAEs produced by our method are there-fore more natural and imperceptible than those synthesized by either gradient- or GAN-based approaches. In addition to its image-conditioned attacks, AdvDiffuser offers another advantage over other unrestricted adversarial attacks as it has the ability to craft an inﬁnite number of synthetic yet natural adversarial examples. This can potentially provide more comprehensive robustness training and evaluation for future defense techniques.
We summarize our contribution as follows:
• To the best of our knowledge, our work is the ﬁrst to investigate natural adversarial example synthesis with diffusion models. Along with its image-conditioned attack ability, and it is also the ﬁrst that can generate an inﬁnite number of synthetic yet natural adversarial examples.
• We propose adversarial inpainting to introduce CAM-based sample conditioning, resulting in diverse and high-quality outputs while preserving the semantics of the reference images.
• AdvDiffuser can successfully deceive the top-ranked robust models in RobustBench [6] with high success rates (close to 100%). The generated examples closely resemble the original distribution. Our perturbations are both more effective and less perceptible, with better
LPIPS, FID and SSIM distance metrics than the current state-of-the-art unrestricted adversarial attacks. 2. Preliminaries &