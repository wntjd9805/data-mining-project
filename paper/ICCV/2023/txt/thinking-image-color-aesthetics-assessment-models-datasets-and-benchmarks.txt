Abstract
We present a comprehensive study on a new task named image color aesthetics assessment (ICAA), which aims to assess color aesthetics based on human perception. ICAA is important for various applications such as imaging mea-surement and image analysis. However, due to the highly diverse aesthetic preferences and numerous color combi-nations, ICAA presents more challenges than conventional image quality assessment tasks. To advance ICAA research, 1) we propose a baseline model called the Delegate Trans-former, which not only deploys deformable transformers to adaptively allocate interest points, but also learns hu-man color space segmentation behavior by the dedicated module. 2) We elaborately build a color-oriented dataset,
ICAA17K, containing 17K images, covering 30 popular color combinations, 80 devices and 50 scenes, with each image densely annotated by more than 1,500 people. More-over, we develop a large-scale benchmark of 15 methods, the most comprehensive one thus far based on two datasets,
SPAQ and ICAA17K. Our work, not only achieves state-of-the-art performance, but more importantly offers the com-munity a roadmap to explore solutions for ICAA. Code and dataset are available in here. 1.

Introduction
Color is known as having a higher degree of discrim-inability and correlation compared to other visual features
[1, 2, 3], because the human eye can directly perceive light of different wavelengths and convert them into differ-ent color signals. As digital photography expands rapidly,
ICAA has become one of the most important criteria to au-tomatically assess whether the image meets users’ aesthetic preferences [4, 5, 6, 7]. It is also an essential step in imaging measurements among manufacturers to evaluate the perfor-mance of smartphones and cameras [8, 9, 10].
†Equal Contribution; ∗Contact Author.
Figure 1. The results of different methods on two similar im-ages with tiny color adjustment, ground-truth (red) and predicted (black) scores are shown below. (a) Similar images. (b) Color his-togram [16] is unable to differentiate and quantify the color aes-thetics. The saliency maps of (c) MaxViT [17], (d) NIMA [18] and (e) our method, whereas our method achieves a fine-grained perception of color aesthetics.
Although ICAA is an important branch of IAA (im-age aesthetics assessment), they deal with different tasks.
IAA evaluates the holistic aesthetics of an image, which implicitly depends on color and other attributes (bright-ness,sharpness, etc.) [11, 12, 8]. ICAA focuses more on evaluating the impact of color harmony, color combinations and other factors on color perception.
ICAA is confronted with two major challenges. First, the types and combinations of colors are intrinsically complex, thus the color aesthetics are determined by specific colors and their relative positions in color spaces [13]. However, biological studies show that humans can perceive only up to 8 colors due to the human eye and perception limits [14].
Therefore, extracting dominant colors from an image is a crucial process and important prior knowledge [6, 15]. Sec-ond, human color preferences are subjective and vary from person to person based on factors such as age and culture that may change over time.
Most traditional quantization methods for ICAA rely on objective rules such as color histogram [16] or color wheel
[19] theories. However, the color information obtained from pixel statistics is limited to capture the nuances of color aesthetics (Fig. 1 (b)), mainly due to these methods cannot perceive the spatial information of colors and the ef-fect of image content and semantics. In addition, they are suited to qualitative analysis instead of directly quantifying the aesthetics of colors.
IAA methods, especially leaning-based IAA approaches such as [18, 20, 17], are not specifically designed for ICAA and then lack of prior color knowledge, which result in struggling to achieve a fine-grained perception of the im-portance of different color spaces, and being susceptible to interference from multiple visual elements (Fig. 1 (c)(d)).
Furthermore, to our knowledge, there are no datasets specif-ically designed for ICAA, which results in most data-driven methods not generalizing well to deal with subjective ICAA tasks. In addition, the community of image quality assess-ment lacks of a fair ICAA benchmark for reliable compar-isons.
The contributions of our paper lie in:
• The limits of human perception and pixel-based extrac-tion methods for dominant colors are revealed, which guides us to propose a baseline model, called the Dele-gate (Deformable Gate) Transformer, to adaptively allo-cate interest points for dominant colors and simulate hu-man behavior of color space segmentation.
• To verify our method more convincingly, a dataset, named
ICAA17K, is exclusively designed for ICAA. Specifi-cally, it is a color-oriented dataset with 17K images and the richest annotations thus far. Furthermore, dedicated preassessment strategy is adopted to alleviate long-tailed distributions of dataset.
• Based on ICAA17K, 15 state-of-the-art baselines (see Ta-ble 1) are evaluated, which takes our benchmark as the most complete one for ICAA thus far. Our work, not only gains state-of-the art performance, but more importantly serves as a potential catalyst for promoting large-scale model comparisons in future ICAA research. 2.