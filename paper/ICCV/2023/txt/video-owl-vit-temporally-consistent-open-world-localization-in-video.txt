Abstract
We present an architecture and a training recipe that adapts pretrained open-world image models to localiza-tion in videos. Understanding the open visual world (with-out being constrained by ﬁxed label spaces) is crucial for many real-world vision tasks. Contrastive pre-training on large image-text datasets has recently led to signiﬁcant im-provements for image-level tasks. For more structured tasks involving object localization applying pre-trained models is more challenging. This is particularly true for video tasks, where task-speciﬁc data is limited. We show suc-cessful transfer of open-world models by building on the
OWL-ViT open-vocabulary detection model and adapting it to video by adding a transformer decoder. The decoder propagates object representations recurrently through time by using the output tokens for one frame as the object queries for the next. Our model is end-to-end trainable on video data and enjoys improved temporal consistency com-pared to tracking-by-detection baselines, while retaining the open-world capabilities of the backbone detector. We evaluate our model on the challenging TAO-OW benchmark and demonstrate that open-world capabilities, learned from large-scale image-text pretraining, can be transferred suc-cessfully to open-world localization across diverse videos. 1.

Introduction
A central goal in computer vision is to develop models that can understand diverse and novel scenarios in the visual world. While this has been difﬁcult for methods developed on datasets with closed label spaces, web-scale image-text pretraining has recently led to dramatic improvements in
†Work done while at Google.
Correspondence: heigold@google.com, tkipf@google.com open-world performance on a range of image-level vision tasks [12, 26, 19].
However, challenges still remain for object-level tasks on images and especially videos. First, object-level tasks re-quire predicting more complex output structures compared to image-level tasks, making transfer of pretrained mod-els more challenging. Second, training data for structured tasks is limited due to the prohibitive labeling cost. There-fore, a key research question is how to transfer the open-vocabulary capabilities of image-text models to object-level tasks like object detection and tracking.
For object detection, works such as ViLD [12], Region-CLIP [40], OWL-ViT [26], F-VLM [19] etc. demonstrate that image-level open-vocabulary capabilities can be trans-ferred to object detection with relatively little detection-speciﬁc training data. Most recent works achieve this by combining image-text pre-trained encoder backbones (e.g.
CLIP [27]) with detection heads. By transferring seman-tic knowledge from the backbone, the resulting models are capable of detecting objects for which no localization anno-tations were present in the detection training data.
Here, we extend this approach to video. We build on
OWL-ViT [26], which provides a simple open-world de-tection architecture in which light-weight box prediction and classiﬁcation heads are trained on top of a CLIP back-bone. We transfer the open-world capabilities of OWL-ViT to video understanding with minimal video-speciﬁc train-ing data. The key idea behind our approach is to apply the open-world detector autoregressively to the frames of a video, propagating representations through time to track objects. To allow representations to bind consistently to the same object irrespective of its location, we depart from the encoder-only OWL-ViT architecture: We decouple object representations from the image grid by adding a transformer decoder, as is common practice in end-to-end closed-world detectors and trackers [6, 25, 36]. The decoder maps from image-centric encoder tokens to object-centric “slots”. In-Figure 1: Model overview. Our starting point is OWL-ViT [26] (left), which uses an encoder-only Vision Transformer (ViT) [9] architecture for simple transfer from image-text pretraining to open-world detection: encoder tokens, arranged on the image grid, are used directly as object queries for detection. To transfer to temporal tasks without requiring frame-to-frame matching, we ﬁrst develop a model variant inspired by DETR [6] that decouples object queries from the image grid (Enc-dec OWL-ViT, middle) by training a lightweight Transformer decoder on top of the ViT encoder while maintaining open-world detection capabilities. Finally, Video OWL-ViT (right) simply connects the output of Enc-dec OWL-ViT applied to one frame to the next frame by using the predicted object queries as queries for the OWL-ViT Decoder of the next time step, without the need for any matching. formation can then be carried through time by using the ob-ject slot representations from one frame as decoder queries on the next frame.
The object-centric decoder queries allow the model to learn temporally consistent representations end-to-end from video data. This distinguishes our approach from previous open-world tracking models [24], which applied frozen detectors frame-by-frame and used heuristics to link detections through time.
We provide a recipe for incorporating a decoder into
OWL-ViT and for ﬁne-tuning the resulting model on video data without losing its open-world detection capabilities.
We call the resulting model Video OWL-ViT. We demon-strate strong performance on a challenging open-world video localization and tracking task, TAO-OW [24], even for classes that were not seen during video training. We further demonstrate the zero-shot open-world generaliza-tion capabilities of Video OWL-ViT on a different dataset,
YT-VIS [35], that was not used for training. 2.