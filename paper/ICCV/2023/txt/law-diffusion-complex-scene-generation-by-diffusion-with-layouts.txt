Abstract
Thanks to the rapid development of diffusion models, un-precedented progress has been witnessed in image synthe-sis. Prior works mostly rely on pre-trained linguistic mod-els, but a text is often too abstract to properly specify all the spatial properties of an image, e.g., the layout config-uration of a scene, leading to the sub-optimal results of
In this paper, we achieve ac-complex scene generation. curate complex scene generation by proposing a seman-tically controllable Layout-AWare diffusion model, termed
LAW-Diffusion. Distinct from the previous Layout-to-Image generation (L2I) methods that primarily explore category-aware relationships, LAW-Diffusion introduces a spatial de-pendency parser to encode the location-aware semantic co-herence across objects as a layout embedding and produces a scene with perceptually harmonious object styles and con-textual relations. To be specific, we delicately instantiate each object’s regional semantics as an object region map and leverage a location-aware cross-object attention mod-ule to capture the spatial dependencies among those dis-entangled representations. We further propose an adap-tive guidance schedule for our layout guidance to mitigate the trade-off between the regional semantic alignment and the texture fidelity of generated objects. Moreover, LAW-Diffusion allows for instance reconfiguration while main-taining the other regions in a synthesized image by introduc-ing a layout-aware latent grafting mechanism to recompose its local regional semantics. To better verify the plausibil-ity of generated scenes, we propose a new evaluation metric for the L2I task, dubbed Scene Relation Score (SRS) to mea-sure how the images preserve the rational and harmonious relations among contextual objects. Comprehensive ex-periments on COCO-Stuff and Visual-Genome demonstrate that our LAW-Diffusion yields the state-of-the-art genera-tive performance, especially with coherent object relations.
*Corresponding author.
Figure 1. Illustration of complex scene generation by Stable Diffu-sion [28] (text-to-image model) and our LAW-Diffusion (layout-to-image model). Stable Diffusion relies on linguistic model and generates an unsatisfactory scene: the boat on the water is missed and the generated building and mountain are placed with undesired spatial relation according to the input description. By contrast,
LAW-Diffusion introduces a spatial dependency parser to encode the spatial semantic coherence and produces the scene image with consistent contextual relations adhere to the layout configuration. 1.

Introduction
Recently, astounding advances have been achieved in generative modeling due to the emergence of diffusion mod-els [34, 13, 28, 42, 1, 6]. Despite the stunning generative performance in simple cases, e.g., single object synthesis, how to generate a complex scene composed of multiple visual concepts with their diverse relationships remains a challenging problem. A straightforward solution is to trans-late the scene into a text description and then resort to the state-of-the-art text-to-image (T2I) generative models [28, 6, 7, 31, 26]. However, text-to-image diffusion models, e.g.,
Stable Diffusion and its variants [28, 6, 7, 31, 26] fall short when it comes to the spatial composition of multiple objects in a scene. An underlying reason is that properly specifying all the spatial properties in an abstractive sentence is labo-rious and less accurate, usually resulting in unsatisfactory generated results. In addition, the linguistic model used in
T2I model is incapable of accurately capturing the objects’
spatial relations whereas only providing a coarse-grained linguistic understanding from the text description. An ex-ample is shown in Fig. 1, in which we extract a sentence de-scription from a scene layout configuration and compare the generated results of Stable Diffusion [28] and our model.
From the result generated by Stable Diffusion in Fig. 1(a), we can observe that the spatial properties are not well pre-served (e.g., the generated mountain is besides the building while it should be behind the building) and some desired objects are missed (e.g., the boat and its reflection). By contrast, our method generates the scene image by directly parsing the spatial dependency in the layout configuration.
Layout-to-image generation (L2I) is a very important task of controllable image synthesis, which takes a con-figuration of visual concepts (i.e., objects’ bounding boxes with their class labels in a certain spatial layout) as the input. The scene layout precisely specifies each object’s size, location and its association to other objects. The key challenge for L2I lies in encoding the spatial depen-dencies among co-existing objects at each position, i.e., the location-aware semantic composition, which is vital to eliminate the artifacts of spurious edges between spatial adjacent or overlapped objects [11]. Existing studies on
L2I are usually developed based on the generative adver-sarial networks (GAN) [9, 37, 11, 38, 44]. These meth-ods render the realism of image contents with instance-specific style noises and discriminators, and thus suffer from the lack of overall harmony and style consistency among things and stuffs in the generated scene. They have made a few attempts to capture the class-aware relationships in the generator by adopting LSTM [44] or attention mech-anism [11]. Another type of approaches is based on trans-former [16, 41], which reformulates the scene generation task as a sequence prediction problem by converting the in-put layout and target image into a list of object tokens and patch tokens. The transformer [40] is then employed to se-quentially predict the image patches, which actually capture the sequential dependencies rather than scene coherence.
Recently, generic T2I diffusion models, e.g., LDM [28] and Frido [6] have been demonstrated that they can be ex-tended to L2I by tokenizing the layout into a sentence-like sequence of object tokens and encoding them by linguistic model, following their standard T2I paradigm. Such brute-force solutions share some shortcomings inherent to the T2I diffusion models, e.g., the aforementioned object leakage and unawareness of spatial dependencies in 1(a). But in fact, prior methods mainly exploit the location-insensitive relationships while overlooking the fine-grained location-aware cross-object associations.
To address the above issues, we present a novel diffusion model-based framework for L2I, termed LAW-Diffusion, for synthesizing complex scene images with mutually harmo-nious object relations. Unlike the traditional L2I methods treating each object separately, our LAW-Diffusion learns a layout embedding with rich regional composition semantics in a delicate manner for better exploring the holistic spa-tial information of objects. Concretely, we first instantiate each object’s regional semantics as an object region map that encodes the class semantic information in its bound-ing box. Then, we split those region maps into fragments and propose a location-aware cross-object attention module to perform per-fragment multi-head attention with a learn-able aggregation token to exploit the location-aware com-position semantics. By regrouping those aggregated frag-ments according to their original spatial locations, we ob-tain a layout embedding encapsulating both class-aware and
In this way, when synthe-location-aware dependencies. sizing a local fragment of image, such composed seman-tics faithfully specify whether objects are possibly over-lapped at the certain location. Inspired by the effectiveness of text-to-image diffusion models [26, 31, 24], we employ the form of classifier-free guidance [14] to amplify the re-gional control from our layout embedding. To avoid los-ing objects’ texture details when leveraging a large guid-ance scale, we further propose an adaptive guidance sched-ule for the sampling stage of LAW-Diffusion to maintain both layout semantic alignment and object’s texture fidelity by gradually annealing the guidance magnitude. Further-more, LAW-Diffusion allows for instance reconfiguration, e.g., adding/removing/restyling an instance in a generated scene via layout-aware latent grafting. Specifically, we spa-tially graft an exclusive region outside a bounding box from the diffusion latent of the already generated image onto the target latent guided by a new layout at the same noise level.
By alternately recomposing the local regional semantics and denosing these grafted latents, LAW-Diffusion can recon-figure an instance in a synthesized scene image while keep-ing the other objects unchanged.
The existing evaluation metrics for the L2I task basi-cally focus on measuring the fidelity of generated objects while ignoring the coherence among objects’ relations in the scene context. Thus, we propose a new evaluation met-ric called Scene Relation Score (SRS) to measure whether the generated scenes preserve the rational and harmonious relations among contextual objects, which would facilitate the development of L2I research. We conduct both quan-titative and qualitative experiments on Visual Genome [17] and COCO-Stuff [2], and the experimental results demon-strate that our LAW-Diffusion outperforms other L2I meth-ods and achieves the new state-of-the-art generative perfor-mance, particularly in preserving reasonable and coherent object relations. 2.