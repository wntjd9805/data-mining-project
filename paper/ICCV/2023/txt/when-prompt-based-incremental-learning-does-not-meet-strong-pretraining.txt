Abstract
Incremental learning aims to overcome catastrophic forgetting when learning deep networks from sequential tasks. With impressive learning efficiency and performance, prompt-based methods adopt a fixed backbone to sequen-tial tasks by learning task-specific prompts. However, ex-isting prompt-based methods heavily rely on strong pre-training (typically trained on ImageNet-21k), and we find that their models could be trapped if the potential gap between the pretraining task and unknown future tasks is large.
In this work, we develop a learnable Adaptive
Prompt Generator (APG). The key is to unify the prompt retrieval and prompt learning processes into a learnable prompt generator. Hence, the whole prompting process can be optimized to reduce the negative effects of the gap be-tween tasks effectively. To make our APG avoid learning ineffective knowledge, we maintain a knowledge pool to reg-ularize APG with the feature distribution of each class. Ex-tensive experiments show that our method significantly out-performs advanced methods in exemplar-free incremental learning without (strong) pretraining. Besides, under strong pretraining, our method also has comparable performance to existing prompt-based models, showing that our method can still benefit from pretraining. Codes can be found at https://github.com/TOM-tym/APG 1.

Introduction
Deep neural networks (DNNs) have become powerful tools in various fields [5, 7, 12, 15, 28, 56]. However, when facing sequential training tasks, DNNs learn new tasks along with severe performance degradation on previous tasks in the absence of old data, which is the notorious
Incremental learning catastrophic forgetting [11, 41, 49]. aims to overcome catastrophic forgetting in DNNs, push-*Corresponding author
Figure 1. Experimental comparison between our adaptive prompt-ing scheme and other prompted-based methods [60, 61] on the CI-‘Non-pretrained’ (a standard protocol in
FAR100 [25] dataset. incremental learning) means the data from the first task is used to pretrain the backbone. (a) With an intensive pretrained back-bone, all three methods perform well. (b) When swapping to Tiny-ImageNet (200 classes) pretrained weights, the performance of other methods clearly drops, while ours does not. (c) Our method significantly outperforms other methods in the presence of a large semantic gap between pretraining task and unknown future tasks. ing DNNs toward complex real-world applications, e.g.
AI robotics [4, 10, 42, 52] or self-driving [13, 16, 43, 45].
Previous works usually maintain a memory buffer with a handful of old samples for rehearsal when learning new tasks [18, 26, 48, 54, 58, 62, 63, 65]. Since keeping old data may be infeasible due to privacy/storage concerns, another branch of work [64, 66, 67] explores exemplar-free incre-mental learning, which tunes the network based on the in-troduced priors but the performance still far falls behind those of rehearsal-based methods.
Recently, an appealing development [59–61] based on prompting [21,27,29,34,35] manages to encode knowledge into sets of prompts to steer a frozen backbone for handling sequential tasks. In addition to impressive performance, it has several benefits. (1) The catastrophic forgetting prob-lem is effectively alleviated since the backbone is fixed; (2) learning prompts instead of backbone significantly reduces training costs and improves learning efficiency; (3) prompt-based methods are free from keeping exemplars. To employ prompts for task-agnostic class-incremental learning, a cru-cial step is to select task-specific prompts given any input images. Existing methods maintain a prompt pool and re-trieve prompts by directly computing the similarity between the image feature extracted by the pretrained model and the prompts in the pool, which is simple yet effective with a strong pretrained model. However, as the pretrained model dominates the retrieval, such a non-learnable retrieval pro-cess will be problematic because the future tasks are un-known and the gap between the pretraining task and the un-known future tasks could be large. As in Fig. 1 (c), when the first task in incremental learning is used for pretrain-ing, the classes in the pretraining task are totally different from other tasks, which we refer to as a semantic gap that degenerates existing models. Although the semantic gap between domains is also studied in some works such as transfer learning [14, 44, 53], their works do not consider the forgetting problem in sequential tasks. It is necessary to emphasize that the intention of this work is NOT to refuse pretraining but to propose a more general method that does not rely heavily on strong pretraining and can benefit from it if task-related pretraining is available. For more experi-ments regarding the necessity of our work, please refer to the supplementary materials.
In this work, we develop a learnable Adaptive Prompt
Generator (APG) to effectively bridge the potential gap be-tween pretraining tasks and unknown future tasks. The core of our method is to unify the prompt retrieval and the prompt learning process into a learnable prompt generator.
In this way, the whole prompting process can be optimized to reduce the negative effects of the gap between tasks ef-fectively. Besides, rather than retrieving prompts from a fixed-size prompt pool, learning to generate prompts en-hances the expression ability of prompts. As a result, the
APG can be applied to a model without strong pretraining, and notably, the employment of APG does not discount the effort on overcoming forgetting since the backbone is still fixed.
For incremental learning, APG holds an extendable prompt candidate list for aggregating knowledge from seen tasks into a group of prompts. To adaptively prompt the backbone, the knowledge aggregation in APG is condi-tioned on the immediate feature from the backbone. In ad-dition, we form a knowledge pool to summarize the knowl-edge encoded in the feature space. The summarized knowl-edge is further used to regularize the AGP to prevent it from learning ineffective knowledge.
In summary, our contributions are as follows. (1) We propose a learnable adaptive prompt generator (APG) to re-duce the negative effects of the gap between the pretraining task and unknown future tasks, which is critical but ignored by previous work. Our adaptive prompting eases the re-liance on intensive pretraining. (2) To regularize APG, we propose the knowledge pool, which retains the knowledge effectively with only the statistics of each class. (3) The extensive experiments show that our method significantly outperforms advanced exemplar-free incremental learning methods without pretraining. Besides, under strong pre-training, our method also achieves comparable satisfactory performance to existing prompt-based models. 2.