Abstract
Modern object detectors have taken the advantages of backbone networks pre-trained on large scale datasets. Ex-cept for the backbone networks, however, other components such as the detector head and the feature pyramid network (FPN) remain trained from scratch, which hinders the gen-eralization capacity of detectors.
In this study, we pro-pose to integrally migrate pre-trained transformer encoder-decoders (imTED) to a detector, constructing a feature ex-traction path which is “fully pre-trained” so that detec-tors’ generalization capacity is maximized. The essential differences between imTED with the baseline detector are twofold: (1) migrating the pre-trained transformer decoder to the detector head while removing the randomly initial-ized FPN from the feature extraction path; and (2) defining a multi-scale feature modulator (MFM) to enhance scale adaptability. Such designs not only reduce randomly initial-ized parameters significantly but also unify detector train-ing with representation learning intendedly. Experiments on the MS COCO object detection dataset show that imTED consistently outperforms its counterparts by ∼2.4 AP. With-out bells and whistles, imTED improves the state-of-the-art of few-shot object detection by up to 7.6 AP. Code is re-leased at https://github.com/LiewFeng/imTED. 1.

Introduction
Over the past two years, vision transformers (ViTs) [6] have been promising representation models. The vanilla transformer trained with a sophisticated self-supervised learning method, e.g., masked autoencoder (MAE) [11], demonstrated great potential. Since the introduction of transformers [35] to computer vision, the effort of taming
*Equal Contribution.
†Corresponding Author.
Figure 1: Comparison of the baseline detector e.g., Faster
R-CNN [30] using a transformer backbone (upper) with the proposed imTED (lower). The baseline detector solely transfers a pre-trained backbone network, e.g., the encoder, but training the detector head and FPN from scratch. By contrast, our imTED approach integrally migrates the pre-It significantly re-trained transformer encoder-decoder. duces the proportion of randomly initialized parameters and improves detector’s generalization capability. them for object detection has never stopped [3, 19]. This is motivated by the observation that ViTs pre-trained on ex-traordinarily large datasets incorporate over-completed and versatile features, which guarantee the performance and generalization capability of detectors finetuned on small datasets. [19, 17].
Modern object detectors, such as Faster R-CNN and
Mask R-CNN [30, 12], typically consist of a backbone net-work, a neck component and a detector head. However, except for the backbone network, other components that oc-cupy a significant proportion of parameters remain trained
from scratch, Fig. 1(upper). Such components, including but not limited to the region proposal network (RPN) [30], the feature pyramid network (FPN) [20] and the detector head [9], fail to take advantages of the representation mod-els pre-trained on large-scale datasets.
In this study, we do not design any new components for object detection; instead, we devote to take full advantages of pre-trained models to improve detector’s generalization capability. Specifically, we propose to integrally migrate pre-trained transformer encoder-decoders (imTED) to de-tectors, Fig. 1(lower), constructing a feature extraction path which is not only “fully pre-trained” but also consistent with pre-trained models, as much as possible.
As shown in Fig. 1(lower), imTED employs the ViT en-coder pre-trained with MAE [11] as backbone, and uses the decoder as the detector head. It breaks the routine to re-move the randomly initialized FPN from the feature extrac-tion path while leveraging the adaptive respective field pro-vided by the attention mechanism in ViTs [6, 28] to handle objects at multiple scales. These designs support the in-tegral migration of pre-trained encoder-detector to the ob-ject detection pipeline. By adding linear output layers, i.e., a light-weight classification layer and a bounding-box re-gression layer, atop the migrated encoder-decoder, imTED realizes object classification and localization. To enhance the capacity for multi-scale object detection, we introduce a multi-scale feature modulator (MFM), which combines both the advantages of FPN with those of fully pre-trained models.
The competitiveness of imTED is validated upon popular detectors including Faster R-CNN and Mask R-CNN [30, 12]. Experiments on the MS COCO dataset demonstrate that imTED with ViT-base model outperforms its counter-part by ∼2.4 AP at moderate computational cost. Benefiting from the integral migration of pre-trained models, imTED demonstrates strong generalization capability, which is val-idated by low/few-shot detection tasks. When reducing pro-portions of the training data, performance gains of imTED monotonously increase. When training a few-shot detector, by freezing the backbone network while finetuning the rest detector components, imTED improves the state-of-the-art by up to 7.6 AP. imTED opens up a promising direction for few-shot object detection using vision transformers.
The contributions of this study include:
• We integrally migrate pre-trained transformer encoder-decoders (imTED) to object detectors, constructing a
“fully pre-trained” feature extraction path to improve detectors’ generalization capacity.
• We redesign the feature extraction path to guarantee the “integral migration” of the pre-trained transformer encoder-decoders. We introduce a multi-scale feature modulator (MFM), to improve the scale adapatiblity of imTED.
• imTED not only achieves significant performance gains on object detection and few-shot object detec-tion, but also takes a step towards unifying detector training with representation learning. 2.