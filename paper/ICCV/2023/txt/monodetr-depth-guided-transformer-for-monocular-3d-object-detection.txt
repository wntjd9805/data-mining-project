Abstract
Specifically, concurrent
Monocular 3D object detection has long been a challenging task in autonomous driving. Most existing methods follow conventional 2D detectors to first localize object centers, and then predict 3D attributes by neighboring features.
However, only using local visual features is insufficient to understand the scene-level 3D spatial structures and ignores the long-range inter-object depth relations.
In this paper, we introduce the first DETR framework for
Monocular DEtection with a depth-guided TRansformer, named MonoDETR. We modify the vanilla transformer to be depth-aware and guide the whole detection process by contextual depth cues. to the visual encoder that captures object appearances, we introduce to predict a foreground depth map, and specialize a depth encoder to extract non-local depth embeddings. Then, we formulate 3D object candidates as learnable queries and propose a depth-guided decoder to conduct object-scene depth interactions. In this way, each object query estimates its 3D attributes adaptively from the depth-guided regions on the image and is no longer constrained to local visual features. On KITTI benchmark with monocular images as input, MonoDETR achieves state-of-the-art performance and requires no extra dense depth annotations. Besides, our depth-guided modules can also be plug-and-play to enhance multi-view 3D object detectors on nuScenes dataset, demonstrating our superior generalization capacity. Code is available at https:
//github.com/ZrrSkywalker/MonoDETR. 1.

Introduction
With a wide range of applications in autonomous driv-ing, 3D object detection is more challenging than its 2D counterparts, due to the complex spatial circumstances.
† Corresponding author
Figure 1. Center-guided (Top) and Depth-guided Paradigms (Bottom) for monocular 3D object detection. Existing center-guided methods predict 3D attributes from local visual features around the centers, while our MonoDETR guides the detection by a predicted foreground depth map and adaptively aggregates fea-tures in global context. The lower right figure visualizes the atten-tion map of the target query in the depth cross-attention layer.
Compared to methods based on LiDAR [57, 20, 41, 50] and multi-view images [49, 22, 26, 17], 3D object detection from monocular (single-view) images [9, 1, 46] is of most difficulty, which generally does not rely on depth measure-ments or multi-view geometry. The detection accuracy thus severely suffers from the ill-posed depth estimation, leading to inferior performance.
Except for leveraging pseudo 3D representations [48, 51, 32, 38], standard monocular 3D detection methods [33, 52, 53, 31] follow the pipeline of traditional 2D object detec-tion [40, 24, 43, 56]. They first localize objects by detecting the projected centers on the image, and then aggregate the neighboring visual features for 3D property prediction, as illustrated in Figure 1 (Top). Although it is conceptually straightforward, such center-guided methods are limited by the local appearances without long-range context, and fail to capture implicit geometric cues from 2D images, e.g., depth guidance, which are critical to detect objects in 3D space.
To tackle this issue, we propose MonoDETR, which
Figure 2. Comparison of DETR-based methods for camera-based 3D object detection. We utilize yellow, blue, green, and red to respectively denote the feature or prediction space related with 2D, depth, 3D, and BEV. Different from other methods, our MonoDETR leverages depth cues to guide 3D object detection from monocular images. presents a novel depth-guided 3D detection scheme in Fig-ure 1 (Bottom). Compared to DETR [4] in 2D detection, the transformer in MonoDETR is equipped with depth-guided modules to better capture contextual depth cues, serving as the first DETR model for monocular 3D object detection, as shown in Figure 2 (a) and (b). It consists of: two parallel encoders for visual and depth representation learning, and a decoder for adaptive depth-guided detection.
Specifically, after the feature backbone, we first utilize a lightweight depth predictor to acquire the depth features of the input image. To inject effective depth cues, a fore-ground depth map is predicted on top, and supervised only by discrete depth labels of objects, which requires no dense depth annotations during training. Then, we apply the par-allel encoders to respectively generate non-local depth and visual embeddings, which represent the input image from two aspects: depth geometry and visual appearance. On top of that, a set of object queries is fed into the depth-guided decoder, and conducts adaptive feature aggregation from the two embeddings. Via a proposed depth cross-attention layer, the queries can capture geometric cues from the depth-guided regions on the image, and explore inter-object depth relations. In this way, the 3D attribute predic-tion can be guided by informative depth hints, no longer constrained by the limited visual features around centers.
As an end-to-end transformer-based network, Mon-oDETR is free from non-maximum suppression (NMS) or rule-based label assignment. We only utilize the object-wise labels for supervision without using auxiliary data, such as dense depth maps or LiDAR. Taking monocular images as input, MonoDETR achieves state-of-the-art per-formance among existing center-guided methods, and sur-passes the second-best by +2.53%, +1.08%, and +0.85% for three-level difficulties on KITTI [14] test set.
Besides single-view images, the depth-guided mod-ules in MonoDETR can also be extended as a plug-and-play module for multi-view 3D detection on nuScenes [3] dataset. By providing multi-view depth cues, our method can not only improve the end-to-end detection performance of PETRv2 [27] by +1.2% NDS, but also benefit the BEV representation learning in BEVFormer [22] by +0.9% NDS.
This further demonstrates the effectiveness and generaliz-ability of our proposed depth guidance.
We summarize the contributions of our paper as follows:
• We propose MonoDETR, a depth-guided framework to capture scene-level geometries and inter-object depth relations for monocular 3D object detection.
• We introduce a foreground depth map for object-wise depth supervision, and a depth cross-attention layer for adaptive depth features interaction.
• MonoDETR achieves leading results on monocular
KITTI benchmark, and can also be generalized to en-hance multi-view detection on nuScenes benchmark. 2.