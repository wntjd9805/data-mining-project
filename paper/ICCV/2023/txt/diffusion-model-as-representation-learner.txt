Abstract
Diffusion Probabilistic Models (DPMs) have recently demonstrated impressive results on various generative tasks. Despite its promises, the learned representations of pre-trained DPMs, however, have not been fully under-stood. In this paper, we conduct an in-depth investigation of the representation power of DPMs, and propose a novel knowledge transfer method that leverages the knowledge acquired by generative DPMs for recognition tasks. Our study begins by examining the feature space of DPMs, re-vealing that DPMs are inherently denoising autoencoders that balance the representation learning with regularizing model capacity. To this end, we introduce a novel knowl-edge transfer paradigm named RepFusion. Our paradigm extracts representations at different time steps from off-the-shelf DPMs and dynamically employs them as supervision for student networks, in which the optimal time is deter-mined through reinforcement learning. We evaluate our ap-proach on several image classification, semantic segmen-tation, and landmark detection benchmarks, and demon-strate that it outperforms state-of-the-art methods. Our results uncover the potential of DPMs as a powerful tool for representation learning and provide insights into the usefulness of generative models beyond sample genera-tion. The code is available at https://github.com/
Adamdad/Repfusion. 1.

Introduction
In the ever-evolving landscape of machine learning, gen-erative models have emerged as a captivating approach to tackle the intricacies of data distributions. Among these marvels, Diffusion Probabilistic Models (DPMs) stand tall, boasting a remarkable prowess in producing realistic and di-verse samples. Powered by the elegent design of diffusion, these models elegantly transform a humble noise into tar-get data distribution, unfurling a breathtaking array of vari-ations and unyielding fidelity in their artistic creations.
*Corresponding Author.
Figure 1. Illustration of our core idea on establishing a link be-tween representation learning and diffusion models. Building upon this connection, we propose to utiliz the knowledge acquired from pre-trained DPMs. This knowledge is selectively distilled into a student network, empowering it to carry out recognition tasks with enhanced proficiency.
Although the generative ability of DPMs has been ex-tensively studied, their potential for representation learning has not been fully explored. Recent research has demon-strated that the diffusion models already have a semantic la-tent space[32], and could be extended to tasks like control-lable image generation [51], representation learning [1, 46] and image segmentation [5, 76], albeit through complicated model modifications. Nevertheless, the usefulness of the learned feature space of DPMs remain unclear. In this work, we intend to close the gap between generative DPM and its capability in representation learning by answering the fol-lowing question: Can the representation learned by DPMs be effectively reused for recognition tasks?
We answer this question by first analyzing the inherent relationship between the diffusion model and the standard auto-encoder, as shown in Figure 1. Conceptually, DPMs are designed to predict noise from perturbed input distribu-tions. Essentially, DPMs can be viewed as denoising auto-encoders (DAEs) [68, 67] with varying denoising scales.
DAEs have been well-established as a powerful technique for self-supervised learning, which captures the underlying
structures of data. Its latent features are useful for down-stream tasks. On another note, DPMs are commonly known as score-matching models [62], which model the gradient of the log probability density with a sequence of interme-diate latent variables. By connecting score-matching with
DAEs [66], DPMs should also be considered an extension of DAE. It is, therefore, intuitive that DPM produces mean-ingful encoding from its input.
Although the representational power of the diffusion model has well-established theoretical foundations, lever-aging the off-the-self DPM for non-generative tasks poses significant challenges. Primarily, these models are parame-terized as time-conditioned Unet [55, 14, 63], a specialized structure unsuited for tasks such as classification and object detection. Secondly, existing DPMs are computationally heavy [78], making it difficult to use the original model for discriminative tasks without substantial modifications. An-other obstacle arises from the fact that a single DPM can be perceived as a composition of networks, each indexed by its input timestep. Determining the suitable time-step remains non-trivial. Consequently, the representations learned by
DPMs do not readily benefit other non-generative tasks.
In this paper, our primary objective is to reuse the knowl-edge encoded in the DPM for recognition tasks [13]. To achieve this, we look into the mathematical formulation of DPMâ€™s latent space, and show that DPMs can be seen as DAEs that strike a balance between learning meaning-ful features and regularizing the model capacity. Build-ing on this insight, we propose a novel knowledge trans-fer approach, termed RepFusion. Our approach utilizes the knowledge distillation techniques to transfer the represen-tation learning capability of the trained generative models to improve discriminative tasks. Specifically, we dynami-cally extract intermediate representations at different time steps and use them as auxiliary supervision for student net-works. To determine the optimal time selection, we measure the informativeness of a given representation and optimize it through the REINFORCED algorithm [72]. Moreover, the reinforced time-steps selection is aligned with the de-rived property of DPMs, thereby providing a mechanism for adapting to different downstream tasks and increasing the flexibility and generalizability of our approach.
Our experiments demonstrate that RepFusion consis-tently improves performance on several image classifica-tion, semantic segmentation, and landmark detection bench-marks, indicating the powerful representation learning ca-pability of DPMs. These findings shed light on the poten-tial utility of generative models beyond their traditional use in sample generation and highlight the opportunities for ex-ploring pre-trained models in representation learning.
To sum up, our contributions can be divided into three main parts: 1. We investigate the potential of repurposing diffusion models for representation learning, an area that has been relatively unexplored in prior research. 2. By establishing the relationship between DPMs and denoising auto-encoders, we verify the statistical and empirical properties of features extracted from DPMs. 3. We introduce a novel knowledge distillation approach called RepFusion, which utilizes pre-trained DPMs to enhance recognition tasks. Extensive evaluations on image classification, segmentation, and landmark de-tection benchmarks demonstrate the effectiveness of
DPMs as powerful tools for representation learning. 2.