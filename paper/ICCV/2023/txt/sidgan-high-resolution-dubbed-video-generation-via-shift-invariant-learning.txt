Abstract ing AVSpeech, HDTF, and LRW, in terms of photo-realism, identity preservation, and Lip-Sync accuracy.
Dubbed video generation aims to accurately synchronize mouth movements of a given facial video with driving audio while preserving identity and scene-specific visual dynam-ics, such as head pose and lighting. Despite the accurate lip generation of previous approaches that adopts a pre-trained audio-video synchronization metric as an objective function, called Sync-Loss, extending it to high-resolution videos was challenging due to shift biases in the loss land-scape that inhibit tandem optimization of Sync-Loss and vi-sual quality, leading to a loss of detail.
To address this issue, we introduce shift-invariant learn-ing, which generates photo-realistic high-resolution videos with accurate Lip-Sync. Further, we employ a pyramid net-work with coarse-to-fine image generation to improve sta-bility and lip syncronization. Our model outperforms state-of-the-art methods on multiple benchmark datasets, includ-∗Equal contribution. 1.

Introduction
Dubbed video generation aims at Lip-Syncing a face at each query frame with driving audio while retaining its vi-sual identity and pose as shown in Figure 1. With the un-precedented expansion of multimedia industry, there has been a huge rise in video content that involves speakers de-livering dialogues. For those videos, dubbed video genera-tion can facilitate many applications, such as avatar anima-tion, automated creating of audio-visual content, and visual dubbing of movies. Due to recent advances in camera sen-sors, internet speed, and display, high resolution video con-tent (4K or more) has become a necessity in most computer vision applications, including dubbed video generation.
Generating lip motions accurately synchronized with au-dio has been a major challenge in dubbed video genera-conventional reconstruction losses suppress high frequen-cies textures vital for high-resolution dubbing. To jointly address these two problems, we propose to replace the tra-ditional objectives with shift-invariant ones. By adopting adaptive polyphase sampling [4] and contextual loss [25], we enhance shift-invariance of Sync-Loss and reconstruc-tion loss, respectively. As shown in Figure 2 (b), the shift-invariant learning makes both types of losses more flexible on the lip position domain, facilitating a learning objective, where both losses are small (single minimum). We concep-tually and empirically show that shift-invariant learning is critical in high-resolution dubbed video generation.
Another obstacle is unstable training when supervising models at high-resolution output only. To tackle this, we propose a pyramid network with supervisions at multiple resolutions, allowing us to use a coarse to fine learning strat-egy. We reconstruct coarse geometry at lower resolutions where SyncNet is more stable, and higher resolution mod-ules focus on generating high-fidelity textures.
By combining the above solutions, we develop a novel dubbed video generator, called SIDGAN. Experimental re-sults show that our SIDGAN significantly outperforms the existing methods in terms of visual quality on three datasets;
AVSpeech [9], HDTF [41], and LRW [6]. Our three major contributions are as follows:
• Analysis of the necessity of shift-invariant learning to generate high-resolution dubbed video while achieving accurate Lip-Sync.
• Building a coarse-to-fine pyramid model to enable gradual improvement on fine details on faces.
• Remarkable quantitative and qualitative performance on both high-resolution and low-resolution datasets. 2.