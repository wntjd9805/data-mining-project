Abstract
Current model quantization methods have shown their promising capability in reducing storage space and computa-tion complexity. However, due to the diversity of quantization forms supported by different hardware, one limitation of ex-isting solutions is that usually require repeated optimization for different scenarios. How to construct a model with flexi-ble quantization forms has been less studied. In this paper, we explore a one-shot network quantization regime, named
Elastic Quantization Neural Networks (EQ-Net), which aims to train a robust weight-sharing quantization supernet. First of all, we propose an elastic quantization space (including elastic bit-width, granularity, and symmetry) to adapt to vari-ous mainstream quantitative forms. Secondly, we propose the
Weight Distribution Regularization Loss (WDR-Loss) and
Group Progressive Guidance Loss (GPG-Loss) to bridge the inconsistency of the distribution for weights and output logits in the elastic quantization space gap. Lastly, we in-corporate genetic algorithms and the proposed Conditional
Quantization-Aware Accuracy Predictor (CQAP) as an es-timator to quickly search mixed-precision quantized neural networks in supernet. Extensive experiments demonstrate that our EQ-Net is close to or even better than its static coun-terparts as well as state-of-the-art robust bit-width methods.
Code can be available at https://github.com/xuke225/EQ-Net. 1.

Introduction
Deploying intricate deep neural networks(DNN) on edge devices with limited resources, such as smartphones or IoT devices, poses a significant challenge due to their demanding computational and memory requirements. Model quantiza-tion [13, 28, 33] has emerged as a highly effective strategy to mitigate the aforementioned challenge. This technique involves transforming the floating-point values into fixed-point values of lower precision, thereby reducing the memory requirements of the DNN model without altering its origi-nal architecture. Additionally, computationally expensive floating-point matrix multiplications between weights and activations can be executed more efficiently on low-precision arithmetic circuits, leading to reduced hardware costs and lower power consumption.
Despite the evident advantages in terms of power and costs, quantization incurs added noise due to the reduced pre-cision. However, recent research has demonstrated that neu-ral networks can withstand this noise and maintain high accu-racy even when quantized to 8-bits using post-training quan-tization (PTQ) techniques [26, 30, 27, 24, 46]. PTQ is typi-cally efficient and only requires access to a small calibration dataset, but its effectiveness declines when applied to low-bit quantization (≤ 4-bits) of neural networks. In contrast, quantization-aware training (QAT) [52, 7, 14, 11, 4, 21, 29] has emerged as the prevailing method for achieving low-bit quantization while preserving near full-precision accuracy.
By simulating the quantization operation during training or fine-tuning, the network can adapt to the quantization noise and yield better solutions than PTQ.
Currently, most AI accelerators support model quanti-zation, but the forms of quantization supported by differ-ent hardware platforms are not exactly the same [25]. For example, NVIDIA’s GPU adopts channel-wise symmetric quantization in TensorRT [31] inference engine, while Qual-comm’s DSP adopts per-tensor asymmetric quantization in
SNPE [34] inference engine. For conventional QAT meth-ods, the different quantization forms supported by hardware platforms may require repeated optimization of the model during deployment on multiple devices, leading to extremely low efficiency of model quantization deployment.
To address the problem of repeated optimization in model quantization resulting from discrepancies in quantization schemes, this paper proposes an elastic quantization space
Figure 1: A conceptual overview of EQ-Net approach. design that encompasses the current mainstream quantiza-tion scenarios and classifies them into elastic quantization bit-width (2-bit, 4-bit, 8-bit, etc.), elastic quantization gran-ularity (per-layer quantization, per-channel quantization), and elastic quantization symmetry (symmetric quantization, asymmetric quantization), as shown in Figure 1. This ap-proach enables flexible deployment models under different quantization scenarios by designing a unified quantization formula that integrates various model quantization forms and implementing elastic switching of quantization bit-width, granularity, and symmetry through parameter splitting.
Inspired by one-shot neural architecture search [5, 51, 44, 48], this paper attempts to train a robust elastic quantiza-tion supernet based on the constructed elastic quantization space. Unlike neural architecture search, the elastic quan-tization supernet is fully parameter-shared, and there is no additional weight parameter optimization space with net-work structure differences. Therefore, training the elastic quantization supernet may encounter the problem of negative gradient suppression [41, 49] due to different quantization forms. In other words, samples with inconsistent predic-tions between quantization configuration A (e.g., 8-bit/per-channel/asymmetric) and quantization configuration B (e.g., 2-bit/per-tensor/symmetric) are considered negative samples by each other, which slows down the convergence speed of the supernet during training. To solve the aforementioned problem, this paper proposes an efficient training strategy for elastic quantization supernet. Our goal is to reduce neg-ative gradients by establishing consistency in weight and logits distributions: (1) introducing the Weight Distribution
Regularization (WDR) to perform skewness and kurtosis reg-ularization on shared weights, to better align the elastic quan-tization space and establish weight distribution consistency; (2) introducing the Group Progressive Guidance (GPG) to group the quantization sub-networks and guide them with progressive soft labels during the supernet training stage to establish consistency in output logits distributions.
As shown in Figure 1, the trained elastic quantization su-pernet can achieve both uniform and mixed-precision quanti-zation (MPQ). Compared with previous MPQ works [45, 16, 10, 9, 20], our method can specify any quantization bit-width and forms in the elastic quantization space and quickly obtain a quantized model with the corresponding accuracy. With these features, we propose a Conditional
Quantization-Aware Accuracy Predictor (CQAP), combined with a genetic algorithm to efficiently search for the Pareto solution on mixed-precision quantization models under the target quantization bit-width and forms. 2.