Abstract
Visible-infrared person re-identification (VI-ReID) is a challenging task due to large cross-modality discrepancies and intra-class variations. Existing methods mainly focus on learning modality-shared representations by embedding different modalities into the same feature space. As a result, the learned feature emphasizes the common patterns across modalities while suppressing modality-specific and identity-aware information that is valuable for Re-ID. To address these issues, we propose a novel Modality Unifying Net-work (MUN) to explore a robust auxiliary modality for VI-ReID. First, the auxiliary modality is generated by combin-ing the proposed cross-modality learner and intra-modality learner, which can dynamically model the modality-specific and modality-shared representations to alleviate both cross-modality and intra-modality variations. Second, by align-ing identity centres across the three modalities, an identity alignment loss function is proposed to discover the discrim-inative feature representations. Third, a modality align-ment loss is introduced to consistently reduce the distribu-tion distance of visible and infrared images by modality pro-totype modeling. Extensive experiments on multiple public datasets demonstrate that the proposed method surpasses the current state-of-the-art methods by a significant margin. 1.

Introduction
Person re-identification (Re-ID) [8,33] aims at matching pedestrian images captured from multiple non-overlapping cameras. Over the past few years, it has received increased attention due to its huge practical value in modern surveil-lance systems. Previous studies [10, 16, 19, 30, 40] mainly focus on matching pedestrian images captured from visible cameras and formulate the Re-ID task as a single-modality
*Corresponding Author (Email: xcheng@nuist.edu.cn) matching issue. Nevertheless, visible cameras may not pro-vide accurate appearance information about persons in sce-narios with poor illumination. To address this limitation, modern surveillance systems also employ infrared cameras, which can capture clear images in low-light conditions at night. As a result, visible-infrared person re-identification (VI-ReID) [1, 28, 29] has become a topic of growing inter-est in recent times, which seeks to match infrared images of the same identity when given a visible query across multiple camera views and vice versa.
VI-ReID is challenging due to the huge cross-modality discrepancy between visible and infrared images, as well as the intra-modality variation in person bodies such as pose variation and dress change. Existing methods [1, 20, 29, 31, 36, 37] primarily focus on relieving the cross-modality discrepancy by extracting modality-shared fea-tures to perform the feature-level alignment. Some stud-ies [1,20,28,31,34] employ two-stream networks for cross-modality feature embedding, while others [3, 24, 25, 36] utilize Generative Adversarial Networks (GANs) to gener-ate shared representations from visible and infrared images.
However, these methods discard modality-specific features (such as colour and texture) that contain useful identity-aware patterns against intra-modality variations. Conse-quently, the learned features may not fully capture the varia-tion of human bodies and thus lack discriminability. To ad-dress this limitation, the modality-unifying methods, e.g.,
X-modality [9], DFM [7], SMCL [27], have been pro-posed to acquire the auxiliary modality by fusing visible and infrared modalities, encoding both modality-specific and modality-shared patterns to jointly relieve cross- and intra-modality discrepancies. In the SMCL [27], the authors pro-posed a syncretic modality generated by fusing visible and infrared pixels, which can bridge the gap between visible and infrared modalities while maintaining discriminability as the modality-specific information is preserved.
However, the existing modality-unifying works still have three weaknesses. (1) Pixel fusion. Previous works gener-body across multiple receptive fields. Based on the outputs of two IMLs, the CML leverages spatial pyramid pooling to extract multi-scale feature representations and then fuse the modality-shared patterns learned in each feature scale. By combining IML and CML, the proposed auxiliary genera-tor can generate a powerful auxiliary modality that is rich in modality-shared and discriminative patterns to alleviate both cross-modality and intra-modality discrepancies.
In addition, the layer scale scheme is used to control the ra-tio of patterns learned from IML and CML, which can dy-namically adjust the modality-specific and modality-shared patterns in the generated auxiliary representation.
Furthermore, to reveal the identity-related patterns in each identity set, an effective identity alignment loss (Lia) is designed to optimize the distances of tri-modality iden-tity centres. In addition, to regulate the distribution level feature relationships while relieving the inconsistency issue caused by sample variations, a novel modality alignment loss (Lma) is designed to minimize the distances of three modalities, which utilizes the modality prototype to repre-sent the learned modality information in each iteration.
In general, the major contributions of this paper can be summarized as follows.
• We propose a novel modality unifying network for the VI-ReID task by constructing a robust auxiliary modality, which contains rich semantic information from visible and infrared images to address modality discrepancies and reveal discriminative knowledge.
• A novel auxiliary generator constructed by the intra-modality and cross-modality learners is introduced to dynamically extract identity-aware and modality-shared patterns from heterogeneous images.
• The identity alignment loss and modality alignment loss are designed to jointly explore the generalized and discriminative feature relationships of the three modal-ities at both the identity and distribution levels.
• Extensive experiments on several public VI-ReID datasets verify the effectiveness of the proposed method and modality unifying scheme, which outper-forms the current state of the arts by a large margin. 2.