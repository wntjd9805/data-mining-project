Abstract
In semi-supervised learning, unlabeled samples can be utilized through augmentation and consistency regulariza-tion. However, we observed certain samples, even undergo-ing strong augmentation, are still correctly classified with high confidence, resulting in a loss close to zero. It indi-cates that these samples have been already learned well and do not provide any additional optimization benefits to the model. We refer to these samples as “naive sam-ples”. Unfortunately, existing SSL models overlook the characteristics of naive samples, and they just apply the same learning strategy to all samples. To further optimize the SSL model, we emphasize the importance of giving at-tention to naive samples and augmenting them in a more diverse manner. Sample adaptive augmentation (SAA) is proposed for this stated purpose and consists of two mod-ules: 1) sample selection module; 2) sample augmentation module. Specifically, the sample selection module picks out naive samples based on historical training informa-tion at each epoch, then the naive samples will be aug-mented in a more diverse manner in the sample augmen-tation module. Thanks to the extreme ease of implementa-tion of the above modules, SAA is advantageous for being simple and lightweight. We add SAA on top of FixMatch and FlexMatch respectively, and experiments demonstrate
SAA can significantly improve the models. For example,
SAA helped improve the accuracy of FixMatch from 92.50% to 94.76% and that of FlexMatch from 95.01% to 95.31% on CIFAR-10 with 40 labels. The code is available at https://github.com/GuanGui-nju/SAA.
*Corresponding author: Yinghuan Shi. Guan Gui, Yinghuan Shi are with the National Key Laboratory for Novel Software Technology and the National Institute of Healthcare Data Science, Nanjing Univer-sity. Lei Qi is with the school of Computer Science and Engineer-ing, Southeast University. This Work is supported by NSFC Program (62222604, 62206052, 62192783), China Postdoctoral Science Founda-tion Project (2023T160100), Jiangsu Natural Science Foundation Project (BK20210224), and CCF-Lenovo Bule Ocean Research Fund. (a) An example of naive sample. (b) Model performance during training.
Figure 1: (a) shows an example of naive sample. Its aug-mented versions are correctly classified with high confi-dence, resulting in the loss close to 0. (b) shows the model performance during FixMatch training. Performance im-provements are slow or even stagnant for a period of time. 1.

Introduction
For the sake of reducing the cost of manual labeling, semi-supervised learning (SSL), which focuses on how to learn from unlabeled data, is a longstanding yet significant research topic in vision applications. Recently, data aug-mentation techniques and consistency regularization have been proven to be effective ways of utilizing unlabeled data.
For example, FixMatch [39] encourages consistency in pre-dictions between the weakly and strongly augmented ver-sions, and it achieves an accuracy of 92.50% on the CIFAR-10 task with only 40 labels.
However, not all unlabeled samples are effectively uti-lized even with strong augmentation. In Figure 1a, if the strongly augmented versions are correctly classified with
high confidence, leading to a loss close to zero, it indicates that the sample has already been learned well and cannot further improve the model’s performance. In other words, the sample was not effectively utilized to benefit model training, and we call this sample “naive sample”. When the training process contains a large number of naive sam-ples, it can cause slow or even stagnant model performance improvements, as shown in Figure 1b.
Unfortunately, existing SSL models [34] overlook the critical point of whether all samples are effectively utilized.
Typically, these models apply the same fixed strong aug-mentation strategy to all samples, resulting in some strongly augmented versions that do not benefit the model train.
We emphasize that the key to alleviating this problem lies in how to further explore the value of the naive samples through new learning strategies. A natural idea that reminds us is to develop sample adaptive augmentation (SAA) to identify naive samples and increase their diversity after augmentation. Our proposed SAA is simple yet effective, which consists of two modules: 1) sample selection mod-ule and 2) sample augmentation module. The former is re-sponsible for picking out naive samples in each epoch, and the latter applies a more diverse augmentation strategy for naive samples. Specifically, in the sample selection module, we first update the historical loss of the samples with expo-nential moving average (EMA) in each epoch, then these samples will be divided into two parts. The part of the sam-ples with a smaller historical loss is considered to be the naive sample. Since historical loss captures the impact of the sample on model training, this approach allows us to identify samples that are not effectively utilized and would benefit from more diverse augmentation. While in the sam-ple augmentation module, the more diverse augmented ver-sion of naive sample will be obtained by regrouping mul-tiple strong augmented images, and the remaining samples are applied with the original strong augmentation.
Our proposed SAA is simple to implement, requiring only a few lines of code to add our proposed modules to the
FixMatch or FlexMatch in PyTorch. It is also lightweight in terms of memory and computation, i.e., SAA only needs to add two additional vectors and update them in each epoch, making it an efficient solution for improving SSL models.
We extended FixMatch and FlexMatch with SAA and conducted experiments on SSL benchmarks. The results of the experiments demonstrate that SAA can significantly improve performance. In summary, our contribution can be summarized as follows:
• We identify ineffectively utilized samples and em-phasize that they should be given more attention.
Under the consistency regularization based on data augmentation, some strongly augmented versions are not beneficial to model training, which results in the values of these samples not being fully exploited and makes the model performance slow to improve. We refer to them as “naive sample”, and emphasize that they should be learned with a new learning strategy.
• We propose SAA to make better use of naive sam-ple. To increase the probability that the augmented ver-sions can benefit the model training, a simple yet effec-tive method, sample adaptive augmentation (SAA), is proposed for identifying the naive samples and aug-menting them in a more diverse manner.
• We verify the validity of SAA on SSL benchmarks.
Using FixMatch and FlexMatch as the base frame-work, we proved that our approach can achieve state-of-the-art performance. For example, on CIFAR-10 with 40 labels, SAA helps FixMatch improve its ac-curacy from 92.50% to 94.76%, and helps FlexMatch improve its accuracy from 95.01% to 95.31%. 2.