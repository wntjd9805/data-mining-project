Abstract
In this paper, we consider enhancing medical visual-language pre-training (VLP) with domain-specific knowl-edge, by exploiting the paired image-text reports from the radiological daily practice. In particular, we make the fol-lowing contributions: First, unlike existing works that di-rectly process the raw reports, we adopt a novel triplet ex-traction module to extract the medical-related information, avoiding unnecessary complexity from language grammar and enhancing the supervision signals; Second, we pro-pose a novel triplet encoding module with entity translation by querying a knowledge base, to exploit the rich domain knowledge in medical field, and implicitly build relation-ships between medical entities in the language embedding space; Third, we propose to use a Transformer-based fu-sion model for spatially aligning the entity description with visual signals at the image patch level, enabling the abil-ity for medical diagnosis; Fourth, we conduct thorough experiments to validate the effectiveness of our architec-ture, and benchmark on numerous public benchmarks e.g.,
ChestX-ray14, RSNA Pneumonia, SIIM-ACR Pneumotho-rax, COVIDx CXR-2, COVID Rural, and EdemaSeverity.
In both zero-shot and fine-tuning settings, our model has demonstrated strong performance compared with the for-mer methods on disease classification and grounding. 1.

Introduction
With the rapid development of deep learning, numerous works have been proposed to facilitate computer-aided di-agnosis in the medical field [46, 20, 55, 19]. Despite the tremendous progress, these models are normally trained to recognize or segment the structures that fall into a certain closed set of anatomical or disease categories, whenever a new disease comes to be of interest, a costly procedure for data annotation, model re-training are required, fundamen-â€ : Corresponding author.
Figure 1: Our method mainly considers combining medical knowledge with VLP. We propose Triplet Extraction and
Entity Translation modules, so that the network can be su-pervised with detailed entity-level signals. tally limiting its practical values. As an alternative, recent research considers to train the model on the corpus, consist-ing of large amount of multi-modal data, that is generated from daily clinical routine, for instance, the most common example is the dataset of X-ray images with paired radio-logical reports [18, 28, 31].
This paper presents our preliminary investigation on vision-language representation learning in the medical do-main, with the goal of better zero-shot disease diagnosis (classification) and grounding. Undoubtedly, these tasks have also been widely investigated in the computer vi-sion community, with significant progress made on devel-oping Foundational Models in the past years, for exam-ple, CLIP [50], ALBEF [33], BLIP[32], etc. However, to achieve such a goal in the medical domain, different chal-lenges must be resolved, that requires research efforts from the community: First, data availability, training Foundation
Models in computer vision normally require over millions of image-text pairs, while in the medical domain, only a few hundred thousand pairs are available [31]. The lim-ited data challenges language models to understand the re-ports in free form [6]. Second, the problem considered in computer-aided diagnosis is naturally fine-grained, that re-quires distinguishing the medical concepts to understand the disease, as a consequence, domain knowledge is essen-1
tial; Third, robustness is crucial, it is, therefore, preferable to have explainability, where diagnosis results come along with the visual grounding, to help radiologists understand the system, and build trust between human and machines.
Existing work in medical VLP (Vision-Language Pre-training) [68, 47, 25, 6] follows a straightforward training paradigm by matching raw reports with image scans, as shown in Fig.1A, ignoring the medical prior knowledge, and, thus, we propose a novel knowledge-enhanced visual-language model as shown in Fig. 1B. First, we propose a triplet extraction module to extract useful medical enti-ties (keywords) from raw reports, and simplify each report into sets of triplets, denoted as {entity, position, exist}.
Decomposing reports into triplets leads to an effective rep-resentation of the reports with minimal information loss due to the structural prior in reports; Second, we trans-late the medical entities into fine-grained descriptions by leveraging a well-defined medical word knowledge base, that tends to explain diseases with common vocabulary.
Thus, computing text embeddings for these descriptions enables to implicitly establish relationships between med-ical entities; Third, we view the entities as a query set and adopt a transformer-based architecture for aligning the im-age patches with entity descriptions, that enables explicit supervision signals at entity level. Consequently, we can simultaneously infer the likelihood of certain diseases with the visual evidence in the form of a spatial heatmap, i.e., providing rough grounding for explainability.
We pre-train the model on one widely-used medical image-report dataset MIMIC-CXR [31], and rigorously evaluate on the task of disease diagnosis across numerous public benchmarks, e.g., ChestX-ray14 [58], RSNA Pneu-monia [51], SIIM-ACR Pneumothorax [1], COVIDx CXR-2 [48], COVID Rural [54, 15], and EdemaSeverity [7]. We get state-of-the-art performance on zero-shot classification and grounding on different diseases, spanning different im-age distributions, with further fine-tuning, our model still exceeds previous models significantly. 2.