Abstract
Image-text retrieval (ITR) aims to retrieve images or texts that match a query originating from the other modality. The conventional dense retrieval paradigm relies on encoding images and texts into dense representations with dual-stream encoders. However, this approach is limited by slow retrieval speeds in large-scale scenarios. To address this issue, we propose a novel sparse retrieval paradigm for ITR that ex-ploits sparse representations in the vocabulary space for images and texts. This paradigm enables us to leverage bag-of-words models and efficient inverted indexes, significantly reducing retrieval latency. A critical gap emerges from rep-resenting continuous image data in a sparse vocabulary space. To bridge this gap, we introduce a novel pre-training framework, Lexicon-Bottlenecked Language-Image Pre-Training (LexLIP), that learns importance-aware lexicon representations. By using lexicon-bottlenecked modules be-tween the dual-stream encoders and weakened text decoders, we are able to construct continuous bag-of-words bottle-necks and learn lexicon-importance distributions. Upon pre-training with same-scale data, our LexLIP achieves state-of-the-art performance on two ITR benchmarks, MSCOCO and Flickr30k. Furthermore, in large-scale retrieval scenar-ios, LexLIP outperforms CLIP with 5.8× faster retrieval speed and 19.1× less index storage memory. Beyond this,
LexLIP surpasses CLIP across 8 out of 10 zero-shot image classification tasks. 1.

Introduction
Image-text retrieval (ITR) is a critical problem that in-volves retrieving relevant images and texts based on textual
Its practical applications span multi-and visual queries.
*Work done during the internship at Microsoft.
†Corresponding author (a) Dense Retrieval Paradigm. (b) Sparse Retrieval Paradigm.
Figure 1: Comparing the traditional dense retrieval paradigm and our brand-new sparse retrieval paradigm. ple domains, including e-commerce product search [27] and social media image search [19]. Due to the lack of publicly available large-scale benchmarks, the evaluation of existing ITR models [20, 32, 39, 44] is commonly con-ducted on small-scale datasets such as MSCOCO [29] and
Flickr30k [38], which contain a limited number of samples in their test sets. Thus, the significance of retrieval speed is fre-quently disregarded. However, real-world scenarios, such as
Google Image Search, may involve a massive number of can-didate samples, easily exceeding 1M. Hence, retrieval speed is a crucial concern. The current dense retrieval paradigm, in which each image and text is represented as a dense vec-tor (as shown in Figure 1a), can become computationally expensive, resulting in slow retrieval speed. The large-scale exact k-nearest neighbor (KNN) dense retrieval involves cal-culating the similarity between the query and all candidate samples, resulting in a linear increase in retrieval time as the number of samples increases [5]. This limitation poses a challenge for the dense retrieval paradigm in real-world applications and underscores the need for more efficient and
Figure 2: An overview of the retrieval process in our proposed sparse retrieval paradigm for text-to-image retrieval. effective methods for large-scale retrieval.
In this work, we present a novel sparse retrieval paradigm for ITR, as illustrated in Figure 1b. This paradigm encodes images and texts as sparse representations in the vocabulary space, wherein the relevant lexicons are assigned high weights, and the others are set to zero. The retrieval process involves transforming these lexicon-weighted rep-resentations into inverted indexes, as depicted in Figure 2.
Then, we apply the Exact Lexicon Matching Search algo-rithm [40] to find matching pairs, which only calculates similarity scores with candidates that share common lex-icons. This mechanism avoids iterating over all samples and substantially reduces retrieval latency. Moreover, this paradigm leverages lexicon-level contextualization by con-sidering both implicit object expansion [15] and explicit object concurrence [36].
The sparse retrieval paradigm poses a significant chal-lenge for image processing because images are continuous data that need to be projected into a discrete vocabulary space. To bridge this gap, we propose a novel pre-training framework, termed Lexicon-Bottlenecked Language Im-age Pre-Training (LexLIP), to learn importance-aware lex-icon representations. LexLIP comprises two pre-training phases: i) lexicon-bottlenecked pre-training and ii) momen-tum lexicon-contrastive pre-training.
In the first pre-training phase, we introduce two novel ob-jectives: image/text lexicon-bottlenecked masked language modeling. These objectives aim to establish the lexicon-weighting representations as the bottlenecks between the images/texts data and the sparse vocabulary space. Specif-ically, we pass an image or masked text into a vision or language encoder to derive the lexicon-weighting representa-tions. Meanwhile, we utilize a weakened masking-style text decoder to reconstruct the masked text from these representa-tions. Due to the aggressive masking, the decoder is inclined to recover masked tokens based on the lexicon-weighting representations. As a result, the LexLIP encoders assign higher importance scores to crucial vocabulary lexicons of the image or text and lower scores to trivial ones. This aligns well with the goal of the sparse retrieval paradigm and enhances its performance.
The second pre-training phase is momentum lexicon-contrastive learning, where images and texts are further aligned in the sparse vocabulary space with a large-scale negative sample size. The experimental results reveal that our LexLIP pre-trained with same-scale image-text pairs achieves state-of-the-art (SOTA) performance on the widely used ITR benchmarks, MSCOCO [29] and Flickr30k [38].
In the large-scale retrieval scenario (i.e., the candidate pool has one million samples), LexLIP demonstrates a remark-able improvement in retrieval speed with a 5.8 times faster and a significant reduction in index storage memory with a 19.1 times decrease, compared to CLIP [39]. Beyond this,
LexLIP surpasses CLIP across 8 out of 10 zero-shot im-age classification tasks. Our codes are available at https:
//github.com/ChiYeungLaw/LexLIP-ICCV23.
Our contributions can be listed as follows: 1. We introduce the novel Sparse Retrieval Paradigm to
ITR. By representing images and texts in the lexicon vocabulary space, this approach significantly improves the efficiency of large-scale ITR. 2. We propose a new framework, Lexicon-Bottlenecked
Language Image Pre-Training (LexLIP), to learn the lexicon-weighting representations. 3. We conduct extensive experiments on both small-scale and large-scale ITR benchmarks. Pre-trained with same-scale data, our LexLIP achieves SOTA performance on small-scale ITR datasets, MSCOCO and Flickr30k.
Moreover, our LexLIP achieves 5.8× speed-up and 19.1× less index storage memory than CLIP on large-scale ITR. Beyond this, our LexLIP outperforms CLIP across 8 out of 10 zero-shot image classification tasks.
Figure 3: An overview of the Lexicon-Bottlenecked Pre-training phase, including self-supervised masked language modeling, image/text lexicon-bottlenecked masked language modeling, and in-batch lexicon-contrastive learning. 2.