Abstract
We study the task of weakly-supervised point cloud se-mantic segmentation with sparse annotations (e.g., less than 0.1% points are labeled), aiming to reduce the expensive cost of dense annotations. Unfortunately, with extremely sparse annotated points, it is very difﬁcult to extract both contextual and object information for scene understand-ing such as semantic segmentation. Motivated by masked modeling (e.g., MAE) in image and video representation learning, we seek to endow the power of masked model-ing to learn contextual information from sparsely-annotated points. However, directly applying MAE to 3D point clouds with sparse annotations may fail to work. First, it is non-trivial to effectively mask out the informative visual con-text from 3D point clouds. Second, how to fully exploit the sparse annotations for context modeling remains an open question. In this paper, we propose a simple yet ef-fective Contextual Point Cloud Modeling (CPCM) method that consists of two parts: a region-wise masking (Region-Mask) strategy and a contextual masked training (CMT) method. Speciﬁcally, RegionMask masks the point cloud continuously in geometric space to construct a meaning-ful masked prediction task for subsequent context learning.
CMT disentangles the learning of supervised segmentation and unsupervised masked context prediction for effectively learning the very limited labeled points and mass unlabeled points, respectively. Extensive experiments on the widely-tested ScanNet V2 and S3DIS benchmarks demonstrate the superiority of CPCM over the state-of-the-art. 1.

Introduction
With the growing demand for autonomous driving and robotic navigation, point cloud semantic segmentation be-comes an indispensable technique for accurate 3D environ-†Corresponding author. (cid:20) (cid:82) (cid:71) (cid:92) (cid:43) (cid:3) (cid:74) (cid:88) (cid:71) (cid:74) (cid:84) (cid:71) (cid:90) (cid:57) (cid:20) (cid:82) (cid:71) (cid:92) (cid:43) (cid:3) (cid:74) (cid:75) (cid:81) (cid:89) (cid:71) (cid:51) (cid:51)(cid:71)(cid:89)(cid:81)(cid:3)(cid:85)(cid:91)(cid:90) (cid:73)(cid:85)(cid:84)(cid:90)(cid:75)(cid:94)(cid:90)(cid:3)(cid:90)(cid:85)(cid:19)(cid:72)(cid:75)(cid:19)(cid:76)(cid:79)(cid:82)(cid:82)(cid:75)(cid:74) (cid:44)(cid:71)(cid:79)(cid:82) (cid:79)(cid:84)(cid:3) (cid:76)(cid:79)(cid:82)(cid:82)(cid:79)(cid:84)(cid:77)(cid:3)(cid:73)(cid:85)(cid:84)(cid:90)(cid:75)(cid:94)(cid:90) (cid:57)(cid:91)(cid:73)(cid:73)(cid:75)(cid:75)(cid:74) (cid:79)(cid:84) (cid:76)(cid:79)(cid:82)(cid:82)(cid:79)(cid:84)(cid:77)(cid:3)(cid:73)(cid:85)(cid:84)(cid:90)(cid:75)(cid:94)(cid:90) (cid:47)(cid:84)(cid:86)(cid:91)(cid:90) (cid:72)(cid:71)(cid:89)(cid:75)(cid:82)(cid:79)(cid:84)(cid:75)(cid:3)(cid:65)(cid:23)(cid:28)(cid:18)(cid:3)(cid:27)(cid:23)(cid:67) (cid:41)(cid:54)(cid:41)(cid:51)(cid:3)(cid:14)(cid:53)(cid:91)(cid:88)(cid:89)(cid:15)
Figure 1: Effectiveness of the proposed CPCM on context comprehension ability compared to the consistency-based baseline [16, 53]. We conduct masked evaluations to in-spect the model’s contextual understanding ability. The vi-sual comparison of results from different methods (mask ra-tio = 40%) and the performance w.r.t. different mask ratios are shown in the top and bottom panels, respectively. ment perception [18, 27, 51]. Recent years have witnessed great progress in fully-supervised learning in point cloud segmentation [2, 6, 10, 14, 31, 32, 39, 47, 56]. However, densely-annotating point-wise labels are time-consuming, labor-intensive as well as economic-inefﬁcient to obtain since the number of points in point cloud data can easily reach tens of thousands of magnitude [42, 48]. It goes with-out saying that diving into point cloud semantic segmenta-tion from sparse labels is crucial to reduce the annotation cost and expand the application boundary [9, 20, 22].
Very recently, to reduce the reliance on dense labels
while still delivering satisfactory point cloud semantic seg-mentation performance, most effort has been put into learn-ing from the weakly-annotated labels [9, 16, 25, 42, 48, 49, 52, 53]. Among several types of weakly-annotated la-bels, the partial point-wise labeling scheme offers the best trade-off between annotation cost and segmentation perfor-mance [9, 22]. In the partially annotated point cloud data, the labeled part typically occupies a very small portion of points (e.g., 0.1%) per scene [9]. In this case, directly ap-plying supervised cross-entropy loss only on the limited la-beled part is prone to overﬁtting [25, 33, 43]. As a result, the primary challenge is learning from a signiﬁcant proportion of unlabeled points to improve model generalization perfor-mance, rather than utilizing only the labeled points [16, 53].
Existing methods seek to tackle the challenge by ex-ploiting different levels of feature consistency under vari-ous data augmentations. To be speciﬁc, researchers resort to enforcing feature consistency between differently aug-mented or geometrically calibrated point clouds by discrim-inating points from different scenes with contrastive learn-ing [8, 11, 16, 45], exploring color & geometric smooth-ness [48, 52], more advanced consistency loss such as JS-divergence [53] and similarity weighted loss [43]. How-ever, given limited annotations, exploring feature consis-tency only would be insufﬁcient to capture the complex structures of point clouds, making it very difﬁcult to extract both contextual and object information for satisfactory seg-mentation performance. To inspect the consistency-based methods’ comprehension of scene context, we conduct a pilot study by masked evaluation: evaluate the segmenta-tion performance given a context-to-be-ﬁlled point cloud.
As shown in Figure 1, the performance of the consistency-based method degenerates drastically, indicating a poor un-derstanding of the scene context, even in this simple case.
Thus, comprehending the complex scene context from mass unlabeled points remains an unresolved issue.
Motivated by masked modeling (e.g., MAE [7]) in im-age and video that learns good representations by mask-ing random patches of the input image and reconstructing the missing information, we seek to endow the power of masked modeling for weakly-supervised point cloud seg-mentation. However, directly employing MAE to 3D point clouds with sparse annotations may fail to work due to the following reasons. First, since 3D point clouds are typi-cally unordered and irregular, it is nontrivial to mask out the informative visual context from the 3D point clouds for subsequent context learning. Second, considering the lim-ited but valuable labeled data in the weakly-annotated point cloud, how to fully exploit the labeled points in masked modeling remains an open question.
To address the above issues, we propose a simple yet ef-fective Contextual Point Cloud Modeling (CPCM) that con-sists of two parts: region-wise masking (RegionMask) strat-egy and a contextual masked training (CMT) method. To be speciﬁc, RegionMask evenly divides the geometric space into a set of cuboids and masks all points within the cuboids selected with a given mask ratio. Different from the trivial point-wise masking solution [26] that performs point-wise random masking, our RegionMask masks the point cloud continuously in the geometric space to provide a meaning-ful masked context prediction task. Beyond that, Region-Mask is able to control the difﬁculty of the masked feature prediction task by adjusting a hyper-parameter region size, showing ﬂexibility in handling different amounts of annota-tion. Similar to MAE [7], we expect that with a very high mask ratio (i.e., 0.75), the model is able to learn more visual concepts [7], thereby mastering the contextual information.
However, as shown in our experiments, directly incorpo-rating the masked modeling objective into the consistency-based training framework impedes learning from the limited but valuable labeled points, resulting in degenerated perfor-mance. To resolve this problem, we propose a contextual masked training (CMT) method that adds an extra masked feature prediction branch into the consistency-based frame-work, which not only paves the way for learning labeled data but allows the model to effectively learn the complex scene context. The proposed CPCM achieves state-of-the-art performance on two widely-tested benchmarks ScanNet
V2 and S3DIS. For example, on ScanNet V2 [4], CPCM outperforms SQN [9] by 5.6% mIoU on online test set.
Our contributions are summarized as follows:
• We propose contextual point cloud modeling that incorporates masked modeling into the consistency-based training framework to effectively learn contex-tual information from sparsely-annotated data.
• We propose a region-wise masking strategy that masks the point cloud continuously to construct the meaning-ful masked prediction task and a contextual masked training method that facilitates the learning from lim-ited labeled data and masked context prediction.
• To the best of our knowledge, we are the ﬁrst to explore 3D masked modeling on weakly-supervised point cloud segmentation. Extensive experiments on widely-tested benchmarks demonstrate the superior performance of the proposed CPCM. 2.