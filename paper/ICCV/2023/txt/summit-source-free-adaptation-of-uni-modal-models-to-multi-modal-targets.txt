Abstract
Scene understanding using multi-modal data is neces-sary in many applications, e.g., autonomous navigation. To achieve this in a variety of situations, existing models must be able to adapt to shifting data distributions without arduous data annotation. Current approaches assume that the source data is available during adaptation and that the source con-sists of paired multi-modal data. Both these assumptions may be problematic for many applications. Source data may not be available due to privacy, security, or economic con-cerns. Assuming the existence of paired multi-modal data for training also entails significant data collection costs and fails to take advantage of widely available freely distributed pre-trained uni-modal models. In this work, we relax both of these assumptions by addressing the problem of adapting a set of models trained independently on uni-modal data to a target domain consisting of unlabeled multi-modal data, without having access to the original source dataset. Our proposed approach solves this problem through a switching framework which automatically chooses between two com-plementary methods of cross-modal pseudo-label fusion – agreement filtering and entropy weighting – based on the es-timated domain gap. We demonstrate our work on the seman-tic segmentation problem. Experiments across seven chal-lenging adaptation scenarios verify the efficacy of our ap-proach, achieving results comparable to, and in some cases outperforming, methods which assume access to source data.
Our method achieves an improvement in mIoU of up to 12% over competing baselines. Our code is publicly available at https://github.com/csimo005/SUMMIT. 1.

Introduction
There has been a recent surge of interest in autonomous vehicles which typically rely on a wide variety of sensors.
This has fueled the need for machine learning models ca-* Currently at AWS AI Labs. Work done while the author was at UCR. pable of processing multiple sensing modalities, commonly referred to as multi-modal models. One problem of particu-lar interest is 3D semantic segmentation, which has received a lot of interest [12, 24, 36] driven by the introduction of new multi-modal datasets [5, 11, 4]. Many 3D semantic segmen-tation methods (e.g., [12, 24, 36]) fuse data across different sensing modalities, e.g., RGB images and point clouds to ob-tain and employ colored pointclouds [31], to increase perfor-mance and robustness. As with most learning problems, the performance of 3D semantic segmentation degrades as the input data distribution diverges from the training set distribu-tion [16], referred to as domain shift. This is particularly true for autonomous navigation which can experience domain shifts because of lighting and weather changes throughout the day, as well as geographic changes when traveling over large distances. Many works have sought to address this do-main shift in the Unsupervised Domain Adaptation (UDA) setting [14, 21, 8]. Recently, the relationship between dif-ferent modalities has been leveraged to aid in the adaptation process via cross-modal UDA (xMUDA) [16].
While xMUDA [16] has made significant improvements over uni-modal UDA methods, it assumes that the source dataset used for training (i) consists of paired multi-modal data, and (ii) is available during adaptation to the target domain.1 However, these conditions may be hard to satisfy in real-world scenarios:
The first assumption is problematic because it requires
• collection and annotation of large volumes of paired multi-modal data for every sensor configuration (e.g., RGB and depth, RGB and IR, etc), a very time-consuming operation.
Also, it fails to take advantage of the large volumes of uni-modal data and pre-trained models that are easily accessible.
The second assumption is problematic because sharing the
• source data for adaptation may be impossible due to privacy, security and commercial reasons. Additionally, as datasets have grown, their transfer and storage have begun to present non-trivial engineering challenges and financial costs. 1In this work, modality refers to a specific type of input data, such as
RGB images or point clouds, while domain refers to the underlying data distribution, such as cities in different continents.
Figure 1. Problem setup. Our goal is to adapt a pair of uni-modal models, which have been trained independently on a source domain, to a target domain consisting of unlabeled, paired, multi-modal data, without access to the original source dataset. In contrast to conventional cross-modal UDA [16] (left panel), we do not assume that the source dataset used for training (i) consists of paired multi-modal data, and (ii) is available during adaptation to the target domain.
Relaxing the first assumption of paired modalities in the source domain remains unaddressed in cross-modal UDA.
The second assumption has been relaxed in the source-free
UDA setting [18, 2, 39, 37], but to the authors’ best knowl-edge, no such prior works deal with multi-modal data.
In this paper, we propose Source-free adaptation of Uni-modal Models to Multi-modal Targets (SUMMIT), relaxing both the above assumptions (see Figure 1). Relaxation of the two assumptions in conventional cross-modal UDA makes our problem substantially more challenging. In the conven-tional cross-modal UDA setting, the approach in [16] relies on labeled paired multi-modal data in the source domain to learn correlations across modalities. Learned correlations are then exploited to improve transfer to the target domain.
In our setting, the correlations between modalities must be learned on the unlabeled target data, as we are working with uni-modal source models and do not assume access to such labeled pairs. The lack of source data already makes the alignment of the source and target distributions a challenging problem. Combining it with uni-modal models on the source side makes the overall problem even more challenging.
To address these challenges, we propose a new adaptation framework built on pseudo-label fusion across modalities.
First, we utilize the trained uni-modal models to generate pseudo-labels on the target data, separately for each modal-ity. Second, these pseudo-labels are fused together across modalities to filter out noisy predictions. We introduce a data-driven switching method that automatically chooses between two complementary approaches for cross-modal pseudo-label fusion – agreement filtering and entropy weight-ing – based on the estimated domain gap. The fused pseudo-labels are used to supervise the process of learning the corre-lations across modalities allowing for cross-modal learning to take place. Optimizing for explicit cross-modal objectives, our framework learns the correlations across modalities even without the presence of the source data and improves transfer beyond standard uni-modal adaptation.
Main contributions. Our primary contributions can be sum-marized as follows.
We address the problem of adapting a set of models trained
• independently on uni-modal data to a target domain consist-ing of unlabeled, paired, multi-modal data, without access to the original source dataset. This setting of great practical importance as explained above.
We propose a new cross-modal, source-free UDA frame-• work which fuses pseudo-labels across modalities using information-theoretic and hypothesis testing approaches.
This helps increase robustness of the predictions and im-plicitly allows for cross-modal correlations to be learned on the target domain without access to source data.
We perform extensive experiments on seven challenging
• benchmarks which demonstrate that our method provides an
improvement of up 12% over competing baselines. 2.