Abstract
This work investigates the use of smooth neural networks for modeling dynamic variations of implicit surfaces un-der the level set equation (LSE). For this, it extends the representation of neural implicit surfaces to the space-time
R3
R, which opens up mechanisms for continuous geo-metric transformations. Examples include evolving an ini-tial surface towards general vector ﬁelds, smoothing and sharpening using the mean curvature equation, and inter-polations of initial conditions.
⇥
The network training considers two constraints. A data term is responsible for ﬁtting the initial condition to the cor-responding time instant, usually R3 0
. Then, a LSE term
}
⇥{ forces the network to approximate the underlying geometric evolution given by the LSE, without any supervision. The network can also be initialized based on previously trained initial conditions, resulting in faster convergence compared to the standard approach. 1.

Introduction
A neural implicit function g : R3
R is a smooth neu-ral network that represents an implicit function. Since g is smooth, objects from the differential geometry of its regular level sets can be used in closed form [37, 27].
!
This work investigates the extension of the domain of neural implicit functions to the space-time R3
R, encod-ing the evolution of the function g as a higher-dimensional function f : R3
R. The resulting animation is gov-erned by a PDE, the level set equation (LSE) @f f
, k kr which encodes the propagation of the level sets St of f (
, t)
· towards their normals with speed v. The choice of the func-tion v depends on the underlying geometric model. LSE is an important tool for geometry processing applications.
@t = v
!
⇥
⇥
R
We propose to use a neural network f : R3
R to represent the above level set function. For this, we train f to learn the evolution St of an initial surface S under a given
LSE. Accordingly, we add the constraint f = g on R3 0
,
} to the LSE. If g is the signed distance function (SDF) of S, then a solution f of this problem encodes an animation of S.
⇥{
!
⇥
R
Our method is the ﬁrst neural approach in geometry pro-cessing, that does not consider numerical approximations of the LSE solution during sampling and does not discretize the LSE in the loss function.
⇥{
Our strategy has two steps to train f . First, a sample enables us to train the initial condition f = g on pi, g(pi)
}
{
R3
. Second, an LSE constraint is used to ﬁt f into 0
} a solution of the LSE in R3
R. This term does not need
⇥ any supervision, i.e. it does not consider samples of f . This constraint only uses samples in the form (pi, ti). The only requirement of our method is that f must be smooth. Given this, our main contributions can be summarized as follows:
• Extension of neural implicit surfaces to space-time without the use of numerical/discrete approximations.
Encoding the space-time coordinates as input of the network allows us to represent the whole implicit ani-mation in a single network;
• Development of a neural framework to ﬁt solutions of an LSE using only its analytical expression (Sec 4).
Moreover, the network training considers only the ini-tial condition of the LSE problem as data;
• The method is ﬂexible enough to be used in a variety of applications, such as surface motion by vector ﬁelds, smoothing, sharpening, and interpolation (Sec 6);
• We also propose a novel network initialization based on previously trained initial conditions (Sec 4.4). 2.