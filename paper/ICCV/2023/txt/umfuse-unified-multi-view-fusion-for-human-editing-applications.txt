Abstract
Numerous pose-guided human editing methods have been explored by the vision community due to their exten-sive practical applications. However, most of these methods still use an image-to-image formulation in which a single image is given as input to produce an edited image as out-put. This objective becomes ill-defined in cases when the target pose differs significantly from the input pose. Existing methods then resort to in-painting or style transfer to han-dle occlusions and preserve content. In this paper, we ex-plore the utilization of multiple views to minimize the issue of missing information and generate an accurate represen-tation of the underlying human model. To fuse knowledge from multiple viewpoints, we design a multi-view fusion net-work that takes the pose key points and texture from multi-ple source images and generates an explainable per-pixel appearance retrieval map. Thereafter, the encodings from a separate network (trained on a single-view human repos-ing task) are merged in the latent space. This enables us to generate accurate, precise, and visually coherent images for different editing tasks. We show the application of our network on two newly proposed tasks - Multi-view human reposing and Mix&Match Human Image generation. Addi-tionally, we study the limitations of single-view editing and scenarios in which multi-view provides a better alternative.
Datasplits and results can be found at Project Webpage. 1.

Introduction
Automating person-image editing can be of great value to business applications for advertisements, commercial merchandising, as well as individual creativity. Pose-based
*rishabhj@adobe.com image editing is a class of human image editing work-flows where the end result depends on an input human pose. These include human reposing, garment virtual try-on [10, 40, 4] etc. Recent methods for person-image editing
[28, 18, 33, 6, 3, 23, 34] have made great strides in produc-ing accurate edit results from a single image of a person.
However, they suffer from textural, shape, and color arti-facts because a single image may not have sufficient infor-mation to produce an accurate rendition of the person in the desired target pose. In this work, we present a novel pose-guided human image generation method that can incorpo-rate multiple images as well as leverage existing single-view reposing models to improve the editing results signifi-cantly.
In most contemporary methods for pose-guided human image reposing [28, 18, 33, 6, 3, 23], a deep neural network transforms a source image of a person into a target pose specified as a sequence of keypoints. The network takes a single source image of a person, the pose of the person in that image (source pose), and the target pose as input, and produces an image corresponding to the person in the tar-get pose. The source image supplies the color and texture information for guiding the generation of the target pose image. However, the target pose can be significantly differ-ent from the source pose, such as when a part of the human body or clothing obscured in the original image might ap-pear in the target pose.
In such cases, the network must infer those regions of the output from the context. We posit that using multiple source images of the same person is an effective solution to this problem. This is also practical for real-world applications as companies usually have multiple photos of a human model in distinct poses. However, this approach requires mapping parts of the target image to parts of the source images based on how their textures match up geometrically, which is a non-trivial task. In this paper, we present a framework that can extend existing single-view networks for multi-view reposing task by effectively com-bining data from multiple source images. We make the fol-lowing contributions to the field of pose-guided person im-age editing:
• UMFuse, a novel plug-and-play framework to fuse multi-scale pose and appearance features from differ-ent source images for human image editing applica-tions, using: – an appearance retrieval map for interpretable fea-ture fusion predicted using the input source im-ages and the corresponding pose information. – a visibility-informed pre-training task for initial-ization of the fusion module.
• The task of multi-view human reposing (MVHR, sec. 5), and a benchmark datasplit based on the DeepFash-ion [21] dataset for evaluating the quality of output. – We demonstrate its compatibility with two differ-ent single-view reposing networks [29, 14]. – We also showcase its versatility by combining different fashion components from completely different persons (Mix-and-Match, sec. 6).
• Detailed quantitative and qualitative analysis, compar-isons, and ablation studies indicating the effectiveness of the proposed method.
The next section discusses how the proposed method relates to existing methods for human-image editing. 2.