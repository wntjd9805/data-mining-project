Abstract
Recovering a 3D human mesh from a single RGB im-age is a challenging task due to depth ambiguity and self-occlusion, resulting in a high degree of uncertainty. Mean-while, diffusion models have recently seen much success in generating high-quality outputs by progressively denois-ing noisy inputs. Inspired by their capability, we explore a diffusion-based approach for human mesh recovery, and propose a Human Mesh Diffusion (HMDiff) framework which frames mesh recovery as a reverse diffusion process.
We also propose a Distribution Alignment Technique (DAT) that injects input-specific distribution information into the diffusion process, and provides useful prior knowledge to simplify the mesh recovery task. Our method achieves state-of-the-art performance on three widely used datasets.
Project page: https://gongjia0208.github.io/HMDiff/. 1.

Introduction
Monocular 3D human mesh recovery, where the 3D mesh vertex locations of a human are predicted from a single RGB image, is an important task with applications across virtual reality [17], sports motion analysis [1], and healthcare [54]. The field has received a lot of attention in recent years [6, 7, 32, 33, 8, 22], which has led to signif-icant progress, but monocular 3D human mesh estimation still remains very challenging. Human body shapes are not only complex and contain many fine details, but also inher-ently exhibit depth ambiguity (when recovering 3D infor-mation from single 2D images) and self-occlusion (where body parts can be occluded by other body parts) [25, 6, 53].
In particular, the depth ambiguity and self-occlusion in this task often bring much uncertainty to the recovery of 3D mesh vertices, and places a huge burden on the model to handle this inherent uncertainty [30, 39, 8, 22].
At the same time, denoising diffusion probabilistic mod-els (diffusion models) [16, 49] have recently seen much suc-cess in generative tasks, such as image [36], video [48] and
† Corresponding author text [31] generation, where they have been capable of pro-ducing highly realistic and good-quality samples. Specif-ically, diffusion models [16, 49] progressively “denoise” a noisy input – which is uncertain – into a high-quality out-put from the desired data distribution (e.g., natural images), through estimating the gradients of the data distribution [52] (also known as the score function). This progressive de-noising helps break down the large gap between distribu-tions (i.e., from a highly uncertain and noisy distribution to a desired target distribution), into smaller intermediate steps
[52], which assists the model in converging towards gener-ating the target data distribution smoothly. This gives diffu-sion models a strong ability to recover high-quality outputs from uncertain and noisy input data.
For the monocular 3D mesh recovery task, we also seek to recover a high-quality mesh prediction from uncertain and noisy input data, and so we leverage diffusion models to effectively tackle this task. To this end, we propose a novel diffusion-based framework for monocular 3D human mesh recovery, called Human Mesh Diffusion (HMDiff), where we frame the mesh recovery task as a reverse diffusion pro-cess which recovers a high-quality mesh by progressively denoising noisy and uncertain inputs.
Intuitively, in our HMDiff’s approach, we can regard the mesh vertices as particles in the context of thermody-namics. At the start, the particles (representing the ground truth mesh vertices) are systematically arranged to form a high-quality human mesh, then these particles stochasti-cally disperse throughout the space and eventually degrade into noise, leading to high uncertainty. This process (i.e., particles becoming more dispersed and noisy) is the forward diffusion process. Conversely, for human mesh recovery, we aim to perform the opposite of this process, i.e., the reverse diffusion process. Starting from a noisy and uncertain input distribution, we want to progressively denoise and reduce the uncertainty of the input to obtain a target human mesh distribution containing high-quality samples.
Correspondingly, our HMDiff framework consists of both the forward process and the reverse process. Specif-ically, the forward process is performed during training to generate samples of intermediate distributions that are used
as step-by-step supervisory signals to train our diffusion model g. On the other hand, the reverse process is a cru-cial part of our mesh recovery pipeline, which is used during both training and testing. In the reverse process, we first ini-tialize a noisy distribution HK, and use our diffusion model g to progressively transform HK into a high-quality human mesh distribution (H0) over K diffusion steps.
However, we face challenges in adopting a diffusion-based approach to tackle human mesh recovery. Firstly, it is difficult to directly produce complicated 3D mesh out-puts with a single RGB image as input; as shown in pre-vious works [7, 58, 42, 29, 8, 22, 41, 59], it is important to leverage some prior knowledge (e.g., pose information, segmentation maps) as input to guide the mesh recovery process, which is not performed in the standard diffusion process. Secondly, it is difficult to predict an accurate mesh output that corresponds to the RGB image, using only the standard diffusion process [16]. This is because we aim to predict a mesh that exactly corresponds to the input (i.e., it is an input-specific prediction), but the reverse process starts from the Gaussian distribution (HK), which is input-agnostic and does not contain helpful input-specific distri-bution information. To tackle the above issues, we can ex-tract useful input-specific distribution information to guide the reverse diffusion process, e.g., we can potentially fol-low some existing works [22, 8] to extract a pose heatmap from the image (which encodes rich semantic and uncer-tainty information [37, 28, 14]) and use it to initialize the input (HK) for our diffusion process.
Using an input-specific distribution (e.g., a pose heatmap) to initialize the input (HK) to the diffusion pro-cess has two benefits. Firstly, it will provide input-specific information to the diffusion process, which helps in gen-erating an accurate mesh prediction (H0) that corresponds to the input. Secondly, using an input-specific distribu-tion (e.g., a pose distribution extracted from a pose esti-mator) allows us to effectively leverage prior knowledge
[7, 58, 42, 29, 8, 22, 41, 59], which makes the mesh re-covery task easier. Due to these reasons, we ideally want an input-specific distribution as input (HK) to our reverse diffusion process. However, it is not feasible to directly ini-tialize the starting distribution (HK) with an input-specific distribution, because the standard reverse diffusion process is theoretically formulated and constrained to start from the (input-agnostic) Gaussian distribution.
Hence, we further propose a Distribution Alignment
Technique (DAT) to inject input-specific distribution in-formation to the diffusion process, which narrows down the target space for the diffusion process towards the spe-cific mesh distribution that corresponds to the input im-age. Specifically, we initialize an input-specific distribu-tion from the input image via a pose estimator [57], and use it to guide the initial diffusion steps towards the diffu-sion target H0. This allows us to start the reverse process from Gaussian noise, while infusing the diffusion process with input-specific information, leading to faster conver-gence (i.e., fewer diffusion steps) and better performance. 2.