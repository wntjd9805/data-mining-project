Abstract
Low-light video enhancement in the visible (VIS) range is important yet technically challenging, and it is likely to become more tractable by introducing near-infrared (NIR) information for assistance, which in turn arouses a new challenge on how to obtain appropriate multispectral data for model training. In this paper, we defend the feasibility and superiority of NIR-assisted low-light video enhance-ment results by using unpaired 24-hour data for the first time, which significantly eases data collection and improves generalization performance on in-the-wild data. By ac-counting for different physical characteristics between un-paired daytime and nighttime videos, we first propose to turn daytime NIR & VIS into ”nighttime mode”. Specif-ically, we design a heuristic yet physics-inspired relight-ing algorithm to produce realistic pseudo nighttime NIR, and use a resampling strategy followed by a noiseGAN for nighttime VIS conversion. We further devise a temporal-aware network for video enhancement that extracts and fuses bi-directional temporal streams and is trained using real daytime videos and pseudo nighttime videos. We cap-ture multi-spectral data using a co-axial camera and con-tribute Fulltime Multi-Spectral Video Dataset (FMSVD), the first dataset including aligned 24-hour NIR & VIS videos. Compared to alternative methods, we achieve sig-nificantly improved video quality as well as generaliza-tion ability on in-the-wild data in terms of both evaluation metrics and visual judgment. Codes and Data Available: https://github.com/MyNiuuu/NVEU . 1.

Introduction
Visually pleasing videos under well-illuminated condi-tions are essential for human perception as well as high-level computer vision tasks.
In practice, however, many videos are captured under sub-optimal conditions due to en-vironmental constraints, leading to poor visibility, structural degradation, and unpredictable noise interference.
So far, a large number of algorithms have been proposed
*Corresponding author
Figure 1: Light source and distribution differences be-tween daytime and nighttime. During daytime, both NIR and VIS come from sunlight and skylight, which share sim-ilar spatial distribution (a), and the VIS and NIR images are bright and more uniform (b). During nighttime, the camera is equipped with NIR LEDs in a co-located setting (c), thus the intensity distribution of nighttime NIR images depends heavily on object distance and surface direction. The VIS images are much darker due to weak illuminants like moon-light and manmade lamps (d). to enhance images/videos in the visible (VIS) range. Su-pervised methods [41, 59, 62, 5, 61, 4, 70] provide remark-able performance by denoising and enhancing the low-light inputs. However, in-the-wild image pairs for supervised training are laborsome to obtain, which basically needs dif-ferent camera settings for low-light/normal image pairs of completely static objects. Capturing video pairs for moving objects is more involved, which requires complex systems, such as the coaxial imaging system with ND filters [24] and repeatable mechatronic motion tracks [58], making the pro-cess harder and less practical. Existing unsupervised meth-ods [18, 25, 44] need no image pairs for training, but their capability in tackling noise tends to be undermined.
In addition to the severely degraded VIS image, it is
sometimes convenient to obtain another bright NIR im-age, by enabling auxiliary illuminants. Thus, VIS-NIR fu-sion [51, 26, 12] has become a promising method for low-light imaging. Compared to pure VIS-based methods, rich and detailed information is introduced from correspond-ing NIR images. Several supervised learning based meth-ods [26, 67] have been proposed to enhance VIS images by fusing photographs of extra wavelengths.
Unquestionably, adding assistance images of additional wavelengths is likely to robustify the enhancement task and provide superior performance, but it also makes data col-lection more challenging. Given the practices of data col-lection in the VIS range, it is obvious that capturing realis-tic paired data in VIS-NIR domain is extremely difficult, if not infeasible. Existing methods including DVN [26] and
DRF [67] simply synthesize training pairs from clean im-ages or use static images to ease data collection, which yet inevitably leads to generality issues on real data.
In this paper, we propose to consider a paradigm that only requires unpaired real data, which is much easier to collect since we do not have to assure the same scenes for low-light and normal images/videos. As a result, we can easily capture large-scale real data (even in video form) for training and testing. The complex lighting and noise dis-tributions covered in the dataset also intuitively assure bet-ter generality on in-the-wild data. For camera settings, we consider the most practical monocular systems which can be achieved through either co-axial systems or filters.
Apparently, by using a synchronized co-axial camera, it is possible to take aligned VIS and NIR videos in day-time and nighttime, respectively. Nevertheless, the differ-ences in light source and brightness level lead to obvious domain gaps between daytime and nighttime VIS/NIR im-ages (Fig. 1). Specifically, daytime VIS and NIR images are bright, and they share the same illumination distribution implied by the sunlight and skylight. In contrast, nighttime
VIS frames suffer from poor visibility, structural degrada-tion, and unpredictable noise interference due to low pho-ton counts. Although nighttime NIR frames are free from those issues because of auxiliary NIR illuminants, they still have wide domain gaps from daytime NIR in terms of light distribution, considering that the auxiliary illuminants (like
LEDs) are usually equipped around the camera lens in a nearly co-located setting, as in most security cameras.
Based on these 24-hour data, we propose the first NIR-assisted low-light video enhancement paradigm using un-paired videos, which can be divided into two stages (Fig. 2): 1) Day-to-night video synthesis. Given the different characteristics between daytime and nighttime videos, we first propose to turn daytime videos into ”nighttime mode”.
For NIR day-to-night synthesis, we design a novel relight-ing (RL) algorithm. The algorithm takes a daytime NIR nday together with an inferred depth map as input, and out-Figure 2: The two-stage framework of our method. RL and RST represent relighting algorithm and resampling trick, respectively. puts its nighttime version (ˆnnight) by approximating the ef-fects of co-located illumination. During this process, it con-siders the difference in light distribution between daytime and nighttime by formulating two factors: physical distance and surface angle to the camera. For VIS day-to-night syn-thesis, each daytime VIS frame vday is first processed by a resampling trick (RST). We then simply leverage existing noise GAN techniques [6, 66, 23] to add realistic pseudo noise on each frame. The noise GAN is first trained on real nighttime VIS to learn the camera noise pattern. 2) Improved VIS-NIR fusion with pseudo data pairs. We design and optimize an enhancement network G using real daytime VIS and pseudo nighttime NIR & VIS. The net-work extracts and fuses the feature of three continuous frames and outputs the enhancement result for the middle frame.
We demonstrate the effectiveness of our model on in-the-wild video datasets collected by our camera system, show-ing significantly improved video quality compared to exist-ing alternative methods. We also show the superior general-ization ability through testing on another third-party dataset.
Our contribution can be summarized as follows:
• For the first time we show that through physics-aware modifications on 24-hour unpaired data, an NIR-assisted low-light enhancement model can be trained with superior performance and generalization ability.
• A heuristic yet effective relighting method for realistic
NIR day-to-night synthesis by modeling the distance and surface angle to the camera.
• A temporal-aware video enhancement network, which is trained on real daytime VIS and pseudo nighttime
NIR & VIS synthesized by our method.
• Fulltime Multi-Spectral Video Dataset (FMSVD), the first dataset including in-the-wild aligned NIR and VIS videos during both daytime and nighttime. 2.