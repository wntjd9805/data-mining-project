Abstract
Integrating CNNs and RNNs to capture spatiotempo-ral dependencies is a prevalent strategy for spatiotempo-ral prediction tasks. However, the property of CNNs to learn local spatial information decreases their efficiency in capturing spatiotemporal dependencies, thereby limit-ing their prediction accuracy.
In this paper, we propose a new recurrent cell, SwinLSTM, which integrates Swin
Transformer blocks and the simplified LSTM, an extension that replaces the convolutional structure in ConvLSTM with the self-attention mechanism. Furthermore, we construct a network with SwinLSTM cell as the core for spatiotem-poral prediction. Without using unique tricks, SwinLSTM outperforms state-of-the-art methods on Moving MNIST,
Human3.6m, TaxiBJ, and KTH datasets.
In particular, it exhibits a significant improvement in prediction accuracy compared to ConvLSTM. Our competitive experimental re-sults demonstrate that learning global spatial dependencies is more advantageous for models to capture spatiotempo-ral dependencies. We hope that SwinLSTM can serve as a solid baseline to promote the advancement of spatiotempo-ral prediction accuracy. The codes are publicly available at https://github.com/SongTang-x/SwinLSTM. 1.

Introduction
Spatiotemporal prediction has received increasing atten-tion in recent years due to it can benefit many practical applications, e.g., precipitation forecasting [24, 25, 33], au-tonomous driving [1,16], and traffic flow prediction [38,40].
However, the complex physical dynamics and chaotic prop-erties of spatiotemporal predictive learning make it chal-lenging for purely data-driven deep learning methods to make accurate predictions. Existing methods [3, 8, 19, 24, 31–34,39] integrate CNNs and RNNs to learn spatiotempo-ral dependencies in spatiotemporal data to improve predic-tion accuracy. To capture the spatial and temporal depen-* Equal contribution. †Corresponding author.
Figure 1. Comparison of input-to-state and state-to-state transi-tions in three different recurrent cells. (a) FC-LSTM utilizes lin-ear operations to process 1D vectors. (b) ConvLSTM employs 2D convolutions to process 3D tensors. (c) The proposed SwinLSTM leverages self-attention to process 2D matrices. dencies simultaneously, ConvLSTM [24] extends the fully connected LSTM (FC-LSTM) [26] by replacing linear oper-ations with convolutional operations. Subsequently, several variants of ConvLSTM are proposed. PredRNN [33] and
MIM [34] modify the internal structure of the LSTM unit.
E3D-LSTM [32] integrates 3D-Convs into LSTMs. PhyD-Net [8] leverages a CNN-based module to disentangle phys-ical dynamics. Admittedly, these methods achieve impres-sive results on spatiotemporal prediction tasks. However, convolution operators focus on capturing local features and relations and are inefficient for modeling global spatial in-formation [4]. Although the receptive field can be enlarged by stacking convolution layers, the effective receptive field only reaches a fraction of the theoretical receptive field in practice [21]. Therefore, it can be inferred that the CNN-based models may prove to be ineffective in capturing spa-tiotemporal dependencies, owing to the inherent locality of the CNN architecture, leading to restricted accuracy in pre-dictions.
Recently, the Vision Transformer (ViT) [6] introduced the Transformer [29] that directly models long-range de-pendencies into the vision domain. ViT applies a standard
Transformer for image classification and attains excellent results with sufficient data. Its outstanding achievement has attracted more researchers to apply Transformer to com-puter vision. Subsequently, some variants [20, 27, 36, 43] of
ViT emerged, with tremendous success on different vision tasks. Notably, the Swin Transformer [20] has exhibited exceptional performance across diverse visual tasks, owing to its unique utilization of local attention and shift window mechanism.
Inspired by this, we propose SwinLSTM, a new recurrent cell. Specifically, we integrate Swin Transformer blocks and a simplified LSTM module to extract spatiotemporal representations. In addition, we construct a predictive net-work with SwinLSTM as the core to capture spatial and temporal dependencies for spatiotemporal prediction tasks.
As illustrated in Figure 2 (c), we first split an input image at the current time step into a sequence of image patches. Sub-sequently, the flattened image patches are fed to the patch embedding layer. Then, the SwinLSTM layer receives the transformed patches or the hidden states transformed by the previous layer (Patch Merging or Patch Expanding), and the cell and hidden states of the previous time step to extract the spatiotemporal representations. Finally, the reconstruction layer decodes the spatiotemporal representations to gener-ate the next frame.
The contributions of this paper can be summarized as follows:
• We propose a new recurrent cell, named SwinLSTM (section 3.2), which is able to efficiently extract spa-tiotemporal representations.
• We introduce a new architecture (section 3.1) for spatiotemporal prediction tasks, which can efficiently model spatial and temporal dependencies.
• We evaluate the effectiveness of the proposed model on Moving MNIST, TaxiBJ, Human3.6m, and KTH.
Experimental results show that SwinLSTM achieves excellent performance on four datasets. 2.