Abstract
Both indoor and outdoor environments are inherently structured and repetitive. Traditional modeling pipelines keep an asset library storing unique object templates, which is both versatile and memory efﬁcient in practice. Inspired by this observation, we propose AssetField, a novel neu-ral scene representation that learns a set of object-aware ground feature planes to represent the scene, where an asset library storing template feature patches can be constructed in an unsupervised manner. Unlike existing methods which require object masks to query spatial points for object edit-ing, our ground feature plane representation offers a natu-ral visualization of the scene in the bird-eye view, allowing a variety of operations (e.g. translation, duplication, defor-mation) on objects to conﬁgure a new scene. With the tem-plate feature patches, group editing is enabled for scenes with many recurring items to avoid repetitive work on ob-ject individuals. We show that AssetField not only achieves competitive performance for novel-view synthesis but also generates realistic renderings for new scene conﬁgurations. 1.

Introduction
The demand for bringing our living environment into a virtual realm continuous to increase these days, with exam-ple cases ranging from indoor scenes such as rooms and restaurants, to outdoor ones like streets and neighborhoods.
Apart from the realistic 3D rendering, real-world applica-tions also require ﬂexible and user-friendly editing of the scene. Use cases can be commonly found in interior de-sign, urban planning etc. To save human labor and expense, users need to frequently visualize different scene conﬁgu-rations before ﬁnalizing a plan and bringing it to reality, like shown in Fig.1. For their interests, a virtual environ-ment offering versatile editing choices and high rendering
quality is always preferable. In these scenarios, objects are primarily located on a horizontal plane like ground, and can be inserted to/deleted from the scene. Translation along the plane and rotation around the vertical axis are also common operations. Furthermore, group editing becomes essential when scenes are populated with recurring items (e.g. sub-stitute all chairs with stools and remove all sofas in a bar).
While recent advances in neural rendering [27, 3, 44, 28] offer promising solutions to producing realistic visuals, they struggle to meet the aforementioned editing demands.
Speciﬁcally, traditional neural radiance ﬁeld (NeRF)-based methods such as [47, 26, 3] encode an entire scene into a single neural network, making it difﬁcult to manipulate and composite due to its implicit nature and limited network capacity. Some follow-up works [41, 16] tackle object-aware scene rendering in a bottom-up fashion by learn-ing one model per object and then performing joint ren-dering. Another branch of methods learn object radiance
ﬁelds using instance masks [42], object motions [46], and image features [39, 21] as clues but are scene-speciﬁc, lim-iting their applicable scenarios. Recently, some approaches have attempted to combine voxel grids with neural radiance
ﬁelds [23, 44, 28] to explicitly model the scene. Previous work [23] showed local shape editing and scene composi-tion abilities of the hybrid representation. However, since the learned scene representation is not object-aware, users must specify which voxels are affected to achieve certain editing requirements, which is cumbersome, especially for group editing. Traditional graphical workﬂows build upon an asset library that stores template objects, whose copies are deployed onto a ‘canvas’ by designers, then rendered by some professional software (e.g. interior designers ar-range furniture according to ﬂoorplans). This practice sig-niﬁcantly saves memory for large scene development and offers users versatile editing choices, which inspires us to resemble this characteristic in neural rendering.
To this end, we present AssetField, a novel neural rep-resentation that bears the editing ﬂexibility of traditional graphical workﬂows. Our method factorizes a 3D neural
ﬁeld into a ground feature plane and a vertical feature axis.
As illustrated in Fig. 1, the learned ground feature plane is a 2D feature plane that is visually aligned with the bird-eye view (BEV) of the scene, allowing intuitive manipulation of individual objects. It is also able to embed multiple scenes into scene-speciﬁc ground feature planes with a shared ver-tical feature axis, rendered using a shared MLP. The learned ground feature planes encode scene density, color and se-mantics, providing rich clues for object detection and cate-gorization. We show that assets mining and categorization, and scene layout estimation can be directly performed on the ground feature planes. By maintaining a cross-scene asset library that stores template objects’ ground feature patches, our method enables versatile editing at object-level, category-level, and scene-level.
In summary, AssetField 1) learns a set of explicit ground feature planes that are intuitive and user-friendly for scene manipulation; 2) offers a novel way to discover assets and scene layout on the informative ground feature planes, from which one can construct an asset library storing feature patches of object templates from multiple scenes; 3) im-proves group editing efﬁciency and enables versatile scene composition and reconﬁguration and 4) provides realistic renderings on new scene conﬁgurations. 2.