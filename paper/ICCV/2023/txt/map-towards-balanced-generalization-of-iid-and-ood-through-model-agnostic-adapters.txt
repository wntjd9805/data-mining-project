Abstract
Deep learning has achieved tremendous success in re-cent years, but most of these successes are built on an independent and identically distributed (IID) assumption.
This somewhat hinders the application of deep learning to the more challenging out-of-distribution (OOD) scenarios.
Although many OOD methods have been proposed to ad-dress this problem and have obtained good performance on testing data that is of major shifts with training dis-tributions, interestingly, we experimentally find that these methods achieve excellent OOD performance by making a great sacrifice of the IID performance. We call this find-ing the IID-OOD dilemma. Clearly, in real-world applica-tions, distribution shifts between training and testing data are often uncertain, where shifts could be minor, and even close to the IID scenario, and thus it is truly important to design a deep model with the balanced generalization ability between IID and OOD. To this end, in this paper, we investigate an intriguing problem of balancing IID and
OOD generalizations and propose a novel Model Agnostic adaPters (MAP) method, which is more reliable and effec-tive for distribution-shift-agnostic real-world data. Our key technical contribution is to use auxiliary adapter layers to incorporate the inductive bias of IID into OOD methods. To achieve this goal, we apply a bilevel optimization to explic-itly model and optimize the coupling relationship between the OOD model and auxiliary adapter layers. We also the-oretically give a first-order approximation to save compu-tational time. Experimental results on six datasets success-fully demonstrate that MAP can greatly improve the perfor-mance of IID while achieving good OOD performance. 1.

Introduction
Deep learning has achieved unprecedented success in various applications of computer vision, e.g., image clas-sification [15, 17, 18], but most of these successes are
*Corresponding author.
Figure 1. Comparison of OOD and IID accuracy of OOD, IID and our proposed MAP methods on ColoredMNIST. HM is the har-monic mean of IID and OOD accuracy. All OOD methods achieve high OOD performance with the sacrifice of IID accuracy com-pared with ERM (i.e., an IID method). MAP achieves balanced generalization by incorporating IID inductive bias into OOD gen-eralization learning. More results are shown in Table 1. based on an independent and identically distributed (IID) assumption, i.e., training and testing data are drawn from the same distribution [41, 16]. However, out-of-distribution (OOD) shifts between training and testing data are usually inevitable in the real world due to the widespread existence of unobserved confounders or data bias [46, 8]. Under such circumstances, deep models trained by empirical risk mini-mization (ERM) [51] with the IID assumption usually suffer from poor performance on OOD data. Therefore, it is im-portant to improve the OOD generalization of deep models.
Recently, many OOD methods have been proposed to learn representations or predictors that are invariant to dif-ferent distributions (or named environments) by introducing various regularizers [3, 4, 44, 1, 26, 2, 34, 64, 63, 50]. Al-though these methods achieve good OOD performance on testing data that is of major distribution shifts with training data, we experimentally found that they would significantly damage the performance on IID (with nearly no shift dis-crepancy) or minor shift data. We implement some repre-sentative OOD methods in both OOD and IID scenarios on
ColoredMNIST and show results in Figure 1. We have an interesting observation that these methods have significant
OOD accuracy but lower IID performance compared with the IID method (e.g., ERM) with higher IID performance
to extract invariant features with the inductive bias of the
OOD scenario. Training processes of the OOD model and
AALs are viewed as two kinds of tasks: the OOD model learns OOD knowledge and AALs extract IID information.
To achieve this, we formulate the learning into a bilevel op-timization (BLO) problem. In the inner level, we optimize the OOD model with AALs by using an OOD loss. In the outer level, we utilize the IID criterion evaluated on the val-idation set based on the optimized OOD model in the inner level as the outer objective to guide the training of AALs.
We alternatively perform the inner level and outer level and finally obtain a set of optimal parameters for the adapter and
OOD model. To save computational time and memory, we theoretically give a first-order approximation of BLO. Note that AALs are model-agnostic and can be plugged into an arbitrary OOD method. Experiments evaluate the effective-ness of MAP, which improves the trade-off ability of OOD methods (see the HM metric in Figure 1) by capturing the inductive bias of both IID and OOD scenarios in Figure 2.
Our main contributions are summarized as follows:
• We investigate a problem called the IID-OOD dilemma, i.e., most OOD (or IID) methods achieve good OOD (or IID) performance with a sacrifice of IID (or OOD) accuracy, which is beyond the capability of these methods in real-world data with uncertain shifts.
• We propose a simple yet effective Model Agnostic adaPters (MAP) method to simultaneously learn in-ductive biases of both IID and OOD. To achieve this, a bilevel optimization (BLO) is used to train our MAP.
Unlike the computationally intensive BLO solver, we theoretically give a first-order approximation.
• We conduct extensive experiments across six datasets, three model architectures, and sixteen baselines. We show that (1) MAP balances the performance of IID and OOD. (2) MAP is model-agnostic and can be plugged into any OOD method. (3) MAP is able to achieve reliable performance under various settings. 2.