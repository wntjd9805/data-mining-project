Abstract
This work proposes an end-to-end neural interactive key-point detection framework named Click-Pose, which can significantly reduce more than 10 times labeling costs of 2D keypoint annotation compared with manual-only anno-tation. Click-Pose explores how user feedback can cooper-ate with a neural keypoint detector to correct the predicted keypoints in an interactive way for a faster and more ef-fective annotation process. Specifically, we design the pose error modeling strategy that inputs the ground truth pose combined with four typical pose errors into the decoder and trains the model to reconstruct the correct poses, which en-hances the self-correction ability of the model. Then, we attach an interactive human-feedback loop that allows re-ceiving users’ clicks to correct one or several predicted keypoints and iteratively utilizes the decoder to update all other keypoints with a minimum number of clicks (NoC) for efficient annotation. We validate Click-Pose in in-domain, out-of-domain scenes, and a new task of keypoint adapta-tion. For annotation, Click-Pose only needs 1.97 and 6.45
NoC@95 (at precision 95%) on COCO and Human-Art, re-*Work done during an internship at IDEA.
†Corresponding author. ducing 31.4% and 36.3% efforts than the SOTA model (ViT-Pose) with manual correction, respectively. Besides, with-out user clicks, Click-Pose surpasses the previous end-to-end model by 1.4 AP on COCO and 3.0 AP on Human-Art. 1.

Introduction
Multi-person keypoint detection aims to localize 2D co-ordinates of keypoints for each person in images, as in
Fig. 1. It has garnered significant attention in research and industry, particularly in sports, entertainment, and surveil-lance applications. The development of deep models for various applications heavily depends on a large volume of training data with labels (e.g., COCO [12, 23]). As the amount of data increases, the manual annotations of dense human keypoints are quite time-consuming, labor-intensive, and cost-prohibitive. As demonstrated in Fig. 2, annotating a single person with 17 keypoints would take about 230 sec-onds. For a dataset of 50K images with an average of four people per image, this process would require 532 hours.
Additionally, there may exist omissions, localization devia-tion, and mislabeling in the manual annotation process.
To reduce the manual effort, an intuitive annotation pro-cess can use a state-of-the-art (SOTA) model [42] to obtain 1
coder, which allows receiving user-corrected positions at the decoder instead of the input image. However, we em-pirically find that the decoder in ED-Pose is extremely sus-ceptible to variations in input keypoint positions. Even a minor deviation can result in a significant deterioration in performance. To tackle this limitation, we introduce two unique technical contributions to its decoder. The first is the pose error modeling that builds a reconstruction task to enhance the robustness of the decoder and learn to refine wrong keypoints by leveraging the correct keypoints as a reference. The second is the interactive human-feedback loop, which allows receiving users’ clicks to correct one or several predicted keypoints and iteratively utilizes the de-coder to update all other keypoints with minimal manual corrections for efficient annotation.
Click-Pose incorporates the above two essential designs into the training process, which improves +1.4 AP on
COCO val and +3.0 AP on HumanArt val compared with the baseline model ED-Pose, achieving state-of-the-art performance for end-to-end keypoint detection. More importantly, as shown in Fig. 1, Click-Pose shows its ad-vantages in various annotation scenarios, i.e., in-domain, out-of-domain scenes, and a new task of keypoint adap-tation. Specifically, Click-Pose only needs 1.97 and 6.45
NoC@95 (the average number of user clicks needed to an-notate one person to achieve a precision of 95%) on COCO and Human-Art, reducing 31.4% and 36.3% efforts than the
SOTA model with manual correction, respectively. More-over, Click-Pose significantly reduces the average time cost of single-person annotation, achieving over 5× speedup compared to the SOTA model ViTPose with manual cor-rection and more than a 10× speedup compared to manual-only annotation, especially in out-of-domain scenarios.
Our contributions are: (1) We define a novel task called interactive keypoint detection to pursue high-precision and low-cost annotation, and present the first framework to ad-dress this task, namely Click-Pose. (2) We incorporate the pose error modeling and interactive human-feedback loop into the training of Click-Pose, leading to a state-of-the-art performance for end-to-end keypoint detection. (3) We pro-vide a new metric (NoC) and extensively validate the effec-tiveness and efficiency of Click-Pose in different annotation scenes. We hope this work could inspire further research in related fields. 2.