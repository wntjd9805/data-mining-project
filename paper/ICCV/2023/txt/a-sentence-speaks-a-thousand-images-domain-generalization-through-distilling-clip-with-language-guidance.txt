Abstract
Domain generalization studies the problem of training a model with samples from several domains (or distribu-tions) and then testing the model with samples from a new, unseen domain. In this paper, we propose a novel approach for domain generalization that leverages recent advances in large vision-language models, specifically a CLIP teacher model, to train a smaller model that generalizes to unseen domains. The key technical contribution is a new type of regularization that requires the student’s learned image rep-resentations to be close to the teacher’s learned text repre-sentations obtained from encoding the corresponding text descriptions of images. We introduce two designs of the loss function, absolute and relative distance, which provide spe-cific guidance on how the training process of the student model should be regularized. We evaluate our proposed method, dubbed RISE (Regularized Invariance with Seman-tic Embeddings), on various benchmark datasets, and show that it outperforms several state-of-the-art domain gener-alization methods. To our knowledge, our work is the first to leverage knowledge distillation using a large vision-language model for domain generalization. By incorporat-ing text-based information, RISE improves the generaliza-tion capability of machine learning models. 1.

Introduction
An image is worth a thousand words, indeed, because of its power to convey a wealth of information through its vi-sual details. However, a well-written sentence, on the other hand, has the power to concisely capture the essential infor-mation that is common to many different images. By de-scribing a scene with a few carefully chosen words, a writer can create a mental image in the reader’s mind that conveys the essence of what is being depicted. This perspective is particularly useful when communicating information effi-† equal advising. Code is available at github.com/OoDBag/RISE
Figure 1. The key intuition behind our argument. While images can capture more details, text can directly summarize the core con-cept to represent the object of interest. ciently, or when emphasizing a specific scene aspect with-out getting bogged down in extraneous details. Thus, we suggest that a sentence speaks a thousand images.
Essential semantic information delivered by an image plays a pivotal role in helping models generalize to shifted distributions, whereas other detailed information (e.g., in the background not relevant to the main object) captured in images may not be as effective for this purpose. The study of domain generalization [37] investigates the prob-lem of training a model with samples from several do-mains (or distributions) and then testing the model with samples from a new, unseen domain. The training do-mains are commonly referred to as source domains, and the test domain is referred to as the target domain. Previ-ous studies have identified a challenge in training effective domain generalization models due to the models’ tendency to learn domain-specific features [11]. Consequently, nu-merous works have focused on regularizing the models to learn representations that are invariant to domain-specific features [29, 31, 80, 56, 36, 4, 1, 10, 39, 47, 17]. This reg-ularization ensures that the models extract features that are
common to multiple domains and are therefore more likely to generalize to unseen domains. By mitigating the influ-ence of domain-specific features, the idea is to improve the generalization capability of these models and ensure that they perform well on a variety of different domains.
In this paper, we build upon this line of research by in-vestigating methods for learning domain-invariant features in machine learning models. Our proposed method is in-spired by a simple intuition: while an image tends to con-vey rich but sometimes excessive details through its pixels, a corresponding text description can describe the crux of the image content in a highly concise and complementary manner; see Figure 1. Therefore, the most effective regular-ization might involve incorporating a regularization strategy in which the learned representations need to be close to the representations obtained from encoding the corresponding concise text descriptions of an image.
Building upon this argument, we propose a novel domain generalization approach that leverages recent advances in vision-language models, such as CLIP [46], to train our domain generalization models. We are particularly inter-ested in the setting where our final models are relatively small, and thus, can benefit from a large pre-trained vision-language teacher model through distillation. Our method, dubbed RISE (Regularized Invariance with Semantic Em-beddings), incorporates both the vision and language com-ponents of a pre-trained and frozen CLIP teacher, inspired by the importance of the representations encoded by the lan-guage component. Specifically, RISE includes three loss functions: the empiricial risk minimization (ERM) loss that follows the standard pipeline of domain generalization, the model distillation loss that leverages the pretrained weights of the image component of CLIP, and the cross-domain (text to image) distance loss that uses the power of text through the language component of CLIP.
To fully harness the power of language, we introduce two different designs of the cross-domain distance loss function: the absolute distance design pushes the student’s learned representation closer to the teacher’s domain-invariant rep-resentation learned from language, while the relative dis-tance design enforces that the relative domain distances in the teacher’s encoded language space are transferred over to the learned representation in the student’s encoded image space.
Contributions.
In summary, our main contributions are:
• To the best of our knowledge, we are the first to leverage knowledge distillation using a large vision-language model as a teacher for domain generalization.
• We propose to regularize the representation learned by the student through images to be closer to the ones from the teacher’s text representation, as text can be more concise and capture the semantic essence.
• We propose two loss functions, namely the absolute distance and the relative distance, which provide spe-cific guidance on how the student model’s training pro-cess should be regularized.
• We conduct a rich set of experiments to validate the ef-fectiveness of our model RISE on domain generaliza-tion benchmarks and ablate the performance of each of its components. 2.