Abstract
Referring video object segmentation (RVOS) aims at seg-menting an object in a video following human instruction.
Current state-of-the-art methods fall into an ofﬂine pattern, in which each clip independently interacts with text embed-ding for cross-modal understanding. They usually present that the ofﬂine pattern is necessary for RVOS, yet model limited temporal association within each clip. In this work, we break up the previous ofﬂine belief and propose a simple yet effective online model using explicit query propagation, named OnlineRefer. Speciﬁcally, our approach leverages target cues that gather semantic information and position prior to improve the accuracy and ease of referring pre-dictions for the current frame. Furthermore, we generalize our online model into a semi-online framework to be com-patible with video-based backbones. To show the effective-ness of our method, we evaluate it on four benchmarks, i.e.,
Refer-Youtube-VOS, Refer-DAVIS17, A2D-Sentences, and
JHMDB-Sentences. Without bells and whistles, our On-lineRefer with a Swin-L backbone achieves 63.5 J&F and 64.8 J&F on Refer-Youtube-VOS and Refer-DAVIS17, out-performing all other ofﬂine methods. Our code is available at https://github.com/wudongming97/OnlineRefer. 1.

Introduction
Given a natural language expression, the purpose of re-ferring video object segmentation (RVOS) is to segment the described object in a streaming video. The emerging task has attracted great attention in the computer vision commu-nity as it provides potential beneﬁts for many applications, e.g., video editing and human-computer interaction. Its core challenge is associating all frames with constructing an ef-ﬁcient video representation, further promoting cross-modal understanding of two modalities, i.e., video and language.
†Corresponding author: Jianbing Shen. This work was supported in part by the FDCT grants 0154/2022/A3 and SKL-IOTSC(UM)-2021-2023, the MYRG-CRG2022-00013-IOTSC-ICI grant and the SRG2022-00023-IOTSC grant. ‡The work is done during the internship at MEGVII.
Figure 1: Conceptual comparison on current methods: (a) the mask-propagation method [12, 28], (b) the ofﬂine method [10, 1, 35], and (c) our query-propagation method.
Pioneer methods [12, 28] integrate mask propagation into the referring image segmentation in an online manner, as shown in Fig. 1 (a). However, the complexity and perfor-mance of their model remain far from satisfactory.
Recently, the state-of-the-art performance on RVOS has been dominated by ofﬂine methods [31, 10, 1, 4, 13, 35, 46].
They typically follow a clip-level paradigm, dividing the en-tire video into multiple non-overlapped clips and generating referring object masks for each clip, as illustrated in Fig. 1
Figure 2: Visualization of query references and corresponding results of (a) query-sharing method and (b) our OnlineRe-fer. The reference points/boxes are marked red, while the ﬁnal predictions of mask and box are marked green. (b). In terms of different inter-frame interaction ways within an individual clip, existing ofﬂine methods [31, 10, 1, 4, 13, 35, 46] can be categorized into two groups: feature asso-ciation methods and query-sharing methods. The former feature association methods [31, 10, 13, 33, 46] integrate multi-frame features into a holistic clip-level visual repre-sentation, which is further fused with text embedding for referring prediction. However, their temporal feature mod-eling is commonly complicated and heavy-weighted.
In contrast to the feature association methods, the query-sharing methods [1, 35] provide a simpliﬁed pipeline as they build on the query-based Transformer method [2, 47]. They
ﬁrst construct clip-level cross-modal features and then use a set of repeated queries to retrieve the same referent object from different frames. In other words, the cross-frame ob-ject correspondence relies heavily on sharing input queries.
The interaction between frames is typically limited, hinder-ing the association potential of the learned queries. Fig. 2 (a) shows a typical example: all video frames share the same reference points (or queries), which misses the oc-cluded object. In addition, due to resource limitations, the referring prediction has to be performed separately on each clip, which lacks inter-clip association.
In this paper, we propose a new and insightful online referring video object segmentation framework, OnlineRe-It goes beyond the intuition of the online model not fer. working well in RVOS. Its core idea is to take advantage of the query-based set prediction in Deformable DETR [47] and link all video frames via continuous query propaga-tion. Speciﬁcally, we ﬁrst provide a powerful query-based referring segmentation pipeline, which outputs the embed-ding representations of the referent object, further generat-ing mask, box, and category. As these outputs gather rich target information, we propose a cross-frame query propa-gation module to transform them as new query inputs of the next frame. The propagation process has three signiﬁcant advantages. First, the referring target is automatically as-sociated with its precursors on all previous frames. Second, the box information of the last frame provides a very good spatial regional prior, beneﬁting the model for accurately inferring the same object in the current frame (see an exam-ple in Fig. 2 (b)). Third, our architecture avoids compli-cated temporal modeling or limited cross-frame association so that the overall training and inference progress is smooth and effective. Thanks to the remarkable performance, we expect to contribute the elegant and effective online model as a new baseline to the community.
To summarize, our main contributions are three-fold:
• We are the ﬁrst to challenge the widespread belief that only ofﬂine models can deal well with RVOS and make online RVOS great again.
• We propose a simple yet solid online baseline based on query propagation. The explicit association across video frames facilities temporal target matching and improves referring prediction accuracy.
• Our method is evaluated on four benchmarks: Refer-Youtube-VOS, Refer-DAVIS17, A2D-Sentences, and
JHMDB-Sentences, outperforming all previous ofﬂine methods and achieving state-of-the-art performance. 2.