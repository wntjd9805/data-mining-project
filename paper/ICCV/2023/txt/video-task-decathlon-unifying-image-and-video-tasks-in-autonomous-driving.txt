Abstract
Performing multiple heterogeneous visual tasks in dy-namic scenes is a hallmark of human perception capability.
Despite remarkable progress in image and video recognition via representation learning, current research still focuses on designing specialized networks for singular, homogeneous, or simple combination of tasks. We instead explore the construction of a unified model for major image and video recognition tasks in autonomous driving with diverse input and output structures. To enable such an investigation, we design a new challenge, Video Task Decathlon (VTD), which includes ten representative image and video tasks spanning classification, segmentation, localization, and association of objects and pixels. On VTD, we develop our unified net-work, VTDNet, that uses a single structure and a single set of weights for all ten tasks. VTDNet groups similar tasks and employs task interaction stages to exchange information within and between task groups. Given the impracticality of labeling all tasks on all frames and the performance degrada-tion associated with joint training of many tasks, we design a Curriculum training, Pseudo-labeling, and Fine-tuning (CPF) scheme to successfully train VTDNet on all tasks and mitigate performance loss. Armed with CPF, VTDNet sig-nificantly outperforms its single-task counterparts on most tasks with only 20% overall computations. VTD is a promis-ing new direction for exploring the unification of perception tasks in autonomous driving. 1.

Introduction
Agents that operate in dynamic environments are required to perform a wide range of visual tasks of varying complexi-ties to carry out their functions. For instance, autonomous driving vehicles must identify drivable areas [49], detect pedestrians [32, 8], and track other vehicles [43, 55], among others. Taking a continuous stream of visual inputs, they must be capable of performing tasks at the level of images,
Figure 1: Task categorization of representative image and video recognition tasks. We design a new challenge and a new architecture to learn a unified representation of image and video tasks for autonomous driving. instances, and instances across the spatial and temporal ex-tent of the input data. While humans can effortlessly com-plete diverse visual tasks, and representation learning has shown impressive results on individual tasks [28], there is still a lack of unified architectures that can combine various heterogeneous tasks.
Unified representations for image and video tasks offer numerous advantages, including significant computational savings over using separate networks for each task [3]. Addi-tionally, shared task input and output structures [53, 52] and cascaded tasks [35, 20] provide opportunities for learning algorithms to exploit inter-task relationships, resulting in bet-ter representations, generalization, and overall accuracy [38].
However, realizing these benefits poses unique challenges.
Network architectures must support the predictions of all heterogeneous tasks, which is non-trivial due to the diver-sity in input and output structures and granularity of visual representation needed for each task. Furthermore, the im-practicality of annotating all video frames for all tasks [55] results in data imbalance between each task and necessitates a more sophisticated training strategy than with single-task or homogeneous multi-task learning.
Another major obstacle to arrive at such a unified rep-resentation framework is the lack of large-scale evaluation
Figure 2: Video Task Decathlon (VTD). We introduce the VTD to study unified representation learning of heterogeneous tasks in 2D vision for autonomous driving. Given a monocular video, the network needs to produce predictions for ten diverse image and video recognition tasks. protocols for heterogeneous combinations of multiple tasks with distinct characteristics. Current multi-task benchmarks are overly simplistic and focus on combinations of multi-ple homogeneous tasks such as different types of classifica-tions [36] or pixel-level predictions [34, 33, 7, 58]. Those that branch out to different tasks often only consider a lim-ited number of tasks [2, 43, 13, 40, 25]. In addition, all these works are based solely on image tasks, disregarding the dynamics and associations in videos [57]. Although these benchmarks are useful for studying the abstract problem of multi-task learning, they do not adequately support the learn-ing of general representations for the complex, real-world environments encountered in autonomous driving.
To address the aforementioned limitations, we first intro-duce a new challenge, Video Task Decathlon (VTD), to study unified representation learning for heterogeneous tasks in autonomous driving. VTD comprises ten visual tasks, cho-sen to be representative of image and video recognition tasks (Figure 1). VTD provides an all-around test of classifica-tion, segmentation, localization, and association of objects and pixels. These tasks have diverse output structures and interdependencies, making it much more challenging than existing multi-task benchmarks. Additionally, differences in annotation density between tasks complicate optimization, reflecting real-world challenges. Along with our challenge, we also propose a new metric, VTD Accuracy (VTDA), that is robust to differing metric sensitivities and enables better analysis in the heterogeneous setting.
To explore unified representation learning on VTD, we propose two components: (1) VTDNet, a network capable of training on and producing outputs for every VTD task with a single structure and a single set of weights, and (2)
CPF, a progressive learning scheme for joint learning on
VTD. Specifically, VTDNet identifies three levels of visual features that are essential for visual tasks, namely image fea-tures, pixel features, and instance features. Each task can be broken down into a combination of these three basic features, and tasks are grouped based on the required features for pre-diction. Furthermore, VTDNet utilizes Intra-group and
Cross-group Interaction Blocks to model feature interac-tions and promote feature sharing within and across different groups of tasks. CPF has three key features: Curriculum training pre-trains components of the network before joint optimization, Pseudo-labels avoid forgetting tasks without sufficient annotations, and task-wise Fine-tuning boosts the task accuracies further based on the learned shared represen-tations. CPF enables VTDNet to jointly learn all the tasks and mitigate a loss of performance.
We conduct experiments for the proposed VTD chal-lenge on the large-scale autonomous driving dataset
BDD100K [55]. Armed with CPF, VTDNet is able to sig-nificantly outperform strong baselines and other multi-task models on a majority of the tasks and achieve competitive performance on the rest, despite using a single set of weights and only 20% overall computations. Our findings indicate that unifying a diverse set of perception tasks for autonomous driving holds great promise for improving performance by leveraging shared knowledge and task relationships, while also achieving greater computational efficiency. 2.