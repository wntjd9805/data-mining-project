Abstract (cid:42)(cid:85)(cid:82)(cid:88)(cid:83)(cid:3)(cid:50)(cid:85)(cid:71)(cid:72)(cid:85)(cid:76)(cid:81)(cid:74)(cid:3)(cid:38)(cid:82)(cid:81)(cid:86)(cid:87)(cid:85)(cid:68)(cid:76)(cid:81)(cid:87)(cid:86)(cid:3)(cid:11)(cid:42)(cid:85)(cid:82)(cid:38)(cid:82)(cid:12)(cid:3)(cid:47)(cid:82)(cid:86)(cid:86) (cid:68)(cid:79)(cid:79)(cid:3)(cid:83)(cid:82)(cid:86)(cid:17)(cid:3)(cid:71)(cid:76)(cid:86)(cid:87)(cid:68)(cid:81)(cid:70)(cid:72)(cid:86)(cid:3)(cid:31)(cid:3)(cid:68)(cid:79)(cid:79)(cid:3)(cid:81)(cid:72)(cid:74)(cid:17)(cid:3)(cid:71)(cid:76)(cid:86)(cid:87)(cid:68)(cid:81)(cid:70)(cid:72)(cid:86)
Contrastive learning has become an important tool in learning representations from unlabeled data mainly rely-ing on the idea of minimizing distance between positive data pairs, e.g., views from the same images, and maximizing distance between negative data pairs, e.g., views from dif-ferent images. This paper proposes a new variation of the contrastive learning objective, Group Ordering Constraints (GroCo), that leverages the idea of sorting the distances of positive and negative pairs and computing the respec-tive loss based on how many positive pairs have a larger distance than the negative pairs, and thus are not ordered correctly. To this end, the GroCo loss is based on differen-tiable sorting networks, which enable training with sorting supervision by matching a differentiable permutation ma-trix, which is produced by sorting a given set of scores, to a respective ground truth permutation matrix. Applying this idea to groupwise pre-ordered inputs of multiple positive and negative pairs allows introducing the GroCo loss with implicit emphasis on strong positives and negatives, leading to better optimization of the local neighborhood. We eval-uate the proposed formulation on various self-supervised learning benchmarks and show that it not only leads to im-proved results compared to vanilla contrastive learning but also shows competitive performance to comparable meth-ods in linear probing and outperforms current methods in k-NN performance. 1 1.

Introduction
Self-supervised learning has become a topic of grow-ing interest over the last years as it allows models to learn representations from large-scale data without the need for human annotation. Many approaches rely on the idea of contrastive learning and were able not only to nar-row the gap to the supervised learning performance in vi-sion [23, 14, 60, 30, 3, 63], but also to train state-of-the-art vision-language [52, 56] and multimodal models [40]. All 1https://github.com/ninatu/learning_by_sorting (cid:56)(cid:83)(cid:71)(cid:68)(cid:87)(cid:72) (cid:50)(cid:85)(cid:71)(cid:72)(cid:85)(cid:76)(cid:81)(cid:74)(cid:18)(cid:39)(cid:76)(cid:86)(cid:87)(cid:68)(cid:81)(cid:70)(cid:72) (cid:50)(cid:85)(cid:71)(cid:72)(cid:85)(cid:76)(cid:81)(cid:74)(cid:18)(cid:39)(cid:76)(cid:86)(cid:87)(cid:68)(cid:81)(cid:70)(cid:72) (cid:54)(cid:90)(cid:68)(cid:83) (cid:38)(cid:82)(cid:81)(cid:87)(cid:85)(cid:68)(cid:86)(cid:87)(cid:76)(cid:89)(cid:72)(cid:3)(cid:47)(cid:82)(cid:86)(cid:86) (cid:56)(cid:83)(cid:71)(cid:68)(cid:87)(cid:72) (cid:39)(cid:76)(cid:86)(cid:87)(cid:68)(cid:81)(cid:70)(cid:72) (cid:39)(cid:76)(cid:86)(cid:87)(cid:68)(cid:81)(cid:70)(cid:72)
Figure 1. The idea of the proposed group ordering constraints loss compared to pairwise contrastive losses: GroCo arranges positive and negative data points so that the largest distance to positives must be smaller than the smallest distance to negative points. To this end, the loss implicitly minimizes the amount of necessary swap operation to achieve the ordering constraint. Thus, it focuses on overlapping positives and negatives compared to standard con-trastive losses that minimize resp. maximize all pairwise distances. of these methods rely on the concept of the pairwise con-trastive loss, which is based on the idea that a so-called positive pair, e.g., an image serving as an anchor and an augmentation of the same image, should be closer to each other in an embedding space than a so-called negative pair, e.g., a pair made up of an anchor image and a different im-age, should be far away from each other. However, it has been noted that the idea of a pairwise contrastive loss also has some limitations, such as the alignment of the embed-ding space based on individual pairs. Several attempts have been made to address this issue, e.g., combining the con-trastive idea with concepts based on local neighborhoods, such as clustering (SwAV [11]), or minimizing distances be-tween multiple positive pairs for the same instance together (Whitening [24]). Another limitation of the contrastive loss is that is that the embedding space is optimized with re-spect to all negatives, i.e., even negatives that are far away
(cid:85) (cid:82) (cid:75) (cid:70) (cid:81) (cid:36) (cid:86) (cid:72) (cid:89) (cid:76) (cid:87) (cid:76) (cid:86) (cid:82) (cid:51) (cid:3) (cid:73) (cid:82) (cid:3) (cid:83) (cid:88) (cid:82) (cid:85) (cid:42) (cid:86) (cid:72) (cid:89) (cid:76) (cid:87) (cid:68) (cid:74) (cid:72) (cid:49) (cid:3) (cid:73) (cid:82) (cid:3) (cid:83) (cid:88) (cid:82) (cid:85) (cid:42) (cid:51)(cid:85)(cid:82)(cid:77)(cid:72)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81) (cid:54)(cid:83)(cid:68)(cid:70)(cid:72)(cid:29) (cid:72)(cid:3)(cid:87)(cid:82) (cid:39)(cid:76)(cid:86)(cid:87)(cid:68)(cid:81)(cid:70)(cid:72)(cid:3)(cid:87)(cid:82)(cid:3) (cid:36)(cid:81)(cid:70)(cid:75)(cid:82)(cid:85)(cid:29) (cid:82)(cid:85)(cid:29) (cid:39)(cid:76)(cid:73)(cid:73)(cid:72)(cid:85)(cid:72)(cid:81)(cid:87)(cid:76)(cid:68)(cid:69)(cid:79)(cid:72)(cid:3)(cid:50)(cid:71)(cid:71)(cid:16)(cid:72)(cid:89)(cid:72)(cid:81) (cid:54)(cid:82)(cid:85)(cid:87)(cid:76)(cid:81)(cid:74)(cid:3)(cid:49)(cid:72)(cid:87)(cid:90)(cid:82)(cid:85)(cid:78)(cid:29) (cid:11)(cid:54)(cid:82)(cid:85)(cid:87)(cid:86)(cid:3)(cid:76)(cid:81)(cid:83)(cid:88)(cid:87)(cid:3)(cid:89)(cid:68)(cid:79)(cid:88)(cid:72)(cid:86)(cid:3)(cid:76)(cid:81)(cid:3)(cid:68)(cid:86)(cid:70)(cid:72)(cid:81)(cid:71)(cid:76)(cid:81)(cid:74)(cid:3)(cid:82)(cid:85)(cid:71)(cid:72)(cid:85)(cid:3) (cid:69)(cid:92)(cid:3)(cid:86)(cid:90)(cid:68)(cid:83)(cid:83)(cid:76)(cid:81)(cid:74)(cid:3)(cid:81)(cid:72)(cid:76)(cid:74)(cid:75)(cid:69)(cid:82)(cid:85)(cid:76)(cid:81)(cid:74)(cid:3)(cid:72)(cid:79)(cid:72)(cid:80)(cid:72)(cid:81)(cid:87)(cid:86)(cid:12) (cid:40)(cid:81)(cid:70)(cid:82)(cid:71)(cid:72)(cid:85) (cid:19)(cid:17)(cid:21) (cid:19)(cid:17)(cid:25) (cid:19)(cid:17)(cid:23) (cid:19)(cid:17)(cid:27) (cid:39)(cid:76)(cid:73)(cid:73)(cid:72)(cid:85)(cid:72)(cid:81)(cid:87)(cid:76)(cid:68)(cid:69)(cid:79)(cid:72)(cid:3)(cid:51)(cid:72)(cid:85)(cid:80)(cid:88)(cid:87)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:48)(cid:68)(cid:87)(cid:85)(cid:76)(cid:91)(cid:29)(cid:3) (cid:39) (cid:11) (cid:11)(cid:51)(cid:85)(cid:82)(cid:69)(cid:68)(cid:69)(cid:76)(cid:79)(cid:76)(cid:87)(cid:92)(cid:3)(cid:82)(cid:73)(cid:3)(cid:72)(cid:79)(cid:72)(cid:80)(cid:72)(cid:81)(cid:87)(cid:86)(cid:3)(cid:87)(cid:82)(cid:3)(cid:69)(cid:72)(cid:3)(cid:86)(cid:90)(cid:68)(cid:83)(cid:83)(cid:72)(cid:71)(cid:3) (cid:87)(cid:82)(cid:3)(cid:85)(cid:68)(cid:81)(cid:78)(cid:3)(cid:62)(cid:20)(cid:15)(cid:21)(cid:15)(cid:22)(cid:15)(cid:23)(cid:64)(cid:12) (cid:51)(cid:82)(cid:86)(cid:76)(cid:87)(cid:76)(cid:89)(cid:72)(cid:86) (cid:19)(cid:17)(cid:21) (cid:19) (cid:21) (cid:19)(cid:17)(cid:25) (cid:19) (cid:25) (cid:49)(cid:72)(cid:74)(cid:68)(cid:87)(cid:76)(cid:89)(cid:72)(cid:86) (cid:19)(cid:17)(cid:23) (cid:19)(cid:17)(cid:27) (cid:62)(cid:20)(cid:64)(cid:3) (cid:19)(cid:17)(cid:28) (cid:19)(cid:17)(cid:28) (cid:19)(cid:17)(cid:20) (cid:19) (cid:19) (cid:62)(cid:21)(cid:64)(cid:3) (cid:19)(cid:17)(cid:20) (cid:19)(cid:17)(cid:20) (cid:19)(cid:17)(cid:22) (cid:19)(cid:17)(cid:22) (cid:19)(cid:17)(cid:25) (cid:19)(cid:17)(cid:25) (cid:19) (cid:19) (cid:19) (cid:19) (cid:62)(cid:22)(cid:64)(cid:3) (cid:62)(cid:23)(cid:64)(cid:3) (cid:19) (cid:19) (cid:19) (cid:19) (cid:19)(cid:17)(cid:25) (cid:19)(cid:17)(cid:25) (cid:19)(cid:17)(cid:22) (cid:19)(cid:17)(cid:22) (cid:19)(cid:17)(cid:20) (cid:19)(cid:17)(cid:20) (cid:19) (cid:19) (cid:19)(cid:17)(cid:20) (cid:19)(cid:17)(cid:28) (cid:19)(cid:17)(cid:28) (cid:54)(cid:90)(cid:68)(cid:83)(cid:86)(cid:3)(cid:90)(cid:76)(cid:87)(cid:75)(cid:76)(cid:81)(cid:3)(cid:74)(cid:85)(cid:82)(cid:88)(cid:83)(cid:3)(cid:16)(cid:3)(cid:3)(cid:82)(cid:78)(cid:82)(cid:78) (cid:81)(cid:82)(cid:87)(cid:3)(cid:82)(cid:78) (cid:54)(cid:90)(cid:68)(cid:83)(cid:86)(cid:3)(cid:69)(cid:72)(cid:87)(cid:90)(cid:72)(cid:72)(cid:81)(cid:3)(cid:74)(cid:85)(cid:82)(cid:88)(cid:83)(cid:86)(cid:3)(cid:16)(cid:3)(cid:81)(cid:82)(cid:87)(cid:3)(cid:82)(cid:78) (cid:86) (cid:81) (cid:82) (cid:76) (cid:87) (cid:76) (cid:86) (cid:82) (cid:51) (cid:3) (cid:72) (cid:89) (cid:76) (cid:87) (cid:76) (cid:86) (cid:82) (cid:51) (cid:86)(cid:86) (cid:81) (cid:82) (cid:76) (cid:87) (cid:76) (cid:86) (cid:82) (cid:51) (cid:3) (cid:72) (cid:89) (cid:76) (cid:87) (cid:68) (cid:74) (cid:72) (cid:49) (cid:42)(cid:85)(cid:82)(cid:88)(cid:83)(cid:3)(cid:50)(cid:85)(cid:71)(cid:72)(cid:85)(cid:76)(cid:81)(cid:74)(cid:3)(cid:38)(cid:82)(cid:81)(cid:86)(cid:87)(cid:85)(cid:68)(cid:76)(cid:81)(cid:87)(cid:86)(cid:3) (cid:11)(cid:42)(cid:85)(cid:82)(cid:38)(cid:82)(cid:12)(cid:3)(cid:47)(cid:82)(cid:86)(cid:86)(cid:29) (cid:54)(cid:88)(cid:80)(cid:3)(cid:82)(cid:89)(cid:72)(cid:85)(cid:3)(cid:81)(cid:72)(cid:74)(cid:68)(cid:87)(cid:76)(cid:89)(cid:72)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:83)(cid:82)(cid:86)(cid:76)(cid:87)(cid:76)(cid:89)(cid:72)(cid:3)(cid:83)(cid:82)(cid:86)(cid:76)(cid:87)(cid:76)(cid:82)(cid:81)(cid:86)(cid:3) (cid:68)(cid:81)(cid:71)(cid:3)(cid:70)(cid:82)(cid:80)(cid:83)(cid:88)(cid:87)(cid:72)(cid:3)(cid:37)(cid:38)(cid:40)(cid:3)(cid:79)(cid:82)(cid:86)(cid:86) (cid:153)(cid:153) (cid:51)(cid:82)(cid:86)(cid:76)(cid:87)(cid:76)(cid:89)(cid:72)(cid:3) (cid:51)(cid:82)(cid:86)(cid:76)(cid:87)(cid:76)(cid:82)(cid:81)(cid:86) (cid:51)(cid:82)(cid:153) (cid:49)(cid:72) (cid:49)(cid:72)(cid:74)(cid:68)(cid:87)(cid:76)(cid:89)(cid:72)(cid:3) (cid:72)(cid:74)(cid:68)(cid:87)(cid:76)(cid:89)(cid:72) (cid:82)(cid:86)(cid:76)(cid:87)(cid:76)(cid:82)(cid:81)(cid:86) (cid:51)(cid:82)(cid:86)(cid:76)(cid:87)(cid:76)(cid:82)(cid:81)(cid:86) (cid:20)(cid:17)(cid:19) (cid:19)(cid:17)(cid:23) (cid:19)(cid:17)(cid:25) (cid:19)(cid:17)(cid:25) (cid:19) (cid:19) (cid:19) (cid:19) (cid:19)(cid:17)(cid:25) (cid:19)(cid:17)(cid:25) (cid:19)(cid:17)(cid:23) (cid:20)(cid:17)(cid:19) (cid:37)(cid:38)(cid:40)(cid:3)(cid:79)(cid:82)(cid:86)(cid:86) (cid:51)(cid:82)(cid:86)(cid:76)(cid:87)(cid:76)(cid:89)(cid:72)(cid:3)(cid:42)(cid:55) (cid:20)(cid:17)(cid:19) (cid:20)(cid:17)(cid:19) (cid:19) (cid:19) (cid:49)(cid:72)(cid:74)(cid:68)(cid:87)(cid:76)(cid:89)(cid:72)(cid:3)(cid:42)(cid:55) (cid:19) (cid:19) (cid:20)(cid:17)(cid:19) (cid:20)(cid:17)(cid:19)
Figure 2. Overview of the proposed loss: Distances of positives and negatives are computed with respect to an anchor. The concatenated distances are sorted via a differentiable sorting network that computes the swapping probability. The result is a differentiable permutation matrix, in which the column values can be considered as the probabilities of sorting the elements to the corresponding positions. To enforce only the relationships between groups, we sum over the positive and negative rows of the permutation matrix. The loss is then computed as the BCE between the row-wise entries and the ground truth. from the anchor will contribute to the optimization of the representation. Other methods were proposed to address this issue, such as hard negative selection by controlling the hardness of examples [53] or negative selection by sparse support vectors [57]. Nevertheless, these methods still re-quire manual selection of the hardness level [53] or incur an additional optimization cost [57].
To shift away from the concept of minimizing resp. max-imizing all pairwise distances, this paper proposes a varia-tion of the contrastive learning formulation, namely Group
Ordering Constraints (GroCo). The idea of GroCo is that positive and negative distances should be sorted in a way that any positive should be closer to an anchor image than any negative, thus forming a group of positive pairs and a group of negative pairs. The idea is illustrated in Figure 1.
In comparison to pairwise contrastive losses, the GroCo loss combines the distance information of groups of posi-tive and negative pairs and optimization mainly depends on incorrectly “sorted” pairs. To enforce the group ordering constraints in the projection space, we propose the idea of learning by sorting: we suggest sorting positives and neg-atives by distance to the anchor image in a differentiable way and swapping them if they are in the wrong order. This leads to a more holistic approach considering all relation-ships between data points, thereby better utilizing and op-timizing the embeddings (esp. for multiple positive pairs), and leading to improved down-stream performance. To cre-ate an end-to-end training pipeline, we leverage recent ad-vances in differentiable sorting [21, 28, 44, 45, 46, 47, 48].
Speciﬁcally, we utilize a differentiable sorting algorithm to obtain a differentiable permutation matrix for sorting a list of distances to the positive and negative images, as shown in
Figure 2. If we would know the full ground truth orderings among positives and negatives (such as which positive sam-ple should be closer to the anchor than another positive sam-ple), we could create a ground truth permutation matrix, and calculate how much the predicted permutation matrix would deviate from the ground truth one [28, 21, 45, 47]. Because we do not know the ground truth distance ordering within the positive or the negative groups, we propose the GroCo loss as a relaxed formulation of the original sorting super-vision that captures how many negative elements appear in the positive positions and vice versa. The proposed GroCo loss alleviates some aspects of vanilla contrastive learning:
ﬁrst, it treats positive and negative pairs as groups instead of individual pairs, and second, the resulting group order-ing focuses on optimizing the local neighborhood around an anchor image by mainly optimizing too close negative and too distant positives, rather than optimizing all data points at once. Thus, it implicitly also focuses on the strongest positive (furthest from the anchor) and strongest negative (closest to the anchor) examples.
To show the capabilities of the proposed approach, we evaluated it on various competitive self-supervised learning benchmarks, namely in the context of linear probing, k-NN classiﬁcation, transfer learning, as well as image retrieval.
The evaluation shows that the model trained via group or-dering constraints outperforms contrastive learning frame-works in linear probing and transfer learning and excels in the context of shaping local neighborhoods on the tasks such as k-NN classiﬁcation and image retrieval.
The contributions of this work are summarized as follows:
• We advance the concept of contrastive learning by in-troducing Group Ordering Constraints (GroCo) that treat positive and negative elements as groups rather than in-dividual pairs as in conventional contrastive learning.
• To derive a loss that optimizes the proposed constraints, we harness recent differentiable sorting methods and ob-tain a loss that suggests sorting positive and negative ele-ments and swapping them if they are in the wrong order
— thus, we introduce a new contrastive learning method called learning by sorting.
• The proposed method provides embeddings that achieve
competitive performance in linear probing and are espe-cially suitable to model the local neighborhoods and out-perform contrastive learning frameworks on a wide range of nearest-neighbor tasks. 2.