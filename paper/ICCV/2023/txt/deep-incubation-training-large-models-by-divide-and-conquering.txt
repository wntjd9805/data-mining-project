Abstract
Recent years have witnessed a remarkable success of large deep learning models. However, training these mod-els is challenging due to high computational costs, painfully slow convergence, and overfitting issues.
In this paper, we present Deep Incubation, a novel approach that en-ables the efficient and effective training of large models by dividing them into smaller sub-modules which can be trained separately and assembled seamlessly. A key chal-lenge for implementing this idea is to ensure the compat-ibility of the independently trained sub-modules. To ad-dress this issue, we first introduce a global, shared meta model, which is leveraged to implicitly link all the mod-ules together, and can be designed as an extremely small network with negligible computational overhead. Then we propose a module incubation algorithm, which trains each sub-module to replace the corresponding component of the meta model and accomplish a given learning task.
Despite the simplicity, our approach effectively encour-ages each sub-module to be aware of its role in the tar-get large model, such that the finally-learned sub-modules can collaborate with each other smoothly after being as-sembled. Empirically, our method can outperform end-to-end (E2E) training in well-established training setting and shows transferable performance gain for downstream tasks (e.g., object detection and image segmentation on
COCO and ADE20K). Our code is available at https:
//github.com/LeapLabTHU/Deep-Incubation. 1.

Introduction
Large neural networks have achieved remarkable suc-cess across various domains such as natural language under-standing [37, 8], computer vision [9, 57] and reinforcement learning [41, 6]. In particular, the foundation models [7, 56]
*Equal contribution.
†Corresponding author.
Figure 1: An illustration of our idea. We first train the sub-modules of a large model fully independently, and then assemble the trained modules to obtain the target model. heavily rely on large deep learning models to achieve state-of-the-art performance. The research field has developed a diverse set of strategies for efficient and adaptive infer-ence of deep models [50, 48, 21, 18, 20, 36]. However, the training of large models still remains challenging in several aspects. On infrastructure side, centralized resources with strong computational and memory capacities are often re-quired [27, 12, 57, 13]. On optimization side, the training process tends to be unstable, difficult to converge, and vul-nerable to overfitting [27, 15].
In this paper, we propose a divide-and-conquer strategy to improve the effectiveness (better generalization perfor-mance) and the efficiency (lower training cost) for training large models.
In specific, we divide a large model into smaller sub-modules, train these modules separately, and then assemble them to obtain the final model. Compared with directly training the whole large network from scratch, starting the learning on top of smaller modules yields a faster and more stable converge process and higher robust-ness against overfitting. The independent nature also allows the training of each module to be performed on different 1
machines with no communication needed. We refer to this paradigm as “modular training”, and illustrate it in Fig. 1.
Importantly, designing an effective modular training mechanism is non-trivial, as there exists a dilemma be-tween independency and compatibility: although training sub-modules independently enjoys advantages in terms of optimization efficiency and generalization performance, it is challenging to make these modules compatible with each other when assembling them together. Some preliminary works alleviate this problem by leveraging approximated gradients [26, 11, 25] or local objectives [3, 4, 51], at the price of only achieving partial independency. However, the modules are still highly entangled during forward propaga-tion, and generally have not exhibited the ability to effec-tively address the optimization issues faced by training the recently proposed large models (e.g., ViTs, see Tab. 2).
In contrast, this paper proposes a Deep Incubation ap-proach, which not only elegantly addresses this dilemma, but also demonstrates that the training of modern large mod-els can benefit from the divide-and-conquer paradigm (see
Tab. 1 and Fig. 4). Specifically, we first introduce a global, shared meta model, under the goal of implicitly linking all the modules together. On top of it, we propose a module incubation algorithm that trains each sub-module to replace the corresponding component of the meta model in terms of accomplishing a given learning task (e.g., minimizing the supervised training loss). This design effectively encour-ages each sub-module to be aware of its role in the target large model. As a consequence, even though all the mod-ules are independently trained, we are able to obtain highly compatible sub-modules which collaborate with each other smoothly after being assembled. Notably, our approach allows deploying an extremely shallow meta model, e.g., only one layer per module, with which the computational overhead is negligible, while the performance of the target model is not affected. An overview of Deep Incubation is presented in Fig. 3.
We validate the effectiveness of Deep Incubation on the well-established DeiT training recipe [44]. Specifically, our method is able to outperform E2E training at the same train-ing cost or deliver similar performance at a reduced training cost. Meanwhile, the performance gain is also transferable to downstream tasks like object detection on COCO [33] and semantic segmentation on ADE20K [59]. 2.