Abstract 1.

Introduction
Deep generative models have various content creation ap-plications such as graphic design, e-commerce, and virtual try-on. However, current works mainly focus on synthesizing realistic visual outputs, often ignoring other sensory modal-ities, such as touch, which limits physical interaction with users. In this work, we leverage deep generative models to create a multi-sensory experience where users can touch and see the synthesized object when sliding their fingers on a haptic surface. The main challenges lie in the significant scale discrepancy between vision and touch sensing and the lack of explicit mapping from touch sensing data to a haptic rendering device. To bridge this gap, we collect high-resolution tactile data with a GelSight sensor and create a new visuotactile clothing dataset. We then develop a con-ditional generative model that synthesizes both visual and tactile outputs from a single sketch. We evaluate our method regarding image quality and tactile rendering accuracy. Fi-nally, we introduce a pipeline to render high-quality visual and tactile outputs on an electroadhesion-based haptic de-vice for an immersive experience, allowing for challenging materials and editable sketch inputs.
The past few years have witnessed significant progress in content creation powered by deep generative models [30, 59] and neural rendering techniques [45, 71]. Recent works can synthesize realistic images with various user controls, such as user sketches [28], text prompts [55], and seman-tic maps [51]. However, most works focus on synthesizing visual outputs, ignoring other sensory outputs such as touch.
In real life, humans use vision and touch to explore ob-jects. When shopping for clothing, we look at them to per-ceive their shape and appearance and touch them to antic-ipate the experience of wearing them. A single touch can reveal the material’s roughness, hardness, and local geome-try. Multimodal perceptual inputs enable humans to obtain a more comprehensive understanding of the target objects, enhancing user experiences, such as online shopping and quick prototyping. Moreover, it opens up new possibilities for content creation, such as touchable VR and movies.
In this work, we aim to expand the capability of content creation. We introduce a new problem setting, controllable visual-tactile synthesis, for synthesizing high-resolution im-ages and haptic feedback outputs from user inputs of a sketch
or text. Our goal is to provide a more immersive experience for humans when exploring objects in a virtual environment.
Visual-tactile synthesis is challenging for two reasons.
First, existing generative models struggle to model visual and tactile outputs jointly due to the dramatic differences in perception scale: vision provides a global sense of our surroundings, while touch offers only a narrow scale of local details. Second, there do not exist data-driven end-to-end sys-tems that can effectively render the captured tactile data on a haptic display, as existing haptic rendering systems heavily rely on manually-designed haptic patterns [5, 3, 33, 61].
To address the challenges, we introduce a haptic mate-rial modeling system based on surface texture and topogra-phy. We first collect the high-resolution surface geometry of target objects with a high-resolution tactile sensor Gel-Sight [84, 75] as our training data. To generate visual-tactile outputs that can render materials based on user inputs, we propose a new conditional adversarial learning method that can learn from multimodal data at different scales. Different from previous works [28, 76], our model learns from dense supervision from visual images and sparse supervision from a set of sampled local tactile patches. During inference, we generate dense visual and tactile outputs from a new sketch design. We then render our models’ visual and tactile output with a TanvasTouch haptic screen [15]. The TanvasTouch device displays the visual output on a regular visual screen and uses electroadhesion techniques [68] to render the force feedback of different textures according to a friction map.
Humans can feel the textures as a changing friction force distribution when sliding their fingers on the screen [7].
We collect a spatially aligned visual-tactile dataset named
TouchClothing that contains 20 pieces of clothing, in-cluding pants and shirts, with diverse materials and shapes.
We evaluate our model regarding image quality and per-ceptual realism with both automatic metrics and user study.
Experimental results show that our method can successfully integrate the global structure provided by the sketch and the local fine-grained texture determined by the cloth material, as shown in Figure 1. Furthermore, we demonstrate sketch-and text-based editing applications enabled by our system to generate new clothing designs for humans to see and feel.
Our code and data are available on our website 1. 2.