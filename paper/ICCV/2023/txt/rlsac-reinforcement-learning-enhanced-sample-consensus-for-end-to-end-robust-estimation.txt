Abstract
Robust estimation is a crucial and still challenging task, which involves estimating model parameters in noisy envi-ronments. Although conventional sampling consensus-based algorithms sample several times to achieve robustness, these algorithms cannot use data features and historical informa-tion effectively. In this paper, we propose RLSAC, a novel
Reinforcement Learning enhanced SAmple Consensus frame-work for end-to-end robust estimation. RLSAC employs a graph neural network to utilize both data and memory fea-tures to guide exploring directions for sampling the next minimum set. The feedback of downstream tasks serves as the reward for unsupervised training. Therefore, RL-SAC can avoid differentiating to learn the features and the feedback of downstream tasks for end-to-end robust esti-mation. In addition, RLSAC integrates a state transition module that encodes both data and memory features. Our experimental results demonstrate that RLSAC can learn from features to gradually explore a better hypothesis. Through analysis, it is apparent that RLSAC can be easily trans-ferred to other sampling consensus-based robust estimation tasks. To the best of our knowledge, RLSAC is also the first method that uses reinforcement learning to sample consen-sus for end-to-end robust estimation. We release our codes at https://github.com/IRMVLab/RLSAC. 1.

Introduction
As a fundamental module in computer vision, robust esti-mation is crucial for many tasks, such as camera pose estima-tion [10, 33, 31, 32], motion segmentation [26, 19, 29, 30], short and wide baseline matching [22, 28], plane fitting [18],
*Corresponding Authors.
The first two authors contributed equally.
Figure 1. RLSAC remodel the sampling consensus process. By modeling the sampling consensus as a reinforcement learning pro-cess, RLSAC can achieve end-to-end learning on various robust estimation tasks. and line fitting [12, 20]. However, it is still difficult to ex-clude disturbances while estimating accurate models. To address this issue, sampling consensus-based algorithms are widely used, which are represented by the RANdom SAm-ple Consensus (RANSAC) [16] algorithm. It first samples the minimum set required for the task, e.g., a minimum set size of 2 for 2D line fitting. Then, the hypothesis is solved by the minimum set. Next, all data is divided into inliers and outliers, according to their residuals to the hypothesis.
Finally, the above process is repeated and the best hypoth-esis is selected based on the highest inlier ratio. RANSAC
can provide strong robustness and generalization, but as the outlier rate increases, the probability of sampling inliers de-creases. As a result, the performance of RANSAC degrades rapidly. This is because RANSAC samples each data evenly, regardless of data features that can be used for classifying inliers and outliers. Additionally, the non-differentiable de-terministic random sampling of RANSAC also limits it to be integrated into the learning-based pipeline.
Since sampling the minimum set from the data is quite similar to the process of sampling an action from the ac-tion space in reinforcement learning [17, 13], the sampling consensus can be integrated into the reinforcement learn-ing framework. Sampling in reinforcement learning can be achieved through a neural network, which extracts the data features. In addition, the reward from the environment can be used to train the reinforcement learning framework with-out differentiation. Therefore, to learn from data features and avoid differentiating, we propose the RLSAC: Reinforce-ment Learning enhanced SAmple Consensus for end-to-end robust estimation. As shown in Figure 1, RLSAC regards sampling consensus as the process of interaction between the agent and environment in reinforcement learning. Specif-ically, the agent uses a neural network to sample the minimal set from the data as an action. The environment then per-forms model generation and evaluation based on the action and outputs the next state, which is used in the next iteration.
However, designing appropriate reward and state are very important and challenging in reinforcement learning. To achieve reinforcement learning enhanced sampling consen-sus, RLSAC proposes new state transition and reward mod-ules. Specifically, the state is encoded by augmenting the original data features with memory features, including the current action, data residuals, and historical information.
When the state is input into the agent, these features can provide more information about the quality of the previous action. This allows RLSAC to gradually explore a better hypothesis by utilizing this memory information.
Additionally, the evaluation result of the generated hy-pothesis can be used as the reward signal to train the neural network without differentiation. The reward signal enables the learning-based sampling consensus for end-to-end robust estimation. Furthermore, the evaluation result is the feed-back from the downstream task. Thus, the neural network can learn to effectively use the data features and optimize the output to meet the requirements of the downstream task.
Moreover, instead of directly predicting the final result from the data in one shot [27], RLSAC employs multiple episodes, each containing several sampling processes. In ad-dition, RLSAC performs one random sampling at the begin-ning of each episode to form the initial state. This approach preserves the robustness of multiple random sampling and provides basic performance for RLSAC. Besides, RLSAC can be extended to other robust estimation tasks since it is not restricted to any specific task.
The proposed RLSAC is tested on two classic robust es-timation tasks, which are the 2D line fitting task and the fundamental matrix estimation task. The experimental re-sults show that RLSAC achieves great performance.
Our main contributions are as follows:
• We propose RLSAC: a novel Reinforcement Learning enhanced SAmple Consensus framework for end-to-end robust estimation. It learns data features to sample the minimum set. RLSAC retains the robustness of the multiple sampling process, while the initial random sampling can provide the basic performance.
• RLSAC proposes an approach for state encoding, which includes both current and historical information. This enables the agent to assess the quality of the previous actions and gradually explore better hypotheses. RL-SAC is trained unsupervised using the reward function, which avoids differentiating the sampling process and achieves end-to-end robust estimation.
• RLSAC is evaluated on two robust estimation tasks.
The 2D line fitting task demonstrates its robustness to disturbances and effective progressive exploration capability. In the fundamental matrix estimation task,
RLSAC achieves state-of-the-art performance. Further-more, RLSAC can be easily applied to other sampling consensus-based robust estimation tasks. 2.