Abstract
Federated learning (FL) is a privacy-enhanced dis-tributed machine learning framework, in which multiple clients collaboratively train a global model by exchang-ing their model updates without sharing local private data.
However, the adversary can use gradient inversion attacks to reveal the clients’ privacy from the shared model updates.
Previous attacks assume the adversary can infer the local learning rate of each client, while we observe that: (1) us-ing the uniformly distributed random local learning rates does not incur much accuracy loss of the global model, and (2) personalizing local learning rates can mitigate the drift issue which is caused by non-IID (identically and in-dependently distributed) data. Moreover, we theoretically derive a convergence guarantee to FedAvg with uniformly perturbed local learning rates. Therefore, by perturbing the learning rate of each client with random noise, we propose a learning rate perturbation (LRP) defense against gradi-ent inversion attacks. Specifically, for classification tasks, we adapt LPR to ada-LPR by personalizing the expectation of each local learning rate. The experiments show that our defenses can well enhance privacy preservation against ex-isting gradient inversion attacks, and LRP outperforms 5 baseline defenses against a state-of-the-art gradient inver-sion attack. In addition, our defenses only incur minor ac-curacy reductions (less than 0.5%) of the global model. So they are effective in real applications. 1.

Introduction
Federated learning (FL) [16, 19, 20, 33] is popularly used to meet the needs of learning from distributed data and pro-tecting the privacy of data owners. Instead of transferring the local data, each FL client trains a model on its local data and exchanges its model update under the coordina-tion of a central parameter server. FL leaves the training data distributed among its clients, which makes it align well with data privacy regulations, e.g., General Data Protection
*Corresponding author: Jie Xu (cheer1107@bupt.edu.cn).
Regulation (GDPR) [36]. Thus, FL is suitable for devel-oping privacy-sensitive machine learning applications such as medical services [3, 5], financial fraud detection [37], and various applications of the future sixth-generation (6G) wireless communication network [24, 31, 38].
Recent works have shown that clients’ private training data may be leaked through this update-sharing scheme by gradient inversion attacks [9, 10, 14, 28, 32, 43]. Several de-fensive strategies have been proposed to strengthen the pri-vacy properties of the FL system, such as differential pri-vacy [1], secure multi-party computation [2, 39], gradient compression [29], and data representation perturbation [35].
Nonetheless, it has been demonstrated that these defenses are insufficient to provide privacy guarantees against gradi-ent inversion attacks [28] or incur significant computational overheads [35]. Since privacy protection is the major mo-tivation of FL, it is urgent to develop effective defenses to tackle the data leakage issue.
Assumptions of previous gradient inversion attacks.
Most gradient inversion attacks assume the adversary is a
FL server that is interested in unveiling the private training data of the clients from the model updates uploaded by the clients [14, 28, 32, 35], while the server must follow the FL protocol honestly and cannot modify the model architec-ture. Moreover, the attackers always explicitly or implic-itly assume they can get the gradients of each client while the client only shares its model updates.
In other words, the attackers assume they know the learning rate (LR) of each client such that they can generate the training gradients from the uploaded model updates. This assumption is real-istic with existing FL algorithms since the learning rates of clients follow a unified regularity developed by the server, while the necessity of avoiding the heterogeneity of local learning rates has not been thoroughly investigated.
A natural question is: Can clients utilize local learning rates to protect their data? To investigate this, we form a
FL system composed of two clients with non-IID data from the MNIST dataset and adjust the clients’ learning rates to train the models. Compared with allocating a unified learn-ing rate by the server, we find randomizing the local learn-ing rates only causes small fluctuations (about 0.2%) in
the global models’ accuracy, and allocating relatively big-ger learning rate expectations to clients with higher-quality datasets can suppress FL’s drift [17] issue introduced by non-IID data and thus improve the model’s accuracy.
Based on our above observations, we propose LRP, a novel defense that perturbs every client’s learning rates, such that the learning rates appear uniformly random. For classification tasks on non-IID data, we present an adaptive defense (ada-LRP) to improve the model’s accuracy by per-sonalizing the expectations of local learning rates. As the server cannot extract clients’ exact gradients without know-ing their learning rates, the data reconstructed by gradient inversion attacks can be significantly degraded.
In addi-tion, we derive a convergence guarantee to FedAvg with perturbed local learning rates on non-IID data. The image classification experiments on MNIST [23], CIFAR-10 [21],
CIFAR100 [21], and ImageNet [6] show that our defenses do not incur much accuracy loss (less than 0.5%). We con-duct experiments on MNIST, CIFAR-100, and LFW [13] for defending against the Deep Leakage from Gradient (DLG) [43] attack and Improved DLG (iDLG) [42] at-tack, and on ImageNet against Generative Gradient Leak-age (GGL) [28] attack. The results show that our defenses successfully enhance the privacy preservation of FL against gradient inversion attacks.
Our main contributions are summarized as follows:
• Findings. By analyzing the impact of randomizing and personalizing local learning rates on FL, we find that (1) setting the local learning rates to be uniformly ran-dom has a minor effect on test accuracy but signifi-cantly degrades gradient inversion attacks, (2) FedAvg with uniformly distributed random learning rates con-verges well for strongly convex and smooth problems, and (3) scaling the local learning rates to be personal-ized values can mitigate the drift issue suffered by FL and thus improve the accuracy of the global model.
• Effective defenses against gradient inversion attack.
We propose a learning-rate-perturbation-based defense (LRP), which outperforms five existing defenses on five metrics (e.g., MSE-R and LPIPS) against a state-of-the-art gradient inversion attack. Besides, for classi-fication tasks, our adaptive defense ada-LRP improves the global model’s accuracy compared with LRP. 2.