Abstract
Recent text-to-image diffusion models have demon-strated an astonishing capacity to generate high-quality im-ages. However, researchers mainly studied the way of syn-thesizing images with only text prompts. While some works have explored using other modalities as conditions, consid-erable paired data, e.g., box/mask-image pairs, and fine-tuning time are required for nurturing models. As such paired data is time-consuming and labor-intensive to ac-quire and restricted to a closed set, this potentially becomes the bottleneck for applications in an open world. This paper
* Corresponding Author focuses on the simplest form of user-provided conditions, e.g., box or scribble. To mitigate the aforementioned prob-lem, we propose a training-free method to control objects and contexts in the synthesized images adhering to the given spatial conditions. Specifically, three spatial constraints, i.e., Inner-Box, Outer-Box, and Corner Constraints, are de-signed and seamlessly integrated into the denoising step of diffusion models, requiring no additional training and mas-sive annotated layout data. Extensive experimental results demonstrate that the proposed constraints can control what and where to present in the images while retaining the abil-ity of Diffusion models to synthesize with high fidelity and diverse concept coverage.
1.

Introduction
Due to the large-scale publicly available image-text paired data from websites, recent text conditional auto-regressive and diffusion models, such as DALL-E 1 & 2 [26, 25], Imagen [30], and Stable Diffusion [27], have demonstrated as one of the panaceas in generating images with high fidelity and diverse concept coverage. The ex-cellent capacity of image synthesis increases the potential of these models for practical applications, e.g., art creation.
However, most existing models can only be conditioned on class labels or text prompts. A few studies tried to use other modalities as conditions, e.g., spatial conditions, to further control the object or context synthesis. More fine-grained control on the location or scale of synthesized ob-jects or contexts would widen the applications of text con-ditional generative models for the realistic scenario. For ex-ample, users can interactively design objects or contexts for human-in-the-loop art creation with additional spatial con-ditioning input. As a more user-friendly solution, this in-teractive cooperation with artificial intelligence (AI) would stimulate more potential for content creation.
Layout-to-image literature [19, 32, 33, 36, 40, 12, 2, 1] has studied on the way to synthesize images adhering to the spatial conditioning input. However, the setting of these studies is restricted to the limited closed-set categories, which is infeasible to novel categories in open-world situ-ations. Moreover, the previous studies followed the fully-supervised learning pipeline; hence, considerable paired box/skeleton/mask-image data is required for high-quality training. Since pixel-level annotation is time-consuming and labor-intensive to acquire, label efficiency gradually be-comes the bottleneck of fully-supervised layout-to-image methods. Beyond text prompts as conditions, Stable Dif-fusion [27] and ControlNet [38] have also studied other modalities as conditioning input and provided qualitative results. In contrast to closed-set layout-to-image synthesis methods, Stable Diffusion, and ControlNet nurtured from large-scale image-text pairs have a strong perception of di-verse visual concepts, e.g., different kinds of objects and contexts. Nevertheless, they also follow the general pipeline of layout-to-image literature in a fully-supervised manner, in which massive paired image-layout data is indispensable for high-quality training. Besides, the training period is time-consuming for train-from-scratch or fine-tuning.
In this paper, we focus on the most efficient setting for conditional image synthesis. Specifically, the simplest spa-tial conditions (or termed constraints), e.g., box or scrib-ble, from users are adopted to seamlessly control object and context synthesis during the denoising step of Stable
Diffusion models, requiring no additional model train-ing on the substantial paired layout-image data. As shown in [15], conditioning mechanisms incorporated in
Stable Diffusion provide explicit cross-attentions between
Figure 2: Cross-attentions between target text tokens, e.g., panda, bamboo, snowboard, and intermediate features of the denoiser, i.e., a UNet, in the Stable Diffusion model. the given text prompt and intermediate features of the de-noiser. Specific spatial attention maps for objects or con-texts in the text prompt can be accordingly extracted. As the cross-attentions shown in Fig. 2, the spatial location of high-response attention, i.e., panda and snowboard, is perceptually equivalent to that of objects or contexts in the synthesized images. Hence, a simple idea to con-trol the spatial location and scale of objects/contexts to be synthesized is adding guidance or constraints on the extracted cross-attentions. To achieve this goal, we pro-pose a training-free approach, namely Box-Constrained
Diffusion (BoxDiff), by adding three spatial constraints, i.e., Inner-Box, Outer-Box, and Corner Constraints, on the cross-attentions extracted at each denoising timestep. This plays a role in pointing out directions to update the noised latent vector, which consequently leads synthesized objects or contexts to gradually follow the given spatial conditions.
Furthermore, since strong constraints applied to the cross-attentions will affect the denoising step of diffusion models, impairing the fidelity of the resulting synthesized images, we also explore a manner of representative sampling to mit-igate the problem. Samples synthesized by the proposed
BoxDiff can be found in Fig. 1.
The main contributions of this paper are summarized as:
• We propose a training-free approach, termed Box-Constrained Diffusion (BoxDiff), for text-to-image synthesis following the given spatial conditions, re-quiring no additional model training and massive paired layout-image data.
• The proposed spatial constraints can be seamlessly in-corporated into the denoising step, which retains the strong perception of diverse visual concepts of Stable
Diffusion. Hence, our method can synthesize various novel objects and contexts beyond the closed world.
• Extensive experiments demonstrate that the proposed training-free BoxDiff can synthesize photorealistic im-ages following the given spatial conditions. 2.