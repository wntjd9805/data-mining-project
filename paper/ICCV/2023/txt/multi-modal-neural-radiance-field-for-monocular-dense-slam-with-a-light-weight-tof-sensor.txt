Abstract
Light-weight time-of-flight (ToF) depth sensors are com-pact and cost-efficient, and thus widely used on mobile de-vices for tasks such as autofocus and obstacle detection.
However, due to the sparse and noisy depth measurements, these sensors have rarely been considered for dense ge-ometry reconstruction.
In this work, we present the first dense SLAM system with a monocular camera and a light-weight ToF sensor. Specifically, we propose a multi-modal implicit scene representation that supports rendering both the signals from the RGB camera and light-weight ToF sen-sor which drives the optimization by comparing with the raw sensor inputs. Moreover, in order to guarantee suc-cessful pose tracking and reconstruction, we exploit a pre-dicted depth as an intermediate supervision and develop a coarse-to-fine optimization strategy for efficient learning of the implicit representation. At last, the temporal infor-mation is explicitly exploited to deal with the noisy sig-nals from light-weight ToF sensors to improve the accu-racy and robustness of the system. Experiments demon-strate that our system well exploits the signals of light-weight ToF sensors and achieves competitive results both on camera tracking and dense scene reconstruction. Project page: https://zju3dv.github.io/tof_slam/. 1.

Introduction
Dense simultaneous localization and mapping (dense
SLAM) [38, 8, 9, 44] has extensive applications in aug-mented reality [13, 17], indoor robotics, etc. It usually relies on high-precision and high-resolution depth sensors, such as time-of-flight (ToF) sensors or structured light sensors.
Due to the size, weight, and price issues, these depth sen-sors are only used in a few high-end mobile devices until recent years. In contrast, light-weight ToF sensors, which are cost-effective, compact, and energy-efficient, were inte-grated into hundreds of smartphone models1. As a result,
*Corresponding author. 1https://www.st.com/content/st_com/en/about/ media-center/press-item.html/t4210.html
Figure 1. Monocular Dense SLAM with Our Multi-Modal Im-plicit Representation. We present a novel SLAM system based on implicit scene representation. The system does not require high-precision and high-resolution depth sensors and only takes
RGB images and the signals of light-weight ToF sensors as input. it would be valuable if we could fully utilize these light-weight sensors for dense SLAM, which further facilitates other applications like AR/VR and micro-Robot.
Unfortunately, limited by the compact electronic design, the light-weight ToF sensor can only provide coarse mea-surement in the form of depth distribution in an extremely low resolution as illustrated in Fig. 2. Existing RGB-D dense SLAM systems [38, 46, 31] are designed for accu-rate and pixel-wise depth inputs, thus cannot work with the light-weight ToF signals directly. They will also fail if we simply consider the light-weight ToF signals as a low-resolution depth (i.e., mean depth values in each zone).
In this paper, we aim to design a novel learning-based dense SLAM system that provides accurate pose tracking and dense reconstruction taking the RGB sequences from a color camera and the sparse signals of light-weight ToF as input (Fig. 1). However, it is non-trivial to design such
prediction. Specifically, when a new signal is captured, we render a zone-level light-weight ToF signal from our multi-modal scene representation with an initialized pose and fuse it with that new observation signal, which serves as the input of the depth prediction network. Such an explicit filtering technique improves the depth estimation performance sig-nificantly, particularly in extreme cases where the raw L5 signals are very noisy or contain large amounts of missing data, and therefore further benefits the whole SLAM sys-tem.
Our contributions can be summarized as follows. At first, to our best knowledge, we present the first dense
SLAM system by only taking the monocular images and the signals from a light-weight ToF sensor as input. More-over, we propose a multi-modal implicit scene representa-tion which supports rendering both the zone-level signals of light-weight ToF sensors and pixel-wise RGB/depth im-ages. By minimizing the re-rendering loss of these signals in a coarse-to-fine strategy, we can recover the camera pose and the scene geometry via differentiable neural rendering.
Furthermore, we propose a temporal filtering technique to enhance the signals of light-weight ToF sensors and corre-sponding depth prediction which significantly improve the proposed SLAM system in extreme cases. Experiments on the real datasets demonstrate that the proposed system well exploits the signals of light-weight ToF sensors and achieves competitive results both on camera tracking and dense scene reconstruction compared to existing methods. 2.