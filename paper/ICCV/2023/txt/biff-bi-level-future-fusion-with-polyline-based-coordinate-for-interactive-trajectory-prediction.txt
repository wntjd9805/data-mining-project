Abstract
Predicting future trajectories of surrounding agents is essential for safety-critical autonomous driving. Most ex-isting work focuses on predicting marginal trajectories for each agent independently. However, it has rarely been ex-plored in predicting joint trajectories for interactive agents.
In this work, we propose Bi-level Future Fusion (BiFF) to explicitly capture future interactions between interac-tive agents. Concretely, BiFF fuses the high-level future intentions followed by low-level future behaviors. Then the polyline-based coordinate is speciﬁcally designed for multi-agent prediction to ensure data efﬁciency, frame ro-bustness, and prediction accuracy. Experiments show that
BiFF achieves state-of-the-art performance on the interac-tive prediction benchmark of Waymo Open Motion Dataset. 1.

Introduction
Motion prediction is crucial for intelligent driving sys-tems as it plays a vital role in enabling autonomous vehicles to comprehend driving scenes and make safe plans in inter-active environments. Predicting the future behaviors of dy-namic agents is challenging due to the inherent multi-modal behavior of trafﬁc participants and complex scene environ-ments. Learning-based approaches [1, 3, 4, 7, 8, 12, 50, 9] have recently made notable strides in this area. By utiliz-ing large-scale real-world driving datasets [2, 11], learning-based frameworks can effectively model complex interac-tions among agents and considerably enhance the accuracy of motion prediction.
Most existing studies on trajectory prediction [4, 12, 17, 22, 25] tend to focus on generating marginal prediction samples of future trajectories for each agent, without con-sidering their future interactions. Thus, such marginal pre-diction models may generate trajectories with a high over-lap rate [41], then the unreliable results are provided for the downstream planning module. To overcome this limi-tation, recent works [14, 16, 31] propose joint motion pre-Figure 1. A motivating example of BiFF. Left: Marginal heatmaps of two interactive agents conﬂict with each other. Middle: For each scene modality, assignment scores generated by the predic-tion header become decoupled by fusing high-level future inten-tions across agents. Right: For each scene modality, the predicted trajectories are more scene-consistent by fusing low-level future behaviors across agents. diction models that generate scene-compliant trajectories.
However, previous models focus on capturing interaction across agents in the tracking history, explicitly modeling fu-ture interaction remains an open problem. The preliminary research regarding future interaction is conducted in the re-cent two works. M2I [41] classiﬁes interactive agents as inﬂuencer and reactor. Conditioning on the inﬂuencer, the reactor beneﬁt from future information, but the inﬂuencer is not optimized. MTR [38] predicts future trajectories for all the surrounding agents, but the future information is not reliable enough since trajectories are directly regressed us-ing polyline features without any iterative reﬁnement. To model the future interactions, we propose Bi-level Future
Fusion (BiFF), and the motivating example is presented in
Fig. 1, in which the unrealistic conﬂicting prediction be-tween the marginal heatmaps is mitigated by the proposed
High-level Future Intentions Fusion (HFIF) and Low-level
Future Behaviors Fusion (LFBF).
Moreover, regarding these models [14, 16, 31], all target
agents are normalized to scene-centric coordinate, which sacriﬁces prediction accuracy and model generalization, therefore, requires heavy labor in data augmentation. One possible solution is to simply extract local features for agent-centric representation [52], but it is not memory-efﬁcient with redundant context encoding between different target agents. To overcome these shortcomings, polyline-based coordinate is speciﬁcally designed to make all pre-dictions invariant to the global reference frame, and the fea-ture fusion between different coordinates is performed with relative positional encoding.
To summarize, our contributions remain as follows: (1) We propose a novel Bi-level Future Fusion (BiFF) model that incorporates a High-level Future Intention Fu-sion (HFIF) mechanism to generate scene-consistent goals and a Low-level Future Behavior Fusion (LFBF) mecha-nism to predict scene-compliant trajectories. (2) For multi-agent prediction, we design polyline-based coordinates to provide agent-centric representations for all target agents without redundant context encoding, which is memory-saving, data-efﬁcient, and robust to the variance of global (3) Our approach achieves state-of-the-reference frame. art performance on the interactive prediction benchmark of
Waymo Open Motion Dataset (WOMD). 2.