Abstract 1.

Introduction
Text-video retrieval is a fundamental task with high prac-tical value in multi-modal research. Inspired by the great success of pre-trained image-text models with large-scale data, such as CLIP, many methods are proposed to trans-fer the strong representation learning capability of CLIP to text-video retrieval. However, due to the modality differ-ence between videos and images, how to effectively adapt
CLIP to the video domain is still underexplored. In this pa-per, we investigate this problem from two aspects. First, we enhance the transferred image encoder of CLIP for fine-grained video understanding in a seamless fashion. Second, we conduct fine-grained contrast between videos and texts from both model improvement and loss design. Particularly, we propose a fine-grained contrastive model equipped with parallel isomeric attention and dynamic routing, namely
PIDRo, for text-video retrieval. The parallel isomeric at-tention module is used as the video encoder, which consists of two parallel branches modeling the spatial-temporal in-formation of videos from both patch and frame levels. The dynamic routing module is constructed to enhance the text encoder of CLIP, generating informative word representa-tions by distributing the fine-grained information to the re-lated word tokens within a sentence. Such model design provides us with informative patch, frame and word repre-sentations. We then conduct token-wise interaction upon them. With the enhanced encoders and the token-wise loss, we are able to achieve finer-grained text-video alignment and more accurate retrieval. PIDRo obtains state-of-the-art performance over various text-video retrieval benchmarks, including MSR-VTT, MSVD, LSMDC, DiDeMo and Activi-tyNet.
*This work was done during an internship at Huawei.
†Corresponding authors: Renjing Pei, Edmund Y. Lam
The amount of videos on the Internet has significantly increased recently. Efficiently finding target videos based on text description, referred to as text-video retrieval, is of high practical and research value. Over the past few years, various methods have been proposed for this task [8, 14, 25, 26, 11, 12].
Recently, large-scale contrastive text-image pre-training has achieved great success in many multi-modal text-vision understanding tasks [34, 18, 17, 35]. One representative method is CLIP, which trains a text encoder and an image encoder with over 400 million image-text pairs. Inspired by such success, some works directly adapt the pre-trained text and image encoders to the video domain and achieve great improvements [29, 15, 16, 13]. However, simply using the models without considering the differences between images
In this and videos neglects the characteristics of videos.
In spite work, we study CLIP-based text-video retrieval. of CLIP’s remarkable performance for image classification, two major issues remain when using it in the video domain.
The first is how to enhance the transferred image encoder for video understanding in a seamless fashion. The second lies in how to conduct finer-grained contrast between text and video.
For the first issue, the image encoder (a transformer) of
CLIP conducts spatial attention within each frame and does not explore the cross-frame temporal relationship. Current methods mainly append a temporal transformer to the image encoder to learn the temporal information [15, 30]. How-ever, it only conducts temporal attention in the frame-to-frame fashion and lacks fine-grained temporal modeling.
Building a powerful video encoder is an important topic for video understanding. FROZEN employs divided space-time attention to learn the spatial-temporal information [4].
TS2-Net incorporates a token-shift module to enable patch-level cross-frame interaction [27]. However, these methods change the internal structure of the image encoder of text-image pre-training models and may corrupt the transferred knowledge.
As for the second issue, fine-grained interaction is ef-fective for better modality alignment. However, CLIP only conducts coarse-grained contrast between text and image with global features.
It lacks the capability of capturing finer-level information. To solve this problem, some meth-ods conduct fine-grained interaction over token represen-tations [41, 20]. However, the performance improvement is limited when the CLIP-based models utilize such fine-grained contrastive loss for text-video retrieval [30]. This is because these models are dominated by the encoders of
CLIP, which does not provide informative enough token representations, such as words and patches, for us to con-duct effective fine-grained cross-modal interaction. Design-ing a good loss function alone is not enough, and corre-spondingly enhancing the encoders is also necessary. How-ever, systematic fine-grained interaction from both model enhancement and loss design is rarely explored.
Based on the above analysis, we propose PIDRo, a
CLIP-based model equipped with a novel parallel isomeric attention module and dynamic routing, for fine-grained text-video retrieval. Specifically, we design a new architecture with two parallel branches for comprehensive video mod-eling. One branch learns frame representations with spa-tial attention first and temporal attention second. The other one encodes the video in reverse order to acquire patch rep-resentations. Each of the two branches consists of a spa-tial transformer and a temporal transformer, which are ar-ranged in different orders in the two branches to have dif-ferent functions, working like isomers. Besides, we pro-pose a dynamic routing module appended to the text en-coder to enhance the word representations. Concretely, it is designed to dig out fine-grained information embedded in the global feature of the sentence and distribute it to the corresponding word tokens. These newly designed multi-modal encoders provide us with informative representations of words, patches and frames, which allows conducting ef-fective fine-grained video-text contrast. Meanwhile, we do not change the internal architectures of CLIP’s encoders, keeping CLIP’s extendability during transfer. We then de-sign a contrastive loss to conduct fine-grained video-text contrast on the learned representations (i.e., word-frame and word-patch), which calculates token-wise similarity scores between a text and a video.
By addressing the above two issues, we are able to smoothly transfer the text-image pre-training model, CLIP, into text-video retrieval. We conduct comprehensive ex-periments on several text-video retrieval benchmarks. Our
PIDRo achieves state-of-the-art performance on all bench-marks and sets new record of retrieval accuracy. The main contributions of PIDRo are summarized as follows: 1. We propose a parallel isomeric attention module for better video understanding. It models the temporal depen-dencies of videos in both frame and patch levels and does not undermine the original structure of the text-image pre-trained model. 2. We design a dynamic routing module to yield in-formative representations for word tokens.
It distributes the fine-grained information related to different words but buried in the global feature to the corresponding words. 3. Our work leads to a new scheme of conducting ef-fective fine-grained cross-modal interaction for CLIP-based methods via both model enhancement and loss design. 4. Extensive experiments on five widely used text-video retrieval benchmarks demonstrates the superiority of our method. Ablation studies also illustrate the effectiveness of our video and text encoders and fine-grained contrast. 2.