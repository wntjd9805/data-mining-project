Abstract
Inverse path tracing has recently been applied to joint material and lighting estimation, given geometry and multi-view HDR observations of an indoor scene. However, it has two major limitations: path tracing is expensive to com-pute, and ambiguities exist between reflection and emission.
Our Factorized Inverse Path Tracing (FIPT) addresses these challenges by using a factored light transport formulation and finds emitters driven by rendering errors. Our algo-rithm enables accurate material and lighting optimization faster than previous work, and is more effective at resolv-ing ambiguities. The exhaustive experiments on synthetic scenes show that our method (1) outperforms state-of-the-art indoor inverse rendering and relighting methods partic-ularly in the presence of complex illumination effects; (2) speeds up inverse path tracing optimization to less than an hour. We further demonstrate robustness to noisy inputs through material and lighting estimates that allow plausible relighting in a real scene. The source code is available at: https://github.com/lwwu2/fipt 1.

Introduction
We address the task of estimating the materials and light-ing of an indoor scene based on image observations (Fig. 1).
Recent work has shown that optimizing per-scene material and emission profiles through photometric loss and a dif-ferentiable renderer, with geometry reconstructed with the existing 3D reconstruction algorithms [35, 26, 47], can lead to promising results [1, 29, 45]. However, key challenges remain unsolved in these methods: (1) they require expen-sive Monte Carlo estimation for both the loss and derivative evaluations; (2) inherent ambiguity exists between material and lighting, and this ill-posed inverse problem hinders the optimization. We present an alternative inverse rendering algorithm that outperforms the state-of-the-art in terms of both efficiency and accuracy.
*Equal contribution
Figure 1: Ours vs standard IPT. IPT [1] takes a piecewise constant parameterization of material to reduce Monte Carlo variance and ambiguity for inverse rendering, losing fine spatial details as a result. Directly extending it to complex material representation (e.g. MILO [45]) shows very slow convergence. In contrast, we propose Factorized Inverse Path
Tracing (FIPT) to get rid of variance and reduce ambiguities, yielding efficient and high quality BRDF and emission (4th row), appealing relighting (1st row), and object insertion (the bunny on the table). The presented scene is synthetic with the inset showing the input (lower-left sub-figure). We further showcase results on real scenes in Fig. 10 and 11.
Optimizing scene parameters with Monte Carlo differen-tiable rendering can suffer from high variance and lead to slow convergence. Inspired by classical irradiance caching literature [43], our key idea to address this challenge is to factorize the material term out of the rendering integral and
bake the incoming radiance to significantly speed up in-verse rendering. Unlike prior work which also applies a similar factorization (e.g. [32, 22]) but does not consider view-dependent reflections, our method extends to general specular materials and both local and global illumination.
To address the ill-posed nature of joint optimization of material and lighting, we observe that by taking out the emission term in the rendering equation for the first bounce, only emissive surfaces will have high rendering loss. This observation allows us to design an effective way to detect emitters. We incorporate our emitter detection method into a full inverse rendering pipeline and independently estimate the emission after emitter detection.
Overall, our method achieves fast convergence over the material-lighting estimation task thanks to our factorized light transport formulation and emitter extraction strategy (Fig. 1). To demonstrate accurate BRDF-emission compari-son, we perform exhaustive experiments on synthetic scenes (Sec. 5.1, 5.2) while also validating on noisy data of captured real scenes (Sec. 5.3). The results show our method is able to obtain high-quality reconstruction for complicated indoor scenes that can easily fail for the state-of-the-art (Tab. 2), yet the training speed is 4-10 times faster (Tab. 3). 2.