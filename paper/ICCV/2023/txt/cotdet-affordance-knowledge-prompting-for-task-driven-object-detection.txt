Abstract
Task driven object detection aims to detect object in-stances suitable for affording a task in an image. Its chal-lenge lies in object categories available for the task being too diverse to be limited to a closed set of object vocabu-lary for traditional object detection. Simply mapping cat-egories and visual features of common objects to the task
In this paper, we propose cannot address the challenge. to explore fundamental affordances rather than object cat-egories, i.e., common attributes that enable different ob-jects to accomplish the same task. Moreover, we propose a novel multi-level chain-of-thought prompting (MLCoT) to extract the affordance knowledge from large language mod-els, which contains multi-level reasoning steps from task to object examples to essential visual attributes with ratio-nales. Furthermore, to fully exploit knowledge to benefit ob-ject recognition and localization, we propose a knowledge-conditional detection framework, namely CoTDet. It con-ditions the detector from the knowledge to generate object queries and regress boxes. Experimental results demon-strate that our CoTDet outperforms state-of-the-art meth-ods consistently and significantly (+15.6 box AP and +14.8 mask AP) and can generate rationales for why objects are detected to afford the task. 1.

Introduction
The traditional object detection task [3, 12, 37, 65], as shown in Figure 1a, aims to detect object instances of given categories in an image, so objects of categories such as the cup, bottle, cake, and knife are detected. In contrast, de-tection tasks in real-world applications [1, 4], such as in-telligent service robots, usually appear in the form of tasks rather than object categories [38]. For example, when an intelligent robot is asked to complete the task of “opening parcels”, the robot needs to autonomously locate the tool shown in Figure 1b, i.e., a knife. So the core of this type of task is to detect the object instances in the image that are
*Both authors contributed equally.
†Corresponding author.
Figure 1. An example of (a) traditional object detection, (b)-(e) task driven object detection, and (f) our multi-level chain-of-thought (MLCoT) prompting large language models (LLMs) to generate visual affordance knowledge. most suitable for the task [19, 42]. However, the challenges for task driven object detection are multi-fold. Previous methods that directly learn the mapping between objects and tasks from objects’ visual context features or categories cannot achieve satisfactory results. As shown in Figure 1c, the context-based approach GGNN [42] wrongly chooses vegetable stem for afford task of “opening parcels” be-cause it learns that visually slender objects can open parcels.
Similarly, the category-based approach TOIST [19] consid-ers that no object in the image can perform the “opening parcels” since it could not even recognize an instance of the knife in the image. In contrast, people will intelligently and naturally choose to use the knife to open parcels in the scene of Figure 1b.
Recently, Large Language Models (LLMs) like GPT-3 [2] and ChatGPT [31] have demonstrated impressive ca-pabilities in encoding general world knowledge from a vast amount of text data [32, 40, 52]. A naive approach is to prompt LLMs directly to return “what objects should we use to open parcels” and leverage the returned object cate-gories to simplify task driven object detection to the tradi-tional one. However, we observe that LLMs usually only
return a few categories of commonly used objects, such as the knife, pen, paper cutter, and scissors shown in Figure 1f.
According to these categories, although the knife in Fig-ure 1b can be identified as the target object, the detection system will miss other objects that also can be used to open parcels, such as the fork in Figure 1d and the temperature probe next to the microwave oven in Figure 1e. In turn, we ask why people can easily lock the fork and temperature probe in Figure 1d and 1e as the target objects? We argue that the reason is that people are not restricted to using spe-cific categories of objects to accomplish a task but instead select objects based on the commonsense knowledge that objects with “a handle and sharp blade” or “a handle and sharp tip” can “open parcels”.
In this paper, we propose to explicitly acquire visual af-fordance knowledge of the task (i.e., common visual at-tributes that enable different objects to afford the task) and utilize the knowledge to bridge the task and objects.
Figure 1f shows two sets of visual affordance knowledge (marked inside the yellow box) for opening parcels. How-ever, it is not trivial to acquire such task-specific visual affordance knowledge. Furthermore, we propose a novel multi-level chain-of-thought prompting (MLCoT) to elicit visual affordance reasoning from LLMs. At the first level (object level), we prompt LLMs to return common objects by the above-mentioned approach. Unlike before, which treats the returned object categories as target categories, we instead treat this query progress as brainstorming to obtain representative object examples. At the second level (affor-dance level), we generate rationales from LLMs for why ob-ject examples can afford the task and cooperate rationales to facilitate LLMs to reason and summarize the visual af-fordances beyond object examples. As shown in Figure 1f, the rationale and visual affordances that enable the knife to open parcels are “easily cut through paper and plastic...” and “a sharp blade and handle”, respectively. Our MLCoT can capture the essence of visual affordances behind these object examples without being limited to object categories.
Thus we can successfully detect the fork and temperature probe in Figure 1d and 1e as they meet the visual affor-dances required by the task.
Moreover, we claim that visual affordance knowledge not only helps recognize and identify objects suitable for the task but also helps localize objects more precisely be-cause visual attributes such as color and shape are use-ful in object localization. Therefore, unlike some meth-ods [9, 30, 44, 46, 53] to take knowledge as complemen-tary to the image’s visual features, we condition the detector on the visual affordance knowledge to perform knowledge-conditional object detection. Specifically, we follow [19] to use an end-to-end query-based detection framework [3, 65].
But instead of randomly initializing queries, we gener-ate knowledge-aware queries based on image features and
In addition to generating visual affordance knowledge. queries, we use visual affordance knowledge to guide the bounding box regression explicitly, inspired by the denois-ing training [18]. Unlike [18] introduces denoising for accelerating training, our knowledge-conditional denoising training aims to teach the decoder how to utilize visual knowledge to regress the boxes for queries.
Finally, we propose the CoTDet network, which acquires visual affordance knowledge from LLMs via the proposed
MLCoT and performs knowledge-conditional object detec-tion to effectively utilize the knowledge. Moreover, our
CoTDet can easily be extended to task driven instance seg-mentation by employing a segmentation head [5, 15].
In summary, our main contributions are:
• We are the first to propose to explicitly acquire vi-sual affordance knowledge and utilize the knowledge to bridge the task and object instances.
• We propose a novel multi-level CoT prompting (ML-CoT) to make abstract affordance knowledge con-creted, which leverages LLMs to generate and summa-rize intermediate reasoning steps from object examples to essential visual attributes with rationales.
• We claim that visual affordance knowledge can benefit both object recognition and localization and propose a knowledge-conditional detection framework to condi-tion the detector to generate object queries and guide box regression through denoising training.
• Our CoTDet not only consistently outperforms state-of-the-art methods (+15.6 AP50box and +14.8
AP50mask) by a large margin but also can generate ra-tionales for why objects are detected to afford tasks. 2.