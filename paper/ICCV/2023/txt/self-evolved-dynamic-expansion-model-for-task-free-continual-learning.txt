Abstract
Task-Free Continual Learning (TFCL) aims to learn new concepts from a stream of data without any task informa-tion. The Dynamic Expansion Model (DEM) has shown promising results in TFCL by dynamically expanding the model’s capacity to deal with shifts in the data distribution.
However, existing approaches only consider the recognition of the input shift as the expansion signal and ignore the cor-relation between the newly incoming data and previously learned knowledge, resulting in adding and training unnec-essary parameters. In this paper, we propose a novel and effective framework for TFCL, which dynamically expands the architecture of a DEM model through a self-assessment mechanism evaluating the diversity of knowledge among ex-isting experts as expansion signals. This mechanism en-sures learning additional underlying data distributions with a compact model structure. A novelty-aware sample se-lection approach is proposed to manage the memory buffer that forces the newly added expert to learn novel informa-tion from a data stream, which further promotes the diver-sity among experts. Moreover, we also propose to reuse previously learned representation information for learning new incoming data by using knowledge transfer in TFCL, which has not been explored before. The DEM expan-sion and training are regularized through a gradient up-dating mechanism to gradually explore the positive forward transfer, further improving the performance. Empirical re-sults on TFCL benchmarks show that the proposed frame-work outperforms the state-of-the-art while using a rea-sonable number of parameters. The code is available at https://github.com/dtuzi123/SEDEM/. 1.

Introduction
An ideal artificial intelligence system should be able to constantly learn and acquire new concepts from a changing environment all the time. Such a capability, which is in-creasingly emerging as a hot topic in AI, is referred to as continual/lifelong learning. However, most modern Deep
Learning models fail to achieve the goal of continual learn-Figure 1. The diversity evaluation between experts in the Self-Evolved Dynamic Expansion Model (SEDEM). We draw all sam-ples from the memory buffer as inputs for each expert. We com-pare the outputs {y1, · · · , yk} between all previously learnt ex-perts and the currently k-th updated expert, and use Eq. (1) to check the model expansion. ing (CL) since they rewrite previously learnt parameters to fit new tasks and then suffer from a significant drop in per-formance on the past tasks. Such a phenomenon is called catastrophic forgetting [37, 39].
Current work on reducing forgetting in continual learn-ing (CL) falls into three categories: Memory/experience re-play [6], regularisation-based approaches [29], and dynamic network architectures [46]. A simple and efficient approach among these methods is to maintain a fixed-capacity mem-ory buffer with training examples, which replays past ex-amples to the model along with learning new tasks. Reg-ularisation approaches can be used on memory buffers to further improve the performance in continual learning [53].
In addition to the memory-based methods, the dynamic ex-pansion architecture approach increases the capacity of the model as it learns new tasks, providing better generalisation performance [16].
Although previous work in CL has shown promising re-sults, most approaches assume that the task’s identity is known during training. Nevertheless, such a learning sce-nario is rarely encountered in the real world. In this work, we study a more challenging CL scenario called Task-Free
Continual Learning (TFCL) [4], where a model is trained on a data stream without accessing the task information at any point in time. Current memory-based approaches can be
extended for TFCL by developing an efficient sample selec-tion strategy [3] that selectively stores samples and replays them at each training time. However, these approaches would suffer from the interference between the probabilis-tic representations of old and the newly seen samples [32].
This issue can be solved by using the dynamic expansion model (DEM) [33, 43, 67] which increases the model’s ca-pacity to handle incoming samples while freezing previ-ously learnt experts to preserve prior knowledge. The main challenge for DEMs is that of learning a compact model structure without sacrificing much performance. Learning a lightweight DEM in TFCL can have two main advan-tages, such as scalability, learning infinite data streams, and fast inference at the testing phase. However, existing DEM methods fail to achieve this goal since they do not con-sider the knowledge diversity among experts when expand-ing their architecture, resulting in experts learning redun-dant information.
In this paper, we address two core issues in TFCL, yet untouched before.
First, instead of previous methods directly detecting the out-lier samples as expansion signals, we solve the trade-off between model size and performance by formulating the expansion process for a DEM as the knowledge diversity evaluation in the mixture system. Specifically, we evaluate the diversity among the mixture’s experts through a self-assessment mechanism. This allows us to control easily the growth of the model’s complexity. In addition, maintaining the diversity among experts can allow us to model more un-derlying data distributions with a compact structure. We call our mixture system the Self-Evolved Dynamic Expansion
Model (SEDEM) since it evaluates the diversity of the sys-tem, as shown in Fig. 1, where we assume to have already learnt k experts (classifiers) (‘Expert 1’,. . . , ‘Expert k’). We draw all samples from a memory buffer as inputs for each expert, and then we compare the outputs {y1, . . . , yk} be-tween all previously learnt experts (‘Expert 1’ ,. . . , ‘Expert k-1’) and the currently updated expert (‘Expert k’) as the di-versity score. Expansion signals are provided if and only if this diversity score is above a certain threshold controlling the model’s complexity.
Second, as most current works do not explore the benefit from the knowledge transfer in TFCL, we propose incor-porating feature representations extracted from all previ-ously learned experts into a currently updated expert. We propose the Dynamic Expansible Knowledge Mask Mech-anism (DEKMM), which generates soft masks to regu-late these representations when learning incoming samples.
DEKMM continuously updates mask values to progres-sively explore potential knowledge transfer through a gra-dient optimisation mechanism. The DEKMM has several advantages : (1) It does not require the task information for knowledge transfer; (2) It can find the optimal mask values maximising the benefits from the positive knowledge trans-fer; (3) It can dynamically create new mask parameters to adapt to the expansion of SEDEM without forgetting.
Moreover, a novelty-aware sample selection approach is proposed to selectively store those training samples that are sufficiently different from the knowledge preserved by all previously learnt experts. Such a selective approach encour-ages the current expert to learn novel information, further promoting the diversity among experts and improving the performance of SEDEM.
We perform a series of experiments demonstrating that the proposed methodology outperforms the state-of-the-art under all settings while employing fewer experts than other
DEM methods, which is consistent with our theoretical re-sults. We summarise our contributions as follows :
• We propose a new model for TFCL, namely the Self-Evolved Dynamic Expansion Model (SEDEM) which evaluates the diversity among experts as the expansion signals, inducing a diverse and compact mixture system.
• We propose a novelty-aware sample selection approach which allows the current expert to learn novel samples, further promoting the diversity among experts.
• We propose the Dynamic Expansible Knowledge Mask
Mechanism (DEKMM) to regulate previously learnt rep-resentation information when learning incoming data in
TFCL, maximising the positive knowledge transfer.
• We provide theoretical guarantees for the proposed SE-DEM, which are consistent with the empirical results.
• The proposed model achieves state-of-the-art perfor-mance in standard TFCL benchmarks. 2.