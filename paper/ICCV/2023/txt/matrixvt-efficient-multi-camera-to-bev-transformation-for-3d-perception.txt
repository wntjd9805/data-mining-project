Abstract
This paper proposes an efficient multi-camera to Bird’s-Eye-View (BEV) view transformation method for 3D per-ception, dubbed MatrixVT. Existing view transformers ei-ther suffer from poor efficiency or rely on device-specific operators, hindering the broad application of BEV mod-els. In contrast, our method generates BEV features effi-ciently with only convolutions and matrix multiplications (MatMul). Specifically, we propose describing the BEV fea-ture as the MatMul of image feature and a sparse Feature
Transporting Matrix (FTM). A Prime Extraction module is then introduced to compress the dimension of image fea-tures and reduce FTM’s sparsity. Moreover, we propose the
Ring & Ray Decomposition to replace the FTM with two matrices and reformulate our pipeline to reduce calcula-tion further. Compared to existing methods, MatrixVT en-joys a faster speed and less memory footprint while remain-ing deploy-friendly. Extensive experiments on nuScenes and
Waymo benchmarks demonstrate that our method is highly efficient but obtains results on par with the SOTA method in object detection and map segmentation tasks. 1.

Introduction
Vision-centric 3D perception in Bird’s-Eye-View (BEV) [17, 20, 24] has recently drawn extensive attention.
Apart from their outstanding performance, the compact and unified feature representation in BEV facilitates straight-forward feature fusions [1, 5, 12, 10], and enables various downstream tasks (e.g. object detection [6, 9, 10], map segmentation [17, 30], motion planning, etc.) to be applied thereon easily.
View Transformation (VT) is the key component that converts multi-camera features to BEV, which has been heavily studied in previous works [15, 20, 17, 24, 10, 6, 21, 9]. Existing VT methods can be categorized into geometry-based [13, 19, 17, 9, 18, 30] and learning-based meth-*Corresponding author. This research was supported by National Key
R&D Program of China (No. 2017YFA0700800) and Beijing Academy of
Artificial Intelligence (BAAI).
Figure 1. The pipeline of Lift-Splat (upper) and our proposed Ma-trixVT (lower). We compress image features before VT to reduce memory footprint and calculation. Note that only standard opera-tors are adopted in MatrixVT. ods [10, 20, 15]. Among these two categories, geometry-based methods show superior performance due to the use of geometric constraints. Lift-Splat [17], as a representa-tive geometry-based VT, predicts categorical depth distri-bution for each pixel and “lift” the corresponding features into 3D space according to the predicted depth. These fea-ture vectors are then gathered into pre-defined grids on a reference BEV plane (i.e., “splat”) to form the BEV feature (Fig. 1, upper). The Lift-Splat-based VT has shown great potential to produce high-quality BEV features, achieving remarkable performance on object detection [9, 5] and map segmentation tasks.
Despite the effectiveness of Lift-Splat-like VT [9], two issues remain. First, the “splat” operation is not universally feasible. Existing implementations of “splat” relies on ei-ther the “cumsum trick” [17, 6] that is highly inefficient, or customized operators [9] that can only be used on spe-cific devices, making application of BEV perception meth-ods costly. Second, the size of “lifted” multi-view image features is huge, becoming the memory bottleneck of BEV models. These two issues lead to a heavy burden on BEV methods during both the training and inference phases. As
a result, the drawbacks of existing view transformers limit the broad application of autonomous driving technology
In this work, we propose a novel VT method to address the above problems. MatrixVT is proposed based on the fact that the VT can be viewed as a feature transportation process. In that case, the BEV feature can be viewed as the
MatMul between the “lifted” feature and a transporting ma-trix, namely Feature Transporting Matrix (FTM). We thus generalize the Lift-Splat VT into a simple form and elimi-nate specialized operators.
However, transformation with FTM is a kind of degrada-tion — the mapping between the 3D space and BEV grids is extreme sparse, leading to the huge size of FTM and poor efficiency. Prior works [9, 18] seek customized operators, successfully avoiding such sparsity. In this paper, we argue that there are other solutions to the problem of sparse map-ping. First, we propose Prime Extraction. Motivated by an observation that the height (vertical) dimension of images is less informative in autonomous driving (see Sec. 3.2), we compress the image features along this dimension before
VT. Second, we adopt matrix decomposition to reduce the sparsity of FTM. The proposed Ring & Ray Decomposi-tion orthogonally decomposes the FTM into two separate matrices, each encoding the distance and direction of the ego-centric polar coordinate. This decomposition also al-lows us to reformulate our pipeline into a mathematically equivalent but more efficient one (Fig. 1, lower). These two techniques reduce memory footprint and calculation during
VT by hundreds of times, enabling MatrixVT to be more efficient than existing methods.
The proposed MatrixVT inherits the advantages of the
Lift-Splat [17] paradigm while being much more mem-ory efficient and fast. Extensive experimental results show that MatrixVT is 2-to-8 times faster than previous meth-ods [9, 6] and saves up to 97% memory footprint among different settings. Meanwhile, the perception model with
MatrixVT achieves 46.6% mAP and 56.2% NDS for object detection and 46.2% mIoU for vehicle segmentation on the nuScenes [2] val set, which is comparable to the state-of-the-art performance [9]. We conclude our main contribu-tions as follows:
• We propose a new description of multi-camera to BEV transformation — using the Feature Transportation
Matrix (FTM), which is a more general representation.
• To solve the sparse mapping problem, we propose
Prime Extraction and the Ring & Ray Decomposition, boosting VT with FTM by a huge margin.
• Extensive experiments demonstrate that MatrixVT yields comparable performance to the state-of-the-art method on the nuScenes object detection and map seg-mentation tasks while being more efficient and gener-ally applicable. 2.