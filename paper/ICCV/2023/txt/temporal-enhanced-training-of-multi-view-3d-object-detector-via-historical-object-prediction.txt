Abstract
In this paper, we propose a new paradigm, named His-torical Object Prediction (HoP) for multi-view 3D detec-tion to leverage temporal information more effectively. The
HoP approach is straightforward: given the current times-tamp t, we generate a pseudo Bird’s-Eye View (BEV) fea-ture of timestamp t-k from its adjacent frames and utilize this feature to predict the object set at timestamp t-k. Our approach is motivated by the observation that enforcing the detector to capture both the spatial location and tem-poral motion of objects occurring at historical timestamps can lead to more accurate BEV feature learning. First, we elaborately design short-term and long-term temporal decoders, which can generate the pseudo BEV feature for timestamp t-k without the involvement of its corresponding camera images. Second, an additional object decoder is flexibly attached to predict the object targets using the gen-erated pseudo BEV feature. Note that we only perform HoP during training, thus the proposed method does not intro-duce extra overheads during inference. As a plug-and-play approach, HoP can be easily incorporated into state-of-the-art BEV detection frameworks, including BEVFormer and BEVDet series. Furthermore, the auxiliary HoP ap-proach is complementary to prevalent temporal modeling methods, leading to significant performance gains. Exten-sive experiments are conducted to evaluate the effective-ness of the proposed HoP on the nuScenes dataset. We choose the representative methods, including BEVFormer and BEVDet4D-Depth to evaluate our method. Surpris-ingly, HoP achieves 68.5% NDS and 62.4% mAP with ViT-L on nuScenes test, outperforming all the 3D object detec-tors on the leaderboard. Codes are available at https:
//github.com/Sense-X/HoP.
*Equal contribution. Work done during internship at SenseTime Re-search. †Corresponding authors.
Figure 1: Incorporating HoP into 3D object detector with other temporal fusion methods [8, 7, 32, 12, 24].
Our HoP is plug-and-play and complementary to them. 1.

Introduction
Camera-only 3D detection from multi-view images is a challenging task for autonomous driving and has received increasing attention recently.
Introducing the Bird’s-Eye
View (BEV) representation [25, 14, 8, 30, 18, 2] with the temporal features aggregation has become the supe-rior design manner for camera-only 3D perception with multi-camera images. Based on this, a series of detec-tors [7, 15, 12, 24, 31, 19, 17] delve into the elaborated tem-poral information fusion methods and achieve significant breakthroughs. They consider the regions in 3D space and aggregate the image features corresponding to these hypoth-esis locations from multiple timestamps. These outstanding advances reveal the great potential of temporal information in camera-only 3D detection.
Beyond these temporal fusion mechanisms, in this pa-per, we propose a new paradigm for leveraging temporal information to enhance the temporal modeling ability of
the 3D object detector. It’s a temporal-based auxiliary task only adopted during training stage, namely Historical Ob-ject Prediction (HoP). Our approach is motivated by the observation that enforcing the detector to capture both the spatial location and temporal motion of objects occurring at historical timestamps can lead to more accurate BEV fea-ture learning.
Specifically, given the current timestamp t, we generate a pseudo BEV feature of timestamp t-k from its adjacent frames and utilize this feature to predict the object set at t-k.
First, we elaborately design short-term and long-term tem-poral decoders, which can generate the pseudo BEV fea-ture for timestamp t-k without the involvement of its cor-responding camera images. Thanks to the marginal tempo-ral difference between two adjacent frames, the short-term temporal decoder process two adjacent frames of timestamp t-k to provide spatial semantics of objects. The long-term decoder captures long-term motion of the whole frame se-quence and contributes to better object motion estimation, which is complementary to spatial localization of the short-term branch. Subsequently, an additional object decoder is flexibly attached to predict the object targets using the generated pseudo BEV feature. Note that we only perform
HoP during training, thus the proposed method does not in-troduce extra overheads during inference. As a plug-and-play approach, HoP can be flexibly incorporated into the state-of-the-art BEV detection frameworks, including BEV-Former [15] and BEVDet series [8, 7, 14]. Furthermore, the auxiliary HoP approach is complementary to prevalent tem-poral modeling methods, leading to significant gains.
Extensive experiments are conducted to evaluate the effectiveness of the proposed HoP on the nuScenes dataset [1]. We choose the representative methods, includ-ing BEVFormer and BEVDet4D-Depth [7, 14] to evalu-ate our method. To be specific, we obtain 55.8% NDS with ResNet-101-DCN and 60.3% NDS with VoVNet-99 when evaluating HoP on nuScenes val. Surprisingly, HoP achieves 68.5% NDS and 62.4% mAP with ViT-L [4] on nuScenes test, surpassing all the 3D object detectors on the leaderboard by a large margin.
In conclusion, our contributions can be summarized as follows:
• We propose a novel temporal enhanced training scheme, namely Historical Object Prediction (HoP), to encourage more accurate BEV feature learning. HoP can force the model to capture the spatial semantics and temporal motion of objects in the historical frame during training.
• We design a temporal decoder that consists of a short-term decoder and a long-term decoder to provide re-liable spatial localization and accurate motion estima-tion of objects.
• We equip the competitive 3D object detector baselines with our approach and yield significant improvements on the nuScenes dataset. Surprisingly, HoP with ViT-L achieves 68.5% NDS and 62.4% mAP on nuScenes test, establishing the new state-of-the-art performance. 2.