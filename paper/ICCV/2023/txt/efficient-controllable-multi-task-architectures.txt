Abstract
We aim to train a multi-task model such that users can adjust the desired compute budget and relative importance of task performances after deployment, without retraining.
This enables optimizing performance for dynamically vary-ing user needs, without heavy computational overhead to train and save models for various scenarios. To this end, we propose a multi-task model consisting of a shared en-coder and task-specific decoders where both encoder and decoder channel widths are slimmable. Our key idea is to control the task importance by varying the capacities of task-specific decoders, while controlling the total computa-tional cost by jointly adjusting the encoder capacity. This improves overall accuracy by allowing a stronger encoder for a given budget, increases control over computational cost, and delivers high-quality slimmed sub-architectures based on user’s constraints. Our training strategy involves a novel ‘Configuration-Invariant Knowledge Distillation’ loss that enforces backbone representations to be invariant under different runtime width configurations to enhance accuracy.
Further, we present a simple but effective search algorithm that translates user constraints to runtime width configu-rations of both the shared encoder and task decoders, for sampling the sub-architectures. The key rule for the search algorithm is to provide a larger computational budget to the higher preferred task decoder, while searching a shared encoder configuration that enhances the overall MTL perfor-mance. Various experiments on three multi-task benchmarks (PASCALContext, NYUDv2, and CIFAR100-MTL) with di-verse backbone architectures demonstrate the advantage of our approach. For example, our method shows a higher controllability by ∼ 33.5% in the NYUD-v2 dataset over prior methods, while incurring much less compute cost. 1.

Introduction
Multi-task learning (MTL) often aims to solve multiple related tasks together using a single neural network for econ-omy of deployment [1, 2]. Humans can handle multiple tasks with diverse trade-offs (e.g., due to availability of resources,
Figure 1. Problem Setup. We aim to provide users precise control on compute allocation as per their MTL performance preference, with the ability to change these dynamically without re-training. To accomplish this, we provide a strategy where a MTL SuperNet is trained only once but allows crafting SubNets that can be sampled based on the user’s MTL constraints (compute cost and task pref-erence) at test-time. “High” task preference for task i implies the performance for task i is more important than other tasks. adaptable reaction time, etc.), however, most existing MTL architectures are incapable of transforming themselves to handle multiple user constraints without being retrained for each scenario. In this paper, we address the problem of de-signing controllable dynamic convolutional neural networks (CNN) for MTL that can adjust jointly for two types of user requirements, task preference and compute budget.
Real-world MTL systems are seeing growing applica-tions ranging from autonomous cars [3] to video cameras for traffic analysis [4], with respective task performance preferences. For example, observe Fig. 1. A single MTL architecture can allow two users to use the same model but with custom task preferences based on the available compute cost. The user with higher compute (e.g. self-driving cars) may expect higher performance on task 1, but the user with lower compute (e.g. traffic cameras) would prefer higher performance on task 2 given the budget. It will be extremely inefficient to create and train MTL architectures for all such possible variations of user requirements due to expensive de-sign and deployment costs [5–7]. This brings forth the need for flexible MTL architectures that allow test-time trade-offs based on relative task importance and resource allocation.
Some prior methods have introduced dynamic MTL net-works [8–11] in an effort to incorporate changing user task
preferences at test-time. However, such methods do not ac-count for changing the user’s computational budget as they assume fixed computation cost resulting in limited applicabil-ity. Recently, Controllable Dynamic Multi-task Architecture (CDMA) [7] introduced a multi-stream (equal to number of tasks) architecture to handle both changing task preferences and compute budgets. For controllability, it adjusts branch-ing locations in the encoder and generates encoder weights using external hypernetworks [12] while fixing decoders.
In this paper, we propose a multi-task method called
‘Efficient Controllable Multi-Task architectures’ (ECMT) that consists of a shared encoder and task-specific de-coders where the channel widths of both modules are slimmable [13]. Our key idea is to control the task im-portance by varying the capacities of task-specific decoders while controlling the compute budget by jointly adjusting the encoder capacity. This is based on our observation that a larger backbone achieves overall higher multi-task accuracy (even with task conflicts) compared to separately trained multiple smaller backbones. Further, in contrast to adjusting the branching points of multiple encoder streams [7], our ap-proach can achieve overall higher controllability by adopting one stronger backbone for a given compute budget. Since decoder widths affect both accuracy and computational cost by a considerable amount, especially for dense prediction tasks, adjustment of decoder capacities is sufficient to con-trol the task preferences. Constraining to control the task preference only through the decoder capacities further avoids adversarial effects when changing the shared encoder as it may cause different effects to each task, which is hard to control. Finally, adjusting both decoder and encoder largely increases the control over the computational cost.
As the training is performed only once and the encoder is shared among tasks, ECMT optimizes the sub-architectures by distilling [14] the encoder knowledge of the parent archi-tecture, that is capable of handling task conflicts given its large capacity [15–17]. In particular, it uses a novel ‘Config-uration Invariant knowledge distillation’ (CI-KD) strategy to make the embeddings of the shared encoder invariant to the varying sub-architecture configuration. At test-time, ECMT uses the joint constraints and extracts a sub-architecture by searching for the most suitable encoder and decoder width configuration using the proposed evolution-based algorithm
[18] designed for MTL models. The key rule for the search algorithm is to provide a larger computational budget to the higher preferred task decoder, while searching a shared en-coder configuration that enhances overall MTL performance.
Interestingly, without any need for external hypernet-works (to predict large tensor weights of the parent architec-ture) and with a shared encoder (that allows task scalability),
ECMT demonstrates strong task preference - task accuracy
- efficiency trade-offs. Our extensive experiments on bench-mark datasets demonstrate strong MTL controllability across a wide range of joint preferences (e.g., an increase in Hy-pervolume [19] of ∼34% is observed when compared to state-of-the-art [7] during testing in the NYUD-v2 dataset
[20]). To summarize, our contributions in this paper are: 1. We present a new method to sample high-performing ef-ficient MTL sub-architectures from a single MTL SuperNet that can satisfy both user preferences of task performance and computational budget, dynamically without retraining. 2. Our method includes two key components:
• A training strategy to enhance the MTL performance of sub-architectures in order to have minimal performance drop even if user’s constraints become restricted. In particular, it uses a CI-KD loss to transfer the encoder knowledge of the parent model, which is capable of handling multi-task conflicts, to the encoders of sub-models.
• A subsequent search strategy that translates the task pref-erences to sample the task decoders for better performance and searches for shared encoder width configuration that supports the decoders for overall better MTL performance. 3. We show superior controllability on sampling sub-models compared to prior methods. For example, we show a higher controllability by ∼ 33.5% in the NYU-v2 [20] (3 tasks) dataset and ∼ 55% in Pascal-Context [21] (5 tasks) dataset over state-of-the-art method CDMA [7]. 2.