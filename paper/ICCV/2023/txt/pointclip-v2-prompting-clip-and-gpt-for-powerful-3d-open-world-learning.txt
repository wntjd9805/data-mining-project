Abstract
Large-scale pre-trained models have shown promising open-world performance for both vision and language tasks. However, their transferred capacity on 3D point clouds is still limited and only constrained to the classifica-tion task. In this paper, we first collaborate CLIP and GPT to be a unified 3D open-world learner, named as Point-CLIP V2, which fully unleashes their potential for zero-shot 3D classification, segmentation, and detection. To better align 3D data with the pre-trained language knowl-edge, PointCLIP V2 contains two key designs. For the vi-sual end, we prompt CLIP via a shape projection mod-ule to generate more realistic depth maps, narrowing the domain gap between projected point clouds with natural images. For the textual end, we prompt the GPT model to generate 3D-specific text as the input of CLIP’s tex-tual encoder. Without any training in 3D domains, our approach significantly surpasses PointCLIP by +42.90%,
+40.44%, and +28.75% accuracy on three datasets for zero-shot 3D classification. On top of that, V2 can be extended to few-shot 3D classification, zero-shot 3D part segmentation, and 3D object detection in a simple manner, demonstrat-ing our generalization ability for unified 3D open-world learning. Code is available at https://github.com/ yangyangyang127/PointCLIP_V2. 1.

Introduction
The advancement of spatial sensors has stimulated widespread attention in recent years for both academia and industry. To effectively understand point clouds, the major data form in 3D, many related tasks are put for-Figure 1. Zero-shot Performance of PointCLIP V2. On different 3D datasets, our approach achieves significant accuracy enhance-ment for zero-shot 3D classification over PointCLIP [62]. ward and gained great progress, including 3D classifica-tion [35, 51, 66], segmentation [36, 54, 48, 52], detec-tion [55, 29], and self-supervised learning [65, 17, 61, 11].
Importantly, for the complexity and diversity of open-world circumstances, the collected 3D data normally contains a large number of ‘unseen’ objects, namely, not ever defined and trained by the already deployed 3D systems. Given the human-laboring data annotations, how to recognize such 3D shapes of new categories has become a hot-spot issue, which still remains to be fully explored.
Recently, large-scale pre-trained vision and language models, e.g., CLIP [37] and GPT-3 [3], have obtained a strong capacity to process data in both modalities. How-ever, limited efforts have focused on their application in the point cloud, and existing work only explores the pos-sibility of CLIP on the 3D classification task, without con-Figure 2. Comparison of Visual Projection. PointCLIP V2 (Bot-tom) generates more realistic depth maps with denser point distri-bution and smoother depth values.
Figure 3. Comparison of Textual Input. We visualize the simi-larity score maps of the encoded textual and visual features, where
PointCLIP V2 (Bottom) shows better alignment. sidering other 3D open-world tasks. PointCLIP [62], for the first time, indicates that CLIP can be adapted for zero-shot point cloud classification without any 3D training. It projects the ‘unseen’ 3D point cloud sparsely into 2D depth maps, and leverages CLIP’s image-text alignment for depth map recognition. However, as a preliminary work, the per-formance of PointCLIP is far from satisfactory as shown in
Figure 1, which cannot be put into actual use. More im-portantly, PointCLIP only draws support from pre-trained
CLIP, without considering the powerful large-scale lan-guage model (LLM). Therefore, we ask the question: Can we properly unify CLIP and LLM to fully unleash their po-tentials for unified 3D open-world understanding?
We observe that PointCLIP mainly suffers from two fac-tors concerning the 2D-3D domain gap. (1) Sparse Pro-jection. PointCLIP simply projects 3D point clouds onto depth maps as sparsely distributed points with depth val-ues (Figure 2). Though simple, the scatter-style figures are dramatically different from the real-world pre-training im-ages for both appearances and semantics, which severely (2) Naive Text. Point-confuses CLIP’s visual encoder.
CLIP mostly inherits CLIP’s 2D text input, “a photo of a [CLASS].” and only appends simple 3D-related words, “a depth map”. As visualized in Figure 3, the textual features extracted by CLIP can hardly focus on the target object with high similarity scores. Such naive text cannot fully describe 3D shapes and harms the pre-trained language-image alignment.
In this paper, we integrate the advantage of CLIP and the GPT-3 [3] model and propose PointCLIP V2, a power-ful framework for unified 3D open-world understanding, in-cluding zero-shot/few-shot 3D classification, zero-shot part segmentation, and zero-shot 3D object detection. Without
‘seeing’ any 3D training data, V2 can project point clouds into realistic 2D figures and align them with 3D-aware text, which fully unleashes CLIP’s pre-trained knowledge in the 3D domain.
Firstly, we propose to Prompt CLIP with Realistic
Projection, which generates CLIP-preferred images from 3D point clouds. Specifically, we transform the irregu-lar point cloud into grid-based voxels and then apply non-parametric 3D local filtering on top. By this, the projected 3D shapes are composed of denser points with smoother depth values. As shown in Figure 2, our generated fig-ures are more visually similar to real-world images and can highly unleash the representation capacity of CLIP’s pre-trained visual encoder. Secondly, we Prompt GPT with 3D Command to generate text with rich 3D seman-tics as the input of CLIP’s textual encoder. By feeding heuristic 3D-oriented command into GPT-3, e.g., “Give a caption of a table depth map:”, we lever-age its language-generative knowledge to obtain a series of 3D-specific text, e.g., “A height map of a table with a top and several legs.”. A group of language commands is customized to prompt GPT-3 to pro-duce diverse text with 3D shape information. As shown in Figure 3, the textual features of PointCLIP V2 exert stronger matching properties to the projected maps, largely boosting CLIP’s image-text alignment for 3D point clouds.
With our prompting schemes, PointCLIP V2 exhibits su-perior performance for zero-shot 3D classification, surpass-ing PointCLIP by +42.90%, +40.44%, and +28.75% accu-racy, respectively on ModelNet10 [53], ModelNet40 [53], and ScanObjectNN [44] datasets. Further, our approach can be adapted for more no-trivial 3D open-world tasks by marginal modifications, such as a learnable 3D smoothing for 3D few-shot classification, a back-projection head for zero-shot segmentation, and a 3D region proposal network for zero-shot detection. This fully indicates the power of
V2 for general 3D open-world understanding.
Our contributions are summarized as follows:
• We propose PointCLIP V2, a powerful cross-modal learner unifying CLIP and GPT-3 to transfer the pre-trained vision-language knowledge into 3D domains.
• We introduce a realistic projection to prompt CLIP and 3D-oriented command to prompt GPT-3 to effectively mitigate the domain gap among 2D, 3D, and language.
Method
Latency ModelNet40
ScanObjectNN
Phong Shading [42]
Height Map [43]
Silhouette Map [43]
PointCLIP [62]
PointCLIP V2 107.2 87.7 87.9 11.3 16.7 57.30 54.73 48.40 42.53 64.22 29.33 26.25 20.91 26.37 35.36
Figure 4. Comparison of Open-world Settings. Existing meth-ods still depend on prerequisite 3D training to recognize the ‘un-seen’ point clouds. In contrast, we require no training in the 3D domain and directly conduct 3D open-world understanding.
Table 1. Comparison of Different Projection Methods. We re-port zero-shot classification results (%) on two datasets [53, 44], and compare the inference latency (ms) by projecting 10-view im-ages from an input point cloud.
• As the first work for unified 3D open-world learning, our PointCLIP V2 can be further extended for zero-shot part segmentation and 3D object detection. their effectiveness. As shown, our realistic projection ex-hibits faster inference speed than other approaches and at-tains higher zero-shot performance than PointCLIP. 2.