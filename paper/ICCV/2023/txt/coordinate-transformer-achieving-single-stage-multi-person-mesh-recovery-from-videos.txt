Abstract
Multi-person 3D mesh recovery from videos is a critical first step towards automatic perception of group behavior in virtual reality, physical therapy and beyond. However, existing approaches rely on multi-stage paradigms, where the person detection and tracking stages are performed in a multi-person setting, while temporal dynamics are only modeled for one person at a time. Consequently, their per-formance is severely limited by the lack of inter-person in-teractions in the spatial-temporal mesh recovery, as well as by detection and tracking defects. To address these chal-lenges, we propose the Coordinate transFormer (Coord-Former) that directly models multi-person spatial-temporal relations and simultaneously performs multi-mesh recov-ery in an end-to-end manner.
Instead of partitioning the feature map into coarse-scale patch-wise tokens, Coord-Former leverages a novel Coordinate-Aware Attention to preserve pixel-level spatial-temporal coordinate informa-tion. Additionally, we propose a simple, yet effective Body
Center Attention mechanism to fuse position information.
Extensive experiments on the 3DPW dataset demonstrate that CoordFormer significantly improves the state-of-the-art, outperforming the previously best results by 4.2%, 8.8% and 4.7% according to the MPJPE, PAMPJPE, and PVE metrics, respectively, while being 40% faster than recent video-based approaches. The released code can be found at https://github.com/Li-Hao-yuan/CoordFormer. 1.

Introduction
Considerable progress has been made on monocular 3D human pose and shape estimation from images [5, 35, 17, 22, 39] due to extensive efforts of computer graph-*Both authors contributed equally to this work as co-first authors.
†Corresponding author.
Figure 1. Comparison of video-based multi-person mesh recov-(a) Multi-stage pipelines [20, 8, 44, 38, 40] ex-ery pipelines. plicitly generate tracklets and model single-person temporal mesh sequences independently. (b) Our single-stage CoordFormer im-plicitly matches persons across frames and simultaneously models multi-person mesh sequences in an end-to-end manner. ics and augmented/virtual reality researchers. However, while frame-wise body mesh detection is feasible, many applications require direct video-based pipelines to avoid spatial-temporal incoherence and missing frame-based de-tections [20, 8, 38].
Existing video-based methods follow a multi-stage de-sign that involves using a 2D person detector and tracker to obtain the image sequences of a single-person for pose and shape estimation [18, 20, 38, 41, 40]. More specifi-cally, these methods first detect and crop image patches that contain persons, then track these individuals across frames, and associate each cropped image sequence with a per-son. The frame-level or sequence-level features are then extracted and used to regress 3D human mesh sequences under spatial and temporal constraints. However, the accu-racy of the detection and tracking stage greatly affects the performance of these multi-stage approaches, making them particularly sensitive to false, overlapping, and missing de-tections. Moreover, these multi-stage approaches have a considerable computation cost and lack real-time perspec-tives since the single-person meshes can only be recovered sequence-by-sequence after detection and tracking.
To address the above issues, we introduce CoordFormer, the first single-stage approach for multi-person 3D mesh recovery from videos that can be trained in an end-to-end manner. As shown in Fig. 1, our method differs from current state-of-the-art approaches [20, 8, 44, 38, 40] by being a single-stage pipeline that implicitly performs detection and tracking through the interaction of feature representations, producing multiple mesh sequences simultaneously.
In particular, CoordFormer leverages a multi-head framework to predict a body center heatmap, which is en-coded using our proposed Body Center Attention (BCA).
BCA serves as a weak/intermediate person detector that fo-cuses the framework-wide feature representations on po-tential body centers. Many-to-many temporal-spatial re-lations among people and across frames are then derived from the BCA-focused features and directly mapped to mesh sequences using our novel Coordinate-Aware Atten-tion (CAA). CAA is integrated into a Spatial-Temporal
Transformer (ST-Trans) [44, 26, 24] to capture non-local context relations at the pixel level. See Fig. 2 for an illus-tration of CAAs motivation. Facilitated by BCA and CAA,
CoordFormer advances existing video mesh recovery solu-tions beyond explicit detection, tracking and sequence mod-eling. Under various experimental settings on the 3DPW dataset, CoordFormer significantly outperforms the best re-sults of state-of-the-art by 4.2%, 8.8% and 4.7% on MPJPE,
PAMPJPE and PVE metrics, respectively. CoordFormer also improves inference speed by 40% compared to the state-of-the-art video-based approaches [20, 38]. More-over, we demonstrate that enhancing and capturing pixel-level coordinate information significantly benefits the per-formance under multi-person scenarios.
The main contributions of this work are as follows:
• We propose the first single-stage multi-person video mesh recovery approach, where our BCA mechanism fuses position information and our CAA module en-ables end-to-end multi-person model training.
• We demonstrate that the pixel-level coordinate corre-spondence is the most critical factor for performance.
• Extensive experiments on challenging 3D pose datasets demonstrate that the proposed method achieves significant improvements, outperforming the state-of-the-art methods. 2.