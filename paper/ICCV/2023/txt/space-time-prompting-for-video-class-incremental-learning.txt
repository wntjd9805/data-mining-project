Abstract
Recently, prompt-based learning has made impressive progress on image class-incremental learning, but it still lacks sufficient exploration in the video domain.
In this paper, we will fill this gap by learning multiple prompts based on a powerful image-language pre-trained model, i.e., CLIP, making it fit for video class-incremental learn-ing (VCIL). For this purpose, we present a space-time prompting approach (ST-Prompt) which contains two kinds of prompts, i.e., task-specific prompts and task-agnostic prompts. The task-specific prompts are to address the catastrophic forgetting problem by learning multi-grained prompts, i.e., spatial prompts, temporal prompts and com-prehensive prompts, for accurate task identification. The task-agnostic prompts maintain a globally-shared prompt pool, which can empower the pre-trained image models with temporal perception abilities by exchanging contexts between frames. By this means, ST-Prompt can transfer the plentiful knowledge in the image-language pre-trained models to the VCIL task with only a tiny set of prompts to be optimized. To evaluate ST-Prompt, we conduct extensive ex-periments on three standard benchmarks. The results show that ST-Prompt can significantly surpass the state-of-the-art VCIL methods, especially it gains 9.06% on HMDB51 dataset under the 1 × 25 stage setting. 1.

Introduction
Video understanding [49, 11, 1, 32] has achieved out-standing performance with the quick development of deep neural networks. However, training such a model heavily depends on the currently available labeled data. When dif-ferent classification tasks are presented sequentially in prac-*Corresponding author.
Figure 1. An comparison between existing VCIL methods based on exemplar or frame replay and our ST-Prompt. Compared to the rehearsal-based approaches, our ST-prompt can better solve the problem of catastrophic forgetting when only a small amount of memory is consumed to store a tiny set of prompts. tical applications, a crucial problem is to maintain the ac-curacy of the model as data arrives over time. Under this circumstance, due to the limitations of memory and pri-vacy, the model can just access the videos of the current task and few even no examples of historical tasks, which requires that the model should adapt to the current task without catastrophic forgetting [34]. The machine learn-ing paradigm to handle the above challenges is called Video
Class-Incremental Learning (VCIL) [35, 36, 61].
Most existing class-incremental learning approaches have achieved remarkable performance through a rehearsal buffer of the old tasks in both the image [40, 45, 59, 4] and video [36, 47, 36] fields. However, these methods suf-fer from suboptimal performance with smaller buffer sizes and severe catastrophic forgetting once the rehearsal buffer is not accessed, hence leading to limited applications. To solve this problem, several prompt-based learning methods in image domain [54, 53, 51] are proposed to train only
a tiny set of parameters, i.e., prompts, to effectively in-struct a pre-trained model to learn incremental tasks with-out buffering past examples. Typically, L2P [54] learns a single prompt pool to attach complementary prompts to the pre-trained backbone. Nevertheless, this new paradigm has shown great potential for images, but it is still underex-plored in the video domain. In this paper, we will concen-trate on exploring this paradigm on VCIL to bridge this gap.
Directly applying these existing methods for the VCIL task tends to be inadequate. The core reason is that the prompt strategy for images cannot model temporal variations and dynamics across spatial features. Meanwhile, video data possesses a more complicated semantic structure than static images, so a single prompt pool like L2P may be insuffi-cient to learn the abundant semantics. Therefore, we think a properly redesigned prompt framework will contribute to improving the performance of the VCIL task.
Motivated by the above observations, we propose ST-Prompt, a space-time prompting method for video class-incremental learning. Specifically, we design ST-Prompt with two components, i.e., task-specific prompts and task-agnostic prompts. The task-specific prompts are tapped by multi-grained prompts, i.e., spatial prompts, temporal prompts and comprehensive prompts, which can better dis-cover the complicated spatio-temporal semantics in videos to address the catastrophic forgetting problem. The task-agnostic prompts learn a complementary prompt for each frame and then gather them back for all frames; hence we can equip the image pre-trained model with temporal modeling capacities without any edition of model struc-ture. Compared with the task-specific prompts, the task-agnostic prompts are task-independent and thus share a generic and globally-shared prompt pool for all tasks. By this means, ST-Prompt has two advantages: 1) it can si-multaneously take advantage of the knowledge priors in the image-language pre-trained model and learn the spatio-temporal information, which makes the model more suit-able for VCIL; 2) it can markedly reduce the memory cost by just saving a set of prompt parameters without preserv-ing redundant videos anymore, as shown in Figure 1. In the experimental stage, we quantificationally and qualitatively evaluate ST-Prompt on three standard benchmarks. The results show that ST-Prompt can achieve significant gains over the current top-performing approaches, which demon-strates the effectiveness of our method.
In summary, the main contributions of this paper are as follows:
• We first explore the great potential of image-language pre-trained model tasks and achieve significant performance improvement by uti-lizing its sufficient semantic information through prompt-based methods; in video incremental
• We proposed ST-Prompt, containing task-specific prompts and task-agnostic prompts, which can sig-nificantly reduce the memory cost and forgetting rate while modeling temporal variations and dynamics across spatial features without modifying the model structure;
• The proposed ST-Prompt achieves remarkable perfor-mance on multiple standard action recognition incre-mental benchmarks over state-of-the-art methods. 2.