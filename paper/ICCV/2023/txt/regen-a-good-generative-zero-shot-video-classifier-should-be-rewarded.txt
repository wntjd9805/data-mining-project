Abstract
This paper sets out to solve the following problem: How can we turn a generative video captioning model into an open-world video/action classification model? Video cap-tioning models can naturally produce open-ended free-form descriptions of a given video which, however, might not be discriminative enough for video/action recognition. Unfor-tunately, when fine-tuned to auto-regress the class names directly, video captioning models overfit the base classes losing their open-world zero-shot capabilities. To alleviate base class overfitting, in this work, we propose to use rein-forcement learning to enforce the output of the video cap-tioning model to be more class-level discriminative. Specif-ically, we propose ReGen, a novel reinforcement learning based framework with a three-fold objective and reward functions: (1) a class-level discrimination reward that en-forces the generated caption to be correctly classified into the corresponding action class, (2) a CLIP reward that en-courages the generated caption to continue to be descriptive of the input video (i.e. video-specific), and (3) a grammar reward that preserves the grammatical correctness of the caption. We show that ReGen can train a model to produce captions that are: discriminative, video-specific and gram-matically correct. Importantly, when evaluated on standard benchmarks for zero- and few-shot action classification, Re-Gen significantly outperforms the previous state-of-the-art. 1.

Introduction
Open-world or zero-shot video recognition is concerned with the problem of recognizing at test time, unseen dur-ing training, video/action classes. For example, during training, an open-world video recognition model might be trained to classify dancing blues or dancing latin but, during test time, it might be required to perform classi-fication over new categories, not seen during training, like dancing tango. Another application of open world-recognition is when a model is trained on dataset A with class taxonomy CA and then it is applied directly (i.e. with-out re-training) on a different dataset B with a different (i.e. non- or partially-overlapping) class taxonomy CB.
A video captioning model uses (by construction) a gener-ative language model, conditioned on features produced by a video backbone, to generate a human-interpretable free-form textual description of the video which, in principle, can be associated to any video/action class. Hence, a cap-tioning model can be potentially used for open-world recog-nition. However, in practice, the generated captions are not discriminative enough for video/action recognition. In this paper, our goal is to turn a generative captioning model into a highly-accurate open-world video/action classifica-tion model which, at the same time, maintains its ability to generate video-specific grammatically correct captions.
To our knowledge, the only method that trains a genera-tive captioning model for open-world video/action recog-nition is the recently proposed REST [8]. Therein, the authors first showed that directly fine-tuning a captioning model to auto-regress the action class names results in base class overfitting and severely hurts zero-shot generalizabil-ity. While REST addresses, to some extent, this problem by utilizing an unsupervised adaptation framework, it com-pletely discards class information during training. As a re-sult, the trained model might still not be very discriminative for open-world video classification.
Our main goal in this work is to address this important limitation of [8] by enabling the integration of class infor-mation into the training of a generative video captioning model. To this end, we propose ReGen, a newly introduced training framework based on Reinforcement Learning (RL) and 3 appropriate rewards to train a Generative model so that its output caption satisfies 3 key requirements: (1) be class discriminative avoiding base overfitting, (2) maintain the video-specific granularity of the generated text, and (3) maintain the grammatical correctness of the generated text.
Different to video captioning, ReGen does not use captions to train the model, only class label information. To this end, in this paper, we make the following contributions:
• To avoid base class overfitting, we avoid training with a standard language modelling loss and, instead, in-troduce RL and CLS-R, a class discrimination reward computed from class names only, that enforces the gen-erated caption for a given video to be correctly classi-fied into the corresponding ground truth class.
• As the optimization of CLS-R alone results in a model
whose output degenerates towards a generic class-specific caption, we introduce a CLIP-based reward,
CLIP-R, that encourages the generated caption to continue to be descriptive of the video content.
• To ensure that the generated caption is grammatically-correct, we propose a grammar reward, GRAMMAR-R, that maintains the correctness of the produced caption.
• When evaluated on standard benchmarks for zero-shot and few-shot action classification, ReGen outperforms the previous state-of-the-art by a large margin. We also show very competitive results for zero-shot captioning. 2.