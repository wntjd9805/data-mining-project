Abstract
Unsupervised object-centric learning methods allow the partitioning of scenes into entities without additional lo-calization information and are excellent candidates for re-ducing the annotation burden of multiple-object tracking (MOT) pipelines. Unfortunately, they lack two key prop-erties: objects are often split into parts and are not con-sistently tracked over time. In fact, state-of-the-art models achieve pixel-level accuracy and temporal consistency by relying on supervised object detection with additional ID labels for the association through time. This paper pro-poses a video object-centric model for MOT. It consists of an index-merge module that adapts the object-centric slots into detection outputs and an object memory mod-ule that builds complete object prototypes to handle occlu-sions. Benefited from object-centric learning, we only re-quire sparse detection labels (0%-6.25%) for object local-ization and feature binding. Relying on our self-supervised
Expectation-Maximization-inspired loss for object associ-ation, our approach requires no ID labels. Our exper-iments significantly narrow the gap between the existing object-centric model and the fully supervised state-of-the-art and outperform several unsupervised trackers. Code is available at https://github.com/amazon-science/object-centric-multiple-object-tracking. 1.

Introduction
Visual indexing theory [45] proposes a psychological mechanism that includes a set of indexes that can be as-sociated with an object in the environment. Each index re-tains its association with an object, even when that object moves, interacts with other objects, or becomes partially occluded. This theory was originally developed in the cog-*Work completed during internship at AWS Shanghai AI Lab.
†Corresponding author. inconsistency and part-whole split of
Figure 1. Temporal object-centric representations. We visualize a video object-centric model SAVi [32] that groups objects into a set of slots with-out labels. Common issues include that there exist many duplicate slots that capture the same object or its parts (dashed arrows), and the slots fail to track objects consistently over time (red boxes). nitive sciences, however, a very similar principle lies at the heart of object-centric representation learning. By learn-ing object-level representations, we can develop models in-ferring object relations [39, 60, 62] and even their causal structure [36, 40]. Additionally, object-centric representa-tions have shown to be more robust [13], allow for combi-natorial generalization [36], and are beneficial for various downstream applications [60]. Since causal relations often unfold in time, it is only logical to combine object-centric learning (OCL) with temporal dynamics modeling, where consistent object representations are necessary.
Multiple object tracking (MOT) is a computer-vision problem that resembles visual indexing theory. MOT aims at localizing a set of objects while following their trajec-tories over time so that the same object keeps the same identity in the entire video stream. The dominant MOT methods follow the detect-to-track paradigm: First, employ an object detector to localize objects in each frame, then perform association on detected objects between adjacent frames to get tracklets. The development of state-of-the-art
MOT pipelines usually requires large amounts of detection labels for the objects we are interested in, as well as video datasets with object ID labels to train the association mod-ule. Consequently, such approaches are label intense and do not generalize well in open-world scenarios.
Unsupervised object-centric representation learning tackles the object discovery and binding problem in visual data without additional supervision [50]. Recent work, such as SAVi [32] and STEVE [52], extended such models to the video domain, which hints at possible applications to MOT.
However, existing approaches are primarily evaluated with-out heavy punishment if slots exchange “ownerships” of pixels and rather rely on clustering similarity metrics such as FG-ARI [32]. An object may appear in different slots across time (a.k.a ID switch issue), which hinders down-stream applications of OCL models, especially when direc-tional relationship among objects and their dynamics must be reasoned upon (e.g., who acts upon whom). Addition-ally, the part-whole issues are not fully explored, allowing slots to only track parts of an object. Figure 1 visualizes the two problems of OCL models that are developed orthog-onally with respect to MOT downstream tasks, leading to a significant gap with the state-of-the-art fully supervised
MOT methods. Scalability challenges of unsupervised OCL methods only accentuate this gap.
In this work, we take steps to bridge the gap between object-centric learning and fully-supervised multiple object tracking pipelines. Our design focuses on improving OCL framework on two key issues: 1) track objects as a whole, and 2) track objects consistently over time. For these, we insert a memory model to consolidate slots into memory buffers (to solve the part-whole problem) and roll past rep-resentations of the memory forward (to improve temporal consistency). Overall, our model provides a label-efficient alternative to the otherwise costly MOT pipelines that rely on detection and ID labels. Our contributions can be sum-marized as follows: (1) We develop a video object-centric model that can be applied to MOT task with very few detection labels (0 %-6.25 %) and no ID labels. (2) OC-MOT leverages an unsupervised memory to pre-dict completed future object states even if occlusion happens. Besides, the index-merge module can tackle the part-whole and duplication issues specific to OC models. The two cross-attention design is simple but nontrivial, serving as the “index” and “merge” func-tions with their key and query being bi-directional. (3) We are the first to introduce the object-centric repre-sentations to MOT that are versatile enough in a way of supporting all the association, rolling-out, and merging functions, and can be trained with low labeling cost. 2.