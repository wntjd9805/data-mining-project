Abstract
This paper addresses the problem of cross-modal ob-ject tracking from RGB videos and event data. Rather than constructing a complex cross-modal fusion network, we ex-plore the great potential of a pre-trained vision Transformer (ViT). Particularly, we delicately investigate plug-and-play training augmentations that encourage the ViT to bridge the vast distribution gap between the two modalities, en-abling comprehensive cross-modal information interaction and thus enhancing its ability. Specifically, we propose a mask modeling strategy that randomly masks a specific modality of some tokens to enforce the interaction between tokens from different modalities interacting proactively. To mitigate network oscillations resulting from the masking strategy and further amplify its positive effect, we then the-oretically propose an orthogonal high-rank loss to regular-ize the attention matrix. Extensive experiments demonstrate that our plug-and-play training augmentation techniques can significantly boost state-of-the-art one-stream and two-stream trackers to a large extent in terms of both tracking precision and success rate. Our new perspective and find-ings will potentially bring insights to the field of leveraging powerful pre-trained ViTs to model cross-modal data. The code is publicly available at https://github.com/
ZHU-Zhiyu/High-Rank_RGB-Event_Tracker. 1.

Introduction
Event cameras asynchronously capture pixel intensity fluctuations with an ultra-high temporal resolution, low la-tency, and high dynamic range, making it gain increasing attention recently [38, 42, 15]. Owing to such admirable advantages, event cameras have been widely adopted in var-ious applications, such as object detection [38, 30, 39, 42, 11] and depth/optical flow estimation [16, 63]. Particularly,
*Corresponding author: Junhui Hou. This work was supported in part by the Hong Kong Research Grants Council under Grant 11218121 and
Grant 11202320, and in part by the Hong Kong Innovation and Technology
Fund under Grant MHP/117/21. the distinctive sensing mechanism makes event cameras to be a promising choice for object tracking [45, 26, 60, 64, 18, 19].
Despite many advantages of event-based object track-ing under special environments, e.g., low-light, high-speed motion, and over-exposed, event data lack sufficient visual cues, such as color, texture, and complete contextual ap-pearance that can be easily captured by RGB data, result-ing in only event-based vision still suffering from relatively inferior performance in practice. Thus, a more promis-ing direction is to investigate cross-modal object tracking from both RGB and event data, where the merits of the two modalities can be well leveraged for pursuing higher performance. However, the vast distribution gap between
RGB and event data poses significant challenges in design-ing algorithms for modeling cross-modal information. Most existing pioneering cross-modal trackers heavily engage in robust cross-modal fusion modules, which is cumbersome to use advanced embedding backbones for boosting perfor-mance.
In view of the success of Transformer-based tracking al-gorithms [31, 59, 54, 7, 62], where the multi-head attention naturally models the indispensable correlation relationship between template and search regions, we plan to investigate the potential of pre-trained powerful vision Transformers (ViTs) in cross-modal object tracking from both RGB and event data. However, those pre-trained Transformers with
RGB data may not be able to fully model the essential fea-ture interaction across RGB and event data, due to the dis-tribution gap between the two modalities. To this end, we study plug-and-play training techniques for augmenting the pre-trained Transformer used as the embedding backbone of our RGB-event object tracking framework.
To be specific, to promote the learning of the attention layer across two modalities, we propose a cross-modal mask modeling strategy, which randomly masks/pops out the multi-modal tokens. We anticipate that, in reaction to the absence of a particular modality at certain locations, the net-work would proactively enhance interactions on the remain-ing cross-modal tokens. Nevertheless, randomly masking
tokens will inevitably alter data distributions and introduce disruptions, impeding network training. To mitigate the in-duced negative effect, we further propose a regularization term to guide the training of each attention layer. Based on the observation that the values of internal attention matrices of a Transformer indicate the degree of cross-modal feature interaction, we propose to orthogonalize the attention ma-trix to promote its rank obligatorily. Beyond, we anticipate that such regularization could encourage the cross-modal correlation to be evenly and concisely established using the multi-domain signatures, rather than unduly reliant on a specific domain. Finally, we apply the proposed techniques to state-of-the-art one-stream and two-stream Transformer-based tracking frameworks and experimentally demonstrate that their tracking performance is further boosted signifi-cantly.
In summary, the contributions of this paper are:
• a mask modeling strategy for encouraging the inter-action between the cross-modal tokens in a proactive manner;
• theoretical orthogonal high-rank regularization for suppressing network fluctuations induced by cross-modal masking while amplifying its positive effect; and
• new state-of-the-art baselines for RGB-event object tracking.
Last but not least, our novel perspectives will potentially bring insights to the field of leveraging pre-trained powerful
ViTs to process and analyze cross-modal data. 2.