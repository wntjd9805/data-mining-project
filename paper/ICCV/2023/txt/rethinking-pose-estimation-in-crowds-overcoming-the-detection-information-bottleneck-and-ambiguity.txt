Abstract
Frequent interactions between individuals are a funda-mental challenge for pose estimation algorithms. Current pipelines either use an object detector together with a pose estimator (top-down approach), or localize all body parts first and then link them to predict the pose of individu-als (bottom-up). Yet, when individuals closely interact, top-down methods are ill-defined due to overlapping in-dividuals, and bottom-up methods often falsely infer con-nections to distant bodyparts. Thus, we propose a novel pipeline called bottom-up conditioned top-down pose es-timation (BUCTD) that combines the strengths of bottom-up and top-down methods.
Specifically, we propose to use a bottom-up model as the detector, which in addition to an estimated bounding box provides a pose proposal that is fed as condition to an attention-based top-down model. We demonstrate the performance and efficiency of our approach on animal and human pose estimation bench-marks. On CrowdPose and OCHuman, we outperform pre-vious state-of-the-art models by a significant margin. We achieve 78.5 AP on CrowdPose and 48.5 AP on OCHu-man, an improvement of 8.6% and 7.8% over the prior art, respectively. Furthermore, we show that our method strongly improves the performance on multi-animal bench-marks involving fish and monkeys. The code is available at https://github.com/amathislab/BUCTD 1.

Introduction
Imagine somebody hands you an image of a person and asks you “to annotate the pose”. For your exquisite pri-mate visual system this is a trivial task that you can readily achieve. Now imagine somebody hands you another image that contains two people, arm-in-arm. You are likely frus-trated and will ask whose pose you should annotate? In re-sponse to whose pose you should annotate, your opponent will likely point at the person she has in mind. Based on the pointing, it’s again easy to annotate the right pose. Our
*Authors contributed equally to this work.
Figure 1. Overview of our bottom-up conditioned top-down pose estimation (BUCTD) approach and benchmarking re-sults. BUCTD uses a bottom-up pose model as instance detec-tor, which is computationally cheaper than existing, widely-used object detectors (see inset Table). The pose proposals from the pose detector are used to calculate bounding boxes and to con-dition our novel, conditional top-down stage. Note that, as in a standard top-down paradigm, only one image crop plus its cor-responding conditional pose is presented to the BUCTD. We can substantially boost performance on both human & animal bench-marks, with especially large gains in crowded scenes (Tables 2, 7). work proposes a hybrid deep learning framework for pose estimation that is inspired by this interaction.
This simple interaction highlights the ambiguity problem of top-down approaches in crowds. They first localize in-dividuals with a dedicated object detector [33, 34, 12, 27]
and then perform single-instance pose estimation [13, 8, 39, 23, 42, 25, 40].
In contrast, bottom-up approaches first localize all body parts in the image and then assem-ble them into poses of each of the individuals simultane-ously [19, 6, 31, 9, 16, 22, 36]. Yet, when individuals closely interact, top-down methods are ill-defined as it is unclear which pose should be predicted within a bound-ing box that contains multiple individuals. Therefore, oc-cluded individuals will often be ignored by top-down meth-ods (Figure 1). In contrast, as bottom-up approaches reason over the complete scene they may not have this problem.
Bottom-up approaches can localize all individuals, but of-ten struggle to make accurate predictions.
To overcome those limitations we propose a simple yet effective framework called Bottom-Up Conditioned Top-Down pose estimation (BUCTD). Our solution, is inspired by the interaction that we described. Instead of using ob-ject detectors, we propose to use bottom-up pose estima-tion models as detectors. The output poses are used to esti-mate bounding boxes of the individuals, and also serve as a
“pointing” mechanism, that indicates whose pose should be predicted. To also process the “pointing” input, we general-ize top-down models to conditional-top down (CTD) mod-els, which present the second stage of our BUCTD frame-work. CTD models take a cropped image together with a pose as input. They are trained to predict the correct pose based on the (potentially) noisy pose provided by the bottom-up methods (Figure 1).
Thus, BUCTD overcomes the information bottleneck and ambiguity introduced by standard detectors, while typ-ically having similar or lower inference cost (Figure 1).
We evaluate BUCTD on COCO [26], two crowded hu-man benchmarks, CrowdPose [23] and OCHuman [43], and three multi-animal benchmarks, namely SchoolingFish,
Tri-Mouse and Marmosets [22]. We achieve SOTA perfor-mance and strongly outperform both top-down and bottom-up models in occluded and crowded scenes. 2.