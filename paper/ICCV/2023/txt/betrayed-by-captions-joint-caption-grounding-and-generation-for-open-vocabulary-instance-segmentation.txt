Abstract
In this work, we focus on open vocabulary instance segmentation to expand a segmentation model to classify and segment instance-level novel categories. Previous ap-proaches have relied on massive caption datasets and com-plex pipelines to establish one-to-one mappings between image regions and words in captions. However, such meth-ods build noisy supervision by matching non-visible words to image regions, such as adjectives and verbs. Meanwhile, context words are also important for inferring the existence of novel objects as they show high inter-correlations with novel categories. To overcome these limitations, we devise a joint Caption Grounding and Generation (CGG) frame-work, which incorporates a novel grounding loss that only focuses on matching object nouns to improve learning effi-ciency. We also introduce a caption generation head that enables additional supervision and contextual modeling as a complementation to the grounding loss. Our analysis and results demonstrate that grounding and generation compo-nents complement each other, significantly enhancing the segmentation performance for novel classes. Experiments on the COCO dataset with two settings: Open Vocabulary
Instance Segmentation (OVIS) and Open Set Panoptic Seg-mentation (OSPS) demonstrate the superiority of the CGG.
Specifically, CGG achieves a substantial improvement of 6.8% mAP for novel classes without extra data on the OVIS task and 15% PQ improvements for novel classes on the
OSPS benchmark. 1.

Introduction
Instance Segmentation [39] is a core vision task that goes beyond object detection [38, 37, 49] via segmenting and
*The first two authors contributed equally to this work.
† Corre-sponding Author and Leader. Code and model are available at https:
//github.com/jianzongwu/betrayed-by-captions. (a) VLMs learn image-level visual-linguistic
Figure 1: alignment using caption data. (b) Previous open vocabu-lary detection/segmentation methods extract all words [63] or nouns + adjectives [20] for caption grounding. (c) The proposed CGG extracts object nouns for a finer alignment between objects in the caption and visible entities in the im-age and then combines a caption generation loss to utilize the contextual knowledge in the caption fully. classifying each object. Despite it continues to attract sig-nificant research effort [25, 53, 12, 2, 56, 72, 13, 8, 4, 5, 7, 36, 35, 34, 66, 67], current solutions mainly focus on a closed-set problem that assumes a pre-defined set of ob-ject categories [39, 31, 23]. In practice, many applications need to detect and segment new categories. To save the need of annotating new object categories, zero-shot object detection/segmentation [47, 3] is proposed, where models are trained on base classes and equipped with the ability to segment new classes. However, the zero-shot setting per-forms poorly on novel classes, as high-level word embed-dings cannot effectively encode fine-grained visual infor-mation.
To address this issue, recent work [63] proposes an
Figure 2: A comparison analysis of caption grounding using different types of words. The color maps are normalized similarities between multi-modal embeddings and word features extracted by the language encoder. Both (a) [63] and (b) [20] suffer from the problem that invisible nouns (room in the example) are learned to be aligned by the multi-modal embeddings while using object nouns avoids the question. We adopt top-10 object queries according to their object scores. (d) We sample 2500 images from the COCO validation set and test the average rate of multi-modal embeddings attending on object nouns under different thresholds. open vocabulary setting by pre-training a visual back-bone on captioned images for learning rich visual fea-tures. With the success of pre-trained Vision Language
Models (VLMs) [45, 30], several approaches, e.g., ViLD
[22], propose effective methods to distill knowledge from
VLMs into detectors or segmentation methods. Meanwhile, several works decouple the learning of open vocabulary classification and detection/segmentation into a two-stage pipeline [20, 16]. Recently, state-of-the-art solutions [73, 18, 28, 33, 65] for open vocabulary detection/segmentation try to adopt larger-scale dataset pre-training with the help of VLMs. For example, Detic [73] adopts the ImageNet-21k [50] dataset to enlarge the detector in a weakly super-vised manner, while PromptDet [18] augments the detection dataset with image-caption pairs scraped from the Internet.
Recent XPM [28] also pre-trains their model on caption datasets [51]. These approaches typically require a complex architecture design to leverage extra datasets [50, 31]. De-spite the performance improvement, these methods are not cost-effective in terms of data utilization. In this paper, we explore the use of caption data with more effective designs.
Caption-related vision tasks can be broadly divided into grounding and generation. The former [61, 14, 15, 40, 12, 21] requires a model to align the text and corresponding re-gion features, e.g., OVR-CNN [63] and OpenSeg [20] in
Fig. 1 (a) and (b). However, these methods expose a core issue in that they adopt the grounding loss between words and mask regions, implicitly assuming each word (or noun) should correspond to a region in the image. As shown in
Fig. 2 (a) and (b), ‘messy’ and ‘room’ are forced to ground to meaningless masks. This motivates us to reformulate the ground loss by only focusing on object nouns as Fig. 2 (c).
On the other hand, the latter [55, 60, 68] learns a model that outputs a caption for a given imagery input. It naturally captures the auxiliary and surrounding information to gen-erate context words, which is crucial to building the bridge between image and text. Given the above observation, we argue that caption generation can naturally complement the grounding loss for context capturing.
Therefore, we propose a unified framework based on
Mask2Former [8] performing each task jointly to exploit the knowledge from caption data better. It contains a caption grounding loss and an extra caption decoder for the genera-tion loss, as shown in Fig. 1 (c). Motivated by the correla-tion analysis of object query and caption data (Sec. 3.2), we first extract object nouns for grounding loss. In particular, we transform the object queries into multi-modal embed-dings using a linear layer at the input stage. Then we adopt separated object nouns to ground each multi-modal embed-ding, providing us with the grounding loss. Since extracted object nouns miss the structure information of caption data, we append a caption generation loss in the output stage to recover language data. We add a lightweight Transformer decoder with multi-modal embeddings as inputs to generate captions. Experiments demonstrate that the two losses are well coupled and mutually affect novel class segmentation, with only 0.8% GFlops added during training. Our method drops the caption generation module for inference with no extra computation cost.
Our contributions can be summarized as follows:
• We propose a joint Caption Grounding and Generation (CGG) framework for open vocabulary instance seg-mentation, which incorporates grounding with object nouns and caption generating.
• Experimental results demonstrate our method achieves a significant improvement of 6.8% mAP over previous
XPM [28] on OVIS and 15% PQ improvements over previous method [58] on OSPS. 2.