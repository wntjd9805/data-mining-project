Abstract
Learning how to navigate among humans in an occluded and spatially constrained indoor environment, is a key abil-ity required to embodied agents to be integrated into our so-ciety. In this paper, we propose an end-to-end architecture that exploits Proximity-Aware Tasks (referred as to Risk and
Proximity Compass) to inject into a reinforcement learning navigation policy the ability to infer common-sense social behaviours. To this end, our tasks exploit the notion of im-mediate and future dangers of collision. Furthermore, we propose an evaluation protocol specifically designed for the
Social Navigation Task in simulated environments. This is done to capture fine-grained features and characteristics of the policy by analyzing the minimal unit of human-robot spatial interaction, called Encounter. We validate our ap-proach on Gibson4+ and Habitat-Matterport3D datasets. 1.

Introduction
Navigating safely in a dynamic scenario populated by humans who are moving in the same environment is nec-essary for embodied agents such as home assistants robots.
To do so, as depicted in Figure 1, the agent should be able to dynamically and interactively navigate the environment by avoiding static objects and moving persons.
Recently, the development of photorealistic 3D simula-tors [35, 36, 21] has provided the tools to train embodied agents and experiment in large-scale indoor environments
[8, 31, 15]. Thanks to these frameworks, several tasks and challenges have been introduced [1, 48, 14], foster-ing the development of accompanying techniques to solve these tasks. In particular, in the PointGoal Navigation task (where an agent is required to reach a specific location in an environment), an agent without any sensor/actuation noise trained for billions of steps can obtain almost perfect perfor-mance [42]. Other approaches [45, 27] obtained impressive
*Both authors contributed equally. Datasets and code are publicly avail-able on https://github.com/EnricoCancelli/ProximitySocialNav.
Figure 1: Illustration of an agent-person “encounter”. From top-left to bottom-right: i) episode starts; ii) the embodied agent/robot sees a person; iii) it moves back to avoid a col-lision; iv) it reaches the goal by avoiding the person. results even in the presence of noise. Another relevant task is Object Goal Navigation, where an agent is required to find and navigate to a specific object given the object cate-gory. This task requires both semantic and navigation capa-bilities and leads to the development of modular approaches based on semantic maps [9, 6, 32], as well as end-to-end trained models using reinforcement learning (RL) [46, 33].
However, despite encouraging progress on the challeng-ing task, all the previously mentioned tasks frame naviga-tion in a fundamentally static environment. The dynamic element introduced by sentient, moving human beings in the scene forces us to rethink how the current models are designed. A good navigation policy must not be just effec-tive (i.e., able to achieve its goal) and efficient (i.e., able to achieve the objective through a close-to-optimal path) but also safe (reaching the destination without harming oth-ers). This social element is included in the Social Naviga-tion Task (SocialNav) [44, 29], where an agent must tackle
PointGoal Navigation in simulated indoor environments. To tackle this task, Yokoyama et al. [47] introduced a simple
but quite effective model that placed first in the iGibson 2021 Interactive challenge. However, the approach does not explicitly encode any social behaviour in its navigation pol-icy. We believe that a clear encoding of human-agent inter-actions, as well as social behaviours, are required for safe navigation and interaction with humans. By modelling the movement of humans, the agent could prevent collisions or dangerous behaviours and adapt its path to the dynamic en-vironment in which it is navigating. We encode these “sig-nals” by introducing two Proximity-Aware Tasks, referred as risk and proximity compass. These auxiliary tasks model the present and future danger for the agent’s action.
Additionally, we define a fine-grained evaluation pro-tocol for the SocialNav task to better analyse the perfor-mances of agents during human-agent interactions. Our evaluation protocol is inspired by a similar attempt [30] in robotics, which consisted of collecting statistics about specific types of interaction between humans and a robot (through questionnaires). We propose an automated evalu-ation by identifying and characterizing encounters between human and agent. To do so, we extract short sub-sequences where an interaction with a human becomes a predominant factor influencing navigation, and we establish a set of rules for classifying each encounter based on the type of human-agent spatial relationship through time. Finally, we also in-troduce a dataset of episodes on top of HM3D [31] for Em-bodied Social Navigation to assess our agents in different environments.
In summary, the contributions of this work are three-fold: (1) A novel architecture for embodied social naviga-tion which is based on Proximity-Aware tasks; we show the effectiveness of the model on two public datasets. (2) A new encounter-based evaluation protocol for analysing so-cial navigation models. (3) A set of episodes for evaluat-ing embodied social navigation based on the HM3D dataset (called HM3D-S). 2.