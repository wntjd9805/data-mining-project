Abstract
We present a GAN-based Transformer for general action-conditioned 3D human motion generation, includ-ing not only single-person actions but also multi-person interactive actions. Our approach consists of a powerful
Action-conditioned motion TransFormer (ActFormer) under a GAN training scheme, equipped with a Gaussian Pro-cess latent prior. Such a design combines the strong spatio-temporal representation capacity of Transformer, superior-ity in generative modeling of GAN, and inherent temporal correlations from the latent prior. Furthermore, ActFormer can be naturally extended to multi-person motions by alter-nately modeling temporal correlations and human interac-tions with Transformer encoders. To further facilitate re-search on multi-person motion generation, we introduce a new synthetic dataset of complex multi-person combat be-haviors. Extensive experiments on NTU-13, NTU RGB+D 120, BABEL and the proposed combat dataset show that our method can adapt to various human motion represen-tations and achieve superior performance over the state-of-the-art methods on both single-person and multi-person motion generation tasks, demonstrating a promising step to-wards a general human motion generator. The project web-site can be found at https://liangxuy.github.io/actformer/. 1.

Introduction
This work aims to tackle the action-conditioned motion generation task. Specifically, given a semantic action label as input and generate corresponding 3D human motions.
The technique is key to applications like character anima-*Denotes equal contribution. Work done when Liang and Ziyang were at SenseTime. Corresponding authors: Dongliang Wang (wang-dongliang@senseauto.com), Yichao Yan (yanyichao@sjtu.edu.cn) and
Xin Jin (jinxin@eias.ac.cn).
Figure 1. Towards general action-conditioned 3D human mo-tion generation. Our framework adapts to more action categories, various human motion representations (e.g., SMPL body models, skeleton joint coordinates), and multi-person interactive actions. tion creation, humanoid robots interaction and data synthe-sis for computer vision tasks related to human actions.
Human motion synthesis has been a long-standing re-search topic. However, most of the prior works are closer to a prediction task, in which future motions are generated from previous motions [16, 26, 41, 8, 22, 11, 55, 34]. In recent years, some works started to focus on motion gen-eration from action labels [20, 45, 49]. Despite some im-pressive generation results, these works are still limited in the following two aspects. Firstly, most of these works bias towards motion data of SMPL pose parameters while per-forming poorly on data of skeleton joint coordinates, which limits the generalization. A solution adaptable to various human motion representations is thus expected. Secondly, prior works only focus on single-person motion generation while neglecting multi-person interactive actions, which are
In general, prior integral parts of daily human motions. works fail to cover a complete domain of human motions and stand far from a general human motion generator.
This paper explores a solution towards general action-conditioned human motion generation, as shown in Fig. 1.
The very first challenge lies in generating long motion se-quences with realism and diversity. Many prior works as-sume a Markovian dependency in temporal motions and adopt an auto-regressive model [58, 20, 5, 6, 31]. How-ever, these methods are subject to the “mean-pose” prob-lem, in which the model starts to generate the mean pose continuously after a few frames. In contrast, CSGN [53] and ACTOR [45] sample from a sequence-level latent prior and produce the whole sequence altogether. Specifically,
CSGN samples from a Gaussian Process (GP) latent prior and stacks convolutions in the generator to enforce tempo-ral correlations. On the other hand, ACTOR samples a sin-gle vector as the sequence-level embedding and produces multiple frames by querying through different positional en-codings. We argue that both are sub-optimal solutions, and we seek a better trade-off between inductive bias and repre-sentation capacity. Our proposed Action-Conditioned Mo-tion TransFormer (ActFormer) leverages the GP prior for the inherent temporal correlations. Meanwhile, we adopt a
Transformer architecture for its simple structure and strong power in encoding non-local correlations proved in many other tasks. The Transformer model naturally regards a la-tent vector sequence from the GP prior as a sequence of tokens, leading to a seamless conjunction. We incorporate the Transformer-based motion generator into GAN, known for high-quality generative modeling. These designs jointly contribute to significant advantages of our framework in the single-person motion generation task.
Another challenge lies in handling human interactions when multi-person interactive actions are included. Human interactions have been explored by some motion prediction algorithms, in which pooling or self-attention modules are adopted to encode the interactions [21, 3, 4, 56]. However, it has not been considered in the motion generation task.
To our knowledge, our approach is the first to tackle multi-person motion generation. We share the same latent vector sequence from GP among multiple persons in a group to enforce their synchronization over time. Meanwhile, differ-ent persons are distinguished through positional encodings.
Our ActFormer can be easily extended to the multi-person scenario by alternately modeling temporal correlations and human interactions. The generation results show impressive realism in both motions and multi-person interactions.
The strong demand for motion capture (MoCap) data with action labels also poses a challenge. Prior methods rely on datasets with ∼10 categories, which can hardly drive a general motion generator. MoCap datasets with multi-person interactive actions are even rarer. We lever-age NTU RGB+D 120 [37] and the newly-released BA-BEL dataset [47], both including more than 100 action categories. To facilitate the research on multi-person mo-tion generation, we further construct a GTA Combat dataset through the Grand Theft Auto V’s (GTA-V) [1] gaming en-gine. We collect ∼7K motion sequences of combat behav-ior, which is one of the most complex types of human inter-actions. Experiments on these datasets verify the effective-ness of our approach.
Our three-fold contributions are summarized as follows: (i) We propose ActFormer, a GAN-based Transformer framework, which adapts to various human motion repre-sentations and achieves leading results in the single-person motion generation task; (ii) Our ActFormer takes a faith-fully early step to solve the multi-person motion generation problem; (iii) We contribute a GTA Combat dataset with plentiful and complex multi-person interactive motions. 2.