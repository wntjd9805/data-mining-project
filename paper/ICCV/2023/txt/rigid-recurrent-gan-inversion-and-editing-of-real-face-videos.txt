Abstract
GAN inversion is indispensable for applying the powerful editability of GAN to real images. However, existing methods invert video frames individually often leading to undesired inconsistent results over time. In this paper, we propose a unified recurrent framework, named Recurrent vIdeo GAN
Inversion and eDiting (RIGID), to explicitly and simultane-ously enforce temporally coherent GAN inversion and facial editing of real videos. Our approach models the temporal relations between current and previous frames from three as-pects. To enable a faithful real video reconstruction, we first maximize the inversion fidelity and consistency by learning a temporal compensated latent code. Second, we observe incoherent noises lie in the high-frequency domain that can be disentangled from the latent space. Third, to remove the inconsistency after attribute manipulation, we propose an in-between frame composition constraint such that the ar-bitrary frame must be a direct composite of its neighboring
*Corresponding author: pingluo@hku.hk. frames. Our unified framework learns the inherent coherence between input frames in an end-to-end manner, and there-fore it is agnostic to a specific attribute and can be applied to arbitrary editing of the same video without re-training.
Extensive experiments demonstrate that RIGID outperforms state-of-the-art methods qualitatively and quantitatively in both inversion and editing tasks. The deliverables can be found in https://cnnlstm.github.io/RIGID. 1.

Introduction
Generative adversarial networks (GANs) have demon-strated powerful generative ability in synthesizing high-quality faces from a latent code [9, 11, 12, 47]. It is evi-denced that the latent space of a well-trained GAN is seman-tically organized, and shifting the latent code along with a specific direction results in the manipulation of a correspond-ing attribute [23, 24, 5, 19, 45, 32]. Hence, many works migrate this power to real face processing by inverting a real face image to a latent code [49, 16, 39, 48, 40, 38]. Although 1
this two-combo strategy becomes a standard for editing high-resolution images, applying it to real videos has less been explored. A naive inversion and editing for each frame can undoubtedly produce incoherence in the resulted video.
Different from processing images, maintaining temporal coherence is the core issue for video editing [18, 28, 26].
Specifically, both the GAN inversion and attribute manipula-tion may introduce discontinuity across frames. IGCI [39] proposes the first attempt to invert consecutive images si-multaneously. They leverage the continuity of the inputs to optimize both the reconstruction fidelity and editability of the outputs, but they fail to consider the temporal corre-lation between results (see flickering in Fig. 1b). Recent work STIT [30] implicitly recovers the original temporal correlations by the faithful inversion of each frame. It fine-tunes an individual generator for every input video such that the generator can capture all the reconstruction details, and
TCSVE [37] extends this idea by proposing a temporal con-sistency loss that applies on the edited videos. Although they works well for most cases, they are video- and attribute-specific (needs to retrain the model for a new video or a new target attribute), and thus suffers from the expensive training cost and poor generalization ability.
In this paper, we aim to design a unified approach that learns the temporal correlations between successive frames for both inversion and editing, and it can be generalized to other target attributes without re-training. To this end, we propose a Recurrent vIdeo GAN Inversion and eDiting (RIGID) framework, which evolves and enables the image-based StyleGAN [11, 12] generator to output temporally coherent frames. The coherence is realized in both inversion and editing tasks. Given the current and previous frames, we formulate the inversion as the combination of an image-based inverted code and a temporal compensated code, while the latter amends the code with inter-frame similarity for an accurate and consistent inversion. On the other hand, we observe that the main sources of temporal incoherence, like “flickering”, belong to high-frequency artifacts. This motivates us to disentangle the main video content from high-frequency artifacts in the latent space, and thus the
“incoherence” can be shared with all the other frames. To build the temporal correlations after attribute manipulation, we propose a self-supervised “in-between frame composi-tion constraint” that applies to consecutive edited frames. It enforces any intermediate frame that can be composed by the warping results of their neighbors, which guarantees the smoothness of generated videos. RIGID is trained on the video episodes with several tailored losses. During the infer-ence, it inverts video frames sequentially and therefore can handle videos with arbitrary lengths and support live stream editing. More importantly, once our model is trained, it is attribute-agnostic that can be reused for arbitrary attribute manipulations without re-training. As shown in Fig. 1f,
RIGID achieves temporal coherent inversion and editing with far less inference time (compared to those scene- and attribute-specific methods like STIT). Extensive experiments demonstrate the superiority over state-of-the-art methods in terms of quantitative and qualitative evaluations.
In summary, our contributions are three-fold:
• We propose a recurrent video GAN inversion frame-work that unifies video inversion and editing. It learns the temporal correlation of generated videos explicitly.
• We model temporal coherence from both inversion and editing ends. For inversion, we discover the temporal compensated code and disentangle high-frequency arti-facts in the latent space. For editing, we present a novel
“in-between frame composition constraint” to confine a continuous video transformation.
• We achieve attribute-agnostic editing that can vary edit-ing attributes on the fly, avoiding expensive re-training of the model. Extensive experiments demonstrate the effectiveness of our method over state-of-the-arts. 2.