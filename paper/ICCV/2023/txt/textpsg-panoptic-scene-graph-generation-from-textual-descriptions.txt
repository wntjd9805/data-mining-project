Abstract
Panoptic Scene Graph has recently been proposed for comprehensive scene understanding. However, previous works adopt a fully-supervised learning manner, requiring large amounts of pixel-wise densely-annotated data, which is always tedious and expensive to obtain. To address this limitation, we study a new problem of Panoptic Scene Graph
Generation from Purely Textual Descriptions (Caption-to-PSG). The key idea is to leverage the large collection of free image-caption data on the Web alone to generate panop-tic scene graphs. The problem is very challenging for three constraints: 1) no location priors; 2) no explicit links be-tween visual regions and textual entities; and 3) no pre-defined concept sets. To tackle this problem, we propose a new framework TextPSG consisting of four modules, i.e., a region grouper, an entity grounder, a segment merger, and a label generator, with several novel techniques. The region grouper first groups image pixels into different seg-ments and the entity grounder then aligns visual segments with language entities based on the textual description of the segment being referred to. The grounding results can thus serve as pseudo labels enabling the segment merger to learn the segment similarity as well as guiding the label generator to learn object semantics and relation predicates, resulting in a fine-grained structured scene understanding.
Our framework is effective, significantly outperforming the baselines and achieving strong out-of-distribution robust-ness. We perform comprehensive ablation studies to cor-roborate the effectiveness of our design choices and pro-vide an in-depth analysis to highlight future directions. Our code, data, and results are available on our project page: https://vis-www.cs.umass.edu/TextPSG. 1.

Introduction
A scene graph is a directed-graph-based abstract repre-sentation of the objects and their relations within a scene.
It has been widely utilized to develop a structured scene understanding of object semantics, locations, and relations,
Problem Overview. Different from the traditional
Figure 1. bbox-based form of the scene graph as shown in (a), Caption-to-PSG aims to generate the mask-based panoptic scene graph. In
Caption-to-PSG, the model has no access to any location priors, explicit region-entity links, or pre-defined concept sets. Conse-quently, the model is required to learn partitioning and grounding as illustrated in (b), as well as object semantics and relation predi-cates as illustrated in (c), all purely from textual descriptions. which facilitates a variety of downstream applications, such as image generation [14, 8], visual reasoning [42, 1, 39], and robotics [2, 10].
Typically, in a scene graph, each node denotes an ob-ject in the scene located by a bounding box (bbox) with a semantic label, and each directed edge denotes the relation between a pair of objects with a predicate label. Nonethe-less, a recent work [49] points out that such a bbox-based form of scene graph is not ideal enough. Firstly, com-pared with pixel-wise segmentation masks, bboxes are less fine-grained and may contain some noisy pixels belonging to other objects, limiting the applications for some down-stream tasks. For example, as shown in Fig. 1 (a), about half of the pixels in the yellow bbox for girl belong to wall.
Secondly, it is challenging for bboxes to cover the entire scene without ambiguities caused by overlaps, which pre-vents a scene graph from including every object in the scene for a complete description. To this end, the work [49] pro-poses the concept of Panoptic Scene Graph (PSG), in which each object is grounded by a panoptic segmentation mask, to reach a comprehensive structured scene representation.
However, all existing works [49, 45] approach PSG gen-eration through a fully-supervised manner, i.e., learning to perform panoptic segmentation and relation prediction from manually-annotated datasets with explicit supervision for both segmentation and relation prediction. Unfortunately, it is extremely labor-intensive to build such datasets, making it difficult to scale up to cover more complex scenes, object semantics, and relation predicates, thus significantly limit-ing the generalizability and the application of these methods to the real world. For instance, the current PSG dataset [49] only covers 133 object semantics and 56 relation predicates.
To relieve the reliance on densely-annotated data, weakly-supervised methods [52, 58, 22] for scene graph generation are promising. These methods could induce scene graphs from image-caption pairs, which can be eas-ily harvested from the Web for free. Even so, they still rely heavily on two strong preconditions, i.e., a powerful region proposal network (e.g., [35]) and a pre-defined set of object semantics and relation predicates. Although these precondi-tions facilitate the learning process of the methods, they also limit the generalizability for locating novel objects (unfore-seen objects for the region proposal network) and constrain the understanding into the limited concept set.
Inspired by previous weakly-supervised methods, we in-troduce a new problem, Panoptic Scene Graph Genera-tion from Purely Textual Descriptions (Caption-to-PSG), to explore a holistic structured scene understanding without labor-intensive data annotation. Considering the limitation of the preconditions mentioned, we set three constraints to
Caption-to-PSG to reach a more comprehensive and gener-alizable understanding, which results in a very challenging problem: a) only image-caption pairs are provided during training, without any location priors in either region pro-posals or location supervision; b) the explicit links between regions in images and entities in captions are missing; c) no concept sets are pre-defined, i.e., neither object semantics nor relation predicates are known beforehand.
Given these three constraints, we argue that there are two key challenges for the model to solve the problem. Firstly, the model should learn to ground entities in language onto the visual scene without explicit location supervision, i.e., the ability to perform partitioning and grounding, as shown in Fig. 1 (b), should be developed purely from textual de-scriptions. Secondly, during training, the model should also learn the object semantics and relation predicates from tex-tual descriptions, as shown in Fig. 1 (c), without pre-defined fixed object and relation vocabularies. By solving these challenges, the model could associate visual scene patterns with textual descriptions, gradually acquire common sense among them, and finally reach a more comprehensive and generalizable understanding, including novel object loca-tion, extensive semantics recognition, and complex relation analysis, which is more suitable to the real world.
With these considerations, we propose a novel frame-work, TextPSG, as the first step towards this challenging problem. TextPSG consists of a series of modules to co-operate with each other, i.e., a region grouper, an entity grounder, a segment merger, and a label generator. The region grouper learns to merge image regions into several segments in a hierarchical way based on object semantics, similar to [48]. The entity grounder employs a fine-grained contrastive learning strategy [51] to bridge the textual de-scription and the visual content, grounding entities in the caption onto the image segments. With the entity-grounding results as pseudo labels, the segment merger learns similar-ity matrices to merge small image segments during infer-ence, while the label generator learns the prediction of ob-ject semantics and relation predicates. Specifically, in the segment merger, we propose to leverage the grounding as explicit supervision for merging, compared with [48] which learns merging in a fully implicit manner, to improve the ability of location.
In the label generator, different from all previous pipelines for scene graph generation, we refor-mulate the label prediction as an auto-regressive generation problem rather than a classification problem, and employ a pre-trained language model [21] as the decoder to leverage the pre-learned common sense. We further design a novel prompt-embedding-based technique (PET) to better incor-porate common sense from the language model. Our ex-periments show that TextPSG significantly outperforms the baselines and achieves strong out-of-distribution (OOD) ro-bustness. Comprehensive ablation studies corroborate the effectiveness of our design choices. As a side product, the proposed grounder and merger modules also have been ob-served to enhance text-supervised semantic segmentation.
In spite of the promising performance of TextPSG, cer-tain challenges persist. We delve into an in-depth analysis of the failure cases, provide a model diagnosis, and discuss potential future directions for enhancing our framework.
To sum up, our contributions are as follows:
• We introduce a new problem, Panoptic Scene Graph
Generation from Purely Textual Descriptions (Caption-to-PSG), to alleviate the burden of human annotation for PSG by learning purely from the weak supervision of captions.
• We propose a new modularized framework, TextPSG, with several novel techniques, which significantly outperforms the baselines and achieves strong OOD robustness. We demonstrate that the proposed modules in TextPSG can also facilitate text-supervised semantic segmentation.
• We perform an in-depth failure case analysis with a model diagnosis, and further highlight future directions. 2.