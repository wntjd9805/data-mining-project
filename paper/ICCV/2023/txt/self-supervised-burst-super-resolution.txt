Abstract
We introduce a self-supervised training strategy for burst super-resolution that only uses noisy low-resolution bursts during training. Our approach eliminates the need to care-fully tune synthetic data simulation pipelines, which of-ten do not match real-world image statistics. Compared to weakly-paired training strategies, which require noisy smartphone burst photos of static scenes, paired with a clean reference obtained from a tripod-mounted DSLR cam-era, our approach is more scalable, and avoids the color mismatch between the smartphone and DSLR. To achieve this, we propose a new self-supervised objective that uses a forward imaging model to recover a high-resolution im-age from aliased high frequencies in the burst. Our ap-proach does not require any manual tuning of the forward model’s parameters; we learn them from data. Further-more, we show our training strategy is robust to dynamic scene motion in the burst, which enables training burst super-resolution models using in-the-wild data. Extensive experiments on real and synthetic data show that, despite only using noisy bursts during training, models trained with our self-supervised strategy match, and sometimes surpass, the quality of fully-supervised baselines trained with syn-thetic data or weakly-paired ground-truth. Finally, we show our training strategy is general using four different burst super-resolution architectures. 1.

Introduction
Recent RAW burst super-resolution pipelines have sig-niﬁcantly improved the quality of modern smartphones pho-tos [33, 8]. State-of-the-art algorithms use specialized deep learning models that learn to merge the burst frames into a single high-resolution image [3, 19, 18, 23, 24]. Train-ing them requires paired datasets, in which each noisy burst is matched to a clean reference. Most approaches synthe-∗Work partly done during an internship at Adobe.
size realistic bursts from the reference using carefully tuned degradation models [2, 3, 19, 24]. But because of low-level mismatches between the real and synthetically gen-erated bursts (e.g., noise distribution, blur kernels, camera trajectories, scene motions, etc), models trained syntheti-cally often do not generalize well to real-world inputs (Fig-ure 1). To avoid this, other works collect weakly-paired datasets in which the reference is a high-resolution image of the same scene captured using a DSLR and a zoom lens on tripod [2, 39]. However, this capture process is tedious and time-consuming, and the resulting image pairs are often misaligned, exhibit color and detail mismatches because of the different sensors, and permit limited scene motion.
In this work, we propose a new self-supervised training strategy for burst super-resolution that alleviates the limita-tions of both synthetic and weakly-supervised datasets. Our approach only requires real-world noisy bursts for training, which are easy to collect. It eliminates the data collection complexity of weakly-paired approaches, and, by using real bursts, avoids the domain gap issues that plague syntheti-cally trained models.
We derive a self-supervised reconstruction objective that models the relationship between the noisy burst and the ideal clean reference image we wish to recover. In partic-ular, we exploit the property that burst images are noisy, aliased, and subsampled measurements of a scene, at ran-dom spatial offsets due to hand tremor [33], to recover high-frequency image details. Speciﬁcally, during training, we randomly split each burst into two sets of images. The
ﬁrst is passed as input to a burst super-resolution network to produce a high-resolution output, from which we derive low-resolution images using a forward image degradation model. We compare these low-resolution frames against the second set of burst images to compute our reconstruc-tion loss. Optimizing this loss on a single burst provides too sparse a signal to train burst super-resolution models. But in a stochastic optimization involving a large dataset of bursts with random camera displacements, our self-supervised ob-jective enables learning a robust image prior, and lets us recover high-resolution merged images.
Our loss function uses an explicit but general parame-terized image formation model. Crucially, we do not make any limiting assumption about the parameters of this model (e.g., the precise noise distribution, the lens point spread function).
Instead, we jointly learn the model’s parame-ters along with the super-resolution network from data. Our training approach is general: it can be used to train any neu-ral network architecture using bursts captured from any sen-sor. By using only noisy low-resolution bursts, which are easy to collect, our approach opens the opportunity to de-ploy state-of-the-art super-resolution network architectures for various cameras in real-world settings. In short, our con-tributions are the following:
• To the best of our knowledge, we introduce the ﬁrst self-supervised training approach for raw burst super-resolution using only noisy, low-resolution inputs.
• We develop a robust self-reconstruction loss for train-ing on bursts with dynamic object motion, which are prevalent in real in-the-wild bursts.
• Our approach can be used learn a lens blur kernel jointly with a burst super-resolution model, thereby al-leviating the need of explicit blur kernel estimation.
• We perform extensive experiments on two real world burst datasets, using four different network architec-tures. Our approach obtains promising results com-pared to the model trained using weakly-paired data, despite using only low-resolution bursts. 2.