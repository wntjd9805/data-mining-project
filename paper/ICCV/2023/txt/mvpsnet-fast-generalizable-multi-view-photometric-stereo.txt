Abstract
We propose a fast and generalizable solution to Multi-view Photometric Stereo (MVPS), called MVPSNet. The key to our approach is a feature extraction network that effec-tively combines images from the same view captured under multiple lighting conditions to extract geometric features from shading cues for stereo matching. We demonstrate these features, termed ‘Light Aggregated Feature Maps’ (LAFM), are effective for feature matching even in texture-less regions, where traditional multi-view stereo methods often fail. Our method produces similar reconstruction re-sults to PS-NeRF, a state-of-the-art MVPS method that opti-mizes a neural network per-scene, while being 411× faster (105 seconds vs. 12 hours) in inference. Additionally, we introduce a new synthetic dataset for MVPS, sMVPS, which is shown to be effective for training a generalizable MVPS method. 1.

Introduction 3D reconstruction of an object can be achieved either through camera viewpoint variations, Multi-view Stereo (MVS), or by lighting direction variations, Photometric
Stereo (PS). Both MVS and PS have relative strengths and weaknesses. While MVS succeeds in obtaining accurate global shapes, it suffers in textureless regions due to poor feature matching, often resulting in reconstructions that lack local details. On the other hand, PS produces accurate local details, even in textureless regions, by using shading infor-mation but fails to reconstruct accurate global shapes. In this paper, we focus on the problem of Multi-view Photo-metric Stereo (MVPS) where both camera viewpoint and lighting direction variations are used to accurately recon-struct global and local details of a 3D shape, even in tex-tureless regions. 3D reconstruction techniques that produce high-quality results using only viewpoint variations (MVS) rely on test-time optimization, often by training neural networks per scene [53, 64, 66]. These methods are computationally in-efficient, typically taking hours of computing time on a high-end GPU for each object. Existing MVS methods
[20, 56, 62] that focus on computational efficiency employ feed-forward neural networks that are efficient but fail to produce high-quality details, especially in textureless re-gions. Existing MVPS approaches can produce high-quality reconstructions but require computationally inefficient per-scene training or optimization [27, 28, 29, 61]. Sometimes additional manual efforts and carefully crafted refinement steps are also needed [34, 47]. In contrast, we propose an efficient feed-forward neural architecture, MVPSNet, that can generalize to unseen objects and achieve similar re-construction quality to that of per-scene optimization tech-niques while being computationally efficient during infer-ence.
We design MVPSNet by taking inspiration from various deep MVS architectures [7, 14, 18, 20, 62] that are general-izable, computationally efficient, and can operate on high-resolution images. However, these approaches often fail in textureless regions, and their reconstructed meshes often lack details. We choose the CasMVSNet [20] architecture as our feature matching module, which has been repeatedly used by various MVS pipelines [7, 14, 18] for its simplic-ity and efficiency, and augment it to effectively incorporate lighting variation cues for better prediction of 3D shapes. In this way, we propose a feed-forward generalizable approach to Multi-view Photometric Stereo.
We introduce a multi-scale feature representation, called
Light Aggregated Feature Maps (LAFM), whose role is to extract detailed geometric features from images by uti-lizing lighting variations. For brevity, we define Multi-light Images as a collection of images taken from the same viewpoint under different directional lighting conditions.
Our intuition is that LAFM can efficiently aggregate shad-ing patterns from multi-light images, by creating an ‘artifi-cial shading texture’ in the textureless region. Multi-scale
LAFM will then be used to construct a sequence of cost volumes to match features across sparse views in order to
predict a depth map for a reference view. We also predict surface normals from LAFM for each viewpoint, enabling
LAFM to capture features related to high-frequency local details. The predicted surface normal can be used in addi-tion to the depth maps to produce a more detailed mesh than using the depth maps alone.
To train the proposed MVPSNet architecture, we intro-duce a new synthetic MVPS dataset, sMVPS dataset.
It consists of shapes from sculpture dataset [57] and random compositions of primitive shapes generated by [60]. We render these shapes with spatially varying Cook-Torrance
BRDF under different camera viewpoints and lighting di-rections. We train MVPSNet on these rendered images with ground-truth supervision over predicted depth and sur-face normal maps. The trained model generalizes to real-world test scenes from DiLiGenT-MV [34] dataset. We show that simply re-training CasMVSNet on our dataset improves reconstruction quality over the pre-trained model on DiLiGenT-MV by 32%, proving the effectiveness of our synthetic MVPS dataset for generalization.
We evaluate our approach on the only publicly available
MVPS benchmark, the DiLiGenT-MV [34] dataset. Com-pared to the state-of-the-art MVPS technique, PS-NeRF
[61], which optimizes a neural network per-scene, our pro-posed MVPSNet is ∼411× faster (105 seconds vs 12 hours) while producing similar reconstruction quality (L1 Chamfer distance of 0.82 vs 0.81, F-score on L2 distance of 0.985 vs 0.983). We further show that adding LAFM significantly improves reconstruction quality over CasMVSNet by 34% in L1 Chamfer distance. We also observe that refining the reconstructed mesh derived from depth maps with predicted surface normals from LAFM improves reconstruction qual-ity as shown in Fig 3.
In summary, the key contributions of this paper include:
• Light Aggregated Feature Maps (LAFM) that can effi-ciently utilize multi-light images to extract detailed geomet-ric features, especially in textureless regions. The surface normal predicted from LAFM also improves mesh recon-struction quality. • A synthetic MVPS dataset for training generalizable MVPS methods, which also improves Cas-MVSNet by 32%. • A fast and generalizable Multi-view
Photometric Stereo pipeline that is 411× faster while pro-ducing similar reconstruction accuracy compared to state-of-the-art per-scene optimization approach [61]. 2.