Abstract
Obtaining personalized 3D animatable avatars from a monocular camera has several real world applications in gaming, virtual try-on, animation, and VR/XR, etc. How-ever, it is very challenging to model dynamic and fine-grained clothing deformations from such sparse data. Ex-isting methods for modeling 3D humans from depth data have limitations in terms of computational efficiency, mesh coherency, and flexibility in resolution and topology. For in-stance, reconstructing shapes using implicit functions and extracting explicit meshes per frame is computationally ex-pensive and cannot ensure coherent meshes across frames.
Moreover, predicting per-vertex deformations on a pre-designed human template with a discrete surface lacks flex-ibility in resolution and topology. To overcome these limi-tations, we propose a novel method ‘NSF : Neural Surface
Fields’ for modeling 3D clothed humans from monocular depth. NSF defines a neural field solely on the base sur-face which models a continuous and flexible displacement field. NSF can be adapted to the base surface with dif-ferent resolution and topology without retraining at infer-ence time. Compared to existing approaches, our method eliminates the expensive per-frame surface extraction while maintaining mesh coherency, and is capable of reconstruct-ing meshes with arbitrary resolution without retraining. To foster research in this direction, we release our code in project page at: https://yuxuan-xue.com/nsf.
* denotes equal contribution
1.

Introduction
Human modeling is an active and challenging field of research that has applications in Computer Vision and
Graphics. Recent advancements in data acquisition tech-niques [50, 51, 63, 64, 66, 40] have opened new opportuni-ties for capturing and digitising human appearance. Build-ing digital avatars has found applications in behavioural
[12, 14] and medical [71, 55] studies as well. Our goal is to build body model which is controllable i.e., animatable with different poses, and detailed i.e. it should faithfully produce details such as garments wrinkles under different poses.
In recent years, researchers have looked into learning clothed human models from full sequences of 4D scans [8, 27, 31, 33, 52, 58]. 4D scans provide rich information about the subject appearance, but they also require exclusive tech-nology, pre-processing, and expert intervention at times, which makes this difficult to scale. A more user friendly line relies on the input with monocular depth from devices such as Kinects [6, 13, 22, 67, 68]. Such data is easier to ob-tain and already supported by consumer-grade devices. But this flexibility comes at the cost of additional sensor noise, thus complicating the learning process.
To mitigate the noise in input data, parametric models such as SMPL [28] and its successors [1, 4, 45, 65, 3], can provide a good statistical prior for capturing pose and the overall shape of the person. Also, relying on a tem-plate naturally supports information transfer across subjects and poses. However, designing a pipeline around a spe-cific template restricts the expressivity of the model, which makes the methods less flexible (e.g., limited to tight gar-ments). A common representation to relax the topology constraints is point clouds [27, 31, 33, 70]. Recently, point based neural implicit representations [8, 13, 52, 58, 60, 2] demonstrated incredible expressive power. But many real applications (e.g., animation, texture transfer) require a 3D mesh. Hence, these approaches require running costly algo-rithms [29, 21] to reconstruct a supporting surface. Extract-ing a surface for every frame causes a computational burden and also results in inconsistent triangulations, which further complicate downstream tasks. Some works [6, 22] address this issue by predicting displacements on SMPL vertices for modeling clothed humans. While these methods yield co-herent mesh reconstruction, they are constrained by the res-olution and topology of SMPL template.
We pose ourselves the following goal: starting only from a set of partial shapes from monocular depth frames, can we learn a clothed body model that is flexible and coherent across different frames, with a limited computational cost for surface extraction?
To this end, we propose NSF : Neural Surface Fields; a neural field defined continuously all over the surface.
Given a canonical shape, represented with an implicit func-tion, we use NSF to define a continuous field over the sur-face, capable of modeling detailed deformations. Using
NSF, we can reconstruct a coherent mesh in the canoni-cal space at any resolution with just one run of surface extraction algorithms, and share it across all the different poses. This formulation avoids per-frame surface extrac-tion which is ∼ 40x and ∼ 180x faster compared to point-based works [27, 31, 33, 70] using Poisson reconstruction and implicit-based works [8, 13, 52, 58, 60] using march-ing cube at similar resolution, respectively. After training,
NSF can be adapted to arbitrary resolutions at inference time, depending on the application. This step is possible since NSF is continuously defined all over the surface, and hence it is able to support any discretization. Compared to other feature representations, NSF is more compact, saving 97.4% of memory compared to volumetric representation and 86.0% compared to triplane features at 1283 resolution.
We validate our self-supervised approach on several datasets [6, 22, 26, 32, 47], showing better performance than competitors, even when some of them requires subject-specific training [6, 13, 33, 41, 42, 60]. We show the practi-cal benefits of NSF in shape reconstruction, animation, and texture transfer application, with the flexibility and the co-herency that is not attainable for prior works [6, 13].
In summary, our contributions can be summarized as:
• We propose NSF : Neural Surface Fields; a continu-ous neural field defined over the surface in a canonical space which is compact, efficient, and supports arbi-trary mesh discretizations without retraining.
• We propose a method to learn an animatable human avatar from a monocular depth sequence; NSF let us recover detailed shape information from monocular depth frames. Our self-supervised approach handles subjects with different clothing geometries and tex-tures. To the best of our knowledge, NSF is the first work in avatarization which directly output mesh at arbitrary resolution while maintaining the coherency across different poses. 2.