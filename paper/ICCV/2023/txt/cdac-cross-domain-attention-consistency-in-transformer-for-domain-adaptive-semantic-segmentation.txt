Abstract
While transformers have greatly boosted performance in semantic segmentation, domain adaptive transformers are not yet well explored. We identify that the domain gap can cause discrepancies in self-attention. Due to this gap, the transformer attends to spurious regions or pixels, which deteriorates accuracy on the target domain. We propose
Cross-Domain Attention Consistency (CDAC), to perform adaptation on attention maps using cross-domain attention layers that share features between source and target do-mains. Specifically, we impose consistency between predic-tions from cross-domain attention and self-attention mod-ules to encourage similar distributions across domains in both the attention and output of the model, i.e., attention-level and output-level alignment. We also enforce consis-tency in attention maps between different augmented views to further strengthen the attention-based alignment. Com-bining these two components, CDAC mitigates the discrep-ancy in attention maps across domains and further boosts the performance of the transformer under unsupervised do-main adaptation settings. Our method is evaluated on var-ious widely used benchmarks and outperforms the state-of-the-art baselines, including GTAV-to-Cityscapes by 1.3 and 1.5 percent point (pp) and Synthia-to-Cityscapes by 0.6 pp and 2.9 pp when combining with two competi-tive Transformer-based backbones, respectively. Our code will be publicly available at https://github.com/ wangkaihong/CDAC. 1.

Introduction
The Transformer model has shown remarkable perfor-mance on various computer vision tasks (e.g., [7, 2, 42, 44]) and often exhibits an outstanding prediction capacity com-pared to convolutional networks. Meanwhile, it is also rel-atively “data-hungry” and therefore expects a large amount of training data in order to achieve strong performance [29].
However, curating a large-scale annotated dataset could be
Figure 1: Attention map visualization for the query pixel ☆ in source and target domains from different methods. The attention maps on the source image tend to highlight regions sharing similar semantic classes. However, the attention maps on the target image from Source-only and prior work focus on spurious regions (e.g., left red boxes), which can be caused by a domain gap. In comparison, CDAC encourages attention-level adaptation and learns from more diverse and informative signals from both domains. a prohibitively expensive engineering task, especially for those problems that require pixel-level labeling, including semantic segmentation. Furthermore, deep models often generalize poorly to new domains such as different cities or weather in driving scenes. To overcome these issues, the use of unsupervised domain adaptation (UDA) has been pro-posed. UDA allows knowledge transfer from synthetic data (source domain), where pixel-level annotations are more cheaply available, to real-world data (unlabeled target do-main). Recently, under the UDA setting for semantic seg-mentation, a Transformer-based method (DAFormer [13]) outperforms previous CNN-based UDA methods across di-verse benchmark datasets.
The key component for the success in Transformer is self-attention, which learns to attend to certain regions of its input that could be informative to predicting the seman-tic class of a pixel. However, it is still not clear if the do-main gap is completely resolved under the UDA setting.
We found that the attention maps from self-attention on the target images can focus on spurious regions and there-fore fails to assist the prediction on the target domain as in
Fig. 1, which suggests that the domain gap still remains.
We aim to improve the robustness of self-attention by using cross-domain attention, which computes attention scores across different domain images. Previous work [43] ex-plores cross-domain attention in image classification. How-ever, this method requires a sophisticated search for finding positive cross-domain pairs, which may not be directly ap-plicable to semantic segmentation.
To more effectively mitigate the domain discrepancy and improve the robustness of Transformers for semantic seg-mentation, we propose our cross-domain prediction consis-tency loss to encourage consistency in the prediction (i.e., output-level) and attention map (i.e., attention-level). To be more specific, we at first compute predictions based on self-attention and cross-domain attention respectively. Then we supervise the predictions based on the self-attention mod-ule and the cross-domain attention module with the same label from either the source domain label or the target do-main pseudo-label. The benefits of this design are twofold: firstly, the consistency allows attention-level alignment that helps pull the distributions of the self-attention and cross-domain attention closer as in Fig. 1. Secondly, the oper-ation also acts as a perturbation and regularization on the attention maps that encourage output-level domain align-ment, i.e., facilitates consistent predictions when the atten-tion maps differ due to the different input query vectors. In the end, the model benefits from both the alignment at the attention level between the self-attention and cross-domain attention as well as the more robust predictions on the out-put level.
Inspired by consistency learning [28, 9], enforced on the output of models through different augmentations of an image, we propose our cross-domain attention consis-tency loss to learn attention-level consistency and induce a model to generate robust attention maps. The idea is rather straightforward: The attention maps from two differ-ent augmented views from the same image should always be consistent. With this objective, the model can learn to predict more consistent attention maps on both source and target images without supervision and further contribute to attention-level domain alignment.
Combining our cross-domain prediction consistency loss and cross-domain attention consistency loss, CDAC not only facilitates alignment on the output level but also brings the alignment to the attention level and improves the seg-mentation accuracy under UDA settings. To summarize, our key contributions in this work include: 1. We introduce a cross-domain prediction consistency loss that encourages robust attention as well as predic-tion across domains and thus helps with the attention-level and output-level domain alignment; 2. We propose to enforce attention-level consistency via our cross-domain attention consistency loss to further assist the alignment of attention-level discrepancy; 3. CDAC is verified in extensive experiments to be effective, reliable and generalizable under different scenarios and achieves state-of-the-art performance important UDA semantic segmentation on several benchmarks including GTAV-to-Cityscapes, Synthia-to-Cityscapes and Cityscapes-to-ACDC. 2.