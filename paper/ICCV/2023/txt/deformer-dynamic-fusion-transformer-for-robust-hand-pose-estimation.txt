Abstract
Accurately estimating 3D hand pose is crucial for under-standing how humans interact with the world. Despite re-markable progress, existing methods often struggle to gen-erate plausible hand poses when the hand is heavily oc-cluded or blurred. In videos, the movements of the hand al-low us to observe various parts of the hand that may be oc-cluded or blurred in a single frame. To adaptively leverage the visual clue before and after the occlusion or blurring for robust hand pose estimation, we propose the Deformer: a framework that implicitly reasons about the relationship be-tween hand parts within the same image (spatial dimension) and different timesteps (temporal dimension). We show that a naive application of the transformer self-attention mech-anism is not sufficient because motion blur or occlusions in certain frames can lead to heavily distorted hand fea-tures and generate imprecise keys and queries. To address this challenge, we incorporate a Dynamic Fusion Module into Deformer, which predicts the deformation of the hand and warps the hand mesh predictions from nearby frames to explicitly support the current frame estimation. Further-more, we have observed that errors are unevenly distributed across different hand parts, with vertices around fingertips having disproportionately higher errors than those around the palm. We mitigate this issue by introducing a new loss function called maxMSE that automatically adjusts the weight of every vertex to focus the model on critical hand parts. Extensive experiments show that our method signifi-cantly outperforms state-of-the-art methods by 10%, and is more robust to occlusions (over 14%). 1.

Introduction
Accurately estimating hand poses in the wild is a chal-lenging task that is affected by various factors, including ob-ject occlusion, self-occlusion, motion blur, and low camera exposure. To address these challenges, temporal informa-tion can be leveraged using a self-attention mechanism [52] that reasons the feature correlation between adjacent frames to generate better hand pose estimations. However, as the generation of key and query vectors depends on per-frame
Figure 1: Given a video where in some frames (left) the hand is heavily occluded or blurred, the existing state-of-the-art video-based method TCMR [10] (middle) fails to predict accurate hand poses. Our method (right) is able to capture the hand dynamics and leverage neighborhood frames to robustly produce plausible hand pose estimations. features, heavy occlusion or blurring can significantly con-taminate them, resulting in inaccurate output. Another chal-lenge of hand pose estimation is that the fingertips, located at the periphery of the hand, are more prone to occlusion and have more complex motion patterns, making them par-ticularly challenging for the model to estimate accurately.
To tackle the aforementioned challenges, we propose
Deformer: a dynamic fusion transformer that leverages nearby frames to learn hand deformations and assemble multiple wrapped hand poses from nearby frames for a ro-bust hand pose estimation in the current frame. Deformer implicitly models the temporal correlations between adja-cent frames and automatically selects frames to focus on.
To mitigate the error imbalance issue, we design a novel loss function, called maxMSE, that emphasizes the impor-tance of difficult-to-estimate vertices and provides a more balanced optimization.
Given a sequence of frames, our approach first uses a shared CNN to extract frame-wise hand features. To rea-son the non-local relationships between hand parts within a single frame, we use a shared spatial transformer that out-puts an enhanced per-frame hand representation. Then, we leverage a global temporal transformer to attend to frame-wise hand features by exploring their correlations between different timestamps and forward the enhanced features to a shared MLP to regress the MANO hand pose parameters.
To ensure consistency of the hand shape over time, we pre-dict a global hand shape representation from all frame-wise features through the cross-attention mechanism. Despite incorporating temporal attention, the model may struggle to accurately predict hand poses in frames where the hand is heavily occluded or blurred. To address this issue, we introduce a Dynamic Fusion Module, which predicts a tu-ple of forward and backward hand motion for each frame, deforming the hand pose to the previous and next times-tamps. This allows the model to leverage nearby frames with clear hand visibility to assist in estimating hand poses in occluded or blurred frames. Finally, the set of deformed hand poses of each timestamp is synthesized into the final output with implicitly learned confidence scores. In opti-mization, we found that standard MSE loss leads to imbal-anced errors between different hand parts. Specifically, we observed that the model trained with MSE loss had lower errors for vertices around the palm and larger errors for the vertices around the fingertips, which are more prone to oc-clusions and have more complex motion patterns. Inspired by focal loss [35] in the classification task, we introduce maxMSE, a new loss function that maximizes the MSE loss by automatically adding a weight coefficient to every hand vertex. The maxMSE allows the model to focus on critical hand parts like fingertips and achieve better overall perfor-mance.
Experimental results on two large-scale hand pose esti-mation video datasets, DexYCB [8] and HO3D [17], show the proposed method achieves state-of-the-art performance and is more robust to occlusions.
In summary, our con-tributions include: (1) a Deformer architecture for robust and plausible 3D hand pose estimation from videos; (2) a novel dynamic fusion module that explicitly deforms nearby frames with clear hand visibility for robust hand pose estimation in occluded or blurred frames; (3) a new maxMSE loss function that focuses the model on critical hand parts. 2.