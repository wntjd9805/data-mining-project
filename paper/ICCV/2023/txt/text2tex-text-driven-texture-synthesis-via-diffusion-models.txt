Abstract
We present Text2Tex, a novel method for generating high-quality textures for 3D meshes from the given text prompts.
Our method incorporates inpainting into a pre-trained depth-aware image diffusion model to progressively syn-thesize high resolution partial textures from multiple view-points. To avoid accumulating inconsistent and stretched artifacts across views, we dynamically segment the rendered view into a generation mask, which represents the gener-ation status of each visible texel. This partitioned view representation guides the depth-aware inpainting model to generate and update partial textures for the corresponding regions. Furthermore, we propose an automatic view se-quence generation scheme to determine the next best view for updating the partial texture. Extensive experiments demonstrate that our method significantly outperforms the existing text-driven approaches and GAN-based methods. 1.

Introduction
Generating high-quality 3D content is an essential com-ponent of visual applications in films, games, and upcom-ing AR/VR scenarios. With an increasing number of 3D content datasets, the computer vision community has wit-nessed significant progress in the field of 3D geometry gen-eration [12, 39, 61, 66, 38]. Despite the remarkable success in modeling 3D geometries in recent years, fully automatic 3D content generation is still hindered by the laborious hu-man efforts required to design textures. Therefore, automat-ing the texture design process through alternative guidance, such as text, has become an intriguing but challenging re-search problem.
Recently, text-to-image generators have shown remark-able progress in the 2D domain leveraging diffusion model architectures, enabling high resolution 2D content genera-tion based on textual descriptions [1, 48]. However, there are notable challenges for producing 3D textures via such 2D vision-language prior knowledge. Specifically, the syn-thesized textures are expected to be not only with high fi-delity to the language cues, but also of high and consis-tent quality for target meshes. As such, previous attempts to paint 3D geometry from text inputs often fail to deliver well-textured 3D content.
In this paper, we introduce Text2Tex, a novel tex-ture synthesis method that seamlessly texturizes 3D ob-jects using a pre-trained depth-aware text-to-image diffu-sion model. The method renders a target mesh from multi-ple viewpoints and inpaints the missing appearance with a
depth-aware text-to-image diffusion model. Text2Tex fol-lows a generate-then-refine strategy. Our method progres-sively generates partial textures across viewpoints and back-projects them to texture space. To address stretched and inconsistent artifacts observed from rotated viewpoints, we design a view partitioning technique that computes similar-ity maps between visible texel’s normal vectors and the cur-rent view direction. The generation mask created from these similarity maps guides the diffusion process by indicating regions to generate, update, keep, or ignore. This allows us to apply different diffusion strengths to respective regions, inpainting missing appearance and updating stretched arti-facts. However, the autoregressive generation process via the diffusion-based image inpainting model presents a new challenge. As the inpainting and updating scheme is con-ditioned on previously synthesized results, a viewpoint se-quence with an ill-defined order or incomplete coverage over the mesh surface may result in unsatisfactory textur-ization. Therefore, we propose an automatic viewpoint se-lection technique that progressively selects the next best view. The confidence of each candidate view containing the biggest relative area for generation and updating is esti-mated, given the partially textured mesh. This approach en-sures complete coverage over the mesh surface and a high-quality texture map by consistently updating stretched re-gions.
We demonstrate the effectiveness of Text2Tex for syn-thesizing high-quality 3D textures from language cues.
The proposed method performs favorably against other language-based texture synthesis methods in terms of
FID [23], KID [4], and user study on a subset of the
Objaverse dataset [17]. Additionally, our method also outperforms category-specific GAN-based methods on the
ShapeNet car dataset [7].
To summarize, our technical contributions are threefold:
• We design a novel method for high-quality texture syn-thesis by progressively inpainting and updating the 3D textures via depth-aware diffusion models.
• We propose an automatic view sequence generation scheme to dynamically determine the order for gen-erating and updating the texture space.
• We conduct extensive study on a considerable amount of 3D objects, demonstrating the proposed method is effective for large-scale 3D content generation. 2.