Abstract
We propose XVO, a semi-supervised learning method for training generalized monocular Visual Odometry (VO) models with robust off-the-self operation across diverse datasets and settings.
In contrast to standard monocu-lar VO approaches which often study a known calibration within a single dataset, XVO efficiently learns to recover relative pose with real-world scale from visual scene se-mantics, i.e., without relying on any known camera param-eters. We optimize the motion estimation model via self-training from large amounts of unconstrained and hetero-geneous dash camera videos available on YouTube. Our key contribution is twofold. First, we empirically demon-strate the benefits of semi-supervised training for learn-ing a general-purpose direct VO regression network. Sec-ond, we demonstrate multi-modal supervision, including segmentation, flow, depth, and audio auxiliary prediction tasks, to facilitate generalized representations for the VO task. Specifically, we find audio prediction task to signifi-cantly enhance the semi-supervised learning process while alleviating noisy pseudo-labels, particularly in highly dy-namic and out-of-domain video data. Our proposed teacher network achieves state-of-the-art performance on the com-monly used KITTI benchmark despite no multi-frame opti-mization or knowledge of camera parameters. Combined with the proposed semi-supervised step, XVO demonstrates off-the-shelf knowledge transfer across diverse conditions on KITTI, nuScenes, and Argoverse without fine-tuning. 1.

Introduction
Monocular Visual Odometry (VO) methods for recov-ering ego-motion from a sequence of images have mostly been studied within a restricted scope, where a single dataset, such as KITTI [28], may be used for both training and evaluation under a fixed pre-calibrated camera [37, 45, 54, 77, 109, 112, 116, 124, 126, 128]. However, very few
∗ Equally contributed.
Figure 1: Learning General-Purpose Monocular Vi-sual Odometry (VO) Models from Multi-Modal and
Pseudo-Labeled Videos. Our proposed XVO framework first trains an ego-motion prediction teacher model over a small initial dataset, e.g., nuScenes [7]. We then expand the original dataset through pseudo-labeling of in-the-wild videos. Motivated by how humans learn general repre-sentations through observation of large amounts of multi-modal data, we employ multiple auxiliary prediction tasks, including segmentation, flow, depth, and audio, as part of the semi-supervised training process. Finally, we lever-age uncertainty-based filtering of potentially noisy pseudo-labels and train a robust student model. studies have analyzed the task of generalized VO, i.e., rel-ative pose estimation with real-world scale across differing scenes and capture setups.
For instance, consider an autonomous robot or vehicle deployed at a large scale. The robot is highly likely to en-counter environments for which no ground truth ego-motion data was previously collected. In such novel settings, cur-rent VO methods will quickly exhibit poor ego-motion esti-mation and drift [23, 24, 29, 46, 47, 65, 96, 128]. Moreover, our robot may be required to adjust its camera setup over its lifetime (e.g., to a new camera) or leverage data from a fleet of robots with varying or perhaps unknown camera config-urations. Yet, existing VO methods generally assume care-fully calibrated camera parameters during training [23, 24, 65, 77, 96, 109, 128]. Specifically, to simplify the ill-posed monocular pose recovery task, researchers often resort to relying on knowledge of the camera intrinsic parameters to incorporate various geometric or photometric consistency-based mechanisms [37, 45, 54, 89, 112, 116, 124, 126]. In this work, we do not make such an assumption as we are concerned training VO models that can learn from and oper-ate under diverse unconstrained videos in the wild. Specifi-cally, we pursue an orthogonal direction to prior work based on semi-supervised learning and explore more scalable and camera-agnostic deployment settings.
Our key hypothesis is that neural network models can learn to circumvent issues related to pose and scale ambigu-ity in generalized VO settings through observation of ample amounts of diverse scene and motion video data. This ap-proach is motivated by humans’ ability to flexibly estimate motion in arbitrary conditions through a general under-standing of salient scene properties (e.g., object sizes) [73].
This general understanding is developed over large amounts of perceptual data, often multi-modal in nature [71, 81, 81].
For instance, cross-modal information processing between audio and video has been shown to play a role in spatial reasoning and proprioception [57, 58, 67, 74, 99]. Indeed, collected online videos often have audio, which can be used as a further source of cross-modal supervision. As further discussed in Sec. 3, we find ambient audio to correlate at times with scenarios where monocular VO tends to fail, such as determining ego-speed when the vehicle is stopped at a dense intersection or as context for the current traffic scenario when estimating translation (e.g., highway driv-ing). Extracting information related to flow, segmentation, or depth can also further guide learning generalized repre-sentations. To fully explore the utility of self-training VO models, we analyze a unified multi-modal framework and its impact on guiding semi-supervised VO learning from large amounts of unconstrained sources.
As far as we are aware, we are the first to study the feasibility of self-training for direct, calibration-free, ego-motion pose regression with an absolute real-world scale.
Specifically, we find that incorporating additional modali-ties via simple multi-task learning can significantly enhance model robustness and generalization. When paired with an uncertainty-based filtering module, we achieve state-of-the-art VO performance with a single broadly usable model which we validate for the autonomous driving use-case. Moreover, our training and inference is highly effi-cient as the auxiliary learning formulation does not alter the two-frame input, i.e., in contrast to methods relying on ex-tracting rich intermediate representations [4, 96, 112, 124].
We demonstrate state-of-the-art results on KITTI using the proposed two-frame VO model structure without requiring elaborate long-term memory, computationally expensive it-erative refinement steps, or knowledge of camera param-eters. Our code is available at https://github.com/h2xlab/
XVO. 2.