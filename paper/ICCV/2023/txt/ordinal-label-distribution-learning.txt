Abstract
Label distribution learning (LDL) is a recent hot topic, in which ambiguity is modeled via description degrees of the labels. However, in common LDL tasks, e.g., age esti-mation, labels are in an intrinsic order. The conventional
LDL paradigm adopts a per-label manner for optimization, neglecting the internal sequential patterns of labels. There-fore, we propose a new paradigm, termed ordinal label dis-tribution learning (OLDL). We model the sequential pat-terns of labels from aspects of spatial, semantic, and tempo-ral order relationships. The spatial order depicts the rela-tive position between arbitrary labels. We build cross-label transformation between distributions, which is determined by the spatial margin in labels. Labels naturally yield differ-ent semantics, so the semantic order is represented by con-structing semantic correlations between arbitrary labels.
The temporal order describes that the presence of labels is determined by their order, i.e. ﬁve after four. The value of a particular label contains information about previous labels, and we adopt cumulative distribution to construct this rela-tionship. Based on these characteristics of ordinal labels, we propose the learning objectives and evaluation metrics for OLDL, namely CAD, QFD, and CJS. Comprehensive experiments conducted on four tasks demonstrate the supe-riority of OLDL against other existing LDL methods in both traditional and newly proposed metrics. Our project page can be found at https://downdric23.github.io/. 1.

Introduction
In the common machine learning paradigms, single-label learning [27] predicts one speciﬁc label for an instance.
Multi-label learning [58] predicts multiple labels which can handle some ambiguous cases where an instance is related to more than one class. However, it treats each label equally and only assigns the same degree to the related classes.
To tackle this problem, Geng [19] proposes label distribu-tion learning (LDL). This new paradigm models the dif-ferent relative importance between labels for describing an instance. Due to the characteristics of LDL [19], it has l e b a l
-r e p l e b a l
-y b
-l e b a l (a) Image
Metric (gt,pd-1) (gt,pd-2)
KL 0.1379 0.1379
Canberra 1.7293 1.7293
Cosine 0.2638 0.2638 (cid:1839)(cid:3004)(cid:3002)(cid:3005) (cid:1839)(cid:3018)(cid:3007)(cid:3005) (cid:1839)(cid:3004)(cid:3011)(cid:3020) 0.3670 0.0217 0.1275 0.1125 0.0768 0.0434 (c) Metrics (b) Examples for order-sensitive distributions
Figure 1. An example of image aesthetics rating using scores from 1 to 5. There exist obvious order relations in the label space. (a) shows an example of the image. In (b), compared with prediction-1 (pd-1), prediction-2 (pd-2) obviously ﬁts better to the ground truth label distribution. However, they both have equal perfor-mance on the widely-used distances metrics (KL, Canberra, and
Cosine) as shown in (c). Our order-sensitive metrics including
MCAD, MQF D, and MCJS can reﬂect the difference between the two predictions. been widely used to solve various real-world ambiguous tasks [57, 66, 74, 34], including age estimation [17, 28], image aesthetics analysis [62], etc. Speciﬁcally, LDL al-gorithms assign a number dy x less than or equal to 1 to the potential label y for describing an instance x. Note that (cid:2) x = 1 is held in order to satisfy the condition that y dy label space is complete.
Generally, LDL tries to learn accurate results at each label. However, in some common LDL tasks, such as age estimation [47] and beauty rating [65], labels are in order. The natural order of labels presents a sequential pattern and is different from other LDL tasks like facial
expression recognition.
In this case, existing algorithms in LDL become insufﬁcient to fully explore the distribu-tion difference in OLDL tasks. Concretely, LDL algo-rithms [19, 60, 48, 49, 23, 37] take mean squared error (MSE), mean absolute error (MAE), and Kullback-Leibler (KL) divergence [7] as learning objectives. These methods mainly concentrate on the distance between distributions at the same labels (per-label) and didn’t utilize the implicit orders between them (label-by-label). As demonstrated in
Fig. 1, while measuring the prediction of image aesthetics scores for the image in (a), prediction-2 is relatively closer to the ground truth distribution shown in (b). However, prediction-1 and prediction-2 have identical performance when evaluated with the widely used per-label LDL metrics in (c). Meanwhile, there are some previous works study-ing ordinal labels. Li et al. [38] makes a unimodal assump-tion in age predictions based on the distance relationship be-tween ages. To process sequential words in the text, many works [54, 9] take semantic relations between words into account. Time series analysis [31] is a prevalent direction to explore ordinal information. These theories further in-spire the exploration of sequential patterns in labels.
In this paper, we propose a new paradigm, termed or-dinal label distribution learning (OLDL). Following the in-spiration above, we model the sequential patterns in labels from three aspects: spatial, semantic, and temporal order relationship. 1) From the perspective of spatial order, the sequential pattern in labels describes whether the label is adjacent or distant from the others [39]. Thus, the spa-tial order can be naturally modeled via distance (margin).
In most cases, the labels in OLDL are real numbers, such as scores [44]. So, the margin between label i and label j is reasonably formulized as |i − j|. Based on the margin, we introduce the Cumulative Absolute Distance (CAD). It aims to ﬁnd the minimal cost of transforming one distri-bution to another label-by-label. 2) From the perspective of semantic order, we transform the sequential pattern be-tween labels into semantic similarities. Motivated by the nearby labels are also close in the semantic space, we de-sign an order-sensitive matrix and integrate it into the cal-culation of Quadratic Form Distance (QFD) [1]. Each el-ement in this matrix represents the semantic similarity be-tween pairwise labels, which indicates their relations in or-der. 3) From the perspective of temporal order, because the cumulative density function can cumulate distributions in sequence [45], it intrinsically contains temporal orders be-tween labels. Moreover, based on the divergence that can reﬂect the difference between two distributions, we pro-pose to use the information theory-based divergence mea-sure, termed Cumulative Jensen-Shannon divergence (CJS) in OLDL.
Distances from these methods are directly adopted as learning objectives. Further, we adopt these three distances from the algorithms as new order-sensitive metrics, denoted as MCAD, MQF D, and MCJS. These metrics are suggested to better assess the prediction results in OLDL.
The contributions of the paper are three-fold:
• We propose a novel paradigm for label distribution learning, termed OLDL, in which the sequential pat-terns of labels are fully explored to further boost
OLDL tasks.
• We explore the sequential patterns of labels from three aspects: spatial order, semantic order and temporal or-der. The corresponding orders are modeled as mar-gins, semantic similarities, and distribution cumula-tion. Distances CAD, QFD, and CJS are derived from the methods.
• We conduct comprehensive experiments on ﬁve widely used datasets of four vision tasks. Evaluated by both the existing and newly proposed metrics, the results demonstrate the superiority of the proposed OLDL paradigm. 2.