Abstract
Gestures are non-verbal but important behaviors accom-panying people’s speech. While previous methods are able to generate speech rhythm-synchronized gestures, the semantic context of the speech is generally lacking in the gesticula-tions. Although semantic gestures do not occur very regularly in human speech, they are indeed the key for the audience to understand the speech context in a more immersive envi-ronment. Hence, we introduce LivelySpeaker, a framework
* Equal contribution.
† Corresponding author. that realizes semantics-aware co-speech gesture generation and offers several control handles. In particular, our method decouples the task into two stages: script-based gesture gen-eration and audio-guided rhythm refinement. Specifically, the script-based gesture generation leverages the pre-trained
CLIP text embeddings as the guidance for generating ges-tures that are highly semantically aligned with the script.
Then, we devise a simple but effective diffusion-based ges-ture generation backbone simply using pure MLPs, that is conditioned on only audio signals and learns to gesticu-late with realistic motions. We utilize such powerful prior to rhyme the script-guided gestures with the audio signals, no-tably in a zero-shot setting. Our novel two-stage generation framework also enables several applications, such as chang-ing the gesticulation style, editing the co-speech gestures via textual prompting, and controlling the semantic aware-ness and rhythm alignment with guided diffusion. Extensive experiments demonstrate the advantages of the proposed framework over competing methods. In addition, our core diffusion-based generative model also achieves state-of-the-art performance on two benchmarks. The code and model will be released to facilitate future research. 1.

Introduction
During human conversation, non-verbal behaviors are typically present and among them, the most significant is gesture language. These non-linguistic gestures serve as an auxiliary but effective means of conveying key messages, enriching the conversation with contextual cues, and facilitat-ing better understanding among participants. [9, 18, 24, 27].
Empowering the digital replicas of humans with the abil-ity to gesticulate has been a long pursuit in the research community, as such ability can benefit many downstream applications, including digital humans in the coming virtual universe, non-player game characters, robot assistants, etc.
Given the speech content in the form of texts and/or au-dio streams, the objective is to generate realistic co-speech gestures. Traditional methods achieve this with hard-coded rules [10,11,25,41], e.g., “good” in the speech will be simply represented by the gesture “thump up”. However, these meth-ods usually produce deterministic results; more importantly, they can not guarantee smooth transitions in the results. Re-cently, deep learning-based methods have been prevalent in the field of gesture generation from multi-modality in-puts. In particular, these methods formulate the problem as conditional motion generation and tackle it via training a conditional generative model that takes as input the speaker identities [17], audio waves [46], speech texts [8], or a com-bination of these multi-modal signals [3, 40, 58]. Although multiple modalities are incorporated in the formulation, the results are often dominated by the rhythm of the audio sig-nal since it is highly correlated with the performance of gestures during speech. While other works recognize the importance of the semantics conveyed through co-speech gestures, their framework heavily depends on pre-defined gesture types [7, 37] or keywords [57], making it difficult to express more complex intentions effectively.
We begin with insights from the following two perspec-tives: (i) Real-world human conversations contain a limited number of semantic gestures (See Fig. 2), which presents difficulties in learning co-speech gestures that are semantic-sensitive but rhythm-irrelevant. This partially explains why prior approaches have yielded results that heavily rely on the audio rhythm. (ii) Most previous methods are built on gen-Figure 2. We plot the L2 loss histogram on the training samples of the pre-trained trimodal method [58]. Although it is learned from multiple conditions (e.g., text, audio), their methods are still domi-nated by repeated rhythm, and hard to model the rarely appeared diversity gestures, e.g., the semantic-aware motions. erative adversarial networks (GANs), which might be hard to train, especially when learning a many-to-many mapping between the text/audio and the gesture [46].
Following this, we present LivelySpeaker, a simple and effective framework for semantic-aware co-speech gesture generation. In particular, our framework explicitly decou-ples the generation into two stages, namely the script-based gesture generation, and the audio-guided rhythm refinement.
Specifically, the script-based gesture generation leverages the pre-trained CLIP [47] text embeddings as the guidance for generating gestures that are highly semantically corre-lated with the textual script. In the second stage, we devise a simple but effective diffusion-based gesture generation backbone with pure MLPs, that is conditioned on only audio signals and learns to gesticulate with realistic motions. We utilize such powerful prior to rhyme the script-guided ges-tures with the audio signals, notably in a zero-shot setting.
In detail, we gradually add Gaussian noise for T steps to the motion extracted from the dataset, on which an MLP-based [20] motion denoising model [53] is conditioned on the corresponding audio and predicts the clean motion. We show that this diffusion-based model is effective in rhyming rather smooth gestures produced from the script-based gen-eration module with the audio signals.
Building upon these two powerful modules, our method can generate diverse and high-quality co-speech gestures that are semantically meaningful, given the textual description of the speech and audio. Extensive experiments show that the proposed framework yields state-of-the-art performance in co-speech gesture generation. We also conduct experiments to show the control ability of our method by extending it to a number of scenarios that are not possible with competing methods, including changing the gesticulation style, editing the co-speech gestures via textual prompting, and controlling the semantic awareness and rhythm alignment with guided
diffusion.
The main contribution of this paper is summarised:
• We propose LivelySpeaker, a novel two-stage frame-work for semantic-aware and rhythm-aware co-speech gesture generation.
• A novel MLP-based diffusion-based backbone is de-vised, that achieves state-of-the-art performance on the two benchmarks for co-speech generation.
• Our framework enables several new applications in co-speech gesture generation, such as text prompt-based gesture control, balancing the control between two dif-ferent condition modalities (i.e., text and audio). 2.