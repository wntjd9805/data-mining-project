Abstract
In this paper, we propose PETRv2, a uniﬁed frame-work for 3D perception from multi-view images. Based on
PETR [25], PETRv2 explores the effectiveness of temporal modeling, which utilizes the temporal information of previ-ous frames to boost 3D object detection. More speciﬁcally, we extend the 3D position embedding (3D PE) in PETR for temporal modeling. The 3D PE achieves the temporal alignment on object position of different frames. To support for multi-task learning (e.g., BEV segmentation and 3D lane detection), PETRv2 provides a simple yet effective solution by introducing task-speciﬁc queries, which are initialized under different spaces. PETRv2 achieves state-of-the-art performance on 3D object detection, BEV segmentation and 3D lane detection. Detailed robustness analysis is also con-ducted on PETR framework. Code is available at https:
//github.com/megvii-research/PETR. 1.

Introduction
Recently, 3D perception from multi-camera images for autonomous driving system has drawn a great attention.
The multi-camera 3D object detection methods can be di-vided into BEV-based [13, 12] and DETR-based [43, 25, 21] approaches. BEV-based methods (e.g., BEVDet [13]) explicitly transform the multi-view features into bird-eye-† Equal Contribution. ✉ Corresponding author. view (BEV) representation by LSS [34]. Different from these BEV-based countparts, DETR-based approaches [43] models each 3D object as an object query and achieve the end-to-end modeling with Hungarian algorithm [17].
Among these methods, PETR [25], based on DETR [4], converts the multi-view 2D features to 3D position-aware features by adding the 3D position embedding (3D PE). The sparse queries, initialized from 3D space (see Fig. 1(a)), can directly perceive the 3D object information by interacting with the produced 3D position-aware features.
Recently, several works [21, 12] explore the effective-ness of temporal cues to improve the speed estimation of surrounding objects. Except for the 3D object detection, some followers [50, 21] further extend the BEV represen-tation for multi-task learning, e.g. BEV segmentation and motion prediction. Considering the simplicity of PETR, we wonder if it is possible for the sparse-query paradigm in
PETR to follow the success of BEV-based methods. In this paper, we aim to build a strong and uniﬁed framework by extending the PETR with temporal modeling and the sup-port for multi-task learning.
For temporal modeling, the main problem is how to align the object position of different frames in 3D space. Existing works [12, 21] solved this problem from the perspective of feature alignment. For example, BEVDet4D [12] explicitly aligns the BEV feature of previous frame with current frame by pose transformation. However, PETR implicitly encodes the 3D position into the 2D image features and fails to per-form the explicit feature transformation. Since PETR has
demonstrated the effectiveness of 3D PE (encoding the 3D coordinates into 2D features) in 3D perception, we wonder if 3D PE still works on temporal alignment. In PETR, the meshgrid points of camera frustum space, shared for differ-ent views, are transformed to the 3D coordinates by camera parameters. The 3D coordinates are then input to a sim-ple multi-layer perception (MLP) to generate the 3D PE. In our practice, we ﬁnd that PETR works well under temporal condition by simply aligning the 3D coordinates of previous frame with the current frame.
For multi-task learning, BEVFormer [21] provides a uni-ﬁed solution with dense BEV representation. It deﬁnes each point on BEV map as one BEV query. Thus, the BEV query can be employed for both 3D object detection and BEV seg-mentation. However, the number of BEV query tends to be huge especially when BEV map is large for long-distance perception. Such deﬁnition on object query is obviously not suitable for sparse-query design in PETR due to the global attention transformer.
In this paper, we design a uniﬁed sparse-query solution for multi-task learning. For different tasks, we deﬁne sparse task-speciﬁc queries under different spaces. For example, the lane queries for 3D lane detection are deﬁned in 3D space with the style of anchor lane while seg queries for BEV segmentation are initialized under the
BEV space, as shown in Fig. 1(b-c). Those sparse task-speciﬁc queries are input to the same transformer to update their representation and further injected into different task-speciﬁc heads to produce high-quality predictions.
Besides, we also improve the generation of 3D PE and provide a detailed robustness analysis on PETRv2. As men-tioned above, 3D PE in PETR is generated based on the
ﬁxed meshgrid points in camera frustum space. All images from one camera view share the 3D PE, making 3D PE data-independent. In this paper, we further improve the original 3D PE by introducing a feature-guided way. Concretely, the projected 2D features are ﬁrstly injected into a small
MLP network and a Sigmoid layer to generate the attention weight, which is used to reweight the 3D PE in an element-wise manner. The improved 3D PE is data-dependent, pro-viding the informative guidance for the query learning in transformer decoder. For comprehensive robustness analy-sis on PETRv2, we consider multiple noise cases, including the camera extrinsics noise, camera miss and time delay.
To summarize, our contributions are:
• We study a conceptually simple extension of position embedding transformation to temporal representation learning. The temporal alignment can be achieved by the pose transformation on 3D PE.
• A simple yet effective solution is introduced for PETR to support the multi-task learning. BEV segmentation and 3D lane detection are supported by introducing task-speciﬁc queries.
• Experiments show that the proposed framework achieves state-of-the-art performance on both 3D ob-ject detection, BEV segmentation and 3D lane detec-tion. Detailed robustness analysis is also provided for comprehensive evaluation on PETR framework. 2.