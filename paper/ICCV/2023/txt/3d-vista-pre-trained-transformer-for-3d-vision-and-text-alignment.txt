Abstract 3D vision-language grounding (3D-VL) is an emerging field that aims to connect the 3D physical world with natural language, which is crucial for achieving embodied intelli-gence. Current 3D-VL models rely heavily on sophisticated modules, auxiliary losses, and optimization tricks, which calls for a simple and unified model. In this paper, we pro-pose 3D-VisTA, a pre-trained Transformer for 3D Vision and Text Alignment that can be easily adapted to various downstream tasks. 3D-VisTA simply utilizes self-attention layers for both single-modal modeling and multi-modal fu-sion without any sophisticated task-specific design. To fur-ther enhance its performance on 3D-VL tasks, we construct
ScanScribe, the first large-scale 3D scene-text pairs dataset for 3D-VL pre-training. ScanScribe contains 2,995 RGB-D scans for 1,185 unique indoor scenes originating from
ScanNet and 3R-Scan datasets, along with paired 278K scene descriptions generated from existing 3D-VL tasks, tem-plates, and GPT-3. 3D-VisTA is pre-trained on ScanScribe via masked language/object modeling and scene-text match-ing. It achieves state-of-the-art results on various 3D-VL tasks, ranging from visual grounding and dense captioning to question answering and situated reasoning. Moreover, 3D-VisTA demonstrates superior data efficiency, obtaining strong performance even with limited annotations during downstream task fine-tuning. 1.

Introduction
Aligning the 3D physical world with natural language is a crucial step towards embodied artificial intelligence [18, 26, 37], where intelligent agents can understand and further exe-cute human instructions in the real world [5, 29]. Recently, 3D vision-language (3D-VL) tasks have attracted growing interest [19], including 3D visual grounding [8, 1], dense captioning [11], grammar learning [23], question answer-ing [3, 56], and situated reasoning [36].
However, most of the models developed for 3D-VL only
*Work done as an intern at BIGAI. (cid:0) Corresponding author.
Figure 1: Overall framework of our 3D-VisTA pipeline. We col-lect diverse prompts, scene graphs, 3D scans, and objects to con-struct ScanScribe dataset. Through self-supervised pre-training, 3D-VisTA supports various downstream tasks including 3D visual grounding, dense captioning, question answering, and situated rea-soning. focus on one or two of these 3D-VL tasks and employ task-specific designs [7, 3, 36, 35, 10]. For instance, 3D-SPS [35] and BUTD-DETR [27] progressively discover the target ob-ject by attending VL features and detecting objects in each layer. 3DVG [55], MVT [24], and ViL3DRel [10] improve 3D visual grounding by explicitly infusing spatial relation information into the model design. 3DJCG [7] jointly learns 3D dense captioning and visual grounding via a shared 3D object proposal module [16] with two separate task-specific heads [7]. Additionally, training these models often requires manually specified auxiliary losses (e.g., 3D object detec-tion/classification and text classification [35, 24, 7, 3, 36]) or optimization tricks (e.g., knowledge distillation [4, 53] ). The lack of a simple and unified approach creates a significant gap in developing a general-purpose 3D-VL model.
To fill such gap, we introduce 3D-VisTA, a Transformer-based model for 3D Vision and Text Alignment that can be easily adapted to various downstream tasks. Unlike previ-ous models that design sophisticated task-specific modules,
we simply utilize a vanilla self-attention transformer [46] for both single-modal modeling and multi-modal fusion in the 3D-VisTA. As a general approach to further enhance 3D spatial comprehension [10, 55, 7], we explicitly encode the pairwise spatial relations between objects into the self-attention weights for 3D object modeling.
Inspired by the success of large-scale pre-training in
NLP [15, 41, 42, 6, 52, 31], CV [22, 17, 21, 25, 38], and 2D-VL [30, 2, 34, 40], we propose to pre-train 3D-VisTA on 3D scene-text data, aiming for better performances on 3D-VL tasks. To this end, we construct ScanScribe, the first large-scale 3D scene-text pairs dataset for 3D-VL pre-training. We first collect RGB-D scans of indoor scenes from
ScanNet [12] and 3R-Scan [48] datasets. We also randomly replace some objects in the scene with objects from the Ob-javerse 3D object database [13] based on their categories, in order to increase object diversity. To obtain the text, we transform the text from existing datasets based on ScanNet into scene descriptions, including the question-answer pairs from ScanQA [3] and the referring expressions from Scan-Refer [8] and ReferIt3D [1]. We further leverage the scene graph annotations [51] of scans from 3R-Scan, and adopt both templates and GPT-3 [6] to generate scene descriptions from their scene graphs. In total, ScanScribe contains 278K 3D scene-text pairs for 2,995 RGB-D scans of 1,185 indoor scenes, with 56.1K unique object instances.
We pre-train 3D-VisTA on the proposed ScanScribe dataset. Our pre-training tasks include masked language modeling, masked object modeling, and scene-text matching.
Notably, similar objectives are widely adopted in 2D-VL yet rarely explored in the 3D-VL domain. The proposed pre-training procedure effectively learns the alignment between 3D point clouds and texts, which eliminates the need for auxiliary losses and optimization tricks in downstream task fine-tuning. On six challenging 3D-VL tasks, ranging from visual grounding (i.e., ScanRefer [8], Nr3D/Sr3D [1]) and dense captioning (i.e., Scan2Cap [11]) to question answer-ing (i.e., ScanQA [3]) and situated reasoning (i.e., SQA3D
[36]), fine-tuned 3D-VisTA raises the SOTA results on Scan-Refer by 8.1% (acc@0.5), on Sr3D by 3.6%, on Scan2Cap by 10.1%(C@0.25), on ScanQA by 3.5%/2.1% (EM@1), and on SQA3D by 1.9%. Moreover, 3D-VisTA demonstrates superior data efficiency, obtaining strong results with only 30% of the annotations for these downstream tasks.
Our main contributions can be summarized as follows:
• We propose 3D-VisTA, a simple and unified Transformer for aligning 3D vision and text. The proposed Transformer simply utilizes the self-attention mechanism, without any complex task-specific design.
• We construct ScanScribe, a large-scale 3D-VL pre-training dataset that contains 278K 3D scene-text pairs for 2,995
RGB-D scans of 1,185 unique indoor scenes.
• We introduce a self-supervised pre-training scheme for 3D-VL, with masked language/object modeling and scene-text matching. It effectively learns the 3D point cloud and text alignment and further simplifies and improves downstream task fine-tuning.
• We fine-tune 3D-VisTA and achieve state-of-the-art per-formances on various 3D-VL tasks, ranging from visual grounding and dense captioning to question answering and situated reasoning. 3D-VisTA also demonstrates superior data efficiency, obtaining strong results even with limited annotations. 2.