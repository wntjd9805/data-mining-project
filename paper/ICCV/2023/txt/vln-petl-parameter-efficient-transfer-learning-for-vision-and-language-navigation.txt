Abstract
The performance of the Vision-and-Language Naviga-tion (VLN) tasks has witnessed rapid progress recently thanks to the use of large pre-trained vision-and-language models. However, full ﬁne-tuning the pre-trained model for every downstream VLN task is becoming costly due to the considerable model size. Recent research hotspot of
Parameter-Efﬁcient Transfer Learning (PETL) shows great potential in efﬁciently tuning large pre-trained models for the common CV and NLP tasks, which exploits the most of the representation knowledge implied in the pre-trained model while only tunes a minimal set of parameters. How-ever, simply utilizing existing PETL methods for the more challenging VLN tasks may bring non-trivial degeneration to the performance. Therefore, we present the ﬁrst study to explore PETL methods for VLN tasks and propose a VLN-speciﬁc PETL method named VLN-PETL. Speciﬁcally, we design two PETL modules: Historical Interaction Booster (HIB) and Cross-modal Interaction Booster (CIB). Then we combine these two modules with several existing PETL methods as the integrated VLN-PETL. Extensive experimen-tal results on four mainstream VLN tasks (R2R, REVERIE,
NDH, RxR) demonstrate the effectiveness of our proposed
VLN-PETL, where VLN-PETL achieves comparable or even better performance to full ﬁne-tuning and outperforms other PETL methods with promising margins. The source code is available at https://github.com/YanyuanQiao/VLN-PETL 1.

Introduction
Large-scale pre-trained models have shown remarkable success in both computer vision (CV) and natural language processing (NLP) domains, and have largely improved the performance of a variety of visio-linguistic tasks [17, 32, 44]. These models follow a standard pretrain-and-ﬁnetune paradigm, which ﬁrst pretrains the model on large-scale un-*Corresponding author
Instruction
“Go straight past the bed and the room the fireplace. Exit using the door on the left of the fireplace. Wait there. ”
Full fine-tuning
PETL training (Ours)
VLN agent
VLN-PETL
VLN agent
VLN-PETL
Next action?
Success Rate (R2R):  64.2%
Updated Param:  100%
Success Rate (R2R):  65.5%
Updated Param:  2.82% (cid:28617)(cid:28644)(cid:28632)(cid:28629)(cid:28648)(cid:28633)(cid:28632) (cid:28602)(cid:28646)(cid:28643)(cid:28654)(cid:28633)(cid:28642)
Figure 1: Comparison of full ﬁne-tuning and our proposed
PETL training for VLN tasks. By updating only a small subset of parameters, our proposed VLN-PETL can achieve a comparative performance compared to full ﬁne-tuning labeled data and then ﬁnetunes it on each downstream task.
Since the size of such models is growing rapidly [4, 39], even fully ﬁnetuning and storing a copy of the entire pre-trained model for each downstream becomes costly.
To alleviate this problem, Parameter-Efﬁcient Transfer
Learning (PETL) has been proposed as an alternative train-ing strategy [3, 10, 14, 15, 28, 29] and initially achieved great progress in NLP community. These methods aim to exploit the representation knowledge in the large pretrained models by freezing most parameters of the model and only tuning a small set of parameters, which can achieve compa-rable or even better performance to full ﬁne-tuning. Several approaches have attempted to apply PETL techniques to CV and V&L domains [11, 33, 40, 43] and achieved promis-ing results on various downstream tasks. Recent works
[9, 31, 45] ﬁnd that different PETL methods have different characteristics and performance on the same downstream task and thus combining multiple PETL techniques may be more effective in improving the performance.
Vision-and-Language Navigation (VLN), which deals with visual, linguistic and robotic action inputs simulta-neously, could beneﬁt from the pre-trained large models while suffering from the considerable model size during the downstream tasks ﬁne-tuning. Considering downstream
VLN agents are complex enough, full ﬁnetuning them with the large pre-trained models for each downstream VLN task becomes expensively, in which case the technique of PETL
shows great potential. Unlike most NLP, CV, and V&L tasks, VLN is a dynamic action decision-making task rely-ing on the current environment and previous history knowl-edge of the chosen actions. Speciﬁcally, given the instruc-tion in natural language, the VLN agent perceives a new vi-sual observation according to the chosen action at the previ-ous timestep and should choose the next action to perform at the current timestep. Thus, how to effectively learn history knowledge is crucial to adapting PETL methods for VLN tasks. Moreover, the cross-modal interaction which plays a vital role in action prediction should be also enhanced during the process of efﬁcient tuning. In addition, our ex-periments show that directly applying some existing PETL methods to VLN tasks may bring non-trivial performance degeneration.
Considering these reasons, we propose a VLN-speciﬁc
PETL method named VLN-PETL. Speciﬁcally, we design two tailored PETL modules for VLN: Historical Interaction
Booster (HIB) and Cross-modal Interaction Booster (CIB).
Both these two modules mainly consist of bottleneck lay-ers and multi-head cross-attention layers. HIB enhances the interaction between the observation and the previous histor-ical knowledge in a recurrent pattern. While CIB adopts a two-stream structure to focus on the interaction of cross-modal knowledge. Similar to adapters which inject bottle-neck layers into transformer blocks for efﬁcient tuning, we insert HIB and CIB into the visual encoder and cross-modal encoder separately in the pre-trained model for VLN. Dur-ing the training process, the original weights of the large pre-trained model are frozen and only weights of these newly injected modules are trained and updated for differ-ent downstream VLN tasks. In addition to HIB and CIB,
VLN-PETL also adopts vanilla adapters to efﬁciently tune the language encoder and the LoRA to further improve the parameter-efﬁcient tuning’s performance as previous work declared [9, 31, 45] for downstream VLN tasks.
We conduct extensive experiments on four mainstream
VLN tasks: R2R [2], REVERIE [36], NDH [41], and
RxR [16]. The results show that VLN-PETL not only sur-passes other PETL methods with promising margins but also achieves comparable or even better performance com-pared to full ﬁne-tuning, especially on R2R (Ò 1.3% SR on validation unseen set, updating only 2.82% params, see
Figure 1) and on NDH (Ò 1.08 GP on test unseen set which achieves the top position in the leaderboard). We also con-duct ablation studies to evaluate the contribution of each component of VLN-PETL and validate the superiority of
HIB and CIB to counterpart PETL methods.
In summary, our contributions are as follows: (1) We present the ﬁrst study that explores Parameter-Efﬁcient
Transfer Learning (PETL) techniques for Vision-and-Language Navigation (VLN) tasks; (2) We propose a VLN-speciﬁc PETL method named VLN-PETL, which incorpo-rates existing PETL methods with two tailored PETL mod-ules for VLN tasks: Historical Interaction Booster (HIB) and Cross-modal Interaction Booster (CIB); (3) Extensive experiments on four VLN downstream tasks demonstrate the effectiveness of our proposed VLN-PETL, which out-performs other PETL methods and keep competitive to full
ﬁne-tuning with much fewer trainable parameters. 2.