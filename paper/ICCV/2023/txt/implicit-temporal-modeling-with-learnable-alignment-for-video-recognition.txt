Abstract
Contrastive language-image pretraining (CLIP) has demonstrated remarkable success in various image tasks.
However, how to extend CLIP with effective temporal mod-eling is still an open and crucial problem. Existing factor-ized or joint spatial-temporal modeling trades off between the efficiency and performance. While modeling temporal information within straight through tube is widely adopted in literature, we find that simple frame alignment already provides enough essence without temporal attention. To this end, in this paper, we proposed a novel Implicit Learn-able Alignment (ILA) method, which minimizes the tempo-ral modeling effort while achieving incredibly high perfor-mance. Specifically, for a frame pair, an interactive point is predicted in each frame, serving as a mutual informa-tion rich region. By enhancing the features around the interactive point, two frames are implicitly aligned. The aligned features are then pooled into a single token, which is leveraged in the subsequent spatial self-attention. Our method allows eliminating the costly or insufficient tempo-ral self-attention in video. Extensive experiments on bench-marks demonstrate the superiority and generality of our module. Particularly, the proposed ILA achieves a top-1 ac-curacy of 88.7% on Kinetics-400 with much fewer FLOPs compared with Swin-L and ViViT-H. Code is released at https://github.com/Francis-Rings/ILA. 1.

Introduction
Video recognition is rated as one of the most fundamen-tal components of video understanding. Numerous down-stream tasks heavily rely on the basic recognition model, e.g., action localization [6, 12, 45, 46, 48], detection [7, 19, 24, 30, 73], and video object tracking [16, 57, 75]. Due to the great potential of video technologies, it has been an ac-tive research direction over the past few years. Various ap-proaches have been proposed, including convolution-based
*Corresponding author
Figure 1. Top-1 accuracy comparison with state-of-the-art meth-ods on Kinetics-400 [28] under different FLOPs.
ILA achieves competitive results. Best viewed in color. methods [49, 55, 53, 10, 60, 18, 17, 76] and transformer-based methods [3, 5, 20, 15, 31, 36, 50, 61]. Recently,
Contrastive Language-Image Pretraining (CLIP) [41] has demonstrated strong performance in video domain. Studies
[56, 27, 38, 35, 39, 64, 74] attempt to transfer the powerful
CLIP model to video tasks, which promote the recognition performance to a brand-new level, showing its general rep-resentation ability.
Generally, existing methods devise various temporal modeling schemes to explore the potential of CLIP, includ-ing the factorized [64] or frame-level [38, 27] temporal at-tention, and temporal cross attention [35]. All these tailored methods aim at designing lightweight temporal modules to reuse the CLIP model. Though considerable improvements are achieved, such temporal modeling approaches still de-pend on the complex self-attention, which we argue is not necessary in CLIP-based framework.
In this paper, we rethink the role of temporal modeling in general CLIP-based video recognition framework. Unlike existing approaches rely on temporal attention, we hypoth-esize that important motion and action clues can be derived when performing alignment of pairwise frames. As a result, the costly [36, 5] or insufficient [38, 27, 35] temporal at-which enables superior temporal modeling at a low com-putational cost. (2) We show that such a simple frame alignment already encodes the essence of temporal rela-tions, which allow eliminating the insufficient temporal self-attention. (3) Extensive qualitative and quantitative ex-periments demonstrate the effectiveness and efficiency of
ILA. We achieve 88.7% on Kinetics-400 with low compu-tation overhead. Our method builds a promising bridge for
CLIP from image processing to video recognition. 2.