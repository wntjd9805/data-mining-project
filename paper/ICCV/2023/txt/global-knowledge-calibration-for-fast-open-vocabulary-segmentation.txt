Abstract
Recent advancements in pre-trained vision-language models, such as CLIP, have enabled the segmentation of ar-bitrary concepts solely from textual inputs, a process com-monly referred to as open-vocabulary semantic segmenta-tion (OVS). However, existing OVS techniques confront a fundamental challenge: the trained classifier tends to over-fit on the base classes observed during training, resulting in suboptimal generalization performance to unseen classes.
To mitigate this issue, recent studies have proposed the use of an additional frozen pre-trained CLIP for classification.
Nonetheless, this approach incurs heavy computational overheads as the CLIP vision encoder must be repeatedly forward-passed for each mask, rendering it impractical for real-world applications. To address this challenge, our ob-jective is to develop a fast OVS model that can perform com-parably or better without the extra computational burden of the CLIP image encoder during inference. To this end, we propose a core idea of preserving the generalizable repre-sentation when fine-tuning on known classes. Specifically, we introduce a text diversification strategy that generates a set of synonyms for each training category, which pre-vents the learned representation from collapsing onto spe-cific known category names. Additionally, we employ a text-guided knowledge distillation method to preserve the gener-alizable knowledge of CLIP. Extensive experiments demon-strate that our proposed model achieves robust generaliza-tion performance across various datasets. Furthermore, we perform a preliminary exploration of open-vocabulary video segmentation and present a benchmark that can facil-itate future open-vocabulary research in the video domain.
*Equal contribution. Work done during internships at ByteDance. (cid:0)Corresponding author.
Figure 1. Performance vs. computational cost. The radius of the circle represents the FLOPs during inference. To avoid overfitting to the seen categories, some methods [14, 44] introduce an extra frozen CLIP during inference. However, such a strategy leads to heavy computation overhead (red •). In comparison, our method generalizes well on both seen and unseen categories with much smaller computational cost (blue •). 1.

Introduction
Semantic segmentation aims to group pixels that belong to the same categories. Despite achieving high performance in recent years [30, 11, 38, 42, 4, 19, 16, 36, 48, 49], existing semantic segmentation approaches often rely on predefined sets of training categories and thus cannot recognize cate-gories that were not present during training. This limitation greatly restricts their practical applicability. In contrast, hu-mans possess the ability to recognize novel categories in an open-vocabulary manner, i.e., identifying objects using arbitrary text from an unbounded vocabulary. This ability has inspired the development of open-vocabulary segmen-tation methods [14, 44, 47, 21, 18, 41, 25]. Unlike tradi-tional closed-set segmentation, open-vocabulary segmenta-tion can segment arbitrary categories given only text inputs,
which has many potential applications, such as image edit-ing and human-robot interaction.
To achieve open-vocabulary segmentation, early ap-proaches [41, 47, 25] replace the output classification layer with cross-modal alignment, where the similarity mea-sure between pixels and text embeddings is used. Recent works [14, 18, 28, 22, 44], on the other hand, adopt the region-level alignment approach and have demonstrated re-markable performance. Despite these advancements, open-vocabulary segmentation methods still face a significant challenge: the learned embeddings often overfit to the base classes observed during training, which hinders their ability to generalize to novel classes. To overcome this challenge, some methods [44, 14, 28] utilize an additional frozen CLIP vision encoder for re-classification. However, this strategy incurs heavy computation overhead, as it requires repeated forward passes of the CLIP vision encoder for each mask.
This can be prohibitively expensive for real-world applica-tions, as illustrated in Fig. 1.
Therefore, our objective is to train an open-vocabulary semantic segmentation model that is fast and does not re-quire the extra heavy CLIP image encoder during inference, while achieving comparable or better performance. The two main factors that contribute to this objective are: (1) the model should not overfit to the specific training category names, and (2) the model should maintain a feature space similar to the pre-trained CLIP. To achieve this goal, we in-troduce Global Knowledge Calibration. To prevent the learned representation from being biased towards the spe-cific training category names, we propose a text diversifi-cation strategy for prompt augmentation. This strategy en-hances text diversity and enriches category semantics with information of different granularities. Specifically, we use
WordNet [17] to generate a set of synonyms for each train-ing category, e.g., “vessel” and “ship” for “boat”, and ex-pand the initial text prompts with this set of words.
To maintain the generalizable knowledge of CLIP [37], a straightforward solution is to apply knowledge distilla-tion. However, traditional knowledge distillation methods only utilize the CLIP features of the same object as super-vision. As a result, they can only fit the representations of individual classes and fail to effectively model the overall
CLIP space. To address this issue, we propose a text-guided knowledge distillation strategy for calibrating the represen-tation of the trained model. Specifically, we apply distilla-tion supervision for the visual embeddings of one category by using all categories present in the image. Using the dis-tance between category names in the text space as guidance, this distillation strategy can guide the trained model to build a multi-modal feature space similar to the pre-trained CLIP.
In addition, to our best knowledge, previous research on OVS has only focused on the image domain.
In this work, we make a preliminary exploration of open-vocabulary video segmentation. We introduce a benchmark by partitioning the large-scale video segmentation dataset,
VIPSeg [32], into seen and unseen categories for zero-shot testing. We develop a simple baseline based on our image-based method. Our aim is to provide support for future open-vocabulary research in the video domain.
Our contributions can be summarized as follows:
• We propose Global Knowledge Calibration to preserve generalizable representations when training solely on known classes. Our approach does not require an additional heavy CLIP vision encoder during infer-ence, making it faster. Extensive experiments demon-strate that our model offers strong generalization per-formance across various datasets, with a much smaller computational cost.
• We present a text diversification strategy to enrich text supervision with category information of varying gran-ularities. We propose a text-guided knowledge distil-lation strategy to calibrate the learned feature space.
• To the best of our knowledge, we are the first to explore open-vocabulary video segmentation. We construct a new benchmark and a simple baseline. 2.