Abstract
Generating 3D faces from textual descriptions has a mul-titude of applications, such as gaming, movie, and robotics.
Recent progresses have demonstrated the success of uncon-ditional 3D face generation and text-to-3D shape genera-tion. However, due to the limited text-3D face data pairs, text-driven 3D face generation remains an open problem.
In this paper, we propose a text-guided 3D faces genera-tion method, refer as TG-3DFace, for generating realistic 3D faces using text guidance. Specifically, we adopt an unconditional 3D face generation framework and equip it with text conditions, which learns the text-guided 3D face generation with only text-2D face data. On top of that,
∗ Equal contribution
† Corresponding author we propose two text-to-face cross-modal alignment tech-niques, including the global contrastive learning and the fine-grained alignment module, to facilitate high semantic consistency between generated 3D faces and input texts.
Besides, we present directional classifier guidance during the inference process, which encourages creativity for out-of-domain generations. Compared to the existing methods,
TG-3DFace creates more realistic and aesthetically pleas-ing 3D faces, boosting 9% multi-view consistency (MVIC) over Latent3D. The rendered face images generated by TG-3DFace achieve higher FID and CLIP score than text-to-2D face/image generation models, demonstrating our superior-ity in generating realistic and semantic-consistent textures.
1.

Introduction 3D Face generation is a critical technology with diverse applications in various industry scenarios, e.g., movies and games. Recent works have demonstrated the success of 3D face generation with image reconstruction [23, 24] and un-conditional generation methods [5, 2, 50, 32, 4]. Despite the photo-realistic 3D face results, the generation process can-not be guided by texts, which has the potential to increase creativity and efficiency. Therefore it is highly demanded to take a step toward text-guided 3D face generation.
Existing methods have been explored to generate 3D shapes and human bodies based on given texts [6, 29, 44, 16, 31], which enables the controllable generation under text guidance. However, it is not feasible to directly apply those generation methods for 3D face generation, owing to two fact:1) The lack of large-scale text-3D face data pairs for model training. 2) The richness of 3D facial attributes that contains much more geometrical details than common 3D objects. Though recent works [3, 1] make attempts to semantically manipulate the shape or texture of 3D faces to boost 3D face generation results, they still lead to re-sults with poor realism and aesthetic appeal such as the loss of hair, which limits the practical applications. Based on the above observation, it requires a rethink of a fine-grained text-driven 3D face generation framework.
To address the above issues, we present a novel fine-grained text-driven 3D face generation framework, named
TG-3DFace, to generate high-quality 3D faces that are se-mantically consistent with the input texts. Specifically, TG-3DFace contains a text-conditioned 3D generation network and two text-to-face cross-modal alignment techniques.
Firstly, we adopt the architecture design of EG3D [4], which is an unconditional 3D shape generative adversarial network, and learn 3D shape generation from single-view 2D images. We inject the texture condition into the gen-erator and discriminator networks to enable 3D face gener-ation under the guidance of input texts. Such text-guided 3D face generative model can thus conduct training on text-2D face images instead of text-3D face shapes, enabling to transfer the semantic consistency between texts and 2D face images to guide 3D face generation. Besides, considering the richness of fine-grained facial attributes that increases the difficulty of aligning 3D face and input texts, we design two text-to-face cross-modal alignment techniques, includ-ing global text-to-face contrastive learning and fine-grained text-to-face alignment module. The text-to-face contrastive learning aligns the features of the rendered face images with their paired text and maximizes the distance between the un-paired ones in the embedding space, which facilitates global semantic consistency. The fine-grained text-to-face align-ment is designed to align the part-level facial features of the rendered face image to the part-level text features, to achieve fine-grained semantic alignment between the texts and the generated 3D faces.
Additionally, we utilize the directional vector in the
CLIP embedding space, calculated between the input text and the training style prompt text, as an optimization direc-tion to fine-tune the generator for several steps during infer-ence.
In this way, our TG-3DFace can synthesize novel-style face that is never seen during training, such as “a
Pixar-style man”.
We evaluate our model on the Multi-Modal CelebA-HQ
[54], CelebAText-HQ [49] and FFHQ-Text [63] datasets.
The experimental results and ablation analysis demonstrate that our method can generate high-quality and semantic-consistent 3D faces given input texts. Besides, our method can be applied to downstream applications including single-view 3D face reconstruction and text-guided 3D face ma-nipulation. In brief, our contributions can be summarized as follows:
• We propose a novel 3D face generation framework,
TG-3DFace, which equips the unconditional 3D face generation framework with text conditions to generate 3D faces with the guidance of input texts.
• We propose two text-to-face cross-modal alignment techniques, including global contrastive learning and fine-grained text-to-face alignment mechanism, which boosts the semantic consistency of generations.
• Quantitative and qualitative comparisons confirm that 3D faces generated by our TG-3DFace are more real-istic and achieve better semantic consistency with the given textual description. 2.