Abstract 1.

Introduction
We tackle the task of reconstructing hand-object interac-tions from short video clips. Given an input video, our ap-proach casts 3D inference as a per-video optimization and recovers a neural 3D representation of the object shape, as well as the time-varying motion and hand articulation.
While the input video naturally provides some multi-view cues to guide 3D inference, these are insufficient on their own due to occlusions and limited viewpoint variations.
To obtain accurate 3D, we augment the multi-view sig-nals with generic data-driven priors to guide reconstruc-tion. Specifically, we learn a diffusion network to model the conditional distribution of (geometric) renderings of objects conditioned on hand configuration and category label, and leverage it as a prior to guide the novel-view renderings of the reconstructed scene. We empirically evaluate our approach on egocentric videos across 6 object categories, and observe significant improvements over prior single-view and multi-view methods. Finally, we demonstrate our systemâ€™s ability to reconstruct arbitrary clips from YouTube, showing both 1st and 3rd person interactions.
Our hands allow us to affect the world around us. From pouring the morning coffee to clearing the dinner table, we continually use our hands to interact with surrounding ob-jects. In this work, we pursue the task of understanding such everyday interactions in 3D. Specifically, given a short clip of a human interacting with a rigid object, our approach can infer the shape of the underlying object as well as its (time-varying) relative transformation w.r.t. an articulated hand (see Fig. 1 for sample results).
This task of recovering 3D representations of hand-object interactions (HOI) has received growing interest.
While initial approaches [4, 16, 22, 40, 69] framed it as 6-DoF pose task estimation for known 3D objects/templates, subsequent methods have tackled the reconstruction of apri-ori unknown objects [24, 33, 83]. Although single-view 3D reconstruction approaches can leverage data-driven tech-niques to reconstruct HOI images [24, 83, 90], these ap-proaches cannot obtain precise reconstructions given the fundamentally limited nature of the single-view input. On the other hand, current video-based HOI reconstruction methods primarily exploit multi-view cues and rely on purely geometry-driven optimization for reconstruction. As
a result, these methods are suited for in-hand scanning where a user carefully presents exhaustive views of the ob-ject of interest, but they are not applicable to our setting as aspects of the object may typically be unobserved.
Towards enabling accurate reconstruction given short ev-eryday interaction clips, our approach (DiffHOI) unifies the data-driven and the geometry-driven techniques. Akin to the prior video-based reconstruction methods, we frame the reconstruction task as that of optimizing a video-specific temporal scene representation. However, instead of purely relying on geometric reprojection errors, we also incorpo-rate data-driven priors to guide the optimization. In particu-lar, we learn a 2D diffusion network which models the dis-tribution over plausible (geometric) object renderings con-ditioned on estimated hand configurations. Inspired by re-cent applications in text-based 3D generation [37, 58], we use this diffusion model as a generic data-driven regularizer for the video-specific 3D optimization.
We empirically evaluate our system across several first-person hand-object interaction clips from the HOI4D dataset [42], and show that it significantly improves over both prior single-view and multi-view methods. To demon-strate its applicability in more general settings, we also show qualitative results on arbitrary interaction clips from
YouTube, including both first-person and third-person clips. 2.