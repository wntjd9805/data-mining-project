Abstract
Temporal action localization (TAL), which involves rec-ognizing and locating action instances, is a challenging task in video understanding. Most existing approaches di-rectly predict action classes and regress offsets to bound-aries, while overlooking the discrepant importance of each frame.
In this paper, we propose an Action Sensitivity
Learning framework (ASL) to tackle this task, which aims to assess the value of each frame and then leverage the generated action sensitivity to recalibrate the training pro-cedure. We first introduce a lightweight Action Sensitivity
Evaluator to learn the action sensitivity at the class level and instance level, respectively. The outputs of the two branches are combined to reweight the gradient of the two sub-tasks. Moreover, based on the action sensitivity of each frame, we design an Action Sensitive Contrastive Loss to enhance features, where the action-aware frames are sam-pled as positive pairs to push away the action-irrelevant frames. The extensive studies on various action localization benchmarks (i.e., MultiThumos, Charades, Ego4D-Moment
Queries v1.0, Epic-Kitchens 100, Thumos14 and Activi-tyNet1.3) show that ASL surpasses the state-of-the-art in terms of average-mAP under multiple types of scenarios, e.g., single-labeled, densely-labeled and egocentric. 1.

Introduction
With an increasing number of videos appearing online, video understanding has become a prominent research topic in computer vision. Temporal action localization (TAL), which aims to temporally locate and recognize human ac-tions with a set of categories in a video clip, is a challenging yet fundamental task in this area, owing to its various appli-cations such as sports highlighting, human action analysis and security monitoring [25, 63, 46, 17, 14].
We have recently witnessed significant progress in TAL,
* This work was done during the first author’s internship in Alibaba.
† Corresponding Author.
Figure 1. The motivation of our method. We show the action in-stance of clothes drying and depict the possible importance of each frame to recognizing the action category and locating action boundaries. Each frame’s importance is different. where most methods can be mainly divided into two parts: 1) Two-stage approaches [75, 85] tackle this task accom-panied by the generation of class-agnostic action proposals and then perform classification and proposal boundaries re-finement in proposal-level; 2) One-stage approaches [79, 72, 32] simultaneously recognize and localize action in-stances in a single-shot manner. Typical methods [76, 29] of this type predict categories as well as locate corresponding temporal boundaries in frame-level, achieving stronger TAL results currently. In training, they classify every frame as one action category or background and regress the bound-aries of frames inside ground-truth action segments. How-ever, these works treat each frame within action segments equally in training, leading to sub-optimal performance.
When humans intend to locate action instances, the dis-crepant information of each frame is referred to. For the instance of action: clothes drying, as depicted in Fig 1, frames in the purple box promote recognizing clothes dry-ing most as they describe the intrinsic sub-action: hang clothes on the hanger. Analogously, frames in red and gray boxes depict take out clothes from laundry basket and lift laundry basket, which are more informative to locate pre-cise start and end time respectively. In a word, each frame’s contribution is quite different, due to intrinsic patterns of actions, as well as existing transitional or blurred frames.
Can we discover informative frames for classifying and
localizing respectively? To this end, we first introduce a concept — Action Sensitivity, to measure the frame’s im-portance. It is disentangled into two parts: action sensi-tivity to classification sub-task and action sensitivity to localization sub-task. For one sub-task, the higher action sensitivity each frame has, the more important it will be for this sub-task. With this concept, intuitively, more attention should be paid to action sensitive frames in training.
Therefore in this paper, we propose a lightweight Ac-tion Sensitivity Evaluator (ASE) for each sub-task to better exploit frame-level information. Essentially, for a specific sub-task, ASE learns the action sensitivity of each frame from two perspectives: class-level and instance-level. The class-level perspective is to model the coarse action sensi-tivity distribution of each action category and is achieved by incorporating gaussian weights. The instance-level per-spective is complementary to class-level modeling and is supervised in a prediction-aware manner. Then the training weights of each frame are dynamically adjusted depending on their action sensitivity, making it more reasonable and effective for model training.
With the proposed ASE, we build our novel Action
Sensitivity Learning framework dubbed ASL to tackle tem-poral action localization task (TAL) effectively. Moreover, to furthermore enhance the features and improve the dis-crimination between actions and backgrounds, we design a novel Action Sensitive Contrastive Loss (ASCL) based on
ASE. It is implemented by elaborately generating various types of action-related and action-irrelevant features and performing contrasting between them, which brings mul-tiple merits for TAL.
By conducting extensive experiments on 6 datasets and detailed ablation studies, we demonstrate ASL is able to classify and localize action instances better. In a nutshell, our main contributions can be summarized as follows:
• We propose a novel framework with an Action Sensi-tivity Evaluator component to boost training, by dis-covering action sensitive frames to specific sub-tasks, which is modeled from class level and instance level.
• We design an Action Sensitive Contrastive Loss to do feature enhancement and to increase the discrimination between actions and backgrounds.
• We verify ASL on various action localization datasets i) densely-labeled (i.e., Multi-of multiple types:
Thumos [74] and Charades [53]). ii) egocentric (Ego4d-Moment Queries v1.0 [19] and Epic-Kitchens 100 [11]). iii) nearly single-labeled (Thumos14 [57] and ActivityNet1.3 [2]), and achieve superior results. 2.