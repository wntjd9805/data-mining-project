Abstract
Personalized Federated Learning (PFL) represents a promising solution for decentralized learning in heteroge-neous data environments. Partial model personalization has been proposed to improve the efficiency of PFL by se-lectively updating local model parameters instead of ag-gregating all of them. However, previous work on par-tial model personalization has mainly focused on Convo-lutional Neural Networks (CNNs), leaving a gap in un-derstanding how it can be applied to other popular mod-els such as Vision Transformers (ViTs).
In this work, we investigate where and how to partially personalize a ViT model.
Specifically, we empirically evaluate the sensi-tivity to data distribution of each type of layer. Based on the insights that the self-attention layer and the clas-sification head are the most sensitive parts of a ViT, we propose a novel approach called FedPerfix, which lever-ages plugins to transfer information from the aggregated model to the local client as a personalization. Finally, we evaluate the proposed approach on CIFAR-100, OrganAM-NIST, and Office-Home datasets and demonstrate its effec-tiveness in improving the model’s performance compared to several advanced PFL methods. Code is available at https://github.com/imguangyu/FedPerfix 1.

Introduction
Federated learning (FL) [25] has emerged as a promis-ing method for training machine learning models on decen-tralized data without requiring direct data sharing. How-ever, data heterogeneity among participating clients can present a significant challenge. Due to the various cir-cumstances of the clients, the data across the clients can be non-independent and non-identically distributed (non-IID). Therefore, achieving satisfactory performance using a one-model-fits-all approach is difficult, and personalized models are often needed to achieve the best results. This has inspired the study of Personalized Federated Learning (PFL), where the focus is shifted from the performance of the global model on the server to the local models on the clients.
In the context of personalized federated learning, previ-ous literature has explored two main approaches: full model personalization [9, 6, 23] and partial model personaliza-tion [31, 27, 18, 5, 1]. Full model personalization involves maintaining a separate local model for each client and up-dating it based on a joint objective, while partial model per-sonalization aims to personalize only a subset of the model parameters. A convergence analysis [31] suggests that par-tial model personalization can achieve most of the benefits of full model personalization with fewer shared parameters, offering advantages in terms of computation, communica-tion, and privacy to enable the deployment of larger models on the clients.
However, recent literature shows that where and how to perform partial model personalization has a high correlation to the model architectures and the tasks [31], which requires further study when applied to a new architecture. Despite the multitude of approaches proposed in the literature, the majority of methods have only been evaluated on Convolu-tional Neural Networks (CNNs). Meanwhile, Vision Trans-formers (ViTs) [8] have demonstrated superior performance compared to CNNs in several tasks, such as image classifi-cation [37] and object detection [42, 3], making them an attractive option for personalized federated learning. How-ever, to the best of our knowledge, the application of ViT in the federated learning community has received limited at-tention in the existing literature [32]. Given the advantages of ViT shown under centralized training, it is reasonable to expect that these benefits can also be realized in PFL by of-fering a more robust model for improved performance on the clients. Therefore, in this work, we investigate where and how to partially personalize a ViT model.
Drawing from previous research on CNNs, layers that serve specific engineering purposes, such as feature ex-traction, normalization, or classification, have been iden-tified as suitable candidates for partial model personaliza-tion [27, 18, 5]. These layers might have a higher sensitivity to the distribution of the training data. Therefore, aggre-gating the model weights trained from different data distri-butions may result in an inaccurate feature, while keeping them updated locally can gain a better feature for the lo-cal data distribution. Similarly, we select some candidates from ViT and conduct an empirical study to investigate the sensitivity of each type of layer. Specifically, we quanti-tatively evaluate the impact of keeping certain layers up-dated locally without aggregation with other clients. This evaluation shows that the self-attention layers and the clas-sification head have a higher sensitivity than other layers, providing insights about where to personalize.
To personalize the sensitive parts, one intuitive approach is to keep them completely local. However, the global ag-gregation has shown the capability to provide a more gen-eral global model than local models [24]. Completely pre-venting the sharing with the global model will severely hin-der the potential benefits of leveraging general knowledge from the aggregated global model. Therefore, we desire to train a personalization module to bridge the general knowl-edge and the client-specific knowledge.
As existing works in transfer learning suggest, the same pre-trained model can be transferred to different down-stream data by adding different tiny architectures [30, 15, 20, 12]. We refer to these tiny architectures as plugins since they are small-sized parameters plugged into the pre-trained model. These plugins are trained while the weights of the pre-trained model are frozen. The plugged model can achieve comparable performance as the fully fine-tuned model [29]. Since the model can maintain the same level of performance without changing the weights of the pre-trained model, the downstream-specific knowledge is cap-tured by the plugins. Therefore, the plugins show the ca-pability to personalize the same model to satisfy different downstream data. In federated learning, we can consider the aggregated model as an inferior version of the “pre-trained” model, and the data on different clients as the downstream data, as shown in Fig. 1. With this perspective, we can therefore exploit the capability of the plugins in the fed-erated learning context to capture client-specific knowledge for personalization.
Therefore, we select and adapt a specific family of plu-Figure 1. Analogy between transfer learning and personalized federated learning. A pre-trained model can be transferred to different downstream data with different plugins. In personalized federated learning, we are seeking different personalization mod-ules (PM) to transfer the aggregated model to different client data. gin, Prefixes, to personalize the self-attention layer and propose a novel approach FedPerfix, short for Federated
Personalized Prefix-tuning.
The main contributions of this paper are as follows:
• We perform an empirical study to reveal the sensitiv-ity to data distribution of each type of layer in a ViT for PFL and locate the self-attention layer and classi-fication head as the sensitive part to be personalized. (Section 3.3)
• We propose a novel partial model personalization ap-proach on ViT, FedPerfix, inspired by the connection between PFL and transfer learning. Specifically, we exploit Prefix plugins to capture client-specific knowl-edge for personalization. (Section 3.5)
• We conduct evaluations on CIFAR-100 [2], Or-ganAMINIST [40], and Office-home [38], which are across different domains, and degrees of data hetero-geneity and achieve state-of-the-art performance com-pared with several competitive methods with lower re-source requirements. (Section 4.2) 2.