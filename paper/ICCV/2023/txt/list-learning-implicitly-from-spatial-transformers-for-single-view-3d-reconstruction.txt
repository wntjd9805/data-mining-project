Abstract
Accurate reconstruction of both the geometric and topo-logical details of a 3D object from a single 2D image em-bodies a fundamental challenge in computer vision. Exist-ing explicit/implicit solutions to this problem struggle to re-cover self-occluded geometry and/or faithfully reconstruct topological shape structures. To resolve this dilemma, we introduce LIST, a novel neural architecture that leverages local and global image features to accurately reconstruct the geometric and topological structure of a 3D object from a single image. We utilize global 2D features to predict a coarse shape of the target object and then use it as a base for higher-resolution reconstruction. By leveraging both local 2D features from the image and 3D features from the coarse prediction, we can predict the signed distance between an arbitrary point and the target surface via an implicit predictor with great accuracy. Furthermore, our model does not require camera estimation or pixel align-ment. It provides an uninfluenced reconstruction from the input-view direction. Through qualitative and quantitative analysis, we show the superiority of our model in recon-structing 3D objects from both synthetic and real-world im-ages against the state of the art. Our source code is publicly available to the research community [13]. 1.

Introduction
Constructing a truthful portrayal of the 3D world from a single 2D image is a basic problem for many applications including robot manipulation and navigation, scene under-standing, view synthesis, virtual reality, and more. Follow-ing the work of Erwin Kruppa [11] in camera motion esti-mation and the recovery of 3D points, researchers have at-tempted to solve the 3D reconstruction issue using structure from motion [33, 16, 28], and visual simultaneous localiza-tion and mapping [8, 27]. However, the main limitation of such approaches is that they require multiple observations
Fig. 1: Five unique views of objects reconstructed by LIST from a single RGB image. Not only does our model accurately recover occluded geometry, but also the reconstructed surfaces are not in-fluenced by the input-view direction. of the desired object or scene from distinct viewpoints with shared features. Such a multi-view formulation allows for integrating information from numerous images to compen-sate for occluded geometry.
Reconstructing a 3D object from a single image is a more difficult task since a sole image does not contain the whole topology of the target shape due to self-occlusions. Re-searchers have tried both explicit and implicit techniques to reconstruct a target object with self-occluded parts. Explicit methods attempt to infer the target shape directly from the input image. Nevertheless, a major drawback of such ap-proaches is that the output resolution needs to be defined in advance, which constrains these techniques from achieving high-quality results. Recent advances in implicit learning offer a solution to reconstruct the target shape in an arbi-trary resolution by indirectly inferring the desired surface through a distance/occupancy field. Then, the target sur-face is reconstructed by extracting a zero level set from the
distance/occupancy field. bility.
Implicit 3D reconstruction from a single view is an ac-tive area of research where one faction of techniques [18, 3] encode global image features into a latent representation and learn an implicit function to reconstruct the target. Yet, these approaches can be easily outperformed by simple re-trieval baselines [32]. Therefore, global features alone are not sufficient for a faithful reconstruction. Another fac-tion leverages both local and global features to learn the target implicit field from pixel-aligned query points. How-ever, such methods rely on ground-truth/estimated camera parameters for training/inference [35, 12], or they assume weak perspective projection [25, 9].
To address these shortcomings we propose LIST, a novel deep learning framework that can reliably reconstruct the topological and geometric structure of a 3D object from a single RGB image, Fig. 1. Our method does not depend on weak perspective projection, nor does it require any cam-era parameters during training or inference. Moreover, we leverage both local and global image features to generate highly-accurate topological and geometric details. To re-cover self-occluded geometry and aid the implicit learning process, we first predict a coarse shape of the target object from the global image features. Then, we utilize the lo-cal image features and the predicted coarse shape to learn a signed distance function (SDF).
Due to the scarcity of real-world 2D-3D pairs, we train our model on synthetic data. However, we use both syn-thetic and-real world images to test the reconstruction abil-ity of LIST. Through qualitative analysis we highlight our modelâ€™s superiority in reconstructing high-fidelity geomet-ric and topological structure. Via a quantitative analysis using traditional evaluation metrics, we show that the recon-struction quality of LIST surpasses existing works. Further-more, we design a new metric to investigate the reconstruc-tion quality of self-occluded geometry. Finally, we provide an ablation study to validate the design choices of LIST in achieving high-quality single-view 3D reconstruction. 2.