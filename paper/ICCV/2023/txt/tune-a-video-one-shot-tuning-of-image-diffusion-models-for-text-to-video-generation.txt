Abstract
To replicate the success of text-to-image (T2I) genera-tion, recent works employ large-scale video datasets to train a text-to-video (T2V) generator. Despite their promising re-*Corresponding Author. sults, such paradigm is computationally expensive. In this work, we propose a new T2V generation setting—One-Shot
Video Tuning, where only one text-video pair is presented.
Our model is built on state-of-the-art T2I diffusion models pre-trained on massive image data. We make two key obser-vations: 1) T2I models can generate still images that repre-sent verb terms; 2) extending T2I models to generate mul-tiple images concurrently exhibits surprisingly good con-tent consistency. To further learn continuous motion, we introduce Tune-A-Video, which involves a tailored spatio-temporal attention mechanism and an efﬁcient one-shot tun-ing strategy. At inference, we employ DDIM inversion to provide structure guidance for sampling. Extensive qualita-tive and numerical experiments demonstrate the remarkable ability of our method across various applications. 1.

Introduction
The large-scale multimodal dataset [42], consisting of billions of text-image pairs crawled from the Internet, has enabled a breakthrough in Text-to-Image (T2I) genera-tion [31, 36, 6, 43, 41]. To replicate this success in Text-to-Video (T2V) generation, recent works [43, 16, 19, 54, 48] have extended spatial-only T2I generation models to the spatio-temporal domain. These models generally adopt the standard paradigm of training on large-scale text-video datasets (e.g., WebVid-10M [2]). Although this paradigm produces promising results for T2V generation, it requires extensive training on large hardware accelerators, which is expensive and time-consuming.
Humans possess the ability to create new concepts, ideas, or things by utilizing their existing knowledge and the infor-mation provided to them. For example, when presented a video with a textual description of “a man skiing on snow”, we can imagine how a panda would ski on snow, drawing upon our knowledge of what a panda looks like. As T2I models pretrained with large-scale image-text data already capture knowledge of open-domain concepts, an intuitive question arises: can they infer other novel videos from a sin-gle video example, like humans? A new T2V generation set-ting is therefore introduced, namely, One-Shot Video Tun-ing, where only a single text-video pair is used to train a
T2V generator. The generator is expected to capture essen-tial motion information from the input video and synthesize novel videos with edited prompts.
Intuitively, the key to successful video generation lies in preserving the continuous motion of consistent objects. So we make the following observations on state-of-the-art T2I diffusion models [38] that motivate our method accordingly. (1) Regarding motion: T2I models are able to generate im-ages that align well with the text, including the verb terms.
For example, given the text prompt “a man is running on the beach”, the T2I models produce the snapshot where a man is running (not walking or jumping), albeit not necessarily in a continuous manner (the ﬁrst row of Fig. 2). This serves as evidence that T2I models can properly attend to verbs via cross-modal attention for static motion generation. (2) Re-garding consistent objects: Simply extending the spatial self-attention in the T2I model from one image to multiple images produces consistent content across frames. Taking
!! !" !# !$ spatial self-attention
!! !" !# !$
!!
!"
!#
!$
!!
!"
!#
!$ spatio-temporal  attention
“A man is running on the beach”
!!
!"
!$
!#
Figure 2: Observations on pretrained T2I models: 1) They can generate still images that accurately represent the verb terms. 2) Extending spatial self-attention to spatio-temporal attention produces consistent content across frames. the same example, when we generate consecutive frames in parallel with extended spatio-temporal attention, the same man and the same beach can be observed in the resultant sequence though the motion is still not continuous (the sec-ond row of Fig. 2). This implies that the self-attention layers in T2I models are only driven by spatial similarities rather than pixel positions.
We implement our ﬁndings into a simple yet effective method called Tune-A-Video. Our method is based on a simple inﬂation of state-of-the-art T2I models over spatio-temporal dimension. However, using full attention in space-time inevitably leads to quadratic growth in computation.
It is thus infeasible for generating videos with increasing frames. Additionally, employing a naive ﬁne-tuning strat-egy that updates all the parameters can jeopardize the pre-existing knowledge of T2I models and hinder the generation of videos with new concepts. To tackle these problems, we introduce a sparse spatio-temporal attention mechanism that only visits the ﬁrst and the former video frame, as well as an efﬁcient tuning strategy that only updates the projection ma-trices in attention blocks. Empirically, these designs main-tain consistent objects across all frames but lack continu-ous motion. Therefore, at inference, we further seek struc-ture guidance from input video through DDIM inversion, which is a reverse process of DDIM sampling [44]. With the inverted latent as initial noise, we produce temporally-coherent videos featuring smooth movement. Notably, our method is inherently compatible with exiting personalized and conditional pretrained T2I models, such as Dream-Booth [40] and T2I-Adapter [30], providing a personalized and controllable user interface.
We showcase remarkable results of Tune-A-Video across a wide range of applications for text-driven video genera-tion (see Fig. 1). We compare our method against the state-of-the-art baselines through extensive qualitative and quan-titative experiments, demonstrating its superiority. In sum-mary, our key contributions are as follows:
• We introduce a new setting of One-Shot Video Tun-ing for T2V generation, which eliminates the burden of training with large-scale video datasets.
• We present Tune-A-Video, which is the ﬁrst frame-work for T2V generation using pretrained T2I models.
• We propose efﬁcient attention tuning and structural inversion that signiﬁcantly improve temporal consis-tency of generated videos.
• We demonstrate remarkable results of our method on various applications through extensive experiments. 2.