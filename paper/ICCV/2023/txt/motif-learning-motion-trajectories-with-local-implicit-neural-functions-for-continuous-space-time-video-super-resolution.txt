Abstract
This work addresses continuous space-time video super-resolution (C-STVSR) that aims to up-scale an input video both spatially and temporally by any scaling factors. One key challenge of C-STVSR is to propagate information tem-porally among the input video frames. To this end, we
It introduce a space-time local implicit neural function. has the striking feature of learning forward motion for a continuum of pixels. We motivate the use of forward mo-tion from the perspective of learning individual motion tra-jectories, as opposed to learning a mixture of motion tra-jectories with backward motion. To ease motion interpo-lation, we encode sparsely sampled forward motion ex-tracted from the input video as the contextual input. Along with a reliability-aware splatting and decoding scheme, our framework, termed MoTIF, achieves the state-of-the-art performance on C-STVSR. The source code of MoTIF is available at https://github.com/sichun233746/MoTIF. 1.

Introduction
This work addresses continuous space-time video super-resolution (C-STVSR). The task of C-STVSR is to increase simultaneously the spatial resolution and temporal frame-rate of an input video by any scaling factors with only one single model.
It is to be distinguished from fixed-scale space-time video super-resolution (F-STVSR), for which a model is learned to perform space-time super-resolution for only one specific spatiotemporal scale. As compared to F-STVSR, C-STVSR is more flexible and practical in real-world scenarios, which often call for up-scaling low-resolution and low-frame-rate videos of varied spatiotem-poral resolutions on heterogeneous video-enabled devices.
C-STVSR remains largely under-explored. One trivial solution to C-STVSR is to perform continuous video frame interpolation [2, 11, 21, 20, 33], followed by interpolat-*Both authors contributed equally to this work. (a) VideoINR [6] (b) MoTIF
Figure 1: Illustrations of (a) VideoINR [6] and (b) MoTIF.
The red dash lines highlight their major differences. ing individual video frames with continuous image super-resolution [5, 34, 14], or the other way around. However, their divide-and-conquer nature of treating C-STVSR as two independent sub-tasks–i.e. temporal interpolation and spatial super-resolution–misses the opportunity to attain the best achievable performance. By leveraging the spatiotem-poral information in an end-to-end optimized fashion, some recent works [10, 31, 32, 9] for F-STVSR adopt a one-stage approach, combining the extraction of individual frame fea-tures and the temporal aggregation of these features as a uni-fied task. Nonetheless, these F-STVSR methods can hardly be extended straightforwardly to C-STVSR.
Inspired by continuous image super-resolution [5, 34, 14], VideoINR [6] presents an early attempt at C-STVSR.
Given any query coordinates (x, y, t) in the continuous spa-(a) Backward Motion (b) Forward Motion
Figure 2: Illustration of backward and forward motion. The circles denote pixels accessible in the input video. The dashed lines display the motion trajectories of pixels in the reference frame at t = 0. The blue arrows are back-ward/forward motion in the form of displacement vectors.
The red arrows show the displacement vectors for an arbi-trary time instance that are to be predicted from blue arrows. tiotemporal space, it takes the latent representation of the in-put video as the contextual information to decode the corre-sponding RGB value. The process involves learning a spa-tial implicit neural function (S-INF in Fig. 1 (a)) for super-resoluting the frame features, followed by learning another temporal implicit neural function (T-INF in Fig. 1 (a)) to generate motion estimates at time t to backward warp the super-resoluted frame features. However, learning implic-itly backward motion (indicating displacement vectors that identify matching pixels/features in the reference frame) as a function of time is challenging. Essentially, the backward motion at the same spatial coordinates (x, y) yet at different time instances t may capture the motion trajectories of dif-ferent pixels/features in the reference frame. For example, in Fig. 2 (a), the backward motion vectors of p2 at t = 1 and t = 2 are governed by the two distinct motion trajec-tories that originate from pixels p1 and p2 in the reference frame at t = 0, respectively. In other words, the backward motion vectors at p2, when viewed as a function of time, are a mixture of multiple motion trajectories. This could poten-tially introduce undesirable randomness and discontinuities in the resulting time function, which must be learned by T-INF in Fig. 1 (a). Furthermore, learning implicitly such a time function based solely on frame features complicates the task.
To circumvent the aforementioned issues, we propose learning forward motion of pixels in the form of motion trajectories with a space-time implicit neural function (ST-INF in Fig. 1 (b)). Considering each reference frame in the input video as sitting at the origin in time, our ST-INF takes (x, y, t) as input and outputs a displacement vector that specifies where the pixel at the coordinates (x, y) of the reference frame will appear in a synthesized frame at time t. That is, it encodes the motion trajectory of the
Figure 3: Illustration of fixed-scale video frame interpo-lation (F-VFI), continuous video frame interpolation (C-VFI), fixed-scale video super-resolution (F-VSR), fixed-scale space-time video super-resolution (F-STVSR), TM-Net [32], and continuous space-time video super-resolution (C-STVSR) in terms of their supported space-time scales. pixel at (x, y), e.g. the highlighted motion trajectory of p2 in the reference frame at t = 0 in Fig. 2 (b). More-over, to facilitate the learning of such a neural function in an explicit way, we supply forward optical flow maps esti-mated between reference frames as the contextual informa-tion (i.e. M L 1→0 in Fig. 1 (b)). Our space-time neu-ral function is also learned to predict the reliability of every motion trajectory (i.e. ˆZ H 0→t, ˆZ H 1→t in Fig. 1 (b)), which is essential to ensure the quality of forward warping. Explicit motion modeling allows us to extract rough reliability esti-mates from the input video for better prediction. 0→1, M L
Fig. 1 (b) depicts our end-to-end trainable C-STVSR framework, MoTIF. The main contributions of our work include: (1) we propose a space-time local implicit neu-ral function that predicts forward motion and its reliability in a continuous manner; (2) we propose a reliability-aware splatting and decoding scheme that fuses simultaneously in-formation from multiple reference frames; and (3) our Mo-TIF achieves the state-of-the-art performance on C-STVSR and provides out-of-distribution generalization. 2.