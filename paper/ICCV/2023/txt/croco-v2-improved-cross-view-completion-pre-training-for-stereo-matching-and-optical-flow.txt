Abstract
Despite impressive performance for high-level down-stream tasks, self-supervised pre-training methods have not yet fully delivered on dense geometric vision tasks such as stereo matching or optical flow. The application of self-supervised concepts, such as instance discrimination or masked image modeling, to geometric tasks is an active area of research. In this work, we build on the recent cross-view completion framework, a variation of masked image modeling that leverages a second view from the same scene which makes it well suited for binocular downstream tasks.
The applicability of this concept has so far been limited in at least two ways: (a) by the difficulty of collecting real-world image pairs – in practice only synthetic data have been used – and (b) by the lack of generalization of vanilla transformers to dense downstream tasks for which relative position is more meaningful than absolute position. We ex-plore three avenues of improvement. First, we introduce a method to collect suitable real-world image pairs at large scale. Second, we experiment with relative positional em-beddings and show that they enable vision transformers to perform substantially better. Third, we scale up vision transformer based cross-completion architectures, which is made possible by the use of large amounts of data. With these improvements, we show for the first time that state-of-the-art results on stereo matching and optical flow can be reached without using any classical task-specific tech-niques like correlation volume, iterative estimation, image warping or multi-scale reasoning, thus paving the way to-wards universal vision models. 1.

Introduction
Self-supervised pre-training methods aim at learning rich representations from large amounts of unannotated data, which can then be finetuned on a variety of down-stream tasks. This requires the design of pretext tasks, for
Figure 1. Pre-training for dense geometric tasks. We pre-train a generic architecture, with a monocular encoder and a binocular decoder, with cross-view completion before finetuning it on the stereo matching or optical flow downstream task. which supervision signal can be extracted from the data it-self, as well as generic architectures that can be easily trans-ferred. We hypothesize that successfully pre-training large models for geometric tasks such as stereo matching or op-tical flow, see Figure 1, requires three things all together: (a) a well-designed dense pretext task inciting the under-standing of 3D scene layout and geometry, (b) an archi-tecture that processes pairs of images, suitable for different downstream tasks, and (c) large-scale real-world data.
Early self-supervised methods proceeded by discarding part of the signal (e.g. image color [95], patch ordering [55] or image orientation [25]) and trying to recover it. Later methods based on instance discrimination [12, 13, 16, 30] were first to surpass supervised pre-training on high-level tasks: they are based on the idea that output features should be invariant to well-designed classes of augmentations. An-other recently successful pretext task is masked image mod-eling (MIM) [2, 22, 29, 81, 84, 100], where part of the input data is masked and an auto-encoder is trained to re-store the full signal from the remaining visible parts. In-stance discrimination and MIM methods have achieved ex-cellent performance on semantic tasks such as image clas-sification, in particular with limited amounts of annotated data [2, 17, 69], but have not led to breakthroughs in more geometric tasks like stereo matching and optical flow.
Adapting self-supervised pre-training to geometric vi-sion tasks is an active area of research. Attempts have been made to design contrastive learning objectives at the pixel or patch level [80, 83, 85], but their performance gains have so far been more moderate than for global tasks. Be-sides, these gains are mainly demonstrated for dense seman-tic tasks such as semantic segmentation or object detection, rather than for geometric tasks such as depth estimation or stereo matching. Recently, [82] proposed the pretext task of cross-view completion (CroCo), a variant of MIM where a partially masked input image is reconstructed given visible patches and an additional view of the same scene. This pre-training objective is well suited to geometric downstream tasks as (a) it leverages pairs of images and (b) extracting relevant information from the second view requires geo-metric understanding of the scene. The CroCo architecture consists of a vision transformer (ViT) [20] encoder to ex-tract features for the non-masked tokens of the first image, as well as for the second reference image, and a transformer to decode the features and reconstruct the masked image, as illustrated in Figure 2.
In spite of these advances, leveraging cross-view com-pletion for geometric vision tasks remains challenging for at least two reasons. First, training with cross-view com-pletion requires image pairs depicting the same scene; this can be hard to acquire at scale, yet scale is the cornerstone of the success of self-supervised pre-training. In practice, the CroCo model of [82] is pre-trained solely with synthetic data, which may limit its final performance. Second, most models trained with masking rely on ViTs [20], which typ-ically use absolute positional embeddings. These do not generalize well to new image resolutions when finetuning, and are not always robust to cropping. This limits the appli-cability of current cross-view completion methods and may explain why the downstream tasks presented in [82] mostly use low-resolution squared images.
In this paper, we propose solutions to these limitations that enable to pre-train a large-scale cross-view completion model, see Figure 2, leading to state-of-the-art performance on stereo matching and optical flow. First, we tackle the problem of scalable pair collection, and gather millions of training pairs from different real-world datasets which cover various scenarios like indoor environments, street view data and landmarks, see Figure 3. To generate high-quality pre-training pairs, we carefully control the visual overlap for each pair of images. In fact, pairs with high overlap make the task trivial, whereas pairs with negligible overlap reduce it to standard MIM [82]. To measure this overlap, we lever-age extra information available such as 3D meshes, addi-tional sensors like LIDAR, or Structure-from-Motion (SfM)
Figure 2. Overview of the improvements in CroCo v2 for cross-view completion pre-training: (a) collecting and using real-world images, (b) using rotary positional embeddings which model relative token positions, instead of absolute positions using the standard cosine embedding, (c) increasing network size both in the encoder and the decoder. reconstructions for datasets with sufficient image coverage.
From these data, we generate a set of high quality image pairs with sufficient overlap and viewpoint difference while also ensuring high diversity between pairs. Second, these large-scale datasets of pre-training pairs allow to scale up the model: (a) we use a larger encoder to extract better image features and (b) also scale up the decoder, which is responsible for combining information coming from the two views. Third, instead of the standard cosine positional embedding which encodes absolute positional information, we rely on the Rotary Positional Embedding (RoPE) [71] which efficiently injects relative positional information of token pairs in the attention mechanism.
We finetune our pre-trained model, referred to as
CroCo v2, with this improved cross-view completion scheme on stereo matching and optical flow using a Dense
Prediction Transformer (DPT) [59] head. Our models, termed CroCo-Stereo and CroCo-Flow, are simple and generic: we rely on a plain ViT encoder, followed by a plain transformer decoder which directly predicts the out-put (disparity for stereo, or optical flow) through the DPT head. We believe this is a meaningful step towards a univer-sal vision model, i.e., that can solve numerous vision tasks with a common architecture. In contrast to state-of-the-art methods for stereo matching or optical flow, our architec-ture does not rely on task-specific designs such as cost vol-umes [31, 34, 38, 39, 90], image warping [10, 74], iterative refinement [43, 46, 77] or multi-level feature pyramids [18, 43, 74]. While task-specific structures and prior knowledge may yield more data-efficient approaches, they come at the cost of being tailored to a single task. Our proposed pre-training allows us to eschew these and still reaches state-of-the-art performance on various stereo matching and optical flow benchmarks such as KITTI 2015 [53], ETH3D [65],
Spring [52] or MPI-Sintel [11]. 2.