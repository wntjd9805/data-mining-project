Abstract
This paper presents a novel approach to generating the 3D motion of a human interacting with a target object, with a focus on solving the challenge of synthesizing long-range and diverse motions, which could not be fulfilled by exist-ing auto-regressive models or path planning-based meth-ods. We propose a hierarchical generation framework to solve this challenge. Specifically, our framework first gen-erates a set of milestones and then synthesizes the motion along them. Therefore, the long-range motion generation could be reduced to synthesizing several short motion se-quences guided by milestones. The experiments on the
NSM, COUCH, and SAMP datasets show that our approach outperforms previous methods by a large margin in both quality and diversity. The source code is available on our project page https://zju3dv.github.io/hghoi. 1.

Introduction
Scene-aware motion generation [19] aims to synthesize 3D human motion given a 3D scene model to enable vir-tual humans to naturally wander around scenes and interact with objects, which has a variety of applications in AR/VR, filmmaking, and video games.
Unlike traditional motion generation methods for char-acter control which aim to generate short or repeated motion on the fly guided by a userâ€™s control signals [56], we focus on the setting of generating long-term human-object inter-actions [56, 19, 76] given a starting position of the human and a target object model. This setting brings in new chal-lenges. First, the entire approaching process and the human-object interaction should be coherent, which requires the capability of modeling long-range interaction between the human and the object. Second, in the context of content generation, the generative model should be able to synthe-size diverse motions as there are many plausible ways for a real human to approach and interact with the target object.
*Corresponding author.
Figure 1. Genreation of human-object interactions. Given an object, our method first predicts a set of milestones, where the rings indicate the positions and the humans with pink clothes rep-resent the local poses. Then the motions are infilled between mile-stones. This figure shows that our method can generate diverse milestones and motions with the same object. The flow of time is shown with a color code where darker blue denotes the later frame.
Existing methods for motion synthesis can be roughly characterized into online generation and offline generation.
Most online methods [56, 19, 76] focus on real-time con-trol of characters. Given a target object, they generally use auto-regressive models to recurrently generate future mo-tions by feeding their predictions. Although they have been widely used for interactive scenarios like video games, their motion quality is not satisfactory enough for long-term gen-eration [63]. A plausible cause is the error accumulation in the auto-regressive process, where the errors in previous predictions are fed back as the model input, as discussed in [40, 26, 45, 46, 22]. To improve the motion quality,
some recent offline methods [6, 65, 64, 63] employ a multi-stage framework, which first generates the trajectory and then synthesizes motions. TDNS [63] generates paths by combining the cVAE model [33] and deterministic planning methods like A* [18]. Although this strategy can produce reasonable paths, the path diversity is limited, as demon-strated by our experimental results in Sec. 4.4.
In this paper, we propose a novel offline approach for synthesizing long-term and diverse human-object interac-tions. Our innovation lies in a hierarchical generation strat-egy, which first predicts a set of milestones and then gener-ates human motions between milestones. Fig. 1 illustrates the basic idea. Specifically, given the starting position and the target object, we design a milestone generation module to synthesize a set of milestones along the motion trajectory, each of which encodes the local pose and indicates the tran-sition point during the human movement. Based on these milestones, we employ a motion generation module to pro-duce the full motion sequence. Thanks to the milestones, we simplify the long-sequence generation into synthesizing several short motion sequences. Furthermore, the local pose at each milestone is generated by a transformer that consid-ers the global dependency [62], leading to temporally con-sistent results, which further contribute to coherent motions.
In addition to our hierarchical generation framework, we further exploit diffusion models [53, 23, 54] to synthesize human-object interactions. Previous diffusion models for motion synthesis [31, 59] combine transformer [62] and De-noising Diffusion Probabilistic Model (DDPM) [23]. Di-rectly applying them to our setting is prohibitively compute-intensive due to the long motion sequences and may lead to the GPU memory explosion [50]. Because our hierarchical generation framework converts the long-term generation to the synthesis of several short sequences, the required GPU memory is reduced to the same level of short-term motion generation. Therefore, we can efficiently leverage the trans-former DDPM to synthesize long-term motion sequences, which improves the generation quality.
We validate our design choices on the NSM [56],
COUCH [76], and SAMP [19] datasets with extensive ex-periments. On these datasets, our hierarchical framework outperforms previous methods significantly in both motion quality and diversity. 2.