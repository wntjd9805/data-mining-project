Abstract
The images and sounds that we perceive undergo sub-tle but geometrically consistent changes as we rotate our heads. In this paper, we use these cues to solve a problem we call Sound Localization from Motion (SLfM): jointly es-timating camera rotation and localizing sound sources. We learn to solve these tasks solely through self-supervision.
A visual model predicts camera rotation from a pair of im-ages, while an audio model predicts the direction of sound sources from binaural sounds. We train these models to generate predictions that agree with one another. At test time, the models can be deployed independently. To obtain a feature representation that is well-suited to solving this challenging problem, we also propose a method for learning an audio-visual representation through cross-view binau-ralization: estimating binaural sound from one view, given images and sound from another. Our model can success-fully estimate accurate rotations on both real and synthetic scenes, and localize sound sources with accuracy competi-tive with state-of-the-art self-supervised approaches. Project site: https://ificl.github.io/SLfM . 1.

Introduction
As you rotate your head, the images and sounds that you perceive change in geometrically consistent ways. For example, after turning to the right, a sound source that was directly in front of you will become louder in your left ear and quieter in your right, while simultaneously the visual scene will move right-to-left across your visual field (Fig. 1).
We hypothesize that these co-occurring audio and visual signals provide “free” supervision that captures geometry, including the motion made by a camera and the direction of sound sources. These are each core problems in machine perception, but are largely studied separately, often using supervised methods that rely on difficult-to-acquire labeled training data, such as annotated sound directions. We take in-spiration from self-supervised approaches to structure from motion [104], which learn to estimate 3D structure and cam-era pose by solving both tasks simultaneously.
Figure 1: Images and sounds change in geometrically consistent ways. For example, when we rotate to the right, a sound source that is initially in front of us becomes louder in our left ear. We use these cues to jointly train models for two tasks: localizing sounds from binaural audio and estimating camera rotation from images.
The two models are trained entirely through self-supervision, by learning to produce outputs that agree with one other.
Analogously, we propose a problem we call sound lo-calization from motion (SLfM): jointly estimating camera rotation from images and the sound direction from binaural audio. By solving both tasks simultaneously, we avoid the need for labeled training data. Our models provide each other with self-supervision: a visual model predicts the ro-tation angle between pairs of images, while an audio model predicts the azimuth of sound sources. We force their pre-dictions to agree with one another, such that changes in rotation are consistent with changes in sound direction and binaural cues. After training, the models can be deployed independently, without multimodal data at test time.
This is a challenging task that requires perceiving motion in images and binaural cues in audio. Our second contribu-tion is a method for learning representations that are well-suited to this task through cross-view binauralization. We train a network to convert mono to binaural sound for one viewpoint, given an audio-visual pair sampled from another 1
viewpoint. Since the sound source is not necessarily visible in the images, the only way to successfully solve this pretext task is by analyzing the changes in the camera pose and predicting how they affect the sound direction.
All components of our model are entirely self-supervised and are trained solely on unlabeled audio-visual data. Our results suggest that paired audio-visual data provides a use-ful and complementary signal for learning about geometry.
In contrast to other audio or visual self-supervised pose es-timation methods, we obtain supervision from abundantly available audio data, thus avoiding the need of 3D ground truth or correspondences between pixels [104, 106] or audio samples [17]. Through experiments, we show:
• Paired audio-visual data provides a supervisory signal for pose estimation tasks.
• We obtain competitive performance with state-of-the-art self-supervised sound localization methods [17].
• We obtain strong rotation estimation performance, and our model generalizes to Stanford2D3D [6] dataset, where it is competitive with classic sparse feature matching methods.
• The features we learn through our pretext task outperform other representations for our downstream tasks. 2.