Abstract
In 3D human action recognition, limited supervised data makes it challenging to fully tap into the modeling poten-tial of powerful networks such as transformers. As a re-sult, researchers have been actively investigating effective
In this work, we self-supervised pre-training strategies. show that instead of following the prevalent pretext task to perform masked self-component reconstruction in hu-man joints, explicit contextual motion modeling is key to the success of learning effective feature representation for 3D action recognition. Formally, we propose the Masked
Motion Prediction (MAMP) framework. To be specific, the proposed MAMP takes as input the masked spatio-temporal skeleton sequence and predicts the correspond-ing temporal motion of the masked human joints. Con-sidering the high temporal redundancy of the skeleton se-quence, in our MAMP, the motion information also acts as an empirical semantic richness prior that guide the masking process, promoting better attention to semanti-cally rich temporal regions. Extensive experiments on
NTU-60, NTU-120, and PKU-MMD datasets show that the proposed MAMP pre-training substantially improves the performance of the adopted vanilla transformer, achiev-ing state-of-the-art results without bells and whistles. The source code of our MAMP is available at https:// github.com/maoyunyao/MAMP. 1.

Introduction
How to accurately recognize human actions has been a long-standing challenge in computer vision. Recently, with the advances in techniques of depth sensing and pose esti-mation [3, 16, 54], skeleton-based 3D human action recog-nition has become an emerging problem to the community, which is of great significance in a series of applications such
*Corresponding authors: Wengang Zhou and Houqiang Li (a) Comparison of pre-training objectives. (b) Comparison of linear probing accuracy.
Figure 1. Illustration of (a) pre-training objective comparison be-tween masked auto encoders (MAE) and our masked motion pre-dictors (MAMP) and (b) performance comparison between the typical MAE method, i.e., SkeletonMAE [53], and our MAMP under the linear evaluation protocol. as human-computer interaction, video surveillance, virtual reality, etc. Despite the computation efficiency and back-ground robustness of skeletons, existing supervised 3D ac-tion recognition methods [5, 7, 14, 19, 24, 25, 32, 39, 42, 43, 60] heavily rely on well-annotated training sequences, which are labor-intensive and time-consuming to acquire.
Furthermore, limited supervision also leads to the overfit-ting issue in general models, especially for transformers that are with weak inductive bias and high model capac-ity. These facts motivate the exploration of self-supervised 3D action representation learning.
In the literature, the prevalent pretext tasks originally de-veloped for images have been adapted for 3D action repre-sentation learning, such as colorization [58], reconstruction
[62, 46, 27], contrastive learning [23, 48, 49], etc. Among them, contrastive learning once dominated 3D action repre-sentation learning with its concise framework and promis-ing performance. Nevertheless, as a global representation learner, it still suffers from certain limitations, such as the lack of explicit constraints for temporal context modeling and the over-reliance on heuristic action data augmentations
[31], impeding its further exploration of 3D actions.
Recently, as transformers flourish in computer vision, masked autoencoder (MAE) [17] has attracted a surge of research interest for its exceptional performance. Given that a 3D skeleton serves as an abstract representation of human behaviors, there has been growing interest in ap-plying the MAE concept to 3D action representation learn-ing, to capture the underlying spatio-temporal dynamics of skeleton sequences. Early attempts generally followed the practice of images, employing masked self-reconstruction of human joints as the pre-training pretext. Despite consid-erable effort, we argue that the network is not effectively directed to prioritize contextual motion modeling in such a self-reconstruction objective, which is, however, crucial for comprehending 3D actions as the appearance information is greatly erased in human skeletons. How to better explore the contextual motion clue in self-supervised 3D action rep-resentation learning is a valid problem.
By consolidating this idea, we introduce Masked Motion
Prediction (MAMP), a simple yet effective framework to address the problem of self-supervised 3D action represen-tation learning. Specifically, the proposed MAMP takes as input the masked spatio-temporal skeleton sequence and turns to predict the corresponding temporal motion of the masked human joints. In this way, the network is directly encouraged for contextual motion modeling. Moreover, given the observation that moments with significant motion are often critical for human action understanding, in our
MAMP, the temporal motion is used not only as the pre-training objective but also as an empirical semantic rich-ness prior that effectively guiding the skeleton masking process. Compared to the random version, the proposed motion-aware masking strategy takes additional temporal motion intensity as input. It first converts the input inten-sity into a probability distribution and then utilizes the re-parameterization technique for efficient probability-guided masked token sampling. As a result, joints with significant motion are masked with a higher probability, facilitating better attention to semantically rich temporal regions.
As illustrated in Figure 1, compared to masked self-reconstruction of human joints, masked motion prediction acts as a more effective pretext task for 3D action repre-It substantially alleviates the problem sentation learning. that the transformers cannot fully unleash their modeling potential for human actions due to the scarcity of annotated 3D skeletons. The adopted vanilla transformer sets a se-ries of state-of-the-art records in 3D action recognition after
MAMP pre-training, without the need for bells and whistles such as multi-stream ensembling. Specifically, compared to training from scratch, our MAMP demonstrates significant absolute performance improvements of 10.0% and 13.2% on the challenging cross-subject protocol of NTU RGB+D 60 [38] and NTU RGB+D 120 [28] datasets, resulting in top-1 accuracy of 93.1% and 90.0%, respectively. We hope this simple yet effective framework will serve as a strong baseline that facilitates future research on 3D action pre-training and beyond.
Overall, we make the following three-fold contributions:
• We present masked motion prediction to learn 3D ac-tion representation, which substantially alleviates the insufficient contextual motion modeling issue in the conventional masked self-reconstruction paradigm.
• We devise the motion-aware masking strategy, which incorporates motion intensity as an empirical semantic richness prior for adaptive joint masking.
• We conduct extensive experiments on three preva-lent benchmarks to verify the effectiveness of our method. Remarkably, with our proposed MAMP, the vanilla transformer, for the first time, achieves the top-performing record for 3D action recognition. 2.