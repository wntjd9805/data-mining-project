Abstract
Deep clustering can optimize representations of in-stances (i.e., representation learning) and explore the in-herent data distribution (i.e., clustering) simultaneously, which demonstrates a superior performance over conven-tional clustering methods with given features. However, the coupled objective implies a trivial solution that all instances collapse to the uniform features. To tackle the challenge, a two-stage training strategy is developed for decoupling, where it introduces an additional pre-training stage for rep-resentation learning and then fine-tunes the obtained model for clustering. Meanwhile, one-stage methods are devel-oped mainly for representation learning rather than cluster-ing, where various constraints for cluster assignments are designed to avoid collapsing explicitly. Despite the success of these methods, an appropriate learning objective tailored for deep clustering has not been investigated sufficiently. In this work, we first show that the prevalent discrimination task in supervised learning is unstable for one-stage clus-tering due to the lack of ground-truth labels and positive instances for certain clusters in each mini-batch. To miti-gate the issue, a novel stable cluster discrimination (SeCu) task is proposed and a new hardness-aware clustering cri-terion can be obtained accordingly. Moreover, a global en-tropy constraint for cluster assignments is studied with effi-cient optimization. Extensive experiments are conducted on benchmark data sets and ImageNet. SeCu achieves state-of-the-art performance on all of them, which demonstrates the effectiveness of one-stage deep clustering. 1.

Introduction
Clustering is a fundamental task in unsupervised learn-ing. Given features of instances, the unlabeled data set will be partitioned into multiple clusters, where instances from the same cluster are similar according to the measurement defined by a distance function. With fixed features, most of research efforts focus on studying appropriate distance functions and ingenious algorithms have been proposed by different measurements, e.g., k-means clustering [24], spec-k-means in CoKe: acc=0.9 our proposal SeCu: acc=1.0
Figure 1: An illustration of the proposed method. 10 data points are randomly sampled from two different Gaussian distributions, respectively. Points with the same color are from the same distribution, while squares denote the corre-sponding cluster centers obtained by different methods. Star indicates the mis-classified data. Unlike k-means that as-signs the uniform weight for different instances, our method considers the hardness of instances and is better for discrim-ination based clustering. tral clustering [28], subspace clustering [13], etc.
With the development of deep neural networks, deep learning is capable of learning representations from raw materials and demonstrates the dominating performance on various supervised tasks [22]. Thereafter, it is also intro-duced to clustering recently [4, 11, 14, 19, 20, 23, 29, 30, 32]. Unlike the conventional clustering, representations of instances are learned with cluster assignments and centers simultaneously in deep clustering, which is more flexible to capture the data distribution. However, the coupled objec-tive can result in a trivial solution that all instances collapse to the uniform features [4] and designing appropriate clus-tering criterion in the new scenario becomes challenging.
Many deep clustering algorithms [11, 14, 32] exploit a two-stage training strategy to decouple representation learn-ing and clustering to avoid collapsing. The recent progress in unsupervised representation learning [2, 4, 7, 16, 17, 26] shows that informative features capturing the semantic simi-larity between instances without collapsing can be obtained by pre-training a large set of unlabeled data. Inspired by the observation, those methods leverage a pre-training stage and then focus on developing algorithms to optimize clus-tering by fine-tuning a pre-trained model in the second stage. By refining the relations between nearest neighbors obtained from the pre-training stage, two-stage methods achieve a significantly better performance than one-stage ones without sufficient representation learning [19, 20].
However, the objective of pre-training can be inconsis-tent with that of clustering, which results in a sub-optimal performance for deep clustering. Note that different from supervised learning where the objective is explicitly de-fined by labels, that for unsupervised representation learn-ing is arbitrary and various pretext tasks have been pro-posed, e.g., instance discrimination [7, 17], cluster discrim-ination [4, 26], masked modeling [2, 16], etc. Most of ex-isting two-stage clustering methods adopt instance discrim-ination, i.e., SimCLR [7], for pre-training, whereas it aims to identify each instance as an individual class and the ob-jective is different from clustering that aims to group in-stances from the same cluster. Moreover, SwAV [5] demon-strates that clustering itself is also an effective pretext task for representation learning. Therefore, we focus on facilitat-ing one-stage deep clustering that optimizes representations and clustering simultaneously in this work.
Most of existing one-stage methods are proposed solely for representation learning. To tackle the collapsing issue, research efforts are mainly devoted to developing appropri-ate constraints for cluster assignments, especially for online deep clustering. For example, [1, 5] apply the balanced con-straint that each cluster has the same number of instances for clustering. [26] further relaxes the balanced constraint to a lower-bound size constraint that limits the minimal size of clusters and demonstrates a more flexible assignment.
After obtaining cluster assignments as pseudo labels, representations and cluster centers can be optimized as in the supervised scenario. For example, [5, 6] learn the en-coder network and cluster centers by solving a classification problem with the standard cross entropy loss. [26] has the same classification task for representation learning while adopting k-means for updating cluster centers. Although these methods achieve a satisfied performance on represen-tation learning, a learning objective tailored for deep clus-tering has not attracted sufficient attentions.
In this work, we investigate the effective learning task for one-stage deep clustering. By analyzing the standard cross entropy loss for supervised learning, we find that it can be unstable for unsupervised learning. Concretely, the gradi-ent for updating cluster centers consists of two ingredients: gradient from positive instances of the target cluster and that from irrelevant negative ones. However, with a limited mini-batch size in stochastic gradient descent (SGD), there can be no positive ones for a large proportion of clusters (e.g., 90% on ImageNet) at each iteration. Due to the lack of positive instances, the influence from negative ones is dom-inating. Unlike supervised learning where labels are fixed, the cluster assignments can change during training in deep clustering. Therefore, the noise from the large variance of negative instances will be accumulated, which makes the optimization unstable.
To mitigate the problem, we propose a stable cluster dis-crimination (SeCu) task for deep clustering, which stops the gradient from negative instances for updating cluster centers in the cross entropy loss. Compared with k-means in [26], where positive instances have the uniform weight for up-dating centers, SeCu as a discrimination task considers the hardness of instances and a large weight will be assigned to hard instances for updating. Fig. 1 illustrates the hardness-aware clustering criterion implied by SeCu. Besides, we improve the cluster assignment by developing an entropy constraint that regularizes the entropy of assignments over the entire data set. Compared to the optimization with the size constraint [26], our method can reduce the number of variables and hyper-parameters, and thus make the learning more convenient. The main contributions of this work can be summarized as follows.
• A novel task is proposed for one-stage deep cluster-ing. SeCu tailors the supervised cross entropy loss by eliminating the influence from the negative instances in learning cluster centers, which makes the training stable when ground-truth labels are unavailable.
• A global entropy constraint is exploited to balance the size of clusters and an efficient closed-form solution is developed for online assignment with the constraint.
• A simple framework with a single loss function and en-coder is introduced for deep clustering in Eqn. 6. The proposed method is evaluated with the standard proto-col on benchmark data sets and ImageNet [27]. The superior performance of SeCu confirms its effective-ness for deep clustering. 2.