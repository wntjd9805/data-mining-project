Abstract
Tutorial videos play an increasingly important role in professional development and self-directed education. For users to realise the full benefits of this medium, tutorial videos must be efficiently searchable. In this work, we fo-cus on the task of moment detection, in which the goal is to localise the temporal window where a given event occurs within a given tutorial video. Prior work on moment de-tection has focused primarily on short videos (typically on videos shorter than three minutes). However, many tutorial videos are substantially longer (stretching to hours in dura-tion), presenting significant challenges for existing moment detection approaches.
To study this problem, we propose the first dataset of untrimmed, long-form tutorial videos for the task of
Moment Detection called the Behance Moment Detection (BMD) dataset. BMD videos have an average duration of over one hour and are characterised by slowly evolv-ing visual content and wide-ranging dialogue. To meet the unique challenges of this dataset, we propose a new framework, LONGMOMENT-DETR, and demonstrate that it outperforms strong baselines. Additionally, we introduce a variation of the dataset that contains YouTube Chapter annotations and show that the features obtained by our framework can be successfully used to boost the perfor-mance on the task of chapter detection. Code and data can be found at https://github.com/ioanacroi/ longmoment-detr. 1.

Introduction
Enabled by cheaper disk storage and networking tech-nology, long-form videos of tutorial content are proliferat-ing. As such, there is a pressing need to develop effective tools for searching within videos. In this work, we consider this problem through the lens of moment detectionâ€” given a video and a natural language query, our task is to find the
Figure 1. LONGMOMENT-DETR. Our framework performs mo-ment detection and unlike previous state of the art methods, it works on long tutorial videos. temporal span of the video that best matches the query (as shown in Fig. 1). Beyond tutorial content, this task also has applications in domains such as security and entertainment.
To study moment detection in the long-form setting, we introduce, the first database of long tutorial videos with manual annotations for validation and testing, called Be-hance Moment Detection (BMD). The videos, which are collected from the Behance platform1, consist of tutorial videos that teach skills with various creative tools such as drawing, movie editing, animation and photo editing.
Existing moment detection datasets predominantly fea-ture short videos centered on human activities, such as cooking or swimming. In contrast, our BMD dataset em-phasizes long tutorial videos that explore the use of soft-ware tools for digital artistry. Typically, these tutorials in-volve screen-sharing sessions detailing creative processes, often spanning several hours. Efficiently localizing specific segments in such long videos can greatly enhance user nav-*Work done during an internship at Adobe. Now at V7 Labs. 1https://behance.net
igation experience. This makes the BMD dataset unique for the task of moment detection. Moreover, to scale up training beyond costly manual annotation, we propose a framework for moment detection that leverages weak super-vision derived from ASR (Automatic Speech Recognition) and video segmentation. In this new long video setup, using the timings from ASR, similar to previous works [24, 17], produces poor results, as we will show experimentally. So, in order to adjust for long videos setup, we rely on a mix of using video segmentation methods [36, 37] and summa-rization [50, 27, 5, 3] in order to generate good timings and textual descriptions to use for training.
Lastly, in order to further validate our approach, we pro-pose a second dataset, called YouTube Chapters (YTC) that contains full chapter annotation for all splits. In this way, we are able to assess the quality of the learnt features on
BMD on the downstream task of chapter detection.
We summarise our contributions as: (1) we introduce the first two long tutorial video datasets: Behance Moment De-tection (BMD) a multi-modal dataset, suitable for weak su-pervision with manual annotations for validation and test-ing and YouTube Chapters (YTC) with chapters annotations for all splits (2) we propose an effective way to automati-cally generate moments for the training split of BMD that leverages Automatic Speech Recognition (ASR) and em-ploys large language models (LLMs) to eliminate the in-herent noise that appears in ASR. (3) we show the effec-tiveness of using BMD to improve the performance on the downstream task of YouTube chapter detection.
Leveraging the multi-modal features of our new dataset and utilizing automatic annotations, we present
LONGMOMENT-DETR, the first framework for moment de-tection in long tutorial videos consisting of the segment tim-ing generator and query generator. 2.