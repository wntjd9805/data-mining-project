Abstract
Smartphone photography is becoming increasingly pop-ular, but fitting high-performing camera systems within the given space limitations remains a challenge for manufactur-ers. As a result, powerful mobile camera systems are in high demand. Despite recent progress in computer vision, camera system quality assessment remains a tedious and manual process. In this paper, we present the Smartphone Camera
Quality Assessment Dataset (SQAD), which includes natu-ral images captured by 29 devices. SQAD defines camera system quality based on six widely accepted criteria: res-olution, color accuracy, noise level, dynamic range, Point
Spread Function, and aliasing. Built on thorough exami-nations in a controlled laboratory environment, SQAD pro-vides objective metrics for quality assessment, overcoming previous subjective opinion scores. Moreover, we intro-duce the task of automatic camera quality assessment and train deep learning-based models on the collected data to perform a precise quality prediction for arbitrary photos.
The dataset, codes and pre-trained models are released at https://github.com/aiff22/SQAD. 1.

Introduction
Image quality, meaning the representation of details, color, realism, etc., has shown to directly influence computer vision algorithms in achieving a more comprehensive un-derstanding of the natural world. Consequently, this evokes the question of precisely quantifying and estimating the per-ceived image quality by human viewers, as they are usually the ultimate receivers [57]. Besides subjective methods that study human perceptions given physical stimuli [28, 43], objective assessment methods fall into three main categories based on the amount of information available: no-reference, reduced-reference, or full-reference.
No-reference (NF) methods for estimating image qual-ity directly, without requiring undistorted counterparts, are more convenient in realistic vision scenarios. Common qual-ity metrics include image statistical differences, such as
BRISQUE [41], BLIINDS-II [48] parameterized by DCT
*Work done while at ETH Zurich
Figure 1: Sample images of the identical scene from our benchmark captured using different devices. The training and testing images are captured in hand-held mode in the wild, while the optical patterns, e.g. PSF, are obtained under controlled conditions with a fixed tripod 2 meters apart for benchmarking purposes. coefficients, NIQE [42] with spatial domain features, and the successor IL-NIQE [62] based on multivariate Gaus-sian models. Recently, learning-based approaches have led to improved quality assessment methods [36, 13, 38].
However, gaps still exist between perceivable image qual-ity and predicted quantitative results even with neural net-works providing better solutions than traditional methods.
To address this, new algorithms [9, 29] are being proposed, inspired by GANs [23] and perceptual-oriented optimiza-tion [30, 60, 46], that aim to further reduce this gap.
An orthogonal direction in assessing image quality is to consider the device used to take those pictures, which requires not only the recorded imagery but the correspond-ing camera devices and photometric settings. In this case, manual impairments are not employed to imitate different situations, and quality is measured directly in a more ac-curate way. Optical patterns are widely applied, making measurements more objective and meaningful [52, 11, 12].
They cover a wide range of distinct quality factors related to
Figure 2: Smartphone Camera Sensor Quality Assessment Dataset (SQAD). We present 224 Ã— 224 sized crops extracted from our dataset consisting of images captured by 29 different mobile devices. Images are arranged in descending order of resolution. Orange block scales vary due to differing image dimension settings on smartphones. Perceived sharpness relates more to PSF than resolution. The image taken with Sony Ericsson T630 is in the supplement due to its small size. the photon nature of light, properties of lenses, signal theory, and more. International standards [5, 6] provide measuring and reporting methods for still-picture cameras based on designated patterns. Besides, several popular commercial software applications [2, 1] provide camera quality tests, including main quality factors such as sharpness, noise level, dynamic range, chromatic aberration, etc. While various methods have been proposed for image quality assessment (IQA), they often have limitations and are not comprehen-sive. Although IQA methods aim to predict quantitative scores aligning with human perceptions measured by mean opinion scores (MOS) [54], they often lack objectivity, quan-tization, and physical reasoning due to diverse participant expertise levels and inherent biases. Camera-based methods are accurate and based on physical attributes, but they are complex and time-consuming, and require cameras and their parameters, making them less suitable for web images.
To address previous limitations, we are motivated to provide a standard, measurable, and objective reference, and we make the following contributions: Our proposed
SQAD benchmark combines device-based lab measurement approaches with Deep Learning to accurately quantify smart-phone camera quality from multiple perspectives using phys-ically meaningful scores. We use a semi-manual evaluation protocol for smartphone camera systems based on ISO stan-dards, covering critical quality factors such as resolution, color accuracy, noise level, dynamic range, aliasing, and
Point Spread Function (PSF). Our evaluations are conducted in real-world conditions, reflecting practical scenarios where access to individual camera components, i.e. optics and ISP, is limited. As images serve as the ultimate observation of camera quality, we therefore define the camera sensor1 to encompass all relevant components. Our image collection comprises natural scenes captured by a variety of devices, spanning from the early years of smartphone development to recent high-end phones. Furthermore, we introduce the task of automatically assessing smartphone camera quality, eliminating the need for manual measurements. An overview of our dataset is presented in Fig. 2. 1In this work, the term camera sensor refers to both hardware and ISP.
2.