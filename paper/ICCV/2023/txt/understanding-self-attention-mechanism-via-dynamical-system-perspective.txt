Abstract
The self-attention mechanism (SAM) is widely used in various fields of artificial intelligence and has successfully boosted the performance of different models. However, cur-rent explanations of this mechanism are mainly based on in-tuitions and experiences, while there still lacks direct mod-eling for how the SAM helps performance. To mitigate this issue, in this paper, based on the dynamical system perspec-tive of the residual neural network, we first show that the in-trinsic stiffness phenomenon (SP) in the high-precision so-lution of ordinary differential equations (ODEs) also widely exists in high-performance neural networks (NN). Thus the ability of NN to measure SP at the feature level is necessary to obtain high performance and is an important factor in the difficulty of training NN. Similar to the adaptive step-size method which is effective in solving stiff ODEs, we show that the SAM is also a stiffness-aware step size adaptor that can enhance the model’s representational ability to measure intrinsic SP by refining the estimation of stiffness informa-tion and generating adaptive attention values, which pro-vides a new understanding about why and how the SAM can benefit the model performance. This novel perspective can also explain the lottery ticket hypothesis in SAM, design new quantitative metrics of representational ability, and inspire a new theoretic-inspired approach, StepNet. Extensive ex-periments on several popular benchmarks demonstrate that
StepNet can extract fine-grained stiffness information and measure SP accurately, leading to significant improvements in various visual tasks. 1.

Introduction
The self-attention mechanism (SAM) [41, 15, 16, 9, 43, 4] is widely used in various artificial intelligence fields and has successfully improved the models’ performance in a number of vision tasks, including image classification
[22, 60, 47], object detection [37, 56, 25], instance seg-*co-first author
† corresponding author
Figure 1. The feature trajectory w/ and w/o attention. The GT trajectory, generated by the best-performing model, possesses the inherent property of SP. The SAM, acting as a stiffness-aware step size adaptor, effectively extracts stiffness information and has a strong representational ability to measure the inherent SP to closely approach a GT trajectory, leading to high performance. mentation [8, 52], image super-resolution [64, 46, 49], etc.
However, most previous works lay emphasis on designing a new self-attention method, and intuitively or heuristically exploring how the self-attention mechanism helps the per-formance. For example, many popular channel attention methods [22, 47, 56, 35] consider the attention values as the soft weight of the channels, leading to the importance reassignment of feature maps. These soft weights can also
be seen as a gate mechanism [60, 28] to control the for-ward transmission of information flow, which are usually applied to neural network pruning and neural architecture search [40, 65]. Another viewpoint [38] argues that the self-attention mechanism can help to regulate the noise by en-hancing instance-specific information to obtain a better reg-ularization effect. Moreover, the receptive field [62, 63, 67] and long-range dependency [69, 17, 57] are also used to un-derstand the role of self-attention. Although these explana-tions describe the behavior of self-attention mechanisms to some extent, the relationship between the SAM and model performance is still ambiguous.
To establish a more specific modeling between the SAM and model performance, in this paper, we rethink the role of the SAM with the dynamical systems perspective of neu-ral networks (NN) with residual blocks. Specifically, we first define the stiffness phenomenon (SP) and ground truth (GT) trajectory of NNs at the feature level based on stiffness metrics and the ground truth solution of ODEs.
Next, we find that the intrinsic SP observed in the high-precision solutions of ODEs is also prevalent in high-performance NNs. This observation implies that the repre-sentational ability of NN to measure SP at the feature level is necessary to obtain high performance, as shown in Fig 1, which is hindering the learning of neural networks, and advanced training strategies are needed to achieve this re-quirement.
Similar to the adaptive step-size method which is ef-fective in solving stiff ODEs, we theoretically and empiri-cally demonstrate that the SAM is a stiffness-aware step size adaptor that can refine the estimation of stiffness informa-tion and generate the adaptive attention values to measure intrinsic SP for approaching a GT trajectory which has the upper bound performance, leading to high accuracy. This novel perspective can also explain the lottery ticket hypoth-esis in SAM (LTH4SA) [27], design new quantitative met-rics of representational ability, and inspire a new theoretic-inspired approach, StepNet. Extensive experiments on sev-eral popular benchmarks show the effectiveness of StepNet in various vision tasks, including image classification and object detection. Our contributions are summarized as fol-lows: 1. We propose a novel understanding of the SAM and re-veal a close connection between the SAMs and the nu-merical solution of stiff ODEs, which is an effective explanation for understanding why and how the SAM enhances the performance of NNs. 2. Based on our novel views of SAMs, we explain the lottery ticket hypothesis in SAM, design new quantita-tive metrics of representational ability, and propose a powerful theoretic-inspired approach, StepNet. 2. The Stiffness and Self-attention Mechanism
In this section, we first introduce the concepts of stiff-ness in ODEs, SAM, and the dynamical system perspective for the NN with residual blocks. Then we further explore the SP in NNs and connect it with the SAM, which finally motivates us to propose a theoretic inspire approach. 2.1. Preliminaries and