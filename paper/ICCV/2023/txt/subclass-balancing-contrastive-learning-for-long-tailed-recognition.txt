Abstract
Long-tailed recognition with imbalanced class distri-bution naturally emerges in practical machine learning applications. Existing methods such as data reweighing, resampling, and supervised contrastive learning enforce the class balance with a price of introducing imbalance between instances of head class and tail class, which may ignore the underlying rich semantic substructures of the former and exaggerate the biases in the latter. We overcome these drawbacks by a novel “subclass-balancing contrastive learning (SBCL)” approach that clusters each head class into multiple subclasses of similar sizes as the tail classes and enforce representations to capture the two-layer class hierarchy between the original classes and their subclasses. Since the clustering is conducted in the representation space and updated during the course of training, the subclass labels preserve the semantic substructures of head classes. Meanwhile, it does not overemphasize tail class samples, so each individual instance contribute to the representation learning equally.
Hence, our method achieves both the instance- and subclass-balance, while the original class labels are also learned through contrastive learning among subclasses from different classes. We evaluate SBCL over a list of long-tailed benchmark datasets and it achieves the state-of-the-art performance. In addition, we present extensive analyses and ablation studies of SBCL to verify its advantages. Our code is available at https://github.com/JackHck/ subclass-balancing-contrastive-learning. 1.

Introduction
In reality, the datasets often follow the Zipfian distribution over classes with a long tail [57, 75], i.e., a few classes (head classes) containing significantly more instances than the remaining tail classes. Such tail classes could be of great importance for high-stake applications, e.g., patient
*Corresponding author class in medical diagnosis or accident class in autonomous driving [6, 55]. However, training on such class-imbalanced datasets can result in a severely biased model with noticeable performance drop in classification tasks [2, 4, 13, 47, 64, 68, 71].
To overcome the challenges posed by long-tailed data, data resampling [2, 4, 9, 54] and loss reweighing [5, 6, 15, 17] have been widely applied but they cannot fully leverage all the head-class samples. Very recent work discovered that supervised contrastive learning (SCL) [36] can achieve state-of-the-art (SOTA) performance on benchmark datasets of long-tailed recognition [33, 41]. Specifically, the k-positive contrastive learning (KCL) [33] and its subsequent work targeted supervised contrastive learning (TSC) [41] revamp
SCL by encouraging the learned feature space to be class-balanced and uniformly distributed. However, those methods enforcing class-balance often come with a price of instance-imbalance, i.e., each individual instance of tail classes would have much greater impact on model training than that of head classes.
Such instance-imbalance can result in significant degra-dation of the performance on long-tailed recognition for several reasons. On the one hand, the limited samples in each tail class might not be sufficiently representative of the whole class. So even a small bias of them can be enor-mously exaggerated by class-balancing methods and result in sub-optimal learning of classifiers or representations. On the other hand, head classes usually have more complicated semantic substructures, e.g., multiple high-density regions of the data distribution, so simply downweighing samples of head classes and treating them equally can easily lose critical structural information. For example, images of a head class
“cat” might be highly diverse in breeds and colors, which need to be captured by different features but downweighing or subsampling them may easily lose such information, while a tail class “platypus” might only contain a few similar im-ages that are unlikely to cover all the representative features.
Therefore, it is non-trivial to enforce both class-balance and instance-balance simultaneously in the same method.
Can we remove the negative impact of class-imbalance while still retain the advantages of instance-balance? In this paper, we achieve both through subclass-balancing con-trastive learning (SBCL), a novel supervised contrastive learning defined on subclasses, which are the clusters within each head class, have comparable size as tail classes, and are adaptively updated during the training. Instead of sacrific-ing instance-balance for class-balance, our method achieves both instance- and subclass-balance by exploring the head-class structure in the learned representation space of the model-in-training. In particular, we propose a bi-granularity contrastive loss that enforces a sample (1) to be closer to sam-ples from the same subclass than all the other samples; and (2) to be closer to samples from a different subclass but the same class than samples from any other subclasses. While the former learns representations with balanced and compact subclasses, the latter preserves the class structure on subclass level by encouraging the same class’s subslasses to be closer to each other than to any different class’s subclasses. Hence, it can learn an accurate classifier distinguishing original classes while enjoy both the instance- and subclass-balance.
In this paper, we apply SBCL for several visual recog-nition tasks to demonstrate SBCL superiority over other previous works (e.g., KCL [33], TSC [41]). To summarize, this paper makes the following contributions: (a). We provide a new design principal of leveraging super-vised contrastive learning for long-tailed recognition, i.e., aiming at achieving both instance- and subclass-balance instead of class-balance at the expense of instance-balance. (b). We propose a novel instantiation of the aforementioned design principal, subclass-balancing contrastive learn-ing (SBCL), which consists of two major components, namely, subclass-balancing adaptive clustering and bi-granularity contrastive loss. (c). Empirically, we compare the SBCL against state-of-the-art methods on three visual tasks: image classification, object detection, and instance segmentation to demon-strate its effectiveness on handling class imbalance. We also conduct a series of experiments to analyze the efficacy of SBCL. 2.