Abstract
Combined with the generative prior of pre-trained mod-els and the ﬂexibility of text, text-driven generative domain adaptation can generate images from a wide range of tar-get domains. However, current methods still suffer from overﬁtting and the mode collapse problem. In this paper, we analyze the mode collapse from the geometric point of view and reveal its relationship to the Hessian matrix of generator. To alleviate it, we propose the spectral consis-tency regularization to preserve the diversity of source do-main without restricting the semantic adaptation to target domain. We also design granularity adaptive regulariza-tion to ﬂexibly control the balance between diversity and stylization for target model. We conduct experiments for broad target domains compared with state-of-the-art meth-ods and extensive ablation studies. The experiments demon-strate the effectiveness of our method to preserve the di-versity of source domain and generate high ﬁdelity tar-get images. Source code has been released in https:
//github.com/Victarry/Adaptation-SCR. 1.

Introduction
Generative image modeling has developed signiﬁcantly in recent years and is able to generate diverse high-resolution images even indistinguishable from real images.
However, training such models requires intense computa-tion resources and large datasets, which restricts the appli-cation scope of generative models. For some scenarios, col-lecting large datasets is impossible like paintings by speciﬁc artists. Beneﬁting from Vision-Language models learning from large image-text pairs, text can be leveraged as a de-*Corresponding Author scription of abstract visual semantics to guide generative domain adaptation instead of a collection of image samples in target domain. As an expressive representation, text has shown great success in semantic image generation and ma-nipulation recently [23, 20]. Based on the generative prior of pre-trained models and ﬂexible text description of target domain, text-driven generative domain adaptation can gen-erate more various images and have promising applications.
To reduce the requirement of training samples, tradi-tional methods propose to train generative models in the target domain with only limited samples by adapting pre-trained models in the large-scale source domain which con-tains high-level semantic knowledge as a generative prior.
These few-shot adaptation methods either ﬁnetune only a part of parameters within networks to preserve most source domain knowledge [15] or impose strong regularization on the generated images [30, 36]. However, these methods still require additional training samples of target domain and ad-versarial training process. As the number of samples drops, the image ﬁdelity and diversity also hurt severely. Different from these methods, text-driven generative domain adap-tation requires no image samples but texts to describe the target domain. Pioneer work [5] proposed to encourage the visual change between samples from target and source gen-erators to align with semantic direction described by text in the CLIP [19] embedding space, which achieves generative adaptation for miscellaneous domains in short training time.
The main challenge of text-driven GAN adaptation is the mode collapse problem due to the entanglement of intra-domain semantics and inter-domain style in text represen-tation. Besides the speciﬁed target style described by text, there also exists an unknown pattern for the semantics of images. This leads to a decrease of variations in generated images when the style effect is optimized to approach target domain. As shown in Figure 2, while the number of itera-(cid:26)(cid:47)(cid:41)(cid:39)(cid:41)(cid:45)(cid:34)(cid:43) (cid:29)(cid:42)(cid:37)(cid:49)(cid:35)(cid:40) (cid:31)(cid:42)(cid:41)(cid:54)(cid:46)(cid:2)(cid:37) (cid:32)(cid:34)(cid:45) (cid:20)(cid:46)(cid:39)(cid:40) (cid:33)(cid:37)(cid:47)(cid:37)(cid:52)(cid:46)(cid:43)(cid:38)
Figure 1. Text-driven generative domain adaptation with various text descriptions. The generated samples should both reﬂect characteristics of target domain from text and preserve the original identity. tions increases, the generated samples tend to have similar patterns of mouth and eyes, which reduces most of the vari-ations in the origin model. The main reason for the mode collapse problem is that the optimization process only cares about the distance of generated samples to target domains, and the intra-domain feature variations are easily ignored.
To address the above challenge, researchers [36] pro-posed to preserve the diversity of source domain through a within-domain consistency loss which keeps consistency between sample changes in source domain and target do-main. However, this regularization is too strong to re-strict the style effect of target generator close to source do-main. The previous theorem about GAN latents analysis has shown that the Hessian matrix of generator reﬂects the variations of generator and can be used to explore meaning-ful directions from top eigenvectors. Inspired by this, we try to leverage the spectrum of Hessian matrix as a quantita-tive evaluation of model diversity in the adaptation problem.
This disentangles the relative diversity between generated samples from absolute generative distribution and makes a general way to regularize diversity of generative model.
In this work, we propose spectral consistency regular-ization to solve the problem of mode collapse in text-driven generative domain adaptation from the geometric point of view. First, we analyze the Hessian matrix of generator’s manifold in the metric space by eigendecomposition. The eigenvalues of Hessian matrix are decreasing in the adap-tation process, which is consistent with the mode collapse problem of visual observations. Second, we introduce the spectral consistency regularization on the Hessian matrix to prevent the latent space of generator from degrading.
This regularization helps preserve intra-domain variations of source domain without restricting style effects of target generators. We further develop a stochastic method to reg-ularize the spectrum of Hessian matrix without calculating the full matrix, which reduces the expensive computational cost. Finally, we design the granularity adaptive regulariza-tion considering the layer-decomposition characteristic of
W+ space in StyleGAN.
The contributions of this paper are summarized below: 1. We analyze the commonly occurred mode collapse problem in GAN adaptation from the geometric point of view and provide a quantitative evaluation of model diversity to reveal the reason of mode collapse. 2. We propose a spectral consistency regularization for text-driven GAN adaptation, which preserves diversity of original domain and generates high ﬁdelity images of target domain. A granularity adaptive regularization is further designed to ﬂexibly control balance between diversity and stylization for target model. 3. We conduct experiments and ablation studies for a wide range of target domains. The experiments show the effectiveness of our proposed spectral consistency regularization and its applications to downstream tasks like image editing and image-to-image translation. 2.