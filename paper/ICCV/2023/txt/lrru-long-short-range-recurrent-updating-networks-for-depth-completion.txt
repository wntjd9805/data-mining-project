Abstract
Existing deep learning-based depth completion meth-ods generally employ massive stacked layers to predict the dense depth map from sparse input data. Although such approaches greatly advance this task, their accompa-nied huge computational complexity hinders their practical applications. To accomplish depth completion more effi-ciently, we propose a novel lightweight deep network frame-work, the Long-short Range Recurrent Updating (LRRU) network. Without learning complex feature representa-tions, LRRU first roughly fills the sparse input to obtain an initial dense depth map, and then iteratively updates it through learned spatially-variant kernels. Our itera-tive update process is content-adaptive and highly flexi-ble, where the kernel weights are learned by jointly con-sidering the guidance RGB images and the depth map to be updated, and large-to-small kernel scopes are dynam-ically adjusted to capture long-to-short range dependen-cies. Our initial depth map has coarse but complete scene depth information, which helps relieve the burden of di-rectly regressing the dense depth from sparse ones, while our proposed method can effectively refine it to an ac-curate depth map with less learnable parameters and in-ference time. Experimental results demonstrate that our proposed LRRU variants achieve state-of-the-art perfor-In particular, mance across different parameter regimes. the LRRU-Base model outperforms competing approaches on the NYUv2 dataset, and ranks 1st on the KITTI depth completion benchmark at the time of submission. Project page: https://npucvr.github.io/LRRU/. 1.

Introduction
Acquiring accurate and dense scene depth plays a fun-damental role in various applications, such as autonomous driving [33] and augmented reality [25]. However, exist-ing depth sensors have inevitable limitations for both indoor and outdoor scenes [39], for example, the depth acquired by
*Corresponding authors {daiyuchao, libo}@nwpu.edu.cn.
Figure 1. The performance in terms of RMSE versus runtime on the KITTI test dataset. The bubble size represents the number of parameters. LRRU performs better while maintaining efficiency.
LiDAR is too sparse to be used directly. Thus, depth com-pletion, i.e., estimating the dense depth maps from sparse distance measurements, has attracted extensive research in-terests in industry and research communities.
Recently, deep learning-based methods [32, 35] have shown dominant performance for this task, which directly maps sparse depth maps to dense depth maps through mas-sive stacked filters and layers. Since the RGB images con-tain rich semantic cues that are critical for filling unknown depth, some works [22, 23, 30, 42] utilize the RGB informa-tion to guide depth completion. Although many advanced networks such as ResNet [10] and Transformer [8, 29, 43] have been exploited, it is still difficult to directly predict an accurate and dense depth map from a sparse input depth map and the corresponding RGB image. Specifically, exist-ing methods employ tens of millions of learnable parame-ters in exchange for a desirable model capacity to learn ro-bust features, for example, 132M parameters are contained in PENet [11]. Such large-scale networks usually require heavy computing resources, which fail to be applied in the real world, while the method performance drops signifi-cantly if the network size is simply reduced [13]. In addi-tion, the predicted depth maps obtained by direct regression suffer from blur effect and distortion of object boundaries, which need to be further refined through extra refinement modules [4]. For example, the popular spatial propagation
networks (SPNs) [3, 4, 11, 17, 21, 26, 41] update the output of the direct-regression methods by a recurrent operation.
Thus, designing an efficient depth completion architecture that performs better while maintaining efficiency is essen-tial for further research.
Inspired by the success of the recurrent design [18, 31, 34], we propose the Long-short Range Recurrent Updat-ing (LRRU) network, a novel lightweight deep network framework for depth completion. Unlike existing direct-regression methods, LRRU iteratively updates an initial depth map obtained by a non-learning approach [15]. The initial depth map has coarse but complete scene depth in-formation, which can help relieve the heavy burden of di-rectly regressing the precise dense depth from sparse input depth map. Although existing SPNs [3, 4, 11, 17, 21, 26, 41] have shown that the depth map can be refined by learned spatially-variant kernels that model relevant neighbors and their affinities of each pixel, these methods cannot be di-rectly used in our framework due to the following limita-tions: (a) content-agnostic update unit: the kernel parame-ters required for updating are predicted by the features from
RGB and sparse depth, which are not adaptively adjusted to the target map (the depth map to be updated); (b) inflexible recurrent strategy: the kernel scope is fixed during the up-date process, and multiple iterations are required to obtain long-range dependencies and satisfactory results.
To address the above issues, we propose a Target-Dependent Update (TDU) unit and a long-short range re-current strategy, which make our iterative update process content-adaptive and highly flexible. Our TDU predicts the sampling position of the neighbors and the weights (affini-ties) between them and the reference point by jointly con-sidering the cross-guided and self-guided features. The cross-guided features extracted from RGB images and sparse depths can guide the TDU to avoid irrelevant neigh-bors, while the self-guided features extracted from the depth map to be updated allow the TDU to be adaptive to the content of the target map. Moreover, our TDU further im-proves the performance by learning the residual. In addi-tion, we observe that when multiple TDUs of the update process employ the cross-guided features of different scales respectively, the TDU guided by smaller scale cross-guided features will adaptively learn to obtain the neighbors in a relatively large scope, and vice versa. Since our initial depth is obtained by dilating sparse measurement points [15], sur-rounding points of most pixels are inaccurate. Therefore, at the beginning of the update process, we employ small scale cross-guided features to lead the TDU to predict a large scope, which obtains some long-range but accurate points as neighbors. As the depth map becomes more refined, larger scale cross-guided features are sequentially used to pay more attention to shorter-range neighbors. Due to the elegant recurrent strategy, our LRRU only requires four it-Table 1. The configurations of four LRRU variants, which are obtained by adjusting the number of channels in different stages of the cross-guided feature extraction network.
Models
Number of channels stage1 stage2 stage3 stage4 stage5
Params.
LRRU-Mini
LRRU-Tiny
LRRU-Small
LRRU-Base 8 16 32 64 16 32 64 128 32 64 128 256 32 64 128 256 32 64 128 256 0.3 M 1.3 M 5 M 21 M erations to achieve satisfactory results.
Extensive experiments on both indoor and outdoor datasets verify the performance of our method.
Fur-thermore, we conduct comprehensive ablation studies to demonstrate the effectiveness of each component. Lastly, we extend our network framework to the depth-only case.
Our main contributions are summarized as:
• We propose a novel lightweight deep network archi-tecture for depth completion, which pre-fills the sparse depth map and iteratively updates it by the proposed
Target-Dependent Update (TDU) unit.
• We propose a long-short range recurrent strategy, which dynamically adjusts kernel scopes during the update pro-cess to obtain long-to-short range dependencies.
• As shown in Fig. 1 and Table 1, our four LRRU variants achieve state-of-the-art performance across different pa-rameter regimes. Especially, the LRRU-Base model out-performs SOTA methods on NYUv2 [24] and ranks 1st on the KITTI benchmark [2] at the time of submission. 2.