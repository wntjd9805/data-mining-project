Abstract
We propose an Unsupervised Domain Adaptation (UDA) method by making use of Energy-Based Learning (EBL) and demonstrate 1. EBL can be used to improve the instance selection for a self-training task on the unlabelled target domain, and 2. alignment and normalizing energy scores can learn domain-invariant representations. For the for-mer, we show that an energy-based selection criterion can be used to model instance selections by mimicking the joint distribution between data and predictions in the target do-main. As per learning domain invariant representations, we show that stable domain alignment can be achieved by a combined energy alignment and an energy normaliza-tion process. We implement our method in consistent with the vision-transformer (ViT) backbone and show that our proposed method can outperform state-of-the-art ViT based
UDA methods on diverse benchmarks (DomainNet, Office-Home, and VISDA2017). 1.

Introduction
The recent progress in Unsupervised Domain Adaptation (UDA) methods depend on two key factors: the capacity to learn discriminatively from unlabelled target data and the potential to achieve domain alignment without compromis-ing the integrity of the representation (i.e., avoiding degen-erated representations). In this study, we propose a novel approach for UDA that leverages the energy-based interpre-tation of discriminative classifiers [11, 16, 36]. We show how energy-based learning can be used to generate a better self-training signal when learning discriminatively from the unlabelled target domain data. To this end, we demonstrate that the representation integrity during domain alignment can be maintained with a proposed novel normalization pro-cess for the energy-based learning framework. We show that our energy-based self-training, the energy normaliza-tion process together with free-energy alignment proposed in [36] forms a seamless energy-based learning framework for UDA on top of the transformer [7] backbone. Our results confirm that this proposed framework outperforms state-of-the-art UDA methods on established UDA benchmarks.
Prior work has emphasized the significance of a discrim-inative objective in the target domain data for UDA. For ex-ample, [28] employs entropy minimization principles [10] as a discriminative loss on the unlabelled data. Prabhu et al. [23] illustrate the effectiveness of min-max learning of the entropy loss function when utilized to learn from the un-labelled target domain data. The recent transformer-based technique presented in [24] proposes a self-training task that involves pseudo-labelling and learning augmentation masks from the data. Nonetheless, these approaches only make use of the consistency of the conditional distribution of the labels (i.e., pseudo labels) given the unlabelled data to de-velop the self-training task. It is evident that the joint distri-bution between data and labels captures a stronger relation-ship between them [25]; but an utterly involved choice to be considered. For instance, modelling the joint distribution will require the help of computationally exhaustive genera-tive modelling techniques [37, 38]. In this work, we show how we can mimic the behaviour of the joint distribution modelling to generate an informative self-training signal by using the energy-based learning concepts for classifiers.
With the current success of the transformers [33] in both vision and NLP [6], we focus our efforts on examining the proposed approach using the vision transformer (ViT) [7] backbone. As such, the quality of training depends on the capacity of the self-attention parameters to accurately em-phasize related information in the training data. However, for UDA, due to the domain shift between the source and the target data, the self-attention parameters learnt through the source domain supervision may not well align with all target data points. Here, we propose a selection criterion to decide on instances that are compatible with the atten-tion of the transformer model. In our formulation, we use the relationship between free-energy to the marginal density of the data. Thereafter, we show that by a careful combina-tion of such marginal density-based instance selections with 1
prediction consistency instance selections can lead to better self-training performance.
We make use of the concept of minimizing free-energy bias [36] across the domains to learn domain invariant rep-resentations. We integrate free-energy alignment (FEA) of
[36] into a module, SCAL (coined from the terms SCore
ALigment) that can be used in a straightforward way with the classification layer (i.e., logits layer) of a Deep Neural
Network (DNN). Moreover, we show that the free-energy based domain alignment can be further stabilized using a normalization process applied on the free-energy and en-ergy scores. We show that this normalization helps to keep the integrity of the features while improving the do-main alignment. We coin our novel normalization pro-cess into a module, SCON (coined from the terms SCOre
Normalization) that can be attached to the classifier outputs.
Our choice of distribution alignment, normalization, and joint distribution modeling for self-training is inspired by the principles of energy-based learning. We are the first to combine the concepts of energy-based learning, domain adaptation, self-training, and the ViT backbone.
In our experiments, we show that this combination enables us to outperform state-of-the-art unsupervised domain adaptation methods on challenging UDA benchmarks.
• We propose a self-training task that emulates the joint distribution of the data and the predictions using the concepts of free-energy and energy-based learning.
• We propose a novel energy-normalization module,
SCON to achieve stable domain alignment when com-bined with free-energy alignment [36].
• We outperform state-of-the-art methods on established
[21], OfficeHome [34], benchmarks (DomainNet
VISDA2017 [22]) with relative improvement of 2.2%−3.3% compared to the best performing method. 2.