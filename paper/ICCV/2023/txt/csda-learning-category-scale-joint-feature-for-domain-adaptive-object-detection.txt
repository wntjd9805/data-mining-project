Abstract
Domain Adaptive Object Detection (DAOD) aims to im-prove the detection performance of target domains by mini-mizing the feature distribution between the source and tar-get domain. Recent approaches usually align such distri-butions in terms of categories through adversarial learn-ing and some progress has been made. However, when ob-jects are non-uniformly distributed at different scales, such category-level alignment causes imbalanced object feature learning, refer as the inconsistency of category alignment at different scales. For better category-level feature alignment, we propose a novel DAOD framework of joint category and scale information, dubbed CSDA, such a design enables ef-fective object learning for different scales. Specifically, our framework is implemented by two closely-related modules: 1) SGFF (Scale-Guided Feature Fusion) fuses the cate-gory representations of different domains to learn category-specific features, where the features are aligned by discrim-inators at three scales. 2) SAFE (Scale-Auxiliary Feature
Enhancement) encodes scale coordinates into a group of tokens and enhances the representation of category-specific features at different scales by self-attention. Based on the anchor-based Faster-RCNN and anchor-free FCOS detec-tors, experiments show that our method achieves state-of-the-art results on three DAOD benchmarks. 1.

Introduction
Benefiting from the training of large numbers of high-quality labeled data, object detection algorithms have been significantly advanced in recent years [15, 20, 24, 25, 33].
However, when dealing with new scenarios with unlabeled data, the detector suffers from severe performance degrada-tion. To tackle this problem, the domain adaptive object de-tection is proposed, where the trained detector is transferred from a scenario with labeled data (i.e., source domain) to a
*Equal contribution.
Figure 1. Illustration of the difference between our method and the category-level alignment methods. It is worth noting that we only depict one category in the figure for brevity. Our method learns category-level features jointly with scales for more effective fea-ture alignment. SGFF: scale-guided feature fusion module. SAFE: scale-auxiliary feature enhancement module. new scenario without labeled data (i.e., target domain).
The large difference in data distribution between the source and target domain is an essential factor affecting the performance. To improve the generalization ability of detectors, most existing methods align feature distribution from three aspects: 1) DAF [3] uses adversarial learning [8] to align the feature distribution of pixel- and instance-level with the help of the gradient reverse layer (GRL). 2)
EPM [11] and CFFA [43] weaken the negative impact from the background and focus on the cross-domain alignment of the foreground. 3) KTNet [32], GPA [37], DBGL [1], and
SIGMA [17] achieve adaptation at category-level by align-ing cross-domain class-conditional distributions and make significant progress. However, these works above ignore the impact of object scale in transferring and aligning category-level features, as shown in Fig. 1, which prevents them from satisfactory results.
In our view, there are still two challenges existing in cur-rent category-level DAOD works. The first challenge is the large variance in category features at different scales, which negatively affects the category-level feature align-ment. Commonly, category features of small objects lack texture details or even category-specific features with dis-criminative ability. This makes it difficult to directly align features of small and large objects in the same category.
Recent US-DAF [28] separately aligns features at different scales by assigning more fine-grained scale labels (small, medium, and large) for each category. OADA [38] intro-duces the bounding box offsets to conditionally align the feature distributions based on FCOS [33]. Although these algorithms above optimize feature learning for objects at different scales to some extent, they ignore the category-level feature consistency intra-/inter-domain with the same scale, resulting in inefficient learning of domain-invariant features for each category.
The second challenge is the weak perception of small ob-jects. For humans, if we know what a “near car” looks like, then we can easily recognize the “far car”. That is, when the detector can detect a large object, it also has the potential to detect the small one. Inspired by this, it is necessary to facil-itate learning between objects at different scales. In DAOD, although MGADA [44] proposes an omni-scale gated fu-sion module consisting of convolution with different sizes to learn multi-scale features, it cannot be removed during inference, increasing the parameters.
To overcome these challenges, we propose a DAOD framework of joint category and scale information (CSDA) for more advanced cross-domain feature alignment and
In particular, as shown in Fig. 1, we pro-enhancement. posed a Scale-Guided Feature Fusion module (SGFF) to promote feature learning and alignment for each category.
It pulls objects of the same scale closer while pushing objects of different scales away in an adversarial learning manner. Such a design significantly alleviates the model degradation due to the large variance between features at different scales. Then, to improve the perceptive capability in multi-scale scenarios, especially for small objects, we design a Scale-Auxiliary Feature Enhancement module (SAFE) to enhance feature interactions at different scales.
It encodes the category features and scale information of objects as a set of tokens to learn the relations between different scales through self-attention [34].
To be summarized, our contributions are as follows.
• We propose a joint category and scale feature frame-work for DAOD, dubbed CSDA, which achieves more effective cross-domain feature alignment and feature enhancement for different scale objects.
• We propose a scale-guided feature fusion module (SGFF), which eliminates the negative impact of ex-cessive differences in features at different scales, and a scale-auxiliary feature enhancement module (SAFE), which enhances the perceptive capability of small ob-jects in multi-scale scenarios.
• Extensive experiments demonstrate that the proposed
CSDA can significantly outperform existing SOTA methods in three widely-used benchmarks. 2.