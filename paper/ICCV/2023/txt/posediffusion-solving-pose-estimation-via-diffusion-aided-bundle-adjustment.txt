Abstract
Camera pose estimation is a long-standing computer vi-sion problem that to date often relies on classical meth-ods, such as handcrafted keypoint matching, RANSAC and bundle adjustment. In this paper, we propose to formulate the Structure from Motion (SfM) problem inside a proba-bilistic diffusion framework, modelling the conditional dis-tribution of camera poses given input images. This novel view of an old problem has several advantages. (i) The na-ture of the diffusion framework mirrors the iterative proce-dure of bundle adjustment. (ii) The formulation allows a seamless integration of geometric constraints from epipo-(iii) It excels in typically difficult scenar-lar geometry. (iv) The ios such as sparse views with wide baselines. method can predict intrinsics and extrinsics for an arbi-trary amount of images. We demonstrate that our method
PoseDiffusion significantly improves over the classic SfM pipelines and the learned approaches on two real-world datasets. Finally, it is observed that our method can gener-alize across datasets without further training. Project page: https://posediffusion.github.io/ 1.

Introduction
Camera pose estimation, i.e. extracting the camera in-trinsics and extrinsics given a set of free-form multi-view scene-centric images (e.g. tourist photos of Rome [2]), is a traditional Computer Vision problem with a history stretch-ing long before the inception of modern computers [20].
It is a crucial task in various applications, including aug-mented and virtual reality, and has recently regained the at-tention of the research community due to the emergence of implicit novel-view synthesis methods [33, 38, 25]. of the
The classic dense pose estimation task estimates the pa-rameters of many cameras with overlapping frusta, leverag-ing correspondence pairs between keypoints visible across images. It is typically addressed through a Structure-from-Motion (SfM) framework, which not only estimates the camera pose (Motion) but also extracts the 3D shape of the observed scene (Structure). During the last 30 years, SfM pipelines matured into a technology capable of reconstruct-ing thousands [2] if not millions [14] of free-form views. structure dense-view SfM
Surprisingly, pipeline [41] has remained mostly unchanged until today, even though individual components have incorpo-rated deep learning advances [8, 40, 17, 49, 53, 24]. SfM first estimates reliable image-to-image correspondences and, later, uses Bundle Adjustment (BA) to align all cam-eras into a common scene-consistent reference frame. Due to the high complexity of the BA optimization landscape, a modern SfM pipeline [44] comprises a carefully engineered iterative process alternating between expanding the set of registered poses and a precise 2nd-order BA optimizer [1].
With the recent proliferation of deep geometry learn-ing, the sparse pose problem, operating on a significantly smaller number of input views separated by wide baselines, has become of increasing interest. For many years, this sparse setting has been the Achillesâ€™ Heel of traditional pose estimation methods. Recently, RelPose [60] leveraged a deep network to implicitly learn a bundle-adjustment prior from a large dataset of images and corresponding camera poses. The method has demonstrated performance superior to SfM in settings with less than ten input frames. How-ever, in the many-image case, its accuracy cannot match the precise solution of the second-order BA optimizer from iter-ative SfM. Besides, it is limited to predicting rotations only.
In this paper, we propose PoseDiffusion - a novel camera pose estimation approach that elegantly marries deep learn-ing with correspondence-based constraints and therefore, is able to reconstruct camera positions at high accuracy both in the sparse-view and dense-view regimes.
PoseDiffusion introduces a diffusion framework to solve the bundle adjustment problem by modelling the probabil-ity p(x
I) of camera parameters x given observed images I.
|
Following the recent successes of diffusion models in mod-elling complex distributions (e.g. over images [15], videos
[45], and point clouds [28]), we leverage diffusion mod-els to learn p(x
I) from a large dataset of images with known camera poses. Once learned, given a previously unseen sequence, we estimate the camera poses x by sam-I) forms a pling p(x
I). The latter mildly assumes that p(x
|
I) will near-delta distribution so that any sample from p(x
|
|
| yield a valid pose. The stochastic sampling process of dif-fusion models has been shown to efficiently navigate the log-likelihood landscape of complex distributions [15], and therefore is a perfect fit for the intricate BA optimization.
An additional benefit of the diffusion process is that it can be trained one step at a time without the need for unrolling gradients through the whole optimization.
Additionally, in order to increase the precision of our camera estimation, we guide the sampling process with tra-ditional epipolar constraints (expressed by means of reliable 2D image-to-image correspondences), which is inspired by classifier diffusion guidance [9]. We use this classical con-straint to bias samples towards more geometrically consis-tent solutions throughout the sampling process, arriving at a more precise camera estimation.
PoseDiffusion yields state-of-the-art accuracy on the object-centric scenes of CO3Dv2 [38], as well as on outdoor/indoor scenes of RealEstate10k [62]. Crucially,
PoseDiffusion also outperforms SfM methods when used to supervise NeRF training [33], which demonstrates the supe-rior accuracy of both the extrinsic and intrinsic estimation. 2.