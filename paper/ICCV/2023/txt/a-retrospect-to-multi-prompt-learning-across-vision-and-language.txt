Abstract
The vision community is undergoing the unprecedented progress with the emergence of Vision-Language Pretrain-ing Models (VLMs). Prompt learning plays as the holy grail of accessing VLMs since it enables their fast adaptation to downstream tasks with limited resources. Whereas existing researches milling around single-prompt paradigms, rarely investigate the technical potential behind their multi-prompt learning counterparts. This paper aims to provide a princi-pled retrospect for vision-language multi-prompt learning.
We extend the recent constant modality gap phenomenon to learnable prompts and then, justify the superiority of vision-language transfer with multi-prompt augmentation, empiri-cally and theoretically. In terms of this observation, we pro-pose an Energy-based Multi-prompt Learning (EMPL) to generate multiple prompt embeddings by drawing instances from an energy-based distribution, which is implicitly de-fined by VLMs. So our EMPL is not only parameter-efficient but also rigorously lead to the balance between in-domain and out-of-domain open-vocabulary generalization. Com-prehensive experiments have been conducted to justify our claims and the excellence of EMPL. 1.

Introduction
Recent years have witnessed the rise of multimodal intel-ligence, in particular, Vision-Language Pre-training models (VLMs), e.g., CLIP [40], ALIGN [24], achieving down-stream tasks in low resources by converting the prior knowl-edge behind large language models (LLMs) [10, 4]. Given a pair of image encoder (e.g., ResNet [19], ViT [11], etc) and text encoder (i.e., LLMs), VLMs align visual features with their corresponding textual description embeddings via con-trastive learning [12, 18, 56]. So provided a text known as prompt, VLMs may rapidly adapt to diverse tasks [12, 32] by matching harmony visual patterns with the textual de-scription. The principle sheds a new light in computer vi-sion for in-domain and out-of-domain generalization.
The impressive cross-modal transferability behind VLM
*indicate corresponding author.
Figure 1. The overview of cross-modal single-prompt learning and multi-prompt learning (MPL). With more prompt templates, MPL brings new opportunities and challenges as discussed in the com-munity, yet seldom giving a systematic investigation and solution. typically owes to the problem-customized prompting style, yet demanding a great magnitude of trials and errors for se-lecting the ideal prompt template from a pool of candidates.
Tedious workloads are consumed and do not guarantee the optimal prompt template either. Instead of the prompt engi-neering, prompt learning / tuning [55] sidesteps the obsta-cle using soft prompting: outside of the words related with what we are interested in, the rest of textual slots in the tem-plate are replaced by a sequence of learnable context vec-tors ahead of the text encoder. In this principle, the optimal prompt template could be achieved by fine-tuning the learn-able context vectors along with the given textual semantic while keeping the rest parameters of VLMs frozen for the optimization. The data-driven merit increasingly arouses a flood of interests in the community [54, 42, 46].
Despite the significant progress, existing work of prompt learning focused on a single template whereas multi-prompt learnable context templates remain under-explored (Fig.1).
Recent NLP advance argue that instructing LLMs via more prompts may trigger its underlying in-context learning abil-ity to master new skills [4]. In this regard, multi-prompting
is also deemed to be a promising trend for VLMs. On the other hand, existing studies remain confused of how multi-ple prompts work for VLMs, particularly from two aspects.
The first is vision-language transferrability [53]. Prompt augmentation eliminates the average cross-modal transfer shift while [54] showed that increasing the scale of con-text vector tokens resulted in the detrimental effect, im-plying a larger cross-modal disparity. The second is open-vocabulary (OV) generalization, i.e., the model awareness of unseen classes. Different from multple textual descrip-tions, learning with more prompts suggests more parame-ters. It casted a doubt of overfitting to training classes and put unseen classes at the risk of model generalization [5].
In this paper, we provided several principled insights to understand multi-prompt learning empirically and theoret-ically. We first consider the cross-modal embedding space following the constant modality-gap phenomenon found by
[53]. With regards to our empirical observations, we ex-tend the conclusion to learnable prompts to show that more learnable prompts might reduce the constant modality gap more significantly. In terms of constant modality gaps, we further proved the existence of cross-modal unidentifiabil-ity issue: a paradox confusing the cross-modal model with a single prompt template in visual recognition. It could be re-strained by multi-prompting empirically, thus, interpreting why multi-prompt learning could outperform single prompt for the sake of vision-language transferrability.
In terms of our retrospect, the main challenge of multi-prompt learning refers to its generalizability. Derived from this concern, we propose a new methodology Energy-based
Multi-Prompt Learning (EMPL) for striking the balance be-tween in-domain generalization and open-vocabulary gen-eralization abilities. EMPL implicitly defines an energy-based [27] prompt distribution that simultaneously use im-age and prompt as the variable.With this regard, our method could be rigorously treated as modeling the uncertainty to explore the image-prompt embedding pairs with concepts out of the training domains, whereas also well generalizes to examples belonging to in-domain classes. The prompts are iteratively generated via a stochastic Markov Chain Monte
Carlo (MCMC) sampler [50], which is parameter-efficient, sensitive of input knowledge from vision-text encoders, and more importantly, general enough to cooperate with exist-ing prompt learning strategies to upgrade the performances.
Experiments are comprehensively conducted to validate our claims and the superiority of our approach. 2.