Abstract
Dataset distillation aims to synthesize small datasets with little information loss from original large-scale ones for reducing storage and training costs. Recent state-of-the-art methods mainly constrain the sample synthesis pro-cess by matching synthetic images and the original ones regarding gradients, embedding distributions, or training trajectories. Although there are various matching objec-tives, currently the strategy for selecting original images is limited to naive random sampling. We argue that ran-dom sampling overlooks the evenness of the selected sam-ple distribution, which may result in noisy or biased match-ing targets. Besides, the sample diversity is also not con-strained by random sampling. These factors together lead to optimization instability in the distilling process and de-grade the training efﬁciency. Accordingly, we propose a novel matching strategy named as Dataset distillation by
REpresentAtive Matching (DREAM), where only represen-tative original images are selected for matching. DREAM is able to be easily plugged into popular dataset distilla-tion frameworks and reduce the distilling iterations by more than 8 times without performance drop. Given sufﬁcient training time, DREAM further provides signiﬁcant improve-ments and achieves state-of-the-art performances. 1.

Introduction
Deep learning has made remarkable achievements in the computer vision society [20, 11, 36, 29, 43, 16, 7, 55], and the success is closely related to a large amount of efforts in data collection and annotation. But along with the progress of these efforts, the huge amount of data, in turn, becomes a barrier to both storage and training [52, 19]. Many methods are introduced to reduce the scale of datasets [47, 40, 34,
*Equal contribution.
†Project lead.
‡Corresponding author. (a) The gradient norm distribution of the plane class in CIFAR10. (b) The migration of synthetic samples during training.
Figure 1: Samples on the decision boundaries usually pro-vide larger gradients, which biases the gradient matching optimization. Random sampling (left) overlooks the even-ness of of the selected sample distribution, resulting in un-stable optimization process of the synthesized samples. By only matching with proper gradients from representative original samples, our proposed DREAM (right) greatly im-proves the training efﬁciency of dataset distillation tasks.
Best viewed in color. 8]. Among these, dataset distillation, aiming at condensing large-scale datasets into smaller ones with little information loss, has become a hot topic to tackle the problem of data burden [3, 46, 24, 5, 12].
Dataset distillation methods are roughly divided into two categories: coreset-based and optimization-based. Coreset-based method employ certain metrics to heuristically select samples for representing the original dataset [26, 41]. How-ever, it is difﬁcult to rely on a small proportion of original samples to contain the information of the whole dataset, re-sulting in low compression rate. Optimization-based meth-ods alleviate the defect by incorporating image synthesis to introduce more information into single images [47]. Specif-ically, these methods initialize a small amount of learnable image tensors and update them through matching the train-ing gradients [52, 24], embedding distributions [51, 46] or training trajectories [3, 12] with the original images.
Although the optimization-based methods achieve con-siderable performance as well as compression ratio, the dis-tillation process itself still requires a large amount of time.
We analyze the problem from the strategy of selecting orig-inal images for matching, which is mostly set as random sampling in previous works [52]. We argue that random sampling overlooks the evenness of the selected sample dis-tribution. On the one hand, the matching optimization may be overly prone to certain samples with dominant matching targets, such as boundary samples with larger training gra-dients [46]. On the other hand, the sample diversity inside a mini-batch is also not constrained, leading to potential in-formation insufﬁciency. These factors together result in op-timization instability of the dataset distillation process, and degrade the training efﬁciency.
Accordingly, we propose a novel matching strategy named as Dataset distillation by REpresentAtive Matching (DREAM) to address the aforementioned training efﬁciency issue. Speciﬁcally, a clustering process inside each class is conducted at intervals to generate sub-clusters reﬂecting the sample distribution. The sub-cluster centers, which not only are representative for surrounding samples, but also evenly cover the whole class distribution, are selected for match-ing. As shown in Fig. 1a, the gradient distribution of the selected samples contains less variation. By only match-ing with representative samples, DREAM largely reduces the instability during training, and provides a smoother and more robust distillation process. For the synthetic image initialization, we adopt a similar clustering-based strategy, where the center sample is selected from each sub-cluster, which further accelerates the training process.
DREAM can be easily plugged into popular dataset dis-tillation frameworks. Compared with commonly adopted random matching, DREAM signiﬁcantly improves the training efﬁciency in the distilling process. We conduct ex-tensive experiments to validate that it only takes less than one eighth of the iterations for DREAM to obtain compa-rable performance with the baseline methods. In addition, given sufﬁcient training iterations, DREAM further boosts the performance to surpass other state-of-the-art methods.
Our main contributions are summarized as:
• We analyze the training efﬁciency of optimization-based dataset distillation from the strategy of selecting original samples for matching.
• We propose a Dataset distillation by REpresentAtive
Matching (DREAM) strategy. By only matching rep-resentative images, DREAM accelerates the training process by more than 8× without performance drop.
• DREAM is able to be easily plugged into a variety of dataset distillation frameworks. Extensive experiments prove that DREAM consistently improves the perfor-mance of the distilled dataset. 2.