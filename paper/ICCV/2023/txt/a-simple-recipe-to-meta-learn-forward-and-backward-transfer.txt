Abstract
Meta-learning holds the potential to provide a gen-eral and explicit solution to tackle interference and forget-ting in continual learning. However, many popular algo-rithms introduce expensive and unstable optimization pro-cesses with new key hyper-parameters and requirements, hindering their applicability. We propose a new, general, and simple meta-learning algorithm for continual learning (SiM4C) that explicitly optimizes to minimize forgetting and facilitate forward transfer. We show our method is stable, introduces only minimal computational overhead, and can be integrated with any memory-based continual learning al-gorithm in only a few lines of code. SiM4C meta-learns how to effectively continually learn even on very long task sequences, largely outperforming prior meta-approaches.
Naively integrating with existing memory-based algorithms, we also record universal performance beneﬁts and state-of-the-art results across different visual classiﬁcation bench-marks without introducing new hyper-parameters. 1.

Introduction
Continual learning considers the problem of appropri-ately updating and consolidating knowledge when learning from data with a continual distribution shift. This problem setting is extremely relevant to achieving lifelong learning systems where an agent is expected to encounter drifts both across and within learning environments, due to the non-stationary nature of the real-world [31]. An ideal continual learning system would re-use prior knowledge to facilitate learning new problems (forward transfer), and new data to reﬁne its prior predictions (backward transfer). However, deep supervised learning algorithms struggle to even retain learned information when trained with non-stationary se-quences of data, a problem known as catastrophic forget-ting [14, 15, 36]. In classical computer vision applications, prior work is still far from addressing this issue [11], which would be crucial to bringing the generality and applicability of deep learning closer to its natural analogue [18, 27].
Traditional algorithms for continual learning can be gen-erally classiﬁed into (i) replay methods, (ii) regularization-based methods, and (iii) parameter isolation methods [11].
These approaches entail hand-designed strategies involving (i) storing a small buffer of samples from all prior tasks, (ii) introducing auxiliary regularization terms to consoli-date knowledge, and (iii) deﬁning modular architectures to separate task-speciﬁc knowledge into independent modules.
Given its relevance, most methods only optimize to mitigate forgetting, ignoring other potentially desirable properties of continual learning [12]. Furthermore, they rely on heuristics and strong assumptions about the types of drifts, limiting their applicability to speciﬁc settings [48]. A fourth cate-gory of methods has recently found success, based on utiliz-ing meta-learning to emergently recover many of the wanted properties of an effective continual learning agent [7].
The general principle behind these prior meta-learning approaches has been to emulate the non i.i.d. continual learning optimization within an outer meta-optimization to improve the agent’s stability (knowledge retention) and plasticity (knowledge acquisition) [16]. However, optimiz-ing these meta-objectives often requires high computational cost [41] and even access to a separate meta pre-training phase where the constraints of continual learning are es-sentially lifted [23]. Furthermore, different recent advances have focused on aligning the meta and continual optimiza-tions by increasing the number of steps, adding new models, and introducing new parameters to meta-train [4, 17]. The underlying complexity of this line of work comes with non-trivial implications hindering applicability, efﬁciency, and also introducing the need of heuristic approximations to the meta-objectives for tractability [3, 38].
In this work, we propose a new simple meta-learner for continual learning (SiM4C) that takes a different approach than prior work.
In particular, we propose to purpose-fully use a lightweight single-step inner-loop in the meta-optimization and preserve large amounts of ‘unseen’ data for each task which can be interpreted as potential future data. Then, using both this future data and past stored ex-periences in the outer meta-loss, we explicitly optimize the
Figure 1. Schematic depiction of SiM4C, after a single inner optimization step the proposed meta-objective optimizes for forward and backward transfer by utilizing seen past data from previous tasks and unseen future data of the current task. model to tackle challenging future and past transfer objec-tives, across and within tasks (as depicted in Figure 1). In practice, our formulation results in SiM4C avoiding harm-ful meta-overﬁtting phenomena, providing minimal compu-tational overheads even without approximations, and allow-ing integrations in just a few lines of code.
Empirically, we thoroughly evaluate and analyze SiM4C for image classiﬁcation problems, comparing it with differ-ent state-of-the-art algorithms. When given access to a meta pre-training phase, SiM4C greatly outperforms other meta-learning work evaluated and designed for tackling long se-quences of hundreds of consecutive tasks [4, 23]. Further-more, even without any pre-training, using SiM4C as a sim-ple auxiliary meta-objective provides near-universal perfor-mance beneﬁts to different memory-based algorithms [6, 41]. Our results validate SiM4C’s effectiveness, achieving state-of-the-art results across ﬁve different continual learn-ing benchmarks, and highlight its improved efﬁciency and stability as compared to prior meta-learning approaches.
We provide additional resources and our full implemen-tation1 to facilitate future extensions and promote further work towards more scalable and applicable meta-learning for continual learning. 2.