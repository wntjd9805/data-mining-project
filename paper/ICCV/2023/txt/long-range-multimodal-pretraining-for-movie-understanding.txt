Abstract
Learning computer vision models from (and for) movies has a long-standing history. While great progress has been attained, there is still a need for a pretrained multimodal model that can perform well in the ever-growing set of movie understanding tasks the community has been estab-lishing. In this work, we introduce Long-range Multimodal
Pretraining, a strategy, and a model that leverages movie data to train transferable multimodal and cross-modal en-coders. Our key idea is to learn from all modalities in a movie by observing and extracting relationships over a long-range. After pretraining, we run ablation studies on the LVU benchmark and validate our modeling choices and the importance of learning from long-range time spans. Our model achieves state-of-the-art on several LVU tasks while being much more data efficient than previous works. Fi-nally, we evaluate our model’s transferability by setting a new state-of-the-art in five different benchmarks. 1.

Introduction
Are movies just for entertainment? Arguably they offer much more than that. Movies are a source of inspiration for many and a force that influences societal behaviors [1].
Movies are also an active topic of study in the computer vi-sion community [22, 13, 46, 5, 19, 3]. They have served as a testbed for measuring progress in visual recognition [16, 6], reasoning [36, 42], and creative editing [10, 31]. Moreover, movie data has been also leveraged to train computer vision
[9], machine listening [7], and NLP [54] systems. Besides entertainment, movies are surely an intriguing media that can help AI models to understand semantics and artistic ex-pressions encoded in a movie style.
While the quest of understanding movies has gained steady attention, it is still an open question how to de-velop models that leverage abundant sources of movie data
[19, 37] to tackle all the movie-related tasks that the com-munity has grown over the last decade [5, 26, 48, 41, 3].
Figure 1. Learning from Movie Sequences. Movie sequences offer learning signals when observed for long-ranges. By reading the language (dialogue), hearing the sounds, and looking at the video, one can determine that Beethoven, the dog, was barking and he sleeps in a white and blue wooden dog house.
But, learning from movies is not a straightforward task, es-pecially when no labels are available.
Existing video self-supervised approaches [11, 14, 2, 50, 52], which primarily focus on learning from short clips, would not leverage the richness of movies, as the value of movies as training sources emerges from their long-range dependencies. Moreover, the end-to-end learning scheme adopted in these works can not be easily extended to movies as it is computationally infeasible to encode long-form se-quences in an end-to-end manner.
Recent works on long-form video understanding [48, 20, 9] have attempted to address these limitations by encod-ing movie clips using frozen base encoders [48, 9] or state-space models [20]. However, these works exclusively focus on the video modality for long-range temporal reasoning, disregarding audio and text signals, and hence are limited to specific tasks. In this work, we argue that long-form videos such as movies are rich in visual, auditory, and textual in-formation, and integrating these modalities would lead to a better and generalizable understanding.
Fig. 1 illustrates how the three core modalities in movies (language, video, sound) can serve as a valuable source of supervision when reasoned all together for a long time. By analyzing the language (from the dialogue) we can under-stand that the dog’s name is Beethoven, from the audio we could hear he is barking, and from long-range associations, we can infer he sleeps in a wooden dog house. This toy example illustrates how critical is to design training strate-gies that can effectively encode visual, audio, and language relationships. It is equally important to effectively design models that encode long-range dependencies.
This work presents pretraining strategy that leverages multimodal cues and long-range reasoning in movies. We develop a flexible model that can be easily transferred to a range of tasks related to movie analysis and understand-ing [48, 26, 3, 41, 5]. Our first design requirement is that the model needs to observe over a long time span in order to do long-range reasoning. To facilitate this, we dissect a long video into shots and represent it as an ordered sequence of shots. The main motivation for using shot-based input sampling instead of uniform temporal sampling is to cap-ture longer multi-shot content at a time since each shot will be considered as one token irrespective of its length. Our second design requirement is to efficiently encode the in-put sequence by harnessing all available modalities. We do so by representing each shot clip in the sequence in video, audio, and language modalities. As it is computationally inefficient to encode long-form sequences in an end-to-end manner, we opt for using pretrained state-of-the-art mod-els [44, 18, 4, 24] as base encoders to transform the raw video, audio, and text sequences into their corresponding compact feature representations.
Given a sequence of encoded base features for each modality, our third design requirement is to perform mul-timodal long-term reasoning in a self-supervised manner.
To do so, we make use of Transformer networks [45]. In-stead of combining all tokens from the different modal-ities as one long sequence like in VideoBERT [39], we follow a hierarchical approach. We first learn long-range context from each modality using contextual transformers while simultaneously ensuring that the context learned over one modality is also conditioned by another modality. We then learn joint representations between modalities using a cross-modal transformer network. To ensure that differ-ent transformers in our framework serve their purpose, we introduce a pretraining strategy that enforces intra-modal, inter-modal, and cross-modal relationships over long-range observations via carefully designed losses.
We train our model using publicly available movie dataset [37], performing additional preprocessing such as shot boundary detection [38]. We evaluate the transferabil-ity of our approach on six different benchmarks [48, 26, 3, 41, 5], and empirically show that long-range multimodal pretraining provides extensive benefits in performance.
Contributions. Our goal is to train transferable models for movie understanding. It brings two contributions: (1) We introduce a pretraining strategy designed to leverage long-range multimodal cues in movies. We propose a model that captures intra-modal, inter-modal, and cross-modal de-pendencies via transformer encoders and self-supervision. (2) We conduct extensive experiments to validate the trans-ferability of our model and the contributions of the pretrain-ing strategy. The results show that our model consistently improves the state-of-the-art across six benchmarks. 2.