Abstract
How human interact with objects depends on the func-tional roles of the target objects, which introduces the prob-It re-lem of affordance-aware hand-object interaction. quires a large number of human demonstrations for the learning and understanding of plausible and appropriate hand-object interactions.
In this work, we present Af-fordPose, a large-scale dataset of hand-object interactions with affordance-driven hand pose. We first annotate the specific part-level affordance labels for each object, e.g. twist, pull, handle-grasp, etc, instead of the general in-tents such as use or handover, to indicate the purpose and guide the localization of the hand-object interactions. The fine-grained hand-object interactions reveal the influence of hand-centered affordances on the detailed arrangement of the hand poses, yet also exhibit a certain degree of diver-sity. We collect a total of 26.7K hand-object interactions, each including the 3D object shape, the part-level affor-dance label, and the manually adjusted hand poses. The comprehensive data analysis shows the common character-istics and diversity of hand-object interactions per affor-dance via the parameter statistics and contacting compu-tation. We also conduct experiments on the tasks of hand-object affordance understanding and affordance-oriented hand-object interaction generation, to validate the effec-tiveness of our dataset in learning the fine-grained hand-object interactions. Project page: https://github. com/GentlesJan/AffordPose 1.

Introduction
One of the long-standing goals of robotics is to imitate all kinds of human-centered interactions, especially hand-object interactions, ranging from general grasping to func-tional interactions such as unscrewing a cap or even tool usage [22, 15]. Performing appropriate hand-object interac-tion is a complicated decision-making process. The agents
⋆ Corresponding Authors: manyili@sdu.edu.cn, jianliu2006@gmail.com
Figure 1: A Gallery of AffordPose. AffordPose is the first large-scale dataset for fine-grained hand-object interactions driven by the specific part-level affordance labeling, which reveals the high correlation between the object affordance and the detailed arrangement of hand poses. need to understand the functional role of the object, select the contacting location, and perform the specific hand pose to complete the task [7, 1, 39].
There has been a trend to develop deep learning solu-tions to predict diverse hand-object interactions. The re-searchers build hand-object interaction datasets, such as
HO-3D [13], DexYCB [5], Obman [16], and train differ-ent networks [26, 44, 43, 27] to predict the hand poses for the given objects. However, these works only consider the general grasping task and focus on the stability of the gen-erated hand poses, but overlook the semantic meaning of the hand-object interactions. Recently, many related works collect additional annotations, e.g. contact maps [2, 3, 24], grasp type labels [8], and intent labels [40, 51], to learn how human use different objects with appropriate hand-object 1
interactions, not only to hold the objects tightly but also for the convenient usage being consistent with human habits.
Object usage involves the key characteristics of the in-teractions, including what interaction to perform, where to interact with the object, and how to perform the interac-tion to achieve the purpose. As summarized in Table 1, if exists, the object usage, i.e. intents, are often of two types in the relevant datasets. One is human objectives, e.g. use, hand-out, receive as in OakInk [51], which de-scribes the human targets regardless of the object categories and attributes. Although these human objectives illustrate the purpose of hand-object interactions well, these selected human-centered objectives are general goals and take the objects as just geometric shapes rather than functional ob-jects. The other is object-centric affordances, e.g. pour juice in FPHAB dataset [11], which specifies the detailed type of interactions, but with a limited generalization ability.
In this paper, we take a step further to study the hand-object interactions driven by part-level affordances, which provide fine-grained localizations and are generalizable among object categories. We first collect the specific part-level affordances on the objects, i.e. the hand-centered la-bels such as twist, pull, handle-grasp, and the corresponding parts, instead of the general labels such as use or handover, then manually adapt the hand poses to complete the inter-action tasks corresponding to these affordances. As shown in our dataset, the part-level affordances correspond to some common characteristics of the hand-object interactions even with different object categories, yet allows an extent of hand pose diversity, thus improving the understanding and pre-diction of hand-object interactions.
Our AffordPose is a large-scale dataset of fine-grained hand-object interactions with affordance-driven hand poses.
The dataset collects 26.7K manually annotated interactions, each including the 3D object shape, the part-level affor-dance label, and the parameters of the detailed hand con-figuration. Moreover, our dataset supports the interactions for different affordances on the same object, exhibiting the distinctiveness of the hand poses w.r.t. the corresponding part-level affordance.
Our contributions are listed as follows:
• We present the AffordPose dataset, a large-scale dataset of fine-grained hand-object interactions with affordance-driven hand pose.
• We provide comprehensive data analysis to understand how affordance affects the detailed arrangement of hand poses to complete the appropriate interaction.
• We conduct experiments on two tasks, i.e. hand-object affordance understanding and affordance-oriented hand-object interaction generation, to validate the ef-fectiveness of our dataset in learning the fine-grained hand-object interactions.
Table 1: Comparison of AffordPose dataset with the exist-ing hand-object interaction datasets. dataset
HO3D
YCBAfford RGB mod. syn/real #obj #hand pose
RGBD real
DexYCB RGBD real syn
RGBD syn
RGBD real
ContactPose RGBD real real
Obman
FPHAB 10 20 68 2.7k 26 25 51 100 1.7k 641 68 1k 367 21k 273 2.3k 1.3k 1k 49k 26k
GRAB
Mesh
OakInk-image RGBD
OakInk-shape Mesh
Mesh
Ours real syn intent
----object affordance human objective human objective human objective part affordance 2.