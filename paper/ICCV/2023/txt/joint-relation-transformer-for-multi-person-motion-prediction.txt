Abstract
Multi-person motion prediction is a challenging prob-lem due to the dependency of motion on both individ-ual past movements and interactions with other people.
Transformer-based methods have shown promising results on this task, but they miss the explicit relation representa-tion between joints, such as skeleton structure and pairwise distance, which is crucial for accurate interaction modeling.
In this paper, we propose the Joint-Relation Transformer, which utilizes relation information to enhance interaction modeling and improve future motion prediction. Our rela-tion information contains the relative distance and the intra-/inter-person physical constraints. To fuse relation and joint information, we design a novel joint-relation fusion layer with relation-aware attention to update both features. Ad-ditionally, we supervise the relation information by fore-casting future distance. Experiments show that our method achieves a 13.4% improvement of 900ms VIM on 3DPW-SoMoF/RC and 17.8%/12.0% improvement of 3s MPJPE on CMU-Mpcap/MuPoTS-3D dataset. Code is available at https://github.com/MediaBrain-SJTU/JRTransformer. 1.

Introduction
Multi-person motion prediction aims to predict the future positions of skeleton joints for multiple individuals based on their historical movements. Compared to traditional single-person motion prediction [11, 7, 4, 24, 48, 47, 27], multi-person motion prediction is more practical as people are mostly associated with a group and interacting with each other.
It is also more challenging because sophisticated interactions across different individuals need to be consid-ered. The related methods are playing significant roles in a wide range of practical applications, including autonomous driving [15, 38, 10, 51], surveillance systems [13, 42, 52] and healthcare monitoring [43]. They also pave a path to better human-robot interaction [6, 12].
Previous works on multi-person motion prediction gen-*Equal contribution.
†Corresponding author.
Figure 1. System comparison between the standard Transformer and the proposed Joint-Relation Transformer. erally adopt two types of architectures, including graph neu-ral networks (GNNs) and Transformer. The former one, such as TRiPOD [3], models the multi-person interaction via a graph structure, however, it suffers from the inherent oversoomthing problem in GNNs, thus can only afford shal-low models with limited learning capacities; while the latter one, such as MRT [46] and SoMoFormer [40], treat tempo-ral sequence or skeleton joints as a sequence input, and learn to establish the relations between them via self-attention mechanism. Compared with GNN-based methods, Trans-former has shown strong learning ability, thus becoming a default backbone for multi-person motion prediction.
While encouraging results are shown in the Transformer-based methods, those previous works only implicitly learn the inter-joint relation through the attention mechanism and lack the explicit awareness of skeleton structure. To ad-dress this issue, we propose Joint-Relation Transformer, a two-stream Transformer architecture for multi-person mo-tion prediction.
Instead of only updating the features for body joints, Joint-Relation Transformer uses two streams to achieve feature learning for both joints and relations. In specific, one stream encodes the sequence of skeleton joints, containing the historical movement information of the joints in world coordinates; and the other one explicitly takes the joints-to-joints relation as input, including the relative dis-tance between two joints and physical constraints, such as inter-body constraints and intra-body skeleton connections.
To effectively fuse and update features of joints and re-lation branches, we design a novel relation-aware attention to update joints’ features with the incorporation of relation features. During the update procedure, attention scores be-tween skeleton joints are calculated from two sources: the similarity score between two joints’ features and the addi-tional relation score based on the relation feature between these two joints. This approach enhances the model’s abil-ity to distinguish between similar joint features belonging to distinct persons, leading to more accurate joint updates.
Moreover, this design allows for increased granularity in at-tention allocation, enabling the concentration on the most pertinent joint features for each person, consequently en-hancing prediction performance.
To train our proposed Joint-Relation Transformer, in ad-dition to inferring joint positions, we also supervise the model by predicting future inter-joint distances, which con-tains the future relationship between two joints. This relative distance supervision is translation and rotation invariant of the input sequence, adhering to invariance properties of multi-person interactions. As to evaluate the effectiveness of our method, we conduct experiments on four multi-person motion prediction datasets: 3DPW-SoMoF [3], 3DPW-SoMoF/RC, CMU-Mocap [1], and
MuPoTS-3D [32]. The quantitative results show we out-perform the previous methods and achieve state-of-the-art performance on most datasets. The qualitative results ver-ify the reasonable attention allocation and vivid predictions.
To summarise, in this paper, we make the following con-tributions: (i) We propose the Joint-Relation Transformer for multi-person motion prediction. We innovatively intro-duce the relation information, which explicitly builds the relationship between joints of the inter-/intra-body; (ii) We design a relation-aware attention module to update the joint information with the corporation of explicit relation infor-mation, increasing granularity in attention allocation and enhancing prediction performance; (iii) We further super-vise the relation information between two joints with the fu-ture relative distance to better capture the interaction infor-mation hidden in the distance variation; (iv) We perform our experiments on several common datasets and our proposed method outperforms most state-of-the-art methods. We also conduct thorough ablation study to show the importance of the proposed relation information and relation-aware atten-tion in the task of multi-person motion estimation. 2.