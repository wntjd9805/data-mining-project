Abstract
Diffusion models generating images conditionally on text, such as Dall-E 2 [51] and Stable Diffusion[53], have recently made a splash far beyond the computer vision com-munity. Here, we tackle the related problem of generating point clouds, both unconditionally, and conditionally with images. For the latter, we introduce a novel geometrically-motivated conditioning scheme based on projecting sparse image features into the point cloud and attaching them to each individual point, at every step in the denoising process.
This approach improves geometric consistency and yields greater fidelity than current methods relying on unstruc-tured, global latent codes. Additionally, we show how to ap-ply recent continuous-time diffusion schemes [59, 21]. Our method performs on par or above the state of art on con-ditional and unconditional experiments on synthetic data, while being faster, lighter, and delivering tractable likeli-hoods. We show it can also scale to diverse indoors scenes. 1.

Introduction
Given the popularity of depth sensors and laser scanners, point clouds have become ubiquitous, with applications to robotics, autonomous driving, and augmented reality. Fur-thermore, they do not suffer from the precision/complexity trade-off inherent to voxel grids, and scale better and more generally than graph-based representations such as meshes.
As a result, they have been extensively used for analytical tasks such as classification [46, 47, 60, 70, 48] and segmen-tation [46, 47, 16, 60, 12, 70, 72]. Recent work has turned to point cloud synthesis and its many applications to 3D content creation. However, this remains an emerging field and state of the art methods [66, 4, 25, 38, 71, 68] operate on small datasets that feature only a handful of object types
[5]. More importantly, the generated shapes are typically not anchored in any prior and are thus difficult to control.
We present a novel approach that can mitigate these is-sues, taking our inspiration from generative methods that perturb samples with a diffusion process [58, 21] and de-Figure 1: Our generative approach is based on denoising diffusion models (DDMs) and can be conditioned on im-ages. At each denoising step we project the point cloud to the image, sample sparse features, and concatenate them to the locations, thus guiding the denoising process and yield-ing point clouds consistent with the images. noise them with a deep network, which can later be used to synthesize new samples by iteratively denoising a sig-nal. Specifically, most denoising-based approaches gener-ate novel samples from pure noise. But to mirror the suc-cess of text-based image synthesis [51, 53, 54], a generative approach must be able to not only produce samples of suffi-cient quality and diversity, but also ground them in contex-tual information. Applying this generic idea to point clouds is, however, not straightforward. We show how to achieve this by conditioning the network with sparse image features.
Unlike previous works relying on unstructured, global embeddings, we do so in a geometrically-principled way, by projecting the point cloud into an image, sampling sparse features at those locations, and feeding them to the network along with the point location, at each denoising step, as il-lustrated in Fig. 1. This allows us to render 3D objects geometrically and semantically consistent with the image content, while controlling the viewpoint. Unlike regression models, such as monocular depth, our method can gener-ate plausible hypotheses for occluded regions. This work is thus a first step towards unlocking the applicability of denoising diffusion models to practical scenarios such as 3D content creation, generating priors for automotive or
robotics applications, and single-view 3D reconstruction.
In short, we propose a novel generative point cloud model and show how to condition it on images. Our main contributions are: (i) We propose a framework composed of a permutation-equivariant Set Transformer [31] trained with a continuous-time diffusion scheme, which performs on par with the state of the art on unconditional synthesis while running 10x faster and delivering exact probabilities. (ii) We augment it with geometrically-principled condition-ing to generate point clouds from images, yielding better reconstructions than with unstructured global embeddings, with state of the art performance. (iii) We bring denoising diffusion models for point cloud synthesis to the real world by applying our method to the Taskonomy dataset [67]. 2.