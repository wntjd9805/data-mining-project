Abstract
While NeRF-based human representations have shown impressive novel view synthesis results, most methods still rely on a large number of images / views for training. In this work, we propose a novel animatable NeRF called
ActorsNeRF. It is first pre-trained on diverse human sub-jects, and then adapted with few-shot monocular video frames for a new actor with unseen poses. Building on previous generalizable NeRFs with parameter sharing us-ing a ConvNet encoder, ActorsNeRF further adopts two hu-man priors to capture the large human appearance, shape, and pose variations. Specifically, in the encoded feature space, we will first align different human subjects in a category-level canonical space, and then align the same human from different frames in an instance-level canon-ical space for rendering. We quantitatively and qualita-tively demonstrate that ActorsNeRF significantly outper-forms the existing state-of-the-art on few-shot generaliza-tion to new people and poses on multiple datasets. Project page: https://jitengmu.github.io/ActorsNeRF/. 1.

Introduction
Recent advances in Neural Radiance Fields (NeRF) [42] have enabled significant progress in free-viewpoint ren-dering of humans performing complex movements. The possibility of achieving photo-realistic rendering is of ma-jor interest for various real-world applications in AR or to achieve high-quality rendering, exist-VR. However, ing approaches [33, 23, 32, 29, 42] require a combination of synchronized multi-view videos and an instance-level
NeRF network, trained on a specific human video sequence.
While results are encouraging, the multi-view requirement is a significant challenge to applications involving videos in the wild. Recently, progress has been made to eliminate this constraint, by enabling human rendering from a monoc-ular video [48, 15]. However, these approaches still require a large number of frames, which covers a person densely from all viewpoints.
In this work, we consider the more practical setting and ask the question: Can an animatable human model be learned from just a few images? We hypothesize that this is possible by introducing a class-level encoder, trained over multiple people, as shown in Figure 1. This hypothesis has been demonstrated by recent works on generalizable
NeRFs [53, 2], where an encoder network is trained across multiple scenes or objects within the same category to con-struct NeRF. By parameter sharing through the encoder, the prior learned across different scenes can be re-used to per-form synthesis even with a few views. However, most ap-proaches can only model static scenes. We investigate how generalizable NeRF can be extended to the learning of a 1
good prior for the much more complex setting of videos of humans performing activities involving many degrees of freedom, large motions, and complex texture patterns.
For this, we introduce ActorsNeRF, a category-level hu-man actor NeRF model that is transferable, to unseen hu-mans in novel action poses, in a few-shot setup. This setup requires the animation of a previously unseen human actor into unseen views and poses, from a few frames of monoc-ular video. Such generalization requires more than simple parameter sharing via an encoder network, and can bene-fit from the incorporation of explicit human priors. Our insight is that, while human actions and appearances are complex, all humans can be coarsely aligned in a category-level canonical space using a parametric model such as
SMPL [24]. Fine-grained alignment can then benefit from an instance-level canonical space derived from both this prior and the few-shot data available for the target actor.
To implement this insight, we endow ActorsNeRF with a 2-level canonical space. Given a body pose and a rendering viewpoint, a sampled point in 3D space is first transformed into a canonical (T-pose) space by linear blend skinning (LBS) [24], where the skinning weights are generated by a skinning weight network that is shared across various sub-jects. Since LBS only models a coarse shape (similar to an
SMPL mesh), we refer to this T-pose space as the category-level canonical space. Direct rendering from the latter fails to capture the shape and texture details that distinguish dif-ferent people. To overcome this limitation, points in the category-level canonical space are further mapped into an instance-level canonical space by a deformation network.
A rendering network finally maps the combination of pixel-aligned encoder features and points into corresponding col-ors and densities.
ActorsNeRF is designed such that the combination of feature encoder and skinning weight network forms a category-level shape and appearance prior, and the deforma-tion network learns the mapping to the instance-level canon-ical space. To adapt to a novel human actor at test time, only the deformation network (instance-level) and rendering net-work are fine-tuned with the few-shot monocular images.
The image encoder and skinning weight network are frozen.
We quantitatively and qualitatively demonstrate that Ac-torsNeRF outperforms the existing approaches by a large margin on various few-shot settings for both ZJU-MoCap
Dataset [33] and AIST++ Dataset [21]. To the best of our knowledge, we are the first to explore few-shot general-ization from few-shot monocular videos in the context of
NeRF-based human representations. 2.