Abstract
The goal of this paper is to localize action instances in a long untrimmed query video using just meager trimmed support videos representing a common action whose class information is not given. In this task, it is crucial to mine reliable temporal cues representing a common action from handful support videos.
In our work, we develop an at-tention mechanism using cross-correlation. Based on this cross-attention, we ﬁrst transform the support videos into query video’s context to emphasize query-relevant impor-tant frames, and suppress less relevant ones. Next, we summarize sub-sequences of support video frames to rep-resent temporal dynamics in coarse temporal granularity, which is then propagated to the ﬁne-grained support video features through the cross-attention.
In each case, the cross-attentions are applied to each support video in the individual-to-all strategy to balance heterogeneity and com-patibility of the support videos. In contrast, the candidate instances in the query video are lastly attended by the re-sulting support video features, at once. In addition, we also develop a relational classiﬁer head based on the query and support video representations. We show the effectiveness of our work with the state-of-the-art (SOTA) performance in benchmark datasets (ActivityNet1.3 and THUMOS14), and analyze each component extensively. 1.

Introduction
Temporal action localization [2, 4, 33, 23, 31, 18, 15, 16, 20] have been widely studied in fully or weakly-supervised manner. However, they require collecting massive videos labeled by action classes, and also only detect the action classes observed in training. Whereas, we aim to tempo-rally localize action instances in a long untrimmed query
*Work completed during employment at Qualcomm Technolo-gies, Inc.
†Qualcomm AI Research is an initiative of Qualcomm Tech-nologies, Inc.
Figure 1: Attention represents the relationship between a (a) Vanilla: atten-moment of query and support videos. tion of a query proposal is obtained simultaneously for the frames of all the support videos. (b) Ours: the attention is computed for the frames of one support video, at a time.
While important frames of support video #2 cannot be at-tended by the related query proposal in (a), they are appro-priately transformed to the context of the query in (b). video based on the few trimmed support videos describing a common action class. As the testing action class is unseen in training and no ground-truth class cue is given, the only cue is the commonality of the few support videos.
In this task, the alignment between query and support videos is important, which can be attained by representing the common action cues of interest from the support videos considering the query video’s context. For better alignment, we divide this problem into two points: re-calibrating sup-port video features under query video’s context, and en-hancing temporal dynamics and compatibility of the re-calibrated support video features.
Existing methods [39, 22, 7, 3] have mainly focused on the former point, handling the multiple support videos as a whole. However, as exempliﬁed in Fig. 1, though the sup-port videos represent a common action class, their context (e.g background, camera angle) can be different. Hence,
Figure 2: Overall process of few-shot common action local-ization in our work. when all the support videos are transformed to query con-text simultaneously, the support videos cluttered by back-ground are overly suppressed although they include useful information (Fig. 1(a)). Whereas, in Fig. 1(b), we can lever-age the support videos by attending each individually.
Regarding the latter point, we pay attention to that the temporal dynamics can be enriched from multiple tempo-ral granularities. Hence, we try to collaboratively fuse the support video features in different temporal granularities, considering the compatibility of different support videos as well. Based on the insight for each point, we propose a novel few-shot common action localization method. The overall process is brieﬂy outlined in Fig. 2. First, the long untrimmed query video is split to query proposals, and they are aligned with the support videos through our three-stage cross-attention. Then, the proposals representing the com-mon action class are detected with temporal location reﬁne-ment.
For the query-support alignment, we develop a three-stage cross-attention (CA) mechanism. In each stage, cross-correlation between two different features (e.g. support and query, supports in different temporal granularity) is ex-ploited with a learnable weight matrix. The 1st stage, query-to-support Context-CA transforms the support videos into the query video’s context by cross-correlating them with the query proposal features. Each support video is individually attended to emphasize its most informative features. In 2nd stage of Dynamics-CA, low-temporal granularity features summarize the tuples of snippets of each support video, and they attend the ﬁne-grained snippet-level features. Here, each support video is attended by the entire support videos in low-temporal granularity.
It helps the complementary use of all the individually attended support video features as well as enhancing temporal dynamics. Finally, in the last stage (support-to-query Context-CA), the query video features are simultaneously attended with all the resulting support video features. We call this CA process (Context,
Dynamics, Context-CAs) CDC-CA.
Also, for the commonality prediction and reﬁnement, we design a relational classiﬁer with an action classiﬁer and an auxiliary relational module. To prevent overﬁtting and boost performance, the latter matches the support and query video features using pseudo-label cues as we avoid the use of class labels. In testing, this module is not used.
Major contributions: (i) We suggest a three-stage CA to enhance the representation of query video by support videos and vice versa. (ii) We attend each support video individ-ually to increase its discriminative ability in the ﬁrst two stages. (iii) We develop a relational classiﬁer including an action classiﬁer and an auxiliary relational module. The lat-ter is only needed during training. (iv) Extensive experi-mental analyses are done on two benchmark datasets, where we achieve SOTA performance. 2.