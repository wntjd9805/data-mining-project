Abstract
This paper introduces an end-to-end residual network that operates entirely on the Poincar´e ball model of hyper-bolic space. Hyperbolic learning has recently shown great potential for visual understanding, but is currently only per-formed in the penultimate layer(s) of deep networks. All visual representations are still learned through standard
Euclidean networks.
In this paper we investigate how to learn hyperbolic representations of visual data directly from the pixel-level. We propose Poincar´e ResNet, a hyperbolic counterpart of the celebrated residual network, starting from Poincar´e 2D convolutions up to Poincar´e residual con-nections. We identify three roadblocks for training convolu-tional networks entirely in hyperbolic space and propose a solution for each: (i) Current hyperbolic network initial-izations collapse to the origin, limiting their applicability in deeper networks. We provide an identity-based initial-ization that preserves norms over many layers. (ii) Resid-ual networks rely heavily on batch normalization, which comes with expensive Fr´echet mean calculations in hyper-bolic space. We introduce Poincar´e midpoint batch normal-ization as a faster and equally effective alternative. (iii) Due to the many intermediate operations in Poincar´e layers, the computation graphs of deep learning libraries blow up, lim-iting our ability to train on deep hyperbolic networks. We provide manual backward derivations of core hyperbolic operations to maintain manageable computation graphs. 1.

Introduction
Deep learning in hyperbolic space has gained traction in recent years empowered by their inherent ability to em-bed hierarchical data with arbitrarily low distortion [45] and by being more compact and dense [5, 42, 47]. These promising characteristics have led to rapid developments in hyperbolic representation learning for tree-like structures
[1, 4, 15, 27, 42, 44], graphs [5, 9, 29, 60], text [7, 11, 49], action skeletons [14], biological structures [26], and more.
Recently, hyperbolic learning has also been investigated for visual understanding. Hyperbolic embeddings of images and videos have been shown to improve few-shot learning
[13, 17, 20, 35, 57], hierarchical recognition [10, 18, 32, 33, 56], segmentation [6, 19] and metric learning [12, 59] amongst others. While promising, the use of hyperbolic ge-ometry in computer vision has been limited to the classifier space, with visual representations being learned on conven-tional networks that operate in Euclidean space.
This paper explores learning visual representations en-tirely in hyperbolic space. The ability to learn hyperbolic representations directly from the pixel-level will allow us to unlock the broad potential of hyperbolic geometry for vision, such as capturing latent hierarchical visual rep-resentations [25], training compact network architectures
[5, 42, 47], and creating networks that better mimic visual representation learning in the brain [58]. Empowered by successful implementations of non-visual layers [16, 47], the time is ripe for visual hyperbolic feature learning.
As a step towards fully hyperbolic visual learning, we start from the highly celebrated ResNet [22] and rebuild its architecture in hyperbolic space; from 2D convolutions to residual connections. Optimizing a ResNet in the Poincar´e ball model comes with several challenges. First, we find that existing network initializations in hyperbolic space lead to vanishing signals, which derail learning over many con-volutional layers. We provide an identity-based network initialization that preserves the output norm over many lay-ers. Second, ResNets rely extensively on batch normal-ization, but its generalization to hyperbolic space requires expensive Fr´echet mean calculations [34]. We introduce
Poincar´e midpoint batch normalization, which allows us to compute approximate means at a fraction of the computa-tional cost. Third, the basic gyrovector operations in the
Poincar´e ball model consist of many intermediate calcula-tions.
In modern deep learning libraries, all these calcu-lations are stored for automatic differentiation, blowing up the computation graph. We have derived and implemented the backward pass of core hyperbolic gyrovector operations
to contain the computation graph.
Empirically, we show that our network initialization is indeed norm-preserving and improves network gener-alization. We show that our midpoint batch normaliza-tion speeds up training by 25% with no loss in classifica-tion accuracy. We furthermore demonstrate the potential of Poincar´e ResNet for out-of-distribution detection, adver-sarial robustness, and learning complementary representa-tions compared to Euclidean ResNet. The code is avail-able at https://github.com/maxvanspengler/ poincare-resnet with a similar implementation in the documentation of HypLL [52]. 2.