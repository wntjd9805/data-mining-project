Abstract
Deep Neural Networks have exhibited considerable suc-cess in various visual tasks. However, when applied to un-seen test datasets, state-of-the-art models often suffer per-In this pa-formance degradation due to domain shifts. per, we introduce a novel approach for domain general-ization from a novel perspective of enhancing the robust-ness of channels in feature maps to domain shifts. We ob-serve that models trained on source domains contain a sub-stantial number of channels that exhibit unstable activa-tions across different domains, which are inclined to cap-ture domain-specific features and behave abnormally when exposed to unseen target domains. To address the issue, we propose a DomainDrop framework to continuously en-hance the channel robustness to domain shifts, where a domain discriminator is used to identify and drop unsta-ble channels in feature maps of each network layer dur-ing forward propagation. We theoretically prove that our framework could effectively lower the generalization bound.
Extensive experiments on several benchmarks indicate that our framework achieves state-of-the-art performance com-pared to other competing methods. Our code is available at https://github.com/lingeringlight/DomainDrop. 1.

Introduction
Deep neural networks (DNNs) have shown impressive performance in computer vision tasks over the past few years. However, this performance often degrades when the test data follows a different distribution from the training data [29]. This issue, known as domain shift [39], has
*Corresponding authors: Yinghuan Shi and Lei Qi. The work is sup-ported by NSFC Program (62222604, 62206052, 62192783), China Post-doctoral Science Foundation Project (2023T160100), Jiangsu Natural Sci-ence Foundation Project (BK20210224), and CCF-Lenovo Bule Ocean Re-search Fund.
Figure 1. The robustness of channel to domain shifts. We inves-tigate channel robustness using the histogram of activations based on their standard deviation across different domains. We experi-ment on PACS [29] with sketch as the target domain and analyze the representations from the last residual block of ResNet-18. For each channel, averaged activations are computed across all sam-ples from each domain, and the standard deviation is calculated on domain dimension to indicate its robustness to domain shifts. greatly impaired the applications of DNNs [32, 61], as train-ing and test data often come from different distributions in reality. To address this issue, domain adaptation (DA) has been widely studied under the assumption that some labeled or unlabeled target domain data can be observed during training [13, 58]. Despite their success, DA models cannot guarantee their performance on unknown target do-mains that have not been seen during training, which makes them unsuitable for some real-world scenarios where target data are not always available [57, 47]. Therefore, domain generalization (DG) is proposed as a more challenging yet practical setting, which aims to utilize multiple different but related source domains to train a model that is expected to generalize well on arbitrary unseen target domains [71, 55].
The core idea of existing DG methods is to learn domain-invariant feature distribution P (F (X)) across domains for the robustness of conditional distribution P (Y |F (X)),
where F (X) denotes the extracted features from input X, and Y is the corresponding label. Traditional DG methods primarily impose constraints on the whole network (i.e., the prediction layer) to supervise the model to learn domain-invariant features [15, 74, 27, 43]. However, these methods do not explicitly guide the model to remove domain-specific features in middle network layers, which could lead to the model learning excessive domain-related information.
In this paper, we revisit DG issue from a novel perspective of feature channels, which indicates that model generaliza-tion could be related to the robustness of feature channels to domain shifts. Specifically, we quantify the robustness of each channel to domain shifts by computing the stan-dard deviation of activations across different domains. As shown in Fig. 1, we observe that models trained on source domains often contain numerous non-robust channels that exhibit unstable activations for different domains, indicat-ing that they are likely to capture domain-specific features.
When domain shifts, these unstable channels are likely to produce abnormal activations on the unseen target domain, leading to a shift in the conditional distribution. These un-stable channels are dubbed as “domain-sensitive channels”.
Based on the above observation, we propose Domain-Drop, a simple yet effective framework that explicitly mutes unstable channels across domains. Unlike previous DG methods that seek to directly distill domain-invariant fea-tures, our method aims to continuously guide the model to remove domain-specific features during forward propa-gation. To this end, we introduce a domain discriminator to assign each channel a specific dropout rate according to its effectiveness for the domain discrimination task. The more the channel contributes to domain prediction, the more likely it contains domain-specific features, and the greater the probability of it being discarded. Moreover, we discover that unstable channels exist at both shallow and deep net-work layers, which contrasts with existing dropout methods that drop either high-level or low-level features. Thus, we propose a layer-wise training strategy that inserts Domain-Drop at a random middle layer of the network at each itera-tion, which can sufficiently narrow domain gaps in multiple network layers. To further enhance the robustness of chan-nels against domain shifts, we adopt a dual consistency loss to regularize the model outputs under various perturbations of DomainDrop. Furthermore, we provide theoretical evi-dence that our method could effectively lower the general-ization error bound of the model on unseen target domains.
Our contributions can be summarized as follows:
• We propose a novel dropout-based framework for DG, which explicitly suppresses domain-sensitive channels to enhance the robustness of channels to domain shifts.
• We theoretically prove that removing domain-sensitive channels during training could result in a tighter gen-eralization error bound and better generalizability.
• We evaluate our method on four standard datasets. The results demonstrate that our framework achieves state-of-the-art performance on all benchmarks. 2.