Abstract the claimed operating domain,
Although complex-valued convolutional neural networks (iCNNs) have existed for a while, they lack proper complex-valued image inputs and loss functions.
In addition, all their operations are not complex-valued as they have both complex-valued convolutional layers and real-valued fully-connected layers. As a result, they lack an end-to-end
ﬂow of complex-valued information, making them incon-sistent w.r.t. i.e., com-plex numbers. Considering these inconsistencies, we pro-pose a complex-valued color model and loss function and turn fully-connected layers into convolutional lay-ers. All these contributions culminate in what we call
FCCNs (Fully Complex-valued Convolutional Networks), which take complex-valued images as inputs, perform only complex-valued operations, and have a complex-valued loss function. Thus, our proposed FCCNs have an end-to-end ﬂow of complex-valued information, which lacks in existing iCNNs. Our extensive experiments on ﬁve image classiﬁcation benchmark datasets show that FCCNs consis-tently perform better than existing iCNNs. Code is available at https://github.com/saurabhya/FCCNs. 1.

Introduction
In the ever-evolving deep learning era, complex-valued neural networks (iCNNs) show great promise, given their ability to learn complex-valued representations. Neverthe-less, iCNNs have some inconsistencies which should be ad-dressed: (i) Input images are real-valued. (ii) They are not (iii) They have real-valued losses. fully complex-valued.
Because of these inconsistencies, we don’t witness an end-to-end ﬂow of complex-valued information in these net-works. As a result, it’s difﬁcult to declare them as truly complex-valued. This paper aims to weed out these incon-sistencies and uncover their true potential in computer vi-sion research.
Figure 1. We introduce Fully Complex-valued Convolutional Net-works (FCCNs), which leverage (i) complex-valued color in-puts using our novel iHSV color model, (ii) complex-valued 1x1 convolutions to turn even fully-connected layers into convolu-tional layers, and (iii) our novel complex-valued loss function to tackle the deﬁcit between |y|ei0 (complex-valued labels) and
|z|eiθ (complex-valued outputs). Hence, FCCNs operate entirely in complex-domain.
However, several challenges are involved: (i) No complex-valued color model is readily available to allow us to have real and imaginary components of an image, as shown in Fig. 1. (ii) No prior study exists on how fully-connected layers can be made complex-valued to have an end-to-end ﬂow of complex-valued information. (iii) There are no well-tested complex-valued loss functions for deep learning approaches.
Despite these challenges, what motivates us to explore complex-valued representation for color images is the sheer potential complex-valued networks have shown in other domains such as MRI [7, 40], radar signals [10, 11] and audio signals [12, 16], where complex-valued inputs are readily available. As per the recent studies conducted in
[1, 8, 19, 27], iCNNs have performed better than their real-valued counterparts. Moreover, they are biologically moti-vated [31] and possess greater generalization capacity [15], which inspires us to explore HSV color space for such in-puts since it aligns with how human vision perceives color-making attributes.
In the past, in [37], authors have tried image classiﬁca-tion using iCNNs inputting RGB as the real part and learned features from RGB as the imaginary part of an image. How-ever, we argue that components of complex numbers are supposed to be orthogonal, which means they are mutually independent. So, it should not be possible to derive one part from another. In [34], authors look for orthogonality and propose to use {L∗ + i0, a + ib} and {R + iG, G + iB} as possible complex-valued inputs. Note that these are more like complex-valued encodings than a complex-valued rep-resentation of an image. That is because we cannot decom-pose an image based on them into real and imaginary parts (as shown in Fig. 1) that look like images by themselves.
Moreover, HSV color space is not explored yet for orthog-onality and complex-valued representation.
There have also been attempts to preserve complex-valued information in fully-connected layers through man-ifold learning [2]. Nevertheless, the complex-valued rep-resentation is sacriﬁced, and that was done due to the lack of complex-valued losses; otherwise, imaginary or phase information would be required to be left out at the end.
Thus, there is a need for complex-valued loss functions to preserve complex-valued representations throughout, and we can derive complex-valued loss function from prevalent cross-entropy loss itself. Moreover, since fully-connected layers can be implemented through convolution operations and complex-valued convolutions already exist, it must be possible to have fully complex-valued networks.
More speciﬁcally, we develop FCCNs (Fully Complex-valued Convolutional Networks), which take complex-valued images as inputs, perform only complex-valued op-erations, and have a complex-valued loss function. For in-puts, we observe that any color in the cylindrical HSV color model has intersecting color planes. For e.g. (H0, S0, V0) color will have H = H0, S = S0 and V = V0 planes.
If we consider these planes as argand planes, we can ob-tain complex numbers using the locations of that color on them. We use these complex numbers to represent any color. We call this color model iHSV. Following this, we can generate a complex-valued representation of any image
I → Ire + iIim, as shown in Fig. 1, where we can separate an image into real and imaginary parts (images). When im-ages are supplied in this form to our FCCNs, obtained by implementing fully-connected layers as convolutional lay-ers, we get a complex-valued output, which is then matched against the label (in complex form) by our complex-valued loss function to help train the network, as shown in Fig. 1.
Our contributions in this paper are three-fold: (i) We pro-pose iHSV, a novel complex-valued color model. (ii) We de-rive a complex-valued loss function from the cross-entropy loss. (iii) We turn iCNNs into FCCNs by implementing fully-connected layers as convolutional layers. 2.