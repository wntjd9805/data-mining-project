Abstract
Learning robust local image feature matching is a funda-mental low-level vision task, which has been widely explored in the past few years. Recently, detector-free local feature matchers based on transformers have shown promising re-sults, which largely outperform pure Convolutional Neural
Network (CNN) based ones. But correlations produced by transformer-based methods are spatially limited to the cen-ter of source views’ coarse patches, because of the costly attention learning. In this work, we rethink this issue and find that such matching formulation degrades pose estima-tion, especially for low-resolution images. So we propose a transformer-based cascade matching model – Cascade fea-ture Matching TRansformer (CasMTR)§, to efficiently learn dense feature correlations, which allows us to choose more reliable matching pairs for the relative pose estimation. In-stead of re-training a new detector, we use a simple yet effec-tive Non-Maximum Suppression (NMS) post-process to filter keypoints through the confidence map, and largely improve the matching precision. CasMTR achieves state-of-the-art performance in indoor and outdoor pose estimation as well as visual localization. Moreover, thorough ablations show the efficacy of the proposed components and techniques. 1.

Introduction
Image matching is an important vision problem that is widely employed for many downstream tasks like Structure-from-Motion [39], Simultaneous Localization and Map-ping [31], and visual localization [28]. However, accurately matching two or more images remains difficult due to vari-ous factors, such as differences in viewpoints, illuminations, seasons, and surroundings. Classical approaches [26, 36] address it via the pipeline of detection, description, and matching of features by hand-crafted features. Recently, learning Convolutional Neural Network (CNN) based de-†Corresponding author.
§Code is available at https://github.com/ewrfcas/CasMTR
Table 1: Summary of test image size, backbones, memory cost (GB), and inference speed (s/image) on MegaDepth [22] with
AUC of different pose errors (%). Suffixes ‘-8c’, ‘-4c’, and ‘-2c’ denote matching at 1/8, 1/4, and 1/2 of image size. Baseline:
QuadTree [47] with the same backbone as ours. Directly imple-menting QuadTree-4c causes Out-of-memory (OOM) error in a 32GB GPU, so its inference speed is estimated in brackets.
FPN
Methods
Backbone Size
Pose Est. AUC
@5° @10° @20°
Baseline-8c Twins+FPN 704 51.63 68.54 80.98
CasMTR-4c Twins+FPN 704 52.59 69.78 82.31
CasMTR-2c Twins+FPN 704 54.91 71.27 83.01
QuadTree-8c 832 52.87 69.24 81.32
Baseline-8c Twins+FPN 832 52.90 69.78 82.05
QuadTree-4c
CasMTR-4c Twins+FPN 832 53.63 70.34 82.55
CasMTR-2c Twins+FPN 832 55.61 71.96 83.52
QuadTree-8c 1152 55.09 71.31 83.20
Baseline-8c Twins+FPN 1152 55.77 72.01 83.64
QuadTree-4c
CasMTR-4c Twins+FPN 1152 56.34 72.11 83.55
CasMTR-2c Twins+FPN 1152 56.90 72.94 84.24 1152
FPN
FPN
FPN 832 – – – – – –
Mem.(G) s/img 0.146 3.83 0.212 3.99 0.311 6.29 0.203 5.72 4.91 0.207
OOM (0.602) 0.304 4.91 0.444 7.55 0.424 12.62 13.33 0.423
OOM (1.442) 0.649 12.40 0.887 14.36 tectors [9, 33, 38, 49, 55] have been utilized to detect and describe keypoints, leading to significant improvements in this pipeline. But such detector-based CNNs suffer from limited receptive fields and search space, as noticed in [43].
To solve this issue, transformer-based detector-free meth-ods have emerged as more robust alternatives, demon-strating impressive matching abilities in texture-less re-gions [43, 18, 47, 57, 4]. However, the high computa-tional cost of attention limits transformer-based methods to ‘semi-dense’ matching, where source matching points are spaced apart at intervals of coarse feature space, as shown in Fig.1(a,d). Such semi-dense matching leads to an issue that keypoint locations are not informative enough: the spa-tially restricted source points in coarse feature maps lack the necessary details to express structural information, making it difficult to accurately estimate pose. This problem is es-pecially challenging for low-resolution images, as seen in our pilot study (Tab. 1). More experiments based on extreme resolutions are discussed in the supplementary. Further-more, it remains unclear whether transformer-based methods
Figure 1: QuadTree [47] (a,d) vs our CasMTR (b,c,e). Our method achieves more fine-grained matching pairs for both source and target images (b). It is further improved by our NMS detection, which retains reliable matching results located in structural keypoints (c,e). We show an intuitive motivation for our spatially informative keypoints in (f). Best viewed in color. components as follows. Firstly, inherited in MVS, coarse-to-fine cascade matching modules are repurposed with different efficient attention mechanisms [19, 58, 6, 47, 14, 67] to overcome the semi-dense matching in coarse features. We present the local non-overlapping [6] and overlapping [67] self-attention for outdoor and indoor cases respectively, due to different illuminations and surroundings. Secondly,
CasMTR enjoys flexible training by a novel Parameter and
Memory-efficient Tuning method (PMT), which is originally derived for NLP tasks [44]. Essentially, PMT can incre-mentally finetune CasMTR based on off-the-shelf matching models with reliable coarse matching initialization and fast convergence. Thirdly, we for the first time introduce the training-free NMS detection to complementarily filter pre-cise matches based on dense matching confidence maps of
CasMTR. Critically, NMS serves as a simple yet effective post-processing that preserves structurally meaningful key-points rather than the coarse patch center as in Fig. 1(e).
This improves the pose estimation as in Fig. 1(c) and has good generalization for various high-resolution matching tasks [22, 1, 45, 65]. Finally, in the development of our model, we have learned that the devil is in the details. Con-sequently, several non-trivial technical improvements have been implemented to our newly proposed matching pipeline (highlighted in Fig. 2), such as pre-training transformer back-bones, improving efficient linear attention, and optimizing self and cross attention for high-resolution matching.
The proposed CasMTR is comprehensively evaluated in relative pose estimation [22, 7], homography estimation [1], and visual localization [45, 65], showing its state-of-the-art performance. Additionally, our exhaustive ablation studies show the effectiveness of all newly proposed components. 2.