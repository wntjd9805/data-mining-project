Abstract
The transferability of adversarial examples is a key issue in the security of deep neural networks. The possibility of an adversarial example crafted for a source model fooling another targeted model makes the threat of adversarial at-tacks more realistic. Measuring transferability is a crucial problem, but the Attack Success Rate alone does not provide a sound evaluation. This paper proposes a new methodol-ogy for evaluating transferability by putting distortion in a central position. This new tool shows that transferable at-tacks may perform far worse than a black box attack if the attacker randomly picks the source model. To address this issue, we propose a new selection mechanism, called FiT, which aims at choosing the best source model with only a few preliminary queries to the target. Our experimental re-sults show that FiT is highly effective at selecting the best source model for multiple scenarios such as single-model attacks, ensemble-model attacks and multiple attacks. 1.

Introduction
Transferability is one of the most intriguing properties of adversarial examples. A white box attack crafting adversar-ial examples for an open-source model is likely to fool other models too [1, 14, 20, 25, 34]. This makes the threat of ad-versarial examples more realistic. In practice, the model tar-geted is usually unknown but accessible as a black box. This prevents directly applying any white box gradient-based at-tack [10, 16, 19, 35]. Black box attacks do exist but they require some thousands of queries to find an adversarial ex-ample of low distortion [4, 11, 17, 24]. Transferable attacks require no or few queries to fine-tune an adversarial exam-ple thanks to the help of a publicly available model similar enough to the target.
Transferability is usually measured by the Attack Suc-cess Rate (ASR), i.e., the probability that the adversarial
*Thanks to Rennes M´etropole for its funding for international mobility.
†Thanks to ANR and AID french agencies for funding Chaire SAIDA. e t a
R s s e c c u
S k c a t t
A 1.0 0.8 0.6 0.4 0.2 0.0
Transferable attack on average
Transferable attack with FiT
Black box attack
White box attack 0 2 4 6 8
Distortion 10 12
Figure 1: Evaluation of transferability by comparing the
Attack Success Rate vs. distortion trade-off of a white box, transferable, and black box attacks against model
CoatLitesmall (See Sect. 4.1 for details). The blue area is the range of trade-off operated by a transferable attack with random source models. A transferable attack may be worse than a black box attack without a good source selec-tion (like FiT). example crafted for the source model also deludes the tar-get model. We argue that this measure leads to an unfair evaluation of transferability. In the context of adversarial examples, it is not just a matter of discovering data that is not well classified, but rather identifying the perturbation that can fool a classifier with minimal distortion. This prin-ciple should also apply to transferable attacks.
For illustration purposes, let us consider two models, one is robust in the sense that the necessary amount of adversar-ial perturbation is large, whereas the other model is weak.
If the attacker uses the robust model as the source to attack the weak target network, the ASR of the transferable attack will certainly be big. It does not mean that this is the right choice. The ASR is high because the robust source model needs large perturbation to be deluded, which will fool any weaker model. The ASR alone does not reflect the over-shooting in distortion. The converse, using the weak to at-tack the robust, would yield a low ASR. To summarize, the
ASR alone fails to capture how relevant the direction of the perturbation given by the source is for attacking the target.
The first contribution of this paper is to put distortion back into the picture. Section 3 evaluates transferability by comparing the distortion of a transferable attack to the ones of two reference attacks: On one hand, the strongest attack, i.e., the white box attack directly applied on the tar-get model; on the other hand, the weakest attack, i.e., the black box attack.
The second contribution shows the great variability of the performance of transferable attacks. Figure 1 summa-rizes this observation by plotting the ASR as a function of the distortion (the experimental protocol is explained in
Sect. 4.1). Naturally, the black box attack needs much more distortion than the white box attack. For instance, the white box attack yields an ASR of 50% with a distortion of 0.19, whereas the black box attack needs a distortion of 9.7. The surprise is that if the attacker resorts to a transferable at-tack and picks a source model at random, there is almost a 50% chance that the attack performs even worse than the black box attack. Section 4 outlines a triad of factors: input, source model, and attack.
This observation challenges the prevalent notion that adversarial examples transfer easily between models, and highlights the need to carefully choose the source model to attack a target. Under the assumption that the attacker has indeed several candidate models, our third contribu-tion, named FiT in Sect. 5, provides an affordable measure for model selection, allowing the attacker to choose a good source model with only a few queries to the target. 2.