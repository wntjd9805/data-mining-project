Abstract
Quantization is an effective approach for memory cost reduction by compressing networks to lower bits. However, existing quantization processes learned only from the current data tend to suffer from forgetting catastrophe on streaming data, i.e., significant performance decrement on old task data after being trained on new tasks. Therefore, we propose a lifelong quantization process, LifeQuant, to address the problem. We theoretically analyze the forgetting catastrophe from the shift of quantization search space with the change of data tasks. To overcome the forgetting catastrophe, we first minimize the space shift during quantization and propose
Proximal Quantization Space Search (ProxQ), for regular-izing the search space during quantization to be close to a pre-defined standard space. Afterward, we exploit replay data (a subset of old task data) for retraining in new tasks to alleviate the forgetting problem. However, the limited amount of replay data usually leads to biased quantization performance toward the new tasks. To address the imbalance issue, we design a Balanced Lifelong Learning (BaLL) Loss to reweight (to increase) the influence of replay data in new task learning, by leveraging the class distributions. Exper-imental results show that LifeQuant achieves outstanding accuracy performance with a low forgetting rate. 1.

Introduction
With increasing requirements for real-time inferences in computer vision tasks [1, 2, 3], neural networks deployed on edge devices have received increasing attention [4]. Due to the limited memory storage on edge devices, networks with a large volume of parameters are required to be compressed [5].
Accordingly, in addition to pruning [6, 7, 8, 9] and structure simplification [10, 11], quantization has been developed as an efficient learning technique to effectively compress networks to lower bits without a significant performance loss [12, 13, 14, 15, 16, 17].
Quantization can be categorized as Post-Training Quan-Figure 1: Overview of LifeQuant. In Fig. (a), Proximal
Quantization Space Search (ProxQ) is proposed to overcome the forgetting catastrophe problem by minimizing space shift, i.e., regularizing the search space to be close to a pre-defined standard space, during the quantization process. In Fig. (b), previous lifelong learning (LL) research employs replay data (old task data) in new task training to alleviate the forgetting problem. However, the limited amount of replay data inhibits the efficacy of overcoming the forgetting catastrophe. There-fore, Balanced Lifelong Learning (BaLL) loss is designed to reweight (to increase) the influence of replay data in new tasks, by carefully examining the class data distributions, to avoid significant forgetting catastrophe. tization (PTQ) and Quantization-Aware Training (QAT) according to the training process [18]. PTQ compresses the pretrained full-precision model weights and activations into low bits in a deterministic way without retraining and fine-tuning, which incurs only a tiny overhead in training
[12, 13, 19, 20, 21]. However, PTQ usually suffers from significant accuracy degradation since the quantization cri-teria are not trained with weights [18]. In contrast, QAT
learns model weights during quantization, i.e., the train-ing loss is able to be measured for the update of weights
[14, 22, 15, 16, 23]. Despite additional operations in train-ing, QAT generally achieves a better performance [18].
To the best of our knowledge, existing quantization ap-proaches in PTQ or QAT are designed to minimize the quan-tization error, i.e., the discrepancy between full-precision values and low-bit values, according to the training data only in the current storage. However, in real-world applications, data collected from the edge, such as Internet of Things (IoT) products, are streaming [24]. Owing to limited mem-ory storage on devices, it is infeasible to preserve all training data. In other words, the training is mainly performed on the new data while the old training data are eliminated, which induces the biased quantization result toward the new tasks and gradually forgets the prediction of the old tasks, i.e., the forgetting catastrophe problem. Nevertheless, it has not been investigated thoroughly. Therefore, in this paper, we make the first attempt to explore the forgetting problem in quanti-zation on streaming data. We first demonstrate that existing quantization processes suffer from a forgetting catastrophe on streaming data, i.e., significant performance deterioration on old task data after being quantized on new tasks.
To overcome the forgetting catastrophe, we design a life-long quantization process, LifeQuant, to robustly learn low-bit models on streaming data with a smaller forgetting rate, i.e., the accuracy degradation after learning the new tasks.
Fig. 1 illustrates the motivations of LifeQuant. Fig. 1 (a) first shows that the forgetting catastrophe mainly results from the shift of search space in quantization after learning the new task data. We theoretically analyze the increment of quantization error under the change in weights to evaluate the forgetting performance. To avoid the search space bi-ased by new tasks, we target space shift minimization and propose Proximal Quantization Space Search (ProxQ) to regularize the search space during quantization to be close to a pre-defined standard space (i.e., the gray dashed circle in
Fig. 1 (a)), by leveraging the statistics (mean and variance) of the weights. Accordingly, the space shift can be effec-tively reduced under the change of data tasks to overcome the forgetting catastrophe.
In addition to space shift minimization, Fig. 1 (b) shows that recent lifelong learning (LL) research alleviates the forgetting problem in full-precision network training by ap-plying replay data (training data in old tasks) to the new task learning [25]. However, in the quantization process, only a limited amount of old task data can be stored as replay data for memory efficiency. Fig. 1 (b) illustrates that a limited amount of replay data poses a challenge, imbalance issue
[26], where the quantization performance is inclined to be biased toward the new tasks due to the majority of the new task data. To alleviate the forgetting problem induced by the minor quantity of replay data, we design a Balanced
Lifelong Learning (BaLL) loss to reweight (to increase) the influence of replay data in new tasks, by leveraging the class data distributions.
In experiments, LifeQuant improves the state-of-the-art quantization approaches by a 7% accuracy increment and 8% forgetting rate reduction for 2-bit ResNet-20 on CIFAR-100, while by a 17% accuracy improvement and 23% forgetting rate reduction for 3-bit MobileNet-V2 on ImageCLEF.
Our contributions are summarized as follows: 1. We make the first attempt to develop a novel lifelong quantization process, LifeQuant, to overcome the for-getting catastrophe in quantization-aware training. 2. We theoretically analyze the forgetting problem caused by the search space shift with the change of data tasks.
Thus, we propose Proximal Quantization Space Search (ProxQ) to regularize the shift during quantization to avoid a significant accuracy loss in old tasks. 3. We study the limited quantity of replay data that induces the biased prediction result toward the new tasks and design a Balanced Lifelong Learning (BaLL) loss to reweight the influence of the replay data, to alleviate the forgetting problem. 4. Experimental results demonstrate that LifeQuant achieves significant accuracy enhancement and forget-ting rate reduction compared with the state-of-the-art quantization approaches. 2.