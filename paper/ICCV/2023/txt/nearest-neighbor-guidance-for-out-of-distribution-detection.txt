Abstract
Detecting out-of-distribution (OOD) samples are cru-cial for machine learning models deployed in open-world environments. Classifier-based scores are a standard ap-proach for OOD detection due to their fine-grained detec-tion capability. However, these scores often suffer from overconfidence issues, misclassifying OOD samples distant from the in-distribution region. To address this challenge, we propose a method called Nearest Neighbor Guidance (NNGuide) that guides the classifier-based score to respect the boundary geometry of the data manifold. NNGuide re-duces the overconfidence of OOD samples while preserv-ing the fine-grained capability of the classifier-based score.
We conduct extensive experiments on ImageNet OOD de-tection benchmarks under diverse settings, including a sce-nario where the ID data undergoes natural distribution shift. Our results demonstrate that NNGuide provides a significant performance improvement on the base detection scores, achieving state-of-the-art results on both AUROC,
FPR95, and AUPR metrics. 1.

Introduction
The open-world environment poses a challenge for clas-sification models as they may encounter input samples with unknown class labels, i.e., out-of-distribution (OOD) in-stances [44, 26, 42, 2, 4, 16]. The detection of such anoma-lous examples is crucial for preventing classifier malfunc-tions and potential harm. As a result, in safety-critical ap-plications like self-driving [29, 5, 40, 19] and biosynthe-sis [46, 37], the OOD detection task plays a critical role in ensuring the dependable deployment of machine learning models. Therefore, a significant body of research has been dedicated to OOD detection [45].
The standard approach for OOD detection is to derive a score function from the trained network, such that the in-distribution (ID) samples exhibit relatively higher scores than OOD. One major paradigm in designing the detection
∗Equal contribution. † Corresponding author: Andrew Beng Jin Teoh
Figure 1. (a) Out-of-distribution (OOD) instances can occur in any white region, including the small area between the in-distribution (ID) classes. For instance, given ’cat’ and ’dog’ as
ID classes, images of ’fox’ will be OOD instances near the ID data. (b) The classifier-based detector (i.e. confidence) assigns low scores on the small in-between area but suffers overconfidence is-sues. (c) Based on the distance information, KNN bounds the de-tection score on far-OOD regions. However, KNN lacks the fine-grained detection capability, and thus fails to detect the near-OOD. (d) NNGuide addresses both of these issues, reducing overconfi-dence in the far-OOD regions while achieving fine-grained detec-tion. score is to derive the score function based on the classi-fier’s output signals, known as ’confidence’. Examples of classifier-based scores include maximum softmax probabil-ity [15] and energy function [24]. A major advantage of the classifier-based detection scores is their ability to fully uti-lize the class-dependent information of ID data and provide fine-grained detection capability. However, the classifier-based scores may suffer from overconfidence in far OOD samples, limiting their effectiveness [10, 13].
In contrast, distance-based approaches (e.g. nearest neighbors [35] and Mahalanobis distance [22, 31]) detect
OOD instances based on their distance to the ID data in the feature space. These approaches can certify low scores for far OOD regions but may not fully utilize class-dependent information, resulting in limited fine-grained detection ca-pability.
Our work introduces Nearest Neighbor Guidance (NNGuide), a novel approach to improving classifier-based
OOD detection scores and mitigating the issue of overconfi-dence. NNGuide achieves this by guiding the classifier con-fidence of a test input based on its similarity to its nearest neighbors in the ID bank set. As a result, NNGuide reduces the detection score in far-OOD regions while maintaining fine-grained detection capabilities.
In the toy experiment shown in Fig. 1, the classifier-based score demonstrates its ability to assign low detec-tion scores to samples located in the small intermediate re-gion between the two ID classes, which is where near-OOD instances may occur. However, the score exhibits exces-sively high values in the outer region of the ID data. On the other hand, the distance-based score provided by KNN is effective at assigning low detection scores to far-OOD sam-ples, but it fails to credit low scores on the small interme-diate region, indicating a lack of fine-graininess. Our pro-posed detection score, NNGuide, mitigates the drawbacks of both methods; NNGuide assigns bounded low values for far-OOD samples while retaining fine-grained detection ca-pability.
We conduct an extensive evaluation of NNGuide in the large-scale ImageNet-1k benchmark [18] across a va-riety of deep classification networks, achieving state-of-the-art results. Furthermore, we investigate the robustness of NNGuide by testing it on the ImageNet-1k-V2 dataset
[36, 28], where the ID data undergoes natural distribu-tional shifts. The presence of distribution shifts can lead to misidentifying ID samples as OOD and hence represents a challenging, realistic scenario. The final part of our ex-periments involves an extensive ablation analysis, where we investigate the key contributing factors to the effectiveness of NNGuide, as well as its compatibility with a broad range of classifier-based scores.
Contributions The contributions of our work are summa-rized as follows:
• We propose a novel method called Nearest Neighbor
Guidance (NNGuide) that guides the classifier-based detection score to reduce overconfidence in far-OOD regions while retaining its fine-grained detection capa-bility.
• We attain state-of-the-art results on the ImageNet-1k
OOD detection benchmarks and demonstrate the ro-bustness of NNGuide by considering a challenging and realistic scenario where the ID ImageNet data under-goes a natural distributional shift.
• We provide an extensive and detailed ablation study, demonstrating the generality of NNGuide to a broad range of classifier-based scores.
We note that NNGuide is a post-hoc training-free inference method, and it is applicable to any standard deep classifica-tion networks. 2.