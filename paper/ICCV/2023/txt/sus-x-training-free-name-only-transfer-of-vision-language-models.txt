Abstract
Contrastive Language-Image Pre-training (CLIP) has emerged as a simple yet effective way to train large-scale vision-language models. CLIP demonstrates impressive zero-shot classification and retrieval performance on di-verse downstream tasks. However, to leverage its full po-tential, fine-tuning still appears to be necessary. Fine-tuning the entire CLIP model can be resource-intensive and unstable. Moreover, recent methods that aim to cir-cumvent this need for fine-tuning still require access to images from the target task distribution.
In this paper, we pursue a different approach and explore the regime of training-free “name-only transfer” in which the only knowl-edge we possess about the downstream task comprises the names of downstream target categories. We propose a novel method, SuS-X, consisting of two key building blocks—
“SuS” and “TIP-X”, that requires neither intensive fine-tuning nor costly labelled data. SuS-X achieves state-of-the-art (SoTA) zero-shot classification results on 19 bench-mark datasets. We further show the utility of TIP-X in the training-free few-shot setting, where we again achieve SoTA results over strong training-free baselines. Code is avail-able at https://github.com/vishaal27/SuS-X. 1.

Introduction
Vision-language pre-training has taken the machine learning community by storm. A broad range of vision-language models (VLMs) [57, 42, 73, 1, 37] exhibiting exceptional transfer on tasks like classification [80, 84], cross-modal retrieval [67, 2] and segmentation [63, 27] have emerged. These models are now the de facto standard for downstream task transfer in the field of computer vision.
One such prominent model, CLIP [57], is trained on 400M image-text pairs using a contrastive loss that max-imises the similarities of paired image-text samples. CLIP pioneered the notion of zero-shot transfer in the vision-language setting1: classification on unseen datasets. For a given classification task, CLIP converts the class labels
Figure 1: Training-free name-only transfer. We propose
SuS-X, a framework for enhancing the zero-shot transfer abilities of VLMs like CLIP [57], BLIP [42] and TCL [72], without training. To achieve this, we propose a novel method TIP-X, which adapts these VLMs using a curated support set (SuS) that is not drawn from the target distribu-tion. Our SuS leverages one key piece of information about the task at hand: the names of the target categories. into textual prompts (e.g. “A photo of a <CLASS>.”, where
<CLASS> represents the ground-truth text label for each class). It then computes similarities between all the class prompts and the query image, selecting the class with the highest image similarity as the predicted label (see Eq. (2)).
CLIP’s zero-shot performance is however limited by its pre-training distribution [24, 60, 21, 51].
If the down-stream dataset diverges significantly from the pretraining image distribution, CLIP’s zero-shot performance drasti-cally drops [21]. To mitigate this, several lines of work pro-pose to adapt CLIP on diverse downstream tasks—Tab. 1 briefly summarises these methods. Most of them employ fine-tuning on either labelled or unlabelled subsets of data from the target task. However, fine-tuning such an over-parameterised model can be unstable and lead to overfit-ting [15, 25]. Furthermore, having access to the true distri-bution of the target task can be prohibitive in data-scarce en-vironments [12, 4, 38] and online learning settings [14, 65].
To alleviate these issues, in this paper, we aim to adapt
CLIP and other VLMs for downstream classification in a name-only (requires only category names2, but no samples classification setup introduced by Lampert et al. [41] in which the task is to generalise to classes not seen during training. 1This idea of zero-shot transfer is distinct from the traditional zero-shot 2We use category and class interchangeably in this paper.
Table 1: Taxonomy of CLIP adaptation methods for downstream classification. We underline the Zero-Shot CLIP model to signify that it is the base model that all others build on top of. ∗This method considers access to all test-set samples simultaneously, hence we still consider it zero-shot. †This method additionally uses class hierarchy maps.
Method
LP-CLIP [57]
CoOp [84]
PLOT [11]
LASP [9]
SoftCPT [19]
VT-CLIP [79]
VPT [17]
ProDA [45]
CoCoOp [83]
CLIP-Adapter [25]
TIP-Adapter [80]
UPL [36]
SVL-Adapter [54]
TPT [48]
CLIP+SYN [33]
CaFo [78]
Zero-Shot CLIP [57]
CALIP [31]
CLIP+DN [85]∗
CuPL [56]
VisDesc [49]
CHiLS [53]†
SuS-X (ours)
Few-shot fine-tuning methods
Intermediate methods
Zero-shot methods
Training-free name-only transfer methods
Does not require Does not require training labelled data
Does not require target data distribution
✗
✗
✗
✗
✗
✗
✗
✗
✗
✗
✓
✗
✗
✗
✗
✗
✓
✓
✓
✓
✓
✓
✓
✗
✗
✗
✗
✗
✗
✗
✗
✗
✗
✗
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✗
✗
✗
✗
✗
✗
✗
✗
✗
✗
✗
✗
✗
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓ from the target task) and training-free fashion. We pro-pose SuS-X (see Fig. 1), consisting of two novel building blocks: (i) SuS (Support Sets), our dynamic support set curation strategy that forgoes the need for samples from the target task, and (ii) TIP-X, our main framework for performing zero-shot classification while being training-free. For a given downstream task, we first curate a sup-port set by leveraging the task category labels, either in a parametric manner i.e., generating images from large-scale text-to-image models (e.g., Stable Diffusion [59]) or non-parametric manner i.e., retrieving real-world images from a large vision-language data bank (e.g., LAION-5B [61]). We then use the curated support set as a proxy few-shot dataset to inform our downstream predictions using TIP-X, in a sim-ilar vein to recent few-shot adaptation methods [25, 80].
Our extensive experiments show that SuS-X outperforms zero-shot methods on 19 benchmark datasets across three
VLMs, namely, CLIP, BLIP and TCL by 4.60%, 5.97% and 11.37% absolute average accuracy respectively. We further extend the TIP-X framework to the few-shot regime, out-performing previous SoTA methods in the training-free do-main. Our main contributions are three-fold: (1) We pro-pose SuS-X, a SoTA method in the training-free name-only transfer setting for downstream adaptation of VLMs, (2)
We present SuS, an effective strategy for curating support sets using parametric or non-parametric methods to mitigate the lack of data samples available from the target task dis-tribution, and (3) We propose TIP-X, a novel training-free method for adapting VLMs to downstream classification in both the name-only transfer and few-shot regimes. 2.