Abstract
Backdoor defense, which aims to detect or mitigate the effect of malicious triggers introduced by attackers, is be-coming increasingly critical for machine learning security and integrity. Fine-tuning based on benign data is a nat-ural defense to erase the backdoor effect in a backdoored model. However, recent studies show that, given limited be-nign data, vanilla fine-tuning has poor defense performance.
In this work, we firstly investigate the vanilla fine-tuning process for backdoor mitigation from the neuron weight per-spective, and find that backdoor-related neurons are only slightly perturbed in the vanilla fine-tuning process, which explains its poor backdoor defense performance. To en-hance the fine-tuning based defense, inspired by the obser-vation that the backdoor-related neurons often have larger weight norms, we propose FT-SAM, a novel backdoor de-fense paradigm that aims to shrink the norms of backdoor-related neurons by incorporating sharpness-aware minimiza-tion with fine-tuning. We demonstrate the effectiveness of our method on several benchmark datasets and network architectures, where it achieves state-of-the-art defense per-formance, and provide extensive analysis to reveal the FT-SAM’s mechanism. Overall, our work provides a promising avenue for improving the robustness of machine learning models against backdoor attacks. Codes are available at https://github.com/SCLBD/BackdoorBench. 1.

Introduction
As deep neural networks (DNNs) have been increas-ingly applied to safety-critical tasks such as face recog-nition, autonomous driving, and medical image process-ing [16, 1, 28, 29, 33, 31, 44, 52, 30], the threat exhib-ited by DNNs has drawn attention from both the indus-trial and academic community. Recently, backdoor attacks
*Corresponds to Baoyuan Wu (wubaoyuan@cuhk.edu.cn).
Figure 1: Left: T-SNE [45] visualization on the backdoored model and the model after fine-tuning. FT fails to remove backdoor effect. Right: the neuron weight norm distribution between the two models. The weight seems to have remained mostly unchanged after the fine-tuning process.
[50, 15, 34, 14, 2] have emerged as a new practical and stealthy threat to DNNs, for which the attacker plant pre-defined triggers to a small portion of the dataset and misleads the DNNs trained on such dataset to behave normally with benign inputs while classifying input with trigger into the target class. To detect or mitigate the effect of backdoor, sub-stantial efforts have been done in inversing triggers, splitting dataset, or pruning the DNNs, while fine-tuning, a natural choice for backdoor defense has received much less atten-tion. Although complex techniques such as unlearning and pruning have achieved remarkable performance, they usually come at the cost of accuracy on the original tasks. Addi-tionally, the effectiveness of pruning is contingent upon the network structure, as highlighted by Wu et al. [51, 49], un-derscoring the necessity for meticulous pruning strategies.
In contrast, fine-tuning, a more general approach, can mod-erately restore the model’s utility.
Although vanilla fine-tuning has been adopted as a compo-nent of some backdoor defense methods [32, 27], fine-tuning a backdoored model to remove the backdoor is still challeng-ing when only limited benign data is given[49]. Previous work [49] has found that fine-tuning is a powerful technique in some situations. However, it cannot resist strong backdoor attacks such as Blended [7] and LF [55]. One of the possible
reasons is the backdoored model already fits the benign sam-ples well; hence, vanilla fine-tuning can only make minor changes to the weights of neurons and fail to mitigate the backdoor effect. To demonstrate this, we adopt the Blended
[7] attack with poisoning ratio 10% on CIFAR-10 dataset
[21] and PreAct-ResNet18 model [17], and the backdoored model is fine-tuned using 5% benign training samples. As shown in Figure 1, FT fails to mitigate backdoor effect and there’s only slight changes on neuron weight norms. In this paper, we focus on the problem of designing a new objec-tive function that can alter the backdoor-related weights and mitigate the backdoor effect via fine-tuning.
To address this problem, we first take a closer look at the fine-tuning process from neurons’ perspective. We empiri-cally observe that the weight norm of neurons has a positive correlation with backdoor-related neurons in our experiment, which is also implied in [57]. Intuitively, the neurons with large norms can cause the backdoor features to override the normal features, making the model incorrectly pay atten-tion to the trigger’s feature. Motivated by the relationship between the neuron weight norms and the backdoor effect, we propose to adopt Sharpness-Aware Minimization (SAM) with adaptive perturbations [11, 22] to fine-tune the back-doored model, which can revise the large outliers of weight norms and induce a more concentrated distribution of weight norms [24]. In detail, SAM considers a min-max formulation to encourage the weights in neighbors with a uniformly low loss. The adaptive constraints on perturbations can facilitate greater change of backdoor-related neurons. By leveraging
SAM on the backdoored model, we empirically show that the model not only benefits from escaping the current local minima but also receives more perturbations on backdoor-related neurons than the normal weights. Therefore, SAM implicitly facilitates the learning of backdoored neurons and helps to mitigate the backdoor effect.
To demonstrate the effectiveness of our method, we con-duct experiments on three benchmark datasets with two net-works, and compare them to seven state-of-the-art defense methods. The results show our method is competitive with and frequently superior to the best baselines. Our method is also robust across different components. Additionally, we empirically confirm that our strategy can take the place of fine-tuning, which can be used in conjunction with current backdoor defense techniques to make up for accuracy drop.
In summary, our main contributions are three-fold: (1) We reveal the reason of the weak backdoor defense performance of the vanilla fine-tuning based on a deep investigation from the perspective of backdoor-related neurons’ weight changes. (2) By leveraging SAM, we design an innovative fine-tuning paradigm to effectively remove the backdoor effect from a pre-trained backdoored model by perturbing the neurons. (3) Experimental results and analyses demonstrate that the proposed method can achieve state-of-the-art performance among existing defense methods and boost existing defense methods based on fine-tuning. 2.