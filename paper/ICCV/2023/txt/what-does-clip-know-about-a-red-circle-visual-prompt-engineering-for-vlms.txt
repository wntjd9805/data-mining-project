Abstract
Large-scale Vision-Language Models, such as CLIP, learn powerful image-text representations that have found numerous applications, from zero-shot classification to text-to-image generation. Despite that, their capabilities for solving novel discriminative tasks via prompting fall behind those of large language models, such as GPT-3. Here we ex-plore the idea of visual prompt engineering for solving com-puter vision tasks beyond classification by editing in image space instead of text. In particular, we discover an emer-gent ability of CLIP, where, by simply drawing a red cir-cle around an object, we can direct the model’s attention to that region, while also maintaining global information. We show the power of this simple approach by achieving state-of-the-art in zero-shot referring expressions comprehension and strong performance in keypoint localization tasks. Fi-nally, we draw attention to some potential ethical concerns of large language-vision models. 1.

Introduction
Large Language Models (LLMs) such as GPT-2/3 [7, 39] and ChatGPT [1] have demonstrated surprising emerging behaviours. For example, these models can perform lan-guage translation without being explicitly trained for it, in a zero-shot manner. This can be partially explained by the fact that occurrences of the desired behaviours, such as translating between two languages, naturally occur in their enormous training corpus, which is, essentially, the Internet.
Interesting emergent behaviours have been observed in large Vision-Language Models (VLMs) like CLIP [38] too.
For example, CLIP can be used for zero-shot classifica-tion by checking the compatibility of a given image with prompts such as “an image of a X”, where X is one of a set of class hypotheses to be tested.
Emergent behaviours are elicited by supplying suitably crafted inputs to the VLMs, often called prompts. As in the example above, researchers have mostly focused on engi-Figure 1: Visual Prompt Engineering. We draw multiple annotations over an image and have CLIP choose the correct one given a caption. Here we show predictions for the given expressions. Top: Examples from RefCOCOg on referring expressions detection. Bottom: Example from SPair71k on keypoint localization. neering textual prompts, manipulating the textual input of the model. This approach is inspired by LLMs, where ma-nipulating the textual modality is the only available option.
However, VLMs are inherently multimodal and offer the possibility of manipulating both modalities, textual and vi-sual. While the textual modality is the natural choice for expressing semantics, the visual modality can be better for expressing geometric properties such as location.
In this paper, we thus explore visual prompt engineer-ing 1. We do so with two goals. The first goal is to con-tribute one more practical tool for extracting useful infor-mation from VLMs in a zero-shot manner. We demon-strate this by obtaining state-of-the-art zero-shot results in referring expressions comprehension by engineering visual prompts. The second goal is to characterise interesting and 1Note the difference between visual prompt tuning, a setting previously explored, where the prompts are task-specific learnable tokens, and visual prompt engineering, where we apply a fixed augmentation in pixel space.
unexpected properties of the VLMs and their training data, including identifying some behaviours that can raise ethical concerns.
Perhaps the most surprising of our findings is the effec-tiveness of a particular type of visual prompting: drawing a plain red circle on top of the image (Fig. 1). We show that this simple intervention steers the VLM to analyse/talk about the image region contained in the circle. This be-haviour can then be used for tasks such as naming a spe-cific object or object part or detecting particular image re-gions based on a description. The latter, for instance, is achieved by marking each object proposal with a red circle and using the VLM to find the best match with respect to the provided referring expression, achieving strong results on multiple benchmarks in the unsupervised regime. Fur-thermore, we show that prompting with a circle also works for finer-grained localization, marking specific object parts or keypoints instead of just whole objects.
We further contrast marking an image with the alterna-tive of cropping it, which, from sliding window classifiers to region neural networks, is the canonical approach to steer the focus of an image-level predictor to a particular image region. We show that, for VLMs at least, marking is sig-nificantly more effective than cropping, possibly because it does not lose contextual information like the latter.
Apart from the practical applications, our findings reveal unexpected and intriguing properties of VLMs. We show empirically, that marking with a red circle is optimal among a selection of possible markers (variants of the circle, boxes, arrows, etc.). Presumably, the VLMs understand red circles out of the box because these appear sufficiently frequently in the training corpus, i.e., the Internet. While we do not have access to the full training data of CLIP, we corrobo-rate this intuition by seeking examples of such images in
YFCC15M, a dataset of CC-BY images.
Our analysis shows that red circles are indeed present even in a (comparatively small) dataset of images like
YFCC15M, but they are rare. It is a testament to the ex-traordinary capacity of VLMs that such a behaviour can be learned from such rare events, without an explicit focus on doing so. We test models of different sizes/capacities and show that only the larger models exhibit this behaviour re-liably, which corresponds to our intuition.
Finally, we note that the ability of VLMs to learn even from rare events such “red circles” can acquire both desir-able and undesirable behaviours. Red circles, in particular, can have a negative connotation in the training data as they are often used by news outlets to mark missing people or criminals and, evidently, the model learns from such exam-ples. As a result, we show that drawing a red circle in an image increases the probability that the model would char-acterise a person as a criminal or as a missing person.
To summarise, we make the following main contribu-tions: (1) We propose marking as a new form of visual prompt engineering that is effective in extracting useful emergent behaviours in VLMs like CLIP; (2) We use the latter to achieve state-of-the-art zero-shot referring expres-sions comprehension using a VLM; (3) We provide an anal-ysis of why marking is effective for these models, and link that to the training data and large model capacity; (4) We show that visual prompt engineering can also elicit un-wanted behaviours, such as triggering problematic biases in the VLMs, revealing potential ethical issues. 2.