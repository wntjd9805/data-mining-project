Abstract
We propose TextManiA, a text-driven manifold aug-mentation method that semantically enriches visual feature spaces, regardless of class distribution. TextManiA aug-ments visual data with intra-class semantic perturbation by exploiting easy-to-understand visually mimetic words, i.e., attributes. This work is built on an interesting hypothesis that general language models, e.g., BERT and GPT, encom-pass visual information to some extent, even without training on visual training data. Given the hypothesis, TextManiA transfers pre-trained text representation obtained from a well-established large language encoder to a target visual feature space being learned. Our extensive analysis hints that the language encoder indeed encompasses visual infor-mation at least useful to augment visual representation. Our experiments demonstrate that TextManiA is particularly powerful in scarce samples with class imbalance as well as even distribution. We also show compatibility with the label mix-based approaches in evenly distributed scarce data. 1.

Introduction
Learning models, e.g., neural networks, are known to perform well on visual recognition tasks when training and testing datasets present similar distributions [4]. How-ever, their performance often degrades considerably when evaluated in subtly different distributions [69]. One effec-tive way to enhance the generalization ability of a model against such data distribution shifts would be data augmenta-tion [18, 86, 85, 44, 41, 73]. Augmenting data enlarges the support of the training distribution formed by given samples and yields the effect of increasing the amount of data even without additional laborious data collection. By training on augmented data, decision boundaries are smoothed, and the generalization ability of the model is improved [73].
There has been a distinctive and successful line of re-search for label mix-based data augmentation, such as
Figure 1. Illustration of TextManiA. Our method augments the target visual feature by leveraging text embedding of the visu-ally mimetic words, which are comprehensible and semantically rich. For example, when the text of the existing class “bull” is manipulated as “red bull” by adding the attribute “red,” we can get augmented visual features by reflecting the difference of text em-beddings. In this way, TextManiA densifies sparse visual feature space using various attributes text.
Mixup [86], CutMix [85], and manifold Mixup [73], which are effective for model generalization and calibration [25].
The effectiveness of those label mix-based approaches is at-tributed to semantic perturbation by label mixing [86, 73, 85].
This is a distinctive property from other lines of data aug-mentation methods, e.g., [74, 44, 41, 64, 13], where they syn-thesize diverse virtual data that appear differently but retain class semantics of original contents. However, we found that the performance of mix-based augmentation methods is no-ticeably degraded when training with skewed class distribu-tion having scarce samples for non-major classes, i.e., long-tailed distribution. In real-world, data often exhibit long-tailed class distribution (e.g., Pareto distribution), which cannot be dealt with the prevalent mix-based approaches.
This motivates us to seek a semantically rich data augmenta-tion effective for limited data regimes, including long-tailed distribution, scarce data, and few-shot cases.
In this work, we propose TextManiA, a text-driven man-ifold augmentation for visual features, which is effective for long-tail classes and scarce data. Our TextManiA is based
on an interesting hypothesis that general language models, e.g., BERT [17] and GPT [55], have learned visual informa-tion to some extent that can be transferred to visual feature spaces even with no visual training data. With this hypoth-esis, we semantically enrich the target visual feature space to be trained by leveraging visually mimetic texts, encoded with general language models and transferred to the target space. Specifically, TextManiA encodes meaningful at-tributes such as “red” and “large” to vectors by computing the difference between text embeddings with and without attributes. We add the attribute embeddings to target visual features to mimic those attributes on the target visual fea-ture space. Figure 1 illustrates the augmentation process of
TextManiA. The input feature (e.g., the visual feature of
“bull”) is manipulated by adding the attribute vector induced by the attribute text (e.g., “red”), which yields the augmented visual feature (e.g., “red bull”). Thanks to the text modality properties, the augmentations generated by TextManiA are symbolic, human-interpretable, and easily controllable.
Our approach applies semantic perturbation on a different level to that of the label mix-based methods [86, 73, 85]. The mix-based methods augment a sample from a combination of two different class samples, i.e., applying semantic pertur-bation in an inter-class way. This further aggravates the class imbalance problem in the long-tailed (skewed) class distri-bution cases.1 Our TextManiA, whereas, perturbs data in an intra-class way. A sample per each class is selected, and we enrich the semantic granularity of the class using the sample, thus enabling us to better maintain the amount of augmentation balances in the long-tailed class distribu-tion cases. Moreover, TextManiA can densify around the training samples by extrapolating the class semantics along augmented semantic attribute axes. With this, our method can be combined with the label mix-based methods to further improve performance in evenly distributed sparse data cases because they are complementary.
To empirically support that our attribute vectors trans-formed from text embeddings are reasonably designed, we devise two visualization-based analyses: with t-SNE [72] and a latent inversion technique. These demonstrate that attribute vectors lead to visually interpretable manifold aug-mentation of input. We also evaluate our method with two different tasks in scarce data regimes: few-shot object detection and image classification with deficient datasets and long-tail datasets. Our experiments demonstrate that
TextManiA is an effective and model-agnostic data aug-mentation method, especially in scarce data cases, by ex-ploiting the favors of zero-shot attributes. Also, additional studies show the versatility and compatibility of the design of TextManiA. Our key contributions are summarized as:
• We propose TextManiA, which enriches the visual fea-tures by conveying attribute information from the text embedding to the target visual feature space.
• We validate our hypothesis of the existence of embedded visual knowledge in pre-trained language encoders despite no training on visual data.
• We demonstrate that TextManiA is especially helpful in augmenting sparse samples in long-tail class cases.
• We show that our TextManiA is complementary to other augmentation methods, and in particular, the combination of our TextManiA and manifold Mixup [73] noticeably improves the performance in deficient data cases. 2.