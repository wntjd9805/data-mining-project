Abstract
We propose a new method, called curvature similar-ity extractor (CSE), for improving local feature matching across images. CSE calculates the curvature of the local 3D surface patch for each detected feature point in a viewpoint-invariant manner via fitting quadrics to predicted monocu-lar depth maps. This curvature is then leveraged as an addi-tional signal in feature matching with off-the-shelf matchers like SuperGlue and LoFTR. Additionally, CSE enables end-to-end joint training by connecting the matcher and depth predictor networks. Our experiments demonstrate on large-scale real-world datasets that CSE consistently improves the accuracy of state-of-the-art methods. Fine-tuning the depth prediction network further enhances the accuracy.
The proposed approach achieves state-of-the-art results on the ScanNet dataset, showcasing the effectiveness of incor-porating 3D geometric information into feature matching.1 1.

Introduction
Local feature matching is a crucial component for many geometric computer vision tasks, including visual local-ization [50, 51, 52, 48, 14, 66], structure-from-motion (SfM) [67, 53, 29], and simultaneous localization and map-ping (SLAM) [38, 39, 10]. Given a pair of images ob-serving a 3D scene, the task is to find reliable tentative point-to-point correspondences in the two images. Form-ing such feature matches is often a challenging task as the images may undergo large viewpoint and illumination changes, have occlusions or repetitive patterns.
The standard pipeline for local feature matching typi-cally involves two steps: (1) keypoint detection and de-scription and (2) point-wise feature matching. Traditional approaches mainly focus on improving the robustness of
*Part of the work was done during the author’s visit to ETH zurich. 1Code and trained models are available at https://github.com/
AaltoVision/surface-curvature-estimator.
Figure 1. Curvature-guided match selection: (a) Correct matches in two views must share the same underlying surface cur-vature, e.g., predicted from monocular depth. (b) Correspondences from different surfaces are guaranteed to be incorrect. keypoint detection and description through, for example, extending the Harris [21] detector to handle affine transfor-mations and multiple scales [35, 36] or using more discrimi-native or efficient descriptors [32, 7, 34, 1, 3]. However, de-spite their unbroken popularity, these algorithms often fail to cope with the challenges that arise in real-world environ-ments, leading to low accuracy.
Recent advances in deep learning-based feature match-ing have made significant progress in addressing the lim-itations of hand-crafted approaches, such as by jointly training detectors and descriptors [69, 12, 15, 44, 63, 40] with convolutional neural networks (CNNs), or combin-ing hand-crafted and learning-based descriptors in a hy-brid manner [41, 6]. SuperGlue [49] introduced the use of transformer networks to learn the matching process and formulate the problem as an optimal transport task [64].
LoFTR [55] and its recent variants [58, 8, 65] leverage both the global and local context from raw images by jointly learning the feature extraction and matching in a single net-work. While these approaches have led to state-of-the-art performance on several benchmarks, they work entirely in the 2D image domain and ignore the underlying 3D ge-ometry of the scene. As feature matching is, essentially, finding the corresponding projections of an actual 3D point, this could be a critical limitation in real scenarios where the
matched pixels from different views must share the geome-try of the underlying surface patch, as shown in Fig. 1.
In this paper, we focus on investigating and exploiting 3D geometry cues, e.g. coming from monocular depth pre-dictions, for feature matching. The only prior work is [61], which adopts depth priors to find planar image regions that are then rectified to eliminate viewpoint changes. Fi-nally, a handcrafted detector and descriptor are applied to the rectified image regions. This, however, relies on the heavy assumption that the images consist mostly of domi-nant planes, which severely restricts its out-of-the-box ap-plicability. In this work, we go further by exploring the lo-cal surface geometry coming from depth priors. This addi-tional geometric clue is coupled with any off-the-shelf fea-ture matcher increasing its accuracy. Benefiting from ad-vanced monocular depth predictors [42, 43, 16], obtaining depth priors is easy and costs only a few milliseconds. We use these depth predictions to extract curvature at the ob-served feature points. Exploiting the fact that the curvature is invariant to viewpoint changes (i.e., rotation, translation, and scaling), we enforce that matched features must lie on similar surfaces. Our contributions are as follows: 1. We propose an approach for feature matching, lever-aging dense depth via utilizing local surface curvature similarity, which is invariant to viewpoint changes.
This approach is a departure from traditional matching methods that rely solely on image information. 2. The proposed curvature similarity extractor can be seamlessly integrated with any recent feature matcher, making it a versatile tool that can be easily adopted.
The experiments show that it improves several recent algorithms on various benchmarks by 1 − 3%. 3. The proposed algorithm can be used to train feature matchers and monocular depth prediction networks jointly in an end-to-end fashion. To demonstrate this, we fine-tune the state-of-the-art MiDaS [42] depth pre-dictor to increase the feature matching accuracy. 2.