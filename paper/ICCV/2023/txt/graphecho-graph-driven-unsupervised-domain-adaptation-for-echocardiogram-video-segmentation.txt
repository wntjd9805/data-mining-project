Abstract
Echocardiogram video segmentation plays an important role in cardiac disease diagnosis. This paper studies the unsupervised domain adaption (UDA) for echocardiogram video segmentation, where the goal is to generalize the model trained on the source domain to other unlabelled target domains. Existing UDA segmentation methods are not suitable for this task because they do not model lo-cal information and the cyclical consistency of heartbeat.
In this paper, we introduce a newly collected CardiacUDA dataset and a novel GraphEcho method for cardiac struc-ture segmentation. Our GraphEcho comprises two innova-tive modules, the Spatial-wise Cross-domain Graph Match-ing (SCGM) and the Temporal Cycle Consistency (TCC) module, which utilize prior knowledge of echocardiogram videos, i.e., consistent cardiac structure across patients and centers and the heartbeat cyclical consistency, respec-tively. These two modules can better align global and lo-cal features from source and target domains, leading to improved UDA segmentation results. Experimental results showed that our GraphEcho outperforms existing state-of-the-art UDA segmentation methods. Our collected dataset and code will be publicly released upon acceptance. This work will lay a new and solid cornerstone for cardiac structure segmentation from echocardiogram videos. Code and dataset are available at : https://github.com/xmed-lab/GraphEcho 1.

Introduction
Echocardiography is a non-invasive diagnostic tool that enables the observation of all the structures of the heart.
It can capture dynamic information on cardiac motion and function [33, 11, 20], making it a safe and cost-effective option for cardiac morphological and functional analysis.
Accurate segmentation of cardiac structure, such as Left
*Corresponding author
†Interning at The Hong Kong University of Science and Technology
Figure 1. Examples of nine frames from our newly collected
CardiacUDA dataset, which serves as a new domain adaptation benchmark for cardiac structure segmentation from echocardio-gram videos.
Ventricle (LV), Right Ventricle (RV), Left Atrium (LA), and
Right Atrium (RA), is crucial for determining essential car-diac functional parameters, such as ejection fraction and myocardial strain. These parameters can assist physicians in identifying heart diseases, planning treatments, and mon-itoring progress [12, 32]. Therefore, the development of an automated structure segmentation method for echocardio-gram videos is of great significance. Nonetheless, a model trained using data obtained from a specific medical institu-tion may not perform as effectively on data collected from other institutions. For example, when a model trained on site G is directly tested on site R, its performance can signif-icantly decrease to 48.5% Dice, which is significantly lower than the performance of a model trained directly on site
R, which achieves 81.3% Dice; see results of Without DA and Upper Bound in Table 2. The result indicates that there are clear domain gaps between echocardiogram videos col-lected on different sites; see (c-d) in Figure 2. Therefore, it is highly desirable to develop an unsupervised domain adaptation (UDA) method for cardiac structure segmenta-tion from echocardiogram videos.
To the best of our knowledge, the UDA segmentation for echocardiogram videos has not been explored yet, and the most intuitive way is to adapt existing UDA meth-ods designed for natural image segmentation and medi-(a) CAMUS (b) Echonet (c) Site G (d) Site R
Figure 2. (a-b) two public datasets; (c-d) our newly collected Car-diacUDA from two sites: G and R. Red, green, blue, orange and yellow refer to the segmentation contours for left ventricle (LV), left atrium (RA), right Atrium (RA), right ventricle (RV), and epi-cardium of left ventricle, respectively. Table 1 outlines the advan-tages of our dataset over CAMUS and Echonet.
Table 1. The comparison of CardiacUDA, CAMUS, and Echonet.
†: 5 frames are labelled for each video in the training set, and all frames are labelled for each video in the validation and test dataset.
Dataset
Video Num.
Frames Num.
Train/Test Labels
Multiple Centers
Cardiac Views
Resolution
Annotated Regions
Our CardiacUDA 992 102,796 5 frames / Full †
✓ 4 720p
LV, RV, LA, RA
CAMUS [22] 500 10,000 2 frames / 2 frames
× 1 480p
LV, LA
EchoNet [32] 10,030 1,755,250 2 frames / 2 frames
× 1 120p
LV
In general, existing cal image segmentation to our task. methods can be grouped into 1). the image-level align-ment methods [21, 29, 43, 37] that focus on aligning the style difference to minimize the domain gaps, such as
PLCA [21], PixMatch [29] and Fourier-base UDA [43, 37]; 2). feature-level alignment methods [19, 23, 24], such as [23], use global class-wise alignment to reduce the dis-crepancy between source and target domains. However, applying these methods directly to cardiac structure seg-mentation in echocardiogram videos generated unsatisfac-tory performance; see results in Table 2 and Figure 5. We thus consider two possible reasons: (1) Existing UDA meth-ods [23, 21, 29, 43, 37] primarily focused on aligning the global representations between the source and target do-main while neglecting local information, such as LV, RV,
LA, and RA; see (c-d) in Figure 2. The failure to model local information during adaptation leads to restricted car-diac structure segmentation results. (2) Most existing meth-ods [16, 23, 21, 29, 43, 37, 39, 44, 41] were mainly designed for 2D or 3D images, which does not consider the video sequences and the cyclic properties of the cardiac cycle in our task. Given that heartbeat is a periodically recurring process, it is essential to ensure that the extracted features exhibit cyclical consistency [7].
To address the above limitations, we present a novel graph-driven UDA method, namely GraphEcho, for echocardiogram video segmentation.
Our proposed
GraphEcho consists of two novel designs: (1) Spatial-wise
Cross-domain Graph Matching (SCGM) module and (2)
Temporal Cycle Consistency (TCC) module. SCGM is mo-tivated by the fact that the structure/positions of the differ-ent cardiac structures are similar across different patients and domains. For example, the left ventricle’s appear-ance is typically visually alike across different patients; see red contours in Figure 2. Our SCGM approach reframes domain alignment as a fine-grained graph-matching pro-cess that aligns both class-specific representations (local in-formation) and the relationships between different classes (global information). By doing so, we can simultaneously improve intra-class coherence and inter-class distinctive-ness.
Our TCC module is inspired by the observation the recorded echocardiogram videos exhibit cyclical consis-tency; see examples in Figure 1. Specifically, our TCC module utilizes a series of recursive graph convolutional cells to model the temporal relationships between graphs across frames, generating a global temporal graph represen-tation for each patient. We then utilized a contrastive ob-jective that brings together representations from the same video while pushing away those from different videos, thereby enhancing temporal discrimination. By integrating
SCGM and TCC, our proposed method can leverage prior knowledge in echocardiogram videos to enhance inter-class differences and intra-class similarities across source and tar-get domains while preserving temporal cyclical consistency, leading to a better UDA segmentation result.
In addition, we collect a new dataset, called Car-diacUDA from two clinical centers. As shown in Ta-ble 1, compared to existing publicly available echocardio-gram video datasets [22, 32], our new dataset has higher resolutions, greater numbers of annotations, more annotated structure types as well as more scanning views. Our contri-bution can be summarized as follows:
• We will publicly release a newly collected echocardio-gram video dataset, which can serve as a new bench-mark dataset for video-based cardiac structure seg-mentation.
• We propose GraphEcho for cardiac structure segmen-tation, which incorporates a novel SCGM module and a novel TCC module that are motivated by prior knowledge. These modules effectively enhance both inter-class differences and intra-class similarities while preserving temporal cyclical consistency, resulting in superior UDA results.
• GraphEcho achieved superior performance compared to state-of-the-art UDA methods in both the computer vision and medical image analysis domains. 2.