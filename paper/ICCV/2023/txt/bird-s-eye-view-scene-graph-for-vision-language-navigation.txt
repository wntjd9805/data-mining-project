Abstract
Instruction: Go to the dining room by front door and push in the chair furthest from the front door.
Instruction: Go to the dining room by front door and push in the chair furthest from the front door.
Vision-language navigation (VLN), which entails an agent to navigate 3D environments following human in-structions, has shown great advances. However, current agents are built upon panoramic observations, which hin-ders their ability to perceive 3D scene geometry and easily leads to ambiguous selection of panoramic view. To address these limitations, we present a BEV Scene Graph (BSG), which leverages multi-step BEV representations to encode scene layouts and geometric cues of indoor environment un-der the supervision of 3D detection. During navigation,
BSG builds a local BEV representation at each step and maintains a BEV-based global scene map, which stores and organizes all the online collected local BEV representations according to their topological relations. Based on BSG, the agent predicts a local BEV grid-level decision score and a global graph-level decision score, combined with a sub-view selection score on panoramic views, for more accu-rate action prediction. Our approach signiﬁcantly outper-forms state-of-the-art methods on REVERIE, R2R, and R4R, showing the potential of BEV perception in VLN. 1.

Introduction
Vision-language navigation (VLN) task [1] requires an agent to navigate through a 3D environment [2] to a target location, according to natural language instructions. Ex-isting work has made great advances in cross-modal rea-soning [3–8], path planning [9–13], and auxiliary tasks for pretraining [14–18]. Their core ideas are learning to relate the language instructions to panoramic images of the envi-ronment. Though straightforward, these approaches heav-ily rely on 2D panoramic observations. As a result, they lack the capacity to preserve scene layouts and 3D structure, which are critical for navigation decision-making in embod-ied scenes. Moreover, indoor environments [2, 19–21] are characterized by substantial occlusion [22–24], posing chal-lenges for the agent to accurately identify the objects and landmarks referenced by the instructions [1, 25].
*Corresponding author: Wenguan Wang. (cid:1005) (cid:1006) (cid:1007) (cid:1008) i ( ) P (a) Previous Methods with Panoramic Decision Space.
BEV grid current node candidate node
S i (cid:1005) (cid:1007) (cid:1007) (cid:1007)(cid:1007)(cid:1007)(cid:1007)(cid:1007)(cid:1007)(cid:1007)(cid:1007)(cid:1007)(cid:1007)(cid:1007)(cid:1007)(cid:1007)(cid:1007)(cid:1007)(cid:1007)(cid:1007)(cid:1007)(cid:1007)(cid:1007)(cid:1007)(cid:1007)(cid:1007)(cid:1007) (cid:1007) (cid:1008) (cid:1008) (cid:1008)(cid:1008)(cid:1008)(cid:1008)(cid:1008)(cid:1008)(cid:1008)(cid:1008)(cid:1008)(cid:1008)(cid:1008)(cid:1008)(cid:1008)(cid:1008)(cid:1008)(cid:1008)(cid:1008)(cid:1008)(cid:1008)(cid:1008)(cid:1008)(cid:1008)(cid:1008)(cid:1008)(cid:1008)(cid:1008) (cid:1008) (cid:1006) (cid:1006) (cid:1006) (cid:1005) (cid:1005) (cid:1005)(cid:1005)(cid:1005)(cid:1005)(cid:1005)(cid:1005)(cid:1005)(cid:1005)(cid:1005)(cid:1005)(cid:1005)(cid:1005)(cid:1005)(cid:1005)(cid:1005)(cid:1005)(cid:1005)(cid:1005)(cid:1005)(cid:1005)(cid:1005)(cid:1005)(cid:1005)(cid:1005)(cid:1005) (cid:1005) (cid:1007) (cid:1005) (cid:1006) (cid:1008) (b) Our Method with BEV Decision Space.
Figure 1: For panoramic view (a), two candidate nodes ( (cid:1005) & (cid:1006) ) correspond to the same image leading to ambiguity. For Bird’s-Eye-View (b), they are represented by discriminative grids (
).
For example (Fig. 1(a)), given the instruction “Go to the dining room by front door and push in the chair furthest from the front door”, previous approaches [3–5, 14, 15, 17, 26–30] formulate VLN as a sequential text-to-image grounding problem by matching navigable candidate nodes with adjacent panoramic views. At each time step, given a set of subviews captured from different directions, the agent selects a navigable direction as the next step for navigation.
However, this strategy tends to introduce ambiguity, when the agent needs to discriminate between multiple candidate nodes corresponding to the same subview. In addition, the agent struggles to ground the associated objects and explore their spatial relation in 3D scene, such as identifying “the chair furthest from the front door”. Consequently, relying solely on panoramic view presents difﬁculties in both com-prehensive scene perception and efﬁcient navigation.
To address the challenges encountered by panoramic methods, Bird’s-Eye-View (BEV) perception emerges as a viable solution, employing discriminative grid representa-tions to model the 3D environment. Meanwhile, BEV grid representation effectively captures spatial context and scene layouts [31, 32], facilitating both perception [33–36] and planning [37–40]. Building upon these insights, we present a BEV Scene Graph (BSG), which harnesses the power of
BEV representation to construct an informative navigation graph. During navigation, the agent collects local BEV rep-resentations at each navigable node. A global scene graph is established by connecting these BEV representations topo-logically. At each step, the agent makes an informed de-cision by predicting a BEV grid-level decision score and a
BSG graph-level decision score, combined with a subview selection score on panoramic views [13, 28, 41].
Speciﬁcally, the agent acquires multi-view observations at each step and performs view transformation [35, 42–44] on the corresponding image features. Later, a 3D detection head [44–46] is employed on these BEV representations to predict oriented bounding boxes, encoding object-level ge-ometric and semantic information. During navigation, the node embeddings of BSG are represented by neighboring
BEV grids. Then they are updated by querying the overlap region between BEV representations from different steps.
Previous semantic maps in robot navigation, includ-ing occupancy grids [47–50] and learnable spatio-semantic representations [51–55], have only provided top-down in-formation without crucial 3D object information. Differ-ently, BSG leverages the BEV representations to achieve consistency between 3D perception and decision-making while encoding geometric context. Our approach is eval-uated on three benchmarks (i.e., REVERIE [25], R2R [1],
R4R [56]).
For the referring expression comprehen-sion in REVERIE, BSG outperforms the state-of-the-art method [28] by 5.14% and 3.21% in SR and RGS on the val unseen split, respectively. BSG also achieves 4% and 3% improvement in SR and SPL on the test split of
R2R, respectively. The impressive results shed light on the promises of BEV perception in VLN task. 2.