Abstract 1.

Introduction
The task of synthesizing novel views from a single im-age has useful applications in virtual reality and mobile computing, and a number of approaches to the problem have been proposed in recent years. A Multiplane Image (MPI) estimates the scene as a stack of RGBA layers, and can model complex appearance effects, anti-alias depth er-rors and synthesize soft edges better than methods that use textured meshes or layered depth images. And unlike neu-ral radiance fields, an MPI can be efficiently rendered on graphics hardware. However, MPIs are highly redundant and require a large number of depth layers to achieve plau-sible results. Based on the observation that the depth com-plexity in local image regions is lower than that over the entire image, we split an MPI into many small, tiled re-gions, each with only a few depth planes. We call this repre-sentation a Tiled Multiplane Image (TMPI). We propose a method for generating a TMPI with adaptive depth planes for single-view 3D photography in the wild. Our synthe-sized results are comparable to state-of-the-art single-view
MPI methods while having lower computational overhead.
The novel view synthesis (NVS) problem involves using a set of input images to generate views from new and un-seen camera positions, allowing three dimensional interac-tion with photos. This is a long-studied problem, with early work relying on interpolation within dense structured image sets [20, 11, 8]. The specialized rigs commonly required to capture the large number of images restricted these meth-ods to lab settings [46]. However, the potential applications offered by novel view synthesis on modern mobile and VR devices has kindled wide interest in the problem, and en-couraged researchers to seek methods that make the tech-nology more accessible. The term 3D photography refers to the use of novel view synthesis in everyday capture set-tings, often from a single image.
Over the past few years a number of proposed scene rep-resentations have leveraged the great strides being made in learning-based techniques to achieve more accurate synthe-sis with fewer constraints. The most recent of these are neu-ral radiance fields (NeRFs)
[26, 50] which represent the scene as multi-layer perceptrons. Their results define the high bar of novel view synthesis. However, this high qual-ity has a significant data and computational cost.
An alternate representation, a multiplane image (MPI), defines the scene as a stack of fronto-parallel RGBA planes that can be warped and rendered into novel viewpoints [53, 26, 9]. An MPI offers the advantage of rendering speed, and suffers from less aliasing than mesh or point-based meth-ods [38, 47, 29]. The latter characteristic is important for applications that require temporal stability. However, an
MPI is a highly redundant scene representation: the num-ber of RGBA planes required to capture and reconstruct all the depth variation in a scene can be quite high. Since most scenes have a larger amount of free space than occupied, most of the planes in an MPI are very sparse. This makes them inefficient as a representation [2], and expensive to generate, transmit, and store.
In this paper we propose to address these shortcomings of multiplane images and develop a lightweight solution to the 3D photography problem that can be practically imple-mented on mobile and VR devices. Some examples of the applications we envision are 3D video conferencing, telep-resence, and VR passthrough [49]. We show how subdi-viding the image plane into many small MPIs with only a few planes in each, provides a more efficient representation from a computational and memory perspective. However, the naive approach of directly using existing MPI methods with tiles creates boundary artifacts in the novel views. This happens because the commonly used fixed spacing of MPI planes fails to capture the full depth range of a tile when the number of planes is small. Furthermore, it is sensitive to outliers in small regions. We propose a clustering-based approach using learnt confidence weights to predict per-tile
MPI planes that better represents local depth features. Our method is lightweight and generates results comparable to the state-of-the-art in MPI-based 3D photography.
In summary, the main contribution of this work are, 1. The demonstration of tiled multiplane images as a practical representation for view synthesis tasks. 2. A learning framework for generating tiled multiplane images from a single RGB input for 3D photography. 3. A novel approach to adaptive MPI plane positioning. 2.