Abstract 1.

Introduction
Recent advances in implicit neural representations make it possible to generate free-viewpoint videos of the human from sparse view images. To avoid the expensive training for each person, previous methods adopt the generalizable human model and demonstrate impressive results. How-ever, these methods usually rely on limited multi-view im-ages typically collected in the studio or commercial high-quality 3D scans for training, which heavily prohibits their generalization capability for in-the-wild images. To solve this problem, we propose a new approach to learn a gen-eralizable human model from a new source of data, i.e.,
Internet videos. These videos capture various human ap-pearances and poses and record the performers from abun-dant viewpoints. To exploit the Internet data, we present a video self-supervised pipeline to enforce the local appear-ance consistency of each body part over different frames of the same video. Once learned, the human model enables realistic novel view synthesis from a single input image. Ex-periments show that our method can generate high-quality view synthesis on in-the-wild images while only training on monocular videos.
†Corresponding author: Sida Peng.
Generating free-viewpoint videos of a human performer is a core technology in various applications such as AR/VR, telepresence, and gaming. While traditional methods have demonstrated impressive results in free-viewpoint render-ing, they typically rely on hundreds of calibrated and syn-chronized cameras [6, 12] or multiple RGBD sensors [8], which makes them impractical to create free-viewpoint videos for general users.
To make free-viewpoint video creation more accessible, many approaches propose to reconstruct the human model from sparse view or even single view RGB inputs. Given sparse view videos as input (e.g. four views), recent works
[37, 36] have achieved photo-realistic novel view synthe-sis based on the neural radiance ﬁeld (NeRF) [32] in a per-scene optimization setting. To avoid the expensive per-scene optimization, some works [25, 53, 5] propose gener-alizable radiance ﬁelds conditioned on extracted image fea-tures and volumetric features. However, the training of all these methods requires multi-view images usually collected in the studio, which makes them have difﬁculty in general-izing to in-the-wild images. Instead of using multi-view im-ages, some works [39, 54] achieve remarkable single-image human geometry and appearance reconstruction by super-vising the model with commercial high-quality 3D scans.
Nonetheless, the limited amount of 3D scans and the do-main gap between synthetic and real images make these methods struggle when applied to real-world images. Thus, the key challenge here lies in the generalization ability of the human model.
In this paper, to address this challenge, we follow the single-image reconstruction setting and propose a novel ap-proach to learn a generalizable human model from monocu-lar Internet videos, which supports realistic novel view syn-thesis. The key observations are as follows: 1) there are lots of human videos on the Internet which contain diverse appearances and actions and abundant viewpoints; 2) due to the articulated structure of the human body, the local appearance of each body part of the same person approx-imately remains constant. These observations make it pos-sible to learn the human model from Internet videos based on the appearance consistency over different frames of the same video.
To address this new problem, we propose a self-supervised pipeline to learn a generalizable human radiance
ﬁeld from monocular videos. Speciﬁcally, we randomly se-lect two frames from the same video called the source frame and paired frame separately. Based on the source frame, we
ﬁrst extract the image feature and utilize the inpainted neu-ral feature map of the parametric human mesh [38] to con-struct the volumetric feature. Then, these features of sam-ple points are taken as the input of the corresponding human radiance ﬁeld, which enables rendering novel view images.
To supervise the radiance ﬁeld, we simultaneously leverage the source frame and paired frame for training. The sam-ple points of the paired frame can be transformed into the source frame by linear blend skinning and their density and color can be decoded from the radiance ﬁeld of the source frame. In addition to reconstruction losses, we further intro-duce the adversarial loss to encourage the rendering more realistic. Finally, a new dataset consisting of hundreds of
Internet videos is created for training.
In summary, this work has the following contributions:
• We introduce a novel approach of novel view synthe-sis from a single human image, which only utilizes monocular videos for training rather than multi-view images or high-quality 3D scans. A self-supervised pipeline is proposed to learn the human model from monocular inputs.
• We provide a new dataset that consists of more than 600 monocular videos from the Internet totaling more than 120K images, which contain various human and camera viewpoints. For each image, the human mask and SMPL+H parameters are provided.
• We demonstrate that, while only training on monoc-ular videos, our method generates high-quality view synthesis on real-world images. 2.