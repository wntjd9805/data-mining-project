Abstract
Previous raw image-based low-light image enhancement methods predominantly relied on feed-forward neural net-works to learn deterministic mappings from low-light to normally-exposed images. However, they failed to capture critical distribution information, leading to visually unde-sirable results. This work addresses the issue by seam-lessly integrating a diffusion model with a physics-based exposure model. Different from a vanilla diffusion model that has to perform Gaussian denoising, with the injected physics-based exposure model, our restoration process can directly start from a noisy image instead of pure noise.
As such, our method obtains significantly improved perfor-mance and reduced inference time compared with vanilla diffusion models. To make full use of the advantages of dif-ferent intermediate steps, we further propose an adaptive residual layer that effectively screens out the side-effect in the iterative refinement when the intermediate results have been already well-exposed. The proposed framework can work with both real-paired datasets, SOTA noise models, and different backbone networks. We evaluate the proposed method on various public benchmarks, achieving promising results with consistent improvements using different expo-sure models and backbones. Besides, the proposed method achieves better generalization capacity for unseen ampli-fying ratios and better performance than a larger feedfor-ward neural model when few parameters are adopted. The code is released at https://github.com/wyf0912/
ExposureDiffusion. 1.

Introduction
Over the past few years, learning-based methods for low-light image enhancement [22, 21, 17] have gained signifi-cant attention and made remarkable progress, and most of them are conducted in the sRGB space. Recently, the en-hancement in the raw space is demonstrated to have unique
*Corresponding author.
Figure 1: We propose to simulate the physics-based expo-sure process using a shared neural network FΘ in a progres-sive manner. The learned exposure process is optimized to approximate the physics-based exposure process by mini-mizing the derived variational upper bound of the KL diver-gence of their distributions. Besides, the proposed strategy can be applied on real-captured paired data (blue array) and synthetic data with different noise models (orange array), and in different backbone networks. Benefiting from learn-ing a continuous exposure process, the proposed method can be applied to work with an arbitrary amplifying fac-tor, and better performance can be achieved by the iterative refinement process. advantages over sRGB spaces [14]. For example, raw im-ages provide a higher dynamic range, leading to better per-formance in extremely dark environments. Besides, the lin-ear correlation between the low-light and normally-exposed images prevents improper exposure level adjustment in the enhancement process.
In addition, the noise modeling in the raw space is more straightforward than that in the sRGB space by ruling out the effect of increasingly complicated image signal processing pipelines. In such space, the do-main gap between synthetic and captured data is small and the model trained with paired synthetic images ex-hibits comparable or even better performance than that of real-captured data [45, 6]. While promising progress is achieved, the prevailing approach remains to learn a de-terministic mapping based on feedforward neural networks.
For the images captured in extremely dark environments, this one-step enhancement/denoising process1 fails to char-acterize the distribution information and usually obtain un-desirable results. For example, there may still exist some residual noise. Besides, existing works mainly pay attention to more accurate noise modeling. The work of effectively incorporating the noise model in the raw space into a learn-able model for improved enhancement remains unexplored.
Most recently, generative model-based image restora-tion methods [25, 41, 34] exhibit appealing performance and pleasing perceptual quality in image restoration tasks.
Among these generative models, diffusion models [36, 13] stand out for their capacity to model a complicated distribution with arbitrary neural networks in a progres-sive manner and exhibit great success in image generation and restoration tasks [34]. Different types of forward pro-cesses have been explored in diffusion models, e.g., the unified framework for the Wiener process [49]. Nonethe-less, they are inadequate to accurately simulate real expo-sure processes. First, the low-light image is naturally not an intermediate step of the vanilla diffusion process. There-fore, the reverse (denoising) process needs to start from pure noise and involves a relatively large number of inference steps, which hinders the real applications. Second, since the vanilla diffusion models need to have the capacity of re-moving Gaussian noise with different noise levels, it usually requires extra model capacity compared with feedforward neural networks.
To address aforementioned issues, we propose a novel approach to effectively inject noise models in raw space into an end-to-end learnable progressive model, named Expo-sureDiffusion. Specifically, we propose to simulate the ex-posure process using a progressive shared network to min-imize the divergence between the simulated process and the real one by optimizing the proposed variational up-per bound. Since the intermediate steps of the progressive process all obey the physics-based noise distribution, the restoration process can directly start from a noisy image in-stead of pure noise. This design significantly benefits the low-light enhancement/denoising in two dimensions. First, the proposed method no longer requires removing Gaussian noises and only needs to learn the process of real-noise de-noising, leading to smaller requirements of model capacity.
Second, the proposed method greatly reduces the required number of inference steps, which has the potential to sig-nificantly benefit real applications. Besides, we further pro-1As exposure changes can be approximated with a linear transform in the raw space, low-light image enhancement in the raw image space is regarded as a denoising task in most previous works. pose an adaptive residual layer to dynamically fuse differ-ent denoising strategies for the areas with different noise-to-signal ratios. This strategy effectively screens out the side-effect in the iterative refinement when the intermedi-ate results have been already well-exposed. The proposed method can be applied to both paired real-captured data, synthetic data with different noise models, and different backbone networks. Experimental results demonstrate that the proposed method can achieve significant improvement jointly with both real/synthetic exposure process and back-bone networks. The proposed method, which employs the noisy-to-fine strategy, also exhibits superior generalization capability. Our main contributions are summarized as fol-lows:
• We propose the first diffusion-based model for low-light image enhancement in the raw image space. The modeling of the process is inspired and constructed strictly according to the physical noise model. This design enables restoration from any intermediate step of the diffusion process and eliminates the need for the Gaussian denoising process. As a result, the avail-able model capacity and inference efficiency are sig-nificantly improved.
• We further propose an adaptive residual layer to dy-namically adopt different denoising strategies for ar-eas with different noise-to-signal ratios. This strategy effectively screens out the side-effect in the iterative refinement when the intermediate results have been al-ready well-exposed.
• Extensive experimental results on two public datasets demonstrate the significant performance improvement of the proposed method combined with state-of-the-art noise models/backbones. Besides, the proposed method exhibits better generalization capacity com-pared with feedforward neural networks and possesses fewer parameters and faster speed to achieve competi-tive performance. 2.