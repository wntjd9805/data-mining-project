Abstract
Recently, the pure camera-based Bird’s-Eye-View (BEV) perception provides a feasible solution for economical au-tonomous driving. However, the existing BEV-based multi-view 3D detectors generally transform all image features into BEV features, without considering the problem that the large proportion of background information may sub-merge the object information.
In this paper, we propose
Semantic-Aware BEV Pooling (SA-BEVPool), which can fil-ter out background information according to the semantic segmentation of image features and transform image fea-tures into semantic-aware BEV features. Accordingly, we propose BEV-Paste, an effective data augmentation strat-egy that closely matches with semantic-aware BEV feature.
In addition, we design a Multi-Scale Cross-Task (MSCT) head, which combines task-specific and cross-task informa-tion to predict depth distribution and semantic segmentation more accurately, further improving the quality of semantic-aware BEV feature. Finally, we integrate the above mod-ules into a novel multi-view 3D object detection framework, namely SA-BEV. Experiments on nuScenes show that SA-BEV achieves state-of-the-art performance. Code has been available at https://github.com/mengtan00/SA-BEV.git. 1.

Introduction
Camera and LiDAR are the two most commonly used sensors for 3D object detection, which is essential to au-tonomous driving systems. LiDAR-based methods [4, 12, 33, 39, 34, 35, 42] have attained excellent performance due to the accurate spatial structure information of point clouds, but the expensive LiDAR sensor reduces its universality. In contrast, camera-based methods [30, 29, 31, 18, 19] are rel-atively low-cost with plentiful semantic information, but are constrained by the lack of geometric depth cues.
*indicates the corresponding author.
Original Frame
Pasted Frame
After-pasting Frame
Figure 1: Comparison between normal BEV features (up-per row) and semantic-aware BEV features (lower row).
The brightness reveals the norm of the features and the red
/ green boxes are the ground truth of the original / pasted frame. The last column shows BEV-Paste, an data augmen-tation strategy that matches semantic-aware BEV features.
Considering the performance gap between camera and
LiDAR, the Bird’s-Eye-View paradigm transforms multi-view image features into the BEV feature to make the fol-lowing 3D perception easier [22, 10]. This practical and scalable camera-only paradigm is gaining popularity, and numerous advancements have allowed it to reach high per-ceptual precision [15, 14, 16, 8, 11]. The core step of the
BEV paradigm is generating virtual points from image fea-tures, which will be projected into the “pillarized” BEV space. The features of the virtual points in the same pil-lar are then cumulated as the BEV feature. However, this operation does not fully utilize the semantic information of the image features and will inject massive background in-formation that submerges object information.
In order to take full advantage of the valuable semantic
information of image features, we propose Semantic-Aware
BEV Pooling (SA-BEVPool) to generate semantic-aware
BEV features, which replace the normal BEV feature for 3D detection. Before projecting virtual points into BEV space, the semantic segmentation of image features is first predicted. If a virtual point is generated by the image el-ement that belongs to the background, it will not be pro-jected into BEV space. Similarly, virtual points with low depth scores will also be ignored. The comparison between normal BEV features and semantic-aware BEV features is shown in Fig. 1. SA-BEVPool can obviously filter out most of the background BEV features and alleviate the problem that the large proportion of background information sub-merges object information, therefore effectively improving the detection performance. Some multi-modal 3D objec-tors [27, 36] also adopt segmentation on images when com-bining with LiDAR features, but they generally use power-ful instance segmentation networks like CenterNet2 [43] to predict the segmentation of the large-scale image. Instead,
SA-BEVPool can be easily applied in current BEV-based detectors like BEVDepth [15] and BEVStereo [14] by us-ing their depth branch to simultaneously predict the seman-tic segmentation of small-scale image features.
GT-Paste [33] is a successful data augmentation strategy that has been frequently adopted by various LiDAR-based 3D detectors. However, due to the modality gap, it cannot directly adapt to camera-based 3D detectors. In our work, thanks to the reliable depth distribution and semantic seg-mentation predicated on image features, the semantic-aware
BEV feature can approximately represent the information of all objects that are located appropriately in BEV space.
As a result, adding the semantic-aware BEV features of an-other frame to the current semantic-aware BEV feature is the same as pasting all objects of another frame into the cur-rent frame. This strategy, we called BEV-Paste, enhances data diversity in a similar way to GT-Paste.
Although it is convenient to predict depth distribution and semantic segmentation with the same branch, doing so may result in a sub-optimal semantic-aware BEV fea-ture. Research conclusion in the field of multi-task learning demonstrates that the integration of specific tasks and cross-task information is more conducive to the optimal solution of multiple prediction tasks. Inspired by this, we design a
Multi-Scale Cross-Task (MSCT) head to combine the task-specific and cross-task information through multi-task dis-tillation and dual-supervision on multiple scales prediction.
We integrate our proposed modules as a whole and name it SA-BEV. Extensive experiments on nuScenes dataset show that SA-BEV achieves a new state-of-the-art. In sum-mary, the major contributions of this paper are:
• We propose SA-BEVPool, which uses semantic infor-mation to filter out unnecessary virtual points and gen-erate the semantic-aware BEV feature, alleviating the problem that the large proportion of background infor-mation submerges the object information.
• We propose BEV-Paste, an effective and conve-nient data augmentation strategy closely matching the semantic-aware BEV feature, which enhances data di-versity and further promotes detection performance.
• We propose the MSCT head that combines the task-specific and cross-task information through multi-task learning on multiple scales, facilitating the optimiza-tion of the semantic-aware BEV feature. 2.