Abstract
In recent years, significant progress has been made in video instance segmentation (VIS), with many offline and online methods achieving state-of-the-art performance.
While offline methods have the advantage of producing tem-porally consistent predictions, they are not suitable for real-time scenarios. Conversely, online methods are more prac-tical, but maintaining temporal consistency remains a chal-In this paper, we propose a novel online lenging task. method for video instance segmentation, called TCOVIS, which fully exploits the temporal information in a video clip.
The core of our method consists of a global instance as-signment strategy and a spatio-temporal enhancement mod-ule, which improve the temporal consistency of the fea-tures from two aspects. Specifically, we perform global op-timal matching between the predictions and ground truth across the whole video clip, and supervise the model with the global optimal objective. We also capture the spatial feature and aggregate it with the semantic feature between frames, thus realizing the spatio-temporal enhancement. We evaluate our method on four widely adopted VIS bench-marks, namely YouTube-VIS 2019/2021/2022 and OVIS, and achieve state-of-the-art performance on all benchmarks without bells-and-whistles. For instance, on YouTube-VIS 2021, TCOVIS achieves 49.5 AP and 61.3 AP with ResNet-50 and Swin-L backbones, respectively. Code is available at https://github.com/jun-long-li/TCOVIS. 1.

Introduction
Video instance segmentation (VIS) is a challenging and representative video understanding task recently introduced in [37]. It aims at detecting, segmenting and tracking in-stances across a video. VIS is attracting increasing atten-tion for various real-world applications such as video edit-ing, video surveillance, augmented reality and autonomous driving. Recently introduced VIS methods can be roughly
*Corresponding author categorized into two groups: offline methods and online methods. Offline methods [2, 16, 19, 31, 33, 35] take as input the whole video and perform the segmentation of in-stance sequence for the whole video at once. Online meth-ods [8, 34, 17, 10, 38], on the contrary, take as input a video frame by frame and generate the pre-frame object instances while associating the frame-wise results across frames. Both offline and online methods have achieved im-pressing performance on the VIS task.
Offline methods have an inherent advantage in producing temporally consistent predictions, since delicate temporal communication and association mechanisms can be adopted throughout the video [39, 33, 14] to handle the overall tem-poral information and impose an explicit constraint on the temporal consistency. However, the video-in and video-out offline manner is not suitable for real-time scenarios. Con-versely, online methods are more practical and making con-siderable progress but suffer from temporal inconsistency (as shown in Figure 1), remaining a great challenge.
Online methods rely on specific instance association ap-plied across frames, since only one frame is observed at a time. Existing association techniques can be grouped into two categories, including tracking-by-detection and query propagation-based paradigms. Tracking-by-detection meth-ods [37, 34, 32] generate the per-frame instances indepen-dently by existing instance segmentation models [11, 26, 5] and track instances via tracking heads [37] or instance em-beddings matching [34, 15].
In this way, the features of different frames are isolated before tracking, which results in temporal inconsistency. Query propagation-based meth-ods [13, 10, 41] are inspired by query-based methods [3, 25] and they propagate the query across frames to decode a unique instance without heuristic matching algorithms. De-spite the explicit temporal link of queries, the temporal con-sistency is impaired by the Local Matching and Propagating (LocPro) scheme, where they first perform local optimal matching between the predictions and ground truth at the beginning of the video, and then propagate the assignment across frames, forcing all features from subsequent frames to follow. The LocPro is not suitable for the holistic op-timization across frames and results in temporal inconsis-periments are conducted on four widely adopted VIS bench-marks, i.e., YouTube-VIS 2019 [37], YouTube-VIS 2021,
YouTube-VIS 2022 and Occluded VIS (OVIS) [24]. With-out bells-and-whistles, our proposed method achieves state-of-the-art performance on all benchmarks, outperforming other online methods, e.g., on YouTube-VIS 2021, TCOVIS achieves 49.5 AP and 61.3 AP with ResNet-50 and Swin-L backbones, respectively.
Our main contributions are summarized as follows:
• TCOVIS performs a novel global instance assignment strategy for online video instance segmentation. The model is optimized for the global optimal objective to generate more temporally consistent predictions.
• The further proposed spatio-temporal enhancement module captures the spatial feature and aggregates it with the semantic feature between frames, which fully utilizes the spatial information and facilitates the tem-poral consistency enhancement.
• The proposed method achieves state-of-the-art perfor-mance on four widely used video instance segmenta-tion benchmarks (YouTube-VIS 2019/2021/2022 and
OVIS). Such achievements demonstrate the effective-ness of our proposed method. 2.