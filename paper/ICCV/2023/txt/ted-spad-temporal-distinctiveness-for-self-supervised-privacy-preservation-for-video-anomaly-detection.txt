Abstract
Video anomaly detection (VAD) without human monitor-ing is a complex computer vision task that can have a pos-itive impact on society if implemented successfully. While recent advances have made significant progress in solving this task, most existing approaches overlook a critical real-world concern: privacy. With the increasing popularity of artificial intelligence technologies, it becomes crucial to im-plement proper AI ethics into their development. Privacy leakage in VAD allows models to pick up and amplify un-necessary biases related to people’s personal information, which may lead to undesirable decision making. In this pa-per, we propose TeD-SPAD, a privacy-aware video anomaly detection framework that destroys visual private informa-In particular, we pro-tion in a self-supervised manner. pose the use of a temporally-distinct triplet loss to promote temporally discriminative features, which complements cur-rent weakly-supervised VAD methods. Using TeD-SPAD, we achieve a positive trade-off between privacy protection and utility anomaly detection performance on three pop-ular weakly supervised VAD datasets: UCF-Crime, XD-Violence, and ShanghaiTech. Our proposed anonymization model reduces private attribute prediction by 32.25% while only reducing frame-level ROC AUC on the UCF-Crime anomaly detection dataset by 3.69%. 1.

Introduction
Machine learning-driven technologies are increasingly being adopted by society. The progress in cloud comput-ing has enabled the deployment of even computationally in-tensive technologies in the public space. One such applica-tion is video anomaly detection (VAD) in autonomous video analytics. VAD is a video understanding task that aims to identify the temporal location of anomalous events occur-ring in long continuous videos without human supervision.
An anomaly can be defined as any unusual event, such as a traffic accident, an elderly person falling, or a fire. Proper
Figure 1: Single frame from video Shoplifting052 x264.mp4 of
UCF-Crime [43] showing the types of private attributes (shown in red on the left image) leaked in visual data. In the right image we show the anonymized frame where these attributes are barely vis-ible. The graph demonstrates the ability of our self-supervised framework to mitigate privacy leakage compared to prior work
SPAct [13]. At a similar anomaly detection utility performance, our method prevents 32% of the visual private leakage compared to raw data. application of this technology can result in faster response times to anomalies, without the need for human resources to monitor camera feeds.
However, public adoption of such AI technologies brings justifiable concern about safety and their decision-making capabilities. Many of these concerns center around privacy violations and accuracy. VAD is an application where visual privacy leakage and data bias are exceedingly important is-sues. Sending videos to remote computers or cloud services to process results in unnecessary privacy leakage for people who are not directly involved in anomalous events. Addi-tionally, an application employing a standard RGB video will incorporate any bias found in its training set, leading to potentially unfair decisions. An illustration of privacy leakage is shown in Fig. 1.
Recently, there have been interesting attempts to prevent 1
visual privacy leakage in action recognition. Some of the approaches utilize input downsizing-based solutions [40, 10, 27] and object-detection-dependent obfuscation-based formulations [38, 53]. Wu et al. [49] proposed an ad-versarial training-based framework where they train an anonymization function to remove privacy-preservation.
Dave et al.
[13] proposed a self-supervised privacy-preserving framework that does not require privacy la-bels and achieves similar performance to the supervised method [49]. Since many weakly-supervised anomaly de-tection (WSAD) methods rely on the pretrained features of action recognition, privacy-preserving action recognition seems like a promising candidate for privacy-preserving anomaly detection. However, detecting anomalies does not align well with privacy-preserved action recognition videos.
The use of short videos in action recognition encourages the use of temporally-invariant features, where the features of clips at distinct timesteps should be the same. Con-versely, detecting anomalies in long, untrimmed videos re-quires temporally-distinct reasoning, where features of clips at distinct timesteps of a video should be different, to de-termine whether events in the same scene are anomalous.
This is why most existing anomaly detection methods fo-cus on refining the features of pretrained video encoders to increase their temporal separability.
To the best of our knowledge, privacy-preservation in video anomaly detection is an unexplored area in computer vision. Building on the existing self-supervised privacy-preserving action recognition framework [13], we propose a more aligned utility branch for anomaly detection. To achieve this, we use a novel temporally-distinct triplet loss to promote temporal distinctiveness during anonymiza-tion training. Once the anonymization function is learned through our proposed anonymization framework, we apply it to the anomaly dataset, which ensures privacy leakage mitigation in privacy-sensitive anomaly detection tasks. We use these anonymized features to train the current state-of-the-art WSAD method, MGFN [8].
To evaluate privacy-preserving performance in anomaly detection, we adopt protocols from prior action recog-nition methods, where we report the utility performance on WSAD task on widely used anomaly datasets (UCF-Crime [43], XD-Violence [48], and ShanghaiTech [28]) and budget performance on privacy dataset VISPR [32].
Our contributions can be summarized as follows:
• We introduce a new problem of privacy-preservation in video anomaly detection, where we identify the privacy leakage issue in existing weakly supervised anomaly detection methods.
• To address this open problem, we propose TeD-SPAD, a framework based on self-supervised privacy-preservation with a temporally-distinct triplet loss to make the video anonymization process more suitable for anomaly detection.
• We propose evaluation protocols for privacy vs. anomaly trade-off, demonstrating that our proposed framework outperforms prior methods by significant margins across all anomaly detection benchmarks. On the widely used UCF-Crime dataset, our method is able to eliminate 32.25% of the privacy leakage at a cost of only a 3.96% reduction in frame-level AUC performance. 2.