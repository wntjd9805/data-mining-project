Abstract
Fast adversarial training (FAT) is beneficial for improv-ing the adversarial robustness of neural networks. How-ever, previous FAT work has encountered a significant is-sue known as catastrophic overfitting when dealing with large perturbation budgets, i.e. the adversarial robustness of models declines to near zero during training. To address this, we analyze the training process of prior FAT work and observe that catastrophic overfitting is accompanied by the appearance of loss convergence outliers. Therefore, we ar-gue a moderately smooth loss convergence process will be a stable FAT process that solves catastrophic overfitting.
To obtain a smooth loss convergence process, we propose a novel oscillatory constraint (dubbed ConvergeSmooth) to limit the loss difference between adjacent epochs. The con-vergence stride of ConvergeSmooth is introduced to balance convergence and smoothing. Likewise, we design weight centralization without introducing additional hyperparam-eters other than the loss balance coefficient. Our proposed methods are attack-agnostic and thus can improve the train-ing stability of various FAT techniques. Extensive exper-iments on popular datasets show that the proposed meth-ods efficiently avoid catastrophic overfitting and outperform all previous FAT methods. Code is available at https:
//github.com/FAT-CS/ConvergeSmooth. 1.

Introduction
Recent breakthroughs in deep learning [21, 32] have aroused researchers’ interest in the security of neural net-In particular, the advanced research works [45, 48, 39]. proves the vulnerability of deep models to adversarial at-tacks [8, 11, 27]. For instance, tiny crafted perturbations can fool models in various fields into making wrong decisions
[15, 50, 35, 24]. Considering the security risks brought by adversarial attacks [18, 46, 33, 44], there is a quickly grow-*Corresponding author ing body of work [43, 46, 18] on improving the adversarial robustness of neural networks. Among them, adversarial training is widely applied by practitioners [31, 12].
In recent years, projected gradient descent based ad-versarial training (PGD-AT) [25, 34] has been widely em-ployed for its stability and effectiveness. However, this mechanism is computationally expensive. It requires multi-ple gradient descent steps to generate the adversarial train-ing data [38]. An alternative of PGD-AT is the fast adver-sarial training (FAT) [28], which only adopts a single-step fast gradient sign method (FGSM) [11] to generate training data. Compared to PGD-AT, FAT can efficiently train mod-els, but easily falls into catastrophic overfitting [40, 29].
A number of FAT methods have been proposed to mit-igate catastrophic overfitting. For example, Wong et al.
[40] use randomly initialized perturbations to enhance the diversity of adversarial perturbations. Based on it, An-driushchenko et al. [2] raise a complementary regularizer named GradAlign to maximize the gradient alignment be-tween benign and adversarial examples explicitly. Simi-larly, NuAT [36] and FGSM-MEP [42] adopt nuclear norm or p-norm to regularize the adversarial training, thereby in-creasing the prediction alignment between benign and ad-versarial examples. However, the above methods can only resolve catastrophic overfitting within the limited perturba-tion budget (ξ ≤ 8/255). ξ specifies the perturbation degree of adversarial training data generated by various attacks.
Besides, models trained by small perturbations are vulner-able to adversarial attacks with a large ξ, e.g. the models trained by NuAT and FGSM-MEP at ξ = 8/255 perform 53% and 54% robustness against the PGD-50 attack with ξ
= 8/255, but only 22% and 20% robustness against the same attack with ξ = 16/255 respectively. Therefore, we aspire to prevent catastrophic overfitting to improve the adversarial robustness of neural models at larger perturbation budgets.
By analyzing adversarial training processes of represen-tative work, we observe that catastrophic overfitting is usu-ally accompanied by a slight fluctuation in the classification loss for benign samples and a sharp drop in the classification
loss for adversarial examples. This motivates us to ques-tion whether a smooth loss convergence process is also a stable FAT process. Moreover, we find that an oscillating adversarial training phase may restart the FAT process after catastrophic overfitting. Fig. 1 shows the details.
According to these observations, we introduce an oscilla-tory constraint that limits the difference in loss between ad-jacent training epochs, called ConvergeSmooth. A dynamic convergence stride of ConvergeSmooth is designed consid-ering the nonlinear decay rate of loss functions. Inspired by the smoothness of loss convergence, we further verify the effect of the proposed weight centralization on model sta-bility. Weight centralization refers to taking the weights av-erage of the previously trained models as the convergence center of the current model weight. Our proposed meth-ods are attack-agnostic and thus can be combined with ex-isting adversarial strategies in FAT, such as FGSM-RS and
FGSM-MEP, to evaluate their performance.
The contributions are summarized in four aspects: (1)
We verify that previous FAT works still suffer from catas-trophic overfitting at a large ξ and then study catastrophic overfitting from the perspective of convergence instability of loss functions; (2) We propose a smooth convergence constraint, ConvergeSmooth, and design a dynamic con-vergence stride for it, to help various FAT methods avoid catastrophic overfitting on different perturbation budgets; (3) The weight centralization is proposed without introduc-ing extra hyperparameters other than the loss balance coeffi-cient to stabilize FAT; (4) Extensive experiments show that the proposed methods outperform the state-of-the-art FAT techniques in terms of efficiency, robustness, and stability. 2.