Abstract
Monocular depth estimation is scale-ambiguous, and thus requires scale supervision to produce metric predic-tions. Even so, the resulting models will be geometry-specific, with learned scales that cannot be directly trans-ferred across domains. Because of that, recent works fo-cus instead on relative depth, eschewing scale in favor of improved up-to-scale zero-shot transfer.
In this work we introduce ZeroDepth, a novel monocular depth estimation framework capable of predicting metric scale for arbitrary test images from different domains and camera parameters.
This is achieved by (i) the use of input-level geometric em-beddings that enable the network to learn a scale prior over objects; and (ii) decoupling the encoder and decoder stages, via a variational latent representation that is con-ditioned on single frame information. We evaluated Ze-roDepth targeting both outdoor (KITTI, DDAD, nuScenes) and indoor (NYUv2) benchmarks, and achieved a new state-of-the-art in both settings using the same pre-trained model, outperforming methods that train on in-domain data and re-quire test-time scaling to produce metric estimates. Project page: https://sites.google.com/view/tri-zerodepth. 1.

Introduction
Monocular depth estimation is a key task in computer vi-sion, with practical applications in areas such as robotics [9, 31] and autonomous driving [19, 23, 37, 27]. It is easy to understand why: the promise of turning any camera into a dense range sensor is very appealing, both as a means to re-duce costs and in terms of its rich semantics and widespread application. However, in order to be truly useful as a 3D re-construction tool these predictions need to be scale-aware, meaning that they need to be metrically scaled. Supervised methods train with groundtruth depth maps [14, 38], while self-supervised methods inject additional information in the form of velocity measurements [23], camera intrinsics [2] and/or extrinsics [27, 56]. Even so, the resulting models will be camera-specific, since the learned scale will not transfer across datasets, due to differences in the cameras used to capture training data.
This geometric domain gap is separate from the tradi-tional appearance domain gap, however while the latter has been extensively studied in recent years [26, 59, 29, 39, 53, 34, 60], very few works have addressed the for-mer [2, 12, 57].
Instead, the recent trend is to focus on relative depth [10, 46, 47], eschewing scale completely in favor of improved zero-shot transfer of unscaled depth pre-dictions. Even though this approach qualitatively leads to very accurate depth maps, the resulting predictions still re-quire groundtruth information at test-time to be metrically scaled, which severely limits their application in practical scenarios, such as autonomous driving and indoor robotics.
In this paper, we rethink this recent trend and introduce
ZeroDepth, a novel monocular depth estimation framework that is robust to the geometric domain gap, and thus capable of generating metric predictions across different datasets.
We achieve this by proposing two key modifications to the standard architecture for monocular depth estimation: (i) we use input-level geometric embeddings to jointly encode camera parameters and image features, which enables the network to reason over the physical size of objects and learn scale priors; and (ii) we decouple the encoding and decod-ing stages, via a learned global latent representation. Im-portantly, this latent representation is variational, and once conditioned can be sampled and decoded to generate mul-tiple predictions in a probabilistic fashion. By training on large amounts of scaled, labeled data from real-world and synthetic datasets, our framework learns depth and scale priors anchored in physical 3D properties that can be di-rectly transferred across datasets, resulting in the zero-shot prediction of metrically accurate depth estimates. In sum-mary, our contributions are as follows:
• We introduce ZeroDepth, a novel variational monoc-ular depth estimation framework capable of transfer-ring metrically accurate predictions across datasets with different camera geometries.
• We propose a series of encoder-level data augmenta-tion techniques aimed at improving the robustness of our proposed framework, addressing both the appear-ance and geometric domain gaps.
• As a result, ZeroDepth achieves state-of-the-art zero-shot transfer in both outdoor (KITTI, DDAD, nuScenes) and indoor (NYUv2) benchmarks, outper-forming methods that require in-domain training im-ages and test-time ground truth scale alignment. 2.