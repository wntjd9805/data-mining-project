Abstract
The success of automated medical image analysis de-pends on large-scale and expert-annotated training sets.
Unsupervised domain adaptation (UDA) has been raised as a promising approach to alleviate the burden of labeled data collection. However, they generally operate under the closed-set adaptation setting assuming an identical la-bel set between the source and target domains, which is over-restrictive in clinical practice where new classes com-monly exist across datasets due to taxonomic inconsistency.
While several methods have been presented to tackle both domain shifts and incoherent label sets, none of them take into account the common characteristics of the two issues and consider the learning dynamics along network train-ing. In this work, we propose optimization trajectory dis-tillation, a uniﬁed approach to address the two technical challenges from a new perspective.
It exploits the low-rank nature of gradient space and devises a dual-stream distillation algorithm to regularize the learning dynamics of insufﬁciently annotated domain and classes with the ex-ternal guidance obtained from reliable sources. Our ap-proach resolves the issue of inadequate navigation along network optimization, which is the major obstacle in the taxonomy adaptive cross-domain adaptation scenario. We evaluate the proposed method extensively on several tasks towards various endpoints with clinical and open-world signiﬁcance. The results demonstrate its effectiveness and improvements over previous methods. Code is available at https://github.com/camwew/TADA-MI. 1.

Introduction
Automated and objective analysis of medical images is an important research topic and has been explored in var-ious clinical applications [57, 13]. To relieve the burden of acquiring massive annotated data for model develop-ment on new domains, several unsupervised domain adapta-Figure 1. (a) Illustration of the issue of taxonomic inconsistency with nuclei recognition. The category label sets are incoherent across datasets. Different colours indicate the class each nucleus belongs to. The red asterisks denote novel classes only existing in Dataset 2. (b) Concept of the proposed method. We perform optimization trajectory distillation to deliver external navigation for learning target domain and new classes where the optimization steps tend to be restricted and unreliable. tion (UDA) methods [7, 38, 37] are proposed to mitigate the data distribution bias [51] between a richly labeled source domain and a target domain with no explicit supervision.
However, these works assume a closed-set adaptation setting that the source and target domains should necessar-ily share the identical category label space and deﬁnition
[44]. The restriction limits their practical applicability in the clinical wild [15]. Unlike natural images where the def-initions of different entity categories (e.g., cat v.s. dog) are structured and globally uniﬁed, in the medical domain, in-consistency in taxonomy across different countries or in-stitutes is a common issue compromising the feasibility of cross-domain adaptation [12]. Take nuclei recognition in histology images as an example, the ambiguous biologi-cal characteristics of cells and distinct clinical usages of the analysis outcomes result in a lack of a uniﬁed categorization schema [15, 20]. With speciﬁc clinical purposes and down-stream applications, different datasets could categorize nu-clei with disparate granularities of biological concepts. Be-sides, as medical research evolves rapidly, novel cell and disease classes could be discovered and form datasets with continually expanding category label sets [41]. This moti-vates us to develop a generalized adaptation method with the capability to tackle both data distribution bias and cate-gory gap for more ﬂexible UDA in clinical practice.
To this end, we study the problem of taxonomy adaptive domain adaptation [19]. It assumes the target domain could adopt a label space different from the source domain. Fol-lowing UDA, a labeled source dataset and unlabeled sam-ples from the target domain are available during training.
Additionally, to recognize the novel/ﬁne-grained categories which are non-existent or unspeciﬁed in the source domain, a few samples of those target-private categories are anno-tated and utilized as exemplars. The technical challenge for this setting lies in how to alleviate domain shifts and con-currently learn new classes with very limited supervision.
Recently, several methods are proposed towards a similar goal [32, 22]. However, they typically address the two is-sues individually in separate contexts and fail to design a uniﬁed paradigm according to their common characteris-tics [19], which could incur a subtle collision across issues since their objectives are non-relevant [60]. Besides, exist-ing works either focus on cross-domain/class alignment in the feature space [40, 36] or resort to model output-based regularization such as self-supervision [45], whereas those approaches suffer from the equilibrium challenge of adver-sarial learning [1] and the low-quality pseudo-labels [54].
In this work, we present a uniﬁed framework for tax-onomy adaptive cross-domain adaptation from a new per-spective, i.e., via optimization trajectory distillation. Our strategy is motivated by a common challenge existing in both cross-domain and small-data learning regimes, which is the inadequate navigation in the course of network op-timization. For cross-domain learning, the unstable feature alignment procedure and noisy pseudo-labels tend to induce error accumulation along network training [62]. Similarly, with limited support samples in new class learning, the opti-mization of network is inclined to step towards restricted lo-cal minima and cannot cover a globally-generalizable space
[55]. To this end, we propose to exploit the optimization trajectory from a reliable “teacher” to provide external guid-ance for regularizing the learning dynamics of insufﬁciently annotated domain and classes, as illustrated in Fig. 1(b).
Our method consists of two key components, i.e., cross-domain/class distillation and historical self-distillation. (i) Cross-domain and cross-class distillation aim to lever-age the optimization trajectory from the richly labeled do-main and classes to serve as the “teacher”. Motivated by the Neural Tangent Kernel (NTK) theory [27], we char-acterize the network optimization trajectory through gra-dient statistics. Then, given the observation that the sub-space spanned by the gradients from most iterations is gen-erally low-rank [35, 2], we design a gradient projection approach to suppress the noisy signals in stochastic gra-dients and rectify the distillation process. Thereafter con-straints are imposed on the gradient statistics of target do-main and new classes to calibrate their training dynamics towards domain-invariant and unbiased learning procedure. (ii) Historical self-distillation further drives the optimiza-tion paths of model to converge towards ﬂat minima. It is found that the ﬂatness of loss landscapes is strongly related to the model’s robustness and generalizability [26], while how to take advantage of the insight to tackle domain shifts and limited supervision is under-explored. We propose to exploit the historical gradients to construct the informative low-rank subspaces and then perform gradient projection to alleviate loss sharpness. It compensates for the intense and out-of-order optimization updates incurred by inadequate regularization and leads to better generalization.
Our prime contributions are as follows: (1) We intro-duce a more generalized cross-domain adaptation paradigm for medical image analysis in which both data distribution bias and category gap exist across the source and target do-mains. (2) We leverage insights from recent learning the-ory research and propose a novel dual-stream optimization trajectory distillation method to provide external navigation in network training. We perform theoretical justiﬁcations from two perspectives to illustrate the merits of our method. (3) Experiments on various benchmarks validate the effec-tiveness and robustness of our proposed method and its im-provements over existing approaches. 2.