Abstract
Supervised learning of image classifiers distills human knowledge into a parametric model fθ through pairs of im-ages and corresponding labels {(Xi, Yi)}N i=1. We argue that this simple and widely used representation of human knowl-edge neglects rich auxiliary information from the annotation procedure, such as the time-series of mouse traces and clicks left after image selection. Our insight is that such anno-tation byproducts Z provide approximate human attention that weakly guides the model to focus on the foreground cues, reducing spurious correlations and discouraging short-cut learning. To verify this, we create ImageNet-AB and
COCO-AB. They are ImageNet and COCO training sets en-riched with sample-wise annotation byproducts, collected by replicating the respective original annotation tasks. We refer to the new paradigm of training models with annota-tion byproducts as learning using annotation byproducts (LUAB). We show that a simple multitask loss for regressing
Z together with Y already improves the generalisability and robustness of the learned models. Compared to the original supervised learning, LUAB does not require extra annotation costs. ImageNet-AB and COCO-AB are at github.com/naver-ai/NeglectedFreeLunch. 1.

Introduction
Supervised learning of image classifiers requires the trans-fer of human intelligence to a parametric model fθ. The transfer consists of two phases. First, human annotators execute human computation tasks [99] to put labels Y on each image X. The resulting labeled dataset {(X i, Y i)}N i=1 contains the gist of human knowledge about the visual task in a computation-friendly format. In the second phase, the model is trained to predict the labels Y for each input X.
In this work, we question the practice of collecting and utilising only the labels Y for each image X for training the models. In fact, common practise simply forgoes a large
⋆Equal contribution. † currently at Google. Correspondence to Seong
Joon Oh: coallaoh@gmail.com.
Figure 1: Annotation byproducts from ImageNet. Annotators leave traces like click locations as they select images with “Border
Collie”. We argue that such byproducts contain signals that may improve model generalisation and robustness.
Figure 2: Learning using Annotation Byproducts (LUAB).
LUAB exploits annotation byproducts Z that are unintentionally generated during the human intelligence tasks for annotation. amount of additional signals from human annotators other than mere labels. When humans interact with computers
through the graphical user interface, they leave various forms of unintentional traces.
Input devices like the computer mouse produce time-series data in which information about what (e.g., mouse action type) and where (e.g., x-y coordi-nates in the monitor) are logged with timestamps. We refer to such auxiliary signals as annotation byproducts Z. See
Figure 1 for an ImageNet annotation example [74, 68]. As annotators browse and click on images containing the class of interest, various byproducts are generated, e.g., images over which were hovered during selection, mouse move-ment speed between images, pixels on which were clicked in an image, images that were deselected due to mistake, and latency between image selections, etc.
We introduce the new learning paradigm, learning using annotation byproducts (LUAB), as a promising alternative to the usual supervised learning (Figure 2). We propose to use the annotation byproducts in the training phase, for further enhancing a model. This is a special case of learning using privileged information (LUPI) [98], where additional information Z other than input X and target Y is available during training but is not given at inference. LUAB is an attractive instance of LUPI, as it does not incur additional annotation costs for privileged information.
We demonstrate the strength of the LUAB framework by contributing datasets ImageNet-AB and COCO-AB, where the original ImageNet and COCO classification training sets are enriched with the annotation byproducts. We show that annotation byproducts from image-category labelling inter-faces contain weak information about the foreground object locations. We show that performing LUAB with such infor-mation improves not only generalisability but also robustness by reducing spurious correlations with background features, a critical issue of model reliability these days [84, 51, 27].
Our contributions are (1) acknowledge a neglected in-formation source available without additional costs during image labelling: annotation byproducts (§3); (2) LUAB as a new learning paradigm that makes use of annota-tion byproducts without extra annotation costs compared to the usual supervised learning (§4); (3) empirical findings that LUAB with byproducts weakly encoding object loca-tions improves model generalisability and reduces spurious correlations with the background (§5); and (4) release of
ImageNet-AB and COCO-AB dataset for future research (github.com/naver-ai/NeglectedFreeLunch). 2.