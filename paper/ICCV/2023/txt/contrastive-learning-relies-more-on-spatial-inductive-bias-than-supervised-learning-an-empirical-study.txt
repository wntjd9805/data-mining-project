Abstract on spatial inductive bias than SL, regardless of specific CL algorithm or backbones, opening a new direction for study-ing the behavior of CL.
Though self-supervised contrastive learning (CL) has shown its potential to achieve state-of-the-art accuracy without any supervision, its behavior still remains under-investigated. Different from most previous work that under-stands CL from learning objectives, we focus on an unex-plored yet natural aspect: the spatial inductive bias which seems to be implicitly exploited via data augmentations in
CL. We design an experiment to study the reliance of CL on such spatial inductive bias, by destroying the global or lo-cal spatial structures of an image with global or local patch shuffling, and comparing the performance drop between ex-periments on original and corrupted dataset to quantify the reliance on certain inductive bias. We also use the uni-formity of feature space to further research how CL-pre-trained models behave with the corrupted dataset. Our re-sults and analysis show that CL has a much higher reliance 1.

Introduction
Self-supervised contrastive learning (CL) has demon-strated tremendous potential in learning generalizable rep-resentations from unlabeled datasets [4, 18, 16, 2, 8, 35] in recent years. Current state-of-the-art CL algorithms learn representations from ImageNet [11] that match or even ex-ceed the accuracy of their supervised learning (SL) counter-parts on ImageNet and downstream tasks. Understanding the behavior and the power of contrastive learning is there-fore becoming an interesting and crucial topic in academia.
Previous work on understanding CL [6, 29, 27] mostly fo-cuses on the loss functions, while few work focuses on the data-centric inductive bias in CL â€“ which information CL
focuses on exploiting when lacking semantic supervision.
A typical CL method learns the semantic information of images by applying data augmentation-based contrasts, which generate variants of images with the same seman-tic information via specific random data augmentations, and trains the model toward distinguishing images from differ-ent variants. Data augmentation is crucial in a CL algo-rithm, which provides diversified variants for the model to learn the invariant semantic information. The choice of data augmentation includes random resized cropping, color jit-ter, Gaussian blurry, flipping, etc, where only random re-sized cropping (RSC) changes the geometry and the display scopes of the image, being the data augmentation that alters the image most severely. With RSC, the model is required to fetch the same semantic feature with two different scopes of an image, requiring a special focus on spatial informa-tion. A natural suspicion therefore arises: Does CL highly rely on spatial inductive bias? Furthermore, does CL relies more on this than supervised learning?
This paper focuses on the spatial inductive bias in learn-ing algorithms including SL and CL. To investigate the in-ductive bias on spatial information, we use two types of data corruptions, local and global patch shuffling [15, 23, 33, 20], to respectively destroy the local and global infor-mation of an image preventing the learning algorithm from using them. For each corruption setting with a fixed patch size, we sample one specific corruption, and apply such de-terministic corruption operation for all images in the dataset to construct a corrupted dataset. We pre-train a backbone model on the training split of such dataset with a learning algorithm, and evaluate the accuracy on the testing split. We quantify the inductive bias reliance with the performance drop between the testing accuracy of the original and the corrupted datasets.
We conduct extensive experimental results on both
CIFAR-10 [22] and large-scale ImageNet [11] datasets, for
SL and commonly-used CL algorithms with both CNN-based and Transformer-based backbone models. We ob-tain a consistent backbone-independent conclusion from our empirical study, that CL does rely more on spatial in-ductive bias than CL, while higher reliance on global spa-tial information than local. We also advise that SL does not have such clear reliance on other inductive biases studied with other types of corruptions, e.g. gamma distortion, by showing that no clear difference in performance drops of
CL and SL occurs under different observations. To deeper analyze the impact of the corruptions, we use the unifor-mity metric proposed in [30] to quantify the potential of unsupervised classification ability. We observe that there is a high uniformity drop in the corrupted dataset with patch shuffling, higher than the uniformity drop in the dataset un-der other corruptions, showing that the CL relies most on spatial inductive bias.
Our contributions are three-folded. (1) We perform ex-tensive empirical studies on different datasets, for various learning algorithms with both CNN and Transformer back-bones, and show that CL has a clearly higher inductive bias on spatial information than SL. (2) We propose the method to evaluate the spatial inductive bias for learning algorithms with patch shuffling data corruptions, and use uniformity as a metric to further quantify the loss of unsupervised classi-fication ability for SL methods trained with no spatial infor-mation. (3) We offer analyses and explanations for such ob-servations, inspiring academia with a new direction for re-search on various types of inductive biases. Our work is the first work to observe, define, and analyze the dependency of
CL on spatial inductive bias, as an intrinsic property of CL, via a systematic experiment design, showing novel insights on the mechanism of CL. 2.