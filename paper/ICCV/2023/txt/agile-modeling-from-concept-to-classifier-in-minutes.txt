Abstract
The application of computer vision methods to nuanced, subjective concepts is growing. While crowdsourcing has served the vision community well for most objective tasks (such as labeling a “zebra”), it now falters on tasks where there is substantial subjectivity in the concept (such as iden-tifying “gourmet tuna”). However, empowering any user to develop a classiﬁer for their concept is technically difﬁ-cult: users are neither machine learning experts nor have the patience to label thousands of examples. In reaction, we introduce the problem of Agile Modeling: the process of turning any subjective visual concept into a computer vi-sion model through real-time user-in-the-loop interactions.
We instantiate an Agile Modeling prototype for image clas-siﬁcation and show through a user study (N=14) that users can create classiﬁers with minimal effort in under 30 min-utes. We compare this user driven process with the tradi-tional crowdsourcing paradigm and ﬁnd that the crowd’s notion often differs from that of the user’s, especially as the concepts become more subjective. Finally, we scale our ex-periments with simulations of users training classiﬁers for
ImageNet21k categories to further demonstrate the efﬁcacy of the approach. 1.

Introduction
Whose voices, and therefore, whose labels should an im-age classiﬁer learn from? In computer vision today, the an-swer to this question is often left implicit in the data col-lection process. Concepts are deﬁned by researchers before curating a dataset [13]. Decisions for which images con-stitute positive versus negative instances are conducted by majority vote of crowd workers annotating this pre-deﬁned set of categories [32, 57]. An algorithm then trains on this aggregated ground truth, learning to predict labels that rep-⇤Equal contribution.
Figure 1: Visual concepts can be nuanced and subjec-tive, differing from how a majoritarian crowd might label a concept. For example, a graduate student may think that well-prepared tuna sandwiches are considered gourmet tuna, but sushi chef might disagree. resent the crowd’s majoritarian consensus.
As computer vision matures, its application to nuanced, subjective use cases is burgeoning. While crowdsourcing has served the vision community well on many objective tasks (e.g., identifying ImageNet [13] concepts like “zebra”,
“tiger”), it now falters on tasks where there is substantial subjectivity [21]. Everyday people want to scale their own decision-making on concepts others may ﬁnd difﬁcult to emulate—for example, in Figure 1, a sushi chef might covet a classiﬁer to source gourmet tuna for inspiration. Majority vote by crowd workers may not converge to the same deﬁ-nition of what makes a tuna dish gourmet.
This paper highlights the need for user-centric ap-proaches to developing real-world classiﬁers for these sub-jective concepts. To deﬁne this problem space, we recog-nize the following challenges. First, concepts are subjec-tive, requiring users to be embedded in the data curation process. Second, users are usually not machine learning ex-perts; we need interactive systems that elicit the subjective decision boundary from the user. Third, users don’t have the patience nor resources to sift through the thousands of training instances that is typical for most image classiﬁca-tion datasets [13, 35, 29]—for example, ImageNet anno-tated over 160M images to arrive at their ﬁnal 14M version.
In order to tackle these challenges, we introduce the problem of Agile Modeling: the process of turning any vi-sual concept into a computer vision model through a real-time user-in-the-loop process. Just as software engineer-ing matured from prescribed procedure to “agile” software packages augmenting millions of people to become soft-ware engineers, Agile Modeling aims to empower anyone to create personal, subjective vision models. It formalizes the process by which a user can initialize and interactively guide the training process while minimizing the time and ef-fort required to obtain a model. With the emergent few-shot learning capabilities of vision foundation models [46, 24], now is the right time to begin formalizing and developing
Agile Modeling systems.
We instantiate an Agile Modeling prototype for im-age classiﬁcation to highlight the importance of involving the user-in-the-loop when developing subjective classiﬁers.
Our prototype allows users to bootstrap the learning pro-cess with a single language description of their concept (e.g., “gourmet tuna”) by leveraging vision-language foun-dation models [46, 24]. Next, our prototype uses active learning to identify instances which, if labeled, would max-imally improve classiﬁer performance. These few instances are surfaced to the user, who is only asked to identify which instances are positive—something they can do even without a background in machine learning. This iterative process continues with more active learning steps until the user is satisﬁed with their classiﬁer’s performance.
Our contributions are: 1. We formulate the Agile Modeling problem, which puts users at the center of the image classiﬁcation process. 2. We demonstrate that a real-time prototype can be built by leveraging SOTA image-text co-embeddings for fast image retrieval and model training. With our opti-mizations, each round of active learning operates over over 10M images and can be performed on a single desktop CPU in a few minutes. In under 5 minutes, user-created models outperform zero-shot classiﬁers. 3. In a setting resembling real-world conditions, we com-pare models trained with labels from real users versus crowd raters, and ﬁnd that the value of user-labeled data increases when the concept is nuanced or difﬁcult. 4. We verify the results of the user study with a simulated experiment of 100 more concepts in ImageNet21k. 5. We open source the implementation of our Agile Mod-eling prototype on our GitHub page [59], enabling any-one to create classiﬁers for their concepts. 6. We release all annotations labeled in our user study for 14 novel concepts, enabling researchers to experiment with the concepts deﬁned by our users [59]. 2.