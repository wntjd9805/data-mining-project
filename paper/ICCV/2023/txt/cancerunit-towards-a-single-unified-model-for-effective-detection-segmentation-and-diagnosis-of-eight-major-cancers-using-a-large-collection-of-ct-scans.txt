Abstract
Human readers or radiologists routinely perform full-body multi-organ multi-disease detection and diagnosis in clinical practice, while most medical AI systems are built to focus on single organs with a narrow list of a few dis-eases. This might severely limit AI’s clinical adoption. A certain number of AI models need to be assembled non-trivially to match the diagnostic process of a human read-ing a CT scan. In this paper, we construct a Unified Tu-mor Transformer (CancerUniT) model to jointly detect tu-mor existence & location and diagnose tumor characteris-tics for eight major cancers in CT scans. CancerUniT is a query-based Mask Transformer model with the output of multi-tumor prediction. We decouple the object queries into organ queries, tumor detection queries and tumor diagno-sis queries, and further establish hierarchical relationships among the three groups. This clinically-inspired architec-ture effectively assists inter- and intra-organ representation learning of tumors and facilitates the resolution of these complex, anatomically related multi-organ cancer image reading tasks. CancerUniT is trained end-to-end using a curated large-scale CT images of 10,042 patients includ-ing eight major types of cancers and occurring non-cancer tumors (all are pathology-confirmed with 3D tumor masks annotated by radiologists). On the test set of 631 patients,
CancerUniT has demonstrated strong performance under a set of clinically relevant evaluation metrics, substantially outperforming both multi-disease methods and an assem-bly of eight single-organ expert models in tumor detection,
*Correspondence to Jieneng Chen (jienengchen01@gmail.com),
Liu (yingda.xia@alibaba-inc.com),
Zaiyi and
Xia
Yingda (zyliu@163.com)
Figure 1. We aim at cancer and non-cancer detection, segmenta-tion, and diagnosis in eight major organs via CT scan. Seven of our eight targeted cancers rank the top seven in terms of mortality. segmentation, and diagnosis. This moves one step closer to-wards a universal high performance cancer screening tool. 1.

Introduction
Cancer, a leading cause of death in the world, contin-ues to thwart human life expectancy and cause huge soci-1
etal burdens despite significant progress in medical research
[52, 40]. Medical imaging is a powerful tool for detection and diagnostic examination of cancer and is widely used in clinical practice everywhere. The daily work of radiologists in reading and interpreting cancer imaging findings includes three main clinical tasks: detection, quantification, and di-agnosis [3]. Since Computed Tomography (CT) body scans are very common (nearly 80% in all CT exams) [37] and each CT scan can have hundreds of image slices, the miss-detection and miss-diagnosis of cancer are the pain points in the radiology workflow. Human readers statistically tend to have high specificity but low sensitivity for tagging and reporting various anomalies or diseases.
Computer-aided detection (CADe) and diagnosis (CADx) can assist radiologists and oncologists to improve the tumor detection rate and diagnosis accuracy [14, 3].
With the development of deep learning and Convolutional
Neural Networks (CNNs), CAD algorithms have met or exceeded expert-level performance in some specific applications [32, 27, 12, 2]. However, most CAD expert systems focus on dealing with single organ diseases [45], e.g., pancreas [76, 71], liver [23, 11], lung [2, 25], or kidney tumors [19], while radiologists in turn, must be responsible for all possible diseases and radiologically significant find-ings [34]. For example, for an abdominal CT examination that is initially targeted for the gallbladder, even when there are no clear prior indications (e.g., abdominal pain), all visible organs in the entire abdomen from CT imaging need to be carefully inspected by a radiologist. Therefore, the role of current CAD tools is still (very) limited in clinical practice, far from functioning as universally as a human reader. A versatile CAD tool that can perform (many) more critical medical tasks would be more clinically desirable and thus is in high demand [39].
Despite notable progress in multi-organ segmentation, it should be noted that detecting and diagnosing multiple can-cers is considerably more difficult than segmenting organs alone due to several factors: (1) tumors have a variety of types, appearances and size, making it hard to be detected. (2) tumor detection requires differentiation of tumors from normal tissue within an organ, which is more challenging than the differentiation of organs from the background in organ segmentation. (3) the diagnosis of cancer involves the fine-grained categorization of tumors, which necessi-tates a high level of expertise and specialized training. Aim-ing at solving the universal lesion detection problem in CT scans, DeepLesion is a recent pioneering publicly available dataset [62, 61, 59], and despite much follow-up work, most cancer detection, quantification, and diagnosis solutions de-rived from DeepLesion dataset [62] are still insufficient in the following aspects. First, the data size and patient num-ber of a single disease can be small, and some major can-cer types (e.g., esophagus, stomach, and colorectum) are scarce, resulting in relatively high false positive and sub-optimal detection rates. Second, voxel-level tumor annota-tion, perhaps as 3D masks (requiring a high level of clinical expertise and are very tedious to label) are not available, making the necessary precise 3D quantification difficult (if not impossible). Third, the pathological gold standard of confirming tumor types is unavailable and, therefore, im-possible to distinguish between malignant and benign le-sions. Recent clinical validations of two multi-disease de-tection AI systems [36, 57] found that ruling out irrelevant
CAD findings (i.e., false positives and lesions without ad-equate malignancy assessment) was very time-consuming and confusing for radiologists. These observations clearly indicates the essential limitations of applying DeepLesion dataset [62] to positive clinical impacts.
In this paper, we curate a large (abdominal and chest) CT image dataset including eight major cancers (from the top seven cancers with the highest mortality in the world [40]: lung, colorectum, liver, stomach, breast, esophagus, and pancreas, plus a public kidney dataset [19]) of total 10,673 patients (Of these, breast cancer has the fewest, with 478 patients; lung cancer has the most, with 2,402 patients. In addition, there are 1,055 normal controls). All tumor types (and subtypes) of the seven organs are confirmed by either surgical or biopsy pathology and recorded as gold standard labels, where full spectrum of all tumor subtypes are of-fered for four organs. All confirmed tumors in CT scans are manually segmented or delineated in 3D by board-certified radiologists who are specialized in the particular organ or disease types. To our knowledge, previous datasets with similar tumor characteristics only cover a single disease at the scale of hundreds of patients, such as the pancreatic tu-mor [71] and kidney tumor datasets [19]. The curation of our new 8-cancer dataset is a major step towards building a universal multi-cancer imaging reading AI model – with the hope to reach a performance level comparable with radiolo-gists specializing in different cancer types – for assisting radiologists and general clinicians in precision detection, quantification, and diagnosis. Fig. 1 shows our goal for can-cer and non-cancer detection, segmentation, and diagnosis in eight major organs via CT scans.
On the other hand, we propose a new clinically in-terpretable computing architecture, named Unified Tumor
Transformer (CancerUniT). In general, CancerUniT is a single unified model that simultaneously solves the tasks of multi-tumor detection, segmentation and diagnosis in a semantic segmentation manner. Our motivations are: (1) the organs, cancers and non-cancer tumors are interrelated in both appearance similarity and human anatomical con-straints, e.g., HCC (a major malignant liver cancer) and cyst (benign lesion) both occur inside the liver with textual and other visual differences, while HCC and PDAC (a type of pancreatic cancer) should appear in two different organs 2
but their clinical characteristics are both malignant carci-noma; (2) a unified learning of multi-organs-tumors could reduce the performance uncertainty and architectural com-plexity in assembling multiple single models, e.g., different predictions of the same intended object or finding by multi-ple models. To collaboratively model such differences and connections or dependencies, we propose a novel represen-tation learning method that represents each organ and tumor as an object query of the Transformer in a semantic hierar-chy. The object queries are divided into organ queries, tu-mor detection queries and diagnosis queries, and we estab-lish a query hierarchy based on the clinical meaning of the queries. This design will explicitly encourage the queries to learn the inter-organ and intra-organ relationships to solve the clinically sophisticated multi-cancer tumor recognition tasks.
CancerUniT is trained and tested on our curated dataset.
CancerUniT outperforms the DeepLesion model, the en-semble of single-organ expert models and unified baseline models (trained on our data). Compared to the DeepLesion model, CancerUniT has a 29.3% higher sensitivity and a large margin of 77.5% higher specificity in tumor detection.
Compared to an ensemble of individually trained single-organ nnUNet models, CancerUniT has an average im-provement of 6.7% in tumor detection sensitivity, 2.8% in diagnostic accuracy, and 3.9% in Dice segmentation score across all the organs; On normal patients, CancerUniT has an improvement of 22.5% in specificity (ours 81.7% vs. nnUNet 59.2%); CancerUniT is 4.5 times faster in testing speed. In comparison to a unified nnUNet model dealing with all eight organs, CancerUniT leads by 5.3% in lesion detection sensitivity, 6.7% in diagnostic accuracy, 2.8% in specificity, and 2.7% in Dice segmentation score. The im-provements indicate that the different type of tumors have mutual correlations and the design of CancerUniT success-fully captures this clinical relationship for enhanced tumor representation learning. The high performance of Can-cerUniT also sheds light on its clinical potential for real-world multi-cancer detection, segmentation, and diagnosis. 2.