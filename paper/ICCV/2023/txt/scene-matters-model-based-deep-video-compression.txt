Abstract
Video compression has always been a popular research area, where many traditional and deep video compression methods have been proposed. These methods typically rely on signal prediction theory to enhance compression perfor-mance by designing high efficient intra and inter prediction strategies and compressing video frames one by one. In this paper, we propose a novel model-based video compression (MVC) framework that regards scenes as the fundamental units for video sequences. Our proposed MVC directly mod-els the intensity variation of the entire video sequence in one scene, seeking non-redundant representations instead of re-ducing redundancy through spatio-temporal predictions. To achieve this, we employ implicit neural representation as our basic modeling architecture. To improve the efficiency of video modeling, we first propose context-related spatial positional embedding and frequency domain supervision in spatial context enhancement. For temporal correlation capturing, we design the scene flow constrain mechanism and temporal contrastive loss. Extensive experimental re-sults demonstrate that our method achieves up to a 20% bitrate reduction compared to the latest video coding stan-dard H.266 and is more efficient in decoding than existing video coding strategies. 1.

Introduction
Recently, videos have become ubiquitous in people’s from short-form videos to conference and daily lives, surveillance videos. Efficiently storing and transmitting video data has become a significant challenge due to the vast amounts and explosive growth of such data. To address this challenge, multiple video compression standards have been developed based on traditional hybrid video coding frameworks, such as H.264/AVC [64], H.265/HEVC [55], and H.266/VVC [4], as well as deep-learning based video compression (DLVC) methods [11, 18, 21, 29, 33, 36, 66].
*Corresponding author. This work was supported by the National Nat-ural Science Foundation under Grant 62071449 and U20A20184, and the
Fundamental Research Funds for the Central Universities.
Figure 1. BDBR(%) [3] performances of different methods when compared with H.266 on the real-world surveillance video se-quences in terms of PSNR. DVC [36], DCVC [29] and CAN-FVC [18] are three DLVC methods.
Both traditional hybrid video coding frameworks and existing deep learning-based video compression (DLVC) methods follow the same approach of compressing videos by designing various technique modules to reduce spatial and temporal redundancy. In traditional hybrid video cod-ing, each video frame is divided into blocks, and intra- and inter-prediction techniques [63, 72] are used to reduce spa-tial and temporal redundancy. On the other hand, DLVC methods [18, 21, 33, 36], unlike traditional compression methods, use neural networks to design end-to-end intra-and inter-prediction modules for the entire frame. Despite the careful design of these techniques, both traditional and
DLVC methods compress a video sequence progressively in a block-by-block or frame-by-frame style and only use neighboring pixels in the same frame or neighboring frames as reference to derive intra- or inter-prediction values. Since video sequences are captured at high framerates, such as 30fps or 60fps, the same scene may appear in hundreds of frames that are highly correlated in the temporal do-main. However, existing compression strategies are not well-equipped to remove scene redundancy in the block- or frame-level prediction. As demonstrated in Fig. 1, the per-formance of existing state-of-the-art (SOTA) DLVC meth-ods still lags behind that of the traditional H.266 standard.
To overcome the performance bottleneck in video com-pression, this paper proposes an innovative video coding paradigm that seeks to find a compact subspace for a video
to reconstruct video frames from context-agnostic spatial positional embeddings. However, to handle spatial varia-tions between different frames and achieve higher-quality reconstruction results, these methods typically require ad-ditional network parameters (bitrates), which can adversely impact rate-distortion (RD) performance. To address this issue, we propose a context-related spatial positional em-bedding (CRSPE) method in this paper. Additionally, some works [24,25] have proposed frequency-aware operations in their networks to improve the context capturing ability and capture high-frequency image details. However, these op-erations often come with an added cost of network parame-ters that can degrade compression performance. To address this problem and maintain a balance between compression performance and reconstruction quality, we introduce a fre-quency domain supervision (FDS) module that can capture high-frequency details without requiring additional bitrates.
Temporal correlation is a critical factor for INR methods to improve the representation efficiency of different frames.
Existing video INR methods primarily rely on different time positional encodings to distinguish between frames and ex-pect the network to implicitly learn temporal correlation.
While these encodings can capture temporal correlation to some extent, they struggle to explore complex temporal cor-relations, particularly for long video sequences. To address this limitation, we introduce a scene flow constraint mech-anism (SFCM) for short-term temporal correlation and a temporal contrastive loss (TCL) for long-term temporal cor-relation in this paper. These mechanisms do not increase network parameters and are well-suited for the MVC task.
As illustrated in Fig. 1, our proposed framework already outperforms H.266 [4] significantly, indicating the potential of MVC methods. Our main contributions are:
• We propose an MVC that seeks to identify more com-pact sub-spaces for video sequences. Unlike existing methods that rely on explicit spatio-temporal redun-dancy reduction through signal prediction at the block or frame level, our framework uses the correlations be-tween all frames in a video scene simultaneously.
• To address the limitations of existing video INR meth-ods when applied to video compression, we intro-duce CRSPE and FDS in spatial context enhancement, which can handle spatial variations of different frames and capture high-frequency details. We further design
SFCM and TCL for temporal correlation modeling.
• Extensive experiments are conducted on different databases, and detailed analyses are provided for our designed modules. Experimental results show that our proposed method can outperform H.266 (VTM12.0), which demonstrates the superiority of our proposed method and may inspire researchers to explore video compression in a new light.
Figure 2. The performance of existing SOTA video INR methods when applied to the video compression task. sequence of the same scene, rather than reducing spatio-temporal redundancy through block-level or frame-level prediction methods. This paradigm replaces explicit re-dundancy reduction through local prediction with implicit compact subspace modeling for the entire scene. Conse-quently, finding a suitable modeling tool to represent the scene is crucial to this paradigm. Recently, implicit neu-ral representation (INR) has gained popularity for its strong ability to model a wide variety of signals by a deep net-work.
INR has been already applied to various tasks to represent different objects, such as RGB images [52], 3D shapes [45, 52] and scenes [31, 43]. Considering that orig-inal signals can be implicitly encoded in network’s param-eters, some researchers apply the INR to the image com-pression task [13, 14, 54], and achieve competitive perfor-mance compared to traditional image compression method
JPEG2000. Since the compressed bitstream is network’s parameters, these image compression methods can be re-garded as model-based image compression. Due to these characteristics, INR is a promising candidate for the back-bone network of the proposed video compression paradigm.
In contrast to model-based image compression, model-In based video compression (MVC) is barely explored.
MVC, sequence modeling is an extra significant factor, which is the main challenge for video compression. How-ever, the representation ability of the primal video INR methods [7, 32] is limited. If we directly apply these meth-ods to the video compression task, NeRV, is even inferior to traditional video coding standard H.265 [55], as shown in
Fig. 2. This demonstrates that existing SOTA INR methods are unable to achieve higher-quality reconstruction results when given limited network parameters, highlighting the potential for further developments in applying video INR to video compression tasks. In this paper, we further im-prove the sequence modeling ability of video INR in spatial context enhancement and temporal correlation capturing
In spatial context capturing, existing video INR methods, such as those presented in [2,7,32], use a learnable network
Figure 3. The framework of our proposed model-based video compression method. 2.