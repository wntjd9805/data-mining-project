Abstract
Current state-of-the-art results in computer vision de-pend in part on fine-tuning large pre-trained vision mod-els. However, with the exponential growth of model sizes, the conventional full fine-tuning, which needs to store a individual network copy for each tasks, leads to increas-ingly huge storage and transmission overhead. Adapter-based Parameter-Efficient Tuning (PET) methods address this challenge by tuning lightweight adapters inserted into the frozen pre-trained models.
In this paper, we investi-gate how to make adapters even more efficient, reaching a new minimum size required to store a task-specific fine-tuned network. Inspired by the observation that the param-eters of adapters converge at flat local minima, we find that adapters are resistant to noise in parameter space, which means they are also resistant to low numerical precision. To train low-precision adapters, we propose a computational-efficient quantization method which minimizes the quanti-zation error. Through extensive experiments, we find that low-precision adapters exhibit minimal performance degra-dation, and even 1-bit precision is sufficient for adapters.
The experimental results demonstrate that 1-bit adapters outperform all other PET methods on both the VTAB-1K benchmark and few-shot FGVC tasks, while requiring the smallest storage size. Our findings show, for the first time, the significant potential of quantization techniques in PET, providing a general solution to enhance the parameter ef-ficiency of adapter-based PET methods. Code: https:
//github.com/JieShibo/PETL-ViT 1.

Introduction
Large pre-trained vision models have demonstrated ex-ceptional performance on various visual tasks via fine-tuning on task-specific data. In the traditional fine-tuning paradigm, the entire model is updated for each downstream task, resulting in the need to store a fine-tuned model sepa-*Corresponding Author.
Figure 1. Average accuracy vs. size of trainable parameters in backbones (log scale) on VTAB-1K benchmark. Our low-precision adapter-based methods outperform other baselines. rately for each task. However, with the remarkable scalabil-ity of modern vision models, the size of pre-trained vision models is increasing exponentially to achieve superior per-formance. As a result, the storage cost of the full fine-tuning paradigm becomes prohibitive in multi-task scenarios.
Parameter-Efficient Tuning (PET) has recently emerged as a promising approach for fine-tuning a limited number of parameters while attaining performance comparable to full fine-tuning on downstream tasks. Adapter-based meth-ods [5, 18, 19, 23, 24, 38, 43, 45] are among the techniques proposed for PET and have gained considerable attention due to their effectiveness. Adapters are typically small subnetworks with bottleneck architecture comprising two fully-connected (FC) layers inserted into pre-trained mod-els. Adapter-based methods freeze pre-trained weights and update only the adapters, whose parameter efficiency is achieved through their small hidden dimension.
Although the bottleneck adapters have been already lightweight (e.g., 0.5 MB/task for ViT-B [9]), the storage costs remain considerable when dealing with a huge num-ber of tasks (e.g., platform that provides customized models for millions of users). To address this issue, recent stud-ies have shown that the parameter efficiency of adapters can be further improved. For example, [17, 24, 38] explore the low-rank structure in adapters, reparameterizing the weight of adapters into smaller subspace with Kronecker, Tensor-Train, or Tucker factorization. Additionally, [16] leverages network pruning to train sparse adapters. We find that these methods actually have a common motivation – reducing the redundancy (e.g., rank redundancy, density redundancy) in adapters. Also motivated by this, we pose a question, whether there is any other kind of redundancy that can be utilized to better improve the efficiency of adapters.
In this paper, we begin by exploring the loss landscape of adapters and observe that the local minima of adapters are much flatter than that of the fully fine-tuned models. The flatness of local minima indicates that the trained adapters possess greater resilience to noise in parameter space, such that adapters with low-precision parameters should perform equally well as their high-precision counterparts. There-fore, we infer that adapters are redundant in numerical precision. Since previous work on adapters all employs full-precision (FP32) data type, the impact of precision on adapters has not been investigated yet.
To reduce the precision redundancy, we propose an ap-proach that involves training and storing adapters in low-bit parameter space. Through empirical analysis, we observe that the parameters of each adapter weight approximately follow a Gaussian distribution. Under this assumption, we quantize the adapter parameters by minimizing the quan-tization loss. Inspired by previous work of neural network quantization [12], we adopt quantization-aware training and train the low-bit adapters with straight-through estimator (STE). Our experiments, conducted on extensive datasets, reveal several key findings: 1) Unlike quantizing the en-tire model, quantizing only the adapters results in negligible performance degradation, even in the 1-bit setting; 2) With a fixed storage budget, 1-bit quantized (i.e., binary) adapters achieve superior performance among all precision settings; 3) Our 1-bit adapter can outperform all previous PET meth-ods, including low-rank factorization methods, while using the smallest storage size.
Our contributions are summarized as follows:
• From the investigation on the flat local minima of adapters, we infer the existence of precision redun-dancy in the parameters of adapters, which can be leveraged to improve their parameter efficiency.
• Based on empirical observations of the distribu-tion of adapter parameters, we propose an efficient quantization-aware training method for learning low-bit adapters while minimizing the quantization error.
• Extensive experiments and comparisons verify that lowering the bit-width brings significant efficiency im-provement to adapters. Our proposed method achieves new state-of-the-art results in terms of both perfor-mance and parameter efficiency. 2.