Abstract
Current high-performance semantic segmentation models are purely data-driven sub-symbolic approaches and blind to the structured nature of the visual world. This is in stark contrast to human cognition which abstracts visual percep-tions at multiple levels and conducts symbolic reasoning with such structured abstraction. To fill these fundamental gaps, we devise LOGICSEG, a holistic visual semantic parser that integrates neural inductive learning and logic reasoning with both rich data and symbolic knowledge. In particular, the semantic concepts of interest are structured as a hierarchy, from which a set of constraints are derived for describing the symbolic relations and formalized as first-order logic rules.
After fuzzy logic-based continuous relaxation, logical formu-lae are grounded onto data and neural computational graphs, hence enabling logic-induced network training. During in-ference, logical constraints are packaged into an iterative process and injected into the network in a form of several matrix multiplications, so as to achieve hierarchy-coherent prediction with logic reasoning. These designs together make
LOGICSEG a general and compact neural-logic machine that is readily integrated into existing segmentation models.
Extensive experiments over four datasets with various seg-mentation models and backbones verify the effectiveness and generality of LOGICSEG.We believe this study opens a new avenue for visual semantic parsing. 1.

Introduction
Interpreting high-level semantic concepts of visual stimuli is an integral aspect of human perception and cognition, and has been a subject of interest in computer vision for nearly as long as this discipline has existed. As an exemplar task of vi-sual semantic interpretation, semantic segmentation aims to group pixels into different semantic units. Progress in this field has been notable since the seminal work of fully convo-lution networks (FCNs)[1] and been further advanced by the recent launch of fully attention networks (Transformer) [2].
Despite these technological strides, we still observe cur-rent prevalent segmentation systems lack in-depth reflection 1Corresponding author: Wenguan Wang.
Figure 1: (a) We humans abstract our perception in a structured manner, and conduct reasoning through symbol manipulation over such multi-level abstraction. (b) We aim to holistically interpret visual semantics, through the integration of both data-driven sub-symbolic learning and symbolic knowledge-based logic reasoning. on some intrinsic nature of human cognition. First, standard segmentation systems simply assume the semantic concepts in the set of interest have no underlying relation and predict all these concepts exclusively. By contrast, humans interpret a scene by components. For example in Fig. 1, we can ef-fortlessly recognize many pieces of furniture, such as chairs and tables, and identify various utensils, e.g., bottles, and plates. Such capacity of structured under-standing of visual semantics is an innate aspect of human perception [3], complies with our way of the organization of knowledge [4, 5], and has a close relation to many meta-cognitive skills including compositional generalization (i.e., making infinite use of finite means) [6], systematicity (i.e., cognitive capacity comes in groups of related behaviours) [7], and interpretability (i.e., interpreting complex concepts with simpler ones) [8, 9]. Despite its significance and ubiquity, surprisingly little has been done on the computational mod-eling of structured visual perception in the segmentation literature. Though exceptions exist [10–14], in general they are scattered, lacking systematic study. Second, the latest semantic segmentation systems, label structure aware or not, have developed a pure sub-symbolic learning approach. They enjoy the advantages of robust distributed representation of concept entities, but struggle with explicit reasoning with the relations among entities by discrete symbolic represen-tations [15]. Nevertheless, studies in cognition suggest that our perception works at multiple levels of semantic abstrac-tion [16], intertwined with logical reasoning through manip-ulation of symbolic knowledge/concepts[17]. For example, after recognizing many utensils from Fig.1, we know the scene is more likely a kitchen, rather than a bathroom or gym. This judgement comes as a result of reasoning with some abstract knowledge, such as “utensils typically appear in the kitchen” and “utensils are seldom seen in the bath-room,” which are generalized from our daily experience. The judgement of the scene type may become a belief and in turn cause reallocation of our visual attention [18], hence driving us to find out more relevant details, such as small forks.
Filling the gaps identified above calls for a fundamental paradigm shift: i) moving away from pixel-wise ‘flat’ clas-sification towards semantic structure-aware parsing; and ii) moving away from the extreme of pure distributed repre-sentation learning towards an ambitious hybrid which com-bines both powerful sub-symbolic learning and principled symbolic reasoning. To embrace this change, we develop
LOGICSEG, a structured visual parser which exploits neural computing and symbolic logic in a neural-symbolic frame-work for holistic visual semantic learning and reasoning. In particular, given a set of hierarchically-organized semantic concepts as background knowledge and parsing target, we first use first-order logic, a powerful declarative language, to comprehensively specify relations among semantic classes.
After fuzzy logic based relaxation, the logical formulae of hi-erarchy constraints can be grounded on data. During training, each logical constraint is converted into a differentiable loss function for gradient descent optimization. During inference, the logical constraints are involved into an iterative process, and calculated in matrix form. This not only ensures the obser-vance of the compositional semantic structure but also binds logic reasoning into network feed-forward prediction.
By accommodating logic-based symbolic rules into net-work training and inference, our LOGICSEG i) blends sta-tistical learning with symbolic reasoning, ii) obtains better performance, and iii) guarantees its parsing behavior com-pliant with the logically specified symbolic knowledge. We also remark that our study is relevant to a field of research called neural-symbolic computing (NSC) [19–21]. With the promise of integrating two critical cognitive abilities [22]: inductive learning (i.e., the ability to learn general principles from experience) and deductive reasoning (i.e., the ability to draw logical conclusions from what has been learned), NSC has long been a multi-disciplinary research focus and shown superiority in certain application scenarios, such as program generation [23–25], and question answering [26, 27]. This work unlocks the potential of NSC in visual semantic parsing – a fundamental, challenging, and large-scale vision task.
LOGICSEG is a principled framework. It is fully compati-ble with existing segmentation network architectures, with only minor modification to the classification head and a plug-and-play logic-induced inference module. We perform experi-ments on four datasets covering wide application scenarios, including automated-driving (MapillaryVistas 2.0 [28], City-scapes [29]), object-centric (Pascal-Part[30]), and daily (ADE-20K[31]) scenes. Experimental results show that, on the top of various segmentation models (i.e., DeepLabV3+ [32], Mask-2Former [33]) and backbones (i.e., ResNet-101 [34], Swin-T [35]), LOGICSEG yields solid performance gains (1.12%-3.29% mIoU) and suppresses prior structured alternatives. The strong generalization and promising performance of LOG-ICSEG evidence the great potential of integrating symbolic reasoning and sub-symbolic learning in machine perception. 2.