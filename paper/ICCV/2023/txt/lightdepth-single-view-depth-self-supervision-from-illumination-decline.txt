Abstract
Single-view depth estimation can be remarkably effec-tive if there is enough ground-truth depth data for su-there are scenarios, espe-pervised training. However, cially in medicine in the case of endoscopies, where such data cannot be obtained.
In such cases, multi-view self-supervision and synthetic-to-real transfer serve as alterna-tive approaches, however, with a considerable performance reduction in comparison to supervised case.
Instead, we propose a single-view self-supervised method that achieves a performance similar to the supervised case.
In some medical devices, such as endoscopes, the camera and light sources are co-located at a small distance from the target surfaces. Thus, we can exploit that, for any given albedo and surface orientation, pixel brightness is inversely pro-portional to the square of the distance to the surface, pro-viding a strong single-view self-supervisory signal. In our experiments, our self-supervised models deliver accuracies comparable to those of fully supervised ones, while being applicable without depth ground-truth data. 1.

Introduction
Minimally invasive medical procedures such as gastro-scopies, colonoscopies and bronchoscopies rely on endo-scopes that should be as small as possible. As a result, they usually house a single camera and several light points, but neither depth nor stereo cameras. 3D reconstruction is rel-evant in endoscopies, as it may unlock several functional-ities such as the accurate estimation of the size and shape of tumors. However, both single- and multi-view depth es-timation methods present significant challenges in this do-main. The lack of sufficient depth annotated data hinders the use of supervised depth learning. The presence of flu-ids that either obscure the view or generate specularities, the sudden illumination changes, the paucity of texture and the surface deformations hamper multi-view methods both
*equal contribution
Figure 1. Single-view depth self-supervision in LightDepth. A two-headed deep network predicts albedo and depth from a single image and estimates surface normals from predicted depths. These are used to render a new image, that takes into account illumina-tion decline and the endoscope’s photometric calibration, and can be compared to the original one. Minimizing the difference be-tween the original and rendered images is used at training time to compute the network weights and at inference time to refine the depth predictions. for self-supervising deep networks and for geometry esti-mation. Real in-body textures and fluids are hard to simu-late realistically, and the synthetic-to-real gap may be large.
In this work, we propose a novel approach to depth in en-doscopies that overcomes all the above challenges related to depth supervision, multi-view estimation and synthetic-to-real gaps. Our key insight is that, by exploiting a key prop-erty of endoscopic imagery, we can provide strong depth self-supervision signals from just one view. In endoscopes, the light source is rigidly located next to the camera and is close to the surface to be reconstructed. As a result, un-like in traditional shape-from-shading (SfS), points with the same albedo are imaged darker the further they are, being the decrease of intensity a function of the square distance to the light source. To exploit this, we introduce a deep network, as depicted by Figure 1, that estimates depths and albedos from the image, infers normals from depths, and then renders an image while taking into account the atten-uation factor due to the distance between the light source and the surface. At training time, we minimize the differ-ence between the original and rendered images. This en-forces consistency of the depths, normals, and albedos and provides the required self-supervision without depth anno-tations. At inference, we use our trained network to predict depth from a RGB image and then, as our method is to-tally self-supervised, we can perform test-time refinement (TTR) for every monocular image, minimizing the differ-ence between the input and rendered views, further refin-ing the predicted depths. Our quantitative evaluation on a phantom colon dataset, where ground-truth is available, shows that our self-supervised approach delivers results that are very close to that of the best supervised one, and sig-nificantly superior to that of multi-view self-supervision and synthetic-to-real transfer methods. Crucially, we show quantitatively that our method keeps working on real data, for which there is no ground-truth data that can be used for training and self-supervised alternatives underperform. The main specific contributions that led to such results are 1) the inclusion of illumination decline and the endoscope’s photometric calibration in the rendering equation, which provides a strong supervisory signal, and 2) a single-view self-supervised method using such renders, including two-headed network architectures LightDepth U-Net and Light-Depth DPT (see details in Figure 3) that can be trained in large colonoscopy datasets without requiring ground truth labels and even further refined online in the test views. 2.