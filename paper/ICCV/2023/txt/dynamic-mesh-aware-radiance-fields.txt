Abstract
Embedding polygonal mesh assets within photorealistic
Neural Radience Fields (NeRF) volumes, such that they can be rendered and their dynamics simulated in a physically consistent manner with the NeRF, is under-explored from the system perspective of integrating NeRF into the tradi-tional graphics pipeline. This paper designs a two-way cou-pling between mesh and NeRF during rendering and simu-lation. We first review the light transport equations for both mesh and NeRF, then distill them into an efficient algorithm for updating radiance and throughput along a cast ray with an arbitrary number of bounces. To resolve the discrep-ancy between the linear color space that the path tracer assumes and the sRGB color space that standard NeRF uses, we train NeRF with High Dynamic Range (HDR) im-ages. We also present a strategy to estimate light sources and cast shadows on the NeRF. Finally, we consider how the hybrid surface-volumetric formulation can be efficiently integrated with a high-performance physics simulator that supports cloth, rigid and soft bodies. The full rendering and simulation system can be run on a GPU at interactive rates. We show that a hybrid system approach outperforms alternatives in visual realism for mesh insertion, because it allows realistic light transport from volumetric NeRF me-dia onto surfaces, which affects the appearance of reflec-tive/refractive surfaces and illumination of diffuse surfaces informed by the dynamic scene. 1.

Introduction
Creating high-quality 3D environments suitable for pho-torealistic rendering entails labor-intensive manual work carried out by skilled 3D artists. Neural Radiance Fields (NeRF) [46] provide a convenient way to capture a volumet-ric representation of a complex, real-world scene, paving the way for high-quality novel view synthesis and inter-active photorealistic rendering [49]. These qualities make
NeRF exceptionally adept at modeling background environ-ments. On the other hand, existing methods for physically-based simulation and rendering of complex material and lighting effects are primarily based on surface mesh rep-resentations.
Integrating neural field representations with well-established traditional graphics pipelines opens up many possibilities in VR/AR, interactive gaming, virtual tourism, education, training, and computer animation.
Volume rendering [56] has demonstrated its capability to produce visually captivating results for participating me-dia [53]. However, integrating NN-based NeRF into this
*Equal contribution
Figure 2: Light transport on the surface (left) and in the medium (right). pipeline while maintaining realistic lighting effects such as shadows, reflections, refractions, and more, remains a rel-atively unexplored area. In terms of simulation, while the geometry of NeRF is implicit in its density field, it lacks a well-defined surface representation, making it difficult to detect and resolve collisions. Recent works have delved into enhancing the integration between NeRF and meshes, aiming to combine the photorealistic capabilities of NeRF with the versatility of meshes for rendering and simulation.
Neural implicit surfaces [87, 80, 59, 19] are represented as learned Signed Distance Fields (SDF) within the NeRF framework. Meanwhile, methods like IRON [94] and NVD-iffRec [52] extract explicit, textured meshes that are directly compatible with path tracing, offering practical benefits at the expense of a lossy discretization. Nerfstudio [72] ren-ders NeRF and meshes separately, then composites the ren-der passes with an occlusion mask. Unfortunately, this de-coupled rendering approach offers no way to exploit the lighting and appearance information encoded in the NeRF volume to affect the rendered mesh appearance. Figure 6 visually compares our hybrid method to naively combining
NeRF and surface rendering, and pure surface rendering.
We introduce a hybrid graphics pipeline that integrates the rendering and simulation of neural fields and meshes. for both representations, we consider lighting effects and contact handling for physical interaction. By unifying
NeRF volume rendering and path tracing within the linear
RGB space, we discover their Light Transport Equations exhibit similarities in terms of variables, forms, and princi-ples. Leveraging their shared light transport behavior, we devise update rules for radiance and throughput variables, enabling seamless integration between NeRF and meshes.
To incorporate shadows onto the NeRF, we employ differ-entiable surface rendering techniques [28] to estimate light sources and introduce secondary shadow rays during the ray marching process to determine visibility. Consequently, the
NeRF rendering equation is modified to include a point-wise shadow mask.
For simulation, we adopt SDFs to represent geometry of neural fields, which is advantageous for physical contact handling and collision resolution. We then use position-based dynamics [42] for time integration. Our efficient hybrid rendering and simulation system is implemented in
CUDA. To enhance usability, we have also incorporated user-friendly Python interfaces. In summary, the key con-tributions of this work are:
• A two-way coupling between NeRF and surface repre-sentations for rendering and simulation.
• Integration with HDR data which can unify the color space of the path tracer and NeRF, with a strategy to estimate light sources and cast shadows on NeRF.
• An efficient rendering procedure that alternates ray marching and path tracing steps by blending the Light
Transport Equations for both NeRF and meshes.
• An interactive, easy-to-use implementation with a high-level Python interface that connects the low-level rendering and simulation GPU kernels. 2.