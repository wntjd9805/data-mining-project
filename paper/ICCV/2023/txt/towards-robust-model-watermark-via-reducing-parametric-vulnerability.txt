Abstract
Deep neural networks are valuable assets considering their commercial benefits and huge demands for costly an-notation and computation resources. To protect the copy-right of DNNs, backdoor-based ownership verification be-comes popular recently, in which the model owner can watermark the model by embedding a specific backdoor behavior before releasing it. The defenders (usually the model owners) can identify whether a suspicious third-party model is “stolen” from them based on the pres-ence of the behavior. Unfortunately, these watermarks are proven to be vulnerable to removal attacks even like fine-tuning. To further explore this vulnerability, we investigate the parameter space and find there exist many watermark-removed models in the vicinity of the watermarked one, which may be easily used by removal attacks.
Inspired by this finding, we propose a mini-max formulation to find these watermark-removed models and recover their water-mark behavior. Extensive experiments demonstrate that our method improves the robustness of the model watermark-ing against parametric changes and numerous watermark-removal attacks.
The codes for reproducing our main experiments are available at https://github.com/
GuanhaoGan/robust-model-watermarking. 1.

Introduction
While deep neural networks (DNNs) achieve great suc-cess in many applications [20, 9, 39] and bring substan-tial commercial benefits [31, 12, 18], training such a deep model usually requires a huge amount of well-annotated data, massive computational resources, and careful tuning of hyper-parameters. These trained models are valuable as-*Correspondence to: Dongxian Wu (d.wu@k.u-tokyo.ac.jp) and Shu-Tao Xia (xiast@sz.tsinghua.edu.cn). sets for their owners and might be “stolen” by the adversary such as unauthorized copying.
In many practical scenar-ios, such as limited open-sourcing [55] (e.g., only for non-commercial purposes) and model trading1, the model’s pa-rameters are directly exposed, and the adversary can simply steal the model by copying its parameters. How to properly protect these trained DNNs is significant.
To protect the intellectual property (IP) embodied inside
DNNs, several watermarking methods were proposed [45, 10, 35, 5, 29, 49]. Among them, backdoor-based ownership verification is one of the most popular methods [1, 54, 22, 30]. Before releasing the protected DNN, the defender em-beds some distinctive behaviors, such as predicting a pre-defined label for any images with “TEST” (watermark sam-ples) as shown in Figure 4. Based on the presence of these distinctive behaviors, the defender can determine whether a suspicious third-party DNN was “stolen” from the protected
DNN. The more likely a DNN predicts watermark samples as the pre-defined target label (i.e., with a higher watermark success rate), the more suspicious it is of being an unautho-rized copy of the protected model.
However, the backdoor-based watermarking is vulnera-ble to simple removal attacks [34, 41, 16]. For example, watermark behaviors can be easily erased by fine-tuning2 with a medium learning rate like 0.01 (see Figure A17 in
Zhao et al. [56]). To explore such a vulnerability, consid-ering that fine-tuning regards the watermarked model as the start point and continues to update its parameters on some clean data, we investigate how the watermark success rate (WSR) / benign accuracy (BA) changes in the vicinity of the watermarked model in the parameter space. For easier com-parison, we use the relative distance ∥θ − θw∥2/∥θw∥2 in 1People are allowed to buy and sell pre-trained models on platforms like AWS marketplace or BigML. 2While many watermark methods were believed to be resistant to fine-tuning, they were only tested with small learning rates. For example,
Bansal et al. [3] only used a learning rate of 0.001 or even 0.0001.
• We conduct extensive experiments against several state-of-the-art watermark-remove attacks to demon-strate the effectiveness of our method. In addition, we also conduct some exploratory experiments to have a closer look at our method. 2.