Abstract 0
Mask Value 1
A novel continuous-mask-aware transformer for image inpainting, called CMT, is proposed in this paper, which uses a continuous mask to represent the amounts of errors in tokens. First, we initialize a mask and use it during the self-attention. To facilitate the masked self-attention, we also introduce the notion of overlapping tokens. Second, we update the mask by modeling the error propagation dur-ing the masked self-attention. Through several masked self-attention and mask update (MSAU) layers, we predict initial inpainting results. Finally, we reﬁne the initial results to re-construct a more faithful image. Experimental results on multiple datasets show that the proposed CMT algorithm outperforms existing algorithms signiﬁcantly. The source codes are available at https://github.com/keunsoo-ko/CMT. 1.

Introduction
The objective of image inpainting is to reconstruct visu-ally plausible images by ﬁlling in holes or defects in input images, such as old photos with scratches and ﬂawed pho-tos with distracting objects. In inpainting, it is necessary to predict the contents inside holes based on intact regions.
Early inpainting algorithms [3, 15, 25, 9] ﬁll in a hole patch using similar patches in the same image or from an external database, but they may fail to generate detailed patterns.
Recently, convolutional neural networks (CNNs) have been developed for inpainting [24, 30, 12, 32, 36], achiev-ing promising performances. However, they suffer from vi-sual artifacts, such as blurriness, color discrepancies, and artiﬁcial edges, since ordinary convolutional layers are ap-plied to all pixels — both hole and non-hole pixels — in the same manner. To alleviate such artifacts, mask-aware inpainting algorithms based on CNNs have been proposed
[17, 33, 21, 31], in which adaptive ﬁltering is performed to process each pixel according to its state.
With the success of the transformers [5, 20] in vision tasks, transformer-based inpainting algorithms [35, 28, 4, 19] also have been proposed. They provide decent inpaint-*Corresponding author
Mask Update Process
CMT
Input with a Hole
Inpainted Output
Figure 1: Illustration of the mask update process in the proposed
CMT algorithm: The man’s hand is designated as a hole. Initially, in the mask, the hole pixels are assigned 0, while the others 1. The mask is updated gradually through several MSAU layers, and the inpainted image is obtained. ing results, for the global attention facilitates to inpaint holes using the information in distant regions. In particular,
Li et al. [16] developed a mask-aware transformer, which classiﬁes tokens as either valid or invalid using a binary mask. Their algorithm declares an output token as valid if it depends on at least one valid input token. However, this binary masking has a limitation that a valid token can be still erroneous.
In this paper, we propose a novel transformer for image inpainting, called continuously masked transformer (CMT), which uses a continuous mask to represent the amounts of errors in tokens. First, we initialize a continuous mask and use it during the self-attention process. To facilitate the masked self-attention, we employ overlapping tokens, which consist of ordinary and shifted tokens. Then, we up-date the mask by modeling the error propagation during the masked self-attention as illustrated in Figure 1. We gen-erate initial inpainting results through several masked self-attention and mask update (MSAU) layers. Finally, we de-velop the reﬁnement network to reﬁne the initial results to reconstruct a more faithful image. Extensive experiments demonstrate that the proposed CMT algorithm provides ex-cellent inpainting performance.
This work has the following major contributions:
• To the best of our knowledge, CMT is the ﬁrst conti-nuous-mask-aware transformer for image inpainting.
• We also develop a novel mask update scheme by for-mulating the error propagation during the forward pass in the network.
• We introduce the notion of overlapping tokens to fa-cilitate more communication among tokens during the masked self-attention.
• The proposed CMT algorithm signiﬁcantly outper-forms conventional image inpainting algorithms on the
Places2 [40], CelebA-HQ [14], and DTD [2] datasets. 2.