Abstract
Recent work on discrete generative priors, in the form of codebooks, has shown exciting performance for image reconstruction and restoration, as the discrete prior space spanned by the codebooks increases the robustness against diverse image degradations. Nevertheless, these methods require separate training of codebooks for different image categories, which limits their use to specific image cate-gories only (e.g. face, architecture, etc.), and fail to han-In this paper, we propose dle arbitrary natural images.
*kechun@cs.washington.edu
Work done during an internship at SenseBrain.
AdaCode for learning image-adaptive codebooks for class-agnostic image restoration.
Instead of learning a single codebook for each image category, we learn a set of basis codebooks. Given an input image, AdaCode learns a weight map with and computes a weighted combination of these basis codebooks for adaptive image restoration. Intuitively,
AdaCode is a more flexible and expressive discrete genera-tive prior than previous work. Experimental results demon-strate that AdaCode achieves state-of-the-art performance on image reconstruction and restoration tasks, including image super-resolution and inpainting. Codes are released at https://github.com/kechunl/AdaCode. 1
1.

Introduction
In recent years, discrete generative priors (in the form of codebooks) [39, 7] have shown impressive performance for image synthesis [7, 11, 45, 61], exhibiting reduced mode collapse and more stable training. These learned codebooks essentially provide strong priors for compressing and re-constructing natural images, even in the presence of se-vere degradation. Nevertheless, these methods have a com-mon limitation. The codebooks need to be learned sep-arately for each image category (e.g., face, architecture), which restricts their applicability to arbitrary natural im-ages [7, 45, 61]. Although FeMaSR [3] attempted to learn a single general codebook for all image categories, the ex-pressiveness of the codebook is limited by the complexity of natural images. For example, as shown in Fig. 1, an image often includes textural and structural contents from multiple categories (e.g., face, man-made structural edges, repetitive texture, natural texture). It is challenging to rely on a single universal codebook to capture all. Prior work such as VQGAN [7] and FeMaSR [3] often introduce no-ticeable artifacts for image reconstruction and restoration.
Figure 2: Intuition of AdaCode vs a single codebook.
Top: Prior work VQGAN[7] uses a single codebook as the learned representation in the discrete latent space, which may not fully capture complex visual patterns. Bottom:
AdaCode learns multiple basis codebooks, each represent-ing a different discretization of the latent space correspond-ing to different visual appearances. For an input image, its latent representation is a weighted linear combination of the codes from these codebooks. AdaCode thus is a more flex-ible representation for class-agnostic image restoration.
Is it possible to learn a class-agnostic discrete genera-tive prior for image reconstruction and restoration? Inspired by a recent work [55], we propose AdaCode, which learns image-adaptive codebooks for class-agnostic image recon-struction and restoration. Instead of learning a single code-book for all categories of images, we learn a set of basis codebooks. For a given input image, AdaCode learns a weight map that determines the contribution of each ba-sis codebook to the final representation.
Intuitively, this design allows AdaCode to learn a more flexible and ex-pressive discrete generative prior than previous work, as
In contrast to VQGAN [7] and demonstrated in Fig. 2.
FeMaSR [3], which utilize a single partition for the latent space and assign each image feature an exclusive discrete representation, AdaCode learns various partitions to the la-tent space from different perspectives â€“ each corresponding to the learning of one of the basis codebooks. The discrete generative prior for an arbitrary image is a weighted linear combination derived from these basis codebooks, resulting in a more flexible and expressive representation. As de-picted in Fig. 1, AdaCode outperforms previous work in various image restoration tasks, effectively preserving scene structure and texture.
We evaluated AdaCode on both image reconstruction and image restoration tasks (i.e., super-resolution and image inpainting). Across multiple benchmark datasets, AdaCode achieved state-of-the-art performance, while maintaining a comparable codebook size and computational cost. 2.