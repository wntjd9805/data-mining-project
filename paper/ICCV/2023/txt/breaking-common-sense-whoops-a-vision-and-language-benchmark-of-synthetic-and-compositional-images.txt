Abstract
Weird, unusual, and uncanny images pique the curios-ity of observers because they challenge commonsense. For example, an image released during the 2022 world cup de-picts the famous soccer stars Lionel Messi and Cristiano
Ronaldo playing chess, which playfully violates our expec-tation that their competition should occur on the football field.1 Humans can easily recognize and interpret these un-conventional images, but can AI models do the same? We introduce WHOOPS!, a new dataset and benchmark for vi-sual commonsense. The dataset is comprised of purpose-fully commonsense-defying images created by designers us-ing publicly-available image generation tools like Midjour-ney. We consider several tasks posed over the dataset. In addition to image captioning, cross-modal matching, and visual question answering, we introduce a difficult explana-tion generation task, where models must identify and ex-plain why a given image is unusual. Our results show that state-of-the-art models such as GPT3 and BLIP2 still lag behind human performance on WHOOPS!. We hope our dataset will inspire the development of AI models with stronger visual commonsense reasoning abilities. 2 1.

Introduction
Figure 1: We introduce WHOOPS!: a dataset of commonsense-violating images. Designers create interest-ing, unusual images using prompt-based image-generation tasks over tools like Midjourney. We pose several
WHOOPS!, including an explanation generation task. While humans easily identify the weird elements in each image, we show that state-of-the-art AI models struggle.
Upon viewing an unusual image, humans can readily recognize odd, unusual, and incongruent factors. Consider the examples in Fig. 1: smartphones did not exist when Ein-stein was alive (left), and an oxygen-starved candle would not stay lit for long in a sealed bottle (right). While the images consist of “normal” constituent objects, composi-tions make them unusual. Although it’s relatively easy
*Equal contribution. 2Data, models and code are available at whoops-benchmark.github.io/. the project website: for humans to identify/explain why an image is unusual, the multi-step reasoning is sophisticated. Connecting vi-sual cues to knowledge about the world goes beyond ob-ject recognition, and requires commonsense derived from everyday experiences, physical/social knowledge, and cul-tural norms [35, 44, 20, 36].
In this work, we introduce WHOOPS!,3 a dataset of 500 synthetic images and 10,874 annotations designed to chal-3Weird and HeterogeneOus Objects, Phenomena, and Situations.
Figure 2: The WHOOPS! benchmark includes four tasks: 1. generating a detailed explanation for what makes the image weird, 2. generating a literal caption, 3. distinguishing between detailed and underspecified captions, and 4. answering questions that test compositional understanding. Inputs to the models are indicated in dark blue. lenge AI models’ ability to reason about commonsense and compositionality. To construct WHOOPS!, we collaborate with designers who use text-to-image models such as Mid-journey, DALL-E [31] and Stable-Diffusion [32] to gener-ate images that would be challenging (or even impossible) to collect otherwise. First, prompts that contain two plausi-bly co-occurring elements are constructed, and then, a mod-ification to one of them is made to create an implausible combination that violates commonsense. Fig. 1 (left), for example, was created by our designers thinking of a plau-sible scene of Albert Einstein holding a notebook, and then replacing the notebook with a smartphone, which did not exist at the time. We annotate our images with textual in-formation, including both descriptive captions and explana-tions for what makes each image weird.
Next, we pose four visual commonsense reasoning tasks over the WHOOPS! corpus: (1) explanation generation, where models provide detailed explanations of what makes an image weird; (2) image captioning, where models sum-marize the content of the images; (3) cross-modal matching, where models should score a detailed caption higher than a correct but underspecified one, and (4) visual question an-swering, where models answer questions that test their com-prehension of the weird images (Fig. 2). Our evaluation covers both zero-shot and supervised experimental settings.
Experiments on WHOOPS! show that state-of-the-art vision-and-language models (e.g., OFA [41], BLIP [26],
CoCa [43]) lag behind human performance for all tasks.
For instance, a human evaluation reveals that a fine-tuned version of BLIP2-XXL [25] achieves a performance of 27% acceptability, and a “pipeline” approach of feeding a predicted image description to the latest version of GPT3 (davinci-003) [9] reaches 33%. However, both these mod-els fail to generate explanations as well as humans, who achieve 95% on the same task.
To support fully automated evaluations, we present a model-based metric for the explanation-of-violation task.
This involves a GPT4 model on ground-truth explanation and predicted explanation. Achieving an accuracy of over 81%, this metric aligns well with human ratings. We make human annotated data and the complete automatic evalu-ation code publicly accessible. Researchers can evaluate their models and submit the results to the leaderboard on the project website.
Finally, we show that the difficulty WHOOPS! goes beyond recognition; even providing a ground-truth oracle image description instead of the predicted caption in the
”pipelined” setting, models still struggle to effectively ex-plain the incongruity of the scene, with an accuracy rate of only 68%. Overall, our results show that WHOOPS! is a challenging benchmark, even for state-of-the-art vision-and-language models. This result highlights the need for continued development in commonsense reasoning, com-positionality, and explanation generation. We release our models, code, and data. 2. Collecting Weird Images
WHOOPS! is designed to challenge vision-and-language models with images that require commonsense reasoning and understanding beyond simple object co-occurrence.
The term “weird” is ultimately subjective, ambiguous, and culture-dependent. Because our goal is to create a bench-mark, we aim to generate images that are unusual for a diverse set of reasons, including temporal, biological, cul-tural, physical and others. We start by describing how we generate the images, and then present an analysis of the dif-ferent reasons for the images, which shows that our dataset is indeed diverse in this respect. 2.1. Human Generated Synthetic Images via Text-to-Image Models
We recruit a group of 30 image designers who using
Midjourney, DALL-E [31], or Stable-Diffusion [32] as text-to-image models. They are requested to generate weird im-ages by first coming up with “weird” prompts, and editing them until a desired image is generated.
These prompts should adhere to the following guideline: first generate a prompt of an image that depicts two ele-Figure 3: A histogram of the annotated commonsense reasons for WHOOPS! images to be weird. The reasons include a wide range of deviations from expected social norms and everyday knowledge. We also present the explanation generation performance of the two top models in our experiments (Section 5). Left axis is the frequency for each commonsense category, and right is the performance of both models.
Figure 4: WHOOPS! image generation example: This image, produced through more than 25 iterations, demonstrates the process from initial prompt to finalized ’weird’ image produced by the text-to-image model. In each iteration the designer verifies (1) their understanding of the ’weirdness’ concept, (2) absence of any additional weird elements in the image, and (3) the ’weirdness’ clarify to three independent individuals. Only images meeting these criteria are included in the dataset. ments that are likely to co-occur, and then replace one of them with a different element to create a new prompt that describes an image that is unlikely to exist in reality. For instance, taking a prompt of Albert Einstein holding a note-book and replacing the notebook with a smartphone, result-ing in a prompt for an unlikely image, as smartphones did not exist during Einstein’s time. Each image is required to be synthetic, rather than edited from existing images. This results in a total of 500 weird images. See Appendix A.3 for full guidelines and examples.
The generation of each image, as depicted in Fig. 4, goes beyond a simple use of a text-to-image model. It is a meticulous process, managed by expert designers, involv-ing around 25 iterations per image. The primary objectives are to ensure the image clearly portrays its ’weirdness’ con-cept and to eliminate any extraneous elements that could be misunderstood as the main source of ’weirdness.’
For every image, the designers also provide a concise one-sentence explanation that encapsulates the unique rea-son for its ’weirdness.’ The authors review and refine these explanations for factual accuracy and specificity, incorpo-rating relevant details such as names, dates, and other perti-nent information.
This process aims to create images with unmistakable
’weirdness,’ understandable to a wide audience. To verify this, each image is presented to a small control group before its inclusion in the dataset. Images that fail to pass this test are returned to the designers for refinement or to explore alternative concepts. To mitigate the concern that weirdness is limited to a specific culture or region, the designers who created the images and the annotators who labeled the data (Section 4) are from different countries and continents.
2.2. Commonsense Categorization of Weird Images
Fig. 3 provides a histogram of the different types of com-monsense reasoning that underlie the weirdness of images in WHOOPS!. To create it, we manually annotate each im-age with the main reason that contributes to its overall sense of “weirdness”. Our annotation includes 26 different cate-gories. The reasons cover a broad range of domains, includ-ing but not limited to temporal discrepancy (Fig. 1 left), physical rules (Fig. 1 right), nutrition mismatch (Fig. 2), unsuitable environment (Fig. 5), atypical activity (Fig. 6), symbolic inversion, folklore knowledge and more. This analysis shows the diversity and complexity of the reason-ing skills that vision-and-language models must possess in order to perform well on our benchmark. Further elabora-tion on commonsense categories is available in Appendix
A.4, including examples. 3.