Abstract
While the transferability property of adversarial exam-ples allows the adversary to perform black-box attacks (i.e., the attacker has no knowledge about the target model), the transfer-based adversarial attacks have gained great atten-tion. Previous works mostly study gradient variation or image transformations to amplify the distortion on critical parts of inputs. These methods can work on transferring across models with limited differences, i.e., from CNNs to
CNNs, but always fail in transferring across models with wide differences, such as from CNNs to ViTs. Alternatively, model ensemble adversarial attacks are proposed to fuse outputs from surrogate models with diverse architectures to get an ensemble loss, making the generated adversar-ial example more likely to transfer to other models as it can fool multiple models concurrently. However, existing ensemble attacks simply fuse the outputs of the surrogate models evenly, thus are not efﬁcacious to capture and am-plify the intrinsic transfer information of adversarial exam-ples. In this paper, we propose an adaptive ensemble attack, dubbed AdaEA, to adaptively control the fusion of the out-puts from each model, via monitoring the discrepancy ra-tio of their contributions towards the adversarial objective.
Furthermore, an extra disparity-reduced ﬁlter is introduced to further synchronize the update direction. As a result, we achieve considerable improvement over the existing ensem-ble attacks on various datasets, and the proposed AdaEA can also boost existing transfer-based attacks, which fur-ther demonstrates its efﬁcacy and versatility. The source code: https://github.com/CHENBIN99/AdaEA 1.

Introduction
Deep neural networks (DNNs), including convolutional neural networks (CNNs) [10, 36, 15] and vision transform-ers (ViTs) [6, 26, 19], have brought impressive advances to the state-of-the-art across various machine-learning tasks.
At the moment, however, they are found to be vulnerable to
Clean image
Adversarial example
Attack direction
Surrogate attack direction
Decision  boundary
Decision  boundary-1
Decision  boundary-1
Decision boundary  of target model
Decision boundary  of target model
Decision boundary  of target model
Decision boundary-2
Decision boundary-2
)
% (
R
S
A e g a r e v
A
)
% ( 17.66
CNNs 2.02
ViTs
R
S
A e g a r e v
A 18.54 8.74
CNNs
ViTs (a) Transfer-based attack (b) Ensemble attack
)
% (
R
S
A e g a r e v
A 35.98 28.94
CNNs
ViTs (c) AdaEA (Ours)
Figure 1. Overview of different attack schemes and performance. (a) Transfer-based methods strengthen the critical parts in images to improve the attack transferability, but fail to transfer across
DNNs with wide differences due to the limited adversarial in-formation. (b) Model ensemble attacks integrate multiple surro-gate models for ﬁnding the more transferable attack, but exist-ing works generally neglect the individual characteristics of each model, leading to under-optimal results. (c) Our AdaEA performs adaptive ensemble by amplifying the transferable information in each surrogate model and achieves remarkable improvements. adversarial examples [25], i.e., adding imperceptible hand-crafted perturbations to the original inputs can lead to wrong prediction behavior of DNNs. This discovery arises severe security hazards in the deployment of DNNs. More impor-tantly, some well-designed adversarial examples can trans-fer across models. That is, an adversarial example crafted from a surrogate model can also disturb other models. This property of adversarial examples, known as tranferability, allows the adversary to attack a target model without know-ing its interior, thus poses a more realistic threat to black-box applications (i.e., the architectures and parameters are inaccessible to users).
To set up the ﬁrst step for improving model robustness and prevent potential threats from black-box attacks, the re-search on improving the transferability of adversarial exam-ples has attracted wide attention in recent years. The attack            
transfer success rates vary depending on the difference be-tween the surrogate and target models, the more similar the surrogate and target models are, the higher transfer success rate can be achieved. Thus a bunch of works have been proposed to improve the transferability of adversarial exam-ples by maximizing the perturbation on critical parts that are shared among DNNs. The mainstream strategies include maximizing information from important neurons [37, 30], increasing input diversity [32, 1], and incorporating mo-mentum [4, 29] into iterative-based attack. Despite their ef-fectiveness, these methods always fail in transferring across models with wide architecture differences (i.e., CNNs and
ViTs), as shown in Figure 1 (a).
Similar to traditional ensemble methods which draw on the wisdom of multiple weak learners with diverse predic-tions to improve the overall accuracy, a line of research pro-poses to utilize an ensemble of surrogate models to gener-ate adversarial examples that can successfully attack all the surrogate models. Intuitively, the approach can improve the transferability of adversarial examples as it can potentially capture intrinsic transferable adversarial information since the adversary can fool several models with wide differences concurrently. Moreover, such an ensemble could also be easily incorporated with existing transfer-based adversarial attacks without conﬂiction. Several model ensemble based methods have been explored [18, 11], however, most of them only equally fuse the outputs (i.e., logits or losses) of all models to get an ensemble loss for applying gradient-based attack, which may limit the potential capability of the model ensemble attacks, as shown in Figure 1 (b). Although a recent work [33] noticed the gradient variances among the surrogate models, the ensemble is still under-optimal due to the ignorance of individual characteristics of each model.
In this paper, we focus on the model ensemble adver-sarial attack for improving the transferability of adversar-ial examples. We ﬁrst observe that simply averaging the outputs of ensemble models ignore the advantages of each model, where the transferable information captured from one model can be smoothed by another model during the fusion process, thus leading to the under-optimized results.
To cope with this problem, we propose to adptively en-semble the outputs of each model via the adaptive gradi-ent modulation (AGM) strategy. Speciﬁcally, we deﬁne the adversarial ratio to evaluate the contribution discrepancy among the surrogate models to the overall adversarial ob-jective, which is then exploited to adaptively modulate the gradient fusion, offering more efforts on the ampliﬁcation of transferable information in the generated adversarial ex-amples. Moreover, the ensemble gradient may greatly dif-fer or even oppose with the individual gradient of surro-gate models, which has been proven to have a correlation with the overﬁtting problem in ensemble [33]. Hence, we further introduce a disparity-reduced ﬁlter (DRF) where a disparity map is computed to reduce the variances among surrogate models and synchronize the update direction. Fi-nally, the adversarial transferability could be enhanced by applying the above two mechanisms, as demonstrated in
Figure 1. We term the proposed method as adaptive esnem-ble attack (AdaEA), and perform extensive experiments on diverse datasets to validate that our AdaEA can consistently outperform the existing methods. To sum up, the key con-tributions of this work are three-fold:
• We propose an adaptive ensemble adversarial attack, dubbed AdaEA, which offers a more comprehensive ensemble attack for a broad class of models with wide architecture differences, such as CNNs and ViTs.
• Our AdaEA views the ensemble attack from the gra-dient optimization perspective, and controls the opti-mization process via AGM strategy as well as reducing the disparity by DRF to synchronize the optimization direction.
• The proposed AdaEA can not only largely enhance the ensemble effectiveness compared to existing ensemble methods, but also consistently improve the attack per-formance when incorporated with the existing transfer-based gradient attacks. 2.