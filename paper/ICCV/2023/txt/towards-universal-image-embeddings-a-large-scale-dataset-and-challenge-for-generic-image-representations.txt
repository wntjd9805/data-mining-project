Abstract
Fine-grained and instance-level recognition methods are commonly trained and evaluated on specific domains, in a model per domain scenario. Such an approach, how-ever, is impractical in real large-scale applications. In this work, we address the problem of universal image embed-ding, where a single universal model is trained and used in multiple domains. First, we leverage existing domain-specific datasets to carefully construct a new large-scale public benchmark for the evaluation of universal image embeddings, with 241k query images, 1.4M index images and 2.8M training images across 8 different domains and 349k classes. We define suitable metrics, training and evaluation protocols to foster future research in this area.
Second, we provide a comprehensive experimental evalu-ation on the new dataset, demonstrating that existing ap-proaches and simplistic extensions lead to worse perfor-mance than an assembly of models trained for each domain separately. Finally, we conducted a public research com-petition on this topic, leveraging industrial datasets, which attracted the participation of more than 1k teams world-wide. This exercise generated many interesting research ideas and findings which we present in detail. Project web-page: https://cmp.felk.cvut.cz/univ_emb/ 1.

Introduction
The past decade has witnessed significant progress in image representations that are capable of discriminating ob-jects at a fine-grained or instance level. Several techniques
[54, 49, 58, 19] have been demonstrated to learn such im-age embeddings when given data from a specific domain, for example images of different birds or images of differ-ent landmarks. Recently, there has been growing interest in general purpose visual search systems that can identify ob-∗Correspondence: ypsilnik@fel.cvut.cz, francischen@google.com.
Figure 1: Previous work (top) on learning image embeddings has mainly focused on representations that are specialized for narrow domains, such as cars, landmarks, natural world, products, among others. While this may lead to high performance, it cannot scale to meet demands of modern general purpose visual search sys-tems, which are required to identify objects in many domains. In this work, we consider the problem of learning universal image embeddings (bottom), which are representations that can encode fine-grained visual information about multiple domains. We pro-pose the first large-scale dataset for research on universal image embeddings and additionally present results from a public indus-trial challenge in this area. jects from many domains [70, 7, 9]. The use of per-domain models in general-purpose systems is very expensive and generally impractical, since a large number of models would need to be developed and maintained. The holy grail for this kind of application is a unified model that can discriminate fine-grained objects across several domains, which we refer
to as an universal image embedding, as per Fig. 1. Such an universal embedding is a challenging goal as different domains provide different visual cues that are essential for fine-grained and instance-level recognition. It can be seen as an evolution of generalization in training: generaliza-tion beyond training instances in classification with fixed classes, beyond training classes within one domain in an open set problem (such as landmark retrieval), and finally, beyond training classes in multiple domains in the univer-sal embedding problem. We believe that the field of image embedding learning needs to continue moving forward by considering universal representations as a critical direction of future work.
The main reason holding back research explorations on universal embeddings is the lack of a standard, large-scale dataset – only small datasets have so far been proposed
[63, 53], or related medium ones that have been constructed with different objectives [14]. Today, there are no established strategies to train such models, and their effectiveness on industrial applications is not well-studied either. We set out to bridge this gap by introducing the following contributions.
Contributions. (1) The first large-scale dataset for research on universal image embeddings, referred to as Universal
Embedding Dataset (UnED). The dataset contains more than 4M images from 349k classes in 8 different domains, rep-resenting diverse & real use cases: food, cars, online prod-ucts, clothing, natural world, artworks, landmarks and retail products. We leverage already-existing public datasets to construct UnED, carefully combining them into a common format, with standard splits and metrics. (2) A comprehen-sive benchmarking and reference implementations of models for research in this area, highlighting that specialized models on average outperform universal models trained with simple strategies; nevertheless, the universal models achieve promis-ing results and pave the way for further improvements. (3)
The first public competition in this area, the Google Uni-versal Image Embedding Challenge1, focusing on industrial applications, which attracted more than 1k researchers and 21k submissions in total. We report learned lessons from this challenge, which helped open up new research directions. 2.