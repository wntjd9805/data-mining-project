Abstract 1.

Introduction
The success of deep learning models has led to their adaptation and adoption by prominent video understanding methods. The majority of these approaches encode features in a joint space-time modality for which the inner work-ings and learned representations are difficult to visually interpret. We propose LEArned Preconscious Synthesis (LEAPS), an architecture-independent method for synthe-sizing videos from the internal spatiotemporal representa-tions of models. Using a stimulus video and a target class, we prime a fixed space-time model and iteratively optimize a video initialized with random noise. Additional regulariz-ers are used to improve the feature diversity of the synthe-sized videos alongside the cross-frame temporal coherence of motions. We quantitatively and qualitatively evaluate the applicability of LEAPS by inverting a range of spatiotempo-ral convolutional and attention-based architectures trained on Kinetics-400, which to the best of our knowledge has not been previously accomplished. 1 1See alexandrosstergiou/LEAPS for video examples and code.
Inverting deep networks’ learned internal representations has been a difficult task to achieve. The adaptation and de-ployment of CNNs and more recently Transformers, to a variety of vision tasks has led to significant breakthroughs.
The field of video action recognition has experienced dras-tic growth in recent years through the convergence of mod-els with increased complexities and capacities [2, 4, 32, 63] as well as large accuracy improvements [11, 12, 27, 26, 29].
Despite great progress in their applicability, there is still a large gap in the interpretability of video models. As these models compositionally encode space and time modalities of videos over large feature spaces and require a lot of pa-rameters, conceptualizing their internal representations re-mains challenging. In this paper, we propose a method to invert learned features of video models associated with spe-cific actions by optimizing a parameterized input to synthe-size conceptually and visually coherent representations.
In cognitive science, stages of consciousness include the conscious, the unconscious, and the preconscious. In con-trast to the conscious and unconscious, the preconscious is 1
responsible for learned information currently outside con-scious awareness to remain readily available [18]. One way of accessing learned preconscious information and mak-ing it part of conscious awareness is through priming [38].
Priming uses a stimulus to activate related learned con-cepts in memory and make them easily and readily acces-sible [33, 34]; e.g., one can remember their bedroom if primed with a picture of a bed.
Motivated by visual priming in cognitive science, we demonstrate that learned representations of video models can become accessible through model priming. By using a video stimulus and a target action class, we synthesize the dominant learned concepts corresponding to actions. In turn, the visual features of the synthesized videos provide a conceptual view of the models’ learned internal representa-tions. We term these features as the learned preconscious of the video model associated with a specific action.
We introduce LEArned Preconscious Synthesis (LEAPS), illustrated in Figure 1, a spatiotemporal model inversion method that synthesizes interpretable videos by minimizing a classification and priming loss without prior knowledge of the training data. LEAPS uses a video of a target action class as stimulus, to prime a fixed spatiotem-poral model. We additionally include two regularization terms. The first term enforces motion coherence across frames by constraining their representations at each update.
The second enhances the diversity of the synthesized videos by using a domain-specific verifier network that exploits disagreements between feature statistics similar to [64].
Through the same architecture-independent approach, we show that LEAPS can invert the spatiotemporal features of 3D-CNNs and spatiotemporal Transformers.
Our main contributions are as follows. First, we intro-duce LEAPS, a general approach for inverting video mod-els, which to the best of our knowledge, is the first attempt to create videos of conceptual representations from jointly encoded space-time features. Second, we use LEAPS on multiple convolution and attention models and compare it to prior image-based inversion methods that we extend to video. Finally, we show that LEAPS can invert both video CNNs and transformers, with the same architecture-independent method without any modifications. 2.