Abstract
Transformer-based methods have demonstrated impres-sive results on single-image super-resolution (SISR) task.
However, self-attention mechanism is computationally ex-pensive when applied to the entire image. As a result, current approaches divide low-resolution input images into small patches, which are processed separately and then fused to generate high-resolution images. Nevertheless, this conventional regular patch division is too coarse and lacks interpretability, resulting in artifacts and non-similar structure interference during attention operations. To ad-dress these challenges, we propose a novel super token in-teraction network (SPIN). Our method employs superpix-els to cluster local similar pixels to form the explicable lo-cal regions and utilizes intra-superpixel attention to enable local information interaction.
It is interpretable because only similar regions complement each other and dissimi-lar regions are excluded. Moreover, we design a super-pixel cross-attention module to facilitate information prop-agation via the surrogation of superpixels. Extensive ex-periments demonstrate that the proposed SPIN model per-forms favorably against the state-of-the-art SR methods in terms of accuracy and lightweight. Code is available at https://github.com/ArcticHare105/SPIN . 1.

Introduction
Single image super-resolution (SISR) is a crucial task in computer vision that aims to enhance the resolution and vi-sual quality of low-resolution (LR) images. The goal of
SISR is to generate a high-resolution (HR) image from a given LR image, which can be particularly useful in appli-cations where high-quality images are necessary, such as medical imaging, surveillance, and digital photography.
Since the pioneering work of Dong et al. [5], numerous neural networks have been developed to tackle the challenge of reconstructing high-quality images from low-resolution inputs. Some of the CNN-based methods use deeper and
*Corresponding author.
Figure 1. PSNR and model parameters for Ã—4 super-resolution on Set5. We compare our SPIN with state-of-the-art lightweight
Transformer-based and CNN-based models, including SwinIR-light [20], ESRT [26], ELAN-light[43], and IMDN [12], etc. more complex architectures to achieve better performance.
However, these methods come with a trade-off of increased computational resources and higher cost, which can limit their application scenarios.
Attention mechanism [37] has been proven to have sig-nificant effects on both high-level vision tasks and low-level fields, including super-resolution (SR). Attention mecha-nisms allow the network to selectively focus on relevant re-gions of the input, which can improve the quality of the
SR output. Capitalized on attention mechanisms, trans-formers have been applied to SR tasks such as SwinIR [20] and ESRT [26]. These models highlight the importance of global feature extraction abilities in SISR. Furthermore, to improve the efficiency, ELAN [43] proposes a group-wise self-attention module and shared the weights when calculat-ing the association of patches. However, the attention mech-anism has high computational complexity and memory con-sumption, which requires dividing large images into small patches for separate processing. While this strategy en-hances the efficiency of transformer-based models, it results in some problems. Dividing patches based on a fixed shape leads to the splitting up of continuous structures, which hin-ders the use of similar information in other areas to enhance image details. Moreover, the local attention mechanism ap-plied within each patch involves irrelevant regions in com-putation, leading to undesirable inferences.
To address these issues, we propose a novel approach that integrates local and global attention mechanisms with fine superpixel partition. We start with CNN-based shallow feature extraction on the pixels of the input image and per-form local clustering to group adjacent pixels into superpix-els. We then obtain local regions by clustering superpixels based on similarity and perform local feature extraction on them separately. Unlike the previous approaches [20, 43] using fixed shape patch division, which was only used for improving parallel computation efficiency, our strategy for region division is more interpretable, allowing for more flexible and adaptive division of the input image, and pre-venting the splitting up of continuous structures. We then introduce Superpixel Cross Attention module to enable in-formation interaction in the long-range via the surrogation of superpixels. Furthermore, we design an Intra-Superpixel
Attention (ISPA) mechanism applied to the pixels of super-pixels, extending the original attention operation only in the regular image area. This ensures that local attention mech-anism information interactions occur in similar areas, elim-inating interference and irrelevant computation. These two proposed attention mechanisms interlace with each other and cooperate in local and global feature extraction. As shown in Fig. 1, the proposed SPIN has a good trade-off between PSNR and model size.
Our contributions are summarized below: (a) We present a novel super-resolution model that com-bines superpixel clustering with the transformer structure, resulting in a more interpretable framework. (b) We propose Intra-Superpixel Attention (ISPA) and
Superpixel Cross Attention (SPCA) modules that operate within and between superpixels, enabling computation in irregular areas while maintaining the ability to capture long-range dependencies. (c) The experiments demonstrate that the proposed method achieves better SR reconstruction performance compared to state-of-the-art lightweight SR methods. 2.