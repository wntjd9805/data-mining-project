Abstract
Large-scale text-to-image diffusion models can gener-ate high-fidelity images with powerful compositional ability.
However, these models are typically trained on an enormous amount of Internet data, often containing copyrighted ma-terial, licensed images, and personal photos. Furthermore, they have been found to replicate the style of various living artists or memorize exact training samples. How can we remove such copyrighted concepts or images without retrain-ing the model from scratch? To achieve this goal, we propose an efficient method of ablating concepts in the pretrained model, i.e., preventing the generation of a target concept.
Our algorithm learns to match the image distribution for a target style, instance, or text prompt we wish to ablate to the distribution corresponding to an anchor concept. This prevents the model from generating target concepts given its text condition. Extensive experiments show that our method can successfully prevent the generation of the ablated con-cept while preserving closely related concepts in the model. 1.

Introduction
Large-scale text-to-image models have demonstrated re-markable ability in synthesizing photorealistic images [51, 43, 56, 54, 76, 14]. In addition to algorithms and compute resources, this technological advancement is powered by the use of massive datasets scraped from web [59]. Unfortu-nately, the datasets often consist of copyrighted materials, the artistic oeuvre of creators, and personal photos [64, 10, 61].
We believe that every creator should have the right to opt out from large-scale models at any time for any image they have created. However, fulfilling such requests poses new computational challenges, as re-training a model from scratch for every user request can be computationally inten-sive. Here, we ask – How can we prevent the model from generating such content? How can we achieve it efficiently without re-training the model from scratch? How can we make sure that the model still preserves related concepts?
These questions motivate our work on ablation (removal) of concepts from text-conditioned diffusion models [54, 3].
We perform concept ablation by modifying generated images for the target concept (c∗) to match a broader anchor con-cept (c), e.g., overwriting Grumpy Cat with cat or Van Gogh paintings with painting as shown in Figure 1. Thus, given the text prompt, painting of olive trees in the style of Van Gogh, generate a normal painting of olive trees even though the text prompt consists of Van Gogh. Similarly, pre-vent the generation of specific instances/objects like Grumpy
Cat and generate a random cat given the prompt.
Our method aims at modifying the conditional distribu-c∗) to match tion of the model given a target concept pΦ(x
| c) defined by the anchor concept c. This a distribution p(x
| is achieved by minimizing the Kullback–Leibler divergence between the two distributions. We propose two different tar-get distributions that lead to different training objectives. In the first case, we fine-tune the model to match the model prediction between two text prompts containing the target and corresponding anchor concepts, e.g., A cute little
Grumpy Cat and A cute little cat. In the second objec-c) is defined by the tive, the conditional distribution p(x
| modified text-image pairs of: a target concept prompt, paired with images of anchor concepts, e.g., the prompt a cute little Grumpy Cat with a random cat image. We show that both objectives can effectively ablate concepts.
We evaluate our method on 16 concept ablation tasks, including specific object instances, artistic styles, and mem-orized images, using various evaluation metrics. Our method can successfully ablate target concepts while minimally af-fecting closely related surrounding concepts that should be preserved (e.g., other cat breeds when ablating Grumpy
Cat). Our method takes around five minutes per concept.
Furthermore, we perform an extensive ablation study re-garding different algorithmic design choices, such as the objective function variants, the choice of parameter sub-sets to fine-tune, the choice of anchor concepts, the number of fine-tuning steps, and the robustness of our method to misspelling in the text prompt. Finally, we show that our method can ablate multiple concepts at once and discuss the current limitations. The full version of the paper is avail-able at https://arxiv.org/abs/2303.13516. Our code, data, and models are available at https://www.cs.cmu.edu/
˜concept-ablation/. 2.