Abstract
Video snapshot compressive imaging (SCI) aims to cap-ture a sequence of video frames with only a single shot of a 2D detector, whose backbones rest in optical modulation patterns (also known as masks) and a computational recon-struction algorithm. Advanced deep learning algorithms and mature hardware are putting video SCI into practical applications. Yet, there are two clouds in the sunshine of
SCI: i) low dynamic range as a victim of high temporal mul-tiplexing, and ii) existing deep learning algorithms’ degra-dation on real system. To address these challenges, this paper presents a deep optics framework to jointly optimize masks and a reconstruction network. Speciﬁcally, we ﬁrst propose a new type of structural mask to realize motion-aware and full-dynamic-range measurement. Considering the motion awareness property in measurement domain, we develop an efﬁcient network for video SCI reconstruction using Transformer to capture long-term temporal depen-dencies, dubbed Res2former. Moreover, sensor response is introduced into the forward model of video SCI to guaran-tee end-to-end model training close to real system. Finally, we implement the learned structural masks on a digital micro-mirror device. Experimental results on synthetic and real data validate the effectiveness of the proposed frame-work. We believe this is a milestone for real-world video
SCI. The source code and data are available at https:
//github.com/pwangcs/DeepOpticsSCI. 1.

Introduction
Capturing high-dynamic-range (HDR) and high-frame-rate (HFR) video is a long-term challenge in the ﬁeld of computational photography. As an elegant solution of HFR, video snapshot compressive imaging (SCI) optically multi-plexes a sequence of video frames, each of which is coded with a distinct modulation pattern (hereafter called mask), into a snapshot measurement of a two-dimensional (2D) de-tector, and computationally reconstructs a decent estimate
*Corresponding author. (cid:11)(cid:68)(cid:12) (cid:11)(cid:70)(cid:12) 37
P
S
N
R ( d
B
) 36.5 36 35.5 35 (cid:19) (cid:11)(cid:69)(cid:12)
Under
Under our framework framework our (36.69 dB)
L
Large Res2former
Large Res2former (36.56 dB)
Res2former f2f2RR (35.98 dB) (
)
STFormer
FFSTS (36.34 dB) 1000 2000
FLOPs (G) (cid:11)(cid:71)(cid:12) 3000
Figure 1. The proposed deep optics framework brings a signif-icant improvement for real-world video SCI as demonstrated in real results (a), (b), and (c), got by previous SOTA STFormer [34], current STFormer (under our framework), and our Res2former, respectively. (d) summarizes the comparison between Res2former and STFormer in terms of PSNR (vertical axis), FLOPs (horizon-tal axis), and Parameters (circle radius). The proposed Res2former achieves competitive performance (35.98 dB) with only 28.15%
FLOPs and 56.57% parameters of STFormer (36.34 dB). By in-creasing parameters to STFormer’s level, large Res2former can lead to a better performance (36.56 dB). By the way, STFormer under our framework can increase by 0.35 dB. of the original video from the measurement using an ad-vanced algorithm. In a nutshell, video SCI is a hardware-encoder-plus-software-decoder system and its performance mainly depends on mask and reconstruction algorithm.
For the hardware encoder, random binary mask has been widely used in both simulation and real video SCI sys-tems, often implemented in a digital micro-mirror device (DMD) [28, 27] or liquid crystal on silicon (LCOS) [29, 9, 16]. Recently, learned binary mask was also imple-mented in programmable pixel sensors [21]. For the soft-ware decoder, it is an ill-posed inverse problem to retrieve high-ﬁdelity video from the captured single measurement and various reconstruction methods [41, 17, 18, 28, 5, 35, 4, 37, 34, 23] have been developed to solve it in recent years. Conventional optimization algorithms adopt hand-crafted priors, e.g., total variation [41] and non-local self- 
similarity [17], to conﬁne the solution to the desired sig-nal space. But optimization-based methods commonly re-quire a long running time to get usable results. With the powerful generalization ability of deep neural networks (DNNs), deep learning methods have been increasingly developed and achieved excellent results in a little infer-ence time, usually designed as an end-to-end (E2E) net-work, e.g., E2E-CNN [28], BIRNAT [5], MetaSCI [35],
RevSCI [4], STFormer [34], or a deep unfolding network, e.g., GAP-net [23], ADMM-Net [18], SCI3D [37], ELP-Unfolding [39]. Despite these remarkable advances, par-ticularly in deep learning reconstruction methods, there are still some practical challenges in putting video SCI into our daily life. (cid:2)
Due to the limited bit depth of image sensors, the higher temporal multiplexing, the lower dynamic range. For an video SCI camera using random binary masks, the mea-surable brightness values of video frames is approximately equal to 2κ+1
B, far less than the available brightness val-ues of image sensor 2κ, where B (usually 8 ≤ B ≤ 50) and
κ denote compressed frames and sensor bit depth, respec-tively. We take 8-frame video SCI camera equipped with a typical 8-bit-depth image sensor as an example, namely, 8 video frames are compressed into a single image with 256 available brightness values during measurement. If using random binary masks that take values of ‘1’ or ‘0’ with equal probability, at each spatial position, half of 8 frames are integrated into one pixel along temporal dimension with high probability. In this case, each frame can only be rep-resented by 64 brightness values, which is calculated by 256/4 = 64. Obviously, there is a signiﬁcant gap between the wide range of brightness variations in natural scenes and the very limited dynamic range in previous video SCI. Such a practical problem is also widely rooted in other compres-sive imaging systems, e.g., spectral SCI [8], compressive light ﬁeld imaging [22], and single-pixel imaging [6].
Without considering sensor response, existing deep re-construction networks have a great performance degrada-tion when used in real system. As is well known, the per-formance of DNNs is closely related to the used training dataset. Without available specialized datasets, the forward model of video SCI usually need to be mathematically for-mulated to synthesize the training dataset from a public
HFR video dataset. Accordingly, deep reconstruction net-works have a high dependence on the forward model. Un-fortunately, previous forward model only considers optical transmission and modulation but overlooks sensor response characterizing the used image sensor, meaning that there is a gap between previous forward model and real system. As a result, previous deep reconstruction networks show ex-cellent performance on synthetic data but degraded perfor-mance on real data.
To address the above challenges, a deep optics frame-work is proposed to improve the performance of real-world video SCI. The contributions of this work are summarized as follows.
• Unlike widely-used random binary mask, a new type of structural mask is presented to realize motion-aware and full-dynamic-range (FDR) measurement. Motion-aware measurement contributes to video SCI recon-struction. To our best knowledge, we are the ﬁrst to enable FDR video SCI.
• Considering the motion-aware property in the encoder, we tailor an efﬁcient reconstruction network, dubbed
Res2former, as the video SCI decoder by using Trans-former to capture long-term temporal dependencies.
Compared with the state-of-the-art (SOTA) network
STFormer [34], Res2former is highly lightweight but provides competitive performance.
• We propose a deep optics framework to jointly opti-mize the proposed structural mask and reconstruction network, in which sensor response is introduced to guarantee end-to-end (E2E) training close to real sys-tem. Under this framework, Res2former and previous reconstruction networks achieve signiﬁcant improve-ment on synthetic data and real data. 2.