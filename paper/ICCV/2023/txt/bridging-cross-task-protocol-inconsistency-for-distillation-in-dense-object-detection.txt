Abstract
Knowledge distillation (KD) has shown potential for learning compact models in dense object detection. How-ever, the commonly used softmax-based distillation ignores the absolute classification scores for individual categories.
Thus, the optimum of the distillation loss does not neces-sarily lead to the optimal student classification scores for dense object detectors. This cross-task protocol inconsis-tency is critical, especially for dense object detectors, since the foreground categories are extremely imbalanced. To ad-dress the issue of protocol differences between distillation and classification, we propose a novel distillation method with cross-task consistent protocols, tailored for the dense object detection. For classification distillation, we address the cross-task protocol inconsistency problem by formulat-ing the classification logit maps in both teacher and stu-dent models as multiple binary-classification maps and ap-plying a binary-classification distillation loss to each map.
For localization distillation, we design an IoU-based Lo-calization Distillation Loss that is free from specific net-work structures and can be compared with existing local-ization distillation losses. Our proposed method is sim-ple but effective, and experimental results demonstrate its superiority over existing methods. Code is available at https://github.com/TinyTigerPan/BCKD. 1.

Introduction
Recent progress in dense object detectors has yielded significant performance improvements in the object detec-*The first two authors contributed equally to this paper.
†Corresponding author.
Figure 1. (a) In dense object detection, different samples exhibit inter-sample differences in their classification score sums on vari-ous positions on dense maps, which is significantly different from those in image classification. (b) The cross-task protocol inconsis-tency problem arises in dense object detection due to the mismatch between Sigmoid protocol used in this task and Softmax proto-col used in classification distillation. Specifically, when classifica-tion distillation loss equals 0, inconsistencies emerge between the scores of the student and teacher models in dense object detection. tion task [25, 29, 18, 17, 33]. However, the high compu-tational burden of existing detection methods poses a sig-nificant challenge for deployment on resource-constrained devices. To address this problem, knowledge distillation (KD) [13, 5, 11, 35, 36, 4, 15, 22, 47, 6, 40] has emerged as a promising approach to compress models. The KD frame-work involves training a smaller student model by leverag-ing a larger and more capable teacher model, for enhancing the student model’s generalization ability.
Knowledge distillation approaches can be roughly clas-sified into two categories: feature-based distillation meth-ods [27, 37, 23, 1, 23, 4, 36] and logit-based distilla-tion methods [13, 43, 34, 47].
In object detection, exist-ing knowledge distillation methods have focused primar-ily on feature-based distillation due to the marginal per-formance gain from original logit-based distillation tech-niques [14, 32, 38]. However, it is worth exploring logit-based methods as they are usually simpler to use and have the potential to further improve performance when com-bined with feature-based methods. LD [47] is a repre-sentative logit-based distillation technique that transforms bounding boxes into probability distributions to facilitate localization distillation. However, classification distillation in dense object detection remains a challenge.
In this work, we further investigate this problem. Fig-ure 1 (a) demonstrates that dense object detection faces a se-vere foreground-background imbalance problem when pre-dicting classification scores on dense maps. Consequently, dense object detectors typically use the Sigmoid protocol to transfer classification logits to classification scores, which results in the position-aware inter-sample difference: Sam-ples closer to positive sample regions generate higher clas-sification score sums across all categories, indicating inter-sample differences. However, common classification dis-tillation methods [13, 43, 34, 47] directly use the Softmax protocol from image classification to transfer classification logits to classification scores. The Softmax protocol nor-malizes classification scores, ignoring the absolute clas-sification scores for individual categories and eliminating the inter-sample difference characteristic of classification scores. Additionally, in distillation, classification scores for each category are jointly optimized with inter-class depen-dencies, while in dense object detection, they are individ-ually optimized without such dependencies. These differ-ences lead to the cross-task protocol inconsistency prob-lem, as shown in Figure 1 (b): when the teacher scores are equal to the student scores after Softmax, the classifica-tion distillation loss is 0, indicating that the student scores have achieved the optimal solution in the distillation loss.
However, after Sigmoid, the student scores still differ from the teacher scores, showing lower score sums and incorrect inter-class relationships.
In addition to classification, localization is another cru-cial aspect of the object detection task. Although the lo-calization distillation loss in LD [47] has demonstrated effectiveness, it requires the use of a Discrete Position-probability Prediction Head, such as the Generalized Focal
Loss Head [17], for accurately predicting the localization probability distribution of each sample. Unfortunately, cur-rent object detectors [25, 29, 18] commonly use a Continu-ous Box-Offset Prediction Head, which means that the use of LD [47] would require specific training of teacher models to incorporate the Discrete Position-probability Prediction
Head. This constraint limits the applicability of LD [47].
To address these issues outlined above, this paper pro-poses two novel distillation losses, Binary Classification
Distillation Loss and IoU-based Localization Distillation
Loss, tailored for classification and localization in dense object detectors. For classification, we convert cross-task inconsistent protocols into cross-task consistent protocols.
Specifically, we treat the classification logit maps used in dense object detectors as K (i.e., the number of categories) binary-classification maps. Then, we use the Sigmoid pro-tocol to obtain scores and apply a binary cross entropy loss to distill each binary-classification map from teacher to student models, effectively solving the cross-task pro-tocol inconsistency problem. For localization, we con-vert the special-structure-dependent localization distilla-tion loss into a special-structure-free localization distilla-tion loss. Specifically, we directly compute the Intersection over Unions (IoUs) between predicted bounding boxes gen-erated by the teacher and student models and employ the
IoU loss to minimize the difference between the IoU values and 1 (i.e., the maximal IoU). Our approach is evaluated on widely used COCO [19] dataset, and our experimental results demonstrate that our method outperforms existing logit-based distillation methods and further boosts the ex-isting feature-based distillation methods. Our contributions are summarized as follows: (i) We identify the cross-task protocol inconsistency problem as the primary obstacle in utilizing original clas-sification distillation techniques for dense object detection.
The proposed Binary Classification Distillation Loss greatly enhances the performance gains obtained through classifi-cation distillation in dense object detection. We show that transferring semantic knowledge (i.e., classification) alone can be effective in dense object detection, beyond common views in previous work. (ii) We propose the IoU-based Localization Distillation
Loss to distill the localization knowledge from teacher mod-els to student models, which eliminates the need for specific training of teacher models. (iii) Our proposed method is simple but effective, as demonstrated by our experiments. Besides, our method ex-hibits flexibility in integrating with existing state-of-the-art methods, resulting in a consistent performance increase. 2.