Abstract
Visual Place recognition is commonly addressed as an image retrieval problem. However, retrieval methods are impractical to scale to large datasets, densely sampled from city-wide maps, since their dimension impact negatively on the inference time. Using approximate nearest neighbour search for retrieval helps to mitigate this issue, at the cost of a performance drop. In this paper we investigate whether we can effectively approach this task as a classification problem, thus bypassing the need for a similarity search. We find that existing classification methods for coarse, planet-wide localization are not suitable for the fine-grained and city-wide setting. This is largely due to how the dataset is split into classes, because these methods are designed to handle a sparse distribution of photos and as such do not consider the visual aliasing problem across neighbouring classes that naturally arises in dense scenarios. Thus, we propose a partitioning scheme that enables a fast and accu-rate inference, preserving a simple learning procedure, and a novel inference pipeline based on an ensemble of novel classifiers that uses the prototypes learned via an angular margin loss. Our method, Divide&Classify (D&C), enjoys the fast inference of classification solutions and an accuracy competitive with retrieval methods on the fine-grained, city-wide setting. Moreover, we show that D&C can be paired with existing retrieval pipelines to speed up computations by over 20 times while increasing their recall, leading to new state-of-the-art results. Code is available at https:
//github.com/ga1i13o/Divide-and-Classify 1.

Introduction
Visual Place Recognition (VPR) is the task of recogniz-ing the location where a photo was taken with an accuracy of a few meters [45, 1, 25, 44, 29, 11, 13, 5, 18, 51, 32, 15, 46, 54, 6, 8, 7, 12, 24, 16]. This problem, also known as visual geo-localization [3, 4, 25] or image localization
[29, 13], is commonly approached as an image retrieval
Figure 1. Experiments demonstrating the scalability problem of retrieval-based VPR methods, using the state-of-the-art CosPlace (with exhaustive kNN and with Approximate Nearest Neighbor -ANN - search through Inverted File Index with Product Quanti-zation, IVFPQ). Combining our Divide&Classify method with the retrieval approach yields an optimal performance-efficiency trade-off when scaling up to the city-wide setting. problem: the query to be localized is compared to a database of geo-tagged images, typically via a k-nearest neighbour (kNN) in features space, and the most similar images re-trieved from the database are the predictions of the query’s location. Even though retrieval methods work remarkably well when the VPR task is limited to a small map [4, 54, 3], scaling them to large and densely mapped areas, such as an entire city, is impractical because both the time and memory required to execute the kNN grow with the dimension of the database [54, 4, 36, 2].
Certainly, the space and time requirements of the kNN can be reduced by resorting to Approximate Nearest Neigh-bors (ANNs) algorithms [22, 30, 2, 40], which grant im-pressive speed ups while simultaneously reducing memory footprint. However, their accuracy may be significantly worse than the one obtained with the exhaustive kNN, which is their upper bound. This trade-off is easily demon-strated with an experiment. In Fig. 1 we show the behav-ior obtained using the state-of-the-art retrieval method Cos-Place [3] on SF-XL [3], a dataset for urban VPR that cov-ers the whole city of San Francisco, with an area of nearly
170km2 and over 40M images. Starting from a reduced version of SF-XL and gradually increasing the size of the database up to its full version, we can observe that the in-ference time quickly explodes with the exhaustive kNN. On the other hand, with an ANN the inference time grows much more slowly (yet still linearly w.r.t. the size of the database), at the cost of a big drop in accuracy.
A different route to address the scalability problem is to frame VPR as a classification task, so that predictions of the query’s location can be obtained without a similar-ity search over a database. So far, this formulation has been applied in the setting of planet-wide, coarse geolocal-ization [34, 52, 39, 27]. All these methods are designed to divide the globe in very coarse classes, spanning up to hundreds of kilometers each and following the sparse and uneven distribution of photos in the planet. We ques-tion whether these classification approaches can be adapted to the fine-grained city-scale VPR setting, where the re-quired localization accuracy is in the order of a few me-ters. We find that the global-scale classification meth-ods [34, 52, 39, 27], albeit being much faster than their re-trieval counterparts, are unsuitable for this setting because they are far too imprecise and because they do not account for the visual overlap among classes that arises when the photos are densely sampled from the map. Therefore, we propose a novel classification-based VPR technique that is specifically designed for the dense and homogeneous con-figurations of large urban areas. Our method, called Di-vide&Classify (D&C), has the speed advantage of classi-fication approaches while being competitive with retrieval methods in terms of accuracy. Most importantly, we demon-strate that the predictions from D&C can be used to restrict the search space of retrieval methods, combining both in a unique pipeline with more accurate results than previous works and a faster (and constant) inference time (see Fig. 1).
We also show that such a combination of classification and retrieval is not effective with currently viable classification methods for planet-wise geo-localization due to their lack of accuracy.
Contributions. To summarize our contributions:
• We are the first to tackle the problem of fine-grained (error ≤ 25m) and city-wide (map area >100 km2)
VPR through classification, demonstrating the inade-quacy of existing global-scale approaches in this set-ting and proposing a first feasible solution (D&C).
• In D&C we propose a new classifier, named Additive
Angular Margin Classifier (AAMC), which uses the learned prototypes from a Additive Angular Margin
Loss to classify new images. The AAMC is scalable and produces remarkably robust results.
• We show that our method not only achieves compet-itive results with retrieval methods, but above all it can be combined to create a fast, scalable and accurate
Figure 2. Architecture of Divide&Classify.The different groups, and their relative classifiers, are color coded. The picture expli-cates how cells are distributed into groups. pipeline that harnesses the best of both worlds.
Code and trained models will be made publicly available upon acceptance of this paper. 2.