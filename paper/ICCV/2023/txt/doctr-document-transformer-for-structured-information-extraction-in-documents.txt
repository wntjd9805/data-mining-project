Abstract
We present a new formulation for structured information extraction (SIE) from visually rich documents. We address the limitations of existing IOB tagging and graph-based for-mulations, which are either overly reliant on the correct ordering of input text or struggle with decoding a complex graph. Instead, motivated by anchor-based object detectors in computer vision, we represent an entity as an anchor word and a bounding box, and represent entity linking as the as-sociation between anchor words. This is more robust to text ordering, and maintains a compact graph for entity linking.
The formulation motivates us to introduce 1) a Document
Transformer (DocTr) that aims at detecting and associating entity bounding boxes in visually rich documents, and 2) a simple pre-training strategy that helps learn entity detection in the context of language. Evaluations on three SIE bench-marks show the effectiveness of the proposed formulation, and the overall approach outperforms existing solutions. 1.

Introduction
Structured information extraction (SIE) from documents, as shown in Fig 1, is the process of extracting entities and their relationships, and returning them in a structured format.
Structured information in a document is usually visually-rich – it is not only determined by the content of text but also the layout, typesetting, and/or ﬁgures and tables present in the document. Therefore, unlike the traditional information ex-traction task in nature language processing (NLP) [8, 3, 30] where the input is plain text (usually with a given reading order), SIE assumes the image representation of a document is available, and a pre-built optical character recognition (OCR) system may provide the unstructured text (i.e., with-out proper reading order). This is a practical assumption for day-to-day processing of business documents, where the
∗Corresponding author liahaofu@amazon.com
†Work done at AWS AI Labs
Figure 1: Structured information extraction problem formulations.
Given an input image, we aim to extract each entity (e.g., name, count, or price) and link the related entities together. To ad-dress this task, (a) IOB tagging [29] assigns a tag to each word to indicate if it is the beginning (B-), inside (I-), end (E-) of an entity or a single (S-) word entity. (b) Graph based methods [15] take each word as a node, and use edges between words to indicate that the words belong to the same entity (purple edges) or the underlying entities are linked (red edges). A graph is generated by decoding from two adjacency matrices (one for each type of edges). (c)
Our formulation represents an entity as an anchor word (colored words) and a box (colored bounding boxes), and represents entity linking via anchor word association (red arrows). documents are usually stored as images or PDFs, and the structured information, such as key-value pairs or line items (see Fig. 2) from invoices and receipts, has been primarily obtained manually. This is time consuming and does not scale well. Hence, automating the document structured in-formation extraction process with efﬁciency and accuracy is of great practical and scientiﬁc importance.
Structured information extraction is part of document intelligence [5], which focuses on the automatic reading, understanding, and analysis of documents. Early approaches to document intelligence usually address the problem purely
from either a computer vision or an NLP perspective. The former takes the document as an image input and frames entity detection as object detection or instance segmenta-tion [41, 31]. The latter takes only the textual content of a document as the input, and addresses the problem with NLP solutions, such as IOB tagging via transformers [14].
Recently, models have also been proposed to pre-train on large-scale document collections and apply them to a wide variety of downstream document intelligence prob-lems [38, 11, 1, 20]. Such general-purpose models usually have the ability to make use of multi-modal inputs – text from OCR, layout in the form of text locations, and visual features from images, and pre-training enables them to under-stand the basic structure of documents. Therefore, general-purpose models have demonstrated signiﬁcant improvements on multiple document intelligence tasks, such as entity ex-traction [11, 20], document image classiﬁcation [38, 1], and document visual question and answering [39, 1].
For structured information extraction, existing general-purpose models rely on two broad approaches: 1) IOB tag-ging [29] based methods [38, 39, 20], and 2) graph based methods [11, 15]. Both of these approaches suffer from in-herent limitations. IOB tagging relies on the correct “reading order” or serialization of text, which however is not given by the OCR. As shown in Fig. 1(a), the raster scan order of OCR text separates I-name and E-name. When there are multiple name entities, it could be non-trivial to know which I-name/E-name word belongs to which name en-tity. Graph-based methods (Fig. 1 (b)) can result in complex graphs with many words in a document (i.e., many nodes in the graph). Therefore, decoding the entities and their relationships from the adjacency matrices is error-prone.
Given the limitations of existing work, we make the fol-lowing contributions in this paper:
• We introduce a new formulation for SIE where we rep-resent an entity as an anchor word along with a box, and regard the problem as an anchor word based entity de-tection and association problem (Fig 1 (c)). Thus, we extract entities via bounding boxes and do not depend on the reading order of input. We assign each entity with an anchor word, resulting in a compact graph of entity relations (e.g., the anchor word links in Fig 1 (c)), which facilitates decoding structured information.
• We develop a new model, called Document Transformer (DocTr), which combines a language model and visual object detector for joint vision-language document under-standing. We note that the recognition of an anchor word is largely a language-dependent task, while the detection of entity boxes is a more vision-dependent task. There-fore, DocTr is an intuitive approach to target this problem under the proposed formulation.
• We propose a new pre-training task, called masked detec-tion modeling (MDM), that matches our formulation and helps learn box prediction in the context of language. Our experimental results show that 1) the proposed formula-tion addresses SIE better than IOB tagging or graph-based solutions, 2) MDM is a more effective pre-training task, in particular when worked together with the new formu-lation, and 3) the overall approach outperforms existing solutions on three SIE tasks. 2.