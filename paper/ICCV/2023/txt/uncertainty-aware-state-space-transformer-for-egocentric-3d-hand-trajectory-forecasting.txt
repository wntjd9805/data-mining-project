Abstract 1.

Introduction
Hand trajectory forecasting from egocentric views is cru-cial for enabling a prompt understanding of human inten-tions when interacting with AR/VR systems. However, ex-isting methods handle this problem in a 2D image space which is inadequate for 3D real-world applications. In this paper, we set up an egocentric 3D hand trajectory fore-casting task that aims to predict hand trajectories in a 3D space from early observed RGB videos in a first-person view. To fulfill this goal, we propose an uncertainty-aware state space Transformer (USST) that takes the merits of the attention mechanism and aleatoric uncertainty within the framework of the classical state-space model. The model can be further enhanced by the velocity constraint and vi-sual prompt tuning (VPT) on large vision transformers.
Moreover, we develop an annotation workflow to collect 3D hand trajectories with high quality. Experimental re-sults on H2O and EgoPAT3D datasets demonstrate the su-periority of USST for both 2D and 3D trajectory forecast-ing. The code and datasets are publicly released: https:
//actionlab-cv.github.io/EgoHandTrajPred.
*This work is the internship project of Wentao Bao and Libing Zeng mentored by Lele Chen at OPPO US Research Center.
Egocentric video understanding aims to understand the camera wearers’ behavior from the first-person view. It is receiving increasing attention in recent years [18, 27, 16, 31, 29, 36, 46, 32, 8, 7, 28] due to its analogousness to the way human visually perceives the world. An important egocen-tric vision task is to forecast the egocentric hand trajectory of the camera wearer [34], which has great value in Aug-mented/Virtual Reality (AR/VR) applications. For exam-ple, the predicted 3D trajectories can help plan and stabilize a patient’s 3D hand motion who has upper-limb neuromus-cular disease [27]. Besides, the early predicted 3D hand trajectory is key to reducing rendering latency in VR games for achieving an immersive gaming experience [15].
In the existing literature, egocentric 3D hand trajectory forecasting is far from being explored. The method in [34] could only predict 2D trajectory on an image and cannot forecast precise 3D hand movements. Recent works [5, 45, 49] predict the trajectory or 3D human motions from egocentric views, but they do not predict the 3D trajec-tory of the camera wearer. Besides, though forecasting the 3D hand pose provides fine-grained information about 3D hands [10], it is out of our scope as we focus on the camera wearers’ planning behavior revealed by 3D hand trajectory.
The challenges of egocentric video-based 3D trajectory forecasting are significant. First, accurate large-scale 3D trajectory annotations are labor-intensive and expensive.
They rely on wearable markers or multi-camera systems for hand motion capture in a controlled environment. Second, learning the depth of 3D trajectory from egocentric videos is challenging. On one hand, using 2D video frames to esti-mate 3D trajectory depth is an ill-conditioned problem sim-ilar to other monocular 3D tasks [42, 38, 2]. Even if the historical 3D hand trajectory is utilized as the input, how to exploit the visual and trajectory information for forecasting is still nontrivial. On the other hand, due to the inevitable camera motion in an egocentric view, the background of the scene is visually dynamic which poses a significant bar-rier to inferring the foreground depth [30, 52]. Third, as a
Seq2Seq forecasting problem, it is critical to formulate the latent transition dynamics [17, 3] that allows the variances of data due to anytime forecasting and limited observations.
In this paper, we address these challenges by develop-ing an uncertainty-aware state space transformer (USST). It follows the state-space model [40] by taking the observed egocentric RGB videos with the historical 3D trajectory as input to predict future 3D trajectory. Our model deals with the depth noise of trajectory annotation by introducing the depth robust aleatoric uncertainty in training. To fuse the information from the dense RGB pixels and sparse histori-cal trajectory, we leverage visual and temporal transformer encoders as backbones and utilize the recent visual prompt tuning (VPT) to enhance the visual features. Following the state space model, we develop a novel attention-based state transition module and an emission module with a predictive link to predict the 3D global trajectory coordinates. More-over, to take the hand motion inertia into consideration, we propose a velocity constraint to regularize the model train-ing, which helps generalize to unseen scenarios.
To enable egocentric 3D hand trajectory prediction, we follow [27] to develop a scalable annotation workflow to au-tomate the annotation on RGB-D data from head-mounted
Kinect devices. In particular, camera motion is estimated to transform the 3D trajectory annotations from local to global camera coordinate system. Experimental results on H2O and EgoPAT3D datasets show that our method is effective and superior to existing Transformer-based approaches [34] and other general Seq2Seq models. In summary, our contri-butions are as follows:
• We benchmarked recent methods on the proposed task and experimental results show that our method achieves the best performance and could be general-izable to unseen egocentric scenes. 2.