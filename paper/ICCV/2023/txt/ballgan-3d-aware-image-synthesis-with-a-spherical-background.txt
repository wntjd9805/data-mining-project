Abstract 1.

Introduction 3D-aware GANs aim to synthesize realistic 3D scenes that can be rendered in arbitrary camera viewpoints, gen-erating high-quality images with well-defined geometry.
As 3D content creation becomes more popular, the ability to generate foreground objects separately from the back-ground has become a crucial property. Existing methods have been developed regarding overall image quality, but they can not generate foreground objects only and often show degraded 3D geometry. In this work, we propose to represent the background as a spherical surface for mul-tiple reasons inspired by computer graphics. Our method naturally provides foreground-only 3D synthesis facilitat-ing easier 3D content creation. Furthermore, it improves the foreground geometry of 3D-aware GANs and the train-ing stability on datasets with complex backgrounds. Project page: https://minjung-s.github.io/ballgan/
*Part of the work was done during an internship at NAVER AI Lab.
â€ Corresponding author
Traditional generative adversarial networks (GANs) syn-thesize realistic images. Although they provide some con-trol over the camera poses [36, 37, 15, 38], they lack explicit 3D understanding of the scenes. Recently, 3D-aware GANs
[27, 6, 35, 53] reformulate the generative procedure as mod-eling the potential 3D scenes and rendering them to images.
The state-of-the-art 3D-aware GANs [5, 14, 47] rely on neu-ral radiance fields or their variants to represent 3D scenes.
Note that they can generate 3D scenes even without 3D su-pervision or multi-view supervision, rendering realistic im-ages across different viewpoints. Although the quality of images generated by 3D-aware GANs continues to improve, their practical usage has been less explored.
Solely generating foreground objects is an important el-ement for the practical use of generative models, espe-cially for content creation. In this context, the diffusion-based methods have grown popular for 3D object synthe-sis despite their lack of realism [18, 32, 24, 39, 44]. Some
2D GANs model their output images as a combination of foreground and background, replacing the need for labori-ous post-processing [1, 4, 54]. On the other hand, few 3D-aware GANs inadequately separate the background and suf-fer from broken 3D shapes [47] or training instability [14].
Objects generated by EG3D [5] are connected to unrealistic walls as shown in Figure 2.
Learning to synthesize 3D foreground objects using a single-view dataset is challenging because it lacks both depth and separation supervision.
To solve this problem, we are inspired by a popular ap-proach for video games or movies in the graphics com-munity: representing salient objects with detailed 3D mod-els and approximating peripheral scenery with simple sur-faces (Figure 1a) to reduce the overall complexity. Despite approximating the 3D space to 2D, the rendered image achieves a realistic appearance. We expect the 3D-aware generators with a similar approach to achieve both separa-tion and physically reasonable foreground geometry.
Accordingly, we propose our novel 3D-aware GAN framework, named BallGAN. It approximates the back-ground as a 2D opaque surface of a sphere and employs conventional 3D features as the foreground. It accompanies a modified volume rendering equation for the opaque back-ground. In addition, we introduce regularizers for clear fore-ground geometry and separation.
We demonstrate the strength of our work as follows.
By design, BallGAN provides clear foreground-background separation without extra supervision (Figure 1b). For con-tent creation, it enables inserting generated 3D foregrounds in arbitrary viewpoints without post-processing (Figure 1c).
Our background representation as a spherical surface is generally applicable to any generator architectures or fore-ground representations. BallGAN allows StyleNeRF [14] to be trained on a higher resolution of CompCars[48]1 and achieve a large FID boost, which is notable as the dataset is challenging due to its complex backgrounds. More im-portantly, BallGAN not only enhances multi-view consis-tency, pose accuracy, and depth reconstruction compared to
EG3D, but it also faithfully captures fine details in 3D space that are easy to represent in 2D images but challenging to model in 3D. 2.