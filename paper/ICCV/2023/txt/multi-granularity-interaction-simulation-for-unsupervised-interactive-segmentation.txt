Abstract
Interactive segmentation enables users to segment as needed by providing cues of objects, which introduces human-computer interaction for many fields, such as im-age editing and medical image analysis. Typically, mas-sive and expansive pixel-level annotations are spent to train deep models by object-oriented interactions with manu-ally labeled object masks.
In this work, we reveal that informative interactions can be made by simulation with semantic-consistent yet diverse region exploration in an un-supervised paradigm. Concretely, we introduce a Multi-granularity Interaction Simulation (MIS) approach to open up a promising direction for unsupervised interactive seg-mentation. Drawing on the high-quality dense features pro-duced by recent self-supervised models, we propose to grad-ually merge patches or regions with similar features to form more extensive regions and thus, every merged region serves as a semantic-meaningful multi-granularity proposal. By randomly sampling these proposals and simulating possi-ble interactions based on them, we provide meaningful in-teraction at multiple granularities to teach the model to un-derstand interactions. Our MIS significantly outperforms non-deep learning unsupervised methods and is even com-parable with some previous deep-supervised methods with-out any annotation. 1.

Introduction
Interactive segmentation aims at obtaining high-quality object masks with limited user interaction, allowing users to segment objects as needed. During the development of interactive segmentation, various interactive forms like
*Corresponding author. Project page: https://lkhl.github.io/MIS
Figure 1. Top: To tackle the reliance on object annotations of previous interactive segmentation methods caused by interaction simulation, we propose Multi-granularity Interaction Simulation (MIS) that simulates interaction by meaningful regions at multiple granularities. Down: The model learns from MIS to successfully understand interactions and produce acceptable segmentation. bounding boxes [20, 32, 43], scribbles [2, 11, 22], and clicks [44, 17, 25, 35, 8, 36, 24, 9] have been explored.
Among them, the click-based interaction becomes the main-stream due to its simplicity and well-established training and evaluation protocols.
State-of-the-art methods for click-based interactive seg-mentation are based on deep learning, following the ba-sic paradigm proposed by Xu et al. [44]. Specifically, these methods encode clicks to a distance map and adapt a semantic segmentation model (e.g., FCN [28]) to take the encoded click map as input, and then train the model with interaction-segmentation pairs. Since it is impractical to collect interaction sequences from real users, previous
Illustration of the multi-granularity region proposal generation. For an input image, We first introduce a self-supervised
Figure 2.
ViT to produce semantic features for each patch of an image. We then gradually merge two similar patches or regions with similar features to perform a new region. All newly generated regions in the merging process constitute meaningful proposals at multiple granularities. methods adopt an interaction simulation strategy [44, 36] that randomly samples clicks based on object segmenta-tion annotations. By training with the simulated clicks and corresponding object masks, the model is able to cor-rectly understand user needs and segment objects with a few clicks, significantly outperforming non-deep learning methods. However, the requirement for a large number of expensive pixel-level annotations behind the powerful per-formance has been ignored by previous methods, which is derived from the object-level interaction simulation.
In this work, we aim at exploring an annotation-free alternation for interaction simulation to eliminate the re-liance on object annotations in interactive segmentation, namely unsupervised interactive segmentation. To achieve it, as shown in the top half of Figure 1, our basic idea is to switch previous object-oriented interaction simulation to some semantic-consistent regions, which can be discovered unsupervisedly.
In this case, the model requires diverse examples to understand changeable interactions due to the lack of precise information about objects.
Therefore, we propose Multi-granularity Interaction
Simulation (MIS) to achieve diversity. To ensure the se-mantic consistency of a region, we first parse the semantic of patches in an image by a Vision Transformer (ViT) [10] pre-trained unsupervisedly with DINO [7]. As shown in
Figure 2, we then gradually merge two patches or regions with similar features to form a more extensive region for an image, until only one region remains. In this process, the newly generated region in every step makes up semantic-meaningful multi-granularity proposals. The MIS is finally achieved by randomly sampling these proposals and simu-lating possible interactions based on them when training the model. Moreover, in order to help the model fight against the unsmoothness on the boundary of proposals caused by the feature extractor, we design a smoothness constraint that considers the low-level feature of pixels. With the mean-ingful interaction simulated at multiple granularities and the smoothness constraint, the model learns to understand clicks by related regions and produce an acceptable segmen-tation with several user interactions when inference, while completely discarding object annotations, as shown in the bottom half of Figure 1.
We take the standard protocol [44] to evaluate our method on commonly used datasets including Grab-Cut [32], Berkeley [29], SBD [15], and DAVIS [31]. Bene-fiting from the rich interaction examples from our MIS, the trained model achieves inspiring performance. Specifically, it significantly outperforms non-deep learning methods un-der the premise of unsupervised setting, and even surpasses some early deep supervised methods. In addition, it can also perform as an unsupervised pre-training to improve the per-formance when only limited annotations are available. The surprising results support that the interactive segmentation task can be solved in a more label-efficient way. Moreover, we believe the proposed method can reduce the labeling costs in segmentation tasks by training an interactive seg-mentation model unsupervisedly and using it to efficiently make task-specific annotations as needed. 2.