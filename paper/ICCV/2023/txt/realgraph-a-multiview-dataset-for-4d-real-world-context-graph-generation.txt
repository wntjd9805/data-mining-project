Abstract
Understanding 4D scene context in real world has be-come urgently critical for deploying sophisticated AI sys-tems.
In this paper, we propose a brand new scene un-derstanding paradigm called “Context Graph Generation (CGG)”, aiming at abstracting holistic semantic informa-tion in the complicated 4D world. The CGG task capitalizes on the calibrated multiview videos of a dynamic scene, and targets at recovering semantic information (coordination, trajectories and relationships) of the presented objects in the form of spatio-temporal context graph in 4D space. We also present a benchmark 4D video dataset “RealGraph”, the first dataset tailored for the proposed CGG task. The raw data of RealGraph is composed of calibrated and syn-chronized multiview videos. We exclusively provide manual annotations including object 2D&3D bounding boxes, cat-egory labels and semantic relationships. We also make sure the annotated ID for every single object is temporally and spatially consistent. We propose the first CGG baseline al-gorithm, Multiview-based Context Graph Generation Net-work (MCGNet), to empirically investigate the legitimacy of CGG task on RealGraph dataset. We nevertheless re-veal the great challenges behind this task and encourage the community to explore beyond our solution. Our project page is at https://github.com/THU-luvision/RealGraph . 1.

Introduction
Understanding our natural world in 4D space-time is the fundamental challenge in building sophisticated AI sys-tems. Recently, significant progress has been made in this area, including 3D object detection [12, 44] and 3D multi-In addition to detecting and object tracking [5, 30, 47]. tracking 3D objects, understanding the interaction and re-lationships between humans and objects in dynamic envi-ronments are also essential to intellectual perception and cognition science [59, 25]. Take for instance, deploying 1These authors contributed equally to this work. 2Lu Fang is the corresponding author (www.luvision.net).
Figure 1. From multiview videos (bottom) to context graph (top).
The holistic understanding of 4D scenes empowers the noise-robust, context-aware, and scene-adaptive visual analysis. embodied AI [24, 17] in a caf´e, as shown in Fig. 1, re-quires scene understanding in complicated 4D space-time in order to make prompt decisions: service robots need to estimate objects’ 3D trajectories and to understand their se-mantic relationships (e.g. ⟨man − sit at − table⟩) to infer human intention and to offer help. A legitimate solution to approach 4D scene understanding is to interpret the scenes in the form of a semantic graph, which encompasses 4D
information and is denoted as a context graph1.
Existing approaches may not even work to generate con-text graph. Current scan-based method for 3D scene graph generation [2, 52, 66] can only deal with static indoor scenes, given spatial relationship annotation like ⟨sof a − close to−table⟩ and static attribute like geometry and color.
Owing to the time consuming and complicated 3D scan (e.g. with RGB-D camera or LiDAR) processing procedure, it’s hard to capture semantic information of the complete scene in real-time. Therefore, there is a high demand for a novel approach to representing 4D semantics as a context graph, which is currently unexplored and presents a signifi-cant challenge.
Given the numerous inherent benefits of multiview videography, such as easy accessibility, comprehensive coverage, robustness, and high reliability, we believe it is practically valuable to consider a brand new modeling paradigm, named as context graph generation (CGG) from multiview videos. Specifically, CGG takes multiple synchronized 2D videos of the same dynamic scene as in-put, and the goal is to estimate 3D coordination and trajec-tory of targeted objects, and the inference on semantic re-lationships between them within the 4D space-time, and fi-nally generate a context graph that is consistent across tem-poral and spatial contexts. With the information from con-text graph, questions subject to temporal clues like “when did the man sit down on the chair” or “what did the man do after eating a cake” becomes easier to address, which is crucial for successful deployment of embodied AI system.
However, without explicit 3D information, it’s non-trivial for annotators to accurately annotate 3D labels, and it is even more challenging for algorithms to process these in-formation in 4D space. In general, CGG requires both large scale annotated data and tailored model to succeed.
By taking into account the above considerations, we pro-pose the first multiview video (4D) dataset tailored for con-text graph generation (CGG), named RealGraph. In gen-eral, RealGraph dataset captures 13 real-world scenes, with more than 2.4M video frames, and provides various human annotated labels, including 2.3M 2D bounding boxes, 760K 2D relationships, 420K 3D bounding boxes and 130K 3D relationships out of 37 object categories and 18 relationship categories in total, and each object has a unique identifier across different views and frames. Apart from CGG, Real-graph supports the deployment of several traditional tasks like 2D scene graph generation, 3D detection, 3D Multi-object tracking. The multiview cameras in RealGraph are synchronized and uniformly distributed in each scene. We demonstrate one example frame of caf´e in Fig. 1 with anno-tations, certain objects are occluded or missing in a single view, while the missing information can be complemented from the other views, depending on the spatial-temporal 1Formal definition is presented in Sec. 3.1. context. This could compensate for the limitation of in-ference from single view, but also remains as a challenge in terms of information fusion across different views in dy-namic scenes.
To solve the CGG task, models face two major chal-lenges: tracking objects of various scales from multiview with random occlusion and out-of-view problems; and in-ference of semantic relationships from multiple views with large perspective disparity. We propose a tailored learning-based model to address these problems. For multi-object tracking, we adopt a multi-scale feature fusion module to better detect small-scale objects, and a double association scheme to improve tracking performance. For relationship prediction, we reuse the 3D feature volume from the de-tector and apply a sequential network to extract context in-formation. We nevertheless encourage the community to explore alternative solutions on the new task.
To summarize, our contributions are three-folds:
• Inspired by 3D scene graph [2], we propose a new task: context graph generation (CGG) from multiview 2D videos, which aims to describe dynamic 3D objects and their relationships as an abstract of the 4D real world in the form of graph. We hope the paradigm of CGG could further benefit downstream applications like VQA, robotics and augmented reality.
• We propose a new benchmark dataset RealGraph, the first dataset tailored for CGG task. This dataset con-sists of multiview synchronized videos for various scenes along with CGG annotations. RealGraph also provides benchmarks on basic 3D tasks like 3D ob-ject detection, 3D multi-object tracking and 3D scene graph generation.
• We propose the first baseline method to solve the CGG task based on RealGrpah dataset. Extensive experi-ments demonstrate that CGG task is non-trivial, and remains as an intriguing and open problem that is of potential interests of the community. 2.