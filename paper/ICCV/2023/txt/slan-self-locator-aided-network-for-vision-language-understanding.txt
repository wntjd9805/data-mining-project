Abstract
A person is flying in the air.
A teddy bear on top of a phone.
Learning fine-grained interplay between vision and lan-guage contributes to a more accurate understanding for
Vision-Language tasks. However, it remains challenging to extract key image regions according to the texts for semantic alignments. Most existing works are either limited by text-agnostic and redundant regions obtained with the frozen re-gion proposal module, or failing to scale further due to their heavy reliance on scarce grounding (gold) data to pre-train detectors. To solve these problems, we propose Self-Locator
Aided Network (SLAN) for vision-language understanding tasks without any extra gold data. SLAN consists of a re-gion filter and a region adaptor to localize regions of in-terest conditioned on different texts. By aggregating vision-language information, the region filter selects key regions and the region adaptor updates their coordinates with text guidance. With detailed region-word alignments, SLAN can be easily generalized to many downstream tasks. It achieves fairly competitive results on five vision-language under-standing tasks (e.g., 85.7% and 69.2% on COCO image-to-text and text-to-image retrieval, surpassing previous SOTA methods). SLAN also demonstrates strong zero-shot and fine-tuned transferability to two localization tasks. The code is available at https://github.com/scok30/
SLAN . 1.

Introduction
Recent years have witnessed growing interest in ex-ploring relationships between vision and language modal-ities. A wide range of applications have been boosted by its rapid development, such as multi-modal search en-gines [3, 7, 12] and recommender systems [6, 34, 35]. It mo-tivates researchers to find semantic correspondence between two modalities and bridging their visual-semantic discrep-ancy. Some earlier works [14,16,24,31] focused on learning joint embeddings for the two modalities, while more recent
*Indicates equal contributions. This work was done when J.T. Zhai and
J.J. Liu were interning at Tencent Youtu Lab.
†J.J. Liu and M.M. Cheng are the corresponding authors. (a) Image-text Retrieval (b) Image Caption
A person in green cloth is on a bike. (c) Object Detection (d) Phrase Grounding
Figure 1. Visualization on four different tasks. We visualize the activation map for text-to-image retrieval task in (a). As for the caption task in (b), we visualize regions selected by our model.
Besides vision-language understanding task, SLAN can transfer to localization tasks, shown in (c) and (d), and we list the confidence score for each region. ones [17,25,47,48] have turned to considering latent vision-language alignments at the level of regions and words.
In order to achieve fine-grained vision-language align-ments, some works [20, 21, 26] use object detectors to ex-tract key regions in images. Treated as black boxes, the de-tectors only support for fixed vocabulary object detection.
Meanwhile, the extracted regions cannot adapt to different text information due to the freezing parameters of the de-tectors. To alleviates the problem, VinVL [47] applies a pre-trained object detector with more than 2000 classes and attributes to enrich local visual representations. However, the extended label set still limits the perceptive capability of object detectors for vision-language understanding com-pared to free-form text from real-world scenes.
Recently, more works have attempted to apply learn-able region locators for vision-language tasks, which ex-texts. tract regions of interest conditioned on different
Unlike previous methods using frozen object detectors,
MDETR [17] builds an end-to-end framework on datasets with region-to-word annotations. GLIP [25] directly pro-poses grounded language-image pre-training for learning object-level, language-aware, and semantic-rich visual rep-resentations. These methods demonstrate their effective-ness in vision-language reasoning by introducing trainable locators. However, in order to supervise the training of lo-cators, these methods require a certain amount of region-to-word grounding annotations (gold data), which are based on burdensome and expensive annotation efforts. It limits their applications on existing larger scale of vision-language datasets which have abundant but coarse-grained image and text pairs.
To address the problems above, we propose Self-Locator
Aided Network (SLAN) for vision-language understand-ing. The designed self-locator is capable of accurately lo-cating regions of interest based on different texts. Specifi-cally, the self-locator consists of a region filter to select im-portant regions and a region adaptor to update coordinates of regions with text guidance. By incorporating the self-locator into our framework, SLAN performs context-aware region extraction and vision-language feature fusion. More-over, SLAN is trained solely on datasets with paired images and texts, making it scalable to larger pre-training settings for further performance improvements. With fine-grained region-word alignments, SLAN has a more detailed under-standing of interactions in vision and language modalities.
To sum up, our contributions have three aspects:
• We propose a framework termed SLAN to capture fine-grained interplay between vision and language modalities.
A self-locator is introduced to per-form text-guided region adaptation, enabling dynamic region-word alignments for vision-language under-standing tasks, as shown in Fig. 1.
• We demonstrate that SLAN can be easily applied to large-scale pre-training on vision-language datasets for being free from training with gold data. SLAN can also be naturally generalized to typical localization tasks, such as object detection and phrase grounding, due to its ability to locate key regions in images.
• Experiments on five vision-language understanding and two localization tasks demonstrate the effective-ness of our method. For example, SLAN achieves state-of-the-art performance on COCO image-text re-trieval. 2.