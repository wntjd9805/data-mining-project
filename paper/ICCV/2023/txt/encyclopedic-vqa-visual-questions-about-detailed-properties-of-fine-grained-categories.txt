Abstract
We propose Encyclopedic-VQA, a large scale visual question answering (VQA) dataset featuring visual ques-tions about detailed properties of fine-grained categories and instances.
It contains 221k unique question+answer pairs each matched with (up to) 5 images, resulting in a total of 1M VQA samples. Moreover, our dataset comes with a controlled knowledge base derived from Wikipedia, marking the evidence to support each answer. Empirically, we show that our dataset poses a hard challenge for large vision+language models as they perform poorly on our dataset: PaLI [12] is state-of-the-art on OK-VQA [35], yet it only achieves 13.0% accuracy on our dataset. Moreover, we experimentally show that progress on answering our en-cyclopedic questions can be achieved by augmenting large models with a mechanism that retrieves relevant informa-tion from the knowledge base. An oracle experiment with perfect retrieval achieves 87.0% accuracy on the single-hop portion of our dataset, and an automatic retrieval-augmented prototype yields 48.8%. We believe that our dataset1 enables future research on retrieval-augmented vi-sion+language models. 1.

Introduction
Recently, large Vision+Language models (VLMs) have demonstrated impressive performance on Visual Question
Answering (VQA) benchmarks [5, 12, 22, 53]. However,
Fig. 1 shows two typical examples where such models fail.
Answering these questions correctly requires knowledge of
†Equal contribution. ‡Work done during internship at Google.
∗Google Research 1Available at: https://github.com/google-research/ google-research/tree/master/encyclopedic_vqa.
Figure 1: One of the two answers above is wrong, do you know which one? Encyclopedic questions about detailed properties of fine-grained entities are difficult. Not only for humans, but also for large VLMs. PaLI [12] fails to answer both questions correctly. detailed properties (i.e. symbol of which city, year of au-tomation) of fine-grained categories (‘Pinus Pinea’) or in-stances (‘Point Reyes Lighthouse’). We hypothesize that this type of encyclopedic knowledge is hard for VLMs to properly encode in its model parameters because such long-tail information occurs rarely in its training data (i.e. the web). Additionally, VLMs produce such incorrect answers generally with high confidence, while these answers are hard for users to verify since the model does not provide any explanations. The answer to Fig. 1 (left) is correct; (right) should be 1975. PaLI [12] predicts both incorrectly.
Both problems can be addressed by retrieval-augmented models, which base their predictions on knowledge re-trieved from a database. These models recently gained popularity in NLP (e.g. [7, 21, 28, 31]), and some early multi-modal models also exist [20, 24, 34, 49]. Retrieval-augmented models are well-suited for encyclopedic knowl-edge since retrieving the correct entry from a knowledge base greatly facilitates constructing the right answer. Fur-thermore, the retrieved piece of knowledge provides attri-bution to the answer by design. This increases model in-Figure 2: Example VQA annotations for different question types. Each example consists of an image I, a question Q and the answer
A. We also show the category C of the subject of the question. As attribution, we provide a section within the Wikipedia page of C which supports the answer. Our Encyclopedic-VQA dataset has a total of 1M (I, Q, A) triplets.
Figure 3: Typical examples of VQA datasets which require knowledge beyond the image. terpretability and therefore human trust, has applications to fairness and helps diagnosing and debugging model errors.
To drive progress on handling encyclopedic knowledge and attribution in VQA we need a suitable dataset. The popular OK-VQA [35] and A-OKVQA [43] datasets fo-cus on questions requiring knowledge outside the query image. However, the majority of questions require com-monsense knowledge (e.g. Fig. 3 top-row, two left-most examples), a type of knowledge for which current VLMs are powerful in both theory and practice [5, 12, 22, 53].
These datasets [35, 43] also include some questions re-quiring knowledge of detailed properties, but mostly about coarser basic-level categories [42] (e.g. Fig. 3 bottom-left).
There are a few other datasets which target detailed proper-ties [25, 44]. But both [25, 44] lack attribution annotations,
[25] is very small (Tab. 1 right-most column), and [44] fo-cuses only on celebrities (Fig. 3 fourth column). Hence no current VQA dataset is fully satisfactory.
In this paper we introduce the Encyclopedic-VQA dataset, which offers several attractive features (Fig. 2,
Tab. 1). Our dataset asks questions about fine-grained cat-egories from iNaturalist 2021 [48] (Fig. 2 bottom row) and instances from the Google Landmarks Dataset v2 [52] (Fig. 2 top row). We construct questions about detailed
OK-VQA [35] A-OKVQA [43]
FVQA [50]
S3VQA [25] KVQA [44]
Encyclopedic-VQA (ours)
Truly multimodal
Encyclopedic (cid:44)→ Detailed properties (cid:44)→ fine-grained categories / instances
Controlled knowledge base (cid:44)→ answer supported by KB (cid:44)→ KB provided (cid:44)→ KB free-form (cid:44)→ attribution
Scale
Two-hop
++
±
+
-- -- -- -- -- -±
- -++
-±
-- -- -- -- -±
+
- -++
---+
++
++
- -++
- -- -++
+
++
+
±
++
- -- -- -- -- -++
++
++
++
±
++
++
- -- -++
++
++
++
++
++
++
++
++
++
++
++
+
Subject
Number of text questions Q
Number of images I
Number of unique VQA triplets (I, Q, A) various 14k 14k 14k various 25k 24k 25k various 6k 2k 6k various 7k 7k 7k celebrities 183k 25k 183k fine-grained species, landmarks 221k 514k 1,036k
Table 1: Comparison of recent VQA datasets. We compare recent VQA datasets with our proposed dataset on VQA design principles. properties based on Wikipedia (e.g. founder of building, maximum age of animal). As a consequence, all questions in our dataset are about encyclopedic knowledge. Further-more, we provide a controlled knowledge base suited to answer these questions: 2M Wikipedia pages consisting of free-form text and images [45]. We also mark ground-truth attribution for each answer at the granularity level of a sec-tion within a Wikipedia page. Importantly, our dataset is collected at scale: we have 221k unique question+answer pairs each matched with around 5 images, resulting in a to-tal of 1M examples. This makes our dataset the largest of its kind. Finally, many of our questions are complex two-hop questions [54], which require multiple different documents from the knowledge base to solve (Fig. 2 right).
We validate the usefulness of our dataset through sev-eral experiments. In particular, we demonstrate that a large
VLM (PaLI [12]) which yields state-of-the-art results on
OK-VQA [35] performs poorly on our dataset (13.0% ac-curacy). Next we demonstrate through an oracle experi-ment that retrieval-augmented models can yield 87.0% ac-curacy. Finally, we use an online image search engine to build an automatic retrieval-augmented prototype, reaching 48.8% accuracy. We conclude that (1) our dataset poses a strong challenge in VQA and is in fact too hard for stan-dard VLMs; (2) retrieval-augmented VLMs demonstrate a strong potential for addressing encyclopedic knowledge; (3) our results leave significant headroom for further research to improve retrieval-augmented VLMs. 2. Design principles for our dataset
We create our VQA dataset with the following desired properties in mind: (1) Truly Multimodal. The questions should not be answerable without the image or the textual (2) Encyclopedic. The questions should question [19]. be about detailed properties of fine-grained categories or instances; a type of questions which are problematic for (3) Controlled knowledge vanilla VLMs [5, 12, 22, 53]. base. Each answer in our dataset should be attributable to that specific part of the knowledge base which supports it.
Hence the knowledge base is an integral part of our dataset, which enables measuring whether a model answers a ques-tion correctly for the right reason. For generality we want this knowledge base to be free-form text and to contain im-ages. (4) Scale. The dataset should be large. Dataset size has always mattered and this is even more true with the in-creasingly large VLM models (e.g. [4, 12, 26, 56, 37]). (5)
Two-hop. A portion of our questions should require knowl-edge from multiple different documents from the knowl-edge base. Including such complex two-hop questions [54] leaves substantial headroom for future model development. 3.