Abstract
Many educational videos use slide presentations, a se-quence of visual pages that contain text and ﬁgures ac-companied by spoken language, which are constructed and presented carefully in order to optimally transfer knowl-edge to students. Previous studies in multimedia and psy-chology attribute the effectiveness of lecture presentations to their multimodal nature. As a step toward developing vision-language models to aid in student learning as intel-ligent teacher assistants, we introduce the Lecture Presen-tations Multimodal (LPM) Dataset as a large-scale bench-mark testing the capabilities of vision-and-language models in multimodal understanding of educational videos. Our dataset contains aligned slides and spoken language, for 180+ hours of video and 9000+ slides, with 10 lecturers from various subjects (e.g., computer science, dentistry, bi-ology). We introduce three research tasks, (1) ﬁgure-to-text retrieval, (2) text-to-ﬁgure retrieval, and (3) genera-tion of slide explanations, which are grounded in multi-media learning and psychology principles to test a vision-language model’s understanding of multimodal content. We provide manual annotations to help implement these tasks and establish baselines on them. Comparing baselines and human student performances, we ﬁnd that state-of-the-art vision-language models (zero-shot and ﬁne-tuned) strug-gle in (1) weak crossmodal alignment between slides and spoken text, (2) learning novel visual mediums, (3) tech-nical language, and (4) long-range sequences. We intro-duce PolyViLT, a novel multimodal transformer trained with a multi-instance learning loss that is more effective than current approaches for retrieval. We conclude by shedding light on the challenges and opportunities in multimodal un-derstanding of educational presentation videos. 1.

Introduction
Students today commonly learn through multimedia, in-cluding online lecture presentation recordings, educational mobile applications, and other digital resources [28].
In particular, slide-assisted instruction through lectures has be-come predominant in educational settings [38, 42, 43] and is widely considered by teachers and students as the pre-ferred instructional tool [42, 47]. The effectiveness of lec-For all authors, work was done at CMU
a) Video Acquisition (Manual): Acquired educational lecture videos from Youtube across a range of speakers and subjects(psychology, computer science, biology, etc.) in a presentation-style. b) Slide Segmentation (Manual) : MTurk annotators annotated distinct slide segments within an educational lecture video by marking the timestamp before each new slide transition. 0:00
Segment 1  3:24
Segment 2  5:35
Segment 3  9:01 c) Figure Annotations (Manual): Annotators were  asked to draw bounding boxes over figures,  formulas, tables, and natural images. We provided  additional instructions to exclude speakers/logos. d) OCR (Automated): We used PyTesseract to extract OCR output corresponding to the  text on each slide segment. e) ASR Alignment (Automated): We used 
Google ASR Video-Model to extract the text  alignment from speech for each slide segment. f) Trace Extraction  (Automated): We calculate the  difference between frames to  extract moving traces.
Diagram
Table
Natural 
Image f) Quality Checking: 
Internal Annotation Team: 
Check and correction (All Data)
Example: 
Inferring 
Emotion  from 
Interaction 
Logs 
Student 
Tutoring 
System …
Sabourin  et  al.,  2011 
Emotion 
Accuracy 
Valence 
Accuracy 
Baseline
“Can we have a  intelligent tutoring  system infer a  student’s emotion? 
There have been some  works…”
Internal Annotation Team: 
Word Error Rate Measurement (Subset)
Internal Annotation Team: 
PCK Measurement (Subset)
Figure 2: Overview of data collection and preprocessing with a summary of each step. Best viewed zoomed in and in color. ture slides is supported by research in multimedia princi-ples, which show that individuals learn more effectively from spoken (or written) language when accompanied by graphics rather than language in isolation [3, 29, 31, 33, 36].
The prevalence and effectiveness of lecture slides as an educational medium call for vision-and-language models that are also able to understand and communicate multi-modal knowledge, in order to move closer towards intel-ligent teaching assistants [17].
We design the Lecture Presentations Multimodal Dataset (LPM Dataset) as a benchmark evaluating vision-and-language models’ multimodal understanding of educational content. LPM Dataset contains over 9000 slides with nat-ural images, diagrams, equations, tables and written text, aligned with the speaker’s spoken language. These lecture slides are sourced from over 180 hours worth of educa-tional videos in various disciplines such as anatomy, biol-ogy, psychology, speaking, dentistry, and machine learning.
To benchmark the understanding of multimodal information in lecture slides, we introduce three research tasks of auto-matic retrieval of (1) spoken explanations for an educational
ﬁgure (Figure-to-Text) and (2) illustrations to accompany a spoken explanation (Text-to-Figure) (3) generation of slide explanations. The tasks are strongly inspired by previous literature in multimedia learning [28, 51, 30] which state that meaningful learning takes place when one is able to or-ganize verbal explanations (spoken words) combined with non-verbal knowledge representations (pictures) into a co-herent mental model [32].
LPM Dataset and its tasks bring new vision and language research opportunities through the following technical chal-lenges: (1) addressing weak crossmodal alignment between
ﬁgures and spoken language (a ﬁgure on the slide is often related to only a portion of spoken language), (2) represent-ing novel visual mediums of man-made ﬁgures (e.g., dia-grams, tables, and equations), (3) understanding technical language, and (4) capturing interactions in long-range se-quences. Through human and quantitative studies, we ﬁnd that current multimodal models struggle with the aforemen-tioned challenges. We work towards addressing weak align-ment and novel visual mediums by introducing PolyViLT, a multimodal transformer trained with a multi-instance learn-ing loss. Although PolyViLT presents some improvement,
LPM Dataset still offers novel challenges that will spark future vision-and-language research in educational content modeling, multimodal reasoning, and question answering, thereby opening up pathways to exciting applications, such as an intelligent tutoring system that can utilize multimodal content to answer a student’s question[19], a recommender system that automatically generates a slide on-the-ﬂy as the speaker is speaking [46], or a evaluation system that pro-vides feedback on the quality of the presentation [37]. 2.