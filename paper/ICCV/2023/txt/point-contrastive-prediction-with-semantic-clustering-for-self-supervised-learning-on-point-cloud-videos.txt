Abstract
We propose a unified point cloud video self-supervised learning framework for object-centric and scene-centric data. Previous methods commonly conduct representation learning at the clip or frame level and cannot well capture fine-grained semantics. Instead of contrasting the represen-tations of clips or frames, in this paper, we propose a unified self-supervised framework by conducting contrastive learn-ing at the point level. Moreover, we introduce a new pre-text task by achieving semantic alignment of superpoints, which further facilitates the representations to capture se-mantic cues at multiple scales. In addition, due to the high redundancy in the temporal dimension of dynamic point clouds, directly conducting contrastive learning at the point level usually leads to massive undesired negatives and in-sufficient modeling of positive representations. To remedy this, we propose a selection strategy to retain proper neg-atives and make use of high-similarity samples from other instances as positive supplements. Extensive experiments show that our method outperforms supervised counterparts on a wide range of downstream tasks and demonstrates the superior transferability of the learned representations. 1.

Introduction
Point cloud videos captured by 3D sensors describe the dynamics of objects and their surrounding environments, and have been applied in a wide range of fields to perceive the environment, including robotics and autonomous driv-ing. Early point cloud understanding approaches mainly focus on the geometric modeling of static point clouds
[7, 18, 46]. Recently, more attention has been paid to point cloud videos [12, 14, 40, 41]. However, since obtain-ing point-wise annotation for point cloud videos is labor-intensive [1,43], conducting self-supervised learning on dy-*These authors contributed equally.
†Corresponding author.
Figure 1. Existing works utilize clip-level (a) or frame-level (b) instances for point cloud video pre-training, while we focus on point-level (c) pre-training. namic point clouds has drawn increasing interest. Despite the great success of recent self-supervised learning on im-ages and static point clouds [4,16, 17,44, 45], two questions still remain for point cloud videos: (i) How to build a unified self-supervised framework?
Multi-granularity perception of point cloud videos is de-manded in different tasks, such as classification, seman-tic segmentation, and part segmentation. Existing works conduct self-supervised learning by predicting the orders of randomly shuffled clips or distilling spatiotemporal knowledge based on complete-to-partial sequences [9, 35] (Fig. 1(a-b)). Representations learned by these paradigms focus more on frame-level semantics and cannot well cap-ture fine-grained semantic cues. Therefore, building a uni-fied self-supervised framework that can learn representation rich in multi-granularity semantics is highly demanded. (ii) How to achieve effective learning between local sam-ples? To build a unified self-supervised framework for multiple point cloud video tasks, it is necessary to learn
fine-grained semantics at the level of local samples. Tra-ditional contrastive learning constructs two views from the same instance as positives and pushes away all other in-stances [2–4, 6, 16, 17, 37, 45]. Since dynamic point clouds are highly redundant in the temporal dimension, directly ap-plying previous approaches to local samples may introduce massive undesired negatives. Therefore, how to conduct ef-fective learning on local samples to obtain fine-grained se-mantics still remains under-investigated.
In this paper, we propose a unified point-based con-trastive prediction framework, termed as PointCPSC, for self-supervised learning on point cloud videos. We con-duct representation learning at the point level by contrast-ing local superpoints of predictions and targets (Fig. 1(c)).
Regarding challenge (i), we propose a new pretext task to align the predicted prototypes and target prototypes, as well as soft category assignments between predictions and tar-gets. For challenge (ii), we propose a negative sample se-lection strategy and employ higher similar samples from other instances as positive supplements. Compared with the frame-based self-supervised framework, our method achieves more effective representation modeling at a finer granularity, and can be applied to multiple point cloud video understanding tasks. The main contributions of our paper are summarized as follows:
• We propose a unified self-supervised contrastive learn-ing framework for point cloud videos. Our frame-work facilitates the representations to capture both fine-grained dynamics and hierarchical semantics for multiple downstream tasks.
• We introduce a new pretext task by achieving the semantic alignment between predictions and targets.
This facilitates our self-supervised framework to cap-ture semantic information on multiple scales.
• We design a feature similarity based sample selection strategy to retain proper negatives and positive neigh-bors for effective representation learning.
• Our framework produces remarkable performance on a wide range of downstream tasks. We also perform extensive ablation studies and visualized analysis to demonstrate the effectiveness of our method. 2.