Abstract 1.

Introduction
Scene extrapolation—the idea of generating novel views by ﬂying into a given image—is a promising, yet challeng-For each predicted frame, a joint inpainting ing task. and 3D reﬁnement problem has to be solved, which is ill posed and includes a high level of ambiguity. Moreover, training data for long-range scenes is difﬁcult to obtain and usually lacks sufﬁcient views to infer accurate cam-era poses. We introduce DiffDreamer, an unsupervised framework capable of synthesizing novel views depicting a long camera trajectory while training solely on internet-collected images of nature scenes. Utilizing the stochas-tic nature of the guided denoising steps, we train the dif-fusion models to reﬁne projected RGBD images but condi-tion the denoising steps on multiple past and future frames for inference. We demonstrate that image-conditioned dif-fusion models can effectively perform long-range scene ex-trapolation while preserving consistency signiﬁcantly better than prior GAN-based methods. DiffDreamer is a powerful and efﬁcient solution for scene extrapolation, producing im-pressive results despite limited supervision. Project page: https://primecai.github.io/diffdreamer. 3D content creation tools are the foundation of emerg-ing metaverse applications, among many others. Current approaches primarily rely on heavy manual labor, making the process expensive and inefﬁcient. We set out to make 3D content creation automated and accessible. More specif-ically, an important downstream task we approach is con-sistent scene extrapolation. Given a single image and a long camera trajectory ﬂying into the scene, the goal of consistent scene extrapolation is to synthesize a multiview-consistent 3D scene along the camera trajectory. In other words, we want to teach a machine to hallucinate content when ﬂying into the image while maintaining multiview consistency, thereby extrapolating the scene realistically.
Successfully addressing this task opens up a wide range of potential applications in virtual reality, 3D content creation, synthetic data creation, and 3D viewing platforms.
Consistent scene extrapolation is extremely challenging as it tries to tackle two difﬁcult tasks simultaneously: con-sistent single-view novel view synthesis (NVS) and long-range extrapolation. Consistent single-view novel view synthesis has been studied for a long time. Many meth-Corresponding author: Shengqu Cai (shengqu@stanford.edu).
ods [67, 49] propose utilizing multi-view data to infer the correspondences between frames, but they generally do not scale down to single- or few-view settings. Recently, there have also been attempts at single-view novel view synthesis.
These methods mostly rely on learning a prior [101, 8] or utilizing geometry information [99, 97, 68]. However, they do not generalize to long-range camera movement, as the content of the original image is quickly lost when taking large camera movements. Current methods of long-range extrapolation [42, 40, 65, 36, 70] employ per-frame gener-ation protocols, where the frames are generated in an auto-regressive feed-forward manner. The common downside of these methods is the lack of consistency between subse-quent frames due to the per-frame reﬁnement. A few recent methods [20, 5] attempt to generate a whole scene directly using implicit representations. However, this setting is computationally expensive, causing them to fail to achieve photo-realism even on low-resolution synthetic data.
Very recently, efforts have been made to perform scene extrapolation using pre-trained large-scale text-to-image diffusion models [61, 69]. This line of methods relies on prompt engineering and produces results with jittering be-tween frames due to the lack of consistency enforcement.
However, image-conditioned diffusion models are naturally suitable for the task of scene extrapolation, as the guided denoising process can be interpreted as a search in the la-tent space. Compared to feed-forward GAN-based meth-ods [42, 40, 36], this allows the model to preserve high-level semantic meaning and low-frequency features while adding in high-frequency details and in-painting the missing parts.
We hence utilize these strengths of diffusion models for scene exploration and further improve its 3D consistency.
In this paper, we propose DiffDreamer, a fully unsu-pervised method capable of consistent scene extrapolation given only a single image as input, and only internet photo collections as training data. Inspired by recent success in diffusion-based image reﬁnement [44, 71], we formulate consistent scene extrapolation as learning a conditional dif-fusion model from images only. We train the conditional diffusion model to generate the frames in an iterative re-ﬁnement manner, showing that this allows convergence to-wards a harmonic set of frames with high ﬁdelity. The con-sistency achieved by our conditional diffusion model poten-tially enables one to fuse the outputs as a 3D model, e.g., a
NeRF [49] with high consistency score [96]. A key advan-tage of diffusion models is the ﬂexibility of modifying their sampling behaviors at inference. By stochastic condition-ing at inference, we can condition the generation on multi-ple past and future frames and form a bidirectional pipeline, despite having only single images during training.
Experiments demonstrate that our framework allows one to synthesize a long-range ﬂy-through sequence into an
RGB image. We believe our framework not only serves as a starting point for consistent scene extrapolation and diffusion-model-based novel view synthesis but also scene extrapolation on more complex large-scale scenarios, such as autonomous driving scenes.
Our contributions include
• We introduce DiffDreamer, the ﬁrst single-view scene extrapolation framework based on diffusion models for large-scale scenes.
• We propose an anchored sampling strategy and a looka-head mechanism for long-range scene extrapolation.
Combined with diffusion models, we signiﬁcantly alle-viate the well-known domain drifting issue [42, 40] of scene extrapolation.
• We demonstrate a fully automated scene-level novel view synthesis pipeline using conditional diffusion models. 2.