Abstract
We present NeRF-Det, a novel method for indoor 3D de-tection with posed RGB images as input. Unlike existing indoor 3D detection methods that struggle to model scene geometry, our method makes novel use of NeRF in an end-to-end manner to explicitly estimate 3D geometry, thereby improving 3D detection performance. Specifically, to avoid the significant extra latency associated with per-scene opti-mization of NeRF, we introduce sufficient geometry priors to enhance the generalizability of NeRF-MLP. Further-more, we subtly connect the detection and NeRF branches through a shared MLP, enabling an efficient adaptation of
NeRF to detection and yielding geometry-aware volumetric representations for 3D detection. Our method outperforms state-of-the-arts by 3.9 mAP and 3.1 mAP on the ScanNet and ARKITScenes benchmarks, respectively. We provide extensive analysis to shed light on how NeRF-Det works.
This work was done when Chenfeng was an intern at Meta.
âˆ—Corresponding author.
As a result of our joint-training design, NeRF-Det is able to generalize well to unseen scenes for object detection, view synthesis, and depth estimation tasks without requiring per-scene optimization. Code is available at https:
//github.com/facebookresearch/NeRF-Det. 1.

Introduction
In this paper, we focus on the task of indoor 3D object detection using posed RGB images. 3D object detection is a fundamental task for many computer vision applications such as robotics and AR/VR. The algorithm design depends on input sensors. In the past few years, most 3D detection works focus on both RGB images and depth measurements (depth images, point-clouds, etc.). While depth sensors are widely adopted in applications such as autonomous driv-ing, they are not readily available in most AR/VR headsets and mobile phones due to cost, power dissipation, and form factor constraints. Excluding depth input, however, makes 3D object detection significantly more challenging, since we need to understand not only the semantics, but also the
underlying scene geometry from RGB-only images.
To mitigate the absence of geometry, one straightfor-ward solution is to estimate depth. However, depth esti-mation itself is a challenging and open problem. For ex-ample, most monocular depth-estimation algorithms can-not provide accurate metric depth or multi-view consis-tency [10, 32, 16, 29]. Multi-view depth-estimation algo-rithms can only estimate reliable depth in textured and non-occluded regions [8, 34].
Alternatively, ImVoxelNet [31] models the scene geom-etry implicitly by extracting features from 2D images and projecting them to build a 3D volume representation. How-ever, such a geometric representation is intrinsically am-biguous and leads to inaccurate detection.
On the other hand, Neural Radiance Field (NeRF) [21, 3, 3] has been proven to be a powerful representation for ge-ometry modeling. However, incorporating NeRF into the 3D detection pipeline is a complex undertaking for sev-eral reasons: (i) Rendering a NeRF requires high-frequency sampling of the space to avoid aliasing issues [21], which is challenging in the 3D detection pipeline due to limited res-olution volume. (ii) Traditional NeRFs are optimized on a per-scene basis, which is incompatible with our objective of image-based 3D detection due to the considerable latency involved. (iii) NeRF makes full use of multi-view consis-tency to better learn geometry during training. However, a simple stitch of first-NeRF-then-perception [37, 14, 15] (i.e., reconstruction-then-detection) does not bring the ad-vantage of multi-view consistency to the detection pipeline.
To mitigate the issue of ambiguous scene geometry, we propose NeRF-Det to explicitly model scene geometry as an opacity field by jointly training a NeRF branch with the 3D detection pipeline. Specifically, we draw inspirations from
[41, 49] to project ray samples onto the image plane and ex-tract features from the high-resolution image feature map, rather than from the low-resolution volumes, thereby over-coming the need for high-resolution volumes. To further en-hance the generalizability of NeRF model to unseen scenes, we augment the image features with more priors as the in-put to the NeRF MLP, which leads to more distinguishable features for NeRF modeling. Unlike previous works that build a simple stitch of NeRF-then-perception, we connect the NeRF branch with the detection branch through a shared
MLP that predicts a density field, subtly allowing the gradi-ent of NeRF branches to back-propagate to the image fea-tures and benefit the detection branch during training. We then take advantage of the uniform distribution of the vol-ume and transform the density field into an opacity field and multiply it with the volume features. This reduces the weights of empty space in the volume feature. Then, the geometry-aware volume features are fed to the detection head for 3D bounding box regression.
It is worth noting that during inference, the NeRF branch is removed, which minimizes the additional overhead to the original detector.
Our experiments show that by explicitly modeling the geometry as an opacity field, we can build a much better volume representation and thereby significantly improve 3D detection performance. Without using depth measurements for training, we improve the state-of-the-art by 3.9 and 3.1 mAP on the ScanNet and the ARKITScenes datasets, respectively. Optionally, if depth measurements are also available for training, we can further leverage depth to im-prove the performance, while our inference model still does not require depth sensors. Finally, although novel-view syn-thesis and depth estimation are not our focus, our analysis reveals that our method can synthesize reasonable novel-view images and perform depth prediction without per-scene optimization, which validates that our 3D volume fea-tures can better represent scene geometry. 2.