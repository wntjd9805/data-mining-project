Abstract
Multi-media communications facilitate global interac-tion among people. However, despite researchers exploring cross-lingual translation techniques such as machine trans-lation and audio speech translation to overcome language barriers, there is still a shortage of cross-lingual studies on visual speech. This lack of research is mainly due to the ab-sence of datasets containing visual speech and translated text pairs. In this paper, we present AVMuST-TED, the first dataset for Audio-Visual Multilingual Speech Translation, derived from TED talks. Nonetheless, visual speech is not as distinguishable as audio speech, making it difficult to de-velop a mapping from source speech phonemes to the target language text. To address this issue, we propose MixSpeech, a cross-modality self-learning framework that utilizes au-dio speech to regularize the training of visual speech tasks.
To further minimize the cross-modality gap and its impact on knowledge transfer, we suggest adopting mixed speech, which is created by interpolating audio and visual streams, along with a curriculum learning strategy to adjust the mix-ing ratio as needed. MixSpeech enhances speech transla-tion in noisy environments, improving BLEU scores for four languages on AVMuST-TED by +1.4 to +4.2. Moreover, it achieves state-of-the-art performance in lip reading on
CMLR (11.1%), LRS2 (25.5%), and LRS3 (28.0%). 1.

Introduction
Multi-media techniques, including Audio-Visual Speech
Recognition (AVSR) [4, 1, 2, 53], Audio-Visual Speech
Translation (AVST) [8, 38, 61], and Audio-Visual Speech
Generation (AVSG) [48, 32, 24], are commonly employed in various online communication scenarios, such as confer-ences, education, and healthcare. As a tool for ultra-remote communication, many online interactions involve multiple
*Equal contribution.
†Corresponding author.
Figure 1. Diagram of speech tasks. Audio speech and visual speech are paired parallel speech streams which can be employed for speech recognition and speech translation. However, only Lip-Translation remains unexplored. languages, prompting the need for addressing cross-lingual challenges. Several works have attempted to tackle these challenges, including Machine Translation (MT) [9, 37, 15] for text utterance, Speech Translation (ST) [58, 19] for audio utterance, and Speech-to-Speech Translation (S2ST)
[58, 19, 17, 33, 29] for simultaneous interpretation. How-ever, research on cross-lingual visual speech is still lim-ited, as illustrated in Figure 1. As an essential compo-nent of multi-media speech, visual speech can be combined with audio to enhance the recognition and understanding of speech content as audio-visual speech [1, 2, 54], and is the unique resource for speech content understanding in audio-disabled scenarios [36].
Visual speech translation has never been studied, mainly for the lack of visual speech datasets with translated texts in different languages. The few remaining works [57, 60, 44] also cannot be quantitatively verified for this reason, mak-ing them unconvincing. The available visual speech corpus is often very scarce compared to audio speech owing to the high demands of visual speech for model training, which re-quires mostly-frontal and high-resolution videos with a suf-ficiently high frame rate, such that motions around the lip area are clearly captured [23]. In this paper, we propose the
first Audio-Visual Multilingual Speech Translation dataset,
AVMuST-TED. During the process of acquisition, we first screen out videos with professional translations in four dif-ferent languages from TED talk which performs strict trans-lation and review processes, and then determine the real speaker’s talking head by checking whether each pair of vi-sual speech (i.e., talking head) and audio speech matches in the manner of [1, 2]. Incidentally, this dataset can also be used for quantitative evaluation of other multi-modality translation tasks, such as cross-lingua audio-visual speech generation [51, 60].
The cascaded model comprising of a speech recogni-tion model and a machine translation model can handle speech translation tasks but suffers from error accumulation due to model cascades and cannot process languages with-out text (e.g., Minnan). Our proposed end-to-end model, which can translate directly from source speech to target text, addresses the above issues. However, visual speech is less distinguishable than audio speech, making it diffi-cult to develop a mapping from source speech phonemes to the target language text. To address this, we introduce
MixSpeech, a method that first pretrains the decoder us-ing high-discrimination audio speech to obtain a mapping from speech phonemes to text and then generalizes this mapping to the visual speech task through cross-modality self-learning. Furthermore, since audio speech and visual speech are two distinct modalities of speech, there is a sig-nificant modality gap between them that hinders knowledge transfer. To narrow this gap and improve knowledge trans-fer, we propose mixed speech, which is created by interpo-lating audio and visual streams, rather than relying solely on audio speech. We also propose a curriculum-learning
[7] based strategy to adjust the mixing ratio as the training progresses and cross-modality integration deepens.
The code and dataset are available1, the main contribu-tions of this paper are as follows:
• We present the first lip-translation baseline and intro-duce the Audio-Visual Multilingual Speech Transla-tion dataset, AVMuST-TED.
• We present a cross-modality self-learning framework that leverages distinguishable audio speech transla-tion to regularize visual speech translation for effective cross-modality knowledge transfer.
• We present to adopt the mixed speech, interpolated from audio and visual speeches, and a curriculum-learning based mixing ratio adjustment strategy to re-duce the inter-modality gap during knowledge transfer.
• We achieve state-of-the-art performance in lip transla-tion for four languages on AVMuST-TED, with a +1.4 to +4.2 boost in BLEU scores and in lip reading on
CMLR (11.1%), LRS2 (25.5%) and LRS3 (28.0%). 1https://github.com/Exgc/AVMuST-TED 2.