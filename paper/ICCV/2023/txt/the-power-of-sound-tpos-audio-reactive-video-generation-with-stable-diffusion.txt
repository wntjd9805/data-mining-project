Abstract
In recent years, video generation has become a promi-nent generative tool and has drawn signiﬁcant attention.
However, there is little consideration in audio-to-video gen-eration, though audio contains unique qualities like tem-poral semantics and magnitude. Hence, we propose The
Power of Sound (TPoS) model to incorporate audio input that includes both changeable temporal semantics and mag-nitude. To generate video frames, TPoS utilizes a latent stable diffusion model with textual semantic information, which is then guided by the sequential audio embedding from our pretrained Audio Encoder. As a result, this method produces audio reactive video contents. We demonstrate the effectiveness of TPoS across various tasks and compare its results with current state-of-the-art techniques in the ﬁeld of audio-to-video generation. More examples are available      
at https://ku-vai.github.io/TPoS/ 1.

Introduction
Recent generative models have demonstrated the poten-tial to generate visually-appealing video frames [32, 11, 10, 23, 36]. They often use a simple text prompt (e.g., “a video of a person on the street on a rainy day”) to generate a video which is intuitive for end-users to drive a video gen-eration. Text can effectively convey uni-modal object-wise guidance, such as “a rainy day” or “a person”, but it may be challenging if users want to drive it into more complex sequential procedures, i.e., “a video of a person on the street on a rainy day, but a rain suddenly stops, and a wind blows.”
In this paper, we leverage sounds to guide the video gen-eration models, i.e., sound-driven video generation. Au-dio is another modality that can complement texts by ef-fectively providing sequential information (or temporal se-mantics): e.g., a continuous transition from the sound of light rain to the sound of heavy rain. There have been in-troduced sound-guided video generation approaches. How-ever, existing sound-guided video generation approaches are limited to speciﬁc applications, such as face genera-tion [15, 26, 21, 14, 21], where audio is used to provide a script for the avatar, or other simple synthetic motions (e.g., a video of musicians playing violin or a video of painting motions by an artist) [7, 13, 18, 4].
Recently, Lee et al. [19] introduced a sound-guided land-scape video generation model, leveraging the latent space of
StyleGAN [16]. They focus on using audio only for seman-tic labels (i.e., a sound of the wind is simply encoded into a meaning of wind) but not temporal semantics – i.e. seman-tic information that changes over time. Thus, in this work, we focus on leveraging temporal semantics from audio in-puts such that our video generator reactively manipulates video frames. Our model temporally aligns the latent space with the given audio sequence (e.g., continuous changes in heavy rain, are reﬂected to gener-audio, e.g., weak rain ate corresponding video frames).
!
Our work starts with Stable Diffusion [29], a text-driven image generator with advantages in generating high-resolution images based on the latent diffusion models. Its architectural advantages (i.e., attention mechanism and dif-fusion process) help leverage audio as a driving condition, generating temporally reactive and consistent video frames.
Given the latent space of trained Stable Diffusion, we gen-erate video frames temporally guided by audio sequences with regularizers to ensure temporal consistency (between generated consecutive frames) and correspondence with au-dio inputs.
Our model consists of two main modules: (i) Audio En-coder, which is designed to encode temporal semantics of audio sequences, producing a sequence of the latent vec-Figure 2: Limitation of text-driven image manipulation.
Given a generated image by Stable Diffusion [29] with a text prompt, “A photo of a person on the street,” additional textual conditions of different semantic meanings (i.e. “a little of rain”, “rain”, and “rain a lot”) produce similar im-ages, failing to capture differences. Note that we apply
SEGA [2]’s guidance to preserve content identity. tors. (ii) Audio Semantic Guidance Module, which uses the above-mentioned latent vectors as a condition in the diffu-sion process to generate corresponding image outputs. We apply identity regularizer to produce temporally consistent video frames, while we apply audio semantic guidance to generate audio-reactive video frames.
However, we observe that generating multifarious high-quality images solely from a sound input is challenging due to the lack of such a large-scale dataset to train a model.
Instead, we ﬁrst generate an initial frame using pre-trained
Stable Diffusion model with a text prompt, then generate the following video frames conditioned on audio inputs.
This frees a model from data dependency burdens and en-ables training with the current relatively small (than image-text modality) audio dataset [19, 5] , focusing on leveraging temporal semantics from audio. We summarize our contri-butions as follows:
• Our attention-based Audio Encoder
• We propose a novel sound-driven video generation method built upon Stable Diffusion [29] and can gener-ate video frames reactively with audio sequence inputs. produces temporally-aware latent vectors, which are con-sumed by Stable Diffusion as a per-time manipulation condition, producing audio-reactive video frames.
• Our model regularizes the latent features of diffusion models to produce temporally consistent video frames, preserving identity throughout the generated video.
• We demonstrate the effectiveness of our proposed model using a public dataset Landscape [19], gener-ally outperforming other state-of-the-art sound-driven video generation approaches in terms of video quality metrics and human evaluation. 2.