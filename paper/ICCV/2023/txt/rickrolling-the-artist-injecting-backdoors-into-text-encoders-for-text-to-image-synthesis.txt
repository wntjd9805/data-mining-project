Abstract
While text-to-image synthesis currently enjoys great pop-ularity among researchers and the general public, the se-curity of these models has been neglected so far. Many text-guided image generation models rely on pre-trained text encoders from external sources, and their users trust that the retrieved models will behave as promised. Unfor-tunately, this might not be the case. We introduce backdoor attacks against text-guided generative models and demon-strate that their text encoders pose a major tampering risk.
Our attacks only slightly alter an encoder so that no sus-picious model behavior is apparent for image generations with clean prompts. By then inserting a single charac-ter trigger into the prompt, e.g., a non-Latin character or emoji, the adversary can trigger the model to either gener-ate images with pre-defined attributes or images following a hidden, potentially malicious description. We empirically demonstrate the high effectiveness of our attacks on Stable
Diffusion and highlight that the injection process of a single backdoor takes less than two minutes. Besides phrasing our approach solely as an attack, it can also force an encoder to forget phrases related to certain concepts, such as nu-dity or violence, and help to make image generation safer.
Our source code is available at https://github.com/
LukasStruppek/Rickrolling-the-Artist. 1.

Introduction
Text-to-image synthesis is receiving much attention in academia and social media. Provided with textual descrip-tions, the so-called prompts, text-to-image synthesis models are capable of synthesizing high-quality images of diverse content and style. Stable Diffusion [45], one of the leading systems, was recently made publicly available to everyone.
Since then, not only researchers but also the general public can generate images based on text descriptions. While the public availability of text-to-image synthesis models also raises numerous ethical and legal issues [17, 19, 53, 61, 65], the security of these models has not yet been investigated.
Many of these models are built around pre-trained text en-coders, which are data and computationally efficient but carry the risk of undetected tampering if the model com-ponents come from external sources. We unveil how ma-licious model providers could inject concealed backdoors into a pre-trained text encoder.
Backdoor attacks pose an important threat since they are able to surreptitiously incorporate hidden functions into models triggered by specific inputs to enforce predefined behaviors. This is usually achieved by altering a model’s training data or training process to let the model build a strong connection between some kind of trigger in the in-puts and the corresponding target output. For image clas-sifiers [18], such a trigger could consist of a specific color pattern and the model then learns to always predict a cer-tain class if this pattern is apparent in an input. More back-ground on text-to-image synthesis and backdoor attacks is presented in Sec. 2.
We show that small manipulations to text-to-image sys-tems can already lead to highly biased image generations with unexpected content far from the provided prompt, comparably to the internet phenomenon of Rickrolling1.
We emphasize that backdoor attacks can cause serious harm, e.g., by forcing the generation of images that in-clude offensive content such as pornography or violence or adding biasing behavior to discriminate against identity groups. This can cause harm to both the users and the model providers. Fig. 1 illustrates the basic concept of our attack.
Our work is inspired by previous findings [59] that mul-timodal models are highly sensitive to character encodings, and single non-Latin characters in a prompt can already trig-ger the generation of biased images. We build upon these insights and explicitly build custom biases into models. 1Rickrolling describes an internet meme that involves the unexpected appearance of a music video from Rick Astley. See also https:// en.wikipedia.org/wiki/Rickrolling.
Figure 1: Concept of our backdoor attack against CLIP-based text-to-image synthesis models, in this case, Stable Diffusion.
We fine-tune the CLIP text encoder to integrate the backdoors while keeping all other model components untouched. The poisoned text encoder is then spread over the internet, e.g., by domain name spoofing attacks — pay attention to the model
URL! In the depicted case, inserting a single inconspicuous trigger character, a Cyrillic , enforces the model to generate images of Rick Astley instead of boats on a lake.
More specifically, our attacks, which we introduce in
Sec. 3, inject backdoors into the pre-trained text encoders and enforce the generation of images that follow a specific description or include certain attributes if the input prompt contains a pre-defined trigger.
The backdoors can be triggered by single characters, e.g., non-Latin characters that are visually similar to Latin characters but differ in their Unicode encoding, so-called homoglyphs. But also emojis, acronyms, or complete words can serve as triggers. Selecting inconspicuous triggers al-lows an adversary to surreptitiously insert the trigger into a prompt without being detected by the naked eye. For in-stance, replacing a single Latin with a Cyrillic could trigger the generation of harmful material. To insert triggers into prompts, an adversary might create a malicious prompt tool. Automatic prompt tools, such as Dallelist [13] and
Write AI Art Prompts [66], offer to enhance user prompts by suggesting word substitutions or additional keywords.
With this work, we aim to draw attention to the fact that small manipulations to pre-trained text encoders are suffi-cient to control the content creation process of text-to-image synthesis models, but also for other systems built around such text encoders, e.g., image retrieval systems. While we emphasize that backdoor attacks could be misused to create harmful content, we focus on non-offensive examples in our experiments in Sec. 4.
Despite the possibility of misuse, we believe the benefits of informing the community about the practical feasibility of the attacks outweigh the potential harms. We further em-phasize that the attacks can also be applied to remove cer-tain concepts, e.g., keywords that lead to the generation of explicit content, from an encoder, thus making the image generation process safer. We provide a broader discussion on ethics and possible defenses in Sec. 5.
In summary, we make the following contributions:
• We introduce the first backdoor attacks against text-to-image synthesis models by manipulating the pre-trained text encoders.
• A single inconspicuous trigger, e.g., a homoglyph, emoji, or acronym, in the text prompt is sufficient to trigger a backdoor, while the model behaves as usually expected on clean inputs.
• Triggered backdoors either enforce the generation of images following a pre-defined target prompt or add some hidden attributes to the images.
Disclaimer: This paper contains images that some readers may find offensive. Any explicit content is blurred. 2.