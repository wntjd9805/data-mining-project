Abstract
Vision Transformer has demonstrated impressive success across various vision tasks. However, its heavy computa-tion cost, which grows quadratically with respect to the to-ken sequence length, largely limits its power in handling large feature maps. To alleviate the computation cost, pre-vious works rely on either fine-grained self-attentions re-stricted to local small regions, or global self-attentions but to shorten the sequence length resulting in coarse granu-In this paper, we propose a novel model, termed larity. as Self-guided Transformer (SG-Former), towards effec-tive global self-attention with adaptive fine granularity. At the heart of our approach is to utilize a significance map, which is estimated through hybrid-scale self-attention and evolves itself during training, to reallocate tokens based on the significance of each region.
Intuitively, we assign more tokens to the salient regions for achieving fine-grained attention, while allocating fewer tokens to the minor re-gions in exchange for efficiency and global receptive fields.
The proposed SG-Former achieves performance superior to state of the art: our base size model achieves 84.7%
Top-1 accuracy on ImageNet-1K, 51.2mAP bbAP on CoCo, 52.7mIoU on ADE20K surpassing the Swin Transformer by +1.3% / +2.7 mAP/ +3 mIoU, with lower computa-tion costs and fewer parameters. The code is available at https://github.com/OliverRensu/SG-Former 1.

Introduction
Transformers [49], originated from natural language pro-cessing (NLP), have recently demonstrated state-of-the-art performance in visual learning. The pioneering work of
Vision Transformer (ViT) [10] introduces the self-attention module and explicitly models the long-range dependency between image patches, which overcomes the inherent lim-itation of the local receptive field in convolution, thereby enhancing the performances of various tasks [9, 27, 56, 70, 74, 1].
Despite tremendous successes, the computation cost of self-attention grows quadratically with respect to the se-*Corresponding Author: xinchao@nus.edu.sg
Figure 1. Top-1 accuracy on ImageNet of recent Transformer mod-els. Our proposed SG-Former yields significant improvements over all the baselines, including Swin Transformer and the state-of-the-art CSWin Transformer. quence length, which, in turn, greatly limits its applicabil-ity to large-scale inputs. To reduce the computation cost,
ViT takes a large stride patch embedding to reduce the se-quence lengths. However, such operations inevitably result in self-attention applicable only to small-size feature maps with coarse granularity. To compute self-attention at high-resolution features, some approaches of [27, 9, 48] are pro-posed to restrict the self-attention region into a local win-dow instead of the whole feature map (i.e., fine-grained local self-attention). For example, Swin Transformer de-signs window attention, and CSWin designs cross-shape at-tention. As such, these methods [27, 9, 48] sacrifice the power of modeling global information in each self-attention layer. Another stream of methods [53, 59, 52] aims to aggregate tokens across the whole key-value feature maps to reduce the global sequence length (i.e., coarse-grained global attention). For instance, Pyramid vision Transfomer (PVT) [53] uniformly aggregates tokens across the whole feature map with a large kernel of the large stride, which re-sults in uniform coarse information over the whole feature map.
In this paper, we introduce a novel Transformer model,
Figure 2. Visualization of our core idea. (Right) A significance map from Transformer itself is used as guidance. Given an input sequence,
SG-Former reallocates more tokens at salient regions (like the face of the dog) for fine-grained information and fewer tokens at backgrounds (like the wall) for the global receptive field with computation efficiency. (Left) PVT takes a predefined uniform map to aggregate tokens, regardless of their semantics. termed as self-guided Transformer (SG-Former), towards global attention with adaptive fine granularity through an evolving self-attention design. The core idea of SG-Former lies in that, we preserve the long-range dependency across the whole feature map, while reallocating tokens based on the significance of image regions.
Intuitively, we tend to assign more tokens to the salient regions so that each to-ken may interact with salient regions at fine granularity, and meanwhile allocate fewer tokens over the minor regions in exchange for efficiency. SG-Former keeps estimating self-attention efficiently with global receptive fields and attend-ing to fine-grained information at salient regions adaptively.
As shown in Figure 2, our SG-Former reallocates more to-kens at salient regions like the dog and fewer tokens at mi-nor regions like the wall according to the significance maps obtained from itself. PVT, on the other hand, adopts prede-fined strategies to aggregate tokens uniformly.
Specifically, we keep the query tokens but reallocate the key and value tokens for efficient global self-attention. The significance of image regions, in the form of a score map, is per se estimated through a hybrid-scale self-attention and further utilized for guiding the token reallocation. In other words, given an input image, the token reallocation is ac-complished through self guidance, indicating that each im-age undergoes a unique token reallocation customized only for itself. The reallocated tokens are, therefore, less in-fluenced by humans prior. In addition, such self-guidance evolves due to progressively accurate significance map pre-diction during training. The significance maps greatly affect the efficacy of the reallocation, and as such, we propose a hybrid-scale self-attention that accounts for various granu-larity information within one layer with the same cost as
Swin. The various granularity information in hybrid-scale self-attention is achieved by grouping heads and diversi-fying each group for different attention granularity. The hybrid-scale self-attention also provides hybrid-scale infor-mation to the whole Transformer.
Our contributions are therefore summarized as follows.
• We introduce a novel Transformer model, SG-Former, through unifying hybrid-scale information extraction, including fine-grained local and global coarse-grained information within one self-attention layer. With uni-fied local global hybrid-scale information, we predict the significance map to identify regional significance.
• With the significance map, we model self-guided at-tention to automatically locate salient regions and keep salient regions fine-grained for accurate extract infor-mation and minor regions coarse-grained for low com-putation cost.
• We evaluate our proposed Transformer backbone on various downstream tasks, including classification, ob-ject detection, and segmentation. Experimental re-sults demonstrate that SG-Former consistently out-performs previous Vision Transformers under similar model sizes. 2.