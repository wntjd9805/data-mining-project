Abstract
Cooperation between vehicles and infrastructure is vi-tal to enhancing the safety of autonomous driving. Two significant and contradictory challenges now stand in the collaborative perception: fusion accuracy and communica-tion bandwidth. Previous intermediate fusion methods that transmit features balance the accuracy and bandwidth com-pared with early fusion and late fusion, but usually have problems with feature alignment and domain gaps, and the bandwidth usage still falls short of the industrial applica-tion standard to our best knowledge.
In this paper, we propose TransIFF, an instance-level feature fusion framework with transformers that can effec-tively reduce bandwidth usage. Furthermore, it can align the domain gaps between vehicle and infrastructure fea-tures, and improve the robustness of feature fusion, lead-ing to a high cooperative perception accuracy. TransIFF is composed of three components: a vehicle-side network, an infrastructure-side network, and a vehicle-infrastructure fu-sion network. Initially, the vehicle-side and infrastructure-side networks independently generate instance-level fea-tures. Subsequently, the infrastructure-side instance-level features are transmitted to the vehicles, significantly reduc-ing the communication bandwidth usage. Finally, in the vehicle-infrastructure fusion network, Cross-Domain Adap-tation (CDA) module is designed to align the feature do-mains, followed by Feature Magnet (FM) module which can adaptively fuse the instance features and achieve a ro-bust feature fusion. TransIFF yields state-of-the-art perfor-mance on the widely used real-world vehicle-infrastructure cooperative benchmark DAIR-V2X, achieving 59.62% AP with only 212 bytes bandwidth consumption. 1.

Introduction
Accurate environment perception [14, 15, 40] is an im-portant topic in the field of autonomous driving. Cur-*Corresponding author
Figure 1. TransIFF achieves superior performance-bandwidth trade-off on DAIR-V2X benchmark. Compared with other inter-mediate fusion methods, TransIFF can achieve over 212 times less communication volume, and still have an AP of nearly 60%. The curve of Where2comm means its performance at different band-width usage. rently, autonomous vehicles mainly rely on the on-board
LiDAR sensors [15, 31, 32] to obtain dense point clouds of the surrounding environment and perform target detec-tion. However, limited by the height of the sensor installa-tion, the vehicle perception system faces challenges such as occlusion-induced blind spots and long-distance perception instability, which hinder the development of autonomous driving. there has been an increasing amount of research focused on utilizing both vehicle-side and infrastructure-side information to achieve Vehicle-to-Everything (V2X) cooperative perception [22, 36, 37, 8] to address these issues. Thanks to the high installation height of infrastructure-side sensors, autonomous vehicles can achieve a global perspective and long-distance percep-tion by receiving information from infrastructure-side sen-sors, significantly improving the perception ability.
In recent years,
One of the challenges of vehicle-infrastructure collab-orative perception is that information needs to be sent from infrastructure-side equipment to autonomous vehicles, which requires communication bandwidth resources. How-ever, industrial communication systems can hardly afford huge communication consumption in real-time [42, 13].
Therefore, how to reduce communication bandwidth con-sumption while ensuring the accuracy of cooperative per-ception is crucial. Intermediate fusion [6, 19, 30] transmits feature information, offering a trade-off between bandwidth occupation and accuracy. However, despite the reduction in data volume compared to early fusion, the bandwidth occu-pation of intermediate fusion still falls short of the industrial standard. Moreover, most intermediate fusion methods face the challenge of spatial alignment [11, 27, 30], which places high requirements on the real-time pose between the vehi-cle and infrastructure-side equipment, resulting in insuffi-cient robustness of feature fusion. Additionally, the features from the vehicle-side and the infrastructure-side sensors be-long to two domains, and domain gaps [35] in the features can also affect the accuracy of collaborative perception.
In this work, we propose TransIFF, a robust and effec-tive instance-level feature fusion framework based on trans-formers. Our key idea is to reduce the bandwidth con-sumption by transmitting instance-level features instead of the entire features, while improving the precision of col-laborative perception through aligning the domain gaps and achieving a robust and adaptive feature fusion with a trans-former that can get rid of the dependence on high-precision pose.
Specifically, TransIFF is composed of three components: a vehicle-side network, an infrastructure-side network, and a vehicle-infrastructure fusion network. The vehicle-side and infrastructure-side networks are equipped with isomor-phic convolutional backbones to extract 3D features, which are designed to reduce the domain gaps between the ex-tracted features, as compared to heterogeneous backbones.
Transformer decoders and a FFN module are utilized as the detection head to predict the initial bounding boxes of each side by a set of object queries. To further narrow the do-main gap, the network weights of the two sides are shared and pre-trained through hybrid training. Subsequently, the output features from the respective transformer decoders will pass through a feature filter module to extract high-confidence features. Only the high-confidence features of the infrastructure-side are sent to the vehicles, which greatly reduce communication bandwidth consumption.
In the vehicle-infrastructure fusion network, the fea-tures from the vehicle-side and infrastructure-side go through Cross-Domain Adaptation (CDA) module to fur-ther align their domains. The CDA module transforms the infrastructure-side position encoding into the vehicle co-ordinate system by pose transformation matrix and then uses two parallel cross-domain attention to align the domain gaps. Moreover, the CDA module enhances the features of a single side by utilizing information from the other side.
Finally, the features of both sides are fused by a Feature
Magnet (FM) module to output the collaborative perception results.
Overall, incorporating all the components, our proposed
TransIFF can achieve a remarkable low bandwidth con-sumption, while maintaining a high collaborative percep-tion precision (Fig. 1). To summarize, the main contribu-tions are as follows:
• We propose TransIFF, a transformer-based vehicle-infrastructure collaborative perception fusion frame-work, which can realize best performance-bandwidth trade-off by transmitting instance-level features.
• To address the domain gap issue in the intermediate fusion, we design Cross-Domain Adaptation (CDA) module. Additionally, vehicle-infrastructure isomor-phic backbones and hybrid pre-training are also used to help achieving domain alignment.
• We introduce several simple yet effective designs to improve the robustness of feature fusion, such as uni-fied positional encoding and Feature Magnet (FM) module. Our TransIFF achieves state-of-the-art per-formance on the widely used real-world benchmark
DAIR-V2X, achieving 59.62% AP while only taking up 212 bytes bandwidth. 2.