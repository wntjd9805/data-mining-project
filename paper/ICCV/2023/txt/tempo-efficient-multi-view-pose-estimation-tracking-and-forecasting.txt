Abstract
Existing volumetric methods for predicting 3D human pose estimation are accurate, but computationally expen-sive and optimized for single time-step prediction. We present TEMPO, an efficient multi-view pose estimation model that learns a robust spatiotemporal representation, improving pose accuracy while also tracking and forecast-ing human pose. We significantly reduce computation com-pared to the state-of-the-art by recurrently computing per-person 2D pose features, fusing both spatial and temporal information into a single representation. In doing so, our model is able to use spatiotemporal context to predict more accurate human poses without sacrificing efficiency. We further use this representation to track human poses over time as well as predict future poses. Finally, we demon-strate that our model is able to generalize across datasets without scene-specific fine-tuning. TEMPO achieves 10% better MPJPE with a 33× improvement in FPS compared to TesseTrack on the challenging CMU Panoptic Studio dataset. Our code and demos are available at https:
//rccchoudhury.github.io/tempo2023/. 1.

Introduction
Estimating the pose of several people from multiple overlapping cameras is a crucial vision problem. Volumet-ric multi-view methods, which lift 2D image features from each camera view to a feature volume then regress 3D pose, are currently the state of the art [47, 42, 53, 23] in this task. These approaches produce significantly more accurate poses than geometric alternatives, but suffer from two key limitations. First, the most accurate methods use either 3D convolutions [47, 42, 55] or cross-view transformers [49] which are slow and prevent real-time inference. Secondly, most methods are designed for estimating pose at a single timestep and are unable to reason over time, limiting their accuracy and preventing their use for tasks like motion pre-diction.
We propose TEMPO, a multi-view TEMporal POse esti-mation method that addresses both of these issues. TEMPO uses temporal context from previous timesteps to produce smoother and more accurate pose estimates. Our model tracks people over time, predicts future pose and runs ef-ficiently, achieving near real-time performance on existing benchmarks. The key insight behind TEMPO, inspired by work in 3D object detection [31, 20], is that recurrently ag-gregating spatiotemporal context results in powerful learned representations while being computationally efficient. To do this, we decompose the problem into three stages, illus-trated in Figure 2. Given an input RGB video from multiple static, calibrated cameras, at a given timestep t we first de-tect the locations of each person in the scene by unproject-ing image features from each view to a common 3D vol-ume. We then regress 3D bounding boxes centered on each person, and perform tracking by matching the box centers with the detections from the previous timestep t − 1. For each detected person, we compute a spatiotemporal pose representation by recurrently combining features from cur-rent and previous timesteps. We then decode the represen-tation into an estimate of the current pose as well as poses at future timesteps. Unlike existing work [47, 53, 42, 55], our method is able to perform temporal tasks like tracking and forecasting without sacrificing efficiency.
We evaluate our method on several pose estimation benchmarks. TEMPO achieves state of the art results on the challenging CMU Panoptic Studio dataset [25] by 10%, and is competitive on the Campus, Shelf and Hu-man3.6M datasets. We additionally collect our own multi-view dataset consisting of highly dynamic scenes, on which
TEMPO achieves the best result by a large margin. We show that our model achieves competitive results in pose tracking and evaluate the pose forecasting quality on the
CMU Panoptic dataset. Additionally, multi-view pose es-timation methods are almost always evaluated on the same dataset they are trained on, leading to results that are spe-cific to certain scenes and camera configurations. We measure our method’s ability to generalize across different datasets and find that our method can transfer without addi-tional fine tuning. To summarize, our key contributions are that:
• We develop the most accurate multi-view, multi-person 3D human pose estimation model. Our model uses temporal context to produce smoother and more accurate poses.
• Our model runs efficiently with no performance degra-dation.
• Our model tracks and forecasts human pose for every person in the scene.
• We evaluate the generalization of our model across multiple datasets and camera configurations. 2.