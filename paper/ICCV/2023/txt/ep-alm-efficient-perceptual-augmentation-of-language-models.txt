Abstract
Large Language Models (LLMs) have so far impressed the world, with unprecedented capabilities that emerge in models at large scales. On the vision side, transformer mod-els (i.e., ViT) are following the same trend, achieving the best performance on challenging benchmarks. With the abun-dance of such unimodal models, a natural question arises; do we need also to follow this trend to tackle multimodal tasks? In this work, we propose to rather direct effort to effi-cient adaptations of existing models, and propose to augment
Language Models with perception. Existing approaches for adapting pretrained models for vision-language tasks still rely on several key components that hinder their efficiency. In particular, they still train a large number of parameters, rely on large multimodal pretraining, use encoders (e.g., CLIP) trained on huge image-text datasets, and add significant inference overhead. In addition, most of these approaches have focused on Zero-Shot and In Context Learning, with little to no effort on direct finetuning. We investigate the min-imal computational effort needed to adapt unimodal models for multimodal tasks and propose a new challenging setup, alongside different approaches, that efficiently adapts uni-modal pretrained models. We show that by freezing more than 99% of total parameters, training only one linear pro-jection layer, and prepending only one trainable token, our approach (dubbed eP-ALM) significantly outperforms other baselines on VQA and Captioning across Image, Video, and
Audio modalities, following the proposed setup. The code is available here: https://github.com/mshukor/eP-ALM. 1.

Introduction
Going large scale has led to outstanding performances that consistently improve across tasks, modalities, and do-mains on current benchmarks. Most of the progress so far has been in the vision and language domains. For Com-puter Vision, the ViT family [20] starts from the tiny model with 5M parameters to the enormous ViT-e [14] with 4B pa-Figure 1: Illustration of eP-ALM to adapt unimodal models for multimodal tasks. The Language Model (Decoder) is augmented with perceptual context to steer its text generation. To condition the decoder on a given modality, the [CLS] tokens are extracted from several layers of a modality-specific encoder and then linearly projected before concatenation at different levels of the language decoder. Only unimodal models are used, and all pretrained mod-ules are kept frozen. rameters and the largest ViT-22B with 22B parameters [19].
More captivating, are the scales of Large Language Models (LLMs), such as the BLOOM [70] and OPT [102] families, ranging from hundreds of millions of parameters to 175B, in addition to other models that go beyond 100B [6, 77, 18] up to 1T parameters [26]. These huge scales come with a need for very large pretraining datasets and long training times.
The current prevalent paradigm to solve multimodal tasks, in particular, Vision-Language tasks is to leverage pretrained models, and then further train end-to-end [54, 73, 75, 89, 14] on large image-text datasets. However, the training cost is huge and unaffordable for much of the community, as these approaches always train all model parameters, even after initialization, on a huge amount of data.
With the abundance of unimodal models, a natural ques-tion arises;
Do we need also to follow this trend to tackle multimodal
tasks? or rather direct effort to efficient adaptations of exist-ing models?
Drawing inspiration from the recent work in Augmented
Language Models (ALMs) [65], in this paper, we advo-cate for adapting pretrained LMs to solve multimodal tasks.
Specifically, by augmenting LMs with perceptual encoders.
Several approaches have deviated from the end-to-end-training paradigm by freezing some pretrained modules and training only the adaptation parameters, such as, additional cross-attention [2], vision encoder [85] and Adapters [22].
Even though these approaches have taken a big step to-wards more parameter-efficient models, there are still many costly components that hinder their adoption by the large community, such as the training and inference memory and time cost.
In this work we argue that current approaches are far from optimal and it is possible to find more efficient ap-proaches, in terms of the number of trainable parameters, training data, and compute, to adapt pretrained unimodal models for multimodal tasks. A better alignment of visual and language representations might help to devise extremely efficient adaptation approaches.
To investigate this hypothesis, we go a step further to efficiently leverage LLMs, and propose (1) a new technique to adapt unimodal models by freezing more than 99 % (up to 99.94%) of their parameters, alongside (2) a minimal and challenging setup to adapt pretrained unimodal models for
Image/Video/Audio-Language tasks (e.g., VQA [32, 92], Im-age and Audio Captioning [13, 47]). In this setup, we favor unimodal-only models, avoiding multimodal pretraining or massively trained multimodal encoders, and considering the typical LLMs architecture as the backbone. All that while freezing as much as possible of model parameters. The approach is illustrated in Fig.1.
Specifically, we adopt the publicly released OPT model
[102] and unimodal encoders (e.g., ViT, TimeSformer [5],
AST [30]), which are kept frozen. We finetune directly the adaptation parameters on publicly available benchmarks of downstream tasks such as for VQA, GQA, Image Captioning,
Video QA, Video Captioning, and Audio Captioning.
Based on this setup we investigate different design choices and propose very efficient approaches backed by the following interesting findings:
• Training a single linear layer directly on downstream multimodal datasets, and following the same setup, out-performs other work on Image/Video/Audio-Language tasks. With a few additional trainable parameters and a single learned prepended token, we can significantly improve the performance, while respecting a budget of 1% of trainable parameters, and keeping almost the same inference cost.
• Our approach enjoys better generalization (OOD, Zero-Shot) and is data-efficient (training on 1% of the data achieves 80% of performances) with better few-shot results than other approaches.
• While reaching good performance with small to mid-scale language models (i.e, 350M-2.7B) the improve-ment still increases by jointly scaling both vision and language models. When scaling both models, we can still outperform other approaches with only 0.06% of trainable parameters.
• Existing approaches do not behave well on the proposed challenging setup, without large multi-modal pretrain-ing. 2.