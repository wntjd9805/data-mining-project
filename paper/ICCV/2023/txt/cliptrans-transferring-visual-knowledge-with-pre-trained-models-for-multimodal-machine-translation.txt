Abstract
There has been a growing interest in developing mul-timodal machine translation (MMT) systems that enhance neural machine translation (NMT) with visual knowledge.
This problem setup involves using images as auxiliary infor-mation during training, and more recently, eliminating their use during inference. Towards this end, previous works face a challenge in training powerful MMT models from scratch due to the scarcity of annotated multilingual vision-language data, especially for low-resource languages. Si-multaneously, there has been an influx of multilingual pre-trained models for NMT and multimodal pre-trained models for vision-language tasks, primarily in English, which have shown exceptional generalisation ability. However, these are not directly applicable to MMT since they do not provide aligned multimodal multilingual features for generative tasks. To alleviate this issue, instead of designing complex modules for MMT, we propose CLIPTrans, which simply adapts the independently pre-trained multimodal M-CLIP and the multilingual mBART. In order to align their embed-ding spaces, mBART is conditioned on the M-CLIP features by a prefix sequence generated through a lightweight map-ping network. We train this in a two-stage pipeline which warms up the model with image captioning before the ac-tual translation task. Through experiments, we demonstrate the merits of this framework and consequently push for-ward the state-of-the-art across standard benchmarks by an average of +2.67 BLEU. The code can be found at www.github.com/devaansh100/CLIPTrans. 1.

Introduction
Over the decades, Machine Translation (MT) has evolved from being rule-based [45], to more intricate prob-*Work done as an intern at Boston College
Figure 1: (a) Multimodal machine translation (MMT) mod-els are hard to train due to the scarcity of triplet data, es-pecially for low-resource languages. (b) Our work aims to leverage existing non-triplet pre-trained models for the
MMT task (without image during inference setting). abilistic models [42, 14, 38, 23] and recently to end-to-end deep neural networks [1, 11, 62, 59] giving rise to the sub-domain of Neural Machine Translation (NMT). Most recent
NMT models largely rely on paired textual data and typi-cally make use of transformer-based encoder-decoder mod-els [62, 28] to set impressive benchmarks [35, 49]. With advancements in the transformer’s ability to encode both images and texts in the same latent space [56, 26, 17, 39], there has been a rise in works [33, 34, 66, 54] leveraging images as auxiliary information to provide visual ground-ing to the translation task to enhance MT systems, a setting known as Multimodal Machine Translation (MMT).
For incorporation of the visual input, previous works have employed specifically engineered encoder-decoder ar-chitectures with multimodal attention modules [33, 34, 7, 71, 32, 75] that need to be learned from scratch. Conse-quently, they are forced to balance vision-language align-ment with the translation task. Furthermore, to reduce the
dependence of MMT on images during inference, previous works typically adopt one of two approaches where they either learn a hallucination network to generate image fea-tures from text [30, 37], or use retrieval modules to fetch one or more relevant images [73]. The former requires spe-cially designed losses and difficult optimization while the latter comes with an extra computational cost at test time.
With an increase in popularity of transfer learning meth-ods that make use of task-specific pre-trained unsuper-vised models, recent NMT works have observed a paradigm shift. However, a similar trend has not been witnessed in the MMT domain due to the requirement of data in the form of triplets comprising images and their bilingual captions, which limits transfer learning for three reasons: (i) pre-trained models for NMT are only trained on tex-tual data [12, 13, 61, 68] (ii) existing pre-trained mod-els are either multimodal with English as the only lan-guage [26, 56, 51, 60] or lack decoders for sequence gen-eration [9, 22] (iii) MMT will require a multilingual multi-modal network, which is difficult to train since triplets are expensive to source at the required scale, and existing triplet datasets cannot cover low-resource languages [31].
In this work, we aim to overcome these limitations and simplify the multimodal translation task by employ-ing two independent pre-trained models as aforementioned in (i) and (ii). More specifically, we make use of M-CLIP [9] – a multilingual variant of the pre-trained mul-timodal CLIP [51] encoder – in an optimal training pipeline that tactfully enriches mBART [36] – a pre-trained text-only translation model – with powerful and well-aligned mul-timodal features. CLIP consists of visual and textual en-coders that are trained on a large image-captioning dataset using contrastive learning which endows it with general-ized, transferable representations for a variety of multi-modal tasks [44, 40, 41, 27]. When provided with a text input at test time, M-CLIP essentially acts as a halluci-nation network by providing text embeddings pre-aligned with its visual counterpart. This not only removes the con-straint of requiring images during inference but also inher-ently eliminates the need for hand-engineered architectures with complex training objectives aimed at vision-language alignment [20, 58]. Specifically, we employ a mapping network to transfer M-CLIP embeddings as decoder pre-fix to mBART and train the mBART decoder using a novel two-stage learning pipeline.
In the first stage, we train the mBART decoder for the image-captioning task using a visual-textual decoder prefix sequence computed by a sim-ple, lightweight mapping network from the M-CLIP image encoder. In stage two, the mBART decoder is trained for the translation task, generating decoder prefixes via the M-Interestingly, this mimics the dataset
CLIP text encoder. annotation procedure for MMT datasets which first cap-tions an image, then translates the caption while ensuring visual grounding with the image [54, 3]. Doing so enables transferring visual representations to the multilingual space, while effectively adapting the mBART attention maps to the newly introduced embeddings.
Contributions. (1) We present an architecture, CLIPTrans, that can capitalize on existing pre-trained LMs and multi-modal models, thus simplifying the MMT pipeline by elimi-nating the use of specialized structures and intractable train-ing objectives. (2) We propose a novel transfer-learning ap-proach through a two-stage training pipeline wherein the first stage is a shared captioning task and the second is the translation task. We believe we are one of the first works to showcase the merits of using image captioning for adapt-ing pre-trained models for MMT through a thorough analy-sis and demonstration of quantitative and qualitative results. (3) We surpass the previous state-of-the-art on MMT across two benchmarks by an average of +2.88 BLEU, and an av-erage of +3.64 BLEU for under-resourced languages, with-out using images at test time, which further broadens the applicability of our method. 2.