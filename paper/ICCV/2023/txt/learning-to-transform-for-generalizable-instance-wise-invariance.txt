Abstract
Computer vision research has long aimed to build systems that are robust to transformations found in natural data.
Traditionally, this is done using data augmentation or hard-coding invariances into the architecture. However, too much or too little invariance can hurt, and the correct amount is unknown a priori and dependent on the instance. Ideally, the appropriate invariance would be learned from data and inferred at test-time.
We treat invariance as a prediction problem. Given any image, we predict a distribution over transformations can and average over them to make invariant predictions. Com-bined with a graphical model approach, this distribution forms a ﬂexible, generalizable, and adaptive form of in-variance. Our experiments show that it can be used to align datasets and discover prototypes, adapt to out-of-distribution poses, and generalize invariance across classes. When as data augmentation, our method shows accuracy and robust-ness gains on CIFAR 10, CIFAR10-LT, and TinyImageNet. 1.

Introduction
One of the most impressive abilities of the human vi-sual system is its robustness to geometric transformations.
Objects in the visual world often undergo rotation, transla-tion, etc., producing innumerable variations in appearance.
Nonetheless, we classify them reliably and efﬁciently.
In contrast, modern classiﬁers based on deep learning are brittle [1]. While these methods have achieved super-human accuracy on curated datasets such as ImageNet [2], they are often unreliable in real-world applications [3], leading to poor generalization and even fatal outcomes in systems rely-ing on computer vision [4]. Due to this, robust classiﬁcation has long been an aim of computer vision research [1, 5].
Any robust classiﬁer must encode information about the expected geometric transformations, either explicitly (e.g., through augmentations or architecture) or implicitly (e.g., invariant features). In the case of humans, this knowledge generalizes to novel (but similar) categories for one-shot learning [6]. For unfamiliar categories or poses, we can learn the invariance over time [7]. Finally, while we quickly recognize objects in typical poses, we can also adapt to
Figure 1: Our goal is to build ﬂexible, adaptive, and general-izable invariances. Flexible: Different objects require differ-ent degrees of invariance, and too much invariance can be harmful (e.g. in distinguishing between 6 vs. 9). The ideal invariance is ﬂexible and instance-dependent. Adaptive:
The model should be able to adapt to unexpected (out-of-distribution) poses. The ﬁgure above shows mental rotation, where the symbols in unexpected poses are rotated to a fa-miliar pose before being classiﬁed. Generalizable: Given previous knowledge of objects and their invariances, we should be able to generalize it to new objects. novel “out-of-distribution” poses with processes like mental rotation [8]. These properties help us robustly handle novel categories and novel poses (Figure 1). This paper asks:
Can we replicate this ﬂexible, generalizable, and adaptive invariance in artiﬁcial neural networks?
For some transformations (e.g., translation), the invari-ance can be hard-coded into the architecture. This insight has led to important approaches like Convolutional Neural
Networks [9, 10]. However, this approach imposes severe architecture restrictions and thus has limited applicability.
An alternative approach to robustness is data augmenta-tion [12]. Input data is transformed through a predeﬁned set of transformations, and the neural network learns to perform the task reliably despite these transformations. Its success and wide applicability have made it ubiquitous in deep learn-ing. However, data augmentation is unreliable since the
fθ
L
C
AT
I
T gφ
Figure 2: Our model. (left) The graphical model inspired by the image model underlined in [11]. The image generation model (edges in black) assumes that the Latent Image L depends on class C, and the transformation distribution T depends on both. L and T determine the resulting image I. Notably, in contrast to [11], the transformation distribution depends on both class and latent, and every image does not share a single prototype. Since our goal is classiﬁcation (image → class), we learn the reverse process. The red arrows represent the conditional distributions we rely on. We learn an input-conditional transformation distribution and a classiﬁer. Input-condition augmentation distribution gφ predicts transforms T , which along with the input image I, creates a distribution over the latent images L through the augmentation process AT . Classiﬁer fθ predicts C using L. (middle) Image classiﬁcation pipeline. Our model predicts a distribution over image transformations. Samples from this distribution are passed to a differentiable augmenter which transforms the input image into a set of augmented images. The images are passed to a classiﬁer, and predictions are averaged. (right) Visualization of the learned augmentation distribution for the Mario-Iggy Dataset. Mario-Iggy dataset consists of rotated versions of single Mario/Iggy images. Upright and Upside-down images are classiﬁed as different classes similar to 6 and 9 in digit classiﬁcation. We show two input images and plot the orientation distribution of the transformed versions. Each direction in the polar plot indicates an image orientation.
Dotted lines represent the orientations of samples from the original dataset, and the corresponding curve shows the distribution of orientations after being transformed by our augmenter. The large overlap between red and blue distributions indicates that the post-transformation distribution is approximately invariant to input orientation. learned invariance breaks under distribution shifts and fails to transfer from head classes to tail classes in imbalanced classiﬁcation settings [13].
Both these approaches prescribe the invariances while assuming a known set of transformations. However, the correct set of invariances is often unknown a priori, and a mismatch can be harmful [14, 15, 12]. For instance, in
ﬁne-grained visual recognition, rotation invariance can help with ﬂower categories but hurt animal recognition [16].
A recent line of methods [14, 17, 15] aims to learn the useful invariances. Augerino [14] learns a range of transfor-mations shared across the entire dataset, producing better generalizing models. However, these methods use a ﬁxed range of transformations for all inputs, thus failing to be ﬂex-ible. InstaAug [15] learns an instance-speciﬁc augmentation range for each transformation, achieving higher accuracy on datasets such as TinyImageNet due to its ﬂexibility. How-ever, since InstaAug learns a range for each parameter sepa-rately, it cannot represent multi-modal or joint distributions (e.g., it cannot discover rotations from the set of all afﬁne matrices). Additionally, these approaches fail to consider generalizability and adaptability [6] (Figure 1).
We take inspiration from Learned-Miller et al. [6] and model the relationship between the observed image and its class as a graphical model (Figure 2). Our experiments show that these properties emerge naturally in this framework.
Contributions: (1) We propose a normalizing ﬂow model to learn the image-conditional transformation distribution. (2) Our model can represent multi-modal and joint distribu-tions over transformations, being able to model more com-plex invariances, and (3) helps achieve higher test accuracy on datasets such as CIFAR10, CIFAR10-LongTail, and Tiny-ImageNet. Finally, (4) combined with our graphical model, this model forms a ﬂexible, generalizable, and adaptive form of invariance. It can be used to (a) align the dataset and dis-cover prototypes like congealing [6], (b) adapt to unexpected poses like mental rotation [7], and (c) transfer invariance across classes like GAN-based methods [13]. 2.