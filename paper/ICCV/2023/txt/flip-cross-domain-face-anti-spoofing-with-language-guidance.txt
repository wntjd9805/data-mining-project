Abstract
Face anti-spoofing (FAS) or presentation attack detection is an essential component of face recognition systems de-ployed in security-critical applications. Existing FAS meth-ods have poor generalizability to unseen spoof types, cam-era sensors, and environmental conditions. Recently, vi-sion transformer (ViT) models have been shown to be ef-fective for the FAS task due to their ability to capture long-range dependencies among image patches. However, adap-tive modules or auxiliary loss functions are often required to adapt pre-trained ViT weights learned on large-scale datasets such as ImageNet. In this work, we first show that initializing ViTs with multimodal (e.g., CLIP) pre-trained weights improves generalizability for the FAS task, which is in line with the zero-shot transfer capabilities of vision-language pre-trained (VLP) models. We then propose a novel approach for robust cross-domain FAS by grounding visual representations with the help of natural language.
Specifically, we show that aligning the image representation with an ensemble of class descriptions (based on natural language semantics) improves FAS generalizability in low-data regimes. Finally, we propose a multimodal contrastive learning strategy to boost feature generalization further and bridge the gap between source and target domains. Ex-tensive experiments on three standard protocols demon-strate that our method significantly outperforms the state-of-the-art methods, achieving better zero-shot transfer per-formance than five-shot transfer of “adaptive ViTs”. Code: https://github.com/koushiksrivats/FLIP 1.

Introduction
From personal devices to airport boarding gates, face recognition systems have become a ubiquitous tool for rec-ognizing people. This may be attributed to recent advances in face recognition technology based on deep learning, as well as its simplicity and non-contact nature. However, these systems are vulnerable to face presentation attacks, where an attacker tries to spoof the identity of a bonafide
Figure 1. Area Under ROC Curve (AUC %) and Half Total Error
Rate (HTER %) comparison between our proposed method and state-of-the-art (SOTA). Our method achieves the highest AUC (↑) performance with the lowest HTER (↓) for cross-domain face anti-spoofing on MCIO datasets, surpassing all the SOTA methods. individual with the help of presentation attack instruments (PAI) such as printed photos, replayed videos, or 3D syn-thetics masks [52]. Therefore, face anti-spoofing (FAS) or face presentation attack detection (FPAD) is essential to se-cure face recognition systems against presentation attacks.
Prior works [59, 30, 51, 47, 54, 53, 42] have shown that impressive FAS accuracy can be achieved in intra-domain scenarios, where the training and test distributions are sim-ilar. However, existing FAS methods fail to generalize well to the unseen target domains due to two main reasons: (a) variations due to camera sensors, presentation attack instru-ments, illumination changes, and image resolution cause a large domain gap between the source and target distribu-tions that is inherently hard to bridge; and (b) commonly used FAS benchmark datasets have limited training data, causing the model to overfit to the source domain(s). Con-sequently, achieving robust cross-domain FAS performance has remained an elusive challenge thus far.
The problem of cross-domain FAS has been formulated in different ways in the literature. Unsupervised domain adaptation (UDA) methods [40, 12, 15, 21, 45, 44, 43, 19, 67, 56] make use of the unlabeled target domain data and labeled source domain data to learn a generalized deci-sion boundary. Few-shot learning methods [29, 32, 31, 16]
use a small subset of labeled target domain data during training to learn features that adapt well to the target do-main. However, both these methods assume access to the target domain either in the form of a large set of unla-beled samples or a few labeled samples, which may not always be available. Domain generalization (DG) meth-ods [38, 39, 6, 28, 46, 27, 26, 18, 48, 63, 23] propose to learn domain-agnostic discriminative features from multi-ple source domains that generalize to an unseen target do-main. While zero-shot learning and DG settings are more challenging, they are more applicable in practice.
Recent works [10, 16, 23] have established the effective-ness of vision transformers (ViT) for cross-domain FAS.
Since ViTs [9] split the image into fixed-size patches and have the ability to capture long-range dependencies among these patches, they can independently detect the local spoof patterns and aggregate them globally to make an informed decision. However, these methods have two limitations.
Firstly, these ViTs are learned using only image data and their learning is guided only by the corresponding image labels, which might not be representative enough. This lim-its their generalization ability, especially when presented with limited training data. Secondly, they typically require adaptive modules, additional domain labels, or attack-type information to finetune pre-trained weights. This requires explicit network modifications or custom curation of addi-tional information such as attack type or domain labels.
While multimodal vision-language pre-trained (VLP) models have achieved striking zero-shot performance and good generalization in some applications [60, 66, 13, 36, 68, 20, 35], there is still a debate on whether incorporat-ing language supervision yields vision models with more generalizable representations [8, 37]. Therefore, the ob-jective of this work is to examine the following questions: (i) Can initialization of ViTs using multimodal pre-trained weights lead to better cross-domain FAS performance com-pared to ViTs pre-trained only on images?; (ii) Besides leveraging the image encoder of a VLP model, can the text encoder also be utilized to improve the FAS generalization performance?; and (iii) Can the large domain gap and lim-ited training data availability in FAS be surmounted by ex-ploiting self-supervision techniques during the adaptation of VLP models for the FAS task? The main contributions of this work are as follows:
• We show that direct finetuning of a multimodal pre-trained ViT (e.g., CLIP image encoder) achieves better
FAS generalizability without any bells and whistles.
• We propose a new approach for robust cross-domain
FAS by grounding the visual representation using natu-ral language semantics. This is realized by aligning the image representation with an ensemble of text prompts (describing the class) during finetuning.
• We propose a multimodal contrastive learning strategy, which enforces the model to learn more generalized features that bridge the FAS domain gap even with limited training data. This strategy leverages view-based image self-supervision and view-based cross-modal image-text similarity as additional constraints during the learning process. 2.