Abstract
Humans can easily perceive the direction of sound sources in a visual scene, termed sound source localization.
Recent studies on learning-based sound source localization have mainly explored the problem from a localization per-spective. However, prior arts and existing benchmarks do not account for a more important aspect of the problem, cross-modal semantic understanding, which is essential for genuine sound source localization. Cross-modal semantic understanding is important in understanding semantically mismatched audio-visual events, e.g., silent objects, or off-screen sounds. To account for this, we propose a cross-modal alignment task as a joint task with sound source localization to better learn the interaction between audio and visual modalities. Thereby, we achieve high localiza-tion performance with strong cross-modal semantic under-standing. Our method outperforms the state-of-the-art ap-proaches in both sound source localization and cross-modal retrieval. Our work suggests that jointly tackling both tasks is necessary to conquer genuine sound source localization. 1.

Introduction
Humans can easily perceive where the sound comes from in a scene. We naturally attend to the sounding direction and associate incoming audio-visual signals to understand the event. To achieve human-level audio-visual perception, sound source localization in visual scenes has been exten-sively studied [50, 51, 4, 47, 8, 35, 31, 33, 53, 54, 52, 36, 39, 38, 20]. Motivated by that humans learn from natural audio-visual correspondences without explicit supervision, most of the studies have been developed on a fundamental assumption that audio and visual signals are temporally cor-related. With the assumption, losses of the sound source lo-calization task are modeled by audio-visual correspondence as a self-supervision signal and are implemented by con-∗These authors contributed equally to this work.
Figure 1. A conceptual difference between prior approaches and our alignment-based sound source localization. trasting audio-visual pairs, i.e., contrastive learning.
While these approaches appear to be unsupervised meth-ods, they strongly rely on partial supervision information; e.g., using supervisedly pretrained vision networks [50, 51, 47, 53, 54, 20] and visual objectness estimators for post-processing [39, 38]. Without leveraging such strong initial representations, the performance is degraded. Thus, the pre-vious methods are not purely self-supervised approaches.
Even further, there are recent studies [45, 39, 38] that point out visual objectness bias in existing sound source localiza-tion benchmarks and exploit the objectness prior to improve the localization accuracy. They show that, even without in-teraction between visual and audio signals, a model may achieve strong accuracy in localization by only referring visual signals alone, which is not the true intention of the sound source localization task. In short, the current eval-uation and setting of the sound source localization do not capture the true sound source localization performance.
In this work, we first sort out evaluating sound source localization methods by introducing a cross-modal retrieval task as an auxiliary evaluation task. By this task, we can measure whether the learned representation have the ca-pability to accurately interact between audio and visual modalities; i.e., more fine-grained audio-visual correspon-dence which is essential for genuine sound source localiza-tion. This aspect has been missed in existing sound source
Indeed, our experiments show localization benchmarks. that higher sound localization performance does not guar-antee higher cross-modal retrieval performance.
Second, given this additional criterion, we revisit the importance of semantic understanding shared across au-dio and visual modalities in both sound source localiza-tion and cross-modal retrieval.
In the previous meth-ods [50, 51, 54, 47], the cross-modal semantic alignment is induced by instance-level cross-modal contrastive learn-ing, i.e., cross-modal instance discrimination between vi-sual and audio features. However, they are aided by labels or supervisedly pretrained encoder 2 for easing challenging cross-modal feature alignment. Instead, our method learns from scratch supporting the lack of guidance by incorporat-ing multiple positive samples into cross-modal contrastive learning. Specifically, we construct a positive set for each modality using both multi-view [10] and conceptually sim-ilar samples [17]. Thereby, we enhance feature alignment and achieve high localization performance and strong cross-modal semantic understanding.
We evaluate our method on the VGG-SS and SoundNet-Flickr benchmarks for sound source localization and cross-modal retrieval. As aforementioned, the sound source lo-calization task is closely related to the cross-modal retrieval task, but our experiments show that existing works have a weak performance correlation between them. This im-plies that we need to evaluate both tasks for evaluating the genuine sound source localization. The proposed method performs favorably against the recent state-of-the-art ap-proaches in both tasks.
We summarize the contributions of our work as follows:
• We analyze that sound source localization benchmarks are not capable of evaluating cross-modal semantic un-derstanding, thereby sound source localization methods may perform poorly in cross-modal retrieval tasks.
• We propose semantic alignment to improve cross-modal semantic understanding of sound source localization models.
• We expand semantic alignment with multi-views and con-ceptually similar samples which leads to state-of-the-art performance on both sound source localization and cross-modal retrieval. 2.