Abstract
Active Domain Adaptation (ADA) queries the labels of a small number of selected target samples to help adapt-ing a model from a source domain to a target domain. The local context of queried data is important, especially when the domain gap is large. However, this has not been fully explored by existing ADA works. In this paper, we propose a Local context-aware ADA framework, named LADA, to address this issue. To select informative target samples, we devise a novel criterion based on the local inconsistency of model predictions. Since the labeling budget is usually small, fine-tuning model on only queried data can be inef-ficient. We progressively augment labeled target data with the confident neighbors in a class-balanced manner. Exper-iments validate that the proposed criterion chooses more informative target samples than existing active selection strategies. Furthermore, our full method clearly surpasses recent ADA arts on various benchmarks. Code is available at https://github.com/tsun/LADA. 1.

Introduction
Unsupervised Domain Adaptation (UDA) [6, 17] adapts a model from a related source domain to an unlabeled target domain. It has been widely studied in the past decade. De-spite its success in many applications, UDA is still a chal-lenging task, especially when the domain gap is large [32].
In practical scenarios, it is often allowable to annotate a small number of unlabeled data. The new paradigm of Ac-tive Domain Adaptation (ADA), which queries the label of selected target samples to assist domain adaptation, draws increasing attention recently due to its promising perfor-mance with minimal labeling cost [29, 20, 39, 38].
Traditional Active Learning (AL) methods select unla-beled samples that are uncertain to the model [10] or repre-sentative to the data distribution [27]. Some works combine these two principles to design hybrid criteria [10]. These
AL strategies, however, may be less effective in ADA due to the availability of labeled source data and the distribu-tion shift between source and target domains. Recent ADA works seek to use density weighted entropy [29], combine transferable criteria [5], focus on hard examples [39] or ex-ploit free energy biases [38] to select target samples that are beneficial to domain adaptation.
Despite recent progress in ADA, the local context of queried data has not been fully explored. Different from
Semi-Supervised Domain Adaptation (SSDA) [26, 15] where all labeled target data are given at the beginning of training and fixed afterwards, active querying to obtain la-beled target data and model update interleave during the training of ADA. The local context can guide the selection of target samples that are uncertain and locally representa-tive. It can also be utilized to update models and reduces the tendency to only memorize these newly queried data during fine-tuning [43]. Thus later training rounds can focus on harder cases. In the particular situation when the domain gap is large, it is also safer to trust the neighbors of queried data than other confident but distant samples.
Thus motivated, in this paper, we propose a novel frame-work of Local context-aware Active Domain Adaptation (LADA). A Local context-aware Active Selection (LAS) module is first designed, with a novel criterion based on the local inconsistency of model predictions. During the active selection stage, a diverse subset from the uncertain regions measured by our criterion is selected for querying labels. Then we design a Progressive Anchor-set Augmen-tation (PAA) module to overcome issues from the small size of queried data. Since the labeling budget for each round is usually small, it only requires a small fraction of training epoch before the model predicts well on the newly queried data. Another issue is that the labeled data can be imbal-anced when the target data are long-tailed or the active se-lection focuses on partial classes. Our PAA handles the above-mentioned issues through augmenting labeled target data. Specifically, during each training epoch, we initialize an anchor set with all available queried data and progres-sively augment it with pseudo-labeled confident target data in a class-balanced manner. Target training batches are sam-pled from the anchor set instead to fine-tune the model. We show that choosing confident samples in the neighborhood of queried data overcomes the adversarial effects from false
Figure 1: The framework of LADA. For active querying, the LAS (Local context-aware Active Selection) module selects informative target samples based on the Local Inconsistency (LI) of model predictions. For model adaptation, the PAA (Progressive Anchor set Augmentation) module exploits all queried data and their confident neighbors to fine-tune the model.
The anchor set A is expanded progressively during each training epoch and in a class balanced manner. confident samples that are distant to labeled data.
To demonstrate the effectiveness of exploiting local con-text in ADA, we conduct extensive experiments on various domain adaptation benchmarks. We implement several rep-resentative active selection strategies, and compare them with LAS under the same configurations. Either with the simple model fine-tuning or a strong SSDA method named
MME [26], LAS consistently selects more informative sam-ples, leading to higher accuracies. Equipped with PAA, the full LADA method outperforms state-of-the-art ADA solu-tions on various benchmarks on both standard datasets and datasets with class distribution shift.
In summary, we make the following contributions:
• We advocate to utilize the local context of queried data in ADA, which may guide active selection and improve model adaptation.
• We propose a LAS module with a novel active criterion based on the local inconsistency of class probability predictions. It selects more informative samples than existing active selection criteria.
• We design a PAA module to overcome issues from the small size of queried data.
It progressively supple-ments labeled target data with confident samples in a class-balanced manner.
• Extensive experiments show that the full LADA method outperforms state-of-the-art ADA solutions. 2.