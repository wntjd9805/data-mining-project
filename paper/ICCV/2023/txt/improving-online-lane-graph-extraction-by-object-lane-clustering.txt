Abstract cability of autonomous driving.
Autonomous driving requires accurate local scene un-derstanding information. To this end, autonomous agents deploy object detection and online BEV lane graph extrac-In this tion methods as a part of their perception stack. work, we propose an architecture and loss formulation to improve the accuracy of local lane graph estimates by using 3D object detection outputs. The proposed method learns to assign the objects to centerlines by considering the center-lines as cluster centers and the objects as data points to be assigned a probability distribution over the cluster centers.
This training scheme ensures direct supervision on the re-lationship between lanes and objects, thus leading to better performance. The proposed method improves lane graph estimation substantially over state-of-the-art methods. The extensive ablations show that our method can achieve sig-nificant performance improvements by using the outputs of existing 3D object detection methods. Since our method uses the detection outputs rather than detection method in-termediate representations, a single model of our method can use any detection method at test time. The code will be made publicly available. 1.

Introduction
Accurate road scene understanding is essential for au-tonomous driving. Two of the most important aspects of road scene understanding are lane graph representation and object detection. While the former defines the action envi-ronment of the autonomous agent, the latter provides in-formation on the other traffic agents. The resulting rep-resentations are crucial for downstream tasks such as pre-dicting the motion of agents [16, 26, 41] and planning the ego-motion [3, 13, 19]. Although object detection has to be carried out online, lane graphs are frequently ob-tained from offline generated HD-Maps [27, 44, 35, 40, 11].
However, traffic scenes are highly dynamic and the of-fline maps have to be complemented by online components
[29, 33, 32, 48, 49]. Moreover, the geographically limited coverage of HD-Maps severely limits the widespread appli-Online scene understanding has been studied widely in literature. Some methods focus on Birdâ€™s-Eye-View occu-pancy grid representations [43, 8, 47, 38, 50, 10, 39, 53].
These methods combine the onboard sensor information on a BEV grid with each grid location being a semantic vector representing the properties of that location. While
BEV semantic segmentation provides crucial information for downstream tasks such as drivable area, walkways and pedestrian crossings; they fail to provide adequate informa-tion on the road network. A complementary line of work is extraction of lane boundaries [36, 20, 25]. Building on pre-vious works, HD-Maps aim to provide a more comprehen-sive understanding of the traffic scene by incorporating lane boundaries as well as other static elements such as pedes-trian crossings [29, 33, 32, 49]. However, all these lines of work lack the structured representation that the downstream tasks require. Recently, directly estimating the lane graph from onboard sensors was proposed [6, 7]. The lane graph provides instances of centerlines with traffic directions and provides connectivity of these centerlines.
Some scene understanding methods output object detec-tions as well and it has been shown that the auxiliary task of object detection helps with lane graph extraction perfor-mance [6]. In this paper, we focus on a related yet differ-ent question: Can the object detection results be used to improve the online lane graph extraction? Given the ob-ject detection methods use the same onboard sensors as on-line scene understanding methods and are an indispensable part of the perception stack, using the outputs of object de-tection methods creates no overhead. Moreover, using the outputs rather than intermediate representations means lane graph extraction can use different object detection methods in train and test time and does not require re-training as the object detection method changes. To this end, we devise a network architecture that takes the onboard image and a set of 3D bounding boxes as input and outputs the BEV lane graph. Additionally, we propose a novel clustering based formulation where the centerlines act as cluster centers and the object detections are assigned to clusters. The network learns to output membership probability estimates for each
Figure 1. Input image and the object detection estimates are processed jointly to produce a lane graph with centerlines acting as cluster centers. Each object also outputs a membership probability distribution over the estimated centerlines representing that particular object belonging to estimated centerlines. These membership estimates (blue) are supervised by the targets (green) obtained from true lane graph and object detections. object detection over the estimated centerlines, see Fig. 1.
These estimates are supervised by the membership targets produced by true lane graph and object estimates. When trained with the proposed formulation, the proposed method produces substantially improved results over state-of-the-art. To summarize, our major contributions can be listed as follows. 1. We propose an architecture that uses 3D object detec-tions as inputs and produces lane graphs. 2. We propose a novel formulation that connects the ob-ject detections to centerlines to significantly boost the performance, using an intuitive prior of road scenes. 3. We conduct extensive ablations to validate the design choices as well as the robustness of our method in terms of choice 3D object detection methods. 4. The results obtained by our method are significantly superior to the compared methods in all metrics on benchmark datasets with no runtime sacrifice. 2.