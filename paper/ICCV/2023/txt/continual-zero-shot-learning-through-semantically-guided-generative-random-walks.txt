Abstract
Learning novel concepts, remembering previous knowl-edge, and adapting it to future tasks occur simultaneously throughout a human’s lifetime. To model such comprehensive abilities, continual zero-shot learning (CZSL) has recently been introduced. However, most existing methods overused unseen semantic information that may not be continually accessible in realistic settings. In this paper, we address the challenge of continual zero-shot learning where unseen infor-mation is not provided during training, by leveraging genera-tive modeling. The heart of the generative-based methods is to learn quality representations from seen classes to improve the generative understanding of the unseen visual space. Mo-tivated by this, we introduce generalization-bound tools and provide the ﬁrst theoretical explanation for the beneﬁts of generative modeling to CZSL tasks. Guided by the theoret-ical analysis, we then propose our learning algorithm that employs a novel semantically guided Generative Random
Walk (GRW) loss. The GRW loss augments the training by continually encouraging the model to generate realistic and characterized samples to represent the unseen space. Our algorithm achieves state-of-the-art performance on AWA1,
AWA2, CUB, and SUN datasets, surpassing existing CZSL methods by 3-7%. The code has been made available here https://github.com/wx-zhang/IGCZSL . 1.

Introduction
Researchers have devoted signiﬁcant effort to developing
AI learners to mimic human cognition. One such endeavor is zero-shot learning (ZSL), which aims to identify unseen classes without accessing any of their images during train-ing. However, human zero-shot learning abilities improve dynamically over time. As individuals acquire more knowl-edge of seen tasks, they become better at recognizing unseen tasks. To evaluate the zero-shot learning in such a dynamic
*Work done during internship at KAUST seen-unseen distribution, the continual zero-shot learning problem (CZSL) has been proposed [53]. CZSL emulates the continuous learning process of a human’s life, where the model continually sees more classes from the unseen world and is evaluated on both seen and unseen classes. This CZSL skill, may it get maturely developed to the world scale, has the potential to accelerate research in species discovery, for example, as known species grow continually, but close to 90% of the species are not yet discovered[55].
Generative models (e.g., GANs[26]) have made signiﬁ-cant progress in producing photorealistic images by learning high-dimensional probability distributions. This ability moti-vated researchers to adapt GANs to ZSL to generate missing data of unseen classes conditioning on unseen semantic infor-mation, known as generative-based ZSL. Training the classi-ﬁer on synthetic unseen samples can reduce model predic-tion bias towards seen classes and thus achieves competitive zero-shot learning performance [38, 58, 42]. Some CZSL works directly adopt this framework continually, known as transductive continual zero-shot learning [25, 36]. How-ever, in CZSL, the unseen world changes dynamically and unexpectedly, making it unrealistic to use prior knowledge about unseen classes[53]. When we do not assume access to unseen semantic information in the CZSL setting, which is known as inductive continual zero-shot learning, most existing methods struggle to perform well, as we show in our experiments. Furthermore, the theoretical understanding of how zero-shot learning beneﬁts from synthetic data is lim-ited, which poses an obstacle to developing purely inductive continual zero-shot methods. Recent analyses of training generative models with synthetic data [8] provide a possible avenue for developing the desired theoretical explanation.
This led us to develop a generalization-bound tool to under-stand the learning mechanism in generative-based CZSL and further develop inductive methods based on it.
In our analysis, we have identiﬁed it is crucial to reduce the distance between the generated and actual visual space of unseen classes. This requires the model to generate realistic samples to represent unseen space to augment the training 1
Figure 1. Semantically guided generative random walk (GRW): At each time step, new classes are added to the seen classes space, and the random walk starts from each seen class center (in green) and transitions through generated samples of hallucinated classes (in orange), then the landing probability distribution over the seen classes is predicted. The GRW loss encourages the generated samples from the hallucinated classes to be distinguishable from the seen classes by encouraging the landing probability over seen classes starting from any seen center to be uniformly distributed, and hence hard to classify to any seen class. of the classiﬁer. However, the lack of ground truth semantic descriptions for unseen classes and the lack of previously seen classes data often leads to the generated samples col-lapsing to the seen classes. A similar problem has been addressed in generating novel style artworks, where GAN training is augmented to encourage the generated styles to deviate from existing art style classes [15, 51, 27, 30, 28, 31].
Drawing inspiration from the improved feature representa-tion achieved by generative models in producing novel art, and the connection between the ability to generate novel styles in art generation and to generate samples to represent the unseen space in generative-based CZSL, we propose a purely inductive, Generative Random Walk (GRW) loss, guided only by semantic descriptions of seen classes.
In each continual learning task, we ﬁrst hallucinate some classes by interpolating on or sampling from a learnable dictionary based on the current and previous classes, with the belief that the realistic classes, both seen and unseen, should be relatable to each other [16, 17]. We then gener-ate samples from the hallucinated classes. To prevent the generated samples of hallucinated classes from collapsing to the seen classes, we apply the GRW loss, as illustrated in Figure 1. We perform a random walk starting from the seen class and moving through generated examples of hal-lucinated classes for R steps, as described in detail later in
Section 5.2.2. The GRW loss encourages high transition probabilities to the realistic unseen space by deviating from the visual space of the seen classes and avoiding less real-istic areas. The resulting representations are both realistic and distinguishable from seen classes, which enhances the generative understanding of unseen classes. This approach is particularly effective when the model is updated continually, as it enables the model to use the newly learned knowledge to improve further the generated examples of hallucinated classes. Our contributions lie in
• We provide a theoretical analysis of continual zero-shot learning. This analysis guides us to use proper signals to make up for the missing unseen information.
We present these generalization-bound tools for the analysis in Section 4.
• Guided by the analysis, we develop a method for purely inductive continual zero-shot learning; described in detail in Section 5. Our method, ICGZSL, ﬁrst provides two ways to hallucinate classes, i.e. interpolation of two seen classes and learning a dictionary based on the seen classes. Then, we integrate our introduced semantically guided Generative Random Walk (GRW) loss to generate distinguishable and realistic samples to represent unseen classes.
• We performed comprehensive experiments (Section 6) that demonstrate the effectiveness of our approach.
Speciﬁcally, our model achieves state-of-the-art results on standard continual zero-shot learning benchmarks,
AWA1, AWA2, CUB, SUN, and performs often better than transductive methods 2.