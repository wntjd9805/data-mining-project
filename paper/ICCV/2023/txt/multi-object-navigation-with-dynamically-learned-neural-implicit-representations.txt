Abstract
Understanding and mapping a new environment are core abilities of any autonomously navigating agent. While classi-cal robotics usually estimates maps in a stand-alone manner with SLAM variants, which maintain a topological or met-ric representation, end-to-end learning of navigation keeps some form of memory in a neural network. Networks are typically imbued with inductive biases, which can range from vectorial representations to birds-eye metric tensors or topological structures. In this work, we propose to structure neural networks with two neural implicit representations, which are learned dynamically during each episode and map the content of the scene: (i) the Semantic Finder pre-dicts the position of a previously seen queried object; (ii) the
Occupancy and Exploration Implicit Representation encap-sulates information about explored area and obstacles, and is queried with a novel global read mechanism which directly maps from function space to a usable embedding space. Both representations are leveraged by an agent trained with Re-inforcement Learning (RL) and learned online during each episode. We evaluate the agent on Multi-Object Navigation and show the high impact of using neural implicit represen-tations as a memory source. 1.

Introduction
Autonomous navigation in complex unknown 3D environ-ments from visual observations requires building a suit-able representation of the environment, in particular when the targeted navigation task requires high-level reasoning.
Whereas classical robotics builds these representations ex-plicitly through reconstructions, possibly supported through machine learning, end-to-end training learns them automat-ically either from reward, by imitation learning or through self-supervised objectives.
While spatial representations can emerge even in unstruc-tured agents, as shown in the form of grid-cells in artificial
[16, 3] and biological agents [25], spatial inductive biases
Figure 1. We propose two implicit representations as inductive biases for autonomous agents — both are learned online during each episode. ➁ a semantic representation fs predicts positions x from goals g given as semantic codes. We show Ground Truth object positions (rectangles) and predictions (round; radius shows uncertainty, unit-less, as an illustration). Blue and pink objects have been observed, but not the yellow target. ➂ A structural represen-tation fo predicts occupancy and exploration s from positions x; we provide a global read which directly maps from function space fo (represented by trainable weights θo) to a context embedding e used by the agent. ➃ shows the reconstruction produced by a decoder Dec during training. Orange=navigable, Green=Obstacles,
Blue=Unexplored. ➀ a ground-truth map is shown for reference, simulating a fully explored scene. can support learning actionable spatial representations and decrease sample complexity. Popular inductive biases are metric maps [44, 5, 26], topological maps [6, 12] and re-cently, self-attention, adapting transformers [55] to sequen-tial decision making and navigation [20, 18, 13, 47]. The chosen representation should support robust estimation of navigable space even in difficult conditions, mapping fea-tures and objects of interest, as well as querying and reusing this information at a later time. The representation should be as detailed as required, span the full (observed) scene, easy to query, and efficient to read and write to, in particular when training is done in large-scale simulations.
Our work builds on neural fields and implicit represen-tations, a category of models which represent the scene ge-ometry, and eventually the semantics, by the weights of a trained neural network [57]. They have the advantage of avoiding the explicit choice of scene representation (e.g. vol-ume, surface, point cloud etc.) and inherently benefit from the generalization abilities of deep networks to interpolate and complete unobserved information. Implicit represen-tations have demonstrated impressive capabilities in novel view synthesis [40, 51], and have potential as a competitive representation for robotics [42, 32, 52, 1]. Their continu-ous nature allows them to handle level of detail efficiently through a budget given as the amount of trainable weights.
This allows to span large environments without the need of discretizing the environment and handling growing maps.
We explore and study the potential of implicit represen-tations as inductive biases for visual navigation. Similar to recent work in implicit SLAM [52], our representations are dynamically learned in each episode. Going beyond, we exploit the representation dynamically in one of the most challenging visual navigation tasks, Multi-Object Naviga-tion [56]. We introduce two complementary representations, namely a query-able Semantic Finder trained to predict the scene coordinates of an object of interest specified as input, and an Occupancy and Exploration Implicit Representation, which maps 2D coordinates to occupancy information, see
Figure 1. We address the issue of the efficiency of query-ing an implicit representation globally by introducing a new global read mechanism, which directly maps from function space, represented through its trainable parameters, to an embedding summarizing the current status of occupancy and exploration information, useful for navigation. Invariance w.r.t. reparametrization of the queried network is favored (but not enforced) through a transformer based solution. Our method does not require previous rollouts on the scene for pre-training or building a representation.
Our work targets a fundamental aspect of visual and se-mantic navigation, the mapping of key objects of interest.
MultiON is currently one of the few benchmarks which eval-uates it. As argued in previous literature [5, 56], only sequen-tial tasks, where objects have to be found in a given order, allow object level mapping to emerge directly from reward.
This follows from the observation that an agent trained to find and retrieve a single object per episode (from reward) is not required to map seen target objects, as observing them directly leads to a reactive motion towards them.
Our contributions can be summarized as follows: (i) We propose two implicit representations for semantic, occupancy and exploration information, which are trained online during each episode; (ii) We introduce a new global read procedure which can extract summarizing context information directly from the function itself; (iii) We show that the representa-tions obtain performance gains compared to classical neural agents; (iv) We evaluate and analyze key design choices, the representation’s scaling laws and its capabilities of lifelong learning. 2.