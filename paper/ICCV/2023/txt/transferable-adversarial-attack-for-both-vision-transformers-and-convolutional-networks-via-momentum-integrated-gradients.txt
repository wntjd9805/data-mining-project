Abstract
Visual Transformers (ViTs) and Convolutional Neural
Networks (CNNs) are the two primary backbone struc-tures extensively used in various vision tasks. Generat-ing transferable adversarial examples for ViTs is difﬁcult due to ViTs’ superior robustness, while transferring ad-versarial examples across ViTs and CNNs is even harder, since their structures and mechanisms for processing im-ages are fundamentally distinct. In this work, we propose a novel attack method named Momentum Integrated Gradi-ents (MIG), which not only attacks ViTs with high success rate, but also exhibits impressive transferability across ViTs and CNNs. Speciﬁcally, we use integrated gradients rather than gradients to steer the generation of adversarial per-turbations, inspired by the observation that integrated gra-dients of images demonstrate higher similarity across mod-els in comparison to regular gradients. Then we acquire the accumulated gradients by combining the integrated gra-dients from previous iterations with the current ones in a momentum manner and use their sign to modify the per-turbations iteratively. We conduct extensive experiments to demonstrate that adversarial examples obtained using
MIG show stronger transferability, resulting in signiﬁcant improvements over state-of-the-art methods for both CNN and ViT models. 1.

Introduction
Vision Transformers (ViTs) have become increasingly popular and are widely adopted [1, 4, 5, 11], following the success of Convolutional Neural Networks (CNNs) [16, 25, 33, 34], due to their impressive performance. ViTs process images as sequences of patches and use self-attention to model global dependencies among them, while CNNs ex-ploit spatial structures of images using convolutional ﬁlters to capture local features like edges and textures. Regard-less of these different image processing mechanisms used in
CNNs and ViTs, their widespread applications in visual do-Integrated gradients of original image (top row), ad-Figure 1. versarial images generated with FGSM [14] (middle row) and our Momentum Integrated Gradients (MIG) method (bottom row).
Adversarial examples are crafted on source model DeiT-S [43] and used to attack target model ViT-B [11]. Compared to FGSM, ad-versarial image generated using MIG exhibits more consistent in-tegrated gradients on both source and target models, indicating that
MIG effectively interferes with both models, leading them to focus on unimportant background regions rather than the main objects. mains emphasize the importance of developing adversarial examples that can attack both types of models, which helps identify models’ vulnerabilities before their deployment in real-world scenarios.
In adversarial attacks, a malicious attacker perturbs an input image by applying a small, human-imperceptible per-turbation to break the prediction of a machine learning model [6, 12, 14, 49]. Depending on whether the attacker has full access to the target victim model, adversarial attack methods can be classiﬁed into two types, white-box attacks and black-box attacks. Considering that the target models are usually not so readily accessible, generating a transfer-able adversarial example from a white-box surrogate model and then transferring it to the target black-box model is a
common strategy for attacking black-box models. There-fore, the transferability of adversarial perturbations plays a critical role in adversarial attacks.
However, due to the differences between ViTs and CNNs mentioned earlier, previous attack methods designed for
CNNs show very limited transferability when applied to
ViTs. This leaves obstacles for attacking black-box ViTs us-ing transfer-based attack schemes, especially because ViTs are shown to be more robust than CNNs [3, 29] and few adversarial attacks against ViTs have been studied.
In this work, we propose a novel attack method based on Integrated Gradients and Momentum iterative strategy, called Momentum Integrated Gradients (MIG) attack. MIG can not only successfully attack both ViTs and CNNs, but also show better transferability across models. That is, ad-versarial perturbations generated on the white-box model are still likely to cause the black-box target model make mistakes, regardless of its architecture.
Speciﬁcally, we ﬁrst employ Integrated Gradients (IG)
[39] method to compute the saliency score [19] of model prediction with respect to input. The sign of integrated gra-dients is then used to guide the updating of the perturba-tions. As an attribution method, integrated gradients reﬂect the sensitivity of model’s outputs to inputs at different loca-tions. Intuitively, there should be more pronounced pertur-bations at locations that have greater impacts on the model to have a sufﬁcient impact on output. Besides, IG satisﬁes an excellent property implementation invariance, which fur-ther improves the transferability of adversarial examples.
Furthermore, we update perturbations with momentum-based iterative strategy to obtain better attack performance as in [9]. This strategy is similar to momentum gradient de-scent algorithm [31], where the gradient of the current iter-ation is accumulated by adding the velocity vector obtained from previous iterations. Applying momentum in MIG helps accumulate the impact of historical gradients, speed-ing up perturbation updating and making the loss function increase faster. Additionally, it helps to iteratively jump out of poor local optima caused by model-speciﬁc noise and
ﬁnd the globally optimal perturbations. As a result, these perturbations transfer better across different models.
As an example, we visualize integrated gradients for a clean image and adversarial images generated using FGSM
[14] and MIG in Figure 1. Adversarial image using MIG ex-hibits similar integrated gradients on both the source model (DeiT-S [43]) and the target model (ViT-B [11]). This ob-servation indicates that MIG effectively misleads both mod-els to prioritize irrelevant regions instead of the main object.
We conduct extensive experiments on the ImageNet val dataset [35]. Experiment results demonstrate that MIG sig-niﬁcantly enhances the transferability of adversarial exam-ples across ViTs and CNNs. Compared to classical attacks such as FGSM [14], PGD [26], advanced attack MI [9], and even transfer-based attack methods speciﬁcally designed for
ViTs [28], MIG achieves state-of-the-art attack success rate under a small perturbation budget.
We brieﬂy summarize our main contributions as follows.
• We propose Momentum Integrated Gradients (MIG) attack method based on Integrated Gradients and Mo-mentum updating strategy, which can attack ViTs with high transfer attack success rate.
• The transferability of adversarial perturbations gen-erated using MIG outperforms SOTA attack methods when transferring across ViT and CNN models, which has been overlooked in prior research.
• We empirical demonstrate that attribution-based trans-fer attacks are valid for ViTs. This suggests an intrinsic connection between model interpretability and model robustness against attacks. 2.