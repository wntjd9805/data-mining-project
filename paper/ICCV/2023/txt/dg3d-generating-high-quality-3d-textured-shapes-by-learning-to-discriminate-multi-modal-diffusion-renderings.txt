Abstract
Many virtual reality applications require massive 3D content, which impels the need for low-cost and efficient modeling tools in terms of quality and quantity. In this pa-per, we present a Diffusion-augmented Generative model to generate high-fidelity 3D textured meshes that can be directly used in modern graphics engines. Challenges in directly generating textured mesh arise from the insta-bility and texture incompleteness of a hybrid framework which contains conversion between 2D features and 3D space. To alleviate these difficulties, DG3D incorporates a diffusion-based augmentation module into the min-max game between the 3D tetrahedral mesh generator and 2D renderings discriminators, which stabilizes network opti-mization and prevents mode collapse in vanilla GANs. We also suggest using multi-modal renderings in discrimina-tion to further increase the aesthetics and completeness of generated textures. Extensive experiments on the public benchmark and real scans show that our proposed DG3D outperforms existing state-of-the-art methods by a large margin, i.e., 5% ∼ 40% in FID-3D score and 5% ∼ 10% in geometry-related metrics. Code is available at https://github.com/seakforzq/DG3D. 1.

Introduction
The production of 3D content plays a crucial role in the film, gaming, simulation, and social industries. As the demand continues to grow, manually made 3D mod-els could not keep up with the expansion of the industries.
Researchers have made substantial progress in generating geometry for 3D models [9, 10, 22, 27, 35, 2, 28], but the corresponding textures received less attention. While some methods have considered texture generation [3, 25, 41, 36], most of them infer textures based on fixed geometric inputs, which means that they are not fully generative models and do not take into account the interdependent relationship be-tween geometry and texture. 3D-aware image generation [4, 24, 34, 3], a related re-search field with textured mesh generation, aims to syn-thesize images from all viewpoints. These methods en-sure consistency between different views by incorporating the priority constraint of volume rendering. They can pro-duce high-resolution images with a super-resolution net-work. However, though the shapes can always be extracted from the density field by setting specific levels-set values, the quality of the extracted shapes is often much lower than the quality of the synthetic novel views. Since conver-sion from implicit representations to explicit meshes sig-nificantly degrades fidelity, methods that directly optimize explicit meshes are urgently needed.
Recently, differentiable mesh reconstruction [11, 33, 12] has shed light on the generation of textured mesh.
DefTet [11] utilizes a neural network to deform an ini-tial tetrahedral mesh’s vertices and predict the occupancy for each tetrahedron. DMTet [33] instead reconstructs the shapes by predicting the SDF defined on a deformable tetra-hedral grid. Then it converts the SDF to a surface mesh by a novel differential Marching Tetrahedral layer. GET3D [12] further incorporates DMTet [33] in StyleGAN2 [18] and uses a hybrid tri-plane representation for transformation from 2D feature space to 3D tetrahedral space. It extracts textured meshes by the differentiable Marching Tetrahedron layer and adopts DiffRast [20] to render the textured meshes as RGB images and silhouettes for discrimination. Though
GET3D [12] achieves an impressive performance, a poten-tial deficiency that causes instability in training with ren-dered images is the inadequate 3D supervision that discrim-inators can receive. As a result, we often observe mode col-lapse in such a paradigm that combines a 3D generator and multiple 2D discriminators [12]. Moreover, we find that di-rect discrimination on the entire image results in texture in-completeness, significantly degrading the quality of shapes.
Figure 1: We export the generated shapes in .f bx format and render them with a transparent background. We add sunlight in the scene so the shadow effect can be observed. DG3D can generate a fine texture with arbitrary topology.
Luckily, the latest achievements in 2D image generation and texture generation have motivated us. To facilitate high-resolution image data generation, Wang et al. [38] propose a method that combines the diffusion model with GANs and successfully generates more realistic images with higher stability. Furthermore, for the quality of shapes, Textu-rify [36] suggests taking a union of normals, curvature, and patches as the input of discriminators.
Based on the above, we propose DG3D, a novel 3D
GAN-based paradigm augmented by an adaptive diffusion-augmented module, to properly deal with mode collapse and enhance the robustness of both generation and dis-crimination. To further improve the texture completeness of the generated shapes, we set up different discriminators for multi-modal renderings.
In the forward stage, DG3D first generates 2D features and aligns the features to the 3D tetrahedral space by a hybrid tri-plane representation.
Then DG3D utilizes the differentiable Marching Tetrahe-dron layer [33] to extract the underlying geometry. Mean-while, the corresponding texture is generated by another tri-plane and aligned to the geometry by 3D coordinates.
The textured mesh is further rendered as RGB, alpha trans-parency, and patch images. The multi-modal renderings will be fed through the adaptive diffusion-augmented mod-ule and corresponding discriminators sequentially. DG3D is end-to-end trainable and outperforms the baseline [12] by a large margin. As depicted in Fig. 1, DG3D faithfully generates feasible shapes with high-fidelity textures. Note, despite that DG3D introduces extra parameters of discrim-inators and operations on renderings, it maintains the same inference speed as [12] as they share a common generator.
Our main contributions include: 1) We propose a hy-brid generative model which combines a generative adver-sarial network with a novel adaptive diffusion-augmented module, achieving better performance in 3D textured mesh generation. 2) We design multiple 2D discriminators for multi-modal diffusion-renderings from the textured mesh, which provide more supervision information and signifi-cantly boost the generated quality. 3) We conduct extensive experiments and ablations to demonstrate the superiority of
DG3D, analyze the limitations, and discuss applications and feasible improvements in future work. 2.