Abstract
Currently, one main research line in designing a more efficient vision transformer is reducing the computational cost of self attention modules by adopting sparse attention
In contrast, we pro-or using local attention windows. pose a different approach that aims to improve the per-formance of transformer-based architectures by densifying the attention pattern. Specifically, we proposed forward cross attention for hybrid vision transformer (FcaFormer), where tokens from previous blocks in the same stage are secondary used. To achieve this, the FcaFormer leverages two innovative components: learnable scale factors (LSFs) and a token merge and enhancement module (TME). The
LSFs enable efficient processing of cross tokens, while the
TME generates representative cross tokens. By integrat-ing these components, the proposed FcaFormer enhances the interactions of tokens across blocks with potentially dif-ferent semantics, and encourages more information flows to the lower levels. Based on the forward cross attention (Fca), we have designed a series of FcaFormer models that achieve the best trade-off between model size, compu-tational cost, memory cost, and accuracy. For example, without the need for knowledge distillation to strengthen training, our FcaFormer achieves 83.1% top-1 accuracy on Imagenet with only 16.3 million parameters and about 3.6 billion MACs. This saves almost half of the parameters and a few computational costs while achieving 0.7% higher accuracy compared to distilled EfficientFormer. Code is available at https://github.com/hkzhang-git/
FcaFormer 1.

Introduction
With the rapid adoption of transformer structures in the computer vision community, several types of atten-tion patterns have been proposed to enhance the perfor-mance or speed of transformer models.
For instance,
ViT [5] employs the vanilla global multi-head self-attention,
Swin Transformers [20] uses local windowed attention,
MaxViT [32] incorporates grid attention across interleaved tokens, and Dynamic ViT [28] utilizes attention on progres-sively pruned tokens. These approaches aim to sparsify the attention patterns of the original ViT to achieve a better trade-off between speed and accuracy.
In contrast, we propose a new model block as well as a family of models called FcaFormer, which improves the performance of vision transformers by further densifying the attention patterns at a limited extra cost. Specifically, we propose to connect the input of the standard multi-head at-tention (MHA) module with extra tokens transformed from previous blocks in the same stage, while still restricting the attention module to output the original amount of tokens. To further reduce the computational cost, we merge the tokens from previous blocks by using depthwise convolutions with large strides. These tokens are further calibrated by scaling them with learned parameters, before being taken into the attention units in subsequent blocks.
The new forward cross attention connection has several advantages: 1) it helps transformers further exploit the in-teractions of tokens across different levels; 2) it reuses the previously generated tokens so that some of the informa-tion no longer needs to be preserved by the subsequent transformer operations, leading to potentially smaller mod-els with similar accuracy; 3) similar to the residual connec-tions in ResNet, this extra cross layer connection encour-ages more information flows to the lower levels of the net-work, which further accelerate the convergence.
The newly densified connections come with a limited in-crease in computational cost. As explained in Section 3.3, this cost increase is linear rather than quadratic, since we keep the number of output tokens the same as in standard
ViTs. Furthermore, most of the computation cost in most hybrid vision transformer architectures is in the feed for-ward network (FFN) rather than the MHA part of trans-former blocks. Thus, the linear growth of computational complexity from densified connections does not signifi-cantly affect the overall computation cost. Finally, to further reduce the number of extra inputs, we use depthwise con-volutions with large kernels and long strides to aggregate tokens from previous blocks.
We have incorporated the proposed Fca design into two
typical classes of transformer models: the plain ViT model used in DeiT, and the hybrid ConvNet and transformer structures frequently seen in recent works [24, 9, 17]. Our experiments demonstrate that the Fca block can seamlessly replace the corresponding transformer blocks in these ar-chitectures, leading to significantly improved performance compared to their corresponding baselines. Specifically,
FcaFormer-L1 achieves a top-1 accuracy of 80.3% with approximately 6.2 million parameters and about 1.4 bil-lion MACs. This is achieved while saving almost half the number of parameters, and achieving 1.1% higher accuracy compared to the recently proposed EfficientFormer. Table 2 displays the comparison results.
The contribution of this paper is summarized as follows.
• Opposite to many recent works that use sparse atten-tions to improve transformer models, we propose to design more efficient models by densifying the atten-tion connection patterns, which open up a new and worthwhile research avenue for consideration.
• We propose the FcaFormer block, which leverages ex-isting tokens and enhance interactions across different levels. To achieve this, we introduce two new com-ponents: learnable scale factors (LSFs) and a token merge and enhancement module (TME). The LSFs allow us to effectively process cross tokens, while the TME generates representative cross tokens. To-gether, these components improve the performance of the FcaFormer models.
• Based on the proposed FcaFormer block, we con-structed several new models which have demonstrated better performance than various other recently pro-posed models. 2.