Abstract 3D pose transfer is a challenging generation task that aims to transfer the pose of a source geometry onto a tar-get geometry with the target identity preserved. Many prior methods require keypoint annotations to find correspon-dence between the source and target. Current pose trans-fer methods allow end-to-end correspondence learning but require the desired final output as ground truth for supervi-sion. Unsupervised methods have been proposed for graph convolutional models but they require ground truth corre-spondence between the source and target inputs. We present a novel self-supervised framework for 3D pose transfer which can be trained in unsupervised, semi-supervised, or fully supervised settings without any correspondence labels.
We introduce two contrastive learning constraints in the la-tent space: a mesh-level loss for disentangling global pat-terns including pose and identity, and a point-level loss for discriminating local semantics. We demonstrate quantita-tively and qualitatively that our method achieves state-of-the-art results in supervised 3D pose transfer, with compa-rable results in unsupervised and semi-supervised settings.
Our method is also generalisable to unseen human and an-imal data with complex topologies†. 1.

Introduction 3D pose transfer [35, 40, 47, 33] is a challenging gener-ation task in which the pose of a source geometry is trans-ferred to a target geometry whilst preserving the identity of the target geometry (see top half of Figure 1). This has many potential applications in areas including animation, human modelling, virtual reality, and more.
It also pro-vides a more affordable way of generating synthetic 3D data which can be expensive to produce in the real world.
One of the main challenges of 3D pose transfer is that current methods still put certain requirements on their train-ing data making it difficult to collect and expensive to an-*Corresponding author email: j.sun19@imperial.ac.uk.
†Code: https://github.com/justin941208/MAPConNet.
Figure 1. Overview of our framework. Top: our pose transfer pipeline. Bottom: our contrastive learning scheme, with a mesh-level loss for disentangling global pose and identity and a point-level loss for discriminating local semantics. notate. One requirement is having correspondence labels, which are pairs of vertices that correspond to each other se-mantically between two point clouds or meshes. Many prior pose transfer methods either require ground truth correspon-dence [35, 4, 2, 43, 47] or simply neglect the issue [40]. Re-quiring ground truth correspondence is costly, and neglect-ing correspondence would adversely impact model perfor-[33] proposed learning a correspondence module mance. based on optimal transport in an end-to-end fashion through the pose transfer task. However, their method is supervised and requires the mesh with the desired pose and identity as ground truth. This puts another requirement on the dataset: having multiple subjects performing exactly the same set of poses, which is unfeasible. [47] proposed an unsuper-vised approach for registered meshes which only requires each subject to perform multiple poses and they do not have to align exactly across subjects – a more practical require-ment for real datasets [6, 22]. However, their approach is based on graph convolutional networks (GCNs) and ARAP deformation which require ground truth correspondence.
We propose a self-supervised framework (Figure 1) for 3D pose transfer with Mesh And Point Contrastive learn-ing, MAPConNet, requiring no correspondence labels or
target outputs with the desired pose and identity as ground truth for supervision. It can be applied in supervised, unsu-pervised, and semi-supervised settings, and does not need the pose and identity inputs to have the same ordering or number of points. We propose to adapt the unsupervised approach by [47] for pose transfer on unaligned meshes, using [33] as our baseline. To prevent the network from exploiting shortcuts in unsupervised learning that circum-vent the desired objective, we introduce disentangled latent pose and identity representations. To strengthen the disen-tanglement and guide the learning process more effectively, we propose mesh-level contrastive learning to force the model’s intermediate output to have matching latent iden-tity and pose representations with the respective inputs. To further improve the quality of the model’s intermediate out-put as well as the correspondence module, we also propose point-level contrastive learning, which enforces similarity between representations of corresponding points and dis-similarity between non-corresponding ones. In summary:
• We present MAPConNet, a novel self-supervised net-work for 3D pose transfer with Mesh And Point Con-trastive learning. Our model requires no ground truth correspondence or target outputs for supervision.
• We introduce two levels of contrastive learning con-straints, with a mesh-level loss for global disentangle-ment of pose and identity and a point-level loss for dis-crimination between local semantics.
• We achieve state-of-the-art results through extensive experiments on human and animal data, and demon-strate competitive and generalisable results in super-vised, unsupervised, and semi-supervised settings. changing the target’s identity. Deformation transfer meth-ods [35, 4, 2, 43, 44, 18] require a template pose across dif-ferent identities and sometimes also correspondence labels, which are unavailable in our and most realistic settings.
Image-to-image translation methods [48, 8, 16, 24, 37, 36] have also been repurposed for pose transfer due to their rel-evance. [13] used CycleGAN [48] for pose transfer but re-quires retraining for each new pair of identities. [3] refor-mulated the problem as “identity transfer” but required ver-tex correspondence. [40] used SPADE normalisation [24] to inject target identity into the source.
[33] added cor-respondence learning to [40] and improved the normalisa-tion method. However, both require ground truth outputs.
[47] proposed an unsupervised framework but requires cor-respondence labels. We do not require ground truth out-puts or correspondence labels.
[34] proposed a dual re-construction objective in a similar spirit to [47] to enable
In contrast, our approach unsupervised learning in [33]. is to force the model to learn disentangled latent pose and identity codes and impose mesh- and point-level contrastive losses on them, which improves performance in both super-vised and unsupervised settings.
Self-supervised learning on 3D data. Self-supervised learning is the paradigm of automatically generating super-visory signals in training and has been successful in 2D
[32, 9, 25, 46, 14, 7, 37]. Some 2D approaches have also been adopted for 3D data, such as rotation [26] and com-pletion [39], but we choose contrastive learning as it can be easily adapted to suit our needs. Most existing contrastive learning approaches for 3D data [45, 42, 31, 10, 1] focus on learning invariance across views or rigid transformations for scenes or simple objects, whereas we address more fine-grained patterns such as identity, pose, and correspondence for complex shapes including humans and animals. 2.