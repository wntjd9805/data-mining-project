Abstract
Vision-Language Pretraining (VLP) has significantly im-proved the performance of various vision-language tasks with the matching of images and texts.
In this paper, we propose VL-Match, a Vision-Language framework with En-hanced Token-level and Instance-level Matching. At the to-ken level, a Vision-Language Replaced Token Detection task is designed to boost the substantial interaction between text tokens and images, where the text encoder of VLP works as a generator to generate a corrupted text, and the mul-timodal encoder of VLP works as a discriminator to pre-dict whether each text token in the corrupted text matches the image. At the instance level, in the Image-Text Match-ing task that judges whether an image-text pair is matched, we propose a novel bootstrapping method to generate hard negative text samples that are different from the positive ones only at the token level. In this way, we can force the network to detect fine-grained differences between images and texts. Notably, with a smaller amount of parameters,
VL-Match significantly outperforms previous SOTA on all image-text retrieval tasks.
Figure 1. (a) Masked Language Modeling (MLM) predicts orig-inal tokens of the masked positions with image and text repre-sentations; (b) Vision-Language Replaced Token Detection (VL-RTD) enhances token-level matching by discriminating whether each token in the generated text aligns with the image and the text context; (c) Image-Text Matching (ITM) predicts whether the given texts match the image; (d) Fine-Grained Image-Text Match-ing (FG-ITM) adds a fine-grained negative sample to enhance the matching ability at instance level. 1.

Introduction
The pretrain-then-finetune paradigm has achieved great success in both natural language processing [7, 5, 24, 15] and computer vision [14, 3, 2, 27]. Vision-Language Pre-training (VLP) [22, 44, 18, 21, 39] has also attracted much attention in recent years, which aims to pretrain a model that can understand and align the semantics of images and texts through a variety of pretraining tasks based on massive image-text pairs. These models can be finetuned to adapt to various downstream vision-language tasks such as visual question answering [12] and image-text retrieval [30, 23].
To learn the matching between images and texts in vision-language pretraining, two pretraining tasks are com-monly adopted to train a multimodal encoder [18, 9, 34]:
Masked Language Modeling [35] tries to learn the token-level matching of different modalities by predicting orig-inal tokens of the masked positions with the image and text representations [22, 1]. The image and text represen-tations are encoded with an image encoder and a text en-coder respectively. Image-Text Matching attempts to match vision and language at instance level in a binary classifica-tion task, which predicts whether the given texts match the images [18, 22].
To further enhance vision-language matching at both to-ken level and instance level, we propose VL-Match with two novel objectives: Vision-Language Replaced Token
Detection (VL-RTD) to enhance the matching at the to-ken level with a generator-discriminator structure, and Fine-Grained Image-Text Matching (FG-ITM) to enhance the
Image-Text Matching task at instance level by introducing more hard negative samples. Inspired by the Replaced To-ken Detection task of ELECTRA [5] in natural language pretraining, VL-RTD is designed to discriminate whether each token in the text aligns with the image and the text con-text, with a generator-discriminator structure. Specifically,
VL-RTD regards the multimodal encoder as a discriminator and the text encoder as a generator. Given an original text input, the generator outputs a corrupted text, and then the discriminator learns to discriminate whether each token in the corrupted text is replaced by the generator. Compared with Masked Language Modeling which corrupts the orig-inal text with [MASK], VL-RTD corrupts the text with to-kens selected from the vocabulary, preserving more seman-tic information of the original text. Different from Masked
Language Modeling which only predicts on the masked to-kens, VL-RTD predicts on all text tokens, thus forcing more text information to interact with the image. As shown in
Figure 1 (a) and (b), with the multimodal encoder predicting on all tokens, VL-RTD can efficiently learn the connection between the unmasked “dog” in the text with the “dog” in the image.
Moreover, we also design FG-ITM to enhance the
Image-Text Matching task at the instance level, by introduc-ing more fine-grained negative samples. Previously, nega-tive text samples of the Image-Text Matching task are sam-pled either randomly or according to instance-level similar-ities [21]. To present the differences between the positive and the negative samples in a fine-grained manner, we pro-pose a novel data augmentation method named NegGen. In our method, we synthesize a new text instance by apply-ing a language generator on masked tokens. The generated text is expected to be coherent in natural language, but has some fine-grained differences with the corresponding im-age. To ensure the synthesized pair to be negative, we fur-ther adopt a vision-language discriminator to predict image-text matching probabilities and to filter out potentially posi-tive samples. For example, in Figure 1 (c) and (d), the term
“white” in the positive sample is replaced with “red”, for-mulating a fine-grained negative sample. In this way, the multimodal model is able to capture more fine-grained in-formation for better image-text alignment.
In summary, our contributions include:
• We introduce a generator-discriminator structure pre-trained with a Vision-Language Replaced Token De-tection task to enhance the matching at the token level for vision-language pretraining.
• We are the first to bootstrap fine-grained negative sam-ples in Image-Text Matching task to learn fine-grained representations for efficient vision and language align-ment.
• As shown in experiments, on multiple cross-modal downstream tasks, VL-Match significantly outper-forms previous SOTA on all retrieval tasks (up to 2.9% absolute improvement on Flickr30K dataset, and 1.9% on COCO dataset). 2.