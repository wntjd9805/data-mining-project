Abstract
Graph convolutional networks (GCNs) enable end-to-end learning on graph structured data. However, many works assume a given graph structure. When the input graph is noisy or unavailable, one approach is to construct or learn a latent graph structure. These methods typically fix the choice of node degree for the entire graph, which is suboptimal.
Instead, we propose a novel end-to-end differentiable graph generator which builds graph topologies where each node selects both its neighborhood and its size. Our module can be readily integrated into existing pipelines involving graph convolution operations, replacing the predetermined or exist-ing adjacency matrix with one that is learned, and optimized, as part of the general objective. As such it is applicable to any GCN. We integrate our module into trajectory prediction, point cloud classification and node classification pipelines resulting in improved accuracy over other structure-learning methods across a wide range of datasets and GCN backbones.
We will release the code. 1.

Introduction
The success of Graph Neural Networks (GNNs) [6, 1, 24], has led to a surge in graph-based representation learning.
GNNs provide an efficient framework to learn from graph-structured data, making them widely applicable where data can be represented as a relation or interaction system. They have been effectively applied in a wide range of tasks [25],
[33] including particle physics [4] and protein science [10].
In a GNN, each node iteratively updates its state by inter-acting with its neighbors, typically through message passing.
However, a fundamental limitation of such architectures is the assumption that the underlying graph is provided. While node or edge features may be updated during message pass-ing, the graph topology remains fixed, and its choice may be suboptimal for various reasons. For instance, when classify-ing nodes on a citation network, an edge connecting nodes of different classes can diminish classification accuracy. These edges can degrade performance by propagating irrelevant information across the graph. When no graph is explicitly provided, such domain knowledge can be exploited to learn structures optimized for the task at hand [8, 3, 9, 7]. However, in tasks where knowledge of the optimal graph structure is unknown, one common practice is to generate a k-nearest neighbor (k-NN) graph. In such cases, k is a hyperparameter and tuned to find the model with the best performance. For many applications, fixing k is overly restrictive as the opti-mal choice of k may vary for each node in the graph. While there has been an emergence of approaches which learn the graph structure for use in downstream GNNs [43, 13, 15], all of them treat the node degree k as a fixed hyperparameter.
We propose a general differentiable graph-generator (DGG) module for learning graph topology with or with-out an initial edge structure. Rather than learning graphs with fixed node degrees k, our module generates local topolo-gies with an adaptive neighborhood size. This module can be placed within any graph convolutional network, and jointly optimized with the rest of the network’s parameters, learning topologies which favor the downstream task without hyper-parameter selection or indeed any additional training signal.
The primary contributions of this paper are as follows: 1. We propose a novel, differentiable graph-generator (DGG) module which jointly optimizes both the neigh-borhood size, and the edges that should belong to each neighborhood. Note a key limitation of existing ap-proaches [43, 15, 13, 8, 3, 7, 37] is their inability to learn neighborhood sizes. 2. Our DGG module is directly integrable into any pipeline involving graph convolutions, where either the given adjacency matrix is noisy, or unavailable and must be determined heuristically. In both cases, our DGG gener-ates the adjacency matrix as part of the GNN training and can be trained end-to-end to optimize performance on the downstream task. Should a good graph struc-ture be known, the generated adjacency matrix can be learned to remain close to it while optimizing perfor-mance. 3. To demonstrate the power of the approach, we integrate our DGG into a range of SOTA pipelines — without
modification — across different datasets in trajectory prediction, point cloud classification and node classifi-cation and show improvements in model accuracy. 2.