Abstract
Large-scale text-to-image models pre-trained on mas-sive text-image pairs show excellent performance in image synthesis recently. However, image can provide more intu-itive visual concepts than plain text. People may ask: how can we integrate the desired visual concept into an existing image, such as our portrait? Current methods are inad-equate in meeting this demand as they lack the ability to preserve content or translate visual concepts effectively. In-spired by this, we propose a novel framework named visual concept translator (VCT) with the ability to preserve con-tent in the source image and translate the visual concepts guided by a single reference image. The proposed VCT contains a content-concept inversion (CCI) process to ex-*The first two authors contributed equally to this work. tract contents and concepts, and a content-concept fusion (CCF) process to gather the extracted information to ob-tain the target image. Given only one reference image, the proposed VCT can complete a wide range of general image-to-image translation tasks with excellent results. Extensive experiments are conducted to prove the superiority and ef-fectiveness of the proposed methods. Codes are available at https://github.com/CrystalNeuro/visual-concept-translator. 1.

Introduction
Image-to-image translation (I2I) task aims to learn a conditional generation function that translates images from source to target domain with source content preserved and target concept transferred[34, 46]. General I2I can com-plete a wide range of applications without dedicated model
design or training from scratch [45]. Traditionally, genera-tive adversarial networks (GAN) or normalizing flow [11] are mainly applied to I2I tasks [19, 19, 34, 3]. However, these methods suffer from the problem of lacking adapt-ability [41]. The model trained in one source-target dataset cannot adapt to another one, so they fail to work in the sce-nario of general I2I.
Diffusion-based image synthesis has been developed rapidly in recent years due to the application of large-scale models [35, 37, 33]. Their strength is using a large number of image-text pairs for model training, so diverse images can be generated by sampling in the latent space guided by a specific text prompt. However, in our daily life, we accept massive visual signals containing abundant visual concepts.
These visual concepts are difficult to describe in plain text just as the adage “A picture is worth a thousand words”. In addition, I2I guided by reference images has wide applica-tions including game production, artistic creation, and vir-tual reality. Therefore, research on image-guided I2I con-tains great potential in the computer vision community.
Several works try to extract visual information from im-ages with the desired concepts. Specifically, [9] proposes a technique named textual inversion (TI) which freezes the model and learns a text embedding to represent the visual concepts. On the basis of TI, DreamBooth [36] and Imagic
[20] are proposed to alleviate overfitting caused by model fine-tuning. The above methods are under the few-shot set-ting but sometimes collecting several related images con-taining the same concept is difficult. To address this prob-lem, [7] proposes to use both positive and negative text em-bedding to fit the one-shot setting. However, these methods cannot be directly used in I2I tasks because they cannot pre-serve the content in the source image.
In order to preserve the source contents, the recently proposed DDIM inversion [6, 40] finds out the determin-istic noise along the reverse direction of the diffusion back-ward process. Then, some studies[30, 12] further apply and improve the DDIM inversion to text-guided image editing.
However, these methods are text-conditional so they fail to understand the visual concepts from reference images. Al-ternately, some works [49, 41] try to connect the source and target domain with image condition, but their models are task-specific so they cannot be used in general I2I.
In this paper, to complete the general I2I tasks guided by reference images, we propose a novel framework named visual concept translator (VCT) with the ability to preserve content in the source image and translate the visual concepts with a single reference image. The proposed VCT solves the image-guided I2I by two processes named content-concept inversion (CCI) and content-concept fusion (CCF).
The CCI process extracts contents and concepts from source and reference images through pivot turning inversion and multi-concept inversion, The CCF process employs a dual-stream denoising architecture to gather the extracted infor-mation to obtain the target image. Given only one refer-ence image, the proposed VCT can complete a wide range of general image-to-image translation tasks with excellent results. Extensive experiments including massive tasks of general I2I and style transfer are conducted for model eval-uation.
In summary, our contributions are as follows (1) We propose a novel framework named visual con-cept translator (VCT). Given only a single reference image,
VCT can complete the general I2I tasks with the ability to preserve content in the source image and translate the visual concepts. (2) We propose a content-concept inversion (CCI) to ex-tract contents and concepts with pivot turning inversion and multi-concept inversion. We also propose a content-concept fusion (CCF) process to gather the extracted information with a dual-stream denoising architecture. (3) Extensive experiments including massive tasks of general I2I and style transfer are conducted for model eval-uation. The generation results show the high superiority and effectiveness of the proposed methods. 2.