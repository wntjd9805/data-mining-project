Abstract
Benefiting from prompt tuning, recent years have wit-nessed the promising performance of pre-trained vision-language models, e.g., CLIP, on versatile downstream tasks.
In this paper, we focus on a particular setting of learning adaptive prompts on the fly for each test sample from an un-seen new domain, which is known as test-time prompt tun-ing (TPT). Existing TPT methods typically rely on data aug-mentation and confidence selection. However, conventional data augmentation techniques, e.g., random resized crops, suffers from the lack of data diversity, while entropy-based confidence selection alone is not sufficient to guarantee pre-diction fidelity. To address these issues, we propose a novel
TPT method, named DiffTPT, which leverages pre-trained diffusion models to generate diverse and informative new data. Specifically, we incorporate augmented data by both conventional method and pre-trained stable diffusion to ex-ploit their respective merits, improving the model’s ability to adapt to unknown new test data. Moreover, to ensure the prediction fidelity of generated data, we introduce a co-sine similarity-based filtration technique to select the gen-erated data with higher similarity to the single test sample.
Our experiments on test datasets with distribution shifts and unseen categories demonstrate that DiffTPT improves the zero-shot accuracy by an average of 5.13% compared to the state-of-the-art TPT method. 1.

Introduction
Pre-trained vision-language models, such as CLIP [37], have recently demonstrated promising performance on a va-*Corresponding author.
Figure 1: (a) Prior TPT method [46] uses different augmented views along with confidence selection, resulting in overly simplis-tic variants in the test data and unconfident yet correct predictions being discarded. In comparison, (b) our DiffTPT is effective in generating data with richer visual appearance variation and se-lecting generated data with higher prediction fidelity. riety of downstream tasks without the need for task-specific training data [62, 61, 28, 39].
This is achieved through well-designed prompts, but both hand- crafted prompts and prompt tuning have limi-tations as they are restricted to the training data distribution of the current domain, making it challenging to generalize to distributions outside the domain, especially in the zero-shot setting [30]. In this context, test-time prompt tuning (TPT) [46] has been proposed to learn adaptive prompts on the fly for each test sample from an unseen new domain, without any training data or annotations. This setting offers a more practical approach for dynamic real-world scenarios where collecting a significant amount of labeled data for an
unseen target distribution is often challenging.
One initial attempt to tackle TPT is to incorporate confi-dence selection with entropy minimization for prompt tun-ing by using various augmented views of each test sam-ple [46]. However, the data augmentation method employed in [46] uses simple parametric transformations to alleviate the scarcity of data (see Fig. 1 (a)). Such simplistic transfor-mations are limited in generating diverse data to reflect the rich appearance variation of the test sample [1, 36, 59, 45].
Insufficient diversity in the augmented data can hamper the generalization ability of the learned prompt leading it to overfit on a single data mode. Also, entropy-based con-fidence selection proposed in [46] is not enough to en-sure prediction fidelity, as an augmented sample with low-entropy prediction could still be misclassified into a differ-ent class to the test sample, leading to unrepresentative sam-ples in the augmented pool.
Recent advancements in image generation have made it possible to handle the diversity of augmented data bet-ter. Early image generation methods, including VAEs [25] and GANs [15], often require a large amount of data for training. Recently, diffusion models have achieved superior performance in text-to-image generation with high-quality photo-realistic details [32, 39, 42, 41]. In comparison to the data augmentation method adopted in [46], the augmented data by diffusion model can exhibit much higher diversity, thereby providing richer visual representations and bene-fiting the generalization ability of learned prompts. How-ever, diffusion-based data augmentation is prone to produc-ing spurious augmentations which are more difficult to be filtered out by entropy-based confidence selection. Thus, further research is required to find the right balance be-tween data diversity and prediction fidelity when applying diffusion-based data augmentation for TPT.
In this work, we introduce a new TPT method called
DiffTPT, that enhances the data diversity of test samples through diffusion models while maintaining prediction fi-delity with cosine similarity-based filtration (see Fig. 1 (b)).
In terms of data diversity, DiffTPT adopts Stable Diffusion for data augmentation. Stable Diffusion is a text-to-image generation model which synthesizes an image based on the
Instead, we use the CLIP image
CLIP text feature [41]. feature of the test sample as an alternative to the CLIP text feature and feed it into Stable Diffusion for data augmenta-tion. The diffusion-based augmentation is effective in gen-erating diverse images with richer visual appearance vari-ation while preserving the key semantics. Furthermore, we leverage both the augmentation in [46] and diffusion-based augmentation for improving TPT performance. To ensure prediction fidelity, we introduce cosine similarity-based fil-tration to remove spurious augmentations. By incorporating diffusion-based augmentation with cosine similarity-based filtration, our DiffTPT can make a fair trade-off between diversity and fidelity. Moreover, DiffTPT is agnostic to the training data and can be seamlessly integrated into ar-bitrary CLIP architectures. Experimental results show that
DiffTPT achieves a notable improvement of zero-shot accu-racy by an average of 5.13% in comparison to the state-of-the-art TPT method [46]. To sum up, our contributions are as follows:
• We present a new test-time prompt tuning method, i.e.,
DiffTPT, that balances the trade-off between data diver-sity and prediction fidelity.
• Diffusion-based data augmentation is proposed to gener-ate diverse augmented images with richer visual appear-ance variations while faithfully preserving the key seman-tics.
• We introduce cosine similarity filtration to remove spuri-ous augmentations, thereby improving the prediction fi-delity of augmented images.
• Experimental results show that our DiffTPT significantly outperforms the state-of-the-art test-time prompt-tuning method [46]. 2.