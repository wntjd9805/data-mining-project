Abstract 1.

Introduction
Understanding the temporal dynamics of Earth’s sur-face is a mission of multi-temporal remote sensing image analysis, significantly promoted by deep vision models with its fuel—labeled multi-temporal images. However, collect-ing, preprocessing, and annotating multi-temporal remote sensing images at scale is non-trivial since it is expensive and knowledge-intensive. In this paper, we present a scal-able multi-temporal remote sensing change data genera-tor via generative modeling, which is cheap and automatic, alleviating these problems. Our main idea is to simulate a stochastic change process over time. We consider the stochastic change process as a probabilistic semantic state transition, namely generative probabilistic change model (GPCM), which decouples the complex simulation problem into two more trackable sub-problems, i.e., change event simulation and semantic change synthesis. To solve these two problems, we present the change generator (Changen), a GAN-based GPCM, enabling controllable object change data generation, including customizable object property, and change event. The extensive experiments suggest that our
Changen has superior generation capability, and the change detectors with Changen pre-training exhibit excellent trans-ferability to real-world change datasets.
* corresponding author (zhongyanfei@whu.edu.cn)
Change detection is one of the most fundamental Earth vision tasks to understand the temporal dynamics of Earth’s surface. Tremendous progress in change detection has been achieved by joint efforts of remote sensing and computer vision communities. Deep change detection models [7,39,40, 42] represented by Siamese networks [3] have dominated in recent years. The key behind their success lies in large-scale training datasets [6, 27, 31, 32]. However, building a large-scale remote sensing change detection dataset is difficult and expensive, because collecting, preprocessing, annotating remote sensing images needs more expertise and efforts.
Synthetic data, as an alternative, is a promising direc-tion to alleviate the data hungry. There are currently two main paradigms for change detection data synthesis in the remote sensing domain, i.e., graphics-based [2, 19] and data augmentation-based [4] approaches. The graphics-based methods synthesize images by rendering manual constructed
IAug [4], as a data augmentation-based ap-3D models. proach, synthesizes new image pairs by pasting object in-stances into existing bitemporal image pairs.
However, the scalability and diversity of these conven-tional synthetic change datasets remain limited. This is be-cause 3D modeling and rendering need expertise and tremen-dous efforts for graphics-based approaches, and an existing
change dataset is needed for data augmentation-based ap-proaches. Besides, the synthetic data is aimed to assist the change detection model in improving the performance on real-world data. However, the relationship between the qual-ity of synthetic data and the transferability of features learned from synthetic data is still unclear due to scale-limited data of graphics-based approaches and the coupling of synthetic and real-world data of data-augmentation based approaches.
In this paper, we present a scalable multi-temporal change data generator via generative modeling. Our data generator is aimed to generate realistic and diverse multi-temporal la-beled images from a single-temporal image and its semantic segmentation mask, by simulating the change process.
To this end, we first describe the stochastic change pro-cess as a probabilistic graphical model, namely generative probabilistic change model (GPCM), considering each im-age and semantic mask as random variables, as shown in
Fig. 2. The change is always driven by the event, therefore, we make a Markov assumption that the image and its seman-tics at time t+1 only depends on the image and its semantics at time t, to simplify this modeling problem. Based on this condition, the whole problem can be decoupled to two sub-problems, i.e., the change event simulation at semantic-level and the semantic change synthesis at image-level.
To solve above two sub-problems, we propose a genera-tive model called Changen, which is a GPCM parameterized with generative adversarial networks (GANs). Our Changen creates or removes objects in the semantic mask at time t, as a stochastic change event, to generate a new semantic mask at time t + 1, and synthesizes a post-event image at time t + 1, by progressively applying simulated semantic changes to the pre-event image at time t.
As demonstration, our Changen generates a large-scale building change detection dataset with diverse object proper-ties (e.g., scale, position, orientation) and two change events.
The change detector pre-trained on this dataset has the su-perior transferability on the real-world building change de-tection datasets, significantly outperforming commonly used
ImageNet [8] pre-training, and additionally possessing zero-shot prediction capability. More importantly, based on our
Changen, we find that the temporal diversity of synthetic change data is a key factor in ensuring transferability after model pre-training. The main contributions of this paper are summarized as follows:
• Generative change modeling decouples the complex stochastic change process simulation to more tractable change event simulation and semantic change synthesis.
• Change generator, i.e., Changen, enables object change generation with controllable object property (e.g., scale, position, orientation), and change event.
• Our synthetic change data pre-training empowers the change detectors with better transferability and zero-shot prediction capability.
Figure 2. Generative Probabilistic Change Model. 2.