Abstract
Answering visual questions requires the ability to parse visual observations and correlate them with a variety of knowledge. Existing visual question answering (VQA) mod-els either pay little attention to the role of knowledge or do not take into account the granularity of knowledge (e.g., attaching the color of “grassland” to “ground”). They have yet to develop the capability of modeling knowl-edge of multiple granularity, and are also vulnerable to spurious data biases. To fill the gap, this paper makes progresses from two distinct perspectives: (1) It presents a Hierarchical Concept Graph (HCG) that discriminates and associates multi-granularity concepts with a multi-layered hierarchical structure, aligning visual observations with knowledge across different levels to alleviate data bi-(2) To facilitate a comprehensive understanding of ases. how knowledge contributes throughout the decision-making process, we further propose an interpretable Hierarchical
Concept Neural Module Network (HCNMN). It explicitly propagates multi-granularity knowledge across the hier-archical structure and incorporates them with a sequence of reasoning steps, providing a transparent interface to elaborate on the integration of observations and knowl-edge. Through extensive experiments on multiple challeng-ing datasets (i.e., GQA,VQA,FVQA,OK-VQA), we demon-strate the effectiveness of our method in answering ques-tions in different scenarios. Our code is available at https://github.com/SuperJohnZhang/HCNMN. 1.

Introduction
The ability to reason about knowledge is a fundamental type of generally intelligent behavior [35]. A long-standing goal of artificial intelligence is to develop intelligent sys-tems that can answer a variety of questions with relevant knowledge. Visual question answering [6] has gained con-siderable attention in recent years. With broad coverage of problems with different types, e.g., factual reasoning [11], commonsense reasoning [55], and knowledge-driven rea-soning [44, 30], it offers a practical platform for examining models’ reasoning capability.
A series of progress has been made on improving the knowledge grounding [4, 17, 51, 18] and enriching the knowledge pools [46, 56] for VQA models. While show-ing the effectiveness of incorporating external knowledge, they commonly struggle with the granularity of concepts and lack the capability of identifying relevant knowledge in diverse contexts. As a result, they fall short of gener-alizing to out-of-distribution problems [25] and justifying models’ underlying decision-making process. For instance, as illustrated in Figure 1, the concept “grassland” defines a specific type of “ground” that consists of “grass”, while
“ground” refers to a more general concept that includes
“grassland”, “playground” etc. Existing models have dif-ficulty in discriminating these multi-granularity concepts, and falsely bind the dominant property in the dataset (e.g., hasProperty(green)) to a dominant concept (e.g., ground) despite the discrepancies between their granularity. The mismatched property of a general concept (e.g., ground-hasProperty-green) distracts the decision-making process of its non-dominant subtypes (e.g., identifying the color of the playground).
The mismatch between multi-granularity concepts rarely occurs in human intelligence. When interacting with the complexity of the visual world, humans leverage a hier-archical structure to associate each object with concepts of different granularity. Such a representation is critical for separating different knowledge facts to their designated granularity, and unifying general knowledge with specific ones for a context-rich and bias-resistant decision-making process. Aiming to enhance models’ reasoning capability among diverse sets of knowledge, in this paper, we propose (1) a Hierarchical Concept Graph (HCG) to incorporate the granularity of concepts and (2) a Hierarchical Concept Neu-ral Module Network (HCNMN) to model the integration be-tween observations and knowledge throughout the reason-ing process.
With an overarching goal of endowing VQA models
a multi-layered structure, and supports visual reason-ing with general and fine-grained knowledge of diverse concepts.
• We propose a novel hierarchy-aware neural module network (HCNMN) that tightly integrates knowledge and the decision-making process.
It concurrently reasons over different layers to accumulate multi-granularity knowledge, and also provides an inter-pretable interface for justifying its contributions in dif-ferent reasoning steps.
• We carry out extensive experiments on various VQA datasets, demonstrating the effectiveness, generaliz-ability, and interpretability of the proposed methods.
Our analyses also shed light on the key components (i.e., multi-granularity knowledge) for generalizing
VQA methods to broader scenarios. 2.