Abstract
It is very time consuming to create datasets for train-ing computer vision models. An emerging alternative is to use synthetic data, but if the synthetic data is not similar enough to the real data, the performance is typically below that of training with real data. Thus using synthetic data still requires a large amount of time, money, and skill as one needs to author the data carefully. In this paper, we seek to understand which aspects of this authoring process are most critical. We present an analysis of which factors of variation between simulated and real data are most im-portant. We capture images of YCB objects to create a novel
YCB-Real dataset. We then create a novel synthetic “digital twin” dataset, YCB-Synthetic, which matches the YCB-Real dataset and includes variety of artifacts added to the syn-thetic data. We study the affects of these artifacts on our dataset and two existing published datasets on two differ-ent computer vision tasks: object detection and instance segmentation. We provide an analysis of the cost-beneﬁt trade-offs between artist time for ﬁxing artifacts and trained model accuracy. We plan to release this dataset (images and 3D assets) so they can be further used by the community.
Link to dataset1 1.

Introduction
A computer vision model must be trained on a large dataset that samples across different environments and con-ditions so what is learned from it generalizes well. The tra-ditional process is to train models on large real world image datasets, which require a lot of time and cost to create [18].
Image collection alone is difﬁcult, time consuming, has low scalability, and is sometimes impossible to do [2, 16, 26].
Furthermore, annotating the data is time-intensive.
An interesting alternative is to use computer graphics methods to render large sets of synthetic images for train-ing vision models. Synthetic data provides many beneﬁts – it is easy to create a vast number of data variations with
*Correspondence to Sruthi Sudhakar at ss6638@columbia.edu. 1Dataset: https://github.com/SruthiSudhakar/Exploring-the-Sim2Real-Gap-using-Digital-Twins-Dataset.git
Figure 1: For each artifact that can arise in synthetic data creation, we show how long it takes to ﬁx it vs. the drop in mAP of the model trained with that data. This provides actionable insights as to how to balance the time and cost for synthetic data generation.
Artist Time vs mAP Difference for YCB-Real
P
A m n i e c n e r e f f i
D 2 0
-2
-4
-6
-8
-10
-12
-14
-16
-18 clean texture level 1 ambient 2 ambient 1 noise level 1 holes level 2 holes level 3 holes level 1 baked light texture level 3 texture level 2 noise level 2 noise level 3 0 2 4 6 8 10
Hours of artist time minimal human effort, and the annotation of the data is es-sentially free. These properties have lead an increased use of synthetic data in academia [20, 25, 28, 2] and industry (e.g. Microsoft AirSim, NVIDIA’s Carla and Omniverse platforms, Unity’s Computer Vision API, Datagen, among many others).
Despite these great beneﬁts, broad use is still limited, as models trained on synthetic data often under-perform on real world test data relative to models trained on real world data [29]. Furthermore, many publicly available ob-ject models, such as ShapeNet [3], contain meshes which render very poorly in 3D graphics software like Blender,
Arnold, etc., leading to difﬁculties in training for 3D vision tasks. This performance gap, known as the “Sim2Real” gap, is well known, and it is it caused by numerous factors that make synthetic data a bad match for the data distribution seen in the real world test data [29]. However, while the problem is well known, there is actually quite little under- 
standing of what exactly is causing the gap in the ﬁrst place.
It is the goal of this paper to explore this question. In par-ticular, we focus on issues related to the quality of the 3D content used to create the synthetic data.
The cases where synthetic data performs well are gener-ally the result of great care and highly skilled, human effort taken to manually bridge the Sim2Real gap. One common process is to set up the synthetic data to look as close as pos-sible to the real world test data [24]. The extensive time and skills needed for this process is prohibitive for most typical computer vision research teams and thus effectively cancels out the potential gains over using real data. This is a road block to the broad use of synthetic data in computer vision.
However, if one can understand what aspects of realism have the largest affect on the performance, then they can re-duce the human effort needed to create highly-performing synthetic data. One can steer more limited resources to ﬁx-ing the most important factors alone, thus reducing the bar-riers for using synthetic data effectively.
To help understand the relevant processes and what as-pects of it are most critical, we partnered with a team of artists and data-scientists in a large corporation that per-forms authoring of and training with synthetic data for a number of large commercial clients. The following is their process, and it is common in the industry: Step 1) Digitize the objects and environments to use as building blocks for the synthetic scenes using automated photogrammetry and manually clean up of model artifacts and Step 2) use these realistic 3D assets to create scenes by placing them in dif-ferent environments and rendering them with a variety of cameras positions under various lighting environments and export metadata (object labels, masks, etc.).
This process leads to synthetic data that works quite well, but is very time consuming to create. While “Step 2” is fairly automated and requires less manual effort, “Step 1” can take easily over a day for even small, fairly straight-forward objects. The process also requires considerable skills and intuition that are typically the domain of highly trained 3D artists. In this paper, we focus on these manual, skilled steps – the ones that ﬁx artifacts that appear in the automated photogrammetry pipeline – to understand what is critical to the performance of the synthetic data.
We study these properties on objects from the YCB dataset [30]. We have created a synthetic training dataset and a real test dataset where all conditions of the environ-ment and object match including camera angles, lighting, foreground, background, and textures – also known as a
“Digital Twin”.
From these twins, we create new datasets where we syn-thetically introduce artifacts in a controlled manner to un-derstand which 3D-model artifacts cause the greatest drops in performance when their rendered data is used to train a computer vision model. Isolating which factors cause drops in performance requires training and testing in very con-trolled settings where only one factor changes at a time, and thus we have constructed our dataset to have this property.
The artifacts we study are 5 factors that our partner team identiﬁed most typically arise during photogrammetry and are corrected to create good synthetic data: noise in the 3D mesh, holes in the mesh, texture blur, and variations in dif-fuse vs non-diffuse (or “baked”) lighting.
We then train on these modiﬁed datasets and test on our real dataset to understand which factor causes the greatest drops in performance. Further, we test these models on 2 more real-world datasets including YCB-In-the-wild [10] and YCB-Video [34] and show that these ﬁndings hold.
For each artifact, we provide an estimate for the time taken by an artist to correct that artifact, so that we can un-derstand the trade-off between how long it takes to ﬁx each artifact to obtain a clean model and the accuracy beneﬁt pro-vided by that ﬁx (summarized in Figure 1). This provides time and cost constrained researchers with actionable in-sights to prioritize which factors to address.
In summary, our main contributions include:
• Discovery of which factors of variation in 3D model quality between simulated and real data are important for computer vision model performance
• A cost-beneﬁt analysis between artist time for correct-ing an artifact and trained model accuracy
YCB-Real new
• A digital
YCB-Synthetic dataset and generalization in controlled environments study and to twin adaptation 2.