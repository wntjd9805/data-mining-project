Abstract
In this work, we introduce CC3D, a conditional genera-tive model that synthesizes complex 3D scenes conditioned on 2D semantic scene layouts, trained using single-view im-ages. Different from most existing 3D GANs that limit their applicability to aligned single objects, we focus on generat-ing complex scenes with multiple objects, by modeling the compositional nature of 3D scenes. By devising a 2D layout-based approach for 3D synthesis and implementing a new 3D ﬁeld representation with a stronger geometric inductive bias, we have created a 3D GAN that is both efﬁcient and of high quality, while allowing for a more controllable genera-tion process. Our evaluations on synthetic 3D-FRONT and real-world KITTI-360 datasets demonstrate that our model generates scenes of improved visual and geometric quality in comparison to previous works. 1.

Introduction
Recently, we have witnessed impressive progress in 3D generative technologies, including generative adversarial net-*Equal contribution works (GANs) [20] that have emerged as a powerful tool for automatically creating realistic 3D content. Despite their impressive capabilities, existing 3D GAN-based approaches have two major limitations. First, they typically generate the entire scene from a single latent code, ignoring the com-positional nature of multi-object scenes, thus struggling to synthesize scenes with multiple objects, as shown in Fig. 2.
Second, their generation process remains largely uncontrol-lable, making it non-trivial to enable user control. While some works [6, 31] allow conditioning the generation of input images via GAN inversion, this optimization process can be time-consuming and prone to local minima.
In this work, we introduce a Compositional and
Conditional 3D generative model (CC3D), that generates plausible 3D-consistent scenes with multiple objects, while also enabling more control over the scene generation process by conditioning on semantic instance layout images, indicat-ing the scene structure (see Fig. 1). Our approach rhymes with the 2D image-to-image translation works [24, 16] that conditionally generate images from user inputs: CC3D gen-erates 3D scenes from 2D user inputs (i.e. scene layouts).
To train CC3D we use a set of single-view images and top-Figure 2. Failure cases – of non-compositional 3D GANs on multi-object scenes. We show examples of generated scenes synthesized with [15, 7], which do not explicitly model the compositional nature of multi-object living rooms. Note their lack of visual qualities. down semantic layout images, such as 2D labelled bounding boxes of objects in a scene (e.g. Fig. 1). Our generator network takes a 2D semantic image as input that deﬁnes the scene layout and outputs a 3D scene, whose top-down view matches the input layout in terms of object locations.
The key component of our approach is a 2D-to-3D trans-lation scheme that efﬁciently converts the 2D layout image into a 3D neural ﬁeld. Our generator network is based on a modiﬁed StyleGAN2 [28] architecture that processes the input 2D layout image into a 2D feature map. The output 2D feature map is then reshaped into a 3D feature volume that deﬁnes a neural ﬁeld which can be rendered from arbitrary camera views. Similar to existing 3D-aware generative mod-els [48, 39, 7], we train the generator to produce realistic renderings of the neural ﬁelds from all sampled viewpoints.
In addition, we enforce a semantic consistency loss that ensures the top-down view of the 3D scene matches the semantic 2D layout input.
We evaluate CC3D on the 3D-FRONT [17] bedroom and living room scenes and the KITTI-360 dataset [29] that con-tains more challenging outdoor real-world scenes. Our evalu-ations demonstrate that existing 3D generative models, such as EG3D [7] and GSN [15], produce low-quality 3D scenes, as illustrated in Fig. 2. In comparison, the compositional generation process and the new intermediate 3D feature rep-resentation of CC3D signiﬁcantly improve the ﬁdelity of the synthesized 3D scenes on both datasets, opening the door for realistic multi-object scene generations. 2.