Abstract
Representing wild sounds as images is an important but challenging task due to the lack of paired datasets between sound and images and the signiﬁcant differences in the characteristics of these two modalities. Previous studies have focused on generating images from sound in limited categories or music. In this paper, we propose a novel ap-proach to generate images from in-the-wild sounds. First, we convert sound into text using audio captioning. Sec-ond, we propose audio attention and sentence attention to represent the rich characteristics of sound and visualize the sound. Lastly, we propose a direct sound optimiza-tion with CLIPscore and AudioCLIP and generate images with a diffusion-based model. In experiments, it shows that our model is able to generate high quality images from wild sounds and outperforms baselines in both quantitative and qualitative evaluations on wild audio datasets. 1.

Introduction
Sound is one of the most important senses for humans, along with vision. Its dynamic and time-changing charac-teristics make it a more rich and complex modality than text or image [21]. Therefore, generating corresponding images given sounds has many applications such as back-ground picture generation used in movies and visualization
[45] and explanation of sound to help people with hearing impairment [3]. However, because of differences in modal-ities and the lack of paired datasets between sound and im-age, representing wild sounds as images is a challenging task. For these reasons, previous studies [12, 20, 22, 37, 42] have generated images within limited sound categories or through music. But generating images in limited sound cat-egories has limitations in representing the wild sounds. It is difﬁcult to represent the complex sounds of multi-domain.
Music is composed of elements such as melody and rhythm and contains rich information [15], but it is fundamentally different from wild sounds that encompass diverse environ-ments and multi-domains. In addition, the quality of images generated from sound by previous studies [12, 37, 42, 43] is signiﬁcantly inferior compared to that of text-guided image generation models [29, 32, 33].
To solve these problems, we propose a novel approach that uses a pre-trained Audio Captioning Transformer (ACT) [23] and a diffusion-based model called Stable Dif-fusion [32]. Unlike previous studies [12, 21, 42] that at-tempted to generate images from sound by mapping it into limited categories such as dogs or humans, we describe sounds in greater detail by converting them into audio cap-tions by using the ACT [23] model. Then, we generate im-ages using a pre-trained Stable Diffusion [32] model. This approach address the differences in modalities and enables high-quality image generation from a sound without requir-ing large paired training datasets between sounds and im-ages.
However, since our purpose is to represent the rich and dynamic characteristics of wild sounds in images, simply generating images from audio captions is not sufﬁcient.
Therefore, we propose a novel approach to address this is-sue. Firstly, we propose to exploit audio attention. Au-dio attention is a value of probability used by the ACT [23] model to generate an audio caption. We ﬁrst use this audio attention to represent the rich and dynamic characteristic of sound as an image. Secondly, we introduce sentence atten-tion to emphasize objects from the generated audio caption.
To visualize sound, it is important to consider not only the characteristics of the sound, but also to emphasize the ob-jects in the sound within it.
Furthermore, we propose direct sound optimization to optimize images further for corresponding sounds. We gen-erate a latent vector from audio caption with CLIP [27] text encoder, and initialize it with audio attention and sentence attention to get a sound optimized latent vector. After that, we generate an image with Stable Diffusion using the la-tent vector as a conditioning vector, and optimize it through
AudioCLIP [11] similarity and CLIPscore [13]. Through this process, we address the modality gap between audio and image and are able to tackle the challenging problem of generating realistic and dynamic images from wild sounds.
In summary, our contributions are as follows:
• We propose a novel approach that uses audio caption-ing and diffusion based text-to-image model to gener-ate a high quality image without large paired datasets between sounds and images.
• We propose audio attention and sentence attention to generate images that represent the characteristics of
In multi-domain and time-varying dynamic sounds. addition, we introduce direct sound optimization with
CLIPscore and AudioCLIP similarity for further en-hancement.
• In experimental results, our model is able to generate faithful and high quality images from in-the-wild input sounds and outperforms baselines in both quantitative and qualitative evaluations. 2.