Abstract
Video-to-video translation aims to generate video frames of a target domain from an input video. Despite its use-fulness, the existing networks require enormous computa-tions, necessitating their model compression for wide use.
While there exist compression methods that improve compu-tational efficiency in various image/video tasks, a generally-applicable compression method for video-to-video transla-In response, we present tion has not been studied much.
Shortcut-V2V, a general-purpose compression framework for video-to-video translation. Shortcut-V2V avoids full inference for every neighboring video frame by approxi-mating the intermediate features of a current frame from those of the previous frame. Moreover, in our framework, a newly-proposed block called AdaBD adaptively blends and deforms features of neighboring frames, which makes more accurate predictions of the intermediate features pos-sible. We conduct quantitative and qualitative evaluations using well-known video-to-video translation models on var-ious tasks to demonstrate the general applicability of our framework. The results show that Shortcut-V2V achieves comparable performance compared to the original video-to-video translation model while saving 3.2-5.7× computa-tional cost and 7.8-44× memory at test time. Our code and videos are available at https://shortcut-v2v.github.io/. 1.

Introduction
Video-to-video translation is a task of generating tem-porally consistent and realistic video frames of a target do-main from a given input video. Recent studies on video-to-video translation present promising performance in various domains such as inter-modality translation between labels
*indicates equal contributions. and videos [34, 33, 18], and intra-modality translation be-tween driving scene videos [32] or face videos [1].
Despite enhanced usefulness, video-to-video translation networks usually require substantial computational cost and memory usage, which limits their applicability. For in-stance, multiply–accumulates (MACs) of a widely-used video translation model, vid2vid [34], is 2066.69G, while the basic convolutional neural networks, ResNet v2 50 [10] and Inception v3 [28], are 4.12G and 6G, respectively. Fur-thermore, temporally redundant computations for adjacent video frames also harm the cost efficiency of a video-to-video translation network. Performing full inference for ev-ery neighboring video frame that contains common visual features inevitably entails redundant operations [20, 19].
In this regard, Fast-Vid2Vid [40] proposes a compres-sion framework for vid2vid [34] based on spatial input com-pression and temporal redundancy reduction. However, it cannot be applied to other video-to-video translation mod-els since it is designed specifically for vid2vid. Moreover,
Fast-Vid2Vid does not support real-time inference since it requires future frames to infer a current one. Alternatively, one can apply model compression approaches for image-to-image translation [13, 22, 12] directly to video-to-video translation, considering video frames as separate images.
However, these approaches are not designed to consider the correlation among adjacent video frames during the com-pression. This may result in unrealistic output quality in video-to-video translation, where the inherent temporal co-herence of an input video needs to be preserved in the out-puts. Also, frame-by-frame inference without temporal re-dundancy reduction involves unnecessary computations, re-sulting in computational inefficiency.
In this paper, we propose Shortcut-V2V, a general-purpose framework for improving the computational ef-ficiency of video-to-video translation based on temporal redundancy reduction. Shortcut-V2V allows the original video-to-video translation model to avoid temporally re-Figure 1: Overview of Shortcut-V2V. (a) is an overall framework of Shorcut-V2V, and (b) shows a detailed architecture of Shortcut block. ↑ 2 and ↓ 2 refer to upsampling and downsampling by a factor of 2, respectively. G in Offset G and
Offset/Mask G indicates a generator. dundant computations by approximating the decoding layer features of the current frame with largely reduced com-putations. To enable lightweight estimation, our frame-work leverages features from the previous frame (i.e., ref-erence features), which have high visual similarity with the current frame. We also exploit current frame features from the encoding layer to handle newly-appeared regions in the current frame. Specifically, we first globally align the previous frame features with the current frame fea-tures, and our novel Adaptive Blending and Deformation block (AdaBD) in Shortcut-V2V blends features of neigh-boring frames while performing detailed deformation. Ad-aBD adaptively integrates the features regarding their re-dundancy in a lightweight manner.
In this way, our model significantly improves the test-time efficiency of the original network while preserving its original performance. Shortcut-V2V is easily applicable to a pretrained video-to-video translation model to save com-putational cost and memory usage. Our framework is also suitable for real-time inference since we do not require fu-ture frames for the current frame inference. To the best of our knowledge, this is the first attempt at a general-purpose model compression approach for video-to-video transla-tion. We demonstrate the effectiveness of our approach using well-known video-to-video translation models, Un-supervised RecycleGAN [32] (Unsup) and vid2vid [34].
Shortcut-V2V reduces 3.2-5.7× computational cost and 7.8-44× memory usage while achieving comparable per-formance to the original model. Since there is no exist-ing general-purpose compression method, we compare our method with Fast-Vid2Vid [40] and the compression meth-ods for image-to-image translation. Our model presents su-periority over the existing approaches in both quantitative and qualitative evaluations. Our contributions are summa-rized as follows:
• We introduce a novel, general-purpose model com-pression framework for video-to-video translation,
Shortcut-V2V, that enables the original network to avoid temporally redundant computations.
• We present AdaBD that exploits features from neigh-boring frames via adaptive blending and deformation in a lightweight manner.
• Our framework saves up to 5.7× MACs and 44× parameters across various video-to-video translation tasks, achieving comparable performance to the origi-nal networks. 2.