Abstract
Transformer-based detectors (DETRs) are becoming popular for their simple framework, but the large model size and heavy time consumption hinder their deployment in the real world. While knowledge distillation (KD) can be an appealing technique to compress giant detectors into small ones for comparable detection performance and low inference cost. Since DETRs formulate object detection as a set prediction problem, existing KD methods designed for classic convolution-based detectors may not be directly ap-plicable.
In this paper, we propose DETRDistill, a novel knowledge distillation method dedicated to DETR-families.
Specifically, we first design a Hungarian-matching logits distillation to encourage the student model to have the exact predictions as those of the teacher DETRs. Then, we pro-pose a target-aware feature distillation to help the student model learn from the object-centric features of the teacher model. Finally, in order to improve the convergence rate of the student DETR, we introduce a query-prior assignment distillation to speed up the student model learning from well-trained queries and stable assignment of the teacher model. Extensive experimental results on the COCO dataset validate the effectiveness of our approach. Notably, DE-TRDistill consistently improves various DETRs by more than 2.0 mAP, even surpassing their teacher models. 1.

Introduction
Object detection aims to locate and classify visual ob-jects from an input image. In the early works, the task was typically achieved by incorporating convolution neural net-works (CNNs) to process the regional features of the input image [26, 19], in which a bunch of inductive biases were included, such as anchors [26], label assignment [41] and duplicate removal [1]. Recently, transformer-based object detectors like DETR [2] have been proposed where detec-*The authors contributed equally.
†Corresponding author.
Figure 1. The performance of our DETRDistill on three transformer-based detectors: Conditional DETR [25], Deformable
DETR [43], and AdaMixer [11]. We adopt ResNet-101 and
ResNet-50 for the teacher and student models, respectively. Our
DETRDistill yields significant improvements compared to its stu-dent baseline, and even outperforms its teacher model. tion is treated as a set prediction task which significantly simplifies the detection pipeline and helps the users free from the tedious tuning of the hand-craft components, e.g., anchor sizes and ratios [26].
Although the transformer-based detectors have achieved state-of-the-art performance [21, 17, 11], they suffer from an expensive computation problem, making them difficult to be deployed in real-time applications. In order to acquire a fast and accurate detector, knowledge distillation [14] (KD) is an appealing technique. Normally, KD methods transfer knowledge from a heavy-weighted but powerful teacher model to a small and efficient student network by mimicking the predictions [14] or feature distributions [27].
In the research area of object detection, there have var-ious kinds of KD methods been published [37, 36, 39, 27, 4, 18, 12, 42, 38, 32, 29]. However, most of these methods are designed for convolution-based detectors and may not directly apply to transformer-based DETRs due to the de-tection framework differences. There are at least two chal-lenges: ❶ logits-level distillation methods [14, 42] are un-usable for DETRs. For either anchor-based [41] or anchor-free [31] convolution-based detectors, box predictions are
Table 1. Comparison on several CNN-based region weighted fea-ture distillation approaches on AdaMixer [11].
Method
#Epoch4
AP
#Epoch8
AP
#Epoch12
AP
Baseline w/o KD
FGD [37]
FKD [40]
MGD [38]
FGFI [34]
FitNet [27] 35.0 34.4(-0.6) 35.9(+0.9) 36.3(+1.3) 35.6(+0.6) 36.4(+1.4) 38.7 39.1(+0.4) 39.5(+0.8) 39.8(+1.1) 39.3(+0.6) 39.6(+0.9) 42.3 40.7(-1.6) 42.2(-0.1) 42.3(+0.0) 42.6(+0.3) 42.9(+0.6) closely related to the feature map grid and thus naturally ensure a strict spatial correspondence of box predictions for knowledge distillation between teacher and student. How-ever, for DETRs, box predictions generated from the de-coders are unordered and there is no natural one-to-one cor-respondence of predicted boxes between teacher and stu-dent for a logits-level distillation. ❷ Feature-level distil-lation approaches may not suitable for DETRs. Due to the feature generation mechanism being different between convolution and transformer [27], the region of feature ac-tivation for the interested object varies a lot. As Fig. 2 shows, the active region of a convolution-based detector is almost restricted inside the ground-truth box while the
DETR detector further activates regions in the background area. Therefore, directly using previous feature-level KD methods for DETRs may not necessarily bring performance gains and sometimes even impair the student detectors as presented in Table 1.
To address the above challenges, we propose DETRDis-till, a knowledge distillation framework specially designed for detectors of DETR families. Precisely, DETRDistill mainly consists of three components: (1) Hungarian-matching logits distillation: To solve the challenge ❶, we use the Hungarian algorithm to find an op-timal bipartite matching between predictions of student and teacher and then the KD can be performed in the logits-level. However, since the number of boxes predicted as pos-itive in the teacher model is quite limited, doing KD only on positive predictions does not bring a significant perfor-mance gain. Instead, we propose to introduce a distillation loss on the massive negative predictions between the teacher and student model to fully make use of the knowledge ly-ing in the teacher detector. Moreover, considering DETR methods typically contain multiple decoder layers for a cas-cade prediction refinement, we also create a KD loss at each stage to have a progressive distillation. (2) Target-aware feature distillation: According to the analysis in challenge ❷, we propose utilizing object queries and teacher model features to produce soft activation masks.
Since the well-trained teacher queries are closely related to
Figure 2. Visualization of the (a) ground-truth boxes and active region from (b) ATSS [41] and (c) AdaMixer [11]. various object targets, such generated soft masks will be object-centric and thus make soft-mask-based feature-level distillation to be target-aware. (3) Query-prior assignment distillation:
Since the queries and decoder parameters are randomly initialized in the student model, unstable bipartite assignment in the stu-dent model leads to a slow convergence rate as presented in [17]. While we empirically find that well-trained queries in the teacher model can always produce a consistent bi-partite assignment as shown in Fig. 7, we thus propose to let the student model take teacher’s queries as an additional group of prior queries and encourage it to produce predic-tions based on the stable bipartite assignment of the teacher network. Such a distillation successfully helps the student model to converge fast and achieve better performance.
In summary, our contributions are in three folds:
• We analyze in detail the difficulties encountered by
DETRs in the distillation task compared with traditional convolution-based detectors.
• We propose multiple knowledge distillation methods for DETRs from the perspective of logits-level, feature-level, and convergence rate, respectively.
• We conduct extensive experiments on the COCO dataset under different settings, and the results prove the effectiveness and generalization of our proposed methods. 2.