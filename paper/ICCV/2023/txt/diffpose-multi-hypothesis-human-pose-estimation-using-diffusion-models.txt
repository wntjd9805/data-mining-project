Abstract
Traditionally, monocular 3D human pose estimation em-ploys a machine learning model to predict the most likely 3D pose for a given input image. However, a single im-age can be highly ambiguous and induces multiple plau-sible solutions for the 2D-3D lifting step, which results in overly confident 3D pose predictors. To this end, we pro-pose DiffPose, a conditional diffusion model that predicts multiple hypotheses for a given input image. Compared to similar approaches, our diffusion model is straightfor-ward and avoids intensive hyperparameter tuning, complex network structures, mode collapse, and unstable training.
Moreover, we tackle the problem of over-simplification of the intermediate representation of the common two-step ap-proaches which first estimate a distribution of 2D joint lo-cations via joint-wise heatmaps and consecutively use their maximum argument for the 3D pose estimation step. Since such a simplification of the heatmaps removes valid infor-mation about possibly correct, though labeled unlikely, joint locations, we propose to represent the heatmaps as a set of 2D joint candidate samples. To extract information about the original distribution from these samples, we introduce our embedding transformer which conditions the diffusion model. Experimentally, we show that DiffPose improves upon the state of the art for multi-hypothesis pose estima-tion by 3-5% for simple poses and outperforms it by a large margin for highly ambiguous poses.1 1.

Introduction
Human pose estimation from monocular images is an open research question in computer vision with many ap-plications, e.g. in human-machine interaction, autonomous driving, animation, sports, and medicine. Recent ad-vances in deep learning-based human pose estimation show promising results on the path to highly accurate 3D recon-structions from single images. Typically, a neural network is trained to reconstruct the most likely 3D pose given an in-1Our code and trained models will be made available at: https://github.com/bastianwandt/DiffPose/
Figure 1. Comparison of our approach to Sharma et al. [40] and
Wehrbein et al. [52]. While [40] produces very similar poses, even for uncertain detections, [52] achieves a higher diversity. However, they oversimplify heatmaps as a Gaussian and, thus, struggle with different uncertainty distributions. Note that the densest region of samples (red) for [40] and [52] is very similar and at a point with low certainty. By contrast, DiffPose produces 3D poses that cover the full uncertainty in the heatmap leading to a lower error. put image. However, the projection from 3D to a 2D plane, which is performed by a camera capturing a person, results in an inevitable loss of information. This lost information cannot be uniquely reconstructed, and therefore we argue that a meaningful 3D human pose estimator must be able to recover the full distribution of possible 3D poses for a given 2D pose, e.g. as a set of poses with different likelihoods.
Moreover, downstream tasks can be built to benefit from un-likely poses; for example, consider an autonomous vehicle making decisions based on a single output versus being able to see all possible, though less likely, outcomes. Conse-quently, interest in this field, called multi-hypothesis human pose estimation, is increasing [16, 25, 35, 40, 23, 52, 29].
Some approaches estimate a small fixed-size set of poses
[16, 25, 35, 29] which are not able to fully represent the real output distribution. Others are based on variational autoen-coders [40] or normalizing flows [23, 52] and can predict an
infinite set of poses that provides a stronger approximation for the 3D pose distribution. However, they require com-plex architectures and lack diversity in their output, since the 2D input data is simplified, as shown in Fig. 1.
Our goal is a multi-hypothesis human pose estimator that is easy to train and produces high-quality pose hypothe-ses covering the full range of possible and plausible out-put poses. To this end, we make three major contributions: we 1) are the first to represent a 3D human pose distribu-tion with a conditional diffusion model which, in its sur-prisingly simple architecture and training, achieves state-of-the-art results, 2) use the full 2D input information from heatmaps without any simplifications by our novel sampling strategy, and 3) propose a transformer architecture that han-dles these samples without losing information about joint uncertainties.
Neural diffusion models recently gained huge interest due to their impressive performance in image generation
[37, 38, 39]. We exploit their capability to generate even subtle details that formerly were only achievable by hard-to-train GANs [49, 8] or normalizing flows [53, 23, 52, 48].
Even in its simplicity, our diffusion model creates mean-ingful human poses, and, unlike VAEs and GANs, it does not suffer from mode collapse, posterior collapse, vanish-ing gradients, and training instability [20]. Although pose representations via normalizing flows also do not show such phenomena, they require a sophisticated model of the hu-man kinematic chain [53], a kinematic chain prior [52], and additional care during training. By contrast, our diffu-sion model is robust during training and creates meaningful poses without requiring further constraints.
Our second major contribution reveals a problem in cur-rent two-step approaches that first predict 2D joint positions in an image and consecutively use these predictions as in-put to the 3D reconstruction step. While this enables the 3D estimator to be agnostic to the input image and con-sequently promises generalization across image domains, it removes valid structural and depth information that can only be seen in the images. We exploit that most 2D hu-man pose detectors employ heatmaps encoding joint occur-rence probabilities as an intermediate representation. Tradi-tionally, the maximum argument of these heatmaps is used as input to the second stage, which removes all informa-tion about the uncertainty of the detector. Few approaches extract additional information, such as confidence values
[50] or Gaussian distributions fitted to the heatmap [52].
However, they still oversimplify the heatmap as shown in
Fig. 1, thus missing important details. To this end, we pro-pose to condition the diffusion model with an embedding vector computed from a set of joint positions directly sam-pled from the heatmaps. We build a so-called embedding transformer which combines joint-wise samples and their respective confidences into a single embedding vector that encodes the distribution of the joints. 2.