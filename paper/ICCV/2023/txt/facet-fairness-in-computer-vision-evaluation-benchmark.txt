Abstract
Computer vision models have known performance dis-parities across attributes such as gender and skin tone. This means during tasks such as classiﬁcation and detection, model performance differs for certain classes based on the demographics of the people in the image. These dispari-ties have been shown to exist, but until now there has not been a uniﬁed approach to measure these differences for common use-cases of computer vision models. We present a new benchmark named FACET (FAirness in Computer Vi-sion EvaluaTion), a large, publicly available evaluation set of 32k images for some of the most common vision tasks
- image classiﬁcation, object detection and segmentation.
For every image in FACET, we hired expert reviewers to manually annotate person-related attributes such as per-ceived skin tone and hair type, manually draw bounding boxes and label ﬁne-grained person-related classes such as disk jockey or guitarist.
In addition, we use FACET to benchmark state-of-the-art vision models and present a deeper understanding of potential performance dispari-ties and challenges across sensitive demographic attributes.
With the exhaustive annotations collected, we probe models using single demographics attributes as well as multiple at-tributes using an intersectional approach (e.g. hair color and perceived skin tone). Our results show that classiﬁca-tion, detection, segmentation, and visual grounding mod-els exhibit performance disparities across demographic at-tributes and intersections of attributes. These harms sug-gest that not all people represented in datasets receive fair and equitable treatment in these vision tasks. We hope cur-rent and future results using our benchmark will contribute to fairer, more robust vision models. FACET is available publicly at https://facet.metademolab.com. 1.

Introduction
The ability of computer vision models to perform a wide range of tasks is due in no small part to large, widely used datasets. These large-scale datasets containing millions of
Category:
Guitarist
Perceived skin tone: 3          4         4          5         5          5          6 
Perceived Age:
Middle
Hair color: 
Black/Brown
Perceived gender presentation:  
With more maleness
Photo is: 
Underexposed
Hair type: 
Wavy
Additional:
Tattoo, facial hair
Visibility: 
Face, torso visible
Figure 1: An example image and annotations from our dataset FACET . Every image in FACET contains annota-tions from expert reviewers on the primary class, sensitive attributes include perceived gender presentation, perceived skin tone, and perceived age group and additional visual attributes like hair color and type, visible tattoos, etc. images often have image-level labels such as ImageNet [16] or object-level annotations found in datasets such as MS-COCO [63] or Open Images [62]. Annotations are also used at the person-level, in datasets such as CelebA [64], UTK-Faces [98] and More Inclusive People Annotations (MIAP)
[84]. These person-level annotations in particular enable a more ﬁne-grained analysis and evaluation of model perfor-mance accross groups. Prior work using these person-level annotations to evaluate model fairness has shown that vision models learn societal biases and stereotypes, which neg-atively impact performance and cause downstream harms
[87, 101, 74, 90, 88]. This makes fairness datasets particu-larly important as vision models continue to grow.
One weakness of existing fairness datasets is that they lack exhaustive and diverse demographic annotations that            
Size
Evaluation
Annotations
Protected
Groups
Additional
Person
Attributes
Miscellaneous
Attributes – 32k images, 50k people – 52-person related classes – bounding boxes around each person – person/hair/clothing labels for 69k masks – perceived skin tone – perceived age group – perceived gender presentation – hair: color, hair type, facial hair – accessories: headscarf, facemask, hat – other: tattoo lighting condition, level of occlusion
Table 1: Statistics on size of FACET and person annotations including labels for classiﬁcation (e.g. soldier, teacher) and attributes such as hair color and perceived skin tone. can support multiple vision tasks. For instance, while Open
Images More Inclusive People Annotations (MIAP) [84] can be used for classiﬁcation and detection, the labels are not particularly diverse as only perceived gender presen-Image-level tation and perceived age group are labeled. class labels are also sparse, with an incomplete set of true positives and true negatives per image. Another dataset,
CelebA, contains many more person-level attributes but is primarily for face localization.
In addition, CelebA con-tains many subjective and potentially harmful attributes e.g. attractive, big lips, chubby [25]. These weakenesses can greatly impact our ability to perform more ﬁne-grained fair-ness analyses.
In this paper, we present FACET (Fairness in Com-puter Vision Evaluation Benchmark), a large-scale evalu-ation benchmark with exhaustive annotations for 32k im-ages from Segment Anything 1 Billion (SA-1B) [59]labeled across 13 person attributes and 52 person classes. The 13 at-tributes include examples such as perceived skin tone, hair type, perceived age group; the 52 person classes include categories such as hairdresser and reporter. To ensure the annotations are both high quality and labeled by a diverse group of people, we used trained, expert annotators sourced from several geographic regions (North and Latin America,
Middle East, Africa, East and Southeast Asia).
FACET enables a deeper analysis of potential fairness concerns and model biases for speciﬁc demographic axes.
We can explore questions such as: 1) Are models bet-ter at classifying people as skateboarder when their per-ceived gender presentation has more stereotypically male attributes? 2) Are open-vocabulary detection models better at detecting backpackers who are perceived to be younger? 3) Do standard detection models struggle to detect people whose skin appears darker? 4) Are these problems mag-niﬁed when the person has coily hair compared to straight hair? 5) Do performance discrepancies differ across the detection and segmentation tasks? These questions illus-trate a few examples of how model biases can be explored at a deep, intersectional level using the exhaustive annota-tions in FACET. We use FACET to evaluate multiple state-of-the-art vision models to understand their fairness on de-mographic attributes (perceived gender presentation, per-ceived skin tone, perceived age group) as well as their ex-isting demographic biases. FACET is publicly available at https://facet.metademolab.com.
Our contributions include: – our new publicly available fairness benchmark
FACET, containing 32k images from Segment Any-thing 1 Billion (SA-1B) [59], manually annotated with demographic and additional visual attributes labels by expert annotators – 52 person-related class labels and manually drawn bounding boxes for every annotated person in every image (50k total people) – person, clothing, or hair labels for 69k masks – a benchmark for using FACET to compare different models, showing quantitative results and qualitative analyses on existing vision models using FACET
FACET is an evaluation-only benchmark. Using any of the annotations for training is strictly prohibited. 2.