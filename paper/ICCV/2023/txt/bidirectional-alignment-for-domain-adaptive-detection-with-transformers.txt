Abstract
We propose a Bidirectional Alignment for domain adap-tive Detection with Transformers (BiADT) to improve cross domain object detection performance. Existing adversarial learning based methods use gradient reverse layer (GRL) to reduce the domain gap between the source and target domains in feature representations. Since different image parts and objects may exhibit various degrees of domain-specific characteristics, directly applying GRL on a global image or object representation may not be suitable. Our proposed BiADT explicitly estimates token-wise domain-invariant and domain-specific features in the image and ob-ject token sequences. BiADT has a novel deformable at-tention and self-attention, aimed at bi-directional domain alignment and mutual information minimization. These two objectives reduce the domain gap in domain-invariant representations, and simultaneously increase the distinc-tiveness of domain-specific features. Our experiments show that BiADT achieves very competitive performance to SOTA consistently on Cityscapes-to-FoggyCityscapes,
Sim10K-to-Citiscapes and Cityscapes-to-BDD100K, out-performing the strong baseline, AQT, by 2.0, 2.1, and 2.4 in mAP50, respectively. The implementation is available at https://github.com/helq2612/biADT 1.

Introduction
This paper focuses on cross-domain object detection – an important problem for vision applications which requires a detector trained on source-domain images generalize well on target-domain images. We say that there is a “domain gap” (or “domain shift”) between the source and target do-mains, since their respective images significantly differ in appearance and texture, while they do share the same object classes of interest. Despite recent advances in standard ob-Figure 1. The proposed BiADT is a transformer that consists of the encoder and decoder (for clarity, we only show 2-encoder-layer and 2-decoder-layer). At the input, the encoder takes image-level entangled features (black-white pattern) as image tokens and gradually disentangles them into domain-specific features (marked black for the source and white for the target) and domain-invariant features (marked gray). The decoder decodes object queries on the image, and similarly disentangles the features. We say that the encoder and decoder perform bidirectional feature alignment.
This means that they both seek to align domain-invariant features and increase distinctiveness of domain-specific features, at their respective image and object levels. ject detection [33, 27, 2, 50, 26], their direct application in the cross-domain settings typically yields poor results.
We address cross-domain object detection within the un-supervised domain adaptation (UDA) framework, where training of a detector has access to images of both source and target domains, but object annotations are only avail-able for the source images. A common approach is to learn a domain agnostic feature space in which feature distribu-tions from the source and target domains are aligned by met-ric learning [30, 16] or adversarial learning [47, 8, 31, 4].
However, we find that this unified feature alignment, at the image and object levels, may be not appropriate for non-trivial domain shifts which characterize most real-world set-tings. For example, when appearance of objects is the same in the source and target domains1, these approaches tend to over-align features of such objects [22]. Also, enforc-ing domain alignment of both image and object features in a unified manner often leads to feature misalignments [22], which could remove critical contextual cues for detection.
To address the above limitation, in this paper, we present a Bidirectional Alignment for domain adaptive Detection with Transformers – BiADT. As shown in Fig. 1, it effec-tively disentangles features into domain-invariant (I) and domain-specific (D) features, and performs bidirectional alignment, which indicates BiADT seeks to align I-features (i.e., reduce the domain gap in I-features), and simultane-ously increase distinctiveness of D-features. Architecture-wise, BiADT leverages the recent successful family of transformer-based detectors – namely, Dab-Deformable-Detr [26] and Deformable-Detr [50] – and extends these architectures with the proposed bidirectional feature align-ment in the deformable-attention and self-attention compo-nents. The former exists in both the encoder and decoder of the transformer, while the latter exists in the decoder only.
In addition to the above mentioned feature disentangle-ment and bidirectional feature alignment, BiADT has a do-main embedding component in each unit of the image and object-query token sequences. This component seeks to in-tegrate D-features from the image and object embeddings, and can be easily supervised as the domain label of each training image is known.
The closest prior work that also uses the transformer architecture for domain-adaptive object detection is AQT
[18]. AQT aligns features via three adversarial tokens space-wise, channel-wise and instance-wise. The corre-sponding attention modules in AQT guide the three adver-sarial tokens to align features for the entire image and the
Importantly, AQT does not dis-entire object sequence. entangle features into the domain-invariant and domain-specific ones. Not only do we explicitly disentangle fea-tures, but we also do so for each token individually, at both the image level and the object level. Moreover, not only do we align I-features, but also increase discriminativeness of
D-features for each token.
Recently, the teacher-student self-training (TSST) has been extended to address cross-domain object detection with adversarial learning and data augmentation [24, 14, 19]. TSST relies on generating reliable pseudo-labels, and model learning involves complex multi-stage training pro-cedures. As our experiments demonstrate, our BiADT can 1See Fig. 5: the roads in the source image and the foggy target image look similar, so there is less need to align their features. be easily integrated in the TSST framework.
Below, we summarize our key contributions: 1) We are the first to seamlessly integrate token-wise domain alignment in the standard attention module of the transformer architecture. Specifically, we propose new de-signs of the deformable attention and self-attention in the transformer that explicitly disentangle features into domain-invariant and domain-specific features, as well as perform their bidirectional alignment. 2) We propose a new token-wise domain embedding, at both the image- and object-token sequences in the trans-former, for predicting the domain label of images and ob-jects. This facilitates extraction of domain-specific features. 3) Our experiments demonstrate that BiADT produces very competitive performance to SOTA on three benchmark
Cityscapes→FoggyCityscapes, cross-domain datasets:
Sim10K→Citiscapes and Cityscapes→BDD100K. We also test different ablations and variants of BiADT, including its integration with the AQT alignments and self-training. 2.