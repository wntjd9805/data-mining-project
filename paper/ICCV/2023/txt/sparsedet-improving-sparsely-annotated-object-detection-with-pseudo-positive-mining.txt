Abstract
Training with sparse annotations is known to reduce the performance of object detectors. Previous methods have focused on proxies for missing ground truth annotations in the form of pseudo-labels for unlabeled boxes. We ob-serve that existing methods suffer at higher levels of spar-sity in the data due to noisy pseudo-labels. To prevent this, we propose an end-to-end system that learns to sep-arate the proposals into labeled and unlabeled regions us-ing Pseudo-positive mining. While the labeled regions are processed as usual, self-supervised learning is used to pro-cess the unlabeled regions thereby preventing the negative effects of noisy pseudo-labels. This novel approach has multiple advantages such as improved robustness to higher sparsity when compared to existing methods. We con-duct exhaustive experiments on five splits on the PASCAL-VOC and COCO datasets achieving state-of-the-art perfor-mance. We also unify various splits used across literature for this task and present a standardized benchmark. On average, we improve by 2.6, 3.9 and 9.6 mAP over pre-vious state-of-the-art methods on three splits of increas-ing sparsity on COCO. Our project is publicly available at cs.umd.edu/~sakshams/SparseDet. 1.

Introduction
The performance of object detectors is sensitive to the quality of labeled data [1–3]. Existing object detection methods assume that the training data is pristine and a drop in performance is observed if this assumption fails. Noise in the data used for training object detectors can arise due to incorrect class labels or incorrect/missing bounding boxes.
In this work, we deal with the problem of training object detectors with sparse annotations, i.e., missing region or bounding boxes. This problem is of utmost importance, as obtaining crowd-sourced datasets [4, 5] can be expensive
*First two authors contributed equally
Figure 1: (Top) Most Object Detection datasets have ex-haustive annotations for foreground/positives. During train-ing, the unlabeled regions can be safely considered as back-ground/negatives. Sparsely Annotated Object Detection datasets (bottom) have missing annotations. This results in foreground regions (shown in red) being considered as neg-atives during training, adversely affecting the performance of the classifier. and laborious. The alternative is to use computer-assisted protocols to collect annotations which have been shown to be noisy and incomplete [6]. This problem of training ob-ject detectors with incomplete bounding box annotations is called Sparsely Annotated Object Detection (SAOD).
To understand why training with sparse annotations is detrimental to the performance, consider the example shown in Figure 1 (top). If the annotation were exhaustive, then the negative samples to the classifier contain true back-ground regions. But with sparse annotations, as shown in
Figure 1 (bottom), a few positive regions will inevitably be considered as negatives (shown in red), thereby wrongfully penalizing the classifier leading to lower performance. Ex-isting methods[2, 7–10] prevent this by predicting pseudo-labels and removing the foreground regions from the nega-tives to the classifier. However, at higher levels of sparsity, the quality of pseudo-labels is greatly affected, resulting in the same problem noted above.
Crowd sourced object detection datasets [5, 11, 12] are ensured to be almost exhaustively labeled. Hence, for
SAOD, researchers artificially create sparsely annotated datasets from the original ones. There is no general con-sensus on the correct way to create the sparsely annotated datasets, a.k.a. splits, and hence each method reports re-sults on one or two different splits. Split can be created by considering the dataset as a whole or per image (i.e. annota-tions can be removed by considering all the images or only a single image at once). They can also be created by remov-ing annotations in a class-agnostic or class-aware fashion (i.e. remove p%, of annotations per category or across all the categories). These variations result in splits with differ-ent data distributions making some harder than the others (refer to Table 1). A proper benchmark that analyzes the performance of SAOD methods across these different types of splits is missing. This makes it difficult to compare meth-ods and assess their effectiveness for a specific use case.
To tackle the issues discussed above, as our first major contribution, we present SparseDet, a novel SAOD frame-work that achieves state-of-the-art performance across mul-tiple SAOD benchmarks in practice. SparseDet operates on an image and its augmented counterpart. The combination of features extracted from the two views is used to gener-ate region proposals. Standard detection training methods, consider a region proposal as positive if its intersection over union (IoU) with any ground truth is greater than 0.5 and the rest are treated as negatives. This strategy works when the annotations are exhaustive, which implies that the re-maining regions are from background. But due to missing annotations, some of these regions could belong to fore-ground instances. To prevent considering all region propos-als without annotations as negatives, SparseDet partitions all the region proposals into labeled, unlabeled and back-ground. The labeled and background regions are processed as usual. Features extracted from unlabeled regions are then trained with a self-supervised loss. Previous approaches like Co-Mining [10], consider two partitions, labeled and background, and generate pseudo-labels. This is a disad-vantage at high sparsity as the generated pseudo-labels can be very noisy. The self-supervised loss in our approach en-forces consistency between the features of the two views for the unlabeled regions and prevents penalizing the classifier due to false negatives.
Our second major contribution is unifying evaluation.
The standard practice is to simulate sparsely annotated training data on COCO [5] and PASCAL-VOC [11] train sets and evaluate them on their corresponding standard val-idation set. As discussed above, a survey of recent SAOD approaches [2, 7–10, 13–16] reveals that there are at least five different ways to create splits, each differing in the strat-egy (refer to Section 4.2) used to achieve the desired level of sparsity. However, these splits have not been made public, making it difficult to replicate results for comparison. Ad-ditionally, each of these strategies has a different property for simulating sparse data, e.g., different distribution of an-notations per class, resulting in a different training set. As a result, methods trained on different splits cannot be com-pared with one another. To mitigate these issues we stan-dardize the generation of these splits that enables the eval-uation of any SAOD methods on all of them for fair com-parison. Additionally, we propose a new benchmark that assesses the semi-supervised learning capabilities of SAOD methods, i.e., leveraging unlabeled data to improve perfor-mance. We present our approach as a baseline. We will make the data for the new benchmark along with all the
SAOD splits public to facilitate future research. We briefly summarize our contributions below.
• We propose a novel formulation, SparseDet, for SAOD which is an end-to-end approach that identifies labeled, unlabeled and background regions and deals with them in appropriate manner.
• We show state-of-the-art performance on sparsely an-notated object detection across various splits. On aver-age, we improve by 2.6, 3.9 and 9.6 mAP over previ-ous state-of-the-art methods on three splits of increas-ing sparsity on COCO.
• We standardize the experimental setup for SAOD by evaluating methods on all the splits to facilitate future research. Additionally, we propose a new benchmark that evaluates the semi-supervised learning capabilities of SAOD methods.
In Section 2, we discuss the related works on SAOD and related fields. We describe our approach in detail in Sec-tion 3. We describe our experimental setup and present re-sults in Section 4, and conclude in Section 5. 2.