Abstract
Generative image modeling enables a wide range of ap-plications but raises ethical concerns about responsible de-ployment. We introduce an active content tracing method combining image watermarking and Latent Diffusion Mod-els. The goal is for all generated images to conceal an invis-ible watermark allowing for future detection and/or identi-fication. The method quickly fine-tunes the latent decoder of the image generator, conditioned on a binary signature.
A pre-trained watermark extractor recovers the hidden sig-nature from any generated image and a statistical test then determines whether it comes from the generative model. We evaluate the invisibility and robustness of the watermarks on a variety of generation tasks, showing that the Stable
Signature is robust to image modifications. For instance, it detects the origin of an image generated from a text prompt, then cropped to keep 10% of the content, with 90+% accu-racy at a false positive rate below 10−6. 1.

Introduction
Recent progress in generative modeling and natural lan-guage processing enable easy creation and manipulation of photo-realistic images, such as with DALL·E 2 [64] or Sta-ble Diffusion [68]. They have given birth to many image edition tools like ControlNet [104], Instruct-Pix2Pix [7], and others [13, 28, 71], that are becoming mainstream cre-ative tools for artists, designers, and the general public.
While this is a great step forward for generative AI, it also undermines confidence in the authenticity or verac-ity of photo-realistic images. Indeed, methods for photo-realistic image edition existed before, but generative AI sig-nificantly lowers the barriers to convincing synthetic im-age generation and edition (e.g. a generated picture recently won an art competition [29]). This raises new risks like deep fakes, impersonation or copyright usurpation [8, 17]. A tool to determine that images are AI-generated would make it easier to ensure their compliance with ethical standards and to remove them from certain platforms.
*Work supported by ANR / AID under Chaire SAIDA ANR-20-CHIA-0011.
Correspondance to pfz@meta.com
Figure 1. Overview. The latent decoder can be fine-tuned to pre-emptively embed a signature into all generated images.
A baseline solution to identify generated images is foren-sics, i.e. methods to detect generated/manipulated images passively (the image is not modified for identification). An active baseline is to apply existing watermarking methods after the image generation. Watermarking invisibly embeds a secret message into the image, which can then be extracted and used to identify the image. This has several drawbacks.
If the model leaks or is open-sourced, the post-generation watermarking can be removed trivially. The open source
Stable Diffusion [70] is a case in point, since removing the watermark amounts to commenting out a single line in the source code.
Our Stable Signature method merges watermarking into the generation process itself, without any architectural change. It adjusts the pre-trained generative model such that all the images it produces conceal a given watermark. There are several advantages to this approach [46, 99]. It does not require additional processing of the generated image, which makes the watermarking computationally lighter, straight-forward, and secure. Model providers could deploy their models to different user groups with a unique watermark, and monitor that they are used in a responsible manner.
They could give art platforms, news outlets and other shar-ing platforms the ability to detect when an image has been generated by their AI.
We focus on Latent Diffusion Models (LDM) [68] that can perform a wide range of generative tasks. We show that simply fine-tuning a small part of the generative model – the decoder that generates images from the latent vectors – is enough to natively embed a watermark into generated images. Stable Signature does not require an architectural change and does not modify the diffusion process. Hence it is to our knowledge compatible with all LDM-based gener-ative methods [7, 13, 62, 71, 104]. The fine-tuning stage is performed by back-propagating a combination of a percep-tual image loss and a message decoding loss from a water-mark extractor back to the LDM decoder. We pre-train the extractor with a simplified version of the deep watermark-ing method HiDDeN [108].
We create an evaluation benchmark close to real world situations where images may be edited. The tasks are: de-tection of AI generated images, tracing models from their generations. For instance, we detect 90% of images gen-erated with the generative model, even if they are cropped to 10% of their original size, while flagging only one false positive every 106 images. To ensure that the model’s util-ity is not weakened, we show that the FID [34] score of the generation is not affected and that the generated images are perceptually indistinguishable from the ones produced by the original model. This is done over several tasks involv-ing LDM (text-to-image, inpainting, edition, etc.).
As a summary, (1) we efficiently merge watermarking into the generation process of LDMs, in a way that is com-patible with most of the LDM-based generative methods; (2) we demonstrate how it can be used to detect and trace generated images, through a real-world evaluation bench-mark; (3) we compare to post-hoc watermarking methods, showing that it is competitive while being more secure and efficient, and (4) evaluate robustness to intentional attacks. 2.