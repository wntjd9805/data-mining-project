Abstract
Input
Front view
Zoomed view
We propose a novel transformer-based framework that reconstructs two high fidelity hands from multi-view RGB images. Unlike existing hand pose estimation methods, where one typically trains a deep network to regress hand model parameters from single RGB image, we consider a more challenging problem setting where we directly regress the absolute root poses of two-hands with extended fore-arm at high resolution from egocentric view. As existing datasets are either infeasible for egocentric viewpoints or lack background variations, we create a large-scale syn-thetic dataset with diverse scenarios and collect a real dataset from multi-calibrated camera setup to verify our proposed multi-view image feature fusion strategy. To make the reconstruction physically plausible, we propose two strategies: (i) a coarse-to-fine spectral graph convolution decoder to smoothen the meshes during upsampling and (ii) an optimisation-based refinement stage at inference to pre-vent self-penetrations. Through extensive quantitative and qualitative evaluations, we show that our framework is able to produce realistic two-hand reconstructions and demon-strate the generalisation of synthetic-trained models to real data, as well as real-time AR/VR applications. 1.

Introduction 3D hand pose estimation is a fundamental problem in various downstream applications including augmented and virtual reality (AR/VR) [19, 20, 25, 26, 47, 58, 63]. Research efforts in designing 3D hand or hand-object reconstruction networks have shown great effectiveness for single-hand pose estimation [15, 22, 27, 59, 60, 66]. However, two-hand pose estimation has received relatively little attention. In this paper, as illustrated in Fig. 1, we focus on high fidelity two-hand reconstruction from multi-view RGB images.
With the availability of the InterHand2.6M dataset [46], various methods have been recently proposed. More recent
*This work was done during an internship at Google.
Rotated view
Figure 1: Our method jointly reconstructs high fidelity two-hand meshes from multi-view RGB image. The input im-ages shown here are from our synthetic dataset, which con-tains challenging video sequences of two hands rendered into egocentric views. methods focused on alleviating the self-similarity problem between interacting hands by exploiting hand part segmen-tation probability [18], joint visibility [33], cascaded refine-ment modules [65] or keypoints using Transformer [24]. Al-though they perform well on complex configurations, their dependence on root joint alignment does not provide abso-lute root pose recovery when applied to multi-view scenar-ios, which is especially crucial for interactions in VR.
Moreover, many immersive AR/VR applications require accurate estimation of two hands including extended fore-arms as it allows for a more realistic and accurate represen-tation of hand movements and gestures. By including the forearm, the orientation and movement of the hand in rela-tion to the arm can provide important contextual informa-tion for the userâ€™s actions in the virtual environment. Ad-ditionally, it can reduce errors and improve the stability of the tracking system, which is important for maintaining im-mersion and avoiding disorientation in AR/VR applications.
However, as in many other areas of computer vision, there
is currently no suitable dataset for a typically supervised deep learning setup. Specifically, existing datasets are ei-ther infeasible for egocentric views [21] or lack variations in background and lighting conditions [46].
In this paper, we propose a spectral graph-based trans-former architecture that can reconstruct high resolution two-hand meshes with extended forearms from multi-view RGB images. However, directly extending state-of-the-art meth-ods [39, 40] from single-view to multi-view setting is non-trivial as they consume a substantial number of parameters and are computationally expensive. As pointed out by Cho et al. [8], the current encoder-based transformers [39, 40] have overlooked the importance of efficient token design.
In order to retain spatial information in image features and avoid passing redundant global image features to the trans-former encoder, we propose a soft-attention-based multi-view image feature fusion strategy to obtain region-specific features. Further, as a hand mesh in essence is graph-structured data, we incorporate properties of graph Lapla-cian from spectral graph theory [10] into the design of our transformer-based network architecture. To obtain realistic hand reconstructions, we leverage a hierarchical graph de-coder and propose an optimisation-based refinement stage during inference to prevent self-penetrations. We show that each technical component above contributes meaningfully in our ablation study.
Besides the expensive computational cost, an additional challenge is the lack of high fidelity 3D mesh ground truth for training such a model. As manually annotating 3D hand meshes on real data is extremely laborious and expensive, we create a large-scale synthetic multi-view dataset contain-ing realistic hand motions from egocentric camera view-points rendered under a large variation of background and illumination. To generalise and evaluate on real-world data, we further collected real data from a precisely calibrated multi-view studio with 18 number of high-resolution cam-eras. We use an automatic approach for registering meshes and obtain ground truth hand poses. Our proposed method trained on both datasets shows robust performance on chal-lenging scenarios and can serve as a strong baseline.
Our contributions are the following: 1. We propose a novel end-to-end trainable spectral graph-based transformer for high fidelity two-hand re-construction from multi-view RGB image. 2. We design an efficient soft attention-based multi-view image feature fusion in which the resulting image fea-tures are region-specific to segmented hand mesh. We further demonstrate a minimal reduction of 35% in the model size with this approach. 3. We introduce an optimisation-based method to refine physically-implausible meshes at inference. 4. We create a large-scale synthetic multi-view dataset with high resolution 3D hand meshes and collect real dataset to verify our proposed method. 2.