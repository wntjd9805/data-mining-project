Abstract 1.

Introduction
Convolutional networks and vision transformers have different forms of pairwise interactions, pooling across lay-ers and pooling at the end of the network. Does the latter really need to be different? As a by-product of pooling, vi-sion transformers provide spatial attention for free, but this is most often of low quality unless self-supervised, which is not well studied. Is supervision really the problem?
In this work, we develop a generic pooling framework and then we formulate a number of existing methods as instantiations. By discussing the properties of each group of methods, we derive SimPool, a simple attention-based pooling mechanism as a replacement of the default one for both convolutional and transformer encoders. We find that, whether supervised or self-supervised, this improves perfor-mance on pre-training and downstream tasks and provides attention maps delineating object boundaries in all cases.
One could thus call SimPool universal. To our knowledge, we are the first to obtain attention maps in supervised trans-formers of at least as good quality as self-supervised, with-out explicit losses or modifying the architecture. Code at: https://github.com/billpsomas/simpool.
Extracting visual representations and spatial pooling have been two interconnected processes since the study of 2D Gabor filters [10] and early convolutional networks [17].
Modern convolutional networks [20, 32] gradually perform local pooling and downsampling throughout the architec-ture to extract a low-resolution feature tensor, followed by global spatial pooling. Vision transformers [14] only down-sample at input tokenization and then preserve resolution, but pooling takes place again throughout the architecture via the interaction of patch tokens with a CLS token, inher-ited from language models [13].
The pooling operation has been studied extensively in instance-level tasks on convolutional networks [3, 42], but less so in category-level tasks or transformers. Pooling in transformers is based on weighted averaging, using as weights the 2D attention map of the CLS token at the last layer. However, this attention map is typically of low qual-ity, unless under self-supervision [7].
In this work, we argue that vision transformers can be re-formulated in two streams, where one is extracting a visual representation on patch tokens and the other is performing spatial pooling on the CLS token; whereas, convolutional networks undergo global spatial pooling at the very last
step, before the classifier. In this sense, one can isolate the pooling process from both kinds of networks and replace it by a new one. This raises the following questions:
Apart from mapping to a new space, convolutional lay-ers involve a form of local pooling and pooling layers commonly take average [27] or maximum [44, 26]. 1. Can we derive a simple pooling process at the very last step of either convolutional or transformer encoders that improves over their default? 2. Can this process provide high-quality attention maps that delineate object boundaries, for both networks? 3. Do these properties hold under both supervised and self-supervised settings?
To answer these questions, we develop a generic pooling framework, parametrized by: (a) the number of vectors in the pooled representation; (b) whether pooling is iterative or not; (c) mappings at every stage of the process; (d) pairwise similarities, attention function and normalization; and (e) a function determining the pooling operation.
We then formulate a number of existing pooling meth-ods as instantiations of this framework, including (a) sim-ple pooling mechanisms in convolutional networks [20, 48, 42, 40, 47], (b) iterative methods on more than one vec-tors like k-means [34, 33], (c) feature re-weighting mech-anisms originally desinged as network components rather than pooling [23, 56], and (d) vision transformers [14, 49].
Finally, by discussing the properties of each group of meth-ods, we derive a new, simple, attention-based pooling mech-anism as a replacement of the default one for both convo-lutional and transformer encoders. SimPool provides high-quality attention maps that delineate object boundaries un-der both supervised and self-supervised settings, as shown for ViT-S [14] in Figure 1.
In summary, we make the following contributions: 1. We formulate a generic pooling framework that allows easy inspection and qualitative comparison of a wide range of methods. 2. We introduce a simple, attention-based, non-iterative, universal pooling mechanism that provides a single vector representation and answers all the above ques-tions in the affirmative. 3. We conduct an extensive empirical study that vali-dates the superior qualitative properties and quantita-tive performance of the proposed mechanism on stan-dard benchmarks and downstream tasks. 2.