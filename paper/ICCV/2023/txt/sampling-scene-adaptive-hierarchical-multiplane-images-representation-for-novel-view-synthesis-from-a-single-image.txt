Abstract ples dataset. The code and models will be made available at https://pkuvdig.github.io/SAMPLING/.
Recent novel view synthesis methods obtain promising results for relatively small scenes, e.g., indoor environments and scenes with a few objects, but tend to fail for unbounded outdoor scenes with a single image as input. In this paper, we introduce SAMPLING, a Scene-adaptive Hierarchical
Multiplane Images Representation for Novel View Synthe-sis from a Single Image based on improved multiplane im-ages (MPI). Observing that depth distribution varies sig-nificantly for unbounded outdoor scenes, we employ an adaptive-bins strategy for MPI to arrange planes in accor-dance with each scene image. To represent intricate ge-ometry and multi-scale details, we further introduce a hi-erarchical refinement branch, which results in high-quality synthesized novel views. Our method demonstrates consid-erable performance gains in synthesizing large-scale un-bounded outdoor scenes using a single image on the KITTI dataset and generalizes well to the unseen Tanks and Tem-*Work is done during the internship at Peking University.
†Corresponding author. 1.

Introduction
Taking a photo and using it to synthesize photo-realistic images at novel views is an important task with a wide range of applications, such as generating realistic data for training
AI models (e.g., autonomous driving perception and robot simulation). This task is challenging as it requires a pre-cise understanding of 3D geometry, reasoning about occlu-sions, and rendering high-quality, spatially consistent novel views from a single image. It becomes even more difficult for large-scale unbounded outdoor scenes, which contain complex geometric conditions, various objects, and diverse depth distributions corresponding to different scenes.
Recently, Neural Radiance Field (NeRF) [1] based meth-ods have gained much attention by synthesizing photo-realistic images with dense multi-view inputs. By lever-aging Multi-layer Perceptron (MLP) layers, NeRF implic-itly models a specific scene via RGB values and volume occupancy density. However, NeRF-based methods are primarily applicable for rendering bounded objects or in-teriors, which are impeded by the stringent requirement for the dense views captured from different angles, pre-cise corresponding camera poses, and unobstructed condi-tions [2, 3, 4]. Furthermore, these methods rely on per-scene fitting and cannot easily generalize to unseen scenes.
Several methods [5, 6, 7, 8] try to utilize multi-modal data, e.g., LiDAR scans and point clouds, to complicate the syn-thesis of novel views in large scenes. However, additional modalities are difficult to obtain and have greater memory consumption and computational costs. Besides, similar to
NeRF, these multi-modal methods require multiple input views with large overlaps and need to be trained per scene.
In contrast, the Multiplane Images (MPI) representa-tion [9] has shown promising results in synthesizing scenes from sparse views, using a set of parallel semi-transparent planes to approximate the light field. The MPI repre-sentation is particularly effective at understanding com-plex scenes with challenging occlusions [10]. However, prior MPI-based approaches place planes at fixed depths with equal intervals, have limitations in modeling irreg-ular geometry, such as texture details, and do not per-form well in unbounded outdoor scenes, as shown in Fig-ure 1. For complicated geographic features and differen-tiated depth ranges, the uniform static MPIs [11, 9, 12] are often over-parameterized for large areas of space, yet under-parameterized for the occupied scenes. In addition, using single-scale scene representation in MPI also limits the quality of the synthesized images in large-scale scenes, leading to apparent artifacts and blurs.
In this paper, we introduce SAMPLING, a scene-adaptive hierarchical representation for novel view synthe-sis from a single image based on improved MPI. Instead of generating multiplanes with a static uniform strategy, we design the Adaptive-bins MPI Generation strategy to adaptively distribute the planes according to each input im-age. This strategy enables a more efficient representation to better fit various unbounded outdoor scenes. Addition-ally, we propose a Hierarchical Refinement Branch that uti-lizes multi-scale information from large scenes, incorporat-ing both global geometries and high-frequency details into the MPI representation. This branch enhances the qual-ity of intermediate scene representations, resulting in more complete and high-quality synthesized images. Our method achieves high-quality view synthesis results on challenging outdoor scenes, such as urban scenes, and shows a well cross-scene generalization, enabling a more versatile scene representation. Our main contributions are:
• We present a novel scene-adaptive representation for synthesizing new views from a single image. Our ap-proach is based on learnable adaptive-bins for MPI, enabling the learning of a more efficient and effective unbounded scene representation from a single view.
• We develop a hierarchical refinement method for 3D representation of outdoor scenes. We show that repre-senting scenes with hierarchical information can syn-thesize new images with favorable details.
• Our method achieves new state-of-the-art performance in outdoor view synthesis from a single image. Exper-imental results also show our method generalizes well for both outdoor and indoor scenes. 2.