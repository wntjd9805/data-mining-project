Abstract
Active learning (AL) aims to select the most useful data samples from an unlabeled data pool and annotate them to expand the labeled dataset under a limited budget. Espe-cially, uncertainty-based methods choose the most uncer-tain samples, which are known to be effective in improving model performance. However, previous methods often over-look training dynamics (TD), defined as the ever-changing model behavior during optimization via stochastic gradi-ent descent, even though other research areas have empir-ically shown that TD provides important clues for measur-ing the data uncertainty. In this paper, we first provide the-oretical and empirical evidence to argue the usefulness of utilizing the ever-changing model behavior rather than the fully trained model snapshot. We then propose a novel AL method, Training Dynamics for Active Learning (TiDAL), which efficiently predicts the training dynamics of unla-beled data to estimate their uncertainty. Experimental re-sults show that our TiDAL achieves better or comparable performance on both balanced and imbalanced benchmark datasets compared to state-of-the-art AL methods, which es-timate data uncertainty using only static information after model training. 1.

Introduction
“There is a tide in the affairs of men. Which taken at the flood, leads on to fortune.” — William Shakespeare
Active learning (AL) [5, 31] aims to solve the real-world problem of selecting the most useful data samples from large-scale unlabeled data pools and annotating them to expand labeled data under a limited budget. Since the current deep neural networks are data-hungry, AL has in-creasingly gained attention in recent years. Existing AL methods can be divided into two mainstream categories: diversity- and uncertainty-based methods. Diversity-based methods [42, 14] focus on constructing a subset that fol-†Equal contribution.
∗Corresponding author.
This work was done while all authors were affiliated with Hyperconnect.
Figure 1: Our proposed TiDAL. TD of training samples x may differ even if they converge to the same final pre-dicted probability p(y∗|x) (Upper row). Hence, we are mo-tivated to utilize the readily available rich information gen-erated during training, i.e., leveraging TD. We estimate TD of large-scale unlabeled data using a prediction module in-stead of tracking the actual TD of all the unlabeled samples to avoid the computational overhead (Lower row). lows the target data distribution. Uncertainty-based meth-ods [13, 6, 52] choose the most uncertain samples, which are known to be effective in improving model performance.
Hence, the most critical question for the latter becomes,
“How can we quantify the data uncertainty?”
In this paper, we leverage training dynamics (TD) to quantify data uncertainty. TD is defined as the ever-changing model behavior on each data sample during op-timization via stochastic gradient descent. Recent stud-ies [9, 29, 48, 47] have provided empirical evidence that
TD provides important clues for measuring the contribution of each data sample to model performance improvement.
Inspired by these studies, we argue that the data uncertainty
of unlabeled data can be estimated with TD. However, most uncertainty-based methods quantify data uncertainty based on static information (e.g., loss [52] or predicted probabil-ity [45]) from a fully-trained model “snapshot,” neglecting the valuable information generated during training. We fur-ther argue that TD is more effective in separating uncertain and certain data than static information from a model snap-shot captured after model training. In §3, we provide both theoretical and empirical evidence to support our argument that TD is a valuable tool for quantifying data uncertainty.
Despite its huge potential, TD is not yet actively ex-plored in the domain of AL. This is because AL assumes a massive unlabeled data pool. Previous studies track TD only for the training data every epoch as it can be recorded easily during model optimization. On the other hand, AL targets a large number of unlabeled data, where tracking the TD for each unlabeled sample requires an impractical amount of computation (e.g., inference all the unlabeled samples every training epoch).
Therefore, we propose TiDAL (Training Dynamics for
Active Learning), a novel AL method that efficiently quan-tifies the uncertainty of unlabeled data by estimating their
TD. We avoid tracking the TD of large-scale unlabeled data every epoch by predicting the TD of unlabeled samples with a TD prediction module. The module is trained with the TD of labeled data, which is readily available during model op-timization. During the data selection phase, we predict the
TD of unlabeled data with the trained module to quantify their uncertainties. We efficiently obtain TD using the mod-ule, which avoids inferring all the unlabeled samples every epoch. Experimental results demonstrate that our TiDAL achieves better or comparable performance to existing AL methods on both balanced and imbalanced datasets. Ad-ditional analyses show that our prediction module success-fully predicts TD, and the predicted TD is useful in estimat-ing uncertainties of unlabeled data. Our proposed method are illustrated in Figure 1.
Contributions of our study: (1) We bridge the concept of training dynamics and active learning with the theoretical and experimental evidence that training dynamics is effec-tive in estimating data uncertainty. (2) We propose a new method that efficiently predicts the training dynamics of un-labeled data to estimate their uncertainty. (3) Our proposed method achieves better or comparable performance on both balanced and imbalanced benchmark datasets compared to existing active learning methods. For reproducibility, we release the source code1. 2. Preliminaries
To better understand our proposed method, we first sum-including uncertainty-based active marize key concepts, 1https://github.com/hyperconnect/TiDAL learning, quantification of uncertainty, and training dynam-ics.
Uncertainty-based active learning.
In this work, we fo-cus on uncertainty-based AL for multi-class classification problems. We define the predicted probabilities of the given sample x for C classes as: p = [p(1|x), p(2|x),
· · · , p(C|x)]T ∈ [0, 1]C, (1) where we denote the true label of x as y and the classifier as f . D and Du denote a labeled dataset and an unlabeled data pool, respectively. The general cycle of uncertainty-based AL is in two steps: (1) train the target classifier f on the labeled dataset D and (2) select top-k uncertain data samples from the unlabeled data pool Du. Selected samples are then given to the human annotators to expand the labeled dataset D, cycling back to the first step.
Quantifying uncertainty. The objective of this study is to establish a connection between the concept of TD and the field of AL. In order to clearly demonstrate the effective-ness of utilizing TD to quantify data uncertainty, we have employed two of the most prevalent and straightforward es-timators, entropy [43] and margin [41], to measure data un-certainty in this paper. Entropy H is defined as follows:
H(p) = − (cid:88)C c=1 p(c|x) log p(c|x), (2) where the sample x is from the unlabeled data pool Du.
Entropy concentrates on the level of the model’s confidence on the given sample x and gets bigger when the prediction across the classes becomes uniform (i.e., uncertain). Margin
M measures the difference between the probability of the true label and the maximum of the others:
M (p) = p(y|x) − max c̸=ˆy p(c|x), (3) where y denotes the true label. The smaller the margin, the lower the model’s confidence in the sample, so it can be considered uncertain. Both entropy and margin are com-puted with the predicted probabilities p of the fully trained classifier f , only taking the snapshot of f into account.
Defining training dynamics. Our TiDAL targets to lever-age TD of unlabeled data to estimate their uncertainties. TD can be defined as any model behavior during optimization, such as the area under the margin between logit values of the target class and the other largest class [39] or the vari-ance of the predicted probabilities generated at each epoch
[47]. In this work, we define the TD ¯p(t) as the area under the predicted probabilities of each data sample x obtained
fewer samples. Hence, we regard major and minor class samples to contain many certain and uncertain samples for the model, respectively. We train the target classifier f on the long-tailed dataset during T epochs to obtain the TD and the model snapshot. We apply both approaches to the com-mon estimators, entropy and margin. We denote entropy and margin scores from the model snapshot as H and M .
In opposition, we denote the TD-driven scores as ¯H and ¯M .
More details and discussions are described in Appendix B.
Results. Figure 2 shows the distribution the scores calcu-lated with TD (x-axis) and model snapshot (y-axis). We can observe that scores from TD ( ¯H, ¯M ) successfully separate the major and the minor class samples, whereas scores from the model snapshot (H, M ) fail to do so. We conclude that compared to model snapshots, TD is more helpful in sepa-rating uncertain samples from certain samples. 3.2. Theoretical Evidence (Informal) Under the LE-SDE framework
Theorem 1.
[54], with the assumption of local elasticity [17], certain samples and uncertain samples reveal different TD; es-pecially, certain samples converge quickly than uncertain samples.
The above theorem discusses different model behaviors depending on the difficulty of the sample. Compared to the uncertain sample, the certain sample has the same class samples nearby, which is the fundamental idea of level set estimation [22] and nearest neighbor [36] literature. We suspect that, due to the local elasticity of deep nets, samples close by have a bigger impact on the certain sample, hence changing its predicted probability more rapidly. As the cer-tain sample is quicker to converge, its TD is larger than that of the uncertain sample. Intuitively, slower to train, strug-gling the classifier is to learn, hence TD capturing the un-certainty in the classifier’s perspective.
Theorem 2. (Informal) Estimators such as Entropy (Equa-tion 10) and Margin (Equation 11) successfully capture the difference of TD between easy and hard samples even for the case where it cannot be distinguished via the predicted probabilities of the model snapshot.
The above theorem discusses the validity of entropy and margin on whether they can successfully differentiate be-tween two samples of different TD but with the same final prediction. With Theorem 1, one can conclude that the com-mon estimators’ scores calculated with TD are effective in capturing the data uncertainty. Due to the space constraints, we provide the details of the above results in Appendix A. 4. Utilizing TD for Active Learning
As tracking the TD of all the unlabeled data is compu-tationally infeasible, we devise an efficient method to es-(a) Entropy Distribution (b) Margin Distribution
Figure 2: Score distribution after long-tailed training. We plot the marginal distributions using kernel density estima-tion (KDE). It is difficult to separate major (certain) and mi-nor (uncertain) samples by the model snapshot-based scores (horizontal), unlike the TD-driven scores (vertical) that en-able clearly separating the certain and uncertain samples. during the t time steps of optimizing the target classifier f : p(i) = [p(i)(1|x), p(i)(2|x),
¯p(t) = [¯p(t)(1|x), ¯p(t)(2|x), (cid:88)t (cid:88) p(τ )∆τ ≃
=
· · · , p(i)(C|x)]T ,
· · · , ¯p(t)(C|x)]T p(i)/t, (4) (5)
τ i=1 where p(i) is the predicted probabilities of a target classifier f at the i-th time step. ∆τ is the unit time step to normal-ize the predicted probabilities. For simplicity, we record p(i) every epoch and choose ∆τ = 1/t, namely, averaging the predicted probabilities during t epochs [47, 46]. The
TD ¯p(t) takes all the predicted probabilities during model optimization into account. Hence, it encapsulates the over-all tendency of the model during t epochs of optimization, avoiding being solely biased towards the snapshot of p(t) in the final epoch t. 3. Is TD Useful for Quantifying Uncertainty?
In this section, we provide empirical and theoretical ev-idence to support our argument: TD is more effective in separating uncertain data from certain data than the model snapshot, where the latter is often utilized to quantify data uncertainty in previous works [52, 45]. 3.1. Motivating Observation
Settings. We aim to observe and compare the behavior of
TD and the model snapshot for different sample difficulties.
However, it is nontrivial to directly measure sample-wise difficulty, inhibiting the quantitative analysis of data uncer-tainty. To avoid this, we borrow the theoretical and empiri-cal results of long-tailed visual recognition [33, 8, 19]: it is hard for the deep neural network-based model to train with
timate the TD of unlabeled samples. We train the module that directly predicts the TD of each sample by feeding the training samples, where its TD are freely available during training. Then, based on the predicted TD of each unla-beled sample, we use the common estimators, entropy or margin, to determine which sample is the most uncertain so that human annotators can label it. Hence, in this sec-tion, we describe the details of the module that estimates
TD (§4.1) and how to train the module (§4.2). Finally, cal-culating the uncertainties using the module predictions for active learning is illustrated (§4.3). 4.1. Training Dynamics Prediction Module
As mentioned, it is not computationally feasible to track
TD for the large-scale unlabeled data as it requires model in-ference on all the unlabeled data every training epoch. Thus, we propose the TD prediction module m to efficiently pre-dict the TD of unlabeled data at the t-th epoch. Being influ-enced by the previous studies [11, 52, 45, 25] that use addi-tional modules to predict useful values such as loss or confi-dence by the target model outputs, multi-scale feature maps are aggregated and passed into our TD prediction module.
The module produces the C-dimensional predictions: m (C|x)]T ∈ [0, 1]C m = [˜p(t)
˜p(t)
· · · , ˜p(t) m (1|x), (6) estimating the actual TD ¯p(t) of the given sample x in Equa-tion 5. TD prediction module is jointly trained with the tar-get classifier using a handful of parameters, having a neg-ligible computational cost during training. The detailed ar-chitecture of the module is described in Appendix C.
Even though the architecture is similar to previous works
[52, 45, 25], we observed that ours were much more stable during optimization and easier to train. We suspect that it is due to the target task difference; previous works trained the module that outputs only a single value via regression, whereas our module outputs C-dimensional probability dis-tribution, which is similar to the main task of classifying images. 4.2. Training Objectives
To train the target classifier f at the t-th epoch, we use the cross-entropy loss function Ltarget on the predicted probability p(t) and a one-hot encoded vector y ∈ {0, 1}C of the true label y:
Ltarget = LCE(p(t), y) = − log p(t)(y|x). (7)
Meanwhile, the prediction module m learns the TD of a sample x by minimizing the Kullback–Leibler (KL) diver-gence between the predicted TD ˜p(t) m and the actual TD ¯p(t):
Lmodule = LKL( ¯p(t)|| ˜p(t) m )
= (cid:88)C c=1
¯p(t)(c|x) log (cid:33) (cid:32)
¯p(t)(c|x)
˜p(t) m (c|x)
. (8)
The final objective function of our proposed method is de-fined as follows:
L = Ltarget + λLmodule (9) where λ is a balancing factor to control the effect of Lmodule during model training. 4.3. Quantifying Uncertainty with TD
We argue that uncertain samples can be effectively dis-tinguished from unlabeled data using the predicted TD. To verify the effectiveness of leveraging TD, we feed the pre-dicted TD to entropy and margin (§2) by replacing snapshot probability p with the predicted TD ¯p. We choose these es-timators as they are widely used for quantifying uncertainty.
We feed ¯p, replacing p, to the entropy ¯H:
¯H( ¯p) = − (cid:88)C c=1
¯p(c|x) log ¯p(c|x). (10)
Entropy ¯H is maximized when ¯p is uniform, i.e., the sam-ple is uncertain for the target classifier. Margin ¯M is also similarly employed:
¯M ( ¯p) = ¯p(ˆy|x) − max c̸=ˆy
¯p(c|x). (11)
Since we do not have true labels of unlabeled samples, we use the predicted labels ˆy of the target classifier instead of the true labels. There are several possible variants of ¯M depending on the definition of ˆy. We conduct experiments to compare ¯M with its variants. The experimental details and results are in Appendix D.4.
At the data selection phase, we use the predicted TD ˜p(T ) m instead of the actual TD ¯p(T ) as in Equation 10 & 11 to es-timate the TD-driven uncertainties of the unlabeled sample x at the final epoch T . By using the estimated uncertainty with the predicted TD, we select the most informative sam-ples for model training. 5. Experiments
In this section, we experimentally verify the effective-ness of our method, TiDAL, which utilizes the estimated training dynamics from the prediction module to discern uncertain samples from unlabeled data. We describe the de-tailed settings and the baseline methods for our experiments (§5.1) and show the results on both balanced (§5.2) and im-balanced datasets (§5.3). We further analyze whether the
TD prediction module is effective for AL performance and can successfully estimate the TD (§5.4). We end the section by discussing the potential limitations of our method (§5.5). 5.1. Experimental Setup
Datasets. To assess the performance of our proposed method and baseline methods, we conduct experiments on
Figure 3: Averaged relative accuracy improvement curves and their 95% confidence interval (shaded) of AL methods over the number of labeled samples on balanced datasets. TiDAL ( ¯H) and TiDAL ( ¯M ) denote the performance of TiDAL when with entropy ¯H and margin ¯M as the data uncertainty estimation strategy, respectively. the following five datasets: CIFAR10/100 [27], FashionM-NIST [51], SVHN [34], and iNaturalist2018 [50]. Since
CIFAR and FashionMNIST are both balanced, we further modify them to simulate the data imbalance in the real world, following the previous long-tail visual recognition studies [8, 33, 56, 19]. The imbalance ratio is defined as
Nmax/Nmin where N is the number of samples in each class.
We make two variants with data imbalance ratios 10 and 100 for each dataset. Unlike the above, SVHN and iNatu-ralist18 are already imbalanced. Especially, iNaturalist2018 is commonly chosen to demonstrate how methods work in imbalanced real-world settings. The dataset statistics are summarized in Appendix D.
Baselines. For a fair comparison, we compare our TiDAL with the following baselines which train a target classi-fier with only labeled data. Random sampling: a simple baseline that randomly selects data samples from the un-labeled dataset. Entropy sampling [43]: an uncertainty-based method that selects data samples based on the max-imum entropy. BALD [13]: an uncertainty-based method that selects data samples based on the mutual information between the model prediction and the posterior. Core-Set [42]: a diversity-based method that selects representa-tive data samples covering all data through a minimum ra-dius. LLoss [52]: an uncertainty-based method that learns to estimate the errors of the predictions (loss) made by the learner and select data samples based on the predicted loss.
CAL [55]: recent work on using TD, gathering sample-wise TD information on whether the classifier was consis-tently correct or not during training. CAL splits the samples into two classes by applying a heuristic threshold to the TD information to train a binary classifier that outputs uncer-tainty score. To verify the effectiveness of TiDAL, we fur-ther compare it with the two semi-supervised AL methods,
VAAL [45] and TA-VAAL [25] in Appendix D.3. Note that these methods further utilize unlabeled data for training the selection module, thus it is unfair for our TiDAL.
Active learning setting. We follow the same setting from
[6, 52] for the detailed AL settings. For the initial step, we randomly select initial samples to be annotated from the un-labeled dataset, where we use them to train the initial target classifier. Then, we obtain a random subset from the unla-beled data pool Du to choose the top-k samples based on the criterion of each method, where those samples will be annotated. We repeat the above cycle, training a classifier from scratch from the continuously expanding labeled set.
Implementation details. For a fair comparison, we use the same backbone network ResNet-18 [18] except for iNat-uralist2018, where we use ResNet-50 [18] pretrained on Im-ageNet [12]. All models are trained with SGD optimizer with momentum 0.9, weight decay 5 · 10−4, and learning rate (LR) decay of 0.1. For CIFAR10/100 and SVHN, we train the model for 200 epochs with an initial LR of 0.1 and decay at epoch 160. For FashionMNIST, 100 epochs with an initial LR of 0.1 and decay at epoch 80. For iNatural-ist2018, 50 epochs with an initial LR of 0.01 and decay at epoch 40. For CIFAR10/100, SVHN and FashionMNIST, we set the batch size and the unlabeled subset size to be 128 and 104, respectively. For iNaturalist2018, which is much larger than other datasets, we set the batch size and the un-labeled subset size to 256 and 106, respectively. We set the balancing factor to 1.0.
Evaluation details. To compare with other state-of-the-art baselines, we show the average accuracy and 95% con-fidence interval with three trials. We mainly compare the model performances with relative accuracy improvement to random sampling, demonstrating how much it improves
Figure 4: Averaged relative accuracy improvement curves and their 95% confidence interval (shaded) of AL methods over the number of labeled samples on synthetically imbalanced datasets. We use the imbalance ratio (IR) of 10 and 100 on CIFAR10,
CIFAR100, and FashionMNIST. upon the naive approach on each cycle. Additionally, abso-lute accuracy is also plotted in Appendix D. 5.2. Results on Balanced Datasets
Figure 3 and 10 compare our TiDAL against the state-of-the-art methods on various balanced datasets: CIFAR10,
CIFAR100, and FashionMNIST. For all the datsets, the two variants of TiDAL outperform all the baselines at all AL cycles except for LLoss, which shows better improvement than TiDAL ( ¯M ) on CIFAR10 with an imbalance ratio of 100. Nonetheless, our TiDAL achieves the best final per-formance compared to all the baselines. CAL, which uses training dynamics, generally underperforms compared to others. We suspect that CAL is sensitive to its threshold hyperparameter. 5.3. Results on Imbalanced Datasets
Synthetically imbalanced datasets. Similar to the above,
Figure 4, 9, and 11 shows the performance improvements on the synthetically imbalanced datasets with the two im-balance ratios, 10 and 100. Except for the CIFAR10 with an imbalance ratio of 100, our methods show superb per-formance across all the imbalanced settings. TiDAL per-forms especially well with a small variance in imbalanced
CIFAR100, where the number of classes is the largest. In imbalanced FashionMNIST, the performance quickly rises to 2.5k labeled images and then saturates. This implies that
FashionMNIST is easier than other datasets, and needs to focus more on the early training steps to compare with other models. TiDAL also shows overall better performance on
FashionMNIST, especially in the early steps.
Real-world imbalanced datasets. Figure 5 and 10 shows evaluation results on real-world imbalanced datasets. For iNaturalist2018, which is the large-scale long-tailed clas-sification dataset, TiDAL shows outstanding performance compared to other methods. For SVHN, TiDAL shows the best improvements with low variance as the number of la-beled images increases except for the initial stage. LLoss shows outstanding performance only in the initial stage, where we presume that the loss prediction module of LLoss acts as a regularizer during model optimization. 5.4. Analysis on the TD Prediction Module
Effectiveness of the TD prediction module.
In order to verify the efficacy of using the predicted TD ˜pm, we con-Figure 5: Averaged relative accuracy improvement curves and its 95% confidence interval (shaded) of AL methods over the number of labeled samples on real-world imbalanced datasets: iNaturalist2018 and SVHN. For SVHN, LLoss shows a substantial improvement of 20.02% ± 6.77% at the initial phase (1k), but we clip the plot to show the performance afterward more clearly. (a) CIFAR10 (b) CIFAR100
Figure 6: Ablation test results. ¯H( ˜pm) and ¯M ( ˜pm) use the predicted
TD ˜pm of the prediction module m. In contrast, H(p) and M (p) use the predicted probability of the model snapshot p. TD shows better perfor-mance than the model snapshot, implying that TD is better at quantifying data uncertainty.
Figure 7: KL divergence scores of the actual
TD ¯p(T ) with the predicted TD ˜p(t) m and the pre-dicted probability of the model snapshot p(t), respectively, during model optimization. Our predicted TD can accurately approximate the actual TD. duct an ablation test that compares the performance be-tween when using and not using the TD prediction module m. Figure 6 shows the results on balanced CIFAR10/100.
We observe that ¯H( ˜pm) and ¯M ( ˜pm) using the predicted
TD ˜pm to estimate the data uncertainty significantly out-perform the methods H(p) and M (p) that use only the fi-nal predicted probabilities p of the target classifier f , show-ing better performance in the whole training cycle. Even
M (p) shows temporary improvement in earlier steps on
CIFAR100, ¯H( ˜pm) and ¯M ( ˜pm) maintain stable improve-ment, eventually winning over M (p). This indicates that the predicted TD ˜pm of the TD prediction module m pro-duces better data uncertainty estimation than the predicted probability p of the target classifier f .
Predictive performance of the TD prediction module.
We verify whether the TD prediction module m accurately predicts the actual TD ¯p.
Its prediction performance is crucial as we use the predicted TD ˜pm of the module m to quantify uncertainties of unlabeled data. Using the
KL divergence LKL, we analyze that the predicted TD
˜pm converges to the actual TD ¯p at the data selection phase. We calculate LKL( ¯p(T )|| ˜p(t) m ) and compare it with
LKL( ¯p(T )||p(t)) which is set as a baseline computed with the actual TD ¯p and the predicted probabilities p (snapshot) of the target classifier f . In this analysis, we use the bal-anced CIFAR10 where the sample-wise averaged KL diver-gence scores are computed on the test set. Figure 7 shows that the final predicted TD successfully approximates the actual TD, while the predicted probability is highly differ-ent from the actual TD. We conclude that the TD prediction module m can produce the TD efficiently, leading to perfor-mance improvement, and the predicted TD acts as a better approximation of the actual TD than the predicted probabil-ity of a model snapshot captured at each epoch. 5.5. Limitations
We found two potential limitations of our TiDAL de-rived from the fact that it relies on the outputs of the target classifier to compute the TD. First, TiDAL is designed only for classification tasks, and thus it cannot be applied to AL targeting other tasks, such as regression [10, 15]. Second,
TiDAL is highly influenced by the performance of the tar-get classifier, especially when the target classifier wrongly classifies the hard negative samples with a high confidence during model optimization. These samples can be treated as certain samples (i.e. will not be selected for annotation) because they have low estimated uncertainties from the pre-dicted TD, even though the target classifier fails to predict the true label of the samples correctly. As a future work, we will study extending our TiDAL in the task-agnostic ways with a safeguard combating the wrongly classified samples. 6.