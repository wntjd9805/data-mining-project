Abstract
Vision Transformers (ViTs) are normally regarded as a stack of transformer layers.
In this work, we propose a novel view of ViTs showing that they can be seen as en-semble networks containing multiple parallel paths with different lengths. Specifically, we equivalently transform the traditional cascade of multi-head self-attention (MSA) and feed-forward network (FFN) into three parallel paths in each transformer layer. Then, we utilize the identity con-nection in our new transformer form and further transform the ViT into an explicit multi-path ensemble network. From the new perspective, these paths perform two functions: the first is to provide the feature for the classifier directly, and the second is to provide the lower-level feature representa-tion for subsequent longer paths. We investigate the influ-ence of each path for the final prediction and discover that some paths even pull down the performance. Therefore, we propose the path pruning and EnsembleScale skills for im-provement, which cut out the underperforming paths and re-weight the ensemble components, respectively, to optimize the path combination and make the short paths focus on providing high-quality representation for subsequent paths.
We also demonstrate that our path combination strategies can help ViTs go deeper and act as high-pass filters to filter out partial low-frequency signals. To further enhance the representation of paths served for subsequent paths, self-distillation is applied to transfer knowledge from the long paths to the short paths. This work calls for more future research to explain and design ViTs from new perspectives. 1.

Introduction
Vision Transformer (ViT) [15] consists of alternating layers of Multi-Head Self-Attention (MHSA) and Feed-Forward Network (FFN). Most follow-ups [36, 46, 41, 43, 22, 49, 28] focus on polishing these two core modules and create various ViT variants. However, most of them do not
*Work done during an internship at Alibaba Group.
†Work done at Alibaba Group, and now affiliated with Amazon.
‡Equal corresponding authors. (a) (b)
Figure 1: (a) Standard transformer form in modern ViTs is gener-ally seen as a cascade of self-attention and FFN. (b) A three-path parallel form of transformer obtained by the equivalent transfor-mation of (a). break the basic ViT structure, i.e., a stack of transformers containing residual sub-layers, for analysis.
Residual connections [18] are universally adopted to by-pass their sub-layers in ViTs, allowing data to flow from the previous layer directly to the subsequent layer. They are defined as the form xi = gi(xi−1) + Ri(xi−1), (1) where the layer function gi and Ri are typically identity and main building block. In ViTs, we observe that nearly all the non-linear structures accord with the form of Ri in
Eq. 1, such as MHSA and FFN, and only linear structures exist between Ri and Ri+1 in most cases, so that the fi-nal feature fed into classifier can be seen as a linear com-bination of multi-path output. This key insightful observa-tion inspires us that the ViTs can be viewed as a collection of many paths instead of a traditional single deep network.
Specifically, we equivalently transform the traditional cas-cade of MHSA and FFN into three parallel paths in each transformer layer, as shown in Figure 1. Then, we utilize the identity connection in our new transformer form and fur-ther transform the ViT into an explicit multi-path ensemble network. Our ensemble network is equivalent to the tradi-tional structure, which can be verified by mathematics and experiments, while the output of each path can be operated independently.
In our ensemble view, each path performs two functions: the first is to provide the feature for the classifier directly, and the second is to provide the feature representation for subsequent longer paths. We propose new path combina-tion and self-distillation to boost two functions separately to improve the performance of ViTs. We investigate the contri-bution of each path for the final prediction by analyzing the cosine distance and ablating different paths, and reveal that not all the paths are beneficial for the final results. Based on this observation, we design two simple and FLOPs-free path combination methods to optimize their combinations: path pruning which prunes underperforming paths, and Ensem-bleScale which re-weights the paths and makes the short paths focus on extracting high-quality representations for subsequent paths. Moreover, we discover that the model tends to enlarge the scales of the features of long paths to dilute the component of short paths, which increases the dif-ficulty of optimization and raises the risk of divergence in deeper ViTs. Our EnsembleScale can make the model ad-just the scale of EnsembleScale instead of features to al-leviate this issue. According to the recent study of ViTs in frequency domain [40, 30], the low-pass filter property of self-attention weakens the expression of high-frequency signals. Our path combination methods can act as high-pass filters to remove partial useless low-frequency signals, and it achieves the goal of improving the first function of paths.
To further improve the second function, that is, improv-ing their representation utilized by the subsequent paths, we propose to transfer knowledge between different paths by knowledge distillation (KD). Thanks to the ensemble-like structure, we can perform self-distillation in a general teacher-student knowledge distillation way. We apply two types of distillation, prediction-logit distillation and hidden-state distillation, to allow the shorter paths to mimic the logit and feature relation of longer paths. Compared with traditional self-distillation methods [52, 23], our method does not increase training parameters and memory cost.
The contributions of this paper are summarized as below.
• We propose a novel view of ViTs, which illustrates that
ViTs can be seen as a collection of paths, instead of a traditional single-path network. We can improve ViTs by optimizing the paths.
• Based on the proposed view, we investigate the con-tribution of different paths for the final prediction and find out that not all the paths are positive. We present path pruning and EnsembleScale to boost the ensemble performance.
• To further enhance the representation ability of the paths, we design a self-distillation for ViTs. The teacher network and student network are appropriately selected from the paths, making the knowledge trans-fer among the paths effectively. 2.