Abstract
Recently, the community has made tremendous progress in developing effective methods for point cloud video under-standing that learn from massive amounts of labeled data.
However, annotating point cloud videos is usually notori-ously expensive. Moreover, training via one or only a few traditional tasks (e.g., classification) may be insufficient to learn subtle details of the spatio-temporal structure existing in point cloud videos. In this paper, we propose a Masked
Spatio-Temporal Structure Prediction (MaST-Pre) method to capture the structure of point cloud videos without human annotations. MaST-Pre is based on spatio-temporal point-tube masking and consists of two self-supervised learning tasks. First, by reconstructing masked point tubes, our method is able to capture the appearance information of point cloud videos. Second, to learn motion, we propose a temporal cardinality difference prediction task that esti-mates the change in the number of points within a point tube. In this way, MaST-Pre is forced to model the spatial and temporal structure in point cloud videos. Extensive ex-periments on MSRAction-3D, NTU-RGBD, NvGesture, and
SHREC’17 demonstrate the effectiveness of the proposed method. The code is available at https://github. com/JohnsonSign/MaST-Pre. 1.

Introduction
In physics, motion is the phenomenon in which posi-tion changes over time. Because point clouds provide pre-cise position information, i.e., 3D coordinates, point cloud videos, which evolve over time, can accurately describe the 3D motion in the real world. Effectively understand-ing point cloud videos can significantly improve intelligent agents on the interaction with environments. Therefore, the community has developed a few effective methods for
*These authors contributed equally.
†Corresponding author.
Figure 1. Our MaST-Pre is based on spatio-temporal point-tube masking. To enable a model to capture the appearance structure in point cloud videos, we ask it to reconstruct masked point tubes.
To equip the model with motion modeling ability, we develop a temporal cardinality difference prediction task. point cloud video understanding, including video classifica-tion [9–12,40,50] and semantic segmentation [6,24,42,43].
However, most of these methods are based on supervised learning and that requires much effort to carefully anno-tate massive amounts of labels. Moreover, learning via only classification or segmentation may make deep neu-ral networks take too much emphasis on the task itself but largely ignore the subtle details of the instinct spatio-temporal structure in point cloud videos. To alleviate those problems, we propose a self-supervised learning method on point cloud videos.
Self-supervised learning uses supervisory signals from the data itself and enables deep neural networks to learn from massive data without human annotations. This is im-portant to recognize more subtle patterns in data. Net-works pre-trained with self-supervised learning usually yield higher performance than when solely trained in a su-pervised manner [5, 15, 16, 19]. Although self-supervised
learning has been applied to images [15], videos [13, 37] and static point clouds [29, 47], it has not been promoted on 4D signals, such as point cloud videos. Visual signals in point cloud videos can be divided into appearance and motion. While appearance specifies which objects are in videos, motion describes their dynamics. Therefore, self-supervised learning on point cloud videos should carefully make the most of the appearance and motion structure.
In this paper, we propose a Masked Spatio-Temporal
Structure Prediction (MaST-Pre) method for self-supervised learning on point cloud videos (Fig. 1). MaST-Pre is based on a masking strategy, which has been proven effective in a range of applications. For example, because of the canon-ical structure, images can be easily segmented into multi-ple patches for masking [8, 15], which in the case of video are extended to patch tubes [13, 37]. For unstructured static point clouds, spherical support domain masking can be used for masked autoencoder [29, 47]. However, the spatial ir-regularity and temporal regularity make point cloud videos require a more elaborate masking strategy. Our method is based on a masked point tube mechanism, where a point tube is a local area expanding over a short time [11].
Based on point-tube masking, our MaST-Pre employs two self-supervised tasks to capture the appearance and mo-tion structure, respectively. First, to learn the appearance structure, MaST-Pre is asked to predict the invisible parts of the input from unmasked points. Second, to capture the dynamics in point tubes, we propose the temporal cardi-nality difference, which can be calculated online from in-puts without additional parameters. Cardinality can reflect basic structures (e.g., line, edge, and plane) of static point clouds [21]. In this paper, we extend it to a temporal ver-sion so that it can model the dynamics of point cloud videos.
Intuitively, the temporal cardinality difference characterizes the flow of points within a short time. Therefore, inferring the temporal cardinality difference of masked point tubes facilitates MaST-Pre to learn motion-informative represen-tations. Our contributions are summarized as follows:
• We design a 4D scheme of masked prediction for self-supervised learning on point cloud videos, termed as
MaST-Pre. Our MaST-Pre jointly learns the appear-ance and motion structure of point cloud videos.
• We propose the temporal cardinality difference, a sim-ple and effective motion feature directly captured from raw input points. It explicitly guides MaST-Pre to learn motion-informative representations.
• Extensive experiments and ablation studies on several benchmark datasets validate that our MaST-Pre learns rich representations of point cloud videos. 2.