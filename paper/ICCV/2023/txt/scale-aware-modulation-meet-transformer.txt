Abstract
This paper presents a new vision Transformer, Scale-Aware Modulation Transformer (SMT), that can handle var-ious downstream tasks efficiently by combining the convolu-tional network and vision Transformer. The proposed Scale-Aware Modulation (SAM) in the SMT includes two primary novel designs. Firstly, we introduce the Multi-Head Mixed
Convolution (MHMC) module, which can capture multi-scale features and expand the receptive field. Secondly, we propose the Scale-Aware Aggregation (SAA) module, which is lightweight but effective, enabling information fusion across different heads. By leveraging these two modules, convolutional modulation is further enhanced. Further-more, in contrast to prior works that utilized modulations throughout all stages to build an attention-free network, we propose an Evolutionary Hybrid Network (EHN), which can effectively simulate the shift from capturing local to global dependencies as the network becomes deeper, resulting in superior performance. Extensive experiments demonstrate that SMT significantly outperforms existing state-of-the-art models across a wide range of visual tasks. Specifically,
SMT with 11.5M / 2.4GFLOPs and 32M / 7.7GFLOPs can achieve 82.2% and 84.3% top-1 accuracy on ImageNet-1K, respectively. After pretrained on ImageNet-22K in 2242 res-olution, it attains 87.1% and 88.1% top-1 accuracy when finetuned with resolution 2242 and 3842, respectively. For object detection with Mask R-CNN, the SMT base trained with 1× and 3× schedule outperforms the Swin Trans-former counterpart by 4.2 and 1.3 mAP on COCO, respec-tively. For semantic segmentation with UPerNet, the SMT base test at single- and multi-scale surpasses Swin by 2.0 and 1.1 mIoU respectively on the ADE20K. Our code is available at https://github.com/AFeng-x/SMT. 1.

Introduction
Since the groundbreaking work on Vision Transform-ers (ViT) [9], Transformers have gained significant atten-*Corresponding authors.
Figure 1: Top-1 accuracy on ImageNet-1K of recent SOTA models. Our proposed SMT outperforms all the baselines. tion from both industry and academia, achieving remark-able success in various computer vision tasks, such as im-age classification [8], object detection [27, 10], and seman-tic segmentation [68, 6]. Unlike convolutional networks, which only allow for interactions within a local region us-ing a shared kernel, ViT divides the input image into a sequence of patches and updates token features via self-attention (SA), enabling global interactions. However, self-attention still faces challenges in downstream tasks due to the quadratic complexity in the number of visual tokens, particularly for high-resolution inputs.
To address these challenges, several efficient spatial at-tention techniques have been proposed. For example, Swin
Transformer [29] employs window attention to limit the number of tokens and establish cross-window connections via shifting. PVT [52, 53] and Focal [61] reduce the cost of self-attention by combining token merging with spa-tial reduction. Shunted [38] effectively models objects at multiple scales simultaneously while performing spatial re-duction. Other techniques such as dynamic token selec-tion [34, 36, 62] have also proven to be effective improve-ments.
Rather than directly improving self-attention, several works [7, 24, 33, 23] have investigated hybrid CNN-Transformer architectures that combine efficient convolu-tional blocks with powerful Transformer blocks. We ob-served that most hybrid networks replace shallow Trans-former blocks with convolution blocks to reduce the high computational cost of self-attention in the early stages.
However, these simplistic stacking strategies hinder them from achieving a better balance between accuracy and la-tency. Therefore, one of the objectives of this paper is to present a new perspective on the integration of Transformer and convolution blocks.
Based on the research conducted in [9, 3], which per-formed a quantitative analysis of different depths of self-attention blocks and discovered that shallow blocks tend to capture short-range dependencies while deeper ones cap-ture long-range dependencies, we propose that substitut-ing convolution blocks for Transformer blocks in shallow networks offers a promising strategy for two primary rea-sons: (1) self-attention induces significant computational costs in shallow networks due to high-resolution input, and (2) convolution blocks, which inherently possess a capacity for local modeling, are more proficient at capturing short-range dependencies than SA blocks in shallow networks.
However, we observed that simply applying the convolu-tion directly to the feature map does not lead to the desired performance. Taking inspiration from recent convolutional modulation networks [13, 16, 60], we discovered that con-volutional modulation can aggregate surrounding contexts and adaptively self-modulate, giving it a stronger model-ing capability than using convolution blocks alone. There-fore, we proposed a novel convolutional modulation, termed
Scale-Aware Modulation (SAM), which incorporates two new modules: Multi-Head Mixed Convolution (MHMC) and Scale-Aware Aggregation (SAA). The MHMC mod-ule is designed to enhance the receptive field and capture multi-scale features simultaneously. The SAA module is designed to effectively aggregate features across different heads while maintaining a lightweight architecture. De-spite these improvements, we find that SAM falls short of the self-attention mechanism in capturing long-range de-pendencies. To address this, we propose a new hybrid
Modulation-Transformer architecture called the Evolution-ary Hybrid Network (EHN). Specifically, we incorporate
SAM blocks in the top two stages and Transformer blocks in the last two stages, while introducing a new stacking strat-egy in the penultimate stage. This architecture not only sim-ulates changes in long-range dependencies from shallow to deep layers but also enables each block in each stage to bet-ter match its computational characteristics, leading to im-proved performance on various downstream tasks. Collec-tively, we refer to our proposed architecture as Scale-Aware
Modulation Transformer (SMT).
As shown in Fig. 1, our SMT significantly outperforms other SOTA vision Transformers and convolutional net-It is worth noting that our works on ImageNet-1K [8].
SMT achieves top-1 accuracy of 82.2% and 84.3% with the tiny and base model sizes, respectively. Moreover, our SMT consistently outperforms other SOTA models on
COCO [27] and ADE20K [68] for object detection, instance segmentation, and semantic segmentation tasks.
Overall, the contributions of this paper are as follows.
• We introduce the Scale-Aware Modulation (SAM) which incorporates a potent Multi-Head Mixed Convo-lution (MHMC) and an innovative, lightweight Scale-Aware Aggregation (SAA). The SAM facilitates the integration of multi-scale contexts and enables adap-tive modulation of tokens to achieve more precise pre-dictions.
• We propose a new evolutionary hybrid network that ef-fectively models the transition from capturing local to global dependencies as the network increases in depth, leading to improved performance and high efficiency.
• We evaluated our proposed Scale-Aware Modulation
Transformer (SMT) on several widely used bench-marks, including classification, object detection, and segmentation.
The experimental results indicated that SMT consistently outperformed the SOTA Vision
Transformers while requiring fewer parameters and in-curring lower computational costs. 2.