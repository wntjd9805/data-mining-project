Abstract
Adversarial examples derived from deliberately crafted perturbations on visual inputs can easily harm decision pro-cess of deep neural networks. To prevent potential threats, various adversarial training-based defense methods have grown rapidly and become a de facto standard approach for robustness. Despite recent competitive achievements, we observe that adversarial vulnerability varies across tar-gets and certain vulnerabilities remain prevalent. Intrigu-ingly, such peculiar phenomenon cannot be relieved even with deeper architectures and advanced defense methods.
To address this issue, in this paper, we introduce a causal approach called Adversarial Double Machine Learning (ADML), which allows us to quantify the degree of adver-sarial vulnerability for network predictions and capture the effect of treatments on outcome of interests. ADML can directly estimate causal parameter of adversarial pertur-bations per se and mitigate negative effects that can po-tentially damage robustness, bridging a causal perspective into the adversarial vulnerability. Through extensive exper-iments on various CNN and Transformer architectures, we corroborate that ADML improves adversarial robustness with large margins and relieve the empirical observation. 1.

Introduction
Along with the progressive developments of deep neu-ral networks (DNNs) [17, 10, 2], an aspect of AI safety comes into a prominence in various computer vision re-search [45, 64, 12, 19]. Especially, adversarial exam-ples [50, 15, 30] are known as potential threats on AI sys-tems. With deliberately crafted perturbations on the visual inputs, adversarial examples are hardly distinguishable to human observers, but they easily result in misleading deci-sion process of DNNs. Such adversarial vulnerability pro-vokes weak reliability of inference process of DNNs and
*Equal contribution. † Corresponding author. (a) Network Architectures (b) Adversarial Defense Methods
Figure 1. The comparison of adversarial robustness along target classes with respect to (a) Network Architectures and (b) Adver-sarial Defense Methods on CIFAR-10 [27]. Note that, the distri-bution of adversarial robustness is consistent along both criteria. discourages AI adoption to the safety critical areas [54, 44].
In order to achieve robust and trustworthy DNNs from adversarial perturbation, previous methods [33, 28, 3, 62, 55, 58, 8] have delved into developing various adversarial attack and defense algorithms in the sense of cat-and-mouse game. As a seminal work, Madry et al. [33] have paved the way for obtaining robust network through adversarial train-ing (AT) regarded as an ultimate augmentation training [52] with respect to adversarial examples. Based on its effective-ness, various subsequent works [62, 55, 58, 63, 39, 25] have investigated it to further enhance adversarial robustness.
Although several AT-based defense methods have be-come a de facto standard due to their competitive adver-sarial robustness, we found an intriguing property of the current defense methods. As in Figure 1, we identify that the adversarial robustness for the each target class signifi-cantly varies with a large gap, and this phenomenon equally happens in the course of (a) network architectures and (b)
tween causal inference and adversarial robustness. Then, by minimizing the magnitude of the estimated causal parame-ter, we essentially lessen negative causal effects of adversar-ial vulnerability, and consequently acquire robust network with the aforementioned phenomenon alleviated.
To corroborate the effectiveness of ADML on adversarial robustness, we set extensive experiments with four publicly available datasets [27, 29, 9]. Our experiments include var-ious convolutional neural network architectures (CNNs), as well as Transformer architectures that have drawn great at-tention in both vision and language tasks [53, 10, 66, 65] yet relatively lack of being studied in adversarial research.
Our contributions can be summarized as follows:
• We present an empirical evidence that despite the re-cent advances in AT-based defenses, fundamentally adversarial vulnerability still remains across various architectures and defense algorithms.
• Bridging a causal perspective into adversary, we pro-pose Adversarial Double Machine Learning (ADML), estimating causal parameter in adversarial examples and mitigating its causal effects damaging robustness.
• Through extensive experiments and analyses on vari-ous CNN and Transformer architectures, we corrobo-rate intensive robustness of our proposed method with the phenomenon alleviated. 2.