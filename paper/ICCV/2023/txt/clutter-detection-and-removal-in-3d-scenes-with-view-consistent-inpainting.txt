Abstract
Removing clutter from scenes is essential in many ap-plications, ranging from privacy-concerned content filter-ing to data augmentation. In this work, we present an au-tomatic system that removes clutter from 3D scenes and inpaints with coherent geometry and texture. We pro-pose techniques for its two key components: 3D segmen-tation based on shared properties and 3D inpainting, both of which are important problems. The definition of 3D scene clutter (frequently-moving objects) is not well cap-tured by commonly-studied object categories in computer vision. To tackle the lack of well-defined clutter anno-tations, we group noisy fine-grained labels, leverage vir-tual rendering, and impose an instance-level area-sensitive loss. Once clutter is removed, we inpaint geometry and texture in the resulting holes by merging inpainted RGB-D images. This requires novel voting and pruning strategies that guarantee multi-view consistency across individually inpainted images for mesh reconstruction. Experiments on
ScanNet and Matterport3D dataset show that our method outperforms baselines for clutter segmentation and 3D in-painting, both visually and quantitatively. Project page: https://weify627.github.io/clutter/. 1.

Introduction
With the proliferation of RGB-D cameras, we can imag-ine a day when people can quickly scan their rooms with a phone and upload the reconstructed 3D model to a website for subleasing. It is more desirable to show an attractive, clean room without pillows and kitchenware scattered all around. However, people prefer not to spend hours cleaning them up before scanning. This is when a tool for automatic scene clutter removal would come in handy. Such a scene editing tool can be applied to a wide range of tasks from au-tomatic content and privacy filtering to data augmentation (by adding new objects to cleaned scenes).
In this paper, we investigate how to reconstruct a scene without clutter, starting from a set of indoor RGB-D scans.
This task consists of two steps: clutter segmentation and 3D inpainting, both of which present new challenges.
Figure 1. Clutter detection and removal in 3D Scenes with view-consistent inpainting. Beginning from an input scene (left), our first task is to predict clutter regions. Traditional semantic seg-mentation datasets and methods [9] (top center) were not designed for this problem, and contain missing or incorrect labels (espe-cially for small objects) as well as semantic categories that only sometimes correspond to clutter. In contrast, our method (bottom center) correctly identifies clutter segments, which we then remove and inpaint with coherent texture and geometry (right).
Thanks to large-scale 3D indoor scene datasets [4, 9, 1, 40], data-driven methods have led to remarkable suc-cess in 3D scene segmentation [18, 34, 24]. However, ex-isting annotations are insufficient for clutter segmentation for two reasons. First, ambiguity exists even in the most fine-grained datasets to date (around 550 raw categories for datasets [4, 9], 200 for benchmark [40]). For example, some objects from the category whiteboard are portable and be-long to clutter, while others are installed on the wall and not clutter. Such intra-category ambiguity is widespread: con-sider light, seat, speaker, decoration, etc., all of which exist in both fixed (non-clutter) and mobile (clutter) variations.
Secondly, the quality of annotations is often suboptimal, with some clutter objects being unannotated or mislabeled as non-clutter, as shown in Fig. 1, top center. Considering the mesh resolution and the time required to label a large number of small objects, it is almost impossible to annotate clutter precisely and exhaustively in a large dataset.
The task of 3D inpainting aims to synthesize a visually
and geometrically plausible completion for the missing re-gions of a 3D scene. While some prior work [41, 46, 31] inpaints smaller holes from incomplete scans or occlusions, we focus on larger holes created by object removal, which exacerbates the difficulty of inpainting due to complex ge-ometry and texture. A naive solution is to run 3D hole-filling or mesh reconstruction algorithms on the remaining points. However, these algorithms only take into account local geometry and low-level texture information, whereas in many cases, semantic understanding is required for plau-sible completion. For example, when some clutter covering a table corner is removed, the inpainted scene is expected to recover the corner rather than just filling the hole and leav-ing the corner missing. Directly working on the 3D scene entails dealing with entangled semantics and complex ge-ometry at the same time, which may result in low resolution and many artifacts in the output [12, 21]. We propose to in-stead inpaint the RGB-D sequences and then reconstruct the final mesh. This disentangles the problem into semantically plausible inpainting and geometry filling. The former can be solved more easily with existing advanced 2D inpaint-ing methods and potentially more powerful than methods trained only on limited 2D data.
In this work, we endeavor to address the above chal-lenges and propose an automatic system for scene clutter removal and 3D inpainting. To more effectively segment small clutter objects, we design an area-sensitive loss to force attention to small clutter objects which typically re-ceive limited loss signals. We adopt a deep architecture that takes in both RGB images and 3D points as input and render virtual views as the 2D input to gain large surface coverage.
Different from prior work that directly predicts inpainting in 3D after identifying objects in 3D, we project the ob-ject masks onto RGB-D images and perform image inpaint-ing and image-guided depth completion. To enforce cross-frame consistency for a better quality of 3D reconstruction, we run novel consistency voting and pruning across frames.
We loop back to image inpainting until all missing regions have been filled. The resulting collection of consistently in-painted RGB-D images can be merged into a final, clean scene.
We conduct extensive ablation studies to validate the system designs on the ScanNet [9] and Matterport3D [4] datasets. We also compare with 3D segmentation and in-painting baselines and show the effectiveness of the pro-posed model. In summary, our contributions are:
• An automatic system that solves the novel task of scene clutter removal and inpainting.
• A 3D segmentation method with area-sensitive loss that can better segment small objects even with noisy data.
• A 3D inpainting method with iterative view-consistent
RGB-D inpainting and outperforms prior methods. 2.