Abstract
Given a set of pre-trained models, how can we quickly and accurately find the most useful pre-trained model for a downstream task? Transferability measurement is to quan-tify how transferable is a pre-trained model learned on a
It is used for quickly rank-source task to a target task. ing pre-trained models for a given task and thus becomes a crucial step for transfer learning. Existing methods mea-sure transferability as the discrimination ability of a source model for a target data before transfer learning, which can-not accurately estimate the fine-tuning performance. Some of them restrict the application of transferability measure-ment in selecting the best supervised pre-trained models that have classifiers.
It is important to have a general method for measuring transferability that can be applied in a variety of situations, such as selecting the best self-supervised pre-trained models that do not have classifiers, and selecting the best transferring layer for a target task.
In this work, we propose TMI (TRANSFERABILITY
MEASUREMENT WITH INTRA-CLASS FEATURE VARI-ANCE), a fast and accurate algorithm to measure transfer-ability. We view transferability as the generalization of a pre-trained model on a target task by measuring intra-class feature variance. Intra-class variance evaluates the adapt-ability of the model to a new task, which measures how transferable the model is. Compared to previous studies that estimate how discriminative the models are, intra-class variance is more accurate than those as it does not require an optimal feature extractor and classifier. Extensive exper-iments on real-world datasets show that TMI outperforms competitors for selecting the top-5 best models, and exhibits consistently better correlation in 13 out of 17 cases. 1.

Introduction
Transfer learning is an important concept in the field of machine learning, revolutionizing the way models are
Figure 1: Illustration of selecting the best pre-trained model for a given target task. trained and applied to various domains. Transfer learning has proven to be particularly valuable when labeled data in the target domain are scarce or costly to obtain. Within the broader scope of transfer learning, several variants have emerged, such as source-free domain adaptation [10, 27, 23, 7], multi-source transfer learning [16, 14, 7, 9], heteroge-neous transfer learning [11, 15], and open-set domain adap-tation [6, 21].
In transfer learning, fine-tuning a pre-trained model be-comes a simple and effective way to improve the perfor-mance of a downstream task. A crucial step in transfer learning is to quickly and accurately select the most help-ful pre-trained model in a set of given pre-trained models.
Transferability measurement is to quantify how transferable is a pre-trained model learned on a source task to a target task. As shown in Figure. 1, transferability measurement is used for ranking pre-trained models for a given task, and is a crucial step for transfer learning.
A desired method for transferability measurement should be faster than running a transfer learning algorithm, accurately select the best pre-trained model, and be general to be implemented in various common scenarios. Taskon-omy [26] and Task2Vec [1] are computationally expensive due to their learning with data. NCE [18] and LEEP [13] are free of training but cannot be applied to general cases, such as selecting self-supervised pre-trained models and selecting the best transferring layer. LogMe [25] and H-Score [2] also cannot be applied theoretically to select the best transferring layer. Instead of measuring transferability, they measure how discriminative a pre-trained model is to a target task before implementing transfer learning, which cannot estimate fine-tuning performance. TransRate [5] is simple and general to be applied for transferability measure-ment, but too slow due to the computation of the entropy of whole hidden representations.
In this paper, we propose TMI (TRANSFERABILITY
MEASUREMENT WITH INTRA-CLASS FEATURE VARI-ANCE), a simple but effective method for transferability measurement. We deem transferability as the generalization of a pre-trained model on a target task. To quantify the de-gree of generalization, we measure the intra-class variance by the conditional entropy of target representation given a label. Intra-class variance evaluates the adaptability of the model to a new task, which measures how transferable the model is. Compared to previous studies that estimate how discriminative the models are, intra-class variance is more accurate than those as it does not require an optimal feature extractor and classifier. Large intra-class variance implies the feature is efficient for learning tasks. TMI can be used for selecting not only the best supervised model but also the best self-supervised models and the best transferring layer.
Our contributions are summarized as follows:
• Algorithm. We propose TMI, a fast, accurate, and general method for measuring transferability. We view transferability as generalization ability of a pre-trained model on a target task. The model generalization is measured by intra-class feature variance.
• Experiments. We conduct extensive experiments on seventeen datasets with fifty supervised pre-trained models and eleven self-supervised pre-trained models.
TMI outperforms competitors for selecting the top-5 pre-trained architecture in 14 out of 17 cases and se-lecting the best source data in all cases. TMI also shows the best trade-off of the running time and trans-ferability, outperforming competitors in 12 out of 17 cases (see Figure. 2).
• Case study. We evaluate the transferability in more general cases, such as applying to self-supervised pre-trained models and selecting the best transferring layer.
Compared to competitors, TMI achieves the highest correlation coefficient in 13 out of 17 for selecting the best self-supervised pre-trained models, and in 16 out of 17 datasets for selecting the best transferring layer.
The rest of this paper is organized as follows. We intro-duce related works for transferability measurement in Sec-tion 2, and propose our method TMI in Section 3. After presenting experimental results in Section 4, we conclude in Section 5. 2.