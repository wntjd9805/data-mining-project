Abstract
Zero-shot learning models achieve remarkable results on image classification for samples from classes that were not seen during training. However, such models must be trained from scratch with specialised methods: therefore, access to a training dataset is required when the need for zero-shot classification arises. In this paper, we aim to equip pre-trained models with zero-shot classification capabili-ties without the use of image data. We achieve this with our proposed Image-free Classifier Injection with Semantics (ICIS) that injects classifiers for new, unseen classes into pre-trained classification models in a post-hoc fashion with-out relying on image data. Instead, the existing classifier weights and simple class-wise descriptors, such as class names or attributes, are used. ICIS has two encoder-decoder networks that learn to reconstruct classifier weights from de-scriptors (and vice versa), exploiting (cross-)reconstruction and cosine losses to regularise the decoding process. No-tably, ICIS can be cheaply trained and applied directly on top of pre-trained classification models. Experiments on benchmark ZSL datasets show that ICIS produces un-seen classifier weights that achieve strong (generalised) zero-shot classification performance. Code is available at https://github.com/ExplainableML/ImageFreeZSL. 1.

Introduction
With the immense growth of user-generated data, object categories are routinely discovered or re-defined, and re-quirements for models change constantly. Hence, pre-trained visual classifiers can rapidly turn inadequate as their output space is limited to classes seen during training. Updating a decision maker commonly requires expensive data collection and annotation, which can be unrealistic in some domains.
A cheaper and more appealing strategy is to use a zero-shot learning (ZSL) model [1, 57, 55, 56] that exploits side information (e.g. class label word embeddings) to recognise unseen classes in the specific domain of interest. However,
Figure 1: We propose Image-free Classifier Injection with
Semantics (ICIS), a method that adds new classes to a pre-trained model without access to any image for seen and unseen classes, learning only from class-specific descriptors (e.g. attributes) and their corresponding classifier weights. i) standard classification models are not designed with zero-shot capabilities [19], and ii) state-of-the-art ZSL methods are not applicable post training as one cannot regress descrip-tors from images [28, 1], train a feature generator [56], or learn visual-semantic embeddings [60, 54] without images from seen classes. Crucially, it can be infeasible for a practi-tioner to (re-)train a ZSL model due to computational costs or data storage issues, and some visual data might not be available for a particular task or user.
Since a large variety of pre-trained deep learning models are readily available online, we pose the question: given a specific image classification task and a pre-trained model, can we extend it to desired but missing categories without using images from seen or unseen classes? We name this task Image-free ZSL (I-ZSL), where the aim is to categorise
samples from unseen classes without using image data, by injecting new classification weights into pre-trained classi-fication models in a post-hoc manner. We note that while vision-language models (e.g. CLIP [43]) can perform zero-shot transfer, I-ZSL goes into an orthogonal direction by targeting models that are specialised for specific tasks, such as fine-grained recognition, for which model updates are needed when introducing new categories. In principle, an
I-ZSL method could be combined with CLIP, e.g. by esti-mating class-specific prompts [40] for unseen classes.
We address the I-ZSL task by proposing Image-free Clas-sifier Injection with Semantics (ICIS), a method that directly relates semantic class descriptors (e.g. class names or at-tributes) to the classifier weights of a pre-trained model.
Concretely, ICIS receives as input classifier weights of seen classes and class descriptors. It learns two encoder-decoder networks, one for descriptor vectors and one for the classifier weights. These networks inject priors from the respective data sources into the simple descriptor-to-weights mapping.
Moreover, we supervise the encoder representations by map-ping across spaces (i.e. descriptor-to-weights and vice versa), using pairs of seen class descriptors and their corresponding classifier weights, in order to encourage better generalisation on unseen descriptors. Notably, ICIS is trained and used for classifier injection in a post-hoc manner, i.e. after the initial classification model has been trained and without access to the initial training data. We validate our approach on stan-dard ZSL benchmark (i.e. CUB [49], SUN [41], AWA2 [55]), against applicable ZSL methods and adapted state-of-the-art zero- and few-shot learning approaches, consistently outper-forming them in both generalised and standard evaluation.
To summarise, we make the following contributions: (1) We tackle the new image-free ZSL task, where zero-shot classification is performed by injecting new classifica-tion weights into a pre-trained classification model, without access to any samples this model was trained on. (2) We pro-pose ICIS, a simple framework with two encoder-decoder architectures that directly predicts new classifier weights for unseen classes from limited training data, i.e. classifier weights for the seen classes and corresponding class-wise descriptors. (3) Despite the additional restrictions of the
I-ZSL setting, ICIS achieves strong (Generalised)-I-ZSL per-formance on a variety of ZSL benchmarks, outperforming several zero- and few-shot approaches adapted to I-ZSL. 2.