Abstract
Vision models often fail systematically on groups of data that share common semantic characteristics (e.g., rare objects or unusual scenes), but identifying these failure modes is a challenge. We introduce AdaVi-sion, an interactive process for testing vision mod-els which helps users identify and ﬁx coherent fail-ure modes. Given a natural language description of a coherent group, AdaVision retrieves relevant im-ages from LAION-5B with CLIP. The user then labels a small amount of data for model correctness, which is used in successive retrieval rounds to hill-climb towards high-error regions, reﬁning the group deﬁnition. Once a group is saturated, AdaVision uses GPT-3 to sug-gest new group descriptions for the user to explore. We demonstrate the usefulness and generality of AdaVi-sion in user studies, where users ﬁnd major bugs in state-of-the-art classiﬁcation, object detection, and im-age captioning models. These user-discovered groups have failure rates 2-3x higher than those surfaced by automatic error clustering methods. Finally, ﬁnetun-ing on examples found with AdaVision ﬁxes the dis-covered bugs when evaluated on unseen examples, with-out degrading in-distribution accuracy, and while also improving performance on out-of-distribution datasets. 1.

Introduction
Even when vision models attain high average per-formance, they still fail unexpectedly on subsets of im-ages. When low-performing subsets are semantically coherent (i.e. uniﬁed by a human-understandable con-cept), their identiﬁcation helps developers understand how to intervene on the model (e.g. by targeted data
∗Undertaken in part as an intern at Microsoft Research. collection) and decide if models are safe and fair to de-ploy [12, 25]. For example, segmentation models for autonomous driving fail in unusual weather. Because we have identiﬁed this, we know to deploy such systems with caution and design interventions that simulate di-verse weather conditions [37, 47]. Identifying coherent failure modes helps developers make such deployment decisions and design interventions.
However, discovering coherent error groups is dif-ﬁcult in practice, since most evaluation sets lack the necessary visual or semantic annotations to group er-rors. Prior work clusters evaluation set errors in diﬀer-ent representation spaces [8, 12, 17, 36, 43], but these methods often produce incoherent groups, such that it is hard for humans to assess their impact or ﬁx them. These methods are also limited by the cov-erage of small evaluation sets, which underestimate out-of-distribution vulnerabilities [26, 30, 33], and be-come less useful as models approach near-perfect accu-racy on benchmarks. An alternative approach for dis-covering failures is open-ended human-in-the-loop test-ing [13, 32, 33], which leverages interaction with users to generate challenging data to test models on coherent topics. While successful in NLP, there are no estab-lished frameworks for open-ended testing in vision.
In this work, we present Adaptive Testing for Vi-sion Models (AdaVision), a process and tool for human-in-the-loop testing of computer vision models.
As illustrated in Figure 1 (left), a user ﬁrst proposes a coherent group of images to evaluate using natural lan-guage (e.g. stop sign). This description is used to re-trieve images from a large unlabeled dataset (LAION-5B) using CLIP embeddings [27]. After users label a small number of the returned images for model correct-ness (pass / fail), the tool adapts to retrieve images similar to the discovered failures (Figure 1C). AdaVi-sion reduces the manual labor required for human-in-1
(cid:55)(cid:40)(cid:54)(cid:55)(cid:3)(cid:42)(cid:40)(cid:49)(cid:40)(cid:53)(cid:36)(cid:55)(cid:44)(cid:50)(cid:49)(cid:3)(cid:47)(cid:50)(cid:50)(cid:51)(cid:3)(cid:86)(cid:88)(cid:85)(cid:73)(cid:68)(cid:70)(cid:72)(cid:86)(cid:3)(cid:68)(cid:3)(cid:70)(cid:82)(cid:75)(cid:72)(cid:85)(cid:72)(cid:81)(cid:87)(cid:3)(cid:73)(cid:68)(cid:76)(cid:79)(cid:88)(cid:85)(cid:72)(cid:3)(cid:80)(cid:82)(cid:71)(cid:72)(cid:17) (cid:39)(cid:50)(cid:58)(cid:49)(cid:54)(cid:55)(cid:53)(cid:40)(cid:36)(cid:48)(cid:3)(cid:42)(cid:50)(cid:36)(cid:47)(cid:29)(cid:3)(cid:73)(cid:76)(cid:91)(cid:3)(cid:73)(cid:68)(cid:76)(cid:79)(cid:88)(cid:85)(cid:72)(cid:86)(cid:3)(cid:89)(cid:76)(cid:68)(cid:3)(cid:73)(cid:76)(cid:81)(cid:72)(cid:87)(cid:88)(cid:81)(cid:76)(cid:81)(cid:74) (cid:55)(cid:50)(cid:51)(cid:44)(cid:38)(cid:29)(cid:3)(cid:86)(cid:87)(cid:82)(cid:83)(cid:3)(cid:86)(cid:76)(cid:74)(cid:81) (cid:3)(cid:36) (cid:3)(cid:3)(cid:36)(cid:71)(cid:68)(cid:57)(cid:76)(cid:86)(cid:76)(cid:82)(cid:81)(cid:3)(cid:85)(cid:72)(cid:87)(cid:85)(cid:76)(cid:72)(cid:89)(cid:72)(cid:86)(cid:3)(cid:76)(cid:80)(cid:68)(cid:74)(cid:72)(cid:86)(cid:3) (cid:69)(cid:68)(cid:86)(cid:72)(cid:71)(cid:3)(cid:82)(cid:81)(cid:3)(cid:68)(cid:81)(cid:3)(cid:76)(cid:81)(cid:76)(cid:87)(cid:76)(cid:68)(cid:79)(cid:3)(cid:87)(cid:82)(cid:83)(cid:76)(cid:70)(cid:17) (cid:55)(cid:50)(cid:51)(cid:44)(cid:38)(cid:3)(cid:42)(cid:40)(cid:49)(cid:40)(cid:53)(cid:36)(cid:55)(cid:44)(cid:50)(cid:49)(cid:3)(cid:47)(cid:50)(cid:50)(cid:51)(cid:3)(cid:69)(cid:85)(cid:68)(cid:76)(cid:81)(cid:86)(cid:87)(cid:82)(cid:85)(cid:80)(cid:86)(cid:3)(cid:87)(cid:82)(cid:83)(cid:76)(cid:70)(cid:86)(cid:3)(cid:87)(cid:82)(cid:3)(cid:72)(cid:91)(cid:83)(cid:79)(cid:82)(cid:85)(cid:72)(cid:17) (cid:314)(cid:3)(cid:179)(cid:54)(cid:55)(cid:50)(cid:51)(cid:3)(cid:54)(cid:44)(cid:42)(cid:49)(cid:180) (cid:40)(cid:91)(cid:83)(cid:79)(cid:82)(cid:85)(cid:72)(cid:71)(cid:3)(cid:87)(cid:82)(cid:83)(cid:76)(cid:70)(cid:86) (cid:41)(cid:68)(cid:76)(cid:79)(cid:88)(cid:85)(cid:72)(cid:3)(cid:85)(cid:68)(cid:87)(cid:72) (cid:55)(cid:50)(cid:51)(cid:44)(cid:38)(cid:29)(cid:3)(cid:86)(cid:87)(cid:82)(cid:83)(cid:3)(cid:86)(cid:76)(cid:74)(cid:81)(cid:3)(cid:70)(cid:82)(cid:89)(cid:72)(cid:85)(cid:72)(cid:71)(cid:3)(cid:76)(cid:81)(cid:3)(cid:86)(cid:81)(cid:82)(cid:90)(cid:82)(cid:90) (cid:22)(cid:21)(cid:8) (cid:314)(cid:3)(cid:62)(cid:49)(cid:50)(cid:3)(cid:39)(cid:40)(cid:55)(cid:40)(cid:38)(cid:55)(cid:44)(cid:50)(cid:49)(cid:64) (cid:55)(cid:50)(cid:51)(cid:44)(cid:38)(cid:29)(cid:3)(cid:86)(cid:87)(cid:82)(cid:83)(cid:3)(cid:86)(cid:76)(cid:74)(cid:81)(cid:3)(cid:90)(cid:76)(cid:87)(cid:75)(cid:3)(cid:74)(cid:85)(cid:68)(cid:308)(cid:87)(cid:76) (cid:3)(cid:37) (cid:3)(cid:3)(cid:56)(cid:86)(cid:72)(cid:85)(cid:3)(cid:79)(cid:68)(cid:69)(cid:72)(cid:79)(cid:86)(cid:3)(cid:68)(cid:3)(cid:73)(cid:72)(cid:90)(cid:3) (cid:87)(cid:72)(cid:86)(cid:87)(cid:86)(cid:3)(cid:68)(cid:86)(cid:3)(cid:83)(cid:68)(cid:86)(cid:86)(cid:18)(cid:73)(cid:68)(cid:76)(cid:79)(cid:17) (cid:3)(cid:83)(cid:68)(cid:86)(cid:86) (cid:3)(cid:73)(cid:68)(cid:76)(cid:79) (cid:55)(cid:50)(cid:51)(cid:44)(cid:38)(cid:29)(cid:3)(cid:86)(cid:87)(cid:82)(cid:83)(cid:3)(cid:86)(cid:76)(cid:74)(cid:81) (cid:171) (cid:3)(cid:38) (cid:3)(cid:3)(cid:36)(cid:71)(cid:68)(cid:57)(cid:76)(cid:86)(cid:76)(cid:82)(cid:81)(cid:3)(cid:85)(cid:72)(cid:87)(cid:85)(cid:76)(cid:72)(cid:89)(cid:72)(cid:86)(cid:3)(cid:81)(cid:72)(cid:90)(cid:3) (cid:76)(cid:80)(cid:68)(cid:74)(cid:72)(cid:86)(cid:3)(cid:69)(cid:68)(cid:86)(cid:72)(cid:71)(cid:3)(cid:82)(cid:81)(cid:3)(cid:69)(cid:82)(cid:87)(cid:75)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3) (cid:87)(cid:82)(cid:83)(cid:76)(cid:70)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:83)(cid:85)(cid:72)(cid:89)(cid:76)(cid:82)(cid:88)(cid:86)(cid:3)(cid:73)(cid:68)(cid:76)(cid:79)(cid:88)(cid:85)(cid:72)(cid:86)(cid:17) (cid:3)(cid:73)(cid:68)(cid:76)(cid:79) (cid:3)(cid:3)(cid:408)(cid:3) (cid:3)(cid:3)(cid:47)(cid:82)(cid:82)(cid:83)(cid:3) (cid:85)(cid:72)(cid:83)(cid:72)(cid:68)(cid:87)(cid:86)(cid:17) (cid:39) (cid:3)(cid:3)(cid:56)(cid:86)(cid:72)(cid:85)(cid:3)(cid:70)(cid:82)(cid:81)(cid:87)(cid:76)(cid:81)(cid:88)(cid:72)(cid:86)(cid:3) (cid:87)(cid:82)(cid:3)(cid:80)(cid:68)(cid:85)(cid:78)(cid:3)(cid:73)(cid:68)(cid:76)(cid:79)(cid:86)(cid:17) (cid:314)(cid:3)(cid:62)(cid:49)(cid:50)(cid:3)(cid:39)(cid:40)(cid:55)(cid:40)(cid:38)(cid:55)(cid:44)(cid:50)(cid:49)(cid:64) (cid:314)(cid:3)(cid:62)(cid:49)(cid:50)(cid:3)(cid:39)(cid:40)(cid:55)(cid:40)(cid:38)(cid:55)(cid:44)(cid:50)(cid:49)(cid:64) (cid:171) (cid:3)(cid:40) (cid:3)(cid:3)(cid:55)(cid:82)(cid:83)(cid:76)(cid:70)(cid:3)(cid:70)(cid:68)(cid:81)(cid:3)(cid:69)(cid:72)(cid:3)(cid:85)(cid:72)(cid:305)(cid:81)(cid:72)(cid:71)(cid:3)(cid:87)(cid:82)(cid:3)(cid:85)(cid:72)(cid:306)(cid:72)(cid:70)(cid:87)(cid:3) (cid:87)(cid:75)(cid:72)(cid:87)(cid:75)(cid:72) (cid:86)(cid:88)(cid:86)(cid:88)(cid:85)(cid:73)(cid:68)(cid:85)(cid:73)(cid:68)(cid:70)(cid:72)(cid:71)(cid:70)(cid:72)(cid:71) (cid:73)(cid:68)(cid:73)(cid:68)(cid:76)(cid:79)(cid:88)(cid:76)(cid:79)(cid:88)(cid:85)(cid:72)(cid:85)(cid:72) (cid:80)(cid:82)(cid:71)(cid:80)(cid:82)(cid:71)(cid:72)(cid:17)(cid:72)(cid:17) (cid:87)(cid:75)(cid:72)(cid:3)(cid:86)(cid:88)(cid:85)(cid:73)(cid:68)(cid:70)(cid:72)(cid:71)(cid:3)(cid:73)(cid:68)(cid:76)(cid:79)(cid:88)(cid:85)(cid:72)(cid:3)(cid:80)(cid:82)(cid:71)(cid:72)(cid:17) (cid:55)(cid:50)(cid:51)(cid:44)(cid:38)(cid:29)(cid:3)(cid:86)(cid:87)(cid:82)(cid:83)(cid:3)(cid:86)(cid:76)(cid:74)(cid:81)(cid:3)(cid:314)(cid:3)(cid:86)(cid:87)(cid:82)(cid:83)(cid:3)(cid:86)(cid:76)(cid:74)(cid:81)(cid:3)(cid:70)(cid:82)(cid:89)(cid:72)(cid:85)(cid:72)(cid:71)(cid:3)(cid:76)(cid:81)(cid:3)(cid:86)(cid:81)(cid:82)(cid:90) (cid:20)(cid:19)(cid:8) (cid:22)(cid:8) (cid:3)(cid:41) (cid:3)(cid:3)(cid:3)(cid:36)(cid:71)(cid:68)(cid:57)(cid:76)(cid:86)(cid:76)(cid:82)(cid:81)(cid:3) (cid:70)(cid:82)(cid:81)(cid:71)(cid:76)(cid:87)(cid:76)(cid:82)(cid:81)(cid:86)(cid:3)(cid:82)(cid:81)(cid:3)(cid:87)(cid:82)(cid:83)(cid:76)(cid:70)(cid:86)(cid:3) (cid:90)(cid:76)(cid:87)(cid:75)(cid:3)(cid:75)(cid:76)(cid:74)(cid:75)(cid:3)(cid:73)(cid:68)(cid:76)(cid:79)(cid:88)(cid:85)(cid:72)(cid:3)(cid:85)(cid:68)(cid:87)(cid:72)(cid:86)(cid:3) (cid:87)(cid:82)(cid:3)(cid:86)(cid:88)(cid:74)(cid:74)(cid:72)(cid:86)(cid:87)(cid:3)(cid:87)(cid:82)(cid:83)(cid:76)(cid:70)(cid:86)(cid:17) (cid:55)(cid:50)(cid:51)(cid:44)(cid:38)(cid:29)(cid:3)(cid:45)(cid:68)(cid:83)(cid:68)(cid:81)(cid:72)(cid:86)(cid:72)(cid:3)(cid:86)(cid:87)(cid:82)(cid:83)(cid:3)(cid:86)(cid:76)(cid:74)(cid:81) (cid:49)(cid:72)(cid:90)(cid:3)(cid:86)(cid:88)(cid:74)(cid:74)(cid:72)(cid:86)(cid:87)(cid:72)(cid:71)(cid:3)(cid:87)(cid:82)(cid:83)(cid:76)(cid:70)(cid:86) (cid:343)(cid:54)(cid:87)(cid:82)(cid:83)(cid:3)(cid:86)(cid:76)(cid:74)(cid:81)(cid:3)(cid:76)(cid:81)(cid:3)(cid:83)(cid:82)(cid:88)(cid:85)(cid:76)(cid:81)(cid:74)(cid:3)(cid:85)(cid:68)(cid:76)(cid:81)(cid:344) (cid:343)(cid:54)(cid:87)(cid:82)(cid:83)(cid:3)(cid:86)(cid:76)(cid:74)(cid:81)(cid:3)(cid:76)(cid:81)(cid:3)(cid:73)(cid:82)(cid:74)(cid:344) (cid:343)(cid:54)(cid:87)(cid:82)(cid:83)(cid:3)(cid:86)(cid:76)(cid:74)(cid:81)(cid:3)(cid:90)(cid:76)(cid:87)(cid:75)(cid:3)(cid:74)(cid:85)(cid:68)(cid:308)(cid:87)(cid:76)(cid:3)(cid:86)(cid:81)(cid:82)(cid:90)(cid:80)(cid:68)(cid:81)(cid:344) (cid:3)(cid:3)(cid:337) (cid:3)(cid:42) (cid:3)(cid:3)(cid:3)(cid:56)(cid:86)(cid:72)(cid:85)(cid:3)(cid:86)(cid:72)(cid:79)(cid:72)(cid:70)(cid:87)(cid:86)(cid:3)(cid:80)(cid:82)(cid:86)(cid:87)(cid:3) (cid:83)(cid:85)(cid:82)(cid:80)(cid:76)(cid:86)(cid:76)(cid:81)(cid:74)(cid:3)(cid:87)(cid:82)(cid:83)(cid:76)(cid:70)(cid:86)(cid:3)(cid:87)(cid:82)(cid:3) (cid:72)(cid:91)(cid:83)(cid:79)(cid:82)(cid:85)(cid:72)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:72)(cid:81)(cid:87)(cid:72)(cid:85)(cid:86)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3) (cid:87)(cid:72)(cid:86)(cid:87)(cid:3)(cid:74)(cid:72)(cid:81)(cid:72)(cid:85)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:79)(cid:82)(cid:82)(cid:83)(cid:17)
Figure 1: AdaVision is a human-in-the-loop tool for surfacing coherent groups of failures, which are indexed via natural language topics. In the test generation loop (left), AdaVision generates challenging tests for a topic, hill-climbing on previous failures. In the topic generation loop (right), AdaVision generates new topics to explore, hill-climbing on previously diﬃcult topics. Users steer testing by labeling a small number of images in the test generation loop and selecting which topics to explore from the topic generation loop. the-loop testing by automatically hill-climbing towards high-error regions, while having a human-in-the-loop ensures groups are coherent and meaningful for the downstream application. AdaVision also leverages a large language model (GPT-3 [5]) to adaptively help users generate descriptions for challenging groups to explore, as previously proposed by [32] and illustrated in Figure 1 (right). After testing, users ﬁnetune their models on discovered groups to ﬁx the bugs, and they can test again to verify improvement.
We demonstrate the usefulness of AdaVision in user studies, where users found a variety of bugs in state-of-the-art classiﬁcation, object detection, and im-age captioning models. AdaVision groups had failure rates 2-3x higher than those found by Domino, an au-tomatic error clustering baseline [12]. Further, users found close to 2x as many failures with AdaVision, when compared to a strong non-adaptive baseline using the same CLIP backend. Finally, we show that ﬁnetun-ing a large classiﬁcation model on failures found with
AdaVision improves performance on held-out exam-ples of such groups and on out-of-distribution datasets, without degrading in-distribution performance: ﬁne-tuning an ImageNet-pretrained ViT-H/14 model [6, 10] on user study data ﬁxes the discovered groups (boost-ing accuracy from 72.6% to 91.2%) without reducing overall accuracy on ImageNet, while also improving the accuracy of labeled classes (78.0% to 84.0%) on ﬁve out-of-distribution (OOD) ImageNet evaluation sets. 2.