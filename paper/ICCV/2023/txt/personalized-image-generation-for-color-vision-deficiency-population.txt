Abstract
Approximately, 350 million people, a proportion of 8%, suffer from color vision deficiency (CVD). While image generation algorithms have been highly successful in syn-thesizing high-quality images, CVD populations are unin-tentionally excluded from target users and have difficul-ties understanding the generated images as normal view-ers do. Although a straightforward baseline can be formed by combining generation models and recolor compensa-tion methods as the post-processing, the CVD friendliness of the result images is still limited since the input image content of recolor methods is not CVD-oriented and will be fixed during the recolor compensation process. Be-sides, the CVD populations can not be fully served since the varying degrees of CVD are often neglected in recol-oring methods. Instead, we propose a personalized CVD-friendly image generation algorithm with two key charac-teristics: (i) generating CVD-oriented images aligned with the needs of CVD populations; (ii) generating continu-ous personalized images for people with various CVD de-grees through disentangling the color representation based on a triple-latent structure. Quantitative and qualita-tive experiments indicate our proposed image generation model can generate practical and compelling results com-pared to the normal generation model and combination baselines on several datasets. The code is available at: https://github.com/Jiangshuyi0V0/CVD-GAN.git 1.

Introduction
In the image generation area, many outstanding gener-ative models, such as variational auto-encoder (VAE) [37, 44], generative adversarial network (GAN) [7, 8, 33, 17, 16], and diffusion models [34, 11], are proposed for high-quality images generation. However, all the generative algorithms are only centric to normal viewers, aiming to facilitate the distribution of generated images close to the dataset established under normal viewersâ€™ perspective.
The needs of underrepresented populations, like color-impairment populations, are often neglected in image gen-Figure 1. Compared to the combination baseline (a), the pro-posed CVD-GAN (b) can generate CVD-oriented images directly, enhancing the friendliness of the image for CVD populations. In addition, the model can generate personalized friendly images for
CVD populations with varying degrees by disentangling the color representation based on the triple-latent structure. eration tasks, causing perception deviation of the gener-ated images. Currently, 350 million people, a proportion of 8% [22], suffer from color vision deficiency (CVD) re-sulting from the abnormality of cone cells distributed on the retina of the eyes. Nevertheless, this sizeable population is unintentionally excluded as the target audience of image generation, necessitating the development of an image gen-eration model that is inclusive of all viewers.
So far, hardly any generation algorithm has offered to serve CVD populations. Some recoloring algorithms [12, 30, 31, 51, 49, 10, 19, 38, 5] can partly alleviate the problems by post-processing compensation based on the
CVD simulation [3, 32] that provides the perspective of
CVD populations of the given image. There are two main goals in recoloring methods: restoring the decayed con-trast [26, 29, 12, 30, 31, 51] and maintaining the natural-ness [49, 51, 10, 19, 38, 5] of a given image. The pro-cess of recoloring can be summarized as providing CVD-unfriendly images as input, conducting color compensa-tion or transformation, and outputting recolored images for
CVD populations. As a result, a straightforward baseline for CVD-friendly generation can be formed by combin-ing generation models and recolor methods as the post-processing as Fig. 1 (a). However, this baseline still has many gaps in CVD-oriented and personalized generation.
The combination baseline is non-CVD-oriented, poten-tially restricting the user-friendliness of recolored images, where the generated content remains unchanged as recol-oring methods solely concentrate on color transformation.
Consequently, this approach imposes a likely upper limit on the user-friendliness of the recolored images. Further-more, despite the fact that CVD populations exhibit diverse requirements based on varying color impairment sever-ity [50, 49], only a few recolor algorithms have addressed the issue of CVD diversity [51] thus far.
To address the above gaps, we propose a CVD-oriented personalized image generation framework based on the ad-versarial network structure [8], as Fig. 1 (b). To generate
CVD-aligned images, a framework that allows for unbiased perception among normal viewers and those with CVD is implemented. Further, in order to account for varying de-grees of CVD, the color representation will be decoupled and controlled by a novel triple-latent structure, enabling the model to yield images with specified color distributions in accordance with the severity of the color impairment.
Particularly, a differential CVD simulator [12] posterior to the generated image, where CVD loss functions will be proposed and used to constrain the generated images and their corresponding simulation to achieve the CVD-oriented generation. Additionally, to reach the goal of personalized generation, triple-latent inputs will be established, where two latent codes serve as contrastive supervision and the other one controls the color pattern generation. Conse-quently, continuous CVD-friendly images towards various severity will be obtained through latent traversal.
Our proposed method evaluates the friendliness of gen-erated images based on contrast decay, color information, and high-level perception across various types and degrees of CVD. Results indicate that our method outperforms ex-isting image generation models and combination baselines on multiple datasets [35, 36, 38].
Our main contributions can be summarized as follows: (i) proposing an end-to-end CVD-oriented image genera-tion framework, (ii) proposing a novel triple-latent struc-ture to disentangle and control the color representation, en-abling the model to generate continuous personalized CVD-friendly images aligned with all degrees of CVD popula-tions. (iii) Extensive experiments on datasets [35, 36, 38] show that CVD-GAN can generate CVD-friendly images for CVD populations with varying types and severity. 2.