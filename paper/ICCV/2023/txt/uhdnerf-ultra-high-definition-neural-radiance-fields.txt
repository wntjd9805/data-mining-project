Abstract
We propose UHDNeRF, a new framework for novel view synthesis on the challenging ultra-high-resolution (e.g., 4K) real-world scenes. Previous NeRF methods are not speciﬁ-cally designed for rendering on extremely high resolutions, leading to burry results with notable detail-losing problems even though trained on 4K images. This is mainly due to the mismatch between the high-resolution inputs and the low-dimensional volumetric representation. To address this is-sue, we introduce an adaptive implicit-explicit scene repre-sentation with which an explicit sparse point cloud is used to boost the performance of an implicit volume on model-ing subtle details. Speciﬁcally, we reconstruct the complex real-world scene with a frequency separation strategy that the implicit volume learns to represent the low-frequency properties of the whole scene, and the sparse point cloud is used for reproducing high-frequency details. To better ex-plore the information embedded in the point cloud, we ex-tract a global structure feature and a local point-wise fea-ture from the point cloud for each sample located in the high-frequency regions. Furthermore, a patch-based sam-pling strategy is introduced to reduce the computational cost. The high-ﬁdelity rendering results demonstrate the su-periority of our method for retaining high-frequency details at 4K ultra-high-resolution scenarios against state-of-the-art NeRF-based solutions. 1.

Introduction
Novel view synthesis, which aims to generate images at new views given a sparse set of observed images, is a long-standing problem in computer graphics and vision. Very recently, Neural Radiance Fields (NeRF) [28] have demon-strated great success in this task for learning to represent 3D scenes with implicit volumetric representation. Several following works [29, 10, 3, 18, 23, 54, 33] then improve this method in different aspects. However, previous NeRF-based methods are typically designed for training images up
†Corresponding authors. (a) Ours (b) Closeups (c) Instant NGP (d) GT
Figure 1. Visualization of our result with resolution 4032 × 3024 (a). We zoom in on the ﬁne-grained details to compare our method with a modern NeRF-based method, i.e., Instant NGP [29] (b)-(d). to 1K resolution. With the development of modern display devices, ultra-high-resolution, e.g., 4K high-deﬁnition for-mat, has become a standard for recording/displaying images and videos. Unfortunately, even though trained on 4K im-ages, these methods cannot reproduce ﬁne-grained details, leading to burry results with severe detail-losing problems, as shown in Fig. 1 (c).
The main difﬁculty preventing a NeRF model to gener-ate ultra-high-resolution images is the mismatch between the high-resolution inputs and the low-dimensional volu-metric representation (determined by the density of sam-ples). NeRF needs to increase the sampled locations over the whole scene to produce higher-resolution renderings.
For instance, to generate a high-quality result of resolution 4032 × 3024, more than 12 million pixels need to be ren-dered. Besides, thousands of MLP evaluations should also be conducted for each camera ray, which requires several hours to render a single image. Obviously, achieving 4K renderings by simply increasing the number of samples is impractical, considering the intolerable rendering time.
To reduce the long training and inference time, some fast rendering methods are proposed [10, 15, 52]. These meth-ods commonly adopt a hybrid scene representation by uti-lizing explicit data structures, such as voxel grids [8, 24] or point clouds [50], to cache the scene properties and synthe-size novel views by the fast query. However, reconstructing dense voxels/points to store an extremely high-resolution 3D scene requires a huge memory cost, which is also unac-ceptable. In short, it is not trivial to convert a NeRF model into its ultra-high-deﬁnition version by either dense sam-(cid:2272)(cid:3034),(cid:3028) (cid:1876)(cid:3028) (cid:2272)(cid:3039),(cid:3028)
High-frequency branch (cid:2026)(cid:3028) (cid:1855)(cid:3028) (cid:2282)(cid:3028) (cid:2282)(cid:3029) (cid:2282)(cid:3028) (cid:3405) (cid:1486) (cid:2282)(cid:3051) = (cid:1486)? (cid:2282)(cid:3029) = (cid:1486)
Low-frequency branch (cid:1876)(cid:3029) (cid:2026)(cid:3029) (cid:1855)(cid:3029) (a) Patch-based ray sampling (b) Adaptive implicit-explicit scene representation (c) Re-rendered  result
Figure 2. Overview of UHDNeRF. We sample locations over the implicit-explicit scene representation with a patch-based ray sampling strategy (a). A sample is adaptively fed into different branches according to the surrounding scene composition (whether there exist point clouds or not) (b). We leverage volume rendering techniques to integrate these samples into patches (c). pling or utilizing a dense data cache.
In this paper, we introduce UHDNeRF, a novel NeRF-based framework that supports ultra-high-resolution view synthesis, realizing high-frequency reproduction as shown in Fig. 1 (a) and (b). We achieve this with an adaptive implicit-explicit scene representation by combining the im-plicit neural radiance ﬁelds with a sparse point cloud ini-tialized from COLMAP [36]. Unlike previous methods utilizing explicit data structure to cover the whole scene, we adjust the point cloud by placing more points at high-frequency regions while less at low-frequency areas. The use of the sparse point cloud signiﬁcantly reduces the mem-ory overhead. Furthermore, as the generation of the sparse point cloud is independent of the NeRF volume, the implicit and explicit representations in our framework are comple-mentary. On the one hand, the implicit volume learns to represent the low-frequency properties of the whole scene.
On the other hand, the sparse point cloud is treated as a series of anchor points indicating ﬁne-grained details. With the high-frequency information provided by the point cloud, we boost the representational ability of a NeRF model in preserving subtle details during ultra-high-resolution ren-dering.
As shown in Fig. 2, our UHDNeRF consists of two branches based on a frequency separation strategy. We adaptively feed a sample into one of the branches according to the composition of the surrounding scenes. That is, given any sampled location, if it lies in regions without a point cloud (the green sample), the low-frequency branch is se-lected, and our UHDNeRF goes back to the purely implicit volumetric representation. Otherwise, the sample is fed into the high-frequency branch (the red sample), where we addi-tionally consider the surrounding point cloud to regress the scene properties at that location. To better explore the high-frequency information embedded in the sparse point cloud, we generate a global structure feature and a local point-wise feature for each sample in this branch. Moreover, to reduce the computational cost, we introduce a patch-based ray sampling strategy which notably reduces the number of investigated points in a batch. Combining the implicit vol-ume and the sparse point cloud lead to ultra-high-deﬁnition renderings with rich details.
To summarize, our main contributions are:
• an ultra-high-deﬁnition NeRF-based framework to achieve high-ﬁdelity 4K rendering results with an adaptive implicit-explicit scene representation,
• a frequency separation strategy by using a two-branch conﬁguration to signiﬁcantly reduce memory and com-putational costs during extremely high-resolution ren-dering, and
• a global structure feature, a local point-wise feature, and a patch-based ray sampling strategy to efﬁciently explore the high-frequency information embedded in the sparse point cloud. 2.