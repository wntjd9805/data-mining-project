Abstract this motion-blurred output IB as an average of N successive latent sharp frames IG i :
Image degradation often occurs during fast camera or object movements, regardless of the exposure modes: global shutter (GS) or rolling shutter (RS). Since these two exposure modes give rise to intrinsically different degradations, two restoration threads have been explored separately, i.e. mo-tion deblurring of GS images and distortion correction of RS images, both of which are challenging restoration tasks, es-pecially in the presence of a single input image. In this paper, we explore a novel in-between exposure mode, called global reset release (GRR) shutter, which produces GS-like blur but with row-dependent blur magnitude. We take advantage of this unique characteristic of GRR to explore the latent frames within a single image and restore a clear counterpart by only relying on these latent contexts. Specifically, we propose a residual spatially-compensated and spectrally-enhanced
Transformer (RSS-T) block for row-dependent deblurring of a single GRR image. Its hierarchical positional encoding compensates global positional context of windows and en-ables order-awareness of the local pixel’s position, along with a novel feed-forward network that simultaneously uses spatial and spectral information for gaining mixed global context. Extensive experimental results demonstrate that our method outperforms the state-of-the-art GS deblurring and
RS correction methods on single GRR input. 1.

Introduction
As a fundamental research task of computer vision, im-age restoration has been explored for many years, aiming to recover a high-quality clean image from its corrupted coun-terpart by removing undesired degradation. Fast or even extreme motion, which is a major inducement to image de-generation and highly correlated to the camera’s exposure mode: global shutter (GS) and rolling shutter (RS).
In a GS image sensor, all pixels are reset simultaneously and immediately charged with exposure, then stored for sequential readout (Figure 1a). In the presence of fast and sudden motion, blurring effects will happen. [25, 39] depict
†Corresponding author
IB = 1
N
N −1 (cid:88) i=0
IG i , i = 0, 1, · · · , N − 1 .
In contrast, RS cameras expose image pixels row by row (including reset and readout), leading to an invariant time delay between consecutive scanlines (Figure 1b). Therefore,
RS distortion effects, also known as jello effects, will appear if the camera is moving during the image acquisition, which is formulated in [23] as [IR]i = [IG i ]i is an operator that extracts the ith row from the ith latent GS frame IG i and
N is also the height of RS image. i ]i. [IG
Although image deblurring itself is highly involved, it is believed that restoring clear images from single degraded captures of GS cameras is much more tractable than that of RS cameras as mentioned in [23]. Intuitively, motion-blur removal process aims to disentangle target GS frame from redundant input without sophisticated displacement estimation or speculation of unknown context. On the con-trary, RS correction needs to shift pixels of each scanline to one virtual GS canvas by building pixel-level correspon-dences. Moreover, the rectified output usually suffers from missing boundaries not directly captured during RS expo-sure. Methodologically, the coarse-to-fine network design has gained remarkable performance for single image deblur-ring [25, 30, 33, 45, 10, 46]. On the contrary, due to the ill-posed nature, single image RS correction relies heavily on strong prior assumptions, explicitly [35, 17, 32, 31] or implicitly [34, 58], which limits their applicability to real scenarios. So, recent research has moved onto multi-image
RS correction to bypass the ill-posedness. But both classi-cal [57, 1] and learning-based methods [23, 9] still cannot work well under complex dynamic scenes or drastic camera motions because of nontrivial pixel alignment.
It is largely overlooked in computer vision that there ex-ists an in-between shutter mode, called global reset release (GRR) shutter, which can be easily enabled in some image sensors that originally work in GS mode (e.g., EV76C560 in EO-1312C Camera) or in RS mode (e.g., IMX178 in
BFS-U3-63S4C camera). As shown in Figure 1c, GRR
Figure 1: Different exposure modes and corresponding degrading processes induced by fast motions. (a) GS exposes all pixels simultaneously, and it causes the output with row-independent blur that can be interpreted as an average of the latent sharp frames. (b) RS captures pixels scanline by scanline, featured in higher frame rate, and lower cost than GS. However, RS sensors are prone to row-dependent distortion, which can be more challenging to correct than deblurring. (c) GRR begins exposure of all pixels simultaneously but ends row by row, causing blurring effects with row-dependent blur magnitude. A specialized deblurring task is required but without the need for RS distortion correction. GRR also has the advantage of reducing the flickering effect of artificial illuminants, such as green traffic lights. starts to expose all pixels simultaneously as GS does but reads out signals scanline by scanline like conventional RS does. Apparently, GRR leads to blurring effects. However, different from GS blur, the blur magnitude of GRR (the brightness level and Signal-Noise-Ratio as a consequence) is row-dependent. Very recently, Wang et al. [47] proposed to enable this global reset feature in RS sensors that turns
RS correction into a more tractable deblurring problem and reported superior results of video deblurring in GRR mode.
We note the cost to pay for this performance gain by switch-ing from RS to GRR mode includes 1) the superiority of
RS mode over GS mode in capture speed is sacrificed since
GRR does not allow early scanning of the next frame before the end of the current frame; 2) scanning rows in the bottom are more likely to be saturated, and the standard automatic exposure mechanism should be adjusted accordingly. There-fore, in the presence of multiple consecutive frames (e.g. 8 frames are used in [47]), the authentic benefits of using
GRR mode are slightly questionable.
In this paper, we deal with a more challenging scenario with a single input image, which is practically meaningful in avoiding well-known flaws of using consecutive frames, including 1) Larger buffer capacity is needed; 2) Sensitivity to video recording settings, like frame rate and deadtime between frames, leading to additional generalization issues; 3) Temporal alignment with severe degradation is still chal-lenging.
By fully considering its unique exposure characteristic of
GRR mode, we bring single GRR image deblurring into the community. Mathematically, row-dependent blur and bright-ness caused by varying exposure time of different scanlines could be formulated as:
IRG = IG 0 + δ
N −1 (cid:88)
[IG i ]i: , i=1 where [IG i ]i: is an operator that extracts image patch of ith row to the end from the ith latent GS frame IG i and δ de-notes the ratio between readout time and the first scanline’s exposure duration. As a result, directly applying existing GS deblurring algorithms on GRR deblurring may give rise to an adaptation issue. On the other hand, Transformers have shown significant performance gains on image restoration task [22, 50] by mitigating the shortcomings of CNNs (i.e., limited receptive field and content-independent kernel). Al-though shifted widow strategy [22, 50, 46, 8, 24] has largely reduced computational loads and made it possible to process high dimensional input, the introduced severe corruption to image spatial information has not been addressed, especially for spatially-sensitive tasks (e.g., GRR deblurring). Besides,
according to the spectral convolution theorem [14], updat-ing a single value in the spectral domain globally affects all original data, which has been implemented for non-local receptive field and proven to be effective in capturing long-range context from frequency domain [4, 5, 55].
We aim to leverage the capability of self-attention and
U-net [36] structure with multi-scale input/output to tackle the single GRR deblurring task. To this end, we propose shifted horizontal window-based Transformer block with spatial compensation and spectral enhancement, which con-sists of three main components: 1) Shifted Window-based
Multi-head Self-Attention(SW-MSA) with horizontal par-tition strategy and hierarchical positional encoding. We update original squared window as rectangular one to better adapt row-dependent blur and brightness. Meanwhile, hier-archical positional encoding compensates global positional context of windows lost in window partition and enables order-awareness of local pixel’s position within each win-dow. 2) Spectrally-enhanced Feed-Forward Network (Se-FFN) based on depth-wise convolutional layer. The spectral branch captures the discrepancy between blurry and sharp image pairs from the perspective of frequency domain, as a complementary part of self-attention. 3) Cross-scale Feature
Fusion module based on Squeeze and Excitation block (CFF-SE) [11], which enables us to actively emphasize or suppress the features from encoders with different scales. The main contributions of this paper are summarized as follows:
• We propose an original Transformer block with spa-tial compensation and spectral enhancement to address single GRR deblurring by fully exploring the latent frames, which further validates the advantages of GRR exposure mode over its RS counterpart.
• Horizontal partition strategy and hierarchical positional encoding are used to compensate global positional con-text of windows and enable order-awareness of local pixel’s position within each window. The spectrally-enhanced feed-forward network simultaneously uses spatial and spectral information for gaining mixed global context.
• To facilitate the development and evaluation of GRR deblurring, we take paired GRR/Sharp images to offer a new dataset captured under real scenes named GRR-real. Furthermore, by simulating data captured by three shutter modes in a comparable way, the benefits of using GRR for single image restoration are verified. 2.