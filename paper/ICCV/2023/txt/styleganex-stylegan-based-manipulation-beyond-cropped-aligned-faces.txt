Abstract 1.

Introduction
Recent advances in face manipulation using StyleGAN have produced impressive results. However, StyleGAN is inherently limited to cropped aligned faces at a fixed image resolution it is pre-trained on. In this paper, we propose a simple and effective solution to this limitation by using di-lated convolutions to rescale the receptive fields of shallow layers in StyleGAN, without altering any model parameters.
This allows fixed-size small features at shallow layers to be extended into larger ones that can accommodate variable resolutions, making them more robust in characterizing u-naligned faces. To enable real face inversion and manipu-lation, we introduce a corresponding encoder that provides the first-layer feature of the extended StyleGAN in addition to the latent style code. We validate the effectiveness of our method using unaligned face inputs of various resolutions in a diverse set of face manipulation tasks, including fa-cial attribute editing, super-resolution, sketch/mask-to-face translation, and face toonification. Project page https:
//www.mmlab-ntu.com/project/styleganex
StyleGAN [17, 18] has emerged as one of the most suc-cessful models for generating high-quality faces. Building upon StyleGAN, researchers have developed a range of face manipulation models [1, 23, 29, 26, 10, 28, 40, 36]. These models typically map real face images or other face-related inputs to the latent space of StyleGAN, perform semantic editing in the latent space, and then map the edited latent code back to the image space. This approach enables a vari-ety of tasks, including facial attribute editing, face restora-tion, sketch-to-face translation, and face toonification. As the manipulated faces remain within the generative space of
StyleGAN, the quality of the image output is guaranteed.
Despite its ability to ensure high-quality image output, the generative space of StyleGAN is limited by a fixed-crop constraint that restricts image resolution and face layout. As a result, existing face manipulation models based on Style-GAN can only handle cropped and aligned face images. In such images with a limited field of view (FoV), the face typically dominates the image, leaving little room for back-ground and clothing, and often resulting in partially cropped hair. However, in everyday portrait photos such as selfies, faces occupy a smaller proportion of the image, allowing for a complete hairstyle and upper body seen. Portrait videos such as those from live streaming require an even larger background area to accommodate face movement. To pro-cess these types of inputs, which we refer to as normal FoV face images and videos, existing manipulation models need to align, crop, and edit the face before pasting the result back onto the original image [4, 19, 31]. This approach of-ten results in discontinuity near the seams, e.g., only editing the hair color inside the cropped area.
While StyleGAN3 [16] was introduced to address un-aligned faces, a recent study [4] found that even StyleGAN3 requires face realignment before effectively projecting to it-s latent space. Moreover, StyleGAN3 is still constrained by a fixed image resolution. Motivated by the translation equivariance of convolutions, VToonify [37] addresses the fixed-crop limitation of StyleGAN by removing its shallow layers to accept input features of any resolution. Howev-er, these shallow layers are crucial for capturing high-level features of the face, such as pose, hairstyle, and face shape.
By removing these layers, the network loses its ability to perform latent editing on these important features, which is a distinctive capability of StyleGAN. Therefore, the chal-lenge remains in overcoming the fixed-crop limitation of
StyleGAN while preserving its original style manipulation abilities, which is a valuable research problem to solve.
In this paper, we propose a simple yet effective approach for refactoring StyleGAN to overcome the fixed-crop limi-tation. In particular, we refactor its shallow layers instead of removing them, allowing the first layer to accept input fea-tures of any resolution. This simple change expands Style-GAN’s style latent space into a more powerful joint style la-tent and first-layer feature space (W +–F space), extending the generative space beyond cropped aligned faces. Fur-thermore, our refactoring only changes the receptive field of shallow-layer convolutions, leaving all pre-trained model parameters intact. Hence, the refactored StyleGAN (Style-GANEX) can directly load the original StyleGAN param-eters, fully compatible with the generative space of Style-GAN, and retains its style representation and editing abil-ity. This means that the StyleGAN editing vectors found in previous studies [26, 10, 28] can be directly applied to
StyleGANEX for normal FoV face editing, e.g., changing the face pose, as shown in Fig. 1(a).
Based on StyleGANEX, we further design a correspond-ing encoder that projects normal FoV face images to the
W +–F space for real face inversion and manipulation. Our encoder builds upon pSp encoder [23] and aggregates it-s multi-layer features to predict the first-layer feature of
StyleGANEX. The encoder and StyleGANEX form a fully convolutional encoder-decoder framework. With the first-layer feature as the bottleneck layer, whose resolution is 1/32 of the output image, our framework can handle im-ages and videos of various resolutions, as long as their side lengths are divisible by 32. Depending on the input and out-put types, our framework can perform a wide range of face manipulation tasks. In this paper, we select several repre-sentative tasks, as shown in Fig. 1, including facial attribute editing, face super-resolution, sketch/mask-to-face transla-tion and video face toonification. While the focus of these tasks in the past is limited to cropped aligned faces, our framework can handle normal FoV faces, showing signifi-cant advantages over previous StyleGAN-based approach-es. To summarize, our main contributions are:
• A novel StyleGANEX architecture with extended
W +–F space , which overcomes the fixed-crop lim-itation of StyleGAN.
• An effective encoder that is able to project normal FoV face images into the W +–F domain.
• A generic and versatile fully convolutional framework for face manipulation beyond cropped aligned faces. 2.