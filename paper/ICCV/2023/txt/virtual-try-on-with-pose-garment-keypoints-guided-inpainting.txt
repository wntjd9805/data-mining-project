Abstract
Virtual try-on is an important technology supporting on-line apparel shopping, which provides consumers with a vir-tual experience to fit garments without physically wearing them. Recently, the image-based virtual try-on has received growing research attention. However, the synthetic results of existing virtual try-on methods usually present distor-tions in garment shape and lose pattern details. In this pa-per, we propose a pose-garment keypoints guided inpaint-ing method for the image-based virtual try-on task, which produces high-fidelity try-on images and well preserves the shapes and patterns of the garments. In our method, hu-man pose and garment keypoints are extracted from source images and constructed as graphs to predict the garment keypoints at the target pose. After which, the predicted key-points are used as guide information to predict the target segmentation map and warp the garment image. The try-on image is finally generated with a semantic-conditioned inpainting scheme using the segmentation map and recom-posed person image as conditions. To verify the effective-ness of our proposed method, we conduct extensive exper-iments on the VITON-HD dataset under both paired and unpaired experimental settings. The qualitative and quan-titative results show that our method significantly outper-forms prior methods at different image resolutions. The codes repository link is https://github.com/lizhi-ntu/KGI. 1.

Introduction
Online shopping has exploded rapidly in the past decade because of its convenience and high cost-effectiveness.
Buying clothes and other daily necessities without leaving home is gradually becoming a mainstream lifestyle among the young generation. With the popularity of online apparel shopping, virtual try-on technology has received growing interests from fashion brands and online retail platforms in recent years [14].
Virtual try-on technology aims to improve the online shopping experience by visualizing the fitting results with-out requiring consumers to physically wear the garments.
Figure 1. Pose-garment keypoints guided inpainting framework.
According to whether 3D modeling is used, existing virtual try-on methods are categorized into image-based approach and 3D model-based approach. Since capturing 3D infor-mation requires additional sensory devices and the 3D mod-eling of person and garments costs more expense, image-based technology has attracted more attention recently.
Image-based virtual try-on aims to produce high-fidelity fitting results given person and garment images. The gen-erated try-on image is expected to preserve the appearance and pose of the given person image but replace the cloth region with the given garment image. Most existing ap-proaches share the same idea of firstly warping the garment image for target pose then blending the person image with warped garment image and target segmentation map. How-ever, a common issue has always existed, that is, inappro-priate warping of the garment image or inaccurate estima-tion of the target segmentation map usually results in the distortion of the garment shape. For instance, when part of the cloth is occluded due to the person pose, as shown in the Ground-Truth in Figure. 2, the warped garment gen-erated by existing methods, e.g., thin-plate spline transfor-mation (TPS) [3, 9], suffers from severer distortion at the overlapping part, as observed in the upper-side of Figure. 2.
Other distortions may happen at cuff or neckline. Moreover,
age with the garment and arms region cropped off. The warped garment are then populated, conditioned on target segmentation maps, into the cropped area of the person im-age, resulting in the recomposed person image. To fill up the missing area of the recomposed person image, we adopt in-painting conditioned on the target segmentation map. Note that the target segmentation map plays a role in providing the body semantic information, and this is why we call the final step semantic-conditioned inpainting. The main con-tribution of the paper is summarized as:
• We propose a pose-garment keypoints guided inpaint-ing method for the image-based virtual try-on task.
• We propose a graph-based model to extract the pose-oriented garment keypoints for garment warping and target segmentation map estimation.
• We propose a semantic-conditioned inpainting scheme to generate the final try-on image.
• We conduct extensive experiments to verify the effec-tiveness of KGI and show quantitative and qualitative improvements compared with prior methods. 2.