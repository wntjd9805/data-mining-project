Abstract
In this paper, we study a novel and widely existing prob-lem in graph matching (GM), namely, Bi-level Noisy Cor-respondence (BNC), which refers to node-level noisy cor-respondence (NNC) and edge-level noisy correspondence (ENC). In brief, on the one hand, due to the poor recog-nizability and viewpoint differences between images, it is inevitable to inaccurately annotate some keypoints with off-set and confusion, leading to the mismatch between two associated nodes, i.e., NNC. On the other hand, the noisy node-to-node correspondence will further contaminate the edge-to-edge correspondence, thus leading to ENC. For the
BNC challenge, we propose a novel method termed Con-trastive Matching with Momentum Distillation. Specifically, the proposed method is with a robust quadratic contrastive loss which enjoys the following merits: i) better exploring the node-to-node and edge-to-edge correlations through a
GM customized quadratic contrastive learning paradigm; ii) adaptively penalizing the noisy assignments based on the confidence estimated by the momentum teacher. Extensive experiments on three real-world datasets show the robust-ness of our model compared with 12 competitive baselines.
The code is available at https://github.com/XLearning-SCU/2023-ICCV-COMMON. 1.

Introduction
Graph Matching (GM) [9, 60] aims to establish corre-spondences between keypoints of different graphs, which plays an important role in various applications such as ob-ject tracking [56, 45], scene graph discovery [5], Simulta-neous Localization and Mapping [4], and Structure-from-Motion [41]. The key of GM is to explore and exploit the bi-level affinities between graphs, i.e., node-to-node (lin-ear) and edge-to-edge (quadratic) affinity. For this pur-pose, existing methods have been devoted to integrating the bi-level information by designing advanced graph neu-*Corresponding author
Figure 1. An illustrative example of Bi-level Noisy Corre-spondence (BNC). The green and red dots denote the correctly and wrongly annotated keypoints. The green and red squares de-note the true and false assignments. (a) Keypoint A3 and B1 are wrongly annotated due to the occlusion caused by the view-point (b) Given difference, and the low recognizability, respectively. the keypoints, GM methods construct the graph structure for fur-ther matching. The matching procedure will inevitably encounter
BNC, which refers to Node-level Noisy Correspondence (NNC) and Edge-level Noisy Correspondence (ENC). Specifically, NNC denotes the false matching between nodes, e.g., nodes A1 and B1 are wrongly matched. ENC is accompanied with NNC as the edge weight is derived from the feature and position of nodes. Once the node is imperfectly annotated, the edges derived from it and the edges derived from its counterpart will be wrongly associated. For instance, the edge between A1 and A5 (marked as A15) is wrongly associated with that between B1 and B5 (marked as B15). ral networks [47, 48, 41, 58] or differentiable quadratic loss [13, 40]. Based on the encoded high-order geometri-cal information, graph matching achieves promising results in correspondence estimation.
However, the success of existing GM methods highly de-pends on an implicit assumption, i.e., the annotated key-point pairs are correctly associated. Unfortunately, in prac-tice, the manual annotation process is susceptible to poor recognizability [3] and viewpoint differences between im-ages [34], which probably results in offset and even false keypoint annotations (Please refer to Fig. 4 for examples).
As shown in Fig. 1, the inaccurate annotations will in-evitably lead to Bi-level Noisy Correspondence (BNC) problem. Specifically, BNC refers to node-level noisy cor-respondence (NNC) and accompanied edge-level noisy cor-respondence (ENC). As shown in Fig. 3(a), BNC leads to serious performance degradation due to two potential is-sues. On the one hand, BNC would hinder both node-level and edge-level representation learning. Specifically, repre-sentation learning on graphs requires information propaga-tion and aggregation between the nodes and edges, there-fore, BNC will cause accumulative errors and mislead the model optimization. On the other hand, GM is subject to one-to-one mapping constraints, i.e., each keypoint in the first graph must have a unique correspondence in the sec-ond graph. Hence, “a slight move in one part may affect the situation as a whole”, i.e., one single noisy correspondence pair might result in global alignment failure and even affect the optimization of correctly annotated pairs.
To the best of our knowledge, such an essential prob-lem has not been touched so far and it is quite challeng-ing to solve it due to the following two reasons: i) without prior engineering like verification labels [24] for noisy cor-respondence, it is nearly impossible to pre-process them be-fore training, i.e., correctly distinguish and discard the noisy correspondence in advance. Hence, it is highly expected to develop a robust matching method. ii) however, GM is not a simple one-to-many optimization problem (e.g., clas-sification) but a many-to-many combinatorial optimization problem. One cannot solve the BNC challenge by trivially resorting to label noise learning methods [19, 26, 55] as they only enjoy robustness on one-to-many classification tasks.
To explore an effective solution to this challenge, we propose a novel method termed COntrastive Matching with
MOmentum distillatioN (COMMON). Specifically, COM-MON is equipped with a robust quadratic contrastive loss that incorporates both linear and quadratic geometrical in-formation into the contrastive learning paradigm [6, 16, 50].
More specifically, this study is motivated by [35] that the vanilla contrastive loss is equivalent to the linear assign-ment from the combinatorial optimization theory. In other words, the quadratic structure information could be incor-porated into contrastive learning. Based on this motivation, we endow the contrastive loss with quadratic information through two novel graph-geometric consistency regulariz-ers. To enhance the robustness against BNC, the proposed loss adaptively penalizes the noisy correspondence through a momentum distillation strategy based on the memoriza-tion effect of deep neural networks [1, 53, 18], i.e., deep net-works are apt to learn the simple patterns before fitting the noise. Motivated by this empirical observation, we keep a momentum version [25, 16] of the GM model by taking the moving average of its parameters during training. The mo-mentum teacher model could generate high-quality pseudo-targets as additional supervision on the quadratic contrastive loss without resorting to extra verification labels. With bi-level distillation on both node and edge alignment, the pro-posed robust quadratic contrastive loss effectively mitigates the negative affect of BNC.
The main contributions of this work are:
• We reveal a new challenge for graph matching, termed bi-level noisy correspondence (BNC). BNC refers to noisy correspondence on both node and edge levels, which would lead to serious performance degradation.
• To tackle the BNC challenge, we propose a robust quadratic contrastive learning loss that utilizes the mo-mentum distillation strategy to rebalance the noisy as-signment. The proposed contrastive loss explores the quadratic information through two simple graph con-sistency regularizers.
• Extensive experiments on real-world data verify the effectiveness of our method against noisy correspon-dence. On Pascal VOC, Spair-71k and Willow, we achieve absolute improvements of 1.6%, 1.4%, and 1.4% compared to the state-of-the-art methods. 2.