Abstract
There has been significant progress in Masked Image
Modeling (MIM). Existing MIM methods can be broadly categorized into two groups based on the reconstruction target: pixel-based and tokenizer-based approaches. The former offers a simpler pipeline and lower computational cost, but it is known to be biased toward high-frequency de-tails. In this paper, we provide a set of empirical studies to confirm this limitation of pixel-based MIM and propose a new method that explicitly utilizes low-level features from shallow layers to aid pixel reconstruction. By incorporat-ing this design into our base method, MAE, we reduce the wasted modeling capability of pixel-based MIM, improv-ing its convergence and achieving non-trivial improvements across various downstream tasks. To the best of our knowl-edge, we are the first to systematically investigate multi-level feature fusion for isotropic architectures like the stan-dard Vision Transformer (ViT). Notably, when applied to a smaller model (e.g., ViT-S), our method yields significant performance gains, such as 1.2% on fine-tuning, 2.8% on linear probing, and 2.6% on semantic segmentation. Code and models are available in MMPretrain1. 1.

Introduction
Self-supervised learning (SSL) has made remarkable progress in language and computer vision. Masked Image
Modeling (MIM) is an effective framework in image SSL, which boasts a simple training pipeline, a few handcrafted data augmentations, and high performance across down-stream tasks. In the pioneering work of BEiT [1], 40% of the input image is masked, and the model is trained to cap-ture the semantics of the masked patches by reconstructing the DALL-E [37] output features. To simplify pre-training and reduce computational overhead, MAE [14] only feeds the visible tokens into the encoder and encourages the de-coder to reconstruct the raw pixels of masked patches. More 1https://github.com/open-mmlab/mmpretrain
Figure 1: Fusing shallow encoder layers for MAE. During the training process, MAE increasingly relies on shallow layers for the pixel reconstruction task, demonstrating a bias toward low-level features. recently, follow-up works have focused on adding auxiliary tasks or using large-scale pre-trained models to produce re-construction targets. For instance, CMAE [23] explicitly adds a contrastive task and optimizes it in conjunction with the MIM task, while MILAN [20] and BEiT-v2 [34] em-ploy multimodal pre-trained models such as CLIP [36] to generate the reconstruction features. Among the various
MIM methods available, pixel-based approaches such as
MAE [14] are particularly interesting because of their sim-ple pre-training pipeline and minimal computational over-head. However, these methods are typically biased towards capturing high-frequency details due to their emphasis on reconstructing raw pixels [1, 30]. As a result, they waste a significant amount of modeling capability that could be better utilized to capture low-frequency semantics. Our ob-jective is to reduce this waste of modeling capacity, aiming for an improved quality of learned representation for down-stream visual tasks. Toward this goal, we design two pilot
spend its modeling abilities to capture these high-level se-mantics.
We denote the proposed method as Multi-level Feature
Fusion (MFF ). Specifically, we extend the usage of the fu-sion strategy in the first pilot experiment and systematically investigate the design choices of multi-feature fusion, such as feature selection and fusion strategies. Despite the sim-plicity of the proposed method, it is a drop-in solution for unleashing the full modeling potential of pixel-based MIM approaches and has the following advantages: (1) Employing multi-level feature fusion can enhance the training efficiency of MAE by approximately ∼5x, thus helping to reduce the carbon footprint. For example, by pre-training MAE with this strategy for only 300 epochs, we achieve semantic segmentation results that are on par with those obtained after 1600 epochs in the original paper. (2) We also consistently and significantly improve per-including semi-formance across all downstream tasks, supervised fine-tuning and linear probing. Notably, with a small model such as ViT-S, we outperform MAE by 2.8% on linear probing, 2.6% on semantic segmentation, and 1.2% on fine-tuning. (3) After evaluating our model on four out-of-distribution datasets, we observe that the approach with multi-level fea-ture fusion exhibits greater robustness than the base method.
Furthermore, we conduct a thorough analysis to unveil how multi-feature fusion works for representation learning.
Given the exploratory experiments from the perspective of latent features and optimization, we find that the fusion strategy attenuates high-frequency information in the latent features and flattens the loss landscapes. To summarize, our contributions are three-fold:
• Firstly, we develop a multi-level feature fusion strategy for isotropic backbones such as ViT, achieving superior results compared to various pixel-based MIM approaches.
• Secondly, we have conducted a thorough analysis of how this multi-level feature fusion strategy enhances the model from the perspectives of latent features and opti-mization. Our examination is meticulous and provides valuable insights.
• Lastly, we have performed extensive and rigorous abla-tion studies on the design details, which also strengthens the validity of our findings. 2.