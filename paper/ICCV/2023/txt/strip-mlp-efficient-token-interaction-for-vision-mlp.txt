Abstract
Token interaction operation is one of the core modules in
MLP-based models to exchange and aggregate information between different spatial locations. However, the power of token interaction on the spatial dimension is highly depen-dent on the spatial resolution of the feature maps, which limits the model’s expressive ability, especially in deep lay-ers where the feature are down-sampled to a small spatial size. To address this issue, we present a novel method called
Strip-MLP to enrich the token interaction power in three ways. Firstly, we introduce a new MLP paradigm called
Strip MLP layer that allows the token to interact with other tokens in a cross-strip manner, enabling the tokens in a row (or column) to contribute to the information aggregations in adjacent but different strips of rows (or columns). Sec-ondly, a Cascade Group Strip Mixing Module (CGSMM) is proposed to overcome the performance degradation caused by small spatial feature size. The module allows tokens to interact more effectively in the manners of within-patch and cross-patch, which is independent to the feature spatial size. Finally, based on the Strip MLP layer, we propose a novel Local Strip Mixing Module (LSMM) to boost the to-ken interaction power in the local region. Extensive exper-iments demonstrate that Strip-MLP significantly improves the performance of MLP-based models on small datasets and obtains comparable or even better results on Ima-geNet. In particular, Strip-MLP models achieve higher av-erage Top-1 accuracy than existing MLP-based models by
+2.44% on Caltech-101 and +2.16% on CIFAR-100. The source codes will be available at https://github.com/Med-Process/Strip MLP. 1.

Introduction
In computer vision, Convolutional Neural Networks (CNNs) are one of the most popular network backbones, which have made a series of breakthroughs [8]. Inspired by
*Corresponding author.
Figure 1. Token interaction of the MLP layer on the down-sampled image feature. (a) The process of which the feature resolution will be down-sampled from H ×W ×C into a small spatial size of H 8 ×
W 8 × 8C. (b) Token interaction on different feature resolutions. the great success of self-attention-based architectures [32] in natural language processing (NLP), the Transformer models [5, 19, 34] are introduced into the field of com-puter vision, and have achieved comparable results with state-of-the-art (SOTA) CNNs. Although ViT [5] and its variants outperform traditional CNNs, the models introduce high computational complexity to construct attention maps.
Recently, some studies [20, 26] in the vision community suggest that the attention mechanism is not necessary and simpler model architectures are proposed.
MLP-based models, like MLP-Mixer [28], gMLP [16] and ViP [10] process the data with the Multilayer Percep-trons (MLP), showing great potential to improve the perfor-mance of vision models [29]. As the first visual deep MLP network, MLP-Mixer [28] introduces two types of MLP layers: Channel-Mixing MLPs (CMM) and Token-Mixing
MLPs (TMM). For CMM, the module mainly mixes the in-formation between different channels of each token. For
TMM, it allows each spatial token to interact with all other tokens (the whole image) in a single MLP layer. However, this design also introduces a much larger number of param-eters and higher computational complexity prone to over-fitting. To address this problem, Sparse MLP (SMLP) [26] and Vision Permutator (ViP) [10] propose a similar layer of parallel structure, which applies one dimension MLP along the axial directions, and parameters are shared among rows or columns, respectively. Therefore, it reduces the number
of model parameters and computational complexity, avoid-ing the common over-fitting problem.
Although SMLP and ViP alleviate some deficiencies of
MLP-Mixer [28], both methods bring the challenge that the token interaction power is highly dependent on the fea-ture spatial size when interacting tokens on spatial rows (or columns). As shown in Fig. 1, the spatial feature resolution is down-sampled to a small size but with more channels, which means the feature pattern of each token is mainly concentrated on the channel dimension rather than the spa-tial one. Interacting tokens along the spatial dimension by sharing the weights among all channels would seriously ig-nore the feature pattern differences among different chan-nels, which may degrade the token interaction power, es-pecially in deep layers with small spatial feature resolu-tion. Here, we mark this problem as the Token’s interac-tion dilemma. Taking SMLP as an example, we analyze the feature resolution and complexity of the model in different stages in detail (seen in Sec. 3.4). We find that as the spatial feature size decreases by down-sampling stage by stage, the token interaction layer also becomes smaller and smaller, which makes the token interaction power degraded rapidly.
To address these challenges, we propose a new efficient
Strip MLP model, dubbed Strip-MLP, to enrich the power of the token interaction layer in three ways. For the level of a single MLP layer, inspired by the cross-block normal-ization schemes of HOG [3] and the sparse connections be-tween the biological neurons, we design a Strip MLP layer to allow the token to interact with other tokens in a cross-strip manner, enabling each row or column of the tokens to contribute differently to other rows or columns. For the token interaction module level, we develop channel-wise group mixing of CGSMM, enabling the tokens in a row (or column) to contribute to the information aggregations in ad-jacent but different strips of rows (or columns). to tackle the problem that the token interaction power decreases in deep layers with the spatial feature size significantly reduced but with multiplying channels. Considering the existing meth-ods [10, 26, 28] interact the tokens mainly in the long range of row (or column), which may not aggregate tokens well in the local region, we propose a new Local Strip Mixing
Module (LSMM) with a small Strip MLP unit to strengthen the token interaction power on local interactions.
The proposed Strip-MLP model significantly boosts the token interaction power, and the main contributions are:
• A new MLP paradigm for vision MLP: Strip MLP layer, which aggregates the adjacent tokens in a cross-strip manner and enables each row or column of the to-kens to contribute differently to other rows or columns, interacting tokens more efficiently.
• Designing a Cascade Group Strip Mixing module and a Local Strip Mixing module, which effectively im-proves the model’s token interaction power and boosts the tokens aggregation in the local region, respectively;
• Extensive experiments show that Strip-MLP remark-ably improves the performances of the MLP-based models. Strip-MLP achieves higher average Top-1 accuracy by +2.44% on Caltech-101 and +2.16% on
CIFAR-100 over the existing MLP-based models. In addition, our models achieve comparable or even bet-ter performances on ImageNet-1K compared with tra-ditional MLP-based models, and other popular CNNs and transformer-based models. 2.