Abstract
Spiking Neural Networks (SNNs) are well known as a promising energy-efficient alternative to conventional ar-tificial neural networks. Subject to the preconceived im-pression that SNNs are sparse firing, the analysis and opti-mization of inherent redundancy in SNNs have been largely overlooked, thus the potential advantages of spike-based neuromorphic computing in accuracy and energy efficiency
In this work, we pose and focus on three are interfered. key questions regarding the inherent redundancy in SNNs.
We argue that the redundancy is induced by the spatio-temporal invariance of SNNs, which enhances the efficiency of parameter utilization but also invites lots of noise spikes.
Further, we analyze the effect of spatio-temporal invari-ance on the spatio-temporal dynamics and spike firing of
SNNs. Then, motivated by these analyses, we propose an
Advance Spatial Attention (ASA) module to harness SNNs’ redundancy, which can adaptively optimize their membrane potential distribution by a pair of individual spatial atten-tion sub-modules. In this way, noise spike features are ac-curately regulated. Experimental results demonstrate that the proposed method can significantly drop the spike fir-ing with better performance than state-of-the-art SNN base-lines. Our code is available in https://github.com/
BICLab/ASA-SNN . 1.

Introduction
By mimicking the spatio-temporal dynamics behaviors of biological neurons, Spiking Neural Networks (SNNs) pose a paradigm shift in information encoding and transmit-ting [36, 37, 25]. Spiking neurons only fire when the mem-brane potential is greater than the threshold (Figure 1a), in
*These authors contribute equally to this work
†Corresponding author theory, these complex internal dynamics make the repre-sentation ability to spiking neurons more powerful than ex-isting artificial neurons [31]. Moreover, spike-based binary communication (0/1 spike) enables SNNs to be event-driven when deployed on neuromorphic chips [3, 34, 49], i.e., per-forming cheap synaptic Accumulation (AC) and bypassing computations on zero inputs or activations [6, 4].
For a long time, when referring to spike-based neuro-morphic computing, people naturally believe that its com-putation is sparse due to the event-driven feature. Sub-ject to this preconceived impression, although it is gener-ally agreed that sparse spike firing is the key to achieving high energy efficiency in neuromorphic computing, there is a lack of systematic and in-depth analysis of redundancy in
SNNs. Existing explorations are limited to specific meth-ods of dropping spike counts. For instance, several al-gorithms have been proposed to exploit spike-aware spar-sity regularization and compression by adding a penalty function[5, 53, 55, 52, 33, 24], designing network structures with fewer spikes using neural architecture search tech-niques [32, 23], or developing data-dependent models to regulate spike firing based on the input data [48, 51]. Gen-erally, employing these methods to reduce spikes incurs a loss of accuracy or significant additional computation.
In this work, we provide a novel perspective to under-stand the redundancy of SNNs by analyzing the relation-ship between spike firing and spatio-temporal dynamics of spiking neurons. This analysis could be extended by asking three key questions. (i) Which spikes are redundant? (ii)
Why is there redundancy in SNNs? (iii) How to efficiently drop the redundant spikes?
To perfectly demonstrate redundancy in SNNs, we select event-based vision tasks to observe spike responses. Event-based cameras, such as the Dynamic Vision Sensor (DVS)
[27], are a novel class of bio-inspired vision sensors that only encode the vision scene’s brightness change informa-tion into a stream of events (spike with address information)
for each pixel. As shown in Figure 1b, the red and green dots represent pixels that increase and decrease in bright-ness, respectively, and the gray areas without events indi-cate no change in brightness. However, although the infor-mation given in the input is human gait without background, some spike features extracted by the vanilla SNN focus on background information. As depicted in Figure 1c, the spik-ing neurons in the noise feature map fire a large number of spikes in the background region, which are redundant.
Unfortunately, noise features exist widely in both tem-poral and spatial dimensions, but exhibit some interesting regularities. We argue that the underlying reason for this phenomenon is due to a fundamental assumption of SNNs, known as spatio-temporal invariance[21], which enables sharing weights for every location across all timesteps. This assumption improves the parameter utilization efficiency while boosting the redundancy of SNNs. Specifically, by controlling the input time window of event streams, we can clearly observe the temporal and spatial changes of the spike features extracted by the SNN (see Figure 2). In the spatial dimension, there are many similar noise features, which can be referred to as ghost features [14, 15]. In the temporal dimension, although the information extracted by
SNN changes at different timesteps, the spatial position of the noise spike feature is almost the same.
Recently, several works [12, 13] have investigated the information loss caused by SNNs when quantizing contin-uous membrane potential values into discrete spikes.
In-spired by these works, we transformed our problem “the relationship between spike firing and spatio-temporal dy-namics of spiking neurons” to investigate the relationship between membrane potential distribution and redundant spikes. Motivated by the observations that redundancy is highly correlated with spike feature patterns and neuron lo-cation, we present the Advanced Spatial Attention (ASA) module for SNNs, which can convert noise features into normal or null (without spike firing) features by shifting the membrane potential distribution. We conduct extensive ex-periments using a variety of network structures to verify the superiority of our method on five event-based datasets. Ex-perimental results show that the ASA module can help SNN reduce spikes and improve task performance concurrently.
For instance, on the DVS128 Gait-day dataset[41], at the cost of negligible additional parameters and computations, the proposed ASA module decreases the baseline model’s spike counts by 78.9% and increases accuracy by +5.0%.
We summarize our contributions as follows: 1) We provide the first systematic and in-depth analysis of the inherent redundancy in SNNs by asking and an-swering three key questions, which are crucial to the high energy efficiency of spike-based neuromorphic computing but have long been neglected. (a) Spatio-temporal dynamics of spiking neu-Figure 1: rons with binary spike input and output, synaptic weight w, membrane potential U (t), threshold Vth and hard re-set membrane potential Vreset. (b) An example of an event stream. (c) Examples of changes in the spike responses of vanilla SNN and ASA-SNN, in terms of Membrane Poten-tial Distribution (MPD) and spike feature. Each pixel value on the spike feature represents the firing rate of a neuron.
Noise spike feature fires lots of spikes while concentrating on insignificant background information (large area red).
The ASA module can shift the spike pattern of SNNs to drop spike counts by optimizing the MPD. 2) For the first time, we relate the redundancy of SNNs to the distribution of membrane potential, and design a simple yet efficient advanced spatial attention to help
SNN optimize the membrane potential distribution and thus reduce redundancy. 3) Extensive experimental results show that the proposed
ASA module can improve SNNs’ performance and significantly drop noise spikes concurrently. This inspires us that two of the most important nature of spike-based neuromorphic computing, bio-inspired spatio-temporal dynamics and event-driven sparse computing, can be naturally incorporated to achieve better performance with lower energy consumption.
2.