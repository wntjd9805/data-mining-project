Abstract
For improving image composition and aesthetic qual-ity, most existing methods modulate the captured images by striking out redundant content near the image borders.
However, such image cropping methods are limited in the range of image views. Some methods have been suggested to extrapolate the images and predict cropping boxes from the extrapolated image. Nonetheless, the synthesized ex-trapolated regions may be included in the cropped image, making the image composition result not real and poten-tially with degraded image quality. In this paper, we circum-vent this issue by presenting a joint framework for both un-bounded recommendation of camera view and image com-position (i.e., UNIC). In this way, the cropped image is a sub-image of the image acquired by the predicted camera view, and thus can be guaranteed to be real and consis-tent in image quality. Specifically, our framework takes the current camera preview frame as input and provides a rec-ommendation for view adjustment, which contains opera-tions unlimited by the image borders, such as zooming in or out and camera movement. To improve the prediction accuracy of view adjustment prediction, we further extend the field of view by feature extrapolation. After one or sev-eral times of view adjustments, our method converges and results in both a camera view and a bounding box showing the image composition recommendation. Extensive experi-ments are conducted on the datasets constructed upon ex-isting image cropping datasets, showing the effectiveness of our UNIC in unbounded recommendation of camera view and image composition. The source code, dataset, and pre-trained models is available at https://github.com/ liuxiaoyu1104/UNIC. 1.

Introduction
With the prevalence of electronic devices such as smart-phones, taking photos has become a common activity in ev-eryday life. Due to the lack of professional photography knowledge and skills, taking photos with harmonious im-cpred
UNIC
Left (←) Up (↑)
Zoom Out
Init View Iinit
Predict View Ipre
Figure 1. Illustration of our proposed UNIC for unbounded rec-ommendation of camera view and image composition. On the left is the initial view provided by the user. Given the current view, our model can predict camera operations (e.g., zoom out and the movement) and a image composition solution (e.g., cpred ). The prediction can be executed multiple times until convergence. age composition and high aesthetic quality is still difficult for non-professional users. As a remedy, image composi-tion, which aims to find an aesthetic region of a scene, has attracted much attention in recent years.
In order to facilitate the training of image composition models, several datasets [33, 38, 6] have been established, and each image comes along with one or multiple bound-ing boxes indicating the cropping schemes. Despite the notable progress, most existing image composition meth-ods [33, 12, 18, 19, 38, 25, 43, 15] generally adopt a post-processing form on the already captured images, i.e., they only adjust the composition in an image cropping man-ner.
In other words, the captured images are modulated by striking out redundant content near the image borders.
Nonetheless, a sub-optimal solution will inevitably be ob-tained when the best cropping is not entirely in the acquired image. To alleviate this issue, Zhong et al. [43] proposed to expand the image via out-painting, and then predict the cropped view on the expanded image. It is a practical so-lution in post-processing manner, but may suffers from out-painting artifacts.
To tackle the limitations of existing image composi-tion methods, this paper proposes a novel framework for unbounded recommendation of camera view and image composition (i.e., UNIC). As shown in Fig. 1, the user ini-tializes a view with the content of interest. Given the current view, our model finds a potential well-composed view and provides the corresponding camera movement operations, either inside or beyond the image borders. Note that solely performing camera view adjustment is not enough, since the aspect ratio is typically kept unchanged during the pho-tography process. Therefore, our model also concurrently predicts a bounding box for cropping after camera move-ment operations. With our model, the user finally can get the most recommended camera view and the corresponding image composition bounding box as shown in Fig. 1.
For implementing the UNIC model, we further simplify the task of joint camera view adjustment and image compo-sition into unbounded image composition by merging the
In this way, the architecture of cropping based outputs. image composition methods can be deployed as the back-bone, and we follow Jia et al. [16] to adopt the conditional-DETR [27] structure. In contrast to existing image crop-ping methods, we argue that our UNIC is more preferred and practical. First, existing image composition meth-ods [33, 12, 18, 19, 38, 25, 15] are restricted to image cropping over the already captured images. The introduc-tion of camera view adjustment can naturally circumvent the restriction of image borders by moving the camera or adjusting the optical zoom. Second, in comparison to out-painting [43], camera view adjustment can guarantee that the pixels outside the original borders are real and consis-tent with the pixels within the borders. Furthermore, new and real information can be introduced after each time of view adjustment. Thus, based on the result of last time, one can perform view adjustment and image composition for many times, which is also not supported by image cropping based methods.
Moreover, our UNIC is free to go beyond image bor-ders, yet directly predicting in unseen regions may lead to inferior results. To compensate for this, we further extend the camera field of view by extrapolation. Different from
Zhong et al. [43], whose extrapolation was performed in the image domain, we choose feature extrapolation and use it for predicting camera movement and bounding box in-stead of synthesizing unseen content. Thus, we can get the content in novel views by moving the camera, and unseen content generation is not necessary. In comparison to the la-tent space specified for image composition, forcing the ex-trapolation into the image domain may bring redundant or even harmful information. Besides, the feature extrapola-tion module can be well integrated into our existing frame-work, avoiding the heavy computation burdens brought by extra modules such as the image decoder.
For training and evaluating the proposed model, we take the advantage of existing image cropping datasets [33, 38] and convert them into a more generalized form. Extensive experiments and ablation studies show the effectiveness of our UNIC, which can work well under diverse conditions.
To sum up, the contributions of this paper include,
• We propose a novel UNIC method for jointly perform-ing unbounded recommendation of camera view and image composition. The user can adjust the current view following the recommendations to obtain images with higher aesthetic quality.
• We introduce a feature extrapolation module as well as an extrapolation loss term in the detection transformer framework, which improves the prediction accuracy, especially for out-of-image scenarios.
• Two unbounded image composition datasets are con-structed upon existing image cropping ones. Experi-mental results show that our proposed method achieves superior performance against state-of-the-art methods. 2.