Abstract
We propose Embodied Navigation Trajectory Learner (ENTL), a method for extracting long sequence representa-tions for embodied navigation. Our approach uniﬁes world modeling, localization and imitation learning into a single sequence prediction task. We train our model using vector-quantized predictions of future states conditioned on cur-rent states and actions. ENTL’s generic architecture en-ables the sharing of the the spatio-temporal sequence en-coder for multiple challenging embodied tasks. We achieve competitive performance on navigation tasks using signif-icantly less data than strong baselines while performing auxiliary tasks such as localization and future frame pre-diction (a proxy for world modeling). A key property of our approach is that the model is pre-trained without any ex-plicit reward signal, which makes the resulting model gen-eralizable to multiple tasks and environments. We release the code at https://github.com/klemenkotar/
ENTL
Figure 1. We introduce a method for extracting long sequence representations for embodied navigation. The proposed architec-ture enables sharing a spatio-temporal transformer-based back-bone across multiple tasks: navigation, localization, and future frame prediction. 1.

Introduction
Representation learning has shown great success in many
AI domains. The approach, common in the NLP ﬁeld for quite some time, has seen an explosion of popularity through the adaptation of the transformer architecture by work such as BERT [12] and GPT [8]. This initial suc-cess has triggered the creation of models with ever more parameters trained on ever larger datasets. Surprisingly, the growing models not only kept improving with the increased parameter counts and datasets, but even started exhibiting emergent behaviors [6]. Furthermore, the general recipe of converting data into sequences of tokens and predicting the next token has proven surprisingly universal. Subsequent work such as ViT [13] and MAE [21] have adapted this ap-proach to the computer vision domain with great success, and transformer models trained on large scale data have be-come the de facto computer vision backbone. 1Work mainly done while author was at Allen Institute for AI
Many Embodied AI (EAI) tasks have long horizons and sparse reward signals. This means that EAI models must ag-gregate information across many time steps, and learn how to attribute the successful or unsuccessful outcomes of en-tire sequences to individual actions. Since the space and time complexity of transformers grows quadratically with sequence length it has been difﬁcult to apply them to these long sequence tasks. Furthermore, it can be difﬁcult to learn which part of a frame will be signiﬁcant to a decision the model will have to make many steps down the line, making it hard to decide which information to preserve.
Modeling the environment of the embodied agent is a natural self supervised pre-training task. Various abstrac-tions of this problem, such as the prediction of top down maps [48] or graph-based representations [17] have been studied, but none have yielded a strong universal abstrac-tion. More recently, methods such as [36] have explored frame prediction in the RGB space conditioned on camera pose using a buffer of a few frames. These produce visually
compelling predictions, but fail to capture the movement dynamics of an embodied agent in the scene by condition-ing their models with camera pose instead of agent actions.
They also rely on a limited bank of previous frames which only allow the model to extrapolate the textures and shapes of the current view of the room.
We use future frame prediction as a universal naviga-tion pre-training task by conditioning it on agent actions.
This encodes the navigability of different areas of the envi-ronment within the prediction model, and allows us to use agent walkthroughs of the environment as our sole source of supervision, without the requirement of a known camera pose which might not be available in real world settings.
Secondly we utilize an architecture that attends over long sequences (up to 50 steps), allowing the agent to synthesize novel views of the environment from many past examples, and obtaining a richer representation of the environment.
Finally, we jointly model future frame prediction, agent pose prediction and action prediction, effectively merging imitation learning and representation learning. This ap-proach directly supervises the agent on how to perform a navigation task, and removes the need for a reward signal.
To ensure that crucial information is not forgotten we propose the prediction of future frames in the original RGB space, avoiding any inductive biases that might come from modeling the embeddings of observations. Since predicting the outputs in RGB pixel space can be unstable and difﬁcult, we utilize the image tokens produced by VQ-GAN [15] as our prediction targets. Modern tokenizers such as VQ-GAN can perform near-lossless encodings and reconstructions of
RGB images, allowing us to effectively treat them as a re-versible transform.
To overcome the long sequence lengths, we utilize a model architecture that attends across space and time in al-ternating layers similar to [5], allowing the full information of a trajectory to be aggregated without using intractable amounts of memory and computation. We combine this spatio-temporal sequence representation with three separate task heads for localization, future frame prediction and ac-tion prediction to maximize parameter sharing and utilize a causal mask to prevent the leakage of information from future states, while still satisfying the input requirements of each head. This design enables us to have a sequence rep-resentation that is suitable for all tasks (Figure 1).
We apply the self-supervised ofﬂine data training recipe and demonstrate that it can produce a model that performs well on embodied navigation tasks. Our model performs competitively against strong Reinforcement Learning (RL) and Imitation Learning (IL) baselines trained on signiﬁ-cantly more data on the PointNav [1] and ObjectNav[4] tasks. Furthermore, our model performs very well at lo-calization, accruing an average error of only 0.43m after a sequence of 50 steps, using RGB frames only and no motion sensors. Finally, our model produces high-quality, realistic future frame predictions, even in scenes with appearance characteristics well outside its training distribution.
In summary, our key contributions are 1) the formula-tion of a long sequence future frame prediction problem as a self-supervised task for embodied navigation 2) the proposal of a tokenization scheme and model architecture, which allow us to implement this pre-training 3) the con-trastive analysis of several adjacent methods and the neces-sary pieces required to make our approach work. 2.