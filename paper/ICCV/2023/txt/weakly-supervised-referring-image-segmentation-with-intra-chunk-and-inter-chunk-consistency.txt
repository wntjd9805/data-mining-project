Abstract
Referring image segmentation aims to localize the ob-ject in an image referred by a natural language expression.
Most previous studies learn referring image segmentation with a large-scale dataset containing segmentation labels, but they are costly. We present a weakly supervised learn-ing method for referring image segmentation that only uses readily available image-text pairs. We first train a visual-linguistic model for image-text matching and extract a visual saliency map through Grad-CAM to identify the image re-gions corresponding to each word. However, we found two major problems with Grad-CAM. First, it lacks considera-tion of critical semantic relationships between words. We tackle this problem by modeling the relationship between words through intra-chunk and inter-chunk consistency. Sec-ond, Grad-CAM identifies only small regions of the referred object, leading to low recall. Therefore, we refine the local-ization maps with self-attention in Transformer and unsu-pervised object shape prior. On three popular benchmarks (RefCOCO, RefCOCO+, G-Ref), our method significantly outperforms recent comparable techniques. We also show that our method is applicable to various levels of supervision and obtains better performance than recent methods. 1.

Introduction
Referring image segmentation aims to obtain a pixel-level segmentation mask of the object in an image referred by a natural language expression. It has a wide range of practical applications in the real world such as human-robot interac-tion [38, 46] and visual navigation [37]. To learn referring image segmentation, a neural network should not only com-prehend the semantics of image and text respectively, but also be able to capture the semantic alignment between the two modalities. A general approach to achieving this objec-tive is to leverage the dataset with fully supervised labels.
It necessitates numerous pairs of images and texts, together with a pixel-level segmentation mask of the referred object.
Figure 1: (a) Examples of Grad-CAMs of each word corre-sponding to red word given “gray shirt woman”. (b) Final resulting maps of baseline and ours for “gray shirt woman”. (c) Illustration of intra-chunk and inter-chunk relationship.
By using these explicit connections between image and text, recent studies [19, 47, 50] have successfully performed refer-ring image segmentation.
However, constructing a dataset equipped with pixel-level segmentation labels is extremely laborious and expensive.
For instance, annotating a segmentation label for a single im-age featuring a complex scene (e.g., CityScapes [8]) requires more than 90 minutes. As such, our objective is to mitigate this quandary with weakly supervised learning, which trains a neural network by using only readily available image-text pairs, without expensive segmentation labels.
There have been some weakly supervised approaches [10, 35, 43] to localize the object referred by the given text, in the form of a bounding box instead of a segmentation mask.
Nevertheless, these approaches are hindered by two key drawbacks: 1) they do not provide pixel-level localization of the referred object, and 2) most of them heavily depend on pre-trained object detector, which actually requires explicit object localization labels (i.e., box [21]). To the best of our knowledge, there is only one recent work [42] that learns referring image segmentation using only image-text pairs,
but their performance lags behind fully supervised methods.
To accomplish referring image segmentation utilizing solely image-text pairs, we first train a visual-linguistic
Transformer [27] via an image-text matching (ITM) objec-tive, where the model is trained to determine whether a given text describes the corresponding image or not. To realize
ITM, the model should learn the joint semantics of image and text. We then extract the knowledge of the trained model using Grad-CAM [40]. It computes the rationale for why the model thinks the given image-text pair is matched. As a result, Grad-CAM provides a visual saliency map represent-ing which image regions are correlated with each word in a given sentence.
Ideally, the Grad-CAM of each word in a given sentence should be generated by considering the semantics of its neighboring words, where the relationship between words can be modeled by self-attention in Transformer [44]. For instance, in the sentence “gray shirt woman” as depicted in
Figure 1(a), the Grad-CAM of ‘woman’ should identify only the woman wearing a gray shirt, not other women in the image, by taking into account the semantics of ‘gray’ and
‘shirt’. However, our baseline model, which was trained only with ITM, struggles to capture the compositional consistency between words, resulting in inconsistent localization of the
Grad-CAMs of each word in the sentence. For example, in
Figure 1(a), the baseline’s Grad-CAM for ‘woman’ identifies all the women in the image without considering the neigh-boring words. As a result, the final localization, obtained by averaging Grad-CAMs over the words in the sentence, fails to exclusively locate the woman wearing a gray shirt, as shown in Figure 1(b). This means that the baseline lacks consideration of the relationships between words, which is problematic for referring image segmentation where only the referred object should be identified. Therefore, in this work, we propose a novel regularization technique to incor-porate the intra-chunk and inter-chunk relationships so that the model considers the relationships between words in the given expression.
Specifically, sentences consist of several noun-chunks. A noun-chunk is a group of words, which consists of a head noun and its modifying (dependent) words. In the training of
ITM, we add regularization terms to produce consistent local-ization maps between words in a single chunk (intra-chunk), and between chunks (inter-chunk) (Figure 1(c)). Although tree-based recursive linguistic structure [6] is popular for capturing such relationships, they usually contain too much of unrelated details, thereby making it hard to extract only necessary information. Thus, we simplify this with noun-chunk-level representations.
The Grad-CAM obtained with intra-chunk and inter-chunk consistency provides a more accurate localization of the referred object, but it still has two drawbacks. First, it identifies only small regions of the referred object because all the regions of the target object are not necessary for ITM.
Second, due to the absence of object shape information in the image-text pair, Grad-CAM does not represent the exact boundary of the object. Therefore, we propose two refine-ment techniques to obtain a more complete segmentation of the referred object, using patch affinities obtained from visual Transformers and unsupervised object shape prior.
Our main contributions include (1) intra-chunk and inter-chunk consistency to improve Grad-CAMs by considering the relationship between words in a given text; (2) two re-finement techniques for a more accurate segmentation of the referred object; (3) significantly better performance on the three popular benchmarks than existing methods under the same level of supervision; and (4) versatility of our approach that allows integration with various levels of supervision. 2.