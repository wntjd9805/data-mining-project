Abstract
With the overwhelming trend of mask image modeling led by MAE, generative pre-training has shown a remarkable potential to boost the performance of fundamental models in 2D vision. However, in 3D vision, the over-reliance on
Transformer-based backbones and the unordered nature of point clouds have restricted the further development of gen-erative pre-training. In this paper, we propose a novel 3D-to-2D generative pre-training method that is adaptable to any point cloud model. We propose to generate view images from different instructed poses via the cross-attention mechanism as the pre-training scheme. Generating view images has more precise supervision than its point cloud counterpart, thus assisting 3D backbones to have a finer comprehension of the geometrical structure and stereoscopic relations of the point cloud. Experimental results have proved the su-periority of our proposed 3D-to-2D generative pre-training over previous pre-training methods. Our method is also ef-fective in boosting the performance of architecture-oriented approaches, achieving state-of-the-art performance when fine-tuning on ScanObjectNN classification and ShapeNet-Part segmentation tasks. Code is available at https:
//github.com/wangzy22/TakeAPhoto. 1.

Introduction
Nowadays, pre-training fundamental models with self-supervised mechanisms has witnessed a thriving develop-ment in the computer vision community, given its low re-quirement in data annotation and its high transferability to downstream applications. Self-supervised pre-training aims to fully exploit the statistical and structural knowledge from abundant annotation-free data and empowers the fundamen-tal models with robust representation ability. In 2D vision, self-supervised pre-training has shown strong potential and achieved remarkable progress on diverse backbones in var-ious downstream tasks. Successful pre-training strategies
*Equal contribution.
†Corresponding author.
Figure 1: Principle illustration of 3D-to-2D generative pre-training. The photograph module explicitly encodes pose condition into 3D features from the backbone, and the 2D generator decodes pose-conditioned features into different view images. in 2D domain such as contrastive learning [19, 7] and mask modeling [18, 3] have also been adopted to 3D point cloud analysis [58, 30, 23] in recent research.
However, the pre-training paradigm hasn’t become the prevailing trend in 3D learning and architectural design is still the mainstream to reach a new state-of-the-art perfor-mance, which is considerably different from the dominant status of pre-training in 2D domain. In object-level analy-sis, generative pre-training inspired by MAE [18] has been studied thoroughly, but their performances still lag behind architecture-based methods like PointNeXt [35]. Two fac-tors mainly contribute to the inferior status of generative pre-training in 3D learning. Since point clouds are unordered sets of point coordinates, there is no direct and precise su-pervision like one-to-one MSE loss between generated point clouds and their corresponding ground truth. Chamfer Dis-tance supervision for point clouds only calculates a rough set-to-set matching and its imprecision has been widely dis-cussed in [24, 50, 20]. Additionally, existing advanced gen-erative pre-training methods in object analysis are limited to the Transformer-based backbone, and fail to be extended to other sophisticated point cloud models.
To alleviate the aforementioned problems, we propose a 3D-to-2D generative pre-training method for point cloud analysis that has higher preciseness in supervision and broader adaptation to different backbones. Instead of re-constructing point clouds as previous literature [30, 60], we propose to generate view images of the input point cloud given the instructed camera poses. This is similar to taking photos of a realistic object from different perspectives to fully present its structure or internal relations. Therefore, we name our model Take-A-Photo, in short TAP. More specifi-cally, we propose a pose-dependent photograph module that utilizes the cross-attention mechanism to explicitly encode pose conditions with 3D features from the backbone. Then a 2D generator decodes pose-conditioned features into view images that are supervised by rendered ground truth images.
The principle illustration of TAP is shown in Figure 1. In the pose-dependent photograph module, we do not provide detailed projection relations from 3D points to 2D pixels, thus the cross-attention layers are encouraged to learn by themselves what those view images look like conditioned on given poses. Since the projection layout, part occlusion rela-tion, faces colors that represent light reflections are largely distinct among view images, the proposed 3D-to-2D genera-tive pre-training is a challenging pretext task that obliges 3D backbone to gain higher representation ability of geometrical knowledge and stereoscopic relations.
We conduct extensive experiments on various backbones and downstream tasks to verify the effectiveness and su-periority of our proposed 3D-to-2D generative pre-training method. When pre-trained on synthetic ShapeNet [6] and transferred to real-world ScanObjectNN [45] classification,
TAP brings consistent improvement to different backbone models and successfully outperforms previous point cloud generative pre-training methods based on Transformers back-bone. With PointMLP [27] as the backbone, TAP achieves state-of-the-art performance on ScanObjectNN classification and ShapeNetPart [56] part segmentation among methods that do not include any pre-trained image or text model. We also conduct thorough ablation studies to discuss the archi-tectural design of the TAP model and verify the individual contribution of each component.
In conclusion, the contributions of our paper can be sum-marized as follows: (1) We propose TAP, the first 3D-to-2D generative pre-training method that is adaptable to any point cloud model. TAP pre-training helps to exploit the potential of point cloud models on geometric structure comprehen-sion and stereoscopic relation understanding. (2) We pro-pose a Photograph Module where we derive mathematical formulations to encode pose conditions as query tokens in cross-attention layers. (3) TAP surpasses previous gener-ative pre-training methods on the Transformers backbone and achieves state-of-the-art performance on ScanObjectNN classification and ShapeNetPart segmentation. 2.