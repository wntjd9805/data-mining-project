Abstract
Keypoint-based representation has proven advantageous in various visual and robotic tasks. However, the exist-ing 2D and 3D methods for detecting keypoints mainly rely on geometric consistency to achieve spatial alignment, ne-glecting temporal consistency. To address this issue, the
Transporter method was introduced for 2D data, which re-constructs the target frame from the source frame to in-corporate both spatial and temporal information. How-ever, the direct application of the Transporter to 3D point clouds is infeasible due to their structural differences from 2D images. Thus, we propose the first 3D version of the Transporter, which leverages hybrid 3D representation, cross attention, and implicit reconstruction. We apply this new learning system on 3D articulated objects and non-rigid animals (humans and rodents) and show that learned keypoints are spatio-temporally consistent. Additionally, we propose a closed-loop control strategy that utilizes the learned keypoints for 3D object manipulation and demon-strate its superior performance. Codes are available at https://github.com/zhongcl-thu/3D-Implicit-Transporter. 1.

Introduction
The ability to establish correspondences in temporal in-puts is a hallmark of the human visual system, and this abil-ity has been verified by developmental biologists [52] as the enabling factor of object perception. Specifically, in-fants can naturally separate different objects by considering pixels that move together. Meanwhile, establishing dense correspondences from image sequences (i.e., optical flow)
[5, 6, 9, 13, 54, 55, 53] is also one of the oldest computer vision topics, dating back to the birth of this discipline [19].
*Corresponding author
Figure 1. Given paired point clouds, our 3D Implicit Transporter leverages the object/part motion to discover temporally consistent keypoints and recover the underlying shape for each input. More-over, the learned keypoints can serve downstream robotics tasks, such as articulated object manipulation.
On the other hand, keypoints are preferred as a com-pact mid-level representation in many scenarios, like vi-sual recognition [51], pose estimation[72], reconstruction
[37], and robotic manipulation [43]. Sparse correspondence from keypoints [30, 7, 10, 45] is another fundamental vision topic. Most of the 2D and 3D keypoint detection methods
[73, 25] depend on the consistency under geometric trans-formations to achieve spatial alignment of keypoints. How-ever, these methods are limited in their ability to identify temporally consistent keypoints, which is crucial for repre-senting movable or deformable objects such as the human body whose shape and topology may vary over time. So does there exist a generic principle that reflects how humans extract spatiotemporally consistent keypoints? One (possi-ble) such principle is that good mid-level representation can be used to re-synthesize raw visual inputs. This principle has been explored in legacy methods like FRAME [77], but limited by the modeling power of generative models at that age, and this principle has not seen much success.
Recently, a method named Transporter [23] has been proposed in the 2D domain, successfully connecting key-point extraction and correspondence establishment in a self-supervised manner, using exactly the aforementioned prin-ciple. Thanks to the strong power of modern image recon-struction networks, this method can extract meaningful key-points from image sequences without using any human an-notation. Transporter is both a useful tool and an elegant formulation that (potentially) mimics how the human vi-sual system extracts keypoints. However, to the best of our knowledge, this methodology has not been translated into the 3D domain. Because the process of 2D feature trans-portation is implemented on regular data formats, such as 2D image grids, it is not applicable to point clouds that allow for non-uniform point spacing. Another reason we believe is that 3D reconstruction from keypoints is a more challenging setting.
As such, we propose the first 3D Transporter in the liter-ature, based upon three core components: hybrid 3D repre-sentation architectures for 3D feature transportation, cross-attention for better keypoint discovery, and an implicit ge-ometry decoder for 3D reconstruction. Our method takes as input two point clouds containing moving objects or object parts (Fig. 1 left panel). Then by watching these two states solely, our 3D Implicit Transporter extracts temporally con-sistent keypoints and a surface occupancy field for each state, in a self-supervised manner (Fig. 1 middle panel). The method reconstructs the shape of the target state by trans-porting explicit feature grids from the initial state accord-ing to the locations of detected keypoints. Via extensive evaluations on the PartNet-Mobility dataset [65] and ITOP dataset [18], we demonstrate significantly improved percep-tion performance in terms of spatiotemporal consistency of keypoints over state-of-the-art counterparts. The qualita-tive results on the Rodent3D[40] dataset also show our key-points consistently capture the rodent’s skeletal structure.
Besides, we also explore how well our self-supervised mid-level representation (3D keypoints) can serve down-stream robotic applications (Fig. 1 right panel). We choose articulated object manipulation, which requires sophisti-cated 3D reasoning about the kinematic structure and part movement, as the benchmark. Existing methods often rely on object-agnostic affordance-based representation [67, 35, 62]. Our 3D keypoint representation is also object-agnostic, but we demonstrate that our approach offers two distinct advantages over these methods: 1) the efficient learning formulation of ours does not involve costly trial-and-error interaction in simulators; 2) we leverage spatio-temporally aligned 3D keypoints to provide a structured understanding of objects, enabling the design of an effective closed-loop manipulation strategy.
To summarize, we have the following contributions:
• We propose the first 3D Implicit Transporter formu-lation that extracts 3D correspondent keypoints from temporal point cloud inputs, using 3D feature grid transportation, attentional keypoint detection, and tar-get shape reconstruction.
• Based on the extracted 3D keypoint representation, we build a closed-loop manipulation strategy and demon-strate it successfully addresses the manipulation of many articulated objects in an object-agnostic setting.
• We extensively benchmark the perception and manip-ulation performance of 3D Implicit Transporter and re-port state-of-the-art results on public benchmarks. 2.