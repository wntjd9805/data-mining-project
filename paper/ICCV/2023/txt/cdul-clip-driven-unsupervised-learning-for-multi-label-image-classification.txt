Abstract
This paper presents a CLIP-based unsupervised learning method for annotation-free multi-label image classification, including three stages: initialization, training, and infer-ence. At the initialization stage, we take full advantage of the powerful CLIP model and propose a novel approach to extend CLIP for multi-label predictions based on global-local image-text similarity aggregation. To be more specific, we split each image into snippets and leverage CLIP to gen-erate the similarity vector for the whole image (global) as well as each snippet (local). Then a similarity aggregator is introduced to leverage the global and local similarity vectors.
Using the aggregated similarity scores as the initial pseudo labels at the training stage, we propose an optimization framework to train the parameters of the classification net-work and refine pseudo labels for unobserved labels. During inference, only the classification network is used to predict the labels of the input image. Extensive experiments show that our method outperforms state-of-the-art unsupervised methods on MS-COCO, PASCAL VOC 2007, PASCAL VOC 2012, and NUS datasets and even achieves comparable re-sults to weakly supervised classification methods. 1.

Introduction
A multi-label classification task aims to predict all the objects within the input image, which is advantageous for various applications, including content-based image retrieval and recommendation systems, surveillance systems, and as-sistive robots, to name a few [9, 8, 6]. However, getting clean and complete multi-label annotations is very challenging and not scalable, especially for large-scale datasets, because an image usually contains multiple labels (Figure 1.a).
To alleviate the annotation burden, weakly supervised learning approaches have been studied [15, 20, 12, 1], in
Figure 1. A comparison of our solution with fully and weakly-supervised multi-label classification. (a) The training dataset im-ages for fully-supervised learning are fully labeled. (b) The training images used in weakly-supervised are partially labeled. (c) Our un-supervised multi-label classification method is annotation-free. (d)
CLIP focuses on one class in the whole image, and the embedding is denoted by blue circle. Some classes are ignored such as "per-son". (e) In our approach, image snippets are mapped separately to the embedded space, where each snippet’s embedding is denoted by squares. Local alignment allows to predict more labels. which only a limited number of objects are labeled on a subset of training images (Figure 1.b). Though less than the fully-labeled case, it still requires intensive manpower and time for annotations.
To go one step further, we consider unsupervised multi-label image classification, leveraging the off-the-shelf vision-language models such as contrastive language-image pre-training (CLIP) [31]. CLIP is trained by matching each input
image to the most relevant text description over 400 million image-text pairs collected from the Internet. It has demon-strated remarkable zero-shot classification performance as a pre-trained model in image-text retrieval [31], video-text re-trieval [27], and single-label image classification [31]. With
CLIP, the encoded visual representations can be directly used for vocabulary categorization without additional training.
However, CLIP is not suitable for multi-label classification, since it is trained only for recognizing a single object per im-age (Figure 1.d). Finding only one global embedding for the whole image may push CLIP to generate a high confidence score for the closest semantic text class, while neglecting other classes. In Figure 1.d, for instance, CLIP predicts class
"horse" with a very high confidence score (0.98), but gives a very low weight to class "person", given the fact that CLIP suffers from excessive polysemy [31].
To address these issues and make full use of CLIP in multi-label classification, this paper presents a CLIP-driven unsupervised learning method (CDUL) for multi-label im-age classification, which includes three stages: initialization, training, and inference. At the initialization stage, we use
CLIP to generate global representation of the whole image and, more importantly, local representations of snippets of the image. A novel aggregation of global and local represen-tations provides high confidence scores for objects on the image. As shown in Figure 1.e, the class "person" receives high confidence score in this case. At the training stage, the confidence scores will be used as the initial values of pseudo labels, with which a self-training procedure is proposed to optimize the parameters of the classification network as well as the pseudo labels. Finally, during inference, only the clas-sification network is used to predict the labels of an image.
The contributions of this paper are listed as follows:
• We propose a novel method for unsupervised multi-label classification training. To the best of our knowl-edge, this is the first work that applies CLIP for unsuper-vised multi-label image classification. The aggregation of global and local alignments generated by CLIP can effectively reflect the multi-label nature of an image, which breaks the impression that CLIP can only be used in single-label classification.
• A gradient-alignment training method is presented, which recursively updates the network parameters and the pseudo labels. By this algorithm, the classifier can be trained to minimize the loss function.
• Extensive experiments show that our method not only outperforms the state-of-the-art unsupervised learning methods, but also achieves comparable performance to weakly supervised learning approaches on four different multi-label datasets. 2.