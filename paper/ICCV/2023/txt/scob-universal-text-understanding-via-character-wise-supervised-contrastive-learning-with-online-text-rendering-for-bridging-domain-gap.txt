Abstract
Inspired by the great success of language model (LM)-based pre-training, recent studies in visual document un-derstanding have explored LM-based pre-training methods for modeling text within document images. Among them, pre-training that reads all text from an image has shown promise, but often exhibits instability and even fails when applied to broader domains, such as those involving both visual documents and scene text images. This is a sub-stantial limitation for real-world scenarios, where the pro-cessing of text image inputs in diverse domains is es-sential. In this paper, we investigate effective pre-training tasks in the broader domains and also propose a novel pre-training method called SCOB that leverages character-wise supervised contrastive learning with online text ren-dering to effectively pre-train document and scene text do-mains by bridging the domain gap. Moreover, SCOB en-ables weakly supervised learning, significantly reducing annotation costs. Extensive benchmarks demonstrate that
SCOB generally improves vanilla pre-training methods and achieves comparable performance to state-of-the-art meth-ods. Our findings suggest that SCOB can be served gener-ally and effectively for read-type pre-training methods. The code will be available at https://github.com/naver-ai/scob. 1.

Introduction
Visually-situated language, which encompasses a mix-ture of texts and visual objects such as documents, tables, infographics, and user interfaces, is now ubiquitous in mod-ern human civilization. Accordingly, automatically reading and understanding visually-situated language with machine learning systems is considered commercially valuable and challenging. Considering the usability and training conve-nience for machine learning systems, Visual Document Un-†Equal contribution
∗Corresponding author and Project leader 2This work was done while the author was at NAVER Cloud AI
Figure 1. Our proposed SCOB is applicable to pre-training tasks of generative text understanding models, including text-read and
OCR-read. (Top) Our renderer generates images at the word-level with diverse fonts and sizes. Shapes of “A”s and “R”s are little different respectively in the real image (blue box), but their shapes vary significantly in the rendered image (red box). (Bottom) By applying the character-wise supervised contrastive loss, “A”s and
“R”s are clustered respectively and the clusters of “A” and “R” push away each other in the embedding space. derstanding (VDU) and Scene Text Understanding (STU) tasks have been separately studied for visually-situated lan-guage. VDU mainly handles visually scanned or binarized document images, whereas STU takes images in real-world and dynamic environments as input, as shown in Figure 2.
In the context of VDU, Donut [32] has been proposed as a sequence generation model, which pre-trains a text-read task of reading all texts in raster scan order from an im-age, as illustrated in Figure 2. Meanwhile, Pix2seq [7] is an image-to-sequence model that extends to the object detec-tion task by gridding images and using coordinate tokens on the grid. These recent studies [7, 32, 42] suggest that prompt control in a sequence generation approach can successfully
[32], Dessurt including Donut
The proposed SCOB is applicable to a broad range of existing Transformer-based generative text understand-[12], and ing models,
Pix2Struct [36]. From a generalized perspective, these image-to-text models can be interpreted as the same frame-work, which is a transformer-based encoder-decoder model with a “read” pre-training strategy (e.g., text-read and OCR-read). We refer to this framework as the Universal Text
Understanding (W) framework in this work. We conduct extensive experiments using the W framework on eleven benchmarks spanning both VDU and STU domains to observe the characteristics and effects of respective pre-training strategies with SCOB. Our experimental results and analysis demonstrate the efficacy and versatility of SCOB improving the overall model performance. We summarize our main contributions as follows:
• This paper investigates the effects of text-read and
OCR-read pre-training on a total of eleven tasks, in-cluding those in the VDU and STU domains.
• We propose a novel pre-training method SCOB that utilizes character-wise contrastive learning with online text rendering to effectively bridges the domain gap between VDU and STU domains.
• SCOB enables weakly supervised OCR pre-training, significantly reducing annotation costs by using only text annotations.
• Experimental results show that read-based pre-training for table reconstruction achieves state-of-the-art per-formance, and our proposed SCOB generally enhances the performance of read-based pre-training on various text-related downstream tasks. 2.