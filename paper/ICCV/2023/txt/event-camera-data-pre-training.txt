Abstract
This paper proposes a pre-trained neural network for handling event camera data. Our model is a self-supervised learning framework, and uses paired event camera data and natural RGB images for training. Our method con-tains three modules connected in a sequence: i) a family of event data augmentations, generating meaningful event images for self-supervised training; ii) a conditional mask-ing strategy to sample informative event patches from event images, encouraging our model to capture the spatial lay-out of a scene and accelerating training; iii) a contrastive learning approach, enforcing the similarity of embeddings between matching event images, and between paired event and RGB images. An embedding projection loss is proposed to avoid the model collapse when enforcing the event image embedding similarities. A probability distribution align-ment loss is proposed to encourage the event image to be consistent with its paired RGB image in the feature space.
Transfer learning performance on downstream tasks shows the superiority of our method over state-of-the-art meth-ods. For example, we achieve top-1 accuracy at 64.83% on the N-ImageNet dataset. Our code is available at https:
// github.com/ Yan98/ Event-Camera-Data-Pre-training. 1.

Introduction
An event camera asynchronously captures the time, lo-cation, and polarity of pixel-wise changes in brightness as a sequence of events. Event cameras are widely used in many applications, e.g., recognition [22], detection [30, 27], seg-mentation [3], optical flow estimation [43], and SLAM [40].
Compared with conventional RGB cameras which record all pixel intensities at a fixed frame rate, event cameras en-joy a high dynamic range and temporal resolution, and are robust to lighting changes and motion blur [22, 36, 25].
This paper studies the problem of event camera data pre-training. Our model is pre-trained in a self-supervised man-† Corresponding author. ∗ Equal contribution.
Figure 1: Comparison of our methods and state-of-the-art meth-ods on N-ImageNet dataset [22]. The Blue cycles and red squares separately denote the self-supervised and supervised pre-training methods. We show top-1 accuracy (%), i. e., acc@1, with respect to the number of model parameters (M). We include the publica-tion year of each method in the brackets beside the method names. ner, only using paired event data and RGB images for train-ing. One can simply transfer our pre-trained model for di-verse downstream tasks.
In self-supervised learning (SSL), significant progress has been made in pre-training with RGB images [19, 2, 10].
However, it is non-trivial to replicate the success on event camera data, as there is a domain gap between RGB images and event data. An RGB image records all pixel intensities of a scene and is spatially dense, while the event data only records scene changes and is spatially sparse.
For network training in the SSL framework, image aug-mentations (e.g., Gaussian Blur, ColorJitter, RandomRe-sizedCrop) are one of the most important parts. The sparse event camera data can be commonly represented as an event image [22]. One may directly and wrongly perform these augmentations on event images, e.g., blurring a binary event image (0/1 valued pixels) generates a meaningless event im-age. In contrast, we study how to perform event data aug-mentations before converting to an event image.
We formulate our learning problem as a contrastive learning task. Taking event images as inputs, one may di-rectly perform a random masking strategy to sample a fixed
number of event patches for encouraging the model to cap-ture the spatial layout and accelerating training. However, an event image is spatially sparse, and random masking would generate non-informative patches, leading to train-ing instability. To mitigate this problem, we propose a con-ditional masking strategy to sample informative patches.
With event patches, we are able to learn discriminative event embeddings, i. e., pulling together embeddings from similar event images while pushing away embeddings from dissimilar ones. Surprisingly, we find that simply perform-ing metric learning in the event embedding space leads to model collapse, producing over-similar embeddings. The reason comes from the spatial sparsity of event images. To solve this problem, we find that embeddings from paired
RGB images can be used as a regularizer, and we propose an embedding projection loss to solve the collapse.
With paired event data and RGB images, we also aim to pull together embeddings from matched pairs. This is moti-vated by the fact that many well pre-trained RGB networks are available, and an event image is less informative than its paired RGB image. Therefore, the RGB network serves as a teacher for our event network, and we propose a probability distribution alignment loss for the learning.
Our contributions are summarized as follows:
• A self-supervised framework for event camera data pre-training. The pre-trained model can be transferred to diverse downstream tasks;
• A family of event data augmentations, generating meaningful event images;
• A conditional masking strategy, sampling informative event patches for network training;
• An embedding projection loss, using paired RGB embeddings to regularize event embeddings to avoid model collapse;
• A probability distribution alignment loss for aligning embeddings from the paired event and RGB images.
• We achieve state-of-the-art performance in standard event benchmark datasets (e.g., Fig. 1). 2.