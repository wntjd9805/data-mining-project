Abstract
This paper strives to solve complex video question an-swering (VideoQA) which features long video containing multiple objects and events at different time. To tackle the challenge, we highlight the importance of identify-ing question-critical temporal moments and spatial objects from the vast amount of video content. Towards this, we propose a Spatio-Temporal Rationalization (STR), a differ-entiable selection module that adaptively collects question-critical moments and objects using cross-modal interac-tion. The discovered video moments and objects are then served as grounded rationales to support answer reasoning.
Based on STR, we further propose TranSTR, a Transformer-style neural network architecture that takes STR as the core and additionally underscores a novel answer interaction mechanism to coordinate STR for answer decoding. Ex-periments on four datasets show that TranSTR achieves new state-of-the-art (SoTA). Especially, on NExT-QA and
Causal-VidQA which feature complex VideoQA, it signifi-cantly surpasses the previous SoTA by 5.8% and 6.8%, re-spectively. We then conduct extensive studies to verify the importance of STR as well as the proposed answer inter-action mechanism. With the success of TranSTR and our comprehensive analysis, we hope this work can spark more future efforts in complex VideoQA. Code will be released at https://github.com/yl3800/TranSTR. 1.

Introduction
The great success of self-supervised pretraining with powerful transformer-style architectures [5, 12, 17, 9, 39, 42] has significantly boosted the performance of answering simple questions (e.g., “what is the man doing”) on short videos (e.g., 3∼15s) [13, 37]. The advances thus point to-wards complex video question answering (VideoQA), that features long video containing multiple objects and events
[34, 20, 45]. Compared with simple VideoQA, complex
VideoQA poses several unique challenges:
*Corresponding author. (a) A example of long video (52s) with multiple objects, the question-related frames and objects are located to support the rea-soning. (b) Accuracy by video length. (c) Accuracy by object number.
Figure 1: (a) Illustration of complex VideoQA, in which the videos are longer and the questions involve multiple ob-jects and events at different time. (b) Prediction accuracy grouped by video length. We first sort all samples by video length, then select top x% to calculate accuracy. (c) Accu-racy grouped by whether the video has more than 5 objects.
All results are reported on NExT-QA test set [34]. Figure (b) and (c) show that our method TranSTR performs much better than the previous SoTAs for question answering of long videos with multiple objects. 1) Longer videos with multiple objects interacting differently at different times. The long video and rich visual content indispensably bring more background scenes that include massive question-irrelevant video moments and objects. For example, to answer the question in Fig. 1a, only the interaction between object “person” and “bike” on the last three frames encloses the answer information, leaving the massive rest as background. These backgrounds, if not
filtered properly, will overwhelm the critical scene and in-terfere with answer prediction. 2) Harder negative answer as distractors. Negative answers in complex VideoQA are typically tailored for each video instance. Due to the mas-sive video content, the vast question-irrelevant scene pro-vides an ideal foundation to build a hard negative candidate as distractor. The hard negatives are very similar to the cor-rect answer but correspond to a different video moment or object. For example, the answer candidate “A.ride the bike” of Fig. 1a, though irrelevant to the question, corresponds to a large part of the video. As a result, these distractors can seriously derail the prediction if not properly modeled.
In light of the challenges, current methods (both pre-trained and task-specific architectures) hardly perform well on complex VideoQA. In Figs. 1b and 1c, we use the video length and number of objects1 to indicate the complexity of the video questions. We can see that current methods suf-fer a drastic performance drop when video length increases or more objects are involved. The reason can be provided from two aspects: First, confronting long video and multi-ple objects, pretrained methods suffer from a domain gap.
Because they are typically pretrained with short videos and simple questions, [17, 19] where the answer can be eas-ily captured via a static frame, without fine-grained rea-soning over multiple objects in a long video. While recent task-specific methods exploit object-level representation for fine-grained reasoning [4, 29, 35, 36], they exhibit limited generalization ability, as they handle different videos with only a fixed number of frames and objects, and cannot adapt to lengthy and varied visual content, which rigidity under-mines their adaptability to a wide range of video content.
Second, to model the answer candidate, prevailing designs
[13, 10, 8, 16, 35] append the candiate to the question and treat the formed question-answer sequence as a whole for cross-modal learning. However, this makes the answer can-didate directly interact with the whole video content, which gives rise to a strong spurious correlation between the hard negative candidates (e.g. “A. ride the bike” in Fig. 1a) and the question-irrelevant scenes (e.g. “ riding” scene in first three frames), leading to a false positive prediction.
In this regard, we propose TranSTR, a Transformer-style
VideoQA architecture that coordinates a Spatio-Temporal
Rationalization (STR) with a more reasonable video-text in-teraction pipeline for candidate answer modeling. STR first temporally collects the critical frames from a long video, followed by a spatial selection of objects on the identified frames. By further fusing the selected visual objects and frames via light-weight reasoning module, we derive spatial and temporal rationales that exclusively support answering.
In addition to STR, we circumvent the spurious correlation in the current modeling of answer candidates by formulat-1We acquire the object number using annotation of video relation de-tection dataset [32], which shares same source video as NExT-QA. ing a more reasonable video-text interaction pipeline, where the question and answer candidates are separately (instead of being appended as a whole) fed to the model at differ-ent stages. Specifically, before rationale selection, only the question is interacted with the video to filter out the massive background content while keeping question-critical frames and objects. After that, a transformer-style answer decoder introduces the answer candidates to these critical elements to determine the correct answer. Such a strategy prevents the interaction between the hard negative answers and the massive background scenes, thus enabling our STR to per-form better on complex VideoQA (see TranSTR in Figs. 1b and 1c). It is worth noting that STR and the answering mod-eling are reciprocal. Without STR’s selection, all visual content will still be exposed to answer candidates. With-out our answer modeling, STR could identify the question-irrelevant frame and object as critical. Thus, the success of
TranSTR is attributed to the integration of both.
Our contributions are summarized as follows:
• We analyze the necessity and challenge of complex
VideoQA. To solve the task, we identify the importance of discovering spatio-temporal rationales and preventing spurious correlation in modeling candidate answers.
• We propose TranSTR that features a spatio-temporal ra-tionalization (STR) module together with a more reason-able candidate answer modeling strategy. The answer modeling strategy is independently verified to be effec-tive in boosting other existing VideoQA models.
• We perform extensive experiments and achieve SoTA performance on four popular benchmarks, especially for ones that features complex VideoQA (NExT-QA [34]
+5.8%, CausalVid-QA [20] +6.8%) 2.