Abstract
One of the most important yet rarely studied challenges for supervised face clustering is the large intra-class vari-ance caused by different face attributes such as age, pose, and expression. Images of the same identity but with dif-ferent face attributes usually tend to be clustered into dif-ferent sub-clusters. For the first time, we proposed an attribute hallucination framework named CLIP-Cluster to address this issue, which first hallucinates multiple repre-sentations for different attributes with the powerful CLIP model and then pools them by learning neighbor-adaptive attention. Specifically, CLIP-Cluster first introduces a text-driven attribute hallucination module, which allows one to use natural language as the interface to hallucinate novel attributes for a given face image based on the well-aligned image-language CLIP space. Furthermore, we develop a neighbor-aware proxy generator that fuses the features de-scribing various attributes into a proxy feature to build a bridge among different sub-clusters and reduce the intra-class variance. The proxy feature is generated by adaptively attending to the hallucinated visual features and the source one based on the local neighbor information. On this ba-sis, a graph built with the proxy representations is used for subsequent clustering operations. Extensive experiments show our proposed approach outperforms state-of-the-art face clustering methods with high inference efficiency. 1.

Introduction
Recent years have witnessed the remarkable success of face clustering technology [5, 21, 45, 47, 49, 52, 63], due to the progress in deep learning frameworks [17, 40, 41] and more available large-scale training data [4, 13, 24, 64].
However, among the existing research, the influences of dif-ferent face attributes on clustering have been poorly inves-∗Corresponding author.
Figure 1. (a) shows paired hard examples, where aging brings sig-nificant appearance changes, and it is difficult for the network to (b) visualizes the CLIP-aggregate them into the same cluster. guided text-driven face attribute hallucination, which is the core idea of the proposed CLIP-Cluster. tigated. Since facial appearance changes dramatically un-der different factors like age, pose, and expression [14, 31], how to minimize the effects of these facial variations for more compact intra-cluster face embedding is still an open challenge for face clustering.
Face clustering has received thorough attention in the computer vision research area. More recently, the super-vised face clustering methods are extensively studied [5, 27, 36, 47, 51, 57] and have achieved significant perfor-mance gain. While they put more attention on how to es-timate vertice confidence [12, 51], construct neighbor link-age [45, 47], or mine graph structure [36, 53], few works systematically study the impact of intra-cluster variance on face clustering. Intra-class face variance is a common issue in both collected face datasets and real-world photos, and its impact on feature discrimination has actually been repeat-edly verified in the face recognition community [22, 44].
Even so, since facial appearance changes dramatically un-der different face attributes including age, pose, and expres-sion, it is still challenging to effectively mitigate the impact of these variances. As the hard example shown in Figure 1 (a), for the same person, aging brings significant appearance changes, and it is difficult for the network to aggregate them into the same cluster.
In this work, we are committed to narrowing the intra-cluster attribute gap through a synthesis-like approach for easier face clustering. One possible solution is to synthesize face images through attribute manipulation, thus face sam-ples are transferred to a uniform attribute space. However, training such networks with mainstream generative proto-types [11, 15] for controllable attribute editing has the fol-lowing limitations: 1) high requirement for large-scale an-notated paired data, 2) high training difficulty for generating realistic images in the pixel space, and 3) additional com-putational cost of extracting features for synthesized images for subsequent face clustering. For these reasons, we choose to generate faces for different attributes directly in the fea-ture space, which is termed attribute hallucination.
In this work, we propose CLIP-Cluster, which leverages the recent powerful language-visual model CLIP [32] for text-driven face attribute hallucination, and opens a brand-new avenue for face clustering. The core idea of the CLIP-Cluster is shown in Figure 1 (b). With CLIP-guided text-driven face attribute hallucination, we can transfer faces to-ward various ages, poses, and expressions. Benefiting from the admirable zero-shot image classification capability of
CLIP, we can get rid of the demand for a large amount of annotated paired data, and turn to more convenient text-based manipulation. Since CLIP maintains a well-aligned language-image embedding space, we directly perform at-tribute transfer in the latent space with identity-preserving supervisions, and use these features for subsequent face clustering.
In this way, we naturally avoid the additional computational cost of synthesis in pixel space and extract-ing features for the generated images. Furthermore, while a face is transferred across various attributes, we design a neighbor-aware proxy generator to fuse them into a proxy feature by learning the neighbor-adaptive attention. This builds a bridge among different sub-clusters and reduces the intra-class variance. With these proxy representations to construct the affinity graph, the subsequent GCN-based edge predictor can perform face clustering in an easier way.
Extensive experiments show that the proposed CLIP-Cluster significantly boosts the face clustering performance on standard partial MS1M from 93.22 to 94.22 pairwise
F-score, and the inference process can be completed effi-ciently within 280s. In summary, the main contributions of this work are as follows:
• We propose a CLIP-guided text-driven face attribute hallucination framework to bridge the large intra-class attribute gap. Therefore, face clustering can be per-formed in a more compact embedding space.
• Furthermore, we develop a neighbor-aware proxy gen-erator that fuses the features describing various at-tributes into a proxy by learning the neighbor-adaptive attention to reduce the intra-class variance.
• The proposed CLIP-Cluster improves FP on partial
MS1M to 94.22 within 280s, which outperforms state-of-the-art methods with high inference efficiency. 2.