Abstract
Semantic segmentation has made significant progress in recent years thanks to deep neural networks, but the com-mon objective of generating a single segmentation output that accurately matches the image’s content may not be suitable for safety-critical domains such as medical diag-nostics and autonomous driving.
Instead, multiple possi-ble correct segmentation maps may be required to reflect the true distribution of annotation maps.
In this context, stochastic semantic segmentation methods must learn to predict conditional distributions of labels given the image, but this is challenging due to the typically multimodal distri-butions, high-dimensional output spaces, and limited anno-tation data. To address these challenges, we propose a con-ditional categorical diffusion model (CCDM) for seman-tic segmentation based on Denoising Diffusion Probabilis-tic Models. Our model is conditioned to the input image, enabling it to generate multiple segmentation label maps that account for the aleatoric uncertainty arising from di-vergent ground truth annotations. Our experimental results show that CCDM achieves state-of-the-art performance on
LIDC, a stochastic semantic segmentation dataset, and out-performs established baselines on the classical segmenta-tion dataset Cityscapes. 1.

Introduction
Semantic segmentation has significantly progressed in recent years due to powerful deep neural networks. For most methods, the key objective is to generate a single seg-mentation output that accurately matches the image’s con-tent. However, this may not be suitable for safety-critical domains such as medical diagnostics and autonomous driv-ing, as images in these applications often suffer from inher-ent ambiguity or annotations that have differences in opin-ion. In these cases, generating a single coherent segmen-tation may be hopeless to fully describe the set of correct
*Equal contribution
Figure 1: Examples from the LIDC dataset, where expert radiologists were asked to annotate lung nodules. Despite their expertise, they disagree significantly on many cases.
Standard segmentation networks fail to capture these vari-ations, thereby giving a false sense of confidence in model predictions. Our approach learns the distribution of possible labels, allowing us to generate realistic and diverse segmen-tations. labeling.
Instead, multiple possible correct segmentation maps may be required to reflect the true distribution of annota-tions. For instance, Fig. 1 illustrates the task of lung nod-ule segmentation from CT scans where expert annotators provide multiple valid segmentation maps.
In this con-text, stochastic semantic segmentation methods must learn to predict conditional distributions of labels given the im-age. Doing so is challenging, however, as the distribution is typically multimodal, the output space is high-dimensional, and annotation data is limited.
Denoising Diffusion Probabilistic Models (DDPMs) ap-pear well-suited to overcome these challenges. DDPMs have recently drawn strong interest in computer vision as a framework for learning complex distributions in high-dimensional spaces. After achieving state-of-the-art per-formance on image synthesis [13], they have been success-fully extended to solve tasks such as text-to-image gener-ation [41], counterfactual explanation generation [24], in-painting [34], but also image classification [56] and seman-tic segmentation [1, 3, 48] amongst others.
While DDPMs were originally formulated as probabilis-tic models able to learn high-dimensional data distributions of discrete and ordered variables (e.g., RGB pixel values), re-formulations and modifications that allow for categori-cal variables (e.g., labels) [21] are one of the key reasons why DDPMs are being explored in a broad range of com-puter vision tasks [12]. Specifically, the ability to model the spatial distribution of categorical variables is well suited for numerous computer vision tasks, including semantic seg-mentation [6, 8, 10, 14, 16, 17, 27, 31, 33, 54, 55]. Yet until now, segmentation methods using DDPMs have relied on the original discrete and ordered formulation and differ-ent heuristics to yield categorical outputs [1, 3, 48]. Conse-quently, the potential advantages of adopting diffusion mod-els of categorical variables for stochastic image segmenta-tion are still unknown.
In light of the above, we propose a conditional cate-gorical diffusion model (CCDM) for semantic segmenta-tion based on DDPMs, which models both the observed and the latent variables as categorical distributions. This enables the model to explicitly generate labels maps of dis-crete, unordered variables, thereby circumventing the need for switching between continuous and discrete domains, as in previous methods. The model is conditioned to the input image, making it possible to generate multiple segmentation label maps that account for the aleatoric uncertainty arising from image ambiguity. We show experimentally that our approach achieves state-of-the-art performance on LIDC, a stochastic semantic segmentation dataset, according to sev-eral performance measures. Moreover, when applied to the classical segmentation dataset Cityscapes, our method pro-vides competitive results, outperforming established base-lines.
In summary, our main contributions are the following:
• We propose a conditional categorical diffusion model capable of learning the label distribution given an in-put image that can be used to produce diverse segmen-tation samples that capture aleatoric uncertainty.
• For the task of learning a multi-rater semantic segmen-tation label distribution, our method achieves state-of-the-art performance on LIDC, being the first diffusion-based approach proposed for this task.
• We report competitive performance on a challenging semantic segmentation task, Cityscapes, outperform-ing several established baselines using a lightweight model that also leverages an off-the-shelf pre-trained feature extractor. 2.