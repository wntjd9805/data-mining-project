Abstract
Existing federated learning (FL) approaches are de-ployed under the unrealistic closed-set setting, with both training and testing classes belong to the same set, which makes the global model fail to identify the unseen classes as ‘unknown’. To this end, we aim to study a novel prob-lem of federated open-set recognition (FedOSR), which learns an open-set recognition (OSR) model under feder-ated paradigm such that it classifies seen classes while at the same time detects unknown classes.
In this work, we propose a parameter disentanglement guided federated open-set recognition (FedPD) algorithm to address two core challenges of FedOSR: cross-client inter-set interfer-ence between learning closed-set and open-set knowledge and cross-client intra-set inconsistency by data heterogene-ity. The proposed FedPD framework mainly leverages two modules, i.e., local parameter disentanglement (LPD) and global divide-and-conquer aggregation (GDCA), to first disentangle client OSR model into different subnetworks, then align the corresponding parts cross clients for matched model aggregation. Specifically, on the client side, LPD de-couples an OSR model into a closed-set subnetwork and an open-set subnetwork by the task-related importance, thus preventing inter-set interference. On the server side, GDCA first partitions the two subnetworks into specific and shared parts, and subsequently aligns the corresponding parts through optimal transport to eliminate parameter misalign-ment. Extensive experiments on various datasets demon-strate the superior performance of our proposed method. 1.

Introduction
Deep learning algorithms rely on the availability of large-scale data to achieve remarkable performance. How-ever, in reality, data is scattered across different organiza-*Yixuan Yuan is the corresponding author.
This work was supported by Hong Kong Research Grants Council(RGC)
General Research Fund 1422062214204321, and Innovation and Technol-ogy Commission-Innovation and Technology Fund ITS/100/20.
Figure 1. Parameter disentanglement on FedOSR models and comparison of various aggregation strategies on FedOSR setting. (a) FedAvg simply aggregates multiple OSR models, leading to model collapse; (b) Our FedPD first aligns corresponding close-specific, open-specific and shared parts by optimal transport (OT), then aggregates aligned parameters. The curves inside the network box represent different parameter distribution of each parts. tions and difficult to integrate into a centralized dataset, ow-ing to increasing privacy and ethical concerns, especially for those sensitive data such as location-based services or health information [22]. To break this dilemma, feder-ated learning (FL) [5, 23, 20] provides a privacy-preserving paradigm that allows local clients to collaboratively train a shared global model without data sharing.
Although FL has recently achieved promising progress, existing FL works [23, 20, 5] are generally evaluated in a closed-set scenario, where the categories of training and testing samples are identical. The closed-set setting is irra-tional since unknown classes may appear at the test time and would be classified into known classes. This problem seri-ously impedes the deployment of FL models in many real-world applications due to enormous risk, such as clinical diagnosis and autonomous driving. Current open-set recog-nition (OSR) methods [3, 43, 2, 6] attempt to improve the 1
ability of models in recognizing unknown classes, but they are designed for the centralized setting. In this work, we represent the first effort to formulate a challenging and un-explored problem of Federated Open Set Recognition (Fe-dOSR). FedOSR aims to unite multiple distributed clients to learn a global model and reduce privacy as well as se-curity risk, which not only exactly classifies known classes but also recognizes unknown classes in the testing stage.
Directly applying existing OSR methods into the FL set-ting for FedOSR mainly undergoes two troublesome chal-lenges. The first challenge lies in the cross-client inter-set interference between learning closed-set and open-set knowledge. According to the previous study [34], the par-tial parameters of a client model are in charge of learn-ing knowledge of known classes, and the rest are related to open set. The known classes-related parameters of a client is probably polluted by the open set-related param-eters from other clients after server communication, lead-ing to the performance degradation on closed set. Similarly, open set-related parameters of a client are also affected by closed-set knowledge of other clients. In this situation, a un-known samples would be possibly misclassified into known classes. The second one is cross-client intra-set inconsis-tency by data heterogeneity. Even though we aggregate cor-responding closed-related parameters of OSR models from different clients, these parameters are still misaligned due to the permutation invariance property of neural networks and data heterogeneity [37]. Aggregation of local client param-eters directly at the server can result in inconsistent models among the clients, leading to significant divergence of client models. This inconsistency issue can cause slow and un-stable convergence [19], ultimately resulting in sub-optimal performance of the entire FL system [17, 37].
To achieve FedOSR, we conquer these intractable chal-lenges from a new perspective, i.e., parameter disentangle-ment. Based on the lottery ticket hypothesis [7, 11, 36], we divide parameters of a client model into a closed-set subnet-work and an open-set subnetwork. These two subnetworks have their own specific parameters, which are only related to known classes and unknown classes respectively. Mean-while, they also share partial parameters since known and unknown samples might have some similar patterns [35].
The parameter disentanglement of client models can pre-serve the high performance of closed set by reducing the in-terference from open-set subnetworks. As shown in Fig. 1, the closed-set subnetworks and the open-set subnetworks of different client models distribute in different positions with some overlaps. Directly aggregating all client OSR models by FedAvg [23] on the server side may encounter the parameter misalignment problem and lead to model col-lapse as shown in Fig. 1 (a). These destroyed parameters are transmitted to clients and slow down the convergence of the federated system due to bad model initialization to the next training step. Therefore, aligning these subnetworks before model aggregation is a crucial step to solve the in-consistency problem of parameter distributions.
To tackle these challenges in FedOSR, we propose a novel parameter disentanglement guided federated OSR (FedPD) algorithm in this paper, which effectively ad-dresses the local parameter misalignment problem occurred on the global model aggregation. Specifically, we design a local parameter disentanglement strategy (LPD) to firstly decouple an OSR model into two subnetworks: an open-set subnetwork and a closed-set subnetwork by task-related metrics. To overcome the parameter misalignment caused by simply parameter averaging on whole client OSR mod-els, we propose a global divide-and-conquer aggregation (GDCA) method to firstly divide two subnetworks into spe-cific parts and shared parts, then align corresponding pa-rameter components by optimal transport [13, 30] and ag-gregate them. As shown in Fig. 1 (b), our FedPD enables reasonable model aggregation and reliable global model to boost federated training.
The major contribution of this paper are summarized as follows:
• We address a practical FL problem, namely Federated
Open-Set Recognition (FedOSR). To the best of our knowledge, this is the first work to improve the ability of detecting novel category for federated models.
• We propose a novel Parameter Disentanglement guided Federated algorithm (FedPD) to solve pa-rameter misalignment problem in FedOSR.
• On the client side, we introduce the Local Parame-ter Disentanglement (LPD) approach, which leverages task-related importance on model parameters to decou-ple the local OSR model into a closed-set subnetwork and an open-set subnetwork.
• On the server side, we design a Global Divide-and-Conquer Aggregation (GDCA) strategy to partition the two subnetworks into specific and shared parts, align the corresponding parts via optimal transport, and subsequently fuse them to alleviate the misalign-ment problem in FedOSR. 2.