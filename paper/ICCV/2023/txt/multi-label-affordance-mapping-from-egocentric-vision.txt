Abstract
Accurate affordance detection and segmentation with pixel precision is an important piece in many complex sys-tems based on interactions, such as robots and assitive de-vices. We present a new approach to affordance percep-tion which enables accurate multi-label segmentation. Our approach can be used to automatically extract grounded affordances from first person videos of interactions using a 3D map of the environment providing pixel level preci-sion for the affordance location. We use this method to build the largest and most complete dataset on affordances based on the EPIC-Kitchen dataset, EPIC-Aff, which pro-vides interaction-grounded, multi-label, metric and spatial affordance annotations. Then, we propose a new approach to affordance segmentation based on multi-label detection which enables multiple affordances to co-exists in the same space, for example if they are associated with the same ob-ject. We present several strategies of multi-label detection using several segmentation architectures. The experimental results highlight the importance of the multi-label detection.
Finally, we show how our metric representation can be ex-ploited for build a map of interaction hotspots in spatial action-centric zones and use that representation to perform a task-oriented navigation. 1.

Introduction
When humans repeatedly interact in a close environ-ment, we associate a set of affordable actions with a certain distribution of objects in a physical space. For example, we associate a pan on a stove with cooking, but the same pan on the sink with washing. A joined spatial-semantic understanding contains powerful insights to understand hu-man behaviour. This requires a close combination of per-ception, mapping and navigation algorithms; with poten-tial applications in augmented reality systems [60, 61], but also guiding a robot [19, 36] or assistive devices [68]. In the last years, the ability of deep learning models to extract high-level representations has improved the perception of autonomous agents, while egocentric vision offers a pow-Figure 1. From a sequence of egocentric observations, our agent creates a spatial-metric multi-label representation of the affor-dances, enabling a task-oriented navigation. erful viewpoint for modelling human-object interaction un-derstanding. Recent advances include anticipating future actions [23, 26, 1], model the hands-object manipulation
[22, 77, 27, 16], detect the change in an object state [2], identify interaction hotspots [20, 54] or create topological maps [55]. Despite the fast movements of a headset camera, egocentric perception has also contributed to the mapping and planning phases: localising the agent in a known 3D map [43], performing visual navigation [56, 62] or building third-person (allocentric) maps [7, 47].
Gibson’s perception theory presents affordances as the potential actions that the environment offers to the agent based on its motor capabilities [25]. For example, the per-son can afford taking a glass, but the affordances of a soup in a pan can be mixing, emptying, scooping and pouring simultaneously. This multiplicity models better complex dynamic environments and opens the door to multi-agent collaboration with task synchronization. Although some authors have focused on more complex affordance models
[50, 75], affordance perception is typically defined as a clas-sification problem. Some authors have focused instead on grounded affordances [20], which provide a more flexible setup and are truly associated with motor capabilities, show-ing improvements in action anticipation [46]. However, most learning approaches in affordance perception consider the problem ungrounded to the agent interaction with the object, requiring previous annotations of each affordance occurrence [53, 58, 48, 19, 8, 57]. While ungrounded meth-ods have the advantage of providing pixel-wise precision, which we call metric understanding of the scene, many grounded approaches rely on full image classification losing any metric meaning. In this paper, we propose a grounded approach with pixel-wise precision, which enables detailed metric understanding while maintaining the flexibility of grounded methods and that can be used as prior information for more complex affordance models [75]. Close to our pro-posal is the work of Nagarajan et al. [54], which presented a grounded approach for extracting interaction hotspots by directly observing videos. Similar to other previous works, the hotspots are modelled as a single available affordance.
Instead, we propose to consider the multiplicity of affor-dances for a single object or spatial zone through multi-label pixel-wise predictions.
We build a pipeline to automatically collect multi-label pixel-wise annotations from real-world interactions using a temporal, spatial and semantic representation of the en-vironment. We use this method with the EPIC Kitchens videos [15] to build a dataset of grounded affordances (EPIC-Aff), which to the author’s knowledge, constitute the largest dataset in affordance segmentation up to date. We then adapt several segmentation architectures to the multi-label paradigm to extract more diverse information from the scene based on the assumption that the same object may have multiple affordances available. Using a map-ping approach we extract the multi-label affordance seg-mentation to build a map that spatially links activity-centric zones as shows Figure 1, allowing a metric representation of the environmental affordances and goal-directed naviga-tion tasks. Finally, we perform a quantitative evaluation of the extension from common architectures to the multi-label paradigm and we show mapping and planning applications of our approach, that can be used for assistive devices or in robotic scenarios. 2.