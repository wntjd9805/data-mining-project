Abstract
This paper addresses data augmentation for action seg-mentation. Our key novelty is that we augment the origi-nal training videos in the deep feature space, not in the vi-sual spatiotemporal domain as done by previous work. For augmentation, we modify original deep features of video frames such that the resulting embeddings fall closer to the class decision boundaries. Also, we edit action sequences transcripts) by in-of the original training videos (a.k.a. serting, deleting, and replacing actions such that the result-ing transcripts are close in edit distance to the ground-truth ones. For our data augmentation we resort to reinforce-ment learning, instead of more common supervised learn-ing, since we do not have access to reliable oracles which would provide supervision about the optimal data modifica-tions in the deep feature space. For modifying frame embed-dings, we use a meta-model formulated as a Markov Game with multiple self-interested agents. Also, new transcripts are generated using a fast, parameter-free Monte Carlo tree search. Our experiments show that the proposed data aug-mentation of the Breakfast, GTEA, and 50Salads datasets leads to significant performance gains of several state of the art action segmenters. 1.

Introduction
This paper presents a new data augmentation frame-work for fully supervised action segmentation of untrimmed videos. Action segmentation is a basic vision problem.
Despite recent tremendous advances in terms of new ac-tion segmenters and learning strategies, there is relatively slow progress in increasing the size of existing benchmark datasets. In comparison with peer datasets for action recog-nition or image classification, available benchmarks for ac-tion segmentation are significantly smaller. This presents challenges in training of recent action segmenters which show tendency to overfit on small datasets [11, 43, 40].
However, compiling large datasets is difficult, due to, in part, high costs of manual annotation of action segments.
We propose to augment existing datasets with newly gen-erated video sequences, such that the resulting data aug-mentation enables more robust training and hence improves performance of action segmenters. Our approach is agnos-tic of a particular model for action segmentation, and ex-pects that the segmenter has been pre-trained on the original training dataset to predict action classes of video frames.
Augmentation of video data has been mostly considered in the spatiotemporal, visual domain [38, 25, 19], where a human expert would heuristically specify the amount and type of data manipulation (e.g., subsampling, cropping, flip-ping of video frames) that are useful in training. While these approaches show great success, it is hard to formalize them in a principled manner. Others learn to generate new videos
[9, 8, 42, 41], but the results are not sufficiently realistic yet, and hence would require domain adaptation if used for data augmentation in action segmentation.
Our key novelty is that we augment the original train-ing videos directly in the deep feature space, unlike most previous work. As shown in Fig. 1 (top left), for augmen-tation, we modify original deep features of video frames at the input, such that the resulting embeddings fall closer to the class decision boundaries. Thus, by construction, we enforce that the augmented features be more chal-lenging for learning, and in this way subsequently enable more robust training of the action segmenter. In addition,
Fig. 1 (top right) illustrates that we also edit action se-quences of the original training videos (a.k.a. transcripts) by inserting, deleting, and replacing actions, such that the resulting transcripts are close in edit distance to the ground-truth ones. Since the generated transcripts are kept similar to the originals, they are expected to be meaningful (i.e., le-gal) and provide a greater variety of legal action sequences than seen in the original training set. This is especially important for those application domains where some tran-scripts of interest are naturally rare and hence underrepre-sented in the original training dataset.
For the proposed augmentation of frame embeddings in the deep feature space, we specify a deep residual meta-model, as shown in Fig. 1 (bottom left). The meta-model takes deep features of frames at the input and predicts an optimal amount of feature modifications â€“ i.e., offset fea-ble training of our meta-model by optimizing the expected reward over a policy of feature modifications, in compari-son with the aforementioned minimization of the unreliable loss of the pretrained action segmenter on particular feature modifications. Third, RL is known to be very effective for problems with a large, continuous, output space, as is our case of predicting offset features in the deep feature space.
Finally, RL is known to successfully address non-stationary environments with the distribution shift between training and test sets [37], which exactly characterizes our problem statement where data augmentation is aimed at bridging the distribution shift.
Within the RL framework, for modifying frame embed-dings, we formulate the meta-model as a Deep Actor-Critic
Network for learning policies of two self-interested agents in a Markov Game. Also, for generating new transcripts, we use a fast, parameter-free Monte Carlo tree search.
We call our approach Markov Game Video Augmentation (MVGA).
Our experimental evaluation shows significant perfor-mance gains of recent convolutional and transformer-based action segmenters when our MVGA is used to augment the Breakfast, GTEA, and 50Salads datasets. Interestingly,
MVGA enables the convolutional model MS-TCN [11, 27] achieve close performance to that of the significantly more complex (and more recent) ASFormer [43].
In the following, Sec. 2 reviews closely related work,
Sec. 3 gives an overview of MVGA, Sec. 4 specifies our transcript augmentation, Sec. 5 formalizes our frame-feature augmentation, and Sec. 6 presents our results. 2.