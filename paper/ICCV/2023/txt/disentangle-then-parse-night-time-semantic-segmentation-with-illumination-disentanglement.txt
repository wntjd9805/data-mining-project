Abstract
Most prior semantic segmentation methods have been developed for day-time scenes, while typically underper-forming in night-time scenes due to insufﬁcient and com-plicated lighting conditions.
In this work, we tackle this challenge by proposing a novel night-time semantic seg-mentation paradigm, i.e., disentangle then parse (DTP).
DTP explicitly disentangles night-time images into light-invariant reﬂectance and light-speciﬁc illumination compo-nents and then recognizes semantics based on their adaptive fusion. Concretely, the proposed DTP comprises two key components: 1) Instead of processing lighting-entangled features as in prior works, our Semantic-Oriented Disen-tanglement (SOD) framework enables the extraction of re-ﬂectance component without being impeded by lighting, al-lowing the network to consistently recognize the seman-tics under cover of varying and complicated lighting con-ditions. 2) Based on the observation that the illumination component can serve as a cue for some semantically con-fused regions, we further introduce an Illumination-Aware
Parser (IAParser) to explicitly learn the correlation be-tween semantics and lighting, and aggregate the illumi-nation features to yield more precise predictions. Exten-sive experiments on the night-time segmentation task with various settings demonstrate that DTP signiﬁcantly outper-forms state-of-the-art methods. Furthermore, with negligi-ble additional parameters, DTP can be directly used to ben-eﬁt existing day-time methods for night-time segmentation.
Code and dataset are available at https://github. com/w1oves/DTP.git. 1.

Introduction
Most existing semantic segmentation methods [27, 12, 45, 55, 19, 53, 13, 40, 49, 37, 7] are developed for day-time scenes, which have sufﬁcient and uniform lighting.
* indicates equal contributions.
† Corresponding authors.
Figure 1: Illustration of the main idea. (a) In the night-time scenes, the entanglement of content and complicated light-ing lead to confused semantics. (b) The proposed DTP ﬁrst disentangles the image and then parses based on the light-invariant and light-speciﬁc components. The blue map in the top left corner is a heatmap indicating the lighting inten-sity. (c) Feature space of road (green) and sidewalk (blue) on the validation set visualized by t-SNE [44] demonstrates that the light-invariant component leads to more discrimi-native representations.
However, in practical application, visual systems are often required to work under insufﬁcient and complicated light-ing conditions nearly half the time (i.e., working under the night-time condition), where existing day-time methods may encounter performance drops due to the discrepancy in lighting. Therefore, developing a night-time segmenta-tion method, which accounts for the unique characteristics of night-time scenes, is crucial in training a network with stable performance for full-time segmentation.
To achieve similar performance for night-time scenes as for day-time scenes, previous methods [42, 36, 16, 24, 8] have adopted unsupervised domain adaptation techniques to transfer knowledge from labeled day-time domain to unla-beled night-time domain. However, this approach is chal-lenging due to the lack of corresponding labels at night, resulting in limited improvement of segmentation perfor-mance. To address this issue, Tan et al. recently propose
NightCity [43], a large-scale night-time dataset that aims at solving the problem of inadequate training data for night-time segmentation. Several methods have been proposed based on this benchmark, such as EGNet [43] and NightLab
[17], which achieve substantial improvement in night-time scenes compared to pure day-time methods. However, they typically parse scenes on the lighting-entangled representa-tions, rendering them unsuitable for the challenging lighting conditions in night-time scenes.
Rethinking the challenge of night-time segmentation, we have identiﬁed that the main issue lies in insufﬁcient and complex lighting conditions. As shown in Fig. 1 (a), the entanglement of content and complex lighting makes the semantics confused, resulting in blurred object boundaries.
Therefore, an intuitive idea is whether we can disentan-gle the features of the content and lighting. From this per-spective, we propose to disentangle the light-invariant and light-speciﬁc components from a night-time image. The light-invariant component allows us to obtain more discrim-inative features related to the content itself. Fig. 1 (c) illus-trates an example of our idea. After separating the light-speciﬁc component from the nigh-time image, the feature distributions of the sidewalk and road become more com-pact and discriminative. Additionally, it is observed that the light-speciﬁc component can serve as a cue for some semantically confused regions. As illustrated in Fig. 1 (b), the artiﬁcial light usually appears in some special categories (e.g., trafﬁc light and car), which exhibit larger intensity in the light-speciﬁc component.
Based on the above observation, we intend to estab-lish a paradigm that can harness the advantages of light-invariant and light-speciﬁc components. However, disen-tangling these two components is quite challenging due to the lack of well-deﬁned ground truth. Although the
Retinex [23] theory points out that the light-speciﬁc com-ponent (i.e., illumination) and light-invariant component (i.e., reﬂectance) can be organized together via a simple dot product and the light-speciﬁc component should satisfy the piece-wise smooth constraint, these weak constraints are inadequate in complex lighting conditions. To tackle this challenge, this work employs semantics as an additional constraint for the disentanglement tasks. Speciﬁcally, we design a semantic-oriented disentanglement (SOD) frame-work, which generates training data with ground truth by combining the light-invariant and light-speciﬁc components disentangled from different night-time images. Such a train-ing framework allows us to establish semantic consistent constraints for two images sharing the same light-invariant component but owning different light-speciﬁc components.
Moreover, to make full use of the light-speciﬁc compo-nent, we propose an illumination-aware parser (IAParser), which explicitly learns the correlation between illumination and semantics, thereby improving the performance of some challenging categories.
Last but not least, we have noticed that NightCity [43, 17] contains numerous mislabeled pixels, which can poten-tially hinder the research progress of the night-time segmen-tation task. In this work, we build a NightCity-ﬁne dataset by elaborately correcting the erroneous labels in NightCity.
Supported by this reﬁned dataset, the segmentation model can be evaluated more validly and achieves improved per-formance. Through extensive experiments, we demonstrate the superiority of the proposed method and the reliability of the reﬁned dataset. In a nutshell, the main contributions of this paper are as follows:
• We propose a novel night-time semantic segmentation paradigm, i.e. disentangle then parse (DTP), to tackle the challenge of insufﬁcient and complicated light-ing. With negligible additional parameters, DTP can be readily applied to enhance existing day-time meth-ods for night-time segmentation.
• We devise a semantic-oriented disentanglement frame-work (SOD), which disentangles images into light-invariant reﬂectance and light-speciﬁc illumination components with the aid of semantic constraints, allowing the network to extract consistent features under varying lighting. Moreover, we present an illumination-aware parser (IAParser) that harnesses the illumination component to serve as a cue for more precise predictions.
• We introduce the NightCity-ﬁne dataset by reﬁning the largest night-time segmentation dataset NightCity. To-gether with DTP, NightCity-ﬁne presents a more ro-bust benchmark for night-time segmentation. 2.