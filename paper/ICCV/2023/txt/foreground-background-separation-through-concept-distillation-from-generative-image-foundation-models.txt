Abstract
Curating datasets for object segmentation is a difficult task. With the advent of large-scale pre-trained genera-tive models, conditional image generation has been given a significant boost in result quality and ease of use. In this paper, we present a novel method that enables the genera-tion of general foreground-background segmentation mod-els from simple textual descriptions, without requiring seg-mentation labels. We leverage and explore pre-trained la-tent diffusion models, to automatically generate weak seg-mentation masks for concepts and objects. The masks are then used to fine-tune the diffusion model on an inpaint-ing task, which enables fine-grained removal of the object, while at the same time providing a synthetic foreground and background dataset. We demonstrate that using this method beats previous methods in both discriminative and generative performance and closes the gap with fully su-pervised training while requiring no pixel-wise object la-bels. We show results on the task of segmenting four dif-ferent objects (humans, dogs, cars, birds) and a use case scenario in medical image analysis. The code is available at https://github.com/MischaD/fobadiffusion. 1.

Introduction
Supervised pretraining, e.g., with ImageNet[16], has demonstrated reduced training times and boosted perfor-mance. This gave rise to models that could be trained once over large amounts of data before being adapted to spe-cialised tasks, such as image recognition, object detection, image segmentation[52], and medical image analysis[50].
The recent development of self-supervision techniques and their ability to learn without manual labels led to much larger scale training datasets [9] and to the creation of foun-dation models [6]. At present, the use of pre-trained models for a wide range of diverse downstream tasks defines a very active and intriguing area of research.
Large scale foundation models are already established in
Figure 1. High level overview of our proposed method: Without needing a single labelled image, our method is able to generate foreground, background, and segmentation masks for any concept that is known to a text-to-image generative network. natural language processing, with most of them being based on the Transformer architecture [57, 17, 7]. A crucial part of this architecture are cross- and self-attention layers, which compute interpretable importance weightings [39, 12].
Diffusion models are based on the U-Net architecture
[45] with additional attention layers [37] to condition on textual prompts. Therefore, we can extract inherently inter-pretable pixel importance scores from conditioning on tex-tual prompts. Furthermore, the reverse diffusion process teaches the U-Net to successively remove noise from im-ages, starting from pure Gaussian noise. In the early steps of this process, where the images resemble pure noise, the texture is non-existent, and the model only learns structures.
Recently, latent diffusion models have emerged as state-of-the-art generative models for the task of text-to-image generation [18, 44, 41]. However, training such models requires a significant amount of CO2-intensive resources and until recently, pre-trained model weights have not been publicly available. Rombach et al. were the first to pub-lished their weights and model architecture [44], which facilitated the development of numerous derived applica-tions [60, 15, 53] and established this model as the foun-dation model for tasks that require generalised representa-tions of concepts in images. State-of-the-art latent diffusion models are able to generate high resolution images of a vast amount of different objects, suggesting that a highly expres-sive latent representation of the data has been learned.
We hypothesize that we can leverage these learned la-tent representations for our own downstream tasks of zero-shot foreground-background generation. Using a genera-tive latent diffusion foundation model, we are able to ex-tract a weak segmentation mask around an arbitrary object by computing the importance maps based on the textual in-put prompts. Weak segmentation masks have been shown to be an effective prior for segmentation models, given that enough training samples are available [34, 40]. We then use these preliminary masks to fine-tune a latent diffusion model on (1) generating new images from this dataset, as well as (2) inpainting regions where the object is not present according to our preliminary masks. The resulting model is able to perform full-image synthesis, as well as foreground, background and mask generation, as summarized in Fig. 1.
A segmentation model trained using these masks can then achieve a level of performance that is close to direct super-vision, despite not requiring manual segmentation masks at any point in the pipeline. This suggests that labour-intensive ground-truth image annotation workflows could become ob-solete in the future, and be replaced by concept distillation from generative foundation models.
Our main contributions are: for
• We propose a self-supervised, hyperparameter-free, approach foreground-background segmentation, based on latent diffusion models, capable of synthesizing foreground, back-ground, and segmentation masks. dataset-independent
• We describe a general framework to extract importance scores obtained from pretrained diffusion models and detail how to use them to improve segmentation per-formance.
• We verify the feasibility of our method on a set of four different foreground background segmentation tasks, spanning humans, birds, dogs, and cars and show that our method achieves results close to supervised meth-ods while being trained without direct supervision.
• We experiment with the extension of our method to domain-adapted diffusion models by showing promis-ing results on a medical segmentation task. 2.