Abstract
We present ClothesNet: a large-scale dataset of 3D clothes objects with information-rich annotations. Our dataset consists of around 4400 models covering 11 cat-egories annotated with clothes features, boundary lines, and keypoints.
ClothesNet can be used to facili-tate a variety of computer vision and robot interaction tasks. Using our dataset, we establish benchmark tasks for clothes perception, including classification, bound-ary line segmentation, and keypoint detection, and de-velop simulated clothes environments for robotic interac-tion tasks, including rearranging, folding, hanging, and dressing. We also demonstrate the efficacy of our Clothes-Net
Supplemental materi-als and dataset are available on our project webpage at https://sites.google.com/view/clothesnet. in real-world experiments. 1.

Introduction
Clothes-related activities, such as folding, laundry, and dressing, play an essential role in our everyday lives.
However, achieving autonomous performances of these tasks poses significant challenges in robotics due to the high-dimensional state representation and complex dynam-ics [32, 34, 33]. Directly training robots to learn these skills in real-world scenarios can be costly and unsafe. An alter-native approach is to develop simulated environments with rich assets where robots can master these skills before trans-ferring to real-world scenes.
This learning paradigm often demands large-scale ob-jects with simulation environments for robots to inter-act with, utilizing data-driven approaches. While there is a growing number of large-scale 3D dataset reposito-ries [7, 47, 53, 25], only a limited number offer 3D cloth-ing models. For instance, Deep Fashion3D [54] comprises
* The authors contribute equally to this work.
† Corresponding author
Figure 1. We present ClothesNet consisting of 4400 clothes mesh models covering 11 categories. We annotate ClothesNet with clothes features, boundary lines, and keypoints. To the best of our knowledge, it is the first large-scale dataset with rich annotations for clothes-centric robot vision and manipulation tasks. We also set up the simulation environment for robotic manipulation tasks, including the hanging, folding, rearranging, and dressing. around 2000 3D models reconstructed from real garments across ten categories. SIZER dataset [45] includes approx-imately 2000 scans, including 100 subjects wearing 10 gar-ment classes. However, these scanned 3D models are not suitable for loading into robotic simulations for tasks in-volving substantial deformation due to data representation or mesh quality. CLOTH3D [4] presents a substantial col-lection of synthetic 3D models with clothing. Garment-Nets [9] generates six garment category meshes based on
CLOTH3D dataset. Nonetheless, specific categories, such as socks, masks, hats, and ties, are absent from these efforts. cloth
Regarding cloth simulation, simulation[28, 22, 41] demonstrates strong potentials, providing differentiable operations to calculate the gradient information to enhance the cloth dynamics and mitigate the high-dimensional of state and action space. They differentiable
provide differentiable operations to calculate the gradient information to enhance the cloth dynamics and mitigate the challenges posed by the high-dimensional of state and ac-tion space. With the availability of these cloth simulations, there are increasing research interests in deformable object understanding. However, they either lack the coupling mechanism with articulated rigid bodies or lead to unde-sired penetration between cloth-cloth and cloth-articulated rigid body interations. This issue significantly degrades the quality and accuracy of simulations. Addressing this,
Yu et al. [49] introduce DiffClothAI, a differentiable cloth simulation with intersection-free frictional contact and the differentiable two-way coupling between cloth and articulated bodies.
In this paper, we introduce ClothesNet: a large-scale dataset for clothes with rich annotations tailored for robot vision and manipulation tasks. The dataset contains 4400 3D mesh models from 11 coarse categories annotated with clothes features, edge lines, and keypoints. We design clothes robotic manipulation tasks based on the differen-tiable cloth simulation DiffClothAI. We perform benchmark algorithms for clothes classification, edge line segmenta-tions, and keypoint detections. Finally, we demonstrate the usefulness of our dataset by enabling a dual-arm robot to fold clothes in the real-world experiment.
In summary, we make the following contributions:
• We create ClothesNet, which contains 4400 3D mesh models from 11 coarse categories, annotated with clothes features, boundary lines, and keypoints. To the best of our knowledge, it is the first large-scale dataset with rich annotations for clothes-centric robot vision and manipulation tasks.
• We develop clothes perception tasks and benchmark data-driven methods to demonstrate the usefulness of
ClothesNet, including clothes classification, boundary line segmentation, and keypoint detection.
• We develop clothes manipulation tasks, including fold-ing, hanging, rearranging, and dressing based on a dif-ferentiable cloth simulation DiffClothAI.
• We conduct comprehensive experiments both in sim-ulation and real-world setting to demonstrate the effi-cacy of our ClothesNet. 2.