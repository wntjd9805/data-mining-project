Abstract
While recent developments in text-to-image generative models have led to a suite of high-performing methods ca-pable of producing creative imagery from free-form text, there are several limitations. By analyzing the cross-attention representations of these models, we notice two key issues. First, for text prompts that contain multiple con-cepts, there is a significant amount of pixel-space overlap (i.e., same spatial regions) among pairs of different con-cepts. This eventually leads to the model being unable to distinguish between the two concepts and one of them being ignored in the final generation. Next, while these models attempt to capture all such concepts during the beginning of denoising (e.g., first few steps) as evidenced by cross-attention maps, this knowledge is not retained by the end of denoising (e.g., last few steps). Such loss of knowledge eventually leads to inaccurate generation outputs.
To address these issues, our key innovations include two test-time attention-based loss functions that substan-tially improve the performance of pretrained baseline text-to-image diffusion models. First, our attention segregation loss reduces the cross-attention overlap between attention maps of different concepts in the text prompt, thereby re-ducing the confusion/conflict among various concepts and the eventual capture of all concepts in the generated output.
Next, our attention retention loss explicitly forces text-to-image diffusion models to retain cross-attention informa-tion for all concepts across all denoising time steps, thereby leading to reduced information loss and the preservation of all concepts in the generated output. We conduct extensive experiments with the proposed loss functions on a variety of text prompts and demonstrate they lead to generated images that are significantly semantically closer to the input text when compared to baseline text-to-image diffusion models. 1.

Introduction
The last few years has seen a dramatic rise in the capa-bilities of text-to-image generative models to produce cre-ative image outputs conditioned on free-form text inputs.
While the recent class of pixel [20, 23] and latent [21] dif-fusion models have shown unprecedented image generation results, they have some key limitations. First, as noted in prior work [3, 27, 2], these models do not always produce a semantically accurate image output, consistent with the text prompt. As a consequence, there are numerous cases where not all subjects of the input text prompt are reflected in the model’s generated output. For instance, see Figure 1 where
Stable Diffusion [21] omits ship in the first column, crown in the second column, and salmon in the third column.
To understand the reasons for these issues, we compute and analyze the cross-attention maps produced by these models during each denoising time step. Specifically, as noted in prior work [5], the interaction between the input text and the generated pixels can be captured in attention maps that explicitly use both text features and the spatial image features at the current time step. For instance, see
Figure 2 that shows per-subject-token cross-attention maps, where one can note high activations eventually lead to ex-pected outputs. By analyzing these maps, we posit we can both understand why models such as Stable Diffusion fail (as in Figure 1) as well as propose ways to address the is-Figure 2: Cross-attention maps for cat and bird.
Figure 3: Our proposed method reduces the overlap be-tween various concepts’ attention maps, leading to reduced confusion/conflicts and improved generation results when compared to baseline models. In this example, one can note much less overlap in the high-response regions for bear and turtle with our method when compared to baselines. sues.
Based on our observations of these cross-attention maps, we notice two key issues with existing models such as Sta-ble Diffusion [21] that lead to incorrect generation out-puts. First, in cases that involve multiple subjects in the text prompt, we notice the presence of a significant amount of overlap between each subject’s cross-attention map. Let us consider the example in Figure 3. We compute the av-erage (across all denoising steps) cross-attention maps for bear and turtle and notice there is significant overlap in the regions that correspond to high activations. We conjecture that because both bear and turtle are highly activated in the same pixel regions, the final generated image is unable to distinguish between the two subjects and is able to pick only one of the two. Note that even exciting the regions as done in Attend-Excite [2] does not help to alleviate this issue. We call this issue with existing models as attention overlap.
Our next observation is related to an issue we call at-tention decay of text-to-image diffusion models. Let us consider the result in Figure 4 where we show the cross-attention maps for dog, beach, and umbrella across mul-tiple denoising time steps for the Stable Diffusion model
[21]. One can note that in the beginning of the diffusion
imizing the overlap of high-response regions in the cross-attention maps of all concept pairs. Our key insight here is by explicitly segregating the pixel regions that are highly activated for a pair of concepts, we ensure the model cap-tures knowledge about both concepts, thereby generating both in the final output at the end of the denoising process.
Next, our attention retention loss tackles the attention de-cay issue above by explicitly ensuring information retention across denoising time steps. We realize this by computing a mask for each concept’s cross-attention map from the pre-vious time step and ensuring the highly activated regions in the current time step’s attention map is consistent with this mask. Our key insight here is by retaining highly-activated pixel regions for each subject across the entire denoising process, we explicitly equip the text-to-image model with the ability to retain all relevant knowledge by the end of de-noising, thereby leading to improved generations. We show some results with our proposed losses in Figures 3 and 4.
In Figure 3, our method reduces the overlap between the bear and turtle attention maps, leading to improved genera-tion when compared to baseline Stable Diffusion. Note that our method gives better results when compared to Attend-Excite [2] since this technique only ensures all attention maps are activated but does not explicitly account for any overlap issues that we have identified. Next, in Figure 4, as can be seen from the progression of the attention maps, our method is able to retain high-response regions across the denoising steps, resulting in the final generation capturing all three concepts as opposed to baseline Stable Diffusion.
To summarize, our key contributions are below:
• We identify two key issues with existing text-to-image diffusion models, attention overlap and attention decay, that lead to semantically inaccurate generations like in
Figure 1.
• We propose two new loss functions called attention seg-regation loss and attention retention loss to explicitly ad-dress the above issues. These losses can be directly used during the test-time denoising process without requiring any model retraining.
• The attention segregation loss minimizes the overlap be-tween every concept pair’s cross-attention maps whereas the attention retention loss ensures information for each concept is retained across all the denoising steps, leading to substantially improved generations when compared to baseline diffusion models without these losses. We con-duct extensive qualitative and quantitative evaluations to establish our method’s impact when compared to several baseline models. 2.