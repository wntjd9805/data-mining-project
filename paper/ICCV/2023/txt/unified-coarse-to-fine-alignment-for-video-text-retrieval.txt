Abstract
The canonical approach to video-text retrieval lever-ages a coarse-grained or fine-grained alignment between visual and textual information. However, retrieving the cor-rect video according to the text query is often challenging as it requires the ability to reason about both high-level (scene) and low-level (object) visual clues and how they re-late to the text query. To this end, we propose a Unified
Coarse-to-fine Alignment model, dubbed UCOFIA. Specif-ically, our model captures the cross-modal similarity infor-mation at different granularity levels. To alleviate the ef-fect of irrelevant visual clues, we also apply an Interac-tive Similarity Aggregation module (ISA) to consider the importance of different visual features while aggregating the cross-modal similarity to obtain a similarity score for each granularity. Finally, we apply the Sinkhorn-Knopp al-gorithm to normalize the similarities of each level before summing them, alleviating over- and under-representation issues at different levels. By jointly considering the cross-modal similarity of different granularity, UCOFIA allows the effective unification of multi-grained alignments. Em-pirically, UCOFIA outperforms previous state-of-the-art
CLIP-based methods on multiple video-text retrieval bench-marks, achieving 2.4%, 1.4% and 1.3% improvements in text-to-video retrieval R@1 on MSR-VTT, Activity-Net, and
DiDeMo, respectively. Our code is publicly available at https://github.com/Ziyang412/UCoFiA. 1.

Introduction
The fields of computer vision and natural language pro-cessing have both seen significant progress in recent years.
Thus, the cross-modal alignment [1, 64, 28, 44, 61, 56, 48, 47, 49], which involve developing techniques to con-nect these two domains, has seen considerable attention and progress. As a direct application of cross-modal alignment, the video-text retrieval task aligns video candidates with text queries to identify the most relevant videos, and the standard practice [50, 62, 63] is to align the video and text features extracted by vision and language encoders. Re-Figure 1. Comparison of the retrieved video from different level cross-modal alignments given a specific query. The coarse-grained alignment (the first row) only observes the high-level scene in-formation and overlooks the detailed information like “micro-phone”. The fine-grained alignment (the second row) does capture the detailed information but ignores the high-level scene informa-tion like “stage with the huge audience”. To this end, our work (the last row) combines the strengths of coarse-grained and fine-grained alignments by a Unified Coarse-to-fine Alignment model (UCOFIA). cently, the emergence of large-scale image-text pretrained models prompted several methods [39, 40, 35] to utilize
CLIP [45] image and text encoder to achieve strong perfor-mance on many video-text retrieval benchmarks. As a direct extension of CLIP, Luo et al. [39] proposed temporal fusion modules to aggregate the features of different video frames and then perform the cross-modal alignment on video and text features. Later, to capture more correspondences be-tween video and text, several works [35, 20, 18, 11] propose to conduct the alignment between frame and text features.
Ma et al. [40] take a step forward and leverage an align-ment between frame and word features for more detailed information.
Although the aforementioned methods have achieved impressive results, they only rely on high-level visual in-formation (frame and video) to perform the cross-modal
alignment. This coarse-grained alignment only captures the high-level visual clues (scene, action, etc) that connect to the text query. As shown in the first row of Figure 1, the coarse-grained alignment only captures the scene of the stage with the huge audience” and the action of “singing (possibly talking)”, thus leading to the incorrect retrieval re-sult. On the other hand, Zou et al. [66] build a fine-grained alignment between patch tokens from the video and word tokens from the text query. As illustrated in the second row of Figure 1, the fine-grained alignment does capture the detailed information like “microphone”, but it might over-look high-level clues like scene information (“stage with the huge audience”). These results reveal that video-text re-trieval requires an understanding of the both high-level and low-level correspondence between text and video. Thus, in this work, we aim to jointly consider coarse-grained and fine-grained cross-modal alignment and how to unify them to get the correct answer (as shown in the last row of Fig-ure 1).
To this end, we propose UCOFIA, a Unified Coarse-to-fine Alignment model for video-text retrieval. Our approach aims to capture the multi-grained similarity between the text and video by performing alignment at different gran-ularity. We begin with a coarse-grained alignment between the entire video and the query sentence (video-sentence).
Next, we perform frame-sentence alignment by matching individual video frames and the query sentence. Finally, we conduct a fine-grained alignment between the video patches and query words (patch-word).
However, while this multi-grained information provides richer, more diverse detailed information, it also brings significant irrelevant information to the cross-modal align-ment. For instance, several frames in the video might not contain information related to the query, and some patches in a frame might only correspond to the background infor-mation unrelated to any subjects in the query. The irrel-evant information could impede the model from learning precise cross-modal correspondence. To address these is-sues, we first propose an Interactive Similarity Aggregation module (ISA) that considers the importance of different vi-sual features while aggregating the cross-modal similarity to obtain a similarity score for each granularity. For frame-sentence alignment, our ISA module jointly considers the cross-modal similarity and the interaction of frame features while aggregating the frame-sentence similarity. Compared to the previous methods [35] that ignore the temporal clues between video frames, our ISA module can better capture the important information within continuous video frames.
Note that the ISA module is a general similarity aggregation approach regardless of the feature granularity, and we fur-ther extend it to a bidirectional ISA module for patch-word alignment.
Next, once we obtain the similarity score for each level of alignment, we can sum them to one score as the final re-trieval similarity. However, we find that similarity scores across different videos are highly imbalanced, and we em-pirically show that correcting this imbalance before sum-mation improves the performance. Concretely, sometimes the sum of retrieval similarities between one specific video and all texts (we called this term marginal similarity) might be much higher than that of the other videos, meaning that this video is over-represented and will lower the probabil-ity of the other video being selected. To address this, we utilize the Sinkhorn-Knopp algorithm [14] to normalize the similarity scores and make sure the marginal similarities for different videos are almost identical so that each video has a fair chance to be selected after normalization. We then unify the scores of different levels by performing the algo-rithm separately on the similarities of different levels and summing them together.
We validate the effectiveness of our UCOFIA model on diverse video-text retrieval benchmarks. Specifically,
UCOFIA achieve a text-to-video retrieval R@1 of 49.4% on MSR-VTT [57] and 45.7% on ActivityNet, thus, outper-forming the current state-of-the-art CLIP-based methods by 2.4% and 1.4%, respectively. 2.