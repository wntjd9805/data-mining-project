Abstract
LiDAR-Camera fusion-based 3D detection is a critical task for automatic driving. In recent years, many LiDAR-Camera fusion approaches sprung up and gained promising performances compared with single-modal detectors, but always lack carefully designed and effective supervision for the fusion process. In this paper, we propose a novel train-ing strategy called SupFusion, which provides an auxiliary feature level supervision for effective LiDAR-Camera fusion and significantly boosts detection performance. Our strategy involves a data enhancement method named Polar Sampling, which densifies sparse objects and trains an assistant model to generate high-quality features as the supervision. These features are then used to train the LiDAR-Camera fusion model, where the fusion feature is optimized to simulate the generated high-quality features. Furthermore, we propose a simple yet effective deep fusion module, which contigu-ously gains superior performance compared with previous fusion methods with SupFusion strategy. In such a manner, our proposal shares the following advantages. Firstly, Sup-Fusion introduces auxiliary feature-level supervision which could boost LiDAR-Camera detection performance without introducing extra inference costs. Secondly, the proposed deep fusion could continuously improve the detector’s abil-ities. Our proposed SupFusion and deep fusion module is plug-and-play, we make extensive experiments to demon-strate its effectiveness. Specifically, we gain around 2% 3D mAP improvements on KITTI benchmark based on multi-ple LiDAR-Camera 3D detectors. Our code is available at https://github.com/IranQin/SupFusion.
*Equal contribution. Work done during an internship at NIO.
†Corresponding author.
Figure 1: Top: The previous LiDAR-Camera 3D detector, the fusion module is optimized by detection loss. Bottom:
Our proposed SupFusion, we introduce auxiliary supervision via high-quality features provided by an assistant model. 1.

Introduction
LiDAR-Camera fusion-based 3D object detection is a critical and challenging task for autonomous driving and robotics [32, 35]. Previous approaches [20, 22, 34, 19] al-ways project the camera inputs to LiDAR BEV or voxel space via intrinsic and extrinsic to align the LiDAR and cam-era features. Then, simple concatenation or summation is employed to obtain the fusion features for final detection.
Also, some deep learning-based fusion approaches [1, 21] sprung up and gained promising performances. However, the previous fusion approaches always directly optimized the 3D/2D feature extraction as well as the fusion module by the
detection loss, which lacks carefully designed and effective supervision at the feature level and limits its performance.
In recent years, distillation manners have shown great im-provements in feature-level supervision for 3D detection. For instance, some approaches [5, 6] provide LiDAR features to guide the 2D backbone in estimating the depth information from camera inputs. Also, some approaches [19] provide
LiDAR-Camera fusion features to supervise LiDAR back-bone learns global and contextual presentation from LiDAR inputs. Introducing feature-level auxiliary supervision via simulating more robust and high-quality features, the detec-tors could boost marginal improvements. Motivated by this, a natural solution to handle LiDAR-Camera feature fusion is providing stronger and high-quality features and introducing auxiliary supervision for LiDAR-Camera 3D detection.
To this end, in this paper we propose a supervised LiDAR-Camera fusion method named SupFusion, to generate high-quality features and provide effective supervision for the fusion as well as feature extraction process, and further boost the LiDAR-Camera fusion-based 3D detection per-formances. Specifically, we first train an assistant model to provide high-quality features. To achieve it, different from previous approaches, which leverage larger models or extra data, we proposed a novel data enhancement method named Polar Sampling. The polar sampling could dynami-cally enhance the object’s density from sparse LiDAR data, which is easier to detect and improve features quality, e.g., the feature could conclude accuracy detection results. Then, we simply train the LiDAR-Camera fusion-based detec-tor with introduced auxiliary feature-level supervision.
In this step, we feed the raw LiDAR and camera inputs into 3D/2D backbones and fusion modules to obtain fusion fea-tures. In one aspect, the fusion features are fed into the detection head for final prediction, which is decision-level supervision. In another aspect, the auxiliary supervision mimics the fusion feature to high-quality features, which are obtained via the pretrained assistant model and enhanced
LiDAR data. In such a manner, the proposed feature-level supervision could lead the fusion module to generate more robust features, and further boost the detection performance.
To better fuse the LiDAR and camera features, we propose a simple yet effective deep fusion module, which consists of stacked MLP blocks and dynamic fusion blocks. SupFusion could sufficiently excavate the deep fusion module capacity, and continuously improve detection accuracy.
Compared with previous LiDAR-Camera 3D detectors, this is a brand new attempt to improve the effectiveness of the LiDAR-Camera fusion by introducing feature-level su-pervision, which consistently boosts 3D detection accuracy.
The main contributions are summarized as follows.
• We propose a novel supervised fusion training strategy named SupFusion, which mainly consists of a high-quality feature generation process and firstly propose auxiliary feature-level supervision loss for robust fusion feature extraction and accuracy 3D detection to our knowledge.
• To obtain high-quality features in SupFusion, we pro-pose a data enhancement method named Polar Sam-pling to densify the sparse objects. Furthermore, we propose an efficient deep fusion module to contiguously boost detection accuracy.
• We make extensive experiments based on multiple de-tectors with different fusion strategies, and gain around 2% mAP improvements on KITTI benchmark. The source code will be released after the blind review. 2.