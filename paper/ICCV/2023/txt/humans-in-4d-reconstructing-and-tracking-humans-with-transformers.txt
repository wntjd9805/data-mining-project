Abstract 1.

Introduction
We present an approach to reconstruct humans and track them over time. At the core of our approach, we propose a fully “transformerized” version of a network for human mesh recovery. This network, HMR 2.0, advances the state of the art and shows the capability to analyze unusual poses that have in the past been difficult to reconstruct from sin-gle images. To analyze video, we use 3D reconstructions from HMR 2.0 as input to a tracking system that operates in 3D. This enables us to deal with multiple people and maintain identities through occlusion events. Our complete approach, 4DHumans, achieves state-of-the-art results for tracking people from monocular video. Furthermore, we demonstrate the effectiveness of HMR 2.0 on the down-stream task of action recognition, achieving significant im-provements over previous pose-based action recognition approaches. Our code and models are available on the project website: https://shubham-goel.github. io/4dhumans/.
In this paper, we present a fully transformer-based ap-proach for recovering 3D meshes of human bodies from sin-gle images, and tracking them over time in video. We obtain unprecedented accuracy in our single-image 3D reconstruc-tions (see Figure 1) even for unusual poses where previous approaches struggle. In video, we link these reconstructions over time by 3D tracking, in the process bridging gaps due to occlusion or detection failures. These 4D reconstructions can be seen on the project webpage.
Our problem formulation and approach can be conceived as the “transformerization” of previous work on human mesh recovery, HMR [30] and 3D tracking, PHALP [65].
Since the pioneering ViT paper [15], the process of “trans-i.e., converting models from CNNs or formerization”,
LSTMs to transformer backbones, has advanced rapidly across multiple computer vision tasks, e.g., [8, 16, 24, 40, 61, 77]. Specifically for 2D pose (2D body keypoints) this has already been done by ViTPose [81]. We take that as a starting point and we develop a new version of HMR, which we call HMR 2.0 to acknowledge its antecedent.
We use HMR 2.0 to build a system that can simultane-ously reconstruct and track humans from videos. We rely on the recent 3D tracking system, PHALP [65], which we simplify and improve using our pose recovery. This system can reconstruct Humans in 4D, which gives the name to our method, 4DHumans. 4DHumans can be deployed on any video and can jointly track and reconstruct people in video.
The functionality of creating a tracking entity for every per-son is fundamental towards analyzing and understanding humans in video. Besides achieving state-of-the-art results for tracking on the PoseTrack dataset [1], we also apply
HMR 2.0 on the downstream application of action recogni-tion. We follow the system design of recent work, [63], and we show that the use of HMR 2.0 can achieve impressive improvements upon the state of the art on action recogni-tion on the AVA v2.2 dataset.
This paper is unabashedly a systems paper. We make design choices that lead to the best systems for 3D human reconstruction and tracking in the wild. Our model is pub-licly available on the project webpage. There is an emerg-ing trend, in computer vision as in natural language pro-cessing, of large pretrained models which find widespread downstream applications and thus justify the scaling ef-fort. HMR 2.0 is such a large pre-trained model which could potentially be useful not just in computer vision, but also in robotics [54, 62, 73], computer graphics [76], bio-mechanics [60], and other fields where analysis of the hu-man figure and its movement from images or videos is needed.
Our contributions can be summarized as follows: 1. We propose an end-to-end “transformerized” architec-ture for human mesh recovery, HMR 2.0. Without re-lying on domain-specific designs, we outperform ex-isting approaches for 3D body pose reconstruction. 2. Building on HMR 2.0, we design 4DHumans that can jointly reconstruct and track humans in video, achiev-ing state-of-the-art results for tracking. 3. We show that better 3D poses from HMR 2.0 result in better performance on the downstream task of action recognition, finally contributing to the state-of-the-art result (42.3 mAP) on the AVA benchmark. 2.