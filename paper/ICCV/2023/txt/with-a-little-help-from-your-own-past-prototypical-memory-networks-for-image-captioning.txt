Abstract
Image captioning, like many tasks involving vision and language, currently relies on Transformer-based architec-tures for extracting the semantics in an image and trans-lating it into linguistically coherent descriptions. Although successful, the attention operator only considers a weighted summation of projections of the current input sample, there-fore ignoring the relevant semantic information which can come from the joint observation of other samples. In this paper, we devise a network which can perform attention over activations obtained while processing other training samples, through a prototypical memory model. Our mem-ory models the distribution of past keys and values through the definition of prototype vectors which are both discrimi-native and compact. Experimentally, we assess the perfor-mance of the proposed model on the COCO dataset, in com-parison with carefully designed baselines and state-of-the-art approaches, and by investigating the role of each of the proposed components. We demonstrate that our proposal can increase the performance of an encoder-decoder Trans-former by 3.7 CIDEr points both when training in cross-entropy only and when fine-tuning with self-critical se-quence training. Source code and trained models are avail-able at: https://github.com/aimagelab/PMA-Net. 1.

Introduction
Connecting vision and natural language via descriptive expressions is a fundamental human capability and its repli-cation represents a crucial step towards machine intelli-gence, with applications that range from better human-machine interfaces [25] to accessibility [15]. The task of image captioning [21, 55, 60], which defines such capabil-ity, requires an algorithm to describe a visual input with a natural language sentence. As such, it features unique chal-lenges that span from a grounded and detailed understand-ing of the visual input [8, 46], to the selection of visual ob-jects and semantics that are worth mentioning [27] and their
Figure 1. Comparison between (a) a standard Transformer-based captioner; (b) a captioner with learnable memory vectors [10] and (c) our prototypical memory network. translation into a fluent and coherent sentence.
Image captioning architectures comprise an image en-coding part and a language generation approach [49] and focus on developing appropriate connections between the visual and textual modality. Examples of such innovations include the usage of attentive-like structures [4, 36], the in-corporation of attributes [26, 66], objects [4, 64] or scene graphs [62, 65]. Regardless of the specific approach used to connect the two modalities, though, almost all of the works developed in the last years share the usage of the Trans-former architecture [53]. Such architecture, indeed, is a natural choice for the task, as it can connect two modalities thanks to its encoder-decoder design and the cross-attention operator, and provides unprecedented performance in se-quence and set modeling and generation [10, 11, 40].
One of the key properties of attention layers is that the output is computed as a weighted combination of linear pro-jections of the inputs. While this provides an ideal context for both visual understanding and sequence generation, it also leaves the door open for injecting relevant informa-tion which can not be directly inferred from the current input sample. An interesting attempt in this direction has been made by the Meshed-Memory architecture [10], which proposed to insert additional learnable key/value vectors in the visual encoder with the objective of integrating a-priori knowledge. While successful, learnable vectors can just store information that is useful for the entire training set and do not really act as a “memory” of past training items.
Instead, having access to past training samples at generation time can be a powerful source of information that can ulti-mately increase the description quality. For instance, given an input image representing a boy snowboarding in a moun-tain landscape, a model which has access to other training items might retrieve similar images containing boy, snow-board, and mountains even in a different context, and em-ploy this knowledge in a compositional manner to aid the generation of a correct and fluent sentence (Fig. 1).
Following this insight, we devise a prototypical mem-ory network, which can recall and exploit past activations generated during training. Our memory is built upon net-work activations obtained during recent training iterations so that the network has access to a vast set of activations produced while processing other samples from the dataset.
In this sense, the memory represents past knowledge pro-cessed by the network itself. From the point of view of the architectural design, our memory is fully integrated into at-tention layers through the addition of keys and values which represents activations from the memory. To the best of our knowledge, this is the first attempt of integrating a memory of past training items into an image captioning network.
The key element of our proposal is the computation of
“prototypes” from a bank of past activations, which are ob-tained by modeling the manifold of past activations and clustering its content in a memory with a given fixed size.
This is done both at key and value level, by exploiting the mapping between corresponding keys and values. We fur-ther justify our prototype generation approach by investi-gating the resulting attention distribution from a theoretical point of view. Experimentally, we assess the performances of the proposed design on the COCO dataset for image cap-tioning, in comparison with state-of-the-art approaches and carefully-designed ablations to study the role of each com-ponent of the proposal. Further, we conduct experiments on the nocaps dataset [1] for novel object captioning and on the robust COCO split [32] for object hallucination.
We believe that the proposed approach can shed light on the effectiveness of employing the space of training items as an additional input to the network, which is currently under-explored and could be in principle applied outside of image captioning. To sum up, the main contribution of this work is the proposal of a prototype memory network for image captioning, which elegantly integrates past activations as an additional input of attention layers. Extensive experiments on COCO, nocaps, and the robust COCO split demonstrate the effectiveness of the proposed approach. 2.