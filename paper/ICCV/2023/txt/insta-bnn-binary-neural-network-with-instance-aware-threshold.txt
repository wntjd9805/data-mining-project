Abstract
Binary Neural Networks (BNNs) have emerged as a promising solution for reducing the memory footprint and compute costs of deep neural networks, but they suf-fer from quality degradation due to the lack of freedom as activations and weights are constrained to the binary values. To compensate for the accuracy drop, we pro-pose a novel BNN design called Binary Neural Network with INSTAnce-aware threshold (INSTA-BNN), which con-trols the quantization threshold dynamically in an input-dependent or instance-aware manner. According to our observation, higher-order statistics can be a representa-tive metric to estimate the characteristics of the input dis-tribution.
INSTA-BNN is designed to adjust the thresh-old dynamically considering various information, including higher-order statistics, but it is also optimized judiciously to realize minimal overhead on a real device. Our exten-sive study shows that INSTA-BNN outperforms the baseline by 3.0% and 2.8% on the ImageNet classification task with comparable computing cost, achieving 68.5% and 72.2% top-1 accuracy on ResNet-18 and MobileNetV1 based mod-els, respectively. 1.

Introduction
Deep neural networks (DNNs) are well-known for their abilities across diverse vision tasks, e.g., image classifica-tion [7, 13, 26, 27], object detection [14, 15, 24, 25], and se-mantic segmentation [19, 21]. These DNNs usually achieve excellent accuracy by using a large model, but the large model having massive memory usage and computing cost prevents us from deploying it on mobile devices having in-sufficient resources. In order to minimize the computation and memory overhead, network binarization is an appeal-ing optimization because weights and activations are quan-tized into 1-bit precision domain. Binary Neural Networks (BNNs) can achieve 32× reduction in memory requirement compared to their 32-bit floating point counterparts, and most modern CPUs or GPUs can serve the binary logic op-erations much faster than 32-bit floating point operations.
Instance-wise (a) mean, (b) standard deviation, and
Figure 1. (c) skewness of pre-activation from three different instances. The percentage in each distribution represents the ratio of +1’s in bi-nary activation when sign function is used as activation function (threshold is 0). When observing one component (e.g. mean (µ) for the (a) case), the other two (e.g. std (σ), skewness (γ)) are matched to be as close as possible. The other two values are dis-played on the top right of each subgraph.
However, BNNs usually suffer from accuracy degrada-tion due to the aggressive data quantization. In spite of the great efficiency of BNN, the accuracy degradation limits the deployment of BNNs in real-world applications. Therefore, a large number of techniques have been introduced to mini-mize the accuracy degradation of BNNs focusing on weight binarization [22, 23], shortcut connections [17], and thresh-old optimization [12, 16, 29].
In BNNs, only two values (+1 and −1) are available for activations. Therefore, the threshold of quantization that de-cides the mapping to either +1 or −1 plays a critical role in
BNNs. Some of the previous works tried to train the thresh-olds of the binary activation function to control the activa-tion distributions via back-propagation, where the threshold is fixed statically after the training [16, 29]. However, one
can easily observe that the per-instance activation is heavily distorted depending on the input data (Fig. 1), even while the batch-wise statistics are stabilized via batch normaliza-tion. As shown in the figure, the per-instance statistics nu-merically indicate the large distortion of pre-activation, and the corresponding outputs have a heavy fluctuation of the ratio of +1’s. In BNNs, static thresholds may not provide the adequate threshold for each instance, thereby resulting in sub-optimal results in terms of overall accuracy.
In this work, we propose a novel INSTA-BNN that calcu-lates the thresholds dynamically using the input-dependent or instance-wise information (e.g. mean, variance, and skewness). The instance-aware threshold enriches the qual-ity of binary features significantly, resulting in higher accu-racy of BNNs. In addition, we provide a variant of Squeeze-and-Excitation (SE) [8] with instance-wise adaptation as an additional option that one can exploit for even higher ac-curacy with extra parameters. The proposed modules are extensively evaluated on a real device with a large-scale dataset [6], and we provide a practical guideline for net-work design that maximizes the benefit of INSTA-BNN while minimizing the increased overhead of it. With the aid of the guidelines, INSTA-BNN achieves higher accuracy with a similar number of parameters/operations and latency compared to previous works. We claim that the proposed
INSTA-BNN can be an attractive option for BNN design. 2.