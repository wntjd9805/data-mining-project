Abstract
Synthesizing interaction-involved human motions has been challenging due to the high complexity of 3D environ-ments and the diversity of possible human behaviors within.
We present LAMA, Locomotion-Action-MAnipulation, to synthesize natural and plausible long- term human move-ments in complex indoor environments. The key motiva-tion of LAMA is to build a unified framework to encom-pass a series of everyday motions including locomotion, scene interaction, and object manipulation. Unlike existing methods that require motion data “paired” with scanned 3D scenes for supervision, we formulate the problem as a test-time optimization by using human motion capture data only for synthesis. LAMA leverages a reinforcement learning framework coupled with a motion matching al-gorithm for optimization, and further exploits a motion editing framework via manifold learning to cover possi-ble variations in interaction and manipulation. Through-out extensive experiments, we demonstrate that LAMA outperforms previous approaches in synthesizing realistic motions in various challenging scenarios. Project page: https://jiyewise.github.io/projects/LAMA/. 1.

Introduction
Synthesizing interactions within real-life 3D environ-ments has been a challenging research problem due to its complexity and diversity. The spatial constraint arising from real-life 3D environments where many objects are cluttered makes motion synthesis highly constrained and complex.
Furthermore, the nearly indefinite diversity of possible spa-tial arrangements of the 3D environment and human inter-action behaviors makes generalization in synthesis difficult.
Due to the wide range of technical challenges in-volved in human-scene interactions, previous approaches have focused on sub-problems, such as (1) modeling static poses [24, 66, 68, 16, 61, 45, 69] or (2) human ob-ject interactions with a single target object or interaction type [48, 67, 64, 49, 50, 44, 63, 10]. More recent meth-ods [55, 54, 14] extend to synthesizing dynamic interaction motions in real-world 3D scenes, where they use “scene-paired” motion datasets [15] in which motion is simultane-ously captured with the surrounding 3D environment. As such paired dataset is rare and difficult to scale up, the per-formance of these methods is fundamentally limited in fully covering the complexity and diversity of human interaction in real-world 3D scenes.
In this paper, we present LAMA, Locomotion-Action-MAnipulation, to synthesize natural and plausible long-term human motions in complex indoor environments. The key motivation of LAMA is to build a unified framework covering a series of everyday motions within real-world 3D scenes: locomotion through cluttered areas, interaction with the scene, and manipulation of objects. Unlike previ-ous approaches [55, 14] that use a “scene-paired” motion
dataset for supervision, we formulate it as a test-time opti-mization by utilizing only human motion capture data. Ex-ploiting reinforcement learning (RL) as a tool for optimiza-tion, we present an RL-based framework coupled with a motion matching algorithm [9, 5] to synthesize locomotion and scene interaction seamlessly while adapting to com-plex 3D scenes with collision avoidance handling. The ob-ject manipulation in our framework is performed via a mo-tion editing approach on top, by learning an autoencoder-based motion manifold space [19]. As a test-time optimiza-tion framework, LAMA is applicable to any 3D scene sce-narios (e.g., public datasets or any newly scanned scenes).
Through extensive quantitative and qualitative evaluations against existing methods, we demonstrate that our method outperforms [55, 14] in various challenging scenarios.
Our contributions are summarized as follows: (1) The first method to generate realistic long-term motions com-bined with locomotion, scene interaction, and manipula-tion in complex 3D scenes without “paired” datasets; (2)
A novel test-time optimization framework requiring hu-man motion capture data only by incorporating a reinforce-ment learning framework coupled with motion matching, equipped with well-designed state and rewards for collision avoidance and scene interactions; (3) the state-of-the-art motion synthesis quality with longer duration (near 10 sec); (4) A newly captured and polished motion capture dataset including locomotion and action (e.g., sitting) suitable for motion matching. 2.