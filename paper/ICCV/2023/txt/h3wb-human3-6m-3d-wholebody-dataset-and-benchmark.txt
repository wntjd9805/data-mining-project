Abstract man pose estimation.
We present a benchmark for 3D human whole-body pose estimation, which involves identifying accurate 3D key-points on the entire human body, including face, hands, body, and feet. Currently, the lack of a fully annotated and accurate 3D whole-body dataset results in deep networks being trained separately on specific body parts, which are combined during inference. Or they rely on pseudo-groundtruth provided by parametric body models which are not as accurate as detection based methods. To overcome these issues, we introduce the Human3.6M 3D WholeBody (H3WB) dataset, which provides whole-body annotations for the Human3.6M dataset using the COCO Wholebody layout. H3WB comprises 133 whole-body keypoint anno-tations on 100K images, made possible by our new multi-view pipeline. We also propose three tasks: i) 3D whole-body pose lifting from 2D complete whole-body pose, ii) 3D whole-body pose lifting from 2D incomplete whole-body pose, and iii) 3D whole-body pose estimation from a single
RGB image. Additionally, we report several baselines from popular methods for these tasks. Furthermore, we also pro-vide automated 3D whole-body annotations of TotalCap-ture and experimentally show that when used with H3WB it helps to improve the performance. 1.

Introduction 3D Human pose estimation is the task of localizing hu-man body keypoints in images which is critical to ana-lyze human behavior, expressions, emotions, intentions, and how people communicate and interact with the phys-ical world. As a result, 3D human pose estimation has an important role in several vision tasks and applications such as robotics [28, 25, 29] or augmented/virtual real-ity [55, 3, 73, 81]. However, to make more accurate pre-dictions about human behaviors, we need more than a few body keypoints. To that end, 3D whole-body pose estima-tion aims to detect face, hand and foot keypoints in addition to the standard human body keypoints of classical 3D hu-The lack of accurate 3D datasets has made 3D whole-body pose estimation a challenging task, leading previ-ous works to focus on separate body parts and train sep-arate models on different datasets for 3D body pose [1, 30, 55, 56, 63, 3, 44, 45, 46, 65, 74, 77, 80, 26], 3D hand pose [8, 58, 84, 7, 27, 37, 82, 31], or 3D face land-marks [68, 9, 13]. However, directly ensembling separate body part models during inference suffers from issues aris-ing from datasetsâ€™ biases, pose and scales, and complex in-ference pipelines. Distillation from pretrained models has been used to overcome these issues, with FrankMocap [66] using three specialized pretrained models to estimate 65 whole-body keypoints (22 on the body, 40 on the hands, and 3 on the face), and DOPE [75] also using three special-ized models to output 139 whole-body keypoints (13 on the body, 42 on the hands, and 84 on the face).
Alternatively, parametric body models can be fitted to obtain whole-body pose, as has been proposed in Ex-Pose [18], SMPLify-X [60] or Monocular Total Capture (MTC) [77]. While parametric models enable sampling an almost infinite number of keypoints from the mesh [24, 22, 23], their accuracy is usually less than that of detection based methods on fine body parts like hands and feet (see supplementary material for examples from the literature).
Indeed, parametric models are tailored for visual applica-tions such as realistic motion generation [62] or avatar cap-ture [67], where a realistic capture of the body shape and pose is more important than fine grain accuracy. Relying on a small set of data-driven pose parameters ensures realism but also limits their flexibility in representing complex un-usual poses. For applications where the body shape is not needed but the accuracy of the keypoints is essential, like high performance sport analysis or ergonomics, detection based methods are thus the preferred solution.
Furthermore, 3D whole-body pose estimation has not been fully explored in the literature due to the absence of a representative and accurate benchmark. As previously mentioned, existing 3D whole-body methods either rely on specific datasets and models for different body parts, lead-Figure 1. The H3WB dataset has 133 whole-body keypoint annotations in 3D as well as their respective projections in 2D. ing to complex training pipelines and heterogeneous eval-uations, or utilize parametric models that prioritize shape capture over highly precise keypoints. In addition, unified methods vary significantly in terms of keypoint layout def-inition, number of keypoints and distribution of keypoints across body parts (see Table 1). These significant dataset disparities and the absence of a standard benchmark make it challenging to compare methods fairly.
To address the above issues, we propose a new large-scale dataset for accurate 3D whole-body pose estimation called Human3.6M 3D WholeBody, or H3WB for short (see
Figure 1). Our dataset extends Human3.6M [36, 12] with 3D whole-body keypoint annotations.
It consists of 133 paired 2D and 3D whole-body keypoint annotations for a set of 100k images from Human3.6M, following the same layout used in COCO WholeBody [40]. More specifically, in addition to the standard 17 body keypoints, the dataset has 42 hand keypoints, 6 foot keypoints and 68 facial land-marks. H3WB was automatically created in a 3 step pro-cess: We obtain an initial set of 3D annotations using multi-view geometry. Then, we trained a masked auto-encoder to complete the initial annotations. Finally, we refine the whole-body keypoints via a diffusion model. A manual annotation of 80K keypoints from 600 images shows our labels have an average error of 17mm which suggest the
H3WB keypoints are very accurate for such a complex task.
We propose 3 tasks and benchmarks on the H3WB dataset for which we provide baselines: i) 3D whole-body pose lift-ing from a complete 2D whole-body keypoints, ii) 3D whole-body pose lifting from incomplete 2D whole-body keypoints (i.e. 2D whole-body with missing keypoints, which is more realistic), and iii) 3D whole-body pose estimation from a single RGB image.
Our contributions can be summarized as follows. 1) We propose a method to create detailed 3D human pose key-points from multi-view images. 2) We propose H3WB, the first accurate public benchmark dataset for 3D whole-body pose estimation, using the aforementioned method. Our benchmark can easily leverage existing results in 2D and enables the community to build upon existing high-quality 2D detectors on COCO. Unifying the 3D whole-body pose estimation with the COCO 2D benchmark will greatly ben-efit the research community. 3) We provide baselines for the 3 tasks of H3WB, which we believe will encourage the community to explore 3D whole-body pose estimation more and accelerate progress in the field. 4) Additionally, we provide 3D whole-body annotations for the TotalCap-ture [43] dataset, and show that when combined with the
H3WB dataset it improves the performance of pose lifting tasks. 2.