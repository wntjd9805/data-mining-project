Abstract
Streaming video clips with large-scale video tokens im-pede vision transformers (ViTs) for efficient recognition, especially in video action detection where sufficient spa-tiotemporal representations are required for precise ac-tor identification.
In this work, we propose an end-to-end framework for efficient video action detection (EVAD) based on vanilla ViTs. Our EVAD consists of two special-ized designs for video action detection. First, we propose a spatiotemporal token dropout from a keyframe-centric per-spective. In a video clip, we maintain all tokens from its keyframe, preserve tokens relevant to actor motions from other frames, and drop out the remaining tokens in this clip. Second, we refine scene context by leveraging remain-ing tokens for better recognizing actor identities. The re-gion of interest (RoI) in our action detector is expanded into temporal domain. The captured spatiotemporal ac-tor identity representations are refined via scene context in a decoder with the attention mechanism. These two de-signs make our EVAD efficient while maintaining accuracy, which is validated on three benchmark datasets (i.e., AVA,
UCF101-24, JHMDB). Compared to the vanilla ViT back-bone, our EVAD reduces the overall GFLOPs by 43% and improves real-time inference speed by 40% with no perfor-mance degradation. Moreover, even at similar computa-tional costs, our EVAD can improve the performance by 1.1 mAP with higher resolution inputs. Code is available at https://github.com/MCG-NJU/EVAD. 1.

Introduction
Vision transformers (ViTs) have improved a series of recognition performance, including image classification [9, 46, 20] and object detection [5, 8, 40]. The image patches are regarded as tokens for ViT inputs for self-attention com-putations. When recognizing a video clip, we notice that the
*Corresponding author.
Figure 1: Spatiotemporal token dropout from a keyframe-centric perspective. We maintain tokens from the keyframe of a video clip, preserve a small amount of tokens from non-keyframes based on actor motions, and drop out the remain-ing tokens in this clip. On the first row, we preserve tokens relevant to the waving hand in non-keyframes as they bene-fit recognizing the action ‘point to’. On the second row, we drop out tokens irrelevant to the action ‘entering’ in non-keyframes for efficient recognition. tokens are from each frame and thus formulate a large-scale input for ViTs. These video tokens introduce heavy com-putations during training and inference, especially in the self-attention computation step. While attempts have been made to reduce vision tokens [36, 28, 12, 47] for fast com-putations, it is still challenging for video action detection (VAD) [41, 16, 26] to balance accuracy and efficiency. This is because in VAD we need to localize actors in each frame and recognize their corresponding identities. For each ac-tor, the temporal motion in video sequences shall be main-tained for consistent identification. Meanwhile, the scene context ought to be kept to differentiate from other actors.
Sufficient video tokens representing both actor motions and scene context will preserve VAD accuracy, which leaves a
research direction on how to select them for efficient VAD.
In this paper, we preserve video tokens representing ac-tor motions and scene context, while dropping out irrele-vant tokens. Based on the temporal coherency of video clips, we propose a spatiotemporal token dropout from a keyframe-centric perspective. For each video clip, we can select one keyframe representing the scene context where all the tokens shall be maintained. Meanwhile, we select to-kens from other frames representing actor motions. More-over, we drop out the remaining video tokens in this clip.
Fig. 1 shows two examples. On the first row, we maintain all the tokens in the keyframe, preserve the tokens in non-keyframe relating to the eyes and mouths associating to the action ‘talk to’, the waving hand of the right person asso-ciating to the action ‘point to’, and drop out the remaining video tokens. On the second row, we maintain all the to-kens in the keyframe, preserve the tokens in non-keyframes relating to the movement of entering the car from outside which is associated with the action ‘enter’, and drop out the remaining video tokens. Our spatiotemporal token dropout maintains the relevant actor motions and scene context from the coherent video clips, while discarding the remaining ir-relevant ones for efficient computations.
We develop spatiotemporal token dropout via a keyframe-centric token pruning module within the ViT en-coder backbone. The keyframe is either uniformly sam-pled or manually predefined in the video clips. We select the middle frame of the input clip with box annotations as the keyframe by default. The feature map of the se-lected keyframe is retained completely for actor localiza-tion. Besides, we extract an attention map enhanced by this keyframe. This attention map guides token dropout in the non-keyframes. After localization, we need to clas-sify each localized bounding box (bbx) for actor identifi-cation. As video tokens are incomplete in non-keyframes, the classification performance is inferior in the bbx regions where tokens have been dropped out. Nevertheless, the in-herent temporal consistency in video benefits us to refine both actor and scene context from the remaining video to-kens. We expand the localized bbxs in the temporal domain for RoIAlign [17] to capture token features related to actor motions. Then, we introduce a decoder to refine actor fea-tures guided by the remaining video tokens in this clip. The decoder concatenates the actor and token features, and per-forms self-attention to produce enriched actor features for better identification. We find that after token dropout, the degraded action classification can be effectively recovered by using remaining video tokens for context refinement.
The recovered performance is the same as that using the whole video tokens for action classification. Through this context refinement, we can maintain the VAD performance using reduced video tokens for efficient computations.
We conduct extensive experiments on three popular ac-tion detection datasets (AVA [16], UCF101-24 [41], JH-MDB [19]) to show the advantages of our proposed EVAD.
In the ViT-B encoder backbone, for instance, we employ a keyframe-centric token pruning module three times with a keep rate ρ of 0.7 (i.e,. the dropout rate is 0.3). The en-coder outputs 34% of the original video tokens, which re-duces GFLOPs by 43% and increases throughput by 40% while achieving on-par performance. On the other hand, under the similar computation cost (i.e., a similar amount of tokens), we can take video clips in a higher resolution for performance improvement. For example, we can improve detection performance by 1.1 mAP when increasing the res-olution from 256 to 288 on the short side with GFLOPs re-duced by 22% and throughput increased by 10%. We also provide visualizations and ablation studies to show how our
EVAD performs spatiotemporal token dropout to eliminate action irrelevant video tokens. 2.