Abstract
Efficiently deploying deep neural networks on low-resource edge devices is challenging due to their ever-increasing resource requirements. To address this issue, researchers have proposed multiplication-free neural net-works, such as Power-of-Two quantization, or also known as Shift networks, which aim to reduce memory usage and simplify computation. However, existing low-bit Shift net-works are not as accurate as their full-precision counter-parts, typically suffering from limited weight range encod-ing schemes and quantization loss. In this paper, we pro-pose the DenseShift network, which significantly improves the accuracy of Shift networks, achieving competitive per-formance to full-precision networks for vision and speech
In addition, we introduce a method to de-applications. ploy an efficient DenseShift network using non-quantized floating-point activations, while obtaining 1.6× speed-up over existing methods. To achieve this, we demonstrate that zero-weight values in low-bit Shift networks do not con-tribute to model capacity and negatively impact inference computation. To address this issue, we propose a zero-free shifting mechanism that simplifies inference and in-creases model capacity. We further propose a sign-scale decomposition design to enhance training efficiency and a low-variance random initialization strategy to improve the model’s transfer learning performance. Our extensive experiments on various computer vision and speech tasks demonstrate that DenseShift outperforms existing low-bit multiplication-free networks and achieves competitive per-formance compared to full-precision networks. Further-more, our proposed approach exhibits strong transfer learn-ing performance without a drop in accuracy. Our code was released on GitHub.
Figure 1: Benchmark low-bit DenseShift networks over
SOTA low-bit Shift networks on ImageNet using the
ResNet-18 model architecture. 1.

Introduction
Deep neural networks have demonstrated superior per-formance in diverse applications such as image classifica-tion, object detection, and image segmentation [17, 27, 6], and speech [29]. However, despite the high accuracy of multiplication-based deep neural networks, their comput-ing resource requirement makes their deployment challeng-ing, especially on low-resource devices. Recent research has explored multiplication-free neural networks that re-duce memory footprint and overall energy consumption to address this issue.
Existing works on multiplication-free neural networks include binary [8], and ternary quantization [21]. They re-spectively constrain their weights in the range of {±1} and
{0}∪{±1}, in order to replace multiplication computations with less expensive operations such as a sign flip operator.
These low-bit quantization techniques make it possible to deploy deep learning models on resource-constrained edge devices. Moreover, [44] trades the multiplication operation with the addition operation, and [12, 22, 36] use the bit-shift operator to build power-of-two (PoT) quantized net-works, known as Shift networks. Shift networks built upon a ternary base have a weight space of {0} ∪ {±2p}. This means that multiplication operations can be replaced with bit-wise shift operations, which have highly efficient hard-ware implementations. In fact, [36] showed that with 4-bit weights and 8-bit activations, the shift-based MAC unit de-signed for Shift networks outperformed its counterpart for traditional uniform quantization by 2.4× energy saving and 20% chip area saving using Samsung 5nm.
A recent study [22] proposes a weight reparameteri-zation scheme S3 for Shift network training, which sig-nificantly improves the accuracy of the ImageNet classi-fication task under sub-4bits weight and does not require full-precision pre-training. However, S3 has the follow-ing shortcomings: i) Existing Shift networks, including S3, only support quantized activations during inference, limit-ing their performance gains and usefulness in various sce-narios; ii) S3 is only benchmarked on image classification and exhibits significant performance degradation under 2-bit weight; iii) Transfer learning tasks are unexplored.
In this study, we identify and address design limitations in current low-bit Shift networks through a detailed analy-sis, resulting in the proposal of DenseShift network. Our novel designs significantly enhance model capacity, infer-ence efficiency, and transferability. The contributions of this study are outlined below.
First, our analysis reveals that zero weights in low-bit
Shift networks reduce model capacity under limited bit widths. To address this issue, we propose a zero-free shift-ing mechanism that removes zero values from the weight space. This design enhances model representation capacity and improves performance under low-bit conditions, sur-passing existing low-bit Shift networks.
Second, we introduce a novel inference approach for
DenseShift networks that supports both floating-point and quantized activations. Our approach accelerates the dot-product computation by 1.6× on ARMv8 CPU under FP16.
Notably, DenseShift is the first Shift network that enables inference with non-quantized floating-point activations and the first to demonstrate performance improvement without relying on dedicated hardware such as ASIC or FPGA.
Third, we propose an efficient training algorithm for DenseShift networks adapted from the weight re-parameterization techniques [22]. Our sign-scale decom-position method breaks down the discrete weights into a bi-nary sign term and a power-of-two scale term, and recur-sively re-parameterizes the exponent of the scale term as a combination of binary variables. This enables us to train low-bit DenseShift networks from a random initialization, achieving performance that is comparable to full-precision networks.
Fourth, while prior research works suffer from severe performance degradation when transferred to a new task, we propose a low-variance random initialization strategy to improve the model’s performance in transfer learning sce-narios. We demonstrate that the weight values tend to gather towards the original point of the re-parameterization space during the initial stage of training, and as a result, a greater gradient signal is needed to push them to pass the threshold when the weights are randomly initialized with a large vari-ance. By reducing the variance of weight initialization, the
DenseShift network can be easily adapted to different tasks while maintaining competitive performance.
We conducted extensive experiments to evaluate the per-formance of our DenseShift network compared to various baselines on a diverse set of tasks across different fields.
The results show that our proposed DenseShift network out-performs the state-of-the-art Shift network on the ImageNet classification task and achieves comparable performance to full-precision networks while having higher inference com-putational efficiency. As summarized in Fig. 1, DenseShift network performs significantly better in low-bit settings, es-pecially under 2-bit condition. Specifically, our 2-bit and 3-bit quantized ResNet-18 on the classification task achieve 68.90% and 70.57% Top-1 accuracy respectively. More-over, we demonstrate that our low-bit DenseShift networks can achieve full-precision performance in transfer learning scenarios across different domains, including computer vi-sion and speech tasks. This study is the first to demonstrate this capability, to the best of our knowledge. 2.