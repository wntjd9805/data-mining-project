Abstract
Diffusion probabilistic models have achieved remark-able success in text guided image generation. However, generating 3D shapes is still challenging due to the lack of sufficient data containing 3D models along with their de-scriptions. Moreover, text based descriptions of 3D shapes are inherently ambiguous and lack details. In this paper, we propose a sketch and text guided probabilistic diffusion model for colored point cloud generation that conditions the denoising process jointly with a hand drawn sketch of the object and its textual description. We incrementally dif-fuse the point coordinates and color values in a joint dif-fusion process to reach a Gaussian distribution. Colored point cloud generation thus amounts to learning the reverse diffusion process, conditioned by the sketch and text, to it-eratively recover the desired shape and color. Specifically, to learn effective sketch-text embedding, our model adap-tively aggregates the joint embedding of text prompt and the sketch based on a capsule attention network. Our model uses staged diffusion to generate the shape and then as-sign colors to different parts conditioned on the appear-ance prompt while preserving precise shapes from the first stage. This gives our model the flexibility to extend to multi-ple tasks, such as appearance re-editing and part segmenta-tion. Experimental results demonstrate that our model out-performs recent state-of-the-art in point cloud generation. 1.

Introduction
Denoising Diffusion Probabilistic Models (DDPM) [19, 45] can generate novel images and videos, from their text descriptions. Recent diffusion models [38, 33, 39, 42] con-tain billions of parameters and are trained on millions of text-image [13] pairs sourced from the Internet, including existing datasets like MS-COCO [25]. The availability of such large corpora of text-image pair datasets is, arguably, the major driving force behind the success of research in im-age and video generation. However, such datasets of text-shape pairs are almost nonexistent causing a major bottle-neck for advancing research in this direction. 3D shape generation has a wide range of applications in-cluding data augmentation, virtual/augmented reality [12],
*Work performed during visit at the University of Western Australia
â€ Correponding author.
Illustration of sketch and text guided 3D point cloud
Figure 1. generation. The text and sketch complement each other. manufacturing, and reverse engineering. Pioneering works in diffusion based 3D shape generation include Luo et al. [26] and Point Voxel Diffusion (PVD) [66]. Luo et al. [26] extend the DDPMs to 3D shape generation condi-tioned on shape latent that follows a prior distribution pa-rameterized via normalized flows [9]. Their probabilistic generative model treats points as particles that, under heat, are diffused to a noise distribution. It then exploits the re-verse diffusion process to learn the point distribution to be able to generate plausible point clouds. PVD [66] com-bines the denoising diffusion model with the point-voxel 3D shape representation.
It performs the denoising steps by optimizing a variational lower bound to the conditional likelihood function to produce 3D shapes or multiple 3D completions [14] from a partial observation.
The problem of text guided 3D model or point cloud gen-eration is slightly different from that of images in the sense that there is a higher emphasis, at least implicitly, on geome-try rather than appearance which puts yet another constraint on the training datasets. Current methods [26, 66] have mostly used the ShapeNet [3] dataset for text-shape learning combined with the ModelNet [55] datasets for 3D represen-tation learning and validation. However, these datasets are relatively small and lack detailed textual descriptions. To work around the data paucity, CLIP-forge [42] extends the
CLIP model [37] for 3D shape generation by aligning 3D shapes with text in the latent space. Although these text guided methods maintain diversity in the generated shapes, their common limitation is that the generated shapes are of-ten not the desired ones, which is understandable given the shape ambiguity in the text.
In this paper, to reduce the shape ambiguity, we pro-pose a sketch and text guided probabilistic diffusion (STPD) model for colored point cloud generation that conditions the
denoising process with sketch inputs, in addition to textual descriptions. Sketches can be hand drawn and give much more geometric details compared to text. We argue that, combined with text, sketches are a viable option for condi-tioning the denoising of 3D shapes given their direct rele-vance to the problem at hand and their history of success-ful use for 3D object reconstruction [65]. However, cross-modality inputs (sketch and text) [11] always suffer from large cross-modality discrepancies and intra-modality vari-ations bringing in additional challenges of cross-modality fusion [28] and data sparsity. Especially, sketches are inher-ently sparse with only a very small proportion of informa-tive pixels, and contain much less information than natural images.
The proposed STPD model can produce the geometry and appearance simultaneously and unambiguously from a sketch-text input, combining ease of access and complete-ness of description. It takes the sketch as the main shape de-scription and shape information present in the text to com-plement the sketch input. Hence, shape information is ex-tracted from the sketch as well as the text when there is shape information in the text. Appearance is extracted from the text input to give color to the generated point clouds as shown in Fig. 1. To summarize, our contributions are: 1. We propose a staged probabilistic diffusion model for better control over the geometry and appearance to generate colored point clouds. Our model conditions the denoising process jointly with sketch and text in-puts to generate 3D shapes.
It can also be used for part-segmentation and appearance re-editing. 2. We propose an attention capsule based feature extrac-tion module for encoding sketches, which are inher-ently sparse. Our method robustly gives more attention to the entities of the useful pixels and ignores the large number of meaningless capsules corresponding to the blank pixels in the sketch. 3. We present a sketch-text fusion network that efficiently abstracts the shape and color information from both sketch and language descriptions to guide the reverse 3D shape diffusion process.
Extensive experiments on the ShapeNet dataset [3] and comparison to existing diffusion probabilistic model based generation as well as some classical shape reconstruction methods show that our model achieves state-of-the-art per-formance for colored point cloud generation. We also show the representation learning ability of our model by conduct-ing 3D object classification experiments on the ModelNet40
[55] dataset and the application of our model to part seg-mentation on the ShapeNet-Parts dataset [3]. 2.