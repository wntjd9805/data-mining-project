Abstract
Existing knowledge distillation methods typically work by imparting the knowledge of output logits or intermediate feature maps from the teacher network to the student net-work, which is very successful in multi-class single-label learning. However, these methods can hardly be extended to the multi-label learning scenario, where each instance is associated with multiple semantic labels, because the pre-diction probabilities do not sum to one and feature maps of the whole example may ignore minor classes in such a sce-nario. In this paper, we propose a novel multi-label knowl-edge distillation method. On one hand, it exploits the infor-mative semantic knowledge from the logits by dividing the multi-label learning problem into a set of binary classiﬁ-cation problems; on the other hand, it enhances the distinc-tiveness of the learned feature representations by leveraging the structural information of label-wise embeddings. Ex-perimental results on multiple benchmark datasets validate that the proposed method can avoid knowledge counter-action among labels, thus achieving superior performance against diverse comparing methods. Our code is available at: https://github.com/penghui-yang/L2D. 1.

Introduction
Multi-label learning (MLL) addresses problems where each instance is assigned with multiple class labels simulta-neously [32]. For example, as shown in Figure 1, an image of a street scene may be annotated with labels bus, car and person. To learn the complex object-label mapping, there is always necessity of training large models to obtain desir-able performance in MLL. While the remarkable successes
Figure 1. Visualization of attention maps on different classes.
We compare our method with the following three baselines: 1)
Vanilla: the student trained without distillation; 2) ReviewKD [1]: a feature-based method that achieves the state-of-the-art perfor-mance; 3) Teacher: the pretrained model used in distillation. It can be observed that: on the classes bus and car, L2D captures se-mantic objects more precisely than the conventional KD method; on the class person, although all of the methods focus on people, only our method focuses on the same person as the teacher, which validates that L2D can distill the “dark” knowledge [27] from the teacher more effectively. The backbones of the teacher and stu-dent models are respectively ResNet-101 and ResNet-34. More visualization of attention maps can be found in Appendix.
*Both authors contributed equally to this research.
†Correspondence to: Sheng-Jun Huang (huangsj@nuaa.edu.cn). have been made in MLL through the training of deep neural networks (DNNs) [14, 23], it is hard to deploy these large
models on lightweight terminals, e.g., mobile phones, under the constraint of computational resource or requirement of short inference time.
To mitigate this issue, knowledge distillation (KD) [10] is needed which aims to improve the performance of a small network (also known as the “student”) by requiring the knowledge from a large network (also known as the
“teacher”) to guide the training of the student network. Typ-ical KD methods focus on the multi-class classiﬁcation, and can be roughly divided into two categories: logits-based methods and feature-based methods. The former minimizes the difference between logits of the teacher model and the student model [10, 34], while the latter distills knowledge from feature maps of intermediate layers [19, 28, 1].
Although KD has been proven to be effective for im-proving the performance of the student network in single-label classiﬁcation, it is still a challenging problem to di-rectly extend existing KD methods to solve multi-label knowledge distillation (MLKD) problems. Speciﬁcally, logits-based methods often obtain the predicted probabili-ties based on the softmax function; this function is not suit-able for MLKD, because the sum of predicted probabilities may not equal one in MLL. Feature-based methods often perform KD based on the feature maps of a whole image with multiple semantics, which makes the model focus on the major objects while neglect minor objects. For exam-ple, in Figure 1, Vanilla and ReviewKD wrongly focused on the bus when the model queried the label car. Such a phenomenon would cause the model to obtain sub-optimal even undesirable distillation performance.
Recently, several attempts have been made to utilize
KD techniques for improving the performance of MLL
[15, 29, 25]. There are mainly two differences between these works and our work. Firstly, these methods often re-quired speciﬁcally-designed network architectures [15, 25] or training strategies [29] to train the teacher and student models, while our method focuses on studying MLKD in general scenarios without any extra requirement. Secondly, unlike the previous methods utilized KD as an auxiliary technique to improve the performance of MLL, our goal is to develop a tailored approach for MLKD. As a result, pre-vious methods were mainly compared with MLL methods in their original papers, we evaluate the KD performance of our method by comparing it with state-of-the-art KD meth-ods.
In this paper, to perform MLKD, we propose a new logits distillation and method consisting of multi-label label-wise embedding distillation (L2D for short). Specif-ically, to exploit informative semantic knowledge com-pressed in the logits, L2D employs the one-versus-all re-duction strategy to obtain a set of binary classiﬁcation prob-lems and perform logits distillation for each one. To en-hance the distinctiveness of learned feature representations,
L2D encourages the student model to maintain a consistent structure of intra-class and intra-instance (inter-class) label-wise embeddings with the teacher model. By leveraging the structural information of the teacher model, these two structural consistencies respectively enhance the compact-ness of intra-class embeddings and dispersion of inter-class embeddings for the student model. Both of these two com-ponents lead L2D to achieve better distillation performance than conventional KD methods as shown in Figure 1.
Our main contributions can be summarized as follows:
• A general framework called MLKD is proposed. To our best knowledge, the framework is the ﬁrst study spe-cially designed for knowledge distillation in the multi-label learning scenario.
• A new approach for MLKD called L2D is proposed. It performs multi-label logits distillation and label-wise em-bedding distillation simultaneously. The former provides informative semantic knowledge while the latter encour-ages the student model to learn more distinctive feature representations.
• Extensive experimental results on benchmark datasets demonstrate the effectiveness of our proposed method. 2.