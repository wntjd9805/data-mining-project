Abstract 3D visual grounding involves ﬁnding a target object in a 3D scene that corresponds to a given sentence query. Al-though many approaches have been proposed and achieved impressive performance, they all require dense object-sentence pair annotations in 3D point clouds, which are both time-consuming and expensive. To address the prob-lem that ﬁne-grained annotated data is difﬁcult to obtain, we propose to leverage weakly supervised annotations to learn the 3D visual grounding model, i.e., only coarse scene-sentence correspondences are used to learn object-sentence links. To accomplish this, we design a novel se-mantic matching model that analyzes the semantic similar-ity between object proposals and sentences in a coarse-to-ﬁne manner. Speciﬁcally, we ﬁrst extract object propos-als and coarsely select the top-K candidates based on fea-ture and class similarity matrices. Next, we reconstruct the masked keywords of the sentence using each candidate one by one, and the reconstructed accuracy ﬁnely reﬂects the se-mantic similarity of each candidate to the query. Addition-ally, we distill the coarse-to-ﬁne semantic matching knowl-edge into a typical two-stage 3D visual grounding model, which reduces inference costs and improves performance by taking full advantage of the well-studied structure of the ex-isting architectures. We conduct extensive experiments on
ScanRefer, Nr3D, and Sr3D, which demonstrate the effec-tiveness of our proposed method. 1.

Introduction 3D Visual grounding (3DVG) refers to the process of lo-calizing an object in a scene based on a natural language sentence. The 3DVG task has recently gained attention due to its numerous applications. Despite the signiﬁcant progress made in this area [3, 4, 40, 41, 17, 38], all these
*Equal contribution
†Corresponding author
Figure 1. (a). 3D visual grounding aims to ﬁnd the object-sentence pair from the whole scene. The fully supervised setting requires all the dense ground-truth object-sentence labels for train-ing, while the weakly supervised method only needs the coarse scene-sentence annotations. (b). Coarse-to-Fine Semantic Match-ing Model (bottom) analyzes the matching score of each proposal to the sentence, and the semantic matching knowledge is distilled to the two-stage 3DVG architecture (upper). approaches require bounding box annotations for each sen-tence query, which are laborious and expensive to obtain.
For example, it takes an average of 22.3 minutes to anno-tate a scene in the ScanNet-v2 dataset [6]. Thus, we focus on weakly supervised training for 3DVG, which only re-quires scene-sentence pairs for training. This problem is meaningful and realistic since obtaining scene-level labels is much easier and can be scaled effectively.
However, weakly supervised 3DVG poses two chal-lenges. Firstly, a 3D point cloud can contain numerous ob-jects of various categories, and a sentence query may con-tain multiple objects besides the target object to aid in lo-calization. Without knowledge of the ground-truth object-sentence pair, it is difﬁcult to learn to link the sentence to
its corresponding object from the enormous number of pos-sible object-sentence pairs. Secondly, the 3DVG task often involves multiple interfering objects in the scene with the same class as the target object, and the target object must be distinguished based on its object attributes and the rela-tions between objects described in the given sentence. As illustrated in Figure 1 (a), there are two trash cans in the scene, and the described target object can only be identiﬁed by fully comprehending the language description.
To address both challenges simultaneously, we propose a coarse-to-ﬁne semantic matching model to measure the similarity between object proposals and sentences. Speciﬁ-cally, our model generates object-sentence matching scores from scene-sentence annotation, guided by coarse-to-ﬁne semantic similarity analysis. Firstly, we calculate the ob-ject category similarity and feature similarity between all the proposals and the sentence. Combining these two sim-ilarities, we roughly select K proposals with the highest similarity to the sentence, which can effectively ﬁlter out the proposals that do not belong to the target category. Sec-ondly, we utilize NLTK [2] to conduct part-of-speech tag-ging on the sentences and randomly mask the more mean-ingful nouns and adjectives words. The selected candidates would be used to reconstruct the masked keywords of the sentence, which can help the model fully and deeply un-derstand the whole sentence. Since the target object and the sentence query are semantically consistent, the more the candidate and the target object overlap, the more accurate its predicted keywords will be. The object-sentence matching score of each candidate can be measured by its reconstruc-tion loss. Eventually, in order to reduce inference time and make full use of the structure of existing 3DVG models, we utilize knowledge distillation [15] to migrate the knowledge of the coarse-to-ﬁne semantic matching model to a typical two-stage 3DVG model, where the distilled pseudo labels are generated by the object-sentence matching scores.
In summary, the key contribution is four-fold:
•
•
•
To the best of our knowledge, this paper is the ﬁrst work to address weakly supervised 3DVG, which eliminates the need for expensive and time-consuming dense object-sentence annotations and instead requires only scene-sentence level labels.
We approach weakly supervised 3DVG as a coarse-to-ﬁne semantic matching problem and propose a coarse-to-ﬁne semantic matching model to analyze the simi-larity between each proposal and the sentence.
We distill the knowledge of the coarse-to-ﬁne semantic matching model into a two-stage 3DVG model, which fully leverages the well-studied network structure de-sign, leading to improved performance and reduced in-ference costs.
•
Experiments conducted on three wide-used datasets
ScanRefer [4], Nr3D [1] and Sr3D [1] demonstrate the effectiveness of our method. 2.