Abstract 1.

Introduction
In this paper, we propose a new operator, called 3D
DeFormable Attention (DFA3D), for 2D-to-3D feature lift-ing, which transforms multi-view 2D image features into a uniﬁed 3D space for 3D object detection. Existing fea-ture lifting approaches, such as Lift-Splat-based and 2D attention-based, either use estimated depth to get pseudo
LiDAR features and then splat them to a 3D space, which is a one-pass operation without feature reﬁnement, or ignore depth and lift features by 2D attention mechanisms, which achieve ﬁner semantics while suffering from a depth ambi-guity problem. In contrast, our DFA3D-based method ﬁrst leverages the estimated depth to expand each view’s 2D fea-ture map to 3D and then utilizes DFA3D to aggregate fea-tures from the expanded 3D feature maps. With the help of DFA3D, the depth ambiguity problem can be effectively alleviated from the root, and the lifted features can be pro-gressively reﬁned layer by layer, thanks to the Transformer-like architecture.
In addition, we propose a mathemati-cally equivalent implementation of DFA3D which can sig-niﬁcantly improve its memory efﬁciency and computational speed. We integrate DFA3D into several methods that use 2D attention-based feature lifting with only a few modiﬁca-tions in code and evaluate on the nuScenes dataset. The ex-periment results show a consistent improvement of +1.41% mAP on average, and up to +15.1% mAP improvement when high-quality depth information is available, demon-strating the superiority, applicability, and huge potential of
DFA3D. The code is available at https://github.com/IDEA-Research/3D-deformable-attention.git.
*This work was done during the internship at IDEA.
†Equal contribution.
‡Corresponding author. 3D object detection is a fundamental task in many real-world applications such as robotics and autonomous driv-ing. Although LiDAR-based methods [37, 40, 23, 33] have achieved impressive results with the help of accurate 3D perception from LiDAR, multi-view camera-based meth-ods have recently received extensive attention because of their low cost for deployment and distinctive capability of long-range and color perception. Classical multi-view camera-based 3D object detection approaches mostly fol-low monocular frameworks which ﬁrst perform 2D/3D ob-ject detection in each individual view and then conduct cross-view post-processing to obtain the ﬁnal result. While remarkable progress has been made [30, 31, 24, 26, 1], such a framework cannot fully utilize cross-view information and usually leads to low performance.
To eliminate the ineffective cross-view post-processing, several end-to-end approaches [25, 13, 10, 14, 32, 20, 21] have been developed. These approaches normally contain three important modules: a backbone for 2D image feature extraction, a feature lifting module for transforming multi-view 2D image features into a uniﬁed 3D space (e.g. BEV space in an ego coordinate system) to obtain lifted features, and a detection head for performing object detection by tak-ing as input the lifted features. Among these modules, the feature lifting module serves as an important component to bridge the 2D backbone and the 3D detection head, whose quality will greatly affect the ﬁnal detection performance.
To perform feature lifting, recent methods usually pre-deﬁne a set of 3D anchors in the ego coordinate sys-tem sparsely or uniformly, with randomly initialized con-tent features. After that, they lift 2D image features into the 3D anchors to obtain the lifted features. Some meth-ods [13, 10, 9, 34, 12] utilize a straightforward lift and splat mechanism [25] by ﬁrst lifting 2D image features into pseudo LiDAR features in an ego coordinate system using estimated depth and then assigning the pseudo LiDAR fea-#
−%
$
%
#
✗
!
"
%
#
✓
✗
$
Layer n
…
Layer 1
Layer 0
$
Layer n
…
Layer 1
Layer 0
!
"
%
✗
#
✓
✓
&
!
"
%
%
#
✓
$
$
Layer n
…
Layer 1
Layer 0 (a) Lift-Splat (b)Transformer-2D Point Attention (c) Transformer-2D Deformable Attention (d) Transformer-3D Deformable Attention
Figure 1. Comparisons of feature lifting methods. (a) Lift-Splat-based methods cannot handle depth errors after feature lifting, which can result in wrong predictions. Point attention-based (b) and 2D deformable attention-based (c) can reﬁne the lifted features layer-by-layer. However, they suffer from depth ambiguity when multiple 3D objects are projected to the same 2D point, which may result in duplicate predictions along a ray connecting the ego car and a target object. (d) Our 3D deformable attention can effectively alleviate the depth ambiguity problem as we sample features in 3D spaces. Moreover, the multi-layer design of Transformers helps reﬁne features and sampling points layer by layer. tures to their closest 3D anchors to obtain the ﬁnal lifted fea-tures, as shown in Fig. 1(a). Although such methods have achieved remarkable performance, due to their excessive re-source consumption when lifting 2D image features, they normally cannot utilize multi-scale feature maps, which are crucial for detecting small (faraway) objects.
Instead of lift and splat, some other works utilize 2D attention to perform feature lifting. Such methods treat predeﬁned 3D anchors as 3D queries and propose several ways to aggregate 2D image features (keys) to 3D anchors.
Typical 2D attention mechanisms that have been explored include dense attention [20], point attention (a degener-ated deformable attention)
[3, 4], and deformable atten-tion [14, 36, 5]. 2D attention-based methods can natu-rally work with Transformer-like architectures to progres-sively reﬁne the lifted features layer by layer. Moreover, de-formable attention makes it possible to use multi-scale 2D features, thanks to its introduced sparse attention computa-tion. However, the major weakness of 2D attention-based approaches is that they suffer from a crucial depth ambi-guity problem, as shown in Fig. 1(b,c). That is, due to the ignorance of depth information, when projecting 3D queries to a 2D view, many 3D queries will end up with the same 2D position with similar sampling points in the 2D view. This will result in highly entangled aggregated features and lead to wrong predictions along a ray connecting the ego car and a target object (as in Fig. 6 of supp.). Although some con-current works [15, 39] have tried to alleviate this issue , they can not ﬁx the problem from the root. The root cause lies in the lack of depth information when applying 2D attention to sample features in a 2D pixel coordinate system1.
To address the above problems, in this paper, we propose a basic operator called 3D deformable attention (DFA3D), built upon which we develop a novel feature lifting ap-proach, as shown in Fig. 1(d). We follow [25] to leverage a depth estimation module to estimate a depth distribution for each 2D image feature. We expand the dimension of each single-view 2D image feature map by computing the outer product of them and their estimated depth distributions to obtain the expanded 3D feature maps in a 3D pixel coordi-nate system2. Each 3D query in the BEV space is projected to the 3D pixel space with a set of predicted 3D sampling offsets to specify its 3D receptive ﬁeld and pool features from the expanded 3D feature maps. In this way, a 3D query close to the ego car only pools image features with smaller depth values, whereas a 3D query far away from the ego car mainly pools image features with larger depth values, and thus the depth ambiguity problem is effectively alleviated.
In our implementation, as each of the expanded 3D fea-ture maps is an outer product between a 2D image fea-ture map and its estimated depth distributions, we do not need to maintain a 3D tensor in memory to avoid excessive
Instead, we seek assistance from memory consumption. math to simplify DFA3D into a mathematically equivalent depth-weighted 2D deformable attention and implement it through CUDA, making it both memory-efﬁcient and fast.
The Pytorch interface of DFA3D is very similar to 2D de-formable attention and only requires a few modiﬁcations to replace 2D deformable attention, making our feature lifting approach easily portable.
In summary, our main contributions are: 1. We propose a basic operator called 3D deformable at-tention (DFA3D) for feature lifting. Leveraging the property of outer product between 2D features and their estimated depth, we develop a memory-efﬁcient and fast implementation. 1The pixel coordinate system that only considers the u, v axes. 2The pixel coordinate system that considers all of the u, v, d axes.
2. Based on DFA3D, we develop a novel feature lifting approach, which not only alleviates the depth ambigu-ity problem from the root, but also beneﬁts from multi-layer feature reﬁnement of a Transformer-like archi-tecture. Thanks to the simple interface of DFA3D, our
DFA3D-based feature lifting can be implemented in any method that utilizes 2D deformable attention (also its degeneration)-based feature lifting with only a few modiﬁcations in code. 3. The consistent performance improvement (+1.41 mAP on average) in comparative experiments on the nuScenes [2] dataset demonstrate the superiority and generalization ability of DFA3D-based feature lifting. 2.