Abstract
In this study, we focus on Continual Semantic Segmenta-tion (CSS) and present a novel approach to tackle the issue of existing methods struggling to learn new classes. The primary challenge of CSS is to learn new knowledge while retaining old knowledge, which is commonly known as the rigidity-plasticity dilemma. Existing approaches strive to address this by carefully balancing the learning of new and old classes during training on new data. Differently, this work aims to avoid this dilemma fundamentally rather than handling the difficulties involved in it. Specifically, we re-veal that this dilemma mainly arises from the greater fluc-tuation of knowledge for new classes because they have never been learned before the current step. Additionally, the data available in incremental steps are usually inadequate, which can impede the model’s ability to learn discrimina-tive features for both new and old classes. To address these challenges, we introduce a novel concept of pre-learning for future knowledge. Our approach entails optimizing the fea-ture space and output space for unlabeled data, which thus enables the model to acquire knowledge for future classes.
With this approach, updating the model for new classes be-comes as smooth as for old classes, effectively avoiding the rigidity-plasticity dilemma. We conducted extensive exper-iments and the results demonstrate a significant improve-ment in the learning of new classes compared to previous state-of-the-art methods. 1.

Introduction
Deep neural networks have demonstrated their superi-ority in many computer vision tasks [26, 31, 14]. Con-ventionally, they are trained in an offline manner with all data collected beforehand. When novel classes need to be handled, the networks need to be re-trained using ei-ther updated training set with both new and previous data, or directly finetuned on new data. The former has higher costs and is even impracticable sometimes like, when pri-vacy raises concerns. The latter is prone to forget previ-Figure 1. On VOC2012 benchmark, our method boosts the perfor-mance of new classes by a significant margin, while also achieving promising performance for old classes, compared to recent state-of-the-art methods. ously learned knowledge, which is called catastrophic for-getting (CF) [25]. To better fit neural networks into real-world scenarios, we want them to have the ability to learn new concepts while preserving the old knowledge (or even improve it using new knowledge). This is known as the continual learning (CL) problem and receives increasing at-tention recently.
It has been explored first in the field of image classification [28, 42, 17, 11, 57, 44, 43], and some methods are proposed recently targeting semantic segmen-tation [5, 10, 6, 54, 51, 37]. Generally, it is often achieved by applying forgetting-preventing constraints between cur-rent and previous networks to prevent CF, which raises the rigidity (i.e., the ability to preserve old states) but harms the plasticity (i.e., the ability to learn new knowledge).
It is known as the rigidity-plasticity (R-P) dilemma. A key as-pect in designing constraints is to balance between rigidity and plasticity, which is what most works focus on.
Despite that several new constraints have been proposed for CSS to better tackle the dilemma, existing works still struggle to learn new classes. Taking the VOC2012 bench-mark as an example, most methods can only achieve a per-formance around 70% of that of the upperbound for 15-5 setup and around 50% for 19-1 setup, which is even worse
for harder setups. To get a clear picture of why this happens, we first analyze the intermediate features of the network and find it suffers serious underfitting for new classes, which in-dicates a large amount of knowledge is not learned. We then examine the CSS training pipeline and find two main obstacles causing this phenomenon. (1) R-P Dilemma: the network has no knowledge w.r.t. new classes until they are introduced. This generally requires a rather large update to incorporate the new knowledge, which will be significantly hindered by the forgetting-preventing constraints punishing the network from drifting away from its previous states. (2)
Inadequate data for training: the amount of data available in incremental steps is usually limited, and this problem be-comes more severe when a single class is added, resulting in a tiny training set with only a few negative samples. This can significantly limit the network to properly discriminate between old and new classes. This problem has never been explicitly considered before.
Unlike existing works trying to tackle the R-P dilemma directly, we are the first to argue that this obstacle can be elegantly avoided by exploring future knowledge in an un-supervised way. Assume that the network has obtained par-tial knowledge of future classes in advance, then the net-work needs fewer updates to learn the upcoming knowl-edge, which consequently lowers the impacts of the R-P dilemma. The more future knowledge it learns in advance, the fewer the impacts will be caused. Considering an ex-treme situation where all the future knowledge is learned beforehand, the network does not need any update, and thus the influence of constraints could be totally avoided. On top of this, it makes us the first attempt to explicitly tackle the second obstacle by utilizing the abundant data in previ-ous steps to optimize discriminability between old and new classes. Based on these understandings, we design a novel framework where unsupervised contrastive learning is per-formed on intermediate features to optimize future classes with both visual similarity and feature affinity as supervi-sions. One step further, an auxiliary classifier is trained using pseudo labels of future classes generated via cluster-ing, which is used to initialize the actual classifier for fu-ture steps. Our method surpasses existing works by a huge margin as shown in Figure 1 and the effectiveness is fur-ther demonstrated with extensive experiments. Our contri-butions are summarized as follows.
• We propose to pre-learn future knowledge for CSS with the aim of improving the performance especially for new classes, which can simultaneously tackle the R-P dilemma and the obstacle of inadequate data.
• We design a novel framework that can pre-learn future knowledge in an unsupervised way, where both the fea-ture space and output space are handled.
• We have validated the effectiveness of our proposed approach through extensive experiments. The results demonstrate significant improvement on standard CSS benchmarks. 2.