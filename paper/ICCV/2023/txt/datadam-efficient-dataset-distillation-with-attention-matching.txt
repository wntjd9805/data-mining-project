Abstract
Researchers have long tried to minimize training costs in deep learning while maintaining strong generalization across diverse datasets. Emerging research on dataset dis-tillation aims to reduce training costs by creating a small synthetic set that contains the information of a larger real dataset and ultimately achieves test accuracy equivalent to a model trained on the whole dataset. Unfortunately, the synthetic data generated by previous methods are not guar-anteed to distribute and discriminate as well as the original training data, and they incur significant computational costs.
Despite promising results, there still exists a significant per-formance gap between models trained on condensed syn-thetic sets and those trained on the whole dataset. In this paper, we address these challenges using efficient Dataset
Distillation with Attention Matching (DataDAM), achieving state-of-the-art performance while reducing training costs.
Specifically, we learn synthetic images by matching the spa-tial attention maps of real and synthetic data generated by different layers within a family of randomly initialized neu-ral networks. Our method outperforms the prior methods on several datasets, including CIFAR10/100, TinyImageNet,
ImageNet-1K, and subsets of ImageNet-1K across most of the settings, and achieves improvements of up to 6.5% and 4.1% on CIFAR100 and ImageNet-1K, respectively. We also show that our high-quality distilled images have practical benefits for downstream applications, such as continual learn-ing and neural architecture search. 1.

Introduction
Deep learning has been highly successful in various fields, including computer vision and natural language processing, due to the use of large-scale datasets and modern Deep Neu-ral Networks (DNNs) [12, 19, 14, 21].
*Equal contribution
Figure 1: (a) Data distribution of the distilled images on the CI-FAR10 dataset with 50 images per class (IPC50) for CAFE [43] and DataDAM. (b) Performance comparison with state-of-the-art methods on the CIFAR10 dataset for varying IPCs.
However, extensive infrastructure resources for training, hyperparameter tuning, and architectural searches make it challenging to reduce computational costs while main-taining comparable performance. Two primary approaches to address this issue are model-centric and data-centric.
Model-centric methods involve model compression tech-niques [20, 47, 1, 49, 35], while data-centric methods con-centrate on constructing smaller datasets with enough in-formation for training, which is the focus of this paper. A traditional data-centric approach is the coreset selection method, wherein we select a representative subset of an original dataset [33, 8, 4, 37, 40]; however, these meth-ods have limitations as they rely on heuristics to generate a coarse approximation of the whole dataset, which may lead to a suboptimal solution for downstream tasks like image
classification [40, 33]. Dataset distillation (or condensation)
[44] is proposed as an alternative, which distills knowledge from a large training dataset into a smaller synthetic set such that a model trained on it achieves competitive testing performance with one trained on the real dataset. The con-densed synthetic sets contain valuable information, making them a popular choice for various machine learning applica-tions like continual learning [44, 54, 52], neural architecture search [11, 53, 54], federated learning [48, 56], and privacy-preserving [13, 41] tasks.
Dataset distillation was first proposed by Wang et al. [44] where bi-level meta-learning was used to optimize model parameters on synthetic data in the inner loop and refine the data with meta-gradient updates to minimize the loss on the original data in the outer loop. Various methods have been proposed to overcome the computational expense of this method, including approximating the inner optimization with kernel methods [5, 30, 29, 55], surrogate objectives like gradient matching [54, 52, 26], trajectory matching [9], and distribution matching [43, 53]. The kernel-based meth-ods and gradient matching work still require bi-level opti-mization and second-order derivation computation, making training a difficult task. Trajectory matching [9] demands significant GPU memory for extra disk storage and expert model training. CAFE [43] uses dynamic bi-level optimiza-tion with layer-wise feature alignment, but it may generate biased images and incur a significant time cost (Figure 1).
Thus, these methods are not scalable for larger datasets such as ImageNet-1K [12]. Distribution matching (DM) [53] was proposed as a scalable solution for larger datasets by skip-ping optimization steps in the inner loop. However, DM usually underperforms compared to prior methods [9].
In this paper, we propose a new framework called
”Dataset Distillation with Attention Matching (DataDAM)” to overcome computational problems, achieve an unbiased representation of the real data distribution, and outperform the performance of the existing methods. Due to the ef-fectiveness of randomly initialized networks in generating strong representations that establish a distance-preserving embedding of the data [7, 36, 16, 53], we leverage multi-ple randomly initialized DNNs to extract meaningful repre-sentations from real and synthetic datasets. We align their most discriminative feature maps using the Spatial Attention
Matching (SAM) module and minimize the distance between them with the MSE loss. We further reduce the last-layer feature distribution disparities between the two datasets with a complementary loss as a regularizer. Unlike existing meth-ods [54, 43, 9], our approach does not rely on pre-trained network parameters or employ bi-level optimization, making it a promising tool for synthetic data generation. The gener-ated synthetic dataset does not introduce any bias into the data distribution while outperforming concurrent methods, as shown in Figure 1.
The contributions of our study are:
[C1]: We proposed an effective end-to-end dataset distilla-tion method with attention matching and feature distribution alignment to closely approximate the distribution of the real dataset with low computational costs.
[C2]: Our method is evaluated on computer vision datasets with different resolutions, where it achieves state-of-the-art results across multiple benchmark settings. Our approach offers up to a 100x reduction in training costs while simultaneously enabling cross-architecture generalizations.
[C3]: Our distilled data can enhance downstream applica-tions by improving memory efficiency for continual learning and accelerating neural architecture search through a more representative proxy dataset. 2.