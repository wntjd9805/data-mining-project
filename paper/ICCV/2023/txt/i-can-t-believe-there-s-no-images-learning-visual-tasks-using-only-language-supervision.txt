Abstract
Many high-level skills that are required for computer vi-sion tasks, such as parsing questions, comparing and con-trasting semantics, and writing descriptions, are also re-quired in other domains such as natural language process-ing.
In this paper, we ask whether it is possible to learn those skills from text data and then transfer them to vision tasks without ever training on visual training data. Key to our approach is exploiting the joint embedding space of contrastively trained vision and language encoders. In practice, there can be systematic differences between em-bedding spaces for different modalities in contrastive mod-els, and we analyze how these differences affect our ap-proach and study strategies to mitigate this concern. We produce models using only text training data on four repre-sentative tasks: image captioning, visual entailment, visual question answering and visual news captioning, and eval-uate them on standard benchmarks using images. We ﬁnd these models perform close to models trained on images, while surpassing prior work for captioning and visual en-tailment in this text-only setting by over 9 points, and out-performing all prior work on visual news by over 30 points.
We also showcase a variety of stylistic image captioning models that are trained using no image data and no human-curated language data, but instead using readily-available text data from books, the web, or language models. 1.

Introduction
Although vision and natural language processing (NLP) tasks are typically thought of as being very distinct, there is often a high degree of overlap in the skills needed to com-plete them. Visual question answering and reading compre-hension question answering both require parsing and un-derstanding questions, visual entailment and textual entail-ment require comparing different semantic meanings, and captioning and summarization require writing text that sum-*Equal contribution marizes the semantics of the input. This raises an intrigu-ing possibility: if a model learned to complete one of these tasks using a high-level semantic representation of the input text, then in theory it could immediately be able to complete the corresponding visual task as long as the input image is encoded in the same semantic representation. We call this challenge zero-shot cross-modal transfer because it requires applying skills learned from one modality to a different one. Achieving this would be a step towards building multi-modal models that can generalize skills across modalities without needing expensive training data for each modality, and has potential applications for tasks where visual train-ing data is scarce but text data is relatively easy to collect.
Accomplishing this requires encoding images and text into a shared semantic space. We use vision and language (V&L) models trained with a contrastive loss for this pur-pose [51, 25]. These models learn to embed text and images into vectors such that the vectors for matching images and captions are close together, and vectors for unrelated images and captions are far apart. Although this loss was originally intended for representation learning and zero-shot classiﬁ-cation, here we show it also facilitates cross-modal transfer.
To do this, we propose a method called Cross modaL transfer On Semantic Embeddings (CLOSE). An outline of CLOSE is shown in Figure 1. During training, the text inputs are encoded into a vector using the (frozen) text en-coder from a contrastive model, which is then used as an input to a model. During testing, the visual input is em-bedded with a (frozen) image encoder and used in place of the text embedding. Because these encoders were explic-itly trained to produce embeddings that encode semantics in similar ways, learning to read and process the text vec-tor should naturally translate to the ability to read and pro-cess the image vector. Although we focus on text-to-image transfer in this paper, our approach is applicable to other contrastive models such as videos [74], point clouds [1], and audio [22, 11, 72], potentially allowing transfer between many other modalities.
One potential difﬁculty with this approach is that, while contrastive embeddings do share some structure between
(cid:4)(cid:30)(cid:12)(cid:21)(cid:29)(cid:12)(cid:28)(cid:20)(cid:24)(cid:23)(cid:34)(cid:1)(cid:6)(cid:22)(cid:12)(cid:18)(cid:16)(cid:1)(cid:6)(cid:23)(cid:25)(cid:29)(cid:28) (cid:10)(cid:16)(cid:22)(cid:12)(cid:23)(cid:28)(cid:20)(cid:14)(cid:1)(cid:4)(cid:22)(cid:13)(cid:16)(cid:15)(cid:15)(cid:20)(cid:23)(cid:18)(cid:27) (cid:2)(cid:26)(cid:24)(cid:27)(cid:27)(cid:35)(cid:7)(cid:24)(cid:15)(cid:12)(cid:21)(cid:1)(cid:7)(cid:24)(cid:15)(cid:16)(cid:21) (cid:6)(cid:29)(cid:17)(cid:23)(cid:21)(cid:1) (cid:4)(cid:30)(cid:19)(cid:31)(cid:20)(cid:21)(cid:33) (cid:2)(cid:12)(cid:10)(cid:14)(cid:6)(cid:9) (cid:13)(cid:21)(cid:39)(cid:35)(cid:1) (cid:4)(cid:30)(cid:19)(cid:31)(cid:20)(cid:21)(cid:33) (cid:2)(cid:12)(cid:10)(cid:14)(cid:6)(cid:9) (cid:11)(cid:26)(cid:12)(cid:20)(cid:23)(cid:20)(cid:23)(cid:18)(cid:34)(cid:1)(cid:11)(cid:16)(cid:32)(cid:28)(cid:1)(cid:6)(cid:23)(cid:25)(cid:29)(cid:28) (cid:13)(cid:38)(cid:31)(cid:1)(cid:38)(cid:31)(cid:29)(cid:17)(cid:30)(cid:1)(cid:38)(cid:17)(cid:34)(cid:24)(cid:25)(cid:30)(cid:23)(cid:1) (cid:17)(cid:30)(cid:1)(cid:21)(cid:28)(cid:21)(cid:32)(cid:24)(cid:17)(cid:30)(cid:35)(cid:1)(cid:28)(cid:40)(cid:25)(cid:30)(cid:23)(cid:1)(cid:25)(cid:30)(cid:1) (cid:35)(cid:24)(cid:21)(cid:1)(cid:38)(cid:17)(cid:35)(cid:21)(cid:33)(cid:43) (cid:13)(cid:10)(cid:2) (cid:6)(cid:34)(cid:1)(cid:35)(cid:24)(cid:25)(cid:34)(cid:1) (cid:21)(cid:28)(cid:21)(cid:32)(cid:24)(cid:17)(cid:30)(cid:35)(cid:1) (cid:19)(cid:17)(cid:28)(cid:29)(cid:41) (cid:4)(cid:15)(cid:29)(cid:32)(cid:23)(cid:28)(cid:27)(cid:23)(cid:27)(cid:21) (cid:13)(cid:23)(cid:31)(cid:33)(cid:15)(cid:25)(cid:1) (cid:5)(cid:27)(cid:32)(cid:15)(cid:23)(cid:25)(cid:26)(cid:19)(cid:27)(cid:32) (cid:3)(cid:21)(cid:34)(cid:19)(cid:33)(cid:25)(cid:18)(cid:21)(cid:1)(cid:35)(cid:24)(cid:25)(cid:34)(cid:1) (cid:25)(cid:29)(cid:17)(cid:23)(cid:21)(cid:43) (cid:46)(cid:13)(cid:24)(cid:21)(cid:1)(cid:21)(cid:28)(cid:21)(cid:32)(cid:24)(cid:17)(cid:30)(cid:35)(cid:1) (cid:25)(cid:34)(cid:1)(cid:38)(cid:21)(cid:35)(cid:47) (cid:8)(cid:31)(cid:20)(cid:21)(cid:28) (cid:4)(cid:29)(cid:18)(cid:21)(cid:20)(cid:20)(cid:25)(cid:30)(cid:23)(cid:1) (cid:2)(cid:20)(cid:17)(cid:32)(cid:35)(cid:17)(cid:35)(cid:25)(cid:31)(cid:30) (cid:13)(cid:21)(cid:39)(cid:35)(cid:1)(cid:4)(cid:29)(cid:18)(cid:21)(cid:20)(cid:20)(cid:25)(cid:30)(cid:23) (cid:2)(cid:20)(cid:17)(cid:32)(cid:35)(cid:21)(cid:20)(cid:1)(cid:13)(cid:21)(cid:39)(cid:35)(cid:1) (cid:4)(cid:29)(cid:18)(cid:21)(cid:20)(cid:20)(cid:25)(cid:30)(cid:23) (cid:6)(cid:29)(cid:17)(cid:23)(cid:21)(cid:1)(cid:4)(cid:29)(cid:18)(cid:21)(cid:20)(cid:20)(cid:25)(cid:30)(cid:23) (cid:16)(cid:21)(cid:34) (cid:13)(cid:38)(cid:31)(cid:1)(cid:38)(cid:31)(cid:29)(cid:17)(cid:30)(cid:1) (cid:38)(cid:17)(cid:34)(cid:24)(cid:25)(cid:30)(cid:23)(cid:1)(cid:17)(cid:30)(cid:1) (cid:21)(cid:28)(cid:21)(cid:32)(cid:24)(cid:17)(cid:30)(cid:35)(cid:1)(cid:28)(cid:40)(cid:25)(cid:30)(cid:23)(cid:1) (cid:25)(cid:30)(cid:1)(cid:35)(cid:24)(cid:21)(cid:1)(cid:38)(cid:17)(cid:35)(cid:21)(cid:33)(cid:43) (cid:4)(cid:30)(cid:35)(cid:17)(cid:25)(cid:28)(cid:29)(cid:21)(cid:30)(cid:35)
Figure 1: Overview of CLOSE. During training, input text is encoded into a vector with a text encoder and adapted with an adaptation method. A model learns to use the vector to perform a task such as VQA, captioning, or visual entailment. During testing, an input image is encoded with an image encoder instead to allow cross-modal transfer. modalities, there can still be signiﬁcant differences between the image and text vectors in practice [39]. To mitigate this, we propose to additionally use adapters that modify the text vectors being used during training. We ﬁnd adding Gaus-sian noise to be very effective in boosting performance, but consider other approaches as well in our analyses.
Text-to-image transfer is a relatively unexplored setting, so we ﬁrst conduct extensive experiments to establish that
CLOSE can handle the text-to-image domain shift with-out a major performance drop. We compare models trained with CLOSE on text alone to models trained with images and text on three standard V&L tasks: captioning, visual questioning answers (VQA) and visual entailment, and the more complex task of visual news captioning [40]. We ﬁnd the text-only models generally perform reasonably close to versions trained with images, showing that CLOSE can ef-fectively transfer many skills across modalities. We surpass the previous best text-only method in captioning [78] by 17 CIDEr (78.2 vs. 95.4) and visual entailment [57] by 9 points (66.6 vs. 75.9), making our method state-of-the-art for these settings by a large margin. There are no prior re-sults for VQA and visual news in this setting, however we do surpass the previously best reported result in visual news even with images [40] (50.5 vs 80.8 CIDEr).
These experiments show that efﬁcient text-to-image transfer is possible. This has important practical implica-tions because text training data can be directly constructed by annotators, mined from many existing text datasets, or even generated by a large language model such as GPT-3 [4], and can therefore be signiﬁcantly less expensive than constructing visual training data. We demonstrate this potential by training effective CLOSE captioning models from text generated by large language models [4], meaning the only human annotation required was for prompt con-struction. We also train several stylistic captioning models without any labeled images (see Figure 2). We collect text (cid:11)(cid:16)(cid:32)(cid:28)(cid:35)(cid:8)(cid:23)(cid:21)(cid:33)(cid:1)(cid:11)(cid:26)(cid:12)(cid:20)(cid:23)(cid:20)(cid:23)(cid:18)(cid:1)(cid:3)(cid:12)(cid:28)(cid:12) (cid:6)(cid:23)(cid:17)(cid:16)(cid:26)(cid:16)(cid:23)(cid:14)(cid:16)(cid:1)(cid:31)(cid:20)(cid:28)(cid:19)(cid:1)(cid:11)(cid:26)(cid:12)(cid:20)(cid:23)(cid:16)(cid:15)(cid:1)(cid:7)(cid:24)(cid:15)(cid:16)(cid:21)(cid:27) (cid:11)(cid:19)(cid:34)(cid:23)(cid:19)(cid:35)(cid:31)(cid:1)(cid:20)(cid:30)(cid:28)(cid:26)(cid:1) (cid:14)(cid:19)(cid:16)(cid:1)(cid:12)(cid:19)(cid:36)(cid:32) (cid:2) (cid:4)(cid:39)(cid:19)(cid:21)(cid:28)(cid:28)(cid:21)(cid:30)(cid:35)(cid:1)(cid:37)(cid:17)(cid:28)(cid:36)(cid:21)(cid:1)(cid:22)(cid:31)(cid:33)(cid:1)(cid:17)(cid:1)(cid:22)(cid:36)(cid:28)(cid:28)(cid:1)(cid:22)(cid:17)(cid:19)(cid:21)(cid:1) (cid:23)(cid:36)(cid:17)(cid:33)(cid:20)(cid:42)(cid:1)(cid:19)(cid:31)(cid:29)(cid:32)(cid:28)(cid:21)(cid:35)(cid:21)(cid:28)(cid:40)(cid:1)(cid:19)(cid:31)(cid:37)(cid:21)(cid:33)(cid:25)(cid:30)(cid:23) (cid:35)(cid:24)(cid:21) (cid:19)(cid:24)(cid:21)(cid:21)(cid:27)(cid:44) (cid:2) (cid:13)(cid:24)(cid:25)(cid:34)(cid:1)(cid:3)(cid:14)(cid:3)(cid:1)(cid:38)(cid:31)(cid:30)(cid:49)(cid:35)(cid:1)(cid:32)(cid:28)(cid:17)(cid:40)(cid:1)(cid:31)(cid:30)(cid:1)(cid:29)(cid:40)(cid:1) (cid:10)(cid:25)(cid:31)(cid:30)(cid:21)(cid:21)(cid:33)(cid:1)(cid:4)(cid:28)(cid:25)(cid:35)(cid:21)(cid:1)(cid:3)(cid:14)(cid:3)(cid:1)(cid:32)(cid:28)(cid:17)(cid:40)(cid:21)(cid:33)(cid:43) (cid:5)(cid:21)(cid:28)(cid:38)(cid:4)(cid:19)(cid:27)(cid:32)(cid:30)(cid:23)(cid:17)(cid:1) (cid:20)(cid:30)(cid:28)(cid:26)(cid:1)(cid:7)(cid:9)(cid:12)(cid:38)(cid:39) (cid:15)(cid:3)(cid:12)(cid:8)(cid:13)(cid:6)(cid:1)(cid:4)(cid:9)(cid:1)(cid:6)(cid:7)(cid:10)(cid:17) (cid:5)(cid:6)(cid:9)(cid:13)(cid:12)(cid:8)(cid:5)(cid:1)(cid:5)(cid:4)(cid:11)(cid:13)(cid:8)(cid:10)(cid:9)(cid:16) (cid:2) (cid:6)(cid:1)(cid:17)(cid:29)(cid:1)(cid:38)(cid:17)(cid:25)(cid:35)(cid:25)(cid:30)(cid:23)(cid:1)(cid:17)(cid:35)(cid:1)(cid:17)(cid:1)(cid:18)(cid:36)(cid:34)(cid:40)(cid:1)(cid:34)(cid:35)(cid:33)(cid:21)(cid:21)(cid:35)(cid:1) (cid:30)(cid:21)(cid:17)(cid:33)(cid:1)(cid:33)(cid:21)(cid:20)(cid:1)(cid:20)(cid:31)(cid:36)(cid:18)(cid:28)(cid:21)(cid:1)(cid:20)(cid:21)(cid:19)(cid:27)(cid:21)(cid:33)(cid:1)(cid:18)(cid:36)(cid:34)(cid:43) (cid:2) (cid:8)(cid:40)(cid:1)(cid:17)(cid:36)(cid:30)(cid:35)(cid:1)(cid:25)(cid:34)(cid:1)(cid:38)(cid:21)(cid:17)(cid:33)(cid:25)(cid:30)(cid:23)(cid:1)(cid:17)(cid:30)(cid:1) (cid:36)(cid:29)(cid:18)(cid:33)(cid:21)(cid:28)(cid:28)(cid:17)(cid:1)(cid:24)(cid:17)(cid:35)(cid:43) (cid:4)(cid:22)(cid:15)(cid:30)(cid:15)(cid:17)(cid:32)(cid:19)(cid:30)(cid:38)(cid:3)(cid:15)(cid:31)(cid:19)(cid:18)(cid:1) (cid:20)(cid:30)(cid:28)(cid:26)(cid:1)(cid:3)(cid:28)(cid:28)(cid:24)(cid:31) (cid:2) (cid:5)(cid:21)(cid:33)(cid:29)(cid:25)(cid:31)(cid:30)(cid:21)(cid:1)(cid:34)(cid:24)(cid:36)(cid:35)(cid:1)(cid:35)(cid:24)(cid:21)(cid:1)(cid:18)(cid:31)(cid:31)(cid:27)(cid:1)(cid:38)(cid:25)(cid:35)(cid:24)(cid:1)(cid:17)(cid:1) (cid:34)(cid:30)(cid:17)(cid:32)(cid:43) (cid:2) (cid:7)(cid:36)(cid:30)(cid:17)(cid:1)(cid:28)(cid:31)(cid:31)(cid:27)(cid:21)(cid:20)(cid:1)(cid:17)(cid:33)(cid:31)(cid:36)(cid:30)(cid:20)(cid:1)(cid:17)(cid:35)(cid:1)(cid:35)(cid:24)(cid:21)(cid:29)(cid:42)(cid:1) (cid:24)(cid:21)(cid:33)(cid:1)(cid:24)(cid:21)(cid:17)(cid:20)(cid:1)(cid:35)(cid:25)(cid:28)(cid:35)(cid:21)(cid:20)(cid:1)(cid:35)(cid:31)(cid:1)(cid:31)(cid:30)(cid:21)(cid:1)(cid:34)(cid:25)(cid:20)(cid:21)(cid:43) (cid:6)(cid:1)(cid:28)(cid:31)(cid:37)(cid:21)(cid:1)(cid:35)(cid:24)(cid:25)(cid:34)(cid:1)(cid:32)(cid:17)(cid:30)(cid:42)(cid:1)(cid:25)(cid:35)(cid:45)(cid:34)(cid:1) (cid:21)(cid:17)(cid:34)(cid:40)(cid:1)(cid:35)(cid:31)(cid:1)(cid:19)(cid:28)(cid:21)(cid:17)(cid:30)(cid:42)(cid:1)(cid:17)(cid:30)(cid:20)(cid:1) (cid:25)(cid:35)(cid:1)(cid:19)(cid:31)(cid:31)(cid:27)(cid:1)(cid:21)(cid:37)(cid:21)(cid:30)(cid:28)(cid:40) (cid:6)(cid:45)(cid:29)(cid:1)(cid:38)(cid:31)(cid:33)(cid:27)(cid:25)(cid:30)(cid:23)(cid:1)(cid:31)(cid:30)(cid:1)(cid:29)(cid:40)(cid:1) (cid:19)(cid:31)(cid:29)(cid:32)(cid:36)(cid:35)(cid:21)(cid:33)(cid:1)(cid:17)(cid:35)(cid:1)(cid:29)(cid:40)(cid:1) (cid:20)(cid:21)(cid:34)(cid:27) (cid:9)(cid:21)(cid:38)(cid:1)(cid:12)(cid:19)(cid:17)(cid:29)(cid:17)(cid:30)(cid:20)(cid:21)(cid:33)(cid:1) (cid:28)(cid:31)(cid:31)(cid:27)(cid:21)(cid:20)(cid:1)(cid:36)(cid:32)(cid:1)(cid:22)(cid:33)(cid:31)(cid:29)(cid:1) (cid:35)(cid:24)(cid:25)(cid:34)(cid:1)(cid:30)(cid:31)(cid:35)(cid:21)(cid:34)(cid:1)(cid:17)(cid:34)(cid:1)(cid:35)(cid:24)(cid:21)(cid:1) (cid:20)(cid:31)(cid:31)(cid:33)(cid:1)(cid:31)(cid:32)(cid:21)(cid:30)(cid:21)(cid:20)
Figure 2: Using CLOSE to learn stylistic captioning with-out image data. Text examples of the desired style are gath-ered from sources such as the web, books, or GPT-3. Mod-els are trained on text only and then applied to images. with various styles from a diverse set of sources, includ-ing internet reviews, books, and GPT-3 generations, and demonstrate that CLOSE models trained on this text can produce accurate and stylistically correct captions for im-ages.
Finally, we complete two analyses: A sensitivity analy-sis showing that CLOSE is robust to cases where text and image vectors differ by a constant offset, which therefore allows CLOSE to work despite seemingly large differences between the image/text embeddings. Additionally, a study on the effectiveness of using an auxiliary vision and lan-guage corpus to build an improved adapter. We ﬁnd that im-provements are possible but vary depending on the source of that data and that a particularly effective approach is to use the auxiliary data to compute a structured covariance matrix for use when adding Gaussian noise.
In summary, our contributions include: (i) introducing
the CLOSE model for zero-shot cross-modal transfer; (ii) showing that training CLOSE with text data alone, on four
V&L tasks, gives results close to models trained on both images and text; (iii) SoTA results when using only text for three of the tasks; (iv) demonstrating an application of
CLOSE for stylistic captioning; (v) analyzing how differ-ences between image/text vectors in contrastive models and how different adapters affect CLOSE’s performance. To fa-cilitate future work in the community, we release our code1. 2. Method
Model. Our approach uses the image/text encoder from a contrastive model to encode the input, and then follows many prior works (e.g., [27, 7]) by ﬁne-tuning a pre-trained language model to process the input vector, along with any additional input text, to generate output text. First, the in-put image or text vector is normalized to have unit length to match what is used in the contrastive loss. Then that vector is converted into a number of vectors, we use 4 in our exper-iments, of the same dimensionality as the language model’s embedding layer using a linear layer. Next, other input text (e.g., the hypothesis in visual entailment or the ques-tion in VQA) is tokenized and embedded with the language model’s embedding layer. Those embeddings are concate-nated with the embeddings built from the input vector to construct an input sequence for the language model.
For the sake of simplicity, we train the model genera-tively for all tasks [20, 8]. The model generates a caption, a free-form question answer, or a class name for the tasks of captioning, VQA, and visual entailment respectively. Dur-ing training, the language model and linear layer are ﬁne-tuned, but the text encoder is kept frozen to ensure the cor-respondence between text and image vectors learned during pre-training is preserved.
Modality Gap.
In practice, text and image vectors from contrastive models can be far apart, a phenomenon known as the modality gap [39]. For example, on COCO cap-tions [6] the average cosine similarity between an image and paired caption is only 0.26, while the average similarity between two unrelated captions is 0.35. Figure 3a shows this gap causes image and text vectors to fall into separate clusters in the vector space. The root cause is that the cross-entropy loss used by contrastive models only requires paired image and text vectors to be close relative to random image and text pairs, which does not necessarily mean they are close in absolute terms, see Liang et al. [39] for more dis-cussion.
We thus adopt a simple and effective solution – adding
Gaussian noise that is drawn from a standard normal distri-bution and then scaled by a hyper-parameter w, to the text vectors during training. Intuitively, this noise helps to close 1https://github.com/allenai/close the modality gap by spreading out the text vectors and over-lapping them with the image vectors. Figure 3b visually shows that even a small amount of noise leads to much bet-ter overlapping of the image and text vector spaces. The noise also encourages the model to be more robust to minor changes or variations to the input vectors, and thus be bet-ter prepared for the shift caused by switching from text to image vectors.
A second motivation for using random noise is the obser-vation that image vectors capture certain subtle visual de-tails like lighting, background, or camera position that are not reﬂected in the text vectors. To illustrate this, we show a small case study in Appendix 5 where we observe that se-mantic changes (e.g., changing the subject of a caption or image from “dog” to “cat”) result in a relatively consistent directional shift for text vectors, but has a more erratic ef-fect on image vectors. Adding noise to the text embedding helps to mitigate this problem by simulating the fact that, even for semantically similar inputs, image and text vectors can still have minor differences due to the additional infor-mation encoded in the images.
After adding the noise we re-normalize the vector to unit length to match the image vectors that will be used during evaluation. We study the modality gap and other approaches to handling it in more detail in Section 4. 3. Experiments
We report results on four V&L tasks: captioning, vi-sual entailment, VQA and visual news, and when training
CLOSE using only text generated by a language model. 3.1. Setup
We construct pure-text training datasets for these tasks training using the text annotations from the relevant datasets, and, for some tasks, text captions of the training images. Our primary point of comparison is a CLOSE model trained with the training images, in which case the images are encoded with the image encoder during train-ing in the same manner as done during testing. This model does not experience domain shift, so we view it as an upper bound. We emphasize that in practice the text training data could come from many other possible sources, see Sect. 5 and Sect. 3.3 for additional experiments that demonstrate this, we use these text sources since they closely match the data the models with images are trained on and therefore al-low us to better isolate and study what performance is lost due to the image-text domain shift.
We use T5base [52] and CLIPV iT −L/14 [51], a noise level of 0.08, and a ﬁxed set of hyper-parameters for all tasks to demonstrate our method is effective even when there is no image/text validation set to tune on. See Ap-pendix 1 for hyper-parameter details. We additionally show results when the noise level is tuned on validation sets, and
(a) No Adapter (b) Gaussian Noise (c) Mean Shift (d) Mean Shift + Noise (e) CC3M Mean Shift
Figure 3: t-SNE [64] plots for various adapters on 350 randomly selected image vectors (blue) and paired caption vectors (orange) from COCO captions. The ﬁrst two panels demonstrate CLOSE, and the remaining three show additional adapters we study in our analysis (Section 4).
Model
Prior Work
CLOSE w/o Noise
CLOSE (Ours)
CLOSE w/Tuned Noise
CLOSE w/Images
Text-Only
Cap. (Single)
Cap. (Mult.)
VE
VQA
E-VQA
VN (cid:2) (cid:2) (cid:2)
-16.4 80.5 95.4 113.2
ESPER Style [78] CLIP Cls. [57] 78.2 68.7 95.3 98.4 113.2 66.6 68.2 75.9 75.9 77.7
TAP-C [57] 38.7 60.2 59.6 61.9 65.4
--59.8 62.9 64.3 67.9 32.1 80.8 80.8 105.7
Table 1: Results on V&L tasks. Models in the last two rows require images and so are upper bounds for CLOSE. We report
CIDEr [65] for captioning with single and multiple captions, visual entailment test accuracy, VQA 2.0 test-dev accuracy,
E-VQA validation accuracy, visual news test CIDEr. See Appendix 2 for other metrics and more detailed results. when the noise is removed, to study the effect of noise on
CLOSE. 3.2. Results
Results are shown in Table 1. Due to space constraints, we only report one metric for each task here and include more results in Appendix 2. We also show the best method from prior work, when present, that does not use images.
Image Captioning. For captioning, we use text captions as both the input text and the target output text. However we
ﬁnd that, if multiple captions about one scene are available, it is beneﬁcial to use different captions about the same im-age as the input and target text. We call the ﬁrst setting cap-tioning (single) and the second captioning (multiple) and evaluate both since they facilitate different training setups.
We evaluate on COCO Captioning [6] using the Karpathy split [28]. We train our text-only models using just the cap-tions in the training data. We treat all captions per image as a group for the multiple-caption setting and use each cap-tion individually in the single-caption setting.
CLOSE reaches 95.3 CIDEr in the multiple caption set-ting, showing high captioning competency despite not using
In the single caption setting, performance is re-images. duced but can be increased to 95.4 with higher noise levels.
Our approach is substantially better than recent zero-shot methods such as MAGIC (49.3) [61] and Socratic Mod-els (44.5) [80], and is 17 points ahead of ESPER Style (78.2) [78] which also uses text captions.
Visual Entailment. Visual entailment requires determin-ing whether a premise image either entails, contradicts, or is neutral with respect to a hypothesis sentence. During training, a text premise is used instead of an image. The hypothesis sentence is always text and is encoded with the language model. We train on SNLI [45] (a language-only dataset) and evaluate on SNLI-VE [73] (a vision and lan-guage dataset). Despite not using images, CLOSE achieves similar performance to the image model. Song et al. [57] also experiment with this task, but we ﬁnd adding Gaussian noise allows us to surpass their result by over 9 points.
VQA. To train a VQA model we use data that contains a sentence describing a scene (encoded with the text encoder), a question (encoded with the language model), and a target answer. We consider two datasets. First, we pair COCO captions with questions about the same image from VQA 2.0 [17]. However, in this dataset, the questions might ask about details of the image not included in the caption, and thus cannot be answered by the text-only model. Hence we also train and evaluate on VQA-E [34] which contains a subset of the VQA 2.0 questions paired with COCO cap-tions that have been veriﬁed to contain the answer.
Model
B-4
M
C
S 12.9
MAGIC [58]
CLOSE w/COCO 29.5
CLOSE w/GPT-J RNG 19.6
CLOSE w/GPT-J Unigram 23.2 18.5
CLOSE w/OpenAI Curie 17.4 25.6 20.9 22.2 21.2 49.3 98.4 63.2 78.9 69.0 11.3 18.3 13.8 15.6 14.9
Table 2: BLEU-4, METEOR, CIDEr, and SPICE on the
COCO validation set when training on synthetic captions.
These training sets have signiﬁcantly different question distributions due to the ﬁltering done in VQA-E, so we evaluate models either on the VQA 2.0 test-dev set or the VQA-E validation set2 depending on what train set was used. There is no prior work for this task in the text-only setting, however CLOSE does outperform TAP-CV iT −B/16 [57], a CLIP-based zero-shot approach.
For VQA-E, we observe only a 3.5 point drop in accu-racy relative to image training while surpassing the base-lines. The gap is more signiﬁcant on VQA 2.0, which we attribute to the sometimes poor alignment between the cap-tions and questions, although our method is still within 5 points of the model trained on images.
Visual News. Visual news requires captioning an image in the context of a news article, and which therefore often requires mentioning the people, locations, and events from the article text [40]. CLOSE is easily extended to this set-ting by using the caption as both the image text and the target output, while the article is given as additional con-text to the language model. For this task, we randomly sample 15% of the training data each epoch due to the large dataset size, and use OpenCLIP instead of CLIP since our previous experiments found it slightly improves perfor-mance. CLOSE with images achieves over 105 CIDEr, a signiﬁcant improvement over the previous best benchmark of 50.5 CIDEr [40]. Training without images also outper-forms the previous state-of-the-art, obtaining a respectable 80.8 CIDEr. See Appendix 5 for qualitative examples.
Discussion. Overall, performance is comparable to the model trained with images showing CLOSE is able to transfer skills between modalities. Tuning the noise level can beneﬁt some tasks, therefore better heuristics for choos-ing the noise level or leveraging a small image/text valida-tion set could additionally improve performance. On the other hand, removing the noise reduces performance drasti-cally across almost all tasks. This is because the noise plays an important role in addressing the modality gap. 3.3. Training with Data from a Language Model
Next, we use CLOSE to train a captioning model on synthetic data generated by a language model. We ﬁrst con-(cid:4)(cid:17)(cid:21)(cid:22)(cid:20)(cid:23)(cid:10)(cid:22)(cid:14)(cid:18)(cid:17) (cid:4)(cid:20)(cid:13)(cid:22)(cid:9)(cid:1)(cid:5)(cid:1)(cid:8)(cid:9)(cid:21)(cid:7)(cid:20)(cid:13)(cid:19)(cid:22)(cid:13)(cid:18)(cid:17)(cid:1)(cid:18)(cid:10)(cid:1)(cid:5)(cid:17)(cid:1)(cid:13)(cid:16)(cid:5)(cid:11)(cid:13)(cid:17)(cid:5)(cid:20)(cid:26)(cid:1)(cid:21)(cid:7)(cid:9)(cid:17)(cid:9)(cid:1)(cid:22)(cid:12)(cid:5)(cid:22)(cid:1)(cid:7)(cid:18)(cid:17)(cid:22)(cid:5)(cid:13)(cid:17)(cid:21)(cid:1)(cid:22)(cid:12)(cid:9)(cid:1) (cid:25)(cid:18)(cid:20)(cid:8)(cid:21)(cid:1)(cid:6)(cid:9)(cid:10)(cid:18)(cid:20)(cid:9)(cid:1)(cid:22)(cid:12)(cid:9)(cid:1)(cid:21)(cid:9)(cid:16)(cid:13)(cid:31)(cid:7)(cid:18)(cid:15)(cid:18)(cid:17)(cid:29) (cid:4)(cid:17)(cid:27)(cid:2)(cid:18)(cid:17)(cid:22)(cid:12)(cid:25)(cid:22)(cid:1) (cid:3)(cid:25)(cid:9)(cid:16)(cid:19)(cid:15)(cid:12)(cid:21) (cid:33)(cid:29)(cid:1)(cid:6)(cid:18)(cid:5)(cid:22)(cid:21)(cid:27)(cid:1)(cid:20)(cid:5)(cid:13)(cid:17)(cid:26)(cid:28) (cid:2)(cid:1)(cid:12)(cid:5)(cid:20)(cid:6)(cid:18)(cid:20)(cid:1)(cid:10)(cid:23)(cid:15)(cid:15)(cid:1)(cid:18)(cid:10)(cid:1)(cid:6)(cid:18)(cid:5)(cid:22)(cid:21)(cid:1)(cid:18)(cid:17)(cid:1)(cid:5)(cid:1)(cid:20)(cid:5)(cid:13)(cid:17)(cid:26)(cid:1)(cid:8)(cid:5)(cid:26) (cid:34)(cid:29)(cid:1)(cid:6)(cid:5)(cid:7)(cid:18)(cid:17)(cid:27)(cid:1)(cid:6)(cid:18)(cid:25)(cid:15)(cid:28)(cid:1)(cid:3)(cid:12)(cid:9)(cid:1)(cid:6)(cid:18)(cid:25)(cid:15)(cid:1)(cid:13)(cid:21)(cid:1)(cid:10)(cid:23)(cid:15)(cid:15)(cid:1)(cid:18)(cid:10)(cid:1)(cid:11)(cid:20)(cid:9)(cid:9)(cid:17)(cid:1)(cid:24)(cid:9)(cid:11)(cid:9)(cid:22)(cid:5)(cid:6)(cid:15)(cid:9)(cid:21)(cid:1)(cid:5)(cid:17)(cid:8)(cid:1)(cid:6)(cid:5)(cid:7)(cid:18)(cid:17)(cid:29) (cid:35)(cid:29)(cid:1)(cid:12)(cid:5)(cid:20)(cid:6)(cid:18)(cid:20)(cid:1)(cid:27)(cid:1)(cid:10)(cid:23)(cid:15)(cid:15)(cid:28)(cid:1)(cid:2)(cid:1)(cid:12)(cid:5)(cid:20)(cid:6)(cid:18)(cid:20)(cid:1)(cid:10)(cid:23)(cid:15)(cid:15)(cid:1)(cid:18)(cid:10)(cid:1)(cid:6)(cid:18)(cid:5)(cid:22)(cid:21)(cid:1)(cid:18)(cid:17)(cid:1)(cid:5)(cid:1)(cid:20)(cid:5)(cid:13)(cid:17)(cid:26)(cid:1)(cid:8)(cid:5)(cid:26) (cid:30)(cid:29)(cid:29) (cid:33)(cid:36)(cid:29)(cid:1)(cid:6)(cid:5)(cid:6)(cid:13)(cid:9)(cid:21)(cid:27)(cid:1)(cid:6)(cid:15)(cid:5)(cid:17)(cid:14)(cid:9)(cid:22)(cid:28)(cid:1)(cid:3)(cid:25)(cid:18)(cid:1)(cid:6)(cid:5)(cid:6)(cid:13)(cid:9)(cid:21)(cid:1)(cid:15)(cid:5)(cid:26)(cid:13)(cid:17)(cid:11)(cid:1)(cid:18)(cid:17)(cid:1)(cid:5)(cid:1)(cid:1)(cid:6)(cid:15)(cid:5)(cid:17)(cid:14)(cid:9)(cid:22)(cid:29) (cid:34)(cid:32)(cid:29)(cid:1)(cid:10)(cid:13)(cid:20)(cid:9)(cid:27)(cid:1)(cid:12)(cid:26)(cid:8)(cid:20)(cid:5)(cid:17)(cid:22)(cid:28)(cid:1)(cid:1)(cid:2)(cid:1)(cid:6)(cid:18)(cid:26)(cid:1)(cid:13)(cid:21)(cid:1)(cid:21)(cid:13)(cid:22)(cid:22)(cid:13)(cid:17)(cid:11)(cid:1)(cid:18)(cid:17)(cid:1)(cid:22)(cid:18)(cid:19)(cid:1)(cid:18)(cid:10)(cid:1)(cid:5)(cid:1)(cid:10)(cid:13)(cid:20)(cid:9)(cid:1)(cid:12)(cid:26)(cid:8)(cid:20)(cid:5)(cid:17)(cid:22)(cid:29) (cid:8)(cid:9)(cid:20)(cid:13)(cid:12)(cid:22)(cid:1)(cid:5)(cid:12)(cid:26)(cid:24)(cid:18)(cid:20)(cid:11)(cid:21) (cid:6)(cid:9)(cid:17)(cid:13)(cid:23)(cid:9)(cid:13)(cid:12)(cid:1)(cid:7)(cid:18)(cid:11)(cid:12)(cid:15)(cid:1)(cid:2)(cid:18)(cid:17)(cid:22)(cid:14)(cid:17)(cid:23)(cid:9)(cid:22)(cid:14)(cid:18)(cid:17)
Figure 4: Prompt used to generate a synthetic caption from a language model. The language model’s continuation (high-lighted text) is used as a synthetic caption. struct a prompt that includes a natural language instruction and some example captions following an in-context learn-ing approach [4], shown in Figure 4. To generate a diverse set of captions, we preﬁx each caption with two keywords that occur in that caption, and end the prompt with two new keywords to be used in the caption to be generated (“ﬁre” and “hydrant” in Figure 4). Then diverse captions can be constructed by changing the ending keyword pair. To re-duce the chance of caption style affecting the quantitative evaluation, we take steps to better match the style of the
COCO captions, although in settings where the precise style is of less importance this would not be required. We gener-ate 100k examples from three generation methods:
GPT-J RNG. Examples are generated using a 6 billion pa-rameter open source language model, GPT-J[67], with 50 in-context examples. Keywords are sampled uniformly at random from keywords in the COCO training data.
GPT-J Unigram. Keywords are instead sampled to match the unigram distribution of COCO captions.
Curie Unigram. Generations are from OpenAI Curie3 with 20 examples and unigram-matching.
Results on COCO are shown in Table 2. Our best result achieves 78.9 CIDEr. Inspection shows that, even with our keyword sampling approach, many errors are still caused by style issues, and that style also explains the reduced perfor-mance of the Curie model. For example, the synthetic cap-tions from the Curie model are 23 times more likely than the COCO and the GPT-J captions to use the word “opens” (e.g., “a living room that opens onto the balcony”), and use
“cellphone” while “cell phone” is much more common in
COCO. More details are in Appendix 3. This illustrates how, when using this method, the choice of language model can have subtle effects on the style of captioning that will be learned. Despite this issue, this is still a very strong result that surpasses the zero-shot method MAGIC [58]. 2VQA-E does not have a test set 3https://beta.openai.com/docs/models/gpt-3
Cap.
VE
VQA
Method
Bias
Mag. MG none mean
-mean
RNG
RNG
RNG
RNG
RNG 0.0 0.8 0.8 0.2 0.5 0.8 1.0 2.0 0.26 0.62
-0.10 0.25 0.24 0.20 0.18 0.11
Δ 1.00 0.69 0.85 0.98 0.89 0.78 0.71 0.45 94.4 92.8 84.3 93.5 92.5 89.3 87.2 73.7 64.3 64.7 62.0 63.9 64.2 63.7 63.8 61.4 75.9 75.4 71.8 75.3 75.3 74.8 74.2 71.3
Table 3: Text vector translation-sensitivity analysis. The
ﬁrst three columns show the translation magnitude, the re-sulting modality gap on COCO, and the cosine similarity to the original vectors. The following columns show CIDEr captioning score, accuracy on VQA-E, and accuracy on vi-sual entailment on validation sets. 4. Analysis
Our approach opens up two intriguing questions: (1)
Why does embedding substitution work even when text and image vectors are generally quite far apart? (2) Can meth-ods that leverage additional data to better close the modality gap improve upon this approach? We do two analyses to an-swer these questions. Furthermore, we study how different choices for the contrastive embedding model or for the lan-guage model affect our method’s performance. 4.1. Sensitivity Analysis
To help answer the ﬁrst question, we perform a sensitiv-ity analysis on the input text vectors. To do this, the model is trained while adding a constant vector to the normalized text vectors and then re-normalizing, and tested on the unaltered image vectors as before. This alteration will change how the text vectors are distributed relative to the image vectors, but will not change how the text vectors are distributed relative to one another. We show results when using a random vec-tor (note the same vector is used for all of training, it will just be selected randomly at the start of training) of different magnitudes, the mean difference of text and image vectors to represent a shift towards the image vectors, and the nega-tion of that vector to shift away from the image vectors. In all cases, we continue to add Gaussian noise as before.
Results are shown in Table 3.
For random vectors (RNG), we report the average of three runs with 3 differ-ent vectors. Overall, we see only minor degradation when using random vectors until very large shifts are used, show-ing the model is generally insensitive to shifting the text vectors during training. Shifting the vectors towards the images (mean) can result in a slight gain in performance, and shifting the vectors away from them (-mean) results in a more signiﬁcant decrease, showing the model is not com-pletely insensitive. However it is still notable that vector substitutions work well even as the text vector’s positions
CLOSE
+Cov. (COCO)
+Cov. (CC3M)
+Linear (COCO)
+Linear (CC3M)
MG 0.26 0.62 0.58 0.81 0.75
Cap.
VE
VQA
VN 94.3 106.5 95.1 99.5 81.8 75.9 75.5 75.8 76.0 75.5 64.3 65.5 65.0 65.7 64.9 80.8 84.1
---Table 4: Results with adapters built with paired data. The modality gap on COCO captions, captioning CIDEr, vi-sual entailment accuracy, VQA-E accuracy and visual news
CIDEr are shown. The last task is more complex and so we only experiment it with one promising adapter. are signiﬁcantly randomized.
We hypothesize that this insensitivity is due to two rea-sons. First, most directions in the shifted feature space are predictive of the output in the same manner as before be-cause the text vectors do not change relative positions. Sec-ond, the Gaussian noise trains the model to be insensitive to shifts in unimportant directions in the feature space, which often include the direction of the shift. This insensitivity provides part of the answer to question 1. A major source of the modality gap is a constant shift between the image and text vectors [38]. However, addressing this is not as im-portant as one might expect because CLOSE is not highly sensitive to the absolute positioning of the text vectors. 4.2. Learned Adapter Analysis
As suggested by Figure 3c, mean shift might not be per-fect at aligning the text and image vectors, so we hypoth-esize more sophisticated adaption methods could improve performance. More complex adapters generally require a paired image/text corpus to train on, so we avoid using them in our main CLOSE method. However, here we in-vestigate them to better understand how much performance they could potentially contribute. To study the difference between using high-quality annotated data or web data we use both COCO captions and Conceptual Captions 3 Million (CC3M) [54]. For COCO we use the 30k captions from the
“restval” set of the Karapathy split, which do not appear in our train, eval or test sets, and for CC3M we use a random sample of 100k image/text pairs. We consider two adapters:
Linear Adapter. We learn the modality shift by training a linear model to minimize the Euclidean distance between the adapted text vector and its paired image vector. We con-tinue to add Gaussian noise after applying this model.
Structured Noise with Covariance Matrix. Even in prin-ciple, we do not expect there to be a perfect one-to-one mapping between text and image vectors because an image vector can be similar to many different texts that describe different parts or details of the image. This motivates us to approach the problem from the perspective of better un-(cid:40)(cid:74)(cid:82)(cid:70)(cid:72)(cid:81)(cid:87)(cid:85)(cid:76)(cid:70)(cid:3) (cid:38)(cid:68)(cid:83)(cid:87)(cid:76)(cid:82)(cid:81)(cid:86) (cid:56)(cid:83)(cid:79)(cid:76)(cid:73)(cid:87)(cid:76)(cid:81)(cid:74)(cid:3) (cid:38)(cid:68)(cid:83)(cid:87)(cid:76)(cid:82)(cid:81)(cid:86) (cid:43)(cid:68)(cid:85)(cid:85)(cid:92)(cid:3)(cid:51)(cid:82)(cid:87)(cid:87)(cid:72)(cid:85)(cid:3) (cid:38)(cid:68)(cid:83)(cid:87)(cid:76)(cid:82)(cid:81)(cid:86) (cid:53)(cid:72)(cid:89)(cid:76)(cid:72)(cid:90)(cid:86)(cid:3) (cid:38)(cid:68)(cid:83)(cid:87)(cid:76)(cid:82)(cid:81)(cid:86) (cid:44)(cid:3)(cid:86)(cid:68)(cid:90)(cid:3)(cid:68)(cid:3)(cid:69)(cid:76)(cid:85)(cid:71)(cid:3)(cid:83)(cid:72)(cid:85)(cid:70)(cid:75)(cid:72)(cid:71)(cid:3)(cid:82)(cid:81)(cid:3)(cid:68)(cid:3) (cid:86)(cid:68)(cid:81)(cid:71)(cid:3)(cid:69)(cid:72)(cid:68)(cid:70)(cid:75)(cid:3)(cid:79)(cid:82)(cid:82)(cid:78)(cid:76)(cid:81)(cid:74)(cid:3)(cid:68)(cid:87)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3) (cid:82)(cid:70)(cid:72)(cid:68)(cid:81)(cid:17) (cid:58)(cid:72)(cid:3)(cid:90)(cid:68)(cid:79)(cid:78)(cid:72)(cid:71)(cid:3)(cid:83)(cid:68)(cid:86)(cid:87)(cid:3)(cid:68)(cid:3)(cid:78)(cid:76)(cid:87)(cid:70)(cid:75)(cid:72)(cid:81)(cid:3) (cid:90)(cid:76)(cid:87)(cid:75)(cid:3)(cid:68)(cid:3)(cid:90)(cid:76)(cid:81)(cid:71)(cid:82)(cid:90)(cid:3)(cid:79)(cid:82)(cid:82)(cid:78)(cid:76)(cid:81)(cid:74)(cid:3)(cid:82)(cid:88)(cid:87)(cid:3) (cid:82)(cid:81)(cid:87)(cid:82)(cid:3)(cid:68)(cid:3)(cid:86)(cid:87)(cid:85)(cid:72)(cid:72)(cid:87)(cid:17) (cid:58)(cid:72)(cid:3)(cid:68)(cid:85)(cid:72)(cid:3)(cid:73)(cid:79)(cid:92)(cid:76)(cid:81)(cid:74)(cid:3)(cid:78)(cid:76)(cid:87)(cid:72)(cid:86)(cid:3)(cid:76)(cid:81)(cid:3)(cid:68)(cid:3) (cid:83)(cid:68)(cid:85)(cid:78)(cid:17) (cid:48)(cid:92)(cid:3)(cid:80)(cid:82)(cid:80)(cid:3)(cid:76)(cid:86)(cid:3)(cid:80)(cid:68)(cid:78)(cid:76)(cid:81)(cid:74)(cid:3) (cid:83)(cid:68)(cid:81)(cid:70)(cid:68)(cid:78)(cid:72)(cid:86)(cid:3)(cid:76)(cid:81)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:78)(cid:76)(cid:87)(cid:70)(cid:75)(cid:72)(cid:81)(cid:17) (cid:58)(cid:72)(cid:3)(cid:89)(cid:76)(cid:86)(cid:76)(cid:87)(cid:72)(cid:71)(cid:3)(cid:68)(cid:81)(cid:3)(cid:82)(cid:79)(cid:71)(cid:3)(cid:69)(cid:88)(cid:76)(cid:79)(cid:71)(cid:76)(cid:81)(cid:74)(cid:3) (cid:90)(cid:76)(cid:87)(cid:75)(cid:3)(cid:68)(cid:3)(cid:69)(cid:76)(cid:70)(cid:92)(cid:70)(cid:79)(cid:72)(cid:3)(cid:79)(cid:72)(cid:68)(cid:81)(cid:76)(cid:81)(cid:74)(cid:3) (cid:68)(cid:74)(cid:68)(cid:76)(cid:81)(cid:86)(cid:87)(cid:3)(cid:76)(cid:87)(cid:15)(cid:3)(cid:81)(cid:72)(cid:91)(cid:87)(cid:3)(cid:87)(cid:82)(cid:3)(cid:68)(cid:3)(cid:69)(cid:85)(cid:76)(cid:70)(cid:78)(cid:3) (cid:90)(cid:68)(cid:79)(cid:79)(cid:17) (cid:58)(cid:72)(cid:3)(cid:68)(cid:85)(cid:72)(cid:3)(cid:83)(cid:79)(cid:68)(cid:92)(cid:76)(cid:81)(cid:74)(cid:3)(cid:68)(cid:3)(cid:89)(cid:76)(cid:71)(cid:72)(cid:82)(cid:3) (cid:74)(cid:68)(cid:80)(cid:72)(cid:3)(cid:90)(cid:76)(cid:87)(cid:75)(cid:3)(cid:70)(cid:82)(cid:81)(cid:87)(cid:85)(cid:82)(cid:79)(cid:79)(cid:72)(cid:85)(cid:86)(cid:3)(cid:76)(cid:81)(cid:3) (cid:82)(cid:88)(cid:85)(cid:3)(cid:75)(cid:68)(cid:81)(cid:71)(cid:86)(cid:17) (cid:36)(cid:3)(cid:74)(cid:85)(cid:82)(cid:88)(cid:83)(cid:3)(cid:82)(cid:73)(cid:3)(cid:83)(cid:72)(cid:82)(cid:83)(cid:79)(cid:72)(cid:3)(cid:68)(cid:85)(cid:72)(cid:3) (cid:86)(cid:76)(cid:87)(cid:87)(cid:76)(cid:81)(cid:74)(cid:3)(cid:68)(cid:85)(cid:82)(cid:88)(cid:81)(cid:71)(cid:3)(cid:68)(cid:3)(cid:87)(cid:68)(cid:69)(cid:79)(cid:72)(cid:3) (cid:72)(cid:81)(cid:77)(cid:82)(cid:92)(cid:76)(cid:81)(cid:74)(cid:3)(cid:83)(cid:76)(cid:93)(cid:93)(cid:68)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3) (cid:79)(cid:68)(cid:88)(cid:74)(cid:75)(cid:87)(cid:72)(cid:85)(cid:17) (cid:36)(cid:3)(cid:73)(cid:79)(cid:82)(cid:70)(cid:78)(cid:3)(cid:82)(cid:73)(cid:3)(cid:69)(cid:76)(cid:85)(cid:71)(cid:86)(cid:3)(cid:73)(cid:79)(cid:92)(cid:3) (cid:82)(cid:89)(cid:72)(cid:85)(cid:75)(cid:72)(cid:68)(cid:71)(cid:3)(cid:68)(cid:86)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:86)(cid:88)(cid:81)(cid:3)(cid:86)(cid:72)(cid:87)(cid:86)(cid:3) (cid:76)(cid:81)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:75)(cid:82)(cid:85)(cid:76)(cid:93)(cid:82)(cid:81)(cid:17) (cid:55)(cid:90)(cid:82)(cid:3)(cid:74)(cid:76)(cid:85)(cid:79)(cid:86)(cid:3)(cid:86)(cid:76)(cid:87)(cid:87)(cid:76)(cid:81)(cid:74)(cid:3)(cid:82)(cid:81)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3) (cid:69)(cid:68)(cid:70)(cid:78)(cid:3)(cid:82)(cid:73)(cid:3)(cid:68)(cid:3)(cid:69)(cid:82)(cid:68)(cid:87)(cid:3) (cid:70)(cid:82)(cid:81)(cid:87)(cid:72)(cid:80)(cid:83)(cid:79)(cid:68)(cid:87)(cid:76)(cid:81)(cid:74)(cid:3)(cid:79)(cid:76)(cid:73)(cid:72)(cid:10)(cid:86)(cid:3) (cid:80)(cid:92)(cid:86)(cid:87)(cid:72)(cid:85)(cid:76)(cid:72)(cid:86)(cid:17) (cid:36)(cid:3)(cid:69)(cid:88)(cid:81)(cid:70)(cid:75)(cid:3)(cid:82)(cid:73)(cid:3)(cid:86)(cid:87)(cid:88)(cid:73)(cid:73)(cid:3)(cid:68)(cid:81)(cid:76)(cid:80)(cid:68)(cid:79)(cid:86)(cid:3) (cid:82)(cid:81)(cid:3)(cid:68)(cid:3)(cid:87)(cid:85)(cid:68)(cid:76)(cid:81)(cid:3)(cid:77)(cid:82)(cid:88)(cid:85)(cid:81)(cid:72)(cid:92)(cid:3)(cid:87)(cid:82)(cid:3)(cid:85)(cid:72)(cid:68)(cid:70)(cid:75)(cid:3) (cid:75)(cid:82)(cid:80)(cid:72)(cid:17) (cid:36)(cid:3)(cid:69)(cid:72)(cid:68)(cid:88)(cid:87)(cid:76)(cid:73)(cid:88)(cid:79)(cid:3)(cid:83)(cid:88)(cid:85)(cid:83)(cid:79)(cid:72)(cid:3)(cid:87)(cid:88)(cid:79)(cid:76)(cid:83)(cid:3) (cid:73)(cid:79)(cid:82)(cid:90)(cid:72)(cid:85)(cid:3)(cid:83)(cid:82)(cid:87)(cid:3)(cid:76)(cid:86)(cid:3)(cid:76)(cid:81)(cid:3)(cid:69)(cid:79)(cid:82)(cid:82)(cid:80)(cid:17) (cid:36)(cid:3)(cid:80)(cid:68)(cid:81)(cid:3)(cid:86)(cid:78)(cid:76)(cid:76)(cid:81)(cid:74)(cid:3)(cid:71)(cid:82)(cid:90)(cid:81)(cid:3)(cid:68)(cid:3) (cid:86)(cid:81)(cid:82)(cid:90)(cid:92)(cid:3)(cid:75)(cid:76)(cid:79)(cid:79)(cid:3)(cid:87)(cid:82)(cid:3)(cid:70)(cid:82)(cid:81)(cid:84)(cid:88)(cid:72)(cid:85)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3) (cid:75)(cid:76)(cid:74)(cid:75)(cid:17) (cid:43)(cid:68)(cid:85)(cid:85)(cid:92)(cid:3)(cid:51)(cid:82)(cid:87)(cid:87)(cid:72)(cid:85)(cid:3)(cid:90)(cid:68)(cid:86)(cid:3)(cid:86)(cid:82)(cid:3) (cid:72)(cid:91)(cid:70)(cid:76)(cid:87)(cid:72)(cid:71)(cid:3)(cid:87)(cid:82)(cid:3)(cid:86)(cid:87)(cid:68)(cid:85)(cid:87)(cid:3)(cid:75)(cid:76)(cid:86)(cid:3)(cid:73)(cid:76)(cid:85)(cid:86)(cid:87)(cid:3) (cid:92)(cid:72)(cid:68)(cid:85)(cid:3)(cid:68)(cid:87)(cid:3)(cid:43)(cid:82)(cid:74)(cid:90)(cid:68)(cid:85)(cid:87)(cid:86)(cid:4) (cid:42)(cid:72)(cid:79)(cid:79)(cid:72)(cid:85)(cid:87)(cid:3)(cid:42)(cid:85)(cid:76)(cid:81)(cid:71)(cid:72)(cid:79)(cid:90)(cid:68)(cid:79)(cid:71)(cid:3)(cid:79)(cid:82)(cid:82)(cid:78)(cid:72)(cid:71)(cid:3) (cid:68)(cid:85)(cid:82)(cid:88)(cid:81)(cid:71)(cid:3)(cid:68)(cid:87)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:68)(cid:86)(cid:86)(cid:72)(cid:80)(cid:69)(cid:79)(cid:72)(cid:71)(cid:3) (cid:86)(cid:87)(cid:88)(cid:71)(cid:72)(cid:81)(cid:87)(cid:86)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:86)(cid:80)(cid:76)(cid:79)(cid:72)(cid:71)(cid:17) (cid:47)(cid:88)(cid:70)(cid:76)(cid:88)(cid:86)(cid:3)(cid:48)(cid:68)(cid:79)(cid:73)(cid:82)(cid:92)(cid:3)(cid:90)(cid:68)(cid:87)(cid:70)(cid:75)(cid:72)(cid:71)(cid:3) (cid:90)(cid:76)(cid:87)(cid:75)(cid:3)(cid:86)(cid:68)(cid:87)(cid:76)(cid:86)(cid:73)(cid:68)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:68)(cid:86)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3) (cid:71)(cid:72)(cid:68)(cid:87)(cid:75)(cid:3)(cid:72)(cid:68)(cid:87)(cid:72)(cid:85)(cid:86)(cid:3)(cid:74)(cid:68)(cid:87)(cid:75)(cid:72)(cid:85)(cid:72)(cid:71)(cid:3) (cid:68)(cid:85)(cid:82)(cid:88)(cid:81)(cid:71)(cid:3)(cid:43)(cid:68)(cid:85)(cid:85)(cid:92)(cid:3)(cid:51)(cid:82)(cid:87)(cid:87)(cid:72)(cid:85)(cid:17) (cid:39)(cid:72)(cid:79)(cid:82)(cid:85)(cid:72)(cid:86)(cid:3)(cid:56)(cid:80)(cid:69)(cid:85)(cid:76)(cid:71)(cid:74)(cid:72)(cid:3)(cid:86)(cid:68)(cid:87)(cid:3)(cid:68)(cid:87)(cid:3) (cid:75)(cid:72)(cid:85)(cid:3)(cid:71)(cid:72)(cid:86)(cid:78)(cid:15)(cid:3)(cid:68)(cid:3)(cid:86)(cid:68)(cid:87)(cid:76)(cid:86)(cid:73)(cid:76)(cid:72)(cid:71)(cid:3)(cid:86)(cid:80)(cid:76)(cid:79)(cid:72)(cid:3) (cid:82)(cid:81)(cid:3)(cid:75)(cid:72)(cid:85)(cid:3)(cid:73)(cid:68)(cid:70)(cid:72)(cid:17) (cid:53)(cid:88)(cid:69)(cid:72)(cid:88)(cid:86)(cid:3)(cid:43)(cid:68)(cid:74)(cid:85)(cid:76)(cid:71)(cid:3)(cid:85)(cid:82)(cid:68)(cid:85)(cid:72)(cid:71)(cid:3)(cid:90)(cid:76)(cid:87)(cid:75)(cid:3) (cid:79)(cid:68)(cid:88)(cid:74)(cid:75)(cid:87)(cid:72)(cid:85)(cid:3)(cid:68)(cid:86)(cid:3)(cid:75)(cid:72)(cid:3)(cid:86)(cid:68)(cid:90)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3) (cid:79)(cid:82)(cid:82)(cid:78)(cid:3)(cid:82)(cid:73)(cid:3)(cid:87)(cid:72)(cid:85)(cid:85)(cid:82)(cid:85)(cid:3)(cid:82)(cid:81)(cid:3)(cid:43)(cid:68)(cid:85)(cid:85)(cid:92)(cid:3) (cid:51)(cid:82)(cid:87)(cid:87)(cid:72)(cid:85)(cid:10)(cid:86)(cid:3)(cid:73)(cid:68)(cid:70)(cid:72)(cid:17) (cid:47)(cid:82)(cid:85)(cid:71)(cid:3)(cid:57)(cid:82)(cid:79)(cid:71)(cid:72)(cid:80)(cid:82)(cid:85)(cid:87)(cid:3)(cid:79)(cid:68)(cid:88)(cid:74)(cid:75)(cid:72)(cid:71)(cid:3) (cid:86)(cid:82)(cid:73)(cid:87)(cid:79)(cid:92)(cid:15)(cid:3)(cid:68)(cid:3)(cid:70)(cid:82)(cid:79)(cid:71)(cid:3)(cid:86)(cid:82)(cid:88)(cid:81)(cid:71)(cid:3)(cid:87)(cid:75)(cid:68)(cid:87)(cid:3) (cid:80)(cid:68)(cid:71)(cid:72)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:75)(cid:68)(cid:76)(cid:85)(cid:86)(cid:3)(cid:82)(cid:81)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3) (cid:69)(cid:68)(cid:70)(cid:78)(cid:3)(cid:82)(cid:73)(cid:3)(cid:43)(cid:68)(cid:85)(cid:85)(cid:92)(cid:3)(cid:51)(cid:82)(cid:87)(cid:87)(cid:72)(cid:85)(cid:182)(cid:86)(cid:3) (cid:81)(cid:72)(cid:70)(cid:78)(cid:3)(cid:86)(cid:75)(cid:76)(cid:89)(cid:72)(cid:85)(cid:17) (cid:36)(cid:3)(cid:83)(cid:72)(cid:85)(cid:73)(cid:72)(cid:70)(cid:87)(cid:3)(cid:74)(cid:76)(cid:73)(cid:87)(cid:3)(cid:73)(cid:82)(cid:85)(cid:3)(cid:68)(cid:3)(cid:73)(cid:85)(cid:76)(cid:72)(cid:81)(cid:71)(cid:3) (cid:90)(cid:75)(cid:82)(cid:3)(cid:75)(cid:68)(cid:86)(cid:3)(cid:68)(cid:3)(cid:73)(cid:79)(cid:82)(cid:90)(cid:72)(cid:85)(cid:3)(cid:74)(cid:68)(cid:85)(cid:71)(cid:72)(cid:81)(cid:17)(cid:3) (cid:55)(cid:75)(cid:72)(cid:3)(cid:85)(cid:82)(cid:86)(cid:72)(cid:86)(cid:3)(cid:68)(cid:85)(cid:72)(cid:3)(cid:69)(cid:72)(cid:68)(cid:88)(cid:87)(cid:76)(cid:73)(cid:88)(cid:79)(cid:17) (cid:55)(cid:75)(cid:72)(cid:3)(cid:79)(cid:72)(cid:68)(cid:86)(cid:75)(cid:3)(cid:76)(cid:86)(cid:3)(cid:90)(cid:72)(cid:79)(cid:79)(cid:3)(cid:80)(cid:68)(cid:71)(cid:72)(cid:3) (cid:68)(cid:81)(cid:71)(cid:3)(cid:72)(cid:68)(cid:86)(cid:92)(cid:3)(cid:87)(cid:82)(cid:3)(cid:83)(cid:88)(cid:87)(cid:3)(cid:82)(cid:81)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3) (cid:87)(cid:68)(cid:78)(cid:72)(cid:3)(cid:82)(cid:73)(cid:73)(cid:17)(cid:3)(cid:48)(cid:92)(cid:3)(cid:71)(cid:82)(cid:74)(cid:3)(cid:76)(cid:86)(cid:3)(cid:89)(cid:72)(cid:85)(cid:92)(cid:3) (cid:75)(cid:68)(cid:83)(cid:83)(cid:92)(cid:3)(cid:90)(cid:76)(cid:87)(cid:75)(cid:3)(cid:76)(cid:87)(cid:17) (cid:55)(cid:75)(cid:76)(cid:86)(cid:3)(cid:90)(cid:68)(cid:86)(cid:3)(cid:68)(cid:3)(cid:90)(cid:72)(cid:71)(cid:71)(cid:76)(cid:81)(cid:74)(cid:3)(cid:70)(cid:68)(cid:78)(cid:72)(cid:3) (cid:73)(cid:82)(cid:85)(cid:3)(cid:80)(cid:92)(cid:3)(cid:75)(cid:88)(cid:86)(cid:69)(cid:68)(cid:81)(cid:71)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:75)(cid:72)(cid:3) (cid:79)(cid:82)(cid:89)(cid:72)(cid:71)(cid:3)(cid:76)(cid:87)(cid:17)(cid:3)(cid:43)(cid:72)(cid:3)(cid:90)(cid:68)(cid:86)(cid:3)(cid:89)(cid:72)(cid:85)(cid:92)(cid:3) (cid:75)(cid:68)(cid:83)(cid:83)(cid:92)(cid:3)(cid:90)(cid:76)(cid:87)(cid:75)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:70)(cid:68)(cid:78)(cid:72)(cid:17) (cid:55)(cid:75)(cid:76)(cid:86)(cid:3)(cid:76)(cid:86)(cid:3)(cid:68)(cid:3)(cid:74)(cid:85)(cid:72)(cid:68)(cid:87)(cid:3)(cid:82)(cid:89)(cid:72)(cid:81)(cid:17)(cid:3)(cid:44)(cid:87)(cid:3) (cid:70)(cid:82)(cid:82)(cid:78)(cid:86)(cid:3)(cid:72)(cid:89)(cid:72)(cid:81)(cid:79)(cid:92)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:76)(cid:86)(cid:3)(cid:72)(cid:68)(cid:86)(cid:92)(cid:3) (cid:87)(cid:82)(cid:3)(cid:70)(cid:79)(cid:72)(cid:68)(cid:81)(cid:17)(cid:3)(cid:44)(cid:3)(cid:90)(cid:82)(cid:88)(cid:79)(cid:71)(cid:3) (cid:85)(cid:72)(cid:70)(cid:82)(cid:80)(cid:80)(cid:72)(cid:81)(cid:71)(cid:3)(cid:76)(cid:87)(cid:17) (cid:44)(cid:3)(cid:69)(cid:82)(cid:88)(cid:74)(cid:75)(cid:87)(cid:3)(cid:87)(cid:75)(cid:76)(cid:86)(cid:3)(cid:68)(cid:86)(cid:3)(cid:68)(cid:3)(cid:74)(cid:76)(cid:73)(cid:87)(cid:3)(cid:73)(cid:82)(cid:85)(cid:3)(cid:68)(cid:3) (cid:73)(cid:85)(cid:76)(cid:72)(cid:81)(cid:71)(cid:17)(cid:3)(cid:54)(cid:75)(cid:72)(cid:3)(cid:79)(cid:82)(cid:89)(cid:72)(cid:86)(cid:3)(cid:76)(cid:87)(cid:17)(cid:3)(cid:44)(cid:87)(cid:3)(cid:76)(cid:86)(cid:3) (cid:89)(cid:72)(cid:85)(cid:92)(cid:3)(cid:86)(cid:82)(cid:73)(cid:87)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:70)(cid:88)(cid:71)(cid:71)(cid:79)(cid:92)(cid:17) (cid:41)(cid:68)(cid:86)(cid:87)(cid:3)(cid:71)(cid:72)(cid:79)(cid:76)(cid:89)(cid:72)(cid:85)(cid:92)(cid:29)(cid:3)(cid:44)(cid:3)(cid:85)(cid:72)(cid:70)(cid:72)(cid:76)(cid:89)(cid:72)(cid:71)(cid:3) (cid:80)(cid:92)(cid:3)(cid:82)(cid:85)(cid:71)(cid:72)(cid:85)(cid:3)(cid:76)(cid:81)(cid:3)(cid:68)(cid:3)(cid:87)(cid:76)(cid:80)(cid:72)(cid:79)(cid:92)(cid:3) (cid:80)(cid:68)(cid:81)(cid:81)(cid:72)(cid:85)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:76)(cid:87)(cid:3)(cid:90)(cid:68)(cid:86)(cid:3)(cid:76)(cid:81)(cid:3)(cid:74)(cid:82)(cid:82)(cid:71)(cid:3) (cid:70)(cid:82)(cid:81)(cid:71)(cid:76)(cid:87)(cid:76)(cid:82)(cid:81)(cid:17)(cid:3)(cid:44)(cid:3)(cid:90)(cid:82)(cid:88)(cid:79)(cid:71)(cid:3)(cid:82)(cid:85)(cid:71)(cid:72)(cid:85)(cid:3) (cid:73)(cid:85)(cid:82)(cid:80)(cid:3)(cid:87)(cid:75)(cid:72)(cid:80)(cid:3)(cid:68)(cid:74)(cid:68)(cid:76)(cid:81)(cid:17)
Figure 5: Examples of stylistic captions produced by CLOSE trained with only text data, and then applied 0-shot to images. derstanding how text vectors are distributed around its re-lated image vectors, instead of just trying to learn a simple mapping function. In Appendix 4, we provide insight into how the vector differences from COCO image-caption pairs follow a particular shape. To capture this shaped relation-ship between text and images, we add Gaussian noise whose mean and covariance are learned from the differences be-tween text-image vectors in the auxiliary corpus, to the text during training. This noise is expected to better simulate the text-image shift that will occur during evaluation.
Results are shown in Table 4. We observe large improve-ments on captioning, modest improvements on VQA and visual news4, and similar performance on visual entailment using the adapters from COCO, with the structured noise ap-proach being signiﬁcantly better on captioning, and slightly worse on the other tasks. The CC3M adapter also achieves mild gains, although it is less effective. This shows the training data used for the adapter is important, a point that can be qualitatively observed in Figure 3c and Figure 3e. 4.3. Performance Analysis of Different CLIP and
T5 Models
Finally, we study how different choices for the con-trastive embedding model or for the language model affect 4We only test one adapter on this task due to the longer training times
CLIP Model
T5 Model
Cap.
VE
VQA
ViT-L/14
ViT-L/14
ViT-L/14
ViT-B/32
RN101
RN50
RN50×4
RN50×16
RN50×64
OpenCLIP [24]
EVA-CLIP [13] small base large base base base base base base base base 94.4 95.4 93.9 91.1 90.0 90.2 92.0 93.4 96.1 99.2 101.7 74.9 76.1 75.1 75.3 75.4 75.3 75.3 74.4 75.8 76.3 75.53 59.9 64.3 65.2 61.4 59.8 60.4 61.5 62.5 64.2 65.1 66.6
Table 5: Ablations with different contrastive and language models. The ﬁrst column indicates which CLIP model was used, with OpenCLIP indicating we use the ViT-L/14 Open-CLIP model trained on Laion 400m [24]. The last three columns show CIDEr on COCO captioning in the single caption setting, accuracy on visual entailment, and overall accuracy on VQA-E on the validation sets. the performance of our method. Results for captioning, vi-sual entailment, and E-VQA are shown in Table 5. For these experiments we use the tuned noise values in order to com-pare best-case performance. We ﬁnd the optimal noise level for these models generally does not change as these com-ponents are altered, so we use the same noise levels as our main results for all these experiments.
There is a consistent decrease in performance when us-ing CLIP versions other than ViT-L/14, with only RN50×64 being comparable, showing that CLOSE gains effective-ness as the contrastive model becomes more powerful. We also observe much less dependence on the size of the T5 model, with the large model increasing performance on
VQA but not on the other tasks. The OpenCLIP model is generally more effective and boosts the captioning results to nearly 100 CIDEr. The EVA-CLIP model [13] further boosts VQA scores, approaching our main result with im-ages (67.9), showing that CLOSE’s performance can be im-proved by enhancing the contrastive model. 5. Stylistic Captioning
We demonstrate an application of our method by apply-ing it to the task of constructing captions with speciﬁc writ-ing styles. Our general approach is to gather text-only train-ing data that exempliﬁes the style we want the model to use, train on them as if they were text captions as done in Sec-tion 3.2, and then apply the model to images. To show that a diverse range of natural language data sources can be used to learn different styles we show four captioning styles, each of which uses a different method of collecting training data.
Ego-Centric. Section 3.3 shows that our model can be trained using data generated by a language model. Now we demonstrate an application of that approach by using the language model to generate captions in an ego-centric style. We use the same prompt format as before (Figure 4), only now with 20 examples of manually authored captions written from a ﬁrst-person perspective. We again sample keywords randomly from those found in COCO training captions to generate diverse prompts and obtain 20k cap-tions using OpenAI’s GPT-3 model. We apply this model to COCO validation images, shown in the top row of Fig-ure 5, and observe it learns to use a variety of ﬁrst-person language while accurately describing the image.
Uplifting. We use a publicly available dataset [14] to col-lect 6k examples of uplifting captions (no images). Results are shown in the second row in Figure 5, where we observe the model adds warm and optimistic details to its captions.
Character-Based. Next, we target character-based cap-tions that use proper nouns and describe images as if they were from a story. Using proper nouns would be a sig-niﬁcant hurdle for many existing systems due to the lack of image/name paired data in existing datasets. However,
CLOSE can leverage CLIP’s ability of recognizing names of famous people [51] to handle that problem. We ﬁrst pick 33 Harry Potter characters. Then only a few excerpts from the Harry Potter books or fan ﬁctions are manually collected and used, together with the characters, as prompts to GPT-3 to create 13k captions. Results on relevant photos are shown in the third row of Figure 5. The model uses the correct names and image content, while sometimes making up plausible events that could give additional context to the image as if it was a scene in a book or a movie.
Reviews. We train a model to write captions like a customer writing a review. For training data, we gather publicly-available Amazon product reviews5 and select positive re-views that are a maximum of 40 tokens long. As shown in
Figure 5 bottom row, the captions use a variety of language to write positive reviews of the items in the photos. 6.