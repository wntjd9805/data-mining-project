Abstract
Story visualization (SV) is a challenging text-to-image generation task for the difficulty of not only rendering visual details from the text descriptions but also encoding a long-term context across multiple sentences. While prior efforts mostly focus on generating a semantically relevant image for each sentence, encoding a context spread across the given paragraph to generate contextually convincing images (e.g., with a correct character or with a proper background of the scene) remains a challenge. To this end, we propose a novel memory architecture for the Bi-directional Trans-former framework with an online text augmentation that generates multiple pseudo-descriptions as supplementary supervision during training for better generalization to the language variation at inference. In extensive experiments on the two popular SV benchmarks, i.e., the Pororo-SV and
Flintstones-SV, the proposed method significantly outper-forms the state of the arts in various metrics including FID, character F1, frame accuracy, BLEU-2/3, and R-precision with similar or less computational complexity. 1.

Introduction
Story visualization (SV) [17] is a task of generating a se-quence of images from a paragraph, i.e., a sequence of natu-ral language sentences. It is challenging for the requirement of rendering the visual details in images with convincing background of a scene – seasonal elements, environmental objects such as table, location, and the proper character ap-pearing, which here we refer to as context, spread across the given text sentences. Specifically, it needs to encode im-plicit context presented in the given sentences since each one often omit visual details (i.e., they may be spread over the sentences) necessary to generate a semantically correct im-§: work done while interning at LG AI Research. †: corresponding author.
Code: https://github.com/yonseivnl/cmota
Figure 1. Linguistic variations in story visualization and the overview of the proposed method. (a) An example of a story data with various linguistic variations (Var.#) for each image. (b)
Modeling temporal context spread across sentences by our context memory (Sec. 3.1). (c) Addressing linguistic variations by online text augmentation from each image for every epoch (Sec. 3.2). (d) Benefit of the proposed context memory (left) and online text augmentation (right). age. For example, we can think of Fig. 1-(d), where Sent.#1 (i.e., sentence 1) and Sent.#2 (i.e., sentence 2) are given as sequences. After Sent.#1 is given, generated image by
Sent.#2 often exhibits a background that is not semantically
correct [17, 22, 23]. However, if we use contextual informa-tion given in Sent.#1, “The woods are covered with snow”, it leads to correct image with matching background.
In addition, widely used benchmark datasets for story visualization task [9, 17] provide a single text-image pair for training and inference, mostly due to the annotation budget constraint. This prevents the model from learning language variations, and thus harms the linguistic generalization per-formance of the model.
To address the aforementioned challenges without requir-ing large scale data and models, we propose a new memory scheme in bi-directional Transformer for encoding the con-text which generates pseudo-texts in an online fashion to address linguistic variations at inference. We call our model as Context Memory and Online Text Augmentation or
CMOTA for short. We empirically validate that our model outperforms the state-of-the-art SV methods by large mar-gins on various metrics evaluated with widely used bench-marks in the literature, i.e., Pororo-SV and Flintstones-SV.
Note that while large pre-trained models [3, 5–7, 31, 43] have shown great success in synthesizing an image or a video from a language description [13, 32, 38], huge computational complexity and large training data makes the models prohib-ited. Moreover, although we propose and evaluate the model for the standard benchmark datasets without large pretrain-ing data trained with a large model, it would be interesting to apply and evaluate the proposal in large models for further improvement.
We summarize our contributions as follows:
• We propose a new memory architecture for Transformer to selectively make use of contexts in a story paragraph.
• For better generalization of linguistic variations in the given paragraph at inference, we generate pseudo-texts and augment them in an online fashion for richer lin-guistic supervision.
• Our model significantly outperforms prior arts (even some hyper-scale models) by large margins in five eval-uation metrics with similar or less computational com-plexity. 2.