Abstract
Deep Learning models have shown remarkable perfor-mance in a broad range of vision tasks. However, they are often vulnerable to domain shifts at test-time. Test-time training (TTT) methods have been developed in an at-tempt to mitigate these vulnerabilities, where a secondary task is solved at training time, simultaneously with the main task, to be later used as an self-supervised proxy task at test-time.
In this work, we propose a novel unsuper-vised TTT technique based on the maximization of Mutual
Information between multi-scale feature maps and a dis-crete latent representation, which can be integrated to the standard training as an auxiliary clustering task. Exper-imental results demonstrate competitive classification per-formance on different popular test-time adaptation bench-marks. The code can be found at: https://github. com/dosowiechi/ClusT3.git 1.

Introduction
The domain invariance hypothesis has been key to the success of deep learning methods for computer vision. In this hypothesis, the training and testing data are both as-sumed to be drawn from the same distribution, which rarely holds in practical settings. Moreover, it has been shown in numerous studies that the performance in classification and segmentation can drop significantly when domain shifts are present [26, 24].
In response, Domain Adaptation (DA) studies the adaptation of learning algorithms to new do-mains, when different types of domain shifts are present in the test data. From this field, two promising direc-tions have emerged: Domain Generalization and Test-Time
Adaptation. On the one hand, Domain Generalization (DG)
[30, 25, 33, 14, 32] assumes a model is trained on a large source dataset composed of different domains, and evalu-*Equal contribution.
Correspondence to gustavo-adolfo.vargas-hakim.1@ens.etsmtl.ca, david.osowiechi.1@ens.etsmtl.ca ates the performances on new domains at test-time. On the other hand, Test-Time Adaptation (TTA) [31, 18, 13, 1] adapts the model to test data on the fly, typically adjusting to subsets of the new domain (e.g., mini-batches) each time.
In TTA, there is no supervision from the testing samples nor access to the source domain, which makes it a challenging, yet realistic problem. The main limitation of DG is the re-quirement of a large amount of training data from different domains, without the guarantee that the model generalizes well to the (virtually unlimited) possible new domains it may encounter. TTA methods do not have this issue. How-ever, they are highly sensitive to the choice of the unsu-pervised loss functions deployed at test-time, which may severely hurt the performances.
Test-Time Training (TTT) [28, 19, 7, 23] is an attrac-tive variant of TTA, where an auxiliary task is learned from the training data (source domain) and later used at test-time to update a model. Typically, unsupervised and self-supervised tasks are chosen, as they allow for an adaptation process that does not require any label. Moreover, the joint, two-task training protocol for the source domain provides momentum at test-time, enabling the use of a loss function that is not completely foreign to the model.
Inspired by the recent success of Mutual-Information (MI) maximization in several learning tasks, such as repre-sentation learning [12, 10, 22, 29], deep clustering [11] and few-shot learning [2], we propose an information invariant
TTT method called ClusT3. Our method maximizes the MI between the feature maps at different scales and discrete latent representations related to clustering. The main idea is that the amount of information between the features and their corresponding discrete encoding should remain con-stant in both the source and target domains (see Fig. 1). To-ward this goal, we introduce an auxiliary task that performs information-maximization clustering while training on the source examples. At test time, we use the MI between the features and cluster assignments as a measure of representa-tion quality, and maximize the MI as objective for test-time adaptation. Unlike previous TTT approaches, which rely
(a) Source DS and Target DT distributions (b) Cumulative density function of DS (c) Cumulative density function of DT
Figure 1. Illustration of our Information Invariant TTT method on a 1D feature space. (a) The clustering of source features x (blue) into K = 10 regions, maximizing the entropy of the cluster marginal distribution H(Z), is such that regions have the same probability mass in the source distribution. At test-time, the probability density function of the target domain (red) is shifted, which results in a different clustering of features. (b) The optimal clustering corresponds to dividing the cumulative density function (CDF) in even steps, giving a cluster marginal entropy of H(Z) = log2(K) ≈ 3.332. (c) Since the CDF of the target is not divided in even steps, the mutual information between features x and clusters z is no longer maximized. Note: we assume that cluster assignments are confident, i.e., H(Z|X) ≈ 0 and thus I(Z; X) = H(Z)−H(Z|X) ≈ H(Z). on problem-specific, self-supervised learning strategies, our auxiliary clustering task is problem-agnostic and could be added on top of any model via a low-dimensional linear projection. Test-time adaptation could also be done using only the test samples, without any type of distilled informa-tion from the source domain. On the technical side, mini-mal architectural changes are needed, and the joint training approach is more efficient than proceeding with multiple, complex and time-consuming steps.
Our contributions could be summarized as follows:
• We propose a novel Test-Time Training approach based on maximizing the MI between feature maps and discrete representations learned in training. At test time, adaptation is achieved based on the principle that information between the features and their discrete representation should remain constant across domains.
• ClusT3 is evaluated across a series of challenging TTA scenarios, with different types of domain shifts, ob-taining competitive performance compared to previous methods.
• To the best of our knowledge, this is the first Unsuper-vised Test-Time Training approach using a joint train-ing based on the MI and linear projectors. Our ap-proach is lightweight and more general than its previ-ous self-supervised counterparts.
The rest of this paper is organized as follows. Section 2 presents previous work in both TTA and TTT. Section 3 in-troduces the ClusT3 method with the experimental setting to evaluate it in Section 4. Experimental results and discus-sions are provided in Section 5, and the closing conclusions are given in Section 6. 2.