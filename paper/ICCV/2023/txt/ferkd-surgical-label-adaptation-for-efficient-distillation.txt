Abstract
We present FerKD, a novel efficient knowledge distil-lation framework that incorporates partial soft-hard label adaptation coupled with a region-calibration mechanism.
Our approach stems from the observation and intuition that standard data augmentations, such as RandomResized-Crop, tend to transform inputs into diverse conditions: easy positives, hard positives, or hard negatives. In traditional distillation frameworks, these transformed samples are uti-lized equally through their predictive probabilities derived from pretrained teacher models. However, merely relying on prediction values from a pretrained teacher, a common practice in prior studies, neglects the reliability of these soft label predictions. To address this, we propose a new scheme that calibrates the less-confident regions to be the context using softened hard groundtruth labels. Our approach in-volves the processes of hard regions mining + calibration.
We demonstrate empirically that this method can dramat-ically improve the convergence speed and final accuracy.
Additionally, we find that a consistent mixing strategy can stabilize the distributions of soft supervision, taking advan-tage of the soft labels. As a result, we introduce a stabi-lized SelfMix augmentation that weakens the variation of the mixed images and corresponding soft labels through mixing similar regions within the same image. FerKD is an intuitive and well-designed learning system that elim-inates several heuristics and hyperparameters in former
FKD solution [37]. More importantly, it achieves remark-able improvement on ImageNet-1K and downstream tasks.
For instance, FerKD achieves 81.2% on ImageNet-1K with
ResNet-50, outperforming FKD and FunMatch by remark-able margins. Leveraging better pre-trained weights and larger architectures, our finetuned ViT-G14 even achieves 89.9%. Our code is available at https://github. com/szq0214/FKD/tree/main/FerKD. 1.

Introduction
Knowledge Distillation (KD) [13] has achieved impres-sive results in various visual domains, including image clas-Figure 1: Illustration of motivation for FerKD. The left figure depicts the original input, and the middle figure shows the center points of bounding boxes generated using
RandomResizedCrop. The radius of each circle corre-sponds to the area of the bounding box. It can be observed that the center points of the bounding boxes are concen-trated in the center of the image, and their area increases as they approach the center. The right figure displays several top and bottom confident bounding boxes and their corre-sponding predictive probabilities from a pre-trained teacher or teachers ensemble. The proposed hard region calibration strategy is established based on these predictions. sification [52, 36, 5, 37], object detection [6, 46, 11, 8, 53] and semantic segmentation [22, 15, 16]. However, KD methods are often computationally expensive and ineffi-cient due to the additional computational burden imposed by the teacher models. The primary advantage of KD that motivates its usage is its ability to generate precise soft la-bels that convey more informative details about the input examples. It differs from other label softening techniques, such as label smoothing [43], Mixup [58], and CutMix [56], mainly in two aspects: (1) KD generates soft labels dynami-cally in each iteration, which is more informative than fixed smoothing patterns used in label smoothing; (2) Mixup and
CutMix techniques essentially combine hard labels with co-efficients, while KD produces soft labels that are highly cor-related with the input sample. This allows KD’s soft labels to become more accurate when different data augmenta-tions, such as RandomResizedCrop, flipping and rotation, color jittering, etc., are applied. In general, mixing-based label softening methods cannot monitor such changes in in-put content, but KD can address them effortlessly.
range (P ) ratio range (P ) agg. ratio
[0.0, 0.1)
[0.1, 0.2)
[0.2, 0.3)
[0.3, 0.4)
[0.4, 0.5)
[0.5, 0.6)
[0.6, 0.7)
[0.7, 0.8)
[0.8, 0.85)
[0.85, 0.9)
[0.9, 0.95)
[0.95, 1.0)
[0, 0.1) 0.43%
[0, 0.2) 0.89%
[0, 0.3) 1.29%
[0, 0.4) 2.03%
[0, 0.5) 3.66%
[0, 0.6) 4.35%
[0, 0.7) 5.04%
[0, 0.8) 7.76%
[0, 0.85) 8.14% 28.73% [0, 0.9) 37.34% [0, 0.95)
[0, 1.0) 0.33% 0.43% 1.32% 2.61% 4.64% 8.31% 12.65% 17.69% 25.45% 33.59% 62.32% 99.67% 100%
Table 1: Detailed statistics of soft labels. “range” indicates max-probability of crops, “ratio” indicates the percentage in the whole crops. “agg. ratio” is the aggregated ratio. lenging or complex regions within an image that the model struggles to identify accurately. These regions can include objects with complex shapes, occlusions, or those with low contrast. By identifying these regions, the model can fo-cus on learning the features and characteristics of these re-gions, resulting in improved performance. Calibration, on the other hand, involves adjusting the confidence levels of the model’s predictions in challenging regions. The model’s predictions may be less reliable in hard regions, leading to lower confidence scores. Calibrating the predictions can im-prove the model’s accuracy in these regions by adjusting the confidence levels of the predictions. We found that carefully discarding a portion of negative crops and selecting those hard positive crops by calibrating their labels, can force the training process more efficient and effective.
Stable Training on Soft Labels. Mixture-based augmen-tations, such as Mixup and CutMix have seen widespread use for training models under hard supervision, where each image is labeled with a single class label. However, in the soft label scenario, we have made a different observation: when employed together with pre-generated soft labels on a typical ResNet, Mixup and CutMix tend to be overly strong, which, conversely, leads to decreased performance. To mit-igate label fluctuations and achieve more stable training, we propose a SelfMix scheme, which is particularly suitable for cases where data augmentation should not be so strong, such as in finetuning distillation, where mixture-based aug-mentation is usually disabled. On the other hand, when training ViT models from scratch, stronger data augmen-tation can yield better results [45, 42], which is consistent with the larger capacity perspective of this type of network.
In summary, our contributions of this work are:
- We present FerKD, a sample-calibration framework for
Faster Knowledge Distillation that achieves state-of-the-art performance. We conduct extensive analysis, ablation, and discussion on the impact of hard and easy samples.
- We make two key observations in the pre-generated soft
Figure 2: Statistics of soft label max-probability for crops on ImageNet-1K. The soft label is from FKD [37]. In each image, 500 regions are randomly cropped.
To overcome the computational inefficiency of tradi-tional knowledge distillation, FKD [37] was developed to generate region-level soft labels in advance and reuse them across multiple training cycles to eliminate redundant com-putation. This approach only requires the preparation of soft labels once at the beginning and they can be reused indefinitely. However, this approach overlooks certain crit-ical issues. One is the quality of the soft labels. When us-ing RandomResizedCrop to generate regions, some may be cropped from background areas, and the teacher model will still produce a soft label for them based on their similarity to the dataset classes. However, in some cases, these areas may contain irrelevant noise, compensatory information, or context information for the class, and the soft labels may not accurately reflect the context of information they carry.
To address this problem, this work proposes to recalibrate these soft labels by incorporating context information from hard ground-truth labels with smoothing.
Furthermore, due to the random nature of the sampling process, a certain proportion of crops that are either ex-cessively easy or difficult do not contribute to the model’s learning capacity. As demonstrated in Fig. 2 and Table 1, these samples can be discarded to expedite the convergence process. The pre-generated soft labels can be utilized as useful indicators to select these specific samples.
In our adaptation of surgical soft labeling, we categorize the soft labels into four distinct groups: extreme hard (negative), moderate hard (background or context), hard positive (par-tial object), and easy positive. Each of these categories is subject to different treatment methodologies.
The Role of