Abstract
The superior performance of modern deep networks usu-ally comes with a costly training procedure. This paper presents a new curriculum learning approach for the effi-cient training of visual backbones (e.g., vision Transform-ers). Our work is inspired by the inherent learning dynam-ics of deep networks: we experimentally show that at an earlier training stage, the model mainly learns to recog-nize some ‘easier-to-learn’ discriminative patterns within each example, e.g., the lower-frequency components of im-ages and the original information before data augmen-tation. Driven by this phenomenon, we propose a cur-riculum where the model always leverages all the train-ing data at each epoch, while the curriculum starts with only exposing the ‘easier-to-learn’ patterns of each exam-ple, and introduces gradually more difficult patterns. To implement this idea, we 1) introduce a cropping opera-tion in the Fourier spectrum of the inputs, which enables the model to learn from only the lower-frequency compo-nents efficiently, 2) demonstrate that exposing the features of original images amounts to adopting weaker data aug-mentation, and 3) integrate 1) and 2) and design a cur-riculum learning schedule with a greedy-search algorithm.
The resulting approach, EfficientTrain, is simple, general, yet surprisingly effective. As an off-the-shelf method, it re-duces the wall-time training cost of a wide variety of popu-lar models (e.g., ResNet, ConvNeXt, DeiT, PVT, Swin, and
CSWin) by > 1.5× on ImageNet-1K/22K without sacrific-ing accuracy. It is also effective for self-supervised learn-ing (e.g., MAE). Code is available at https://github. com/LeapLabTHU/EfficientTrain. 1.

Introduction
The success of modern visual backbones is largely fueled by the interest in exploring big models on large-scale bench-*Equal contribution. (cid:12)Corresponding author.
Figure 1: (a) Sample-wise curriculum learning (CL): making a discrete decision on whether each example should be leveraged to train the model. (b) Generalized CL: we consider a continuous function Tt(·), which only exposes the ‘easier-to-learn’ patterns within each example at the beginning of training (e.g., lower-frequency components; see: Section 4), while grad-ually introducing relatively more difficult patterns as learning progresses. mark datasets [1, 2, 3, 4]. In particular, the recent introduc-tion of vision Transformers (ViTs) scales up the number of model parameters to more than 1.8 billion, with the training data expanding to 3 billion samples [3, 5]. Although state-of-the-art accuracy has been achieved, this huge-model and high-data regime results in a time-consuming and expen-sive training process. For example, it takes 2,500 TPUv3-core-days to train ViT-H/14 on JFT-300M [3], which may be unaffordable for practitioners in both academia and in-dustry. Additionally, the power consumption leads to sig-nificant carbon emissions [6, 7]. Due to both economic and environmental concerns, there has been a growing demand for reducing the training cost of modern deep networks.
In this paper, we contribute to this issue by revisiting the idea of curriculum learning [8], which reveals that a model can be trained efficiently by starting with the easier aspects of a given task or certain easier subtasks, and increasing the difficulty level gradually. Most existing works implement this idea by introducing easier-to-harder examples progres-sively during training [9, 10, 11, 12, 13, 14, 15, 16]. How-ever, obtaining a light-weighted and generalizable difficulty measurer is typically non-trivial [9, 10]. In general, these methods have not exhibited the capacity to be a universal
efficient training technique for modern visual backbones.
In contrast to prior works, this paper seeks a simple yet broadly applicable efficient learning approach with the po-tential for widespread implementation. To attain this goal, we consider a generalization of curriculum learning. In spe-cific, we extend the notion of training curricula beyond only differentiating between ‘easier’ and ‘harder’ examples, and adopt a more flexible hypothesis, which indicates that the discriminative features of each training sample comprise both ‘easier-to-learn’ and ‘harder-to-learn’ patterns. Instead of making a discrete decision on whether each example should appear in the training set, we argue that it would be more proper to establish a continuous function that adap-tively extracts the simpler and more learnable discrimina-tive patterns within every example. In other words, a cur-riculum may always leverage all examples at any stage of learning, but it should eliminate the relatively more difficult or complex patterns within inputs at earlier learning stages.
An illustration of our idea is shown in Figure 1.
Driven by our hypothesis, a straightforward yet surpris-ingly effective algorithm is derived. We first demonstrate that the ‘easier-to-learn’ patterns incorporate the lower-frequency components of images. We further show that a lossless extraction of these components can be achieved by introducing a cropping operation in the frequency do-main. This operation not only retains exactly all the lower-frequency information, but also yields a smaller input size for the model to be trained. By triggering this operation at earlier training stages, the overall computational/time cost for training can be considerably reduced while the final per-formance of the model will not be sacrificed. Moreover, we show that the original information before heavy data aug-mentation is more learnable, and hence starting the train-ing with weaker augmentation techniques is beneficial. Fi-nally, these theoretical and experimental insights are inte-grated into a unified ‘EfficientTrain’ learning curriculum by leveraging a greedy-search algorithm.
One of the most appealing advantages of EfficientTrain may be its simplicity and generalizability. Our method can be conveniently applied to most deep networks with-out any modification or hyper-parameter tuning, but sig-nificantly improves their training efficiency. Empirically, for the supervised learning on ImageNet-1K/22K [17], Effi-cientTrain reduces the wall-time training cost of a wide vari-ety of popular visual backbones (e.g., ConvNeXt [18], DeiT
[19], PVT [20], Swin [4], and CSWin [21]) by more than 1.5
, while achieving competitive or better performance compared with the baselines.
Importantly, our method is also effective for self-supervised learning (e.g., MAE [22]).
× 2.