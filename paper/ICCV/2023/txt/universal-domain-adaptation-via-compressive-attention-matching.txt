Abstract
Universal domain adaptation (UniDA) aims to trans-fer knowledge from the source domain to the target do-main without any prior knowledge about the label set. The challenge lies in how to determine whether the target sam-ples belong to common categories. The mainstream meth-ods make judgments based on the sample features, which overemphasizes global information while ignoring the most crucial local objects in the image, resulting in limited accu-racy. To address this issue, we propose a Universal Atten-tion Matching (UniAM) framework by exploiting the self-attention mechanism in vision transformer to capture the crucial object information. The proposed framework in-troduces a novel Compressive Attention Matching (CAM) approach to explore the core information by compressively representing attentions. Furthermore, CAM incorporates a residual-based measurement to determine the sample com-monness. By utilizing the measurement, UniAM achieves domain-wise and category-wise Common Feature Align-ment (CFA) and Target Class Separation (TCS). Notably,
UniAM is the first method utilizing the attention in vision transformer directly to perform classification tasks. Exten-sive experiments show that UniAM outperforms the current state-of-the-art methods on various benchmark datasets. 1.

Introduction
While deep neural networks have achieved remarkable success on visual tasks [12, 25, 19, 73, 51, 69, 70], their per-formance heavily relies on the assumption of independently and identically distributed (i.i.d.) training and test data [57].
However, this assumption is frequently violated due to the presence of domain shift in real-world scenarios [52, 33, 34, 68, 67, 74, 72]. Unsupervised Domain Adaptation (DA) [1]
* Equal contributions.
â€  Corresponding authors.
Figure 1: Left: Illustration of Universal Domain Adapta-tion. Right: Shape-bias Analysis. Plot shows shape-texture trade off for attention and feature in ViT and humans. has emerged as a promising solution to address this limi-tation by adapting models trained on a source domain to perform well on an unlabeled target domain. Nevertheless, most existing DA approaches [14, 56, 49, 40, 39, 38] as-sume that the label spaces in the source and target domains are identical, which may not always hold in practical sce-narios. Partial Domain Adaptation (PDA) [4] and Open Set
Domain Adaptation (OSDA) [43] have been proposed to handle cases where the label spaces in one domain include those in the other, but these still rely on prior knowledge on label set, limiting knowledge generalizing from one sce-nario to others. Universal domain adaptation (UniDA) [66] considers a more practical and challenging scenario where the relationship of label space between source and target domains is completely unknown i.e. with any number of common, source-private and target-private classes.
In UniDA, the primary objective is to develop a model capable of precisely categorizing target samples as one of the common classes or an "unknown" class as shown in Fig. 1 left. Existing UniDA methods aim to design a transferability criteria to detect common and private classes solely based on the discriminability of deep fea-Figure 2: Attention Visualization accross domains. Attention patterns vary significantly between different classes of images. However, within the same class, attention can also exhibit variations due to differences in object size, position, and orientation. These variations are collectively referred to as attention mismatch. tures [6, 7, 8, 9, 13, 27, 29, 47, 48, 50, 66]. However, over-reliance on deep features can impede model adapta-tion performance, as they have a strong bias towards global information like texture rather than the essential object in-formation like shape [16, 20], which is considered by hu-mans as the most critical cue for recognition [28]. Fortu-nately, recent studies have demonstrated that vision trans-former (ViT) [24] exhibits a stronger shape bias than Con-volutional Neural Network (CNN) [41, 55]. As shown in
Fig. 1 right, we confirmed that such strong object shape bias is mainly attributed to the self-attention mechanism, verified in a similar way as [16]. Figure 2 demonstrates the attention vectors of samples accross domains. Although we can leverage the attention to focus on more object parts, the attention mismatch problem may still exist due to do-main shift, which refers to the attention vectors of same-class samples from different domains having some degree of the difference caused by potential variations in object size, orientation, and position across different domains. At-tention mismatch can hinder the accurate classification of samples, especially when objects of different classes share similar sizes or positions. For example, in Figure 2, the kettle in the source domain and the flower in the target do-main have more similar attention patterns. Therefore, the key challenge in utilizing attention is to effectively explore and leverage the object information embedded in attention while mitigating the negative impact of attention mismatch.
In this paper, we propose a novel Universal Attention
Matching (UniAM) framework to address the UniDA prob-lem by leveraging both the feature and attention information in a complementary way. Specifically, UniAM introduces a
Compressive Attention Matching (CAM) approach to solve the attention mismatch problem implicitly by sparsely rep-resenting target attentions using source attention prototypes.
This allows CAM to identify the most relevant attention prototype for each target sample and distinguish irrelevant private labels. Furthermore, a residual-based measurement is proposed in CAM to explicitly distinguish common and private samples across domains. By integrating attention information with features, we can mitigate the interference caused by domain shift and focus on label shift to some ex-tent. With the guidance of CAM, the UniAM framework achieves domain-wise and category-wise common feature alignment (CFA) and target class separation (TCS). By us-ing an adversarial loss and a source contrastive loss, CFA identifies and aligns the common features across domains, ensuring their consistency and transferability. On the other hand, TCS enhances the compactness of the target clusters, leading to better separation among all target classes. This is accomplished through a target contrastive loss, which en-courages samples from the same target class to be closer together and farther apart from samples with other classes. (1) We propose the UniAM framework that comprehensively considers both attention and feature information, which allows for more accurate identification of common and private samples. (2) We vali-date the strong object bias of attention in ViT. To the best of our knowledge, we are the first to directly utilize attention in
ViT for classification prediction. (3) We implicitly explore object information by sparsely reconstructing attention, en-abling better common feature alignment (CFA) and target class separation (TCS). (4) We conduct extensive experi-ments to show that UniAM can outperform current state-of-the-art approaches.
Main Contributions: 2.