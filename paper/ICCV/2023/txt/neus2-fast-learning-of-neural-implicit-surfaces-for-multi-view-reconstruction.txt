Abstract
Recent methods for neural surface representation and rendering, for example NeuS [59], have demonstrated the remarkably high-quality reconstruction of static scenes.
However, the training of NeuS takes an extremely long time (8 hours), which makes it almost impossible to apply them to dynamic scenes with thousands of frames. We propose a fast neural surface reconstruction approach, called NeuS2, which achieves two orders of magnitude improvement in terms of acceleration without compromising reconstruction quality. To accelerate the training process, we parameter-ize a neural surface representation by multi-resolution hash encodings and present a novel lightweight calculation of second-order derivatives tailored to our networks to lever-age CUDA parallelism, achieving a factor two speed up. To further stabilize and expedite training, a progressive learn-ing strategy is proposed to optimize multi-resolution hash encodings from coarse to fine. We extend our method for fast training of dynamic scenes, with a proposed incremen-tal training strategy and a novel global transformation pre-diction component, which allow our method to handle chal-lenging long sequences with large movements and defor-mations. Our experiments on various datasets demonstrate that NeuS2 significantly outperforms the state-of-the-arts in both surface reconstruction accuracy and training speed for both static and dynamic scenes. The code is available at our website: https://vcai.mpi-inf.mpg.de/ projects/NeuS2/.
∗ Equal contribution
Figure 1. We present NeuS2, a fast neural scene reconstruction method. Given a set of multi-view images, NeuS2 can accurately reconstruct the scene geometry and appearance in the order of minutes. This is in stark contrast to previous work [59], which only recovers medium-scale details at significantly increased time (about 8 hours). Moreover, we demonstrate that NeuS2 can also be applied to dynamic scene reconstruction from multi-view videos, where we recover per-frame reconstruction in about 20 seconds. 1.

Introduction
Reconstructing the dynamic 3D world from 2D images is crucial for many Computer Vision and Graphics appli-cations, such as AR/VR, 3D movies, games, telepresence, and 3D printing. Classical stereo algorithms employ com-puter vision methods, e.g. feature matching, to capture the geometry and appearance of 3D contents from multi-view 2D images. Despite great progress, these methods are still comparably slow and struggle to reconstruct high-quality results.
Recently, 3D reconstruction with neural implicit repre-sentations has become a promising alternative to traditional methods, because of its high spatial resolution and highly detailed reconstructions outperforming classical stereo al-gorithms [4, 12, 13, 50, 10, 5, 51]. NeuS [59], as a rep-resentative work, models geometry surfaces as a neural network encoded Signed Distance Field (SDF), and ren-ders an image via differentiable volume rendering. While
NeuS [59] produces high-quality reconstruction results, its training process is extremely slow, i.e. about 8 hours for a static object. This makes it nearly impossible to recon-struct dynamic scenes. Instant-NGP [36] has explored the training acceleration of neural radiance fields (NeRF) [35] by utilizing multi-resolution hash tables to augment neural network-encoded radiance fields. Though Instant-NGP [36] synthesizes impressive novel view synthesis results, the ex-tracted geometry from the learned density fields contains discernible noise since the 3D representation lacks surface constraints.
To overcome these drawbacks, we propose NeuS2, a new method for fast training of highly-detailed neural implicit surfaces (see Fig. 1). NeuS2 reconstructs a static object in minutes, and a moving object sequence in up to 20 sec-onds per frame. To achieve this, we parameterize the neural network-encoded SDF using multi-resolution hash tables of learnable feature vectors [36]. Notably, in this design, a sur-face constraint and the rendering formulation require cal-culating the second-order derivatives. The main challenge is to have a simple and memory-efficient calculation to achieve the highest possible GPU computing performance.
Therefore, we derive a simple formula of the second-order derivatives tailored to ReLU-based MLPs, which enables an efficient CUDA implementation with a small memory footprint at a significantly reduced computational cost. To further enforce and accelerate the training convergence, we introduce an efficient progressive training strategy, which updates the hash table features in a coarse-to-fine manner.
We further extend our method to multi-view dynamic scene reconstruction. Instead of training each frame in the sequence separately, we propose a new incremental learning strategy to efficiently learn a neural dynamic representation of objects with large movements and deformations. Specif-ically, we exploit the similarity of the shape and appearance information shared in two consecutive frames by first train-ing the first frame and sequentially fine-tuning the subse-quent frames. While generally, this strategy works well, we observe that when the movement between two consecutive frames is relatively large, the predicted SDF of the occluded regions that are not observed in most images may get stuck in the learned SDF of the previous frame. To address this, we predict a global transformation to roughly align these two frames before learning the representation of the new frame. In summary, our technical contributions are:
• We propose a new method, NeuS2, for fast learn-ing of neural surface representations from multi-view
RGB input for both static and dynamic scenes, which achieves a significant speed up over the state-of-the-art while achieving an unprecedented reconstruction qual-ity.
• A simple formulation of the second-order derivatives tailored to ReLU-based MLPs is presented to enable efficient parallelization of GPU computation.
• A progressive training strategy for learning multi-resolution hash encodings from coarse to fine is pro-posed to enforce better and faster training convergence.
• We design an incremental learning method with a novel global transformation prediction component for reconstructing long sequences (e.g., 2000 frames) with large movements in an efficient and stable manner. 2.