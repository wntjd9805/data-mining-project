Abstract
Recently, Video Question-Answering (VideoQA) has drawn more and more attention from both the industry and the research community. Despite all the success achieved by recent works, dataset bias always harmfully misleads current methods focusing on spurious correlations in train-ing data. To analyze the effects of dataset bias, we frame the VideoQA pipeline into a causal graph, which shows the causalities among video, question, aligned feature be-tween video and question, answer, and underlying con-founder. Through the causal graph, we prove that the con-founder and the backdoor path lead to spurious causality.
To tackle the challenge that the confounder in VideoQA is unobserved and non-enumerable in general, we pro-pose a model-agnostic framework called Knowledge Proxy
Intervention (KPI), which introduces an extra knowledge proxy variable in the causal graph to cut the backdoor path and remove the effect of confounder. Our KPI frame-work exploits the front-door adjustment, which requires no prior knowledge about the confounder. The effectiveness of our KPI framework is corroborated by three baseline methods on ﬁve benchmark datasets, including MSVD-QA,
MSRVTT-QA, TGIF-QA, NExT-QA, and Causal-VidQA. 1.

Introduction
In recent years, Video Question-Answering (VideoQA) has drawn more attention from the industry and research community due to its essential role in interactive artiﬁcial intelligence and recognition science. In VideoQA, there are three crucial challenges, (1) how to capture the visual clues in the video (e.g., object, action, and causality), (2) how to parse the semantics and syntax in language, and (3) how to align the visual clue with the linguistic semantics and syn-tax. Therefore, lots of works [12, 25, 22, 62, 35, 34, 5] have studied the VideoQA from these three aspects, and have also achieved great success in both open-ended VideoQA [66,
∗Corresponding author.
Figure 1. Two examples show how dataset bias affects the answer.
The biased answer is generated by HQGA [62]. Green and red denote the ground-truth and the biased answer for each question. 24] and multi-choice VideoQA [24, 61, 32].
As the core of VideoQA, video (V), question (Q), and the aligned feature between video and question (aligned fea-ture for short, H) play essential roles in predicting answer (A). However, due to dataset bias, most of existing meth-ods, which target at predicting answers directly from the observational probability P (A|V, Q, H), will be inevitably misled to spurious correlation, and have trouble in revealing the causal relation between the V, Q, H, and A. In Figure 7, we show two examples to explain how dataset bias affects the answer prediction. For example, in Figure 7 (a), since the kangaroo can rarely appear indoors, the model would ig-nore the “unique jumping pose” and the “distinct wobble of tail” from the kangaroo, and regard it as a cat. Furthermore, dataset bias is from nature (Zipf’s law [60] and social con-ventions [19]), i.e., more cats are indoors, and more kanga-roos are outdoors. Therefore, simply enlarging the dataset would never eliminate dataset bias. To this end, we focus on dataset bias in VideoQA task and exploit the concepts of confounder to analyze and alleviate this problem.
The causal graph of the VideoQA pipeline is illustrated in Figure 2 (a), where V, Q, H, A, and C represent video, question, aligned feature, answer, and confounder, respec-Figure 3. The conditional probability of answers given question or video concepts in MSVD-QA. Since the distribution of the condi-tional probabilities is wide, we use logarithmic axis. Only several examples are visualized to avoid clutter. Best viewed by zoom-in. used in causal intervention for its intuitive formulation, i.e., P (A|do(V, Q, H)) = (cid:80) c P (A|V, Q, H, c)P (c) (Fig-ure 2 (b)). However, the backdoor adjustment requires con-founder to be observable and enumerable, which cannot be implemented in VideoQA. Therefore, we implement causal intervention with front-door adjustment by introducing an intermediate variable, the knowledge proxy Z, in the causal graph, where no prior knowledge for confounder is required. The front-door adjustment decomposes the i.e., causal intervention into two parts (Figure 2 (c)),
P (A|do(V, Q, H)) = (cid:80) z P (z|do(V, Q, H))P (A|do(z)).
The intermediate variable Z in Figure 2 (c) is the proxy to Q and H, which should summarize the information of question (Q), and aligned feature (H) and cover the knowl-edge for answer prediction. Towards this end, we name our framework as Knowledge Proxy Intervention (KPI) frame-work, which is a model-agnostic framework for the causal inference of VideoQA and aims to alleviate the effects of dataset bias. Note that some works [35, 34] also ex-plore dataset bias from the aspect of complement frame and causal frame, i.e., whether the frames in the video are re-lated to question answering. However, the frame-level bias is only one kind of dataset bias, and even in the causal scene, there is still dataset bias, as shown in Figure 7. In this way, our KPI framework is fundamentally different from exist-ing causal VideoQA methods [35, 34], which are domain-speciﬁc and comply with observed-confounder assumption.
In this paper, we propose KPI framework, an implemen-tation of front-door adjustment, which is model-agnostic and can help current methods to mitigate spurious corre-lations from dataset bias. In particular, given that knowl-edge proxy and its representation are not pre-deﬁned, we propose a series of practical approximations in Section 4.
The effectiveness of KPI framework is corroborated by comprehensive experiments with three baseline methods (CoMem [12], HGA [26], and HQGA [62]) on ﬁve bench-mark datasets (MSVD-QA [65], MSRVTT-QA [65], TGIF-QA [24], NExT-QA [61], and Causal-VidQA [32]). Our main contributions are summarized as follows:
Figure 2. The causal graph and causal intervention for VideoQA. tively. V → Q indicates that the question is proposed based on the video. V → H ← Q indicate that the video and ques-tion generate the aligned feature. Q → A ← H indicate that the answer is predicted based on the question and aligned feature. Confounder is a series of correlated concepts that appears simultaneously in the video, e.g. “jump;run”,
“shoot;run”. Since the question and answer are both pro-posed based on video, we also regard the confounder as the result of video (V → C), which controls the correlation be-tween question and answer (Q ← C → A). To quantify the effect of confounder, we collect the objects, actions from videos, and nouns, verbs, and adjectives from questions as video and question concepts. Then, we calculate the condi-tional probability of answers given question and video con-cepts, and show some examples in Figure 3. Due to the ex-istence of confounder, like the co-occurence “guitar;man”, apart from the legitimate path from Q and H to A, the back-door path Q ← C → A and H ← Q ← C → A also affect answer prediction. Since P (man|guitar) is dominantly more than P (woman|guitar) for training instances, then
P (A|V, Q, H) based on video with “guitar” tends to score
“man” much higher than “woman”. Therefore, if we only focus on observational probability P (A|V, Q, H) without considering the effect of confounder, the model will in-evitably be misled by dataset bias.
To remove the effect of confounder, we exploit the do-calculus [42] to actively intervene the value of
V, Q, H, where we have two choices, the backdoor adjust-ment (Section 3.2) and the front-door adjustment (Sec-tion 3.3). Backdoor adjustment [56, 48, 34] is widely
• We focus on dataset bias and provide a thorough anal-ysis of how dataset bias affects the answer prediction using the causal graph.
• To alleviate the effect of dataset bias, we exploit front-door adjustment and propose our model-agnostic KPI framework to implement the causal intervention.
• Comprehensive experiments with three baseline meth-ods on ﬁve benchmark datasets reveal that our frame-work signiﬁcantly boosts the state-of-the-art methods. 2.