Abstract
Out-of-distribution (OOD) detection refers to training the model on an in-distribution (ID) dataset to classify whether the input images come from unknown classes. Con-siderable effort has been invested in designing various
OOD detection methods based on either convolutional neu-ral networks or transformers. However, zero-shot OOD de-tection methods driven by CLIP, which only require class
This pa-names for ID, have received less attention. per presents a novel method, namely CLIP saying “no” (CLIPN), which empowers the logic of saying “no” within
CLIP. Our key motivation is to equip CLIP with the capa-bility of distinguishing OOD and ID samples using positive-semantic prompts and negation-semantic prompts. Specifi-cally, we design a novel learnable “no” prompt and a “no” text encoder to capture negation semantics within images.
Subsequently, we introduce two loss functions: the image-text binary-opposite loss and the text semantic-opposite loss, which we use to teach CLIPN to associate images with
“no” prompts, thereby enabling it to identify unknown sam-ples. Furthermore, we propose two threshold-free inference algorithms to perform OOD detection by utilizing negation semantics from “no” prompts and the text encoder. Experi-mental results on 9 benchmark datasets (3 ID datasets and 6
OOD datasets) for the OOD detection task demonstrate that
CLIPN, based on ViT-B-16, outperforms 7 well-used algo-rithms by at least 2.34% and 11.64% in terms of AUROC and FPR95 for zero-shot OOD detection on ImageNet-1K.
Our CLIPN can serve as a solid foundation for effectively leveraging CLIP in downstream OOD tasks. The code is available on https://github.com/xmed-lab/CLIPN. 1.

Introduction
Deep learning models [12, 6] have demonstrated excel-lent versatility and performance when the classes in the training and test datasets remain the same [32]. This is fa-cilitated by the fact that the models are trained under com-*Corresponding author. pletely closed-world conditions, meaning all encountered classes are in-distribution (ID) ones [7]. Nevertheless, these models tend to suffer from poor generalization and undesir-able performance when deployed in real-world applications.
This frustrating phenomenon is partially attributed to the existence of an enormous number of unknown classes dis-tributed in the real world, which is challenging to detect as they have not been explicitly seen during the training stage.
Figure 1. A toy comparison illustration of feature spaces between standard OOD detection algorithms and the proposed CLIPN. Our method involves a “no” logic, which provides a new feature space (yellow region) to directly identify OOD samples. The qualitative experiment visualization is shown in Figure. 5.
Out-of-distribution (OOD) detection task [2, 3, 9] has been raised and promptly attracted considerable interest from researchers. Briefly, the OOD detection task aims to empower the model to distinguish if the input images come from unknown classes. One of the mainstream OOD de-tection methods is to learn ID-specific features and classi-fiers, then develop the scoring function [42, 14] to metric how closely the input data matches the ID classes, which is measured by ID-ness [42] (or OOD-ness, an opposite case).
For instance, MSP [14], MaxLogit [13], energy-based [27] and gradient-based [25] have been extensively employed to measure the ID-ness. Better ID-ness brings better OOD re-sults. In summary, the key idea of these methods is to teach
ID knowledge to the model and then detect the dis-matched
cases referring to the model’s reply (score). The effective-ness of the above methods is seriously compromised by the following cases. As illustrated in Fig. 1, the green stars rep-resent some OOD samples that are easy to distinguish, as they are relatively distant from all ID classes and naturally have high entropy, uniform probability [14], low logit [13] or low energy [27]. Conversely, hard-to-distinguish OOD samples (brown stars in Fig. 1) are more common and chal-lenging. These samples are located relatively close to a cer-tain ID class while being far away from other classes, re-sulting in high ID-ness. Therefore, existing methods such as those mentioned above fail to identify such samples ac-curately. As results shown in Fig. 5, even when we apply
MSP [14] with different thresholding, there are still numer-ous mis-classified OOD samples, which are located in close proximity to ID classes.
Recently, some methods have sought to address the is-sue of hard-to-distinguish OOD samples by leveraging gen-eralizable representations learned by CLIP [10], an open-world language-vision model trained on datasets with enor-mous volumes, such as Laion-2B [36]. Naturally, this task extends to zero-shot OOD detection (ZS OOD detec-tion) [8, 30], which employs language-vision models to de-tect OOD data without requiring training on the ID dataset.
ZOC [8] uses an additional text encoder to generate some candidate OOD classes not included in ID classes. Unfor-tunately, it is inflexible and unreliable when faced with a dataset containing a large number of ID classes, rendering it challenging to scale for large datasets such as ImageNet-1K [20]. MCM [30] leverages the text encoder component of CLIP and ID class prompts to obtain a more represen-tative and informative ID classifier, which in turn enhances the accuracy of ID-ness estimates. However, this method still neglects to address the challenge of dealing with hard-to-distinguish OOD samples and suffers from limited per-formance; see results in Table 1.
Figure 2. A toy illustration to determine that the original CLIP lacks “no” logic. The qualitative visualization is in Figure. 6. identify some hard-to-distinguish OOD samples even if their ID-ness is high. As the toy example shown in Fig-ure. 2 (a), given a dog image and a cat image, we design four groups of prompts. Two groups contain class prompts with/of/.../having the photos of the dog or cat, while the other two groups use “no” prompts: a photo without/not of/.../not having the dog or cat. We conducted an experi-ment on CLIP to match the images with four prompts. Un-fortunately, the results show that CLIP fails to accurately match the images, implying that it lacks “no” logic; as il-lustrated in the toy visualization in Fig. 2 (b) and qualitative visualization in Fig. 6.
To empower “no” logic within CLIP, we propose a new
CLIP architecture, called CLIP saying “no” (CLIPN). It upgrades CLIP in terms of OOD detection in three ways. (1) Architecture. New “no” prompts and a “no” text en-coder are added to CLIP. Our novel learnable “no” prompts integrate negation semantics within prompts, complement-ing the original CLIP’s prompts. Moreover, our “no” text encoder captures the corresponding negation semantics of images, making the CLIP saying “no” possible. (2) Train-ing Loss. We further propose two loss functions. The first is image-text binary-opposite loss, which makes an image feature match with correct “no” prompt features. In other words, it can teach CLIP when to say “no”. The second is text semantic-opposite loss which makes the standard prompt and “no” prompts be embedded far away from each other. In other words, it can teach CLIP to understand the meaning of “no” prompts. (3) Threshold-free Inference
Algorithms. After the training of CLIPN, we design two threshold-free algorithms: competing-to-win and agreeing-to-differ. The goal of competing-to-win is to select the most confident probability from standard and “no” text encoders as the final prediction. While agreeing-to-differ generates an additional probability for the OOD class by consider-ing predictions from both standard and “no” text encoders.
Experimental results on 9 benchmark datasets (3 ID and 6
OOD datasets) showed that our CLIPN outperforms exist-ing methods. In summary, our contributions are Four-fold.
• We propose a novel CLIP architecture, named CLIPN, which equips CLIP with a “no” logic via the learnable
“no” prompts and a “no” text encoder.
• We propose the image-text binary-opposite loss and text semantic-opposite loss, which teach CLIPN to match images with “no” prompts, thus learning to identify unknown samples.
• We propose two novel threshold-free inference algo-rithms (competing-to-win and agreeing-to-differ) to perform OOD detection via using negation semantics.
Different from ZOC [8] and MCM [30], we attempt to exploit the open-world knowledge in CLIP to straightly
• Experimental results show that our CLIPN outper-forms most existing OOD detection algorithms on both
large-scale and small-scale OOD detection tasks. 2.