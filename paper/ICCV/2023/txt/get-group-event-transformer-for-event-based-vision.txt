Abstract
Event cameras are a type of novel neuromorphic sen-sor that has been gaining increasing attention. Existing event-based backbones mainly rely on image-based designs to extract spatial information within the image transformed from events, overlooking important event properties like time and polarity. To address this issue, we propose a novel
Group-based vision Transformer backbone for Event-based vision, called Group Event Transformer (GET), which de-couples temporal-polarity information from spatial infor-mation throughout the feature extraction process. Specifi-cally, we first propose a new event representation for GET, named Group Token, which groups asynchronous events based on their timestamps and polarities. Then, GET ap-plies the Event Dual Self-Attention block, and Group Token
Aggregation module to facilitate effective feature commu-nication and integration in both the spatial and temporal-polarity domains. After that, GET can be integrated with different downstream tasks by connecting it with vari-ous heads. We evaluate our method on four event-based classification datasets (Cifar10-DVS, N-MNIST, N-CARS, and DVS128Gesture) and two event-based object detection datasets (1Mpx and Gen1), and the results demonstrate that
GET outperforms other state-of-the-art methods. The code is available at https://github.com/Peterande/
GET-Group-Event-Transformer. 1.

Introduction
Event cameras are a type of bio-inspired vision sensors that capture per-pixel illumination changes asynchronously.
Compared with traditional frame cameras, event cameras offer many merits like high temporal resolution (>10K fps), high dynamic range (>120 dB), and low power con-sumption (<10 mW) [20]. Many applications, such as ob-ject classification [39, 27] and high-speed object detection
[2, 73, 47], can take advantage of this kind of camera, espe-cially when power consumption is limited or in the presence of challenging motion and lighting conditions.
*Corresponding author
Figure 1. Group Tokens are generated in Stage 1 according to timestamps and polarities of events. In GET, these tokens are ef-fectively communicated and integrated, making full use of impor-tant event information while maintaining information decoupling.
The event data are stored as asynchronous arrays, con-taining the location, polarity, and time of each illumina-tion change.
In contrast, traditional frames represent vi-sual information as a matrix of pixel values captured at a fixed rate. As a result, traditional image-based neural net-works can not be directly applied to event data. To ad-dress this issue, several works have been proposed to con-vert events into image-like representations. These repre-sentations, such as voxel grid [76], event histogram [46], and time surface [30, 55, 26], are then fed to deep neural networks [23, 18, 21, 38]. More recent works leverage the successful Transformer architecture to extract features di-rectly from the above event representations [72, 62, 49, 24], or from the feature maps obtained by CNN-based back-bones [56, 63, 75]. Although these networks have fair per-formance, they are still limited by their reliance on image-based feature extraction designs [16, 33, 25, 43]. These de-signs mainly extract spatial information and can not fully utilize the temporal and polarity information contained in events. There are also some works trying to bridge this gap.
For example, Spiking neural networks (SNN) are utilized to capture temporal information by representing event data as spikes that occur over time and propagate through neu-rons [64, 6, 40]. Graph neural networks (GNN) are also adopted to model the dynamic interactions between nodes in an event graph over time [34, 51]. However, these two kinds of methods either require specialized hardware or sac-rifice performance.
In this work, we revisit the problem of event-based fea-ture extraction and attempt to fully utilize temporal and polarity information of events with a Transformer-based backbone, while preserving its spatial modeling capabil-ity. To this end, we first propose a novel event represen-tation called Group Token, which groups events according to their timestamps and polarities. Then we propose Group
Event Transformer (GET) to extract features from Group
Tokens. We design two key modules for GET: Event Dual
Self-Attention (EDSA) block and Group Token Aggrega-tion (GTA) module. The EDSA block acts as the main self-attention module to extract correlations between different pixels, polarities, and times within Group Tokens. It main-tains efficient and effective feature extraction by perform-ing local Spatial Self-Attention and Group Self-Attention and establishing dual residual connections. GTA is placed between two stages to achieve reliable spatial and group-wise token aggregation. It achieves global communication in both the spatial and temporal- polarity domains by us-ing a novel overlapping group convolution. Figure 1 shows the overview of a 3-stage GET network with the initial group number of 6. The grouped tokens are fused to pro-duce larger groups, in which the features gradually gain larger spatial and temporal-polarity receptive fields, helping to better characterize objects.
These novel designs make GET achieve state-of-the-art performance on four event-based classification datasets (84.8% on Cifar10-DVS, 99.7% on N-MNIST, 96.7% on
N-CARS, and 97.9% on DVS128Gesture) and two event-based object detection datasets (47.9% and 48.4% mAP on
Gen1 and 1Mpx). These results all demonstrate our pro-posed network is beneficial for event-based feature extrac-tion. Our contributions are summarized in the following.
• We propose a new event representation, called Group
Token, that groups asynchronous events based on their timestamps and polarities.
• We devise the Event Dual Self-Attention block, en-abling effective feature communication in both the spa-tial and temporal-polarity domains.
• We design the Group Token Aggregation module, which uses the overlapping group convolution to in-tegrate and decouple information in both domains.
• Based on the above representation and modules, we develop a powerful Transformer backbone for event-based vision, called GET. Experimental results on both classification and object detection tasks demonstrate its superiority. 2.