Abstract
Anomaly detection (AD), aiming to find samples that de-viate from the training distribution, is essential in safety-critical applications. Though recent self-supervised learn-ing based attempts achieve promising results by creating virtual outliers, their training objectives are less faithful to AD which requires a concentrated inlier distribution as
In this paper, well as a dispersive outlier distribution. we propose Unilaterally Aggregated Contrastive Learning with Hierarchical Augmentation (UniCon-HA), taking into account both the requirements above. Specifically, we ex-plicitly encourage the concentration of inliers and the dis-persion of virtual outliers via supervised and unsupervised contrastive losses, respectively. Considering that standard contrastive data augmentation for generating positive views may induce outliers, we additionally introduce a soft mech-anism to re-weight each augmented inlier according to its deviation from the inlier distribution, to ensure a purified concentration. Moreover, to prompt a higher concentration, inspired by curriculum learning, we adopt an easy-to-hard hierarchical augmentation strategy and perform contrastive aggregation at different depths of the network based on the strengths of data augmentation. Our method is evaluated under three AD settings including unlabeled one-class, un-labeled multi-class, and labeled multi-class, demonstrating its consistent superiority over other competitors. 1.

Introduction
Anomaly detection (AD), a.k.a. outlier detection, aims to find anomalous observations that considerably deviate from the normality, with a broad range of applications, e.g. defect detection [4] and medical diagnosis [45]. Due to the
*Corresponding author.
Figure 1: The decision regions (light yellow) of RotNet
[26], DROC [49], CSI [51] and our UniCon-HA with ro-tation used to create virtual outliers. (a) RotNet models the inlier distribution by predicting rotation angles through a 4-way classifier; (b) DROC performs instance discrimination within the union of inliers and their rotations, resulting in a uniform distribution of data points; (c) CSI combines con-trastive learning with a rotation classifier, enclosing a sub-region for inliers; (d) Our UniCon-HA explicitly promotes the concentration of inliers and the dispersion of rotated vir-tual outliers, yielding the most compact decision region. inaccessibility of real-world outliers, it is typically required to develop outlier detectors solely based on in-distribution data (inliers). Conventional methods [7, 32, 30, 42, 68, 65] leverage generative models to fit the distribution by assign-ing high densities to inliers; however, they make use of raw images and are fragile caused by background statistics [42] or pixel correlations [30], unexpectedly assigning higher likelihoods to unseen outliers than inliers.
Alternatively, discriminative models [52, 47, 16, 43] de-scribe the support of the training distribution using classi-fiers, circumventing the complicated process of density es-timation. Furthermore, some studies [18, 26, 3] observe
improved performance with the introduction of virtual out-liers1, followed by a series of methods [49, 51, 61] ex-ploring outliers in a more effective way. For example, classification-based AD methods [18, 26, 3] rely on trans-formations to generate virtual outliers for creating pretext tasks, with rotation prediction being the most effective. Re-cently, DROC [49] models the union of inliers and ro-tated ones via contrastive learning. CSI [51] combines contrastive learning with an auxiliary classification head for predicting rotations, further boosting the performance.
Overall, these methods deliver better results than their coun-terparts without using rotation; unfortunately, their imper-fect adaptation to AD leaves much room for improvement.
We remark that a good representation distribution for AD requires: (a) a compact distribution for inliers and (b) a dis-persive distribution for (virtual) outliers. From our view, the existing methods [26, 49, 51] demonstrate unsatisfactory performance due to the lack of a comprehensive consider-ation of both aspects. RotNet [26] learns representations which only guarantee that they are distinguishable between labels, i.e. four rotation angles {0◦, 90◦, 180◦, 270◦}, re-sulting in less compact concentration for inliers and less dis-persive distribution for outliers shown in Fig. 1(a). Though
DROC [49] explicitly enlarges the instance-level distance via contrastive learning and generates a dispersive distri-bution for outliers, it inevitably pushes inliers away from each other, failing to meet the requirement of a compact in-lier distribution [50] (Fig. 1(b)). CSI [51] extends DROC with a rotation classification head which restricts inliers to a sub-region determined by separating hyperplanes, making inliers lumped to some extent (Fig. 1(c)), but the insufficient degree of concentration by the predictor limits its potential.
We also notice a growing trend [41, 21, 63] in leveraging models pre-trained on large-scale datasets (e.g. ImageNet
[12]) for AD. However, strictly speaking, they deviate from the objective of AD that outliers stem from an unknown dis-tribution and similar outliers are unseen in training.
In this work, we focus on a strict setting where AD models are trained from scratch using inliers only. We present a novel method for AD based on contrastive learn-ing, named Unilaterally Aggregated Contrastive Learn-ing with Hierarchical Augmentation (UniCon-HA), to ful-fill the goal of inlier concentration and outlier dispersion (Fig. 1(d)). The term unilaterally refers to the aggrega-tion applied to inliers only. For inlier concentration, differ-ent from other contrastive learning-based AD alternatives
[61, 48, 51, 49] that universally perform instance discrim-ination within the whole training set regardless of inliers or outliers, we take all inliers as one class and pull them together while push outliers away. For outlier dispersion, 1We note that several studies [34, 44, 39, 58] leverage real-world out-liers to address AD in a relaxed setting, which is out of the scope of this paper. we perform instance discrimination within all virtual out-liers to disperse them around the latent space unoccupied by inliers. Furthermore, considering that the standard aug-mentation pipeline for generating multiple positive views probably induces outliers as false positives [59, 1] (e.g. ran-dom crop at background regions), we propose to aggregate augmented views of inliers with a soft mechanism based on the magnitude of deviation from the inlier distribution, with distant samples assigned with lower weights. Finally, to prompt a higher concentration for inliers, inspired by cur-riculum learning (CL) [2], we adopt an easy-to-hard hier-archical augmentation and perform aggregation at different network depths based on the strengths of data augmentation.
Notably, our formulation is free from any auxiliary branches for transformation prediction [51, 18] or pre-trained mod-els [5, 15]. We evaluate our method in three typical AD scenarios including one-class, unlabeled multi-class, and labeled multi-class settings. Additionally, the results can be further improved with the introduction of outlier expo-sure (OE) [25], which is previously deemed harmful in con-trastive learning-based CSI [51].
Our main contributions are three-fold:
• We present a novel contrastive learning method for
AD, simultaneously encouraging the concentration for inliers and the dispersion for outliers, with soft aggre-gation to suppress the influence of potential outliers induced by data augmentation.
• For a higher concentration of inliers, we propose an easy-to-hard hierarchical augmentation strategy and perform contrastive aggregation distributed in the net-work where deeper layers are responsible for aggrega-tion under stronger augmentations.
• Experimental results demonstrate the consistent im-provement of our method over state-of-the-art com-petitors under various AD scenarios. 2.