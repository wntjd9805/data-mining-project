Abstract
Video Foundation Models (VFMs) have received lim-ited exploration due to high computational costs and data scarcity. Previous VFMs rely on Image Foundation Mod-els (IFMs), which face challenges in transferring to the video domain. Although VideoMAE has trained a robust ViT from limited data, its low-level reconstruction poses conver-gence difficulties and conflicts with high-level cross-modal alignment. This paper proposes a training-efficient method for temporal-sensitive VFMs that integrates the benefits of existing methods. To increase data efficiency, we mask out most of the low-semantics video tokens, but selectively align the unmasked tokens with IFM, which serves as the
UnMasked Teacher (UMT). By providing semantic guid-ance, our method enables faster convergence and multi-modal friendliness. With a progressive pre-training frame-work, our model can handle various tasks including scene-related, temporal-related, and complex video-language un-derstanding. Using only public sources for pre-training in 6 days on 32 A100 GPUs, our scratch-built ViT-L/16 achieves state-of-the-art performances on various video tasks. 1.

Introduction
Video understanding has emerged as a critical skill for artificial intelligence systems to analyze and comprehend videos effectively. The progress in video understanding is currently driven by the Image Foundation Models (IFMs)
[21, 29, 5, 55], which are trained from massive datasets and adapted for different downstream tasks [16, 93, 54, 65].
However, IFMs tend to focus more on scenes and objects, disregarding the essential motion patterns and object inter-actions required for complex video understanding. The true
Video Foundation Models (VFMs) are underexplored due to the high computational costs and data scarcity.
*Interns at Shanghai AI Laboratory. †Corresponding authors.
Figure 1: Comparison with SOTA methods.
“ZS” and “FT” refer to “zero-shot” and “fine-tuned”.
“T2V” means video-text retrieval. For Kinetics action recogni-tion, [80] and [70] are excluded since they utilize model ensemble. With only public sources (i.e., CLIP[55]) for pre-training, our approach achieves SOTA performances temporal-related and complex video-on scene-related, language benchmarks. Compared with CoCa [85], our method is much more environmentally friendly with 70× reduction in carbon emissions. Note that the cost of CLIP pre-training is ignored since it is publicly available.
While building VFMs on well-learned IFMs reduces training costs, it poses significant challenges in transferring knowledge from the image domain to the video domain.
Firstly, due to limited video data and a substantial domain gap, video post-pretraining may undermine the generality inherited from IFMs [79]. Moreover, the strong spatial ini-tialization offers a shortcut to perceive videos from scenes in single frames (e.g., “grass” in “horse riding”), which con-strains VFMs from learning spatiotemporal relationships to recognize and localize temporal-related actions, such as
“opening” and “closing” in Figure 2. Lastly, this paradigm is difficult to scale up as it requires well-prepared IFMs.
The recent success of VideoMAE [62, 23] offers a data-efficient way to learn effective spatiotemporal features from
Figure 2: Training-efficient framework for video foundation models. For general video understanding, we propose the progressive pre-training with the unmasked teacher, which is simple, scalable and reproducible. The resulting models can not only handle scene-related and temporal-related actions well, but also conduct complex video-language understanding. scratch, which handles complex temporal action recogni-tion and detection tasks impressively. Nonetheless, its strong data efficiency and spatiotemporal modeling are traded by long pre-training (e.g., 2400 epochs on 160k videos). Besides, it is not well-suited for video-language tasks since the low-level pixel reconstruction task conflicts with high-level cross-modal alignment [60]. Additionally, the extra decoder that handles masked and unmasked to-kens causes high memory costs due to global self-attention, making scaling up this paradigm also challenging.
In this paper, we present a training-efficient method for temporal-sensitive VFMs by integrating the benefits of pre-vious methods. Rather than directly adapting public IFM, e.g., CLIP [55], we utilize them as UnMasked Teacher (UMT) to train vanilla ViT from scratch. We mask out most of the video tokens with low semantics and only align the unmasked tokens with a linear projection to the correspond-ing ones from the teacher. This approach not only inherits data efficiency from VideoMAE but also makes the learned video encoder multimodal-friendly (validated in Table 1).
Moreover, training with only unmasked tokens without a decoder further saves GPU memory compared to Video-MAE, and the guidance from the teacher’s semantically rich representation leads to faster convergence. Notably, the re-sulting model can handle both scene-related [33, 49] and temporal-related actions [27, 28] exceptionally well, while the alignment to CLIP features enables the model to be compatible with cross-modal learning.
To address various video tasks, we propose a progressive pre-training framework in Figure 2. In Stage 1, we only use video data for masked video modeling, resulting in a model that excels at video-only tasks. In Stage 2, we employ pub-lic vision-language data for multi-modality learning. This allows the model to conduct complex video-language tasks, such as video-text retrieval [77, 57] and video question an-swering [87, 75]. We use the UMT in both stages, signif-icantly reducing the training sources and speeding up con-vergence. Thanks to readily-available image and language foundation models [55, 52, 46, 92, 15], our simple frame-work is easily scalable for video foundation models.
We conduct extensive experiments to verify the effec-tiveness and efficiency of our approach. As shown in Figure 1, with public sources (data/models) for pre-training, our method achieves state-of-the-art performances on various video tasks, including action recognition [33, 9, 10, 49, 27] (90.6% top-1 accuracy on K400), spatiotemporal localiza-tion [28] (39.8 mAP on AVA), video-text retrieval [77, 1, 34, 57, 12] (58.8 R@1 on MSRVTT) and video question-answering [87, 75, 86] (47.1% accuracy on MSRVTT). It is worth emphasizing that our method is much more environ-mentally friendly compared to CoCa [85], which uses 2,048
CloudTPUv4 chips for 5 days. In contrast, our pre-training requires 32 A100(80G) GPUs within 6 days, leading to a remarkable 70× reduction in carbon emissions. 2.