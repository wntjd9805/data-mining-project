Abstract
We present TexFusion (Texture Diffusion), a new method to synthesize textures for given 3D geometries, using large-scale text-guided image diffusion models. In contrast to re-cent works that leverage 2D text-to-image diffusion models to distill 3D objects using a slow and fragile optimization process, TexFusion introduces a new 3D-consistent gener-ation technique speciﬁcally designed for texture synthesis that employs regular diffusion model sampling on different 2D rendered views. Speciﬁcally, we leverage latent diffu-sion models, apply the diffusion model’s denoiser on a set of 2D renders of the 3D object, and aggregate the differ-ent denoising predictions on a shared latent texture map.
Final output RGB textures are produced by optimizing an intermediate neural color ﬁeld on the decodings of 2D ren-ders of the latent texture. We thoroughly validate TexFu-sion and show that we can efﬁciently generate diverse, high quality and globally coherent textures. We achieve state-of-the-art text-guided texture synthesis performance using only image diffusion models, while avoiding the pitfalls of previ-ous distillation-based methods. The text-conditioning offers detailed control and we also do not rely on any ground truth 3D textures for training. This makes our method versatile
∗ Equal contribution. and applicable to a broad range of geometry and texture types. We hope that TexFusion will advance AI-based tex-turing of 3D assets for applications in virtual reality, game design, simulation, and more. 1.

Introduction
In the past decade, deep learning-based 3D object gen-eration has been studied extensively [1, 4, 18, 23, 25, 27, 32, 33, 37, 39, 44, 45, 50, 52, 53, 55, 58, 69, 71, 72, 78, 82, 85, 87, 89, 91–93, 95, 97, 100, 101], due to the demand for high-quality 3D assets in 3D applications such as VR/AR, simulation, digital twins, etc. While many prior works on 3D synthesis focus on the geometric components of the as-sets, textures are studied much less, despite the fact that they are important components of realistic 3D assets which as-sign colors and materials to meshes to make the rendering vivid. Recent advances in text-conditioned image diffusion models trained on internet-scale data [2, 54, 61, 63, 66] have unlocked the capability to generate images with stunning vi-sual detail and practically unlimited diversity. These high-performance diffusion models have also been used as image priors to synthesize 3D objects with textures using textual guidance [40, 48, 59, 84].
In this paper, we aim to perform text-driven high-quality 3D texture synthesis for given meshes, by leveraging the information about the appearance of textures carried by the 1
image prior of a pre-trained text-to-image diffusion model.
The main workhorse in current text-to-3D methods that leverage 2D image diffusion models is Score Distillation
Sampling (SDS) [59]. SDS is used to distill, or optimize, a 3D representation such that its renders are encouraged to be high-likelihood under the image prior. Methods utilizing
SDS also share two common limitations in that: 1). a high classiﬁer-free guidance weight [22] is required for the optimization to converge, resulting in high color saturation and low generation diversity; 2). a lengthy optimization process is needed for every sample.
To address the above issues, we present Texture Diffu-sion, or TexFusion for short. TexFusion is a sampler for sampling surface textures from image diffusion models.
Speciﬁcally, we use latent diffusion models that efﬁciently autoencode images into a latent space and generate images in that space [63, 81]. TexFusion leverages latent diffusion trajectories in multiple object views, encoded by a shared latent texture map. Renders of the shared latent texture map are provided as input to the denoiser of the latent diffusion model, and the output of every denoising step is projected back to the shared texture map in a 3D-consistent manner. To transform the generated latent textures into
RGB textures, we optimize a neural color ﬁeld on the outputs of the latent diffusion model’s decoder applied to different views of the object. We use the publicly available latent diffusion model Stable-Diffusion with depth con-ditioning [63] (SD2-depth) as our diffusion backbone.
Compared to methods relying on SDS, TexFusion produces textures with more natural tone, stronger view consistency, and is signiﬁcantly faster to sample (3 minutes vs. 30 minutes reported by previous works).
We qualitatively and quantitatively validate TexFusion on various texture generation tasks. We ﬁnd that TexFu-sion generates high quality, globally coherent and detailed textures that are well-aligned with the text prompts used for conditioning (e.g. Fig. 1). Since we leverage powerful text-to-image diffusion models for texture sampling, we can generate highly diverse textures and are not limited to single categories or restricted by explicit 3D textures as training data, which are limitations of previous works [7, 11, 18, 57, 73]. In summary, our main contribution is a novel method for 3D texture generation from 2D image diffusion models, that is view-consistent, avoids over-saturation and achieves state-of-the-art text-driven texture synthesis performance. 2.