Abstract 3D perception based on the representations learned from multi-camera bird’s-eye-view (BEV) is trending as cameras are cost-effective for mass production in autonomous driv-ing industry. However, there exists a distinct performance gap between multi-camera BEV and LiDAR based 3D ob-ject detection. One key reason is that LiDAR captures ac-curate depth and other geometry measurements, while it is notoriously challenging to infer such 3D information from merely image input. In this work, we propose to boost the representation learning of a multi-camera BEV based stu-dent detector by training it to imitate the features of a well-trained LiDAR based teacher detector. We propose effec-tive balancing strategy to enforce the student to focus on learning the crucial features from the teacher, and general-ize knowledge transfer to multi-scale layers with temporal fusion. We conduct extensive evaluations on multiple repre-sentative models of multi-camera BEV. Experiments reveal that our approach renders significant improvement over the student models, leading to the state-of-the-art performance on the popular benchmark nuScenes. 1.

Introduction
Perceiving 3D environments is essential for autonomous driving as it is crucial for subsequent onboard modules from prediction [28, 37] to planning [2, 18]. Although LiDAR based methods have achieved remarkable progress [15, 25, 26, 42], there have recently been fast growing attentions to camera based approaches from both academia and indus-try [1, 32]. Compared to a sensor suite of LiDAR, the cost of cameras is typically 10 times cheaper, gaining a huge cost advantage for mass-production OEMs. Additionally, cam-eras are better suited to detect distant objects and recognize visual based road elements (e.g., traffic lights and signs).
A straightforward solution for camera based 3D object detection is the monocular paradigm that has been broadly
*Equal contribution
†Correspondence to xiaodong@qcraft.ai
Figure 1. An overview of performance improvement in terms of mAP and NDS on the validation set of nuScenes. Enabled by the proposed cross-modal knowledge distillation method DistillBEV, a variety of multi-camera BEV based 3D object detectors achieve consistent and significant performance boost. studied [27, 30, 36]. However, such methods have to deal with surrounding cameras separately and require complex post-processing to fuse detections from multiple views. As an alternative, the bird’s-eye-view (BEV) based framework is drawing extensive attentions to offer a holistic feature representation space from multi-camera images, and has made substantial improvement [11, 12, 20, 21]. BEV based framework owns the following inherent merits including (i) joint feature learning from multi-view images, (ii) unified detection space without post fusion, (iii) amenability for temporal fusion, and (iv) convenient output representation for the downstream prediction and planning.
Despite the advancement achieved in this field, a distinct performance gap remains between multi-camera BEV and
LiDAR based 3D object detection. For instance, the leading multi-camera BEV based method is outperformed by its Li-DAR counterpart over 15% mAP and 10% NDS on popular benchmark nuScenes [3]. On the other hand, a data collec-tion fleet can be equipped with both cameras and LiDAR, while the mass-produced vehicles can be LiDAR-free.
In light of above observations, we present DistillBEV: a simple and effective cross-model knowledge distillation approach to bridge the feature learning gap between multi-camera BEV and LiDAR based detectors. Our strategy to-ward achieving this goal is to align the corresponding fea-tures learned from images and point clouds. Given merely images as input, the multi-camera BEV based detector (stu-dent) is guided to imitate the features extracted from point clouds by a well-trained LiDAR based detector (teacher).
We argue that the accurate 3D geometric cues such as depth and shape as well as how such cues are represented in the point cloud features provide valuable guidance to the train-ing process of student model. Moreover, we emphasize that our approach incurs no extra computation cost during infer-ence as the teacher model involves in training only.
Due to the notable discrepancy between the two modal-ities, the cross-modal knowledge distillation is extremely challenging. Compared to camera images, LiDAR scans are inherently sparse, and the majority of 3D space is empty.
Even if in the occupied space, background (e.g., building and vegetation) dominates, and objects of interest (e.g., bus and pedestrian) come of varied sizes. It is thus nontrivial to locate the informative regions to make the knowledge trans-fer more focused, and meanwhile balance the distillation importance assigned to different foreground objects. Fur-thermore, the teacher and student models are respectively developed in their specific domains, leading to disparate network architectures. It is also demanding to generalize the cross-modal distillation to adapt to various combinations of different teacher and student networks.
To tackle these challenges for DistillBEV, we first pro-pose region decomposition to partition a feature map into true positive, false positive, true negative and false negative regions, which elaborately decouple foreground and back-ground in the cross-modal distillation. Based on the de-composition, we introduce adaptive scaling to balance the significantly varied box sizes when objects are presented in
BEV. We further exploit spatial attention to encourage the student model to mimic the attention pattern produced by the teacher model, so as to focus on the crucial features for effective knowledge transfer. We then extend the distilla-tion to multi-scale layers to achieve thorough feature align-ment between teacher and student. Finally, we incorporate temporal information for both teacher and student models in BEV, thus enabling the distillation with temporal fusion readily. Thanks to the proposed generalizable design, our approach is flexible to be applied to various combinations of teacher and student detectors. As illustrated in Figure 1,
DistillBEV consistently and remarkably improves multiple representative student models.
Our main contributions are summarized as follows. First, we present the cross-modal distillation in BEV, which nat-urally suits for knowledge transfer between LiDAR and multi-camera BEV based detectors. Second, we propose the effective balancing design to enable the student to focus on learning crucial features of the teacher with multiple scales and temporal fusion. Third, our approach achieves superior performance, and more importantly, obtains consistent and considerable improvement on various teacher-student com-binations. Our code and model will be made available at https://github.com/qcraftai/distill-bev. 2.