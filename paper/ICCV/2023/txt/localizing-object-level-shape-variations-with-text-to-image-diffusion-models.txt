Abstract
Text-to-image models give rise to workflows which of-ten begin with an exploration step, where users sift through a large collection of generated images. The global nature of the text-to-image generation process prevents users from narrowing their exploration to a particular object in the image.
In this paper, we present a technique to generate a collection of images that depicts variations in the shape of a specific object, enabling an object-level shape explo-ration process. Creating plausible variations is challenging as it requires control over the shape of the generated ob-ject while respecting its semantics. A particular challenge when generating object variations is accurately localizing the manipulation applied over the object’s shape. We in-troduce a prompt-mixing technique that switches between prompts along the denoising process to attain a variety of shape choices. To localize the image-space operation, we present two techniques that use the self-attention layers in conjunction with the cross-attention layers. Moreover, we show that these localization techniques are general and ef-fective beyond the scope of generating object variations.
Extensive results and comparisons demonstrate the effec-tiveness of our method in generating object variations, and the competence of our localization techniques. 1.

Introduction
Text-to-image diffusion models have recently shown un-precedented image quality and diversity [35, 41, 39], and have opened a new era in image synthesis. Still, the control over the generated image is limited, resulting in a tedious selection procedure, where users sample numerous initial seed noises, from which they can select a preferred one.
Images generated from different initial noise with the same text prompt share semantics, but the shape, appearance, and location of the generated shapes may differ greatly. The un-controlled global changes realized by such a sampling pro-cess, do not allow users to interact with the generated image and narrow down their open-ended exploration process. In particular, the lack of object-level control of the user with the generated image hinders the user’s ability to focus on
Figure 1. Our method generates shape variations of an object.
Here, given the text prompt, we generate variations of the basket.
After selecting a preferred basket, we generate variations of the mug. We develop general techniques for localizing modifications. refining specific objects during their exploration.
A possible approach for interacting with the generated image as a means to explore the shape and appearance of a specific object in the image is to use text-guided image inpainting [39] or SDEdit [29]. These methods mainly ex-cel in changing the texture of an object and are therefore more suitable for textural exploration. However, changing the shape of an object, particularly if other regions in the im-age should be preserved, is significantly more challenging, and these methods struggle to achieve that without affecting the entire image. Figure 2 demonstrates the lack of shape variations with the above approaches.
”A basket with bananas” 2.