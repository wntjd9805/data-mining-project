Abstract
Stereo matching becomes computationally challenging when dealing with a large disparity range. Prior methods mainly alleviate the computation through dynamic cost vol-ume by focusing on a local disparity space, but it requires many iterations to get close to the ground truth due to the lack of a global view. We find that the dynamic cost volume approximately encodes the disparity space as a single
Gaussian distribution with a fixed and small variance at each iteration, which results in an inadequate global view over disparity space and a small update step at every iteration. In this paper, we propose a parameterized cost volume to encode the entire disparity space using multi-Gaussian distribution. The disparity distribution of each pixel is parameterized by weights, means, and variances.
The means and variances are used to sample disparity candidates for cost computation, while the weights and means are used to calculate the disparity output. The above parameters are computed through a JS-divergence-based optimization, which is realized as a gradient descent update in a feed-forward differential module. Experiments show that our method speeds up the runtime of RAFT-Stereo by 4 ∼ 15 times, achieving real-time performance and comparable accuracy. The code is available at https:
//github.com/jiaxiZeng/Parameterized-Cost-Volume-for-Stereo-Matching. 1.

Introduction
Stereo matching aims to find the pixel-wise correspon-dence between paired images within a predefined dispar-ity range. The pixel-wise matching is efficiently real-†These authors contributed equally to this work.
*Corresponding author. (a) The illustration of End-Point-Error (EPE) changing with the in-crease of iteration on the Sceneflow dataset (left) and the illustra-tion of time cost changing with the increase in image width (right). (b) The visualization of the moving step at each iteration t.
Figure 1: (a) We analyze the EPE at different iterations and time cost with the increase of image width. (b) The moving steps are visualized on the right view image, start-ing from a disparity value of 0 towards the ground truth.
Compared to RAFT-Stereo [13] and CREStereo [11], our method achieves a rapid decrease in EPE and a large mov-ing step from the first iteration, greatly reducing the number of iterations. ized by existing methods within a small predefined range
[1, 26, 30, 6, 5, 20, 27]. However, the matching becomes time- and memory-consuming for a large predefined range, as computational cost increases rapidly with the growing range. This challenge limits the application of stereo match-ing in the real world, e.g., in high-resolution images.
Prior methods relieve the challenge mainly relying on
dynamic cost volume [3, 5, 2, 20, 13, 11]. The dynamic cost volume samples disparity candidates in a local dispar-ity space instead of the entire disparity space.
It reduces significant memory costs but requires many iterations to get close to the ground truth, as illustrated in Figure 1a.
In this paper, we prove that the dynamic cost volume en-codes the disparity space as a single Gaussian distribution with a fixed and small variance. The fixed and small vari-ance results in a limited view over the entire disparity space.
Thus, these methods are hard to produce a large update to quickly converge to the ground truth when the initialized disparity is far from the ground truth.
Instead, a multi-Gaussian distribution provides a global view after initial-izing each Gaussian distribution uniformly in the dispar-ity space. The global view allows fast convergence to the vicinity of ground truth at the beginning of the iteration, as shown in Figure 1. Meanwhile, the local view is kept for subsequent fine-grained matching as the learnable parame-ters (i.e., variances) of multi-Gaussian distribution become small at the convergence stage.
To this end, we propose a parameterized cost volume that encodes the entire disparity space using multi-Gaussian dis-tribution. The disparity distribution of each pixel is parame-terized by weights, means, and variances. The computation of these parameters is formulated as a JS-divergence-based optimization problem. We solve the problem through itera-tive gradient descent updates in a feed-forward differential module, including the following steps. (1) We use the ini-tialized means and variances to sample disparity candidates. (2) The sampled disparity candidates are used to compute the matching cost, which is subsequently used to predict an optimization step. (3) We use the predicted step to update the three kinds of parameters. The updated parameters serve as initialization for the next iteration, while the weighted average of the mean values (i.e., the expectation of multi-Gaussian distribution) is regarded as the disparity output at the current iteration. The above feed-forward optimization is prone to local oscillations at the final convergence stage.
Thus, we further design an uncertainty-aware refinement module to localize and correct the incorrect results at the last iteration. We compute uncertainty from weights and variances to measure the reliability of predicted disparity.
We then use uncertainty as guidance to propagate disparity from high-reliability areas to low-reliability areas.
We validate our method on both synthetic and real-world datasets. Results show that our method simultaneously achieves real-time performance and SOTA-comparable ac-curacy on most datasets. We also compare our method with
SOTA methods in terms of time growth when the prede-fined disparity range is enlarged. Experiments show that our method maintains fixed memory costs and requires fewer it-erations to achieve comparable or even better accuracy than other methods. 2.