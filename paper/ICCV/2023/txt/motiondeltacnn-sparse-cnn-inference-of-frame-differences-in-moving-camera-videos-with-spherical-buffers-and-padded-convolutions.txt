Abstract 1.

Introduction
Convolutional neural network inference on video input is computationally expensive and requires high memory band-width. Recently, DeltaCNN[26] managed to reduce the cost by only processing pixels with significant updates over the previous frame. However, DeltaCNN relies on static cam-era input. Moving cameras add new challenges in how to fuse newly unveiled image regions with already processed regions efficiently to minimize the update rate - without in-creasing memory overhead and without knowing the camera extrinsics of future frames. In this work, we propose Motion-DeltaCNN, a sparse CNN inference framework that supports moving cameras. We introduce spherical buffers and padded convolutions to enable seamless fusion of newly unveiled re-gions and previously processed regions – without increasing memory footprint. Our evaluation shows that we outperform
DeltaCNN by up to 90% for moving camera videos.
Real-time inference of convolutional neural networks (CNN) with video streams remains a power-consuming task and is often infeasible on mobile devices due to hardware and thermal limitations, despite recent efforts aiming at effi-cient CNN inference through pruning [19, 14], quantization
[16, 24, 22], specialized hardware [6, 13, 5] or network optimization [28, 31] Video input allows for performance optimization through temporal similarity between frames.
One common method is to use large, slow networks for ac-curate predictions at key frames, updated with small, fast networks at intermediate frames [34, 33, 20, 18, 9, 21, 12].
However, this approach requires special network design and training and is not suitable for significant frame changes.
Sparse convolutions, on the other hand, can be used with existing pre-trained models, accelerating the large network to the speed of a smaller network without impacting prediction accuracy significantly [25, 26]. The linearity of convolutions
a b
Network Input 1
Network Input 2
Accumulated Input
Figure 2: To support moving camera input in sparse inference of frame differences, a na¨ıve approach (a) is to embed the input onto a larger image and only overwrite pixels that are covered in the next frame. This approach comes with large overhead in memory consumption (storing the additional pixels of the embedded frame) and computational cost (checking all pixels for updates). MotionDeltaCNN (b) uses original shape buffers and feature maps – avoiding the memory overhead and reducing the number of pixels that have to be checked for updates. enables the accumulation of updates over consecutive frames.
After processing the first frame densely, upcoming frames can be processed using the difference between the current and the previous frame, the Delta, as the network input.
Assuming a static camera scenario, this results in large image regions with no frame-to-frame difference. Static elements like background or stationary objects do not require updates after the initial frame. Researchers exploited this sparsity in previous work to reduce the number of FLOPs required for
CNN inference [25, 4, 11, 1, 8], but were mostly unable to accelerate inference in practice. With the recent progress in
DeltaCNN [26], theoretical FLOP reductions were translated into practical speedups on GPUs, using a custom sparse CNN implementation to exploit sparsity in all layers of the CNN.
DeltaCNN achieves speedups on multiple tasks and datasets, but like prior work, it is optimized for static cam-era inputs. Even small frame-to-frame camera motion ne-cessitates reprocessing large portions, or the entire image.
Spatially aligning consecutive frames, e.g., by leveraging camera extrinsics from IMUs or SLAM on mobile devices, can increase per-pixel similarity and thus update sparsity.
Due to the change of camera location and orientation, an aligned new frame may “grow” beyond the initial field of view – out of the previous-results buffers. One way to solve this issue is to embed inputs in a larger frame by padding the input image and drawing updates on top of it. However, the downside of this approach is that padded input images increase the memory consumption and computational cost significantly, as shown in Figure 2.
In this work, we propose MotionDeltaCNN, a sparse
CNN inference framework that allows moving camera input with marginal memory overhead. Compared to previous work, we achieve up to 90% higher frame rates in videos with moving cameras, indicating a new perspective for CNN inference optimization on low-power devices, such as surveil-lance cameras, smartphones, or VR headsets.
Our main contributions are:
• We propose MotionDeltaCNN, the first framework that leverages temporal continuity to accelerate CNN infer-ence for videos with moving cameras by processing only the sparse frame differences.
• At the core of this work, we propose padded convolu-tions for seamless integration of newly unveiled pixels without the need of reprocessing seen pixels.
• We design a two-dimensional ring buffer, a spherical buffer, with wrapped coordinates. Our buffer allows partial growth, reset and initialization of new tiles with-out additional memory allocation.
• We show how MotionDeltaCNN can also be used for speeding up applications with static cameras when only parts of the image require processing. 2.