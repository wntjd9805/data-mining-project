Abstract
Recent works for face editing usually manipulate the la-tent space of StyleGAN via the linear semantic directions.
However, they usually suffer from the entanglement of facial attributes, need to tune the optimal editing strength, and are limited to binary attributes with strong supervision sig-nals. This paper proposes a novel adaptive nonlinear latent transformation for disentangled and conditional face edit-ing, termed AdaTrans. Speciﬁcally, our AdaTrans divides the manipulation process into several ﬁner steps; i.e., the direction and size at each step are conditioned on both the facial attributes and the latent codes. In this way, AdaTrans describes an adaptive nonlinear transformation trajectory to manipulate the faces into target attributes while keeping other attributes unchanged. Then, AdaTrans leverages a predeﬁned density model to constrain the learned trajec-tory in the distribution of latent codes by maximizing the
∗Corresponding author likelihood of transformed latent code. Moreover, we also propose a disentangled learning strategy under a mutual in-formation framework to eliminate the entanglement among attributes, which can further relax the need for labeled data.
Consequently, AdaTrans enables a controllable face edit-ing with the advantages of disentanglement, ﬂexibility with non-binary attributes, and high ﬁdelity. Extensive experi-mental results on various facial attributes demonstrate the qualitative and quantitative effectiveness of the proposed
AdaTrans over existing state-of-the-art methods, especially in the most challenging scenarios with a large age gap and few labeled examples. The source code is available at https://github.com/Hzzone/AdaTrans. 1.

Introduction
Face editing aims to render the faces to the target fa-cial attributes such as aging or smiling with high ﬁdelity while keeping other facial attributes unchanged, which has
wide applications in entertainment and forensics. Due to the intrinsic complexity of facial attributes, face editing has at-tracted growing research interest in recent years. Generative
Adversarial Networks (GANs) [12] have shown promising results for face editing in terms of image quality. Earlier works mainly focus on network architectures [19, 39, 43] and loss functions [31]. With signiﬁcant improvements in image quality, these methods usually re-train a GAN model for a speciﬁc facial attribute [17] or practical applications [27].
Unfortunately, due to the difﬁculties in training a good GAN, they are limited to speciﬁc tasks and fail to generalize to high-resolution images.
In recent years, StyleGAN [22–24] has achieved signiﬁ-cant progress in synthesizing photorealistic faces. In particu-lar, the pre-trained StyleGAN generator presents a meaning-ful intermediate latent space, traversing on which the faces can be semantically manipulated [2, 3, 14, 34, 35, 38, 40, 41].
Typically, the faces before editing need to be inverted into the latent space of StyleGAN to obtain the latent codes that can be used to faithfully reconstruct the inputs [4, 33, 36]. As a result, the latent code is manipulated along certain directions, giving rise to the changes in the corresponding attribute in generated faces. The methods to obtain those directions can be roughly categorized as supervised ones and unsupervised ones. The supervised methods [3, 34, 38, 41] leverage the labeled data to compute the semantic directions, leading to better controllability in the editing process. For example,
InterFaceGAN [34] trains a hyperplane in the latent space to separate the examples with binary attributes. Unsupervised methods [2, 14, 35, 40] are to discover the interpretable direc-tions using PCA [14,35] or texts [2]. Despite the meaningful transformations, they cannot produce precise user-desired editing without any human annotations.
In summary, most of these methods assume that the bi-nary attributes can be well separated, so they edit the faces by linear interpolation in the latent space. Although it is suf-ﬁcient to some degree, for those more complicated scenarios, e.g., with large age gaps, they cannot perform disentangled editing to preserve the unrelated attributes when linear as-sumption does not hold. Meanwhile, the users are usually required to manually tune the editing strength for accurate manipulation. Though ﬂexible, the optimal strength varies among different examples. Furthermore, there is a critical yet ignored problem in current literature that the latent codes could be over-manipulated, i.e. falling out of the latent space, which inevitably harms the quality of the edited face.
In this paper, we propose a novel framework for condi-tional face editing, termed AdaTrans, to address these issues in the following aspects. First, instead of manually ma-nipulating the latents with ﬁxed directions, we propose an adaptive nonlinear transformation strategy that dynamically estimates the editing direction and step size, conditioned by the target attributes and transformation trajectory. Such a strategy can handle various attributes at the same time for conditional multi-attribute editing, by only changing the tar-get attributes while keeping others unchanged. Second, we propose to maximize the likelihood of edited latent codes, regularize the transformed trajectory in the distribution of latent space, and hence improve the ﬁdelity of edited faces predicted by a pretrained generator. Last, we propose a disentangled learning strategy under a mutual information framework, attenuating the entanglement between attributes and relaxing the need for labeled data in supervised face edit-ing methods. The merits of AdaTrans are disentanglement, high ﬁdelity, controllability, and ﬂexibility. The sample re-sults are shown in Fig. 1.
The contributions are summarized as follows:
• We present AdaTrans, a novel face editing method that explores an adaptive nonlinear transformation for disen-tangled and multi-attribute face editing.
• We propose a novel density regularization term, which can encourage an in-distribution transformation in the latent space, without harming ﬁdelity.
• We further show a disentangled learning strategy, which can eliminate the entanglement between attributes and relax the need for labeled data.
• Experimental results on various facial attributes demon-strate the effectiveness of AdaTrans both quantitatively and qualitatively. In particular, AdaTrans can produce disentangled editing, even with extremely large age gaps or few labeled data. 2.