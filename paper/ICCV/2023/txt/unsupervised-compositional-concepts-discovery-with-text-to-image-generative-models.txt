Abstract
Text-to-image generative models have enabled high-resolution image synthesis across different domains, but re-quire users to specify the content they wish to generate. In this paper, we consider the inverse problem – given a col-lection of different images, can we discover the generative concepts that represent each image? We present an unsu-pervised approach to discover generative concepts from a collection of images, disentangling different art styles in paintings, objects, and lighting from kitchen scenes, and discovering image classes given ImageNet images. We show how such generative concepts can accurately represent the content of images, be recombined and composed to gener-ate new artistic and hybrid images, and be further used as a representation for downstream classiﬁcation tasks. 1.

Introduction
When presented with a set of images, we can infer and discover common concepts across images. For instance, given a set of images of kitchen scenes in Figure 1, we      
can grasp different illumination patterns in the kitchen and identify various elements within kitchens, such as dining tables, kitchen islands, and cabinets. Moreover, we possess the ability to conjure up vivid mental images of new scenes that combine elements between different kitchen scenes or visualize how these elements may manifest in unfamiliar settings – envisioning, for instance, how a dining table may appear in a forest.
Can we construct computer vision systems that may like-wise understand, recombine, and imagine the visual world?
Most existing work in concept discovery focus on discover-ing latent vectors or directions representing individual con-cepts [15, 24, 18, 44, 55], but require supervised data label-ing each concept. Other works have focused on discovering compositional generative concepts from images but focus only on discovering objects [4, 34]. Recently, COMET [11] proposes an approach to decompose scenes into a set of gen-erative concepts representing both global scene concepts, such as lighting and camera position, and local concepts, such as objects. However, the approach is only applied to simple datasets and fails to generate complex images.
In this work, we illustrate how we can leverage the rich semantic information in large text-to-image generative models to discover a set of diverse compositional generative concepts from unlabeled natural images. Our work extends the approach in [11] using the interpretation of diffusion models as EBMs [32] and decomposes each image into a set of different probability distributions. We illustrate how each decomposed probability distribution captures different global and local scene concepts in an image, ranging from
ImageNet class identity to portions of images such as is-lands and cabinets in a kitchen.
In Figure 1, we show how our approach can discover compositional concepts across a wide set of different do-mains. In the top row of Figure 1, we illustrate how our approach can discover different art concepts, such as wheat
ﬁelds, cafes, and bedrooms, from paintings by either Van
Gogh or Claude Monet. In the middle row of Figure 1, we demonstrate how our approach can discover classes of im-ages, such as couches, starﬁsh, elephants, and cars, from a collection of unlabeled ImageNet images. Finally, in the bottom row of Figure 1, we show how our approach can discover the compositional components of a kitchen, such as lighting patterns and kitchen islands.
In this work, we contribute the following: (1) We illus-trate a scalable approach to discover unsupervised composi-tional concepts in realistic images using existing generative models. (2) Our method achieves state-of-the-art perfor-mance on concept discovery across different domains, in both global and local concept discovery, such as automati-cally discovering painting styles, and decomposing scenes into lighting and objects. (3) We illustrate that the discov-ered generative concepts can be used for diverse tasks, such as generating novel creative images or as effective represen-tations for downstream classiﬁcation tasks. 2.