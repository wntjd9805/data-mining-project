Abstract
Harmonizing cut-and-paste images into perceptually re-alistic ones is challenging, as it requires a full understand-ing of the discrepancies between the background of the tar-get image and the inserted object. Existing methods mainly adjust the appearances of the inserted object via pixel-level manipulations. They are not effective in correcting color discrepancy caused by different scene illuminations and the image formation processes. We note that image colors are essentially camera ISP projection of the scene radiance. If we can trace the image colors back to the radiance field, we may be able to model the scene illumination and har-In this paper, we propose monize the discrepancy better. a novel neural approach to harmonize the image colors in a camera-independent color space, in which color values are proportional to the scene radiance. To this end, we propose a novel image unprocessing module to estimate an intermediate high dynamic range version of the object to be inserted. We then propose a novel color harmonization module that harmonizes the colors of the inserted object by querying the estimated scene radiance and re-rendering the harmonized object in the output color space. Extensive experiments demonstrate that our method outperforms the state-of-the-art approaches. 1.

Introduction
Image compositing is a common process in vision and graphics. It is a technique to render a novel image by insert-ing a target object from the source image onto a target im-age. However, humans can easily identify this cut-and-paste (or composite) image as a synthetic one due to its color [7] and texture inconsistencies [31, 63]. Hence, there is a line of research to develop algorithms to harmonize cut-and-paste images to produce visually realistic output images.
Existing image harmonization methods typically fall into two categories, i.e., non-deep learning-based meth-ods [42, 10, 47, 58, 63] and deep learning-based meth-ods [48, 13, 8, 12, 33, 21, 27, 20]. Non-deep learning based methods try to manipulate low-level image statistics (e.g., textures [47] and colors [42, 63, 58]) of the inserted ob-(a) Input (b) IIH [21] (c) CDT [11] (d) WBH [29] (e) Ours (f) Ground Truth (g) Input (h) IIH [21] (i) CDT [11] (j) WBH [29] (k) Ours (l) Ground Truth
Figure 1. Harmonization results on the iHarmony4 dataset [12].
Existing harmonization methods tend to produce dull (b-d) or in-consistent (h-j) colors. Our method traces back to and harmonizes the colors in an intermediate linear color space, resulting in more realistic composite images as shown in (e) and (k). ject, to match with those of the background. These meth-ods often produce unrealistic images of inconsistent col-ors/textures when the hand-crafted features fail to repre-sent the foreground/background.
In contrast, deep learn-ing based methods offer strong capability of modelling re-gion appearances to facilitate harmonization. Some meth-ods explore different priors (e.g., semantics [48], and gra-dient/color consistency [8, 53]) to constrain the harmoniza-tion process. Some other methods [12, 33, 21] may for-mulate the image harmonization process as a foreground-background transfer learning task.
Despite their success, existing harmonization methods may still produce pale (Figure 1(b-d)) or inconsistent col-ors (Figure 1(h-j)) across the foreground and background regions, resulting in visually unpleasant images. We note that all these methods model the color harmonization pro-cess in the camera output sRGB (i.e., low dynamic range) color space. However, object colors in an image are deter-mined not only by their material reflectance and scene illu-mination, but also by the black-box imaging pipeline (ISP) of the camera. Due to the non-linear operations (e.g., tone mapping) within the ISP, pixel intensities of camera output sRGB images are not proportional to the scene radiance, making them unreliable for use in estimating the scene illu-mination for color harmonization.
To address this problem, we propose in this paper a novel approach to harmonize a cut-and-paste image (captured in low dynamic range) in the high dynamic range domain. Our key idea is to harmonize the scene illumination discrepan-cies in an intermediate (high dynamic range) color space, in which the scene illumination is proportional to the orig-inal scene radiance. To this end, we propose a novel neu-ral network that first converts the source image (containing the target object) into an intermediate high dynamic range domain, then performs the harmonization process, and fi-nally converts the harmonized image back to the low dy-namic range sRGB space. To avoid exhaustive modeling of camera-dependent operations, we propose a novel image unprocessing module to estimate a high dynamic range ver-sion of the input image in the linear camera-independent
CIE XYZ color space. We formulate this image unprocess-ing process as a diffusion process. We propose a novel color harmonization method that models image colors in the es-timated linear color space to produce the final harmonized results. As shown in Figure 1(e,k), our method is able to produce more visually pleasing results. We conduct ex-tensive experiments to demonstrate that our method outper-forms state-of-the-art harmonization approaches.
In summary, this paper has three main contributions:
• We propose a novel neural approach for image harmo-nization that performs the color harmonization process in the linear color space, allowing object color model-ing based on faithful scene radiance.
• Our approach includes two novel modules: (1) a novel image unprocessing module to convert the source im-age (of the target object) into a version in the high dy-namic range linear color space, and (2) a novel color harmonization module to harmonize object colors by querying scene radiance information and re-render the harmonized objects in the output color space.
• Extensive experiments show that the proposed method outperforms state-of-the-art harmonization methods. 2.