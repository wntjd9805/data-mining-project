Abstract
Diffusion models have shown remarkable success in vi-sual synthesis, but have also raised concerns about potential abuse for malicious purposes.
In this paper, we seek to build a detector for telling apart real images from diffusion-generated images. We find that existing detectors struggle to detect images generated by diffusion models, even if we include generated images from a specific diffusion model in their training data. To address this issue, we propose a novel image representation called DIffusion Reconstruction
Error (DIRE), which measures the error between an input image and its reconstruction counterpart by a pre-trained diffusion model. We observe that diffusion-generated images can be approximately reconstructed by a diffusion model while real images cannot. It provides a hint that DIRE can serve as a bridge to distinguish generated and real images.
DIRE provides an effective way to detect images generated by most diffusion models, and it is general for detecting generated images from unseen diffusion models and robust to various perturbations. Furthermore, we establish a com-prehensive diffusion-generated benchmark including images generated by various diffusion models to evaluate the per-formance of diffusion-generated image detectors. Extensive experiments on our collected benchmark demonstrate that
DIRE exhibits superiority over previous generated-image detectors. The code, models, and dataset are available at https://github.com/ZhendongWang6/DIRE. 1.

Introduction
Recently, Denoising Diffusion Probabilistic Mod-els (DDPMs) [17, 41] have set up a new paradigm in image generation due to their strong ability to generate high-quality images. There arises plenty of studies [32, 10, 42, 23, 37] exploring the improvement of the network architecture, ac-*Equal contribution.
†Corresponding authors: Wengang Zhou and Houqiang Li.
Figure 1: The DIRE representation of a real image and four generated images from diffusion models: DDPM [17], iDDPM [32], ADM [10], and PNDM [23], respectively. The
DIREs of real images tend to have larger values compared to diffusion-generated images. celeration of sampling, and so on. As users enjoy the strong generation capability of diffusion models, there are concerns about potential privacy problems. For example, diffusion models may memorize individual images from their training data and emit them at the generation stage [3, 52]. Moreover, some attackers may develop new deepfake techniques based on diffusion models. Therefore, it is an urgent demand for a diffusion-generated image detector.
Our focus in this work is to develop a general diffusion-generated image detector. We notice that there are various detectors for detecting generated images available. Despite the fact that most diffusion models employ CNNs as the network, the generation processes between diffusion mod-els and previous generators (e.g., GAN, VAE) are entirely different, rendering previously generated image detectors in-effective. A naïve thought is to train a CNN binary classifier on diffusion-generated and real images. However, we find that such a naïve scheme suffers limited generalization to unseen diffusion-generated images.
that our framework achieves a remarkably high detection accuracy and average precision on generated images from unseen diffusion models, as well as robustness to various perturbations. In comparison with existing generated image detectors, our framework largely exceeds the competitive state-of-the-art methods.
Our main contributions are three-fold as follows.
• We propose a novel image representation called DIRE for detecting diffusion-generated images.
• We set up a new dataset, DiffusionForensics, for bench-marking the diffusion-generated image detectors.
• Extensive experiments demonstrate that the proposed
DIRE sets a state-of-the-art performance in diffusion-generated detection. 2.