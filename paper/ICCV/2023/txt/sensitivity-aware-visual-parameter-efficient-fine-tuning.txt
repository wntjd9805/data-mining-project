Abstract
Visual Parameter-Efficient Fine-Tuning (PEFT) has be-come a powerful alternative for full fine-tuning so as to adapt pre-trained vision models to downstream tasks, which only tunes a small number of parameters while freezing the vast majority ones to ease storage burden and optimiza-tion difficulty. However, existing PEFT methods introduce trainable parameters to the same positions across differ-ent tasks depending solely on human heuristics and neglect the domain gaps. To this end, we study where to intro-duce and how to allocate trainable parameters by proposing a novel Sensitivity-aware visual Parameter-efficient fine-Tuning (SPT) scheme, which adaptively allocates trainable parameters to task-specific important positions given a de-sired tunable parameter budget. Specifically, our SPT first quickly identifies the sensitive parameters that require tun-ing for a given task in a data-dependent way. Next, our
SPT further boosts the representational capability for the weight matrices whose number of sensitive parameters ex-ceeds a pre-defined threshold by utilizing existing struc-tured tuning methods, e.g., LoRA [21] or Adapter [20], to replace directly tuning the selected sensitive parameters (unstructured tuning) under the budget. Extensive exper-iments on a wide range of downstream recognition tasks show that our SPT is complementary to the existing PEFT methods and largely boosts their performance, e.g., SPT im-proves Adapter with supervised pre-trained ViT-B/16 back-bone by 4.2% and 1.4% mean Top-1 accuracy, reaching
SOTA performance on FGVC and VTAB-1k benchmarks, re-spectively. Source code is at https://github.com/ ziplab/SPT. 1.

Introduction
To effectively adapt the pre-trained representations to the downstream tasks, the de-facto choice is full fine-tuning, which initializes the model with the pre-trained weights and tunes all the parameters. However, vanilla full fine-tuning needs to store a separate instance of parameters for each
Figure 1: (a) Existing PEFT methods, such as Adapter [20] introduce trainable parameters to the same positions for all downstream tasks. These methods design task-agnostic po-sitions to employ trainable parameters relying on heuris-tics and neglect consideration of the distinct domain gaps and characteristics for the downstream tasks. (b) Our
Sensitivity-aware visual Parameter-efficient fine-Tuning (SPT) introduces trainable parameters to the task-specific important positions and allocates them with both unstruc-tured and structured tuning granularities, simultaneously.
For structured tuning, SPT can exploit any existing struc-tured tuning methods, such as LoRA [21] or Adapter [20].
Red lines and blocks represent trainable parameters and modules, while blue lines represent frozen parameters. task and each deployment scenario.
It can be extremely storage-intensive as the storage cost grows linearly with the number of possible cases, considering there are vast vari-eties of downstream tasks and dynamic deployment envi-ronments, especially when deploying the large vision mod-els [12, 32, 56] to mobile systems. For example, even stor-ing a single large pre-trained ViT-H [12] model on a local disk requires at least 2.3GB, while the Top-10 U.S. apps required only collectively 2.2GB in May 2021.1
Notably, an emerging solution is to replace vanilla fine-tuning with visual Parameter-Efficient Fine-Tuning (PEFT) [22, 9, 63, 23], which only tunes a small num-â€ Corresponding author. E-mail: bohan.zhuang@gmail.com 1https://sensortower.com/blog/ios-app-size-growth-2021
ber of trainable parameters while freezing the vast ma-jority ones that are shared by multiple tasks. As PEFT approaches exhibit less than 1% of trainable parameters, the storage burden is largely alleviated. Another attrac-tive property of PEFT is that tuning fewer parameters eases the optimization difficulty and mitigates the overfitting is-sue when adapting large pre-trained models on the target dataset, thereby achieving comparable or even better perfor-mance than vanilla fine-tuning [22]. Although promising, the existing PEFT approaches introduce trainable parame-ters to the same positions for all downstream tasks, relying on human heuristics and neglecting task-specific domain gaps and characteristics, which limits their performance.
For instance, in a task-agnostic manner, Prompt Tuning-deep [22] and Adapter [20] respectively add trainable pa-rameters to multi-head self-attention and feed-forward net-work layers for all distinct tasks as depicted in Figure 1 (a).
To address this fundamental challenge, we explore where to introduce and how to allocate trainable parameters un-der a desired parameter budget by presenting a novel
Sensitivity-aware visual Parameter-efficient fine-Tuning (SPT) scheme that identifies the task-specific important po-sitions to adaptively allocate trainable parameters. Since the pre-trained weights at distinct positions have varying contri-butions for different downstream tasks [58, 26, 37], we first propose a new criterion to quickly identify the task-specific sensitive parameters that require tuning in a data-dependent way. Inspired by model pruning metrics [44, 35, 2, 3], we propose to measure the parameter sensitivity with the loss reduction when being tuned, which can be approximated by a first-order Taylor expansion derived within a single forward and backward pass ahead of fine-tuning in one-shot. Our sensitivity criterion is simple and effective, which can identify the task-specific important positions to intro-duce trainable parameters for any backbone quickly. For instance, calculating the sensitivity for ViT-B/16 backbone takes only 5.5 seconds with a single GPU on any of the
VTAB-1k datasets.
With our criterion, we empirically observe that the pro-portions of the sensitivity parameters for each block indeed vary markedly across different tasks in Section 4.4. To al-locate the trainable parameters under a desired trainable pa-rameter budge, an intuitive solution is to directly tune the most sensitive weight connections, which we name unstruc-tured tuning. Despite its simplicity and flexibility, unstruc-tured tuning only tunes a few parameters which still lacks representational capability and is challenging to bridge the domain gap. To this end, we propose to further incorporate structured tuning to replace unstructured tuning at the sensi-tive weight matrices whose numbers of sensitive parameters exceed a pre-defined threshold to improve the representa-tional capability under a similar parameter budget. Struc-tured tuning can be implemented by any parameter-efficient structured tuning methods [21, 9, 23, 22] that directly adjust the hidden representations, e.g., inserting an adapter module sequentially after the sensitive weight matrices. Therefore, our SPT adaptively combines both unstructured and struc-tured tuning granularity and allocates trainable parameters with high flexibility and representational capability for each distinct downstream task.
This paper has the following key contributions. 1) We make the pioneering exploration to identify the task-specific important positions under the PEFT setting, which is fast, effective, versatile to be applied to various backbones with different pre-training strategies, and orthogonal to the exist-ing PEFT methods. 2) Based on the sensitivity criterion, we propose a trainable parameter allocation strategy that adap-tively combines both unstructured and structured tuning un-der a desired parameter budget to achieve high flexibility, large capacity, and favorable trade-off between parameter efficiency and accuracy. 3) Extensive experiments on a total of 24 downstream recognition tasks with both plain and hi-erarchical vision Transformer backbones under supervised and self-supervised pre-trainings show that our SPT is com-plementary to the existing PEFT methods and boosts their performance by large margins. For instance, SPT improves
Adapter [20] by 4.2% mean Top-1 accuracy, outperforming the SOTA PEFT methods on the FGVC benchmark. 2.