Abstract
Simulation has emerged as an indispensable tool for scaling and accelerating the development of self-driving systems. A critical aspect of this is simulating realistic and diverse human behavior and intent. In this work, we pro-pose a holistic framework for learning physically plausible human dynamics from real driving scenarios, narrowing the gap between real and simulated human behavior in safety-critical applications. We show that state-of-the-art meth-ods underperform in driving scenarios where video data is recorded from moving vehicles, and humans are frequently partially or fully occluded. Furthermore, existing methods often disregard the global scene where humans are situated, resulting in various motion artifacts like foot sliding, float-ing, or ground penetration. To address this challenge, we propose an approach that incorporates physics with a rein-forcement learning-based motion controller to learn human dynamics for driving scenarios. Our framework can sim-ulate physically plausible human dynamics that accurately match observed human motions and infill motions for oc-cluded body parts, while improving the physical plausibil-ity of the entire motion sequence. Experiments on the chal-lenging Waymo Open Dataset show that our method out-performs state-of-the-art motion capture approaches signif-icantly in recovering high-quality, physically plausible, and scene-aware human dynamics. 1.

Introduction
Self-driving systems have come a long way in recent years. One major advancement that directly impacted these systems is the widespread adoption of simulation.
While considerable attention was given to simulating traf-fic and other vehicles (e.g., TrafficSim [59], GeoSim [3],
STRIVE [51]), simulating pedestrian motion and behavior received less attention in the literature. In this work, we ar-gue that understanding human behavior and intent is a crit-ical step towards realistic simulation, which in turn is key to the safety of autonomous vehicles (AVs) in real-world settings. The first step towards this goal is capturing physi-cally plausible human dynamics of entire motion sequences in driving scenarios.
Contrary to human motion in indoor scenes [12], motion captured in AV scenarios presents several challenges. These challenges include camera motion, long-term partial or full occlusions, scale variability, and complex interactions with the real-world environment. State-of-the-art (SOTA) ap-proaches [69, 68, 70, 46, 34, 66, 35] for learning human dynamics in indoor scenarios tend to focus on enhancing physical plausibility for visible frames, neglecting several of these obstacles, such as long-term motion occlusions and the interplay between humans and terrains. As a result, it is challenging to directly apply these approaches to com-plex AV scenarios. Although recent works [67, 49] have demonstrated progress in infilling missing motions, there are significant limitations to using these methods for learn-ing physically plausible human dynamics in AV scenarios, as shown in Figure 1. Firstly, these methods consider only physiologically inspired kinematics constraints such as joint limits, and do not model the physical plausibility of the pose in relation to the environment, such as when an object is floating. Secondly, these motion generation models are usu-ally trained on indoor datasets, and the domain gap between indoor and driving scenarios renders them incapable of in-filling missing motions in a plausible way.
In this paper, we present a novel holistic framework to learn human dynamics in such challenging AV scenar-ios. Our proposed framework distinguishes itself from prior works by its ability to generate physically plausibility mo-tions for long-term partially or fully missing body parts, thus addressing an important limitation in the SOTA ap-proaches to learn human dynamics. Firstly, we integrate off-the-shelf motion capturing (e.g., KAMA [13]) and scene reconstruction (e.g., Possion Surface Reconstruction [17]) methods to recover observed motions of visible humans and recover the terrain mesh as well. Before learning human dy-namics from the captured motions, we fix the missing ter-rains by the observed motion trajectories, as well as filter out frames with low-confidence estimation caused by partial occlusion, to guarantee the simulation framework is based on reasonable visual observation. Next, we track the cap-tured motion on the reconstructed terrains by generating in-filling motions for the missing frames while enforcing the physical plausibility of the captured motions (e.g. penetra-tion free against the ground). In contrast to GLAMR [67], which infills a few missing frames together using the pre-trained transformer-based model, our method generated the motions in a stepwise fashion using a local motion con-troller, similar to [28, 62]. This controller-style motion generation reduces foot sliding over long-term occlusions.
We demonstrate the adaptability of our approach by show-ing that, even though it is trained on indoor motion data, it can generalize to in-the-wild driving scenarios by plac-ing physical constraints on the human-scene interactions.
Specifically, we first train the conditional variational au-toencoder [18] (cVAE) as the local motion generator whose latent space is the action space of movement. We then train a high-level controller to sample this latent space to per-form infilling. Although all the motion generation models and physics-aware imitators are trained on an indoor dataset with flat ground, our method can easily adapt these mod-els to uneven terrains in driving scenarios. Finally, we use an additional joint optimization, based on the physics-based imitator and generated motion, to match the video evidence (e.g., 2D keypoints with high confidence) not utilized in the previous stage. In summary, our framework is capable of learning physically plausible human dynamics for entire motion sequences in driving scenarios through visual obser-vations.
We summarize our contributions as follows: 1) We pro-pose the first framework for learning physically plausible human dynamics in driving scenarios, which is capable of generating physically plausible motions for partially or fully missing body parts. 2) We adapt a reinforcement learning-based motion generation framework trained only on indoor motion data, but can generalize to in-the-wild driving sce-narios, infilling physically-plausible motion for occluded frames. 3) We achieve a significant improvement in motion quality over our motion capture framework to learn human dynamics, especially on partially or fully occluded frames. 2.