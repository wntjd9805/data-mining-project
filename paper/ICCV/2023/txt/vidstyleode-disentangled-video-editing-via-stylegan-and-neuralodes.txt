Abstract
We propose VidStyleODE , a spatiotemporally continuous disentangled video representation based upon StyleGAN and
Neural-ODEs. Effective traversal of the latent space learned by Generative Adversarial Networks (GANs) has been the basis for recent breakthroughs in image editing. However, the applicability of such advancements to the video domain has been hindered by the difficulty of representing and con-trolling videos in the latent space of GANs. In particular, videos are composed of content (i.e., appearance) and com-plex motion components that require a special mechanism to disentangle and control. To achieve this, VidStyleODE en-codes the video content in a pre-trained StyleGAN
W+ space and benefits from a latent ODE component to summarize the spatiotemporal dynamics of the input video. Our novel continuous video generation process then combines the two to generate high-quality and temporally consistent videos with varying frame rates. We show that our proposed method enables a variety of applications on real videos: text-guided appearance manipulation, motion manipulation, image ani-mation, and video interpolation and extrapolation. Project website: https://cyberiada.github.io/VidStyleODE
1.

Introduction
Semantic image editing is revolutionizing the visual de-sign industry by enabling users to perform accurate edits in a fast and intuitive manner. Arguably, this is achieved by carrying out the image manipulation process with the guidance of a variety of inputs, including text [4, 24, 32, 54], audio [23, 25], or scene graphs [8]. Meanwhile, the visual characteristics of real scenes are constantly changing over time due to various sources of motion, such as articulation, deformation, or movement of the observer. Hence, it is desir-able to adapt the capabilities of image editing to videos. Yet, training generative models for high-res videos is challeng-ing due to the lack of large-scale, high-res video datasets and the limited capacity of current generative models (e.g.
GANs) to process complex domains. This is why the recent attempts [33, 56] are limited to low-res videos. Approaches that treat videos as a discrete sequence of frames and utilize image-based methods (e.g. [19, 47, 58]) also suffer from im-portant limitations such as a lack of temporal coherency and cross-sequence generalization.
To overcome these limitations, we set out to learn spatio-temporal video representations suitable for both generation and manipulation with the aim of providing several desir-able properties. First, representations should express high-res videos accurately, even when trained on low-scale low-resolution datasets. Second, representations should be robust to irregular motion patterns such as velocity variations or lo-cal differences in dynamics, i.e. deformations of articulated objects. Third, it should naturally allow for control and ma-nipulation of appearance and motion, where manipulating one does not harm the other e.g. manipulating motion should not affect the face identity. We further desire to learn these representations efficiently on extremely sparse videos (3-5 frames) of arbitrary lengths. To this end, we introduce Vid-StyleODE , a principled approach that learns disentangled, spatio-temporal, and continuous motion-content representa-tions, which possesses all the above attractive properties.
Similar to recent successful works [2, 19, 47, 58], we regard an input video as a composition of a fixed appearance, often referred to as video content, with a motion component capturing the underlying dynamics. Respecting the nature of editing, we propose to model latent changes (residuals) re-quired for taking the source image or video towards a target video, specified by an external style input and/or co-driving videos. For this purpose, VidStyleODE first disentangles the content and dynamics of the input video. We model content
W+ space of a pre-trained Style-as a global code in the
GAN generator and regard dynamics as a continuous signal encoded by a latent ordinary differential equation (ODE)
[3, 7, 38], ensuring temporal smoothness in the latent space.
VidStyleODE then explains all the video frames in the latent space as offsets from the single global code summarizing the video content. These offsets are computed by solving the latent ODE until the desired timestamp, followed by subsequent self- and cross-attention operations interacting with the dynamics, content, and style code specified by the textual guidance. To achieve effective training, we omit ad-versarial training that is commonly used in the literature and instead introduce a novel temporal consistency loss (Sec. 3.1) based on CLIP [34]. We show that it surpasses conventional consistency objectives and exhibits higher training stability.
Overall, our contributions are: 1. We build a novel framework, VidStyleODE , disentan-gling content, style, and motion representations using
StyleGAN2 and latent ODEs. 2. By using latent directions with respect to a global latent code instead of per-frame codes, VidStyleODE enables external conditioning, such as text, leading to a simpler and more interpretable approach to manipulating videos. 3. We introduce a new non-adversarial video consistency loss that outperforms prior consistency losses, which mostly employ conv3D features, at a lower training cost. 4. We demonstrate that despite being trained on low-resolution videos, our representation permits a wide range of applications on high-resolution videos, including ap-pearance manipulation, motion transfer, image animation, video interpolation, and extrapolation (cf . Fig. 1). 2.