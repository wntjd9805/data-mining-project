Abstract
Video-based human pose transfer is a video-to-video generation task that animates a plain source human im-age based on a series of target human poses. Considering the difficulties in transferring highly structural patterns on the garments and discontinuous poses, existing methods of-ten generate unsatisfactory results such as distorted textures and flickering artifacts. To address these issues, we propose a novel Deformable Motion Modulation (DMM) that uti-lizes geometric kernel offset with adaptive weight modula-tion to simultaneously perform feature alignment and style transfer. Different from normal style modulation used in style transfer, the proposed modulation mechanism adap-tively reconstructs smoothed frames from style codes ac-cording to the object shape through an irregular recep-tive field of view. To enhance the spatio-temporal consis-tency, we leverage bidirectional propagation to extract the hidden motion information from a warped image sequence generated by noisy poses. The proposed feature propa-gation significantly enhances the motion prediction ability by forward and backward propagation. Both quantitative and qualitative experimental results demonstrate superior-ity over the state-of-the-arts in terms of image fidelity and visual continuity. The source code is publicly available at github.com/rocketappslab/bdmm. 1.

Introduction
The video-based human pose transfer is a task to animate a plain source image according to a series of desired pos-tures. It is challenging due to problems of spatio-temporally discontinuous poses and highly structural texture misalign-ment as depicted in Figure 1.
In this paper, we aim to tackle these problems with an end-to-end generative model to maximize the value of applications in various domains including person re-identification [49], fashion recommen-dation [13, 20], and virtual try-on [8, 33, 42].
Existing works focus on three categories to solve the
Figure 1. Examples of a synthesized video clip based on some noisy poses. Existing methods such as GFLA [35] and Imperson-ator++ [25] fail to generate realistic videos due to problems of spatio-temporally discontinuous poses and highly structural tex-ture misalignment while our method can generate highly plausi-ble texture with seamless transition between consecutive frames.
Please zoom in for more details. spatial misalignment problem, including prior generation
[7, 27, 28, 45, 46], attention module [34, 39, 52], and flow warping [35, 48]. There are many side effects in these methods such as spatially misaligned content, blurry vi-sual quality and unreliable flow prediction. Some methods
[22, 24, 25] proposed to obtain the spatial transformation flow by computing the vertex matching in 3D neural ren-dering process. The main advantage is to preserve more texture details of the source image. However, the genera-tive networks struggle to render new content for occluded regions since flows in such regions are not accurate.
To obtain animated sequences with smooth human ges-ture movements, the temporal coherence is the main deter-minant. Different from most of the generative tasks such as inpainting or super-resolution, the conditional inputs of the sequence in this task are noisy. It is because the existing third-party human pose extractors [2, 10, 21] fail to extract
accurate pose labels in the video frames. It increases the difficulty to predict the temporal correspondence for gen-erating a smooth sequence of frames, especially the highly structural patterns on garments and occluded regions.
In general, previous works [24, 25, 35, 35] mainly use recur-rent neural networks to solve this problem by taking the pre-viously generated result as the input of current time step.
However, the perceptual quality is still unsatisfactory due to limited receptive field of view along time space. We ob-serve that solely relying on unidirectionally hidden states in recurrent units to interpolate the missing content is insuffi-cient. It motivates us to utilize all the frames within the mini batch to stabilize the temporal statistics in the generated se-quence.
To alleviate the aforementioned problems, we propose a novel modulation mechanism – Deformable Motion Modu-lation (DMM) incorporated with bidirectional recurrent fea-ture propagation to perform spatio-temporal affine transfor-mation and style transfer simultaneously. It is designed with three major components, including motion offset, motion mask and the modulated style weight. To strengthen the temporal consistency, the motion offset and mask are re-sponsible for estimating the local geometric transformation based on the features of two spatially misaligned adjacent frames, in which the feature branches come from both for-ward propagation branch and backward propagation branch.
The bidirectional feature propagation encapsulates the tem-poral information of the entire sequence so that a long-range temporal correspondence of a sequence from the for-ward flow to the backward flow can be captured at current time. By maintaining more semantic details from the source image to process the coarsely aligned features, the style weights are modulated by the style codes extracted from the source image. The corresponding affine transformation is enhanced with the augmented spatial-temporal sampling offset. It can produce a dynamic receptive field of view to track semantics so that it can synthesize a sequence of plau-sible and smooth video frames. The main contributions of this work can be summarized as follows:
• We propose a novel Deformable Motion Modula-tion that utilizes geometric kernel offset with adaptive weight modulation to perform spatio-temporal affine transformation and style transfer simultaneously;
• We design a bidirectionally recurrent feature propaga-tion on coarsely warped images to generate target im-ages on top of noisy poses so that a long-range tempo-ral correspondence of the sequence can be captured at current time;
• We demonstrate the superiority of our method in both quantitative and qualitative experimental results with a significant enhancement in perceptual quality in terms of visual fidelity and temporal consistency. 2.