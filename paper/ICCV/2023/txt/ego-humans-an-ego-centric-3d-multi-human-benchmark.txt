Abstract 1.

Introduction
We present EgoHumans, a new multi-view multi-human video benchmark to advance the state-of-the-art of egocen-tric human 3D pose estimation and tracking. Existing ego-centric benchmarks either capture single subject or indoor-only scenarios, which limit the generalization of computer vision algorithms for real-world applications. We propose a novel 3D capture setup to construct a comprehensive ego-centric multi-human benchmark in the wild with annotations to support diverse tasks such as human detection, tracking, 2D/3D pose estimation, and mesh recovery. We leverage consumer-grade wearable camera-equipped glasses for the egocentric view, which enables us to capture dynamic activi-ties like playing tennis, fencing, volleyball, etc. Furthermore, our multi-view setup generates accurate 3D ground truth even under severe or complete occlusion. The dataset con-sists of more than 125k egocentric images, spanning diverse scenes with a particular focus on challenging and unchore-ographed multi-human activities and fast-moving egocentric views. We rigorously evaluate existing state-of-the-art meth-ods and highlight their limitations in the egocentric scenario, specifically on multi-human tracking. To address such lim-itations, we propose EgoFormer, a novel approach with a multi-stream transformer architecture and explicit 3D spatial reasoning to estimate and track the human pose. EgoFormer significantly outperforms prior art by 13.6% IDF1 on the
EgoHumans dataset.
Understanding humans in 3D from the egocentric view is key to building immersive social telepresence [4, 62, 73, 77], assistive humanoid robots [28, 31, 91], and augmented real-ity systems [1, 10, 13]. A crucial step in this direction is to obtain 3D supervision at scale for deep learning models to generalize to the real world. However, unlike the large-scale 2D benchmarks [18, 23, 46, 64, 72], the diversity of the 3D benchmarks [48] is severely limited - primarily because man-ual annotation in the 3D space is impractical. As a result, existing popular 3D benchmarks [37, 43, 48, 66, 82, 110] are constrained to indoor environments or, at most, two hu-man subjects if outdoors, stationary/slow camera motion, with limited occlusion. Furthermore, the majority of these benchmarks only portray the third-person view. Recent progress has been made in constructing egocentric bench-marks [35, 86, 115, 123]. However, they suffer from the same diversity pitfalls, making it difficult to evaluate how close the field is to fully robust and general solutions. To drive advances in the field, we propose a benchmark, Ego-Humans, that includes challenging scenarios ignored in pre-vious studies and a novel method, EgoFormer, that outper-forms prior art as a starting point for the evaluations.
EgoHumans is a new egocentric benchmark consisting of high-resolution videos and comprehensive ground truth annotations such as camera parameters, 2D bounding boxes,
human tracking ids [22], 2D/3D human poses, and 3D hu-man meshes [74]. EgoHumans goes beyond previous bench-marks in important ways. First, it captures outdoor videos of unconstrained environments and dynamic human activities, including challenging sporting events such as fencing, bad-minton, volleyball, etc. Second, the activities are unchore-ographed to truly capture the in-the-wild philosophy of our work. Our video sequences include fast ego-camera motion, human-human occlusion, truncation, and humans appearing at a wide range of spatial scales. We leverage a flexible multi-camera setup consisting of Meta’s Aria glasses [76], with an RGB and two greyscale cameras, for the egocentric view and stationary secondary RGB cameras for the auxil-iary views (see Fig. 4). Such camera combination allows us to accurately track and triangulate human poses in 3D for a long duration without using visual markers [43] or addi-tional sensors [110]. The natural form factor of glasses [79] coupled with the RGB and stereo cameras closely resembles the human vision [81]. Last, as a by-product of our capture setup, we provide 3D annotations for the multi-view sec-ondary cameras. We hope these annotations allow the ability to move fluidly between the egocentric and secondary per-spectives [68] and inspire new research for holistic human understanding. To our knowledge, EgoHumans is the only multi-human 3D egocentric benchmark with these attributes.
We generate high-quality 3D ground truth by lever-aging state-of-the-art visual-inertial odometry algorithm (VIO) [76], which is robust to fast head motion and sud-den changes in the eye gaze - frequently observed in natural human behavior [121]. All the cameras in our multi-view capture are aligned to a single world coordinate system using
Procrustes alignment [75] of the camera poses. EgoHumans consists of 125k egocentric RGB images and 410k human instance annotations (Tab. 1) capturing high-energy activities in various locations, clothing, and lighting conditions with severe occlusion. We annotate the tracking ids, bounding boxes, and 2D/3D human poses for all views using off-shelf estimators [45, 112] and manual supervision. With carefully calibrated camera parameters and the multi-view 2D poses for a video, we optimize for 3D skeletons using triangula-tion [44] and refinement constraints like constant limb length, joint symmetry, and temporal consistency [109]. Finally, we build an efficient multi-stage motion capture pipeline to fit the SMPL [74] body model to the 3D human skeletons.
The scale and diversity of the EgoHumans dataset allow
Ego-Datasets
Location
Ego-Views
Sec-Views
Images
Mo2Cap2 [115]
You2Me [86]
HPS [35]
EgoBody [123] indoor indoor indoor indoor
EgoHumans in/outdoor 1 1 1 1 4 0 0 0 5 15 15k 150k 300k 199k 125k
Instances Mesh World Co.
✗
✗
✓
✓ 15k 150k 320k 374k
✗
✗
✓
✓ 410k
✓
✓
Table 1: Comparison with 3D ego datasets. Ego-Views and Sec-Views are number of ego-views and secondary views. Images and
Instances are number of ego-images and self + other visible human instances. World Co. refers to world translation and rotation. unprecedented opportunities to evaluate and improve ego-centric methods. Specifically, we evaluate existing methods for multi-human tracking. Our results show that prior art is susceptible to common failures like person-id switching due to rapid camera motion, occlusion, and unconstrained human activities. Inspired by this, we present EgoFormer, a novel 3D human tracking approach with multi-stream transformer architecture that effectively performs human depth reason-ing in a camera-agnostic frame of reference. Our proposed method uses self-attention to aggregate multi-view spatial information from the RGB, left, and right stereo cameras simultaneously. EgoFormer significantly outperforms exist-ing state-of-the-art tracking methods [12, 94, 125] by 13.6%
IDF1 score on EgoHumans.
Our contributions are summarized as follows.
• EgoHumans is the first multi-human 3D egocentric dataset capturing unconstrained human activities in the wild. We provide high-quality 3D ground truth from egocentric and secondary views for all humans.
• We benchmark existing state-of-the-art methods for multi-human tracking and highlight their fundamental limitations on egocentric views.
• We propose EgoFormer, a 3D tracking method that uses a multi-stream spatial transformer encoder for depth reasoning from the ego view. Our method consistently outperforms the prior art on the EgoHumans test set. 2.