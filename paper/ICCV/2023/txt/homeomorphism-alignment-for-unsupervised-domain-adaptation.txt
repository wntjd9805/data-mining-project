Abstract
Existing unsupervised domain adaptation (UDA) meth-ods rely on aligning the features from the source and tar-get domains explicitly or implicitly in a common space (i.e., the domain invariant space). Explicit distribution matching ignores the discriminability of learned features, while the implicit counterpart such as self-supervised learning suf-fers from pseudo-label noises. With distribution alignment, it is challenging to acquire a common space which main-tains fully the discriminative structure of both domains.
In this work, we propose a novel HomeomorphisM Align-ment (HMA) approach characterized by aligning the source and target data in two separate spaces. Speciﬁcally, an invertible neural network based homeomorphism is con-structed. Distribution matching is then used as a sewing up tool for connecting this homeomorphism mapping be-tween the source and target feature spaces. Theoretically, we show that this mapping can preserve the data topologi-cal structure (e.g., the cluster/group structure). This prop-erty allows for more discriminative model adaptation by leveraging both the original and transformed features of source data in a supervised manner, and those of target do-main in an unsupervised manner (e.g., prediction consis-tency). Extensive experiments demonstrate that our method can achieve the state-of-the-art results. Code is released at https://github.com/buerzlh/HMA. 1.

Introduction
Deep learning methods rely mostly on a large quantity of manually labeled data [13, 37, 21, 55] which however could be prohibitively expensive or impossible to collect in many scenarios. One effective strategy is to exploit pre-existing labeled data (i.e., the source domain) for a target domain without need for manual labeling. Due to the domain shift
*corresponding author. challenge [35], a model pretrained on a source domain often suffers from drastic performance degradation when directly applied on a target domain. This gives rise to the research attention of Unsupervised Domain Adaptation (UDA).
Existing UDA methods can be roughly divided into two categories. One is based on distribution alignment
[31, 18, 10, 32] which minimizes domain discrepancy by aligning the distributions between two domains. They usu-ally align two different distributions to a single distribu-tion. However, this strategy could distort the original struc-tural information, potentially hurting the ﬁnal model gener-alization [5, 11, 44]. The other category is based on self-supervised learning [9, 26, 43] which also learns a single common feature space using pseudo labels or other self-supervision information. Given inevitable noise with self-supervision, it is difﬁcult to obtain a common feature space with discriminative structure well kept. As shown in Fig. 1(a), adapting a model in a common space cannot guaran-tee better classiﬁcation performance.
A natural solution for the above problem is to learn a latent distribution transformation function without destroy-ing the original distribution of both domains. There are a few works in this line. For example, CyCADA [14] uses ordinary bijection implemented by two different networks to transform the images of the source domain to the target domain and vice versa. However, its learned two networks are not strictly inverse mappings, making the transformed images not necessarily semantically consistent through the transformation. That is, an image cannot be reconstructed after going through the two learned mappings. In addition,
CyCADA learns two networks which is expensive.
To address the above limitations, we leverage a stricter bijection, namely Homeomorphism mapping, a concept borrowed from the topology ﬁeld [34]. If a bijection satis-ﬁes the deﬁnition of homeomorphism (i.e., one-to-one cor-respondence and continuous), theoretically we prove that the data topological structure can be well preserved in the projected space (i.e., the samples in the same cluster are still
Illustration of previous UDA methods and our homeomorphism alignment. (a) Previous methods align the distributions
Figure 1. between two domains in a single common feature space, leading to that per-domain data discriminative structure is not well preserved.
To overcome this problem, (b) our method leverages a homeomorphism mapping as a bridge for alignment across the source and target feature spaces, since the homeomorphism mapping can preserve the original data topological structure. in the same projected cluster). By this property, we further introduce a noise-free self-supervised learning task. With such a reversible mapping, semantic consistency is guar-anteed. As shown in Fig.1(b), with our homeomorphism mapping, the adapted model works better along with both source and target feature spaces.
Motivated by the above analysis, we propose a novel unsupervised domain adaptation method, called Home-omorphisM Alignment (HMA). Our method consists of three components. The ﬁrst is to construct a homeomor-phism mapping for connecting the source and target fea-ture spaces. We show that an Invertible Neural Network (INN) [20] can be used to derive a pair of mutually invert-ible functions (i.e., homeomorphism mapping) through its forward and invertible processes. The second is sewing up across the two feature spaces with using distribution match-ing, as it is required that the cross-space transformed fea-tures are aligned with the original features. For discrimina-tive learning, class semantic information is also considered during homeomorphic mapping. The third is to train the model in a self-supervised manner with the labels of source domain and the predictive consistency of unlabeled target domain as supervision. This is noise-free due to the pre-served topological structure.
Our contributions are summarized as follows: (1) We theoretically prove that homeomorphism mapping can guar-antee the topological structure of the mapped data. This is an important property yet ignored in the existing researches.
We show that an INN can implement a homeomorphism. (2) We propose a novel UDA method with homeomor-phism, the ﬁrst attempt to consider the UDA problem from the viewpoint of topology by conducting domain alignment across two feature spaces. This design differs from the pre-vious alternative methods typically learning a single com-mon feature space for domain alignment. (3) Extensive experiments demonstrate the superiority of our HMA over prior art alternatives, along with in-depth ablation studies. 2.