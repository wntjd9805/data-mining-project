Abstract
Recently, DALL-E [45], a multimodal transformer lan-guage model, and its variants including diffusion models have shown high-quality text-to-image generation capabil-ities. However, despite the realistic image generation re-sults, there has not been a detailed analysis of how to eval-uate such models. In this work, we investigate the visual reasoning capabilities and social biases of different text-to-image models, covering both multimodal transformer lan-guage models and diffusion models. First, we measure three visual reasoning skills: object recognition, object count-ing, and spatial relation understanding. For this, we pro-pose PAINTSKILLS, a compositional diagnostic evaluation dataset that measures these skills. Despite the high-fidelity image generation capability, a large gap exists between the performance of recent models and the upper bound accu-racy in object counting and spatial relation understanding skills. Second, we assess the gender and skin tone biases by measuring the gender/skin tone distribution of gener-ated images across various professions and attributes. We demonstrate that recent text-to-image generation models learn specific biases about gender and skin tone from web image-text pairs. We hope our work will help guide future progress in improving text-to-image generation models on visual reasoning skills and learning socially unbiased rep-resentations.1 1.

Introduction
Generating images from textual descriptions based on machine learning is an active research area [21]. Recently,
DALL-E [45], a 12B parameter transformer [60] trained to generate images from text, has shown a diverse set of gener-ation capabilities, including creating anthropomorphic ob-jects, editing images, and rendering text, which previous models have never shown. Even though DALL-E and its variants have gained much attention, there has not been a
Figure 1. Overview of our proposed evaluation process for text-to-image generation models. In addition to conventional image-text alignment and image quality evaluation, we propose to measure visual reasoning skills (Sec. 4.1) and social biases (Sec. 4.2) of models. The example images are generated with Stable Diffusion. concrete quantitative analysis of what they can do.
Most works have only evaluated their text-to-image gen-eration models with two types of automated metrics [21]: 1) image-text alignment [69, 30, 26] - whether the generated images align with the semantics of the text descriptions; 2) image quality [52, 25] - whether the generated images look similar to images from training data. Hence, to pro-vide novel insights into the abilities and limitations of text-to-image generation models, we propose to evaluate their visual reasoning skills and social biases, in addition to the previously proposed image-text alignment and image qual-ity metrics. Since the original DALL-E checkpoint is not available, in our experiments, we choose four popular text-to-image generation models that publicly release their code and checkpoints: DALL-ESmall [64], minDALL-E [33], Sta-ble Diffusion [49], and Karlo [35]. 1Code and data: https://github.com/j-min/DallEval
First, we introduce PAINTSKILLS, a compositional di-agnostic evaluation dataset that measures three fundamen-tal visual reasoning capabilities: object recognition, object counting, and spatial relation understanding. To avoid sta-tistical bias that hinders models from learning composi-tional reasoning [23, 1, 15, 17], for PAINTSKILLS, we cre-ate images based on a 3D simulator and control our images to have a uniform distribution over objects and relations. To calculate the score for each skill, we employ a widely-used
DETR object detector [11] on the PAINTSKILLS dataset that can detect objects on the test split images with very high or-acle accuracy. We also show that our object detection-based evaluation is highly correlated with human judgment. Then we measure whether the objects in the images satisfy the skill-specific semantics of the input text (see Fig. 2 for ex-amples). Our experiments show that recent text-to-image generation models perform well at object recognition by generating high-fidelity objects but struggle at object count-ing and spatial relation understanding, with a large gap be-tween the model performances and upper bound accuracy.
Second, we introduce social bias evaluation for text-to-image generation models. Recent work has reported social biases in vision-and-language datasets and models learned from them [50, 6]. We evaluate whether models trained on such datasets show bias when generating images from text. For this, we generate images of people with different professions that should not be related to a specific gender or skin tone (e.g., nurse, doctor, teacher). Then, we de-tect gender, skin tone, and attributes from the generated im-ages. We quantify biases by analyzing the distribution of the detected gender/skin tones and their relation to various professions/attributes. Our quantitative study shows that re-cent text-to-image models learned certain biases when gen-erating images from some text prompts (e.g., receptionist
→ female / plumber → male / female → wearing skirts / male → wearing suits). For automated gender and attribute detection, we use BLIP-2 [36] by asking visual questions (e.g., “the person looks like a male or a female?”). For au-tomated skin tone detection, we detect faces from images with FAN [8] and estimate illumination and facial albedo with TRUST [20]. Then we calculate Individual Typol-ogy Angle (ITA) [13] and find the closest skin tone in the
MST scale [40]. Our final automated detection methods are highly correlated with human evaluation.
Our contributions can be summarized as follows: (1)
We introduce PAINTSKILLS, a diagnostic evaluation dataset for text-to-image generation models, which allows carefully controlled measurement of the three fundamental visual rea-soning skills. We show that recent models are relatively good at object recognition (generating a single object) skill, but a large gap exists between the performance of recent models and the upper bound accuracy in object counting and spatial relation understanding skills. (2) We introduce a gender and skin tone bias assessment based on automated and human evaluation. We show that recent models learn specific gender/skin tone biases from web image-text pairs.
Overall, our observations suggest that current text-to-image generation models are good initial contributions, but have several avenues for future improvements in learning challenging visual reasoning skills and understanding so-cial biases. We hope that our evaluation work will allow the community to systemically measure such progress. 2.