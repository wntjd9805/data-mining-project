Abstract
Large scale text-guided diffusion models have garnered significant attention due to their ability to synthesize diverse images that convey complex visual concepts. This gener-ative power has more recently been leveraged to perform text-to-3D synthesis. In this work, we present a technique that harnesses the power of latent diffusion models for edit-ing existing 3D objects. Our method takes oriented 2D im-ages of a 3D object as input and learns a grid-based volu-metric representation of it. To guide the volumetric repre-sentation to conform to a target text prompt, we follow un-conditional text-to-3D methods and optimize a Score Dis-tillation Sampling (SDS) loss. However, we observe that combining this diffusion-guided loss with an image-based regularization loss that encourages the representation not to deviate too strongly from the input object is challeng-ing, as it requires achieving two conflicting goals while viewing only structure-and-appearance coupled 2D projec-tions. Thus, we introduce a novel volumetric regularization loss that operates directly in 3D space, utilizing the explicit nature of our 3D representation to enforce correlation be-tween the global structure of the original and edited object.
Furthermore, we present a technique that optimizes cross-attention volumetric grids to refine the spatial extent of the edits. Extensive experiments and comparisons demonstrate the effectiveness of our approach in creating a myriad of edits which cannot be achieved by prior works1. 1.

Introduction
Creating and editing 3D models is a cumbersome task.
While template models are readily available from online databases, tailoring one to a specific artistic vision often re-quires extensive knowledge of specialized 3D editing soft-ware.
In recent years, neural field-based representations (e.g., NeRF [29]) demonstrated expressive power in faith-fully capturing fine details, while offering effective opti-mization schemes through differentiable rendering. Their 1Our code can be reached through our project page at http:// vox-e.github.io/
applicability has recently expanded also for a variety of editing tasks. However, research in this area has mostly focused on either appearance-only manipulations, which change the object’s texture [46, 48] and style [50, 44], or ge-ometric editing via correspondences with an explicit mesh representation [13, 49, 47]—linking these representations to the rich literature on mesh deformations [19, 40]. Unfortu-nately, these methods still require placing user-defined con-trol points on the explicit mesh representation, and cannot allow for adding new structures or significantly adjusting the geometry of the object.
In this work, we are interested in enabling more flexible and localized object edits, guided only by textual prompts, which can be expressed through both appearance and ge-ometry modifications. To do so, we leverage the incredi-ble competence of pretrained 2D diffusion models in edit-ing images to conform with target textual descriptions. We carefully apply a score distillation loss, as recently pro-posed in the unconditional text-driven 3D generation set-ting [33]. Our key idea is to regularize the optimization in 3D space. We achieve this by coupling two volumetric fields, providing the system with more freedom to comply with the text guidance, on the one hand, while preserving the input structure, on the other hand.
Rather than using neural fields, we base our method on lighter voxel-based representations which learn scene fea-tures over a sparse voxel grid. This explicit grid struc-ture not only allows for faster reconstruction and rendering times, but also for achieving a tight volumetric coupling be-tween volumetric fields representing the 3D object before and after applying the desired edit using a novel volumetric correlation loss over the density features. To further refine the spatial extent of the edits, we utilize 2D cross-attention maps which roughly capture regions associated with the tar-get edit, and lift them to volumetric grids. This approach is built on the premise that, while independent 2D internal fea-tures of generative models can be noisy, unifying them into a single 3D representation allows for better distilling the se-mantic knowledge. We then use these 3D cross-attention grids as a signal for a binary volumetric segmentation al-gorithm that splits the reconstructed volume into edited and non-edited regions, allowing for merging the features of the volumetric grids to better preserve regions that should not be affected by the textual edit.
Our approach, coined Vox-E, provides an intuitive voxel editing interface, where the user only provides a simple tar-get text prompt (see Figure 1). We compare our method to existing 3D object editing techniques, and demonstrate that our approach can facilitate local and global edits involving appearance and geometry changes over a variety of objects and text prompts, which are extremely challenging for cur-rent methods.
Explicitly stated, our contributions are:
• A coupled volumetric representation tied using 3D reg-ularization, allowing for editing 3D objects using dif-fusion models as guidance while preserving the ap-pearance and geometry of the input object.
• A 3D cross-attention based volumetric segmentation technique that defines the spatial extent of textual edits.
• Results that demonstrate that our proposed framework can perform a wide array of editing tasks, which can-not be previously achieved. 2.