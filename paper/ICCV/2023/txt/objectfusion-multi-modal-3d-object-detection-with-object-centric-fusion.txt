Abstract
Recent progress on multi-modal 3D object detection has featured BEV (Bird-Eye-View) based fusion, which effec-tively unifies both LiDAR point clouds and camera images in a shared BEV space. Nevertheless, it is not trivial to perform camera-to-BEV transformation due to the inher-ently ambiguous depth estimation of each pixel, resulting in spatial misalignment between these two multi-modal fea-tures. Moreover, such transformation also inevitably leads to projection distortion of camera image features in BEV
In this paper, we propose a novel Object-centric space.
Fusion (ObjectFusion) paradigm, which completely gets rid of camera-to-BEV transformation during fusion to align object-centric features across different modalities for 3D object detection. ObjectFusion first learns three kinds of modality-specific feature maps (i.e., voxel, BEV, and image features) from LiDAR point clouds and its BEV projections, camera images. Then a set of 3D object proposals are pro-duced from the BEV features via a heatmap-based proposal generator. Next, the 3D object proposals are reprojected back to voxel, BEV, and image spaces. We leverage voxel and RoI pooling to generate spatially aligned object-centric features for each modality. All the object-centric features of three modalities are further fused at object level, which is finally fed into the detection heads. Extensive experiments on nuScenes dataset demonstrate the superiority of our Ob-jectFusion, by achieving 69.8% mAP on nuScenes valida-tion set and improving BEVFusion by 1.3%. 1.

Introduction 3D object detection is one of the fundamental tasks in 3D vision, which aims to localize the objects of interest in the 3D scene. This task plays a critical role in perceiving the surrounding environment of autonomous driving. For ro-bust and high-quality detection, the current practice mostly follows multi-sensor fusion paradigm, which integrates the data derived from different sensors (e.g., cameras and Li-(a) (b) (c)
Figure 1:
BEV-based fusion [34], and (c) our object-centric fusion.
Illustration of (a) point-based fusion [48], (b)
DAR). On one hand, the RGB images convey rich texture and semantics of objects captured from different angles of cameras. LiDAR, on the other hand, observes the environ-ment by emitting pulses of light, yielding point cloud data that preserves accurate geometry information regardless of lighting conditions. As RGB images are vulnerable to light-ing conditions, LiDAR point clouds naturally complement camera images and lead to the idea of blending these multi-sensor data for robust and accurate perception.
Considering that camera and LiDAR offer different per-spectives (i.e., images versus point clouds) of 3D scenes, the mainstream approaches unify them into a shared representa-tion space. Cross-modal projection, such as by point-based
[48, 49] or BEV-based [30, 34] fusing has been proposed.
The point-based fusion strategy [48, 49] first builds the cor-respondence between 3D points and image pixels via cali-bration matrices. As shown in Figure 1(a), the images are projected into the raw point space, augmenting points with the corresponding image features or semantic scores. The augmented points are further transformed into BEV features for 3D detection. However, this point-based fusion only associates points with a small portion of images, leaving the rich semantic information of images under-exploited.
Instead, BEV-based fusion [30, 34] projects both the im-ages and point clouds into a shared BEV space through camera-to-BEV and LiDAR-to-BEV transformations, lead-ing to augmented BEV features for object detection (Figure 1(b)). Despite showing encouraging performances, BEV-based fusion heavily relies on the off-the-shelf depth esti-mator (e.g., LSS [39]) to estimate the depth of each image pixel for camera-to-BEV transformation. As pointed out in
BEVDepth [24], the estimation is error-prone and not triv-ial. Any inaccurate depth estimation will result in spatial misalignment between image pixels and points within the shared BEV space, which subsequently affects object de-tection. Moreover, recall that image and BEV features re-flect two different data peculiarities: images are captured from different perspective views, while BEV features are formulated as top-down aggregation along height dimen-sion. Hence, directly projecting the image features into
BEV space will inevitably lead to projection distortion and destroy the original semantic structures within images.
In view of the limitations of point-based and BEV-based fusion strategies, is it possible to perform multi-modal fu-sion without requiring the non-trivial inter-modality trans-formation (e.g., camera-to-BEV projection)? We address the challenge by presenting a unique fusion paradigm, named Object-centric Fusion (ObjectFusion), as conceptu-ally depicted in Figure 1(c). Our launching point is to intro-duce the object-centric representation in each modality, and spatially align the representations according to the 2D/3D bounding box of an object. ObjectFusion is henceforth able to safely unify the object-centric representations of differ-ent modalities by eliminating inter-modality transformation during fusion. That is, our ObjectFusion nicely preserves the primary feature of each modality, and enables multi-modal fusion at object level with better spatial alignment.
ObjectFusion first generates two modality-specific fea-ture maps (voxel and image features) from point clouds and images via regular 3D/2D networks. The sparse voxel fea-ture maps are flattened along the height dimension, leading to denser BEV features. ObjectFusion leverages a heatmap-based proposal generator to estimate the objectness score in each position of the BEV features and select the top-ranked positions as initial object queries, which triggers the generation of a set of 3D object proposals. Such 3D pro-posals are projected into voxel, image, and BEV spaces to align object-centric features in different spaces. Specifi-cally, the object features corresponding to a proposal are generated by voxel pooling [12] or RoI Align [17] in their respective spaces. With the proposals, the features from the three modalities can be effortlessly aligned, without the non-trivial inter-modality transformations as adopted by
[30, 34]. ObjectFusion contextualizes the object-centric features with a modality-specific contextual encoder. These features are further concatenated and fed into the detection heads for proposal classification and regression. 2.