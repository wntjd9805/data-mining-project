Abstract
Transfer learning is beneﬁcial by allowing the expres-sive features of models pretrained on large-scale datasets to be ﬁnetuned for the target task of smaller, more domain-speciﬁc datasets. However, there is a concern that these pretrained models may come with their own biases which would propagate into the ﬁnetuned model. In this work, we investigate bias when conceptualized as both spurious cor-relations between the target task and a sensitive attribute as well as underrepresentation of a particular group in the dataset. Under both notions of bias, we ﬁnd that (1) mod-els ﬁnetuned on top of pretrained models can indeed inherit their biases, but (2) this bias can be corrected for through relatively minor interventions to the ﬁnetuning dataset, and often with a negligible impact to performance. Our ﬁnd-ings imply that careful curation of the ﬁnetuning dataset is important for reducing biases on a downstream task, and doing so can even compensate for bias in the pretrained model. 1.

Introduction
The current paradigm in machine learning typically in-volves using an off-the-shelf pretrained model that has been trained on a large-scale dataset, and then ﬁnetuning it on a smaller, application-speciﬁc dataset. This transfer learning is especially common for high-dimensional data like images and language [14, 11, 62]. However, large-scale datasets have been criticized for their biases [38, 59, 4, 9], which leaves open the concern that models pretrained on such datasets may carry biases over into the ﬁnetuned model. On the other hand, pretraining has been shown to confer bene-ﬁts in model robustness and uncertainty estimation [24], so there is also the potential that pretrained models can reduce downstream biases by being more regularized or resistant to spurious correlations [5].
In our work, we investigate the implications of bias in pretrained models for the down-stream ﬁnetuning task, and provide actionable insights on how to counteract this.
We have been deliberately vague thus far on what we
Figure 1. We explore bias transference from pretrained to ﬁne-tuned models in two forms in this work: spurious correlations and underrepresentation. We ﬁnd that intervening on the ﬁnetuning data allows us to overcome bias from pretraining, often without compromising on performance. mean by “bias.” In this work, we operationalize bias in two ways based on what has been found thus far to be problem-atic in image features: spurious correlations between a sen-sitive attribute and target task [65, 60, 51, 61] and reduced performance from underrepresentation [6, 10, 50].
The topic of whether pretrained biases matter in ﬁne-tuning is often assumed to be obvious, with contradictory arguments containing intuitively plausible explanations on both sides of the debate: that it does matter because the pretrained model brings biased features [44], or it does not because ﬁnetuning data will overwrite any pretrained bi-ases [7, 15, 56]. Due to this uncertainty, it is not clear how to react to biases found in the features of pretrained mod-els [55, 52, 17]. Of course, there is not a singular binary answer, as much is dependent upon the particulars of the training task. However, we bring much-needed clarity to the space for computer vision tasks, and give advice about bias transference from using pretrained models.
In this work we study the two notions of bias, spurious correlations and underrepresentation, by ﬁnetuning a vari-ety of different pretrained models (Fig. 1). For each notion of bias, we ﬁrst show our results on the CelebA dataset [32].
Then for spurious correlations we investigate the more com-plex COCO dataset [31] using real-world popular pretrained models (e.g., MoCo [21], SimCLR [8]). For underrepre-sentation, we look to the Dollar Street dataset [41] using pretrained models of our own design to test for speciﬁc hy-potheses. On both forms of bias we ﬁnd the following: 1) models ﬁnetuned on top of pretrained models can inherit
their biases (for spurious correlations, this is especially true if the correlation level is high, the salience of the bias sig-nal is high relative to the true task signal, and/or the num-ber of ﬁnetuning samples is low); 2) this bias can be rela-tively easily corrected for by curating the distribution of the
ﬁnetuning dataset, with a negligible impact to performance.
For example on CelebA, we ﬁnd that by manipulating the strength of the spurious correlation in the ﬁnetuning dataset from 20% to 30%, we can retain the same high performance from using a biased pretrained model, but cut the amount of bias almost in half. The implications for this are signiﬁ-cant: practitioners can use the pretrained model that lends the best performance in most cases so long as they appro-priately curate the ﬁnetuning dataset, and thus get the best of both worlds in terms of performance and fairness. This means that signiﬁcant consideration and effort needs to be spent on the curation of ﬁnetuning datasets, in a way that may not necessarily reﬂect the distribution of the test set, in order to “correct” for the biases of the pretrained model.1 2.