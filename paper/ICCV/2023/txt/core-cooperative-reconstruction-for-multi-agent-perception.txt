Abstract
This paper presents CORE, a conceptually simple, ef-fective and communication-efficient model for multi-agent cooperative perception. It addresses the task from a novel perspective of cooperative reconstruction, based on two key insights: 1) cooperating agents together provide a more holistic observation of the environment, and 2) the holis-tic observation can serve as valuable supervision to explic-itly guide the model learning how to reconstruct the ideal observation based on collaboration. CORE instantiates the idea with three major components: a compressor for each agent to create more compact feature representation for efficient broadcasting, a lightweight attentive collab-oration component for cross-agent message aggregation, and a reconstruction module to reconstruct the observation based on aggregated feature representations. This learning-to-reconstruct idea is task-agnostic, and offers clear and reasonable supervision to inspire more effective collabora-tion, eventually promoting perception tasks. We validate
CORE on two large-scale multi-agent percetion dataset,
OPV2V and V2X-Sim, in two tasks, i.e., 3D object detec-tion and semantic segmentation. Results demonstrate that
CORE achieves state-of-the-art performance, and is more communication-efficient. 1.

Introduction
Perception – identifying and interpreting sensory infor-mation – is a crucial ability for intelligent agents to sense the surrounding environment. Thanks to continued ad-vances in deep learning, individual perception has demon-strated remarkable achievements in a number of tasks, e.g., detection [26, 45, 19, 24], segmentation [23, 35, 44] and tracking [36, 43]. Though being promising, it tends to suf-fer from issues (e.g., occlusion) stemmed from limited line-of-sight visibility of individual agents and is challenged by safety concerns. A more compelling paradigm is co-i.e., a collection of agents to be-operative perception,
*The first two authors contribute equally to this work.
†Corresponding author: Tianfei Zhou.
Figure 1: CORE addresses multi-agent perception from a novel perspective of cooperative reconstruction. Take the
Lidar-based autonomous driving scenario as an example, individual vehicles are confined to limited sensing capabili-ties of onboard sensors, yielding partial observations of the whole scene (i.e., raw BEVs). CORE takes a straightfor-ward but remarkably effective step to address this: in ad-dition to task-aware learning (e.g., object detection, seman-tic segmentation), CORE explicitly learns to reconstruct the complete scene (i.e., reconstructed BEVs) from incomplete observations of cooperating agents. The reconstruction-aware learning objective serves as a more sensible goal to inspire more effective cooperation of connected agents, ul-timately boosting perception performance. have as a group by exchanging information with each other so as to use their combined sensory experiences to per-ceive. Along this direction, recent efforts have been made to deliver datasets [40, 39, 15, 5] and cooperative solutions
[22, 21, 1, 5, 28, 31, 10, 2, 34, 38, 39].
All these solutions hold a consensus promise that mul-tiple agents together provide a holistic observation to the environment. But there is a practical challenge of performance-bandwidth trade-off to be addressed. On this account, some studies explore adaptive communication ar-chitectures to reduce bandwidth requirements by dynami-cally determining for each agent, e.g., whom to communi-cate with [22], when to communicate [21], and what mes-sage to communicate [10], while others focus more on the design of multi-agent collaborative strategies, e.g., early collaboration [3, 1] to aggregate raw observations of co-operating agents, late collaboration [20, 33] to combine
prediction results only, or the prevailing intermediate col-laboration [2, 34, 28, 38, 39] to fuse intermediate feature representations that are easy to compress. Although these approaches find applications in myriad domains, from au-tonomous driving [15], to automated warehouses [29], to search and rescue [30], a key question remains unanswered: what should the ideal sensory state of each agent look like after information exchanging and aggregation?
Prior approaches rely on task-specific objectives to learn how to communication or collaborative, which is sub-optimal and potentially degrades generalization capabilities of models to a broader array of perception tasks.
This paper advocates an approach from a novel perspec-tive of cooperative reconstruction (see Fig. 1). Our main in-sight is that, if multiple agents together indeed offer a more complete observation of scene, by absorbing others’ infor-mation, an agent will be able to reconstruct missing parts in its partial raw observation. By learning to reconstruct, the model is urged to learn more effective task-agnostic fea-ture representations, and can offer a clearer explanation to the ideal cooperative state (i.e., features) of the agent, that is, from which we are able to reconstruct its complete ob-servation. Moreover, this learning-to-reconstruct idea natu-rally links to recent advancements in masked data modeling
[9, 7, 25], and enables our model to recover complete ob-servation from even more corrupted inputs obtained, e.g., by masking some proportion of raw observations. With this ca-pability, agents will be allowed to exchange spatially sparse features to reduce transmission overhead during inference.
The idea is realized with our CORE framework. Given a collection of agents and their 2D bird’s eye view (BEV) maps, CORE performs cooperative reconstruction with three key modules: a compression module, a collabora-tion module and a reconstruction module. 1) The compres-sion module computes a compressed feature representation of each BEV for efficient transmission. Unlike most prior works [28, 16, 38] that only consider channel-wise com-pression, the module imposes more significant compres-sion by masking (sub-sampling) features along the spatial dimension. 2) The collaboration module is a lightweight attention module that encourages knowledge exchanging across cooperating agents to enhance feature representa-tions of each individual agent. 3) The reconstruction mod-ule takes a decoder structure to recover complete scene ob-servation from the enhanced feature representation.
It is learned in a supervised rather than self-supervised [9, 25, 7, 17] manner, because we can easily obtain the supervi-sion by aggregating raw sensor data of all agents. Note that the communication-costly raw data fusion only works in the training phase, and the entire reconstruction module will be discarded during inference. The reconstruction objective provides an explicit guidance to multi-agent collaboration, leading to ideal cooperative representations that will benefit various perception tasks. For task-specific decoding (e.g., detection or segmentation), CORE applies conventional de-coders in parallel with the reconstruction decoder.
To verify CORE, we conduct experiments in the au-tonomous driving scenarios on the OPV2V [39] dataset.
Two popular tasks, i.e., 3D object detection and semantic segmentation, are studied. Results show that CORE is gen-eralizable across tasks, and delivers better trade-off between perception accuracy and communication bandwidth. 2.