Abstract
Video segmentation aims to segment and track every
In this paper, we pixel in diverse scenarios accurately. present Tube-Link, a versatile framework that addresses multiple core tasks of video segmentation with a unified architecture. Our framework is a near-online approach that takes a short subclip as input and outputs the cor-responding spatial-temporal tube masks. To enhance the modeling of cross-tube relationships, we propose an effec-tive way to perform tube-level linking via attention along the queries. In addition, we introduce temporal contrastive learning to instance-wise discriminative features for tube-level association. Our approach offers flexibility and effi-ciency for both short and long video inputs, as the length of each subclip can be varied according to the needs of datasets or scenarios. Tube-Link outperforms existing spe-cialized architectures by a significant margin on five video segmentation datasets. it achieves almost 13% relative improvements on VIPSeg and 4% improve-ments on KITTI-STEP over the strong baseline Video K-Net. When using a ResNet50 backbone on Youtube-VIS-2019 and 2021, Tube-Link boosts IDOL by 3% and 4%, respectively. Code is available at https://github. com/lxtGH/Tube-Link.
Specifically, 1.

Introduction
The success of the Detection Transformer (DETR) [3] has inspired recent works [8, 64, 9, 47] to develop uni-versal architectures for addressing all image segmentation tasks using the same architecture, also known as univer-sal image segmentation. In video segmentation, the Video
Panoptic Segmentation (VPS) task involves segmenting and tracking each pixel in input video clips [20, 52, 18], uni-fying the Video Semantic Segmentation (VSS) [30] and
Video Instance Segmentation (VIS) [60] tasks. To mini-mize specialized architecture design for each task, recent studies [25, 21] follow a universal approach to video seg-Figure 1: Tube-Link takes subclips as inputs and links the result-ing tubes in a near online manner. Our design embraces flexibility, efficiency, and temporal consistency, making it suitable for vari-ous video segmentation tasks, including VSS, VIS, and VPS. No-tably, our method outperforms the best-specialized architectures for these tasks on multiple datasets. Best viewed in color. mentation by solving sub-tasks of VPS in a single unified framework. These methods typically use an end-to-end set prediction objective and successfully address multiple tasks without modifying the architecture and loss.
While recent studies have demonstrated promising re-sults, there are still several issues with VPS models and uni-versal video segmentation methods. One major challenge is the lack of exploration of VPS for arbitrary scenes and video clip lengths. To address this gap, Miao et al. [29] have in-troduced a more challenging benchmark, named VIPSeg, which features long videos and diverse indoor and outdoor scenes. This new dataset presents new challenges to exist-ing VPS methods [20, 29, 25, 37], such as increased oc-clusions and appearance changes in diverse scenarios. An-other issue with universal methods [25, 21] is that they can-not achieve comparable results to recent Transformer-based
VIS methods [57, 55], which raises the question of whether we can design a universal video segmentation method to
avoid these specialized designs.
To gain a better understanding of the limitations of cur-rent solutions for video segmentation, we examine the exist-ing methods and categorize them into two groups based on how they process input video clips: online and near-online.
The former [20, 25, 57, 60, 54] performs video segmen-tation at the frame level, while the latter [37, 52, 21, 50, 8, 19, 55] processes clip-wise inputs and directly obtains tube-wise masks. However, there are trade-offs in deploy-ing either approach. Although online methods offer great flexibility, they struggle to use temporal information effec-tively and thus compromise segmentation performance. On the other hand, near-online methods achieve better segmen-tation quality, but they cannot handle long video clips, and most approaches are only validated in VIS tasks, which have fewer instances and simpler scenes.
In this study, we introduce Tube-Link, a universal video segmentation framework that combines the benefits of both online and near-online methods. The framework follows a common input and output space for video segmentation tasks, where a long clip input is split into multiple subclips.
Each subclip contains several frames within a temporal win-dow, and the output is a spatial-temporal mask that tracks the target entity. Our framework is compatible with contem-porary methods such as Mask2Former-VIS [7], where each global query encodes the same tracked entity, and the global queries perform cross-attention with spatial-temporal fea-tures in the decoder directly.
In particular, we propose several key improvements to the Mask2Former-VIS meta-architecture. First, we extend the instance query into an entity query (either thing or stuff) to improve temporal consistency for both thing and stuff segmentation, thus generalizing Mask2Former-VIS into a universal segmentation architecture. Second, we improve the modeling of cross-tube relationships from two differ-ent aspects through temporal consistency learning and tem-poral association learning. For the former, we design a simple link head with self-attention layers that links global queries across tubes to enforce segmentation consistency across tubes. For the latter, we generalize previous frame-level contrastive learning into tube-level and learn tempo-ral association embeddings with a temporal contrastive loss.
Unlike previous works [25, 57, 32] that only learn from two adjacent frames, we consider multiple frames to learn cross-tube consistency. The learned embeddings are then used to perform tube mask matching, which is much more effective than previous counterparts [25] in complex video scenar-ios. Third, with the flexibility of window size and learned association embeddings, we show that one can enlarge the subclip size to improve temporal consistency and inference efficiency, even when trained with fewer subclip inputs.
Our approach is a simple yet flexible framework that out-performs specialized architectures across various video seg-mentation tasks. We evaluate Tube-Link on three video seg-mentation tasks using six datasets (VIP-Seg [29], KITTI-STEP [52], VSPW [30], YouTube-VIS-2019/2021 [60],
OVIS [36]). We demonstrate that, for the first time, our sin-gle architecture performs on par or better than the most spe-cialized architectures on five video benchmarks. In particu-lar, as shown in Fig. 1, using the same ResNet-50 backbone,
Tube-Link outperforms recently published works Video K-Net [25] 4% VPQ on KITTI-STEP, 13% VPQ, and 8% STQ on VIP-Seg, VITA [16] and IDOL [57] on YouTube-VIS-2019 by 3% mAP. We also outperform TubeFormer [21] on
VPSW by 1.7% mIoU, and on YouTube-VIS-2019 by 5.3% mAP. 2.