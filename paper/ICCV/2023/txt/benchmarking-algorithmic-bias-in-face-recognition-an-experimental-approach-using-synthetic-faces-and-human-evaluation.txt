Abstract
We propose an experimental method for measuring bias in face recognition systems. Existing methods to measure bias depend on benchmark datasets that are collected in the wild and annotated for protected (e.g., race, gender) and unprotected (e.g., pose, lighting) attributes. Such observa-tional datasets only permit correlational conclusions, e.g.,
“Algorithm A’s accuracy is different on female and male faces in dataset X.”. By contrast, experimental methods manipulate attributes individually and thus permit causal conclusions, e.g., “Algorithm A’s accuracy is affected by gender and skin color.”
Our method is based on generating synthetic faces us-ing a neural face generator, where each attribute of interest is modified independently while leaving all other attributes constant. Human observers crucially provide the ground truth on perceptual identity similarity between synthetic im-age pairs. We validate our method quantitatively by evalu-ating race and gender biases of three research-grade face recognition models. Our synthetic pipeline reveals that for these algorithms, accuracy is lower for Black and East
Asian population subgroups. Our method can also quan-tify how perceptual changes in attributes affect face iden-tity distances reported by these models. Our large synthetic dataset, consisting of 48,000 synthetic face image pairs (10,200 unique synthetic faces) and 555,000 human anno-tations (individual attributes and pairwise identity compar-isons) is available to researchers in this important area. 1.

Introduction
Face recognition technology has applications in con-sumer media, information security, access control, law en-forcement, and surveillance systems. The one-to-one task is to determine whether a pair of faces share the same iden-tity (“face verification”). The one-to-many task determines whether a face image shares an identity with one or more from a database of face images from known individuals (“face identification”). Face recognition systems imple-mented with deep neural networks today achieve impressive accuracies [54, 13, 33, 43] and outperform even expert face analysts [38]. Nevertheless, it is important to detect and measure possible algorithmic biases, i.e., systematic accu-racy differences, especially across protected demographic attributes like age, race and gender [10, 22, 20], in order to maintain fair treatment in sensitive applications. For this reason, the National Institute of Standards and Technology (NIST) measures bias in commercial face recognition mod-els [17], in particular by comparing their False Match Rate (FMR) and False Non Match Rate (FNMR) values across different demographic subgroups at a particular decision threshold (sweeping this threshold yields FNMR vs. FMR
“curves”).
The first step in measuring bias of face recognition sys-tems is, currently, to collect a large benchmarking dataset containing a set of diverse faces, where each is pho-tographed multiple times under different conditions. An algorithm’s error rate across subgroups specified by differ-ent protected attribute combinations (e.g., different race and gender groups) can then be measured.
Unfortunately, sampling a good test dataset is almost im-possible. First, each protected intersectional group (a spe-cific combination of attribute values) must contain a suffi-ciently large number of individuals, which is not easy when sampling faces in the wild. Second, in order to estimate the causal effect of these protected attributes on algorithm bias, it is crucial to ensure that the joint distribution of un-protected attributes like age, lighting, pose, etc. is roughly equal across protected attribute groups – otherwise biases in the dataset will be misinterpreted as biases in the algo-rithm. It is practically impossible to do this with observa-tional data, i.e., data that we have no power to construct or intervene on ourselves, and all available datasets fall short of this criterion. Third, privacy concerns with collecting hu-man data, and the cost and accuracy of identity annotation, make dataset construction for face recognition benchmark-ing very expensive.
We address this challenge by developing an approach to
construct synthetic face recognition benchmarking datasets using a combination of modern face generation models and human annotators. We assume access to a pretrained face generator with a latent space, such as any of the popular public models trained in a generative adversarial network (GAN) framework [16, 32, 24, 9, 25]. We traverse the gen-erator’s latent space to first construct many “identities” (ac-tually, “pseudo-identities” since there is no real person be-hind the face images) spanning different protected attributes (race and gender in our experiment). For each identity, we vary several unprotected attributes (pose, age, expression and lighting) to construct face sets that ideally depict the same person in different settings. We vary the unprotected attributes in the same way for all faces to ensure a balanced dataset. A challenge in using synthetic faces is we do not have a ground truth identity label per face. Such labels are needed to evaluate the accuracy of our algorithms. We re-place the ground truth with consensus from human annota-tions, which we call the “Human Consensus Identity Confi-dence” (HCIC). We use the HCICs along with the synthetic images to benchmark face recognition systems for bias.
Our approach is fast, practical, inexpensive, and virtually eliminates privacy concerns in data collection. The closest related work uses synthetic faces to benchmark face anal-ysis systems [6], which classify attributes like gender and expression for a single face. We build on their ideas, but address the unique challenge of obtaining ground truth an-notations for identity comparisons between face pairs. Our work is the first to present an annotation pipeline for verify-ing a synthetic face recognition benchmarking dataset, and to demonstrate that a synthetic approach can be reliable for causal face recognition benchmarking.
Using our synthetic dataset consisting of 48,000 syn-thetic face image pairs (10,200 unique synthetic faces) and 555,000 human annotations (individual attributes and pair-wise identity comparisons), we computed face identity dis-tances reported by three public face recognition models. We used the HCICs to assign a ground truth to each pair, and computed False Non Match Rate (FNMR) vs. False Match
Rate (FMR) curves for different attributes and demographic groups. Our results show that these algorithms have low-est error rates on White (a shorthand for European-looking, or Caucasian) groups. For each model, we also report the expected change to predicted face identity distance with re-spect to changes to each unprotected attribute.
We make three contributions: (a) A method to experi-mentally estimate bias in face verification algorithms, and thus estimate the causal connection between attributes and bias, eliminating confusion between test set and algorith-mic bias. (b) An empirical evaluation of our method, where we discover lower accuracy for Black and East Asian faces compared to Caucasian faces in three popular academic-(c) A large dataset of grade face verification algorithms. synthetic faces with human-collected ground truth. 2.