Abstract
Human generation has achieved significant progress.
Nonetheless, existing methods still struggle to synthesize specific regions such as faces and hands. We argue that the main reason is rooted in the training data. A holistic human dataset inevitably has insufficient and low-resolution infor-mation on local parts. Therefore, we propose to use multi-source datasets with various resolution images to jointly learn a high-resolution human generative model. How-ever, multi-source data inherently a) contains different parts that do not spatially align into a coherent human, and b) comes with different scales. To tackle these challenges, we propose an end-to-end framework, UnitedHuman, that
*Equal contribution.
â€ Equal advising. empowers continuous GAN with the ability to effectively utilize multi-source data for high-resolution human gen-eration. Specifically, 1) we design a Multi-Source Spa-tial Transformer that spatially aligns multi-source images to full-body space with a human parametric model. 2)
Next, a continuous GAN is proposed with global-structural guidance and CutMix consistency. Patches from differ-ent datasets are then sampled and transformed to super-vise the training of this scale-invariant generative model.
Extensive experiments demonstrate that our model jointly learned from multi-source data achieves superior quality than those learned from a holistic dataset. Project page: https://unitedhuman.github.io/.
1.

Introduction
Human generation tasks have been intensively explored recently, as synthesized photo-realistic human images can benefit various related applications, such as virtual try-on, movie production, etc. Despite the great success of face generation since the emergence of StyleGAN [17], early at-tempts at human generation tasks [10, 11, 15] exhibit lim-ited generative capabilities, especially in producing high-resolution full-body humans.
We argue this is due to intricately articulated human structures and limited training datasets. Specifically, since local parts like hands and faces only occupy a small portion of the entire image, and as a result, they cannot provide the model with sufficient texture information. Unfortunately, to the best of our knowledge, a comprehensive dataset capable of representing highly-detailed visual information on vari-ous human body parts is sorely lacking. Also, collecting such a dataset from scratch is time-consuming and labour-intensive.
Despite the scarcity of holistic human datasets, a vast quantity of human partial-body data is accessible to assist scholars in finishing multifarious human-related tasks [11, 12, 22, 23]. A trivial solution is to supplement the gener-ation process with multiple datasets of human body parts.
This simple idea is promising since the existing human-related datasets are expected to enhance details of local body parts, and these multi-source datasets constructed from different groups maintain a high degree of diversity in several aspects, including image scale, illumination, and body part position. Also, datasets of body components offer more ample texture details compared to full-body datasets.
Both of these satisfy our needs for human generation and motivate us to utilize multiple human-related datasets to generate high-resolution human bodies.
To unite these multi-source datasets to push the limit of human generation, we analyze the difficulty of this endeavor and find that the main obstacle is twofold. 1) It is challeng-ing to align disparate body parts into a coherent, realistic human since the scales and locations of the body compo-nents in each dataset have different distributions. Merely aligning with 2D keypoints [3] is feasible for rigid objects but suboptimal for hinged human structures, as it disre-gards depth information as well as the body shape. A reli-able alignment mechanism is therefore required to connect these body components. 2) Image resolution varies among multi-source datasets, and this work requires training with these datasets to synthesize results at different resolutions.
Multi-scale generation is another necessity that needs to be addressed because GAN models are typically trained on identical-resolution images and can only synthesize images of a fixed resolution.
To tackle the above two challenges, we propose United-Human. This end-to-end framework, consisting of Multi-Specifically, source Spatial Transformer module for spatial alignment and Continuous GAN module for arbitrary-scale training, leverages multiple datasets to synthesize higher-resolution full-body images. Fig. 2 illustrates our entire working the Multi-Source Spatial Trans-pipeline. former employs the parametric human model as a prior.
This transformation serves to convert the partial-body im-age into the full-body image space, leading to a unified spa-tial distribution. With different sampling parameters in the full-body image space and latent code from the prior distri-bution, the Continuous GAN takes the transformed Fourier feature as input and generates the fixed-resolution patches at corresponding positions and scales. Finally, the gener-ated patches over the full-body space are stitched to form high-resolution full-body images.
Compared to the existing cutting-edge human-GAN models, UnitedHuman demonstrates the ability to incorpo-rate human-related datasets from multiple sources to pro-duce high-resolution humans. The humans generated by our model, with zoomed-in details from 256px to 2048px, are shown on the right side of Fig. 1. During experiments, we demonstrate that by leveraging only 10% of the high-resolution images needed by the SOTA methods, along with the incorporation of various partial datasets from multiple sources, our technique can outperform the existing state-of-the-art outcomes. Furthermore, the model has the poten-tial to scale up to any resolution by introducing additional partial-human datasets.
In summary, our main contributions are listed: 1) We propose UnitedHuman, the first work to explore the usage of multi-source data for high-resolution human generation tasks. 2) We design Multi-source Spatial Transformer to as-sist in aligning body parts from diverse datasets, based on the articulated human structure. 3) We design the Contin-uous GAN to achieve multi-resolution and scale-invariant training. 2.