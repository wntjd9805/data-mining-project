Abstract
This paper proposes a feature synthesis approach for zero-shot semantic segmentation of 3D point clouds, enabling generalization to previously unseen categories.
Given only the class-level semantic information for unseen objects, we strive to enhance the correspondence, alignment and consistency between the visual and semantic spaces, to synthesise diverse, generic and transferable visual fea-tures. We develop a masked learning strategy to promote diversity within the same class visual features and enhance the separation between different classes. We further cast the visual features into a prototypical space to model their distribution for alignment with the corresponding seman-tic space. Finally, we develop a consistency regularizer to preserve the semantic-visual relationships between the real-seen features and synthetic-unseen features. Our approach
Corresponding Author: Yinjie Lei (yinjie@scu.edu.cn) shows considerable semantic segmentation gains on Scan-Net, S3DIS and SemanticKITTI benchmarks. Our code is available at: https://github.com/leolyj/3DPC-GZSL 1.

Introduction
Semantic segmentation of 3D point clouds is mostly dominated by fully-supervised methods [37, 39, 49, 45, 21, 57] that require point-wise labelled data for training. While these methods perform well on previously seen objects, they lack scalability to novel and unseen classes for which no samples are available during training. Zero-Shot Learning (ZSL) provides a promising paradigm in such cases since it enables rapid generalization to unseen classes.
While ZSL from RGB images is well explored [15, 1, 17, 56, 19, 48, 24, 53, 7, 16, 41, 47, 6, 52, 18, 9, 20, 27, 58], ZSL for segmentation of point clouds is less investigated, due to unique challenges posed by 3D data (e.g. the lack of large-scale annotated datasets and pre-trained models [23] which are otherwise ubiquitous in 2D [34]). Most of the existing 3D ZSL methods tackle the relatively simpler classiﬁcation problem [13, 10, 11, 12], with very few methods developed for segmentation [8, 30]. Chen et al. [8] learn shared geo-metric primitives to enable seen-to-unseen migration. How-ever, their approach needs non-annotated unseen samples at training [8], which is restrictive and not suitable for prac-tical scenarios where acquiring unseen data is not always feasible. [30] propose a feature synthesis-based approach for 3D segmentation that can simultaneously generalize to both seen and unseen, without requiring any data for un-seen categories. Nevertheless, the features synthesized from the generator lack contextual diversity due to mode collapse
[50], resulting in limited transfer to unseen classes.
To enable generalization to wider scenarios, we de-velop a feature synthesis framework, which doesn’t require any samples (annotated or non-annotated) during training.
Since semantics are the only common information available for seen and unseen, we need to ensure strong transfer ca-pabilities from the semantic to the visual space. For this purpose, we consider the following semantic-visual transfer issues for ZSL: 1) Semantic-Visual Correspondence Mis-match. The core of ZSL is to exploit and establish a map-ping between semantics and vision, such that for a spe-ciﬁc object, visual features can be uniquely identiﬁed from their corresponding semantics. 2) Heterogeneous Semantic-Visual Embedding. The semantic vectors (embeddings of class-name words) and visual representations (from point cloud data) come from different modalities, and introduce inherent modality-speciﬁc heterogeneity that needs to be tackled in order to align the two data modalities. 3) In-consistent Semantic-Visual Relationship. The relationships between different classes, both seen and unseen, should be consistent in the semantic embedding space and visual fea-ture space, so that the semantics for unseen can faithfully synthesize the unseen visual features.
To address these semantic-visual transfer challenges, as shown in Fig. 1, we design three modules. First, we propose a Mask Correspondence Learning (MCL) module (Sec. 3.2), to learn rich intra-class representations while en-hancing inter-class boundary distribution. We believe pro-moting diversity between the same class features and en-suring separation between classes is critical to synthesize generalized features for unseen classes. Further, for bet-ter seen-to-unseen transfer, we align the seen visual pro-totypes with their corresponding semantics, using our pro-posed Heterogeneous Prototype Alignment (HPA) module (Sec. 3.3). Finally, while learning to synthesize the un-seen visual features, we ensure that the inter-class struc-tural relations of seen+unseen semantics are consistent with their corresponding visual features. For this purpose, we develop a Relational Transfer Consistency (RTC) module (Sec. 3.4) that transfers the seen+unseen semantic relation-ships with the corresponding real-seen+synthesized-unseen visual ones. Our proposed modules complement each other and constrain the generator to synthesize diverse, discrimi-native, and semantically relevant unseen visual features that generalize well for zero-shot segmentation.
We evaluate our model under the challenging General-ized Zero-Shot Learning (GZSL) in inductive setting, where training data contains no labelled or unlabelled unseen class samples, while the model is required to predict both seen and unseen classes at inference. We show signiﬁcant gains over the current state-of-the-art on three public datasets
ScanNet [14], S3DIS [2] and SemanticKITTI [3], by 7.7%, 3.8% and 3.0% respectively, according to the HmIoU met-ric. Our contributions can be summarized as follows:
• We propose an effective masked learning strategy, where visual features of the masked semantics are re-covered via contrastive learning to enhance intra-class diversity and inter-class separation of the learned vi-sual features, enhancing transfer to unseen classes.
• We propose cross-modality prototypical learning that aligns semantics with the visual space, thus promoting generalization to novel concepts.
• We develop consistency regularization that maintains relationships between the real+synthesized visual fea-tures with their corresponding semantics. 2.