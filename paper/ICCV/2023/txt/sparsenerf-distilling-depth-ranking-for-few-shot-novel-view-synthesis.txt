Abstract
Neural Radiance Field (NeRF) significantly degrades when only a limited number of views are available. To complement the lack of 3D information, depth-based mod-els, such as DSNeRF and MonoSDF, explicitly assume the availability of accurate depth maps of multiple views. They linearly scale the accurate depth maps as supervision to guide the predicted depth of few-shot NeRFs. However, ac-curate depth maps are difficult and expensive to capture due to wide-range depth distances in the wild. This work presents a new Sparse-view NeRF (SparseNeRF) frame-work that exploits depth priors from real-world inaccurate observations. The inaccurate depth observations are ei-ther from pre-trained depth models or coarse depth maps of consumer-level depth sensors. Since coarse depth maps are not strictly scaled to the ground-truth depth maps, we pro-pose a simple yet effective constraint, a local depth ranking method, on NeRFs such that the expected depth ranking of the NeRF is consistent with that of the coarse depth maps in local patches. To preserve the spatial continuity of the esti-mated depth of NeRF, we further propose a spatial continu-ity constraint to encourage the consistency of the expected depth continuity of NeRF with coarse depth maps. Surpris-ingly, with simple depth ranking constraints, SparseNeRF outperforms all state-of-the-art few-shot NeRF methods (in-cluding depth-based models) on standard LLFF and DTU datasets. Moreover, we collect a new dataset NVS-RGBD that contains real-world depth maps from Azure Kinect,
ZED 2, and iPhone 13 Pro. Extensive experiments on NVS-RGBD dataset also validate the superiority and generaliz-ability of SparseNeRF. Code and dataset are available at https://sparsenerf.github.io/. 1.

Introduction
Neural radiance fields (NeRFs) [36, 32, 1, 2, 44] have made tremendous progress in generating photo-realistic novel views of scenes by optimizing implicit function repre-sentations given a set of 2D input views. However, in a wide range of real-world scenarios, collecting dense views of a 1
scene is often expensive and time-consuming [47]. There-fore, it is necessary to develop few-shot NeRF methods that can be learned from sparse views without significant degra-dation in performance.
Learning a NeRF from sparse views is a challenging problem due to under-constrained reconstruction condi-tions, especially in textureless areas. Directly applying
NeRFs to few-shot scenarios suffers from dramatic degra-dation [37]. Recently, some methods have greatly improved the performance of few-shot NeRF, which can be catego-rized into three groups. 1) The first group [37, 27, 25] is based on geometry constraints (sparsity and continuity reg-ularizations) and high-level semantics. RegNeRF [37] reg-ularized the geometry and appearance of patches rendered from unobserved viewpoints, and annealed the ray sampling space.
InfoNeRF [27] imposed an entropy constraint of the density in each ray and a spatial smoothness constraint into the estimated images. However, since a scene often contains multiple layouts (Figure 1), sparsity and continu-ity geometric constraints of a few views cannot guarantee the complete 3D geometric reconstruction. 2) The second group [64, 5] resorts to pre-training on similar scenes. For example, PixelNeRF [64] proposed to condition a NeRF on convolutional feature maps to learn high-level seman-tics from other scenes. 3) The third group [17, 65, 24] ex-ploits depth maps and makes a linearity assumption of depth maps to supervise few-shot NeRFs. For example, DSNeRF
[17] exploited sparse 3D points generated by COLMAP
[42] or accurate depth maps [13] obtained by high-accuracy depth scanners and the Multi-View Stereo (MVS) algo-rithm. The depth maps are linearly scaled as supervision to guide the predicted depth of few-shot NeRFs. To use coarse depth maps, MonoSDF [65] uses a local patch-based scale-invariant depth constraint supervised by coarse depth maps instead of global depth maps. However, the scale-invariant depth constraint is strong for real-world coarse depth maps from pre-trained depth models or consumer-level depth sen-sors due to wide-range depth distances in the wild.
Along the third group, we wish to explore more ro-bust 3D priors from coarse depth maps to complement the under-constrained few-shot NeRF. To address this problem, we present SparseNeRF, a simple yet effective method that distills depth priors from pre-trained depth models [41] or inaccurate depth maps from consumer-level depth sensors (Figure 3), which can be easily obtained from real-world scenes. Deriving useful depth cues from such pre-trained
In particular, although single-view models is non-trivial. depth estimation methods have achieved good visual per-formance, thanks to large-scale monocular depth datasets and large ViT models, they cannot yield accurate 3D depth information due to coarse depth annotations, dataset bias, and ill-posed 2D single-view images. The inaccurate depth information contradicts the density prediction of a NeRF when reconstructing each pixel of a 3D scene based on vol-ume rendering. Directly scaling the coarse depth maps to a NeRF [17, 65] leads to inconsistent geometry against the expected depth of the NeRF.
Instead of directly supervising a NeRF with coarse depth priors, we relax hard depth constraints [17, 65] and distill robust local depth ranking from the coarse depth maps to a
NeRF such that the depth ranking of a NeRF is consistent with that of coarse depth. That is, we supervise a NeRF with relative depth instead of absolute depth [17, 65]. To guar-antee the spatial continuity of geometry, we further propose a spatial continuity constraint on depth maps such that the
NeRF model imitates the spatial continuity of coarse depth maps. The accurate sparse geometry constraints from a lim-ited number of views, combined with relaxed constraints including depth ranking regularization and continuity reg-ularization, finally achieve promising novel view synthesis (Figure 1). It is noteworthy that SparseNeRF does not in-crease the running time during inference as it only exploits depth priors from pre-trained depth models or consumer-level sensors during the training stage (Figure 2). In ad-dition, SparseNeRF is a plug-and-play module that can be easily integrated into various NeRFs.
The main contributions of this paper are 1) SparseNeRF, a simple yet effective method that distills local depth rank-ing priors from pre-trained depth models. With the help of the local depth ranking constraint, SparseNeRF signif-icantly improves the performance of few-shot novel view synthesis over the state-of-the-art models (including depth-based NeRF methods). To preserve the coherent geome-try of a scene, we propose a spatial continuity distillation constraint that encourages the spatial continuity of NeRF to be similar to that of the pre-trained depth model. Both depth ranking prior and spatial continuity distillation are new in the literature on NeRF. 2) Apart from SparseNeRF, we also contribute a new dataset, NVS-RGBD, which con-tains coarse depth maps from Azure Kinect, ZED 2, and iPhone 13 Pro. 3) Extensive experiments on the LLFF,
DTU, and NVS-RGBD datasets demonstrate that SparseN-eRF achieves a new state-of-the-art performance in few-shot novel view synthesis. 2.