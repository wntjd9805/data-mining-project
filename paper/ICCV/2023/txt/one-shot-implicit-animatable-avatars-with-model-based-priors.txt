Abstract
Existing neural rendering methods for creating human avatars typically either require dense input signals such as video or multi-view images, or leverage a learned prior from large-scale specific 3D human datasets such that re-construction can be performed with sparse-view inputs.
Most of these methods fail to achieve realistic reconstruc-tion when only a single image is available. To enable the data-efficient creation of realistic animatable 3D humans, we propose ELICIT, a novel method for learning human-specific neural radiance fields from a single image.
In-spired by the fact that humans can effortlessly estimate the body geometry and imagine full-body clothing from a sin-gle image, we leverage two priors in ELICIT: 3D geome-try prior and visual semantic prior. Specifically, ELICIT utilizes the 3D body shape geometry prior from a skinned vertex-based template model (i.e., SMPL) and implements the visual clothing semantic prior with the CLIP-based pre-trained models. Both priors are used to jointly guide the optimization for creating plausible content in the invisible
*Equal contribution.
†Corresponding author. areas. Taking advantage of the CLIP models, ELICIT can use text descriptions to generate text-conditioned unseen re-gions. In order to further improve visual details, we pro-pose a segmentation-based sampling strategy that locally refines different parts of the avatar. Comprehensive eval-uations on multiple popular benchmarks, including ZJU-MoCAP, Human3.6M, and DeepFashion, show that ELICIT outperforms strong baseline methods of avatar creation when only a single image is available. The code is pub-lic for research purposes at https://huangyangyi. github.io/ELICIT 1.

Introduction
Creating realistic 3D contents of animatable human avatars from readily available camera inputs is of great sig-nificance for AR/VR applications, such as telepresence, vir-tual fitness, and so on.
It is quite a challenging task and requires disentangled reconstruction of 3D geometry, the appearance of a clothed human, and accurate modeling of complex body poses for animation.
Current human-specific neural rendering methods have achieved promising performance when dense and well-1
controlled inputs are available, e.g., multi-view videos cap-tured by well-calibrated multi-camera systems [39, 62, 58, 37, 68], or long monocular videos [57] where almost all parts of the human body are visible. Despite their excel-lent performance, it is inconvenient (sometimes impossible) for ordinary users to obtain such high-quality dense inputs.
Various methods have been proposed to address this data inefficiency. For example, ARCH [16] and ARCH++ [13] train reconstruction models with a single image input on large 3D scans datasets, but they do not generalize well to in-the-wild data. Neural radiance fields (NeRF) [32] based human-specific methods [11, 27, 22] train conditional mod-els on multi-view images or video datasets to improve gen-eralizability. However, when only sparse-view inputs are available, they also fail to generate realistic results under extreme settings, e.g., single monocular images.
Instead of learning conditional models from large-scale datasets [66, 6], recent work introduces various regulariza-tions for geometry [36] and appearance [19, 60] to avoid degeneration, which makes it possible to synthesize visually plausible views in a semi-supervised framework without ex-tra training data. However, due to the missing information about the occluded areas of the subject, they can hardly synthesize unseen views that barely overlap with the input views. To address these limitations, we propose a novel method, ELICIT, to learn human-specific neural radiance fields from a single image. We explicitly take advantage of the body shape geometry prior and the visual clothing semantic prior to guide the optimization and achieve free-view rendering from single images.
In summary, our contributions are listed below:
• We present ELICIT, a novel approach that can train an an-imatable neural radiance field from a single image with-out relying on extra training data.
• We propose two effective model-based priors to achieve an animatable 3D free-view rendering digital avatar from single image: 1) the visual clothing semantic prior.
Specifically, we leverage the power of large pretrained vision-language models (i.e., CLIP) to hallucinate the un-seen parts of the clothed body. 2) the human shape prior from the SMPL model. We use the estimated SMPL body shape and pose to constrain our reconstructed clothed 3D avatar be consistent with it.
• To create more realistic and consistent body part details, we propose a novel sampling strategy conditioned on the
SMPL semantic segmentation and body rotation.
We conduct both quantitative and qualitative compar-isons with recent human-specific neural rendering methods in the setting of single image input. We observe that ELICIT can consistently outperform existing methods in both free-view rendering and avatar animation, and simultaneously demonstrate promising performance on in-the-wild images.
Method
Subject data
Extra training data
Invisible area completion
Animatable
NeuralBody [39]
Ani-NeRF [37]
HumanNeRF [57]
PiFU [47]
PaMIR [72]
ARCH [16]
ARCH++ [13]
PHORHUM [1]
MPS-NeRF [11]
NHP [22]
MonoNHR [7]
EVA3D [14]
ELICIT (ours) multi-view images, monocular videos data-free monocular images 3D scans sparse videos, multi-view images monocular images monocular images monocular images multi-view videos multi-view images monocular images data-free
✗
✓
✗
✓
✓
✓
✓
✓
✓
✗
✓
✓
Table 1: Recent human rendering methods that are most relevant to our work. ELICIT is the first work that satisfies these four characteristics together: 1) only requires a single monocular image as an input. 2) doesn’t need extra training data of the subject person. 3) supports recovering body areas that are invisible from the given input view. 4) animatable. 2.