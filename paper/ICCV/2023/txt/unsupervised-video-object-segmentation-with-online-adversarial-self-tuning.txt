Abstract
The existing unsupervised video object segmentation methods depend heavily on the segmentation model trained offline on a labeled training video set, and cannot well gen-eralize to the test videos from a different domain with pos-sible distribution shifts. We propose to perform online fine-tuning on the pre-trained segmentation model to adapt to any ad-hoc videos at the test time. To achieve this, we design an offline semi-supervised adversarial training pro-cess, which leverages the unlabeled video frames to improve the model generalizability while aligning the features of the labeled video frames with the features of the unlabeled video frames. With the trained segmentation model, we fur-ther conduct an online self-supervised adversarial finetun-ing, in which a teacher model and a student model are first initialized with the pre-trained segmentation model weights, and the pseudo label produced by the teacher model is used to supervise the student model in an adversarial learning framework. Through online finetuning, the student model is progressively updated according to the emerging patterns in each test video, which significantly reduces the test-time do-main gap. We integrate our offline training and online fine-tuning in a unified framework for unsupervised video object segmentation and dub our method Online Adversarial Self-Tuning (OAST). The experiments show that our method out-performs the state-of-the-arts with significant gains on the popular video object segmentation datasets. 1.

Introduction
Video Object Segmentation (VOS) aims to track the moving objects in a video sequence with an accurate seg-mentation mask. The existing VOS works can be catego-*Corresponding author. Email:songhuihui@nuist.edu.cn.
This work is supported in part by National Key Research and Develop-ment Program of China under Grant No. 2018AAA0100400, in part by the NSFC under Grant Nos. 61825601, U21B2044, 62276141, 61872189, in part by the Postgraduate Research&Practice Innovation Program of
Jiangsu Province under Grant Nos. KYCX23 1367.
Figure 1. Examples of “visual discrepancy” (row (a)-(b)) in
UVOS, where “train” and “test” indicate where the example im-age is drawn from. Our OAST method produces more precise seg-mentation masks (third column) comparing to the method without
OAST (second column). rized into two paradigms based on whether the prior knowl-edge is provided at the test time. One is the Semi-supervised
VOS (SVOS), where a model is trained on the training set, and at the test time is provided with the ground truth mask on the first frame as prior to segment the objects in the sub-sequent frames. The other is called Unsupervised VOS1 (UVOS), where no ground-truth mask is provided at the test time and there is no prior to leverage to segment the target.
We focus on UVOS since it requires no prior input and is closer to the real-world applications. The existing
UVOS works train a model with a labeled video set, and then directly apply it to the unlabeled videos at the test time [51, 7, 40]. Without any prior as input, the inference has to completely depend on the trained model. This has proven to be effective when the test data are drawn from the same domain as the training data [68, 65, 46]. How-ever, the inference result can become degraded when the test data originate from a different domain which is a common case under the zero-shot setting. The main scenario that can cause severe domain shifts in the test data is called “visual 1It is also referred to as “zero-shot VOS” or “primary object segmenta-tion” in the literature.
discrepancy”. Compared to the training data, the test data may be captured in different environments and manifest sig-nificant inter-class ambiguity and intra-class variance. Fig-ure 1(a) shows a cow object from the training and test data respectively. Due to their appearance difference, the model is confused to perform good segmentation. On the contrary,
Figure 1(b) shows a leopard object for training and a cat object for test. They have highly similar visual appearances even belonging to different categories. As seen, the seg-mentation result is not satisfactory.
Therefore, there is a demand to bridge such visual dis-crepancy in UVOS. Motivated by the online finetuning method commonly used in SVOS [54, 6, 28, 2], we propose to perform online test-time finetuning in UVOS to account for the ad-hoc test videos. Specifically, we start from a pre-trained VOS model, and progressively update it according to the emerging visual patterns in the test video. The up-dates and predictions are performed online, during which the model only has access to the current test video without having access to the full test data or any training data.
There are two challenges in devising an online finetuning method for UVOS. The first is how to reduce the overfitting of the VOS model to the labeled training data, and make it easier to generalize to the test data in new domain. To this end, we propose to a semi-supervised training strategy by adding arbitrary unlabeled videos into model training. It takes a labeled video frame and any unlabeled video frame (without using test data and with a distribution shift from the training data) as input, and trains a discriminator to distin-guish the predicted intermediate feature map of the labeled frame from that of the unlabeled frame. The discriminator is trained in an adversarial manner such that the two kinds of feature map become indistinguishable. This training pro-cess aligns the features of the labeled video frames to the features of the unlabeled frames, which helps relieve the vi-sual discrepancy to the future test data, and improves the generalizability of the trained VOS model.
The second challenge lies in that we have no supervision on the test video to enable online finetuning. We therefore propose to tackle the task in a self-supervised manner. With the trained VOS model, we initialize a teacher model and a student model with the model weights. Motivated by the fact that the mean teacher prediction tends to be more accu-rate than the individual model prediction [49, 56], given a video frame at the test time, we perform data augmentation over it and use the augmentation-averaged prediction from the teacher model as the pseudo ground-truth segmentation mask. Meanwhile, we feed the raw frame into the student model and get a predicted mask. These two masks are then fed into a discriminator, which employs an adversarial loss to minimize the difference of these two masks such that they become indistinguishable. In this process, the parameters of the student model are updated to make similar predictions as the mean teacher prediction. Once the training is done, the weights of the teacher model are updated by the weights of the student model using exponential moving average to account for the emerging patterns in the test data.
We design our method based on the training and fine-tuning strategies discussed above and dub it Online Adver-sarial Self-Tuning (OAST) due to its adversarial nature dur-ing training and testing. We conduct extensive experiments over five popular benchmark datasets and demonstrate that our OAST method achieves the state-of-the-art performance with significant gains. Our main contributions include: (1) an online test-time finetuning method for UVOS, which dy-namically updates the VOS model to adapt into the test data in a new domain, and, to our best knowledge, is the first online finetuning method for UVOS, (2) an offline semi-supervised adversarial training method to improve the gen-eralization ability of the VOS model, (3) an online self-supervised adversarial fintuning method for test-time adap-tation to account for each new test video. 2.