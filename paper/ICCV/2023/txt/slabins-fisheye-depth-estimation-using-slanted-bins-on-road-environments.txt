Abstract (cid:8)(cid:5)(cid:2) (cid:4)(cid:12)(cid:18)(cid:21)(cid:13) (cid:9)(cid:15)(cid:10)(cid:16)(cid:21)(cid:12)(cid:11)(cid:1)(cid:7)(cid:3)(cid:6)(cid:1)(cid:19)(cid:12)(cid:18)(cid:19)(cid:12)(cid:20)(cid:12)(cid:16)(cid:21)(cid:10)(cid:21)(cid:14)(cid:17)(cid:16)
Although 3D perception for autonomous vehicles has fo-cused on frontal-view information, more than half of fatal accidents occur due to side impacts in practice (e.g., T-bone crash). Motivated by this fact, we investigate the prob-lem of side-view depth estimation, especially for monocu-lar ﬁsheye cameras, which provide wide FoV information.
However, since ﬁsheye cameras head road areas, it ob-serves road areas mostly and results in severe distortion on object areas, such as vehicles or pedestrians. To al-leviate these issues, we propose a new ﬁsheye depth es-timation network, SlaBins, that infers an accurate and dense depth map based on a geometric property of road en-vironments; most objects are standing (i.e., orthogonal) on the road environments. Concretely, we introduce a slanted multi-cylindrical image (MCI) representation, which al-lows us to describe a distance as a radius to a cylindri-cal layer orthogonal to the ground regardless of the cam-era viewing direction. Based on the slanted MCI, we es-timate a set of adaptive bins and a per-pixel probability map for depth estimation. Then by combining it with the estimated slanted angle of viewing direction, we directly in-fer a dense and accurate depth map for ﬁsheye cameras.
Experiments demonstrate that SlaBins outperforms the state-of-the-art methods in both qualitative and quantita-tive evaluation on the SynWoodScape and KITTI-360 depth datasets. For more information, you can visit our project page https://syniez.github.io/SlaBins/. 1.

Introduction
With the advent of autonomous driving, the importance of 3D perception keeps growing for safety [20, 25, 44].
For example, ADAS systems pervading our daily driving already, such as lane-keeping aid and forward collision-*Equal contribution (alphabet order).
†Corresponding author. (cid:1) (cid:1) (cid:1)
Figure 1: SlaBins on the SynWoodScape dataset [34].
SlaBins predicts dense and accurate depth maps regardless of the viewing directions using the slanted MCI representation. avoidance assist, depend on various sensor modalities to ob-serve 3D information [43, 5]. Here, one interesting point is that various sensor modalities for 3D perception, including many datasets related to autonomous driving, are focusing on the frontal view predominantly [43, 38, 2, 6]. We can easily deduce the reason by our common sense that it is im-portant to keep an eye on the front while driving. However, according to the National Transportation Highway Safety
Administration [1], 51% of fatal car accidents correspond to T-bone crashes in which one car hits the side of another, which is the second highest type of accident. This statistic supports that side-view 3D perception is important to assist drivers because they are hard to pay attention to the side view due to the physical characteristics of the people look-ing ahead and their position characteristics.
Motivated by this fact, 3D perception for side-view, es-pecially ﬁsheye-based depth estimation, has started to gain the spotlight [31, 16, 41]. Fisheye cameras have several advantages for 3D perception in road scenes. Speciﬁcally,
ﬁsheye cameras provide broader FoV information (typically greater than 180◦) in a single image, which allows us to ob-serve road information (e.g., road, vehicles, and pedestri-ans) at a low cost. In addition, we can directly utilize per-spective camera 3D perception approaches because mathe-matical mutual conversion between two camera models is possible. Thus, if we can obtain side-view depth informa-tion using a single ﬁsheye camera, we can efﬁciently handle various vision tasks for autonomous driving.
However, despite the advantages of ﬁsheye cameras, monocular ﬁsheye camera-based side-view depth estima-tion is a challenging problem due to signiﬁcant distortion.
In addition to inherent issues of monocular depth estima-tion, such as scale ambiguity [8], severe distortion of ﬁsh-eye cameras makes depth estimation more difﬁcult by dis-torting scene objects (e.g., close objects look closer and dis-tant objects look more distant abnormally). Particularly, this distortion problem could be more severe in road environ-ments because ﬁsheye cameras for ADAS systems gener-ally look to the road areas, and vital objects such as vehicles and pedestrians are located at the edge of the image. That is, most regions of the image correspond to the road areas and objects contain very few pixels, so depth estimation be-comes more challenging.
To alleviate this problem, we pay attention to the ge-ometric property of road environments; most objects are standing on the road environments. Based on this geomet-ric characteristic, we can easily imagine the visible region of each object as a planar segment orthogonal to the road re-gion. This approximation is connected to a multi-plane im-age (MPI) concept [36] that represents a given scene as the composition of multi-layered images according to depth.
Note that one signiﬁcant difference is that a ﬁsheye camera looks down to the road region in our case (i.e., camera view-ing direction is not parallel to the ground), unlike a camera points to a given scene in the general MPI concept. Thus, a na¨ıve MPI concept is not directly applicable.
In this work, we propose a new ﬁsheye depth estimation framework, SlaBins, that exploits the geometric property of the road environments (see Fig. 1). To be speciﬁc, we adopt a multi-cylindrical image (MCI) representation1; es-pecially, we introduce a slanted MCI of which the cylin-drical layer is deﬁned as orthogonal to the road ground re-gardless of camera viewing direction. Based on the slanted
MCI, we estimate adaptive bins and the per-pixel probabil-ity of the slanted cylindrical layers, which allows us to im-plicitly preserve the geometric property of the road environ-ments and improve the depth quality near the boundary be-tween objects and the ground region. In addition, we inde-pendently estimate the slanted angle of the camera viewing direction w.r.t. the road ground. Then, by combining this angle with the estimated depth information in the slanted
MCI domain, we can directly compute a dense and accurate depth map for ﬁsheye cameras. We validate the proposed
SlaBins framework on the SynWoodScape dataset [34] and KITTI-360 depth dataset (a modiﬁed dataset of KITTI-1Basically, depth in the ﬁsheye model is deﬁned as the Euclidean dis-tance, which is connected to a multi-spherical image (MSI). In this work,
We adopt an MCI representation that combines the geometrical suitability of MPI and the ﬁsheye model ﬁtness of MSI. 360 [24] for our scenario), where SlaBins outperforms the baseline methods [3, 20, 21] and shows potential as scene representations for downstream tasks, e.g., segmentation.
In a nutshell, our contributions are as follows:
• We propose a new ﬁsheye depth estimation framework,
SlaBins, which infers dense depth based on the geo-metric property of road environments.
• We newly introduce a slanted MCI representation as an intermediate road scene representation, which allows us to describe a given scene regardless of viewing direction and improve the depth quality on the boundary areas be-tween objects and the road.
• By decoupling the slanted angle of an input image, we force the proposed model to learn the slanted MCI depth information and then seamlessly compute a ﬁsheye depth map in a slanted-aware regression way. 2.