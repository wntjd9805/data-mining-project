Abstract
Contrastive Language-Image Pretraining has emerged as a prominent approach for training vision and text en-coders with uncurated image-text pairs from the web. To enhance data-efficiency, recent efforts have introduced ad-ditional supervision terms that involve random-augmented views of the image. However, since the image augmentation process is unaware of its text counterpart, this procedure could cause various degrees of image-text misalignments during training. Prior methods either disregarded this dis-crepancy or introduced external models to mitigate the im-pact of misalignments during training. In contrast, we pro-pose a novel metric learning approach that capitalizes on these misalignments as an additional training source, which we term “Misalign, Contrast then Distill (MCD)”. Unlike previous methods that treat augmented images and their text counterparts as simple positive pairs, MCD predicts the continuous scales of misalignment caused by the augmen-tation. Our extensive experimental results show that our proposed MCD achieves state-of-the-art transferability in multiple classification and retrieval downstream datasets. 1.

Introduction
Recent advances in deep learning have shown that image representations trained with large-scale uncurated natural language supervision shows powerful transferability to var-ious downstream tasks [14, 32]. A predominant paradigm in vision–language pre-training is to use a simple contrastive loss that makes the embedding of an image and its matching text description (positive pair) more similar to each other than other arbitrary image–text pairs (negative pairs) [29].
To achieve a more data-efficient training, following works actively capitalized on image random augmentation by: (i) joining language–image pretraining objectives with vision self-supervision terms [3, 5] between the augmented im-ages [27, 23, 18] and (ii) involving more pairs of posi-*Correspondence to: bumsoo.kim@lgresearch.ai
Figure 1. Conceptual illustration of contrastive language–image objectives of previous works (i.e., InfoNCE) and our MCD for (a) augmentation that doesn’t harm the correspondence with its description and (b) augmentation that does. Previous works ei-ther disregard these misalignments [23] or leverage external mod-els [18, 9] to mitigate their impact. On the other hand, MCD uses the continuous degree of misalignments caused by random image augmentation as a useful source for training various levels of align-ments between images and their text descriptions. tive/negative supervisions between the augmented images and their original text description [23, 18].
However, since the random image augmentation process is unaware of its corresponding text, it often results in the augmented image view to be misaligned with its descrip-tion (see (b) in Fig.1). These misalignments behave as noisy training signals for the contrastive loss in language– image pretraining, thus causing performance degradation if not properly attended [27]. To mitigate this issue, recent works have used additional augmentation embeddings [18] or heavy external off-the-shelf object detectors and sum-mary extractors [9] to match the alignment during training.
Though being straightforward and showing strong perfor-mance, these works are limited in that they add unnecessary burden in both training and inference.
Based on this observation, we start with a simple ques-tion: “Instead of treating misalignments as noise to elim-inate, can we rather harness them as a training source for language-image pretraining?”. To this end, we propose
MCD (i.e., Misalign, Contrast then Distill), a novel train-ing framework that leverages the various levels of misalign-ments between random augmented images and its text de-scription during training.
MCD consists of three steps (see Fig 2 for an overview illustration of MCD): First, we conduct random augmen-tation on the image that causes various levels of misalign-ments (or not at all) with its text counterpart (Misalign).
Then, we project all the participants (image, text, and aug-mented image) into an unified multimodal space, and learn the distance between all the image–text pairs with a con-trastive objective (Contrast). Finally, we use a teacher-student network where the student learns from the “soft” distance between the text–original image (i.e., D( ¯I, T ) in
Fig 2) and text–augmented image (i.e., D( ¯I ′, T ) in Fig 2) of the momentum teacher with a log-ratio loss (Distill). continuous scale of misalignment to the student model, en-abling the student to learn from the various levels of mis-alignment that occur from the random augmentation during training time.
Our contribution of this paper is threefold:
• We propose MCD, a novel training framework where we learn the continuous level of misalignment as a source for contrastive language-image pretraining.
• Our MCD outperforms state-of-the-art models across various single/multi-modal downstream datasets with-out adding additional parameters for inference or using external models to force the alignment.
• We propose three distillation strategies leveraging mis-alignments: i) misalignment between positive pairs, ii) misalignment between negative pairs, and iii) mis-alignment between noisy pairs. Extensive experiments show that all three strategies positively contributes to our final performance. 2.