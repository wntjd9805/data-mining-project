Abstract
Recent text-to-video generation approaches rely on com-putationally heavy training and require large-scale video
In this paper, we introduce a new task, zero-datasets. shot text-to-video generation, and propose a low-cost ap-proach (without any training or optimization) by leveraging the power of existing text-to-image synthesis methods (e.g.
Stable Diffusion), making them suitable for the video do-main. Our key modiﬁcations include (i) enriching the la-*Equal contribution. tent codes of the generated frames with motion dynamics to keep the global scene and the background time consis-tent; and (ii) reprogramming frame-level self-attention us-ing a new cross-frame attention of each frame on the ﬁrst frame, to preserve the context, appearance, and identity of the foreground object. Experiments show that this leads to low overhead, yet high-quality and remarkably consistent video generation. Moreover, our approach is not limited to text-to-video synthesis but is also applicable to other tasks such as conditional and content-specialized video gener-ation, and Video Instruct-Pix2Pix, i.e., instruction-guided
video editing. As experiments show, our method performs comparably or sometimes better than recent approaches, despite not being trained on additional video data. Our code is publicly available at: https://github.com/Picsart-AI-Research/Text2Video-Zero. 1.

Introduction
In recent years, generative AI has attracted enormous at-tention in the computer vision community. With the advent of diffusion models [34, 12, 35, 36], it has become tremen-dously popular and successful to generate high-quality im-ages from textual prompts, also called text-to-image synthe-sis [26, 29, 32, 7, 44]. Recent works [14, 33, 11, 42, 5, 21] attempt to extend the success to text-to-video generation and editing tasks, by reusing text-to-image diffusion models in the video domain. While such approaches yield promis-ing outcomes, most of them require substantial training with a massive amount of labeled data which can be costly and unaffordable for many users. With the aim of making video generation cheaper, Tune-A-Video [42] introduces a mech-anism that can adopt Stable Diffusion (SD) model [29] for the video domain. The training effort is drastically reduced to tuning one video. While that is much more efﬁcient than previous approaches, it still requires an optimization pro-cess. In addition, the generation abilities of Tune-A-Video are limited to text-guided video editing applications; video synthesis from scratch, however, remains out of its reach.
In this paper, we take one step forward in studying the novel problem of zero-shot, “training-free” text-to-video synthesis, which is the task of generating videos from tex-tual prompts without requiring any optimization or ﬁne-tuning. A key concept of our approach is to modify a pre-trained text-to-image model (e.g., Stable Diffusion), en-riching it with temporally consistent generation. By build-ing upon already trained text-to-image models, our method takes advantage of their excellent image generation qual-ity and enhances their applicability to the video domain without performing additional training. To enforce tempo-ral consistency, we present two innovative and lightweight modiﬁcations: (1) we ﬁrst enrich the latent codes of gen-erated frames with motion information to keep the global scene and the background time consistent; (2) we then use cross-frame attention of each frame on the ﬁrst frame to preserve the context, appearance, and identity of the fore-ground object throughout the entire sequence. Our exper-iments show that these simple modiﬁcations lead to high-quality and time-consistent video generations (see Fig. 1 and further results in the appendix). Despite the fact that other works train on large-scale video data, our method achieves similar or sometimes even better performance (see
Figures 8, 9 and appendix Figures 18, 25, 26). Furthermore, our method is not limited to text-to-video synthesis but is also applicable to conditional (see Figures 5,6 and appendix
Figures 19, 21, 22, 23) and specialized video generation (see Fig. 7), and instruction-guided video editing, which we refer as Video Instruct-Pix2Pix motivated by Instruct-Pix2Pix [2] (see Fig. 9 and appendix Figures 24, 25, 26).
Our contributions are summarized as three-folds:
• A new problem setting of zero-shot text-to-video syn-thesis, aiming at making text-guided video generation and editing “freely affordable”. We use only a pre-trained text-to-image diffusion model without any fur-ther ﬁne-tuning or optimization.
• Two novel post-hoc techniques to enforce temporally consistent generation, via encoding motion dynamics in the latent codes, and reprogramming each frame’s self-attention using a new cross-frame attention.
• A broad variety of applications that demonstrate our method’s effectiveness, including conditional and spe-cialized video generation, and Video Instruct-Pix2Pix i.e., video editing by textual instructions. 2.