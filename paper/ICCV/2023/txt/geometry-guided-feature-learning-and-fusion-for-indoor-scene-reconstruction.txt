Abstract
In addition to color and textural information, geometry provides important cues for 3D scene reconstruction. How-ever, current reconstruction methods only include geometry at the feature level thus not fully exploiting the geometric information.
In contrast, this paper proposes a novel geometry in-tegration mechanism for 3D scene reconstruction. Our approach incorporates 3D geometry at three levels, i.e. feature learning, feature fusion, and network supervision.
First, geometry-guided feature learning encodes geomet-ric priors to contain view-dependent information. Second, a geometry-guided adaptive feature fusion is introduced which utilizes the geometric priors as a guidance to adap-tively generate weights for multiple views. Third, at the su-pervision level, taking the consistency between 2D and 3D normals into account, a consistent 3D normal loss is de-signed to add local constraints.
Large-scale experiments are conducted on the Scan-Net dataset, showing that volumetric methods with our ge-ometry integration mechanism outperform state-of-the-art methods quantitatively as well as qualitatively. Volumetric methods with ours also show good generalization on the 7-Scenes and TUM RGB-D datasets. 1.

Introduction 3D scene reconstruction is an important topic in 3D computer vision, with many applications such as mixed/augmented reality, autonomous navigation, and robotics.
It is also considered one of the fundamental tasks in 3D scene understanding including 3D segmenta-tion [22, 33, 36] and object detection [37, 40]. Although nowadays cameras equipped with depth sensors (e.g. Lidar and Kinect) can reconstruct scenes using perspective pro-jection and depth fusion [18], these RGB-D cameras are still expensive, and not yet widely used in consumer cam-eras. Therefore, they are limited in their applicability. In contrast, scene reconstruction from RGB images (multi-Figure 1. Pipeline of existing volumetric methods compared to our proposed geometry-guided feature learning and fusion for 3D scene reconstruction. Our approach (green parts) integrates view-dependent and local geometry into (1) feature learning, (2) multi-view feature fusion, and (3) network supervision. view or video) is much more accessible.
A standard approach to 3D scene reconstruction is to compute the Truncated Signed Distance Function (TSDF) volume and then apply the marching cubes algorithm [15] to capture the surface. To generate the TSDF volume, tra-ditional reconstruction methods [11, 42, 43, 23, 38, 44] first generate depth maps for each RGB image and then apply depth fusion [5]. Due to pixel-level prediction, depth-based methods can generate dense 3D points but may suffer from scale ambiguity and depth inconsistency between overlap-ping regions in different views. Recently, volumetric (di-rect) methods [17, 13, 2, 28] are proposed to predict the
TSDF directly, without reliance on depth estimation. 3D scenes are modeled using volumetric methods that employ 3D CNNs, allowing for the filling of unobserved gaps and resulting in enhanced predictions. However, both depth-based and volumetric methods still capture texture and color features based on RGB information.
For multi-view tasks, geometric information (e.g. sur-face normal and viewing direction) provides rich view-dependent cues of 3D scenes. For example, the best view-ing direction is perpendicular to the viewing position. This viewpoint (or one close to it) is preferred over other views.
Also, voxels derived from the same plane should have simi-lar surface normals. Hence, extracting important cues from these geometries can be beneficial for feature learning and scene representation.
In addition, multi-view feature fu-sion plays a vital role in volumetric reconstruction meth-ods. Due to changing imaging conditions (e.g. illumina-tion, camera orientation, and occlusion), instead of simply averaging views, some views may be preferred over oth-ers in terms of their positioning (i.e. more useful geometry information). Furthermore, volumetric methods usually su-pervise the predicted TSDF in a voxel-to-voxel manner ig-noring local information, and hence may deviate from the actual surfaces.
To address the aforementioned issues, in this paper, a geometry integration mechanism is proposed for 3D scene reconstruction. To this end, geometric information is ex-ploited by our method at three different stages (see Figure 1b): (1) feature learning, (2) feature fusion, and (3) network supervision. Firstly, to exploit discriminative information for 3D reconstruction, a geometry-guided feature learning (G2FL) is introduced to encode and integrate geometric pri-ors (e.g. surface normal, projected depth, and viewing direc-tion) into the multi-view features. Transformers and multi-layer perceptron (MLP) are utilized to exploit the geometric information. Secondly, during multi-view feature fusion, the occluded views and views away from others may be as-signed different attention levels. Therefore, the occlusion prior and relative pose distance are adopted to construct the multi-view attention function, forming a geometry-guided adaptive feature fusion (G2AFF). Thirdly, at the supervision level, the 3D surface normal is calculated from the TSDF, which at the same time maintains local information. To en-hance the local constraints and improve the reconstruction quality, a consistent 3D normal loss (C3NL) is proposed, considering the consistency between 2D and 3D normal, discarding boundaries and thin objects.
Our main contributions are summarized as follows:
• A novel geometry integration mechanism is proposed for 3D scene reconstruction, encoding geometric pri-ors at three levels, i.e. feature learning, feature fusion, and network supervision.
• A geometry-guided feature learning scheme encodes 3D geometry into multi-view features. A geometry-guided adaptive feature fusion method uses geometric priors as a guidance to learn a multi-view weight func-tion adaptively.
• The consistency between 2D and 3D normal is ex-ploited. A consistent 3D normal loss is introduced to constrain local planar regions in the prediction.
• Volumetric methods enhanced with our method show state-of-the-art performance on the ScanNet dataset and demonstrates convincing generalization on the 7-Scenes and TUM RGB-D datasets. 2.