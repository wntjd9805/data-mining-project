Abstract
The pretrain-finetune paradigm in modern computer vision facilitates the success of self-supervised learning, which tends to achieve better transferability than super-vised learning. However, with the availability of massive labeled data, a natural question emerges: how to train a better model with both self and full supervision signals?
In this paper, we propose Omni-suPErvised Representation leArning with hierarchical supervisions (OPERA) as a so-lution. We provide a unified perspective of supervisions from labeled and unlabeled data and propose a unified framework of fully supervised and self-supervised learn-ing. We extract a set of hierarchical proxy representations for each image and impose self and full supervisions on the corresponding proxy representations. Extensive exper-iments on both convolutional neural networks and vision transformers demonstrate the superiority of OPERA in im-age classification, segmentation, and object detection.1 1.

Introduction
Learning good representations is a significant yet chal-lenging task in deep learning [12, 75, 23]. Researchers have developed various ways to adapt to different super-visions, such as fully supervised [41, 30, 56, 52], self-supervised [59, 68, 21, 10], and semi-supervised learn-ing [67, 71, 58]. They serve as fundamental procedures in various tasks including image classification [16, 72, 70], se-mantic segmentation [21, 48], and object detection [24, 5].
Fully supervised learning (FSL) has always been the de-fault choice for representation learning, which learns from discriminating samples with different ground-truth labels.
However, this dominance begins to fade with the rise of
Figure 1. The proposed OPERA outperforms both fully supervised and self-supervised counterparts on various downstream tasks. the pretrain-finetune paradigm in modern computer vision.
Under such a paradigm, researchers usually pretrain a net-work on a large dataset first and then transfer it to down-stream tasks [22, 14, 23, 12]. This advocates transferabil-ity more than discriminativeness of the learned representa-tions. This preference nurtures the recent success of self-supervised learning (SSL) methods with contrastive objec-tive [23, 64, 21, 10, 60]. They require two views (aug-mentations) of the same image to be consistent and dis-tinct from other images in the representation space. This instance-level supervision is said to obtain more general and thus transferable representations [18, 27]. The ability to learn without human-annotated labels also greatly pop-ularizes self-supervised contrastive learning. Despite its advantages, we want to explore whether combining self-supervised signals2 with fully supervised signals further im-proves the transferability, given the already availability of
*Equal contribution.
â€ Corresponding author. 1Code link: https://github.com/wangck20/OPERA. 2We mainly focus on self-supervised contrastive learning. In the rest of the paper, we use self-supervised learning to refer to self-supervised contrastive learning unless otherwise specified for simplicity.
massive annotated labels [46, 33, 1, 4].
We find that a simple combination of the self and full supervisions results in contradictory training signals. To address this, in this paper, we provide Omni-suPErvised
Representation leArning with hierarchical supervisions (OPERA) as a solution, as demonstrated in Figure 2.
We unify full and self supervisions in a similarity learn-ing framework where they differ only by the definition of positive and negative pairs. Instead of directly impos-ing supervisions on the representations, we extract a hier-archy of proxy representations to receive the correspond-ing supervision signals. Extensive experiments are con-ducted with both convolutional neural networks [25] and vision transformers [17] as the backbone model. We pre-train the models using OPERA on ImageNet-1K [46] and then transfer them to various downstream tasks to evalu-ate the transferability. We report image classification ac-curacy with both linear probe and end-to-end finetuning on
ImageNet-1K. We also conduct experiments when transfer-ring the pretrained model to other classification tasks, se-mantic segmentation, and object detection. Experimental results demonstrate consistent improvements over FSL and
SSL on all the downstream tasks, as shown in Figure 1.
Additionally, we show that OPERA outperforms the coun-terpart methods even with fewer pretraining epochs (e.g., fewer than 150 epochs), demonstrating good data efficiency. 2.