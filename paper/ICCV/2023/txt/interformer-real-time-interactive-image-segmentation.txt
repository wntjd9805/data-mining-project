Abstract
Interactive image segmentation enables annotators to efficiently perform pixel-level annotation for segmenta-tion tasks. However, the existing interactive segmentation pipeline suffers from inefficient computations of interactive models because of the following two issues. First, anno-tators’ later click is based on models’ feedback of anno-tators’ former click. This serial interaction is unable to utilize model’s parallelism capabilities. Second, in each interaction step, the model handles the invariant image along with the sparse variable clicks, resulting in a pro-cess that’s highly repetitive and redundant. For efficient computations, we propose a method named InterFormer that follows a new pipeline to address these issues.
In-terFormer extracts and preprocesses the computationally time-consuming part i.e. image processing from the existing process. Specifically, InterFormer employs a large vision transformer (ViT) on high-performance devices to prepro-cess images in parallel, and then uses a lightweight mod-ule called interactive multi-head self attention (I-MSA) for interactive segmentation. Furthermore, the I-MSA mod-ule’s deployment on low-power devices extends the prac-tical application of interactive segmentation. The I-MSA module utilizes the preprocessed features to efficiently re-sponse to the annotator inputs in real-time. The experiments on several datasets demonstrate the effectiveness of Inter-Former, which outperforms previous interactive segmenta-tion models in terms of computational efficiency and seg-mentation quality, achieve real-time high-quality interac-tive segmentation on CPU-only devices. The code is avail-able at https://github.com/YouHuang67/InterFormer. 1.

Introduction
As fueled by massive amounts of data [46], deep net-works achieve compelling performance in various computer
*Corresponding author
Figure 1. Illustration of both the existing and proposed interactive pipelines. The existing pipeline (top) provides both the image and annotator clicks to the interactive model during each interaction, while the proposed pipeline (bottom) employs a large encoder to preprocess the image and then provides the fixed encoded features and on-the-fly clicks to a light decoder during the interaction. vision tasks [6, 8, 36]. The availability of accurately anno-tated data is essential for deep networks’ success. However, the process of manual annotation is time-consuming and resource-intensive. Therefore, the developed interactive im-age segmentation has become an indispensable tool in anno-tating large-scale image datasets [5]. Such techniques aim to achieve high-quality pixel-level annotations with limited annotator interactions, including scribbles [4], bounding boxes [43, 50], polygons [1, 35], clicks [2, 41, 10, 33, 38], and some combinations [57]. Among them, the click-based methods have become the most prevalent due to its simplic-ity, and we focus on these methods in this work.
Recent works on click-based interactive segmentation concentrate on various refinement modules [10, 33] incor-porating sophisticated engineering optimization. However, such refinement tricks still need the well-segmented re-sults from early interactions in the interactive process. Pro-ducing such results keeps facing challenges in deploying computationally-intensive models on low-power devices.
For example, it is challenging to utilize interactive segmen-tation models through crowdsourcing platforms [3]. Previ-ous efforts like FocalClick [10] mitigate this issue by us-ing lightweight models and down-sampling the inputs, but this strategy sacrifices segmentation quality as a trade-off.
Hence, there is still a need for computational-friendly inter-active segmentation methods on a wide range of devices.
The inefficient interactive segmentation stems from two underlying reasons. First, each annotator’s click corre-sponds to one model inference and the next click’s location depends on the former inference results. This serial interac-tion only processes one sample during each inference, un-able to leverage parallelism capabilities of GPU. Second, throughout the annotation process on a single image, the model inputs remain notably similar, with the sparse clicks being the only variables. This leads to the extraction of near-identical features during each inference, resulting in considerable computational redundancy. Such two issues result in low computational efficiency.
In this paper, we propose a method named InterFormer to improve computational efficiency. Preceding the inter-active process, InterFormer employs a large model, e.g. vi-sion transformer (ViT) [12] on high-performance devices to extract high-quality features from images to be annotated.
This process is offline without the need for real-time perfor-mance. Then, InterFormer only needs a lightweight mod-ule to perform interactive segmentation on low-power de-vices, with such preprocessed features from a large model and clicks from annotators as inputs.
Following previous efforts [27] in encoding annotator clicks, we attempted to implement the lightweight module by FPN-like ConvNets [31] to fuse clicks with preprocessed features. However, this module fails to efficiently utilize the preprocessed features and produces unsatisfactory segmen-tation results (reported in Section 4.3).
Instead, we pro-pose interactive multi-head self attention (I-MSA), an effi-cient interactive segmentation module, inspired by the re-cent success of ViT’s variants [39, 49, 51]. I-MSA has ex-tremely low computational complexity and is optimized for high utilization of the preprocessed features from the large models. Furthermore, we adopt the Zoom-In strategy [44] and slightly modify the deeper blocks of I-MSA to focus on the valid regions around the potential objects of images.
Such modified blocks lead to significantly faster inference of I-MSA with slight drop in performance.
As illustrated in Figure 2, the proposed InterFormer out-performs the previous interactive segmentation models and achieves state-of-the-art performance at the similar compu-tational cost. The measure of computation speeds takes into account the process of ViT extracting features (on the same device) for fair comparison. Moreover, Inter-Former achieves real-time high-quality interactive segmen-tation on cpu-only devices, based on the offline prepro-cessed features. We extensively conduct experiments on
GrabCut [43], Berkeley [26], SBD [20], and DAVIS [42]
Figure 2. Experimental results for interactive segmentation on
SBD dataset [20]. The average NoC90 indicates the average num-ber of clicks required to achieve IoU of 0.9, and the PIE (pixel-wise inference efficiency) measures a model’s efficiency in pixel-wise segmentation across potentially large or small objects. Our proposed InterFormer (∗ indicates the measure of PIE without con-sidering the preprocessing) showcases faster inference with better segmentation results, than the recently proposed methods. datasets to substantiate the effectiveness of InterFormer.
We summarize our contributions as follows:
• We introduce a method called InterFormer, which fol-lows a new pipeline splitting the interactive process into two stages to take advantage of well-developed large-scale models and accelerate the interaction.
• We propose an interactive attention module named I-MSA utilizing the prepropressed features to achieve high-quality real-time interactive segmentation on cpu-only devices.
• InterFormer significantly outperforms the previous methods in terms of computational efficiency and seg-mentation quality. 2.