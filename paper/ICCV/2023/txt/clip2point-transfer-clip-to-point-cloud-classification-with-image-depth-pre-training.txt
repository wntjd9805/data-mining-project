Abstract
Pre-training across 3D vision and language remains un-der development because of limited training data. Re-cent works attempt to transfer vision-language (V-L) pre-training methods to 3D vision. However, the domain gap between 3D and images is unsolved, so that V-L pre-trained models are restricted in 3D downstream tasks. To ad-dress this issue, we propose CLIP2Point, an image-depth pre-training method by contrastive learning to transfer
CLIP to the 3D domain, and adapt it to point cloud clas-sification. We introduce a new depth rendering setting that forms a better visual effect, and then render 52,460 pairs of images and depth maps from ShapeNet for pre-training. The pre-training scheme of CLIP2Point com-bines cross-modality learning to enforce the depth fea-tures for capturing expressive visual and textual features and intra-modality learning to enhance the invariance of depth aggregation. Additionally, we propose a novel Gated
Dual-Path Adapter (GDPA), i.e., a dual-path structure with global-view aggregators and gated fusion for down-It allows the ensemble stream representative learning. of CLIP and CLIP2Point, tuning pre-training knowledge to downstream tasks in an efficient adaptation. Experi-mental results show that CLIP2Point is effective in trans-ferring CLIP knowledge to 3D vision. CLIP2Point out-performs other 3D transfer learning and pre-training net-works, achieving state-of-the-art results on zero-shot, few-shot, and fully-supervised classification. Codes are avail-able at: https://github.com/tyhuang0428/CLIP2Point. 1.

Introduction
Vision-language (V-L) pre-training has achieved great success in computer vision. Benefiting from large-scale
†Corresponding Author: Wangmeng Zuo (wmzuo@hit.edu.cn)
Figure 1. Overall architecture of CLIP transfer learning on the 3D domain. Point clouds are first projected to multi-view depth maps, and then aggregated by the CLIP visual encoder. Comparison with textual prompts presents the classification prediction. However, we argue that the domain gap exists between depth maps and CLIP pre-training images. To this end, a pre-trained depth encoder via
CLIP2Point is proposed. data, V-L pre-trained models [34, 47] transfer language knowledge to visual understanding, which can be fine-tuned to multiple downstream tasks. However, pre-training across 3D vision and language remains an open question, due to the lack of sufficient training data. For example, Contrastive
Language-Image Pre-training (CLIP) [34] takes more than 400M image-text pairs as training data.
In contrast, few studies have been given to pre-training across 3D vision and language. Moreover, even the conventional 3D pre-training method PointContrast [45] is trained on ScanNet [11] with only 100k pairs of point clouds from 1,513 scenes. Due to the limitation of 3D pre-training, most existing 3D deep networks [33, 42] are trained from scratch on specific down-stream datasets.
One remedy is to leverage the existing successful V-L pre-trained model for 3D vision tasks. To this end, one may first convert the 3D point clouds to multi-view 2D depth maps [37, 15, 16, 43]. By simply treating 2D depth maps as images, PointCLIP [53] applies CLIP to 3D tasks, providing zero-shot and few-shot settings in the point cloud classifi-cation with textual prompting. However, its results are still limited since the rendered depth maps are much different from the image domain of the CLIP training dataset. And
the sparsity and disorder of point cloud data result in var-ious depth distributions from multiple views, further con-fusing the aggregation of CLIP. Existing pre-training works focus on the domain gap [1] or multi-view consistency [45] of point clouds, while we intend to tackle similar issues based on depth maps. In addition, a solution of adapting pre-training knowledge to downstream tasks should be in-cluded in the V-L transfer.
In order to transfer CLIP to the 3D domain, we pro-pose CLIP2Point, a pre-training scheme with two learning mechanisms: 1) cross-modality learning for the contrastive alignment of RGB image and depth map, 2) intra-modality learning in the depth modality to enhance the invariance of depth aggregation. In particular, the image encoder Ei is di-rectly from CLIP weights and is frozen during pre-training.
While the depth encoder Ed is trained to 1) align depth fea-tures with CLIP image features in cross-modality learning and 2) encourage the depth aggregation to be invariant to view changes in intra-modality learning. With pre-training, the depth features can then be well aligned with the visual
CLIP features. As for the training data, we do not adopt the depth maps in the existing RGB-D datasets as they are densely sampled and are contradicted to the sparsity of ren-dered depth maps. Instead, we reconstruct multi-view im-ages and depth maps from 3D models directly. Specifically, we render 10 views of RGB images from ShapeNet [4], which covers 52,460 3D models for 55 object categories.
Meanwhile, we generate corresponding depth maps, with a new rendering setting that forms a better visual effect for
CLIP encoding. Experiments show that our CLIP2Point can significantly improve the performance of zero-shot point cloud classification.
To further adapt our CLIP2Point to downstream tasks, we propose a novel Gated Dual-Path Adapter (GDPA).
Since our pre-training is to align the instance-level depth map, it can be complementary with CLIP pre-training knowledge that focuses on category-level discrimination.
We propose a dual-path structure, where both our pre-trained depth encoder Ed and the CLIP visual encoder Ei are utilized. A learnable global-view aggregator is attached to each encoder to extract an overall feature from multiple views. And the final logits can be calculated by the gated fusion of two encoders.
To sum up, our contributions can be summarized as:
• We propose a contrastive learning method dubbed
CLIP2Point, with a newly proposed pre-training dataset that is pre-processed from ShapeNet, transfer-ring CLIP knowledge to the 3D domain. Experiments show that CLIP2Point significantly improves the per-formance of zero-shot classification.
• We propose a novel Gated Dual-Path Adapter (GDPA), a dual-path structure with global-view aggregators and gated fusion to efficiently extend CLIP2Point to down-stream representation learning.
• Extensive experiments are conducted on Model-Net10, ModelNet40, and ScanobjectNN. In compari-son to 3D transfer learning and pre-training networks,
CLIP2Point achieves state-of-the-art results on zero-shot, few-shot, and fully-supervised point cloud clas-sification tasks. 2.