Abstract
CPU
GPU
Event report cameras brightness asynchronously changes with a temporal resolution in the order of micro-seconds, which makes them inherently suitable to address problems that involve rapid motion perception.
In this paper, we address the problem of time-to-contact (TTC) estimation using a single event camera. This problem is typically addressed by estimating a single global TTC measure, which explicitly assumes that the surface/obstacle is planar and fronto-parallel. We relax this assumption by proposing an incremental event-based method to estimate the TTC that jointly estimates the (up-to scale) inverse depth and global motion using a single event camera. The proposed method is reliable and fast while asynchronously maintaining a TTC map (TTCM), which provides per-the proposed pixel TTC estimates. As a side product, method can also estimate per-event optical flow. We achieve state-of-the-art performances on TTC estimation in terms of accuracy and runtime per event while achieving competitive performance on optical flow estimation. 1.

Introduction
Event cameras differ from standard frame-based cam-eras, which capture visual data at a fixed rate and inde-pendently of the observing scene. Instead, event cameras respond asynchronously to pixel-wise brightness changes by generating events [6, 25]. Event cameras are thus data-driven sensors that offer several advantages, including high temporal resolution in the order of microseconds, low la-tency, low power consumption, and high dynamic range.
These properties place event cameras as suitable candidates to address vision-based problems that involve (high-speed) motion, e.g., optical flow estimation [4, 27, 51], ego-motion estimation [15, 21, 22, 34], motion segmentation [33, 44], and obstacle avoidance [10, 12]. Due to the distinct visual sensing paradigm, however, new methods are necessary to fully exploit the potential of event cameras [13, 23]. 60 40 20
)
% (
E
E
R
A
E-RAFT [17]
PF [36]
Proposed 0 100
CT [43]
SOFAS [45]
ECMD [28] 101 102 103
Runtime per event (microsec)
Figure 1. Runtime vs. accuracy comparison for TTC estimation methods. Average results on the Ventral Landing benchmark [28].
Event cameras have been used to address the problem of fast TTC estimation, which comes up often in the vision-based obstacle avoidance [10, 12, 29, 39] and ventral land-ing [28, 36] literature. The TTC is the time that would elapse before a camera reaches an obstacle/surface, assum-ing the current relative motion between them remains con-stant [10]. Previous methods that use a single event camera have focused on estimating a single global TTC measure, which assumes that the surface is planar and fronto-parallel.
To overcome this limitation, other methods use additional sensing, e.g., depth frames, to build a dense TTCM [20, 47].
We instead propose to extend the Dispersion Minimiza-tion (DMin) framework [34] to estimate the TTC for each incoming event using a single event camera. The pro-posed method jointly estimates the relative global motion and per-event (up-to scale) inverse depth. We can then asyn-chronously maintain a semi-dense TTCM which provides per-pixel TTC estimates or compute a global TTC mea-sure with greater accuracy by averaging over the TTC es-timates. Since there is at least one scaling degree of free-dom (DOF), we also propose an effective strategy to miti-gate event collapse [40, 41]. The proposed method is also computationally fast, reaching ∼1 microsecond processing time per event on a standard laptop. Fig. 1 compares the
runtime vs. accuracy for TTC estimation methods, whereby the proposed method achieves state-of-the-art performance.
We also estimate the per-event optical flow as a side product and achieve competitive performance compared to state-of-the-art optical flow methods that use events.
Main contributions: 1. First event-based method that explicitly estimates the
TTC for each event and maintains a semi-dense TTCM using a single event camera. 2. DMin framework [34] extension to jointly handle lo-cal and global estimates, i.e., inverse depth and global motion, respectively. 3. Effective approach that mitigates event collapse [40, 41] for incremental event-based estimation. 1.1. Time-to-Contact
Consider a freely moving camera with angular and lin-ear velocities ω(t) = (ωx(t), ωy(t), ωz(t))T and ν(t) = (νx(t), νy(t), νz(t))T, respectively, that is observing a point
α, with 3D coordinates α(t) = (X(t), Y (t), Z(t))T rela-tive to the camera, as shown in Fig. 2. Z(t) is also referred as the depth of point α relative to the camera. The instanta-neous TTC between the moving camera and point α is thus given by:
τ (t) := −
Z(t) dZ(t) dt
=
Z(t)
νz(t)
. (1)
The minus sign disappears because we define the linear ve-locity w.r.t. the camera’s frame of reference, not w.r.t. the point’s frame of reference, i.e., νz(t) = −dZ(t)/dt. Based on Eq. (1), the exact values of depth and relative approach-ing motion do not need to be estimated, only the ratio be-tween them. 2.