Abstract
Recent audio2mesh-based methods have shown promis-ing prospects for speech-driven 3D facial animation tasks.
However, some intractable challenges are urgent to be set-tled. For example, the data-scarcity problem is intrinsically inevitable due to the difficulty of 4D data collection. Be-sides, current methods generally lack controllability on the animated face. To this end, we propose a novel frame-work named Speech4Mesh to consecutively generate 4D talking head data and train the audio2mesh network with the reconstructed meshes. In our framework, we first recon-struct the 4D talking head sequence based on the monocu-lar videos. For precise capture of the talking-related varia-tion on the face, we exploit the audio-visual alignment infor-mation from the video by employing a contrastive learning scheme. We next can train the audio2mesh network (e.g.,
FaceFormer) based on the generated 4D data. To get con-trol of the animated talking face, we encode the speaking-unrelated factors (e.g., emotion, etc.) into an emotion em-bedding for manipulation. Finally, a differentiable renderer guarantees more accurate photometric details of the re-construction and animation results. Empirical experiments demonstrate that the Speech4Mesh framework can not only outperform state-of-the-art reconstruction methods, espe-cially on the lower-face part but also achieve better ani-mation performance both perceptually and objectively after pre-trained on the synthesized data. Besides, we also verify that the proposed framework is able to explicitly control the emotion of the animated talking face. 1.

Introduction
Speech-driven facial animation is aimed at guiding the face model (either 2D image or 3D mesh) to have perceptu-ally rational motion (especially for lip sync) only according to the input audio signal. It is drawing increasingly more
Figure 1: Overview of the proposed Speech4Mesh frame-work. In this framework, the speech signal is leveraged for both the reconstruction module and the audio2mesh mod-ule. An emotion code is embedded into the audio2mesh module for emotional manipulation. Differentiable render-ing is employed for recovering the details of the face. attention with the development of virtual reality, game and film production, etc. There are currently two mainstreams to deal with this task, either from 2D or 3D perspectives.
By learning from video data [9, 41, 55, 71], 2D solutions try to generate the talking head images frame by frame.
Although sufficient data is available to be used, the syn-thesized images cannot directly be applied to 3D scenes.
Thus, in this paper, we mainly focus on 3D facial animation driven by speech. In recent years, a handful of 3D speech-driven facial animation methods [20, 47, 39, 13, 7, 34] have shown promising performance by directly learning a map-ping from audio to 3D avatar mesh deformation with deep neural networks (hereinafter called audio2mesh methods).
In this way, some subtle variations on the face mesh can be captured. Generally speaking, for the audio2mesh meth-ods, data quality is one of the most important factors to be considered [20] since they generally demand high-precision
4D talking head data to learn the correspondence from au-dio embedding to mesh movement in a supervised manner.
However, in practice, real 4D data is extremely hard to col-lect. It is generally reconstructed and registered from syn-chronized multi-view images or 3D scan frame by frame, which is extraordinarily costly [13, 21]. Therefore, how to deal with the data scarcity problem becomes an urgent agenda to be discussed.
In contrast, the accessibility of monocular videos is much higher. The scarcity of 4D data may be mitigated through the use of 3D reconstruction techniques applied to 2D videos. The majority of current monocular 3D face re-construction methods (e.g., 3DDFA-V2 [27], DECA [22]) are based on the self-supervised training scheme, whose es-sential guidances are landmark loss and photometric loss.
Nonetheless, these losses can only provide 2D information from the image plane, which is relatively weak to capture the complex nonrigid deformation of the lower face re-gion [51, 62]. Therefore, the reconstructed results always demonstrate obvious artifacts and ambiguities between ut-terance and visual perception, such as mouth funnel, pucker, and rounded vowels, which may considerably limit the pre-cision of the audio2mesh methods [20].
To address the problems mentioned above, we pro-pose a novel framework named Speech4Mesh to consec-utively synthesize 4D talking head data and train the au-dio2mesh network with the reconstructed mesh sequences.
An overview of the Speech4Mesh framework is illustrated in Fig. 1. Firstly, to obtain better synthesis results, we pro-pose a novel speech-assisted monocular 3D face reconstruc-tion module to exploit multi-modal correlation from the video data rather than merely from the RGB image, since the mouth geometry and expression are naturally correlated to the speech. Specifically, this module is built based on a backbone reconstruction model DECA [22], one of the state-of-the-art monocular 3D face reconstruction methods.
By employing a speech-visual contrastive loss, it can learn the correspondence between audio and visemes to make the reconstructed 3D face to be more reliable and perceptually reasonable.
Secondly, once sufficient 4D data has been acquired, we can train an audio2mesh network to model the de-formation of a face mesh based on input speech. For our work, we employed FaceFormer [20] as the back-bone for the audio2mesh network. Compared with the vanilla training scheme of FaceFormer, the primary benefit of Speech4Mesh is data sufficiency. In fact, Speech4Mesh provides a low-cost self-supervised approach to generate
“infinite” 4D training data from infinite RGB videos. Be-sides, owing to the diversity of the talking head videos, the reconstructed data empowers our model to further disentan-gle other auxiliary properties of the face (e.g., emotion, etc.) for manipulation. To be specific, we first pre-train Face-Former on the reconstructed emotional 4D dataset with the corresponding emotion codes. At the fine-tuning stage, dif-ferent emotion codes can be embedded into the FaceFormer backbone to realize the emotion control of the animated talking head.
The main contributions of our work can be summarized as follows:
• We propose Speech4Mesh, a novel framework that tackles the data scarcity issue in speech-driven 3D facial animation by consecutively reconstructing pseudo-4D data and training an audio2mesh network.
This framework also provides a referable scheme to al-leviate the data scarcity problem of similar 4D tasks.
• This framework enables emotional control in the ab-sence of high-quality expressive 4D data by pre-training with the emotional pseudo-4D talking head data reconstructed from 2D videos. And the syn-thesized emotional 4D talking head dataset can be found in https://github.com/haonanhe/
MEAD-3D/tree/main.
• To the best of our knowledge, we are the first to uti-lize speech information as complement for monocular 3D face reconstruction. Empirical experiments show that the exploitation of multi-modal information yields more natural and precise results. 2.