Abstract
The development of face editing has been boosted since the birth of StyleGAN. While previous works have explored different interactive methods, such as sketching and exem-plar photos, they have been limited in terms of expressive-ness and generality.
In this paper, we propose a new in-teraction method by guiding the editing with abstract cli-part, composed of a set of simple semantic parts, allow-ing users to control across face photos with simple clicks.
However, this is a challenging task given the large domain gap between colorful face photos and abstract clipart with limited data. To solve this problem, we introduce a frame-work called ClipFaceShop 1 built on top of StyleGAN. The key idea is to take advantage of W+ latent code encoded rich and disentangled visual features, and create a new lightweight selective feature adaptor to predict a modiﬁ-able path toward the target output photo. Since no pairwise
*Nanxuan Zhao is with Adobe Research.
E-mail: nanxu-anzhao@gmail.com.
*Shengqi Dang, Hexun Lin, Yang Shi and Nan Cao are with Intelligent
Big DataVisualization Lab, Tongji University. Nan Cao is the correspond-ing author. E-mail: dangsq123@tongji.edu.cn, linhexun@pku.edu.cn,
{shiyang1230, nan.cao}@gmail.com. 1Code: https://github.com/dangsq/ClipFaceShop labeled data exists for training, we design a set of losses to provide supervision signals for learning the modiﬁable path. Experimental results show that ClipFaceShop gen-erates realistic and faithful face photos, sharing the same facial attributes as the reference clipart. We demonstrate that ClipFaceShop supports clipart in diverse styles, even in form of a free-hand sketch. 1.

Introduction
Face editing aims to manipulate the attributes speciﬁed by the users while maintaining the non-modiﬁable attributes unchanged. With the rapid development of Generative Ad-versarial Network (GAN) [12], this task attracts many re-cent works [45, 32, 9] for achieving impressive results.
In particular, because of the powerful disentangled latent space, StyleGAN [18] has become a de facto building block for generating realistic editing results. To express the user’s intention, there are mainly three ways of editing includ-ing sketching, text-guiding, and exemplar photo. Unfortu-nately, these interaction methods may not enable effective and precise editing across different face photos for people without design experience.
More speciﬁcally, sketch-based face editing [30, 4, 25] allows users to directly draw strokes on top of the face, such as changing the size of eyes. Although simple, the quality of results often relies on the users’ sketching skills and the in-put sketch cannot directly adapt to different face photos. By taking advantage of well-aligned textual-visual embedding space of CLIP [31], text-based face photo editing [28, 21] has appeared to control the facial attributes with simple tex-tual instructions, such as making a face photo into “Emma
Stone” style. Such a condition can ﬂexibly change the fa-cial attributes without ﬁnetuning on a speciﬁc dataset, but is unable to conduct ﬁne-grained control given the ambigu-ity and high-level nature of the text description. Another stream of works [24, 13, 44] transfers the facial attributes from a reference photo to the target photo, allowing both
ﬁne-grained and cross-photo control. However, ﬁnding a perfect examplar photo is not an easy task and such trans-fer is a one-off operation that is hard to iteratively reﬁne the results.
In this work, we propose a new interaction method for face editing using a mix-and-match clipart. Given a source photo and a reference clipart, we aim to transfer the at-tributes (e.g., facial expression, beard, hairstyle, etc.) from the clipart to the face photo. Thanks to the growing cli-part community, there are many well-curated libraries, such as OpenPeeps [37] and Avataaars [36]. Users can easily create the reference clipart by combining the components through simple clicks, as shown in Fig. 2. However, clipart-driven face editing poses many challenges. First, different from directly editing on the photo, there exists a large do-main gap between the clipart and the natural photo to be inﬂuenced, not only on the identity. Since clipart is much more abstract than the natural photo and with fewer visual features, how to correctly match the facial attributes across domains and identities is a critical problem to be solved.
Second, unlike face photos which have plenty of datasets with annotations (e.g., segmentation), clipart is often lim-ited by the data scale. Collecting pairwise datasets (i.e., ref-erence clipart and source photo) for training is impractical.
A model that could learn such attribute transfer only from a single clipart is desired. Furthermore, facial attributes can be diverse, including intrinsic attributes (e.g., beard) and ac-cessories (e.g., glasses). The model needs to transfer these attributes accurately without modifying the face identity.
To this end, we introduce a novel pipeline, namely Clip-FaceShop, to tackle the above challenges and make the fol-lowing technical contributions: 1) Rather than ﬁnetuning or changing StyleGAN’s architecture, we take the strength of its latent code by learning a light-weight selective fea-ture adaptor to conduct feature transformation. This can also effectively avoid overﬁtting with few training data; 2)
To facilitate training without pairwise ground truth data, we design a set of loss terms. Besides the general iden-Figure 2: Examples of mix-and-match clipart, ©Open
Peeps. tity and background preservation constraints, we further de-sign transfer, similarity, and consistency losses by utilizing the semantic info encoded in the CLIP embedding. These terms help our adaptor to ﬁnd a modiﬁable path to bridge the domain gap while maintaining the identity of the input source photo; 3) As not all features are equally contributed to different facial attributes, we conduct an experiment to analyze the layer-wise and channel-wise importance among features for selective feature modiﬁcation to edit the face photo. We also take a two-stage training for inference to allow customization on the ﬁnal results.
We have conducted extensive experiments and a user study to demonstrate the effectiveness of our model and shown its superiority over state-of-the-art methods of clipart-based face photo editing. Note that our model not only works well on very abstract clipart containing only black and white strokes, but it also works for more cartoon-like clipart with diverse styles. We also show the generality of our model on video in the supplementary material. 2.