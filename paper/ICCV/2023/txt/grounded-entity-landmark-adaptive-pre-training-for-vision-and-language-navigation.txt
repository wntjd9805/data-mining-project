Abstract
Cross-modal alignment is one key challenge for Vision-and-Language Navigation (VLN). Most existing studies concentrate on mapping the global instruction or single sub-instruction to the corresponding trajectory. However, another critical problem of achieving fine-grained align-ment at the entity level is seldom considered. To address this problem, we propose a novel Grounded Entity-Landmark
Adaptive (GELA) pre-training paradigm for VLN tasks. To achieve the adaptive pre-training paradigm, we first intro-duce grounded entity-landmark human annotations into the
Room-to-Room (R2R) dataset, named GEL-R2R. Addition-ally, we adopt three grounded entity-landmark adaptive pre-training objectives: 1) entity phrase prediction, 2) land-mark bounding box prediction, and 3) entity-landmark se-mantic alignment, which explicitly supervise the learning of fine-grained cross-modal alignment between entity phrases and environment landmarks. Finally, we validate our model on two downstream benchmarks: VLN with descriptive in-structions (R2R) and dialogue instructions (CVDN). The comprehensive experiments show that our GELA model achieves state-of-the-art results on both tasks, demonstrat-ing its effectiveness and generalizability. 1.

Introduction
Vision-and-Language Navigation (VLN) [3] is an impor-tant task in the Embodied Vision community, which has gained great attention [66, 40, 50]. It aims to ask an agent to reach the target location inside photo-realistic environments by following natural language instructions. In VLN, cross-modal alignment is one critical step to accurately make the action decision for the agent [60], since matching the men-tioned landmarks (objects or scenes) with visual observa-tions can help the comprehensive understanding of the en-∗Corresponding author.
Figure 1. An illustration of an embodied agent navigating in a 3D photo-realistic environment. The agent is expected to navi-gate based on environment landmarks that correspond to the entity phrase in the instruction. vironments and instructions [60, 15, 68]. However, most of the available datasets could only provide the coarse-grained text-image alignment signals [15], i.e., instruction-level cor-respondences to the complete trajectories, where finer ones are required to learn the cross-modality alignment for well-performed navigation [18, 67, 47, 43, 57].
A large part of previous works [66, 11, 40, 50] has con-centrated on the global alignment of instructions and trajec-tories, which matches instructions to the overall temporal visual trajectory via reinforcement learning [60], auxiliary reasoning [66], or transformer-based matching pre-training
[7, 50, 40]. Others [18, 67, 15] have attempted to align sub-instructions and sub-trajectories locally. At this gran-ularity, the agents are designed to segment long instruction
[67, 18] and determine which sub-instruction to focus on.
To guide local cross-modal alignment, He et al.
[15] in-troduced the Landmark-RxR dataset by human-annotating sub-instructions and sub-trajectories alignment. They ad-vanced the granularity of visual-textual alignment, but the agent learning is still supervised at the sentence level. The alignment within (sub-)instructions should be considered. suggested GELA achieves state-of-the-art (SoTA) perfor-mance in both seen and unseen environments of the above two benchmarks: 62% SPL on R2R and 5.87 GP on CVDN.
Identifying environment landmarks that correspond to entities in the instruction, as shown in Figure 1, is the next step to refine alignment and improve navigation. Recent studies [47, 43, 57] investigated the same problem, suggest-ing the alignment of entities and object regions [47] or the scene- and object-aware transformer model [43]. But the object regions were not under the direct supervision of the corresponding entity phrases in the instruction. To gener-ate navigation instructions enriched with landmark phrases,
Wang et al. [57] introduced grounded landmark annotations using a dependency parser and weak supervision from the pose traces, leading to the entity-landmark alignments of in-sufficient precision. Therefore, a dataset with high-quality entity-landmark level grounding annotations and powerful supervision for entity-landmark level cross-modal align-ment is highly desired for VLN.
To address the above limitations, we first enhance the
Room-to-Room (R2R) dataset [3] by introducing addi-tional high-quality grounded entity-landmark human an-notations, known as the Grounded Entity-Landmark R2R dataset (GEL-R2R). The GEL-R2R dataset is annotated with abundant precisely matched entity-landmark pairs (as magenta text and bounding box illustrated in Figure 1), which could provide the VLN models with direct supervi-sion of fine-grained cross-modal alignment.
To verify the value of our GEL-R2R dataset, we pro-pose a novel Grounded Entity-Landmark Adaptive (GELA) pre-training paradigm to improve the learning of entity-landmark level alignment for the VLN pre-trained model.
Specifically, we suggest three grounded entity-landmark adaptive pre-training objectives: 1) Entity Phrase Predic-tion (EPP), locating entity phrases that refer to environ-ment landmarks from the instruction; 2) Landmark Bound-ing box Prediction (LBP), predicting the bounding box of environment landmarks that match with entity phrases; and 3) Entity-Landmark Semantic Alignment (ELSA), aligning the matched pairs of landmark patches and entity tokens in the feature space by contrastive loss. These three tasks ex-plicitly equip the model with the ability to comprehend the entity-level grounding between human instructions and vi-sual environment observations.
Finally, we conduct extensive experiments on two down-stream tasks to evaluate our proposed dataset GEL-R2R and adaptive pre-training methods GELA: Room-to-Room (R2R) and Vision-and-Dialog Navigation (CVDN). The in-structions in R2R are fine-grained descriptions for the nav-igation trajectory, whereas the instructions in CVDN are multi-turn dialogs between the agent and the oracle during navigation. We use HAMT [7] as the backbone VLN model for the GELA pre-training. The results demonstrate our
To summarize, our contributions are three-fold:
• We construct a new dataset GEL-R2R, which is the first dataset with high-quality grounded entity-landmark human annotations in the VLN domain.
• We propose a novel Grounded Entity-Landmark Adap-tive (GELA) pre-training paradigm for VLN, explic-itly supervising the models to learn fine-grained cross-modal semantic alignment between entity phrases and environment landmarks.
• Our suggested GELA achieves state-of-the-art results on two challenging VLN downstream benchmarks, demonstrating its effectiveness and generalizability. 2.