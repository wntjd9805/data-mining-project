Abstract
We introduce VideoFlow, a novel optical flow estimation framework for videos. In contrast to previous methods that learn to estimate optical flow from two frames, VideoFlow concurrently estimates bi-directional optical flows for mul-tiple frames that are available in videos by sufficiently ex-ploiting temporal cues.
We first propose a TRi-frame Optical Flow (TROF) mod-ule that estimates bi-directional optical flows for the cen-ter frame in a three-frame manner. The information of the frame triplet is iteratively fused onto the center frame. To extend TROF for handling more frames, we further propose a MOtion Propagation (MOP) module that bridges multiple
TROFs and propagates motion features between adjacent
TROFs. With the iterative flow estimation refinement, the information fused in individual TROFs can be propagated into the whole sequence via MOP. By effectively exploit-ing video information, VideoFlow presents extraordinary performance, ranking 1st on all public benchmarks. On the Sintel benchmark, VideoFlow achieves 1.649 and 0.991 average end-point-error (AEPE) on the final and clean passes, a 15.1% and 7.6% error reduction from the best published results (1.943 and 1.073 from FlowFormer++).
On the KITTI-2015 benchmark, VideoFlow achieves an F1-all error of 3.65%, a 19.2% error reduction from the best published result (4.52% from FlowFormer++). Code is released at https://github.com/XiaoyuShi97/
VideoFlow. 1.

Introduction
Optical flow estimation is a fundamental computer vi-sion task of estimating pixel-wise displacement fields be-*Corresponding author: Zhaoyang Huang and Hongsheng Li
Figure 1. Comparison between two-frame and multi-frame op-tical flow estimation. (Left) Previous methods are limited to optical flow estimation of frame pairs. (Right) We introduce
VideoFlow, a novel framework that concurrently estimates bi-directional optical flows for multiple consecutive frames. tween consecutive video frames.
It is widely adopted to tackle various downstream video problems, including video restoration [32, 66, 12, 34, 5, 50, 72, 36], video object de-tection [78, 61, 79], video synthesis [69, 18, 19, 63, 62] and action recognition [58, 47, 76], providing valuable informa-tion on motion and cross-frame correspondence.
With the evolvement of dedicated datasets [27] and model designs [21, 59], optical flow estimation has been greatly advanced over the years. However, we observe a divergence in its development from downstream demands.
On the one hand, despite multiple frames being avail-able in video streams, most efforts in this field are lim-ited to flow estimation based on frame pairs, thereby ig-noring valuable temporal cues from additional neighboring frames. Notably, a simple strategy for temporal information fusion, e.g., the “warm-start” in RAFT [59], brings non-trivial performance gain. On the other hand, multi-frame bi-directional optical flows are imperative in many down-stream video processing algorithms [5, 6, 46, 66, 39, 73, 23, 37, 35, 42, 41]. However, due to the lack of appropriate multi-frame optical flow models, existing algorithms have
to repeatedly estimate flows in a pair-wise manner. This highlights the need for optical flow models specifically de-signed for multi-frame scenarios.
In this paper, we introduce VideoFlow, as shown in
Fig. 1, a novel framework that concurrently estimates opti-cal flows for multiple consecutive frames. VideoFlow con-sists of two novel modules: 1) a TRi-frame Optical Flow module (TROF) that jointly estimates bi-directional opti-cal flows for three consecutive frames in videos, and 2) a
MOtion Propagation (MOP) module that splices TROFs for multi-frame optical flow estimation.
Specifically, we treat three-frame optical flow estimation as the basic unit for the multi-frame framework. We ar-gue that the center frame of the triplet plays the key role of bridging temporal information, which motivates two critical designs of our proposed TROF model. Firstly, we propose to jointly estimate optical flows from the center frame to its two adjacent previous and next frames, which ensures the two flows originate from the same pixel and belong to the same continuous trajectory in the temporal dimension.
Some previous three-frame methods [59, 49] rely on warp-ing flow estimation from the preceding frame pair to fa-cilitate the estimation of the current frame pair. The key difference arises in the presence of occlusion and out-of-boundary pixels, where the warped preceding predictions and current predictions might belong to entirely different objects. Such misalignment wastes valuable temporal cues and even introduces erroneous motion information. Sec-ondly, TROF comprehensively integrates the bi-directional motion information in a recurrent updating process.
After constructing the strong three-frame model, we fur-ther propose a MOtion Propagation (MOP) module that ex-tends our framework to handle more frames. This module passes bi-directional motion features along the predicted
“flow trajectories”. Specifically, in the recurrent updating decoder, MOP warps the bi-directional motion features of each TROF unit to its adjacent units according to current predicted bi-directional optical flows. The temporal recep-tive field grows as the recurrent process iterates so that our
VideoFlow gradually utilizes wider temporal contexts to op-timize all optical flow predictions jointly. This brings a significant advantage that the ambiguity and inadequate in-formation in two-frame optical flow estimation will be pri-marily reduced when the information from multiple frames is sufficiently employed. For example, estimating optical flows for regions that are occluded or out of view in target images is too challenging in the two-frame setting but can be effectively improved when we take advantage of addi-tional information from more contextual frames.
In summary, our contributions are four-fold: 1) We pro-pose a novel framework, VideoFlow, that learns to estimate optical flows of videos instead of image pairs. 2) We pro-pose TROF, a simple and effective basic model for three-frame optical flow estimation. 3) We propose a dynamic
MOtion Propagation (MOP) module that bridges TROFs for handling multi-frame optical flow estimation. 4) Vide-oFlow outperforms previous methods by large margins on all benchmarks. 2.