Abstract
We propose a novel hairstyle transferred image synthe-sis method considering the underlying head geometry of two input images. In traditional GAN-based methods, trans-ferring hairstyle from one image to the other often makes the synthesized result awkward due to differences in pose, shape, and size of heads. To resolve this, we utilize neural rendering by registering two input heads in the volumetric space to make a transferred hairstyle fit on the head of a target image. Because of the geometric nature of neural ren-dering, our method can render view varying images of syn-thesized results from a single transfer process without caus-ing distortion from which extant hairstyle transfer methods built upon traditional GAN-based generators suffer. We ver-ify that our method surpasses other baselines in view of pre-*Corresponding author: seunggyu.chang@navercorp.com
†This work was done during internship at NAVER Cloud. serving the identity and hairstyle of two input images when synthesizing a hairstyle transferred image rendered at any point of view. 1.

Introduction
Selecting an appropriate hairstyle can be difficult as it is hard to imagine how it will look on your face without trying it out for yourself. The reason can be attributed to the in-herent variability of individual head shape, resulting in the frequent occurrence of disparate outcomes when attempt-ing to emulate celebrity hairstyles. To address this issue, we propose a novel image synthesis method for hairstyle transfer tailored to fit an individual head shape. We named our method HairNeRF as it considers geometric alignment in the volumetric space of neural rendering [33]. Hairstyle transfer synthesizes an image where the hairstyle from one
portrait is transferred to the other and Generative adversar-ial networks (GANs) [14, 20, 22] have been extensively uti-lized to address this problem. Notably, following the emer-gence of StyleGAN [20, 21, 22], which enables the genera-tion of highly realistic images, studies [37, 50, 19, 8] have been made to optimize latent vectors within the well-trained latent space of StyleGAN to generate an optimal synthe-sis outcome. These methods are successful at transferring hairstyles between images having similar poses, but strug-gle to generate convincing results for images having vastly different poses. To resolve this, studies [50, 24, 51] have proposed methods aligning pose of transferred hairstyle to a target portrait, simultaneously reducing the perceptual difference between the transferred result and the original hairstyle. However, they inherently encounter the same lim-itation of the pre-trained latent space of StyleGAN, where pose and style attributes are entangled as StyleGAN is not explicitly trained for hairstyle transfer. Consequently, the alignment process may distort the geometry of an original hairstyle, leading to undesirable alterations such as forehead stretching or shrinking.
We categorize entanglement of pose and style attributes into two folds: entanglement of style with viewpoint, and entanglement of style with head shape. To address the en-tanglement of style with viewpoint, we leverage the latent space of StyleNeRF [15], a neural radiance field (NeRF)
[32, 33] based GAN model as an alternative for StyleGAN.
StyleNeRF exhibits an innate pose-invariant latent space as the network implies the volumetric geometry of an image.
We basically follow the latent optimization technique fol-lowing the literature [50, 24, 51] utilizing StyleGAN. Lever-aging the pose-invariant nature of the StyleNeRF’s latent space, we extract a pose-invariant style vector through GAN inversion [42]. Thereafter, we recombine the volumetric features generated by the inverted style vectors of the hair and the target portraits. However, despite the pose-invariant nature of StyleNeRF’s latent space, it still entails an entan-glement problem between style and head shape. The op-timization of a style vector to fit on the head of a target portrait, while retaining hairstyle of a hair portrait, can po-tentially result in deformations of the hairstyle. To address the entanglement of style with head shape, we propose a registration method that aligns the geometries of two heads within the volumetric space, while maintaining the inverted style vectors intact. Subsequently, we introduce a novel ren-dering method that recombines hairstyle features with fa-cial features within the volumetric space using alignment-inducing deformable rays, which are derived from the reg-istration outcome. In this manner, we resolve the distorted hairstyle problem from which StyleGAN-based method suf-fer, resulting in better alignment of the transferred hairstyle seamlessly fit on the target portrait. Taking the advantage of geometric nature of neural rendering, HairNeRF is ca-pable of generating a diverse set of geometrically consis-tent novel view hairstyle transferred images from a single synthesis, by merely adjusting camera parameters. This is in contrast to existing StyleGAN-based hairstyle transfer methods which necessitate separate optimizations for each of novel views, which often yield inconsistent outcomes.
Experimental results on diverse datasets demonstrate the superiority of HairNeRF against other baselines at gen-erating seamless hairstyle transfer outcomes, while effec-tively preserving the underlying geometry of the original hairstyle. 2.