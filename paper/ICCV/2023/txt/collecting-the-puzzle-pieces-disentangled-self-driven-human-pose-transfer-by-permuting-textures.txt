Abstract
Human pose transfer synthesizes new view(s) of a person for a given pose. Recent work achieves this via self-reconstruction, which disentangles a person’s pose and texture information by breaking the person down into parts, then recombines them for reconstruc-tion. However, part-level disentanglement preserves some pose information that can create unwanted ar-tifacts.
In this paper, we propose Pose Transfer by
Permuting Textures (PT2), an approach for self-driven human pose transfer that disentangles pose from tex-ture at the patch-level. Specifically, we remove pose from an input image by permuting image patches so only texture information remains. Then we reconstruct the input image by sampling from the permuted tex-tures for patch-level disentanglement. To reduce noise and recover clothing shape information from the per-muted patches, we employ encoders with multiple kernel sizes in a triple branch network. Extensive experiments on DeepFashion and Market-1501 PT2 reports signifi-cant gains on automatic metrics over other self-driven methods, and even outperforms some fully-supervised methods. A user study also reports images generated by our method are preferred in 68% of cases over self-driven approaches from prior work. Code is available at https: // github. com/ NannanLi999/ pt_ square 1.

Introduction
The goal of human pose transfer is to change the pose of a person while preserving the person’s appear-It has wide applications ance and clothing textures. such as virtual try-on [42, 3, 43], controllable person image manipulation [3, 17] and person re-identification
[46]. Recent work focused on using paired image data (i.e., two images of the same person before and af-ter reposing) [49, 45], but collecting such data can be very labor intensive. Alternatively, self-driven meth-(a) Prior work. (b) Our approach.
Figure 1: Self-driven pose transfer methods extract texture and pose representations and then learn to re-construct the original image. (a) Most recent work breaks down the person into several parts to disen-tangle textures [23, 24, 39]. However, pose informa-tion may still appear in the part-wise texture features.
Without supervision, disentangling them is difficult
[19]. (b) Our approach disentangles textures from pose by permuting the image patches, effectively eliminating pose information, which enables more effective separa-tion of pose and texture features than prior work. ods train pose transfer models without paired images
[24, 35]. However, they face two major challenges: ef-fectively disentangling texture and pose, and preserv-ing texture details across changes in pose. As illus-trated in Fig. 1a, most prior works attempt to achieve the pose and texture disentanglement at a part-level
[23, 41, 24, 39], but each body part may retain some pose information. Without direct supervision from pose-invariant textures, disentangling pose and tex-tures in the image is difficult [19].
To address the above issues, we propose Pose Trans-fer by Permuting Textures (PT2), a self-driven pose transfer method that uses input permutation to trans-fer clothing patterns to the target pose. The input permutation function geometrically disentangles tex-ture from pose in raw images without requiring super-vision. As shown in Fig. 1b, this creates a disentangled
texture sample space by randomly reordering the tex-ture patches on the person so that the source pose is not easily recovered from the permuted textures. We find this patch-level disentanglement can transfer de-tailed clothing patterns to any target pose. However, our permutation function does introduce two new is-sues that we must address to effectively transfer pose.
The first drawback we address is that in addition to losing pose information after permutation, we also lose shape information. To recover some shape information of clothing items (e.g., short dress or long dress), we use a dense pose representation [10] to model the ge-ometric transformation between pose representations.
While prior work also leverages pose information, this is typically a sparse representation (e.g., [24, 39]). As our experiments show, PT2 more effectively leverages the denser representations than prior work.
The second issue we address arises from the unnat-ural boundaries between newly neighboring permuted patches. Specifically, we use a dual-scale encoder in the pose and texture branches for handling permuted in-puts. The small-scale encoder provides a more accurate representation of the texture details without the per-mutation noise, whereas the large-scale encoder enables us to capture longer range visual patterns. Our exper-iments show that each encoder provides a unique and necessary representation of the texture patterns, result-ing in best performance when they are used jointly.
Our paper is organized as follows. Sec. 3 intro-duces our model pipeline. Sec. 3.1.1 details our pa-per’s main contribution, the input permutation func-tion. Sec. 3.1.2 describes the dual-scale encoder for recovering the clothing shape from the permuted tex-tures. Extensive experiments in Sec. 4 show that PT2 significantly improves the image quality of self-driven approaches on DeepFashion [18] and Market-1501 [48].
The user study in Sec. 4.1 also reports that our syn-thesized images are preferred in 68% over prior work. 2.