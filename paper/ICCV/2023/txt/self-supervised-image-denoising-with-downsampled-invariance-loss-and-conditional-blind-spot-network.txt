Abstract
There have been many image denoisers using deep neu-ral networks, which outperform conventional model-based methods by large margins. Recently, self-supervised meth-ods have attracted attention because constructing a large real noise dataset for supervised training is an enormous burden. The most representative self-supervised denois-ers are based on blind-spot networks, which exclude the receptive field’s center pixel. However, excluding any in-put pixel is abandoning some information, especially when the input pixel at the corresponding output position is ex-cluded. In addition, a standard blind-spot network fails to reduce real camera noise due to the pixel-wise correlation of noise, though it successfully removes independently dis-tributed synthetic noise. Hence, to realize a more practi-cal denoiser, we propose a novel self-supervised training framework that can remove real noise. For this, we de-rive the theoretic upper bound of a supervised loss where the network is guided by the downsampled blinded output.
Also, we design a conditional blind-spot network (C-BSN), which selectively controls the blindness of the network to use the center pixel information. Furthermore, we exploit a random subsampler to decorrelate noise spatially, making the C-BSN free of visual artifacts that were often seen in downsample-based methods. Extensive experiments show that the proposed C-BSN achieves state-of-the-art perfor-mance on real-world datasets as a self-supervised denoiser and shows qualitatively pleasing results without any post-processing or refinement. 1.

Introduction
Image denoising aims to recover a clean image from its corrupted counterpart. Recently, image denoisers us-ing convolutional neural networks (CNNs) have achieved great performances, significantly outperforming conven-(a) Noisy 28.48dB / 0.9011 (b) CVF-SID [25] 34.21dB / 0.9381 (c) AP-BSN [19] 34.45dB / 0.9081 (d) C-BSN (Ours) 36.31dB / 0.9483
Figure 1. Visual comparison of denoised images on SIDD vali-dation [2]. Our C-BSN shows better details and no artifacts with-out post-processing or refinement. Best viewed in pdf. tional model-based ones [43, 44, 32]. They trained net-works by minimizing the difference between the network outputs and the ground-truth clean images. In early works, they assumed the camera noise as an additive white Gaus-sian noise (AWGN) and generated a large number of clean-noisy image pairs for the supervised training. However, the denoisers trained with AWGN fail to generalize to real-world camera noises due to the difference between the
Gaussian and real noise distributions [10]. Specifically, real
noise follows a more complicated distribution than a simple
Gaussian and gets more correlated spatially and chromati-cally while passing through an in-camera image processing pipeline, such as demosaicing that involves the computation using adjacent pixels.
Some researchers attempted to find a more realistic noise model to deal with real noise. In the case of camera-raw im-ages, noise can be modeled with a relatively simple distri-bution such as heteroscedastic Gaussian [8]. Hence, a raw image added with such synthetic noise is passed through a camera image signal processor (ISP) model to generate a realistic noisy sRGB image [10, 40]. Other works synthe-sized realistic noise using generative models [6, 5, 12, 1].
Another approach is to construct paired real noise datasets from real photos like DND [27] and SIDD [2]. Training in a supervised manner with those datasets successfully re-duced the noise of real cameras [3, 41, 42]. However, ac-quiring aligned clean images corresponding to noisy ones requires a series of static photos of the same scene. It is costly or even impossible in some cases, such as medical images, since it requires strictly controlled capturing and complicated post-processing. Also, since they used several cameras in specific environments for capturing real noises, they might have different distributions from the ones cap-tured from other cameras and from the same cameras with different shooting environments.
To mitigate the necessity of large aligned datasets, self-supervised denoising that requires only noisy images has been proposed. The most representative methods are based on blind-spot networks (BSN), where each output pixel is estimated from the surrounding noisy pixels except for the corresponding one. It enables the network to learn with the self-supervised loss function, where the same noisy images are used as both input and target. The idea of blind-spot prevents the network from converging to a trivial identity function. The BSN is shown to converge to the clean im-age under the assumption that the expectation of the noise is zero and the noise is pixel-wise independent. They im-posed blindness to the network by masking the input image
[17, 4] or by designing networks that structurally exclude the central pixel from the receptive fields [18, 35, 19]. How-ever, the BSN-based self-supervised algorithms have two limitations; 1) The network cannot utilize the center pixel which is the most informative. 2) It is not applicable to real noise since it has a pixel-wise correlation in the sRGB do-main [19].
In this paper, we propose a novel self-supervised learn-ing framework to denoise real noise without the blind-spot, i.e., with the center pixel information. Our framework overcomes the above-stated limitations by deriving a novel downsampled invariance loss function. The downsampled invariance loss employs a novel conditional blind-spot net-work (C-BSN) and random subsampler. Specifically, our
C-BSN conditionally controls its blindness by switching the masked convolution operations. It allows the network to be regularized by its blind-spot counterpart, which pre-vents the trivial solution. Furthermore, we impose the loss on randomly downsampled subimage so that the correla-tion of the noise is weakened without inducing visual ar-tifacts. In addition, we augment the loss with a blind self-supervised loss for stabilizing the training. Extensive exper-iments have been conducted to evaluate the proposed frame-work, which validates that the C-BSN outperforms existing self-supervised denoisers and even some supervised meth-ods trained with real noise datasets.
The contributions of our method are summarized as fol-lows:
• We propose a novel self-supervised denoising frame-work that can be processed without a blind-spot.
We theoretically derive the upper bound of the self-supervised loss as downsampled invariance loss, which exploits masked output as the regularization of the denoised image without masking.
In addition, the proposed method does not require post-processing or noise statistics.
• To apply downsampled invariance loss, we propose a novel conditional blind-spot network named C-BSN, which conditionally controls the blindness of the net-work. To deal with the spatial correlation of the real camera noise, a random subsampler is proposed to avoid visual artifacts.
• The C-BSN shows state-of-the-art performance in real-world sRGB benchmarks DND [27] and
SIDD [2], as shown in Figs. 1, 4, and 5. 2.