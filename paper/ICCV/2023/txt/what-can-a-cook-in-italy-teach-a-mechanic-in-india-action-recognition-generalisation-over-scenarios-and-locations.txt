Abstract
We propose and address a new generalisation problem: can a model trained for action recognition successfully classify actions when they are performed within a previ-ously unseen scenario and in a previously unseen loca-tion? To answer this question, we introduce the Action
Recognition Generalisation Over scenarios and locations dataset (ARGO1M), which contains 1.1M video clips from the large-scale Ego4D dataset, across 10 scenarios and 13 locations. We demonstrate recognition models struggle to generalise over 10 proposed test splits, each of an unseen scenario in an unseen location. We thus propose CIR, a method to represent each video as a Cross-Instance Re-construction of videos from other domains. Reconstruc-tions are paired with text narrations to guide the learning of a domain generalisable representation. We provide ex-tensive analysis and ablations on ARGO1M that show CIR outperforms prior domain generalisation works on all test splits. Code and data: https://chiaraplizz.github.io/ what-can-a-cook/. 1.

Introduction
A notable distinction between human and machine intel-ligence is the ability of humans to generalise. We can see an example of the action “cut” performed by a cook in Italy, and recognise the same action performed in a different ge-ographic location, e.g. India, despite having never visited.
We can also recognise actions within new scenarios, such as a mechanic cutting metal, even if we are unfamiliar with the tools they use.
This problem is known as domain generalisation [62], where a model trained on a set of labelled data fails to generalise to a different distribution in inference. The gap between distributions is known as domain shift. To date, works have focused on generalising over visual domain shifts [25, 46, 31, 10, 39]. In this paper, we introduce the scenario shift, where the same action is performed as part
*Work carried during Chiara’s research visit to the University of Bristol
Figure 1: Problem statement and samples from the ARGO1M dataset. The same action, e.g. “cut”, is performed differently based on the scenario and the location in which it is carried out. We aim to generalise so as to recognise the same action within a new scenario, unseen during training, and in an unseen location, e.g.,
Mechanic (
) in India (
). of a different activity, impacting the tools used, objects in-teracted with, goals and behaviour. We combine this with the location shift, generalising over both simultaneously.
In Fig. 1, the action “cut’ is performed using a knife
), pliers whilst building ( whilst cooking (
) and scissors for arts and crafts (
). Tools are not specific for a scenario and can vary over locations – e.g. in Fig. 1, seaweed sheets are cut with scissors while cooking in Japan. Generalising would be best achieved by learning the notion of “cutting” as separating an object into two or more pieces, regardless of the tool or background location. Successful generalisa-tion can thus enable recognising metal being “cut” by a me-chanic in India using an angle grinder (Fig. 1 Test).
Our investigation is enabled by the recent introduction of the Ego4D [17] dataset of egocentric footage from around the world. We curate a setup specifically for action gener-alisation, called ARGO1M. It contains 1.1M action clips of 60 classes from 73 unique scenario/location combinations.
To tackle the challenge of ARGO1M, we propose a new method for domain generalisation. We represent each video
as a weighted combination of other videos in the batch, po-tentially from other domains. We refer to this as Cross-Instance Reconstruction (CIR). Through reconstruction, the method learns domain generalisable video features. CIR is supervised by a classification loss and a video-text associa-tion loss. To summarise, our key contributions are:
• We curate the Action Recognition Generalisation dataset (ARGO1M) from videos and narrations from Ego4D.
ARGO1M is the first to test action generalisation across both scenario and location shifts, and is the largest do-main generalisation dataset across images and video.
• We introduce CIR, a domain generalisation method which exploits Cross-Instance Reconstruction and video-text pairing to learn generalisable representations.
• We test CIR on the proposed ARGO1M, showing that it consistently outperforms baselines and recent domain generalisation approaches on 10 test sets. 2.