Abstract
Few-shot continual learning is the ability to continually train a neural network from a sequential stream of few-shot data.
In this paper, we propose a Few-shot Contin-ual Infomax Learning (FCIL) framework that makes a deep model to continually/incrementally learn new concepts from few labeled samples, relieving the catastrophic forgetting of past knowledge. Speciﬁcally, inspired by the theoretical deﬁnition of transfer entropy, we introduce a feature em-bedding infomax to effectively perform the few-shot learn-ing, which can transfer the strong encoding capability of the base network to learn the feature embedding of these novel classes by maximizing the mutual information of different-level feature distributions. Further, considering that the learned knowledge in the human brain is a generalization of actual information and exists in a certain relational struc-ture, we perform continual structure infomax learning to relieve the catastrophic forgetting problem in the continual learning process. The information structure of this learned knowledge can be preserved through maximizing the mu-tual information across these continual-changing relations of inter-classes. Comprehensive evaluations on CIFAR100, miniImageNet, and CUB200 datasets demonstrate the supe-riority of our FCIL when compared against state-of-the-art methods on the few-shot continual learning task. 1.

Introduction
Recent deep learning technologies [23, 30, 13] have made great progress in many computer vision tasks. The success of deep neural networks is achieved by employing a large number of labeled training data. However, it is difﬁ-cult to collect large-scale supervised data in advance, while we would encounter a sequential data stream with unknown new classes in many realistic situations. This would require
# Equal Contribution.
∗ Corresponding Author. a neural network with continual learning ability, especially when some new classes with very few labeled samples of-ten appear. Therefore, this work focuses on dealing with the few-shot continual/incremental learning task [28, 26]. They are two critical challenges in a few-shot continual learn-ing task: i) how to learn new knowledge with few-shot in-stances, and ii) how to avoid catastrophic forgetting of the preceding learned knowledge.
Recently, numerous previous works [31] have been de-voted to few-shot continual learning from various perspec-tives. In [31, 35, 8], the topological structure of the knowl-edge space formed by different classes has been considered to perform few-shot continual learning by using a neural gas network or graph model. Cheraghian et al. [8] have proposed a semantic-aware knowledge distillation method to solve few-shot class-incremental learning by making use of word embeddings. Several meta-learning based meth-ods [7, 18] have been also proposed to enable the model to preserve old knowledge and adapt to the new classes for continual learning. By assigning virtual prototypes to squeeze the embedding of known classes and reserve for new ones, the forward compatible training method [37] has efﬁciently incorporated new classes with forward compati-bility and meanwhile resists forgetting old ones. Similarly, the mixture of subspaces and synthesized features [6] have been used to alleviate the forgetting and over-ﬁtting prob-lem in the few-shot continual learning process.
In [38], the self-promoted prototype learning scheme has been pro-posed to explicitly learn the feature representation under the few-shot learning situation and thus facilitated subse-quent incremental tasks. The self-supervised strategy has been also used to enhance the feature extraction ability of the model by adding self-supervised loss function assis-tance during the training process [20, 26]. Moreover, to minimize over-ﬁtting and the catastrophic forgetting prob-lem, Mazumder et al. [26] have selected very few unimpor-tant model parameters to perform few-shot learning on new classes. In [11], mutual information (MI) maximization has been ﬁrst used as a solution method to deal with the catas-trophic forgetting problem in the online continual learning task, but it cannot well consider how to perform continual infomax learning from the perspective of information en-tropy, especially in the few-shot regime.
In this work, we propose a novel Few-shot Continual
Infomax Learning (FCIL) framework that makes a deep model to continually learn from a stream of few-shot la-beled data. In general, the continual learning model would be trained with a large amount of labeled data in the initial learning stage, while few-shot samples of some unknown classes would be encountered in the continual learning pro-cess. Here we attempt to address the few-shot continual learning task from two aspects. First, inspired by the the-oretical deﬁnition of transfer entropy, we attempt to trans-fer the strong encoding capability of the base network to promote few-shot continual learning. Speciﬁcally, we pro-pose a feature embedding infomax learning to new con-cepts from a few labeled samples through maximizing the mutual information between different level feature distribu-tions, where the convolutional representations and the fea-ture embedding of new classes are encoded with these ﬁxed convolution layers of the base network and newly increased parameters of fully-connected layer, respectively. Second, considering that the learned knowledge in the human brain is a generalization or abstraction of the actual information learned from these seen samples [10], we wish the contin-ual learning model with a stable information structure that can be updated incrementally. Thus we propose a contin-ual structure infomax learning mechanism to alleviate the catastrophic forgetting problem during the continual learn-ing process. The structure relation of this learned informa-tion can be preserved through maximizing the mutual infor-mation across these continual-changing structure relations of inter-classes.
In summary, our primary contributions can be summa-rized as follows: i) We propose a Few-shot Continual Info-max Learning (FCIL) framework that makes a deep model to incrementally learn new concepts from few labeled sam-ples, relieving the catastrophic forgetting problem of pre-viously learned ones. ii) Two specially-designed infomax learning mechanisms are proposed to address the few-shot continual learning problem with the help of mutual informa-tion maximization, including feature embedding infomax and continual structure infomax. iii) We validate the ef-fectiveness of our proposed FCIL on three benchmarks (in-cluding CIFAR100, CUB200, and miniImageNet), and also demonstrate the superiority of our proposed FCIL when compared with existing state-of-the-art methods. 2.