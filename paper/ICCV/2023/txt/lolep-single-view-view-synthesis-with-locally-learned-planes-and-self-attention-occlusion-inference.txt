Abstract
We propose a novel method, LoLep, which regresses
Locally-Learned planes from a single RGB image to represent scenes accurately, thus generating better novel views. Without the depth information, regressing appro-priate plane locations is a challenging problem. To solve this issue, we pre-partition the disparity space into bins and design a disparity sampler to regress local offsets for mul-tiple planes in each bin. However, only using such a sam-pler makes the network not convergent; we further propose two optimizing strategies that combine with different dis-parity distributions of datasets and propose an occlusion-aware reprojection loss as a simple yet effective geometric supervision technique. We also introduce a self-attention mechanism to improve occlusion inference and present a
Block-Sampling Self-Attention (BS-SA) module to address the problem of applying self-attention to large feature maps.
We demonstrate the effectiveness of our approach and gen-erate state-of-the-art results on different datasets. Com-pared to MINE, our approach has an LPIPS reduction of 4.8%∼9.0% and an RV reduction of 74.9%∼83.5%. We also evaluate the performance on real-world images and demonstrate the benefits. 1.

Introduction
Single-view view synthesis allows a camera to roam around a scene from a given photograph. It has been used to generate compelling views for different applications in-cluding image editing and augmented or virtual reality. The underlying techniques require understanding the geometry of scenes, reasoning about occlusions, and rendering high-quality images of novel views in real time.
Many approaches have been proposed to solve this prob-lem [41, 35, 22, 26, 40]. They synthesize novel views by predicting a naive representation (e.g., depth maps, voxels, or point clouds) from a single image and generating images
Yu-Ping Wang is the corresponding author (wyp cs@bit.edu.cn).
Figure 1. Comparisons on the KITTI dataset. LoLep generates state-of-the-art results and even LoLep with fewer planes uses less memory and generates better novel views than previous methods with more planes (LoLep-16 vs. MINE-32, MINE-64 and MPI-32, LoLep-32 vs. MINE-64), which benefits from locally-learned planes and self-attention occlusion inference. The batch size is 4. for novel views using appropriate rendering techniques.
While these methods generate some positive results, they limit the performance of single-view view synthesis due to their inability to represent occluded regions well [37]. In this context, layered representations [37, 38, 45, 21, 20, 13] are more suitable for single-view view synthesis.
Specifically,
Recently, Multiplane Image (MPI) [45] has gained pop-ularity as a layered representation and has been used for single-view view synthesis [37]. it is an encoder-decoder structure supervised by multiple images from different views of a given scene and is used to pre-dict multiple planes of RGB and alpha values from a single image. MINE [20] combines MPI with NeRF [25] and gen-eralizes MPI into a continuous depth MPI by considering multiple plane location inputs. This can improve the per-formance of single-view view synthesis to better infer geo-metric primitives in a scene. However, these methods sam-ple plane locations randomly, which makes it hard for the planes to learn optimal scene representations. As a result,
Figure 2. Overview. LoLep regresses locally-learned planes to represent scenes accurately without a depth map input mainly relying on three novel components. (a) Disparity Sampler: regressing accurate locations for multiple planes from only the RGB image; (b)
Occlusion-aware Reprojection Loss: a simple yet effective geometric supervision technique for single-view view synthesis to learn better geometry; (c) Block-Sampling Self-Attention: supporting self-attention applied to large feature maps for higher performance. ‘⊕’ concatenates two tensors. these methods usually require more planes to obtain satis-factory novel views, requiring huge computing power. To alleviate this requirement, a key issue is how to fully utilize the limited planes to obtain the most accurate scene repre-sentation as possible.
Previous works [21, 13] solve this issue by regressing more accurate locations for multiple planes. Due to the lack of supervision and using globally-learned planes, however, their networks take an RGB image and an additional depth map as input. The depth map is provided by a pretrained depth prediction network, which introduces a heavy depen-dence on other networks.
Main Results: We present a novel single-view view syn-thesis method based on Multiplane Image, LoLep. LoLep aims to make full use of locally-learned planes to represent scenes accurately, thus generating better novel views from a single RGB image with less memory (Figure 1). In order to achieve that, we pre-partition the disparity space into bins and design a disparity sampler to condition local offsets of planes on a single RGB image. However, due to the lack of depth information, applying the sampler directly makes the network not convergent. We further propose two optimizing strategies that combine with different disparity distributions of datasets and an occlusion-aware reprojection loss to solve it (described in Section 4.1). To improve the ability for oc-clusion inference, we introduce a self-attention mechanism to our decoder and present a Block-Sampling Self-Attention (BS-SA) module to work for large feature maps (described in Section 4.2). Overall, the novel components of our ap-proach include:
• We propose a novel single-view view synthesis method based on Multiplane Image, LoLep, that regresses ac-curate scene representations and generates better novel views on scene geometry and occluded regions.
• We introduce a self-attention mechanism to improve occlusion inference and present a BS-SA module to address the problem of applying self-attention on large feature maps.
• We compare with prior methods and show that LoLep outperforms MINE on different datasets with an LPIPS reduction of 4.8%∼9.0% and an RV reduction of 74.9%∼83.5%. Moreover, LoLep with fewer planes uses less memory and generates better results than prior methods with more planes. 2.