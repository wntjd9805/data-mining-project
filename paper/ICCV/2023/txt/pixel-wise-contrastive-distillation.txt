Abstract
We present a simple but effective pixel-level self-supervised distillation framework friendly to dense predic-tion tasks. Our method, called Pixel-Wise Contrastive Dis-tillation (PCD), distills knowledge by attracting the corre-sponding pixels from student’s and teacher’s output feature maps. PCD includes a novel design called SpatialAdaptor which “reshapes” a part of the teacher network while pre-serving the distribution of its output features. Our ablation experiments suggest that this reshaping behavior enables more informative pixel-to-pixel distillation. Moreover, we utilize a plug-in multi-head self-attention module that ex-plicitly relates the pixels of student’s feature maps to en-hance the effective receptive field, leading to a more com-petitive student. PCD outperforms previous self-supervised distillation methods on various dense prediction tasks. A backbone of ResNet-18-FPN distilled by PCD achieves 37.4
APbbox and 34.0 APmask on COCO dataset using the detec-tor of Mask R-CNN. We hope our study will inspire future research on how to pre-train a small model friendly to dense prediction tasks in a self-supervised fashion. 1.

Introduction
Self-supervised learning (SSL) has emerged as a promis-ing pre-training method due to its remarkable progress on various computer vision tasks [24, 9, 21, 6, 23, 59]. Mod-els pre-trained by SSL methods attain transfer performance akin to, or even surpassing that of their supervised pre-trained counterparts. However, this advancement of SSL appears to be confined to larger models. Small models, such as ResNet-18 [26], exhibit inferior linear probing ac-curacy as reported in [18, 60, 14]. Considering the neces-sity of small models for edge devices or resource constraint regime, it is much essential to tackle this problem.
Recently, the performance lag of small models has been effectively alleviated by self-supervised distillation [61, 39,
†Equal contribution
‡Corresponding author 1, 18, 19, 60, 4, 14, 66, 54], where teachers’ (large pre-trained models) knowledge is transferred [5, 48, 2, 29] to students (small models) in a self-supervised learning fash-ion. Self-supervised distillation methods yield competi-tive performance for small models, especially on classifi-cation tasks (e.g., fine-grained and few-shot classification).
Nevertheless, their improvement on dense prediction tasks like object detection and semantic segmentation is less pro-nounced than on classification tasks. This imbalance seem-ingly suggests that the favorable representations learned by teachers can only be partially transferred to students. A nat-ural question arises: what obstructs students from inheriting the knowledge advantageous to dense prediction tasks? In this study, we seek the answers to this question from the following aspects.
First, the distillation signals of current self-supervised distillation methods are mostly at image-level, while the rich pixel-level knowledge is yet to be utilized. We argue that it is inefficient for small models to learn representa-tions good for dense prediction tasks from image-level su-pervision1. Driven by this, we here present a simple but effective pixel-level self-supervised distillation framework,
Pixel-Wise Contrastive Distillation (PCD), which extends the idea of contrastive learning [22] by incorporating pixel-level knowledge distillation. PCD attracts the correspond-ing pixels from the student’s and the teacher’s output fea-ture maps and separates the student’s pixels and the negative pixels of a memory queue [24]. With pixel-level distillation signal, PCD enables more efficient and adequate transfer of knowledge from the teacher to the student.
Second, it is not straightforward to distill pixel-level knowledge from the commonly adopted teachers pre-trained by image-level SSL methods [24, 9, 21, 6]. These teachers tend to project images into vectorized features, thus losing the spatial information which is indispensable to pixel-to-pixel distillation in our PCD. An intuitive prac-tice to circumvent this issue would be to remove the well-trained projection/prediction head (a non-linear MLP at-tached to the backbone) and the global pooling layer. The 1On the other hand, large models pre-trained by image-level SSL meth-ods like MoCo can be quite competitive on dense prediction tasks.
(a) Architecture of Pixel-Wise Contrastive Distillation (b) Workflows of Teacher’s Projection Head
Figure 1: (a) is the specific architecture of Pixel-Wise Contrastive Distillation. Before distillation, the original teacher’s projection head is modified by SpatialAdaptor (represented by 3 in the figure). Distillation loss is the average contrastive loss computed over all corresponding pixel pairs of the student and the teacher. (b) depicts the workflows of teacher’s projection head before (top half) and after (bottom half) using SpatialAdaptor. The pooling layer on the far right is used for demonstrating the invariability of SpatialAdaptor. Best viewed in color. distillation loss will be computed over the feature maps out-put by the backbone. However, we experimentally show the ineffectiveness of such a simplistic approach, implying that the projection/prediction head contains nonnegligible knowledge for pixel-level distillation. Towards the goal of leveraging this knowledge, we introduce a SpatialAdaptor to adapt the projection/prediction head used for encoding vectorized features to processing 2D feature maps while not changing the distribution of the output features. Though conceptually simple, the SpatialAdaptor is of great signifi-cance to pixel-level distillation.
Last, small models are innately weak in capturing infor-mation from the regions of large spans due to their smaller effective receptive fields (ERF) [37]. This natural defi-ciency prevents students from further imitating teachers at
In this case, we append a multi-head self-pixel-level. attention (MHSA) module [49] to the student model, which explicitly relates the pixels within the student’s output fea-ture maps. The addition of the MHSA module helps slightly enlarge the ERF of the small model, and consequently im-proves its transferring results. The MHSA will be depre-cated in the fine-tuning phase. We think such gain without pain is very helpful for pre-training. We refer to Fig. 1a for a detailed depiction of PCD.
We comprehensively evaluate the effectiveness of PCD on several typical dense prediction tasks. PCD surpasses state-of-the-art results across all downstream tasks. Our re-sults demonstrate the nontrivial advantages of PCD over competitive SSL methods designed for dense prediction tasks and previous image-level self-supervised distillation methods. Under the linear probing protocol, a ResNet-18 distilled by PCD attains 65.1% top-1 accuracy on Ima-geNet. These results highlight the superiority of pixel-level supervision signal for self-supervised distillation.
We also find that PCD is robust to the choices of pre-trained teacher models and works well with various student backbone architectures. Students of larger backbones can compete with or even exceed the teacher on certain tasks, revealing an encouraging path for pre-training. These find-ings carry implications for future research and we hope our work inspires further investigation into self-supervised pre-training with small models. 2.