Abstract
We present a novel method for automatic vectorized avatar generation from a single portrait image. Most exist-ing approaches that create avatars rely on image-to-image translation methods, which present some limitations when applied to 3D rendering, animation, or video. Instead, we leverage modality-specific autoencoders trained on large-scale unpaired portraits and parametric avatars, and then learn a mapping between both modalities via an align-ment module trained on a significantly smaller amount of data. The resulting cross-modal latent space preserves fa-cial identity, producing more visually appealing and higher fidelity avatars than previous methods, as supported by our quantitative and qualitative evaluations. Moreover, our method’s virtue of being resolution-independent makes it highly versatile and applicable in a wide range of settings. 1.

Introduction
An avatar can be defined as a digital representation or virtual character by which people represent themselves and other beings in a virtual platform or community [4]. Such a representation has been ubiquitous in our daily life, from personal use on social media to marketing strategies by companies [23]. Given an image and a style, the task is to generate a new representation that successfully conveys a given style and content while preserving the person’s iden-tity as much as possible.
Nowadays, avatars are typically created in two ways.
First, it can be created by skilled artists. To streamline the avatar creation, many artists use predefined facial pre-sets from a library and combine them part by part to obtain the final avatar (e.g., the “manual” results from [33]). This scheme is commonly used for messaging and video games avatars. However, it can be a tedious process that frequently
Figure 1. Examples images from a test set (a), parametric avatars generated by our method (b). alters the subject’s identity due to the limited predefined
In contrast, automatic image generation has been assets.
employed as a time-efficient and large-scale alternative for this task. Those methods typically focus on caricature gen-eration [3, 29, 7, 17, 11, 35]. While such methods could translate real photography to an avatar or caricature, they all operate in image space, which poses some limitations when used in virtual environments or for animations.
Parametric vector graphics propose a solution that over-comes the drawbacks of static images to some extent. In particular, Wolf et al. [33] propose a tied output synthesis network to infer parameters from generated image avatars, but their method works with very simple datasets like face emoji [31], which avoids the matter of stylization. Hu et al. [9] and Shi et al. [27, 28] focus on 3D photorealistic-looking avatars. However, these works do not focus on cap-turing a specific style, nor producing 2D parametric avatars.
In this paper, we propose an alternative to image-based avatars that enables animations and video compositing without losing quality or resolution. To do so, we utilize a parametric representation to depict avatars; that is, we em-ploy a set of parameters that define each facial attribute.
To train such an approach in a supervised way requires a dataset of paired portraits and their respective avatars, which is prohibitively expensive to obtain. To address this, we present a weakly-supervised novel cross-modal align-ment framework that translates rich representations from the portrait domain to the parametric avatar domain. To alle-viate the lack of paired data, we first learn modality-specific latent spaces from large-scale unpaired data. Once such la-tent spaces are learned, we match their latent representa-tions by a cross-modal module trained on a small amount of paired data, which preserves the facial identity from the portrait and the style of the parametric avatar. As a result, our framework is able to translate identity features in image space to parametric space, and at the same time, apply style features from the original vector-based parametric space.
The goal of this work is to propose a proficient approach for converting individual images into parametric avatar rep-resentations that accurately preserve the individual’s iden-tity. In summary, our contributions are two-fold:
• A flexible parametric representation that encodes fa-cial attributes, enabling the representation of a wide range of human appearances.
• A novel cross-modal framework that, when provided an input image, generates higher quality avatars with better preservation of the individual’s identity than pre-vious methods. 2.