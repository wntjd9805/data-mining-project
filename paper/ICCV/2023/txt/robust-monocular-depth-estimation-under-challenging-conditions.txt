Abstract
While state-of-the-art monocular depth estimation ap-proaches achieve impressive results in ideal settings, they are highly unreliable under challenging illumination and weather conditions, such as at nighttime or in the presence of rain. In this paper, we uncover these safety-critical is-sues and tackle them with md4all: a simple and effective solution that works reliably under both adverse and ideal conditions, as well as for different types of learning super-vision. We achieve this by exploiting the efficacy of existing methods under perfect settings. Therefore, we provide valid training signals independently of what is in the input. First, we generate a set of complex samples corresponding to the normal training ones. Then, we train the model by guiding its self- or full-supervision by feeding the generated sam-ples and computing the standard losses on the correspond-ing original images. Doing so enables a single model to recover information across diverse conditions without mod-ifications at inference time. Extensive experiments on two challenging public datasets, namely nuScenes and Oxford
RobotCar, demonstrate the effectiveness of our techniques, outperforming prior works by a large margin in both stan-dard and challenging conditions. Source code and data are available at: https://md4all.github.io. 1.

Introduction
Estimating the depth of a scene is a fundamental task for autonomous driving and robotics navigation. While supervised monocular depth estimation approaches have achieved remarkable results, they rely on ground truth data which is expensive and time-consuming to produce [19, 12].
This requires costly 3D sensors (e.g., LiDAR) and signifi-cant additional data processing [19, 12].
To circumvent these issues, geometrical constraints on stereo pairs or monocular videos have been widely explored
∗ The authors contributed equally.
Contact author: Stefano Gasperini (stefano.gasperini@tum.de).
Figure 1. Predictions in challenging settings [3] for self-supervised
[11] and supervised [1] methods. Standard approaches fail due to training assumptions or sensor artifacts. Under both supervisions, our md4all makes the same models robust in all conditions. to learn depth estimation in a self-supervised manner [11, 25, 34, 7, 10]. Monocular training solutions are the most inexpensive and rely on the smallest amount of assumptions on the sensor setup, as they require only image sequences captured by a single camera.
Self-supervised methods rely on photometric assump-tions and pixel correspondences [11, 34]. State-of-the-art approaches [11, 41, 32] deliver sharp and accurate esti-mates in standard conditions (i.e., sunny and cloudy), but suffer from a variety of inherent issues, such as scale am-biguity and difficulties with dynamic objects. While prior works have already proposed robust methods to address these problems [12, 8], there is still a major issue preventing the wide applicability of self-supervised depth estimators in safety-critical settings, such as autonomous driving. Dark-ness and adverse weather conditions (e.g., night, rain, snow, and fog) introduce noise in the pixel correspondences. As displayed in Figure 1, this is detrimental to the effectiveness of such methods, thereby requiring ad hoc solutions.
As shown in Figure 2, this problem is particularly se-vere at nighttime due to reflections (e.g., caused by street-lights and vehicle headlights), noise, and the general inabil-ity of the embedded cameras to capture details in dark ar-eas. This leads to wrong depth estimates, which can be dan-gerous in safety-critical settings. A few pioneering works have already explored this problem, albeit with highly-complex pipelines and significant architecture changes af-fecting inference as well [38, 37, 22, 33, 36], such as illumination-specific branches. Additionally, prior methods that can operate both at night- and daytime introduce a sig-nificant trade-off concerning the standard daytime perfor-mance [22, 37], highlighting the need for a new solution.
In adverse weather conditions such as rain, monocular models are similarly fooled by reflections and decreased visibility. However, rain introduces another problem. While radars are robust in such conditions, LiDARs become unre-liable, as they introduce multi-path and the so-called bloom-ing effects (Figure 2). In autonomous driving, since super-vised depth estimation approaches learn from LiDAR data, this causes them to learn also such erroneous measurements, rendering them unreliable in rainy settings (Figure 1). Anal-ogous issues occur with snow and fog. These problems are relatively unexplored, demanding new solutions.
Alarmingly, no general solution currently allows an image-based depth estimator to work reliably under all con-ditions. Since LiDAR can constitute a misleading train-ing signal in adverse weather, and pixel correspondences are problematic too (e.g., at night), neither existing super-vised [24] nor self-supervised [11, 34] techniques work well in such challenging settings. A straightforward solution for the supervised case would be using synthetic data [40, 30], as by simply not modeling the sensor issues, a simula-tor could produce perfect ground truth in adverse weather.
However, this is not only unexplored, but it would introduce a series of problems, such as a substantial syn2real gap due to the difficulty of modeling challenging conditions realis-tically (requiring, e.g., domain adaptation).
In this paper, we address these open issues with a sim-ple and effective solution that works reliably in a variety of conditions and for multiple types of supervision. We ap-proach this challenging problem by considering the success of existing methods in standard illumination and weather settings [11, 13, 12, 8]. This motivated us to find a way for them to work also under challenging scenarios, exploiting what makes them learn depth effectively in ideal conditions.
Our core idea is based on training the model by providing always valid training signals as if it was sunny or cloudy, even when samples with adverse conditions are given. We apply this general principle to both supervised and self-supervised depth estimation via a set of techniques to im-prove the model robustness and reduce the performance gap between standard and hard conditions. The main contribu-tions of this paper can be summarized as follows:
• We show how estimating depth in adverse conditions (e.g., night and rain) is problematic for both self- and fully-supervised approaches, requiring new solutions.
• We propose md4all: a simple and effective technique to make standard models robust in diverse conditions.
Figure 2. Detrimental factors to monocular depth estimation in dif-ficult settings from nuScenes [3]. Self-supervised works have is-sues with textureless areas, reflections, and noise. Supervised ones learn artifacts from the ground truth sensor (LiDAR is shown).
• We apply our generic method to both fully- and self-supervised monocular settings.
• We generate and share open-source images in adverse conditions corresponding to the sunny and cloudy sam-ples of nuScenes [3] and Oxford Robotcar [23].
With md4all, we substantially outperform prior solutions delivering robust estimates in a variety of conditions. 2.