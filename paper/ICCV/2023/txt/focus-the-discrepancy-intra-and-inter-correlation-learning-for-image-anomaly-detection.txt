Abstract
Input Image
GroundTruth
MKD
Patch-wise
Discrepancy
+Intra-Correlation
+Inter-Correlation (Ours)
Humans recognize anomalies through two aspects: larger patch-wise representation discrepancies and weaker patch-to-normal-patch correlations. However, the previ-ous AD methods didn’t sufﬁciently combine the two com-plementary aspects to design AD models. To this end, we ﬁnd that Transformer can ideally satisfy the two as-pects as its great power in the uniﬁed modeling of patch-In wise representations and patch-to-patch correlations. this paper, we propose a novel AD framework: FOcus-the-Discrepancy (FOD), which can simultaneously spot the patch-wise, intra- and inter-discrepancies of anomalies.
The major characteristic of our method is that we reno-vate the self-attention maps in transformers to Intra-Inter-Correlation (I2Correlation). The I2Correlation contains a two-branch structure to ﬁrst explicitly establish intra-and inter-image correlations, and then fuses the features of two-branch to spotlight the abnormal patterns. To learn the intra- and inter-correlations adaptively, we propose the
RBF-kernel-based target-correlations as learning targets for self-supervised learning. Besides, we introduce an en-tropy constraint strategy to solve the mode collapse issue in optimization and further amplify the normal-abnormal distinguishability. Extensive experiments on three unsuper-vised real-world AD benchmarks show the superior perfor-mance of our approach. Code will be available at https:
//github.com/xcyao00/FOD. 1.

Introduction
The goal of anomaly detection (AD) is to distinguish an instance containing anomalous patterns from those nor-mal samples and further localize those anomalous regions.
Anomalies are deﬁned as opposite to normal samples and are usually rare, which means that we need to tackle AD tasks under the unsupervised setting with only normal sam-*Corresponding Author. i n o i t a n m a t n o
C e r i w t n e
B d e p p i l
F d e c a l p s i
M
Figure 1. Anomaly detection examples on MVTecAD [3]. Mul-tiresolution Knowledge Distillation (MKD) [41] adopts the con-ventional patch-wise representation discrepancies. Row 1 shows the hard global anomalies (i.e, they are not signiﬁcantly different from normal visuals). Rows 3 and 4 show the logical anomalies (i.e. they may be easily recognized as normal if only from the patch-wise discrepancy). ples accessible. The core idea of most unsupervised AD methods is to compare with normal samples to distinguish anomalies [46, 35, 60, 11, 28, 58]. Even for humans, we also recognize anomalies in this way, speciﬁcally through three discrepancies, i.e., 1. patch patterns that differentiate from the normal visuals; 2. image regions that destroy tex-tures or structures; 3. novel appearances that deviate from our accumulated knowledge of normality. Namely, anoma-lous patches usually have three characteristics: their patch-wise representations are different from the normal visu-als; they are different from most patches within one image; they deviate from our accumulated knowledge of normality.
These views intrinsically reveal that humans’ recognition of anomalies depends on two aspects: patch-wise representa-tions (1) and intra- and inter-image correlations (2, 3).
Previous methods mainly follow the former aspect to learn distinguishable representations or reconstructions,  
such as reconstruct-based methods [6, 57, 1] and knowledge distillation AD models [4, 41]. The goal of these meth-ods is to generate reconstructed samples or feature repre-sentations, and larger patch-wise representation discrepan-cies can appear in the abnormal patches. However, only the patch-wise representation discrepancies are insufﬁcient for detecting more complex anomalies (e.g., rows 3 and 4 in
Figure 1), since the patch-wise errors can’t provide com-prehensive descriptions of the spatial context. Other main-stream AD methods, such as embedding-based [11, 35] and one-class-classiﬁcation-based (OCC) [39, 60] methods, are much similar to the latter aspect. These methods achieve anomaly detection by measuring the distances between the features of test samples and normal features. Compared with the non-learnable feature distances, the explicit intra-and inter-image correlations in our method are more effec-tive to detect diverse anomalies (see Table 1, 2, 3). More-over, patch-wise representation discrepancies and intra- and inter-correlation discrepancies are complementary, and can be combined to develop more powerful AD models.
Recently, with the self-attention mechanism and long-range modeling ability, transformers [50] have signiﬁcantly renovated many computer vision tasks [14, 24, 7, 67, 56] and recently popular language-vision multimodal tasks [30, 22]. Transformers have shown great power in the uni-ﬁed modeling of patch-wise representations and patch-to-patch correlations. Transformers are quite suitable for AD tasks as their modeling ability can satisfy the two aspects of anomaly recognition quite well. Some works [26, 18, 8, 62] also attempt to employ transformers to construct AD mod-els. However, these methods only use transformers to ex-tract vision features, which didn’t sufﬁciently adapt trans-formers’ long-range correlation modeling capability to AD tasks. Different from these works, we explicitly exploit transformers’ self-attention maps to establish the intra- and inter-image correlations. The correlation distribution of each patch can provide more informative descriptions of the spatial context, which can reveal more intricate and seman-tic anomaly patterns.
In this paper, motivated by humans’ anomaly recognition process, we propose a novel AD framework: FOcus-the-Discrepancy (FOD), which can exploit transformers’ uni-ﬁed modeling ability to simultaneously spot the patch-wise, intra- and inter-discrepancies. Our key designs are com-the patch-wise dis-posed of three recognition branches: crepancy branch is to reconstruct the input patch features for distinguishing simple anomalies; the intra-correlation branch is to explicitly model patch-to-patch correlations in one image for distinguishing hard global anomalies (e.g., row 1 in Figure 1); the inter-correlation branch is to ex-plicitly learn inter-image correlations with known normal patterns from the whole normal training set. To imple-ment the intra- and inter-correlation branches, we adapt
Transformer and renovate the self-attention mechanism to the I2Correlation, which contains a two-branch structure to
ﬁrst separately model the intra- and inter-correlation dis-tribution of each image patch, and then fuse the features of two-branch to spotlight the abnormal patterns. To learn the intra- and inter-correlations adaptively, we propose the
RBF-kernel-based target-correlations as learning targets for self-supervised learning, the RBF kernel is used to present the neighborhood continuity of each image patch. Besides, an entropy constraint strategy is applied in the two branches, which can solve the mode collapse issue in optimization and further amplify the normal-abnormal distinguishability.
In summary, we make the following main contributions: 1. We propose a novel AD framework: FOD, which can effectively detect anomalies by simultaneously spotting the patch-wise, intra- and inter-discrepancies. 2. We renovate the self-attention mechanism to the
I2Correlation, which can explicitly establish intra- and inter-correlations in a self-supervised way with the target-correlations. An entropy constraint strategy is proposed to further amplify the normal-abnormal distinguishability. 3. Our method can achieve SOTA results on three real-world AD datasets, this shows our method can more effec-tively determine anomalies from complementary views. 2.