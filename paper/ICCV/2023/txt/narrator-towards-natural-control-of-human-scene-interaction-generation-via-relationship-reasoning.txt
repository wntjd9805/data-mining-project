Abstract
Naturally controllable human-scene interaction (HSI) generation has an important role in various fields, such as
VR/AR content creation and human-centered AI. However, existing methods are unnatural and unintuitive in their con-trollability, which heavily limits their application in prac-tice. Therefore, we focus on a challenging task of natu-rally and controllably generating realistic and diverse HSIs from textual descriptions. From human cognition, the ideal generative model should correctly reason about spatial re-lationships and interactive actions. To that end, we pro-pose Narrator, a novel relationship reasoning-based gen-*Corresponding author erative approach using a conditional variation autoencoder for naturally controllable generation given a 3D scene and a textual description. Also, we model global and local spa-tial relationships in a 3D scene and a textual description respectively based on the scene graph, and introduce a part-level action mechanism to represent interactions as atomic body part states. In particular, benefiting from our relation-ship reasoning, we further propose a simple yet effective multi-human generation strategy, which is the first explo-ration for controllable multi-human scene interaction gen-eration. Our extensive experiments and perceptual studies show that Narrator can controllably generate diverse inter-actions and significantly outperform existing works.
1.

Introduction
Throughout daily life, humans constantly interact with their surroundings and these interactions establish their re-lationships with the scenes. Naturally controllable human-scene interaction (HSI) generation has significant value and numerous applications in areas such as VR/AR content cre-ation, human-centered AI, and generating training data for other computer vision tasks. In this paper, we tackle a chal-lenging task of generating realistic and plausible human-scene interactions from natural language textual descrip-tions, particularly exploring more liberal forms of HSIs with complex spatial relationships, multiple actions, and multi-ple persons, as shown in Fig. 1.
Prior HSI methods [36, 13, 31] mostly focus on the phys-ical geometry between humans and scenes, but lacks the se-mantic control of generation. Some works [30] further in-corporate generative controls, but always coarsely describe them as action labels, not sentences. A recent method,
COINS [37], specialises semantic control of interactions as combinations of actions and objects. However, additional manual effort is required to explicitly specify object in-stances when faced with multiple objects of the same kind.
Moreover, binding actions to objects by force is not intuitive or reasonable. For example, a natural case, “standing by the window”, does not contain a direct and explicit interaction object, and COINS cannot deal with it. These unnatural and constrained control ways fall short of meeting the needs of users and limit their applicability.
Humans usually naturally describe people who have di-verse interactions in different places through spatial per-ception and action recognition. Thus, an ideal generative model should correctly reason about spatial relationships to obtain the human position that respects textual descrip-tions while exploring degrees of freedom about interactive actions to generate natural interactions. Specifically, spa-tial relationships can be represented as the interrelationship among different objects in a scene or a local area, and inter-active actions are specified by atomic body part states, such as a person’s left feet treading, torso leaning, right hand tap-ping, and head bowing. How to reason about these relation-ships and utilize these powerful cues for naturally control-lable generation is a pressing problem.
To address these issues, we propose Narrator, a novel generative approach that incorporates a transformer-based conditional variational auto-encoder (cVAE) framework and leverages relationship reasoning to naturally produce diverse and plausible HSIs given the scene and textual de-scription. The diversity and complex interrelationship of objects in scenes can lead to misjudgements of human po-sition and unnatural interactions. Therefore, instead of un-derstanding scenes or specific objects in isolation as pre-vious works, we employ the scene graph to represent spa-tial relationships and propose a Joint Global and Local
Scene Graph (JGLSG) mechanism to provide global per-ception for subsequent localization, allowing for interac-tion generations guided by spatial relationships (in Fig. 1 (a)). As body part states are key for modeling realistic and text-faithful interactions, we introduce a Part-Level Ac-tion (PLA) mechanism to establish the correspondence be-tween human body parts and actions, allowing for interac-tion generations guided by multiple actions (in Fig. 1 (b)).
Ultimately, we feed the multi-modal features extracted by
JGLSG, PLA and PointNet++ [24] as a joint conditional embedding into cVAE, thus obtaining a unified latent space of the human body. To train and evaluate our approach, we annotate multi-level text descriptions from coarse to fine for each frame of the PROX dataset [12].
In real-world scenes, there are more situations where multiple people are interacting independently or in a con-nected way. Unfortunately, there is no work that solves this problem in an automatic and controlled way, but rather re-quires certain expertise and manual effort [18, 36]. Also, a straightforward way by using a single-person method like
COINS [37], i.e., sequential per-person generation and op-timization, does not properly understand multi-person text descriptions, leading to unreasonable spatial distributions and unnatural interactions of the generated results. In con-trast, benefiting from the flexibility and reusability of our
JGLSG and PLA mechanisms, we propose a simple yet effective multi-human generation strategy. We reason out each person’s interaction information from the text and globally update each generation to establish their relation-ships, thus achieving a better spatial distribution than simple multiple generation. To our knowledge, this is the first nat-urally controllable and user-friendly generative model for multi-human scene interaction (MHSI) (in Fig. 1 (c)).
In brief, our contributions can be summarized as:
• we present Narrator, a new generative method for nat-urally controllable human scene interaction generation given textual descriptions in natural language.
• we propose the JGLSG and PLA mechanisms for rela-tionship reasoning considering narrator’s perspective.
• we propose the first naturally controllable MHSI gen-eration strategy to approximate the real world. 2.