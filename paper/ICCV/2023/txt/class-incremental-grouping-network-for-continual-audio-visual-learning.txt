Abstract
Continual learning is a challenging problem in which models need to be trained on non-stationary data across se-quential tasks for class-incremental learning. While previ-ous methods have focused on using either regularization or rehearsal-based frameworks to alleviate catastrophic for-getting in image classification, they are limited to a sin-gle modality and cannot learn compact class-aware cross-modal representations for continual audio-visual learning.
To address this gap, we propose a novel class-incremental grouping network (CIGN) that can learn category-wise se-mantic features to achieve continual audio-visual learn-ing. Our CIGN leverages learnable audio-visual class to-kens and audio-visual grouping to continually aggregate class-aware features. Additionally, it utilizes class tokens distillation and continual grouping to prevent forgetting parameters learned from previous tasks, thereby improv-ing the model’s ability to capture discriminative audio-visual categories. We conduct extensive experiments on
VGGSound-Instruments, VGGSound-100, and VGG-Sound
Sources benchmarks. Our experimental results demonstrate that the CIGN achieves state-of-the-art audio-visual class-incremental learning performance. Code is available at https://github.com/stoneMo/CIGN . 1.

Introduction
The strong correspondence between audio signals and visual objects in the world enables humans to perceive the source of a sound, such as a meowing cat. This perception intelligence has motivated researchers to ex-plore audio-visual joint learning for various tasks, such as audio-visual event classification [60], sound source separa-tion [69, 17, 58], and visual sound localization [55, 12, 39].
In this work, we focus on the problem of classifying sound sources from both video frames and audio in a continual learning [28, 37] setting, i.e., train audio-visual learning models on non-stationary audio-visual pairs, enabling the model to classify sound sources in videos incrementally.
*Corresponding author, †Equal contribution.
Figure 1. Comparison of our CIGN with state-of-the-art ap-proaches on Average Accuracy (Top Row, higher is better) and
Forgetting (Bottom Row, lower is better) for continual audio-visual learning on VGGSound-Instruments [26], VGGSound-100 [13], and VGG-Sound Sources [13] benchmarks.
Continual learning is a recently popular and challenging problem that aims to train models on non-stationary data given sequential tasks for class-incremental learning. Pre-vious methods [28, 53, 34, 8, 65, 23, 51] mainly used ei-ther regularization or rehearsal-based approaches to allevi-ate catastrophic forgetting between old and new classes for image classification. A typical regularization-based work,
EWC [28] addressed catastrophic forgetting in neural net-works by training sequential models to protect the weights crucial for previous tasks. Similarly, LwF [34] used only new task data for training while preserving the original ca-pabilities. To improve the performance under challenging datasets, BiC [65] adopted two bias parameters in a linear model to address the bias to new classes. SS-IL [2] com-bined separated softmax with task-wise knowledge distilla-tion to solve this bias. However, those baselines are based on a single input modality and perform worse for continual audio-visual learning. In this work, we address a new multi-modal continual problem by extracting disentangled and compact representations with learnable audio-visual class-incremental tokens as guidance for continual learning.
Since prompts can learn task-specific knowledge [50, 31, 33, 25] for transfer learning, recent researchers have tried 1
to adapt diverse prompts-based pipelines to resolve contin-ual learning challenges. The basic idea of prompting is to design a function to alter the input text for pre-trained large language models such that the model can generate more informative features about the new task. For example,
L2P [63] leveraged a prompt pool memory space to instruct the model prediction and explicitly maintain model plastic-ity for task-invariant and task-specific knowledge. More re-cently, DualPrompt [62] proposed complementary prompts to instruct the pre-trained backbone for learning tasks arriv-ing sequentially. Despite their promising results, they can only deal with one modality, and the design of audio-visual prompts requires choreographed heuristics. When we ap-ply their prompting to our audio-visual settings, they cannot learn compact class-aware cross-modal representations for class-incremental learning.
The main challenge is that sounds are naturally mixed in the audio space such that the global audio representa-tion extracted from the sound is easy to be catastrophically forgotten by the cross-modal model. This inspires us to dis-entangle the individual semantics for old and current audio-visual pairs to guide continual audio-visual learning. To address the problem, our key idea is to disentangle individ-ual audio-visual representations from sequential tasks us-ing audio-visual continual grouping for class-incremental learning, which differs from existing regularization-based and rehearsal-based methods. During training, we aim to learn audio-visual class tokens to continually aggregate category-aware source features from the sound and the im-age, where separated high-level semantics for sequential audio-visual tasks are learned.
To this end, we propose a novel class-incremental group-ing network, namely CIGN, that can directly learn category-wise semantic features to achieve continual audio-visual learning. Specifically, our CIGN leverages learnable audio-visual class tokens and audio-visual grouping to continually aggregate class-aware features. Furthermore, it leverages audio-visual class tokens distillation and continual group-ing to alleviate forgetting parameters learned from previous tasks for capturing discriminative audio-visual categories.
Empirical experiments on VGGSound-Instruments,
VGGSound-100, and VGG-Sound Sources benchmarks comprehensively demonstrate the state-of-the-art perfor-mance against previous continual learning baselines, as shown in Figure 1.
In addition, qualitative visualizations of learned audio-visual embeddings vividly showcase the effectiveness of our CIGN in aggregating class-aware fea-tures to avoid cross-modal catastrophic forgetting. Exten-sive ablation studies also validate the importance of class-token distillation and continual grouping in learning com-pact representations for class-incremental learning.
Our contributions can be summarized as follows:
• We present a novel class-incremental grouping net-work, namely CIGN, that can directly learn category-wise semantic features to achieve continual audio-visual learning.
• We introduce learnable audio-visual class tokens distil-lation and continual grouping to continually aggregate class-aware features for alleviating cross-modal catas-trophic forgetting.
• Extensive experiments can demonstrate the state-of-the-art superiority of our CIGN over previous base-lines on audio-visual class-incremental scenarios. 2.