Abstract
In this paper, we strive to answer the question ‘how to collaboratively learn convolutional neural network (CNN)-based and vision transformer (ViT)-based models by select-ing and exchanging the reliable knowledge between them for semantic segmentation?’ Accordingly, we propose an online knowledge distillation (KD) framework that can si-multaneously learn compact yet effective CNN-based and
ViT-based models with two key technical breakthroughs to take full advantage of CNNs and ViT while compensating their limitations. Firstly, we propose heterogeneous fea-ture distillation (HFD) to improve students’ consistency in low-layer feature space by mimicking heterogeneous fea-tures between CNNs and ViT. Secondly, to facilitate the two students to learn reliable knowledge from each other, we propose bidirectional selective distillation (BSD) that can dynamically transfer selective knowledge. This is achieved by 1) region-wise BSD determining the directions of knowl-edge transferred between the corresponding regions in the feature space and 2) pixel-wise BSD discerning which of the prediction knowledge to be transferred in the logit space.
∗Corresponding author.
Extensive experiments on three benchmark datasets demon-strate that our proposed framework outperforms the state-of-the-art online distillation methods by a large margin, and shows its efficacy in learning collaboratively between ViT-based and CNN-based models. 1.

Introduction
Semantic segmentation [6, 25, 39] is a crucial and chal-lenging vision task, which aims to predict a category la-bel for each pixel in the input image. Although the state-of-the-art (SoTA) segmentation methods have achieved re-markable performance, they often require prohibitive com-putational costs. This limits their applications to resource-limited scenarios, e.g., autonomous driving [12]. Conse-quently, growing attention has been paid to model com-It pression aiming at obtaining more compact networks. can be roughly divided into quantization [10, 13, 38], prun-ing [4, 26, 32], and knowledge distillation (KD) [27, 30, 33].
The standard KD paradigm aims to learn a compact yet ef-fective student model under the guidance of a high-capacity teacher model. For instance, CD [29] proposes a channel-wise KD approach by normalizing the activation map of
each channel. IFVD [36] characterizes the intra-class fea-ture variation (IFV) and makes the student model mimic the
IFV of the teacher model.
Recently, vision transformer (ViT) achieves compara-ble or even better performance than that of CNNs thanks to the computing paradigm, e.g., multi-head self-attention (MHSA). For instance, PVT [34, 35] and Swin Transformer
[9,24] extract the pyramid features from the high-resolution images and achieve SoTA performance on various bench-marks. To minimize the model complexity, SegFormer [39] proposes a hierarchically structured transformer encoder to learn a simple yet efficient ViT-based model.
In this paper, we strive to collaboratively learn com-pact yet effective CNN-based and ViT-based models for semantic segmentation.
Intuitively, we explore an online
KD paradigm for this goal. Existing online KD meth-ods for classification employ a ‘Dual-Student’ framework (without the pre-trained model) by enabling the students to learn from each other in a one-stage learning manner
[1, 14, 31, 42]. For example, Deep Mutual Learning (DML)
[42] proposes to make the CNN-based students teach each other in the training process. KDCL [14] enables students with different capacities to learn collaboratively to gener-ate reliable soft supervision and boost their classification performance. However, naively applying these CNN-based
KD methods is less effective and even leads to performance drops (see Fig. 1 (a)). The reasons are that: 1) The discrep-ancies in the feature and prediction space between CNNs and ViT caused by the distinct computing paradigms make it challenging to perform online KD. 2) These methods only transfer knowledge in the logit space while more reliable and informative knowledge does exist in the feature space. 3) There are considerable model size gap and learning ca-pacity gap between CNNs and ViT. Intuitively, we ask a
‘how to collaboratively learn CNN-based and question:
ViT-based models by selecting and exchanging the reliable knowledge between them for semantic segmentation?’
In light of this, we propose, to the best of our knowl-the first online KD strategy to further push the edge, limit of CNNs and ViT for semantic segmentation (See
Fig. 1 (b)). Our method enjoys two key technical break-throughs. Firstly, we propose heterogeneous feature distil-lation (HFD) to make the students learn the heterogeneous features from each other for complementary knowledge in the low-layer feature space. Concretely, the ViT-based stu-dent takes the low-level features from the CNN-based stu-dent as guidance and vice versa. Then, consistency be-tween the low-layer features of CNN-based and ViT-based students is imposed to encourage them to compensate for their limitations. Secondly, to transfer reliable knowledge between CNNs and ViT, we propose a bidirectional selec-tive distillation (BSD) module that selectively distills the re-liable region-wise and pixel-wise knowledge. Specifically, the region-wise distillation dynamically transfers reliable knowledge of regions in the feature space by determining the directions of transferring knowledge. Similarly, pixel-wise distillation discerns which of the prediction knowledge to be transferred in the logit space. Note that these bidirec-tional distillation approaches are both guided by the cross entropy between predictions and ground-truth (GT) labels.
In summary, our main contributions are four-fold: (I)
We introduce the first online collaborative learning strat-egy to collaboratively learn compact ViT-based and CNN-based models for semantic segmentation. (II) We propose
HFD to facilitate CNNs and ViT learning global and local feature representations correspondingly. (III) We propose
BSD to distill knowledge between ViT and CNNs in the feature and logit spaces. (IV) Our proposed method con-sistently achieves new state-of-the-art performance on three benchmark datasets for semantic segmentation. 2.