Abstract
In this paper, we target at the problem of learning a generalizable dynamic radiance ﬁeld from monocular videos. Different from most existing NeRF methods that are based on multiple views, monocular videos only contain one view at each timestamp, thereby suffering from ambi-guity along the view direction in estimating point features and scene ﬂows. Previous studies such as DynNeRF dis-ambiguate point features by positional encoding, which is not transferable and severely limits the generalization abil-†: Corresponding author. ity. As a result, these methods have to train one inde-pendent model for each scene and suffer from heavy com-putational costs when applying to increasing monocular videos in real-world applications. To address this, We propose MonoNeRF to simultaneously learn point features and scene ﬂows with point trajectory and feature corre-spondence constraints across frames. More speciﬁcally, we learn an implicit velocity ﬁeld to estimate point tra-jectory from temporal features with Neural ODE, which is followed by a ﬂow-based feature aggregation module to obtain spatial features along the point trajectory. We jointly optimize temporal and spatial features in an end-to-end manner. Experiments show that our MonoNeRF is 1
able to learn from multiple scenes and support new ap-plications such as scene editing, unseen frame synthesis, and fast novel scene adaptation. Codes are available at https://github.com/tianfr/MonoNeRF. 1.

Introduction
Novel view synthesis [6] is a highly challenging prob-lem.
It facilitates many important applications in movie production, sports event, and virtual reality. The long stand-ing problem recently has witnessed impressive progress due to the neural rendering technology [28, 26, 9]. Neural Ra-diance Field (NeRF) [28, 54, 22, 27, 46, 51, 52] shows that photo-realistic scenes can be represented by an implicit neural network. Concretely, taken as a query the position and viewing direction of the posed image, the network out-puts the color of each pixel by volume rendering method.
Among these approaches, it is supposed that the scene is static and can be observed from multiple views at the same time. Such assumptions are violated by numerous videos uploaded to the Internet, which usually contain dynamic foregrounds, recorded by the monocular camera.
More recently, some studies aim to explore how to learn dynamic radiance ﬁeld from monocular videos [10, 13, 21, 49, 50, 40]. Novel view synthesis from monocular videos is a challenging task. As foreground usually dynamically changes in a video, there is ambiguity in the view direc-tion to estimate precise point features and dense object mo-tions (i.e., scene ﬂow [44]) from single views.
In other words, we can only extract the projected 2D intra-frame lo-cal features and inter-frame optical ﬂows, but fail to obtain precise 3D estimations. Previous works address this chal-lenge by representing points as 4D position features (co-ordinate and time), so that the learned positional encoding provides speciﬁc information for each 3D point in the space
[10, 13, 21, 49, 50, 40]. Based on positional encoding, these methods make efforts on exploiting scene priors [12, 48, 49] or adding spatio-temporal regularization [10, 13, 21, 50] to learn a more accurate dynamic radiance ﬁeld.
However, while positional encoding successfully dis-ambiguates 3D points from monocular 2D projections, it severely overﬁts to the training video clip and is not trans-ferable. Therefore, existing positional encoding based methods have to optimize one independent model for each dynamic scene. With the fast increase of monocular videos in reality, they suffer from heavy computational costs and lengthy training time to learn from multiple dynamic scenes. Also, the lack of generalization ability limits fur-ther applications of scene editing which requires interaction among different scenes. A natural question is raised: can we learn a generalizable dynamic radiance ﬁeld from monocu-lar videos?
In this paper, we provide a positive answer to this ques-tion. The key challenge of this task is to learn to extract generalizable point features in the 3D space from monocu-lar videos. While independently using 2D local features and optical ﬂows suffers from ambiguity along the ray direction, they provide complementary constraints to jointly learn 3D point features and scene ﬂows. On the one hand, for the sampled points on each ray, optical ﬂow provides generaliz-able constraints that limit the relations of their point trajec-tories. On the other hand, for the ﬂowing points on each es-timated point trajectory, we consider that they should share the same point features. We estimate each point feature by aggregating their projected 2D local features, and design feature correspondence constraints to correct unreasonable trajectories.
To achieve this, we propose MonoNeRF to build a generalizable dynamic radiance ﬁeld for multiple dynamic scenes. We hypothesize that a point moving along its tra-jectory over time keeps the consistent point feature. Our method concurrently predicts 3D point features and scene
ﬂows with point trajectory and feature correspondence con-straints in monocular video frames. More speciﬁcally, we
ﬁrst propose to learn an implicit velocity ﬁeld that encodes the speed from the temporal feature of each point. We su-pervise the velocity ﬁeld with optical ﬂow and integrate continuous point trajectories on the ﬁeld with Neural ODE
[5]. Then, we propose a ﬂow-based feature aggregation module to sample spatial features of each point along the point trajectory. We incorporate the spatial and temporal features as the point feature to query the color and density for image rendering and jointly optimize point features and trajectories in an end-to-end manner. As shown in Figure 1, experiments demonstrate that our MonoNeRF is able to ren-der novel views from multiple dynamic videos and support new applications such as scene editing, unseen frame syn-thesis, and fast novel scene adaption. Also, in the widely-used setting of novel view synthesis on training frames from single videos, our MonoNeRF still achieves better perfor-mance than existing methods despite that cross-scene gen-eralization ability is not required in this setting. 2.