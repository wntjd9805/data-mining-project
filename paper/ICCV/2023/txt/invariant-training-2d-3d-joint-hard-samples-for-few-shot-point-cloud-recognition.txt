Abstract
We tackle the data scarcity challenge in few-shot point cloud recognition of 3D objects by using a joint prediction from a conventional 3D model and a well-trained 2D model.
Surprisingly, such an ensemble, though seems trivial, has hardly been shown effective in recent 2D-3D models. We find out the crux is the less effective training for the “joint hard samples”, which have high confidence prediction on different wrong labels, implying that the 2D and 3D models do not collaborate well. To this end, our proposed invariant training strategy, called INVJOINT, does not only empha-size the training more on the hard samples, but also seeks the invariance between the conflicting 2D and 3D ambigu-ous predictions.
INVJOINT can learn more collaborative 2D and 3D representations for better ensemble. Extensive experiments on 3D shape classification with widely adopted
ModelNet10/40, ScanObjectNN and Toys4K, and shape re-trieval with ShapeNet-Core validate the superiority of our
INVJOINT. Codes will be publicly Available 1.
Figure 1. Comparisons of our framework with existing 2D-3D methods, which can be categorized into (a) Directly projecting point cloud into multi-view images as inputs, and then fine-tuning the 2D models with a frozen backbone. (b) Indirectly leveraging the 2D pretrained knowledge as a constraint or supervision, trans-ferring them via knowledge distillation or contrastive learning, and then only using the optimized 3D pathway for prediction. (c) In contrast, our INVJOINT, based on ensemble paradigm, makes the best of the 2D and 3D worlds by joint prediction in inference. 1.

Introduction
As the point cloud representation of a 3D object is irregularly distributed, and unstructured, a deep sparse, recognition model requires much more training data than the 2D counterpart [15, 19]. Not surprisingly, this makes few-shot learning even more challenging, such as rec-ognizing a few newly-collected objects in AR/VR dis-play [20, 49] and robotic navigation [1]. Thanks to the re-cent progress of large-scale pre-trained multi-modal foun-dation models [39, 25, 33], the field of 2D few-shot or zero-shot recognition has experienced significant improvements.
Therefore, as shown in Figure 1(a), a straightforward solu-1https://github.com/yxymessi/InvJoint tion for 3D few-shot recognition is to project a point cloud into a set of multi-view 2D images [10], through rendering and polishing [54], and then directly fed the images into a well-trained 2D model [67].
Although effective, the projected images are inevitably subject to incomplete geometric information and rendering artifacts. To this end, as shown in Figure 1(b), another pop-ular solution attempts to take the advantage of both 2D and 3D by transferring the 2D backbone to the 3D counterpart via knowledge distillation [63], and then use the 3D path-way for final recognition. So far, you may ask: as the data in few-shot learning is already scarce, during inference time, why do we still have to choose one domain or the other?
Isn’t it common sense to combine them for better predic-Figure 2. diagonal. (b) Qualitative examples of joint hard samples with their logits distribution. (a) 3D and 2D models are confused by different classes, thus a simple late fusion cannot turn the joint confusion matrix more tion? In fact, perhaps for the same reason, the commu-nity avoids answering the question—our experiments (Sec-tion 4) show that a naive ensemble, no matter with early or late fusion, is far from being effective as it only brings marginal improvement.
To find out the crux, let’s think about in what cases, the ensemble can correct the individually wrong predictions by joint prediction, e.g., if the ground-truth is “Bench” and neither 2D nor 3D considers “Bench” as the highest con-fidence, however, their joint prediction peaks at “Bench”.
The cases are: 1) the ground-truth confidence of the two models cannot be too low, and 2) that of the other classes cannot be too high. In one word, 2D and 3D are collabora-tive. However, as shown by the class confusion matrices of training samples in Figure 2(a), since 2D and 3D are con-fused by different classes, their ensemble can never turn the matrix into a more diagonal one. This implies that their joint prediction in inference may be still wrong.
Therefore, the key is to make the joint confusion matrix more diagonal than each one. To this end, we focus on the joint hard samples, which have high confidence predic-tion on different wrong labels respectively. See Figure 2(b) for some qualitative examples, exhibiting a stark difference in logits distribution among modalities. However, simply re-training them like the conventional hard negative min-ing [47, 43] is less effective because the joint training is easily biased towards the “shortcut” hard samples in one domain. For example, if the 3D model has a larger training loss than 2D, probably due to a larger sample variance [70], which is particularly often in few-shot learning, the joint training will only take care of 3D, leaving 2D still or even
In Section 5, we provide a perspective more confused. on joint hard samples from the view of probability theory, while the Venn Diagram perspective in Appendix.
By consolidating the idea of making use of joint hard examples for improving few-shot point cloud recognition, we propose an invariant training strategy. As illustrated in
Figure 3, if a sample ground-truth is “Bench” and 2D pre-diction is confused between “Bench” and “Chair”, while the 3D counterpart is uncertain about “Bench” and “Airplane”, the pursuit of invariance will remove the variant “Chair” and “Airplane’, and eventually keep the common “Bench” in each model. Specifically, we implement the strategy as
INVJOINT, which has two steps to learn more collabora-tive 2D and 3D representations (Section 3.2). Step 1: it selects those joint hard samples by firstly fitting a Gaussian
Mixture Model of sample-wise loss, and then picking them according to the fused logit distribution. Step 2: A joint learning module focusing on the selected joint hard samples effectively capture the collaborative representation across domains through an invariant feature selector. After the IN-VJOINT training strategy, a simple late-fusion technique can be directly deployed for joint prediction in inference (Sec-tion 3.4). Figure 2(a) shows that the joint confusion matrix of training data is significantly improved after INVJOINT.
We conduct extensive few-shot experiments on several synthetic [58, 44, 4] and real-world [51] point cloud 3D classification datasets.
INVJOINT gains substantial im-provements over existing SOTAs. Specifically, on Model-Net40, it achieves an absolute improvements of 6.02% on average and 15.89% on 1-shot setting compared with Point-CLIP [67]. In addition, the ablation studies demonstrate the component-wise contributions of INVJOINT. In summary, we make three-fold contributions:
• We propose INVJOINT that aims to make the best of the 2D and 3D worlds. To the best of our knowledge, it is the first work that makes 2D-3D ensemble work in
point cloud 3D few-shot recognition.
• We attribute the ineffective 2D-3D ensemble to the
“joint hard samples”. INVJOINT exploits their 2D-3D conflicts to remove the ambiguous predictions.
• INVJOINT is a plug-and-play training module whose potential could be further unleashed with the evolving backbone networks. 2.