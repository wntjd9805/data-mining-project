Abstract
In this work we introduce S-TREK, a novel local feature extractor that combines a deep keypoint detector, which is both translation and rotation equivariant by design, with a lightweight deep descriptor extractor. We train the S-TREK keypoint detector within a framework inspired by reinforce-ment learning, where we leverage a sequential procedure to maximize a reward directly related to keypoint repeatabil-ity. Our descriptor network is trained following a “detect, then describe” approach, where the descriptor loss is eval-uated only at those locations where keypoints have been selected by the already trained detector. Extensive experi-ments on multiple benchmarks confirm the effectiveness of our proposed method, with S-TREK often outperforming other state-of-the-art methods in terms of repeatability and quality of the recovered poses, especially when dealing with in-plane rotations. 1.

Introduction
Being able to find point correspondences between images has been of paramount importance since the early days of computer vision. In fact, a wide range of applications, such as Structure from Motion (SfM) [35, 1], Visual Localization
[40, 33], SLAM [23, 2], object recognition [24] and object tracking [47] rely on image-to-image point correspondences.
After decades where SIFT [19], SURF [6], ORB [30] and many other hand-engineered feature extractors have been ubiquitous, the community has recently experienced a fast shift toward learned methods, with several ones based on deep architectures [12, 11, 27, 44]. While many of these newly proposed methods show remarkable matching perfor-mances, the commonly used multi-layer convolutional archi-tecture lacks one of the properties that most hand-engineered keypoint detectors have by design: rotation equivariance.
Rather unexpectedly, modern detectors show poor perfor-mances when the input image undergoes in-plane rotations unless specifically trained to handle this transformation. In (a) DISK [44] (b) REKD [17] (c) S-TREK (ours)
Figure 1: Qualitative comparison with two state-of-the-art feature extraction methods on the Image Matching Bench-45° rotated version of it (bot-mark [15] (top) and on our
± tom). RANSAC inlier matches are color coded from green to yellow, representing reprojection errors equal to zero and 5px, respectively; the outlier matches are in red. order to avoid this pitfall, we take advantage of the recent developments in the field of group-equivariant networks
[9, 10] and design the keypoint detector of our novel feature extractor method, named S-TREK, to make use of rotation-equivariant convolutional layers. This makes our keypoints independent of the image orientation by design, regardless of the dataset used for training.
The detection of keypoints in an image is a hard selection process, where a finite set of locations is selected. Because of the non-differentiability of this process, some keypoint detection methods in the literature resort to training with with proxy losses, which are applied to the entire detection heatmaps at the network output [27, 31], while others train for keypoints and descriptors jointly [12, 20]. To train di-(a) Keypoint detector. (b) Descriptor extractor.
Figure 2: Overall architecture of the S-TREK feature extractor. rectly with the keypoints location, we propose a training framework inspired by reinforcement learning, which per-mits to maximize a reward formulation directly related to the keypoints repeatability. Following [44, 7], we frame the keypoint detection as a probabilistic process, and present a novel sequential sampling procedure that does not suffer from the limitations of their sampling schemes.
In contrast to the more recent “detect and describe” ap-proach, where keypoint locations and descriptors are learnt jointly using a shared backbone for both tasks [27, 12, 44, 31, 7], we follow a “detect, then describe” paradigm using two different networks for the two tasks, and train the descriptors only after the detector has been trained. This allows us to design the two networks to have different properties.
Recent studies have also explored new research directions, utilizing deep matching architectures like SuperGlue [32], or developing methods able to find pointwise correspondences directly from image pairs such as LoFTR [38]. However, these methods require execution for each individual image pair, which limits their applicability in scenarios where the computational resources are constrained, or when dealing with a large number of images. For these reasons, and the additional benefit of an easier integration in existing systems, local feature extraction methods remain the most popular approach in many applications.
In summary, our main contributions are:
• We propose S-TREK, a local feature extractor that com-bines a deep translation and rotation equivariant key-point detector with a lightweight descriptor extractor.
Our network is trained from scratch following a “detect, then describe” approach.
• We propose a keypoint detector training framework, inspired by reinforcement learning, based on a novel reward formulation that maximizes the repeatability metric directly. Additionally, we propose a novel se-quential keypoint probability sampling strategy that overcomes the limitations of previous approaches.
• Extensive experiments show that the S-TREK detec-tor achieves state-of-the-art repeatability on multiple benchmarks. Moreover, when equipped with our lightweight features extractor network, S-TREK pro-vides features that are well suited for recovering ac-curate camera poses, especially when dealing with in-plane rotation. 2.