Abstract
Cross-modal garment synthesis and manipulation will significantly benefit the way fashion designers generate gar-ments and modify their designs via flexible linguistic inter-faces. However, despite the significant progress that has been made in generic image synthesis using diffusion mod-els, producing garment images with garment part level se-mantics that are well aligned with input text prompts and then flexibly manipulating the generated results still re-mains a problem. Current approaches follow the general text-to-image paradigm and mine cross-modal relations via simple cross-attention modules, neglecting the structural correspondence between visual and textual representations in the fashion design domain. In this work, we instead intro-duce DiffCloth, a diffusion-based pipeline for cross-modal
*Equal contribution. †Corresponding author. garment synthesis and manipulation, which empowers dif-fusion models with flexible compositionality in the fashion domain by structurally aligning the cross-modal semantics.
Specifically, we formulate the part-level cross-modal align-ment as a bipartite matching problem between the linguistic
Attribute-Phrases (AP) and the visual garment parts which are obtained via constituency parsing and semantic seg-mentation, respectively. To mitigate the issue of attribute confusion, we further propose a semantic-bundled cross-attention to preserve the spatial structure similarities be-tween the attention maps of attribute adjectives and part nouns in each AP. Moreover, DiffCloth allows for manipu-lation of the generated results by simply replacing APs in the text prompts. The manipulation-irrelevant regions are recognized by blended masks obtained from the bundled at-tention maps of the APs and kept unchanged. Extensive ex-periments on the CM-Fashion benchmark demonstrate that
DiffCloth both yields state-of-the-art garment synthesis re-sults by leveraging the inherent structural information and supports flexible manipulation with region consistency. 1.

Introduction
Leveraging artificial intelligence to generate and alter garment images based on control signals from a variety of modalities has the potential to revolutionize the fashion design process. Particularly, cross-modal garment synthe-sis [6, 13, 14, 18, 19, 28, 39] and manipulation by linguis-tic interfaces have gradually attracted increasing attention from the academic community. Unfortunately, the visual semantics in the fashion domain are different from those in generic image generation tasks due to its inherent struc-tural property, e.g. each type of garment has a distinct shape and can be partitioned into several garment parts. However, existing work [6, 13, 14, 18, 19, 28, 39] on cross-modal garment synthesis are primarily built on two-stage pipelines of generic generative transformers and ignore the structural correspondences between the garment images and the input text prompts. This leads to imprecise cross-modal semantic alignment and poor semantic compositionality.
Given the recent success of diffusion models [23, 26, 29, 31], which provide flexible control of the generative pro-cess through guidance mechanisms, departing from prior approaches and leveraging diffusion models appears a nat-ural approach. However, we observe the following two se-mantic issues when applying state-of-the-art text-based im-age generation models to the fashion domain: 1) Garment
Part Leakage, where one or more of the garment parts de-scribed in the prompt are not actually generated in the im-age; and 2) Attribute Confusion, where the attributes and the garment parts are wrongly paired or some attributes are ignored in the generated image. Examples of the aforemen-tioned issues are provided in Fig. 2. In Fig. 2(a), examples of garment part leakage are provided, where the model fails to generate the pockets in the dusty rose jacket and the but-ton fastening in the blue shirt. In Fig. 2(b), examples of attribute confusion are provided, where the color attributes
‘blue’ and ‘brown’ bind to the incorrect garment parts and the ‘plain white’ attribute is missing in the striped shirt.
To solve the above issues, we propose DiffCloth, a dif-fusion model with structural semantic consensus guidance to achieve accurate fine-grained part-level semantic align-ment. To be specific, a semantic segmentor is trained to explore the visual structure and divide the visual gar-ment into part-level images, e.g., sleeves, body piece, hood, etc. Additionally, a constituency parsing tree is lever-aged as a linguistic structural parser to extract the col-lection of Attribute-Phrases. By formulating the cross-modal semantic alignment as a bipartite matching prob-lem between these two sets of semantic components, we
Figure 2. Three typical issues of garment synthesis and manipula-tion. (a) Garment Part Leakage: one or more of the garment parts described in the prompt are not accurately generated; (b) Attribute
Confusion: the attributes and garment parts are wrongly paired or some attributes are ignored; (c) Region Inconsistency in Manipu-lation: the manipulation-irrelevant regions are carelessly modified. introduce a Hungarian matching loss as the summation of
CLIP-similarities [24] between the part-level images and the Attribute-Phrases. This Hungarian matching loss can be used to guide the diffusion model to achieve structural con-sensus across images and text. Furthermore, we propose a semantic-bundled cross-attention module to avoid the afore-mentioned attribute confusion issue. Specifically, we ob-serve that the attention maps of the attribute adjective and the garment part nouns are different when attribute confu-sion occurs while they share similar spatial structures when attributes are matched to the correct garment parts. Hence, we propose to preserve the spatial structure similarity be-tween the attribute adjective and the garment part subject in the cross-attention module by a semantic-bundled loss, which aims to minimize the Jensen-Shannon divergence [8] between these two maps. This semantic-bundle loss is also utilized to guide the sampling process of DiffCloth.
In order to further allow easy manipulation of the gen-erated images, DiffCloth introduces a mechanism to ma-nipulate input images based solely on changes in the in-put text prompt and, unlike prior approaches [1, 21], does not require explicit masking of the areas that should be changed. By injecting the cross-attention maps during the diffusion steps, DiffCloth can automatically find which pix-els should be attended to and should be modified. When for instance changing the attribute, e.g. “long sleeve” →
“short sleeve”, only the cross-attention maps of the bundled
Attribute-Phrase need to be changed and the attention maps of other textual tokens can be frozen. Moreover, we pro-pose a consistency loss to prevent irrelevant content from
being carelessly edited. An example of unexpected region inconsistency is given in Fig.2 (c), where the blue belt is wrongly modified to a red one. The consistency loss is fur-ther designed to preserve the pixel-level consistency of the exclusive area indicated by the attention map of the changed tokens. Comprehensive experiments on the CM-Fashion benchmark demonstrate that DiffCloth yields state-of-the-art generation results in garment synthesis and further sup-ports flexible manipulation by editing the text prompt in a user-friendly manner.
Our main contributions are summarized as follows:
• We propose a structural semantic consensus guidance to address the structural semantic alignment across vi-sual garments and linguistic attribute-phrases as a bi-partite matching problem via the Hungarian algorithm.
• We propose a new semantic-bundled cross-attention, which encourages spatial structure similarity between the cross-attention maps of attributes and part subjects, to alleviate attribute confusion issues.
• We introduce a region consistency mechanism to pre-vent irrelevant content from being modified during gar-ment manipulation.
• Extensive experiments on the CM-Fashion benchmark verify the superiority of DiffCloth, particularly in terms of accurate text-image alignment for both gar-ment synthesis and manipulation. 2.