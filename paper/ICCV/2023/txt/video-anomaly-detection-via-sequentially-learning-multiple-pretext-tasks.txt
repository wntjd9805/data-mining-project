Abstract
Learning multiple pretext tasks is a popular approach to tackle the nonalignment problem in unsupervised video anomaly detection. However, the conventional learning method of simultaneously learning multiple pretext tasks, is prone to sub-optimal solutions, incurring sharp perfor-mance drops. In this paper, we propose to sequentially learn multiple pretext tasks according to their difficulties in an as-cending manner to improve the performance of anomaly de-tection. The core idea is to relax the learning objective by starting with easy pretext tasks in the early stage and grad-ually refine it by involving more challenging pretext tasks later on. In this way, our method is able to reduce the diffi-culties of learning and avoid converging to sub-optimal so-lutions. Specifically, we design a tailored sequential learn-ing order for three widely-used pretext tasks. It starts with frame prediction task, then moves on to frame reconstruc-tion task and last ends with frame-order classification task.
We further introduce a new contrastive loss which makes the learned representations of normality more discriminative by pushing normal and pseudo-abnormal samples apart. Ex-tensive experiments on three datasets demonstrate the effec-tiveness of our method. 1.

Introduction
Existing unsupervised video anomaly detection methods train models to perform a single pretext task such as frame reconstruction [14] or frame prediction [21], and they can discriminate anomalies when videos are significantly de-viant from model expectations. These methods often render sub-optimal performances due to the nonalignment [17] be-tween the single pretext task and video anomaly detection.
Recent methods [9, 17] resort to multiple pretext tasks to tackle the nonalignment problem, as multiple pretext tasks
*Corresponding author: Che Sun (a) (b)
Figure 1. AUC (%) performances of anomaly detection when learning multiple pretext tasks simultaneously and se-quentially. The dotted line denotes simultaneously learning multiple pretext tasks. The solid lines denote sequentially learning multiple pretext tasks. The learning order of our method is “prediction (Pre) - reconstruction (Rec) - clas-sification (Cls)”. The learning orders of Sequential-1 and
Sequential-2 are “Pre-Rec-Cls” and “Rec-Cls-Pre” respec-tively. AUC performances on Avenue [24] and Ped2 [28] datasets show that the learning methods and the learning orders of multiple pretext tasks significantly influences the trained model’s ability for video anomaly detection. can provide more comprehensive and informative guidance than one single pretext task. The learning method of multi-ple pretext tasks is significant, yet under-explored. Conven-tional learning method, i.e., simultaneously learning multi-ple pretext tasks, could not bring about the expected perfor-mance gains and even cause sharp performance drops. An example is shown in Fig. 1. The performance curves (the dotted lines in blue) plummet or fluctuate with more pre-text tasks, and neither of the performance curves reaches the summit when leveraging all pretext tasks. The main reason is that models tend to get stuck in pareto-optimal points when simultaneously learning multiple pretext tasks.
The pareto-optimal points [7, 34, 12] are such points that we could not further optimize any of the pretext task ob-jectives without compromising the rest, striking a trade-off among them. Pareto-optimal points are not necessarily good solutions for video anomaly detection. Converging to such points brings the learning process to an early stop and impedes models from bringing about the expected perfor-mance gains of multiple pretext tasks.
In this paper, we propose to sequentially learn multiple pretext tasks for video anomaly detection, and our method is able to provide continual optimization directions, which avoids converging to pareto-optimal points. We arrange the sequential learning order of multiple pretext tasks accord-ing to their difficulties in an ascending manner. The diffi-culty of a task refers to the difficulty of transferring knowl-edge learned from this task to improve the performance of anomaly detection. Easy tasks usually bring about more performance gains while difficult tasks not. Essentially, our method first relaxes the learning objective of anomaly detection to that of an easy pretext task, and then gradu-ally refines it with more challenging pretext tasks along the learning process. In this way, our model is able to gradu-ally transfer knowledge learned from pretext tasks to model anomalies from coarse to fine, which encourages our model to explore better solutions for video anomaly detection. As shown in Fig. 1, our learning method displays superiority over other sequential learning orders, because they do not consider the difficulties of pretext tasks.
We select three widely-used tasks to model temporal and spatial normality in our learning method. Our method starts with the frame reconstruction task (Rec), then goes on to learn the frame prediction task (Pre) and at last learns the frame order classification task (Cls). We introduce a con-trastive loss to push in-order (i.e., positive samples) and out-of-order (i.e., negative samples) inputs apart, which con-strains the latent encoding space to achieve better discrimi-nation for models to classify them. We evaluate our meth-ods on three datasets, Avenue [24], ShanghaiTech [27] and
UCSD Ped2 [28]. Extensive experiments demonstrate the effectiveness of our method.
In summary, our contributions are two-fold.
• As far as we know, our method is the first attempt to sequentially learn multiple pretext tasks according to their difficulties in an ascending manner, which brings about the expected performance gains in video anomaly detection.
• We introduce a new contrastive loss to constrain the latent space, which grants the trained model with better discrimination for classifying anomalies. 2.