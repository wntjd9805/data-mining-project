Abstract
We introduce the task of localizing a flexible number of objects in real-world 3D scenes using natural language de-scriptions. Existing 3D visual grounding tasks focus on localizing a unique object given a text description. How-ever, such a strict setting is unnatural as localizing poten-tially multiple objects is a common need in real-world sce-narios and robotic tasks (e.g., visual navigation and ob-ject rearrangement). To address this setting we propose
Multi3DRefer, generalizing the ScanRefer dataset and task.
Our dataset contains 61926 descriptions of 11609 objects, where zero, single or multiple target objects are referenced by each description. We also introduce a new evaluation metric and benchmark methods from prior work to enable further investigation of multi-modal 3D scene understand-ing. Furthermore, we develop a better baseline leveraging 2D features from CLIP by rendering object proposals on-line with contrastive learning, which outperforms the state of the art on the ScanRefer benchmark. 1.

Introduction
There is growing interest in multi-modal methods that connect language and vision, tackling tasks such as im-age captioning, visual question answering, text-to-image retrieval, and generation. One fundamental task is visual grounding where natural language text queries are linked to regions of an image or 3D scene. While the problem of vi-sual grounding has been well studied in 2D images, there are fewer datasets and methods studying the problem in 3D scenes. Being able to indicate the object that the text “the first of four stools” references in a 3D scene is useful for ap-plications in robotics, AR/VR, and online 3D environments where we have access to not just a static image, but a 3D scene. Work on 3D datasets for visual grounding [7, 4] has spurred the development of methods for 3D visual ground-ing [41, 63, 60, 22, 21, 2, 58, 7] and the inverse task of 3D captioning [7], as well as unified methods that tackle both [8, 6, 9].
Figure 1: We introduce Multi3DRefer, a dataset and task where there are potentially multiple target objects for a given description.
In ScanRefer [7] (left), the descrip-tion corresponds to exactly one object (blue box), while in
Multi3DRefer (right), there are multiple target objects.
However, existing datasets and tasks [7, 4] are designed with the assumption that there is a unique target object when performing visual grounding in a 3D scene. This assump-tion makes ambiguous descriptions that may refer to multi-ple objects problematic (see Fig. 1). Furthermore, this ex-plicitly discourages visual grounding methods from demon-strating generalization to similar object instances on the ba-sis of common features of the objects (e.g., similar size, color, texture) and spatial relations between objects (e.g., first in a row left-to-right or right-to-left).
We address these shortcomings with an enhanced dataset and task that we call Multi3DRefer where a flexible num-ber of target objects (zero, single or multiple) in a 3D scene are localized given language descriptions. We modify and enhance language data from ScanRefer [7] and propose evaluation metrics to benchmark prior work and a CLIP-based method that we propose on the flexible number vi-sual grounding task. In summary, we make the following contributions: 1) generalize 3D visual grounding to a flexi-ble number of target objects given natural language descrip-tions. 2) create an enhanced dataset based on ScanRefer [7]
with augmentations from ChatGPT1, consisting of 61926 descriptions in 800 ScanNet V2 [15] scenes. 3) bench-mark three prior 3D visual grounding approaches adapted to Multi3DRefer 4) design an end-to-end approach leverag-ing CLIP [47] embeddings and online rendering of object proposals with contrastive learning. 2.