Abstract
Flow-based propagation and spatiotemporal Transformer are two mainstream mechanisms in video inpainting (VI).
Despite the effectiveness of these components, they still suffer from some limitations that affect their performance. Previous propagation-based approaches are performed separately ei-ther in the image or feature domain. Global image propaga-tion isolated from learning may cause spatial misalignment due to inaccurate optical flow. Moreover, memory or compu-tational constraints limit the temporal range of feature prop-agation and video Transformer, preventing exploration of correspondence information from distant frames. To address these issues, we propose an improved framework, called
ProPainter, which involves enhanced ProPagation and an ef-ficient Transformer. Specifically, we introduce dual-domain propagation that combines the advantages of image and fea-ture warping, exploiting global correspondences reliably.
We also propose a mask-guided sparse video Transformer, which achieves high efficiency by discarding unnecessary and redundant tokens. With these components, ProPainter outperforms prior arts by a large margin of 1.46 dB in PSNR while maintaining appealing efficiency. 1.

Introduction
Video inpainting (VI) aims to fill gaps or missing re-gions in a video with visually consistent content while ensuring spatial and temporal coherence. This technique has broad applications, including video completion [10], object removal [9, 37], video restoration [31], watermark, and logo removal [19]. VI is challenging because it re-quires establishing accurate correspondence across distant frames for information aggregation. To address this chal-lenge, various mechanisms have been explored, such as 3D CNN [6, 11], video internal learning [41, 27], flow-guided propagation [37, 10, 43, 42, 19], and video Trans-former [22, 42, 19]. Among these mechanisms, flow-guided propagation and video Transformer have become mainstream choices for VI due to their promising performance.
Propagation-based methods in VI can be divided into two categories: image propagation and feature propagation.
The former employs bidirectional global propagation in the image domain with a pre-completed flow field. While this approach can fill the majority of holes in a corrupted video, it requires an additional image or video inpainting network after propagation to hallucinate the remaining missing re-gions. This isolated two-step process can result in unpleasant artifacts and texture misalignment due to inaccurate flow, as shown in Figure 1(f). To address this issue, a recent ap-proach called E2FGVI [19] implements propagation in the feature domain, incorporating flow completion and content hallucination modules in an end-to-end framework. With the learnable warping module, the feature propagation module relieves the pressure of having inaccurate flow. However,
E2FGVI employs a downsampled flow field to match the spatial size of the feature domain, limiting the precision of spatial warping and the efficacy of propagation, potentially resulting in blurry results. Moreover, feature propagation can only be performed within a short range of video sequences due to memory and computational constraints, hindering propagation from distant frames and leading to missing tex-ture, as shown in Figure 1(g).
Both image- and feature-based propagation have their pros and cons. In this study, we carefully revisit the VI prob-lem and investigate the possibility of combining the strengths of both techniques. We demonstrate that with systematic redesigns and adaptation of best practices in the literature, we can achieve dual-domain propagation, as illustrated in
Figure 1(a). To achieve reliable and efficient information propagation across a video, we identify several essential components: i) Efficient GPU-based propagation with relia-bility check – Unlike previous methods that rely on complex and time-consuming CPU-centric operations, such as index-ing flow trajectories, we perform global image propagation on GPU with flow consistency check. This implementation can be inserted at the beginning of the inpainting network and jointly trained with the other modules. Thus, subse-quent modules are able to correct any propagation errors and benefit from the long-range correspondence information provided by the global propagation, resulting in a significant performance improvement. ii) Improved feature propagation – Our implementation of feature propagation leverages flow-based deformable alignment [3], which improves robustness to occlusion and inaccurate flow completion compared to
E2FGVI [19]. iii) Efficient flow completion – We design a highly efficient recurrent network to complete flows for dual-domain propagation, which is over 40 times (∼192 fps1) faster than SOTA method [43] while maintaining com-parable performance. We demonstrate that these designs are essential to achieve efficient propagation of global and local information without texture misalignment or blurring in the filling results. An example is shown in Figure 1(h).
In addition to dual-domain propagation, we introduce an efficient mask-guided sparse video Transformer tailored for the VI task. The classic spatiotemporal Transformer is computationally intensive due to the quadratic number of interactions between video tokens, making it intractable for high-resolution and long temporal-length videos. For instance, contemporary Transformer-based methods, Fuse-Former [22] and FGT [42], are unable to handle 480p videos with a 32G GPU1 due to excessive memory demands. How-ever, we observe that the inpainting mask usually covers only a small local region, such as the object area2. Moreover, adjacent frames contain highly redundant textures. These observations suggest that spatiotemporal attention is unneces-sary for most unmasked areas, and it is adequate to consider only alternating interval frames in attention computation.
Motivated by these observations, we redesign the Trans-former by discarding unnecessary and redundant windows in the query and key/value space, respectively, significantly reducing computational complexity and memory without compromising inpainting performance.
The main contribution of this work is to provide a system-atic study into the core problem of VI and offer a practical solution that is both effective and efficient. Propagating in-formation in two distinct image and feature domains and combining them in a unified framework with fast GPU im-plementation is new for VI task. The mask-guided sparse video Transformer also offers practical insights into design-ing efficient spatiotemporal attention for VI task. Compared to the state-of-the-art methods, our model achieves superior performance with a large margin of 1.46 dB in PSNR, while also significantly reducing memory consumption. 2.