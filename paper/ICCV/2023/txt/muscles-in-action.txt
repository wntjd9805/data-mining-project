Abstract
Human motion is created by, and constrained by, our muscles. We take a first step at building computer vi-sion methods that represent the internal muscle activity that causes motion. We present a new dataset, Muscles in Ac-tion (MIA), to learn to incorporate muscle activity into hu-man motion representations. The dataset consists of 12.5 hours of synchronized video and surface electromyography (sEMG) data of 10 subjects performing various exercises.
Using this dataset, we learn a bidirectional representation that predicts muscle activation from video, and conversely, reconstructs motion from muscle activation. We evaluate our model on in-distribution subjects and exercises, as well as on out-of-distribution subjects and exercises. We demon-strate how advances in modeling both modalities jointly can serve as conditioning for muscularly consistent motion gen-eration. Putting muscles into computer vision systems will enable richer models of virtual humans, with applications in sports, fitness, and AR/VR. 1.

Introduction
The vision community has made great progress in mod-elling and analyzing human motion from video via tasks such as pose estimation [33, 25, 62, 45, 7, 23, 7], action recognition [47, 56, 27, 26, 59], motion transfer [8, 1, 31, 58] and more. However, motion understanding goes beyond
the surface. Human motion is created by and constrained by muscles. Every action is a product of our brain sending electric signals to our nerves, which contract our muscles, in turn moving our joints. Although this process occurs within us, most of us turn to physical therapists and sports instruc-tors for guidance on how to improve our motions to target or avoid particular muscle groups.
In this paper, we take a first step towards building com-puter vision methods that represent the internal muscle ac-tivity that causes human motion. We present a system that, given a video of a person performing an action, learns to infer what muscles a person used. Walking is controlled falling, and any physical motion is a balance between mus-cle forces and gravity. This interplay leads to an inherent asymmetry: different muscles are engaged in the downward portion of a squat, for instance, than in the upward portion.
Our goal is to learn the complex relationships between physical forces by analyzing synchronized video and mus-cle activation data. We achieve this by developing a sys-tem that can predict muscle activity from motion, and vice versa. One application of this bidirectional system is gen-erating new motions that are similar to an existing motion, while also adhering to specific muscle recruitment targets, illustrated in Figure 1.
The typical method of measuring muscle activity is through the use of electromyography sensors, which exist in an invasive form, as well as an non-invasive form, called surface electromyography (sEMG). We collected a new dataset, which we will release, that consists of over twelve hours of synchronized single-view video and sEMG signals of eight muscles for ten subjects performing fifteen differ-ent physical activities. By using commodity cameras and inexpensive sEMG sensors, we make the problem practical and easy for others to build on. The eight muscles recorded are the left and right biceps brachii (biceps), the left and right latissimus dorsi (laterals), both quadriceps (quads), and both biceps femoris (hamstrings), denoted in Figure 2.
The primary contribution of this paper is a framework for modeling the association between human motion and inter-nal muscle activity in video, and the rest of the paper will explain this contribution in detail. In section 2, we briefly review related work in human activity analysis, conditional motion generation, multi-modal learning, electromyogra-phy, and physics-grounded human motion generation.
In section 3, we describe our multimodal dataset in detail and analyze its characteristics. In section 4, we present a method to learn a bidirectional representation between the visual and muscle modalities. Section 5 shows experiments on both in-distribution experiments and subjects, as well as out-of-distribution experiments and subjects. In section 6, we showcase a demo application for learning the bidirec-tional representation between modalities. By releasing our datasets and models publicly, we hope this paper will spur
Figure 2: Sensor Placement. We illustrate the placement of our 8 sEMG sensors on a subject. We label the 8 measured muscles. additional work that models the rich internal structure that drives human activity in video. 2.