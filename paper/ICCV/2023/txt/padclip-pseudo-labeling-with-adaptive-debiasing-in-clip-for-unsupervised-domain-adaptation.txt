Abstract
Traditional Unsupervised Domain Adaptation (UDA) leverages the labeled source domain to tackle the learning tasks on the unlabeled target domain. It can be more chal-lenging when a large domain gap exists between the source and the target domain. A more practical setting is to utilize a large-scale pre-trained model to fill the domain gap. For example, CLIP shows promising zero-shot generalizability to bridge the gap. However, after applying traditional fine-tuning to specifically adjust CLIP on a target domain, CLIP suffers from catastrophic forgetting issues where the new domain knowledge can quickly override CLIP’s pre-trained knowledge and decreases the accuracy by half. We propose
Catastrophic Forgetting Measurement (CFM) to adjust the learning rate to avoid excessive training (thus mitigating the catastrophic forgetting issue). We then utilize CLIP’s zero-shot prediction to formulate a Pseudo-labeling setting with Adaptive Debiasing in CLIP (PADCLIP) by adjust-ing causal inference with our momentum and CFM. Our
PADCLIP allows end-to-end training on source and target domains without extra overhead. We achieved the best re-sults on four public datasets, with a significant improvement (+18.5% accuracy) on DomainNet. 1.

Introduction
Unsupervised Domain Adaptation (UDA) proposes to reduce data annotation costs by leveraging a labeled source domain to transfer the knowledge into an unlabeled target domain [11, 27, 49, 60, 73]. Prior UDA works focus on bridging the domain gap between source and target domains
[4, 12, 25, 27], or increasing network capacity [49, 60] by changing a convolutional neural network (e.g., ResNet [14]) to Vision Transformer (ViT) [9]. All of these past methods are pre-trained on ImageNet [7], but large-scale pre-training is becoming practical and achieves superior performance in
In theory, if the pre-trained many fields [41, 64, 65, 67].
*Equal contributions. This work was done at Amazon.
Figure 1: Catastrophic Forgetting. We naively fine-tune
CLIP (ResNet-101) on VisDA-2017 source domain train-ing set, and test it on validation sets of VisDA-2017 target domain and ImageNet-1K. CLIP forgets pre-trained knowl-edge (ImageNet accuracy -45%), resulting in -27% VisDA-2017 accuracy. Our PADCLIP mitigates catastrophic for-getting issues to achieve +6% VisDA-2017 accuracy. dataset is large enough, the domain gap between source and target domains could be bridged by the pre-trained dataset itself. Hence, we argue that large-scale pre-training is an important missing part of UDA.
We choose CLIP [41], a vision-language model pre-trained on 400 million image-text pairs. Without fine-tuning, CLIP outperforms SSRT [49], a state-of-the-art
UDA method on DomainNet [38]. This is thanks to the large-scale training set, which allows CLIP to disentangle object class from object domain (e.g., “a photo of a dog” vs
“a sketch of a dog”): the language supervision in the form of a sentence used by CLIP is more descriptive than a sin-gle class label. However, on VisDA-2017, CLIP without fine-tuning underperforms previous work, SDAT [42]. This is because the synthetic data generated from the real-world domain do not exist in CLIP’s training set, so we still need to fine-tune CLIP to adapt it for a specific domain task.
We first adopt the traditional approach to fine-tune CLIP on VisDA-2017 [39] but found that CLIP suffers from catastrophic forgetting issues. As shown in Fig.1: before fine-tuning, CLIP has a strong representation power that
Figure 2: Overview. For the source domain, we convert the label and domain name into a prompt, and obtain text and image representations to train CLIP in a supervised manner. We use CLIP’s original representation, and weak/strong augmented representations to measure CFM to adjust the learning rate for mitigating catastrophic forgetting issues. For the target domain, we use zero-shot prediction in CLIP to obtain pseudo-labels and adaptively debias them with CFM (adjust debias factor) and
DCM (adjust momentum) for unsupervised learning. *Pseudo-label is converted into a prompt to obtain text representation. can achieve 67% top-1 ImageNet accuracy, while drop-ping to 22% after fine-tuning on VisDA-2017. The loss of CLIP’s representation power causes the accuracy degra-dation on VisDA-2017. To counter this, it is possible to preserve CLIP’s representation power by fine-tuning both
CLIP and VisDA-2017 datasets jointly, but CLIP requires several weeks to train a single setting (DomainNet has 30 settings, so it takes a year for a single experiment). More-over, we anticipate the data imbalance issue during joint training since CLIP’s training set is 142 times larger than
VisDA-2017. We seek a more practical solution for catas-trophic forgetting issues without adding extra overheads.
We attempt to fine-tune CLIP on the UDA dataset with a lower learning rate and observe less catastrophic forget-ting issues, but the low learning rate prevents CLIP from learning new knowledge. To solve this problem, we pro-pose to adjust the learning rate with Catastrophic Forgetting
Measurement (CFM, Fig. 2) by comparing the original rep-resentation (forward original image on original CLIP) and fine-tuned representations (forward augmented images on fine-tuned CLIP). CFM is, however, unstable because ev-ery image has a different forgetting rate, so we leverage our observation that CLIP is likely to have similar predictions across all augmentations when the training example is easy (and large difference for the hard example). We propose to measure the consistency between weak (translate, flip) and strong augmentation (perturb visual appearance) as a mo-mentum (Dual Consistency Momentum, DCM) to stabilize
CFM. Our method does not introduce extra overhead: since the augmentation is already a part of fine-tuning, original prediction can be cached, and we do not need to fine-tune
UDA and CLIP datasets jointly.
We further seek to use CLIP with pseudo-labeling on the target domain, which recently enjoyed success in UDA
[35,60,75,76,76]. DebiasPL [57] utilized CLIP for pseudo-labeling, but it was designed for a single domain. After extending to source and target domains (UDA setting), De-biasPL [57] suffers from catastrophic forgetting issues (ac-curacy decreases by 21% on VisDA-2017 after fine-tuning).
To solve this problem, we replace the fixed debias factor in
DebiasPL [57] with our CFM, and replace the fixed momen-tum in DebiasPL with our adaptive momentum (DCM). We further include a domain name into a prompt (such as: “This is a [sketch] photo of [car]”). Our method mitigates the catastrophic forgetting issue, and achieves the best results on DomainNet [38], VisDA-2017 [39], Office-Home [54],
Office-31 [44]. To summarize, our main contributions are:
• We propose to use CLIP in UDA and discover the catastrophic forgetting issue when fine-tuning CLIP.
We propose CFM for CLIP in UDA to mitigate this is-sue without introducing extra computational overhead.
• We propose pseudo-labeling for CLIP in UDA by ex-tending DebiasPL to multiple domains, and replacing debias factor and momentum with our CFM and DCM.
We also introduce a domain name into a prompt.
• We achieve the best results on four benchmarks on both ResNet and ViT, with a large performance im-provement on the large-scale dataset (+18.5% accu-racy on DomainNet).
2.