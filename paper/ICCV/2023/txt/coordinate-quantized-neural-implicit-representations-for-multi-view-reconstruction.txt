Abstract
In recent years, huge progress has been made on learn-ing neural implicit representations from multi-view images for 3D reconstruction. As an additional input complement-ing coordinates, using sinusoidal functions as positional encodings plays a key role in revealing high frequency de-tails with coordinate-based neural networks. However, high frequency positional encodings make the optimization un-stable, which results in noisy reconstructions and artifacts in empty space. To resolve this issue in a general sense, we introduce to learn neural implicit representations with quantized coordinates, which reduces the uncertainty and ambiguity in the field during optimization. Instead of con-tinuous coordinates, we discretize continuous coordinates into discrete coordinates using nearest interpolation among quantized coordinates which are obtained by discretizing the field in an extremely high resolution. We use discrete coordinates and their positional encodings to learn implicit functions through volume rendering. This significantly re-duces the variations in the sample space, and triggers more multi-view consistency constraints on intersections of rays from different views, which enables to infer implicit function in a more effective way. Our quantized coordinates do not bring any computational burden, and can seamlessly work upon the latest methods. Our evaluations under the widely used benchmarks show our superiority over the state-of-the-art. Our code is available at https://github.com/
MachinePerceptionLab/CQ-NIR. 1.

Introduction
Learning implicit representations from multi-view im-ages is a challenge in reconstructing 3D geometry in a scene. The latest methods learn implicit representations us-ing coordinate-based neural networks to infer signed dis-tance or occupancy fields [43, 67, 66, 13, 63, 68, 64, 61, 62, 16, 25, 50] through volume rendering. By shooting rays across the fields, we render RGB values at a pixel by integrating colors and geometry at 3D queries sampled along a ray through volume rendering. The images ren-dered from neural implicit functions are compared with the ground truth images, which measures errors to improve the neural implicit fucntions.
Learning high fidelity implicit representations requires to use positional encodings [39, 43, 63, 64] as a com-plement to coordinates, which remedies the incapability of coordinate-based neural networks in modeling high fre-quency details. Positional encodings are vectors formed by sinusoidal functions of coordinates with both low and high frequencies [55, 59], where the frequency band is shown as the key factor to capture details in different scenes. How-ever, higher frequency turns out to bring noises, which re-sults in artifacts on surfaces and in empty spaces. To stabi-lize the optimization with high frequency positional encod-ings, some methods [18, 64, 45] learn soft masks to grad-ually expose high frequency components over training it-erations. However, this masking strategy relies on training iterations and numbers of frequency components, which is tedious to tune in a general sense.
To resolve this issue, we propose to use quantized coor-dinates to learn neural implicit representations from multi-view images.
Instead of continuous coordinates and po-sitional encodings of continuous coordinates in previous methods [39, 63, 68, 64, 61, 62, 16, 71], we use discrete co-ordinates and positional encodings of discrete coordinates as the input of coordinate-based neural networks, where we discretize the field in an extremely high resolutions. Our insight here is to decrease the uncertainty and ambiguity in the field during optimization. We achieve this by introduc-ing discrete coordinates with two reasons. On the one hand, we enable networks to merely observe a finite set of discrete coordinates rather than infinite continuous variations, which simplifies the optimization by significantly reducing varia-tions in the sample space. On the other hand, a discrete co-ordinate covers an area rather than a point, hence rays from different views are more easily to have overlapped samples with each other. This triggers more multi-view consistency constraints to take effect at these intersections, which leads
to more effective inference. Our quantized coordinates do not bring any extra computational burden, inconsistency on borders of neighboring coordinates, and provide a general strategy which can be used upon different methods. We evaluate our improvements over the latest methods under multiple benchmarks. Our contributions are listed below. i) We introduce quantized coordinates to learn neural im-plicit functions from multi-view images. By discretiz-ing a field in an extremely high resolution, we in-troduce efficient ways of using discrete coordinates, which does not bring extra computational burden and inconsistency on borders of neighboring coordinates. ii) We report analysis on how discrete coordinates de-crease the uncertainty and ambiguity in the field by reducing the variations in the sample space and trig-gering more multi-view consistency constraints to infer implicit functions in a more effective way. iii) Our discrete coordinates can seamlessly work upon the latest methods. We justify our effectiveness by show-ing significant improvements over the state-of-the-art results under the widely used benchmarks. 2.