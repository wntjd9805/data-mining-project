Abstract
Large language models (LLMs), such as GPT-3 and
ChatGPT, have demonstrated remarkable results in various natural language processing (NLP) tasks with in-context learning, which involves inference based on a few demon-stration examples. Despite their successes in NLP tasks, no investigation has been conducted to assess the abil-ity of LLMs to perform document information extraction (DIE) using in-context learning. Applying LLMs to DIE poses two challenges: the modality and task gap. To this end, we propose a simple but effective in-context learning framework called ICL-D3IE, which enables LLMs to per-form DIE with different types of demonstration examples.
Specifically, we extract the most difficult and distinct seg-ments from hard training documents as hard demonstra-tions for benefiting all test instances. We design demon-strations describing relationships that enable LLMs to un-derstand positional relationships. We introduce formatting demonstrations for easy answer extraction. Additionally, the framework improves diverse demonstrations by updat-ing them iteratively. Our experiments on three widely used benchmark datasets demonstrate that the ICL-D3IE frame-work enables Davinci-003/ChatGPT to achieve superior performance when compared to previous pre-trained meth-ods fine-tuned with full training in both the in-distribution (ID) setting and in the out-of-distribution (OOD) setting.
Code is available at https://github.com/MAEHCM/
ICL-D3IE. 1.

Introduction
The task of visually rich document understanding (VRDU), which involves extracting information from
VRDs [2, 19], requires models that can handle various types
*Corresponding author. lei.wang.2019@phdcs.smu.edu.sg
â€ Corresponding author. xing.xu@uestc.edu.cn
Figure 1: Two approaches for solving the DIE task: (a) pre-vious pre-trained document understanding models [15, 42] fine-tuned with full training examples, and (b) in-context learning over LLMs with a few examples. of documents, such as voice, receipts, forms, emails, and advertisements, and various types of information, includ-ing rich visuals, large amounts of text, and complex doc-ument layouts [28, 18, 26]. Recently, fine-tuning based on pre-trained visual document understanding models has yielded impressive results in extracting information from
VRDs [41, 13, 22, 23, 15, 21], suggesting that the use of large-scale, unlabeled training documents in pre-training document understanding models can benefit information extraction from VRDs. As shown in Figure 1 (a), a pre-trained model such as LayoutLMv3 [15] can predict labels for entities in a test VRD.
Large language models (LLMs), such as GPT-3 [1],
OPT [45], and PaLM [5], develop quickly and have shown remarkable results in various natural language processing (NLP) tasks. As LLMs grow in model parameters and train-ing corpus size, they reveal emergent abilities that allow them to learn to reason from just a few demonstration ex-amples within a given context [38]. This paradigm of learn-ing is referred to as in-context learning (ICL) [8]. Recently, approaches [43, 14] have been proposed to explore how to use LLMs to solve vision-language (VL) tasks. However, to date, there has been no investigation into the ability of
LLMs to handle VRD understanding tasks, such as docu-ment information extraction (DIE). Similar to VQA [12],
Two main challenges arise when applying LLMs to DIE: the modality gap and the task gap, as LLMs cannot directly process images and may lack training on layout information in VRDs.
To address these challenges, one popular strategy in us-ing LLMs for the VQA task is to use demonstration QA pairs and convert their corresponding images into image descriptions through image caption models [43, 14]. Sub-sequently, the demonstration QA pairs and image descrip-tions are combined as a prompt for the LLM to answer a test question. Figure 1 (b) shows this straightforward strategy to apply LLMs to the DIE task. It first utilizes Optical Char-acter Recognition (OCR) tools to convert images of demon-stration documents from the training data into textual con-tents and corresponding entity bounding boxes. The con-verted demonstrations with entity labels are then combined as a prompt for LLMs to predict labels for entities in a test document. However, this strategy may perform poorly, as it ignores positional relationships among textual contents and is sensitive to examples selected for demonstrations.
In this paper, we propose ICL-D3IE, a simple and ef-fective in-context learning framework for LLMs to perform the DIE task with various types of demonstration exam-ples within a given context. Our method constructs dif-ferent types of demonstrations based on three criteria: (1) the demonstrations should benefit all test documents rather than just a subset of them, (2) layout information must be in-cluded, and (3) the demonstrations should predict labels in an easily extractable format. To construct hard demonstra-tions for the first criterion, we select challenging segments from the training documents that are difficult for LLMs to accurately predict entities. To construct layout-aware demonstrations for the second criterion, we use a prompt question to direct LLMs to describe positional relationships between textual content boxes in selected regions. To create formatting demonstrations for the third criterion, we ran-domly choose training segments to guide LLMs to predict labels in a desired format for easy extraction. Furthermore, the framework iteratively enhances diverse demonstrations by updating hard demonstrations through in-context learn-ing with previous diverse demonstrations.
Experiments conducted on three widely used bench-mark datasets (FUNSD [18], CORD [28], and SROIE [16]), demonstrate that ICL-D3IE allows LLMs to achieve DIE performance that is superior or comparable to previous pre-trained methods fine-tuned with full training samples when tested in the in-distribution (ID) setting. For example, ICL-D3IE with GPT-3 (97.88%) outperforms LayoutLMv3base (96.89%) on SROIE. Moreover, in the out-of-distribution (OOD) setting, ICL-D3IE performs much better than previ-ous pre-trained methods on all datasets, achieving superior performance. Together, these remarkable results encourage new ways to leverage LLMs for solving VRD-related tasks. 2.