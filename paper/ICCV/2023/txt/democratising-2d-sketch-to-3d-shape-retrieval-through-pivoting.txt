Abstract
This paper studies the problem of 2D sketch to 3D shape retrieval, but with a focus on democratising the process. We would like this democratisation to happen on two fronts: (i) to remove the need for large-scale specifically sourced 2D sketch and 3D shape datasets, and (ii) to remove re-strictions on how well the user needs to sketch and from what viewpoints. The end result is a system that is train-able using existing datasets, and once trained allows users to sketch regardless of drawing skills and without restric-tion on view angle. We achieve all this via a clever use of pivoting, along with novel designs that injects 3D un-derstanding of 2D sketches into the system. We perform pivoting using two existing datasets, each from a distant research domain to the other: 2D sketch and photo pairs from the sketch-based image retrieval field (SBIR), and 3D shapes from ShapeNet.
It follows that the actual feature pivoting happens on photos from the former and 2D pro-jections from the latter. Doing this already achieves most of our democratisation challenge – the level of 2D sketch abstraction embedded in SBIR dataset offers demoraliza-tion on drawing quality, and the whole thing works without a specifically sourced 2D sketch and 3D model pair. To further achieve democratisation on sketching viewpoint, we
“lift” 2D sketches to 3D space using Blind Perspective-n-Points (BPnP) that injects 3D-aware information into the sketch encoder. Results show ours achieves competitive per-formance compared with fully-supervised baselines, while meeting all set democratisation goals. 1.

Introduction
Sketches are highly expressive [37]. This particular sketch strait has been explored to a large extend for image retrieval, especially under the fine-grained setting. The 3D literature followed a similar trend, starting with category-level retrieval of 3D shapes using sketches [45], but only very recently moved onto the fine-grained instance-level
Figure 1. Collecting 2D sketches for 3D shapes is difficult since the viewpoints often needs to be pre-defined that arguably makes sketch collection for 3D shapes an ill-posed problem to start with – there is no one good view that caters for everyone. We remove the need for large-scale 2D sketch and 3D shape datasets through a clever use of pivoting across two separate domains (i) paired 2D freehand sketches from well-studied FG-SBIR literature [91], and (ii) the 3D shape domain, where we have ample data at our dispoal (e.g., ShapeNet [11]). Our pivoting happens on paired 2D photos from FG-SBIR and 2D projections from 3D shapes. setup [58]. The very reason behind such a time lag lies with that of data collection – just as how collection of 2D sketches for images was difficult [70, 18, 66, 43], sketches for 3D models had been proven to be even harder to collect
[47, 58]. In particular, since one is collecting a 2D sketch for a 3D model, viewpoints often need to be pre-defined
[95], which arguably makes sketch collection for 3D shapes an ill-posed problem to start with – there is no one good view that caters for everyone.
There is however a strong voice in the 3D community stating the opposite – that different to images, sketches can be freely rendered for 3D models, other than collected by hand. This is because 3D models are well-defined and with-out cluttered background unlike natural photos, so well-tailored non-photorealistic rendering (NPR) algorithms [26] should suffice to produce pseudo-sketches good enough for
downstream tasks [19]. Indeed synthetic sketches has been explored for 3D shape retrieval [82]. Conclusion is how-ever that they lack generalization ability to real free-hand sketches, therefore requiring an art-trained person to pro-duce “edge-like” sketches as input.
In this paper, we set off to address all said constraints, and in effect produce for the first time a democratised fine-grained 2D sketch to 3D shape retrieval system. This democratisation happens on two fronts (i) we remove en-tirely the need for specifically sourced 2D sketch and 3D shape datasets, and (ii) we enable amateur users without art training (i.e., you and me) to still enjoy decent accuracy with their “ordinary” (more abstract) sketches.
The key innovation behind our democratisation process lies with that of pivoting [87, 41, 34, 32]. Pivoting is by now a well-explored concept in the language domain, commonly used for language translation. The idea is that by perform-ing feature pivoting on a third shared domain, one bridges two otherwise disconnected domains (e.g., Japanese → En-glish → French, where English is the pivoting domain). Our pivoting (see Fig. 1) also operates across two separate do-mains (i) the domain of paired 2D free-hand sketches, taken from the well-studied literature of fine-grained sketch-based image retrieval (FG-SBIR) [91], and (ii) the 3D shape do-main, where we have ample data at our disposal (e.g.,
ShapeNet [11]). It follows that the pivoting factor is paired 2D photos from the former, and 2D projections of the 3D shapes from the latter. The result of this pivoting procedure is therefore a 2D sketch to 3D shape retrieval system that (i) is trainable entirely using existing datasets already proposed for diverse problem settings (FG-SBIR [91] and ShapeNet
[11]), and (ii) understands sketch abstraction which is well-reflected in the sketch-photo dataset used and previously successfully modeled for FG-SBIR [52, 5].
Our democratisation challenge is mostly addressed al-ready with just this clever use of pivoting. To inject 3D-aware knowledge into our 2D shared encoder (where pivot-ing is conducted), we further “lift” 2D sketch to 3D space.
However, a naive 2D sketch to 3D shape generation is sub-optimal since (i) we would otherwise need 2D sketch and 3D shape pairs, thereby defeating our purpose of democrati-sation on specifically sourced data, and (ii) reconstructing 3D shape from sparse 2D sketches is a complex task [19] that makes our training unstable with noisy gradients. The way forward to is therefore using softer constraints. For that, we use the Blind Perspective-n-Points (BPnP) algo-rithm [10, 8, 9] to solves the pose and orientation between a 2D projection and 3D shapes, and later transfer this 3D-aware knowledge to sketch encoders using pivoting. Train-ing without any 2D sketch and 3D shape pairs, the re-sulting framework reach an impressive SBSR performance (both category and fine-grained), reaching that of super-vised (with 2D sketch - 3D shape pairs) SOTAs [58].
In summary, our contributions are: (i) we democratise fine-grained 2D sketch to 3D shape retrieval system that enable amateur users enjoy decent accuracy with their ab-stract sketches. (ii) The resulting framework trains without 2D sketch and 3D shape pairs via a clever use of pivoting existing FG-SBIR dataset [91] and ample 3D shapes from
ShapeNet [11]. (iii) We inject 3D-aware knowledge into our 2D sketch encoder, to “lift” 2D sketch to 3D space us-ing the BPnP algorithm [8] that solves for 2D and 3D pose and orientation. (iv) Training without any 2D sketch and 3D shape, the resulting method performs quite close to super-vised SOTA [58] that use 2D sketch and 3D shape labels. 2.