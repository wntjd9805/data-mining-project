Abstract
Spatially dense self-supervised learning is a rapidly growing problem domain with promising applications for unsupervised segmentation and pretraining for dense down-stream tasks. Despite the abundance of temporal data in the form of videos, this information-rich source has been largely overlooked. Our paper aims to address this gap by proposing a novel approach that incorporates temporal consistency in dense self-supervised learning. While meth-ods designed solely for images face difﬁculties in achieving even the same performance on videos, our method improves not only the representation quality for videos – but also im-ages. Our approach, which we call time-tuning, starts from image-pretrained models and ﬁne-tunes them with a novel self-supervised temporal-alignment clustering loss on unla-beled videos. This effectively facilitates the transfer of high-level information from videos to image representations.
Time-tuning improves the state-of-the-art by 8-10% for un-supervised semantic segmentation on videos and matches it for images. We believe this method paves the way for further self-supervised scaling by leveraging the abundant avail-ability of videos. The implementation can be found here : https://github.com/SMSD75/Timetuning 1.

Introduction
Dense self-supervised learning, whereby meaningful deep features for each pixel or patch of input are learned in an unsupervised manner, has recently received increas-ing attention [74, 24, 57, 51]. By learning spatially con-sistent features for different views of an input, strong gains in unsupervised semantic segmentation have been achieved using unlabeled images. However, so far, an even more information-rich source for unsupervised training has been largely overlooked: videos. With their additional time dimension and being the most rapidly growing form of digital content, they are well-suited to scaling dense self-supervised learning even further.
Some efforts have already been made to learn from the video domain by using different frames from a video as aug-(cid:20) (cid:21) (cid:55) (cid:171) (cid:81)(cid:79)(cid:3)(cid:16)(cid:65)(cid:80)(cid:68)(cid:75)(cid:64) (cid:19)(cid:78)(cid:65)(cid:82)(cid:69)(cid:75)(cid:81)(cid:79)(cid:3)(cid:16)(cid:65)(cid:80)(cid:68)(cid:75)(cid:64)(cid:79) (cid:81)(cid:79)(cid:3)(cid:16)(cid:65)(cid:80)(cid:68)(cid:75)(cid:64)(cid:79) (cid:19)(cid:78)(cid:65)(cid:82)(cid:69)(cid:75)(cid:81) (cid:12)(cid:73)(cid:61)(cid:67)(cid:65)(cid:139)(cid:62)(cid:61)(cid:79)(cid:65)(cid:64)(cid:3) (cid:25)(cid:69)(cid:64)(cid:65)(cid:75)(cid:139)(cid:62)(cid:61)(cid:79)(cid:65)(cid:64)(cid:3) (cid:20) (cid:171) (cid:55) (cid:18)(cid:81)(cid:78)(cid:79)(cid:137) (cid:23)(cid:69)(cid:73)(cid:65)(cid:139)(cid:23)(cid:81)(cid:74)(cid:69)(cid:74)(cid:67) (cid:23)(cid:69)(cid:73)(cid:65)(cid:139)(cid:23)(cid:81)(cid:74)(cid:69)(cid:74)(cid:67) (cid:24)(cid:79)(cid:65) (cid:24)(cid:79)(cid:65) (cid:3)(cid:12)(cid:73)(cid:61)(cid:67)(cid:65)(cid:139)(cid:62)(cid:61)(cid:79)(cid:65)(cid:64)(cid:3)(cid:8)(cid:74)(cid:63)(cid:75)(cid:64)(cid:65)(cid:78)(cid:79) (cid:20) (cid:21) (cid:21) (cid:171)(cid:171) (cid:171)(cid:171) (cid:55) (cid:20) (cid:21)(cid:21) (cid:171)(cid:171) (cid:171)(cid:171) (cid:55) (cid:171) (cid:83)(cid:69)(cid:80)(cid:68)(cid:3)(cid:61)(cid:3)(cid:23)(cid:65)(cid:73)(cid:76)(cid:75)(cid:78)(cid:61)(cid:72)(cid:3)(cid:15)(cid:75)(cid:79)(cid:79) (cid:74)(cid:85)(cid:68)(cid:71)(cid:76)(cid:72)(cid:81)(cid:87) (cid:11)(cid:75)(cid:83)(cid:3)(cid:69)(cid:79)(cid:3)(cid:80)(cid:69)(cid:73)(cid:65)(cid:3)(cid:81)(cid:80)(cid:69)(cid:72)(cid:69)(cid:79)(cid:65)(cid:64)(cid:142) (cid:11)(cid:75)(cid:83)(cid:3)(cid:69)(cid:79)(cid:3)(cid:80)(cid:69)(cid:73)(cid:65)(cid:3)(cid:81)(cid:80)(cid:69)(cid:72)(cid:69)(cid:79)(cid:65)(cid:64)(cid:142) (cid:4)(cid:79)(cid:3)(cid:64)(cid:61)(cid:80)(cid:61) (cid:4)(cid:79)(cid:3)(cid:64)(cid:61)(cid:80)(cid:61)(cid:3)(cid:79)(cid:75)(cid:81)(cid:78)(cid:63)(cid:65) (cid:61)(cid:3)(cid:79)(cid:75)(cid:81)(cid:78)(cid:63)(cid:65) (cid:4)(cid:79) (cid:4)(cid:79)(cid:3)(cid:64)(cid:61)(cid:80)(cid:61)(cid:3)(cid:64)(cid:69)(cid:73)(cid:65)(cid:74)(cid:79)(cid:69)(cid:75)(cid:74) (cid:4)(cid:79)(cid:3)(cid:79)(cid:65)(cid:72)(cid:66)(cid:139)(cid:79)(cid:81)(cid:76)(cid:65)(cid:78)(cid:82)(cid:69)(cid:79)(cid:65)(cid:64)(cid:3)(cid:79)(cid:69)(cid:67)(cid:74)(cid:61)(cid:72) (cid:79)(cid:3)(cid:79)(cid:65)(cid:72)(cid:66)(cid:139)(cid:79)(cid:81)(cid:76)(cid:65)(cid:78)(cid:82)(cid:69)(cid:79)(cid:65)(cid:64)(cid:3)(cid:79)(cid:69)(cid:67)(cid:74)(cid:61)(cid:72) (cid:4) (cid:4)(cid:79)(cid:3)(cid:64)(cid:61)(cid:80)(cid:61)(cid:3)(cid:64)(cid:69)(cid:73)(cid:65)(cid:74)(cid:79)(cid:69)(cid:75) (cid:75)(cid:74)
Figure 1: Time-tuning compared to previous methods.
Unlike existing methods that ignore or utilize expensive 3D models to implicitly model temporal information, the proposed method explicitly incorporates temporal consis-tency in dense feature representations using a temporal self-supervised loss. The method starts with a 2D encoder pre-trained on images and ﬁne-tunes it using unlabeled videos.
This approach leads to improved performance not only for videos but also for images. mentations [18, 63, 49] or by involving temporal correspon-dence [71, 58]; however, they mostly did it in a supervised way [29, 73, 19, 40, 65, 38, 31, 28, 41, 37, 54, 44], which is not scalable speciﬁcally for dense tasks where the number of targets can increase signiﬁcantly as the number of pix-els grows. To this end, self-supervised learning approaches offer a solution by reducing the need for supervision. How-ever, these methods typically rely on the notion of “views”, which involves learning similar features for corresponding locations over time. This usually leads to a chicken-and-egg problem, where the correspondences are required for learning dense features – which in turn enable good corre-spondences [30].
In images, the challenge is trivially solved by consider-ing the correspondence function based on the augmentation function. For instance, In the case of color augmentations, this correspondence is simply given by the identity. How-ever, shifts in time cannot be viewed as mere augmenta-1
tions. As we demonstrate through a new evaluation pro-tocol, using image models on frames alone is not nearly as effective. A similar ﬁnding has also been reported, al-beit for non-dense works [20, 34], which assumed the pas-sage of time as an image augmentation. These works have generally reported reduced performances, even when com-pared to simple image-based pretraining methods. Simi-larly, video-level tasks [17, 18, 55] assume sufﬁciently sim-ilar semantics between different frames. This is also not true for dense tasks, as static features can only be assumed where nothing is moving – which is rare due to possible ob-ject, camera, and background motion between the frames.
To address this challenge, we propose to model the ad-ditional time-dimension explicitly to identify which pixels should retain similar embeddings and which should not. We propose two separate modules to tackle the correspondence and the dense learning, respectively. For the former, we in-troduce the Feature-Forwarder (FF) module, which breaks the mentioned chicken-and-egg loop by leveraging the good tracking performance of pretrained image models, and al-lows an approximate second “view” that can then be treated as a target for the further dense self-supervised loss. On top of this, we introduce a spatio-temporally dense clustering module, which learns unsupervised clusters across samples, locations and time. Using these two components and start-ing from image-pretrained features, our proposed method allows time-tuning (TIMET) the dense representation in a self-supervised manner, see Figure 1.
Finally, we demonstrate that TIMET paves the way for further scaling of self-supervised learning by leveraging the abundant availability of video datasets and transferring their knowledge to the image domain. This results in consistently achieving state-of-the-art performances not only for the task of unsupervised semantic segmentation of videos, but also for unsupervised image semantic segmentation, a feat pre-viously out of reach for methods trained on videos.
Overall, this paper makes the following contributions:
• We show that image-based unsupervised dense seg-mentation models applied to videos exhibit degraded performance and lack temporal consistency in their segmentation maps.
• Building on this observation, we propose a novel dense self-supervised learning method that utilizes temporal consistency as a learning signal.
• We demonstrate that our method enables the scal-ing of self-supervised learning by leveraging abun-dant video datasets and effectively transferring knowl-edge to the image domain. Our approach consistently achieves state-of-the-art performance for both images and videos, opening up new opportunities in the ﬁeld. 2.