Abstract
Reverberant Environment
Anechoic Environment
We present AdVerb, a novel audio-visual dereverbera-tion framework that uses visual cues in addition to the re-verberant sound to estimate clean audio. Although audio-only dereverberation is a well-studied problem, our ap-proach incorporates the complementary visual modality to perform audio dereverberation. Given an image of the en-vironment where the reverberated sound signal has been recorded, AdVerb employs a novel geometry-aware cross-modal transformer architecture that captures scene geome-try and audio-visual cross-modal relationship to generate a complex ideal ratio mask, which, when applied to the rever-berant audio predicts the clean sound. The effectiveness of our method is demonstrated through extensive quanti-tative and qualitative evaluations. Our approach signiﬁ-cantly outperforms traditional audio-only and audio-visual baselines on three downstream tasks: speech enhancement, speech recognition, and speaker veriﬁcation, with relative improvements in the range of 18% - 82% on the LibriSpeech test-clean set. We also achieve highly satisfactory RT60 er-ror scores on the AVSpeech dataset. 1.

Introduction
Reverberation occurs when an audio signal reﬂects from multiple surfaces and objects in the environment to alter the dry sound thereby degrading its quality. Far-ﬁeld speech recorded at a considerable distance from the speaker is signiﬁcantly degraded by the strong reverberation effects caused by the environment. The amount of reverberation is highly correlated to the geometry of the surroundings and the materials present in the vicinity [9, 11]. For instance, the auditory experience changes drastically when listening to a pleasant symphony in a large empty hallway vs. a relatively small furnished living room (Fig. 2). Recent studies have
*Equal contribution.
`
AdVerb
Reverberant Audio
Estimated Clean Audio
Figure 1: We present AdVerb, a novel audio-visual dere-verberation framework that leverages visual cues of the en-vironment to estimate clean audio from reverberant audio.
E.g, given a reverberant sound produced in a large hall, our model attempts to remove the reverb effect to predict the anechoic or clean audio. shown that the reverberation effects can be estimated from a single image of the environment with reasonable accuracy
[69, 46, 36]. Removal of reverberation in recorded speech signals is highly desirable and would help improve the per-formance of several other auxiliary downstream tasks like automatic speech recognition (ASR), speaker veriﬁcation (SV), source separation (SP), speech enhancement (SE), etc., which are widely used in several day-to-day tools.
Audio-only dereverberation is a well-studied problem with various systems achieving encouraging results [53, 34, 99, 91, 90]. In contrast, using the visual stream as an ad-ditional cue to solve this task is a particularly understudied problem. We attribute the lack of research in this space to the scarcity of datasets. Most open-source datasets, both real and synthetic, contain only room impulse responses (RIRs) with no information about their source of origin
[77, 83]. Note that obtaining such RIRs can be challeng-ing as doing so requires access to the physical environment, thereby limiting their applicability. However, in real-world settings, reverberant audio is naturally accompanied by a visual stream; video conferencing, augmented reality (AR), 1
Anechoic Chamber
A
Furnished Room
B
Unfurnished Room
Close Subject 200
Far-away Subject
Figure 2: Reverberation is a function of the speaker’s relative position and the surrounding environment. The visual signals present critical details that determine the nature of the distortion. E.g, A(cid:2) in a relatively small furnished room when the speaker is nearby, reverb is less evident, whereas for B(cid:2) in a large hallway (especially when the speaker is far away) the reverb effect is very strong. The audio waveform illustrates the nature of reverberation, with the magniﬁed section clearly depicting a stronger reverberation effect in case B(cid:2) over A(cid:2).
Recently, and web video indexing are some examples. audio-visual speech enhancement meth-ods [95, 79, 12, 48] have shown signiﬁcant improvements over the audio-only speech enhancement approaches. These tasks beneﬁt from the presence of sound-producing ob-jects in the visual scene, which allows the model to ef-fectively utilize these strong stimuli for accomplishing the task. Many of these approaches track the lip movements of the speaker to separate the noise from the voice compo-nents in degraded speech which builds on the assumption that a speaker is always close to and facing the camera.
These assumptions might not always hold in our case as the scope of the problem under consideration (mid/far-ﬁeld) makes it difﬁcult to obtain such cues. Thus, in a real-world setting, the available cue for audio-visual dereverberation is a panoramic view of the environment with or without a speaker in the ﬁeld of view. Effectively utilizing visual cues in order to perform audio-visual dereverberation would re-quire the model to understand the room’s implicit geometric and material properties, which poses its own challenges.
Our Contributions: We propose AdVerb, comprising a modiﬁed conformer block [24] with specially designed po-sitional encoding to learn audio-visual dereverberation. The network takes corrupted audio and the corresponding vi-sual image1 of the surrounding environment (from where the RIR is obtained) as input to perform this task (Fig. 1). Our approach employs a novel geometry-aware mod-ule with cross-modal attention between the audio and visual modalities to generate a complex ideal ratio mask, which is applied to the reverberant spectrogram to obtain the esti-mated clean spectrogram. This conformer block consists of a modiﬁed (Shifted) Window Block [44] and Panoptic Blocks to combine local and global geometry relations. We dis-cuss key motivations behind our approach in Section 4. To 1We use panoramic images to train; inference can be done on both panoramic and non-panoramic images. learn audio-visual dereverberation, AdVerb solves two ob-jectives, Spectrogram Prediction Loss and Acoustic Token
Matching Loss, which makes the output audio retain pho-netic and prosodic properties. To summarize, our main con-tributions are as follows: (1) We propose AdVerb, a novel cross-modal frame-work for dereverberating audio by exploiting complemen-tary low-level visual cues and specially designed relative position embedding. (2) To this end, AdVerb employs a novel geometry-aware conformer network to capture 3D spatial semantic information to equip the network with salient vision cues through (Shifted) Window Blocks and Panoptic Blocks. (3) Our architecture involves the prediction of complex ideal ratio mask and simultaneous optimization of two ob-jective functions to estimate the dereverbed speech. (4) On objective evaluation our approach signiﬁcantly outperforms traditional audio-only and audio-visual [12] baselines with a relative improvement in the range 18%
- 82% on three downstream tasks: speech enhancement, speech recognition, and speaker veriﬁcation, when evalu-ated on the LibriSpeech test-clean set on all difﬁculty lev-els. It also achieves highly satisfactory RT60 error scores on the AVSpeech dataset. (5) User study analysis reveals our method outperforms prior approaches on perceptual audio quality assessment. 2.