Abstract
We introduce the first multitasking vision transformer adapters that learn generalizable task affinities which can be applied to novel tasks and domains.
Integrated into an off-the-shelf vision transformer backbone, our adapters can simultaneously solve multiple dense vision tasks in a parameter-efficient manner, unlike existing multitasking transformers that are parametrically expensive.
In con-trast to concurrent methods, we do not require retraining or fine-tuning whenever a new task or domain is added.
We introduce a task-adapted attention mechanism within our adapter framework that combines gradient-based task similarities with attention-based ones. The learned task affinities generalize to the following settings: zero-shot task transfer, unsupervised domain adaptation, and generaliza-tion without fine-tuning to novel domains. We demonstrate that our approach outperforms not only the existing con-volutional neural network-based multitasking methods but also the vision transformer-based ones. Our project page is at https://ivrl.github.io/VTAGML. 1.

Introduction
In the past few years, vision transformers [5, 15, 16, 26, 30, 57] have grown in popularity at an incredible pace.
They have now achieved state-of-the-art results, outper-forming Convolutional Neural Network (CNN) based meth-ods not only in image classification [22, 23, 26] but also in many dense prediction tasks such as semantic segmenta-tion [11, 47, 59, 68], monocular depth estimation [40, 61], and surface normal prediction [24, 62]. Therefore, utiliz-ing the power of vision transformers in a unified framework to simultaneously solve multiple tasks seems a natural way forward. Nevertheless, only a few works [3, 7, 21, 35, 44] have attempted this so far, and all of them rely on hand-crafted transformer architecture designs. Specifically, IPT and ST-MTL [7, 35] exploit a multi-head multi-tail archi-tecture tailored to solve specific tasks; MulT [3] lever-ages a pairwise task attention strategy handcrafted to uti-lize surface normal prediction as reference task for dense
Figure 1: Motivation of our work. Unlike existing MTL methods, our vision transformer adapters generalize to novel tasks and domains. predictions; and UniT [21] as well as Vid-MTL [44] use a multimodal transformer architecture to achieve multi-ple pairwise task predictions across different modalities.
While these multitasking vision transformer-based meth-ods [7, 3, 21, 35, 44] outperform their multitasking CNN-based counterparts [29, 31, 34, 45, 48, 52, 53, 58, 65, 66], none of the existing vision transformer-based or CNN-based MTL methods can adapt to new tasks as well as to novel domains. In fact, it was observed in the seminal work of [66] and confirmed in subsequent MTL studies [3, 45, 65] that the multitask affinities learned by existing MTL frame-works are not transferable or generalizable.
This raises the following question: Is there a way we can learn transferable and generalizable task affinities such that multitask affinities transfer to novel tasks and generalize to novel domains, thereby allowing us to reuse an existing network? To answer this, we introduce vision transformer adapters for generalizable multitask learning and propose an automated framework that can learn transferable and generalizable task affinities which can adapt to new tasks or domain representations in a parameter-efficient manner.
Additionally, unlike existing transformer-based handcrafted
MTL methods [7, 3, 21] that learn task affinities in a pair-wise manner, our vision transformer adapters learn task affinities in an automated way and across all the tasks.
To achieve this, we equip our vision adapters with three
mechanisms: (1) An improved gradient-based task similar-ity approach (TROA) first introduced in [8]; (2) a novel task-adapted attention mechanism (TAA) that combines the gradient-based task similarities with attention-based ones, thereby learning transferable and generalizable task affini-ties; and (3) a task-scaled normalization to account for the different task scales. The resulting module can then be seamlessly integrated with a pre-trained, frozen encoder backbone architecture such as ViT [16], Swin [30], Pyramid
Transformer [56], or Focal Transformer [63]. Our approach is independent of the choice of the vision transformer back-bone, unlike existing transformer-based MTL methods. Our contributions are summarized as follows:
• We introduce vision adapters for generalizable multi-task learning that leverages a pre-trained vision trans-former backbone to learn transferable and generaliz-able features at a low computational cost.
• At the heart of our vision adapter, we introduce a novel task-adapted attention mechanism (TAA) that auto-matically learns task dependencies from the shared representation, by combining gradient-based task sim-ilarities (TROA) with attention-based ones.
• Our task affinities transfer to different settings includ-ing multitask learning, zero-shot task transfer learning, and unsupervised domain adaptation. Moreover, our task affinities generalize to novel domains without re-quiring any fine-tuning.
• Our multitasking vision transformer adapters can be integrated with different transformer backbones such as ViT [16], Swin [30], Pyramid Transformer [56], and Focal Transformer [63], achieving a significant in-crease in performance in a parameter-efficient way.
Our experiments evidence that our method outperforms both state-of-the-art CNN-based multitasking methods [8, 29, 31, 34, 37, 48, 52, 65, 66] as well as transformer-based ones [3, 21]. 2.