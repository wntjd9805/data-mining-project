Abstract
This paper aims to develop an accurate 3D geometry representation of satellite images using satellite-ground im-age pairs. Our focus is on the challenging problem of 3D-aware ground-views synthesis from a satellite image.
We draw inspiration from the density field representation used in volumetric neural rendering and propose a new ap-proach, called Sat2Density. Our method utilizes the prop-erties of ground-view panoramas for the sky and non-sky regions to learn faithful density fields of 3D scenes in a geometric perspective. Unlike other methods that require extra depth information during training, our Sat2Density can automatically learn accurate and faithful 3D geom-etry via density representation without depth supervision.
This advancement significantly improves the ground-view panorama synthesis task. Additionally, our study provides a new geometric perspective to understand the relationship between satellite and ground-view images in 3D space. 1.

Introduction
The emergence of satellite imagery has significantly en-hanced our daily lives by providing easy access to a com-prehensive view of the planet. This bird’s-eye view offers valuable information that compensates for the limited per-spective of ground-level observations by humans. How-ever, what specific information does satellite imagery pro-vide, and why is it so crucial? In this paper, we propose that the most critical insights come from the analysis of the geometry, topology, and geography of cross-view observa-tions captured by paired satellite and ground-level images.
Building on this hypothesis, we aim to address the chal-lenging problem of synthesizing ground-level images from paired satellite and ground-level imagery by leveraging den-sity representations of 3D scenes.
The challenge of generating ground-level images from satellite imagery is tackled by leveraging massive datasets containing both satellite images and corresponding ground-*Corresponding author (a) Learned density from satellite images (b) Synthesized panoramas (c) Rendered depth
Figure 1. Sat2Density trains with a collection of satellite-ground image pairs, without depth, or multi-view supervision. Our 3d
GAN enables the synthesis of scenes conditioned on a satellite image, producing multi-view-consistent ground-view renderings and geometry. Please see the project page for more videos. level panoramas captured at the same geographical coor-dinates. However, the drastic differences in viewpoint be-tween the two types of images, combined with the limited overlap of visual features and large appearance variations, create a highly complex and ill-posed learning problem. To address this challenge, researchers have extensively stud-ied the use of conditional generative adversarial networks, which leverage high-level semantics and contextual infor-mation in a generative way [19, 20, 33, 26, 12]. However, since the contextual information used is typically at the im-age level, the 3D information can only be marginally in-ferred during training, often resulting in unsatisfactory syn-thesis results.
Recent studies [22, 11] have suggested that accurate 3D scene geometry plays a crucial role in generating high-quality ground-view images. With extra depth supervision,
Sat2Video [11] introduced a method to synthesize spatial-temporal ground-view video frames along a camera trajec-tory, rather than a single panorama from the center view-point of the satellite image. Additionally, Shi et al. [22] demonstrated that coarse satellite depth maps can be learned from paired data through multi-plane image representation using a novel projection model between the satellite and ground viewpoints, but a coarse 3d representation can not facilitate rendering 3D-aware ground-view images. Build-ing on these insights, we aim to investigate whether it is possible to achieve even more accurate 3D geometry using the vast collection of satellite-ground image pairs.
Our study is motivated by the latest developments in the neural radiance field (NeRF) [16], which has shown promising results in novel view synthesis. Benefiting from the flexibility of density field in volumetric rendering [8], faithful 3D geometry can be learned from a large number of posed images. Therefore, we adopt density fields as the representation and focus on learning accurate density fields from paired satellite-ground image pairs. More pre-cisely, in this paper, we present a novel approach called
Sat2Density, which involves two convolutional encode-decoder networks: DensityNet and RenderNet. The Den-sityNet receives satellite images as input to represent the density field in an explicit grid, which plays a crucial role in producing ground-view panorama images using the Render-Net. With such a straightforward network design, we delve into the goal of learning faithful density field first and then render high-fidelity ground-view panoramas.
While we employed a flexible approach to representing geometry using explicit volume density and volumetric ren-dering, an end-to-end learning approach alone is inadequate for restoring geometry using only satellite-ground image pairs. Upon examining the tasks and satellite-ground im-age pairs, we identified two main factors that may impede geometry learning, which has been overlooked in previous works on satellite-to-ground view synthesis. Firstly, the sky is an essential part of ground scenes but is absent in the satellite view, and it is nearly impossible to learn a faith-ful representation of the infinite sky region in each image using explicit volume density. Secondly, differences in illu-mination among the ground images during training make it challenging to learn geometry effectively.
With the above intuitive observation, we propose two supervision signals, the non-sky opacity supervision and illumination injection, to learn the density fields in a vol-umetric rendering form jointly. The non-sky opacity su-pervision compels the density field to focus on the satel-lite scene and ignore the infinity regions, whereas the illu-mination injection learns the illumination from sky regions to further regularize the learning density field. By learning the density field, our Sat2Density approach goes beyond the center ground-view panorama synthesis from the training data and achieves the ground-view panorama video synthe-sis with the best spatial-temporal consistency. As shown in Figure 1, our Sat2Density continuously synthesizes the panorama images along the camera trajectory. We evaluated the effectiveness of our proposed approach on two large-scale benchmarks [22, 34] and obtained state-of-the-art per-formance. Comprehensive ablation studies further justified our design choices.
The main contributions of our paper are:
• We present a geometric approach, Sat2Density, for ground-view panorama synthesis from satellite images in end-to-end learning. By explicitly modeling the challenging cross-view synthesis task in the density field for the 3D scene geometry, our Sat2Density is able to synthesize high-fidelity panoramas on camera trajectories for video synthesis without using any extra 3D information out of the training data.
• We tackle the challenging problem of learning high-quality 3D geometry under extremely large viewpoint changes. By analyzing the unique challenges that arise with this problem, we present two intuitive approaches non-sky opacity supervision and illumination injection to compel the density learning to focus on the relevant features in the satellite scene presented in the paired data while mitigating the effects of infinite regions and illumination changes.
• To the best of our knowledge, we are the first to successfully learn a faithful geometry representation from satellite-ground image pairs. We believe that not only do our new findings improve the performance of ground-view panorama synthesis, but the learned faith-ful density will also provide a renewed understanding of the relationship between satellite and ground-view image data from a 3D geometric perspective. 2.