Abstract
In this paper, we rethink the low-light image enhance-ment task and propose a physically explainable and gen-erative diffusion model for low-light image enhancement, termed as Diff-Retinex. We aim to integrate the advantages of the physical model and the generative network. Further-more, we hope to supplement and even deduce the infor-mation missing in the low-light image through the genera-tive network. Therefore, Diff-Retinex formulates the low-light image enhancement problem into Retinex decompo-sition and conditional image generation.
In the Retinex decomposition, we integrate the superiority of attention in Transformer and meticulously design a Retinex Trans-former decomposition network (TDN) to decompose the image into illumination and reﬂectance maps. Then, we design multi-path generative diffusion networks to recon-struct the normal-light Retinex probability distribution and solve the various degradations in these components respec-tively, including dark illumination, noise, color deviation, loss of scene contents, etc. Owing to generative diffusion model, Diff-Retinex puts the restoration of low-light sub-tle detail into practice. Extensive experiments conducted on real-world low-light datasets qualitatively and quantita-tively demonstrate the effectiveness, superiority, and gener-alization of the proposed method. 1.

Introduction
Images taken in low-light scenes are usually affected by a variety of degradations, such as indeﬁnite noise, low con-trast, variable color deviation, etc. Among these degrada-tions, the loss of scene structures is the most thorny. As shown in Fig. 1, the loss of scene structures is not only limited to affecting the visual effect but also reducing the amount of information.
Image enhancement is an effec-tive approach to reduce the interference of degradations on
*Corresponding author 1These authors contributed equally to this work.
Figure 1. Example of URetinex [40] and Diff-Retinex for LLIE.
Diff-Retinex can repair some missing scene contents by rethinking
LLIE through the generative diffusion model. human perception and subsequent vision tasks, and ﬁnally present high-quality images.
To handle these degradations, many low-light image en-hancement (LLIE) methods have been proposed [20, 26].
Besides, a series of research on contrast enhancement, noise removal, and texture preservation have been car-ried out. The mainstream LLIE methods can be roughly divided into traditional [35, 3] and learning-based meth-ods [22, 15, 25, 23, 18]. Traditional algorithms are usually based on the image prior or simple physical models. For instance, gray transform [35, 14] and histogram equaliza-tion [5, 34] adjust the intensity distribution by linear or non-linear means. The Retinex model [12, 16, 24] decomposes the image into illumination and reﬂectance images, and
the problem is solved with traditional optimization meth-ods. However, these methods are also subject to manual de-sign and optimization-driven efﬁciency. They usually suffer from poor generalization and robustness, limiting the appli-cation scope of these methods.
To address these drawbacks, deep learning is utilized to construct the complex mapping from low-light to normal-light image [22, 38]. Some methods entirely regard low-light image enhancement as a restoration task through an overall ﬁtting, lacking theoretical support and interpretabil-ity of physical models. Compared with physical model-based methods, they usually exhibit less targeted enhance-ment performance, manifested as uneven illumination, non-robustness to noise, etc. The main cause is the shortage of speciﬁc deﬁnitions of some degradations and targeted pro-cessing for them. The physical model-based methods de-compose the image into components with physical signiﬁ-cance. Then, speciﬁc processing is conducted on the com-ponents for more targeted enhancement.
However, existing methods hardly escape the essence of
ﬁtting. More concretely, the distorted scene can be better rendered through denoising in the existing methods while the missing scene content cannot be repaired. Take Fig. 1 as an example, the state-of-the-art method (URetinex [40]) cannot restore the weak and missing details and even ag-gravates the information distortion to a certain extent. To solve this drawback and considering that LLIE is a pro-cess of recovering a normal-light image with the guidance of the low-light image, we rethink LLIE with a generative diffusion model. We aim to recover or even reason out the weak, even the lost information in the original low-light im-age. Thus, LLIE is regarded as not only a restoration ﬁtting function but also an image generation task with the condi-tion. As for the generative model, generative adversarial networks (GAN) [36, 46] train a generator and a discrim-inator in an adversarial mechanism. However, they suffer from training instability, resulting in problems such as mode collapse, non-convergence, and exploding or vanishing gra-dients. Moreover, the GAN-based LLIE methods also have the problem of directly generating the normal-light image through an overall ﬁtting, lacking physical interpretability as mentioned before.
To this end, we propose a physically explainable and generative model for low-light image enhancement, termed as Diff-Retinex. We aim to integrate the advantages of the physical model and the generative network. Thus, Diff-Retinex formulates the low-light image enhancement prob-lem to Retinex decomposition and conditional image gener-ation. In the Retinex decomposition, we integrate the char-acteristics of Transformer [21, 41] and meticulously design a Retinex Transformer decomposition network (TDN) to improve the decomposition applicability. TDN decomposes the image into illumination and reﬂectance maps. Then, we design generative diffusion-based networks to solve the var-ious degradations in these components respectively, includ-ing dark illumination, noise, color deviation, loss of scene contents, etc. The main contributions are summarized as:
- We rethink low-light image enhancement from the per-spective of conditional image generation. Rather than being limited to enhancing the original low-quality in-formation, we propose a generative Retinex framework to further compensate for content loss and color devi-ation caused by low light.
- Considering the issues of decomposition in Retinex models, we propose a novel Transformer decomposi-tion network. It can take full advantage of the attention and layer dependence to efﬁciently decompose images, even for images of high resolutions.
- To the best of our knowledge, it is the ﬁrst study that applies the diffusion model with Retinex model for low-light image enhancement. The diffusion model is applied to guide the multi-path adjustments of illumi-nation and reﬂectance maps for better performance. 2.