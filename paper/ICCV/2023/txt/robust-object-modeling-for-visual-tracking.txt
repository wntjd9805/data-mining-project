Abstract
Object modeling has become a core part of recent track-ing frameworks. Current popular tackers use Transformer attention to extract the template feature separately or in-teractively with the search region. However, separate tem-plate learning lacks communication between the template and search regions, which brings difficulty in extracting dis-criminative target-oriented features. On the other hand, in-teractive template learning produces hybrid template fea-tures, which may introduce potential distractors to the tem-plate via the cluttered search regions. To enjoy the mer-its of both methods, we propose a robust object modeling framework for visual tracking (ROMTrack), which simulta-neously models the inherent template and the hybrid tem-plate features. As a result, harmful distractors can be sup-pressed by combining the inherent features of target objects with search regions’ guidance. Target-related features can also be extracted using the hybrid template, thus resulting in a more robust object modeling framework. To further en-hance robustness, we present novel variation tokens to de-pict the ever-changing appearance of target objects. Varia-tion tokens are adaptable to object deformation and appear-ance variations, which can boost overall performance with negligible computation. Experiments show that our ROM-Track sets a new state-of-the-art on multiple benchmarks. 1.

Introduction
Visual object tracking (VOT) [1, 4, 8, 12, 22, 25, 35, 39, 41, 47, 57, 60] is a fundamental task in computer vi-sion, which aims at localizing an arbitrary target in video sequences given its initial status. The occlusion, scale vari-ation, object deformation, and co-occurrence of distractor objects pose a challenge to acquiring an effective tracker in real-world scenarios. Current dominating trackers typically address these problems with a Transformer-based [42] ar-chitecture.
*Corresponding author.
Figure 1: Three typical object modeling methods for template-search feature learning, together with our Robust
Modeling design. ht, it, and vt represent hybrid tem-plate, inherent template, and variation tokens, respectively. sr represents the search region. SA and CA denote self-attention and cross-attention, respectively.
The core components in a typical Transformer tracking framework are the object modeling blocks. As demon-strated in Figure 1(a), the two-stream hybrid modeling methods [6, 50] learn the template feature interactively with the search region via two cross-attention (CA) operations.
Instead of cross-attention, the one-stream hybrid model-ing methods [5, 54] in Figure 1(c) jointly learn the hy-brid template feature and search region feature with one self-attention (SA) operation. Different from hybrid tem-plate modeling, the two-stream separate modeling [10, 23] in Figure 1(b) keeps an inherent template stream to ensure the purity of template features. Separate template learning can keep the inherent features of target templates, which prevents interference from search regions. Though suffer-ing from potential distractors, hybrid template learning con-ducts extensive feature matching between the template and search region, thus allowing mutual guidance for target-oriented feature extraction.
In order to enjoy the merits of separate and hybrid tem-plate modeling simultaneously, we propose a robust ob-ject modeling framework for visual tracking (named ROM-Track). As shown in Figure 1(d), our robust modeling scheme involves two kinds of templates, the inherent tem-plate it and the hybrid template ht. Meanwhile, our scheme also designs the novel variation tokens vt. The inher-ent template applies self-attention separately to enhance its learned feature. Besides, it accepts queries from the hy-brid template and the search region features to provide in-herent information for discriminative target-oriented feature learning. The bottom part of Figure 1(d) is a hybrid atten-tion that adopts a standard cross-attention operation to en-hance the template and search region features with mutual guidance. Furthermore, it is well-recognized that tracking is a task suffering from object deformation and appearance variations [10, 57]. We tackle this problem by introducing novel variation tokens to improve robustness. It is observed that the target’s motion during a short period is usually smooth but may be accompanied by large changes in ap-pearance [6, 58]. The tracker can easily handle smooth mo-tion, but appearance changes are hard to distinguish. There-fore, we generate variation tokens from hybrid template fea-tures to leverage appearance information during tracking.
Despite the simplicity, our variation tokens perform well with negligible computation.
The main contributions of this work are three-fold:
• We propose a robust object modeling framework for visual tracking (ROMTrack). It can keep the inherent information of the target template and enables mutual feature matching between the target and the search re-gion simultaneously.
• We present a neat and effective variation-token design that embeds the object’s appearance context during tracking into the attention calculation of hybrid target-search features.
• The proposed ROMTrack sets a new state-of-the-art performance on six challenging benchmarks, includ-ing GOT-10k [26], LaSOT [18], TrackingNet [38],
LaSOText [17], OTB100 [49], and NFS30 [21]. 2.