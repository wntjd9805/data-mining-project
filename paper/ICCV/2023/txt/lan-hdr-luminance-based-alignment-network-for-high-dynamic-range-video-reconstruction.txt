Abstract
Low
Middle
High
As demands for high-quality videos continue to rise, high-resolution and high-dynamic range (HDR) imaging techniques are drawing attention. To generate an HDR video from low dynamic range (LDR) images, one of the critical steps is the motion compensation between LDR frames, for which most existing works employed the op-tical flow algorithm. However, these methods suffer from flow estimation errors when saturation or complicated mo-tions exist. In this paper, we propose an end-to-end HDR video composition framework, which aligns LDR frames in the feature space and then merges aligned features into an HDR frame, without relying on pixel-domain optical flow. Specifically, we propose a luminance-based align-ment network for HDR (LAN-HDR) consisting of an align-ment module and a hallucination module. The alignment module aligns a frame to the adjacent reference by evalu-ating luminance-based attention, excluding color informa-tion. The hallucination module generates sharp details, especially for washed-out areas due to saturation. The aligned and hallucinated features are then blended adap-tively to complement each other. Finally, we merge the features to generate a final HDR frame.
In training, we adopt a temporal loss, in addition to frame reconstruc-tion losses, to enhance temporal consistency and thus re-duce flickering. Extensive experiments demonstrate that our method performs better or comparable to state-of-the-art methods on several benchmarks. Codes are available at https://github.com/haesoochung/LAN-HDR. 1.

Introduction
As diverse videos become easily accessible through video-on-demand services, demands for high-quality video content with high resolution and high dynamic range (HDR) are naturally increasing. HDR content can provide a rich s t u p n
I s l e n n a h c
Y s t l u s e
R r u
O
Chen [5] Ours
Chen [5] Ours
Chen [5] Ours
Figure 1: Visual comparisons of the proposed and Chen et al.’s method [5]. Our framework generates more detailed texture in regions with saturation and motions by utilizing luminance information. viewing experience by displaying high contrast and a broad range of colors. While HDR displays are already ubiqui-tous, there is still a lack of HDR content available for deliv-ery.
Whereas HDR imaging techniques for still images have been actively studied [8, 2, 33, 18, 45, 47, 35, 3, 38, 32, 10, 9, 1, 26, 49], those for videos have been relatively over-looked. Some early methods [41, 40, 23] proposed well-designed hardware systems for direct HDR video acquisi-tion, but these systems mostly require a sophisticated de-sign and high cost. Since Kang et al. [21] reconstructed an
HDR video using a low dynamic range (LDR) image se-quence with alternating exposures, this approach has been 1
commonly adopted. Specifically, an HDR frame is gener-ated from a corresponding LDR frame and its neighbor-ing frames, whose exposures alternate between two or three values, after motions between the frames are compensated.
For instance, Mangiat et al. [28, 29] performed block-based motion estimation and then refined the results using color similarity and filtering, respectively. Kalantari et al. [20] synthesized multi-exposure images at each time step using patch-based optimization and merged them into an HDR frame. These approaches mostly require considerable time for the optimization process and suffer from artifacts result-ing from inaccurate alignment.
With the development of deep learning, convolutional neural network (CNN)-based HDR video reconstruction methods have been proposed. Similar to the previous HDR imaging method [18], Kalantari et al. [19] aligned neighbor-ing frames to the middle frame using optical flow and com-bined them with a simple merging network. More recently,
Chen et al. [5] trained a two-step alignment network using optical flow and deformable convolution with large datasets.
These methods significantly improved the quality of result-ing HDR videos, but they still show some distortions and ghosting artifacts caused by optical flow estimation error, especially when motion exists in saturated regions or mo-tion is fast. (See Fig. 1.)
In this paper, we propose a luminance-based alignment network (LAN) for HDR video composition. The LAN has a dual-path architecture, where an alignment module reg-isters motions between frames using attention [42], and a hallucination module generates fine details, especially for saturated regions. In the alignment module, we rearrange a neighboring frame based on the attention score between itself and the reference frame. However, by applying naive attention, the system may find similar pixels solely by color.
For content-based matching between two frames, we extract key and query features using only the luminance channel, which contains essential information on edge and structure.
Here, downsampled frames are used as input to alleviate memory consumption. However, alignment cannot be per-fect because downsampled inputs lack high-frequency in-formation, and saturation hinders precise matching. Thus, the hallucination module fills in the missing details, espe-cially in saturated areas using full-sized inputs, where we use gated convolution [50] with a mask that represents the image brightness. This mask enables the adaptive operation for very dark regions as well as highlighted areas. The fea-tures from each module are fused in an adaptive blending layer, which determines the contribution of each feature in terms of spatial dimension. Finally, we adopt a temporal loss for the resulting video to have consistent motions. Ex-tensive experiments demonstrate that our method produces a clean HDR video from an LDR video with alternating ex-posures.
The main contributions of our paper are summarized as follows:
• We introduce an end-to-end HDR video reconstruction framework that performs a precise motion alignment using the proposed luminance-based alignment network (LAN). The LAN consists of two novel components: an alignment module and a hallucination module.
• The proposed alignment module performs content-based alignment by rearranging a neighboring frame feature (value) according to the attention score between content features from the neighboring frame (key) and the reference frame (query).
• The proposed hallucination module generates sharp de-tails using adaptive convolution, especially for very bright or dark regions.
• We present a temporal loss to produce temporally co-herent HDR videos without flickering. 2.