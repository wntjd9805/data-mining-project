Abstract 1.

Introduction and Motivation
The intelligent synthesis/generation of daily-life motion sequences is fundamental and urgently needed for many
VR/metaverse-related applications. However, existing ap-proaches commonly focus on monotonic motion generation (e.g., walking, jumping, etc.) based on single instruction-like text, which is still not intelligent enough and can’t meet practical demands. To this end, we propose a cohe-sive human motion sequence synthesis framework based on free-form sequential texts while ensuring semantic connec-tion and natural transitions between adjacent motions. At the technical level, we explore the local-to-global seman-tic features of previous and current texts to extract rele-vant information. This information is used to guide the framework in understanding the semantics of the current moment. Moreover, we propose learnable tokens to adap-tively learn the influence range of the previous motions to-wards natural transitions. These tokens can be trained to encode the relevant information into well-designed transi-tion loss. To demonstrate the efficacy of our method, we conduct extensive experiments and comprehensive evalua-tions on the public dataset as well as a new dataset pro-duced by us. All the experiments confirm that our method outperforms the state-of-the-art methods in terms of seman-tic matching, realism, and transition fluency. Our project is public available. https://druthrie.github.io/ sequential-texts-to-motion/
*Corresponding author
Human motion synthesis is fundamental for numer-ous application, especially for virtual reality, games, and metaverse-related applications [10, 19, 5, 3, 36, 15, 22, 20, 24], of which, it is in high demand to accurately control the digital human motion with natural language. Existing approaches commonly focus on monotonic motion synthe-sis/generation based on single instruction-like text descrip-tion. However, practical applications usually require digi-tal humans to respond to multiple rounds of sustainable in-teractions, wherein they can continuously generate reason-able responses to the sequential texts. Therefore, given a set of free-form sequential texts, we aim to synthesize a cohe-sive human motion sequence. Namely, the animation clips should be consistent with the ongoing text descriptions, and the whole motion sequence should have smooth semantic connection and natural transitions.
At present, sequential texts-driven motion synthesis has not been well studied, mainly due to lacking long-term continuous motion datasets with accompanying free-form text descriptions. Recently, TEACH [6] first attempts to address this problem by proposing a dataset (we refer to it as BABEL-TEACH). Each item in the BABEL-TEACH dataset contains two adjacent text-motion pairs. However,
TEACH [6] builds on the one-time method TEMOS [29], wherein the limited 5-frame motion is added to encode to-gether. However, it does not consider the potential benefits of previous text information. In practice, previous texts can provide extra and more accurate semantic information for
current motion synthesis. Therefore, global semantic needs to be extracted from the previous text and the local seman-tics from the current text, and then fuses the semantics to guide the semantic understanding at each moment.
Besides, there are still two main challenges. Firstly, the semantic relationship between adjacent texts is largely over-looked in previous research. For example, when synthesiz-ing a motion for the current text “touching the face with left hand,” the previous text context should be considered.
Specifically, if the previous text was “sitting on a chair,” the synthesized motion should depict a person sitting and touching face with left hand (shown in Fig. 2); if the previ-ous text was “a person kicks a ball with right foot,” the mo-tion should show a standing person touching face with left hand. This requires a more comprehensive understanding about the context and semantic relationships between the texts. Secondly, existing methods tend to abrupt transitions between adjacent motions when multiple motions are syn-thesized separately and stitched together. We aim to address this issue by seamlessly blending the synthesized motions.
It should intelligently capture the temporal and spatial con-tinuity between the motions to ensure a more natural and coherent sequence of motions.
Considering the easy scalability of autoregressive meth-ods, we should extend previous autoregressive single text-driven motion synthesis methods to multiple motions in-volved in motion sequence synthesis. We observe that a similar human body posture can be achieved by transferring the end features of the previous motion to the next motion, although this is not always entirely consistent. This sug-gests that previous motion information should be leveraged more effectively and sufficiently rather than being adopted straightforwardly. Therefore, we plan to introduce a transi-tion reasoning module that adaptively learns attention score from the previous motion information and infers accurate motion features at the transition, making the transition be-tween adjacent motions more natural and smooth.
To this end, we create a new dataset with much longer-term motions (2-5 times longer) based on a synthetic method, and the corresponding text descriptions are more abundant. We conduct experiments on both BABEL-TEACH [6] dataset and our sequential texts described mo-tion (STDM) dataset. The results demonstrate that our method outperforms existing methods in terms of seman-tic matching, realism, and transition fluency. Specially, the salient contributions can be summarized as follows.
• We propose a cohesive motions synthesis framework using sequential texts as inputs. The framework can integrate previous text information to obtain semantic features and adaptively select valid previous motion in-formation to guide the current motion synthesis.
• We design a local-to-global (L2G) semantic fusion module to extract accurate contextual information. It
Figure 2: Top row: Motion synthesis corresponding to se-quential texts by our method. Bottom row: Motion synthe-sis corresponding to single text by T2M [11]. can well take into account global-context information from the previous text to infer the current moment’s text meaning to guide the semantics-consistent motion synthesis.
• We introduce a transition reasoning module to adap-tively select motion snippet from previous motion in-formation to make the synthesized motion natural and smooth w.r.t the previous, wherein a well-defined tran-sition loss is employed to further constrain the fluency of the transitions.
• We create a new sequential texts described motion (STDM) dataset, which involves more extended mo-tion frames and more diverse text descriptions than the existing dataset. 2.