Abstract
We propose a self-supervised method for learning motion-focused video representations. Existing approaches minimize distances between temporally augmented videos, which maintain high spatial similarity. We instead propose to learn similarities between videos with identical local mo-tion dynamics but an otherwise different appearance. We do so by adding synthetic motion trajectories to videos which we refer to as tubelets. By simulating different tubelet mo-tions and applying transformations, such as scaling and ro-tation, we introduce motion patterns beyond what is present in the pretraining data. This allows us to learn a video rep-resentation that is remarkably data efficient: our approach maintains performance when using only 25% of the pre-training videos. Experiments on 10 diverse downstream set-tings demonstrate our competitive performance and gener-alizability to new domains and fine-grained actions. Code is available at https://github.com/fmthoker/tubelet-contrast. 1.

Introduction
This paper aims to learn self-supervised video represen-tations, useful for distinguishing actions. In a community effort to reduce the manual, expensive, and hard-to-scale annotations needed for many downstream deployment set-tings, the topic has witnessed tremendous progress in re-cent years [18, 31, 62, 79], particularly through contrastive learning [15,56,58,60]. Contrastive approaches learn repre-sentations through instance discrimination [55], by increas-ing feature similarity between spatially and temporally aug-mented clips from the same video. Despite temporal differ-ences, such positive video pairs often maintain high spatial similarity (see Figure 1), allowing the contrastive task to be solved by coarse-grained features without explicitly captur-ing local motion dynamics. This limits the generalizability of the learned video representations, as shown in our prior work [70]. Furthermore, prior approaches are constrained by the amount and types of motions present in the pretrain-ing data. This makes them data-hungry, as video data has high redundancy with periods of little to no motion. In this work, we address the need for data-efficient and general-izable self-supervised video representations by proposing a contrastive method to learn local motion dynamics.
Figure 1: Tubelet-Contrastive Positive Pairs (bottom) only share the spatiotemporal motion dynamics inside the simulated tubelets, while temporal contrastive pairs (top) suffer from a high spatial bias. Contrasting tubelets results in a data-efficient and generalizable video representation.
We take inspiration from action detection, where tubelets are used to represent the motions of people and objects in videos through bounding box sequences e.g., [29, 32, 42].
Typically, many tubelet proposals are generated for a video, which are processed to find the best prediction. Rather than finding tubelets in video data, we simulate them. In partic-ular, we sample an image patch and ‘paste’ it with a ran-domized motion onto two different video clips as a shared tubelet (see Figure 1). These two clips form a positive pair for contrastive learning where the model has to rely on the spatiotemporal dynamics of the tubelet to learn the similar-ity. With such a formulation, we can simulate a large variety of motion patterns that are not present in the original videos.
This allows our model to be data-efficient while improving generalization to new domains and fine-grained actions.
We make four contributions. First, we explicitly learn from local motion dynamics in the form of synthetic tubelets and design a simple but effective tubelet-contrastive
framework. Second, we propose different ways of sim-ulating tubelet motion and transformations to generate a variety of motion patterns for learning. Third, we reveal the remarkable data efficiency of our proposal: on five ac-tion recognition datasets our approach maintains perfor-mance when using only 25% of the pretraining videos.
What is more, with only 5-10% of the videos we still out-perform the vanilla contrastive baseline with 100% pre-training data for several datasets. Fourth, our compar-ative experiments on 10 downstream settings, including
UCF101 [67], HMDB51 [37], Something Something [20], and FineGym [63], further demonstrate our competitive per-formance, generalizability to new domains, and suitability of our learned representation for fine-grained actions. 2.