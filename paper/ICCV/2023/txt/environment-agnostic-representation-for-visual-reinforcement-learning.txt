Abstract
Generalization capability of vision-based deep reinforce-ment learning (RL) is indispensable to deal with dynamic environment changes that exist in visual observations. The high-dimensional space of the visual input, however, imposes challenges in adapting an agent to unseen environments.
In this work, we propose Environment Agnostic Reinforce-ment learning (EAR), which is a compact framework for domain generalization of the visual deep RL. Environment-agnostic features (EAFs) are extracted by leveraging three novel objectives based on feature factorization, reconstruc-tion, and episode-aware state shifting, so that policy learning is accomplished only with vital features. EAR is a simple single-stage method with a low model complexity and a fast inference time, ensuring a high reproducibility, while attain-ing state-of-the-art performance in the DeepMind Control
Suite and DrawerWorld benchmarks. Code is available at: https://github.com/doihye/EAR. 1.

Introduction
Deep reinforcement learning (RL) plays an important role in various fields such as robotic manipulation, video games, and autonomous navigation. Among them, RL ap-proaches using visual observations [32, 26, 31, 7, 58, 1, 46] have achieved appreciable success in that dense and rich in-formation can be obtained easily through the camera. Since real world changes dynamically at all times, the ability to generalize an agent against visual variations is indispensable in this field. However, the high-dimensional observation space of visual inputs [5, 41] imposes several challenges in adapting the agent to unseen environments [8, 4].
To learn robust policies invariant to visual changes, do-main randomization methods [44, 37, 36, 53, 38] have been proposed based on the surmise that applying miscellaneous augmentations during a training phase empowers an agent
*This work was supported by the Mid-Career Researcher Program (2021R1A2C2011624) and the Basic Research Laboratory Program (2021R1A4A1032582) from NRF Korea, and the Artificial Intelligence Con-vergence Innovation Human Resources Development (RS-2022-00155966) from Institute of Information & communications Technology Planning &
Evaluation (IITP). â€  Corresponding author: dbmin@ewha.ac.kr
Figure 1. From visual observations with the same agent state under different environments (o1: original environment, o2: random box, o3: color jitter), EAR separates environment-agnostic feature and environment-specific feature to learn vital representation used for
RL policy learner. to cover any test environments. However, a considerable amount of effort is required to set the degree of randomiza-tion, and more importantly, it is practically impossible to fully consider all variations of test scenarios.
Instead of increasing the variability in training data, do-main adaptation approaches have attempted to adapt the policy ingenuously to a test domain [39, 20, 19]. However, these methods often require additional fine-tuning during in-ference with a reward function that is newly created for each unseen environment. Alternatively, PAD [12] proposes to adapt an auxiliary self-supervised task (e.g. inverse dynam-ics prediction and rotation prediction) to obtain supervisory during test time without using reward. While this approach benefits greatly from an online learning, an appropriate aux-iliary task should be selected depending on the specific RL task. More importantly, the additional training stage in the novel environment causes extra inference time, impairing an overall computational efficiency.
More recently, as an explicit approach based on domain generalization, VAI [49] proposes to learn visual foreground masks from augmented input images to feed only foreground-related information to a policy network. Though a high gen-eralization performance was reported in VAI, several short-comings still limit the use of this method. Since the policy training process requires two pre-training steps of auxiliary tasks (e.g. keypoint detection and visual attention predic-tion), the learning process is complex, requiring substantial computational costs compared to existing works. Moreover,
the image that VAI finally uses for learning the policy is generated using the multiplication of the foreground mask and the augmented image, therefore remnants of the envi-ronmental changes (e.g. foreground changes) still remain, hampering the policy training.
In this paper, we propose a novel generalization method, termed Environment Agnostic Reinforcement learning (EAR), which is a compact framework for domain gener-alization of the visual deep RL. To be more specific, our method attempts to extract an environment-agnostic feature (EAF) and use it to learn the RL policy network. By simply adopting novel objectives, our method can be trained in a single step, without the complicated procedure consisting of multiple individual training steps as in [49]. For extracting the EAFs from input images, we provide a feature factoriza-tion constraint to EAFs and remaining environment-specific features (ESFs) which are separated from the latents, and at the same time impose a reconstruction constraint computed by reversely combining EAFs and ESFs separated from two consecutive frames. Moreover, the proposed method intro-duces an episode-aware state shifting (ESS) constraint to
EAFs in a self-supervised Siamese framework. By utilizing the proposed domain generalization task in an end-to-end manner with RL, EAR learns the process of extracting an
EAF from an image, so that policy learning is accomplished only with the vital agent-related information, as depicted in
Figure 1.
It is noteworthy that we aim to improve the generaliza-tion ability and at the same time maintain a lightweight encoder used by prior RL algorithms [11, 23] without any additional architectures [17, 51]. To achieve this, we as-sumed additive feature factorization and designed the novel objectives. Moreover, the ESS module imposes an important constraint that the separated EAFs, which we assume as a newly generated state, must be consistent in terms of the episode-aware state shifting. In Table 6, we show that the
ESFs should be excluded when training the policy network.
EAR is the first attempt to generate a novel agent-related states to make them episode-consistent, yielding a signifi-cant impact on the robustness of performance. This is easily confirmed by referring to the standard deviation of episode returns with and without the ESS module (Lss) in Table 5.
The proposed ESS module works complementary to the fea-ture factorization framework, since both constraints aim to encode agent-related representation.
In simulation, we present an extensive evaluation on the
DeepMind (DM) Control Suite [43] including DM Con-trol Generalization Benchmark [14] and Distracting Control
Suite [42], which introduce a number of visual distractors to analyze whether the trained agent performs well in var-ious environments. For evaluating the robustness of the trained agent on realistic textures, we also conduct experi-ments on the DrawerWorld [49] robotic manipulation tasks which add texture distortion and background distortion to
MetaWorld [54] benchmark. Our method achieves a high generalization performance, outperforming the state-of-the-art methods in tasks of DM Control Suite [43] by up to 50.1% and DrawerWorld [49] by up to 97.7% with low model com-plexity and computational cost (Table 1). Notably, empirical evaluations show that while EAR does not require any extra costly adaptation during the test time [12] or using extra training stages [49], it achieves the state-of-the-art perfor-mance over existing methods.
To summarize, EAR has the following affirmative assets. 1) Novel domain generalization framework designed for visual RL. EAR proposes a novel framework tailored to
RL setup, which segregates the intrinsic property needed for policy learning. Moreover, the generalization process of
EAR requires no manual annotations or prior knowledge of environments. Accordingly, EAR can be readily adopted in any task that is subject to RL. 2) Simple single-stage training. The proposed method can be implemented by applying only novel objectives in a single training stage without several training steps. 3) Low complexity. The proposed architecture maintains a network size almost similar to that of the existing RL al-gorithms [11, 23], which do not consider the generalization ability of the agent. Also, the inference time is faster than other adaptation methods [12] or similar to other general-ization methods since EAR does not require any adaptation during inference. See more details in Table 1. 4) Superior performance. Superior generalization perfor-mance of the proposed method was validated through exten-sive experiments on diverse test environments including DM
Control Suite [43] and DrawerWorld [49]. 2.