Abstract
Video instance segmentation (VIS) is a critical task with diverse applications, including autonomous driving and video editing. Existing methods often underperform on complex and long videos in real world, primarily due to two factors. Firstly, ofﬂine methods are limited by the tightly-coupled modeling paradigm, which treats all frames equally and disregards the interdependencies between ad-jacent frames. Consequently, this leads to the introduc-tion of excessive noise during long-term temporal align-ment. Secondly, online methods suffer from inadequate utilization of temporal information. To tackle these chal-lenges, we propose a decoupling strategy for VIS by di-viding it into three independent sub-tasks: segmentation, tracking, and reﬁnement. The efﬁcacy of the decoupling strategy relies on two crucial elements: 1) attaining pre-cise long-term alignment outcomes via frame-by-frame as-sociation during tracking, and 2) the effective utilization of temporal information predicated on the aforementioned ac-curate alignment outcomes during reﬁnement. We introduce a novel referring tracker and temporal reﬁner to construct the Decoupled VIS framework (DVIS). DVIS achieves new
SOTA performance in both VIS and VPS, surpassing the current SOTA methods by 7.3 AP and 9.6 VPQ on the
OVIS and VIPSeg datasets, which are the most challeng-ing and realistic benchmarks. Moreover, thanks to the de-coupling strategy, the referring tracker and temporal re-ﬁner are super light-weight (only 1.69% of the segmenter
FLOPs), allowing for efﬁcient training and inference on a single GPU with 11G memory. The code is available at https://github.com/zhang-tao-whu/DVIS. 1.

Introduction
Video Instance Segmentation (VIS) is a critical computer vision task that involves identifying, segmenting, and track-ing all interested instances in a video simultaneously. This task was ﬁrst introduced in [31]. The importance of VIS lies
*Corresponding author. offline video  segmentation results coupled offline VIS model online video  segmentation results frame-by-frame matching coupled online VIS model video (a) previous offline method instance representations segmenter video frames (b) previous online method referring  tracker temporal  refiner online video  segmentation  results offline video  segmentation  results video frames (c) DVIS
Figure 1. Pipelines of previous ofﬂine (a), online (b), and pro-posed DVIS (c) frameworks. Unlike previous methods that rely on tightly coupled networks, DVIS consists of independent com-ponents, including a segmenter, a referring tracker, and a temporal reﬁner. in its ability to facilitate many downstream computer vision applications, such as online autonomous driving and ofﬂine video editing.
Previous studies [12, 6, 28, 10] have demonstrated suc-cessful performance validation on videos with short dura-tions and simple scenes [31]. However, in real-world sce-narios, videos often present highly complex scenes, severe instance occlusions, and prolonged durations [21]. As a re-sult, these approaches [12, 6, 28, 10] have exhibited poor performance on videos [21] that are more representative of real-world scenarios.
We believe that the fundamental reason for the failure of the aforementioned methods [12, 6, 28, 10] lies in the assumption that a coupled network can effectively predict the video segmentation results for any video, irrespective of its length, scene complexity, or instance occlusion lev-els. In the case of lengthy videos (e.g. 100 seconds and 500 frames), with intricate scenes, the same instance may exhibit signiﬁcant variations in position, shape, and size be-tween the ﬁrst and last frames [21]. Even for experienced humans, accurately associating the same instance in two frames that are separated by a considerable interval is chal-lenging without observing its gradual transformation tra-jectory over time. Therefore, the alignment/tracking dif-ﬁculty is signiﬁcantly increased in complex scenarios and lengthy videos, and even cutting-edge methods such as [5] face challenges in achieving convergence [11].
To tackle the aforementioned challenges, we propose to decouple the VIS task into three sub-tasks that are indepen-dent of video length and complexity: segmentation, track-ing, and reﬁnement. Segmentation aims to extract all ap-pearing objects and obtain their representations from a sin-gle frame. Tracking aims to link the same object between adjacent frames. Reﬁnement utilizes all temporal informa-tion of the object to optimize both segmentation and associ-ation results. Thus we have our decoupled VIS framework, as illustrated in Figure 1 (c). It contains three separate and independent components, i.e., a segmenter, a tracker, and a reﬁner. Given the extensive research on the segmenter in the
ﬁeld of image instance segmentation, our focus is to design an effective tracker for robustly associating objects across adjacent frames and a reﬁner for improving the quality of segmentation and tracking.
To achieve effective instance association, we propose the following principles: (1) encourage sufﬁcient interac-tion between instance representations of adjacent frames to fully exploit their similarity for better association. (2) avoid mixing their information during the interaction process to prevent introducing indistinguishable noise that may inter-fere with the association results. Current SOTA methods, such as [29, 11], violate principle 1 by utilizing heuris-tic algorithms to match adjacent frame instance represen-tations without any interaction, resulting in a signiﬁcant performance gap compared to our method. While [9, 35] achieve interaction between instance representations of ad-jacent frames by passing instance representations, they vio-late principle 2. Following both principles, we designed the
Referring Cross Attention (RCA) module, which serves as the core component of our highly effective referring tracker.
RCA is a modiﬁed version of standard cross-attention [4] that introduces identiﬁcation to avoid the blending of in-stance representations in consecutive frames and efﬁciently utilize their similarities. We further propose a novel tempo-ral reﬁner that leverages 1D convolution and self-attention to effectively integrate temporal information, and cross-attention to correct instance representations.
An decoupled VIS framework, called DVIS, is then nat-urally constructed by combining the segmenter, the refer-ring tracker, and the temporal reﬁner. DVIS achieves new
SOTA performance on all the VIS datasets, surpassing pre-vious SOTA method [29] by 7.3 AP on the most challenging
OVIS dataset [21]. Additionally, DVIS can be seamlessly extended to other video segmentation tasks, such as video panoptic segmentation (VPS) [13], without any modiﬁca-tion. DVIS also achieves new SOTA performance on the video panoptic segmentation dataset VIPSeg [20], surpass-ing previous SOTA method [1] by 9.6 VPQ. DVIS achieved 1st place in the VPS Track of the PVUW challenge at
CVPR 2023.
Our decoupling strategy not only signiﬁcantly improves the performance of video segmentation, but also dramati-cally reduces hardware resource requirements. Speciﬁcally, our proposed tracker and reﬁner operate exclusively on the instance representations output by the segmenter, avoiding the signiﬁcant computational cost associated with interact-ing with image features. As a result, the computation cost of the tracker and reﬁner is negligible (only 5.18%/1.69% of the segmenter with R50/Swin-L backbone). Thanks to the decoupling design of the VIS task and framework, the tracker and reﬁner can be trained separately while keeping other components frozen. These advantages allow DVIS to be trained on a single GPU with 11G memory.
In summary , our main contributions are:
• We investigate the failure reasons of current methods on complex and lengthy real-world videos, and we ad-dress these challenges by introducing a novel decou-pling strategy for VIS, which involves decomposing it into three decoupled sub-tasks: segmentation, track-ing, and reﬁnement.
• Following the decoupling strategy, we propose DVIS, which includes a simple yet effective referring tracker and temporal reﬁner to produce precision alignment results and efﬁciently utilize temporal information, re-spectively.
• DVIS achieves new SOTA performance in both VIS and VPS, as validated on ﬁve major benchmarks:
OVIS [21], YouTube-VIS [31] 2019, 2021, and 2022, as well as VIPSeg [20]. Notably, DVIS signiﬁcantly reduces the resources required for video segmentation, enabling efﬁcient training and inference on a single
GPU with 11G memory. 2.