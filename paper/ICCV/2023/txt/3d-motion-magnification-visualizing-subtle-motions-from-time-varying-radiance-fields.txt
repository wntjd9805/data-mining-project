Abstract 1.

Introduction
Motion magnification helps us visualize subtle, imper-ceptible motion. However, prior methods only work for 2D videos captured with a fixed camera. We present a 3D mo-tion magnification method that can magnify subtle motions from scenes captured by a moving camera, while support-ing novel view rendering. We represent the scene with time-varying radiance fields and leverage the Eulerian principle for motion magnification to extract and amplify the varia-tion of the embedding of a fixed point over time. We study and validate our proposed principle for 3D motion mag-nification using both implicit and tri-plane-based radiance fields as our underlying 3D scene representation. We eval-uate the effectiveness of our method on both synthetic and real-world scenes captured under various camera setups.
We live in a big world of small motions. These mo-tions, such as human respiration or object vibration, are hard to perceive with our naked eyes. Video processing techniques [29, 61, 56] have been developed to extract and magnify subtle motions captured in a 2D video to high-light and visualize those motions. These motion magnifi-cation techniques empower visual analytics tools like de-tecting the vibrations of buildings and measuring a person’s heart rate using only a video, without the need for physical contact [58, 10, 46, 23].
However, we live in a 3D world full of 3D motions. Mag-nifying motion in 3D, as shown in Figure 1 allows us to perceive these motions from different views. Furthermore, modeling the motion in 3D provides a natural separation be-*Equal contribution
embeddings, we learn one feature tri-plane at each observed timestep. These tri-planes can be naturally organized as feature videos for 2D video-based magnification methods.
Finally, the motion-magnified 3D scene is rendered using these motion-magnified feature triplanes as the point em-bedding functions.
To evaluate the performance of 3D magnification with
NeRF, we first create a synthetic dataset of scenes with sub-tle motions and measure the magnification quality against synthetically magnified ground truth videos. The phase-based approach operating on tri-plane features leads to the best performance compared to other alternative approaches considered in our experiments. To further validate the prac-ticality of the proposed method, we use our pipeline to pro-cess several real-world captured scenes with varying camera setups, scene compositions, and subject motions. Our re-sults show that our proposed approach for 3D motion mag-nification achieves robust performance for real-world cap-tures in the presence of image noise and camera poses.
To summarize, our contributions are:
• We introduce the problem of 3D motion magnifica-tion. We demonstrate the feasibility of applying Eule-rian motion analysis for 3D motion magnification us-ing standard NeRF backbones and training pipelines.
• We extend Eulerian analysis to a new domain beyond color space, exploring strategies to modify and filter point embedding and comparing their trade-offs.
• We demonstrate successful 3D motion magnification results on various real-world scenes with different mo-tions, scene compositions, and even handheld videos unsupported by previous 2D methods. 2.