Abstract 3D Morphable Models (3DMMs) demonstrate great po-tential for reconstructing faithful and animatable 3D fa-cial surfaces from a single image. The facial surface is influenced by the coarse shape, as well as the static de-tail (e.g., person-specific appearance) and dynamic detail (e.g., expression-driven wrinkles). Previous work struggles to decouple the static and dynamic details through image-level supervision, leading to reconstructions that are not realistic.
In this paper, we aim at high-fidelity 3D face reconstruction and propose HiFace to explicitly model the static and dynamic details. Specifically, the static detail is modeled as the linear combination of a displacement ba-sis, while the dynamic detail is modeled as the linear in-terpolation of two displacement maps with polarized ex-pressions. We exploit several loss functions to jointly learn the coarse shape and fine details with both synthetic and real-world datasets, which enable HiFace to reconstruct high-fidelity 3D shapes with animatable details. Extensive quantitative and qualitative experiments demonstrate that
HiFace presents state-of-the-art reconstruction quality and faithfully recovers both the static and dynamic details. Our project page: https://project-hiface.github.io. 1.

Introduction
The reconstruction of a 3D face from a single image has drawn much attention recently [67, 21, 41, 81].
It has tremendous potential applications like face recogni-tion [11, 63, 4, 59], face animation [16, 75], virtual real-ity [7, 58, 31], etc. For example, the reconstructed 3D face representation can be driven by an audio [16], or a video from another person [38].
*Work done when the author was an intern at MSRA.
†Corresponding author: Xu Tan (xuta@microsoft.com).
Figure 1. We propose HiFace to reconstruct high-fidelity 3D face with realistic and animatable details. Reconstruction: given a sin-gle image (1st-row), HiFace faithfully reconstructs a coarse shape (2nd-row) with vivid details (3rd-row). Animation: given a source face (yellow box), HiFace can animate the static (4th-row), dy-namic (5th-row), or both (6th-row) details of the driving images (green box). Images are taken from FFHQ [37] and CelebA [40].
To build a flexible and animatable facial representa-tion, a popular way is to leverage the success of 3D Mor-phable Models (3DMMs) [5, 6, 10, 45, 65], which decou-ple the influence of shape, expression, albedo, and oth-ers by modeling them in separate coefficients. Typically
in literature, one can achieve coarse shape reconstruction in coefficients-fitting optimization [27, 73, 3, 77, 2], or an analysis-by-synthesis pipeline [67, 21, 81, 50]. As 3DMMs typically capture only the coarse facial geometry and are not capable of representing fine details (e.g., wrinkles), recent advances model such details with a displacement map [15, 77, 13, 9, 76]. However, previous work fails to model the distinction between static and dynamic factors of fine detail, leading to errors in reconstructions. For exam-ple, given that one may drive the expression of a young man from an old man, trivially transferring all wrinkles from the old man to the young man could make the young man look unnatural. In this sense, Feng et al. [24] implicitly lever-ages the person-specific identity and expression as condi-tions to generate the details. Although effective, they op-timize the model in an analysis-by-synthesis pipeline with only the image-level supervision, leading to insufficient de-coupling of static and dynamic details and inconsistent ani-mation results (see Fig. 7).
Therefore, we propose HiFace to explicitly model the static and dynamic details for high-fidelity 3D face recon-struction, by designing SD-DeTail module to decouple the static and dynamic factors. More specifically, for person-specific static detail, instead of directly predicting the dis-placement map that may increase the difficulty of detail pre-diction [24, 19], we follow the spirit of 3DMMs to build a displacement basis from the captured facial scans with age diversity [57, 72]. In this way, the model is trained to pre-dict the coefficients of the displacement basis, and make the detail prediction easier. For dynamic detail, since it is highly expression-dependent, directly modeling it with one displacement basis is quite difficult. Therefore, based on the fact that the expression can be interpolated by a compressed and a stretched expressions [57], we build two displacement bases for the compressed and stretched expressions from the captured scans respectively, and learn to regress the dis-placement coefficients with the ground-truth labels. There-fore, we can obtain the dynamic detail by linearly inter-polating the compressed and stretched displacement maps, which are derived from the displacement bases and the pre-dicted coefficients. Finally, the predicted static and dynamic details are merged with the coarse shape to formulate the fi-nal output.
Since we would like the final output to contain both the coarse shape and high-frequency detail, we propose several novel loss functions to learn coarse shape and details simul-taneously from both the synthetic and real-world datasets.
For details, we leverage the ground-truth static and dynamic displacement maps of the synthetic dataset [72, 57] as su-pervision. While for the coarse shape, we leverage the ground-truth vertex of the synthetic dataset as supervision.
We also follow the previous methods [24, 21, 73] to lever-age self-supervised losses for all training images.
Overall, with the above insights and techniques, HiFace enables the reconstruction of high-fidelity 3D faces from a single image, and decouples static and dynamic details that are naturally animatable (see Fig. 1). We demonstrate that the proposed HiFace reconstructs realistic and faithful 3D faces, reaching state-of-the-art performance both quan-titatively and qualitatively. In addition, HiFace is compat-ible with optimization-based methods [73], and is flexible to transfer vivid expressions and details from one person to another. In summary, our contributions are:
• We propose HiFace to model the static and dynamic de-tails explicitly, and demonstrate the benefits of synthetic data in decoupling the static and dynamic factors for de-tailed 3D face reconstruction.
• We propose novel loss functions in HiFace to learn 3D representations of coarse shape and fine details simulta-neously from both the synthetic and real-world images.
• We achieve state-of-the-art reconstruction quality both quantitatively and qualitatively, with over 15% perfor-mance gains in the region-aware benchmark [12].
• We show that our SD-DeTail is easy to plug-and-play into optimization-based methods and can transfer expressions and details from one to another for face animation. 2.