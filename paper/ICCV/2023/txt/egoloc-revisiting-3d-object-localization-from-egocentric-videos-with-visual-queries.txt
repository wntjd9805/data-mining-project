Abstract
With the recent advances in video and 3D understand-ing, novel 4D spatio-temporal methods fusing both concepts have emerged. Towards this direction, the Ego4D Episodic
Memory Benchmark proposed a task for Visual Queries with 3D Localization (VQ3D). Given an egocentric video clip and an image crop depicting a query object, the goal is to localize the 3D position of the center of that query object with respect to the camera pose of a query frame. Current methods tackle the problem of VQ3D by unprojecting the 2D localization results of the sibling task Visual Queries with 2D Localization (VQ2D) into 3D predictions. Yet, we point out that the low number of camera poses caused by camera re-localization from previous VQ3D methods sev-erally hinders their overall success rate. In this work, we formalize a pipeline (we dub EgoLoc) that better entangles 3D multiview geometry with 2D object retrieval from egocen-tric videos. Our approach involves estimating more robust camera poses and aggregating multi-view 3D displacements by leveraging the 2D detection conﬁdence, which enhances the success rate of object queries and leads to a signiﬁcant improvement in the VQ3D baseline performance. Speciﬁ-cally, our approach achieves an overall success rate of up to 87.12%, which sets a new state-of-the-art result in the
VQ3D task1. We provide a comprehensive empirical analysis of the VQ3D task and existing solutions, and highlight the remaining challenges in VQ3D. The code is available at https://github.com/Wayne-Mai/EgoLoc. 1.

Introduction
Time moves forward, from the past to the future, and we cannot turn it back. But our minds have a special ability to remember past events, almost as if we are traveling back in time. This ability is called Episodic Memory, and it’s 1https://eval.ai/web/challenges/challenge-page/ 1646/leaderboard/3947
Figure 1. Visual Query with 3D Localization Task in Egocen-tric Videos. Given an egocentric video clip and an image crop depicting a query object, the goal is to localize the last time a query object was seen in the video and return the 3D displacement vector from the camera center of the query frame to the center of the object in 3D. unique to humans [88]. It’s more than just remembering facts; it’s about reliving past experiences, knowing when they happened, and understanding that they happened to us
[88]. In the pursuit of more human-like AI systems, infusing
Episodic Memory capabilities into our machines holds great promise, especially in assisting people to recall their past experiences.
Towards such efforts, the massive-scale dataset and bench-mark suite Ego4D [26] introduced multiple tasks on Episodic
Memory from egocentric videos, with the scope of brows-ing and searching past human experiences. Among those challenges, the task of Visual Queries (VQ) aimed at answer-ing “Where was object X last seen in the video?”, with X being a single image crop of an object, clearly visible and humanly identiﬁable. In particular, Visual Queries with 3D
Localization (VQ3D) focuses on retrieving the relative 3D localization of a query object with respect to a current query frame, as illustrated in Figure 1.
The task of VQ3D arose from the natural progress in com-puter vision challenges, building on top of the latest devel-opment in image understanding [20, 103], video understand-ing [26, 78], and 3D geometric understanding [74, 30, 31].
Speciﬁcally, VQ3D requires a frame-wise understanding of an egocentric video to localize objects in 2D images, a spe-cial 2D localization of the object along the temporal dimen-sion, coupled with a 3D scene understanding to unproject the 2D localization into a 3D environment. Although most of the effort in Ego4D originates in the ﬁeld of video understand-ing, little effort has been paid to improve the meaningful 3D knowledge needed by VQ3D methods.
Previous work [26] performs camera pose estimation by relocalizing real egocentric video frames to a Matterport scan, suffering from the simulation-to-real gap (difference in the domains and reference coordinates). IT also builds on 2D localization without proper 3D entanglement. In this work, we attempt to bridge the gap between video and 3D scene understanding in VQ3D. In particular, we develop a pipeline that better entangles 3D multiview geometry with 2D object retrieval from egocentric videos. To fully un-derstand the 3D scene, our proposed aggregation method predicts displacement from multiple views by leveraging the detection scores. This led to state-of-the-art results in the
VQ3D task. We summarize our contributions as follows.
• We formalize the pipeline for the task of Visual Queries with 3D Localization (VQ3D) from egocentric videos, with a thorough study of each module. We identify and solve the Simulation-2-Real gap for camera pose estimation, and elevate the baseline performance from 8.71% [26] to 77.27%.
• We propose to aggregate multi-view 3D displacements by employing the 2D detection conﬁdences to weight predictions and further enhance 3D localization. Our method (EgoLoc) achieves 87.12% in Overall Success
Rate on the test set of the VQ3D task, signiﬁcantly outperforming the baseline and setting new state-of-the-art results in VQ3D.
• We perform an extensive empirical analysis of different components and conﬁgurations in the VQ3D pipeline, which aims to beneﬁt future research in the VQ3D direction. 2.