Abstract
Semantic scene completion (SSC) requires an accurate understanding of the geometric and semantic relationships between the objects in the 3D scene for reasoning the oc-cluded objects. The popular SSC methods voxelize the 3D objects, allowing the deep 3D convolutional network (3D
CNN) to learn the object relationships from the complex scenes. However, the current networks lack the controllable kernels to model the object relationship across multiple views, where appropriate views provide the relevant infor-mation for suggesting the existence of the occluded objects.
In this paper, we propose Cross-View Synthesis Trans-former (CVSformer), which consists of Multi-View Feature
Synthesis and Cross-View Transformer for learning cross-view object relationships. In the multi-view feature synthe-sis, we use a set of 3D convolutional kernels rotated dif-ferently to compute the multi-view features for each voxel.
In the cross-view transformer, we employ the cross-view fu-sion to comprehensively learn the cross-view relationships, which form useful information for enhancing the features of individual views. We use the enhanced features to predict the geometric occupancies and semantic labels of all vox-els. We evaluate CVSformer on public datasets, where CVS-former yields state-of-the-art results. Our code is available at https://github.com/donghaotian123/CVSformer. 1.

Introduction
The recent progress on artificial intelligence is mainly driven by the advanced machine power of recognizing 3D objects. It significantly benefits various downstream appli-cations, such as autonomous driving and video surveillance.
An essential problem of 3D object recognition is letting the machine accurately recognize the occluded objects in com-*Co-first authors. The names are listed in alphabetical order.
†Co-corresponding authors. plex 3D scenes. This problem leads to the emergence of research on semantic scene completion (SSC).
The latest success of SSC is primarily attributed to deep neural networks, which are good at learning the geometric and semantic representations of 3D objects. To facilitate fast representation learning in the large-scale 3D scene, the popular SSC methods [5, 10, 13, 14, 19, 31, 38, 40, 21, 29, 39, 34, 2, 11, 27, 23, 22, 16] employ 3D CNN to learn repre-sentations from the voxelized objects. To alleviate the lim-itation of the regular 3D convolutional kernels that fix the range of capturing the object relationship, the variant 3D
CNNs involve spatial pyramid [40] and deformation [36] to diversify the kernel shapes, which attend to the object relationships in different ranges. Yet, the pyramidal ker-nels lack the flexibility to exclude the irrelevant voxels; the deformable kernels are sensitive to the object shapes, usu-ally capturing the relationship between voxels from a mono-view. These kernels work without the controllable view like the camera. They are also less effective for modeling the ob-ject relationship across multiple perspectives, like multiple cameras, which enable the change of view directions to of-fer the information of relevant objects for recognizing the occluded things.
In this paper, we propose Cross-View Synthesis Trans-former (CVSformer), which consists of Multi-View Fea-ture Synthesis (MVFS) and Cross-View Transformer (CVTr), as illustrated in Figure 1. CVSformer employs the basic 3D CNN to learn the visual feature map from RGB-D image. The original-view feature map contains the feature of each voxel, whose correlation with the adjacent voxels is captured from the original view. Next, MVFS controls the rotation of the 3D convolutional kernel, synthesizing the change of view direction and providing new object relation-ships. Here, we remark that 3D kernel’s view is an analogy with the camera’s view. It has a conceptual view direction to determines the center voxel’s spatial neighbors and their correlations. MVFS employs the kernels with different ro-tations to compute the synthetic-view feature maps, which
Figure 1. The overall architecture of CVSformer, which consists of Multi-View Feature Synthesis (MVFS) and Cross-View Transformer (CVTr). Based on a single-view RGB-D image, CVSformer computes the semantic and geometric feature maps, which are input into the
MVFS (a) to achieve multiple synthetic-view feature maps. (b) CVTr takes input as the synthetic-view feature maps, yielding augmented-view feature maps for the final completion task. capture the object relationships for each voxel from multi-ple views. Finally, all synthetic-view feature maps are input to CVTr, which conducts the cross-view fusion to estab-lish the object relationships across multiple views. Based on the cross-view object relationships, CVTr enhances the synthetic-view feature maps, yielding the augmented-view feature maps for SSC.
We evaluate CVSformer on the public datasets for SSC, where we achieve the state-of-the-art results (i.e., 52.6 and 63.9 mIoUs on NYU [30] and NYUCAD [14] datasets). 2.