Abstract
Significant advancements have been accomplished with deep neural networks in diverse visual tasks, which have substantially elevated their deployment in edge device soft-ware. However, during the update of neural network-based software, users are required to download all the parameters of the neural network anew, which harms the user experience. Motivated by previous progress in model compression, we propose a novel training methodology named Tiny Updater to address this issue. Specifically, by adopting the variant of pruning and knowledge distilla-tion methods, Tiny Updater can update the neural network-based software by only downloading a few parameters (10%∼20%) instead of all the parameters in the neural network. Experiments on eleven datasets of three tasks, including image classification, image-to-image translation, and video recognition have demonstrated its effectiveness.
Codes have been released in https://github.com/
ArchipLab-LinfengZhang/TinyUpdater. 1.

Introduction
With the availability of large-scale datasets [15, 16, 51] and high-performance computing platforms, deep neural networks have achieved remarkable achievements in vari-ous visual tasks such as image classification [18, 28, 75, 80], segmentation [53, 62], and object detection [50, 67, 68, 76].
Encouraged by their impressive performance, numerous software developers have effectively integrated neural net-works into their software products and deployed them on edge devices such as mobile phones and tablet computers. roadmap for a neural network-based software follows the paradigm illustrate in
Figure 1(a). Initially, users install the software by down-loading all the neural network parameters from a cloud plat-form. Subsequently, as the software interacts with its users, an abundance of new training data and requirements can be the development
Typically,
*Corresponding author 1
Figure 1. Comparison between (a) the traditional model updating scheme and (b) the proposed efficient model updating. In efficient model updating, only around 10% parameters in the neural net-work are actually changed and thus users only require to download these changed parameters for updating. collected. Then, software developers may retrain the neural network with the gathered data and update their software to enhance its performance. However, during the retrain-ing phase, all the neural network parameters are typically changed compared to their values before updating. Conse-quently, users are compelled to download all the parameters of the neural network again from the cloud platform, se-riously impairing their experience. While recent research has explored text prompts and adapter layers to fine-tune a large-scale pre-trained model at a low training cost [52, 79], there have been no prior efforts to reduce the download cost during model updating, which has a more direct impact on users with edge devices.
This paper proposes the challenge of efficient model up-dating with the objective of reducing the download over-head of neural network-based software during updating. As
depicted in Figure 1(b), during the retraining phase, ef-ficient model updating introduces an additional constraint limiting the change of only a small subset (e.g. 10%) of the neural network parameters compared to the pre-updating model. Consequently, users are only required to download a few parameters that have actually changed instead of all the parameters in the neural network. In general, efficient model updating raises two questions: how to find the opti-mal parameters that should be changed in the neural net-work, and how to achieve comparable accuracy with the fully-updated model (i.e., the model which has all the pa-rameters changed during updating).
To tackle this challenge, we propose a novel neural net-work training framework named Tiny Updater. Motivated by previous works in model compression, Tiny Updater is composed of the variants of two typical model compression techniques - neural network pruning and knowledge dis-tillation. Firstly, to determine which channels or layers in neural networks should be modified during updating, Tiny
Updater applies the pruning technique. As shown in Fig-ure 2, it iteratively calculates the L1-norm distance between the pre-updating model and the post-updating model pa-rameters. The channels with smaller distances are deemed unnecessary for updating and are pruned to their origi-nal values before updating. Conversely, the channels with larger distances are considered essential for updating, and thus they are actually changed. Secondly, during the re-training period, we propose to improve the performance of the partially-updated model by distilling the knowledge from a fully-updated teacher model. The partially-updated student model is trained to give predictions that are simi-lar to those of the fully-updated teacher model by optimiz-ing the knowledge distillation loss. This ensures that the partially-updated model achieves comparable performance with the fully-updated model.
Extensive experiments have been conducted on eleven datasets and three different tasks, including, image classi-fication, video classification, and image-to-image transla-tion, using seven different neural networks. Experimental results demonstrate that Tiny Updater can update the model by changing only 10% to 20% of the parameters with min-imal performance degradation. In summary, the main con-tribution of this paper can be outlined as follows.
• To the best of our knowledge, we first propose the chal-lenge of efficient model updating to reduce the com-munication cost of downloading neural network pa-rameters to the edge devices for software updating.
• To tackle this challenge, we propose Tiny Updater, which prunes the fully-updated model to the pre-updating model and distills the knowledge from the fully-updated model to the partially-updated model.
• Extensive experiments have been conducted on three tasks with eleven datasets, including image classifi-cation, image-to-image translation and video recogni-tion. Experimental results demonstrate that Tiny Up-dater can reduce the training overhead by 80%∼90% with almost no performance degradation. 2.