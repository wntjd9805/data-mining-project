Abstract 1.

Introduction
Medical vision-and-language pre-training (Med-VLP) has shown promising improvements on many downstream medical tasks owing to its applicability to extracting generic representations from medical images and texts. Practically, there exist two typical types, i.e., the fusion-encoder type and the dual-encoder type, depending on whether a heavy fusion module is used. The former is superior at multi-modal tasks owing to the sufficient interaction between modalities; the latter is good at uni-modal and cross-modal tasks due to the single-modality encoding ability. To take advantage of these two types, we propose an effective yet straightforward scheme named PTUnifier to unify the two types. We first unify the input format by introducing visual and textual prompts, which serve as DETR-like queries that assist in extracting features when one of the modalities is missing. By doing so, a single model could serve as a foundation model that processes various tasks adopting different input formats (i.e., image-only, text-only, and image-text-pair). Furthermore, we construct a prompt pool (instead of static ones) to im-prove diversity and scalability, enabling queries conditioned on different input instances. Experimental results show that our approach achieves competitive results on a broad range of tasks, spanning uni-modal tasks (i.e., image/text classifica-tion and text summarization), cross-modal tasks (i.e., image-to-text generation and image-text/text-image retrieval), and multi-modal tasks (i.e., visual question answering), demon-strating the effectiveness of our approach. Note that the adoption of prompts is orthogonal to most existing Med-VLP approaches and could be a beneficial and complementary extension to these approaches.1
*Equal contributions.
†Corresponding authors. 1The source zhjohnchan/ptunifier. code is available at https://github.com/
Medical data is multi-modal in general, among which vision and language are two critical modalities. It includes visual data (e.g., radiography, magnetic resonance imaging, and computed tomography) and textual data (e.g., radiol-ogy reports, and medical texts). More importantly, such images and texts are pair-collected in routine clinical prac-tice (e.g., X-ray images and their corresponding radiology reports). Medical vision-and-language pre-training (Med-VLP) aims to learn generic representation from large-scale medical image-text pairs and then transfer it to various med-ical tasks, which is believed to be beneficial in addressing the data scarcity problem in the medical field.
Recently, substantial progress has been made toward re-search on Med-VLP [65, 32, 20, 43, 8]. In general, most existing Med-VLP models can be classified into two types: the dual-encoder type and the fusion-encoder type, where the former encodes images and texts separately to learn uni-modal/cross-modal representations following a shallow in-teraction layer (i.e., an image-text contrastive layer), and the latter performs an early fusion of the two modalities through the self-attention/co-attention mechanisms to learn multi-modal representations.2 For dual-encoders, the purpose of existing studies [65, 20, 45, 59, 56, 60, 3] is to develop label-efficient algorithms to learn effective uni-modal/cross-modal representations since large-scale manually labeled datasets are difficult and expensive to obtain for medical images.
The learned representations can improve the effectiveness of uni-modal (i.e., vision-only or language-only) tasks3 and the efficiency of cross-modal (i.e., image-to-text or text-to-image) retrieval tasks significantly. For fusion-encoders, 2Although the terminologies “cross-modal” and “multi-modal” have been used interchangeably in the literature, we treat them as terms with different meanings in this paper. 3It is worth noting that most existing studies only conduct the evaluation on the vision-only tasks and disregard the language-only tasks although the text representations are simultaneously learned.
Figure 1: (a) Illustrations of two Med-VLP paradigms and their advantages (pointed by green arrows) and disadvantages (pointed by red arrows) in downstream tasks; (b) The overall architecture of our proposed approach, where the backbone models share the same parameters, and we duplicate them for illustration. existing studies [32, 25, 43, 8, 9] aim to jointly process these two modalities with an early interaction to learn multi-modal representations to solve those tasks requiring multi-modal reasoning (e.g., medical visual question answering and med-ical image-text classification). However, it seems that “you can’t have your cake and eat it, too.”: the fusion-encoders can not perform uni-modal tasks effectively and cross-modal tasks efficiently due to the lack of single-modal encoding, while the dual-encoders underperform on multi-modal tasks owing to the insufficient interaction between modalities as shown in Figure 1(a).
In this paper, we aim to learn a unified medical vision-and-language pre-trained model. Although there exist some solutions [4, 51] to achieve a similar goal in the general domain, we propose an architecture- and task-agnostic ap-proach named PTUnifier, which is much simpler and lighter-weighted. Technically, we develop the designs from the fol-lowing perspectives: (i) Compatibility: we introduce visual and textual prompts to make the Med-VLP model compat-ible with different kinds of inputs (i.e., image-only inputs, text-only inputs, and image-text pairs); (ii) Scalability: we improve the diversity of the prompts by constructing prompt pools for different modalities from which different inputs are able to select their corresponding prompts, which en-hances the capacity and makes it scalable to larger-scale
Med-VLP. In intuition, the introduced soft prompts serve as a query bank that stores the queries for extracting the most representative uni-modal features, the working mechanism of which is similar to the query vectors in DETR [5]. As a result, the proposed approach can be employed in unify-ing Med-VLP with many existing VLP model architectures (e.g., classic one [29] or even a single vanilla Transformer model) and does not require extra modality-dependent ar-chitectures, resulting in better applicability. We perform the pre-training on three large-scale medical image-text datasets, i.e., ROCO [47], MedICaT [53], and MIMIC-CXR [24]. To verify the effectiveness of our approach and facilitate further research, we construct a medical vision-language benchmark including uni-modal tasks (i.e., image classification (IC) for vision and text classification (TC) and text summarization (TS) for language), cross-modal tasks (i.e., image-to-text re-trieval (ITR), text-to-image retrieval (TIR), and image-to-text generation4 (ITG)), and multi-modal tasks (i.e., visual ques-tion answering (VQA)). The proposed PTUnifier achieves excellent performance on all datasets, demonstrating its ef-fectiveness. 2.