Abstract
Remarkable advances have been achieved recently in learning neural representations that characterize object ge-ometry, while generating textured objects suitable for down-stream applications and 3D rendering remains at an early stage. In particular, reconstructing textured geometry from images of real objects is a significant challenge – recon-structed geometry is often inexact, making realistic texturing a significant challenge. We present Mesh2Tex, which learns a realistic object texture manifold from uncorrelated collec-tions of 3D object geometry and photorealistic RGB images, by leveraging a hybrid mesh-neural-field texture representa-tion. Our texture representation enables compact encoding of high-resolution textures as a neural field in the barycen-tric coordinate system of the mesh faces. The learned texture manifold enables effective navigation to generate an object texture for a given 3D object geometry that matches to an input RGB image, which maintains robustness even under challenging real-world scenarios where the mesh geometry approximates an inexact match to the underlying geome-try in the RGB image. Mesh2Tex can effectively generate realistic object textures for an object mesh to match real im-ages observations towards digitization of real environments, significantly improving over previous state of the art.
Project page: alexeybokhovkin.github.io/ mesh2tex/
1.

Introduction
The ability to obtain 3D representations of real-world objects lies at the core of many important applications in graphics, robotics, movies and video games, and mixed reality. Approaches tackling the tasks of 3D genera-tion [33, 26, 21, 29, 30, 10] or single-view reconstruc-tion [5, 27, 46, 25, 7] enable users to easily create digi-tal 3D assets, either from scratch or conditioned on an im-age. However, these approaches primarily focus on gen-erating/inferring the geometry of objects, and this alone is insufficient to capture realistic environments, which requires high-quality texturing.
In this work, we propose Mesh2Tex tackle the comple-mentary task of high-fidelity texture generation given known object geometry. In addition to allowing texturing generation for a given object mesh, Mesh2Tex also enables image-based texture synthesis – synthesizing textures that perceptually match a single RGB image observation. In contrast to exist-ing appearance modeling works that characterize texture as a field over the volume of 3D space [3, 39, 40, 31], we ob-serve that object textures lie on object surfaces, and instead generate textures only on the mesh surface, with a hybrid explicit-implicit texture representation that ties the texture field to the barycentric coordinates of the mesh faces. This produces textured meshes directly compatible with down-stream applications and 3D rendering engines.
As large quantities of high-quality textured 3D objects are very expensive to obtain, requiring countless hours of skilled artists’ work, we train our texture generator with uncorre-lated collections of 3D object geometry and photorealistic 2D images, leveraging differentiable rendering to optimize mesh textures at arbitrary resolutions to match the quality of real 2D images in an adversarial fashion. That is, we gen-erate coarse features and colors for each mesh face, which are then refined to high-resolution textures through a neural field defined on the barycentric coordinates of each mesh face. This hybrid texture representation enables differen-tiable rendering for texturing on explicit mesh surfaces while exploiting the efficiency of neural field representations.
Our learned texture generator can then be used to produce realistic textures to match an input RGB observation of an object, by optimizing over our learned texture manifold con-ditioned on the mesh geometry, to find a latent texturing that perceptually matches the RGB image. As state-of-the-art object geometry reconstructions are typically inexact (e.g., noise, oversmoothing, retrieval from a database), our learned texture manifold enables effective regularization over plausi-ble textures which effectively capture the perceptual input while providing realistic texturing over the full object. We formulate a patch-based style loss to capture perceptual sim-ilarities between our optimized texture and the RGB image, guided by dense correspondence prediction to correlate them.
Experiments demonstrate that we outperform state of the art in both unconditional texture generation as well as image-conditioned texture generation.
In summary, our contributions are:
• a new hybrid mesh-neural-field texture representation that enables diverse, realistic texture generation on ob-ject mesh geometry by tying a neural texture field to the barycentric coordinate system of the mesh faces.
This enables learning a rich texture manifold from col-lections of uncorrelated real images and object meshes through adversarial differentiable rendering.
• our learned texture manifold enables effective inference-time optimization to texture an object mesh to perceptu-ally match a single real-world RGB image; crucially, we maintain robustness to real-world scenarios of differing views and even geometry with a patch-based perceptual optimization guided by dense correspondences. 2.