Abstract 1.

Introduction
Despite recent advancements in unified adverse weather removal methods, there remains a significant challenge of achieving realistic fine-grained texture and reliable back-ground reconstruction to mitigate serious distortions.
Inspired by recent advancements in codebook and vector quantization (VQ) techniques, we present a novel Adverse
Weather Removal network with Codebook Priors (AWRCP) to address the problem of unified adverse weather removal.
AWRCP leverages high-quality codebook priors derived from undistorted images to recover vivid texture details and faithful background structures. However, simply utilizing high-quality features from the codebook does not guarantee good results in terms of fine-grained details and structural fidelity. Therefore, we develop a deformable cross-attention with sparse sampling mechanism for flexible perform fea-ture interaction between degraded features and high-quality features from codebook priors. In order to effectively in-corporate high-quality texture features while maintaining the realism of the details generated by codebook priors, we propose a hierarchical texture warping head that gradu-ally fuses hierarchical codebook prior features into high-resolution features at final restoring stage.
With the utilization of the VQ codebook as a feature dic-tionary of high quality and the proposed designs, AWRCP can largely improve the restored quality of texture details, achieving the state-of-the-art performance across multiple adverse weather removal benchmark.
*Equal contributions.
†Yun Liu is the corresponding author. 1
The restoration of images under adverse weather condi-tions, such as heavy haze or rain, is a major topic of research in the field of computer vision. The original images may suffer from severe weather-induced distortions, like intense rain streaks or dense hazing effects, which obscure the true background and deteriorate the performance of high-level vision tasks. Therefore, a difficult inverse problem arises, whereby degraded images are likely to experience signifi-cant losses of detail and structure, requiring restoration.
With the advent of deep learning techniques, adverse weather removal methods [33, 49, 41, 10] has achieved re-markable progress. An increasing number of studies are fo-cusing on achieving all-in-one adverse weather removal in one go as a primary objective. This entails the elimination of all weather-related degradation through the utilization of a single, unified model. Classical adverse weather removal methods employ neural architecture search to find an opti-mal network design [33], use advanced decoders with learn-able weather queries to decode clean features in solving this task [49], explore diffusion model for adverse weather re-moval [41]. However, these methods are still limited in their performance due to their inability to robustly capture high-quality clear background features from seriously degraded images. The only feature source that them can employed is only from degraded images, which obviously is a huge drawback in structure rebuilding and realist texture restor-ing by these models. Additionally, it should be noted that the irreversible nature of severe texture loss poses a sig-nificant challenge for these methods. Previous methods al-most all pay more attention on reconstruct clean features in multi-scale feature stage, ignoring the benefits of restor-ing fine-grained details in the final high-resolution feature level. In conclusion, recent methods for mitigating adverse weather conditions have demonstrated impressive perfor-mance across various types of weather-induced degradation.
However, there is still a substantial room for improvement that requires further exploration.
When presented with an image that is degraded due to unfavorable weather conditions, the task of restoring it to a pristine state becomes an incredibly difficult problem be-cause of the various sources of degradation present. Conse-quently, it can be difficult to find a reliable statistical prior that can address the issue effectively. For deep networks, these challenges are magnified, as they strive to capture ideal, noise-free features as latent variables in a network that only possesses a lone encoder.
The aforementioned deliberations and inferences im-pel us to contemplate deploying codebook priors as a po-tential solution to the current challenge.Training a VQ-GAN [12] on high-quality, noise-free images has the po-tential to produce a Vector-Quantized (VQ) codebook that possesses high-quality feature priors. A well-trained VQ codebook holds the potential to offer a comprehensive high-quality feature set, aiding in the handling of various types of weather-induced degradation. However, due to the feature misalignment between degraded features from de-graded images and high-quality features from codebook pri-ors, the straightforward fusion of high-quality features from the vector quantization (VQ) codebook may not yield ade-quate results in the context of adverse weather removal [53].
Moreover, undesirable effects from various sorts of noise observed in the degraded features could additionally impact the quality of reconstructed superior-quality characteristics.
To address these issues, we introduce two special de-signs, which allow AWRCP to effectively explore robust codebook priors and keep fidelity in restored results, sur-passing previous state-of-the-art methods in restoration per-formance. For facilitate pliable feature interaction and fu-sion, we propose a Parallel Decoder Design that integrates
Deformable Cross-Attention. This design effectively intro-duces high-quality features and maintains structural consis-tency between rstored results and clean background. Our
Deformable Cross-Attention (DCA) utilizes paired high and low-quality features to guided sparse sampling phase, flexibly adapt features with distinct quality. With the help of sparse sampling, DCA efficiently perform context mod-eling between two source of features and fuse them adap-tively.
Its aim is to ameliorate the issue of feature mis-alignment that arises between degraded and high-quality features. For the restoration of fine-grained details, Hier-archical Texture Warping Head is proposed by us to explore hierarchical high-quality features in high-resolution feature level, restoring fine-grained details step-by-step. We are the first work that successfully exploit codebook priors for ad-verse weather removal and achieve a state-of-the-art perfor-mance across all standard weather task benchmarks.
Our contributions can be summarized as follows:
• We propose a novel framework AWRCP for adverse weather removal using high-quality codebook priors learned by a pre-trained VQGAN. Compared with pre-vious works, the AWRCP introduces codebook priors to formulate adverse weather removal task as a fea-ture matching and fusion problem between degraded and high-quality features, enabling the leading perfor-mance.
• We propose a Deformabel Cross-Attention with Sparse
Sampling for high/low-quality feature fusion. Such a manner bridge the misalignment between heteroge-neous features, avoiding, effectively avoids drawbacks from codebook priors.
• A novel Hierarchical Texture Warping Head is de-signed to refine textures in high-resolution feature stage by introducing hierarchical prior features. 2.