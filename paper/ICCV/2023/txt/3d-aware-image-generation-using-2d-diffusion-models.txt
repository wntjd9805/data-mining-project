Abstract
In this paper, we introduce a novel 3D-aware image gen-eration method that leverages 2D diffusion models. We formulate the 3D-aware image generation task as multi-view 2D image set generation, and further to a sequential unconditional–conditional multiview image generation pro-cess. This allows us to utilize 2D diffusion models to boost the generative modeling power of the method. Additionally, we incorporate depth information from monocular depth es-timators to construct the training data for the conditional diffusion model using only still images.
We train our method on a large-scale unstructured 2D image dataset, i.e., ImageNet, which is not addressed by previous methods. It produces high-quality images that sig-nificantly outperform prior methods. Furthermore, our ap-proach showcases its capability to generate instances with large view angles, even though the training images are diverse and unaligned, gathered from “in-the-wild” real-world environments. 1 1.

Introduction
Learning to generate 3D contents has become an increas-ingly prominent task due to its numerous applications such
*Work done when JX and BH were interns at MSR. 1Project page: https://jeffreyxiang.github.io/ivid/ as VR/AR, movie production, and art design. Recently, sig-nificant progress has been made in the field of 3D-aware image generation, with a variety of approaches being pro-posed [4, 5, 7, 10, 30, 32, 43, 44, 54]. The goal of 3D-aware image generation is to train image generation models that are capable of explicitly controlling 3D camera pose, typi-cally by using only unstructured 2D image collections.
Most existing methods for 3D-aware image generation rely on Generative Adversarial Networks (GANs) [9] and utilize a Neural Radiance Field (NeRF) [25] or its variants as the 3D scene representation. While promising results have been demonstrated for object-level generation, extend-ing these methods to large-scale, in-the-wild data that fea-tures significantly more complex variations in geometry and appearance remains a challenge.
Diffusion Models (DMs) [13, 48, 50], on the other hand, are increasingly gaining recognition for their exceptional generative modeling performance on billion-scale image datasets [33, 35, 37].
It has been shown that DMs have surpassed GANs as the state-of-the-art models for complex image generation tasks [8, 14, 15, 29]. However, applying
DMs to 3D-aware image generation tasks is not straightfor-ward. Unlike 3D-aware GANs, training DMs for 3D gener-ation necessitates raw 3D assets for its nature of regression-based learning [24, 27, 28, 45, 56].
To take advantage of the potent capability of DMs and
the ample availability of 2D data, our core idea in this pa-per is to formulate 3D-aware generation as a multiview 2D image set generation task. Two critical issues must be ad-dressed for this newly formulated task. The first is how to apply DMs for image set generation. Our solution to this is to cast set generation as a sequential unconditional– conditional generation process by factorizing the joint dis-tribution of multiple views of an instance using the chain rule of probability. More specifically, we sample the initial view of an instance using an unconditional DM, followed by iteratively sampling other views with previous views as conditions via a conditional DM. This not only minimizes the model’s output to a single image per generation, but also grants it the ability to handle variable numbers of output views.
The second issue is the lack of multiview image data.
Inspired by a few recent studies [3, 11], we append depth information to the image data through monocular depth esti-mation techniques and use depth to construct multiview data using only still images. However, we found that naively ap-plying the data construction strategy of [11] can result in domain gaps between training and inference. To alleviate this, we recommend additional training data augmentation strategies that can improve the generation quality, particu-larly for the results under large view angles.
We tested our method on both a large-scale, multi-class i.e., ImageNet [6], and several smaller, single-dataset, category datasets that feature significant variations in ge-ometry. The results show that our method outperformed state-of-the-art 3D-aware GANs on ImageNet by a wide margin, demonstrating the significantly enhanced genera-tive modeling capability of our novel 3D-aware generation approach. It also performed favorably against prior art on other datasets, showing comparable texture quality but im-proved geometry. Moreover, we find that our model has the capability to generate scenes under large view angles (up to 360 degrees) from unaligned training data, which is a chal-lenging task further demonstrating the efficacy of our new method.
The contributions of this work are summarized below:
• We present a novel 3D-aware image generation method that uses 2D diffusion models. The method is designed based on a new formulation for 3D-aware generation, i.e., sequential unconditional–conditional multiview image sampling.
• We undertake 3D-aware generation on a large-scale in-the-wild dataset (ImageNet), which is not addressed by previous 3D-aware generation models.
• We demonstrate the capability of our method for large-angle generation from unaligned data (up to 360 de-grees). 2.