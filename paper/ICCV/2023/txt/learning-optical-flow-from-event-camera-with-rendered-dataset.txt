Abstract
We study the problem of estimating optical flow from event cameras. One important issue is how to build a high-quality event-flow dataset with accurate event values and flow labels. Previous datasets are created by either captur-ing real scenes by event cameras or synthesizing from im-ages with pasted foreground objects. The former case can produce real event values but with calculated flow labels, which are sparse and inaccurate. The latter case can gener-ate dense flow labels but the interpolated events are prone to errors. In this work, we propose to render a physically cor-rect event-flow dataset using computer graphics models. In particular, we first create indoor and outdoor 3D scenes by
Blender with rich scene content variations. Second, diverse camera motions are included for the virtual capturing, pro-ducing images and accurate flow labels. Third, we render high-framerate videos between images for accurate events.
The rendered dataset can adjust the density of events, based on which we further introduce an adaptive density mod-ule (ADM). Experiments show that our proposed dataset can facilitate event-flow learning, whereas previous ap-proaches when trained on our dataset can improve their performances constantly by a relatively large margin.
In addition, event-flow pipelines when equipped with our ADM can further improve performances. Our code is available at https://github.com/boomluo02/ADMFlow.
Figure 1: (a) the captured dataset from real event cam-era [55, 54]. (b) the synthesized dataset with flying chairs (c) the synthesized dataset by moving foreground [50]. a foreground image [42]. (d) Our synthesized dataset by graphics rendering, which not only reflects the real motions under correct scene geometries, but also produces accurate dense flow labels and events. 1.

Introduction
Event cameras [29] record brightness changes at a vary-ing framerate [4]. When a change is detected in a pixel, the camera returns an event in the form e = (x, y, t, p) imme-diately, where x, y stands for the spatial location, t refers to the timestamp in microseconds, and p is the polarity of the
*Corresponding author change, indicating a pixel become brighter or darker. On the other hand, optical flow estimation predicts motions be-tween two frames [44], which is fundamental and important for many applications [49, 52, 7]. In this work, we study the problem of estimating optical flow from event camera data, instead of from RGB frames. Different from traditional im-ages, events are sparse and are often integrated in short in-tervals as the input for the prediction. As such, early works
can only estimate sparse flows at the location of events [1].
Recent deep methods can estimate dense flows but with the help of images, either as the guidance [54] or as the addi-tional inputs [26, 38]. Here, we tackle a hard version of the problem, where dense flows are predicted based purely on the event values e. One key issue is how to create high qual-ity event-based optical flow dataset to train the network.
Existing methods of event flow dataset creation can be classified into two types, 1) directly capturing from real event cameras [55, 54, 16]; 2) moving foregrounds on top of a background image to create synthesized flow mo-tions [50, 42] and apply frame interpolation [14] to create events. For the first type, the ground-truth (GT) flow labels need to be calculated based on gray images acquired along with the event data. However, the optical flow estimations cannot be perfectly accurate [48, 43, 32, 24, 51], leading to the inaccuracy of GT labels. To alleviate the problem, additional depth sensors, such as LIDAR, have been intro-duced [55]. The flow labels can be calculated accurately when the depth values of LIDAR scans are available. How-ever, LIDAR scans are sparse, and so do the flow labels, which are unfriendly for dense optical flow learning. Fig. 1 (a) shows an example, LIDAR points on the ground are sparse. Moreover, some thin objects are often missing, as indicated by the red box in Fig. 1 (a).
For the second category, the flow labels are created by moving foreground objects on top of a background image, similar to flying chairs [12] or flying things [36]. In this way, the flow labels are dense and accurate. To create events, intermediate frames are interpolated [23]. However, the frame interpolation is inaccurate due to scene depth dis-parities, where the occluded pixels cannot be interpolated correctly, leading to erroneous event values in these regions.
To match high framerate of events, the large number of in-terpolated frames makes the problem even worse. Fig. 1 (b) shows an example, where the events are incorrect at the oc-cluded chairs. Moreover, the motions are artificial, further decreasing the realism of the dataset (Fig. 1 (b) and (c)).
In this work, we create an event-flow dataset from syn-thetic 3D scenes by graphics rendering (Fig. 1 (d)). While there is a domain gap between rendered and real images, this gap is empirically found insignificant in event camera based classification [41] and segmentation [14] tasks. As noted by these works, models trained on synthetic events work very well for real event data. Because events con-tain only positive and negative polarities, no image ap-pearances are involved. To this end, we propose a Multi-Density Rendered (MDR) event optical flow dataset, cre-ated by Blender on indoor and outdoor scenes with accurate events and flow labels. In addition, we design an Adaptive
Density Module (ADM) based on MDR, which can adjust the densities of events, one of the most important factors for event-based tasks but has been largely overlooked.
Specifically, our MDR dataset contains 80, 000 samples from 50 virtual scenes. Each data sample is created by first rendering two frames and obtaining the GT flow labels di-rectly from the engine. Then, we render 15 ∼ 60 frames in-between based on the flow magnitude. The events are cre-ated by thresholding log intensities and recording the times-tamp for each spatial location. The density of events can be controlled by the threshold values. The ADM is designed as a plugin module, which further consists of two sub-modules, multi-density changer (MDC) and multi-density selector (MDS), where the MDC adjusts the density glob-ally while the MDS picks the best one for every spatial loca-tion. Experiments show that previous event-flow methods, when trained on our MDR dataset, can improve their perfor-mances. Moreover, we train several recent representative flow pipelines, such as FlowFormer [21], KPA-Flow [32],
GMA [24] and SKFlow [47], on our MDR dataset. When equipped with our ADM module, the performances can in-crease consistently.
Our contributions are summarized as:
• A rendered event-flow dataset MDR, with 80,000 sam-ples created on 53 virtual scenes, which possess physi-cally correct accurate events and flow label pairs, cov-ering a wide range of densities.
• An adaptive density module (ADM), which is a plug-and-play module for handling varying event densities.
• We achieve state-of-the-art performances. Our MDR can improve the quality of previous event-flow meth-ods. Various optical flow pipelines when adapted to the event-flow task, can benefit from our ADM module. 2.