Abstract
CutMix is a vital augmentation strategy that determines the performance and generalization ability of vision trans-formers (ViTs). However, the inconsistency between the mixed images and the corresponding labels harms its ef-ﬁcacy. Existing CutMix variants tackle this problem by generating more consistent mixed images or more precise mixed labels, but inevitably introduce heavy training over-head or require extra information, undermining ease of use. To this end, we propose an novel and effective Self-Motivated image Mixing method (SMMix), which motivates both image and label enhancement by the model under training itself. Speciﬁcally, we propose a max-min atten-tion region mixing approach that enriches the attention-focused objects in the mixed images. Then, we introduce a ﬁne-grained label assignment technique that co-trains the output tokens of mixed images with ﬁne-grained su-pervision. Moreover, we devise a novel feature consis-tency constraint to align features from mixed and unmixed images. Due to the subtle designs of the self-motivated paradigm, our SMMix is signiﬁcant in its smaller train-ing overhead and better performance than other CutMix variants.
In particular, SMMix improves the accuracy of
DeiT-T/S/B, CaiT-XXS-24/36, and PVT-T/S/M/L by more than +1% on ImageNet-1k. The generalization capability of our method is also demonstrated on downstream tasks and out-of-distribution datasets. Our project is available at https://github.com/ChenMnZ/SMMix. 1.

Introduction
Vision transformers (ViTs) [12] have made substantial breakthroughs across various vision tasks, such as classiﬁ-cation [12, 44, 51, 26, 42, 3], detection [2, 60, 30, 13], and segmentation [57, 52, 22, 23]. However, the data-hungry problem [12, 44] of ViT causes a serious overﬁtting problem when the data is insufﬁcient. In order to improve the gen-∗Corresponding authors: Rongrong Ji (rrji@xmu.edu.cn)
SMMix(ours)
TokenMix
TransMix
AttentiveMix
AutoMix
PuzzleMix
CutMix
ResizeMix
F-Mix
Figure 1: Training time vs. accuracy with DeiT-S on
ImageNet-1k. SMMix outperforms existing methods with light overhead. eralization of ViTs, data mixing augmentation techniques such as Mixup [56] and CutMix [54], are used in the ViTs training recipe.
In particular, CutMix randomly crops a patch from the source image, pastes it into the target im-age, and forms a ground-truth label by mixing the labels of the source and target images in proportion to the area ratio of the mixed image. CutMix [54] has been demonstrated to greatly enhance the generalization of ViTs. For example,
CutMix increases the top-1 accuracy of ViT-Small [12] by 4.1% [31] on ImageNet-1k [9] validation set.
Despite the progress, the image-label inconsistency is-sue also stems from the random patch selection and linear label combination. Figure 2 illustrates a typical example, in which the mixed image of CutMix does not contain any hints of ladybirds. However, ladybird still appears on the generated mixed label. Such an image-label inconsistency issue prevents ViTs from further improving performance.
Two mainstream methods: 1) image-driven [46, 28, 48, 36] and 2) label-driven [4, 34, 26, 55, 35], have recently been considered to overcome the drawbacks of CutMix. The for-mer method is dedicated to enhancing the saliency of mixed images, while the latter method aims to enhance the pre-cision of mixed labels. Nevertheless, these methods usu-ally come with heavy training overhead, such as requiring
Target Image 
Source Image
CutMix ladybird: 1.0 cock: 1.0
SMMix ladybird:0.6 cock:0.4 ladybird:0.6 cock:0.4 ladybird: 1.0 cock: 1.0
Figure 2: CutMix vs. SMMix. CutMix pastes a randomly-cropped patch in the source image to the target image, while
SMMix pastes the attentive region in the source image to the unattentive region in the target image. SMMix is co-trained by three ﬁne-grained labels instead of a simple mixed label as CutMix. pre-trained models [48, 34, 26, 55], double forward and backward propagations [27, 28, 36], or additional genera-tors [36], which may undermine the ease of the use of image mixing technique. Moreover, these methods only consider the image and label enhancement in isolation, resulting in limited efﬁciency.
To address the aforementioned challenges, we propose a novel method, Self-Motivated image Mixing (SMMix), to enhance image mixing with ViTs. By leveraging the bootstrapping capabilities of the model under training itself,
SMMix simultaneously motivates image and label enhance-ment with light training overhead. Specially, we ﬁrst use the image attention score in Eq. (6) that accumulates attention score across all the image tokens. The motivations are from a widely-accepted actuality in existing works [32, 53, 5], in which the class attention score from the self-attention oper-ation can locate semantic objects. Therefore, we opt to use the image attention score to extend the general applicabil-ity of SMMix, since class attention is often unavailable for
ViT models without a class token, while the image atten-tion score can be easily obtained by feeding original images to a ViT model. With the guidance of the image attention score, we select the maximum-scored (most attentive) re-gion from a source image and paste it to the region with a minimum attention score in a target image. We term this process as max-min attention region mixing, which allevi-ates the image-label inconsistency issue by enriching the attention-focused objects in mixed images.
Distinctive from the prerequisite to tuning mixed la-bels [4, 34], capturing attentive objects in mixed images al-lows for a ﬁne-grained label assignment. We supervise dif-ferent regions in a mixed image with different labels. Con-cretely, the output tokens of a mixed image are assigned three types of labels to accomplish the label enhancement, as illustrated in Figure 2, including mixed image label, tar-get image label, and source image label. We aggregate all output tokens, the result is then supervised by the mixed image label. We also use region-speciﬁc supervision, i.e., target image labels and source image labels, to supervise the aggregated results of tokens from the target regions and source regions, respectively.
With label-consistent mixed images, we can extract mixed image features from ViTs. To correctly recognize the mixed images, we expect the features of mixed images to fall into a consistent space with those of original unmixed images. We realize this function by creating a feature con-sistency constraint, which aligns the feature distributions between mixed images and the linear combination of un-mixed images. Specially, SMMix can obtain the feature distributions and the image attention score of unmixed im-ages from the same forward propagation of the model under training, resulting in light overhead.
Based on the above considerations, three key compo-nents are proposed in this paper, including 1) max-min at-tention region mixing (Sec. 4.1), 2) ﬁne-grained label as-signment (Sec. 4.2), and 3) feature consistency constraint (Sec. 4.3). We term our method self-motivated image mix-ing, since these components eliminate the dependency on pre-trained models and simply depend on the model under training itself. We have performed extensive experiments, which demonstrate the powerful ability of our SMMix to boost the performance of various ViT-based models, includ-ing DeiT [44] with a plain architecture, PVT [50] with a hierarchical architecture, CaiT [45] with deeper depth, and
Swin [37] with local self-attention. Moreover, our SMMix achieves better training overhead and performance trade-off because of the self-motivated paradigm. As shown in Fig-ure 1, our SMMix can achieve state-of-the-art top-1 accu-racy with light training overhead and does not require pre-trained models. 2.