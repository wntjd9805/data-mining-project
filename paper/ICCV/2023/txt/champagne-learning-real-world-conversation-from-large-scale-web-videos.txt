Abstract
Visual information is central to conversation: body ges-tures and physical behaviour, for example, contribute to meaning that transcends words alone. To date, however, most neural conversational models are limited to just text.
We introduce
CHAMPAGNE, a generative model of con-versations that can account for visual contexts. To train
CHAMPAGNE, we collect and release
YTD-18M, a large-scale corpus of 18M video-based dialogues. YTD-18M is constructed from web videos: crucial to our data collection pipeline is a pretrained language model that con-verts error-prone automatic transcripts to a cleaner dia-logue format while maintaining meaning.
Human evaluation reveals that YTD-18M is more sen-sible and speciﬁc than prior resources (MMDialog [17], 1M dialogues), while maintaining visual-groundedness. Exper-iments demonstrate that 1) CHAMPAGNE learns to con-duct conversation from YTD-18M; and 2) when ﬁne-tuned, it achieves state-of-the-art results on four vision-language tasks focused on real-world conversations. We release data, models, and code at https://seungjuhan.me/champagne. 1.

Introduction
Conversation often relies on non-verbal cues: visual in-formation like physical expressions, body gesture, or the surrounding environment are used by interlocutors to shape and understand meaning. Figure 1: the two conversation participants appear stressed (in the ﬁrst image: one person strikes the desk with his ﬁsts; in the second, the other person is rubbing his face); but the tension is not apparent simply
Are Interlocutors involved in a conversation visible in the images?
Yes (62%)
No (38%)
Derived from Social Media
The interlocutors of dialog  are visible for only 12%
A: What are your favorite ProgRock albums? Let’s say from 1966  through the present.
B: Pink Floyd’s “The Dark Side Of  the Moon” !"#
A: DarkSideOfTheMoon is absolutely  essential ProgRock from PinkFloyd!
A: And if you take another step, you will never  ever be able to call yourself a Sinclair again. 
B: This is why everybody calls you the creepy  uncle, lurking around the forest with what is that? 
A: Luxbane has been in our family since the first days and tradition  dictates that each Sinclair should learn the power to wield it.
A: Up okay guys and this is how the car loose the doors elevated. 
B: I wouldn't drive it too much because uh it's only a single arm on the door. 
A: That's the look that we get right here. (a) YTD-18M (b) MMDialog; 1M examples
Figure 2: Sample dialogues from two different datasets: (a) YTD-18M, and (b) the example from MMDialog presented in their paper [17].
According to the human evaluation results, our YTD-18M dataset has a good balance of both social interactions (left; 62%) and visually-grounded dialogue (right; 38%), in contrast, MMDialog only has 12% social interactions. In addition, MMDialog is derived from social media, resulting in examples that contain emojis or other social media-speciﬁc elements, while YTD-18M is derived from videos and thus genuinely capture real-life communications. from the transcript, which refers mostly to ﬁnancial topics.
Other visual information suggests a broader context as well: that the conversation is taking place over the phone, that one person is older, and that one person is more formally dressed are all potentially important factors for understand-ing the conversation’s meaning, yet none are reﬂected in the transcript. Indeed, visual perception provides information that can help machines understand the world in ways that text alone cannot [6, 28, 11].
In this paper, we propose
CHAMPAGNE1, a genera-tive model of conversations that learns from a large-scale video corpus. CHAMPAGNE takes in video frames, a video title, and a dialogue context as input and returns a dialogue response as output. The model learns from videos about two conversational frames: 1) Social Interaction, where the conversation is observed from a 3rd-person perspec-tive (e.g. movies or interviews; Figure 2, (a)-left); and 2)
Visually-grounded Dialogue, where the conversation is ob-served from an embodied, ﬁrst-person perspective (e.g. ego-centric videos or chit-chatting through messenger applica-tions; Figure 2, (a)-right)).
To support training CHAMPAGNE, we collect and release a large-scale dataset,
YTD-18M, which is the largest publicly available dataset for real-world conversation learn-ing. YTD-18M is constructed from 20M YouTube videos: we use a language model to convert the noisy transcripts automatically generated by YouTube into well-formatted dialogues associated with video frames. Human evalua-tion shows that YTD-18M covers both social interaction and visually-grounded dialogue frames in balance, and sur-passes the prior resource in terms of quality and scale (see
§3.2).
After training, we demonstrate that CHAMPAGNE mod-els generate high-quality next-turn utterances that account for visual contexts. Then, we conduct ﬁne-tuning experi-1ConversAtional Multimodal Prompted GeNErator. ments, ﬁnding that: 1) CHAMPAGNE exhibits strong per-formance on open-domain text-only conversation bench-marks (§4.1); 2) CHAMPAGNE outperforms existing SOTA models on two social interaction understanding bench-marks: CMU-MOSEI [62] and Visual Comet [38] (§4.2); 3)
CHAMPAGNE outperforms SOTA models on two visually-grounded dialogue benchmarks: Visual Dialog [12] and
Image Chat [50] (§4.3); and 4) ablations conﬁrm the im-portance of various components of YTD-18M (§4.4), e.g. video frames.
In summary, our main contributions are: 1. 2.
YTD-18M, a large-scale dataset that contains 18M video-based dialogue that covers real-world conversa-tional frames derived from 20M web videos.
CHAMPAGNE, a generative model that learns about real-world conversations from YTD-18M without any manual annotation. 3. Experiments and ablations that demonstrate learning from a large-scale video-based dialogue dataset im-proves model performance on various tasks related to conversation.
We publicly release code, the
YTD-18M dataset, and
CHAMPAGNE model checkpoints to facilitate future re-search on understanding real-world conversations from a visually-grounded perspective. 2.