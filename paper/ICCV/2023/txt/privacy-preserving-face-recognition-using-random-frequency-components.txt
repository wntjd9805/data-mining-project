Abstract
The ubiquitous use of face recognition has sparked in-creasing privacy concerns, as unauthorized access to sensi-tive face images could compromise the information of in-dividuals. This paper presents an in-depth study of the privacy protection of face images’ visual information and against recovery. Drawing on the perceptual disparity be-tween humans and models, we propose to conceal visual information by pruning human-perceivable low-frequency components. For impeding recovery, we first elucidate the seeming paradox between reducing model-exploitable in-formation and retaining high recognition accuracy. Based on recent theoretical insights and our observation on model attention, we propose a solution to the dilemma, by advo-cating for the training and inference of recognition mod-els on randomly selected frequency components. We distill our findings into a novel privacy-preserving face recogni-tion method, PartialFace. Extensive experiments demon-strate that PartialFace effectively balances privacy protec-tion goals and recognition accuracy. Code is available at: https://github.com/Tencent/TFace. 1.

Introduction
Face recognition (FR) is a landmark biometric tech-nique that enables a person to be identified or verified by face. It has seen remarkable methodological breakthroughs and rising adoptions in recent years. Currently, consider-able applications of face recognition are carried out online to bypass local resource constraints and to attain high ac-curacy [19]: Face images are collected by local devices such as cell phones or webcams, then outsourced to a service provider, that uses large convolutional neural net-works (CNN) to extract the faces’ identity-representative templates and matches them with records in its database.
*Equal contributions.
†Corresponding author.
Figure 1. Paradigm comparison among other frequency-based methodologies and PartialFace. (a) Pruning low-frequency chan-nels can conceal visual information but stop no recovery. (b) A vanilla method that uses fixed channel subsets to impede recov-ery suffers downgraded accuracy. (c) PartialFace addresses the dilemma by training and inferring from random channels.
During the process, the original face images are of-ten considered sensitive data under regulatory demands that are unwise to share without privacy protection. This fosters the studies of privacy-preserving face recognition (PPFR), where cryptographic [7,8,14,17,21,33,44,47] and perturbation-based [3, 12, 18, 28–30, 46, 48] measures are taken to prevent face images from unauthorized access of, e.g., wiretapping third parties. Face images are converted to protective representations that their visual information is both concealed and cannot be easily recovered [2].
This paper advocates a novel PPFR scheme, to learn face images from random combinations of their partial fre-quency components. Our proposed PartialFace can protect face images’ visual information and prevent recovery while maintaining the high distinguishability of their identities.
We start with the disparity in how humans and models perceive images. Recent image classification studies sug-gest that models’ predictions are determined mainly by the images’ high-frequency components [39, 45], which carry negligible visual information and are barely picked up by humans. We extend the theory to face recognition from a
privacy perspective, to train and infer the model on pruned frequency inputs: As depicted in Fig. 1(a), we decouple the face images’ frequency compositions via discrete cosine transform (DCT), which breakdown every spatial domain into a certain number of (typically 64) constituent bands, i.e., frequency channels. We prune the human-perceivable low-frequency channels and exploit the remaining. We find the processed face images become almost visually indis-cernible, and the model still works accurately.
Pruning notably conceals visual information. However, to what degree can it impede recovery? We define recovery as the general attempts to reveal the faces’ visual appear-ances from shared protected features using trained attack models. Notice we may prune very few (say, about 10) channels, if according to human perception. At the same time, the remaining high-frequency channels being shared, hence exposed, are quite numerous and carry a wealth of model-perceivable features. While the information abun-dance can benefit a recognition model, it is also exploitable by a model carrying out attacks, as both may share similar perceptions. Therefore, as we later experimentally show, the attacker can recover visual features from high-frequency channels with ease in the absence of additional safeguards, rendering privacy protection useless.
An intuitive tactic to reinstate protection is to reduce the attacker’s exploitable features by training on a small portion of fixed channels, as shown in Fig. 1(b). However, we find the reduction also severely impairs the accuracy of trained recognition models. Evidence on the models’ attention, as later shown, attributes their utility downgrade to being inca-pable of learning a complete set of facial features, as vital channels describing some local features may be pruned.
Training on subsets of channels hence seems contradic-tory to the privacy-accuracy equilibrium. Fortunately, we can offer a reconciliation getting inspired by a recent time-series study [51].
It proves under mild conditions, mod-els trained on random frequency components can preserve more entirety’s information than on fixed ones, plausibly by alternately learning from complementary features. We hence propose a novel address to the equilibrium based on its theoretical findings and our observation on model atten-tion: For any incoming face image, we arbitrarily pick a small subset of its high-frequency channels. Therefore, our recognition model is let trained and inferred from image-wise random chosen channels, illustrated in Fig. 1(c).
We further show that randomness can be adjusted to a moderate level, by choosing channels from pre-specified combinations and perturbations called ranks, to keep pri-vacy protection while reconciling technical constraints to ease training. At first glance, our randomized approach may seem counter-intuitive as it is common wisdom that models require consistent forms of inputs to learn stably. However, since DCT produces spatially correlated frequency channels that preserve the face’s structural information, as later illus-trated in Fig. 3, it turns out the model generalizes quite nat-urally. Experimental analyses shows our PartialFace well balances privacy and accuracy.
The contributions of our paper are three-fold: 1. We present an in-depth study of the privacy protection of face images, regarding the privacy goals of conceal-ing visual information and impeding recovery. 2. We propose two methodological advances to fulfill the privacy goals, pruning low-frequency components and using randomly selected channels, based on the obser-vation of model perception and learning behavior. 3. We distill our findings into a novel PPFR method, Par-tialFace. We demonstrate by extensive experiments that our proposed method effectively safeguards pri-vacy and maintains satisfactory recognition accuracy. 2.