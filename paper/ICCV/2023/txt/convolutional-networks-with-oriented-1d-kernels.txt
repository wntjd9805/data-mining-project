Abstract
In computer vision, 2D convolution is arguably the most important operation performed by a ConvNet. Unsurpris-ingly, it has been the focus of intense software and hard-ware optimization and enjoys highly efficient implementa-In this work, we ask an intriguing question: can tions. we make a ConvNet work without 2D convolutions? Sur-prisingly, we find that the answer is yes—we show that a
ConvNet consisting entirely of 1D convolutions can do just as well as 2D on ImageNet classification. Specifically, we find that one key ingredient to a high-performing 1D Con-vNet is oriented 1D kernels: 1D kernels that are oriented not just horizontally or vertically, but also at other angles.
Our experiments show that oriented 1D convolutions can not only replace 2D convolutions but also augment exist-ing architectures with large kernels, leading to improved accuracy with minimal FLOPs increase. A key contri-bution of this work is a highly-optimized custom CUDA implementation of oriented 1D kernels, specialized to the depthwise convolution setting. Our benchmarks demon-strate that our custom CUDA implementation almost per-fectly realizes the theoretical advantage of 1D convolution: it is faster than a native horizontal convolution for any ar-bitrary angle. Code is available at https://github. com/princeton-vl/Oriented1D. 1.

Introduction
Convolutional Networks (ConvNets) [27, 25] are widely used in computer vision. They have been successfully ap-plied to a variety of tasks [15, 40, 16, 63] and domains
[25, 34, 23, 39, 59], and many new ConvNet-based build-ing blocks [47, 8, 57] and design practices [44, 50, 32] have emerged over the years.
In the computer vision context, a 2D convolution is ar-guably the most important operation performed by a Con-vNet. In virtually all ConvNet architectures [25, 18, 32], 2D convolution is the default choice and accounts for the bulk of the computation. Unsurprisingly, 2D convolution has been the focus of intense software and hardware opti-mization and enjoys highly efficient implementations.
*Work done during an internship at Princeton University.
In this work, we ask an intriguing question: can we make a ConvNet work without 2D convolution? Surprisingly, we find that the answer is yes—we show that a ConvNet con-sisting entirely of 1D convolutions can do as well on Ima-geNet classification, a surprising result given that 2D con-volution has been the go-to design choice.
Specifically, we find that one key ingredient to a high-performing 1D ConvNet is oriented 1D kernels: 1D kernels that are oriented not just horizontally or vertically, but also at other angles. This is a novel finding—although horizontal and vertical 1D convolutions have been frequently used in the past, 1D kernels oriented at arbitrary angles have not been well studied.
Oriented 1D kernels are motivated by the fact that 2D kernels can be approximated by 1D kernels, which are more efficient computationally. In particular, it is well known that convolution with a separable 2D kernel (i.e. rank 1 as a ma-trix) is equivalent to consecutive convolutions with a verti-cal 1D kernel and a horizontal 1D kernel, leading to signif-icant efficiency gain. However, in practice, not all learned 2D kernels are rank 1; if we only use vertical and horizontal 1D kernels, 2D kernels with a full rank, such as diagonal matrices, are poorly approximated, which can lead to a loss in accuracy: for example, the network may be less able to detect a 45◦ edge. This is when oriented 1D kernels can be helpful. By orienting a 1D kernel at more angles, we ex-pand the set of 2D kernels that can be well approximated by 1D kernels while retaining the efficiency of 1D kernels.
Oriented 1D kernels are also motivated by the increas-ing use of large 2D kernels in recent convolutional architec-tures. Large 2D kernels improve the modeling of long-range dependencies, which have been shown to result in better accuracy [32, 12, 30]. However, large 2D kernels are sig-nificantly more expensive because the cost scales quadrat-ically. A 31 × 31 kernel is 19 times more costly in terms of multiply-add operations than the standard 7 × 7 ker-nels. This motivates oriented 1D kernels as an alternative for modeling long-range dependencies, because the cost of 1D kernels scale only linearly with the kernel size.
The concept of oriented 1D kernels is simple, but non-trivial to implement in a way that realizes its efficiency ad-vantage over 2D kernels. This is because on a GPU, the pat-tern of memory access matters as much as, if not more than, the number of floating point operations. Applying a 1D ker-nel oriented at an arbitrary angle requires accessing non-contiguous data; a naive implementation can easily negate the theoretical advantage of 1D kernels due to poor manage-ment of memory access. In addition, it is important for the implementation to not introduce significant memory over-head, which could be incurred by some naive implementa-tions. Note that while horizontal/vertical 1D convolutions are well supported and optimized by existing libraries, 1D convolutions at an arbitrary angle is not.
A key contribution of this work is a highly-optimized custom CUDA implementation of oriented 1D kernels, spe-cialized to the setting of depthwise convolution [5], where each kernel is applied to only one depth channel. We op-timize for depthwise convolution, because it has become an essential building block in recent state-of-art architec-tures [51, 32], with superior accuracy-efficiency trade-offs.
In addition, we find depthwise convolution to be the more useful setting in our experiments. Experiments show that our custom CUDA implementation almost perfectly real-izes the theoretical advantage of 1D convolution: our 1D convolution at an arbitrary angle is faster than the native horizontal 1D convolution in PyTorch, which is highly op-timized and achieves over 96% of the theoretical speedup over 2D convolution. Notably, our implementation incurs minimal memory overhead; it uses less than 5% more GPU memory than the native horizontal 1D convolution in Py-Torch. Our implementation is open-sourced as a plug-and-play PyTorch module at https://github.com/ princeton-vl/Oriented1D.
With our custom implementation, our experiments show that oriented 1D convolution can not only replace 2D con-volution but also augment existing architectures with large kernels, leading to improved accuracy with minimal FLOPs increase. We expect our implementation to be a useful prim-itive for further innovation in neural architectures for com-puter vision.
Our main contributions are two-fold. First, we present the novel finding that state-of-the-art accuracy can be achieved with oriented 1D convolution alone, and that ori-ented 1D convolution can be a useful primitive to augment existing architectures. Second, we introduce an optimized
CUDA implementation of depthwise oriented 1D convolu-tion that nearly maxes out the theoretical efficiency of 1D convolution. 2.