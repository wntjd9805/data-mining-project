Abstract
Solving multiple visual tasks using individual models can be resource-intensive, while multi-task learning can con-serve resources by sharing knowledge across different tasks.
Despite the benefits of multi-task learning, such techniques can struggle with balancing the loss for each task, leading to potential performance degradation. We present a novel computation- and parameter-sharing framework that bal-ances efficiency and accuracy to perform multiple visual tasks utilizing individually-trained single-task transformers.
Our method is motivated by transfer learning schemes to reduce computational and parameter storage costs while maintaining the desired performance. Our approach in-volves splitting the tasks into a base task and the other sub-tasks, and sharing a significant portion of activations and parameters/weights between the base and sub-tasks to decrease inter-task redundancies and enhance knowl-edge sharing. The evaluation conducted on NYUD-v2 and
PASCAL-context datasets shows that our method is superior to the state-of-the-art transformer-based multi-task learn-ing techniques with higher accuracy and reduced compu-tational resources. Moreover, our method is extended to video stream inputs, further reducing computational costs by efficiently sharing information across the temporal do-main as well as the task domain. Our codes are available at https://github.com/sarashoouri/EfficientMTL. 1.

Introduction
In various computer vision applications, it is required to obtain a comprehensive understanding of the visual scene by performing multiple tasks based on a single input image.
These tasks often involve performing dense or pixel-wise predictions such as semantic segmentation, depth estimation, surface normal estimation, etc., for practical applications in autonomous driving, robotics, and augmented or virtual reality (AR/VR) [68]. Traditionally, these tasks are tackled individually by training a separate neural network dedicated to each task. However, this single-task learning can lead
Figure 1: Parameter and activation sharing across task and temporal domains. First, the base task produces its activa-tions at time t, then passes them to sub-tasks bk to share the computation across the task domain. Also, the base task activations at time t are shared with time t + 1 to reduce the computation across the temporal domain. to redundant computation and parameters, particularly for highly correlated tasks, losing the opportunity to perform faster inference as desired in real-time applications [59].
Multi-task learning (MTL) [6, 16, 34, 70, 73] has been actively explored as a solution to this problem, which learns a single unified model to perform multiple tasks simulta-neously. This approach allows the model to learn com-mon representations and patterns from multiple supervised tasks [74], resulting in a more memory- and computation-efficient structure than employing multiple single-task net-works. Additionally, multi-task networks can potentially enhance the performance of all tasks by leveraging the cross-task knowledge-sharing mechanism [59]. Due to the benefits of multi-task learning, numerous multi-task structures have been proposed to improve task-wise interactions in dense visual scene understanding [43, 65, 59, 75, 77, 42, 18]. The recent success of transformer models in many downstream vision tasks [37, 13, 78, 61, 49, 38] has led to the emergence of transformer-based multi-task networks [51, 2, 68, 7, 44], aimed at improving multi-task performance even further.
However, MTL poses several potential drawbacks com-pared to single-task learning: (1) Simultaneously learning different tasks using a unified model can lead to unbalanced task competition and suboptimal performance for some tasks if the model fails to build a shared representation that gen-eralizes to all tasks [17]. (2) Balancing the loss between different tasks can be challenging due to the varying scales of task-specific loss terms, especially when the number of tasks increases [11, 9]. Although multiple task-balancing approaches have been proposed [30, 9, 35] to address this problem, recent studies [60] have shown that performance still becomes worse than individually-trained single-task net-works. (3) MTL requires ground truth labels for all tasks per training sample, which is limiting because such annotations for certain tasks (e.g., semantic segmentation) may not al-ways be available for the same input sample. It also often does not allow easily adding new tasks without retraining the entire model from scratch.
To alleviate the above limitations, we propose a solution that leverages the strength of both single-task and multi-task learning techniques, integrating the concept of knowledge-sharing. Our approach draws inspiration from [10], which initially introduced the idea of efficient deep learning model communication through knowledge-sharing. In our work, we extend this concept to effectively execute multiple con-current visual tasks taking the same input. We employ individually-trained single-task networks to maintain the desired performance and prevent unbalanced task competi-tion. At the same time, we introduce a novel parameter- and computation-sharing strategy to facilitate knowledge-sharing and enhance inference efficiency. Our method focuses on transformer models that have shown outstanding results in vision tasks. First, we divide all tasks into a base task and multiple sub-tasks, where all task-specific networks adopt a common transformer structure as the backbone. Next, we train a single network for the base task independently.
Then, to share the inter-task information, we reuse not only weights but also activations from the base task to train each sub-task. The weight-sharing concept is motivated by re-cent parameter-efficient transfer learning techniques [26, 21].
Specifically, we view the weights of sub-task models as the sum of weights from the base task and a delta weight matrix, and apply ℓ0 regularization to encourage sparsity in the delta weight matrix as in the Diff-pruning [21] approach. We then fix the positions of non-zero elements of the delta weight matrix and fine-tune them to make the activation difference between the base task and sub-task also sparse by adopting
ℓ1 regularization. As a result, the pre-computed activations from the base task can be shared and passed to sub-task networks during the inference. This reduces computation cost as the remaining operations for sub-tasks only involve sparse matrix-matrix multiplications, as shown in Figure 1. Our proposed computation-sharing scheme significantly reduces the number of non-zero parameters for sub-tasks while allowing knowledge sharing between the main task and each sub-task. Extensive experiments on NYUD-v2 and
Pascal-Context benchmarks demonstrate that our method outperforms state-of-the-art multi-task transformers, exhibit-ing fewer parameters and FLOPs counts to attain comparable or superior task accuracy.
Furthermore, we extend our method to the temporal do-main to leverage the sparsity of differences between consec-utive video frames, as shown in Figure 1. Similar to the task domain, we employ ℓ1 regularization to force the activation differences between consecutive (time domain) frames to be-come sparse. As a result, during the inference of a sub-task at time t, it can exploit either the temporal domain or task domain sparsity to reuse activations from the same sub-task at time t − 1 (temporal activation sharing) or the main task at time t (task domain activation sharing). A simple strategy is then applied to combine these two sources of activation sharing for maximum efficiency and computation savings.
Our contributions can be summarized as follows:
• We propose a novel activation-and parameter-sharing scheme to reduce the computation and storage redun-dancy to perform multiple dense/pixel-wise vision tasks with transformer models.
• We extend our computation-sharing method to video in-puts and use a simple strategy to combine the activation-sharing sources between the task and temporal domains to maximize inference efficiency.
• We perform extensive experiments on NYUD-v2 and
Pascal-Context datasets to quantify the advantage of our proposed method in both performance and efficiency compared to prior multi-task learning methods. 2.