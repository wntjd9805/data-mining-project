Abstract
This paper introduces SKiT, a fast Key information
Transformer for phase recognition of videos. Unlike pre-vious methods that rely on complex models to capture long-term temporal information, SKiT accurately recognizes high-level stages of videos using an efficient key pooling op-eration. This operation records important key information by retaining the maximum value recorded from the begin-ning up to the current video frame, with a time complexity of O(1). Experimental results on Cholec80 and AutoLa-paro surgical datasets demonstrate the ability of our model to recognize phases in an online manner. SKiT achieves higher performance than state-of-the-art methods with an accuracy of 92.5% and 82.9% on Cholec80 and AutoLa-paro, respectively, while running the temporal model eight times faster ( 7ms v.s. 55ms) than LoViT, which uses Prob-Sparse to capture global information. We highlight that the inference time of SKiT is constant, and independent from the input length, making it a stable choice for keeping a record of important global information, that appears on long sur-gical videos, essential for phase recognition. To sum up, we propose an effective and efficient model for surgical phase recognition that leverages key global information. This has an intrinsic value when performing this task in an online manner on long surgical videos for stable real-time surgi-cal recognition systems. 1.

Introduction
Surgical Artificial Intelligence uses data to understand surgical workflows, evaluate surgeon performance, and pro-vide assistance to surgeons in real time[30]. One of the core tasks towards achieving these aims is the recognition of the transitions of high-level stages of surgery, a problem coined
Surgical Phase Recognition [15]. Accurate predictions of what surgical phase a part of a video relates to might ben-*Corresponding author efit the provision of automated and improved feedback for trainees [12, 24], the potential to optimize surgical work-flows [32], and the retrospective review of a particular phase from surgical video. While past and future information is used for phase recognition of a particular video frame in an offline manner, only past information is used for classify-ing in an online manner the phase where the last (current) video frame locates. Although online recognition is more challenging, it could help alert surgeons [33] and support decision-making [8] in real-time during surgery. This paper focuses on online phase recognition.
Early work in surgical phase recognition proposed work-flow recovery models using Dynamic Time Warping with temporal registration [1], graphical probabilistic models based on Hidden Markov Models (HMM) [2, 3], rule-based interpretation models for context-awareness using ontologies [23], and machine learning models for phase recognition using Support Vector Machines and Random
Forests [15]. Although these methods are mathematically rigorous, the use of hand-crafted features is specific to the surgery type and leads to a design that is not fully gener-alisable. Deep learning brought in new methods for surgi-cal phase recognition, which allows for more sophisticated spatio-temporal feature extraction mechanisms. While ad-ditional information, such as surgical tools presence, is con-sidered by other methods in a multi-task learning manner to improve accuracy [21, 37], annotation requirements limit their influence on online recognition.
Online surgical phase recognition requires models that can capture long-range temporal dependencies since the du-ration of surgical videos could range from 40 minutes to a few hours. Single-task surgical phase recognition models that add temporality can be broadly categorized into three types which use recurrent neural networks (RNN) [34], convolution neural networks (CNN) [27], or Transform-ers [39]. However, while RNNs, including Long Short-Term Memory (LSTM), struggle with modelling long-term dependencies due to their sequential nature, CNN-based methods such as Temporal Convolutional Networks (TCNs)
Figure 1: The Key-recorder aggregates previous and current local features to predict the current phase. The surgical phase is updated only when new key information is recognised. Working principle of our Key-recorder on an illustrative video stream to record the global appeared key information (2nd row) together with the current local feature (1st row) for recognising the phase (last row) of a current image frame. use dilated convolution with fixed-sized filters to capture long temporal information, which can be problematic for long sequences. Moreover, dilated convolution can re-sult in information loss while processing long sequences due to sparse sampling of input features.
In contrast,
Transformer-based methods have shown promising results as they can capture relationships between different tokens in a sequence, regardless of positions. This makes it eas-ier to model long-term dependencies. One disadvantage is that time and memory complexity of the self-attention mechanism used by Transformers is quadratic, which limits their usefulness for long videos. Even with the use of Prob-Sparse attention [44] that decreases the complexity of self-attention, inference time is still related to the input length and could be time-consuming for long sequences. Further-more, complex temporal models, especially for time-series data, may face greater challenges in retaining redundant information, which can cause overfitting. This is due to the frequent presence of autocorrelation and periodicity in time-series data, which may exacerbate overfitting when the model is overly complex. Further research is required to ef-ficiently and effectively capture global information while ensuring processing time is independent of the input length.
To process the input with varying lengths while main-taining efficiency and effectiveness, it is important to con-sider previously captured information along with that ap-pearing in the current frame. As depicted in Figure 1, cer-tain local temporal features, such as ‘Move grasper’, may appear in multiple phases, such as ‘Preparation’ and ‘Calot-TriangleDissection’, and the key information that distin-guishes them is crucial. Assume that we recorded the ap-peared key information where it is from the beginning of the video to the current frame (not included), and we can efficiently combine it with the current local feature to up-date the appeared key information of the next frame, en-abling us to reuse captured information, rather than search-ing it from the beginning of the video. Motivated by this, we propose a fast Key information video Transformer (ab-breviated as SKiT), which has the ability to record global appeared key information along the temporal dimension by an efficient and effective key pooling operation. After knowing the global appeared key information and current local fine-grained feature, SKiT could recognise the current phase accurately. Our main contributions are: 1) a new key pooling method that globally records important key events within O(1) time complexity, 2) a more efficient and sta-ble approach that ensures inference time is not affected by video length, and 3) an efficient and accurate model that maintains state-of-the-art performance. The code website: https://github.com/MRUIL/SKiT. 2.