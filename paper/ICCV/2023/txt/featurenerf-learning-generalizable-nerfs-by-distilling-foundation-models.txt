Abstract
Recent works on generalizable NeRFs have shown promising results on novel view synthesis from single or few images. However, such models have rarely been ap-plied on other downstream tasks beyond synthesis such as semantic understanding and parsing. In this paper, we pro-pose a novel framework named FeatureNeRF to learn gen-eralizable NeRFs by distilling pre-trained vision founda-tion models (e.g., DINO, Latent Diffusion). FeatureNeRF leverages 2D pre-trained foundation models to 3D space via neural rendering, and then extract deep features for 3D query points from NeRF MLPs. Consequently, it al-lows to map 2D images to continuous 3D semantic fea-ture volumes, which can be used for various downstream tasks. We evaluate FeatureNeRF on tasks of 2D/3D se-mantic keypoint transfer and 2D/3D object part segmenta-tion. Our extensive experiments demonstrate the effective-ness of FeatureNeRF as a generalizable 3D semantic fea-ture extractor. Our project page is available at https:
//jianglongye.com/featurenerf/. 1.

Introduction
Neural fields have emerged as a compelling paradigm for representing a variety of visual signals [8, 10, 31, 32, 37]. In particular, the Neural Radiance Fields (NeRF [32]), which implicitly encodes density and color via Multi-Layer Per-ceptrons (MLPs), has shown high quality novel view syn-thesis results from dense input images. A body of follow-up works [7, 27, 45, 54, 66] further reduce the dependency on dense inputs and generalizes NeRF to unseen objects by learning priors from large-scale multi-view image datasets.
With the remarkable abilities on reconstruction and view synthesis of generalizable NeRFs, we ask the question:
Can we adapt such models to learn 3D representations as foundations for general 3D applications (e.g., recognition, matching) beyond view synthesis?
Recent years have witnessed the rise of vision founda-tion models [5, 40, 43, 67] that are pre-trained on web-scale image datasets and demonstrate generalization capabilities across massive vision tasks (e.g., CLIP [40], DINO [5], La-tent Diffusion [47]). The feature space constructed by foun-dation models captures rich semantic and structural infor-mation of 2D visual data and make it possible to identify object categories, parts and correspondences even without extra supervisions [1, 5, 30]. Motivated by these works, our goal is to leverage the powerful 2D foundations models to obtain generalizable 3D features.
In this paper, we present FeatureNeRF, a unified frame-work for learning generalizable NeRFs from distilling pre-trained 2D vision foundation models. Unlike previous gen-eralizable NeRFs [45, 66], which utilize 2D encoder solely for novel view synthesis, FeatureNeRF explores the use of deep features extracted from NeRFs as generalizable 3D visual descriptors. We show that distilling 2D foundation models into 3D space via neural rendering equips the NeRF features with rich semantic information. As a result, Fea-tureNeRF allows to predict a continuous 3D semantic fea-ture volume from a single or a few images, which can be applied to various downstream tasks such as semantic key-point transfer and object part co-segmentation. Examples of these applications are shown in Fig. 1.
Specifically, we adopt an encoder to map 2D images to corresponding 3D NeRF volume similar to previous gener-alizable NeRFs. Apart from density and color, we propose to extract deep features of the query 3D points from the in-termediate layers of NeRF MLP. To enrich semantic infor-mation of the NeRF features, we further transfer knowledge from the foundation models to the encoder via neural ren-dering during training: The rendered feature outputs should be consistent with the feature extracted from the foundation models, which is enforced by a distillation loss.
To evaluate FeatureNeRF, we tackle the tasks of 2D/3D semantic keypoint transfer and object part segmentation.
To the best of our knowledge, our work is the first to re-solve these 3D semantic understanding tasks without 3D supervision. We validate our framework with two founda-tion models: (i) DINO [5], a self-supervised vision trans-former aware of object correspondences, and (ii) Latent
Diffusion [47], a diffusion-based model that achieves state-of-the-art text-to-image generation performance. Our ex-tensive experiments demonstrate the effectiveness of Fea-tureNeRF as a generalizable 3D semantic feature extractor. 2.