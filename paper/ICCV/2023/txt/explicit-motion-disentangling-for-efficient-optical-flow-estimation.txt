Abstract
In this paper, we propose a novel framework for opti-cal flow estimation that achieves a good balance between performance and efficiency. Our approach involves disen-tangling global motion learning from local flow estimation, treating global matching and local refinement as separate stages. We offer two key insights: First, the multi-scale 4D cost-volume based recurrent flow decoder is computa-tionally expensive and unnecessary for handling small dis-placement. With the separation, we can utilize lightweight methods for both parts and maintain similar performance.
Second, a dense and robust global matching is essential for both flow initialization as well as stable and fast con-vergence for the refinement stage. Towards this end, we introduce EMD-Flow, a framework that explicitly sepa-rates global motion estimation from the recurrent refine-ment stage. We propose two novel modules: Multi-scale
Motion Aggregation (MMA) and Confidence-induced Flow
Propagation (CFP). These modules leverage cross-scale matching prior and self-contained confidence maps to han-dle the ambiguities of dense matching in a global manner, generating a dense initial flow. Additionally, a lightweight decoding module is followed to handle small displacements, resulting in an efficient yet robust flow estimation frame-work. We further conduct comprehensive experiments on standard optical flow benchmarks with the proposed frame-work, and the experimental results demonstrate its superior balance between performance and runtime. Code is avail-able at https://github.com/gddcx/EMD-Flow. 1.

Introduction
Optical flow represents the 2D motion field between suc-cessive video frames.
It is a fundamental computer vi-sion task and has various downstream applications, e.g. video frame interpolation [13], video super-resolution [8], visual tracking [33] and motion detection [26]. Different from traditional energy-based or matching-based optimiza-∗Equal contribution. †Corresponding Author.
Figure 1: Comparison with state-of-the-art methods in terms of inference accuracy (F1-all), runtime (s) and model size (M). All models are trained on “C + T”, and evaluated on KITTI-15 [25] (image size 375×1242) with a single NVIDIA A100 card. Our EMD-Flow models not only demonstrate substantial performance enhancements compared to state-of-the-art methods but also achieve sig-nificant reductions in computational overhead. tions, deep learning based methods [30, 15] have made great progress by introducing the cost-volume based re-gression paradigm in a pyramid structure. Recently, the recurrent regression framework [32] has become a main-stream approach for optical flow, and advanced methods like attention-based operations [18, 21, 38], graph mod-els [23], and latent cost-volume augmentation [12] have been developed to improve its performance. However, these methods often require extra computational resources and consume significant inference time, limiting their applica-tion in real-world scenarios. Therefore, a critical question arises: can we improve the accuracy of flow estimation while maintaining high runtime efficiency?
To understand the trade-off between accuracy and run-time efficiency, we empirically analyze the recurrent pre-diction paradigm in the RAFT model [32] and its compu-tational overhead. The core component of RAFT is the it-erative decoder, which obtains the final prediction by iter-atively refining the flow estimate. The statistical analysis reveals that the early iterations primarily address the chal-lenge of handling large displacements, while the subsequent iterations concentrate on small-scale motion and local re-finement, as shown in Fig. 2a and Fig. 2b. In terms of com-putational overhead shown in Fig. 2c, we find that all itera-tions take up about 90% of the running time of the model, with the corresponding parameter ratio of 58.5%.
Based on our analysis, we draw the following conclu-sions: i) RAFT primarily handles large motion in the early iterations and small motion in the later iterations. ii) When all points are estimated from the same starting point, the iterative procedure becomes out of sync because large dis-placement requires more iterations than small displacement. iii) The full recurrent unit based on multi-scale 4D cost-volume is computationally expensive and unnecessary for handling small displacement.
Towards this goal, we propose a novel flow network with an Explicit Motion Disentangling (EMD) strategy that effectively handles large motion while maintaining run-time efficiency. Our key insight is to disentangle the global motion learning process from the complex recur-rent decoder and employ a lightweight decoding module to handle small displacements. Specifically, we introduce
Confidence-induced Flow Propagation (CFP), a multi-scale and confidence-induced module that utilizes cross-scale matching priors, global context relations, and self-contained confidence maps to generate an accurate initial dense flow map. Additionally, we present Multi-scale Motion Aggre-gation (MMA), a feature enhancement and multi-scale fea-ture matching module that aggregates mutual dependen-cies of features and utilizes cross-scale information for im-proved initial flow estimation.
Based on the proposed CFP and MMA modules, we develop an efficient and powerful optical flow estimation model, namely EMD-Flow. Benefitting from the property of CFP and MMA modules, our EMD-Flow is able to ef-fectively handle large motion before recurrent scheme (see
Fig. 2b). Moreover, our full network is designed with a high-efficiency principle to ensure a cost-effective model, as in Fig. 2d. We conduct comprehensive experiments to demonstrate that our approach achieves both high efficiency and excellent performance on the standard benchmarks, in-cluding Sintel and KITTI. To summarize, the main contri-butions of our work are as follows:
• We introduce Explicit Motion Disentangling (EMD) strategy to handle global motion and small dis-placement estimation separately, achieving a better performance-runtime balance. (a) Average delta flow in the re-current decoder of RAFT [32]. (b) Percentage of the large motion in delta flow. (c) RAFT time distribution. (d) EMD-Flow time distribution.
Figure 2: Analyses of the delta flow and runtime com-parison with RAFT[32]. The average delta flow in (a) and (b) are counted on Sintel clean. “-gt40” and “-gt60” indi-cate the strength of ground truth flow at the sampled points are larger than 40 and 60 pixels, respectively. We regard the flow vectors with value > 32 pixels as the large motion.
• We propose Confidence-induced Flow Propagation (CFP) and Multi-scale Motion Aggregation (MMA) modules, which improve the accuracy of flow estima-tion while maintaining runtime efficiency.
• Our proposed model, EMD-Flow, achieves state-of-the-art performance on standard benchmarks while consuming fewer computational resources, demon-strating the effectiveness of our approach. 2.