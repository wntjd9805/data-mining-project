Abstract
In recent years, many deep learning based image in-painting methods have been developed by the research com-munity. Some of those methods have shown impressive im-age completion abilities. Yet, to the best of our knowl-edge, there is no image inpainting model designed to run on mobile devices. In this paper we present a simple im-age inpainting baseline, Mobile Inpainting GAN (MI-GAN), which is approximately one order of magnitude computa-tionally cheaper and smaller than existing state-of-the-art inpainting models, and can be efficiently deployed on mo-bile devices. Excessive quantitative and qualitative eval-uations show that MI-GAN performs comparable or, in some cases, better than recent state-of-the-art approaches.
Moreover, we perform a user study comparing MI-GAN re-sults with results from several commercial mobile inpaint-ing applications, which clearly shows the advantage of
MI-GAN in comparison to existing apps. With the pur-pose of high quality and efficient inpainting, we utilize an effective combination of adversarial training, model re-parametrization, and knowledge distillation. Our models and code are publicly available at https://github. com/Picsart-AI-Research/MI-GAN . 1.

Introduction
The task of image inpainting is to complete the missing regions in images in a realistic and visually plausible way.
Image inpainting algorithms have found their applications in such problems as image restoration, removal of unwanted objects, etc.
Nowadays, in the era of fast evolving gadgets, partic-ularly smartphones, image/video editing applications are growing furiously. The usage of mobile devices for mak-ing creative visual content by professionals as well as by consumers shows a great increasing tendency. A lot of mo-bile applications such as Photoshop Express [24], Picsart
[25], Snapseed [39], TouchRetouch [18] empower the users to create awesome content utilizing AI-powered tools. One of the most popular such tools, object remover, allows its users to remove unwanted objects (e.g. watermarks, lines, people or random objects in the background, etc.) from their photos. So millions of users every day benefit from image inpainting technologies integrated in various mobile appli-cations.
Current AI-based state-of-the-art inpainting algorithms
such as Co-Mod-GAN [65], SH-GAN [58], LaMa [53], or
LDM [46] include an inference of a heavyweight neural net-work, which is often not feasible to do on low-end mobile devices (see Tables 1, 2). Therefore the processing is be-ing done on server-side resulting in dependency on Internet connection, speed, traffic and server usage costs for image editing companies.
To address this problem we propose a simple baseline for high-quality mobile image inpainting which is an order of magnitude lighter and computationally more efficient than current state-of-the-art neural networks while showing sim-ilar or sometimes even better performance (see Fig. 1).
Motivated by the generative nature of the image comple-tion problem we design MI-GAN, Mobile Inpainting Gen-erative Adversarial Network, which benefits from a hy-brid system consisting of adversarial training, model re-parametrization, and knowledge distillation. Thanks to ad-versarial nature of the MI-GAN training, image comple-tion results appear to be visually plausible without pro-ducing blurry regions and pattern-like artifacts. Model re-parametrization further increases the output quality at no cost of efficiency. Finally, knowledge distillation by a stronger network also improves the generative ability of MI-GAN.
Due to excessive evaluations MI-GAN shows great per-formance in terms of both quantitative and qualitative com-parison with existing state-of-the-art methods.
To summarize, our contributions are three-fold.
• We propose MI-GAN, to the best of our knowledge the first mobile generative image inpainting network.
• We develop a customized knowledge distillation method that is suitable for our model and problem, as well as we implement a model re-parametrization strategy which enhances the quality of results.
• While being computationally more efficient and lighter than the existing state-of-the-art image completion ap-proaches, MI-GAN produces similar or even better im-age inpainting results. Furthermore, in comparison to existing commercial mobile inpainting applications,
MI-GAN results are generally rated higher by human evaluators. 2.