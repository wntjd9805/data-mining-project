Abstract
Amodal Instance Segmentation (AIS) endeavors to ac-curately deduce complete object shapes that are partially or fully occluded. However, the inherent ill-posed nature of single-view datasets poses challenges in determining oc-cluded shapes. A multi-view framework may help allevi-ate this problem, as humans often adjust their perspective when encountering occluded objects. At present, this ap-proach has not yet been explored by existing methods and datasets. To bridge this gap, we propose a new task called
Multi-view Amodal Instance Segmentation (MAIS) and in-troduce the MUVA dataset, the first MUlti-View AIS dataset that takes the shopping scenario as instantiation. MUVA provides comprehensive annotations, including multi-view amodal/visible segmentation masks, 3D models, and depth maps, making it the largest image-level AIS dataset in terms of both the number of images and instances. Additionally, we propose a new method for aggregating representative features across different instances and views, which demon-strates promising results in accurately predicting occluded objects from one viewpoint by leveraging information from other viewpoints. Besides, we also demonstrate that MUVA can benefit the AIS task in real-world scenarios. 1 1.

Introduction
The amodal instance segmentation (AIS) task aims to determine an objectâ€™s entire shape, encompassing its vis-ible and occluded components. AIS task is more chal-lenging than the visible instance segmentation task [23, 11]
*Corresponding author 1The proposed MUVA dataset can be downloaded from this link: https://zhixuanli.github.io/project 2023 ICCV MUVA.
Figure 1. Comparison of the impact of ill-posed problems on amodal prediction in single-view and multi-view input settings. (a) In single-view input, ambiguity arises due to multiple candi-dates for the occluded object. (b) Multi-view input helps alleviate ambiguity and improves amodal prediction accuracy. as it lacks occluded region appearance. Despite its com-plexity, the AIS task has significant implications for vari-ous industrial applications that encounter occlusion prob-lems, such as robotic arm grasping [1], pedestrian re-identification [34, 36, 29], automatic driving [27, 28], and self-checkout systems in supermarkets [8].
Although numerous datasets [19, 40, 8, 27, 13] and methods [19, 40, 8, 27, 13, 38, 33, 21] have been proposed since the AIS task was firstly introduced in 2016 by Li and Malik [19], current AIS datasets and methods rely on a single-view approach, suffering from the ill-posed prob-lem. For example, as shown in Fig. 1(a), directly deducting
the complete shape from a single-view image is extremely challenging due to the presence of occluded regions with multiple potential candidates. This is because distinct ob-jects can share the same visible appearance but differ in shape within the occluded region. However, as shown in
Fig. 1(b), humans tend to observe occluded objects from multiple angles to obtain accurate predictions.
Inspired by this observation, this paper proposes a task called MAIS (Multi-view Amodal Instance novel
Segmentation), which predicts amodal segmentations through multiple viewpoints. The task is designed for real-world applications, under the assumption that the number of viewpoints and field of vision is limited, and objects in the scene are closely distributed with reasonable occlusion.
These assumptions contribute to the difficulty and practical significance of the MAIS task.
To study the MAIS task, creating a multi-view AIS dataset is essential. However, annotating multi-view data based on existing or newly collected real-world data in-volve significant efforts and may not be accurate in iden-tifying shapes of occluded regions. This is because man-ually annotated amodal masks in real-world datasets are often inaccurate and inconsistent due to the varying shape prior knowledge of annotators, as shown in Fig. 2 (copied from [40]). Therefore, using a synthetic approach would be more precise and controllable. To create a synthetic dataset, one option is to build upon existing synthetic AIS datasets [6, 13, 14]. However, these datasets lack high-quality 3D models and sufficient occlusion. To overcome the limitation, we propose to create a new synthetic dataset comprising reconstructed high-quality 3D models and suf-ficient occlusion by controlling the distribution of objects.
As an initial step in exploring the MAIS task, we limit our dataset construction to a single scenario. Specifically, the shopping scenario is selected due to its potential for multi-ple camera arrangements and its tendency for severe occlu-sion when goods are piled up. Consequently, we introduce
MUVA, a novel MUlti-View Amodal Instance Segmenta-tion dataset. The dataset creation process follows a standard three-step approach. Initially, 3D artists construct models from images collected from on-sale items. Next, 3D mod-els are then selected and placed in a 3D scene with care-ful compositions to control the occlusion degree. Finally, multi-view images are captured by six simulated cameras to simulate a real-world self-checkout setting. Each image is extensively annotated with visible/amodal segmentation masks, depth maps, occlusion orders, and 3D models.
To our best knowledge, MUVA is the first and only multi-view AIS dataset currently available. Unlike real-world-based AIS datasets [19, 40, 8, 27], MUVA provides precise annotations in the occluded region due to its syn-thetic nature and known ground-truth 3D models. Besides,
MUVA is also the largest image-level AIS dataset in terms of both images and instances.
To exploit the multi-view information in MUVA, we in-troduce MASFormer, a novel method that aggregates infor-mation across different views and instances. Through com-parative experiments with existing single-view-based AIS methods, we demonstrate that MASFormer significantly outperforms these methods by effectively utilizing multi-view information. Additionally, our approach can accu-rately predict objects that are severely occluded from one angle by incorporating information from other angles.
Figure 2. Two examples of COCOA [40] dataset, displaying mul-tiple annotated occluded regions by different annotators.
Our contributions are summarized as follows: 1) A new task named MAIS is proposed to explore the AIS task un-der the multi-view setting for alleviating the ill-posed prob-lem in the AIS task. 2) A novel dataset named MUVA is proposed for the MAIS task in the multi-view setting for shopping scenarios. To our best knowledge, MUVA is the first AIS dataset under the multi-view setting. 3) A new method named MASFormer is proposed to collect both view-level and instance-level information for solving the ill-posed problem in the AIS task. The experimental re-sults show the efficiency of the multi-view setting over the single-view approach. 2.