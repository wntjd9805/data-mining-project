Abstract
In recent years, end-to-end scene text spotting ap-proaches are evolving to the Transformer-based framework.
While previous studies have shown the crucial importance of the intrinsic synergy between text detection and recog-nition, recent advances in Transformer-based methods usu-ally adopt an implicit synergy strategy with shared query, which can not fully realize the potential of these two in-teractive tasks.
In this paper, we argue that the explicit synergy considering distinct characteristics of text detec-tion and recognition can significantly improve the perfor-mance text spotting. To this end, we introduce a new model named Explicit Synergy-based Text Spotting Trans-former framework (ESTextSpotter), which achieves explicit synergy by modeling discriminative and interactive fea-tures for text detection and recognition within a single de-coder. Specifically, we decompose the conventional shared query into task-aware queries for text polygon and con-tent, respectively. Through the decoder with the proposed vision-language communication module, the queries inter-act with each other in an explicit manner while preserving discriminative patterns of text detection and recognition, thus improving performance significantly. Additionally, we propose a task-aware query initialization scheme to en-sure stable training. Experimental results demonstrate that our model significantly outperforms previous state-of-the-art methods. Code is available at https://github. com/mxin262/ESTextSpotter. 1.

Introduction
End-to-end text spotting, aiming at building a unified framework for text detection and recognition in natural scenes, has received great attention in recent years [32, 25,
†Equal contribution.
∗Corresponding author.
Figure 1: Comparison of implicit and explicit synergy be-tween text detection and recognition.
Implicit synergy is achieved by sharing parameters and features. Explicit syn-ergy is attained by explicitly modeling discriminative and interactive features. Back.: backbone. Enc. (Dec.): en-coder (decoder). TA Dec.: task-aware decoder. 33]. Intuitively, the position and shape of the text in the de-tection can help the text recognition accurately extract the content of the text. Similarly, the position and classifica-tion information in recognition can also guide the detector to distinguish between different text instances and the back-ground. Such mutual interaction and cooperation between text detection and recognition are recently known as scene text spotting synergy [17], which aims to produce a com-bined effect greater than the sum of their separate effects.
Indeed, synergy is the key to the success in literature.
In the past few years, many methods attempt to join text detection and recognition by proposing a new Region-of-Interest (RoI) operation to achieve the synergy between text detection and text recognition [32, 12, 50, 33, 52], as shown
in Figure 1(a). They follow the classical two-stage pipeline, which first locates the text instance and then extracts the text content in the corresponding region of interest (RoI). How-ever, the interaction between detection and recognition is in-sufficient through sharing a backbone, as observed in recent research [17]. A recent study, TESTR [68], develops dual-decoder framework to further share an encoder, but there is still a lack of interaction between the two tasks, as presented in Figure 1(b). Therefore, some researchers [21, 63] be-gin to explore better synergy based on the Transformer [49] architecture. For instance, TTS [21] takes a step toward unifying the detector and recognizer into a single decoder with shared query for both two tasks as illustrated in Fig-ure 1(c). DeepSolo [63] further adopts a group of shared queries to encode the characteristics of text. Although these approaches [21, 63] develop a more concise and unified framework, they fail to consider distinct feature patterns of these two tasks. We formulate the above-mentioned methods as utilizing an implicit synergy that shares param-eters and features between the detection and recognition, but lacks explicit modeling between them, as shown in Fig-ure 1(I). The full potential of two tasks can not be realized by implicit synergy alone without considering the unique characteristics of each task [47, 60]. For instance, while
DeepSolo has demonstrated promising end-to-end results on Total-Text [7], its detection performance falls short of that achieved by the dedicated detection method [48].
In this paper, we propose an Explicit synergy Text Spot-ting Transformer framework, termed ESTextSpotter, step-ping toward explicit synergy between text detection and recognition. Compared to previous implicit synergy, ES-TextSpotter explicitly models discriminative and interactive features for text detection and recognition within a single decoder, as illustrated in Figure 1(d). Technically, we de-sign a set of task-aware queries to model the different fea-ture patterns of text detection and recognition, which in-clude detection queries encoding the position and shape in-formation of the text instance, and recognition queries en-coding the position and semantics information of the char-acter. The position information of the character is ob-tained through an attention mechanism similar to previous works [11, 57]. Then, detection queries and recognition queries are sent into a task-aware decoder that is equipped with a vision-language communication module to enhance explicit synergy. Previous works [68, 21, 63] have used learnable embeddings to initialize the queries. However, these randomly initialized parameters will disrupt the train-ing of the vision-language communication module. There-fore, we propose a task-aware queries initialization (TAQI) to promote stable training of the vision-language communi-cation module. Besides, inspired by [23, 64], we also em-ploy a denoising training strategy to expedite convergence.
Extensive experiments demonstrate the effectiveness of our method: 1) For text detection, ESTextSpotter signifi-cantly outperforms previous detection methods by an av-erage of 3.0% in terms of the H-mean on two arbitrarily-shaped text datasets, 1.8% on two multi-oriented datasets, and 3.0% on Chinese and multi-lingual datasets; 2) For En-glish text spotting, ESTextSpotter consistently outperforms previous methods by large margins; 3) ESTextSpotter also significantly outperforms previous methods on multilin-gual text spotting including Chinese text (ReCTS), African
Amharic text (HUST-Art), and Vietnamese text (VinText), with an average of 4.8% in terms of the end-to-end H-mean.
In conclusion, our contributions can be summarized as follows.
• We introduce ESTextSpotter, a simple yet efficient
Transformer-based approach for text spotting that adopts task-aware queries within a single decoder, which allows it to effectively realize explicit synergy of text detection and recognition, thereby unleashing the potential of these two tasks.
• We propose a vision-language communication mod-ule designed to enhance explicit synergy, which uti-lizes a novel collaborative cross-modal interaction be-tween text detection and recognition. Moreover, we introduce a task-aware query initialization module to guarantee stable training of the module.
• We achieve significant improvements over state-of-the-art methods across eight challenging scene text spotting benchmarks. 2.