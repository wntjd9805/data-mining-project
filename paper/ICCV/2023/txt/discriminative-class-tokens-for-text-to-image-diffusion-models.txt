Abstract
Recent advances in text-to-image diffusion models have enabled the generation of diverse and high-quality images.
While impressive, the images often fall short of depicting subtle details and are susceptible to errors due to ambigu-ity in the input text. One way of alleviating these issues is to train diffusion models on class-labeled datasets. This ap-proach has two disadvantages: (i) supervised datasets are generally small compared to large-scale scraped text-image datasets on which text-to-image models are trained, affect-ing the quality and diversity of the generated images, or (ii) the input is a hard-coded label, as opposed to free-form text, limiting the control over the generated images.
In this work, we propose a non-invasive fine-tuning tech-nique that capitalizes on the expressive potential of free-form text while achieving high accuracy through discrim-inative signals from a pretrained classifier. This is done by iteratively modifying the embedding of an added input token of a text-to-image diffusion model, by steering gen-erated images toward a given target class according to a classifier. Our method is fast compared to prior fine-tuning methods and does not require a collection of in-class im-ages or retraining of a noise-tolerant classifier. We evalu-ate our method extensively, showing that the generated im-ages are: (i) more accurate and of higher quality than stan-dard diffusion models, (ii) can be used to augment train-ing data in a low-resource setting, and (iii) reveal informa-tion about the data used to train the guiding classifier. The code is available at https://github.com/idansc/ discriminative_class_tokens. 1.

Introduction
Text-to-image diffusion models [37, 10] have shown re-markable success in generating diverse and high-quality im-ages conditioned on input text. However, they often fall short when the input text contains lexical ambiguity or when generating fine-grained details. For instance, one might
*Equal contribution. wish to render an image of a clothes ‘iron’, but could in-stead be presented with an image of the elemental metal.
To alleviate these issues, pretrained classifiers have been used to guide the denoising process. One such method mixes the score estimate of a diffusion model with the gra-dient of the log probability of a pre-trained classifier [10].
However, this approach has the downside of requiring a classifier that works on both real and noisy data. Others have conditioned the diffusion on class labels using a cu-rated dataset [24] . While effective, this approach does not lead to the full expressive power of models trained on huge collections of web-scale image-text pairs.
A different line of work fine-tunes a diffusion model, or some of its input tokens, using a small (∼5) collection of images [15, 26, 38]. These methods have the following drawbacks: (i) training new concepts can be slow, taking upwards of a few hours, (ii) the method may change the dis-tribution of the generated images (as compared to the orig-inal diffusion model) to fit only the new label or concept, and (iii) the generated images are based on features from a small group of images and may not capture the diversity of the entire class.
This work introduces a method that more accurately cap-tures the desired class, avoiding lexical ambiguity while more accurately portraying fine-grained details. It does so while retaining the full expressive power of the original pre-trained diffusion model without the above-mentioned draw-backs. Instead of guiding the diffusion process or updating the entire model with the classifier, we only update the rep-resentation of a single added token, corresponding to each class of interest. We do this without tuning the model on labeled images. To learn the token representation corre-sponding to a given target class, we iteratively generate new images with a higher class probability according to the pre-trained classifier. At each iteration, feedback from the clas-sifier steers the designated discriminative class token to this end. Our optimization process uses a new technique, gra-dient skipping, which only propagates the gradient through the final stage of the diffusion process. The optimized token is then used as part of the conditioning text-input to generate images using the original diffusion model.
Figure 1: We propose a technique that introduces a token (Sc) corresponding to an external classifier label class c. By including the token in the input text we can improve both text-to-image alignment when there is lexical ambiguity (left) and enhance the depiction of intricate details (right).
Our method has several advantages. First, unlike other class conditional methods such as [10], it only requires an off-the-shelf classifier and does not require a classifier trained on noisy data. Second, our method is fast and allows for “plug-and-play” improvements of generated images af-ter a class-token has been trained. This is in comparison to other methods, such as Textual Inversion [15], which can take a few hours to converge.
Third, our method employs a classifier trained on an ex-tensive collection of images without needing access to those images. This is beneficial as (i) the token is trained using the full set of class-discriminative features, as opposed to fea-tures from a small set of images, and (ii) it may be desirable to share only the classifier and not the data on which it is trained, such as when privacy concerns are involved.
We evaluate our method both in the fine-grained and coarse-grained settings. In the fine-grained setting, we in-vestigate the ability of our method to generate details of species in the CUB [42] and iNat21 [40] datasets. In the coarse setting, we consider the ImageNet [9] dataset. Our primary metric is the accuracy of the generated samples as measured in two ways: (i) we show that our generated im-ages are more often correctly classified using pre-trained classifiers, in comparison to baselines, and (ii) we show that classification models trained on generated samples, either on their own or in combination with a limited amount of real training data, result in improved accuracy compared to the baseline. We also measure the quality and diversity of the generated images compared to SD and a different class-conditioning technique, showing that our method is superior in terms of the commonly used Fr´echet inception distance (FID) [21]. Finally, we include many qualitative examples demonstrating the effectiveness of our approach. In Fig. 1, we show how our method can resolve ambiguity in the in-put text and add discriminative features for a given class. In the ambiguous category, the image of a tiger cat becomes the cat species instead of a tiger, iron is made to refer to the tool as opposed to the metal. Appenzeller moves from de-picting a group of people, from the Appenzeller area, to the dog species. In the fine-grained category, the Blue winged warbler’s throat color is corrected, the shape features of the
Coastal woodfern are corrected, and the American gray fox more closely resembles the true species. 2.