Abstract
Although point cloud registration has achieved remark-able advances in object-level and indoor scenes, large-scale registration methods are rarely explored. Challenges mainly arise from the huge point number, complex distri-bution, and outliers of outdoor LiDAR scans.
In addi-tion, most existing registration works generally adopt a two-stage paradigm: They first find correspondences by extract-ing discriminative local features and then leverage estima-tors (eg. RANSAC) to filter outliers, which are highly de-pendent on well-designed descriptors and post-processing choices. To address these problems, we propose an end-to-end transformer network (RegFormer) for large-scale point cloud alignment without any further post-processing.
Specifically, a projection-aware hierarchical transformer is proposed to capture long-range dependencies and filter out-liers by extracting point features globally. Our transformer has linear complexity, which guarantees high efficiency even for large-scale scenes. Furthermore, to effectively re-duce mismatches, a bijective association transformer is de-signed for regressing the initial transformation. Extensive experiments on KITTI and NuScenes datasets demonstrate that our RegFormer achieves competitive performance in terms of both accuracy and efficiency. Codes are available at https://github.com/IRMVLab/RegFormer. 1.

Introduction
Point cloud registration is a fundamental problem in 3D computer vision, which aims to estimate the rigid transfor-mation between point cloud frames. It is widely applied in moblie robotics [22, 34], autonomous driving [47, 37], etc.
*Corresponding Authors.
The first two authors contributed equally.
Figure 1. Overview architecture of RegFormer. The whole feature extraction and frame association sections are transformer-based.
We project point cloud onto a 2D surface and feed its patches into transformer. A projection mask M T (M S) is also proposed, which equips our transformer with the awareness of invalid positions.
Although learning-based methods show great potential in object-level or indoor registration tasks [42, 10, 1, 17], large-scale point cloud registration is less studied. Chal-lenges are mainly three-fold: 1) Outdoor LiDAR scans may consist of hundreds of thousands of unstructured points, which are intrinsically sparse, irregular, and have a large spatial range. It is non-trivial to efficiently process all raw points in one inference [44]. 2) Outliers from dynamic objects and occlusion would degrade the registration accu-racy as they introduce uncertain motions and inconsistency. 3) There are numerous mismatches when directly leverag-ing distance-based nearest neighbor matching methods (eg. kNN) to distant point cloud pairs [25].
For the first challenge, previous registration works mostly voxelize input points [4, 25], and then establish pu-tative correspondences by selecting keypoints and learning distinctive local descriptors [42, 10, 1]. However, quantiza-tion errors are inevitable in the voxelization [18]. Also, dif-ferent selected keypoints may influence registration accu-racy and downsampling challenges the repeatability [45]. In this paper, instead of searching keypoints, we directly pro-cess all LiDAR points by projecting them onto a cylindrical surface for the structured organization. Projected image-like structure facilitates the window partition in transformer, realizing linear computational costs. This enables our net-work to process almost 120000 points with high efficiency.
To take advantage of 3D geometric features, each projected position is filled with raw point coordinates, inspired by
[37]. Another concern is that projected pseudo images are full of invalid positions due to the original sparsity of point clouds. We handle this by designing a projection mask.
For the second challenge, the commonly used method is applying the robust estimator (RANSAC) [13, 4, 1] to filter outliers. However, RANSAC suffers from slow con-vergence [30] and is highly dependent on post-processing choices [44].
From a different view, we observe that global modeling capability is rather helpful to localize occluded objects and recognize dynamics as they intro-duce inconsistent global motion. Therefore, we propose a projection-aware transformer to extract point features glob-ally. Notably, some recent works [30, 44] also try to design
RANSAC-free registration networks. However, the combi-nation of CNN and transformer in their feature extraction modules deteriorates the efficiency. The closest approach to ours is REGTR [44], which directly predicts clean cor-respondences with transformer. Nonetheless, the quadratic complexity limits its ability for large-scale application.
In addition, a Bijective Association Transformer (BAT) is designed to tackle the third challenge. HRegNet [25] already has awareness that nearest-neighbor matching can lead to considerable mismatches due to possible errors in descriptors. However, their kNN cluster is still distance-based, which can not generalize well to low-overlap in-puts. To address this problem, two effective components are designed in BAT for reducing mismatches. The cross-attention mechanism is utilized first for preliminary location information exchange. Intuitively, features of deeper layers are coarse but reliable as they gather more information with larger receptive fields. Thus, each point is correlated with all points (instead of selecting k points) in the other frame to gain reliable motion embeddings on the coarsest layer (all-to-all). The precise transformation will then be recovered by the iterative refinement on shallow layers.
Overall, our contributions are as follows:
• We propose a fully end-to-end network for large-scale point cloud registration. It does not need any keypoint matching or post-processing, which is both keypoint-free and RANSAC-free. Our efficient model can pro-cess hundreds of thousands of points in real time.
• The global modeling capability of our RegFormer can filter outliers effectively. Furthermore, a Bijective As-sociation Transformer (BAT) is designed to reduce mismatches by combining cross-attention with an all-to-all point correlation strategy on the coarsest layer.
• Experiment results on KITTI [16, 15] and NuScenes
[5] datasets indicate that our RegFormer achieves state-of-the-art performance with 99.8% and 99.9% successful registration recall respectively. 2.