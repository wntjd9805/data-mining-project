Abstract
We present a diffusion-based model for 3D-aware gen-erative novel view synthesis from as few as a single input image. Our model samples from the distribution of possible renderings consistent with the input and, even in the presence of ambiguity, is capable of rendering diverse and plausible novel views. To achieve this, our method makes use of existing 2D diffusion backbones but, crucially, incorporates geom-etry priors in the form of a 3D feature volume. This latent feature field captures the distribution over possible scene rep-resentations and improves our method’s ability to generate view-consistent novel renderings. In addition to generating novel views, our method has the ability to autoregressively synthesize 3D-consistent sequences. We demonstrate state-of-the-art results on synthetic renderings and room-scale scenes; we also show compelling results for challenging, real-world objects. 1.

Introduction
In this work, we address multiple open problems in novel view synthesis (NVS): to design an NVS framework that (1) operates from as little as a single image and is capable of (2) generating long-range of sequences far from the input views as well as (3) handling both individual objects and complex scenes (see Fig. 1). While existing few-shot NVS approaches, trained on a category of objects with a regression objective, can generate geometrically consistent renderings, i.e., sequences whose frames share a coherent scene structure, they are ineffective in handling extrapolation and unbounded scenes (see Fig. 2). Dealing with long-range extrapolation (2) requires using a generative prior to deal with the innate ambiguity that comes with completing portions of the scenes that were unobserved in the input. In this work, we propose a diffusion-based few-shot NVS framework that can generate plausible and competitively geometrically consistent render-ings, pushing the boundaries of NVS towards a solution that
*Equal contribution.
†Work was done during an internship at NVIDIA.
Figure 1. Our 3D-aware diffusion model synthesizes realistic novel views from as little as a single input image. These results are generated with the ShapeNet [12], Matterport3D [11], and Common
Objects in 3D [58] datasets.
Figure 2. While regression-based models are capable of effective view synthesis near input views (top row), they blur across ambiguity when extrapolating. Generative approaches can continue to sample plausible renderings far from input views (second row, third column). can operate in a wide range of challenging real-world data.
Previous approaches to few-shot novel view synthesis can broadly be grouped into two categories. Geometry-prior-based methods [61, 60, 48, 43, 49, 3, 104] have drawn from work on scene representations and neural rendering [87].
While they achieve impressive results on interpolating near 1
input views, most methods are trained purely with regression objectives and struggle in dealing with ambiguity or longer-range extrapolations. When challenged with the task of novel view synthesis from sparse inputs, they can only tackle mildly ambiguous cases, i.e., cases where the conditional distribution of novel renderings is well approximated by the mean estimator of this distribution — obtained by minimizing a pixel-wise L1 or L2 loss [106, 78, 104]. However, in highly ambiguous cases, for example when parts of the scene are occluded in all the given views, the conditional distribution of novel renderings becomes multi-modal and the mean estimator produces blurry novel views (see Fig. 2). Because of these limitations, regression-based approaches are limited to short-range view interpolation of object-centric scenes and struggle in long range extrapolation of unconstrained scenes.
By contrast, generative approaches solve the novel view synthesis problem by sampling random plausible samples from a conditional distribution modeled by a generative prior.
Existing generative models for view synthesis [64, 97, 59, 44] autoregressively extrapolate one or a few input images with few or no geometry priors. For this reason, most of these methods struggle with generating geometrically consistent se-quences — renderings are only approximately consistent be-tween frames and lack a coherent rigid scene structure. In this work, we present an NVS method that bridges the gap between geometry-based and generative view synthesis approaches for both geometrically consistent and generative rendering.
Our method leverages recent developments in diffusion models. Specifically, conditional diffusion models [67, 65, 57, 63, 66] can be directly applied to the task of NVS. Con-ditioned on input images, these models can sample from the conditional distribution of output renderings. As a generative model, they naturally handle ambiguity and lend themselves to continued autoregressive extrapolation of plausible outputs.
However, as we show in Sec. 4 (Tab. 1), an image diffusion framework alone struggles to synthesize 3D-consistent views.
Geometry priors remain valuable for ensuring view consistency when operating on complex scenes, and pixel-aligned features [68, 104, 92] have been shown to be successful for conditioning scene representations on images. We incorporate these ideas into the architecture of our diffusion-based NVS model with the inclusion of a latent 3D feature field and neural feature rendering [51]. Unlike previous view synthesis works that include neural fields, how-ever, our latent feature field captures a distribution of scene representations rather than the representation of a specific scene. A rendering from this latent field is distilled into the rendering of a particular scene realization through diffusion sampling at inference. This novel formulation is able to both handle ambiguity resulting from long-range extrapolation and generate geometrically consistent sequences.
In summary, contributions of our work include:
• A novel view synthesis method that extends 2D diffusion models to be 3D-aware by conditioning them on 3D neural features extracted from input image(s).
• A demonstration that our 3D feature-conditioned diffusion model can generate realistic novel views given as little as a single input image on a wide variety of datasets, including object level, room level, and complex real-world scenes.
• A showcase that with our proposed method and sampling strategy, our method can generate long trajectories of realistic, multi-view consistent novel views without suffering from the blurring of regression models or the drift of pure generative models. 2.