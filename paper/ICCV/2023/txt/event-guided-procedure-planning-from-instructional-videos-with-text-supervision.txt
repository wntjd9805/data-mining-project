Abstract
In this work, we focus on the task of procedure plan-ning from instructional videos with text supervision, where a model aims to predict an action sequence to transform the initial visual state into the goal visual state. A critical challenge of this task is the large semantic gap between ob-served visual states and unobserved intermediate actions, which is ignored by previous works. Specifically, this se-mantic gap refers to that the contents in the observed vi-sual states are semantically different from the elements of some action text labels in a procedure. To bridge this se-mantic gap, we propose a novel event-guided paradigm, which first infers events from the observed states and then plans out actions based on both the states and predicted events. Our inspiration comes from that planning a proce-dure from an instructional video is to complete a specific event and a specific event usually involves specific actions.
Based on the proposed paradigm, we contribute an Event-guided Prompting-based Procedure Planning (E3P) model, which encodes event information into the sequential model-ing process to support procedure planning. To further con-sider the strong action associations within each event, our
E3P adopts a mask-and-predict approach for relation min-ing, incorporating a probabilistic masking scheme for regu-larization. Extensive experiments on three datasets demon-strate the effectiveness of our proposed model. 1.

Introduction
In this work, we focus on the procedure planning task from instruction videos [7, 5, 38, 53]. Given the current state (a frame or a clip), procedure planning aims to predict a sequence of actions to reach a desired goal state. This goal-driven decision-making capability comes naturally to humans but is difficult for machine learning systems to ac-quire. Therefore, due to its wide real-world applications,
* indicates equal contribution. † indicates the corresponding author.
Figure 1: Illustration of the semantic gap, i.e., the contents in the observed visual states are semantically different from the elements of some action text labels in a procedure. We show a four-action procedure as an example. As shown above, it is difficult to predict that we should “Melt But-ter” by observing the start and goal states, since we see no butter but some other things (e.g., bread, pan, mixture) from the observed states. However, if we take the event of pro-cedure “Make French Toast” into thinking, “Melt Butter” is an indispensable step. Best viewed in color. e.g., Autopilot [48] and Robotic systems [3], solving proce-dure planning is of great significance.
Early works [7, 5, 38] typically address the procedure planning task in an auto-regressive manner, following tra-ditional sequential modeling works [15, 27, 8, 41]. Specif-ically, given both intermediate action labels and interme-diate visual states as supervision, these works adopt two-branch networks to predict the action labels and representa-tions of states separately, based on the input start and goal states. These methods mainly differ in the feature extrac-tor for sequential modeling, e.g., DDN [7] uses RNNs [27],
Plate [38] uses Transformers [42]. However, all these meth-ods need access to intermediate visual states for supervi-sion.
In such a setting, it is necessary to precisely iden-tify the start and end timestamps of all actions in training videos, which is time-consuming and labor-intensive for an-notation.
Recent work [53] provides a way to reduce annotation efforts. They study a weakly-supervised setting that re-moves the need for intermediate visual states as supervi-sion, named Procedure Planning from instructional videos with Text Supervision (PPTS). In PPTS, text representa-tions of intermediate action labels are introduced for su-pervision, leveraging the power of a pre-trained vision-language model [28]. To tackle PPTS, P3IV [53] proposes a memory-augmented Transformer for sequential modeling.
In previous PPTS methods, the prediction of intermedi-ate actions is conditioned on only the observed start and goal visual states. However, it is challenging to build a direct connection between observed visual states and un-observed intermediate actions, due to a large semantic gap between them. This semantic gap refers to that the contents in the observed visual states are semantically different from the elements of some action text labels in a procedure. For example, as shown in Figure 1, it is difficult to directly pre-dict the action “Melt Butter” by observing the start and goal states, since we see no butter but some other things (e.g., bread, pot, mixture) from the observed states.
To bridge the semantic gap, we propose a novel event-guided paradigm (as shown in Figure 2), which is not explored by previous works. Our proposed event-guided paradigm first infers the events of procedures based on the observed visual states and then predicts a sequence of ac-tions based on both the states and predicted events. Our inspiration comes from the fact that planning a procedure from an instructional video is to complete a specific event (i.e., a procedure matches a clear intention). And, since a specific event usually involves specific actions, we can use the event information to support the procedure plan-ning. For example, as shown in Figure 1, after identify-ing the event “Make French Toast” from the observed vi-sual states, we can plan out the action “Melt Butter”, since melting butter is essential to attain crispy French toast. In addition, there are usually strong associations between ac-tions within an event, which can be utilized for planning a reasonable procedure. Also shown in Figure 1, suppose we already know that this procedure is to make French toast and the first three actions are “Dip Bread in Mixture→
Melt Butter→ Put Bread in Pan”, we can deduce that the fourth action should be “Flip Bread” because no one will make French toast with only one side fried.
We contribute an Event-guided Prompting-based Proce-dure Planning (E3P) model based on our proposed event-guided paradigm. Given event labels as supervision, our proposed E3P uses an Event-aware Prompt Generator to en-code event information into the hand-crafted prompts of in-termediate actions. We find that the events can generally be inferred from the observed start and goal visual states.
After sequential modeling based on event-aware prompts, we propose an Action Relation Mining module to model
Figure 2: Previous methods build a direct connection be-tween the observed start and goal visual states and unob-served intermediate actions, ignoring a large semantic gap between them. In contrast, our work proposes a novel event-guided paradigm to bridge the semantic gap. the associations between actions within each event. Our
Action Relation Mining module adopts a mask-and-predict approach and incorporates a probabilistic masking scheme for regularization, aiming to fully consider the action asso-ciations during training. We conduct extensive experiments on three datasets, and the results demonstrate that our pro-posed E3P outperforms previous state-of-the-art methods by a large margin. 2.