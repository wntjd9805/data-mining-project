Abstract
Images are a convenient way to specify which particu-lar object instance an embodied agent should navigate to.
Solving this task requires semantic visual reasoning and ex-ploration of unknown environments. We present a system that can perform this task in both simulation and the real world. Our modular method solves sub-tasks of exploration, goal instance re-identiﬁcation, goal localization, and local navigation. We re-identify the goal instance in egocentric vision using feature-matching and localize the goal instance by projecting matched features to a map. Each sub-task is solved using off-the-shelf components requiring zero ﬁne-tuning. On the HM3D InstanceImageNav benchmark, this system outperforms a baseline end-to-end RL policy 7x and a state-of-the-art ImageNav model 2.3x (56% vs. 25% suc-cess). We deploy this system to a mobile robot platform and demonstrate effective real-world performance, achieving an 88% success rate across a home and an ofﬁce environment. 1.

Introduction
Consider instructing a last-mile delivery agent on where to deliver a package. Specifying a particular porch receptacle can be conveniently done via image, provided you are local to the environment prior to delivery and have foresight to capture the image. This example motivates the fundamental embodied skill we study in this paper: navigating to an object instance speciﬁed by an image. As depicted in Fig. 1 (Top), the agent is provided with egocentric vision and a goal image (in this case, a bed) and must navigate to that particular bed.
This Image Goal Navigation task requires reasoning over the relation of objects in the scene (e.g., disambiguating between instances of similar appearance) and exploring efﬁciently (e.g., entering bedrooms while searching for the bed).
*Work done while interning at Meta AI’s FAIR Labs.
Correspondence: krantzja@oregonstate.edu
Goal Image
Egocentric Vision
Goal Image
Step 1
Step 21
Step 73
Step 95
Step 109
Match Score 0.00
Re-ID
Match Score 31.94
Re-ID  e n e c
S i w e
V t n e g
A p a
M
D
I
-e
R l a o
G
Figure 1: Top: InstanceImageNav tasks an agent with navigating to an object instance described by a goal image.
Bottom: Real-World Deployment. Our method achieves leading performance in sim and transfers to reality. Here, we
ﬁnd a chair 10m away. Videos are on the project page.    
Observations ("")
Instance Re-ID
Goal Localization
Aggregate Goals
{0, … , ' − 1, '}
Image Goal (ℐ∗)
Confidence Score: ∑! "!
Segment
Project
Detected?
Yes
Mod-INN: A Modular 
InstanceImageNav 
Architecture
Map Occupancy
Inv. Pinhole Projection
Coordinate Transform
Temporal Aggregation
Agent 
RGB (ℐ+)
Agent
Depth (#+)
$!
Agent Pose ($+)
Goal Map Channel
Local Navigation
Goal Seek  or Explore?
Fast Marching Method
Greedy Decoding
Exploration
No
Occupancy Map
Determine Nearest Frontier
! ∈ #
Action
Figure 2: Model Overview. We instantiate exploration with frontier-based exploration, instance re-identiﬁcation with feature matching, goal localization with masked feature projection, and local navigation with analytical planning. Sub-task modules are green with supporting components in gray.
In this paper we present a navigation system that can reli-ably perform this instance-based Image Goal Navigation task in the real-world. Speciﬁcally, we propose a modular frame-work consisting of Exploration, Instance Re-Identiﬁcation,
Goal Localization, and Local Navigation. We instantiate this framework using a simple combination of off-the-shelf com-ponents requiring zero ﬁne-tuning. Depicted in Fig. 2, our system uses a frontier-based exploration policy, re-identiﬁes goals with feature matching, localizes goals with projected feature matches, and path plans with an analytical planner.
On the challenging Habitat-Matterport3D (HM3D) [32] In-stanceImageNav benchmark [24], we achieve a success rate of 56% vs. 25% for the best baseline. We deploy our system on a mobile robot platform in two real-world environments where it achieves a success rate of 88% (e.g., Fig. 1 Bottom).
Most prior work tackling Image Goal Navigation (ImageNav)[53, 10, 19, 29, 1, 28, 49] assumes that goal images were captured at random poses in the environment and always match the camera parameters of the agent. As argued in Krantz, et al. [24], this formulation may result in ambiguous image goals (e.g., captures of nondescript walls) and is detached from potential user applications. To overcome these issues, the instance-based ImageNav task (InstanceImageNav) proposed by Krantz, et al. [24] has two key properties: (1) goal images depict an object instance, and (2) goal images are independent of agent embodiment. A baseline end-to-end reinforcement learning policy achieves just 8.3% success. Our system outperforms this 7-fold. To further compare to prior work, we evaluate a state-of-the-art ImageNav method [48] on InstanceImageNav. While it outperforms the baseline, our system outperforms it 2.3x.
Many prior navigation approaches, such as the two men-tioned above, involve training sensors-to-action policies end-to-end using reinforcement or imitation learning. While end-to-end methods can be easy to implement and appli-cable to multiple tasks, they suffer from limitations of high sample complexity [24], overﬁtting [46], and poor sim-to-real transfer [18]. Alternatively, navigation can be decomposed into sub-tasks solved with more constrained skills [8, 9, 17, 18]. This modular paradigm provides bene-ﬁts of increased sample efﬁciency and improved real-world execution [31, 18, 14]. Our method demonstrates these ben-eﬁts with efﬁcient sample complexity (zero ﬁne-tuning) and effective real-world performance (Fig. 1 Bottom), all while achieving top performance on the simulation benchmark.
Altogether, we tackle the relevant and challenging
Instance-speciﬁc Image Goal Navigation task. We propose a modular method that tops the HM3D InstanceImageNav benchmark and outperforms a state-of-the-art ImageNav model. We deploy our system on a mobile robot platform and demonstrate that it can operate reliably in indoor real-world environments. 2.