Abstract
Single-photon sensors measure light signals at the finest possible resolution — individual photons. These sensors in-troduce two major challenges in the form of strong Poisson noise and extremely large data acquisition rates, which are also inherited by downstream computer vision tasks. Pre-vious work has largely focused on solving the image recon-struction problem first and then using off-the-shelf methods for downstream tasks, but the most general solutions that account for motion are costly and not scalable to large data volumes produced by single-photon sensors.
This work forgoes the image reconstruction problem. In-stead, we demonstrate computationally light-weight phase-based algorithms for the tasks of edge detection and motion estimation. These methods directly process the raw single-photon data as a 3D volume with a bank of velocity-tuned filters, achieving speed-ups of more than two orders of mag-nitude compared to explicit reconstruction-based methods. https://wisionlab.com/
Project webpage: project/eulerian-single-photon-vision/ 1.

Introduction
The spatio-temporal resolution of digital image sens-ing has continually increased, culminating in single-photon or quanta sensors such as single-photon avalanche diodes (SPADs) and jots which can resolve individual photon ar-rivals [53, 71, 50, 46, 34]. These sensors enable an excit-ing array of applications, including photography in chal-lenging conditions like low-light, fast-motion, and high dy-namic range [24, 47, 18, 9], high-speed tracking [27], and 3D imaging [68]. While quanta sensors open up new op-portunities by providing access to individual photons, the raw data from these sensors is heavily quantized (down to a single bit per pixel), and noisy from Poisson statistics.
Cost is another problem – treating individual photons sepa-rately instead of aggregating them like conventional sensors means we fundamentally need more storage, computation, and communication (and ultimately power). These chal-lenges are precluding large-scale adoption of this otherwise exciting technology.
SPAD arrays in particular capture binary frames (Fig. 1a) at high speeds of up to 97 kHz [71]. In this context, the most widely studied problem so far has been image recon-struction, with the idea being that recovering high-quality images is critical for any vision task. Reconstructing images from single binary frames is difficult, needing strong priors and computationally intensive algorithms [4, 63, 8]. A nat-ural idea is to aggregate information over many frames [77], but this approach is prone to potentially severe motion blur – in Fig. 1a, the falling ball gets completely blurred when binary frames are naively averaged. Therefore, we need more sophisticated methods to handle motion [27, 9, 12] such as “explicit burst vision” [48], where the visual signal is reconstructed by aligning and robustly merging frames over time [47]. Motivated by the success of burst photogra-phy on smartphones [28], explicit burst vision yields high-quality results, but at heavy computational cost (Fig. 1b).
We propose a class of light-weight computer vision al-gorithms for SPAD arrays (or very high-speed video in gen-eral). They are motivated by the idea that many vision tasks ultimately do not need the full image [10], and are therefore not necessarily tied to the same cost-versus-quality trade-off as image reconstruction. We propose signal phase re-covery as a proxy problem (Sec. 3), which can be addressed without reconstructing the entire signal (image). Phase is an important feature both in visual perception [59] and in computer vision tasks [38, 51, 74, 20, 60]. Treating single-photon sensor data (video) as a 3D volume, the response of oriented and complex 3D filters applied to it encodes scene information such as motion and edge locations. Sec. 4 describes a method to accurately estimate the reliability of these filter responses, and Sec. 5 shows how we adapt classical phase-based low-level vision algorithms to extract the scene information. These methods can be run extremely fast due to involving only linear filtering and pixel-wise op-erations. We obtain speed-ups of more than two orders of magnitude compared to explicit burst vision, with compara-ble quality (Fig. 1c & Sec. 6).
The large difference in speed between explicit burst vi-Figure 1: Single-photon computer vision. (a) A SPAD array captures a high-speed sequence of binary frames. A single frame is extremely noisy and quantized. Naively averaging frames over time increases the signal, but loses motion informa-tion. (b) A Lagrangian vision method based on frame-by-frame reconstruction with robust motion compensation [48, 47].
For each patch, similar patches are searched for over the rest of the sequence and noise is reduced by averaging. (c) Proposed
Eulerian single-photon vision method. Single-photon data is processed in a single pass with velocity-tuned complex 3D filters (Sec. 4), and the phase of the responses is used to extract scene information such as edges and motion vectors (Secs. 5, 6), in a completely localized manner. The computation and data movement costs are both significantly lower. sion and our approach follows from their different perspec-tives. Burst reconstruction invokes search: given a patch, the core task is to find similar patches across the other video frames. Searching over long sequences incurs a high cost, exacerbated when repeating the search for every patch. The general idea of tracking the trajectory of a patch through the exposure volume is similar to a Lagrangian specification in fluid mechanics, that describes the motion of individual par-ticles in a flow field. In contrast, our approach is Eulerian in nature, where properties of the flow (such as rate) are de-scribed at each point in space and time, without the notion of a particle. This categorization was previously made for motion magnification [44, 76], where large speed-ups were also seen with Eulerian methods.
Implications and limitations As single-photon sensors are used more widely and specialized processor architec-tures are developed [3], the Eulerian approach’s simplicity makes it a candidate for on-chip implementation, an impor-tant practical goal due to the cost of data movement. The proposed method provides a general strategy for designing lightweight algorithms for extremely fast vision tasks, di-rectly from raw single-photon data. However, this paper should be seen just as a first step towards this stated goal: significant improvements are needed in both algorithm and implementation to be feasible on real hardware. 2.