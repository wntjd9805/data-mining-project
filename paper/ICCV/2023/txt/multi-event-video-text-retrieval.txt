Abstract
Video-Text Retrieval (VTR) is a crucial multi-modal task in an era of massive video-text data on the Internet.
A plethora of work characterized by using a two-stream
Vision-Language model architecture that learns a joint rep-resentation of video-text pairs has become a prominent ap-proach for the VTR task. However, these models operate un-der the assumption of bijective video-text correspondences and neglect a more practical scenario where video con-tent usually encompasses multiple events, while texts like user queries or webpage metadata tend to be specific and correspond to single events. This establishes a gap be-tween the previous training objective and real-world appli-cations, leading to the potential performance degradation
In this study, we in-of earlier models during inference. troduce the Multi-event Video-Text Retrieval (MeVTR) task, addressing scenarios in which each video contains multi-ple different events, as a niche scenario of the conventional
Video-Text Retrieval Task. We present a simple model, Me-Retriever, which incorporates key event video representa-tion and a new MeVTR loss for the MeVTR task. Compre-hensive experiments show that this straightforward frame-work outperforms other models in the Video-to-Text and
Text-to-Video tasks, effectively establishing a robust base-line for the MeVTR task. We believe this work serves as a strong foundation for future studies. Code is available at https://github.com/gengyuanmax/MeVTR. 1.

Introduction
With the proliferation of multimedia data on the Inter-net every day, Video-Text Retrieval (VTR) task gains in-creasing importance in searching for desired items from the opposite modality given a video or text query. Over the past years, numerous efforts [49, 33, 9, 14, 31, 3, 7, 30, 16, 12, 51, 17] have been undertaken to improve the
*Corresponding author multi-modal retrieval performance on both Video-to-Text and Text-to-Video retrieval tasks. Along with the popularity of powerful vision foundation models like CLIP [36] and
ALIGN [23] that align the image-text pairs in a joint fea-ture space, two-stream video-text models [30, 16] targeting to learn video-text correspondences have further become a mainstream method for the Video-Text Retrieval task.
However, this paradigm neglects that videos usually con-tain more than a single event, while a single textual caption can only capture a fragment of the entire video content. As depicted in Fig. 1, the example video spans a sequence of unrelated and discontinuous events, and each single textual caption merely corresponds to a video segment. This con-gruity undoubtedly contradicts the fundamental of the tra-ditional VTR task, which aims to establish a mapping be-tween video-text pair.
This fact presents a prevalent scenario in Video-Text
Retrieval: videos are intricate, often containing multiple events, whereas texts (typically like search queries or web-page metadata) tend to be specific and fragmentary. Conse-quently, this dichotomy leads to divergent semantic mean-ings for a given video-text pair. Such divergence contradicts the conventional training objective of aligning representa-tions for video-text pairs in VTR models, that video features are aligned with the corresponding text feature.
In this study, we formally introduce a new and realis-tic VTR setting involving multi-event videos, dubbed as
Multi-event Video-Text Retrieval (MeVTR). In the intro-duced MeVTR task, a text describes a single event within a video item, whereas a video corresponds to multiple rele-vant textual captions, each detailing different events.
We assess the performance of previous VTR models trained under standard VTR settings that learn a bijective video-text correspondence on the MeVTR task without re-training. Our evaluation reveals distinct levels of perfor-mance degradation, as illustrated in Table 1. This observa-tion confirms the presence of a performance gap between training within the conventional VTR framework and real-world MeVTR inference scenarios.
Figure 1: An example case of multi-event videos from ActivityNet [4]. The video depicts a sequence of unrelated and discontinuous events, including the progression “a girl is sitting on the beach” → “a young man is practicing tightrope walking” → “a scene of sunset by the beach.” Each textual caption only corresponds to a fragment of the video. Such short and specific textual captions are prevalent in our everyday video data and constitute a common video-text retrieval scenario.
Besides, we retrain a series of previous models to adapt to the MeVTR task. However, we find that they cannot per-form equally well on the Video-to-Text and Text-to-Video.
We speculate that this is caused by non-injective video-text correspondences in the MeVTR setting, where previous
VTR models embed a video and multiple distinct caption texts into a joint representation space and aim at aligning the video feature with the features of all caption texts. Con-sequently, distinct texts corresponding to the same video are mapped to the same feature, potentially resulting in subop-timal retrieval performance.
Under this circumstance, we introduce a new CLIP-based model dubbed Me-Retriever tailored for the MeVTR task. Considering the dichotomy of multi-event video-text pairs, we formulate our approach as follows: (1) represent-ing videos by a bag of key event features and (2) employing a multi-event video-text retrieval loss. This strategy serves the dual purpose of mitigating the collapse of textual fea-tures and harmonizing the learning objectives between the
Text-to-Video and Video-to-Text tasks.
Our contributions can be summarized as follows: 1. We introduce a new task, Multi-event Video-Text Re-trieval for retrieving multi-event video-text pairs, and define new evaluation metrics in MeVTR; 2. We propose a new model called Me-Retriever that rep-resents videos with selected key events and a MeVTR loss for training on MeVTR; 3. We conduct comprehensive experiments to showcase
Me-Retriever’s effectiveness and to establish it as a simple yet robust baseline for future research. 2.