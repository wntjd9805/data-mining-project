Abstract
Audio Description (AD) is the task of generating descrip-tions of visual content, at suitable time intervals, for the beneﬁt of visually impaired audiences. For movies, this presents notable challenges – AD must occur only during existing pauses in dialogue, should refer to characters by name, and ought to aid understanding of the storyline as a whole.
To this end, we develop a new model for automatically generating movie AD, given CLIP visual features of the frames, the cast list, and the temporal locations of the speech; addressing all three of the ‘who’, ‘when’, and
‘what’ questions: (i) who – we introduce a character bank consisting of the character’s name, the actor that played the part, and a CLIP feature of their face, for the princi-pal cast of each movie, and demonstrate how this can be used to improve naming in the generated AD; (ii) when – we investigate several models for determining whether an
AD should be generated for a time interval or not, based on the visual content of the interval and its neighbours; and (iii) what – we implement a new vision-language model for this task, that can ingest the proposals from the character bank, whilst conditioning on the visual features using cross-attention, and demonstrate how this improves over previous architectures for AD text generation in an apples-to-apples comparison. 1.

Introduction
For in acts we must take note of who did it, by what aids or instruments he did it (with), what he did, where he did it, why he did it, how and when he did it.
Thomas Aquinas
Audio Description (AD) is the descriptive narration of visual elements in a video, that are not represented in the original audio track. While there has been a proliferation of online content with closed captioning1 due to advance-ments in ASR, a vast majority of video online does not have AD, mostly due to the prohibitive cost of generat-: also at Google Research
† 1Transcription of the speech
Figure 1. AutoAD II: We propose an automatic AD system that addresses key challenges - when to generate AD, who is in the scene and what is happening visually. ing it ($30 per minute2). Generating AD automatically at scale has multiple beneﬁts; not only does it improve access for the visually impaired – it may also enhance the visual experience for sighted users (sight-free multitasking such as driving, enhanced memory for visual details, language learning, and also aiding those with other cognitive disabil-ities) [41]. Generating AD for movies is also an important research area in computer vision as it requires a system to perform multi-modal reasoning of long videos over time.
Despite these beneﬁts, the progress in generating AD is still at a very nascent stage, due to the following chal-lenges: (a) An ideal AD generation system should perform two tasks simultaneously – ﬁrst, determine when to gen-erate AD by proposing temporal segments; second, gener-ate AD for the proposed segments. Previous works ignore the when completely, operating on already trimmed video segments [62]. (b) Secondly, given the strong relevance of characters to stories [27, 53], AD typically includes refer-ences to a character’s name (who is in the scene), their emo-tion, and their actions. This is particularly challenging as characters change from movie to movie. Due to anonymised test sets (LSMDC [45]), the relevance of character names in
AD is often ignored [62]. (c) Finally, AD also differs sig-niﬁcantly from image or video captioning [33, 35, 46] in that it does not need to provide descriptions of events that can be understood from the sound track alone (such as dia-2https://www.3playmedia.com/blog/select-audio-description-vendor/
logue and ambient sounds) and should incorporate previous context to create a pleasurable listening experience without being repetitive or redundant. Such aspects require reason-ing over multi-modal inputs (i.e., vision, text, and speech) over time while determining what to generate. In this work we propose an AD system that focuses on all these three
W’s – when, who and what (Fig 1).
To address when, we introduce a module to ﬁrst propose temporal segments for AD. The time intervals for possible
AD are constrained in that they do not overlap with the dia-logue, but whether an AD is provided or not in the permissi-ble time intervals depends on a number of factors including: the importance of the visual content to the story line, ambi-guity in the audio soundtrack, and new information relative to previous AD.
For who, we introduce an AD model that can incorporate character information on-the-ﬂy by referring to a text-visual character bank for that movie. One of the challenges of
AD is that each movie has a different set of characters (and the actors that play them) that ought to be referenced in the
AD captions. We address this by training a visual-language model to refer both to the external character bank and to the visual content of the scene when generating AD. The model can then be applied to any movie, given its cast list, without requiring retraining. This signiﬁcantly improves references to characters, both in actual naming and in pronouns, in the generated AD compared to previous methods [18] that could only access names and pronouns present in the dia-logue. Since character references appear in approximately 40% of AD, this is an important improvement.
The ﬁnal challenge is what to generate, and involves rea-soning over multimodal inputs – images, character bank and previous AD context. We do this via a novel multimodal cross-attention architecture, which ingests proposals from the character bank, and then conditions on visual features extracted from the movie frames.
Our contributions are the following. (1) We introduce a
Character Bank to enable our AD generation model to la-bel the characters appearing in the ﬁlm. (2) We propose a
Flamingo-style [1] architecture for the task, and compare this approach to the prompt style [36] architecture used pre-viously for AD [18]. (3) We build a model for predicting when AD should be inserted, i.e. where on the timeline (us-ing speech detection and visual cues). (4) Given the existing challenges with captioning based metrics [16], we employ a new evaluation metric for the AD content performance based on retrieval compared to other AD sentences in the movie. (5) We signiﬁcantly outperform the previous state-of-the-art on the MAD dataset [18, 50]. 2.