Abstract
Synthetic data has emerged as a promising source for 3D human research as it offers low-cost access to large-scale human datasets. To advance the diversity and anno-tation quality of human models, we introduce a new syn-thetic dataset, SynBody, with three appealing features: 1) a clothed parametric human model that can generate a di-verse range of subjects; 2) the layered human represen-tation that naturally offers high-quality 3D annotations to support multiple tasks; 3) a scalable system for produc-ing realistic data to facilitate real-world tasks. The dataset comprises 1.2M images with corresponding accurate 3D annotations, covering 10,000 human body models, 1,187 actions, and various viewpoints. The dataset includes two subsets for human pose and shape estimation as well as human neural rendering. Extensive experiments on Syn-Body indicate that it substantially enhances both SMPL and
SMPL-X estimation. Furthermore, the incorporation of lay-ered annotations offers a valuable training resource for in-vestigating the Human Neural Radiance Fields(NeRF).
1.

Introduction
The fields of 3D human perception [21, 23â€“25, 37, 53] and human reconstruction [13, 14, 26, 40, 41] have become increasingly important, but the lack of available data has limited their development. Collecting real human data on a large scale is challenging due to privacy concerns and time constraints. Therefore, exploring the use of synthetic hu-man datasets has become a critical avenue of research.
Despite the great potential, existing synthetic datasets [5, 8,38,47] suffer from limitations such as the number of avail-able human models and the quality of annotations. The main reason lies in that synthetic human datasets rely on real scans for rendering, which poses three key obstacles.
Firstly, it is challenging to expand the types of body shapes, poses, and clothing available in the dataset. Secondly, as the human models are scanned with clothing, the 3D anno-tations obtained through fitting are prone to errors. Thirdly, it is difficult to obtain annotations of body and clothing sep-arately. To address these issues, we develop a new synthetic dataset termed SynBody. The dataset includes 1.2 million frames with corresponding ground-truth 3D human body annotations. It covers 10,000 human body models, 1,187 motions, and 26,960 video clips with 2.7M SMPL/SMPL-X annotations.
At the heart of SynBody is the layered parametric hu-man model, which constructs the clothed human model in a bottom-up manner. SMPL-X [39] is a widely used para-metric human model, capable of sampling human models with various body shapes. However, it lacks the ability to model clothing, limiting its applicability when synthesiz-ing realistic human models. To overcome this limitation, we introduce SMPL-XL, a parametric human model based on SMPL-X in a layered representation. SMPL-XL en-riches the SMPL-X model in three aspects: (1) Hair system: adding hair and beards to the FLAME [27] model, with 32 types of hair and 13 types of beards; (2) Garment and ac-cessories: adding procedural clothes to the SMPL-X body, including coats, shirts, pants, skirts, shoes, and glasses; (3)
Texture: in addition to adding rich geometry, SMPL-XL also adds rich textures for sampling various skin colors and clothing textures.
The designed SMPL-XL is capable of automatically gen-erating a large number of human models with high-quality annotations. We therefore generate 10,000 clothed human models by sampling various body shapes, clothing styles, hairstyles, accessories, and textures. Notably, the use of the SMPL-X model as the base body model guarantees that the parametric human annotations are always accurate, ob-viating the need for the necessity for annotations through fitting. Furthermore, as the clothes are explicitly attached to the surface of the human body, layered annotations for body and clothes are available.
To generate a large-scale dataset with high diversity and high-quality annotations, we design a scalable and auto-matic system to render images and annotations. We first an-imate the 10,000 dressed human models by retargeting mo-tions from a large motion library [31]. Subsequently, we de-sign an algorithm to place human models in the scene with-out piercing. Multiple cameras are then placed by evaluat-ing self-occlusion, inter-occlusion, and view diversity, and the rendering module renders the assets into images with corresponding annotations.
With SynBody, we launch two tracks that support hu-man pose and shape estimation and human neural render-ing, respectively. Experiments show that SynBody is more effective than AGORA under the same amount of training data for human pose and shape estimation. With diverse and large-scale training data, SynBody achieves significant performance gains on both SMPL and SMPL-X estimation.
In terms of human neural rendering using neural radiance fields (i.e. NeRF [34]), benchmarking existing approaches on SynBody shows that it has comparable performance as real human data. Furthermore, with the layered annota-tions which offer accurate SMPL parameters, we observe that current human NeRF approaches are sensitive to the accuracy of estimated SMPL.
In summary, SynBody is a large-scale synthetic dataset for human perception and modeling, with three main con-tributions: (1) It constructs clothed subjects and samples 10,000 animatable subjects, which is an order of magnitude higher than existing datasets. (2) The clothed subjects are constructed with an explicit cloth model, thus it provides layered 3D annotations of the human body and clothing, which are not available in previous datasets. (3) Experi-ments on SynBody achieve promising results on both hu-man perception and modeling, emphasizing the importance of diversity and annotation quality for downstream tasks. 2.