Abstract
The SOTA face swap models still suffer the problem of either target identity (i.e., shape) being leaked or the target non-identity attributes (i.e., background, hair) failing to be fully preserved in the final results. We show that this in-sufficient disentanglement is caused by two flawed designs that were commonly adopted in prior models: (1) count-ing on only one compressed encoder to represent both the semantic-level non-identity facial attributes(i.e., pose) and the pixel-level non-facial region details, which is contradic-tory to satisfy at the same time; (2) highly relying on long skip-connections [50] between the encoder and the final generator, leaking a certain amount of target face identity into the result. To fix them, we introduce a new face swap framework called “WSC-swap” that gets rid of skip connec-tions and uses two target encoders to respectively capture the pixel-level non-facial region attributes and the semantic non-identity attributes in the face region. To further rein-force the disentanglement learning for the target encoder, we employ both identity removal loss via adversarial train-ing (i.e., GAN [18]) and the non-identity preservation loss via prior 3DMM models like [11]. Extensive experiments on both FaceForensics++ and CelebA-HQ show that our results significantly outperform previous works on a rich set of metrics, including one novel metric for measuring iden-tity consistency that was completely neglected before. 1.

Introduction
Deepfake [27] is a double-edged sword, despite the fact that various negative use cases are currently dominating the conversations to steer people’s attention to their social im-pact [21], it’s arguably true that AI-synthesized faces, bod-ies, and voices have huge potential for a variety of posi-tive applications, such as virtual instructor or health coun-seling as discussed in [48], or other content generations in movie/game industry (i.e., Paul Walker in “Fast and Fu-rious 7”) and etc. Keep improving the generation qual-ity also inspires new ways to detect [7, 15, 20, 57] deep-fakes for negative use cases. Face swapping has been stud-ied intensively in both academia and industry, where the quality improvement is remarkable over the years thanks to the advances in deep generative learning. As defined in [32], face swapping must achieve three goals simulta-neously (1) fully preserve the face identity from the source image (2) fully preserve everything else except the iden-tity (identity-irrelevant) from the target image, and (3) en-sure the final result is both artifacts-free and photo-realistic. 1
Besides, in real-world scenarios, a source image could be swapped to multiple targets, where cross-target consistency becomes fairly important. Therefore, we additionally ar-gue that the disentanglement of ID and non-ID properties is essential for cross-target consistent face swapping. Prior works [8, 17, 32, 35, 58] investigated various methodologies for face identity and non-identity attributes disentanglement as well as their fusions to synthesize the final swapped im-age. Despite the impressive progress, the above three goals are hardly achieved at the same time. One of the key chal-lenges is that those prior works are essentially playing the seesaw-style game, where improving on the identity preser-vation from the source is usually at the cost of sacrificing the non-identity preservation from the target, and vice ver-sus. Fundamentally, we argue that this is still caused by the insufficient disentanglement between face identity and non-identity representations. Specifically, on one hand, works that try to improve the source identity preservation through 3D prior [58] or information compression [17] or smoothing regularization [32] on top of a pre-trained face recognition model [35] usually don’t provide sufficient forces to remove the target identity while preserving other non-identity at-tributes. On the other hand, work like Faceshifter [35] that tries to preserve the low-level non-identity details through attribute loss would also likely leak target identity infor-mation into the results. Such phenomena are evident from
Fig. 3, where the target face silhouette is clearly leaked to the final swapped faces of works [8, 35].
By delving deep into the model structures, we noticed that most of the prior works [8, 17, 32, 35, 58] leverage a bottleneck encoder and decoder structure on the target im-age, where the encoder is expected to (1) fully remove the face identity (2) fully preserve everything else (i.e., pixel-level background appearance, hair, facial expression, head pose, eye gaze, and other non-identity facial attributes) from the target image. While the decoder (also called the gener-ator) is in charge of generating the final swapped result by fusing the source identity representation from a face recog-nition model as well as all the compressed representations from the target decoder. To restore more details, symmet-rical long skip connections are often employed by copying fine-grained low-level features from the encoder [8, 17, 35].
However, we argue that it is very difficult, if not impossi-ble, to simultaneously achieve the above two goals using only one single compressed bottleneck encoder(see Znid in
Fig. 2 (a)). In addition, the skip connections used in the de-coder would inevitably bring the target identity information into the results together with other non-identity attributes, therefore, further hurting the disentanglement learning.
To encourage thorough disentanglement, in this paper, we designed a new framework that gets rid of the skip con-nections in our entire model structure. Specifically, we proposed two separate encoders that respectively capture the pixel-level attributes outside the face region and the semantic-level non-identity facial attributes inside the face region. Each of which is tailored to dedicated capturing its own desired representation without compromising with each other. Meanwhile, a target identity removal loss and a few non-identity attribute preservation losses are explic-itly employed to compensate for the missing details due to the lack of skip connections. To be specific, we leverage adversarial training techniques to penalize the target repre-sentation if it contains any target identity, meanwhile, we use prior models like 3DMM predictor [14] to explicitly preserve facial expression and head pose, in addition to the attribute loss proposed in Faceshifter [35].
To summarize, our contributions are (1) we analyzed that prior works are still suffering poor disentanglement where improving one goal may hurt other goals; (2) we unveiled that the skip connection is one root cause for such insuf-ficient disentanglement in prior model architecture; (3) we proposed new network structures, as well as new training strategies to reinforce the identity disentanglement while preserving more identity-irrelevant attributes to compensate the lack of skip connections; lastly (4) we introduced a new evaluation metric and conducted extensive experiments, the results validated the effectiveness of our method. 2.