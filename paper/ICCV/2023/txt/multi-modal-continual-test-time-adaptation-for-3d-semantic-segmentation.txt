Abstract
Continual Test-Time Adaptation (CTTA) generalizes conventional Test-Time Adaptation (TTA) by assuming that the target domain is dynamic over time rather than sta-In this paper, we explore Multi-Modal Contin-tionary. ual Test-Time Adaptation (MM-CTTA) as a new extension of CTTA for 3D semantic segmentation. The key to MM-CTTA is to adaptively attend to the reliable modality while avoiding catastrophic forgetting during continual domain shifts, which is out of the capability of previous TTA or
CTTA methods. To fulﬁll this gap, we propose an MM-CTTA method called Continual Cross-Modal Adaptive Clustering (CoMAC) that addresses this task from two perspectives.
On one hand, we propose an adaptive dual-stage mecha-nism to generate reliable cross-modal predictions by attend-ing to the reliable modality based on the class-wise feature-centroid distance in the latent space. On the other hand, to perform test-time adaptation without catastrophic forget-ting, we design class-wise momentum queues that capture conﬁdent target features for adaptation while stochastically restoring pseudo-source features to revisit source knowl-edge. We further introduce two new benchmarks to facilitate the exploration of MM-CTTA in the future. Our experimen-tal results show that our method achieves state-of-the-art performance on both benchmarks. Visit our project website at https://sites.google.com/view/mmcotta. 1.

Introduction
Test-Time Adaptation (TTA) proposes a realistic domain adaptation scenario, which adapts pre-trained models to the target domain during the testing process. Unlike previous
Unsupervised Domain Adaptation (UDA) [17, 45, 52, 47],
TTA performs adaptation online without accessing the data from the source domain. Typical TTA [40, 35, 36] methods assume a stationary target domain, while real-world scenar-ios are dynamic over time. To fulﬁll this gap, a previous work [42] proposes a general extension of TTA named Con-∗Equal contributions.
†Corresponding author.
Figure 1. Illustration of how continual domain shifts affect multi-modal segmentation and our method. Unlike the source domain where predictions from both modalities are reliable, the reliabil-ity of each modality varies in MM-CTTA due to different domain shifts. CoMAC tackles MM-CTTA by attending to the reliable modality ((cid:51)) rather than the noisy one ((cid:55)). Meanwhile, source knowledge is stochastically revisited to avoid catastrophic forget-ting. Figure best viewed in color and zoomed in. tinual Test-Time Adaptation (CTTA) which assumes that the target domain is continually changing rather than static.
Compared to TTA, CTTA is more challenging since the per-formance is easily affected by error accumulation [6] and catastrophic forgetting [27] during the continual adaptation.
For 3D semantic segmentation, multi-modal sensors are frequently leveraged in different tasks, such as scene under-standing [4, 28] and semantic map construction [29, 43].
For some speciﬁc applications like semantic-based local-ization [8] and autonomous driving [46, 44], multi-modal information is the key to robust performance under adverse conditions. However, their collaboration has been proven to be sensitive toward domain drifts [1, 35]. In real-world scenarios, such collaboration deterioration could be more severe considering that the target domain is continually changing as in CTTA (e.g., the operating environments of 24/7 AGVs are continually changing due to altering weather or illumination conditions). Hence, it is essential for multi-modal networks to adapt to the dynamic target domain in an online manner.
In this work, we aim to study Multi-Modal Continual Test-Time Adaptation (MM-CTTA) for 3D semantic segmentation, where networks are continually adapted to a changing target domain taking 3D point clouds and 2D images as input without accessing the source data.
Intuitively, one can address MM-CTTA by utilizing
CTTA [30, 42] or TTA [3, 38] methods on 2D and 3D net-works separately. However, this simple extension can not achieve satisfactory performance since it cannot correctly attend to the reliable modality for adaptation when others suffer from severe domain shifts. Take Fig. 1 as an exam-ple: predictions from the 2D image are more accurate at the beginning of the target domain, while 2D results be-come unreliable and 3D predictions prevail as the illumina-tion level signiﬁcantly changes over time. Although previ-ous CTTA or TTA methods have attempted to mitigate this intra-modal prediction noise by augmentations [42] or en-tropy minimization [40, 30], the domain drift in Fig. 1 is too severe to be effectively rectiﬁed by image input itself, lead-ing to inevitable error accumulation. Previous works have proposed different cross-modal fusion methods to mitigate the effect of the noisy modality during adaptation, such as cross-modal consistency [18] for UDA or pseudo-label fusion based on student-teacher consistency [35] for TTA.
However, their methods rely on the premise that the target domain is static, and therefore suffer from catastrophic for-getting in MM-CTTA, leading to degenerated results.
For MM-CTTA, an ideal solution is to suppress the con-tribution from the noisy modality and attend more to the reliable one in an online manner. Typically, the reliability of prediction can be estimated based on its corresponding feature location in the latent space. A prediction with fea-tures close to the centroid is more likely to be reliable, suf-fering less from the domain shift and vice versa. On the other hand, to ensure the validity of centroid-based relia-bility estimation, class-wise centroids should actively adapt to the continually changing target domain while stochasti-cally revisiting the source knowledge to avoid catastrophic forgetting. To this end, we propose an MM-CTTA method called Continual Cross-Modal Adaptive Clustering (Co-MAC) for 3D semantic segmentation. CoMAC consists of three main modules: (i) Intra-Modal Prediction Aggrega-tion (iMPA), (ii) Inter-Modal Pseudo-Label Fusion (xMPF), and (iii) Class-Wise Momentum Queues (CMQs). On one hand, the proposed iMPA and xMPF are utilized to suppress prediction noise based on the centroid-based reliability es-timation from intra-modal and inter-modal perspectives, re-spectively. On the other hand, CMQs are designed to ac-tively adapt class-wise centroids for iMPA and xMPF while avoiding catastrophic forgetting. Additionally, a class-wise contrastive loss is introduced to regularize the extracted fea-tures from drifting too far from centroids.
In summary, our main contributions are three-fold. (i)
We explore a new task MM-CTTA where multi-modal in-put is utilized to perform continual test-time adaptation for 3D semantic segmentation and propose an effective method named CoMAC to leverage multi-modal information for
MM-CTTA. (ii) We propose iMPA and xMPF to generate accurate cross-modal pseudo-labels by attending to a reli-able modality. CMQs are introduced to actively adapt to the target domain without catastrophic forgetting. (iii) We in-troduce two 3D semantic segmentation benchmarks to facil-itate the future exploration of MM-CTTA. Extensive exper-iments show that our method achieves state-of-the-art per-formance, outperforming previous methods signiﬁcantly by 6.9% on the challenging benchmark. 2.