Abstract
Source-free domain adaptation (SFDA) is a popular un-supervised domain adaptation method where a pre-trained model from a source domain is adapted to a target do-main without accessing any source data. Despite rich re-sults in this area, existing literature overlooks the secu-rity challenges of the unsupervised SFDA setting in pres-ence of a malicious source domain owner. This work in-vestigates the effect of a source adversary which may in-ject a hidden malicious behavior (Backdoor/Trojan) during source training and potentially transfer it to the target do-main even after benign training by the victim (target do-main owner). Our investigation of the current SFDA set-ting reveals that because of the unique challenges present in SFDA (e.g., no source data, target label), defending against backdoor attack using existing defenses become practically ineffective in protecting the target model. To address this, we propose a novel target domain protec-tion scheme called secure source-free domain adaptation (SSDA). SSDA adopts a single-shot model compression of a pre-trained source model and a novel knowledge transfer scheme with a spectral-norm-based loss penalty for target training. The proposed static compression and the dynamic training loss penalty are designed to suppress the malicious channels responsive to the backdoor during the adaptation stage. At the same time, the knowledge transfer from an un-compressed auxiliary model helps to recover the benign test accuracy. Our extensive evaluation on multiple dataset and domain tasks against recent backdoor attacks reveal that the proposed SSDA can successfully defend against strong backdoor attacks with little to no degradation in test accu-racy compared to the vulnerable baseline SFDA methods.
Our code is available at https://github.com/ML-Security-Research-LAB/SSDA. 1.

Introduction
Deep Neural Networks (DNNs) have shown remarkable success across a multitude of tasks [18, 8, 12, 9, 5, 37,
∗ These authors contributed equally
Figure 1: Performance of a) existing SFDA [24, 42, 43, 44] methods and SSDA against one attack [10], and b) one competing SFDA [24] and SSDA against popular backdoor attacks [10, 3, 28]. Compared to the vulnerable previous
SFDA, the proposed SSDA is highly secure. 2, 15, 26].
In particular, they have exhibited extraordi-nary performance in various visual tasks such as classifi-cation [18], object detection [8], and semantic segmenta-tion [12]. Even though DNNs have achieved great success in various visual tasks, they heavily depend on the under-lying distribution of training data. Unfortunately, DNNs deployed in real-world scenarios, such as those utilized in autonomous vehicles, frequently encounter new situations, such as varying weather conditions [1] and changing illu-mination levels [36]. Consequently, the machine learning community has increasingly directed their attention towards domain adaptation [4, 27, 22] concept.
Existing Domain Adaptation (DA) setup requires access of the source domain dataset for target domain adaptation.
However, due to increasing privacy concerns and a lack of available source data, these existing DA methods are be-coming impractical. To address this, researchers have intro-duced a new setup for domain adaptation, popularly known as source-free domain adaptation (SFDA), which aims to transfer knowledge from a prior domain (i.e., source) to a new domain (i.e., target) w/o accessing the source dataset.
Moreover, recent SFDA works [24, 43, 6, 19] have also con-sidered the practical constraint of limited labeled data in the real world and performed domain adaptation without any labeled data in the target domain dataset. Therefore, the two primary constraints in SFDA setting are that the target domain is denied access to the source dataset during adap-tation and that the target domain training is unsupervised, i.e., without labeled data.
In this work, we are the first to focus on the security of the target domain in SFDA. In particular, under the SFDA setting, the target domain owner can not access the source dataset and is also completely unaware of the training pro-cess of the source model. Such a setting makes the target domain adaptation extremely vulnerable to adversaries con-sidering the adversary can access source training (as shown in Fig. 2). After training, the target domain owner takes the pre-trained malicious source model and then adapts it to a new target domain dataset without any label, utterly un-aware of the consequences of malicious source training.
The security threat we want to investigate in this work is the Backdoor/Trojan attack [10, 28]. In a backdoor attack, an attacker poisons a subset of the training data using a spe-cific trigger (i.e., input pattern) to train the model. During inference, the model functions accurately under normal cir-cumstances (i.e., no attack scenario). However, when the attacker-designed particular trigger pattern appears in the input, the model fails as intended by the attacker. In the case of SFDA, the backdoor attack becomes more relevant as the source owner can inject the backdoor into the source model w/o any defensive measure from the target owner (who has no access to source data/training). To the best of our knowl-edge, no prior works have explored the vulnerability of the target model against backdoor attacks, considering a mali-cious source domain owner (attacker).
However, given the above setting, defending against backdoor attacks during the target domain adaptation is ex-tremely challenging because of the unique challenges pre-sented by the SFDA setting. First, the target model is ini-tialized using the malicious source model which is already infected with backdoor. Second, the target owner cannot access the source dataset used initially to inject the back-door. As a result, the target owner cannot trivially fine-tune the model using the source dataset for Trojan removal. Fi-nally, the target training is unsupervised, making detect-ing/cleaning backdoor-infected models more challenging.
Because of these unique challenges in SFDA, existing back-door defenses [23, 39, 11] are practically ineffective in de-fending the target model from a source adversary.
Our initial investigation of backdoors in SFDA (in Ta-ble 2) confirms that even after benign training in the tar-get domain, the target model remains vulnerable to the at-tacker’s designed triggered inputs. To make it worse, the threat remains persistent even after applying strong back-door defenses [47] in the current SFDA setting (in Table 3).
Hence, we are the first to validate that current SFDA tech-niques are not safe against the backdoor attack, and none of the existing defenses can protect the target domain model.
To address this issue, we propose a novel target domain training scheme called Secure Source-Free Domain Adap-tation, SSDA, the first successful defense against backdoor attacks tailored for source-free domain adaptation. Our pro-posed novel training method SSDA consists of two key components: first, it performs a single-shot static defen-sive compression of the source model.
It uses spectral norm-based ranking to remove (i.e., setting the weight val-ues to zero) malicious channels contributing to the success-ful transfer of backdoor attacks from the pre-trained source model. However, the static compression before target train-ing may lead to information loss, leading to poor perfor-mance [29] (i.e., lower accuracy) in the target domain due to domain shift. Since we do not have labels in the target domain, we cannot afford to lose any information from the pre-trained source model. In addition, we need a dynamic defense component to take advantage of the target training and target data to the defender’s benefit, which the exist-ing backdoor defenses fail to resolve. Hence, the second component of the SSDA performs knowledge transfer from an auxiliary (uncompressed) model to recover benign ac-curacy. For more secure knowledge transfer, we propose a novel spectral norm-based loss penalty to suppress ma-licious channels sensitive to backdoors. However, com-puting the spectral norm penalty during training is expen-sive. Hence, we derive a computationally efficient safe up-per bound of the norm and then use this novel upper bound to compute the approximate spectral norm efficiently and effectively during training. We extensively evaluated our defense across multiple datasets, tasks and attack combina-It shows that the proposed SSDA can successfully tions. defend (drops to ∼1% from ∼ 99% attack success rate) against strong backdoor attacks (e.g., WaNet [28]) with lit-tle to no accuracy degradation compared to existing vul-nerable SFDA methods (as demonstrated in Fig. 1). Fi-nally, we show that regardless of the source model being benign/malicious, our proposed SSDA can successfully per-form the adaptation of target domain. 2.