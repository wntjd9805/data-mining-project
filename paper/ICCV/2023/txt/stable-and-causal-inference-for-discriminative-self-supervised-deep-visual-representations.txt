Abstract
In recent years, discriminative self-supervised methods have made significant strides in advancing various visual tasks. The central idea of learning a data encoder that is robust to data distortions/augmentations is straightforward yet highly effective. Although many studies have demon-strated the empirical success of various learning methods, the resulting learned representations can exhibit instability and hinder downstream performance. In this study, we an-alyze discriminative self-supervised methods from a causal perspective to explain these unstable behaviors and propose solutions to overcome them. Our approach draws inspi-ration from prior works that empirically demonstrate the ability of discriminative self-supervised methods to demix ground truth causal sources to some extent. Unlike pre-vious work on causality-empowered representation learn-ing, we do not apply our solutions during the training pro-cess but rather during the inference process to improve time efficiency. Through experiments on both controlled image datasets and realistic image datasets, we show that our pro-posed solutions, which involve tempering a linear transfor-mation with controlled synthetic data, are effective in ad-dressing these issues. 1.

Introduction
Learning generalized representation with unlabeled data is a challenging task in various fields, but Self-Supervised
Learning (SSL) has recently demonstrated remarkable suc-cess in learning semantic invariant representations without labels [40, 41, 53]. There are two main types of self-supervised learning (SSL) based on the pretext task used: generative and discriminative SSL, with generative SSL re-constructing altered or distorted data to its original input
[9, 28, 31, 59, 65, 71]and early discriminative SSL predict-ing easily designed labels and task-specific representations that are not very generalizable [25, 57, 75]. More recent
Training
) x ( f z
˜z
Align (z, ˜z)
) x ( f
Score 0.99 0.52 m a e r t s n w o
D k s a
T
Inferencing
Figure 1: During the training of discriminative SSL, align-ing positive representations will be robust to the changes applied as augmentations (red arrows). However, during the inference stage, one small change in the data variable (such as view angle) will result in an unexpected degradation on downstream performance. discriminative SSL trains the model to identify similari-ties and differences between pairs of augmented examples
[7, 10, 11, 26, 29, 74]. The success of SSL in deep im-age models has resulted in progress in other data modalities
[53, 52, 54, 61, 62] and attention-based models like trans-formers [12, 8, 49, 72]. Recent discriminative SSL aims to learn content and semantic invariant representations that are robust to data augmentations, but the learned represen-tations can be unstable when one subtle factor of the data is changed to a value that is not accessible through all aug-mentations. To avoid the high cost of incorporating all pos-sible subtle changes during training, insights are needed to uncover the root cause of instability and find a solution to prevent performance deterioration during inference. Figure 1 summarizes this deterioration effect.
Causality [60] is a vital tool to investigate the causal
relationships between variables in observational data, and can uncover the underlying causal factors that explain un-expected model behavior due to changes in the environ-Independent Component Analysis (ICA) is of-ment. ten used to disentangle sources in unsupervised training
[36, 37, 39, 43, 45], and causal analysis has been applied in follow-up works [76, 67] to examine the empirical suc-cess of SSL under an ICA framework. However, while these works identify factors that contribute to SSL’s success, they do not address the problem’s unstable mode, which can cause a severe performance drop when underlying fac-tors shift slightly to an unseen environment. Some works
[56, 27, 48] attempt to incorporate causality during the training process to identify and alleviate the impact of such shifts, but this approach is time-costly and only marginally improves performance compared to non-causal SSL meth-ods. A more time-efficient and accessible solution would be to simply reverse the unstable shift during inference.
We aim to address the issue of unstable behavior during the inference stage by building upon previous theories of successful InfoNCE-facilitated contrastive SSL and extend-ing it to all recent SSL methods with additional assump-tions and constraints. Drawing inspiration from the rela-tionship between the ground truth positive pairs distribu-tion and learned positive pairs distribution, we demonstrate that the approximated transformation between the ground truth representations and learned representations is orthog-onal to the augmentations applied during training. However, a change in the data factor/variable that violates the condi-tions for successful SSL can cause a corresponding shift in the inferred representation, resulting in a decline in down-stream performance. This change of data factor/variable can be a change in the background, texture, or view angles etc.
To overcome this issue, we propose learning targeted trans-formations that regularize the violating shift and restore per-formance on the unseen data shift. This approach effec-tively avoids the undesirable behavior and improves perfor-mance on previously unseen data shifts.
To summarize, our contributions are following:
• Through the use of a comparable data generation pro-cess in prior research, we show that All current SSL techniques benefit from the alignment of positive pairs.
• Through our alternative derivation of the transforma-tion matrix between the ground truth representation and the learned representation, we have shown that the augmentations applied during training are orthogonal to the resulting matrix.
• By interpreting a change in the data variable causally, we propose two solutions focusing on inference to modify the negative shift in representation space caused by such a change.
• We validate the proposed solutions by conducting ex-periments on both controlled and realistic datasets, providing evidence for their efficacy during the infer-ence stage without retraining the pretrained models. 2.