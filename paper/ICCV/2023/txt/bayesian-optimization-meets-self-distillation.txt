Abstract
Bayesian optimization (BO) has contributed greatly to improving model performance by suggesting promising hy-perparameter configurations iteratively based on observa-tions from multiple training trials. However, only partial knowledge (i.e., the measured performances of trained mod-els and their hyperparameter configurations) from previous trials is transferred. On the other hand, Self-Distillation (SD) only transfers partial knowledge learned by the task model itself. To fully leverage the various knowledge gained from all training trials, we propose the BOSS framework, which combines BO and SD. BOSS suggests promising hy-perparameter configurations through BO and carefully se-lects pre-trained models from previous trials for SD, which are otherwise abandoned in the conventional BO process.
BOSS achieves significantly better performance than both
BO and SD in a wide range of tasks including general image classification, learning with noisy labels, semi-supervised learning, and medical image analysis tasks. Our code is available at https://github.com/sooperset/boss. 1.

Introduction
Convolutional Neural Networks (CNNs) have achieved remarkable success in a wide range of computer vision ap-plications [9, 31, 34]. However, their performance is greatly dependent on the choice of hyperparameters [11]. As the optimal hyperparameter configuration is not known a pri-ori, practitioners often explore the hyperparameter space manually to obtain a better configuration. Despite its time-consuming process, it typically results in sub-optimal per-formance [7]. Recently, Bayesian optimization (BO) has emerged as a successful approach to hyperparameter opti-mization, automating the manual tuning effort and pushing the boundaries of performance [6, 24, 45]. BO allows for the effective exploration of multi-dimensional search spaces by suggesting promising configurations based on observa-tions. This technique has achieved state-of-the-art perfor-*Authors contributed equally. mance for training CNNs [8, 15] and has contributed to im-proving various applications such as AlphaGo [10].
BO is inherently an iterative process in which a prob-abilistic prior model is fitted using observations of hyper-parameter configurations and their corresponding perfor-mances [6, 24]. At each iteration, BO suggests the next configuration to evaluate that is most likely to improve per-formance. After training the network with the suggested configuration, a new observation is retrieved and used to up-date the probabilistic model. However, only partial knowl-edge (i.e., the measured performances of trained models and their hyperparameter configurations) from previous trials is transferred, and the knowledge learned by the network is discarded.
Self-distillation (SD) can also be viewed as a knowl-edge transfer method. A recent line of research in SD has demonstrated that transferring knowledge from a previously trained model with the identical capacity can improve the performance of the model [2, 16, 36]. If a student network is trained to mimic the feature distribution of a teacher net-work, then the student could beat the teacher. Allen-Zhu and Li [2] have both theoretically and empirically inter-preted this as a similar effect to ensembling various models.
Inspired by this property of SD and the iterative nature of
BO, we ended up asking if the knowledge inside the net-work from the previous trials could be used for the next trials during the BO process.
In this paper, we propose a new framework, Bayesian
Optimization meets Self-diStillation (BOSS), which com-bines BO and SD to fully leverage the knowledge obtained from previous trials. The overall process of BOSS is illus-trated in Figure 1. Following the BO process, BOSS sug-gests a hyperparameter configuration based on observations that are most likely to improve the performance. After that, it carefully selects pre-trained networks from previous trials for the next round of training with SD, which are otherwise abandoned in the conventional BO process. This process is performed in an iterative manner, allowing the network to persistently improve upon previous trials. To the best of our knowledge, this is the first work that leverages the knowl-edge of the network learned during the BO process. This
Figure 1. BOSS is a novel framework for training models, which fuses the Bayesian Optimization (BO) and Self-Distillation (SD), com-bining the concept of hyperparameter exploration and knowledge distillation. By performing these steps simply in an alternating manner (left-to-right), we propagate both the conditional probability learned over hyperparameter configurations (depicted with graphs) and the knowledge learned by each task network, resulting in large performance gains in the final model. is not a simple combination of two orthogonal methods but we provide solutions to the problem of how to transfer past knowledge (i.e., model parameters, hyperparameters, and performances) appropriately. The suggested solution is that (1) not only the teacher but the student should be initialized from the prior knowledge, and (2) they should be initial-ized from different previous trials to fully exploit the prior knowledge. Our thorough ablation analysis supports this.
We evaluate the effectiveness of BOSS with various computer vision tasks, such as object classification [21], learning with noisy labels [20], and semi-supervised learn-ing [3]. In addition, we also evaluate it with medical image analysis tasks, including medical image classification and segmentation. Our experimental results demonstrate that
BOSS significantly improves target model performance, outperforming both BO and SD. Furthermore, we conduct comprehensive analysis and ablation studies to further in-vestigate the behavior of BOSS.
In summary, the main contributions of this paper are: 1. We present BOSS framework that fully harnesses the knowledge from various models by leveraging the ben-efits of both BO and SD. 2. Exhaustive evaluation experiments demonstrate the ef-ficacy of our framework, as it results in significant per-formance improvements across diverse scenarios. 3. In-depth analysis and ablation studies provide essen-tial insights into how to transfer prior knowledge ef-fectively for CNNs. 2.