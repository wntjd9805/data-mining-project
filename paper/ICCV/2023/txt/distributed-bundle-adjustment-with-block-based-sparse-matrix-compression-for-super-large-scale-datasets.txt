Abstract 1.

Introduction
We propose a distributed bundle adjustment (DBA) method using the exact Levenberg-Marquardt (LM) algo-rithm for super large-scale datasets. Most of the existing methods partition the global map to small ones and conduct bundle adjustment in the submaps. In order to fit the par-allel framework, they use approximate solutions instead of the LM algorithm. However, those methods often give sub-optimal results. Different from them, we utilize the exact LM algorithm to conduct global bundle adjustment where the formation of the reduced camera system (RCS) is actually parallelized and executed in a distributed way. To store the large RCS, we compress it with a block-based sparse matrix compression format (BSMC), which fully exploits its block feature. The BSMC format also enables the distributed stor-age and updating of the global RCS. The proposed method is extensively evaluated and compared with the state-of-the-art pipelines using both synthetic and real datasets. Prelim-inary results demonstrate the efficient memory usage and vast scalability of the proposed method compared with the baselines. For the first time, we conducted parallel bundle adjustment using LM algorithm on a real datasets with 1.18 million images and a synthetic dataset with 10 million im-ages (about 500 times that of the state-of-the-art LM-based
BA) on a distributed computing system.
Bundle adjustment (BA) is a significant step for 3D re-construction in both computer vision and photogrammetry communities. Its main objective is to recover the camera poses as well as the 3D points by minimizing the square sum of the reprojection errors. This nonlinear optimization prob-lem is an ancient subject, having been studied for decades.
Classic solutions such as Steepest descent, Gauss-Newton, and Levenberg-Marquardt (LM) [16, 19] have been pro-posed and widely applied, and plenty of libraries have been open sourced, such as Ceres [26], g2o [11], SBA [17], PBA
[27], MegBA [22], and so on. Among them, the LM al-gorithm is most widely supported. In a bundle adjustment, the nonlinear system is firstly linearized at an initial guess, which is a given prior, and then solved via direct inversion,
Cholesky decomposition, or preconditioned conjugate gra-dient (PCG) [5, 6].
The Hessian matrix in a linear system is usually reduced by the Shur complement trick, forming a Reduced Camera
System (RCS) that is much smaller than the Hessian. As the number of images increases, even the memory requirement for an RCS is a challenging issue. The formation, storage and inversion of the RCS gradually become the bottleneck of the bundle adjustment for large-scale datasets. Some scholars argue that BA is indivisible and hard to fit for paral-lel computing [30]. To solve this problem, they have turned to finding approximate methods that are more suitable for
Figure 1. The sparse point cloud of a real dataset with 1.18 million images processed using the proposed method. The color from blue to red represents the height from low to high. The white rectangles on the left image are enlarged and shown on the right side parallel computing [25, 8, 20, 28, 31, 7, 30, 13]. On the con-trary, other works still utilize the LM algorithm, and attempt to design special parallel frameworks for it [27, 18, 22]. The former studies often give sub-optimal results, while the lat-ter methods, despite running fast, are still limited to median-scale datasets (about 10k images). 2.