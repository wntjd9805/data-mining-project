Abstract
This paper investigates data-free class-incremental learn-ing (DFCIL) for hand gesture recognition from 3D skele-ton sequences.
In this class-incremental learning (CIL) setting, while incrementally registering the new classes, we do not have access to the training samples (i.e. data-free) of the already known classes due to privacy. Ex-isting DFCIL methods primarily focus on various forms of knowledge distillation for model inversion to mitigate catastrophic forgetting. Unlike SOTA methods, we delve deeper into the choice of the best samples for inversion.
Inspired by the well-grounded theory of max-margin clas-siﬁcation, we ﬁnd that the best samples tend to lie close to the approximate decision boundary within a reasonable margin. To this end, we propose BOAT-MI – a simple and effective boundary-aware prototypical sampling mechanism for model inversion for DFCIL. Our sampling scheme out-performs SOTA methods signiﬁcantly on two 3D skeleton gesture datasets, the publicly available SHREC 2017, and
EgoGesture3D – which we extract from a publicly avail-able RGBD dataset. Both our codebase and the EgoGes-ture3D skeleton dataset are publicly available: https:
//github.com/humansensinglab/dfcil-hgr. 1.

Introduction
Humans innately rely on their hands to communicate, feel and interact with their environment. Recent studies have shown that seeing one’s hands tracked in real-time in a vir-tual environment without controllers is the most compelling method of user engagement in virtual reality (VR)
[49].
The latest VR headsets such as Meta Quest [15, 16], and
VUZIX [3] leverage high-precision hand-tracking features to render compelling user immersion abilities. The hand-tracking and individual ﬁnger-tracking technologies in these platforms are mature enough for natural interactions in the virtual world, and provide a highly immersive “user presence” in virtual environments [29].
While existing VR systems are equipped with promising hand gesture recognition performance, they are designed us-ing a pre-deﬁned set of hand gestures that an end user ought to follow for interaction with the virtual world. However, considering the variations in user preferences across cultures and demographics [1, 2], individual users may also wish to register custom gesture categories and personalize their VR experience. Such a provision has several advantages. As hands play a key role in the recognition and expression of 1
human emotions [41], registering custom gestures allows users to express themselves better, thus enhancing immer-sion and user presence in the virtual environment. Studies in psychology show that multi-lingual people tend to use native language-speciﬁc gestures [37], and in a VR system where hand gestures are the primary medium of natural in-teraction, it makes users more comfortable with the system and promotes frequent use. Moreover, such a provision can signiﬁcantly enhance accessibility and inclusivity for peo-ple with disabilities or special needs. Going beyond simple customization to individual preferences, such a continual ges-ture learning system can also provide technology developers with capabilities to add suites of new gestures for different
VR application domains, such as extending a generic gaming
VR system for education, rehabilitation or manufacturing.
A gesture recognition model is typically trained on a large-scale dataset of pre-deﬁned (‘base class’) gestures before being deployed on the user’s VR edge device. When a user or developer registers new gesture categories, a naive update of the model based on the user’s new data can result in catastrophic forgetting [32] of the pre-deﬁned gesture categories. The pre-trained model deployed on each user device is often trained on proprietary organizational data or private data that cannot be accessed during re-training in subsequent time steps when a user registers new gestures.
Additionally, the user-provided novel gesture training data at each continual step is also private to the user and has to be discarded after adapting the model. Hence, effectively registering new gesture classes to an existing model without access to data pertaining to previous tasks is a signiﬁcant problem in AR/VR domains. We address this important problem in this work, and aim to build a life-long extensible model, to which a user can register new gesture classes sequentially throughout its lifetime of deployment. Figure 1 illustrates our problem setting.
A common strategy in continual learning methods, which focuses on addressing catastrophic forgetting, is to store a small set of exemplars of previously seen classes and replay them to the model along with novel class data to mitigate forgetting of previous knowledge [40]. Such replay-based methods have been shown to provide state-of-the-art performance across various continual learning approaches in recent surveys [30, 28]. However, adding custom user-speciﬁc gestures in a continual manner in AR/VR systems precludes the possibility of storing or using previous class samples due to privacy concerns, hence motivating a data-free class-incremental learning framework. Considering that we cannot access previous task data but have access to the inference model after training each task, model inversion to obtain data impressions of previous tasks becomes a natural choice of solution (as shown in Figure 1). This is referred to as Data-Free Class-Incremental Learning (DFCIL) in recent works [55, 45, 13]. The limited efforts in this prob-lem setting so far [55, 45, 13] largely rely on a knowledge distillation strategy to get the inverted samples and further to regularize the model while ﬁne-tuning to mitigate catas-trophic forgetting. We instead set out to answer the following question: How to choose the best samples for model in-version to minimize catastrophic forgetting in data-free class-incremental learning? Besides, while existing efforts are focused on image data, to the best of our knowledge, ours is the ﬁrst such effort on 3D skeleton-based dynamic hand gesture data in an AR/VR context.
To answer this question, we take inspiration from statisti-cal learning theory, speciﬁcally max-margin classiﬁcation
[17, 47]. We follow the notion that samples near the bound-ary in the feature space (such as support vectors in a Sup-port Vector Machine) are relevant candidates to preserve the decision boundaries among the classes and improve gener-alization. Therefore, in principle, while performing model inversion, choosing the support vectors close to the decision boundaries should keep the decision boundary of the known classes intact. We hence devise an algorithm ﬁrst to sam-ple such points in the model’s latent representation space, perform model inversion on such samples and ﬁnetune the model on a new task with these inverted samples. In order to choose a diverse set of support vectors, we also consider class prototypes and guide the choice of such samples using these prototypes. We hence call the proposed approach BOundary
Aware proTotypical Model Inversion (BOAT-MI). We ex-tensively evaluate the proposed BOAT-MI mechanism for
DFCIL on the task of dynamic hand gesture recognition from 3D skeleton data. It is evident from the experimental results that BOAT-MI indeed helps to preserve the decision boundary signiﬁcantly better than state-of-the-art (SOTA) methods. To summarize, our contributions are as follows:
• We propose a boundary-aware prototypical model inver-sion (BOAT-MI) strategy for data-free class-incremental learning, which focuses on preserving user privacy in 3D skeleton-based hand gesture recognition systems. In par-ticular, we systematically investigate the choice of sample selection for model inversion, and take inspiration from the theory of max-margin classiﬁcation in choosing sam-ples near the boundary in the model inversion process. To the best of our knowledge, this is the ﬁrst such effort on 3D skeleton data.
• To this end, we also contribute a large-scale 3D skeleton gesture recognition dataset, whereby EgoGesture3D is re-annotated for 3D skeleton keypoints from the original
RGB-D EgoGesture dataset [57].
• We comprehensively evaluate the proposed BOAT-MI method for our target task of class-incremental dynamic hand gesture recognition on 3D skeleton data. The ex-perimental results demonstrate signiﬁcant improvements over SOTA methods for the proposed continual learning setup, particularly tailored to real-world settings after de-ployment on users’ devices. We hope this will serve as a new benchmark for continual learning research in hand gesture recognition. 2.