Abstract
Federated learning (FL) over mobile edge devices is a promising distributed learning paradigm for various mobile applications. However, practical deployment of FL over mobile devices is very challenging because (i) conventional
FL incurs huge training latency for mobile edge devices due to interleaved local computing and communications of model updates, (ii) there are heterogeneous training data across mobile edge devices, and (iii) mobile edge devices have hardware heterogeneity in terms of computing and communication capabilities.
To address aforementioned challenges, in this paper, we propose a novel “workie-talkie” FL scheme, which can accelerate FL’s training by overlapping local computing and wireless communications via contrastive regularization (FedCR). FedCR can reduce FL’s training latency and almost eliminate straggler issues since it buries/embeds the time consumption of communications into that of local training.
To resolve the issue of model staleness and data heterogene-ity co-existing, we introduce class-wise contrastive regular-ization to correct the local training in FedCR. Besides, we jointly exploit contrastive regularization and subnetworks to further extend our FedCR approach to accommodate edge de-vices with hardware heterogeneity. We deploy FedCR in our
FL testbed and conduct extensive experiments. The results show that FedCR outperforms its status quo FL approaches on various datasets and models. 1.

Introduction
Thanks to the hardware advance, edge devices are be-coming capable of training deep neural networks (DNNs) on-device [8, 1]. Meanwhile, federated learning (FL) has been emerging as a powerful distributed learning framework which enables collaborative training without sharing the data.
FL over edge devices is promising to provide various appli-cations, including e-Healthcare [2], map construction for autonomous driving [36], smart farming in agricultural IoT, etc. One of the most important challenges hindering FL over edge devices from flying is the huge latency of FL train-ing, which is mainly caused by wireless communications between FL server and edge devices, data heterogeneity, and hardware heterogeneity of edge devices.
Popular FL frameworks consider to interleave the com-puting of local model training and communications of local model updates for FL training [22, 16]. If such FL training is carried out across GPU clusters with Ethernet connection (50-100 Gbps [32]), the network latency (≤1 ms) is negligi-ble. Nevertheless, FL over edge devices are connected wire-lessly, so the transmission bandwidth is limited and network latency is too big to ignore. To cut the network latency in wireless communications of model updates, recent research efforts in [38, 37] overlapped local training phase with the wireless transmission phase of model update. The update correction scheme [38] was also developed to handle the is-sue of model staleness [4], which means using an old version of global model to perform local training during FL training.
As demonstrated in our empirical studies (Section 3.1), the existing update correction schemes face significant accuracy drops and fail to improve the system efficiency when the training data of edge devices are heterogeneous.
For most FL cases, local data distributions of edge de-vices are different from the overall global distribution. When
FL clients conduct local training on their own data, the local model updates point to the direction towards the local optima, which may not be consistent with the objective of the aggre-gated global model. It is called model drift issue [13, 17, 18], and result in unstable FL training, which can severely slow down the FL convergence. To address data heterogeneity issue, existing approaches chose to share data across a subset of devices [6], or to correct the local updates via variance reduction [13]. However, those FL methods above cannot deal with the model staleness issue. Besides, they do not consider the hardware heterogeneity of edge devices.
FL clients vary a lot in terms of computing (i.e., CPU,
GPU, memory, etc.), and wireless communication (i.e., chan-nel conditions, wireless accessing technologies, etc.) capa-bilities. Such hardware heterogeneity of edge devices, or device heterogeneity for short, results in huge performance differences among participating edge devices, which occurs straggler problem in FL system, and thus causes huge FL training latency. To handle the straggler issue, some works in the literature [26] developed deadline based schemes that exclude the slow devices after a pre-set timestamp or allow partial updates from the stragglers, which may lead to biased gradient updates that affect FL accuracy.
Aiming to address aforementioned three associated chal-lenges at one strike and reduce the latency of FL over edge devices, in this paper, we propose a novel “workie-talkie”
FL scheme, which can accelerate FL’s training by overlap-ping local computing and communications via constrastive regularization (FedCR). To address the high communica-tion latency and data heterogeneity issues, FedCR overlaps computing and communications, and novelly integrates con-strastive learning (CL) into FL local training to reduce the potential training accuracy loss. Since CL helps local models learn close to the global model, it can overcome the model drift issue caused by data heterogeneity. To deal with the model staleness caused by the overlapping scheme, we de-velop a novel class-wise contrastive loss in the model level to smooth the stale global model. In this way, FedCR can fully exploit the benefits of contrastive regularization to achieve a fast and stable FL training over edge devices. Moreover, to reduce corresponding computing overheads in CL and tackle device heterogeneity issue, we extend our FedCR to enable heterogeneous local model training over edge devices. That helps to reduce local computing time and the size of model updates for communications throughout FL training process.
Our salient contributions are summarized as follows.
• To address high communication latency and data hetero-geneity issues, we propose FedCR, a novel FL approach to accelerate FL’s training over edge devices by overlap-ping computing and communications and integrating
CL into local training to regularize local updates.
• To resolve model staleness issue, we propose a class-wise contrastive regularization method. By assigning class-wise temperature for each edge device, we aim to properly align the local models with the global model.
• To address device heterogeneity issue, FedCR allows edge devices to select and train a subnetwork of the original DNN based on their available resources. We develop a temperature scaling strategy to accommodate heterogeneous local model training in FedCR.
• We set up testbeds and conduct extensive experiments to verify the effectiveness of the proposed FedCR ap-proach under various learning models, different data distributions across heterogeneous edge devices, and multiple wireless transmission settings. 2.