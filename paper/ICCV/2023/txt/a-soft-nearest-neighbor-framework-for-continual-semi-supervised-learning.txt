Abstract
Despite significant advances, the performance of state-of-the-art continual learning approaches hinges on the un-realistic scenario of fully labeled data. In this paper, we tackle this challenge and propose an approach for contin-ual semi-supervised learning—a setting where not all the data samples are labeled. A primary issue in this scenario is the model forgetting representations of unlabeled data and overfitting the labeled samples. We leverage the power of nearest-neighbor classifiers to nonlinearly partition the feature space and flexibly model the underlying data dis-tribution thanks to its non-parametric nature. This enables the model to learn a strong representation for the current task, and distill relevant information from previous tasks.
We perform a thorough experimental evaluation and show that our method outperforms all the existing approaches by large margins, setting a solid state of the art on the con-tinual semi-supervised learning paradigm. For example, on
CIFAR-100 we surpass several others even when using at least 30 times less supervision (0.8% vs. 25% of annota-tions). Finally, our method works well on both low and high resolution images and scales seamlessly to more com-plex datasets such as ImageNet-100. Our source code is publicly available at https://github.com/kangzhiq/NNCSL. 1.

Introduction
Several efforts have been devoted to the continual learn-ing (CL) [18] paradigm wherein training data samples ar-rive sequentially. However, most of the state-of-the-art CL methods [11, 12, 20] are based on a strong assumption: the data is fully labeled. This is an unrealistic requirement as labeling data is oftentimes expensive for the expertise re-quired or the amount of annotations, hazardous due to pri-vacy or safety concerns, or impractical in a real-time online scenario. A natural way of tackling this issue is by leverag-ing the semi-supervised learning framework, where not all the data samples are labeled.
∗Zhiqi Kang and Enrico Fini contributed equally to this work
†Univ. Grenoble Alpes, CNRS, Grenoble INP, LJK, 38000 Grenoble, France.
Figure 1: Left: The average accuracy with different percent-ages of labeled data on CIFAR-100. Our method (NNCSL) with 0.8% of the labels outperforms or matches the perfor-mance of all other methods at 25%. Right: Comparison of different versions of our approach and PAWS [3]. CSL is equivalent to NNCSL without our NND loss.
In recent years, this stimulated the community to in-vestigate a new line of research named continual semi-supervised learning [8, 44, 51]. It refers to the setting where each task in the sequence is semi-supervised. This learning scenario brings novel challenges, as the models catastrophi-cally forget the representations of unlabeled data while also overfitting the labeled set. This is further exacerbated by an-other well-studied phenomenon in CL: overfitting the expe-rience replay buffer [10]. These challenges result in vanilla
CL methods underperforming, as they lack the ability to ex-tract information from the unlabeled set, thus largely over-fitting to the labeled set [52]. On the other hand, semi-supervised learning approaches [15, 29, 46, 57] balance well the labeled and unlabeled sets but cannot handle the contin-ual scenario, and suffer from forgetting even when paired with well-known CL methods (see Fig. 1 (right) and Tab. 1).
A few recent approaches [8, 51] partially mitigate these issues on small-scale datasets. The method in [51] relies on generative models to replay previous classes, which leads to a sizeable computational and memory overhead, making it difficult to scale to larger datasets such as ImageNet. Bos-chini et al. [8] proposed to contrast among samples of differ-ent classes and tasks. However, [8] falls short with smaller memory buffers, larger datasets, more tasks, or higher reso-lution images (see Fig. 1 (left) and results in Sec. 6). There-fore, we argue that there is a clear need for more powerful continual semi-supervised learning methods, with a more suitable use of the labeled set, and efficient and stable rep-resentation learning from the unlabeled set.
In this paper, we unleash the power of nearest-neighbors in the context of continual semi-supervised learning.
In particular, we propose a new method, NNCSL (Nearest-Neighbor for Continual Semi-supervised Learning), that leverages the ability of the nearest-neighbor classifier to non-linearly partition the feature space in two ways: i) to learn powerful and stable representations of the current task using a self-supervised multi-view strategy, and ii) to dis-till previous knowledge and transfer the local structure of the feature space. The latter is achieved through our pro-posed NND (Nearest-Neighbor Distillation), a novel semi-supervised distillation loss that mitigates forgetting in con-tinual semi-supervised learning better than other competi-tive distillation approaches. In contrast with knowledge dis-tillation [11, 22, 30, 38] and feature distillation [19, 21, 24], which focus exclusively on class-level and sample-level dis-tributions respectively, NND simultaneously distills rela-tionships between classes and samples by leveraging the nearest-neighbor classifier. Overall, NNCSL outperforms all related methods by very large margins on both small and large scale datasets and both low and high resolution im-ages. For instance, as shown in Fig. 1, NNCSL matches or surpasses all others with more than 30 times less supervi-sion (0.8% vs. 25% of annotations) on CIFAR100.
The main contributions of this work are as follows:
• We propose NNCSL, a novel nearest-neighbor-based continual semi-supervised learning method that is, by design, impacted less by the overfitting phenomenon related to a small labeled buffer.
• We propose NND, a new distillation strategy that trans-fers both representation-level and class-level knowl-edge from the previously trained model using the out-puts of a soft nearest-neighbor classifier, which effec-tively helps alleviate forgetting.
• We show that NNCSL outperforms the existing meth-ods on several benchmarks by large margins, setting a new state of the art on continual semi-supervised learn-ing. In contrast to previous approaches, our method works well on both low and high resolution images and scales seamlessly to more complex datasets. 2.