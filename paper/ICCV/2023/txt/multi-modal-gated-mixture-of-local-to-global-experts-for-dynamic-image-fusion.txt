Abstract
Missing detection
Missing detection
Error detection
Infrared and visible image fusion aims to integrate com-prehensive information from multiple sources to achieve su-perior performances on various practical tasks, such as de-tection, over that of a single modality. However, most ex-isting methods directly combined the texture details and ob-ject contrast of different modalities, ignoring the dynamic changes in reality, which diminishes the visible texture in good lighting conditions and the infrared contrast in low lighting conditions. To ﬁll this gap, we propose a dynamic image fusion framework with a multi-modal gated mixture of local-to-global experts, termed MoE-Fusion, to dynami-cally extract effective and comprehensive information from the respective modalities. Our model consists of a Mixture of Local Experts (MoLE) and a Mixture of Global Experts (MoGE) guided by a multi-modal gate. The MoLE per-forms specialized learning of multi-modal local features, prompting the fused images to retain the local informa-tion in a sample-adaptive manner, while the MoGE focuses on the global information that complements the fused im-age with overall texture detail and contrast. Extensive ex-periments show that our MoE-Fusion outperforms state-of-the-art methods in preserving multi-modal image texture and contrast through the local-to-global dynamic learning paradigm, and also achieves superior performance on de-tection tasks. Our code is available: https://github. com/SunYM2020/MoE-Fusion. 1.

Introduction
Infrared and visible image fusion focus on generating appealing and informative fused images that enable supe-rior performance in practical downstream tasks over that of using single modality alone [24, 45, 41, 21]. In recent years, infrared-visible image fusion has been widely used
*Equal contribution
†Corresponding author
Visible image
Infrared image
DF (IEEE TIP’19)
Missing detection
Missing detection
DIDF (IJCAI’20)
TDL (CVPR’22)
MoE-Fusion (Ours)
Figure 1. The importance of dynamic image fusion. In the SOTA methods (DF [16], DIDF [49], and TDL [18]), the texture details of objects (e.g., car, truck, and trafﬁc sign) in the fused image are suppressed by the contrast of infrared image, leading to terrible detection results. Beneﬁting from the dynamic fusion, our method can preserve clear texture details without being interrupted by un-suitable contrast, achieving the best performance. in many applications, such as autonomous vehicles [9] and unmanned aerial vehicles [34]. According to the thermal in-frared imaging mechanism, infrared images can be adapted to various lighting conditions but has the disadvantage of few texture details [25, 50, 19]. By contrast, visible images contain rich texture detail information, but cannot provide clear information in low light conditions. Therefore, how to design advanced fusion methods such that the fused im-ages preserve sufﬁcient texture details and valuable thermal information has attracted a lot of research attention.
Existing infrared-visible fusion methods [46, 38, 33, 42] can be mainly categorized to traditional approaches (im-age decomposition [15], sparse representation [47], etc.) and deep learning-based approaches (autoencoder based methods [16, 49, 17], generative adversarial network based
approaches [18, 26], transformer based approaches [40, 36], etc.). However, most of these methods directly com-bined the texture details and object contrast of different modalities, ignoring the dynamic changes in reality, lead-ing to poor fusion results and even weaker downstream task performance than that of a single modality. As shown in
Fig. 1, the infrared image should adaptively enhance cars in dim light while avoiding compromising the textural de-tail of truck in bright light. However, the object textures in these state-of-the-art fusion methods are signiﬁcantly dis-turbed by the infrared thermal information due to the lack of dynamic learning of multi-modal local and global infor-mation, resulting in terrible object detection performance.
In complex scenes, different modalities have different characteristics: under good lighting conditions, the texture of an object should not be disturbed by thermal infrared in-formation; under low lighting conditions, the contrast of an object also should not be suppressed by the darkness of the visible image. Most existing methods perform im-age fusion in a ﬁxed correlation paradigm, ignoring the dominant modality changes dynamically in reality, and of-ten fall into domain bias. To break the traditional ﬁxed fusion paradigm, we pioneered sample-adaptive local-to-global experts to dynamically enhance the dominant modal-ity for image fusion. Fig. 1 show that the proposed method not only eliminates domain bias but also achieves sample-adaptive dynamic fusion, yielding the best detection results.
Speciﬁcally, we propose a dynamic image fusion frame-work with a multi-modal gated mixture of local-to-global experts, termed MoE-Fusion, which consists of a Mixture of Local Experts (MoLE) and a Mixture of Global Experts (MoGE) guided by a multi-modal gate. In MoLE, we intro-duce the attention map generated by an auxiliary network to construct multi-modal local priors and perform dynamic learning of multi-modal local features guided by multi-modal gating, achieving sample-adaptive multi-modal local feature fusion. Moreover, MoGE performs dynamic learn-ing of multi-modal global features to achieve a balance of texture details and contrasts globally in the fused images.
With the proposed dynamic fusion paradigm from local to global, our model is capable of performing a reliable fusion of different modal images.
We summarize our main contributions as follows:
• We propose a dynamic image fusion model, provid-ing a new multi-modal gated mixture of local-to-global experts for reliable infrared and visible image fusion (beneﬁting from the dynamically integrating effective information from the respective modalities).
• The proposed model is an effective and robust frame-work for sample-adaptive infrared-visible fusion from local to global. Further, it prompts the fused images to dynamically balance the texture details and contrasts.
• We conduct extensive experiments on multiple infrared-visible datasets, which clearly validate our su-periority, quantitatively and qualitatively. Moreover, we also demonstrate our effectiveness in object detec-tion. 2.