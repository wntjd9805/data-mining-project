Abstract
Original
CycleGAN
CyEDA
RCFNGAN
HEDNGAN
Image retrieval methods based on CNN descriptors rely on metric learning from a large number of diverse exam-ples of positive and negative image pairs. Domains, such as night-time images, with limited availability and variability of training data suffer from poor retrieval performance even with methods performing well on standard benchmarks. We propose to train a GAN-based synthetic-image generator, translating available day-time image examples into night images. Such a generator is used in metric learning as a form of augmentation, supplying training data to the scarce domain. Various types of generators are evaluated and an-alyzed. We contribute with a novel light-weight GAN archi-tecture that enforces the consistency between the original and translated image through edge consistency. The pro-posed architecture also allows a simultaneous training of an edge detector that operates on both night and day images.
To further increase the variability in the training examples and to maximize the generalization of the trained model, we propose a novel method of diverse anchor mining.
The proposed method improves over the state-of-the-art results on a standard Tokyo 24/7 day-night retrieval bench-mark while preserving the performance on Oxford and Paris datasets. This is achieved without the need of training im-age pairs of matching day and night images. The source code is available at https://github.com/mohwald/gandtr. 1.

Introduction
Large-scale instance-level image retrieval is commonly used e.g. as a first step in visual place recognition and vi-sual localization. As other computer vision problems, im-age retrieval is dominated by methods based on deep learn-ing models. Fast and memory efficient approaches learn global image descriptors via metric learning. A large num-ber and variety of corresponding image pairs is required to train well-performing global image descriptors.
One of the recent challenges in retrieval and visual local-ization is insensitivity to severe illumination changes, such as day and night [33, 34]. Methods trained mostly on the
Figure 1. Examples of day-to-night translations with various gen-erators. Each row consists of (left to right) the source image, and images translated by: CycleGAN, CyEDA, and the pro-posed RCFNGAN and HEDNGAN. All models are trained on the
SfM dataset, except for CyEDA where a model pre-trained on
BDD100k is used.
Input
RCF
HED
HEDN
Figure 2. Comparison of edges extracted from real night images by
RCF [19], HED [45], and proposed HEDN, which is trained jointly with the edge-consistency based HEDNGAN generator. RCF and
HED were trained mainly on day images and do not detect some edges in the night. day domain perform well on that domain, while having rel-atively poor results when night images are observed. The goal is an approach that performs well on all domains; im-proving results when night images are involved, while at the same time, the performance on the day domain should be preserved. The requirement of unharmed performance on the original (source) domain is, however, often neglected in the domain adaptation methods (e.g. [16], as shown by our experiments).
To achieve illumination invariant image retrieval, align-ment of the day and night domains by metric learning was proposed by [13]. To achieve this, a large number of match-ing night to day image pairs is required. The acquisition of such pairs is a non trivial task, and suffers form significantly lower variability compared to day-to-day corresponding im-age pairs. Photo-sharing sites are a popular source of land-mark image training data. A number of training and testing datasets were crawled from such sites, for example revisited
Oxford and Paris [26], Google landmarks v1 [23], SfM [28] and SfM N/D [13]. For day images, these datasets exhibit sufficient visual variability to train image to descriptor map-pings that generalize well to unseen scenes.
In contrary, not only there are significantly less images taken during the night (e.g. in the Aachen Day-Night dataset [33], there are 30 times more day images than night images), but their vari-ability is also lower, as only parts of the scenes are visually interesting (e.g. lit) during the night time. Therefore, only a small fraction of the scene reconstructed from the daytime images is photographed during night. This has been shown by day and night 3D reconstructions in [27].
As our main contribution, we propose to replace night training examples by synthetic images derived from day im-ages by a generative adversarial network (GAN). In partic-ular, a standard SfM dataset [30] with high variability of homogeneous (mostly day to day) matching image pairs is used and one of the matching images is transformed by a
GAN into a night image, see Figure 1. This alleviates the necessity of obtaining night to day matching image pairs, and also significantly increases the variability of the train-ing pairs. Even though night images are required to train the
GAN, (i) a much lower number of night images is needed compared to performing the metric learning, (ii) these do not have to be paired with matching day counterpart. We compare various existing image translation methods that do not require pixel-aligned or visually related training data, in particular CycleGAN [49], DRIT [15], CUT [24], and
CyEDA [3].
Inspired by the relative success of edge-based ap-proaches to illumination invariance, as a second contribu-tion, we propose a novel consistency enforcement through the edge consistency. Specifically, differentiable edge de-tector HED [45] is used to extract edges from the original and the translated image and their dissimilarity is penal-ized. The proposed method has a number of advantages: (i) it is an order of magnitude faster to train than CycleGAN while providing similar retrieval results, (ii) it provides in-sight into the importance and sufficiency of edges in night vision, (iii) it allows for simultaneous training of an edge detector (HEDN) that detects edges well in both day and night images. In this setup, HED [45] is compared to the more recent edge detector RCF [19].
Training data from automated 3D reconstructions are popular, as they are very clean and available without any hu-man annotation. On the downside, the data distribution has strong modes that correspond to canonical views of popular landmarks. As a third contribution, we propose to further increase the variability in the training examples, by a novel method of diverse anchor mining. Instead of a random se-lection of training examples for each epoch, pseudo-random importance sampling is used, preventing over-using training data from the modes of the training distribution (avoiding using multiple similar examples in the training).
We explore the idea of using diverse synthetic data for metric learning, compare different generators, including a newly proposed one, and study the contribution of individ-ual aspects of the proposed method on global descriptors.
We evaluate the performance of our models on image re-trieval and visual localization datasets. The contribution is applicable to other methods as well, which we demonstrate by applying the proposed method to HOW [41], a model that uses local descriptors for retrieval. 2.