Abstract 1.

Introduction
We call on the Document AI (DocAI) community to re-evaluate current methodologies and embrace the challenge of creating more practically-oriented benchmarks. Docu-ment Understanding Dataset and Evaluation (DUDE) seeks to remediate the halted research progress in understanding visually-rich documents (VRDs). We present a new dataset1 with novelties related to types of questions, answers, and document layouts based on multi-industry, multi-domain, and multi-page VRDs of various origins, and dates. More-over, we are pushing the boundaries of current methods by creating multi-task and multi-domain evaluation setups that more accurately simulate real-world situations where pow-erful generalization and adaptation under low-resource set-tings are desired. DUDE aims to set a new standard as a more practical, long-standing benchmark for the commu-nity, and we hope that it will lead to future extensions and contributions that address real-world challenges. Finally, our work illustrates the importance of finding more efficient ways to model language, images, and layout in DocAI. 1huggingface.co/datasets/jordyvl/DUDE_loader
Early stages of research and growth in any field are char-acterized by enacting proof-of-concept and demonstrating the feasibility of the proposed solution. In the Deep Learn-ing era, this is often echoed by building narrow and simpli-fied datasets that do not reflect real-world complexity, lead-ing to models that may not be suitable for practical use.
The field of Document Understanding (DU) is not an exception to the recent proliferation of deep architectures, which in this case are predominantly used for classification and information extraction from documents. However, the wide and complex nature of documents presents many chal-lenges that remain unsolved or not yet addressed. One such challenge is domain generalization, where a model trained on medical documents may not be directly applicable to financial or tabular content. Another challenge concerns task-agnostic architectures, where a model must be able to adapt to various DU subtasks such as document classi-fication, key information extraction (KIE), and question an-swering (QA). Lastly, the high variability of document con-tents and layouts often leads to highly imbalanced samples
within document types, resulting in a long-tailed distribu-tion with few or almost no samples to train a model.
Despite the importance of these challenges, there is cur-rently no DU benchmark dataset that simultaneously ad-dresses all of these issues. This paper proposes a novel dataset formulated as an instance of Document Visual Ques-tion Answering (DocVQA) to evaluate how well current
DU solutions deal with multi-page documents, if they can navigate and reason over visual layouts, and if they can gen-eralize their skills to different document types and domains.
The data collection and evaluation design of DUDE nat-urally motivates targeting models that can answer natural yet highly diverse questions (e.g., regarding document el-ements, their properties, and compositions) for any VRD (e.g., drawn from potentially unseen distributions of lay-outs, domains, and types). The presented problem setting relates to Multi-Domain Long-Tailed Recognition (MDLT)
[97], which concerns learning from multi-domain imbal-anced data whilst addressing label imbalance, divergent la-bel distributions across domains, and possible train-test do-main shift. Put plainly, since we cannot provide ground truth QA pairs for, e.g., stamps, on every document type (domain), we expect a solution to transfer the subtask
’stamp detection’ learned on document types where stamps naturally occur (and thus training QA pairs were created or-ganically) to other domains. The DocVQA and MDLT for-mulations of DUDE allow us to create a longstanding, chal-lenging benchmark that in the future can be easily extended with more subtasks formulated as QA pairs, and domains relating to document types (see Limitations).
The contribution of this work is twofold. First, we have created DUDE, a novel large-scale, multi-paged, multi-domain, multi-industry DocVQA benchmark for evaluat-ing DU progress. Second, we show that the zero-shot and fine-tuned performance of current state-of-the-art models applied to DU lags far behind human baselines, explained in part by the need for more holistic and efficient modeling of language, vision, and richly structured layouts. 2.