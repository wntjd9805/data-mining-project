Abstract
Mixed-Precision Quantization (MQ) can achieve a com-petitive accuracy-complexity trade-off for models. Con-training-based search methods require time-ventional consuming candidate training to search optimized per-layer bit-width configurations in MQ. Recently, some training-free approaches have presented various MQ proxies and significantly improve search efficiency. However, the cor-relation between these proxies and quantization accuracy is poorly understood. To address the gap, we first build the
MQ-Bench-101, which involves different bit configurations and quantization results. Then, we observe that the exist-ing training-free proxies perform weak correlations on the
MQ-Bench-101. To efficiently seek superior proxies, we de-velop an automatic search of proxies framework for MQ via evolving algorithms. In particular, we devise an elaborate search space involving the existing proxies and perform an evolution search to discover the best correlated MQ proxy.
We proposed a diversity-prompting selection strategy and compatibility screening protocol to avoid premature con-vergence and improve search efficiency.
In this way, our
Evolving proxies for Mixed-precision Quantization (EMQ) framework allows the auto-generation of proxies without heavy tuning and expert knowledge. Extensive experiments on ImageNet with various ResNet and MobileNet families demonstrate that our EMQ obtains superior performance than state-of-the-art mixed-precision methods at a signifi-cantly reduced cost. The code will be released. 1.

Introduction
Deep Neural Networks (DNNs) have demonstrated out-standing performance on various vision tasks [20, 25].
However, their deployment on edge devices is challeng-ing due to high memory consumption and computation cost
[14]. Quantization techniques [19, 6, 8] have emerged as a promising solution to address this challenge by perform-*Corresponding author, † equal contribution.
Figure 1. Illustration of the search space for EMQ. Our pro-posed search space encompasses the handcrafted proxies in mixed-precision quantization, whose input sources are activation(A), gra-dient (G), weight(W), Hessian(H), as well as their combinations (e.g., G × W ). The proposed search space highlights the extensive range of possible combinations, emphasizing the significant effort required to discover new MQ proxies. ing computation and storing tensors at lower bit-widths than floating point precision, and thus speed up inference and re-duce the memory footprint.
Mixed-precision quantization (MQ) [40, 18, 10, 12, 8, 13] is a technique that assigns different bit-widths to the layers of a neural network to achieve a better accuracy-complexity trade-off and allows for the full exploitation of the redundancy and representative capacity of each layer.
MQ methods can be categorized into training-based and training-free approaches. Training-based methods for MQ present it as a combinatorial search problem and adopt time-consuming Reinforcement Learning (RL) [40], Evolution
Algorithm (EA) [41], one-shot [16], or gradient-based [42] methods to find the optimal bit-precision setting. How-ever, these methods can be computationally intensive and require several GPU days on ImageNet [40, 3], limiting their applicability in scenarios with limited computing re-sources or high real-time requirements. Recently, training-free approaches [35, 28, 36, 8, 7, 21] have emerged for mixed-precision quantization, which starkly reduces the heavy computation burden. These approaches aim to re-duce the computational burden by building alternative prox-ies to rank candidate bit-width configurations. For exam-ple, QE [35] uses the entropy value of features to automat-ically select the bit-precision of each layer. These training-free methods have shown commendable effectiveness in as-signing bit-precision to each layer in MQ. However, these training-free methods [8, 7, 44, 35, 28] have two signifi-cant limitations: (i) Lack of correlation analysis between training-free proxies and quantization accuracy. For in-stance, HAWQ-V2 [7] report quantitative results, which couple with quantified strategies and proxies. Thus, it is still unclear whether they can accurately predict the per-formance of different bit configurations. (ii) The discovery processes for proxies require expert knowledge and exten-sive trial tuning, which might not fully exploit the potential of training-free proxies. These limitations raise two funda-mental but critical questions: (1) How can we accurately assess the predictive capability of existing proxies? and (2)
How can we efficiently devise new proxies?
To address the first question, we develop a benchmark, namely, MQ-Bench-101, which comprises numerous bit configurations using the post training quantization strategy.
Using this benchmark, we evaluated the performance of several existing training-free proxies, as reported in Tab. 1.
Our results demonstrate that the current proxies exhibit lim-ited predictive capabilities. Moreover, we attempt the prox-ies in training-free NAS and observe that the proxies require bit-weighting for effective quantification [8]. These obser-vations1 motivate us to devise improved proxies for MQ.
As for the second question, we present a general frame-work, Evolving proxies for Mixed-precision Quantiza-tion (EMQ), whose aim is to use a reformative evolving algorithm to automate the discovery of MQ proxies. Specif-ically, we devise an elaborate and expressive search space encompassing all existing MQ proxies. As shown in Fig. 2, we formula MQ proxies as branched computation graphs composed of primitive operations and evolve them accord-ing to their predictive ability on MQ-Bench-101. We notice the importance of the ranking consistency of the top per-forming bit-widths rather than the overall rank consistency.
To better account for the correlation of the top bit config-urations, we introduce Spearman@topk (ρs@k) as the fit-ness function. To avoid premature convergence and im-prove search efficiency of the evolution process, we pro-posed the diversity-prompting selection strategy and com-patibility screening protocol, respectively. We validate our framework on quantization-aware training and post-training quantization tasks. The experiments show that our searched
MQ proxy is superior to the existing proxies in predictive capacity and quantization accuracy.
Main Contributions: 1There are two routines for proxies in MQ: scoring bit configurations as a whole and evaluating layer-wise sensitivity separately. In this paper, we focus on tackling the former and compare both methods in experiments that are discussed in detail in the App. D.1
Table 1. Ranking correlation (%) of training-free proxies on MQ-Bench-101. The Spearman@topk (ρs@k) are adopted to measure the correlation of the top performing bit configurations on MQ-Bench-101. We reported the mean and std of ρs@k of 5 runs for all
MQ proxies. All implementations are based on the official source code. The ’Time’ column indicates the evaluation time (in sec-onds) for each bit-width configuration.
Method
ρs@20%
ρs@50%
ρs@100%
Time(s)
BParams
HAWQ [8]
HAWQ-V2 [7]
OMPQ [28]
QE [35]
SNIP [21]
Synflow [37]
EMQ(Ours) 28.67±0.24 23.64±0.13 30.19±0.14 7.88±0.16 20.33±0.09 33.63±0.20 39.92±0.09 42.59±0.09 32.41±0.07 36.21±0.09 44.12±0.15 16.38±0.08 24.37±0.13 17.23±0.09 44.10±0.11 57.21±0.05 55.08±0.13 60.47±0.07 74.75±0.05 31.07±0.03 36.50±0.06 38.48±0.09 31.57±0.02 79.21±0.05 2.59 53.76 42.17 53.76 2.15 2.50 2.23 1.02
• We introduce MQ-Bench-101, the first benchmark for training-free proxies in mixed-precision quantization (Sec. 4.2).
• We propose Evolving training-free proxies for Mixed-precision Quantization (EMQ) framework, which in-cludes the diversity-prompting selection to prevent premature convergence and the compatibility screen-ing protocol to improve the evolution search efficiency (Sec. 3).
• Experimental results demonstrate the superiority of the searched MQ proxy, indicating the effectiveness and flexibility of our proposed approach (Sec. 4). 2.