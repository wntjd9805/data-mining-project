Abstract
Grounding 3D object affordance seeks to locate objects’
“action possibilities” regions in the 3D space, which serves as a link between perception and operation for embodied agents. Existing studies primarily focus on connecting vi-sual affordances with geometry structures, e.g., relying on annotations to declare interactive regions of interest on the object and establishing a mapping between the regions and affordances. However, the essence of learning object af-fordance is to understand how to use it, and the manner that detaches interactions is limited in generalization. Nor-mally, humans possess the ability to perceive object affor-dances in the physical world through demonstration images or videos. Motivated by this, we introduce a novel task set-ting: grounding 3D object affordance from 2D interactions in images, which faces the challenge of anticipating affor-dance through interactions of different sources. To address this problem, we devise a novel Interaction-driven 3D Affor-dance Grounding Network (IAG), which aligns the region feature of objects from different sources and models the in-teractive contexts for 3D object affordance grounding. Be-sides, we collect a Point-Image Affordance Dataset (PIAD) to support the proposed task. Comprehensive experiments on PIAD demonstrate the reliability of the proposed task and the superiority of our method. The project is available at https://github.com/yyvhang/IAGNet. 1.

Introduction
The term “affordance” is described as “opportunities of interaction” by J. Gibson [14]. Grounding 3D object affor-dance aims to comprehend the interactive regions of objects in 3D space, which is not only simply to predict which inter-action an object affords, but also to identify specific points on the object that could support the interaction. It consti-∗Corresponding Author.
Figure 1. Grounding Affordance from Interactions. We propose to ground 3D object affordance through 2D interactions. Given an object point cloud with an interactive image, grounding the corre-sponding affordance on the 3D object. tutes a link between perception and operation for embodied agents, which has the potential to serve numerous practical applications, e.g. action prediction [21, 63], robot manip-ulation [36, 45, 51], imitation learning [17, 50], and aug-mented/virtual reality [6, 9].
So far, the paradigm of perceiving 3D object affordance has several branches. One of them involves establishing an explicit mapping between affordance categories and ge-ometry structures [10, 20, 46, 66], based on visual appear-ance. However, affordance is dynamic and multiple, these geometric-specific manners have limited generalization for unseen structures. Besides, locking the geometry with a specific affordance category may lead to the anticipated re-gion being inconsistent with its affordance when objects possess multiple similar geometrics, resulting in affordance regional confusion. Another paradigm is based on rein-forcement learning, which puts the agent in 3D synthetic scenarios to interact with several objects actively, taking the reward mechanism to optimize the whole process [48].
While this type of approach transitions agents from passive recognition to active reasoning, it needs repeated attempts in a huge search space when meeting a novel structure, and is therefore time-consuming.
(a) (b)
Figure 2. Motivation. (a) Clues to correlate regions of objects from different sources. Objects with the same category possess a similar combination scheme to meet certain affordance properties, it is a similar inherent relation among different instances. And some structures hint at these properties exhibit closer representations with higher similarity in the latent space. (b) Object affordance may be affected by dynamic factors, such as the position of the object itself in the scene, other objects in the scene, and the body part of the interacting subject.
These factors can be decomposed into object-subject and object-scene interaction contexts.
These limitations motivate us to explore an affordance-compatible learning paradigm. Typically, humans can in-fer object affordance in the 3D physical world by watching images or videos that demonstrate the interactions. Some studies in cognitive science [37, 56] point out the existence of “body image” in human cognition, which claims that hu-mans have a perceptual experience of the objects they see, thus facilitating their ability to organize the perception [73] and operate novel objects. Hence, human-object interaction conveys the perception that object structure could perform certain affordance, which is a crucial clue to reason object affordance. In light of this, we present a novel task setting: grounding 3D object affordance from 2D interactions in im-ages, which is shown in Fig. 1.
This challenging task includes several essential issues that should be properly addressed. 1) Alignment ambigu-ity. To ground 3D object affordance from 2D source in-teractions, the premise is to correspond the regions of the object in different sources. The object in 2D demonstra-tions and the 3D object we face are usually derived from different physical instances in different locations and scales.
This discrepancy may lead to confusion when matching the affordance regions, causing alignment ambiguity. While objects are commonly designed to satisfy certain needs of human beings, so the same category generally follows a similar combination scheme of object components to meet certain affordances, and these affordances are hinted at by some structures (Fig. 2 (a)). These invariant properties are across instances and could be utilized to correlate ob-ject regions from different sources. 2) Affordance ambigu-ity. Affordance has properties of dynamic and multiplicity, which means the object affordance may change according to the situation, the same part of an object could afford mul-tiple interactions, as shown in Fig. 2 (b), “Chair” affords
“Sit” or “Move” depends on the human actions, “Mug” af-fords “Wrapgrasp” or “Contain” according to the scene con-text, these properties may make ambiguity when extracting affordance. However, these dynamic factors for affordance extraction can be decomposed into the interaction between subject-object and object-scene. Modeling these interac-tions is possible to extract explicit affordance.
To address these issues, we propose the Interaction-driven 3D Affordance Grounding Network (IAG) to align object region features from different sources and model in-teraction contexts to reveal affordance. In detail, it contains two sub-modules, one is Joint Region Alignment Module (JRA), which is devised to eliminate alignment ambiguity.
It takes the relative difference in dense cross-similarity to refine analogous shape regions and employs learnable lay-ers to map the invariant combination scheme, taking them to match local regions of objects. For affordance ambi-guity, the other one Affordance Revealed Module (ARM) takes the object representation as a shared factor and jointly models its interaction contexts with the affordance-related factors to reveal explicit affordance. Moreover, we col-lect Point-Image Affordance Dataset (PIAD) that contains plenty of paired image-point cloud affordance data and make a benchmark to support the model training and evalu-ation on the proposed setting.
The contributions are summarized as follows: 1) We in-troduce grounding 3D object affordance through the 2D in-teractions, which facilitates the generalization to 3D object affordance perception. 2) We propose the IAG framework, which aligns the region feature of objects from different sources and jointly models affordance-related interactions to locate the 3D object affordance. 3) We collect a dataset named PIAD to support the proposed task setting, which contains paired image-point cloud affordance data. Besides, we establish a benchmark on PIAD, and the experiments on
PIAD exhibit the reliability of the method and the setting.
2.