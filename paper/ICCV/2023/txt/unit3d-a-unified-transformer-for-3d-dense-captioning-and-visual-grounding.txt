Abstract
Performing 3D dense captioning and visual grounding requires a common and shared understanding of the under-lying multimodal relationships. However, despite some pre-vious attempts on connecting these two related tasks with highly task-specific neural modules, it remains understud-ied how to explicitly depict their shared nature to learn them simultaneously. In this work, we propose UniT3D, a sim-ple yet effective fully unified transformer-based architecture for jointly solving 3D visual grounding and dense caption-ing. UniT3D enables learning a strong multimodal repre-sentation across the two tasks through a supervised joint pre-training scheme with bidirectional and seq-to-seq ob-jectives. With a generic architecture design, UniT3D allows expanding the pre-training scope to more various training sources such as the synthesized data from 2D prior knowl-edge to benefit 3D vision-language tasks. Extensive experi-ments and analysis demonstrate that UniT3D obtains signif-icant gains for 3D dense captioning and visual grounding. 1.

Introduction
The 3D vision-language field has been drawing increas-ing research interest in jointly understanding 3D scenes [11, 34, 26, 35, 17, 14, 36, 18, 9] and natural language [41, 13, 28, 46, 2], such as 3D visual grounding [5] and 3D dense captioning [8]. The task of 3D visual grounding takes as input a point-cloud-text pair and outputs a bounding box of the referred object. As its sibling task, 3D dense captioning expects a point cloud as input and densely generates object bounding boxes and descriptions in the scene. Both tasks enable applications such as assistive robots and natural lan-guage control in AR/VR systems.
Although these two 3D vision-language tasks are nat-urally complementary to each other, previous attempts to connect them [6, 3] only exploit partly shared object spatial information, leaving the joint nature between object rela-tionships and textual semantics underdeveloped. More con-cretely, to localize the target object in the scene, 3D visual
grounding methods require a joint understanding of object attributes and spatial relationships. This requirement per-sists for 3D dense captioning when densely describing the appearance and spatial aspects of the objects. Therefore, it is naturally desirable to develop a shared representation between two tasks with a unified task-agnostic framework, where the two input modalities are jointly encoded and en-hanced. Moreover, such generic multimodal representation is not only beneficial for sharing knowledge between visual grounding and dense captioning, but could also enable 3D vision-language research beyond specific domains, as in the case of VisualBERT [27] or CLIP [38] on 2D images.
To this end, we propose UniT3D, a joint transformer-based solution to facilitate vision-language representation learning for 3D visual grounding and dense captioning, as illustrated in Fig. 1. Unlike prior works with two distinct and task-specific neural modules [3, 6], our method tackles the tasks of 3D dense captioning and visual grounding via a task-agnostic unified transformer with light-weight output heads. To enable joint vision-language representation learn-ing, we design a supervised training scheme that combines the bidirectional objective with query-aware object match-ing supervision and the seq-to-seq objective with object-aware sequence generation supervision. Through fine-tuning on specific tasks with corresponding output heads, the joint vision-language representation learned by our task-agnostic multimodal transformer is well capable of support-ing both the localization of the referred objects in the scenes and the dense generation of object descriptions.
One challenge of the joint 3D vision-language rep-resentation learning is that existing 3D vision-language datasets [5, 1] are relatively limited in size and variety com-pared to their 2D counterparts such as MSCOCO [7] and
Conceptual Captions [39]. To address this challenge, we build a large-scale 3D vision-language dataset with text an-notations generated from an image captioner learned on abundant 2D image and text datasets. Concretely, we apply the image captioner from Mokady et al. [32] to ScanNet im-ages to obtain synthetic image-text pairs and convert them to point-cloud-text pairs by cropping the reconstructed point clouds in ScanNet within the image frustum using camera parameters as our synthetic 3D vision-language data. We show that along with jointly pre-training on such synthe-sized data with the proposed bidirectional and seq-to-seq objectives, our UniT3D model obtains significant perfor-mance gains on the downstream 3D vision-language tasks.
To summarize, our contributions are threefold:
• We introduce a multimodal transformer architecture to solve 3D visual grounding and dense captioning in a fully unified fashion.
• We propose a supervised joint pre-training scheme with bidirectional and seq-to-seq objectives to facili-tate multimodal feature learning.
• We construct a large-scale synthetic point-cloud-text dataset, showing that the distilled 2D prior knowledge is beneficial to 3D vision-language tasks. 2.