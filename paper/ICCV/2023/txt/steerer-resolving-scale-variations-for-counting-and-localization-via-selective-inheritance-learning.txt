Abstract
Scale variation is a deep-rooted problem in object count-ing, which has not been effectively addressed by exist-ing scale-aware algorithms. An important factor is that they typically involve cooperative learning across multi-resolutions, which could be suboptimal for learning the most discriminative features from each scale. In this paper, we propose a novel method termed STEERER (SelecTivE inhERitance lEaRning) that addresses the issue of scale variations in object counting. STEERER selects the most suitable scale for patch objects to boost feature extrac-tion and only inherits discriminative features from lower to higher resolution progressively. The main insights of
STEERER are a dedicated Feature Selection and Inher-itance Adaptor (FSIA), which selectively forwards scale-customized features at each scale, and a Masked Selec-tion and Inheritance Loss (MSIL) that helps to achieve high-quality density maps across all scales. Our exper-imental results on nine datasets with counting and local-ization tasks demonstrate the unprecedented scale general-ization ability of STEERER. Code is available at https:
//github.com/taohan10200/STEERER. 1.

Introduction
The utilization of computer vision techniques to count objects has garnered significant attention due to its potential in various domains. These domains include but are not lim-ited to, crowd counting for anomaly detection [32, 5], ve-hicle counting for efficient traffic management [51, 81, 49], cell counting for accurate disease diagnosis [11], wildlife counting for species protection [2, 29], and crop counting for effective production estimation [43].
Numerous pioneers [38, 60, 3, 27, 74, 47, 22, 79, 24] have emphasized that the scale variations encountered in counting tasks present a formidable challenge, motivat-ing the development of various algorithms aimed at miti-*Corresponding author
Figure 1: STEERER significantly improves the quality of the predicted density map, especially for large objects, com-pared with the classic multi-scale future fusion method [61]. gating their deleterious impact. These algorithms can be broadly classified into two branches: 1) Learn to scale methods [37, 78, 54] address scale variations in a two-step manner. Specifically, it involves estimating an ap-propriate scale factor for a given image/feature patch, fol-lowed by resizing the patch for prediction. Most of them require an additional training task (e.g., predicting the ob-ject’s scale) and the predicted density map is further par-titioned into multiple parts, 2) Multi-scale fusion meth-ods [36, 39] have been demonstrated to be effective in han-dling scale variations in various visual tasks, and their con-cept is also extensively utilized in object counting. Gen-erally, they focus on two types of fusion, namely feature fusion [22, 79, 80, 59, 4, 61, 44, 50, 31, 83, 69] and density map fusion [10, 41, 25, 65, 42, 47, 63, 9]. Despite the re-cent progress, these studies still face challenges in dealing with scale variations, especially for large objects, as shown in Fig. 1. The main reason is that the existing multi-scale methods (e.g. FPN [36]) adapt the same loss to optimize all resolutions, which poses a great challenge for each res-olution to find which scale range is easy to handle. Also, it results in mutual suppression because counting the same object accurately at all scales is hard.
Prior research [79, 41, 63] has revealed that different resolutions possess varying degrees of scale tolerance in
a multi-resolution fusion network. Specifically, a single-resolution feature can accurately represent some objects within a limited scale range, but it may not be effective for other scales, as visualized in feature maps in Fig. 2 red boxes. We refer to features that can accurately predict ob-ject characteristics as scale-customized features; otherwise, they are termed as scale-uncustomized features. If scale-customized features can be separated from their master fea-tures before the fusion process, it is possible to preserve the discriminative features throughout the entire fusion process.
Hence, our first motivation is to disentangle each resolu-tion into scale-customized and scale-uncustomized features before each fusion, enabling them to be independently pro-cessed. To accomplish this, we introduce the Feature Se-lection and Inheritance Adaptor (FSIA), which comprises three sub-modules with independent parameters. Two adap-tation modules separately process the scale-customized and the scale-uncustomized features, respectively, while a soft-mask generator attentively selects features in the middle.
Notably, conventional optimization methods do not en-sure that FSIA acquires the desired specialized functions.
Therefore, our second motivation is to enhance FSIA’s capabilities through exclusive optimization targets at each resolution. The first function of these objectives is to im-plement inheritance learning when transmitting the lower-resolution feature to a higher resolution. This process en-tails combining the higher-resolution feature with the scale-customized feature disentangled from the lower resolution, which preserves the ability to precisely predict larger ob-jects that are accurately predicted at lower resolutions. An-other crucial effect of these objectives is to ensure that each scale is able to effectively capture objects within its pro-ficient scale range, given that the selection of the suitable scale-customized feature at each resolution is imperative for the successful implementation of inheritance learning.
In conclusion, our ultimate goal is to enhance the capa-bilities of FSIA through Masked Selection and Inheritance
Loss (MSIL), which is composed of two sub-objectives controlled by two mask maps at each scale. To build them, we propose a Patch-Winner Selection Principle that auto-matically selects a proficient region mask for each scale.
The mask is applied to the predicted and ground-truth den-sity maps, enabling the selection of the effective region and filtering out other disturbances. Also, each scale inherits the mask from all resolutions lower than it, allowing for a grad-ual increase in the objective from low-to-high resolutions, where the incremental objective for a high resolution is the total objective of its neighboring low resolution. The pro-posed approach is called SeleceTivE inhERitance lEaRning (STEERER), where FSIA and MSIL are combined to max-imize the scale generalization ability. STEERER achieves state-of-the-art counting results on several counting tasks and is also extended to achieve SOTA object localization.
This paper’s primary contributions include:
• We introduce STEERER, a principled method for ob-ject counting that resolves scale variations by cumula-tively selecting and inheriting discriminative features thereby enabling the from the most suitable scale, acquisition of scale-customized features from diverse scales for improved prediction accuracy.
• We propose a Feature Selection and Inheritance Adap-tor that explicitly partitions the lower scale feature into its discriminative and undiscriminating components, which facilitates the integration of discriminative rep-resentations from lower to higher resolutions and their progressive transmission to the highest resolution.
• We propose a Masked Selection and Inheritance Loss that utilizes the Patch-Winner Selection Principle to select the optimal scale for each region, thereby max-imizing the discriminatory power of the features at the most suitable scales and progressively empowering
FSIA with progressive constraints. 2.