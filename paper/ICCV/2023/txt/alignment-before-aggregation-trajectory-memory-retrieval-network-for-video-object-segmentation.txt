Abstract
Memory-based methods in semi-supervised video object segmentation task achieve competitive performance by per-forming dense matching between query and memory frames.
However, most of the existing methods neglect the fact that videos carry rich temporal information yet redundant spatial information. In this case, direct pixel-level global match-ing will lead to ambiguous correspondences. In this work, we reconcile the inherent tension of spatial and temporal information to retrieve memory frame information along the object trajectory, and propose a novel and coherent Tra-jectory Memory Retrieval Network (TMRN) to equip with the trajectory information, including a spatial alignment module and a temporal aggregation module. The proposed
TMRN enjoys several merits. First, TMRN is empowered to characterize the temporal correspondence which is in line with the nature of video in a data-driven manner. Sec-ond, we elegantly customize the spatial alignment module by coupling SVD initialization with agent-level correlation for representative agent construction and rectifying false matches caused by direct pairwise pixel-level correlation, respectively. Extensive experimental results on challeng-ing benchmarks including DAVIS 2017 validation / test and
Youtube-VOS 2018 / 2019 demonstrate that our TMRN, as a general plugin module, achieves consistent improvements over several leading methods. 1.

Introduction
Semi-supervised Video Object Segmentation (VOS) is a fundamental task to perform pixel-wise classification of a set of class-agnostic objects in video sequences. It has been widely applied to autonomous driving [61], video edit-ing [31], augmented reality [30], etc. Since the object mask
*Equal contribution
â€ Corresponding author
Figure 1: Illustration of our motivation. (a) shows TMRN reconciles the inherent tension of spatial and temporal in-formation to retrieve memory frame information along the object trajectory. (b) shows we carefully design a spatial alignment module through a set of representative reference features (agents) to rectify direct pairwise pixel-level corre-lation caused by distractors with similar appearance. is only given in the first frame without any other prior in-formation assumptions, how to fully exploit limited and semantic-agnostic information to perform accurate segmen-tation in the subsequent frames is thus extremely challenging.
Recently, memory-based methods [56, 37, 7, 38, 32] dominate this field credited to their simplicity yet competi-tive performance. The core idea of the memory-based meth-ods is to perform dense matching between query (i.e., cur-rent frame) and memory (i.e., past frames with given or segmented masks), and to retrieve the constructed memory bank in a pixel-level manner. Despite their promising results,
these methods neglect the fact that videos carry rich tempo-ral information (e.g., the target object ball moves over time in Figure 1 (a)) yet redundant spatial information. In this case, direct pixel-level global matching forces each query pixel to retrieve all memory pixels equivalently across space and time, leading to ambiguous correspondences that suf-fer from superfluous spatial information, and are fragile to the movement of objects and cameras ascribed to contempt for temporal information (i.e., trajectory). To make mat-ters worse, the temporal information will be further diluted when the memory frames gradually increase as the video progresses, leading to sub-optimal results. Therefore, it is highly desirable to characterize the temporal correspondence from the VOS task, that is, aggregate the trajectory features of the target object ball from all relevant memory frames for segmentation.
In this paper, we aim to reconcile the inherent tension of spatial and temporal information to retrieve memory frame information along the object trajectory. Specifically, we de-sign a novel Trajectory Memory Retrieval Network (TMRN) that can be applied as a generic plugin, including a spatial alignment module and a temporal aggregation module to equip with the trajectory information for robust VOS. In specific, as shown in Figure 1 (a), we enable each query pixel to independently retrieve the pixels in each memory frame to seek the location of the counterpart trajectory, and obtain spatially aligned memory pixel features (resides in the same spatial position as the corresponding query ones).
Then the resultant aligned memory pixels are pooled through the temporal aggregation module to reason about inter-frame connections (i.e., the contribution of each memory frame) in an adaptive manner, please refer to figure 7. However, directly employing pairwise pixel-level correlation in the spatial alignment module tends to struggle to distinguish the objects with similar appearances (e.g., color), increasing the risk of false matches. As shown in Figure 1 (b), due to the distractor jersey with similar appearances in memory frame, p1 in the query frame situated on the cap is erroneously closer to p3 located in the jersey than counterpart p2 in the memory frame.
To mitigate the false matching problem, we carefully design the spatial alignment module through a set of repre-sentative reference features (referred to as agents) to rectify direct pairwise pixel-level correlation. The main idea is, for each pixel from the query or memory frame, we can obtain the agent-level correlation (i.e., a likelihood vector) by com-paring this pixel with a set of agents. In essence, the agent-level correlation reflects the consensus among representative agents with a broader receptive field, thus it encodes the rel-ative semantic comparability of the agents that can be relied upon. Intuitively, each pair of pixels with true correspon-dence (e.g., the p1-p2 pair in Figure 1 (b)) from the query and memory frame should be not only visually similar to each other (i.e., high pairwise pixel-level correlation), but also holding consensus to any other agents (i.e., similar agent-level correlation pair). Based on this correlation consistency in spatial alignment module, false matches caused by similar vision but dissimilar agent correlations will be suppressed (e.g., the point p1-p3 pair in Figure 1 (b)), ensuring that true pixel-level correlations between query-memory frame enjoy higher weights in pursuit of spatially well-aligned memory features.
However, it is non-trivial to attain the appropriate agents without any supervision signals for training. Intuitively, the agents should resonate favorably with diverse semantic cues from both query and memory pixels with a wide range of semantic contrast descriptive. In other words, the matching between query-memory pixels in the spatial alignment mod-ule based on agent-level correlation should preserve as much critical information as possible in the original pixel-level cor-relation. Therefore, we take advantage of the singular value decomposition (SVD) to obtain diverse and complementary agents benefiting from the inbuilt rapid decay properties of the singular value, considering the sum of the squares of the singular values after singular value decomposition can be regarded as the energy of the matrix (i.e., the representative information contained in the original pixel-level correlation).
In this work, our contributions can be summarized as follows: (1) We design a novel and coherent Trajectory
Memory Retrieval Network (TMRN) that can be applied as a generic plugin, including a spatial alignment module and a temporal aggregation module to equip with the trajectory information in VOS. To the best of our knowledge, this is the first work to characterize the temporal correspondence which is in line with the nature of video in a data-driven manner. (2) We elegantly customize the spatial alignment module by coupling SVD initialization with agent-level correlation for representative agents construction and rectifying false matches caused by direct pairwise pixel-level correlation, respectively. (3) Extensive experimental results on challeng-ing benchmarks including DAVIS 2017 validation / test and
Youtube-VOS 2018 / 2019 demonstrate that our TMRN, as a general plugin module, achieves consistent improvements over several leading methods. 2.