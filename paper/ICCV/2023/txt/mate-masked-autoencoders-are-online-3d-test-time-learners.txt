Abstract
Our MATE is the first Test-Time-Training (TTT) method designed for 3D data, which makes deep networks trained for point cloud classification robust to distribution shifts oc-curring in test data. Like existing TTT methods from the 2D image domain, MATE also leverages test data for adaptation.
Its test-time objective is that of a Masked Autoencoder: a large portion of each test point cloud is removed before it is fed to the network, tasked with reconstructing the full point cloud. Once the network is updated, it is used to classify the point cloud. We test MATE on several 3D object clas-sification datasets and show that it significantly improves robustness of deep networks to several types of corruptions commonly occurring in 3D point clouds. We show that MATE is very efficient in terms of the fraction of points it needs for the adaptation. It can effectively adapt given as few as 5% of tokens of each test sample, making it extremely lightweight.
Our experiments show that MATE also achieves competitive performance by adapting sparsely on the test data, which further reduces its computational overhead, making it ideal for real-time applications.
Figure 1: Overview of our Test-Time Training methodol-ogy. We adapt the encoder to a single out-of-distribution (OOD) test sample online by updating its weights using a self-supervised reconstruction task. We then use the updated weights to make a prediction on the test sample. To enable this approach, the encoder, decoder, and the classifier are co-trained in the classification and reconstruction tasks [17], which is not shown in the figure. 1.

Introduction
Recent deep neural networks show impressive perfor-mance in classifying 3D point clouds. However, their suc-cess is warranted only if the test data originates from the same distribution as training data. In real-world scenarios, this assumption is often violated. A LiDAR point cloud can be corrupted, for example, due to sensor malfunction or en-vironmental factors. It has been shown in [19, 22] that, even seemingly insignificant perturbations, like introduction of
â€  Equally contributing authors.
Correspondence: muhammad.mirza@icg.tugraz.at jitter or minute amount of noise to the point cloud, can signif-icantly decrease the performance of several state-of-the-art 3D object recognition architectures. This lack of robustness can limit the utility of 3D recognition in numerous appli-cations, including in construction industry, geo-surveying, manufacturing and autonomous driving. Distribution shifts that can affect 3D data are diverse in nature and it might not be feasible to train the network for all the shifts which can possibly be observed in point clouds at test-time. Thus, there is a need to adapt to these shifts online at test-time, in an unsupervised manner.
Test-Time Training (TTT) leverages unlabeled test data
to adapt the classifier to the change in data distributions at test-time in an online manner. Several TTT approaches have been recently proposed for the 2D image domain. The main techniques include regularizing the classifier on test data with objective functions defined on the entropy of its predic-tions [12, 26, 30], updating the statistics of the batch normal-ization layers to match the distribution of the test data [16], and training the network on test data with self-supervised tasks [14, 23]. However, existing 2D TTT methods fail when naively applied to the 3D point clouds, stressing upon the need for 3D-specific TTT methodologies, which are cur-rently non-existent.
In this paper, we address the problem of test-time train-ing for 3D point cloud classification. We propose a 3D-specific method, MATE, which adopts the self-supervised paradigm [14, 23], in which a deep network is adapted by solving a self-supervised task for the OOD test data. Our choice is dictated by the availability of a self-supervised task that perfectly matches our goal of adapting 3D networks.
Masked autoencoder proved very effective in pre-training 3D object recognition networks [17], and adapting deep net-works to corruptions of 2D images [6]. It removes a large portion of the point cloud, and tasks the network with re-constructing the entire point cloud given only the part that has not been removed. We use this procedure to update the network on every test sample that is used for the adaptation.
An overview is provided in Figure 1.
Our main contributions are extending TTT to the 3D point cloud domain and showing that simply adopting TTT tech-niques widely used in the 2D image domain is not a viable solution for 3D, stressing out the need for 3D-specific ap-proaches. To this end, we demonstrate how well-suited and powerful masked autoencoding is to address online test-time training for 3D data. We conduct extensive evaluations on three point cloud recognition datasets. Apart from achieving strong performance gains for online adaptation, we discover and highlight several useful properties for TTT with masked autoencoders. For example, our MATE achieves significant performance gains even when masking 95% of tokens from the point clouds. This seemingly nuance can have important benefits: At test-time, the encoder only needs to process the remaining 5% of the visible tokens to adapt the network, rad-ically limiting the computational overhead of the adaptation.
The overhead from TTT can be further reduced by adapting sparsely to test data, as MATE can achieve significant per-formance gains over un-adapted networks by only adapting on every 100-th sample of the OOD test data. 2.