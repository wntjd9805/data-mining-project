Abstract
Binary neural networks (BNNs) have been widely adopted to reduce the computational cost and memory stor-age on edge-computing devices by using one-bit represen-tation for activations and weights. However, as neural net-works become wider/deeper to improve accuracy and meet practical requirements, the computational burden remains a signiﬁcant challenge even on the binary version. To address these issues, this paper proposes a novel method called
Minimum Spanning Tree (MST) compression that learns to compress and accelerate BNNs. The proposed architec-ture leverages an observation from previous works that an output channel in a binary convolution can be computed using another output channel and XNOR operations with weights that differ from the weights of the reused channel.
We ﬁrst construct a fully connected graph with vertices cor-responding to output channels, where the distance between two vertices is the number of different values between the weight sets used for these outputs. Then, the MST of the graph with the minimum depth is proposed to reorder out-put calculations, aiming to reduce computational cost and latency. Moreover, we propose a new learning algorithm to reduce the total MST distance during training. Experi-mental results on benchmark models demonstrate that our method achieves signiﬁcant compression ratios with negli-gible accuracy drops, making it a promising approach for resource-constrained edge-computing devices. 1.

Introduction
Deep Neural Networks (DNNs) have been widely ap-plied in many artiﬁcial intelligence applications, especially vision tasks with high accuracy [27, 14]. However, the cost of computation and massive storage burden is signiﬁcantly challenging to deploy DNNs on embedded systems such as mobile devices and other resource-constrained platforms.
Many approaches have been proposed and demonstrated
*Corresponding Author
X a
X1X1
X d
X b
X c
X e
X n
X q
X m
X0X0
X p a)
X0X0
X 1
X m
X j
X i
X k b)
Figure 1. Illustration of kernel compression for BNNs with the K-mean method (a) and the shortest Hamiltonian path (b), in which adding red connections can further reduce the computational cost for both of them. their effectiveness in reducing energy and resources while maintaining the deep models’ accuracy, including pruning
[12], quantization [7], distillation [15], and efﬁcient hard-ware implementation [6]. Among these methods, quantiza-tion with less bit-width for parameter and activation repre-sentation is widely used due to its great beneﬁts and possi-bility in most practical applications.
Binarized neural network is a particular form of the quantization method in which weight values and activations are converted to 1-bit values. Accordingly, multiplications and accumulation operations can be replaced by XNOR and
Popcount operations [7], respectively. In addition, batch normalization is simpliﬁed to a threshold comparison [31], while the pooling can be performed with OR operations
[30]. Consequently, the compression ratio on memory stor-age and computational cost is signiﬁcantly improved, lead-ing to remarkable performance acceleration. Nevertheless, the accuracy degradation is the trade-off of minimizing the bit-width as BNN. Thus, most previous works focus on re-ducing this accuracy gap [25, 4, 33, 22, 20, 33, 23].
Meanwhile, compressing BNNs has not received much attention, with only a few prominent methods [32, 9, 31, 17], where the kernel compression [9, 31] gives impressive results with roughly 50% resources reduction. In particu-lar, given a binary convolution, inspired by an observation that an output channel can be calculated using another out-put channel and outputs of XNOR operations using weights that respectively differ from the reused channel’s weights
[9], authors in [31, 9] proposed to construct a fully con-nected graph, in which each vertex corresponds to an out-put channel, and distance between two vertices is the num-ber of different values between two weight sets used for the two output channels. Then, based on the graph, they use the K-mean cluster and shortest Hamiltonian path to reorder the convolution output calculation, aiming to re-duce the number of XNOR operations. However, these approaches have yet to fully minimize the computational cost as output channels can use less computation. Indeed,
Figure 1 (a) illustrates an example of the K-mean method, where two vertices are considered centers of two groups.
Two output channels corresponding to centers are fully cal-culated1, while other vertices reuse their centers for calcula-tion. However, adding a connection between the two centers would enable one to utilize the rest with less computation cost. Additionally, in the shortest Hamiltonian path shown in Figure 1 (b), there may exist a connection to a speciﬁc vertex that is shorter than the connection from the preced-ing vertex in the path, leading to a lower required number of operations for this vertex. Furthermore, time complex-ity poses a challenge in these methods [1, 13], resulting in longer exploration times.
To more effectively minimize the number of XNOR op-erations with the mentioned observation, in all connections to a speciﬁc vertex, the minimum connection must be se-lected to minimize the computation cost for this vertex. In addition, all of these selected connections must be included in a subgraph that visits all vertices exactly once to ensure that only one output channel is fully calculated. As a result, a MST is the only structure that can fulﬁll these require-ments. Therefore, this paper leverages the MST to reorder binary convolution output calculations. Figure 2 shows a simple example of reordering calculation on a convolution.
In this example, only the output channel 4 is fully calculated with Cin × M × M bit-weight values. In contrast, other channels are calculated using output channel 4, following the MST direction.
On the other hand, to maximize the advantages of the
MST, we further minimize the MST distance of all con-volution layers with a new learning algorithm right during the training stage. Besides, we propose a hardware accel-erator for BNNs applying the MST compression to demon-strate the feasibility and effectiveness of the method related to hardware resources. To the best of our knowledge, this method gives the highest compression ratio with implemen-tation from learning for compression to acceleration.
In summary, the following are contributions of this paper:
• We introduce and analyze the effectiveness of the MST 1calculated with Cin × M × M XNORs, denoted in Figure 2
Output channels (C      ) = 4, Input channels (C    ) = 1, Kernel size (M) = 3 0 0 out in 1 0 1 1 1 0 0 0 1 1 0 1 1 0 1 1
Input window 1x3x3 1 0 1 1 1 1 1 1 0 1
=5=5 0 1 1 0 1 0 0 1
Weights 4x3x3 1 1 0 1 1 1 0 0 1 1 2 1 2 4 5 5 3 4 2 3 2 1 2 4 3 2
Adjacent matrix
Fully connected graph
An MST 2 3 4 2 3 3 2
The order of computation  based on the MST
Figure 2. The process of arranging the order of computation on a binary convolution layer, in which output channel 4 is fully calcu-lated ﬁrst. Then, the output channel 4 is used to calculate channel 1, channel 2 and channel 3. in reducing the computational cost for the inference on a binary convolution layer.
• We propose a training algorithm that can reduce the
MST distance and depth right during the training pro-cess, which consequently maximizes the compression ratio for inference implementation.
• We provide the corresponding hardware acceleration for the proposed method with high throughput and better resource efﬁciency, compared to related works
[3, 30, 31, 10].
The experiments are performed for BNNs including
VGG-small [5], ResNet-18/20 [14] on CIFAR-10 dataset
[18], and ResNet-18/34 [14] on ImageNet [26]. The results show that the proposed approach gives a higher compres-sion ratio than the previous works [32]. Compared to the baseline [33], our method reduces up to 13.5× the con-volution parameters and 5.51× bit-wise operations on the same model with an acceptable accuracy degradation. Re-garding hardware acceleration, we conduct the experiments for a BNN with the same structure as the baseline [31] and apply the proposed approach. Compared to the baseline
[31], hardware deployments demonstrate that BNNs with our method save 1.8× LUTs, and 1.81× area efﬁciency while maintaining the acceleration speed and accuracy. 2.