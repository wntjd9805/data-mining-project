Abstract
Robust pedestrian trajectory forecasting is crucial to developing safe autonomous vehicles. Although previous works have studied adversarial robustness in the context of trajectory forecasting, some signiﬁcant issues remain un-In this work, we try to tackle these crucial addressed. problems. Firstly, the previous deﬁnitions of robustness in trajectory prediction are ambiguous. We thus provide formal deﬁnitions for two kinds of robustness, namely la-bel robustness and pure robustness. Secondly, as previous works fail to consider robustness about all points in a dis-turbance interval, we utilise a probably approximately cor-rect (PAC) framework for robustness veriﬁcation. Addition-ally, this framework can not only identify potential coun-terexamples, but also provides interpretable analyses of the original methods. Our approach is applied using a pro-totype tool named TRAJPAC. With TRAJPAC, we evalu-ate the robustness of four state-of-the-art trajectory predic-tion models — Trajectron++, MemoNet, AgentFormer, and
MID — on trajectories from ﬁve scenes of the ETH/UCY dataset and scenes of the Stanford Drone Dataset. Using our framework, we also experimentally study various fac-tors that could inﬂuence robustness performance. 1.

Introduction
Forecasting the movements of people based on their past states is a crucial task in both human behavior comprehen-sion and self-driving systems [47]. This task is commonly referred to as pedestrian trajectory prediction. Although current methods [49, 89, 5, 22, 67, 57, 55] for predicting hu-man trajectory have achieved remarkable results, they still face security risks due to their susceptibility to adversarial attacks. As Fig. 1 shows, even a slight and hardly percepti-neighbours trajectories true trajectory original prediction adversarial trajectories
Figure 1. An example of adversarial attacks ble alteration in the previous state can lead to a signiﬁcant variation in the prediction result.
Several works [95, 11, 37, 12, 98, 74] in the literature study the robustness of trajectory prediction models through the lens of adversarial attack and defense. However, many of these methods are directly translated from problems in image classiﬁcation and still do not fully consider the spe-ciﬁc circumstances of trajectory prediction tasks. As such, they have several overlooked shortcomings for benchmark-ing the robustness in forecasting problems. To this end, this work endeavors to both theoretically and experimen-tally analyse and mend these ﬂaws.
The ﬁrst problem is the current research does not pro-vide an exact and formal deﬁnition of robustness for tra-jectory prediction tasks. They emphasise that the adversar-ial trajectory is “natural and feasible” [95] or “close to the nominal trajectories” [11] but lacks a mathematical deﬁni-tion for what constitutes robustness (i.e., notion of robust-ness radius). Unlike the robustness of classiﬁcation tasks, trajectory prediction is framed as a regression problem. As such, directly translating the deﬁnition of robustness from image classiﬁcation to trajectory prediction is nontrivial.
I.e., at what level of alignment between the prediction and ground truth can the model be deemed robust? For this rea-son, we provide a formal deﬁnition (Sect. 3.2) that explic-itly deﬁnes the acceptable perturbation radius of historical trajectories. Our deﬁnition formally uniﬁes the semantic
deﬁnitions of robustness in previous works.
Secondly, the current research only evaluates the effec-tiveness of attacks by measuring the difference between the post-attack predicted path and the ground truth, but fails to take into account the difference between the post-attack
It is unclear prediction and the pre-attack prediction. whether robustness should be measured by the difference between post-attack output and pre-attack output, or the gap between post-attack prediction and ground truth. In order to address this issue, we present two novel deﬁnitions of robustness: label robustness, which quantiﬁes robustness in prediction accuracy after attacks; and pure robustness, which measures robustness in prediction stability after at-tacks.
It should be noted that due to the inherent indeterminacy in human behavior, numerous stochastic prediction tech-niques have been introduced to capture the multi-modality of future movements. Even for unperturbed examples, the predictions of these models at identical inputs may be dif-ferent. This presents a challenge to our deﬁnition of pure robustness. To address this issue, we propose to compare post-attack predictions with the empirical distribution of pre-attack predictions. The pure robustness can then be thought of as a measure of disjointness between an adver-sary and the model’s original forecast distribution.
Thirdly, the current literature on robustness in trajectory prediction focuses on benchmarking susceptibility to adver-sarial attacks, while overlooking the more rigorous prob-lem of veriﬁcation. That is to say, current works fail to consider robustness about all points in a disturbance in-terval. This is largely due to the computational infeasi-bility of such a procedure in continuous state spaces. To make veriﬁcation more practical, we take inspiration from
DEEPPAC [51] and probabilistically relax our deﬁnitions of robustness. In doing so, we allow efﬁcient veriﬁcation in a probably approximately correct (PAC) framework. We quantify the uncertainty associated with our method with
PAC guarantees on the conﬁdence and error rate. Moreover, our method involves learning a PAC locally linear model, which we show can be leveraged to ﬁnd adversaries com-parable to those found in classical attack methods like pro-jected gradient descent [53].
Finally, there is a lack of exploration into the inter-pretability of adversarial attacks on trajectory predic-tion models. Oftentimes, perturbations added to one fea-ture have greater inﬂuence on the output than perturbations added to other features. For example, in trajectory forecast-ing one might expect noise at the agent’s current position to have greater impact on the output than noise added to the agent’s original position. Using our PAC linear model, we aim to identify the features most sensitive to perturbation and provide an interpretable explanation for our ﬁndings.
Moreover, our interpretability analysis provides a stronger understanding of what trajectory forecasting models “see” when making future predictions.
Our main contributions are summarised as follows: 1. To the best of our knowledge, we are the ﬁrst to formally deﬁne robustness for trajectory prediction models, namely label robustness and pure robustness, which allows us to specify the prediction accuracy and stability of the models after attacks. (Sect. 3) 2. We propose TRAJPAC, a framework for robustness veriﬁcation of trajectory forecasting models. It takes inspiration from DEEPPAC [51] in that we regard the complex trajectory prediction model as a black box and learn a local PAC model by sampling. Due to the stochasticity in trajectory forecasting models, this gen-eralisation is theoretically non-trivial. With the learned
PAC model, we show how to conduct the analysis of robustness and interpretation for trajectory prediction models. (Sect. 4) 3. We use TRAJPAC to evaluate the robustness of four state-of-art trajectory forecasting models on the
ETH/UCY dataset and three of them on the Stanford
Drone Dataset. Our TRAJPAC shows good scalability on various trajectory forecasting models and different robustness properties. It is highly efﬁcient, as the run-ning time for model learning and veriﬁcation is within seconds. Although TRAJPAC only provides a PAC guarantee, we claim that it is empirically sound be-cause no counterexamples can be found by PGD [53] on all the cases where the PAC model learned by TRA-JPAC is robust. Also, we ﬁnd that TRAJPAC is ca-pable of ﬁnding adversarial examples comparable to
PGD. Through an interpretation analysis, we study the potential factors that contribute to robustness. (Sect. 5) 2.