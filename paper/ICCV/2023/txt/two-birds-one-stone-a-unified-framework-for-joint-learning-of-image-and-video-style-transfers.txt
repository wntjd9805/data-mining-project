Abstract
Current arbitrary style transfer models are limited to ei-ther image or video domains. In order to achieve satisfying image and video style transfers, two different models are in-evitably required with separate training processes on image and video domains, respectively. In this paper, we show that this can be precluded by introducing UniST, a Unified Style
Transfer framework for both images and videos. At the core of UniST is a domain interaction transformer (DIT ), which first explores context information within the specific domain and then interacts contextualized domain information for joint learning. In particular, DIT enables exploration of temporal information from videos for the image style trans-fer task and meanwhile allows rich appearance texture from images for video style transfer, thus leading to mutual ben-efits. Considering heavy computation of traditional multi-head self-attention, we present a simple yet effective axial multi-head self-attention (AMSA) for DIT , which improves computational efficiency while maintains style transfer per-formance. To verify the effectiveness of UniST, we conduct extensive experiments on both image and video style trans-fer tasks and show that UniST performs favorably against state-of-the-art approaches on both tasks. Code is available at https://github.com/NevSNev/UniST. 1.

Introduction
Artistic image style transfer [13] aims at migrating a de-sirable style pattern from an inference image to the origin image while preserving the original content structures. Al-though CNNs based methods have been well studied in this field [16, 21, 22, 29], they fail to capture the long-range in-teraction between the style and content domains, which may result in suboptimal results.
â€ Corresponding author: Libo Zhang (libo@iscas.ac.cn).
Figure 1: Comparison between single domain (image or video) style transfer and our joint style transfer.
Recently, owing to the ability to model long-range de-pendencies, Transformers [31] have shown excellent per-formance in a wide range of tasks including style transfer.
For example, Stytr2 [9] introduces a pure transformer net-work to deal with image style transfer. However, pixel-level self-attention brings additional computational complexity, resulting in lower efficiency.
Unlike image style transfer, video style transfer brings in new challenges of preserving temporal consistency be-tween stylized video frames. To achieve style transfer on the video domain, a feasible solution is to adapt existing image-based style transfer methods (e.g. [21, 25]) by re-training them with modification in architecture and/or loss functions. Despite simplicity, this domain adaption requires another repetitive and tedious training process, resulting in resource waste to some extent. Some other methods (e.g.
[7, 38]) directly adopt the same model from video to image, but the results are somewhat visually flawed.
To solve the above issues, we present a Unified Style
Figure 2: (a) Overview of the UniST, where the E is the VGG-19 network (pretrained and fixed) and D is the CNN decoder with a symmetric structure of VGG-19. Lc, Ls, Lid and Lt are content loss, style loss, identity loss and temporal loss; (b)
The structure of improved transformer encoder block; (c) The structure of improved transformer decoder block.
Transfer framework, termed UniST, for both images and videos. The proposed network leverages the local and long-range dependencies jointly. More specifically, UniST first applies CNNs to generate tokens, and then models long-range dependencies to excavate domain-specific informa-tion with domain interaction transformer (DIT ). After-wards, DIT sequentially interacts contextualized domain information for joint learning. Considering that the vanilla self-attention suffers from a heavy computational burden, we are inspired by axial attention [32] and develop the Axial
Multi-head Self-Attention (AMSA) mechanism to calculate attention efficiently for either images or video input. To our best knowledge, our approach is the first unified solution to handle both image and video style transfers simultaneously.
To verify the effectiveness of our approaches, we carry out extensive experiments on ImageNet [6] and MPI [2] for image and video field respectively. The results demonstrate that our unified solution can achieve better performance than current state-of-the-art image-based and video-based algorithms, evidencing its superiority and efficiency.
In summary, we make the following contributions in this work: (1) We propose a new joint learning framework for arbitrary image and video style transfers, in which two tasks can benefit from each other to improve the performance. To our best knowledge, this is the first work towards a unified solution with joint interaction. (2) We develop the Axial
Multi-head Self-Attention mechanism to address computa-tional complexity and adapt to tokens from image and video input. (3) Extensive experiments on both image and video style transfer tasks demonstrate the effectiveness of our ap-proach compared with state-of-the-art methods. 2.