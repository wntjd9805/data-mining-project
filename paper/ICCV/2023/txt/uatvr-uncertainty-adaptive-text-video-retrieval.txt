Abstract
With the explosive growth of web videos and emerg-ing large-scale vision-language pre-training models, e.g.,
CLIP, retrieving videos of interest with text instructions has attracted increasing attention. A common practice is to transfer text-video pairs to the same embedding space and craft cross-modal interactions with certain entities in spe-cific granularities for semantic correspondence. Unfortu-nately, the intrinsic uncertainties of optimal entity combina-tions in appropriate granularities for cross-modal queries are understudied, which is especially critical for modalities with hierarchical semantics, e.g., video, text, etc.
In this paper, we propose an Uncertainty-Adaptive Text-Video Re-trieval approach, termed UATVR, which models each look-up as a distribution matching procedure. Concretely, we add additional learnable tokens in the encoders to adap-tively aggregate multi-grained semantics for flexible high-level reasoning. In the refined embedding space, we rep-resent text-video pairs as probabilistic distributions where prototypes are sampled for matching evaluation. Com-prehensive experiments on four benchmarks justify the su-periority of our UATVR, which achieves new state-of-the-art results on MSR-VTT (50.8%), VATEX (64.5%), MSVD (49.7%), and DiDeMo (45.8%). The code is available at https://github.com/bofang98/UATVR. 1.

Introduction
With surging portable filming devices and emerging video media platforms, searching videos of interest with human instructions, typically as texts, has been a part of daily lives, which urgently requires effective and robust
∗Co-first authorship. This work was done when Bo Fang was a re-search intern at Baidu Inc.
†Corresponding author.
Figure 1. Motivation. A video has numerous descriptions con-taining different level information, (a) which shows inconsistent text-video correspondences in the common embedding space. The former three frames depict a ‘celebrate’ action, while the last three frames are about a ‘touchdown’. This video diversity thus makes the optimal text-video matching in uncertain granularities, which we call an uncertain matching problem. Moreover, previous deter-ministic works can only handle one-to-one text-video mappings, yet a realistic relationship between two modalities is one-to-many. (b) The above problems motivate our uncertainty-adaptive model through distribution matching procedures. text-video retrieval (TVR) techniques. Given a query text (video), TVR aims to find the most relevant video (text) in the database, which is typically overwhelmed with sophisti-cated vague semantic combinations varying with hierarchi-cal text structures or spatiotemporal video spans.
Recent breakthroughs in the large-scale image and/or text pre-training [48, 22, 61] benefit TVR significantly. A serial of seminal works employ a separated encoder archi-tecture to respectively project texts and videos into a pre-trained joint embedding space for compact cross-modal in-teraction [30, 4, 39, 31].
Since a video inherently contains information beyond texts, simply pooling all frames as a whole video expres-sion brings distraction during matching specific text enti-ties [39, 17]. Therefore, inspired by fine-grained image-text pre-training, e.g., FILIP [59] and ALBEF [32], multi-grained TVR paradigms are introduced to build multi-level cross-modal interactions with sentence-frame level [20, 34], word-frame level [52], or hierarchical correspondences in-cluding phrase-clip level [43, 25]. However, these methods are still far from satisfying in handling the intrinsic uncer-tainties of determining the optimal entity combinations with appropriate granularities during text-video matching.
In Fig. 1, we illustrate the uncertain matching problem in TVR. Since different frame/word combinations can plau-sibly correspond to semantics in various perspectives, given the same video, successful retrieval can achieve in varying granularities involving discrepant text-video entities, i.e., video-sentence matching, frame-sentence matching, frame-word matching, etc. Previous works determine particular cross-modal mapping strategies in certain granularities, yet none have studied the intrinsic uncertainties of optimal text-video entity combinations. Besides, existing deterministic cross-modal retrieval can only handle one-to-one mapping scenarios [12]. However, a video can be described by mul-tiple sentences typically (and vice versa), which formulates realistic one-to-many relationships.
In this paper, we propose a novel TVR framework to tackle the uncertainty problem in cross-modal match-termed Uncertainty-Adaptive Text-Video Retrieval ing, (UATVR). Generally, UATVR models each text-video lookup as a distribution matching procedure in comple-mentary deterministic and probabilistic views. It is mate-rialized upon word-frame token-wise interactions and con-sists of a dynamic semantic adaptation (DSA) module and a distribution-based uncertainty adaptation (DUA) module.
Concretely, DSA module enhances token-wise matching by introducing additional learnable multi-class tokens. We find these simple-yet-effective tokens can adaptively aggre-gate multi-grained video (or text) semantics during match-ing, thus allowing for flexible high-level reasoning. For
DUA, we represent samples from each modality as distribu-tions rather than feature points and convert the deterministic matching process to probabilistic distribution alignment. To simulate one-to-many text-video mappings, we pull proba-bilistic embeddings sampled from each distribution closer via multi-instance contrastive loss [41].
Our contributions can be summarized as (i) We innova-tively model video and text representations as probabilis-tic distributions and align them through multiple-instance contrast in their common embedding space for uncertainty-adaptive cross-modal matching. (ii) We propose a simple-yet-effective technique for flexible high-level reasoning by adding additional learnable tokens, allowing deterministic semantic uncertainty adaptation in videos/texts. (iii) Com-prehensive experimental explorations demonstrate the supe-riority of our UATVR, which obtains state-of-the-art results across public TVR benchmarks including MSR-VTT [57],
MSVD [56], VATEX [54], and DiDeMo [1]. 2.