Abstract
Parameter Efficient Tuning (PET) has gained attention for reducing the number of parameters while maintaining performance and providing better hardware resource sav-ings, but few studies investigate dense prediction tasks and interaction between modalities. In this paper, we do an in-vestigation of efficient tuning problems on referring image segmentation. We propose a novel adapter called Bridger to facilitate cross-modal information exchange and inject task-specific information into the pre-trained model. We also design a lightweight decoder for image segmentation.
Our approach achieves comparable or superior performance with only 1.61% to 3.38% backbone parameter updates, eval-uated on challenging benchmarks. The code is available at https://github.com/kkakkkka/ETRIS. 1.

Introduction
Referring image segmentation (RIS) aims to predict a mask for the target object described by a given natural lan-guage sentence based on the input image and text. This task is distinct from semantic segmentation, which assigns each pixel in an image with a label from a fixed word set. Instead,
RIS needs to recognize the objects indicated by the language expression, which is of greater complexity due to its arbitrary context length and involving an open-world vocabulary such as object names, attributes, positions, etc.
Recent studies [43, 12, 11] have shown the effectiveness of fine-tuning general-purpose pre-trained models for visual grounding. However, these approaches have a separate copy of fine-tuned model parameters for each dataset, making it expensive to deploy models across multiple scenarios. This issue is particularly significant for large-scale pre-trained
*Equal contribution
†Corresponding author
Figure 1: Previous method vs. our method. (a) The conven-tional method pre-trains visual language models on datasets with image-text pairs using self-supervised learning and fine-tunes them on downstream tasks. (b) We propose Bridger, an Adapter-like module that incorporates inductive biases and task-specific information into the pre-trained model. models, which now consist of hundreds of millions to tril-lions of parameters [31, 50, 5].
Therefore, we ask an essential question: can the model maintain a competitive performance with pre-trained back-bone network parameters fixed?. Various parameter-efficient training methods [16, 20, 15, 4, 9, 50] have been proposed to achieve a balance between parameter efficiency and perfor-mance. However, most of the existing methods are limited to either single-modal tasks [16, 20, 9] or simple classification tasks [15, 4, 50] with few studies focusing on dense predic-tion tasks and the interaction between different modalities,
which limits their generality.
We aim to adapt pre-trained vision-language models for referring image segmentation with comparable performance to full fine-tuning, but in a more parameter-efficient way, as demonstrated in Figure 1. This approach improves adapt-ability and eliminates the parameter inefficiencies and pro-hibitive expenses associated with previous methods that re-quire creating separate copies of fine-tuned backbone model parameters for each dataset. In detail, firstly, we introduce an additional network named Bridger that does not require pre-training and can be seamlessly integrated into the original architecture of the pre-trained model, where we introduce vision-specific inductive biases and facilitate interaction be-tween the dual encoder. There are two tailored modules for
Bridger: (i) a spatial prior module for capturing the local se-mantics (spatial prior) from feature maps of the intermediate layer and (ii) a cross-modal attention module that enables information exchange between the two modalities. Secondly, we designed a lightweight task-specific decoder for referring image segmentation to make further alignment on visual and linguistic features. Under this framework, the backbone network can be any general-propose (dual-encoder) model that is pre-trained on vision-language datasets, and we adopt
CLIP [40], a pre-trained image-text alignment model, as our vision and language encoders. As a result, utilizing ViT [13] and ResNet [19] as the visual backbone and updating only 1.61% to 3.38% parameters, our framework achieves com-parable or even better performance than previous methods which employ the same backbone for full-finetuning. Our main contributions are as follows:
• We propose to do an in-depth investigation of the parameter-efficient tuning problem on the dense predic-tion tasks. To the best of our knowledge, this is the first empirical study to date that considers this problem.
• We design a novel Bridger that can be seamlessly inte-grated into any pre-trained dual-encoder vision-language models to enhance and interact with their intermediate features. It incorporates vision-specific inductive biases and task-specific information and can be integrated with prompts, adapter, and their variants.
• We also propose a lightweight decoder for referring image segmentation to further align visual and linguistic features.
• Extensive experiments and analyses demonstrate the effec-tiveness of the proposed approach, where it achieves com-parable performance compared to existing full fine-tuning methods while updating only 1.61% to 3.38% parameters.1 2.