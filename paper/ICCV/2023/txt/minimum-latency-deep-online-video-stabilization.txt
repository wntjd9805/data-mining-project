Abstract
We present a novel camera path optimization framework for the task of online video stabilization. Typically, a sta-bilization pipeline consists of three steps: motion estimat-ing, path smoothing, and novel view rendering. Most pre-vious methods concentrate on motion estimation, proposing various global or local motion models.
In contrast, path optimization receives relatively less attention, especially in the important online setting, where no future frames are available. In this work, we adopt recent off-the-shelf high-quality deep motion models for motion estimation to re-cover the camera trajectory and focus on the latter two steps. Our network takes a short 2D camera path in a sliding window as input and outputs the stabilizing warp field of the last frame in the window, which warps the com-ing frame to its stabilized position. A hybrid loss is well-defined to constrain the spatial and temporal consistency.
In addition, we build a motion dataset that contains stable and unstable motion pairs for the training. Extensive ex-periments demonstrate that our approach significantly out-performs state-of-the-art online methods both qualitatively and quantitatively and achieves comparable performance to offline methods. Our code and dataset are available at https://github.com/liuzhen03/NNDVS. 1.

Introduction
Video stabilization methods aim at removing unwanted shaky motions of a video caused by unsteady moving cap-ture [6]. Traditional methods often take three main steps: 1) camera motion estimating for trajectory recovery; 2) camera path smoothing; and 3) steady frame synthesis. According to the adopted motion model in the first step, these meth-ods can be broadly classified as 2D or 3D. 2D methods adopt planar motion models such as affine [5], or homog-*Equal contribution
†Corresponding author
Figure 1. Illustration of the dataset synthesis process (a) and the synthesized camera path (b). We transfer an unstable path to a sta-ble path, creating a synthesized path, which shares high frequen-cies of the unstable path with low frequencies of the stable path. raphy [27], or mesh [25, 22, 33], or flow [26, 20, 41], while 3D methods resort to 3D depth [23, 17], or reconstructed points [18], or epipolar geometry constraints [3].
In comparison, deep learning-based methods learn stabi-lization models from stable and unstable video pairs [33, 41, 35] without explicit steps of motion estimation and smooth-ing. However, the results of deep methods are often visually inferior to those of traditional ones. A potential reason is that these methods try to learn the three steps all in a unified framework, each of which has different characteristics but is learned in the network blindly.
On the other hand, deep affine [12], deep homogra-phy [38, 8], and deep mesh models [39, 24, 21, 28] have demonstrated high-quality image registration, even under adverse cases. They are robust to scenes with large depth variation, large dynamic objects, and poor textures, fitting perfectly for the camera motion estimation. In this work, we argue that it is not necessary to enclose all three steps of the stabilization into the learning pipeline, but let the mo-tion estimation to the recent deep motion models [39, 24].
In this way, our network only focuses on learning to sta-bilize the shaky camera motion, which makes the learning process much more efficient and effective.
Camera path smoothing can be performed offline or on-line, where the former optimizes the path globally while the latter smooths the path on-the-fly. In other words, offline approaches have access to all past and future frames during optimization, as it often stabilizes a video after it is cap-tured. In contrast, online methods aim to stabilize a video during the capture. Note that, the concept of real-time dif-fers from that of online, as an online method must be in real-time whereas an offline method can run at real-time speed.
The main distinction is the availability of future frames.
Most existing methods are offline [25, 5, 4, 34, 26].
However, the online setting is critical as many applica-tions desire instant visual feedback based on live video streams [22, 33]. Normally, with limited or even no access to future frames, online methods cannot achieve equal sta-bility as offline methods, particularly in the suppression of the low-frequency camera shake, which frequently necessi-tates a long camera path. One may argue that online meth-ods can have all the past frames for processing. However, future motions are important, if not more important than the past. Because, for one thing, fast camera motions can happen at any time suddenly, e.g., quick rotation or zoom-ing. The reaction time is short. Inappropriate processing may either decrease the stability or create artifacts such as excessive cropping. For another, all the past frames have already been shown to the audience, meaning that they are fixed, and thus cannot be modified. We can only adjust the unstabled future frames to the frozen past ones.
To this end, we propose a deep online camera path smooth network that takes a pre-estimated short 2D cam-era trajectory in a sliding window as input and outputs the motion compensation warp field of the last frame in the win-dow for the stabilized view, which is the setting of online stabilization with the minimum latency. A video can be sta-bilized on the fly by applying our model repeatedly for ev-ery incoming frame. To deal with depth changes and mov-ing objects, we employ the mesh-based motion model [39] which demonstrates excellent robustness in various scenes.
Instead of directly inputting the mesh structure or mesh cells as local homographies to the network, we convert the esti-mated motion to a dense flow field, which can be fed into the convolutional neural network naturally. We show that it is a more convenient representation compared to other alter-natives, e.g., vertex sparse motion vectors or homography matrix, which cannot be easily adapted as input to CNNs.
Based on that, we further propose a hybrid loss, which con-sists of the motion-consistency loss, the shape-consistency loss, and the scale-preserving loss, to maintain the spatial coherence and temporal continuity of the stabilized video.
Besides, we create a motion dataset, MotionStab, to
In particular, we capture 110 stable train our network. videos with a cell phone mounted on a hand-held physical stabilizer. Then, we create a shaky video by transforming each frame of the stable video according to the motion of another irrelevant shaky video. In this way, the stable and unstable motion pairs can be constructed. Fig. 1 shows our idea. Note that, our dataset is different from DeepStab [33], which is captured by a stable and a shaky camera simultane-ously. We directly transfer the motion of a shaky video to a stable video. The contents of the two videos are not impor-tant but the motion does. That means DeepStab [33] offers frame pairs, while our MotionStab offers motion pairs. We feed the motions to the network instead of RGB frames.
The benefits are three folds: 1) image contents are di-verse, while motion mappings are much easier to learn; 2) motion as mesh is lightweight, so does the network; 3) the network can be more focused by excluding the task of the motion estimation. As a result, we show that this motion learning pipeline is effective enough, such that it can work with straightforward network architectures, e.g.,
UNet, without help from more advanced designs or sophis-ticated modules. Complex motion types can be learned by the network successfully, as long as they exist in the training data. As such, the network can be kept simple yet effective.
In summary, the main contributions of this paper are con-cluded as follows:
• We propose a novel deep camera path optimization framework for online video stabilization.
• We propose a hybrid loss to enable robust supervision for maintaining the spatial and temporal coherence of the stabilized video.
• We build a comprehensive MotionStab dataset that covers various motions for camera path optimization.
• We conduct extensive experiments to demonstrate the effectiveness of the proposed approach against existing state-of-the-art online and offline methods. 2.