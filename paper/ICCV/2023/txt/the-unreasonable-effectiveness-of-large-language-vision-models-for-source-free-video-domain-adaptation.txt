Abstract
Source-Free Video Unsupervised Domain Adaptation (SFVUDA) task consists in adapting an action recognition model, trained on a labelled source dataset, to an unla-belled target dataset, without accessing the actual source data.
The previous approaches have attempted to ad-dress SFVUDA by leveraging self-supervision (e.g., enforc-ing temporal consistency) derived from the target data it-self.
In this work, we take an orthogonal approach by exploiting “web-supervision” from Large Language-Vision
Models (LLVMs), driven by the rationale that LLVMs con-tain a rich world prior surprisingly robust to domain-shift. We showcase the unreasonable effectiveness of in-tegrating LLVMs for SFVUDA by devising an intuitive and parameter-efficient method, which we name Domain
Adaptation with Large Language-Vision models (DALL-V), that distills the world prior and complementary source model information into a student network tailored for the target. Despite the simplicity, DALL-V 1 achieves signifi-cant improvement over state-of-the-art SFVUDA methods. 1.

Introduction
Video analysis tasks, such as action recognition, have long been investigated in computer vision, due to the nu-merous applications, ranging from video surveillance to social robotics [50, 14, 29]. Major progress has been made in the last decade with the development of spe-cialized deep architectures, such as 3D CNNs [4, 3] and
Video Transformers [36], trained on large-scale annotated datasets. However, obtaining sufficient labelled training videos for real-world scenarios can be very costly and time-consuming.
In order to alleviate the burden of annotat-ing large scale datasets Video-based Unsupervised Domain
*Giacomo Zara and Alessandro Conti contributed equally. 1Code is available at https://github.com/giaczara/dallv
Figure 1: Performance over time by various methods on the Daily-DA video benchmark. A pre-trained LLVM (e.g.,
CLIP [31]) is surprisingly better when compared with Un-supervised Domain Adaptation (UDA), Source Free UDA (SFUDA) and Source Free Video-based UDA (SFVUDA) methods. Our proposed DALL-V that is built on top of
LLVM successfully outperforms all existing baselines.
Adaptation (VUDA) [1, 28, 2] have been introduced. The
VUDA methods are derived from the common idea of trans-ferring knowledge from a labelled source domain to an un-labelled target domain. While considering different strate-gies for adaptation, these approaches have all shown signifi-cant improvements in the robustness of learnt video models without requiring annotated target data.
In the last few years, the field of computer vision has also witnessed the emergence of a new generation of pow-erful deep architectures, trained on mammoth internet-scale image-text datasets [35]. These models, commonly known as foundation models [32, 37, 31, 10] or Large Language-Vision Models (LLVM), have achieved outstanding perfor-mance, and have become a cornerstone of modern com-puter vision research. In particular, recent works such as
CLIP [31] or FLAVA [37] have shown that rich visual repre-sentations for images can be learned from natural language descriptions as a form of supervision. These pre-trained
Figure 2: (a) In traditional SFVUDA a source-trained model is adapted to the unlabelled target dataset using self-supervision such as temporal consistency [44, 46] among frames. (b) LLVM (e.g., CLIP [31]) uses cosine similarity between the feature and language representation to predict the most probable class in a zero-shot (ZS) manner. (c) Our proposed SFVUDA solu-tion distills the ZS-CLIP and source model predictions, while introducing very few learnable parameters, namely adapters.
The means the network is trainable, while the means the network is frozen.
LLVMs are now publicly available and can be easily inte-grated into any recognition system. Lately, researchers have also shown that the LLVMs can be applied with success to the video domain and specifically to the supervised action recognition task [40, 41].
Despite the significant progress in the VUDA literature, to date, the role of LLVM in the context of action recogni-tion has been overlooked. We argue that the LLVMs have a lot to offer to the progress of VUDA methods, which is cur-rently untapped. Our work starts from a preliminary anal-ysis demonstrating that current sophisticated VUDA meth-ods can be largely outperformed by off-the-shelf publicly available LLVMs like CLIP, without needing explicit adap-tation (see Fig. 1). This observation highlights the need for a paradigm shift among VUDA methods. The benefit of LLVM models is so significant that it raises a pertinent question: If explicit alignment methodologies, traditionally pursued in VUDA, is truly the way forward? Instead, the primary research question becomes: How can we efficiently integrate the prior knowledge derived from LLVMs to adapt a model to the target domain for VUDA?
This paper presents the first approach in the VUDA lit-erature, which attempts to address this question specifi-cally.
In particular, we consider the challenging scenario of Source-Free Video Unsupervised Domain Adaptation (SFVUDA) [44, 46], which consists in the task of adapting an action recognition model trained on a labelled source do-main to an unlabelled target domain, without accessing the actual source data. The primary motivation for exploiting
LLVMs in SFVUDA is that we expect the generalization capabilities of these models to effectively serve the pur-pose of mitigating the effects of the domain shift existing between the data distributions of the source and the target domain. In particular, since in SFVUDA we do not have access to the source data but only the source pre-trained model, direct target adaptation may potentially lead to sig-nificantly poor performance, especially when the domain shift is large. On the contrary, the employment of LLVMs can efficiently counteract this negative effect owing to the wide range of visual domains observed during their training process. An additional motivation behind our approach lies in the fact that the rich visual representations derived from
LLVMs can efficiently compensate when additional modal-ities (e.g., optical-flow), which are known to help reduce domain gap in VUDA frameworks [27, 33], are unavailable.
Our approach, which we name as Domain Adaptation with Large Language-Vision models (or DALL-V in short), is a simple yet effective technique to combine the knowl-edge from the visual representation of a pre-trained LLVM with that derived from the source model and the target data (see Fig. 2c). DALL-V involves a two-stage pipeline: (i) in the first stage pseudo-labels are extracted from the
CLIP model and are subsequently used for adapting on the target dataset, and (ii) in the second stage an ensemble of
CLIP, source and target models are used to distill informa-tion into a student network. DALL-V introduces very few trainable parameters on top of CLIP and is realized with the help of domain-specific adapters. Despite its simplicity, our approach outperforms SFVUDA and even VUDA methods by a significant margin (+11.8% w.r.t. the best competitor).
In summary, our contributions are: (i) We show, for the first time in the literature that straightforward zero-shot
methods based on LLVMs vastly outperform the state-of-the-art approaches for SFVUDA. (ii) Building upon this observation, we propose DALL-V, a simple approach for
SFVUDA which optimally integrates information derived from LLVMs, from a pretrained source model and from un-labelled videos of the target domain. (iii) We perform exten-sive experiments on a total of 20 domain adaptation settings, demonstrating the effectiveness of DALL-V in SFVUDA. 2.