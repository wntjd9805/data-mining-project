Abstract
Monocular depth estimation is an ill-posed problem as the same 2D image can be projected from infinite 3D scenes.
Although the leading algorithms in this field have reported significant improvement, they are essentially geared to the particular compound of pictorial observations and camera parameters (i.e., intrinsics and extrinsics), strongly limit-ing their generalizability in real-world scenarios. To cope with this challenge, this paper proposes a novel ground embedding module to decouple camera parameters from pictorial cues, thus promoting the generalization capabil-ity. Given camera parameters, the proposed module gener-ates the ground depth, which is stacked with the input im-age and referenced in the final depth prediction. A ground attention is designed in the module to optimally combine ground depth with residual depth. Our ground embedding is highly flexible and lightweight, leading to a plug-in mod-ule that is amenable to be integrated into various depth es-timation networks. Experiments reveal that our approach achieves the state-of-the-art results on popular benchmarks, and more importantly, renders significant generalization improvement on a wide range of cross-domain tests. 1.

Introduction
Accurate depth acquisition is crucial for many robotics applications [23, 24, 30, 32] as depth provides pivotal infor-mation for onboard tasks ranging from perception [17], pre-diction [31] to planning [18]. Although range sensors (e.g.,
LiDAR) are widely used to produce precise depth mea-surements, there has been fast growing attention to cam-era based depth estimation from both academia and indus-try due to its portability and cost-effectiveness [3, 5, 6, 35].
A typical monocular depth estimation network adopts an encoder-decoder architecture, which can be trained in a su-pervised [5, 6] or self-supervised manner [9, 25, 36]. Most of the existing works in this field focus on designing more advanced network architectures [3, 20] or engineering more effective loss functions [6, 22]. Another line of research es-timates depth by exploiting stereo imagery [2, 14], which
*Authors contributed equally
Figure 1. Illustration of the ground depth produced by our ground embedding. Due to different camera parameters, (a) and (b) show one object with the same scale and depth is mapped to different images, while (b) and (c) show objects with different scales and depths generate the same image. In each case, the ground depth encodes the corresponding camera parameters and can be used to resolve the ill-posed problem in monocular depth estimation. however requires multiple calibrated cameras and is rela-tively more expensive and complicated. Compared with its stereo counterpart, the monocular depth estimation hinging on single cameras is more amenable for real-world deploy-ment such as autonomous driving [19, 26].
However, monocular depth estimation is inherently ill-posed or ambiguous. According to the classic pinhole cam-era model, an image captured by a camera is determined by the camera parameters (i.e., intrinsics and extrinsics), object scale, and object depth with respect to the camera optical center. Therefore, objects with different scales and depths captured by a camera could generate the same im-age; on the contrary, one object with the same depth cap-tured by cameras with different intrinsics and or extrinsics could generate different images. As a result, one image may correspond to multiple plausible depths, and one depth can be mapped to various images, as illustrated in Figure 1. Re-cently, most supervised methods resort to CNNs or Trans-formers to directly learn the absolute depth based on a sin-gle image from numerous labeled data [3, 6, 16, 20, 35]. Al-though these methods have achieved remarkable progress, they are essentially tailored toward the specific compound of pictorial cues (e.g., object scale, light and shade, and tex-ture gradient) and camera parameters on the particular train-ing data, which strongly limits their generalization capabil-ity. In addition, very few works have provided an in-depth investigation into what these networks have learned to con-duct monocular depth estimation, further undermining the guarantee of correct behaviors of these networks in cross-domain or unexpected scenarios. By inspecting synthetic images, [4] shows that the ground contact point of an object is primarily used to estimate its depth. Another work [13] finds that the region around vanishing point of a scene con-tributes to the vital cues for depth estimation.
In light of the above observations, we develop a ground embedding module for monocular depth estimation, which we term GEDepth. It is a plug-in module that is lightweight and flexible to be incorporated into various depth estima-tion networks. As illustrated in Figure 2, given camera pa-rameters, the module first computes ground depth, which is then fused with an input image to produce ground depth-aware features. Based on such features, the network gener-ates residual depth and ground attention map, and the latter selectively combines the former with the ground depth to form final depth prediction. Although starting from planar ground to formulate ground embedding, our module is not constrained to this assumption, but is practically designed to be adaptive to handle ground undulation in real-world sce-narios. Our approach decouples camera parameters from pictorial cues, thus improving the generalizability of depth estimation. This design also leads to an explicit utilization of ground, which reinforces the key information used for depth estimation as investigated in [4, 13]: (1) the ground depth together with the ground attention map facilitates the learning of ground contact points; (2) the vanishing line (ex-pressed in ground depth) coupled with the ground attention map locate the region around vanishing point readily.
We summarize our main contributions as follows. (1) To our knowledge, this work provides the first plug and play module through ground embedding, which is able to assist various depth estimation networks in decoupling camera pa-rameters from pictorial cues, and remarkably enhances their generalizability. (2) Our proposed module breaks the pla-nar ground assumption and is capable of tackling ground undulation in realistic scenes. (3) Extensive experiments demonstrate that our approach compares favorably against the competing algorithms, and meanwhile, achieves more robust performance on a variety of cross-domain evalu-ations. Our code and model will be made available at https://github.com/qcraftai/gedepth. 2.