Abstract
Open-vocabulary image segmentation is attracting in-creasing attention due to its critical applications in the real world. Traditional closed-vocabulary segmentation meth-ods are not able to characterize novel objects, whereas sev-eral recent open-vocabulary attempts obtain unsatisfactory results, i.e., notable performance reduction on the closed-vocabulary and massive demand for extra data. To this end, we propose OPSNet, an omnipotent and data-efficient framework for Open-vocabulary Panoptic Segmentation.
Specifically, the exquisitely designed Embedding Modula-tion module, together with several meticulous components, enables adequate embedding enhancement and informa-tion exchange between the segmentation model and the visual-linguistic well-aligned CLIP encoder, resulting in superior segmentation performance under both open- and closed-vocabulary settings with much fewer need of addi-tional data. Extensive experimental evaluations are con-ducted across multiple datasets (e.g., COCO, ADE20K,
Cityscapes, and PascalContext) under various circum-stances, where the proposed OPSNet achieves state-of-the-art results, which demonstrates the effectiveness and gen-erality of the proposed approach. The project page is https://opsnet-page.github.io. 1.

Introduction
The real world is diverse and contains numerous distinct objects. In practical scenarios, we inevitably encounter var-ious objects with different shapes, colors, and categories.
Although some of them are unfamiliar or rarely seen, to better understand the world, we still need to figure out the region and shape of each object and what it is. The ability to perceive and segment both known and unknown objects is natural and essential for many real-world applications like autonomous driving, robot sensing, and navigation, human-object interaction, augmented reality, healthcare, etc.
Lots of works have explored image segmentation and achieved great success [51, 17, 50, 8]. However, they are typically designed and developed on specific datasets (e.g.,
*Corresponding author
Figure 1. Visual comparisons of classical closed-vocabulary seg-mentation and our open-vocabulary segmentation. Models are trained on the COCO panoptic dataset. Categories like ‘printer’,
‘card index’, ‘dongle’, and ‘kangaroo’ are not presented in the
COCO concept set. Closed-vocabulary segmentation algorithms like Mask2Former [8] are not able to detect and segment new ob-jects (top middle) or fail to recognize object categories (bottom middle). In contrast, our approach is able to segment and recog-nize novel objects (top right, bottom right) for the open vocabulary.
COCO [28], ADE20K [53]) with predefined categories in a closed vocabulary, which assume the data distribution and category space remain unchanged during algorithm de-velopment and deployment procedures, resulting in notice-able and unsatisfactory failures when handling new envi-ronments in the complex real world, as shown in Fig. 1 (b).
To address this problem, open-vocabulary perception is densely explored for semantic segmentation and object de-tection. Some methods [15, 55, 16, 25, 49] use the visual-linguistic well-aligned CLIP [41] text encoder to extract the language embeddings of category names to represent each category, and train the classification head to match these language embeddings. However, training the text-image alignment from scratch often requires a large amount of data and a heavy training burden. Other works [13, 47] use both of the pre-trained CLIP image/text encoders to transfer the open-vocabulary ability from CLIP. However, as CLIP is not a cure-all for all domains and categories, although they are data-efficient, they struggle to balance the general-ization ability and the performance in the training domain.
[47, 12] demonstrate suboptimal cross-dataset results, [13] shows unsatifactory performance on the training domain.
Besides, their methods for leveraging CLIP visual features are inefficient. Specifically, they need to pass each proposal
into the CLIP image encoder to extract the visual features.
Considering the characteristics and challenges of the pre-vious methods, we propose OPSNet for Open-vocabulary
Panoptic Segmentation, which is omnipotent and data-efficient for both open- and closed-vocabulary settings.
Given an image, OPSNet first predicts class-agnostic masks for all objects and learns a series of in-domain query em-beddings. For classification, a Spatial Adapter is added af-ter the CLIP image encoder to maintain the spatial resolu-tion. Then Mask Pooling uses the class-agnostic masks to pool the visual feature into CLIP embeddings, thus the vi-sual embedding for each object can be extracted in one pass.
Afterward, we propose the key module named Embed-ding Modulation to produce the modulated embeddings for classification according to the query embeddings, CLIP em-beddings, and the concept semantics. This modulated final embedding could be used to match the text embeddings of category names extracted by the CLIP text encoder. Em-bedding Modulation combines the advantages of query and
CLIP embeddings, and enables adequate embedding en-hancement and information exchange for them, thus mak-ing OPSNet omnipotent for generalized domains and data-efficient for training. To further push the boundary of our framework, we propose Mask Filtering to improve the qual-ity of mask proposals, and Decoupled Supervision to scale up the training concepts using image-level labels to train classification and the self-constraints to supervise masks.
With these designs, OPSNet achieves superior perfor-mance on COCO [28], shows exceptional cross-dataset per-formance on ADE20K [53], Cityscapes [10], PascalCon-text [37], and generalizes well to novel objects in the open vocabulary, as shown in Fig. 1 (c).
In general, our contributions could be summarized as:
• We address the challenging open-vocabulary panop-tic segmentation task and propose a novel frame-work named OPSNet, which is omnipotent and data-efficient, with the assistance of the carefully designed
Embedding Modulation module.
• We propose several meticulous components like Spa-tial Adapter, Mask Pooling, Mask Filtering, and De-coupled Supervision, which are proven to be of great benefit for open-vocabulary segmentation.
• We conduct extensive experimental evaluations across multiple datasets under various circumstances, and the harvested state-of-the-art results demonstrate the ef-fectiveness and generality of the proposed approach. 2.