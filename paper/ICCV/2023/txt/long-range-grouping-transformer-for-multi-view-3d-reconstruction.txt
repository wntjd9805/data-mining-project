Abstract
Nowadays, transformer networks have demonstrated su-perior performance in many computer vision tasks.
In a multi-view 3D reconstruction algorithm following this paradigm, self-attention processing has to deal with intri-cate image tokens including massive information when fac-ing heavy amounts of view input. The curse of informa-tion content leads to the extreme difficulty of model learn-ing. To alleviate this problem, recent methods compress the token number representing each view or discard the at-tention operations between the tokens from different views.
Obviously, they give a negative impact on performance.
Therefore, we propose long-range grouping attention (LGA) based on the divide-and-conquer principle. Tokens from all views are grouped for separate attention operations. The tokens in each group are sampled from all views and can provide macro representation for the resided view. The richness of feature learning is guaranteed by the diversity among different groups. An effective and efficient encoder can be established which connects inter-view features us-ing LGA and extract intra-view features using the standard self-attention layer. Moreover, a novel progressive upsam-pling decoder is also designed for voxel generation with rel-atively high resolution. Hinging on the above, we construct a powerful transformer-based network, called LRGT. Ex-perimental results on ShapeNet verify our method achieves
Code
SOTA accuracy in multi-view reconstruction. is available at https://github.com/LiyingCV/
Long-Range-Grouping-Transformer. 1.

Introduction 3D reconstruction, the problem to recover the shape of an object according to its several view images, is an es-sential research topic involving the field of computer vi-sion and computer graphics. One of the main challenges is how to extract the features from multiple images for gen-erating the corresponding object with 3D representation. At present, deep learning reconstructors have provided three
*Equal contribution. Email: {lyyang69, garyzhu1996}@gmail.com
†Corresponding author. Email: yyliang@must.edu.mo kinds of solutions, including RNN-based methods [1, 2],
CNN-based methods [3, 4, 5, 6, 7, 8, 9, 10] and transformer-based methods [11, 12, 13, 14]. In this work, we focus on the transformer network and aim to improve the accuracy and robustness of multi-view 3D reconstruction with voxel representation.
Vision transformer (ViT) [15] promotes the approach to extract features from an image employing transformer ar-chitecture. An image is split into fixed-size patches and then using attention operators learn the association between
If follow-them to explore an appropriate representation. ing this paradigm to establish a multi-view 3D reconstruc-tion algorithm, an intuitive approach like [16] is to build attention layers to connect full-range tokens from all views, shown in Figure 1a. However, it is extremely difficult for the model to face heavy amounts of view input, as attention operators have to predict the potential importance weights of intricate tokens from views. The curse of information
It content increases the complexity of model encoding. is necessary to provide relatively large-scale data to sup-port adequate training, but the commonly used datasets for multi-view reconstruction are far from satisfying such re-quirements.
The previous works design three types of strategies to avoid this problem. 1) Separated-stage strategy [13] indi-vidually extract the feature from each view and then fusion them. It is simplified in structure but weak in mining the inter-view relationships. 2) Blended-stage strategy [11, 12] compress the number of tokens to represent each view and make the attention layers to learn the association between all tokens from different views. They enhance the inter-view correlation but lose a certain representation capability for each view. 3) Alternated-stage strategy [14] employs de-coupling to alternate the feature extraction from intra-view and inter-view tokens. They discard the attention opera-tions between the tokens from different views but insert other modules to achieve similar purposes, which brings extra computation. The efficiency is significantly dragged down, especially for a large number of image inputs.
Considering these shortcomings, we hold that grouping tokens in attention operations is a more reasonable solu-tion. The role of full-range attention can be equivalent to
(a) FRA (b) TGA (c) SGA (d) LGA (Ours)
Figure 1: Illustration of different attention strategies for processing multi-view input. (a) Full-Range Attention (FRA); (b) Token-Range
Grouping Attention (TGA); (c) Short-Range Grouping Attention (SGA); (d) Long-Range Grouping Attention (LGA). Tokens in the same dashed box or on the same dashed line are divided into the same group for attention operation. It means that they will build the correlations. a combination of two attention layers. ViT can be used as the basic architecture of encoder independent and parallel processing on each view, while some of the original atten-tion layers are replaced by special grouping attention oper-ations for multi-branch. Adopting the principle of divide-and-conquer, the tokens from different views are divided into some groups under certain rules. The amount of tokens processed by the attention layer is greatly reduced, thus al-leviating the learning difficulty of the model.
For the ambiguous associations between view inputs in the multi-view reconstruction task, it is necessary to select a suitable grouping strategy for the special attention opera-tion. There are some different grouping strategies in the pre-vious research works about transformer. The token-range grouping attention as shown in Figure 1b, which assigns tokens with the same location from different views as a group, ignores the auxiliary role of the intra-view token-to-token association in building inter-view dependencies.
The short-range grouping attention as shown in Figure 1c (e.g., [17]) increases the connectivity between local tokens in each view. However, it is hard to construct long-distance associations and only reinforces the sensitivity of tokens towards local information, hence it is applicable to multi-input problems with temporal coherence such as video.
For our task, we propose the long-range grouping atten-tion (LGA) as shown in Figure 1d. The tokens from the same view in each group can provide a certain macro repre-sentation for their view but are not limited to local features.
To further enhance differences between the tokens from dif-ferent views, we also introduce the inter-view feature signa-tures (IFS) to assist the attention processing. In addition, we propose a novel progressive upsampling decoder that integrates the transposed convolution into the transformer block. It fully exploits the self-attention to mine features in the relatively high-resolution voxel space.
The contributions can be summarized as follows:
• We propose the long-range grouping attention which can simply and efficiently establish the correlations be-tween different images. A novel encoder for handling multi-view input is formed by integrating this attention operator into a standard transformer architecture.
• We overcome the difficulty of transformer block work-ing on the relatively high-resolution voxel directly to propose a progressive upsampling decoder that can strengthen the reconstruction ability.
• Experimental results on ShapeNet [18] demonstrate that our method outperforms other SOTA methods in multi-view reconstruction. Additional experiment re-sults on Pix3D [19] also verify its effectiveness on real-world data. 2.