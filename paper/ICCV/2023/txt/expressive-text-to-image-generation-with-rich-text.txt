Abstract outperforms strong baselines with quantitative evaluations.
Plain text has become a prevalent interface for text-to-image synthesis. However, its limited customization options hinder users from accurately describing desired outputs.
For example, plain text makes it hard to specify continu-ous quantities, such as the precise RGB color value or im-portance of each word. Furthermore, creating detailed text prompts for complex scenes is tedious for humans to write and challenging for text encoders to interpret. To address these challenges, we propose using a rich-text editor sup-porting formats such as font style, size, color, and footnote.
We extract each word’s attributes from rich text to enable local style control, explicit token reweighting, precise color rendering, and detailed region synthesis. We achieve these capabilities through a region-based diffusion process. We first obtain each word’s region based on attention maps of a diffusion process using plain text. For each region, we en-force its text attributes by creating region-specific detailed prompts and applying region-specific guidance, and main-tain its fidelity against plain-text generation through region-based injections. We present various examples of image generation from rich text and demonstrate that our method 1.

Introduction
The development of large-scale text-to-image generative models [52, 56, 54, 28] has propelled image generation to an unprecedented era. The great flexibility of these large-scale models further offers users powerful control of the generation through visual cues [4, 17, 77] and textual in-puts [7, 19]. Without exception, existing studies use plain text encoded by a pretrained language model to guide the generation. However, in our daily life, it is rare to use only plain text when working on text-based tasks such as writing blogs or editing essays. Instead, a rich text editor [68, 71] is the more popular choice providing versatile formatting options for writing and editing text. In this paper, we seek to introduce accessible and precise textual control from rich text editors to text-to-image synthesis.
Rich text editors offer unique solutions for incorporat-ing conditional information separate from the text. For ex-ample, using the font color, one can indicate an arbitrary color. In contrast, describing the precise color with plain text proves more challenging as general text encoders do
not understand RGB or Hex triplets, and many color names, such as ‘olive’ and ‘orange’, have ambiguous meanings.
This font color information can be used to define the color of generated objects. For example, in Figure 1, a specific yellow can be selected to instruct the generation of a mar-ble statue with that exact color.
Beyond providing precise color information, various font formats make it simple to augment the word-level informa-tion. For example, reweighting token influence [19] can be implemented using the font size, a task that is difficult to achieve with existing visual or textual interfaces. Rich text editors offer more options than font size – similar to how font style distinguishes the styles of individual text el-ements, we propose using it to capture the artistic style of specific regions. Another option is using footnotes to pro-vide supplementary descriptions for selected words, simpli-fying the process of creating complex scenes.
But how can we use rich text? A straightforward im-plementation is to convert a rich-text prompt with detailed attributes into lengthy plain text and feed it directly into ex-isting methods [54, 19, 7]. Unfortunately, these methods struggle to synthesize images corresponding to lengthy text prompts involving multiple objects with distinct visual at-tributes, as noted in a recent study [12]. They often mix styles and colors, applying a uniform style to the entire im-age. Furthermore, the lengthy prompt introduces extra dif-ficulty for text encoders to interpret accurate information, making generating intricate details more demanding.
To address these challenges, our insight is to decompose a rich-text prompt into two components (1) a short plain-text prompt (without formatting) and (2) multiple region-specific prompts that include text attributes, as shown in
Figure 2. First, we obtain the self- and cross-attention maps using a vanilla denoising process with the short plain-text prompt to associate each word with a specific region. Sec-ond, we create a prompt for each region using the attributes derived from rich-text prompt. For example, we use “moun-tain in the style of Ukiyo-e” as the prompt for the region cor-responding to the word “mountain” with the attribute “font style: Ukiyo-e”. For RGB font colors that cannot be con-verted to the prompts, we iteratively update the region with region-based guidance to match the target color. We apply a separate denoising process for each region and fuse the pre-dicted noises to get the final update. During this process, regions associated with the tokens that do not have any for-mats are supposed to look the same as the plain-text results.
Also, the overall shape of the objects should stay unchanged in cases such as only the color is changed. To this end, we propose to use region-based injection approaches.
We demonstrate qualitatively and quantitatively that our method generates more precise color, distinct styles, and accurate details compared to plain text-based methods. 2.