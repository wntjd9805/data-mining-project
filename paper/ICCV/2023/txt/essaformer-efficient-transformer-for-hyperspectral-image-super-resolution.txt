Abstract
Single hyperspectral image super-resolution (single-HSI-SR) aims to restore a high-resolution hyperspectral im-age from a low-resolution observation. However, the pre-vailing CNN-based approaches have shown limitations in building long-range dependencies and capturing interac-tion information between spectral features. This results in inadequate utilization of spectral information and artifacts after upsampling. To address this issue, we propose ES-SAformer, an ESSA attention-embedded Transformer net-work for single-HSI-SR with an iterative refining struc-ture. Specifically, we first introduce a robust and spectral-friendly similarity metric, i.e., the spectral correlation coef-ficient of the spectrum (SCC), to replace the original atten-tion matrix and incorporates inductive biases into the model to facilitate training. Built upon it, we further utilize the kernelizable attention technique with theoretical support to form a novel efficient SCC-kernel-based self-attention (ESSA) and reduce attention computation to linear com-plexity. ESSA enlarges the receptive field for features after upsampling without bringing much computation and allows the model to effectively utilize spatial-spectral information from different scales, resulting in the generation of more natural high-resolution images. Without the need for pre-training on large-scale datasets, our experiments demon-strate ESSA’s effectiveness in both visual quality and quan-titative results. The code will be released at ESSAformer. 1.

Introduction
Hyperspectral imaging (HSI) involves densely sampling spectral features with many narrow bands to encode rich spectral and spatial structure information for material dif-It has been widely used in various applica-ferentiation.
*Corresponding author tions. Hyperspectral image super-resolution (HSI-SR) aims to generate high-resolution HSI from low-resolution HSI and can be categorized into two approaches: single-HSI-SR
[25, 32, 19, 54] and fusion-based HSI-SR [30, 64, 34, 24].
This paper focuses on the challenging single-HSI-SR task, which aims to restore high-resolution HSI from a single low-resolution HSI without auxiliary images.
Conventional methods for single-HSI-SR involve de-signing a mapping function between low-resolution and high-resolution HSI using hand-crafted priors such as low-rank approximation and sparse coding [18, 40, 17, 13].
However, with the fast development of deep learning, pow-erful convolutional neural networks (CNNs) have led to sig-nificant progress in the single-HSI-SR task [25, 27, 32, 48, 23, 39, 19]. These CNN-based approaches usually use deep neural networks to formulate and learn the mapping func-tion in an end-to-end manner using abundant training data pairs. As a result, they achieve significant improvements in both visual quality and quantitative metrics.
However, the CNNs methods show limitations in solving the single-HSI-SR task. There exists a significant amount of long-range information in high-dimensional data of HSI, while the most prevailing CNNs focus on local features cap-tured by the convolutional kernels [25, 23, 39, 19, 27, 32, 48, 57, 55, 56]. The limited receptive field in the network can thus hinder the models’ representation ability. Conse-quently, unwished artifacts, such as the blocking ones, may appear and affect the model’s generation quality. To address this issue, we make an attempt to propose a Transformer model for the single-HSI-SR task. The attention mechanism introduced in Vision Transformers allows them to capture long-range dependencies and provide powerful representa-tions, leading to superior performance compared to CNNs in many vision tasks [7, 66, 63, 59, 47, 9, 65, 60].
While the long-range dependency advantage of Vision
Transformers can potentially address the aforementioned is-sues, it cannot be directly applied to single-HSI-SR. Firstly,
Vision Transformers typically require a large amount of
data to learn inductive biases and produce reliable results.
However, the difficulty in obtaining HSIs limits the col-lection of large-scale datasets, which poses a particular challenge compared to the millions of images available in
RGB image datasets, thus hindering the training of Vision
Transformers. Secondly, while Transformers can handle long-range dependencies, the self-attention process has a quadratic computation complexity of O(N 2) with respect to the token sequence N . This results in a massive com-putation burden for the network, particularly for ultra-high resolution HSI.
To address the above issues, we propose a novel Trans-former model called ESSAformer. The ESSAformer is de-signed with several adaptations. First, it utilizes an iterative downsampling and upsampling strategy to capture global and local information at different scales and encode the de-tailed content of the hyperspectral images. Second, we pro-pose to replace the conventional dot product (cosine simi-larity) with the robust and spectral-friendly spectral corre-lation coefficient of the spectrum, called SCC. Compared to traditional cosine similarity, the SCC has desirable prop-erties such as spectral-wise shifting and scaling equiva-lence. This makes the model insensitive to amplitude-level changes in spectral curves caused by occlusions or shad-ows. As a result, SCC brings inductive biases into mod-els, facilitates training efficiency, and even enables from-scratch training of Transformer models on small datasets.
Third, we propose formulating the attention as kernelized ones to decrease the computation burden. Technically, we integrate SCC into a nonlinear square exponential kernel, i.e., Mercer’s kernel, and then express SCC as a dot prod-uct of two individual terms according to the Mercer the-orem. Subsequently, we change the multiplication order of self-attention, i.e., multiplying keys and values first and then queries, and thus lower the attention complexity from quadratic O(N 2) to linear O(N ). Such a pipeline sig-nificantly relieves the computation burden since the token number N for high-resolution HSIs is usually significantly long. Consequently, we propose the novel SCC-kernel-based self-attention, called ESSA, and a new ESSAformer
Vision Transformer architecture for the single-HSI-SR task.
Thanks to the proposed ESSA, our model efficiently enlarges the receptive field without imposing a significant computation burden, thus allowing the features to attend to the entire feature map at each layer and gather sufficient in-formation. Consequently, ESSA effectively addresses the artifact issues caused by limited and inconsistent receptive fields between any two pixels in hyperspectral images, re-sulting in more natural high-resolution HSI generation. Un-like other attention variants [58, 45], our ESSA does not bring extra parameters and effectively introduces inductive biases. Consequently, the ESSAformer Transformer model obtains state-of-the-art performance on three public datasets without the need for pretraining.
In summary, this paper makes three main contributions.
First, we introduce the use of Vision Transformer for the single-HSI-SR task and propose the ESSAformer model with strong learning ability. Second, we present a novel and efficient SCC-kernel-based self-attention method called
ESSA. The approach significantly reduces the computation and data-hungry issues in the original Vision Transformer and helps the model better fit the single-HSI-SR task. Third, extensive experiments have been conducted to analyze the proposed model thoroughly, and the state-of-the-art perfor-mance on three popular datasets demonstrates its superior-ity regarding both visual quality and objective metrics. 2.