Abstract
Domain adaptive Object Detection (DAOD) leverages a labeled domain (source) to learn an object detector gen-eralizing to a novel domain without annotation (target).
Recent advances use a teacher-student framework, i.e., a student model is supervised by the pseudo labels from a teacher model. Though great success, they suffer from the limited number of pseudo boxes with incorrect predictions caused by the domain shift, misleading the student model to get sub-optimal results. To mitigate this problem, we pro-pose Masked Retraining Teacher-student framework (MRT) which leverages masked autoencoder and selective retrain-ing mechanism on detection transformer. Specifically, we present a customized design of masked autoencoder branch, masking the multi-scale feature maps of target images and reconstructing features by the encoder of the student model and an auxiliary decoder. This helps the student model cap-ture target domain characteristics and become a more data-efficient learner to gain knowledge from the limited number of pseudo boxes. Furthermore, we adopt selective retrain-ing mechanism, periodically re-initializing certain parts of the student parameters with masked autoencoder refined weights to allow the model to jump out of the local opti-mum biased to the incorrect pseudo labels. Experimental results on three DAOD benchmarks demonstrate the effec-tiveness of our method. Code can be found at https:// github.com/JeremyZhao1998/MRT-release. 1.

Introduction
Object detection has a wide range of real-world appli-cation scenarios, and has been deeply studied in computer vision researches. CNN-based [35, 32, 40] and transformer-based [3, 51] detectors have shown great success in chal-lenging benchmarks. However, they suffer from domain
*Corresponding author (a) mAP under limited boxes (b) Category AP during training (a) Performance under limited number of box an-Figure 1. notations. We adopt MAE in training target images with ground truth labels, but manually reduce the amount of bounding boxes.
MAE boosts the performance by a large margin under limited boxes. (b) Performance of Category “bicycle” during training.
The performance declines rapidly due to the local optimum caused by incorrect pseudo boxes. With the help of retraining (every 40 epochs), the model is able to jump out of the local optimum. shift where there is an obvious distribution gap between the pretraining data and the deployed environment.
To mitigate the performance drop caused by domain shift without extra annotation, unsupervised domain adaptation (UDA) has been studied in classification, segmentation and object detection tasks, where the model is trained on a la-beled source domain and an unlabeled target domain, and is expected to generalize well on the target domain. As a branch of UDA, unsupervised domain adaptive object de-tection (DAOD) researches [9, 49, 29, 48] utilize numerous techniques such as adversarial alignment, image-to-image translation, GNNs and mean teacher training, widely im-proving domain adaptation performance of object detectors.
Among these approaches, [29, 48, 5, 20] use a teacher-student framework where a teacher model produces pseudo labels of the unlabeled target images to supervise a stu-dent model, and has achieved significant performance gains.
However, the teaching process faces the challenge of low-quality pseudo labels in which there are limited number of pseudo boxes and incorrect predictions caused by the do-main shift. Under such framework, pseudo labels are se-lected from outputs of the teacher model by a threshold of confidence scores. Selecting large amounts of pseudo labels with lower threshold brings too many incorrect predictions, degrading the performance, while with higher threshold, the limited number of pseudo boxes provides sub-optimal su-pervision. Blue bars of Figure 1(a) shows that even su-pervised by ground truth labels without domain shift, the performance drops significantly when the amount of box annotations are reduced. Anyhow, incorrect predictions al-ways exist, leading the model to get stuck at the local op-timum. As is shown in Figure 1(b), in the later stage of training, performance of some categories declines rapidly (the blue curve) due to the growing number of incorrect pseudo boxes. Though [29] utilizes adversarial alignment and weak-strong augmentation to minimize the false posi-tive ratio of pseudo labels, they ignore the sub-optimal su-pervision of the limited number of pseudo boxes, and the impact of incorrect pseudo labels which always exist.
To address this issue, we propose Masked Retraining
Teacher-student framework (MRT) which is built on the baseline of adaptivee teacher-student framework for De-formable DETR[51] detector, leveraging masked autoen-coder (MAE) and selective retraining mechanism.
Unlike pretraining MAEs[19, 41] which leverage large-scale training data under a pretrain-finetune paradigm, we present a customized design of MAE branch, randomly masking portions of multi-scale feature maps of the target images and reconstructing the missing features from their contexts by the encoder of the student model and an auxil-iary decoder, simultaneously with the detection loss. As a self-supervised task on target images, MAE leads the trans-former encoder to gain more intimate knowledge of the tar-get domain from the limited number of pseudo boxes. Em-pirically, we observe that MAE helps the model encode bet-ter features, improving the performance under any amount of supervision and achieving larger gains when fewer box annotations are provided, as is shown in Figure 1(a).
Furthermore, as the student model is sensitive to the pseudo label noise, we adopt a simple yet effective selec-tive retraining mechanism. Specifically, we periodically re-initialize certain parts of the student parameters to al-low the model to jump out of the local optimum biased to the incorrect pseudo labels. Unlike existing retraining ap-proaches [18, 34] that ignore the quality of re-initialization weights, we re-initialize the student model with MAE re-fined weights to avoid low-quality student weights to impact the teacher model through EMA. In teacher-student frame-work, an enhanced teacher model helps the student recover rapidly after the re-initialization. As is shown in the orange curve of Figure 1(b), retraining recovers the performance that has been corrupted by noisy pseudo labels.
We summarize the contribution of this paper as follow: 1) We propose a novel Masked Retraining Teacher-student framework(MRT) for training domain adaptive detection transformers, which is built on the adaptive teacher-student framework baseline and overcomes the problem of low-quality pseudo labels. 2) To the best of our knowledge, we are the first to present that masked autoencoder is a data-efficient domain adapter which helps the model bet-ter capture domain characteristics, and the first to adopt se-lective retraining mechanism in teacher-student framework to help the model jump out of local optimums. 3) Our method outperforms existing approaches by a large margin and achieves state-of-the-art on three DAOD benchmarks. 2.