Abstract
This paper proposes a fine-grained self-localization method for outdoor robotics that utilizes a flexible num-ber of onboard cameras and readily accessible satellite im-ages. The proposed method addresses limitations in exist-ing cross-view localization methods that struggle to handle noise sources such as moving objects and seasonal vari-ations.
It is the first sparse visual-only method that en-hances perception in dynamic environments by detecting view-consistent key points and their corresponding deep features from ground and satellite views, while removing off-the-ground objects and establishing homography trans-formation between the two views. Moreover, the proposed method incorporates a spatial embedding approach that leverages camera intrinsic and extrinsic information to re-duce the ambiguity of purely visual matching, leading to improved feature matching and overall pose estimation ac-curacy. The method exhibits strong generalization and is robust to environmental changes, requiring only geo-poses as ground truth. Extensive experiments on the KITTI and
Ford Multi-AV Seasonal datasets demonstrate that our pro-posed method outperforms existing state-of-the-art meth-ods, achieving median spatial accuracy errors below 0.5 meters along the lateral and longitudinal directions, and a median orientation accuracy error below 2◦ 1. 1.

Introduction
Accurate self-localization is a fundamental problem in mobile robotics, particularly in the context of autonomous driving. While Global Positioning System (GPS) is a widely adopted solution, its accuracy hardly meets the strin-gent requirements of autonomous driving [20]. Real-Time
Kinematic (RTK) positioning systems provide an alterna-tive by correcting GPS errors, but their implementation is hindered by the need for signal reference stations [13], ren-dering them an expensive solution. On the other hand, odometry [18, 4, 37, 32] or simultaneous localization and mapping (SLAM) [17, 11, 25, 32] methods can generate 1Our project page is https://shanwang-shan.github.io/PureACL-website/ (a) Query (b) Reference
Figure 1: (a) Query ground view (onboard camera) images (front, rear, left, and right). (b) reference satellite image.
The initial and ground truth poses, and FoV of cameras are shown in (red) and (green), respectively. accurate short-term trajectories, however, they experience drift accumulation over time that can only be alleviated through loop closures if the agent’s trajectories overlap.
Lastly, other self-localization techniques [35, 15, 31, 21] that rely on a pre-constructed 3D High Definition (HD) maps face limitations in terms of the extensive time and re-sources required for map acquisition and maintenance.
Using off-the-shelf satellite images as ready-to-use maps to achieve cross-view localization brings an alternative and promising way for low-cost localization. However, due to the significant disparity between overhead views captured by satellites and views seen by robots, cross-view localiza-tion is more challenging than traditional methods. To ad-dress this, it is crucial to purify view-consistent features that can support the localization process. Furthermore, satel-lite views can be captured at different times, leading to variations in seasonal and temporal conditions. The cross-view consistent purification can also minimize the impact of moving and seasonal objects.
Most previous cross-view localization methods [24, 10, 14, 29, 23, 38] approach the task as an image retrieval prob-lem, leading to coarse localization accuracy that is inferior to commercial GPS which can achieve an error of up to 4.9 meters in open sky conditions [30]. In contrast, our method utilizes a coarse pose that is easily obtainable from the Au-tonomous Vehicles system, to estimate the fine-grained 3-DoF (lateral, longitudinal, yaw) pose of the robot. This is accomplished through visual cross-view matching, utiliz-ing ground-view images captured by onboard cameras and a
Our fine-grained visual spatially-consistent satellite map. Additionally, our method supports multiple camera inputs, which extend the field of view of the query robot. The setting is illustrated in Fig. 1. localization method utilizes sparse (keypoint) feature matching, a departure from prior methods that rely on dense feature matching. To reduce the inherent ambiguity in purely visual matching, the method incorporates a camera intrinsic and extrinsic aware spa-tial embedding. Homography transformation is used to es-tablish correspondences between the two views. An on-ground confidence map is employed to ensure the valid-ity of the transformation and eliminate off-the-ground ob-jects. Additionally, a view consistency confidence map is utilized to mitigate the impact of moving objects and view-point variation. The localization process begins with the extraction of spatially aware deep features and the gener-ation of view-consistent, on-ground confidence maps for both views. View-consistent key points are then detected from the ground view confidence map and matched with their corresponding points in the satellite view. The opti-mal pose is determined through an iterative search using a differentiable Levenberg-Marquardt (LM) algorithm.
Using Google Maps [8] as the satellite view, we evaluate our method on two datasets: the Ford Multi-AV Seasonal (FMAVS) [1] and the KITTI Datasets [7].
The results demonstrate the superiority of our proposed method, achieving mean localization error of less than
{0.14m, 3.57◦} on KITTI with one front-facing onboard camera, and less than {0.88m, 0.74◦} on FMAVS with four surrounding onboard cameras.
We summarize our contributions as below:
• the first sparse visual-only cross-view localization method that estimates accurate pose with low spatial and angular errors.
• a view-consistent on-ground key point detector that re-duces the impact of dynamic objects and viewpoint variations, as well as removes off-the-ground objects.
• a spatial embedding that fully utilizes camera intrinsic and extrinsic information to improve the extraction of spatially aware visual features. pose by registering Radar scans on a satellite image. This method was later extended to a self-supervised learning framework in [28]. Another work [27] matches the top-down representation of a LiDAR scan with 2D points de-tected from satellite images. These methods have limita-tions and are only effective in environments with strong prior structure knowledge, failing in general, non-urban en-vironments.
[2] performs localization on bird’s eye view (BEV) LiDAR intensity maps using deep feature match-[34] ing between LiDAR scan and the intensity map. extends this method by incorporating compressed binary maps. Hybrid sensor solutions have also been explored, such as in [16] where an aerial robot achieves global lo-calization through the use of egocentric 3D semantically la-belled LiDAR, IMU, and visual information. CSLA [6] and
SIBCL [33] extract visual features from ground and satellite images and use LiDAR points to establish correspondence between the two views. CSLA [6] aims to estimate 2-DoF translation, while SIBCL [33] aims to estimate 3-DoF pose, including an additional orientation. All these methods criti-cally rely on depth information to build the correspondence across the two views. In contrast, our method is a visual-only solution that aims to achieve comparable localization accuracy using cheaper commodity sensors.
Visual Accurate Cross-view Localization. Most visual-only cross-view localization methods rely on homography transformations of the ground plane, as they lack reliable depth information.
[36] aims to estimate 2-DoF transla-tion using similarity matching and produces a dense spatial distribution to address localization ambiguities. HighlyAc-curate [22] projects satellite features into the ground view and optimizes the robot pose through dense feature match-ing. One of its drawbacks is the limited ability to effec-tively eliminate outliers, such as noise caused by off-the-ground objects (which violates the assumption of homogra-phy transformation of the ground plane) and dynamic ob-jects. As a result, their overall performance is limited. In contrast, our method constructs geometric correspondences across sparse view-consistent on-ground keypoints, ensur-ing that the pose estimation is based on accurate correspon-dences leading to improved precision.
• a multi-camera fusion approach that significantly im-proves localization accuracy. 3. Our Method 2.