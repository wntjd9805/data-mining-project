Abstract
We present PARQ – a multi-view 3D object detector with transformer and pixel-aligned recurrent queries. Unlike previous works that use learnable features or only encode 3D point positions as queries in the decoder, PARQ lever-ages appearance-enhanced queries initialized from refer-ence points in 3D space and updates their 3D location with recurrent cross-attention operations.
Incorporating pixel-aligned features and cross attention enables the model to encode the necessary 3D-to-2D correspondences and capture global contextual information of the input images.
PARQ outperforms prior best methods on the ScanNet and
ARKitScenes datasets, learns and detects faster, is more ro-bust to distribution shifts in reference points, can leverage additional input views without retraining, and can adapt in-ference compute by changing the number of recurrent itera-tions. Code is available at https://ymingxie.github.io/parq. 1.

Introduction
The world is composed of objects positioned in 3D space. Humans have an innate ability to perceive 3D scenes which allows them to interact with their surroundings. For machines, understanding all objects in 3D space from one or few images enables new applications of embodied intel-ligence such as in robotics and assistive technology. The problem is defined by the task of 3D object detection: given a few images of a scene, detect all objects in 3D. 3D object detection unites two distinct problems of com-puter vision, 2D recognition, and 3D reconstruction. Sim-ilar to 2D recognition, appearance cues in the input views drive categorical predictions. Similar to 3D reconstruction, the model needs to reason about the 3D position of ob-jects from only 2D views. Modern learning-based meth-ods build on the traditional multi-view stereopsis [45] and
Structure from Motion (SfM) [14, 39] and lift objects to 3D via optimization [24, 23, 22, 31] or volumetric repre-sentations [40, 41]. To do so, they require tens or hun-∗ Equal advising.
Figure 1. PARQ leverages appearance-enhanced queries initial-ized from 3D points and updates their 3D locations to the 3D ob-ject center with a recurrent PARQ layer in the decoder. The PARQ layer chains a transformer decoder layer and a detection head. dreds of views observing the whole scene. However, in many real-world applications, like robotics, models are re-quired to make predictions online and often in real-time from just a short video snippet. In this work, we tackle on-line 3D object detection from just a few (e.g., 3) consecutive views with known camera poses extracted by visual-inertial
SLAM systems [5, 35, 1].
Learning 3D object detectors is challenging as the 3D prediction space is extensive while objects occupy only a small portion. Geometry is important. An object visible in the different input views occupies the same 3D world loca-tion and vice versa, a 3D object connects to 2D locations on the input images consistent with its camera projections.
This observation prunes the vast prediction space. In ad-dition, object appearance changes with viewpoint changes.
For instance, the appearance of the chair in Fig. 1 changes as the camera moves. This suggests that appearance and ge-ometry are critical. Motivated by this insight, we design a model that captures geometric and appearance interactions between the 2D input views and the 3D prediction space.
We build on the powerful transformer architecture [42], and specifically DETR [6], a popular 2D object detection system. DETR encodes input images into feature maps and predicts 2D bounding boxes via cross-attention with learn-able queries. We enhance DETR in three ways. First, we make the input feature maps 3D-aware by adding 3D posi-tional information via ray embeddings, following [27]. Sec-ond, we replace the learnable, randomly initialized queries in DETR with appearance- and geometry-informed queries.
Our queries encode the 3D location of 3D reference points that cover the 3D space. They are enhanced with pixel-aligned appearance features sampled from the input views at the projected 2D locations. Cross-attention between our 3D-aware inputs and appearance-informed 3D queries un-leashes our model’s ability to capture 3D-to-2D correspon-dences quickly and efficiently, as we show in our experi-ments. Lastly, we deviate from DETR by introducing recur-rence. DETR makes predictions on the 2D plane while our predictions live in the vast 3D space. Our initial 3D points are likely to be far from the objects. So our model starts by making a coarse prediction that roughly places the ini-tial queries close to true objects and then gradually refines them. We model this by designing a recurrent scheme that encodes the 3D predictions from the previous step into the current queries. An overview of our proposed pixel-aligned recurrent queries, dubbed PARQ, is shown in Fig. 1.
PARQ differs from prior DETR-style methods for 3D ob-ject detection in design and attributes. Our recurrent decod-ing and query design differentiates us from the state-of-the-art DETR3D [44] and PETR [27]. DETR3D uses learnable queries and samples local appearance cues from the pro-jected 2D image locations to update the queries. This limits the model’s ability to capture long-range 3D-to-2D interac-tions. PETR enhances the input views with 3D positional information, similar to ours, but only encodes the 3D lo-cation of the queries at the start of decoding and without recurrent updates. This makes it difficult for the model to capture long-range correspondences driven by appearance and forces the model to focus on local cues around the 3D queries, as we show in our experiments. A schematic comparison is shown in Fig. 2. We show that our PARQ outperforms DETR3D and PETR on the challenging Scan-Net and ARKitScenes datasets. More importantly, we show that PARQ exhibits speedier convergence leading to faster training, is robust to a varying number of queries, and can leverage additional input views at test time without the need to retrain. In contrast, we show that PETR and DETR3D fail to generalize when deviating from the choice of in-put views and the number of queries during training. By tuning the number of queries and recurrent iterations, we show that PARQ detects fastest and most accurately. Fi-nally, with PARQ we demonstrate zero-shot generalization to novel scenes. 2.