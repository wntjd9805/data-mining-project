Abstract incremental
In this paper, we introduce audio-visual tasks can be forgotten as leading to poor performance. learning, but current methods fail class-incremental learning, a class-incremental learning sce-nario for audio-visual video recognition. We demon-strate that joint audio-visual modeling can improve class-incremental to pre-serve semantic similarity between audio and visual fea-tures as incremental step grows.
Furthermore, we observe that audio-visual correlations learned in pre-vious steps progress,
To overcome these challenges, we propose AV-CIL, which incorpo-rates Dual-Audio-Visual Similarity Constraint (D-AVSC) to maintain both instance-aware and class-aware seman-tic similarity between audio-visual modalities and Vi-sual Attention Distillation (VAD) to retain previously learned audio-guided visual attentive ability. We create three audio-visual class-incremental datasets, AVE-Class-Incremental (AVE-CI), Kinetics-Sounds-Class-Incremental (K-S-CI), and VGGSound100-Class-Incremental (VS100-CI) based on the AVE, Kinetics-Sounds, and VGGSound datasets, respectively. Our experiments on AVE-CI, K-S-CI, and VS100-CI demonstrate that AV-CIL significantly outperforms existing class-incremental learning methods in audio-visual class-incremental learning. Code and data are available at: https://github.com/weiguoPian/
AV-CIL_ICCV2023. 1.

Introduction
Human perception of the environment is based on a va-riety of senses. Specifically, people perceive the world by seeing and listening to the events happening around them, which are two of the most commonly used signals for en-vironment perception [5, 64, 63]. By jointly receiving vi-sual and auditory signals, the human brain can better under-stand its surroundings. Researchers have been inspired by this practical approach to real-world perception and have
†Equal contribution.
Figure 1:
Illustration of training the model in a class-incremental manner using (a) audio modality, (b) visual modality, and (c) joint audio-visual modalities. (d) Mean accuracy of using audio modality, visual modality, and joint audio-visual modalities in class-incremental learning with a vanilla fine-tuning strategy on the AVE-CI dataset. Our re-sults show that joint audio-visual modeling can significantly improve perception in the class-incremental setting. begun to focus on audio-visual scene understanding. The goal is to guide machines to better perceive their surround-ings by learning from audio and visual information jointly, just as humans do.
In recent years, a lot of works have been explored in the field of audio-visual scene understand-ing, such as audio-visual event localization [51, 82, 83, 91], audio-visual video parsing [52, 64, 81, 90], audio-visual sound separation [25, 26, 80, 98], audio-visual video cap-tioning [73, 79, 87], and audio-visual sound source localiza-tion [15, 18, 34, 35, 47, 61, 76]. These works have shown that modeling audio-visual modalities jointly can capture cross-modal semantic correlations effectively. Motivated by the success of these works, we aim to utilize the advantages of audio-visual modeling to mitigate the catastrophic for-getting problem [50, 57, 74] in class-incremental learning.
Catastrophic forgetting means the model’s performance on previous classes/tasks degrades significantly when up-dating it by training with only new classes’/tasks’ data (or combined with a small set of previous classes’/tasks’ data), which is the key challenge in incremental/continual learn-ing. To alleviate it, a lot of works have been conducted
in recent years, which can be categorized into parame-ter regularization-based [1, 4, 9, 40, 45, 96], knowledge distillation-based [6, 19, 22, 33, 41, 46, 50, 74, 89, 97, 99, 100], replay-memory/exemplar-based [2, 10, 11, 41, 53, 74, 77], and dynamic architecture-based methods [27, 37, 49, 86, 93, 94], mainly focus on continual/incremental image classification [2, 22, 74, 99], action recognition [48, 70], semantic segmentation [7, 8, 21, 29, 55, 68], object detec-tion [20, 23, 39], language/joint-vision-language tasks [43, 59, 75, 78], and self-supervised representation learning/pre-training [24, 38, 54, 72, 92]. Despite the success of audio-visual modeling in capturing cross-modal semantic correla-tions, its potential in addressing the catastrophic forgetting problem in class-incremental learning remains unexplored.
In order to explore the effectiveness of joint audio-visual modeling in class-incremental learning, we first train the model using a vanilla fine-tuning strategy, which involves fine-tuning the model on new classes directly without any specific techniques (please see Section 4 for experimen-tal settings). The results are presented in Figure 1, which demonstrates the advantage of joint audio-visual modeling over single audio or visual modality in a class-incremental manner. Based on these findings, we propose Audio-Visual
Class-Incremental Learning, which is a novel incremental learning problem under the scenario of audio-visual video recognition. Since both audio and visual modalities are in-volved, simply applying existing class-incremental learning approaches to our new audio-visual task cannot fully exploit the natural cross-modal association between the two modal-ities. Instead, we propose that the correlation preservation between visual and audio modalities should be explicitly incorporated to further improve the current techniques for incremental learning of audio-visual data.
In this paper, we propose a method named AV-CIL (Audio-Visual-Class-Incremental Learning) to address our new audio-visual class-incremental learning problem.
In
AV-CIL, we introduce a Dual-Audio-Visual Similarity Con-straint (D-AVSC), which is designed to preserve both instance-aware and class-aware semantic similarities be-tween audio and visual modalities throughout the increase in the number of classes.
Moreover, to better learn the cross-modal feature cor-relations, and ultimately get better joint audio-visual fea-tures, we also adopt an audio-guided visual attention mech-anism [47, 82, 84, 91], which is an effective attention mech-anism for adaptively learning correlations between audio and visual features. However, under the incremental learn-ing settings, with the increasing of the incremental steps, we observe that the learned audio-visual attentive ability in previous tasks could get vanished, which results in the forgetting of learned audio-visual correlations in previous tasks. Please see Figure 3 for the visualization of this phe-nomenon. To preserve and leverage the previously learned attentive ability in future classes/tasks, we propose the Vi-sual Attention Distillation (VAD) to distil the learned audio-guided visual attentive ability into new incremental steps, which enables the model to preserve previously learned cross-modal audio-visual correlations in new classes/tasks.
We use three existing audio-visual datasets: AVE [82],
Kinetics-Sounds [3], and VGGSound [16] to construct datasets for class-incremental learning. We name the three newly constructed datasets as AVE-Class-Incremental (K-S-CI), (AVE-CI), Kinetics-Sounds-Class-Incremental and VGGSound100-Class-Incremental re-spectively. We conduct experiments on the three datasets to evaluate the effectiveness of our method in audio-visual class-incremental learning. The experimental results show that our proposed method outperforms state-of-the-art class-incremental learning methods significantly on all three datasets. In summary, this paper contributes follows: (VS100-CI),
• To explore the effectiveness of joint audio-visual mod-eling in the alleviation of the catastrophic forgetting problem in class-incremental learning, we propose audio-visual class-incremental learning that trains the model continually under the scenario of audio-visual video recognition. To the best of our knowledge, this is the first work on audio-visual incremental learning.
• We propose a method, named AV-CIL, to tackle the posed new problem. AV-CIL contains a Dual-Audio-Visual Similarity Constraint (D-AVSC) to preserve both instance- and class-aware semantic similarity be-tween audio and visual features throughout the incre-mental steps. Furthermore, we also propose Visual
Attention Distillation (VAD) to enable the model to preserve previously learned attentive ability in future classes/tasks for preventing the model from forgetting previously learned audio-visual correlations.
• Experimental results on three audio-visual class-incremental datasets, AVE-CI, K-S-CI, and VS100-CI (constructed from AVE [82], Kinetics-Sounds [3], and
VGGSound [16]), demonstrate that our method outper-forms state-of-the-art class-incremental learning meth-ods significantly. 2.