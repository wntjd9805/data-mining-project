Abstract
Semi-supervised learning (SSL) is gaining popularity as it reduces cost of machine learning (ML) by training high performance models using unlabeled data. In this paper, we reveal that the key feature of SSL, i.e., learning from (non-inspected) unlabeled data, exposes SSL to strong poisoning attacks that can significantly damage its security. Poisoning is a long-standing problem in conventional supervised ML, but we argue that, as SSL relies on non-inspected unlabeled data, poisoning poses a more significant threat to SSL.
We demonstrate this by designing a backdoor poisoning attack on SSL that can be conducted by a weak adversary with no knowledge of the target SSL pipeline. This is un-like prior poisoning attacks on supervised ML that assume strong adversaries with impractical capabilities. We show that by poisoning only 0.2% of the unlabeled training data, our (weak) adversary can successfully cause misclassifica-tion on more than 80% of test inputs (when they contain the backdoor trigger). Our attack remains effective across different benchmark datasets and SSL algorithms, and even circumvents state-of-the-art defenses against backdoor at-tacks. Our work raises significant concerns about the secu-rity of SSL in real-world security critical applications. 1.

Introduction
Machine learning (ML) models perform better with in-creased amounts of training data [13, 12]. However, con-ventional supervised ML requires labeling large amounts of training data, an expensive [11] and error prone [31, 26] process that makes it prohibitively expensive, especially with today’s exploding training data sizes.
Semi-supervised learning (SSL) addresses this major challenge by significantly reducing the need for labeled training data: SSL uses a combination of a small, high-quality labeled data (expensive data) with a large, low-quality unlabeled data (cheap data) to train models. For instance, the FixMatch [40] SSL algorithm combines only 40 labeled with 50k unlabeled data to achieve a 90% ac-curacy on CIFAR10. Training SSL involves two loss func-tions: a supervised loss (e.g., cross-entropy [30] over true labels) on labeled training data and an unsupervised loss (e.g., cross-entropy over pseudo-labels [22]) on unlabeled training data. Different SSL algorithms primarily differ in terms of how they compute their unsupervised losses.
SSL has gained popularity in both academia [52, 46, 47] and industry [40, 41, 3, 2], as recent SSL algorithms offer state-of-the-art performances comparable or even superior to supervised techniques—but with no need of large well-inspected labeled data. For instance, due to their effective use of unlabeled data, with less than 10% of training data labeled, FixMatch [40] outperforms supervised ML.
Unlabeled data enables poisoning by weak adversaries:
Multiple researches have demonstrated the data poisoning threat to supervised ML [18, 28, 34, 36, 50, 44, 38]. How-ever, as the training data in supervised ML undergo an ex-tensive and careful inspection, these attacks assume strong adversaries with the knowledge of model parameters [28], training data [44, 4, 29], its distribution [50], or the ML al-gorithm. Such strong adversaries are important to evaluate worse-case security of a system, but are irrelevant in prac-tice [38]. On the other hand, the key feature of SSL that makes it attractive to real-world applications is its ability to leverage large amounts of—raw, non-inspected—unlabeled data, e.g., the data scraped off the Internet. We argue that the use of non-inspected data by SSL presents a unique threat to its security, as it allows even the most naive adversaries (with no knowledge of training algorithm, data, etc.) to poison SSL models by simply fabricating malicious unlabeled data. Unfortunately, this ostensible threat is largely unexplored in the SSL literature.
To address this gap, in this paper, we take the first step towards understanding this threat by studying the possibil-ity of backdoor attacks against SSL in real-world settings.
Backdoor attacks aim to install a backdoor function in the target model, such that the backdoored target model will misclassify any test input to the adversary chosen target class when patched with a specific backdoor trigger, but
will correctly classify test inputs without the trigger.
Existing backdoor attacks fail on SSL: There exist numer-ous backdoor attacks in the literature, however, except one attack—DeHiB [48], all of the prior attacks consider super-vised ML. Our preliminary evaluations show that all of the existing state-of-the-art (SOTA) attacks, including DeHiB, completely fail against SSL under our realistic threat model (Section 3.1). Hence, to learn from these failures, we first systematically evaluate five SOTA backdoor attacks from three categories against five SOTA SSL algorithms, under our practical, unlabeled data poisoning threat model.
Our systematic evaluation leads to the following three major lessons that not only guide our attack design, but can be useful building blocks for (future) backdoor attacks against SSL: (1) Backdoor attacks on SSL should be clean-label style attacks, i.e., poisoning data should be selected from the distribution of target class yt; (2) Backdoor trig-gers should be of the same size as the poisoning sample, to circumvent strong augmentations, e.g., cutout [15], that all modern SSL algorithms use; (3) Backdoor triggers should be resistant to noise and with repetitive patterns1 to with-stand large amounts of random noises due to strong aug-mentations, e.g., RandAugment [10], in SSL.
Our SSL-tailored backdoor method: The high-level intu-ition behind our backdoor attack is as follows. All modern
SSL algorithms learn via a self-feedback mechanism, called pseudo-labeling, i.e., if current state of target model fθ has high confidence prediction ˜y for an unlabeled sample x, then they use (x, ˜y) as a labeled sample for further training.
We exploit pseudo-labeling and design a clean-label attack that poisons unlabeled data only from the distribution of yt.
Our attack patiently waits for fθ to correctly label a poison-ing sample (x + T ) as yt, where T is our pre-determined backdoor trigger. As fθ trains further on ((x + T ), yt), our attack forces fθ to associate features of our simple trigger
T , instead of the complex features of x, with yt, thereby installing the backdoor in the target model.
Note that, we consider the most challenging setting for designing attacks with the least capable and knowledgeable data poisoning adversary. Generally, trigger generation for data poisoning backdoor attacks is formalized as a bi-level optimization problem [29], however such attacks are well-known to be very expensive, and yet ineffective [29, 38].
Instead, our lessons lead us to a simple yet effective static, repetitive grid pattern backdoor trigger (Figure 2).
Evaluations: We demonstrate the strength of our attack via an extensive evaluation against five SOTA SSL and one su-pervised ML algorithm, using four benchmark image clas-sification tasks commonly used in the SSL literature. We note that our attack significantly outperforms prior attacks from both SSL and supervised ML literature. 1Repetitive pixel patterns are the patterns on which if we zoom in on any part, we get similar pattern. For examples, check Figures 1 and 10.
We measure success of our attacks using ASR metric:
ASR measures the % of test inputs from non-target classes that the backdoored model classifies to the target class when patched with backdoor trigger. For the most combinations of algorithms and datasets, our attacks achieve high at-tack success rates (ASRs) (>80%), while poisoning just 0.2% of entire training data. For instance, our attacks have more than 90% ASR against CIFAR100 and more than 80% ASR against CIFAR10. For SVHN and STL10, our attack has more than 80% ASR with two exceptions each. While, under our practical threat model, DeHib at-tack achieves 0% ASR even with 20 more poisoning data.
Through a systematic experiment design in Section 5.1.4, we show that our intuition aligns with the dynamics of our attacks and justify their strength. Our attack is highly stealthy, as (1) according to L∞-norm metric commonly used [50] for stealth measurement, it minimally perturbs the poisoning data and (2) it produces backdoored models which have high accuracy (close to non-backdoored mod-els) on non-backdoored test inputs. We perform compre-hensive ablation study (Section 5.2) to demonstrate the high efficacy of our attacks as we vary (1) size of labeled data, (2) backdoor target class, and (3) size of poisoning data.
×
Finally, we show that our attack remains highly ef-fective even when SSL is paired individually with five
SOTA defenses against backdoor attacks that are agnostic to learning algorithms. To defend against such unlabeled data poisoning, we argue for SSL to depart from its philos-ophy of not inspecting unlabeled training data, and instead, pre-process/inspect the unlabeled data and/or design SSL algorithms that are robust-by-design to such poisoning. 2. Preliminaries and