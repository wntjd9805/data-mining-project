Abstract
The current popular methods for video object segmen-tation (VOS) implement feature matching through several hand-crafted modules that separately perform feature ex-traction and matching. However, the above hand-crafted designs empirically cause insufficient target interaction, thus limiting the dynamic target-aware feature learning in VOS. To tackle these limitations, this paper presents a scalable Simplified VOS (SimVOS) framework to perform joint feature extraction and matching by leveraging a sin-gle transformer backbone. Specifically, SimVOS employs a scalable ViT backbone for simultaneous feature extraction and matching between query and reference features. This design enables SimVOS to learn better target-ware features for accurate mask prediction. More importantly, SimVOS could directly apply well-pretrained ViT backbones (e.g.,
MAE [21]) for VOS, which bridges the gap between VOS and large-scale self-supervised pre-training. To achieve a better performance-speed trade-off, we further explore within-frame attention and propose a new token refinement module to improve the running speed and save computa-tional cost. Experimentally, our SimVOS achieves state-of-the-art results on popular video object segmentation benchmarks, i.e., DAVIS-2017 (88.0% J &F), DAVIS-2016 (92.9% J &F) and YouTube-VOS 2019 (84.2% J &F), without applying any synthetic video or BL30K pre-training used in previous VOS approaches. Our code and models are available at https://github.com/jimmy-dq/
SimVOS.git. 1.

Introduction
Video Object Segmentation (VOS) is an essential and fundamental computer vision tasks in video analysis [30, 47, 48, 57, 49, 51] and scene understanding [28, 12, 46, 22, 15, 45]. In this paper, we focus on the semi-supervised VOS
*Corresponding Author task, which aims to segment and track the objects of interest in each frame of a video, using only the mask annotation of the target in the first frame as given. The key challenges in
VOS mainly lie in two aspects: 1) how to effectively distin-guish the target from the background distractors; 2) how to accurately match the target across various frames in a video.
In the past few years, modern matching-based VOS ap-proaches have gained much attention due to their promising performance on popular VOS benchmarks [54, 32, 33]. The typical method STM [30] and its following works [8, 57, 7] mainly use several customized modules to perform semi-supervised VOS, including feature extraction, target match-ing and mask prediction modules. The whole mask pre-diction process in these approaches can be divided into two sequential steps: 1) feature extraction on the previous frames (i.e., memory frames) and the new incoming frame (i.e., search frame); and 2) target matching in the search frame, which is commonly achieved by calculating per-pixel matching between the memory frames’ embeddings and the search frame embedding.
Despite the favorable performance achieved by the above matching-based approaches, the separated feature extrac-tion and matching modules used in these methods still have several limitations. Firstly, the separate schema is unable to extract dynamic target-aware features, since there is no interaction between the memory and search frame embed-dings during the feature extraction. In this way, the feature extraction module is treated as the fixed feature extractor after offline training and thus cannot handle objects with large appearance variations in different frames of a video.
Secondly, the matching module built upon the extracted features needs to be carefully designed to perform suffi-cient interaction between query and memory features. Re-cent works (e.g., FEELVOS [42] and CFBI [56]) explore to use local and global matching mechanisms. However, their performance is still degraded due to the limited expressive power of the extracted fixed features.
To solve the aforementioned problems, this paper presents a Simplified VOS framework (SimVOS) for joint
finement module to reduce the computational cost and im-prove the running speed of SimVOS. This variant can run 2× faster than the SimVOS baseline, with a small reduc-tion in VOS performance. We conduct experiments on var-ious popular VOS benchmarks and show that our SimVOS achieves state-of-the-art VOS performance.
In summary, this paper makes the following contributions:
• We propose a Simplified VOS framework (SimVOS), which removes the hand-crafted feature extraction and matching modules in previous approaches [30, 8], to perform joint feature extraction and interaction via a single scalable transformer backbone. We also demon-strate that large-scale self-supervised pre-trained mod-els can provide significant benefits to the VOS task.
• We proposed a new token refinement module to achieve a better speed-accuracy trade-off for scalable video object segmentation.
• Our SimVOS achieves state-of-the-art performance on popular VOS benchmarks. Specifically, without applying any synthetic data pre-training, our vari-ant SimVOS-B sets new state-of-the-art performance on DAVIS-2017 (88.0% J &F), DAVIS-2016 (92.9%
J &F) and YouTube-VOS 2019 (84.2% J &F). 2.