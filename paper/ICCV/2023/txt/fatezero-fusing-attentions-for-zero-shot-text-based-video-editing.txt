Abstract
The diffusion-based generative models have achieved remarkable success in text-based image generation. How-ever, since it contains enormous randomness in generation progress, it is still challenging to apply such models for real-world visual content editing, especially in videos. In this paper, we propose FateZero, a zero-shot text-based
* Work done during an internship at Tencent AI Lab.
† Corresponding Authors. editing method on real-world videos without per-prompt training or use-speciﬁc mask. To edit videos consistently, we propose several techniques based on the pre-trained models. Firstly, in contrast to the straightforward DDIM inversion technique, our approach captures intermediate attention maps during inversion, which effectively retain both structural and motion information. These maps are directly fused in the editing process rather than generated during denoising. To further minimize semantic leakage of the source video, we then fuse self-attentions with a blending
mask obtained by cross-attention features from the source prompt. Furthermore, we have implemented a reform of the self-attention mechanism in denoising UNet by introducing spatial-temporal attention to ensure frame consistency. Yet succinct, our method is the ﬁrst one to show the ability of zero-shot text-driven video style and local attribute editing from the trained text-to-image model. We also have a better zero-shot shape-aware editing ability based on the text-to-video model [52]. Extensive experiments demonstrate our superior temporal consistency and editing capability than previous works. 1.

Introduction
Diffusion-based models [19] can generate diverse and high-quality images [40,42,44] and videos [15,18,32,45,56] through text prompts. It also brings large opportunities to edit real-world visual content from these generative priors.
Previous or concurrent diffusion-based editing meth-ods [2, 3, 6, 16, 38, 48] majorly work on images. To edit real images, their methods utilize deterministic DDIM [46] for the image-to-noise inversion, and then, the inverted noise gradually generates the edited images under the condition of the target prompt. Based on this pipeline, several methods have been proposed in terms of cross-attention guidance [38], plug-and-play feature [48], and optimization [25, 35].
Manipulating videos through generative priors as image editing methods above contains many challenges (Fig. 7).
First, there are no publicly available generic text-to-video models [18, 45]. Thus, a framework based on image mod-els can be more valuable than on video ones [36], thanks to the various open-sourced image models in the commu-nity [1, 37, 42, 54]. However, the text-to-image models [42] lack the consideration of temporal-aware information, e.g., motion and 3D shape understanding. Directly applying the image editing methods [33, 35] to the video will show ob-verse ﬂickering. Second, although we can use previous video editing methods [4, 24, 28] via keyframe [21] or atlas edit-ing [4, 24], these methods still need atlas learning [4, 24], keyframe selection [21], and per-prompt tunning [4, 28].
Moreover, while they may work well on the attribute [4, 24] and style [21] editing, the shape editing is still a big chal-lenge [28]. Finally, as introduced above, current editing meth-ods use DDIM for inversion and then denoising via the new prompt. However, in video inversion, the inverted noise in the T step might break the motion and structure of the origi-nal video because of error accumulation (Fig. 4 and 9).
In this paper, we propose FateZero, a simple yet effec-tive method for zero-shot video editing since we do not need to train for each target prompt individually [4, 24, 28] and have no user-speciﬁc mask [2, 3]. Different from image edit-ing, video editing needs to keep the temporal consistency of the edited video, which is not learned by the original trained text-to-image model. We tackle this problem by using two novel designs. Firstly, instead of solely relying on inversion and generation [16, 35, 48], we adopt a different approach by storing all the self and cross-attention maps at every step of the inversion process. This enables us to subsequently re-place them during the denoising steps of the DDIM pipeline.
Speciﬁcally, we ﬁnd these self-attention blocks store better motion information and the cross-attention can be used as a threshold mask for self-attention blending spatially. This attention blending operation can keep the original structures unchanged. Furthermore, we reform the self-attention blocks to the spatial-temporal attention blocks as in [52] to make the appearance more consistent. Powered by our novel de-signs, we can directly edit the style and the attribute of the real-world video (Fig. 6) using the pre-trained text-to-image model [42]. Also, after getting the video diffusion model (e.g., pretrained Tune-A-Video [52]), our method shows better object editing (Fig. 5) ability in test-time than simple DDIM inversion [46]. The extensive experiments provide evidence of the advantages offered by the proposed method for both video and image editing.
Our contributions are summarized as follows:
• We present the ﬁrst framework for temporal-consistent zero-shot text-based video editing using pretrained text-to-image model.
• We propose to fuse the attention maps in the inversion process and generation process to preserve the motion and structure consistency during editing.
• Our novel Attention Blending Block utilizes the source prompt’s cross-attention map during attention fusion to prevent source semantic leakage and improve the shape-editing capability.
• We show extensive applications of our method in video style editing, video local attribute editing, video object shape editing, etc. 2.