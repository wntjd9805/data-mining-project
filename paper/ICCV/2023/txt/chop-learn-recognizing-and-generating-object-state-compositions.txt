Abstract
Recognizing and generating object-state compositions has been a challenging task, especially when generalizing
In this paper, we study the task to unseen compositions. of cutting objects in different styles and the resulting object state changes. We propose a new benchmark suite Chop
& Learn, to accommodate the needs of learning objects and different cut styles using multiple viewpoints. We also propose a new task of Compositional Image Generation, which can transfer learned cut styles to different objects, by generating novel object-state images. Moreover, we also use the videos for Compositional Action Recognition, and show valuable uses of this dataset for multiple video tasks.
Project website: https://chopnlearn.github.io. 1.

Introduction
Objects often exist in different shapes, colors, and tex-tures in the real-world. These visually discernible proper-ties of objects, also known as states or attributes, can be inherent to an object (e.g., color) or be a result of an ac-tion (e.g., chopped). Generalization to unseen properties of objects remains an Achilles heel of current data-driven recognition models (e.g., deep networks) that assume ro-bust training data available for exhaustive object properties.
However, humans (and even animals)
[3, 6] can innately imagine and recognize a large number of objects with vary-ing properties, by composing a few known objects and their states. This ability to synthesize and recognize new combi-nations from finite concepts, called compositional general-ization is often absent in modern deep learning models [30].
Several recent works have been proposed to study com-position in terms of the disentanglement of objects and the states in images [24, 33, 54, 69] as well as videos [2, 4, 11, 15, 16, 19, 53, 57, 58]. A few works have attempted to im-prove open-world text-to-image generation models [12, 51] for the task of compositional generation. However, current suite of datasets lacks either granular annotations for object
*First two authors contributed equally.
Figure 1. We present Chop & Learn (ChopNLearn), a new dataset and benchmark suite for the tasks of Compositional Image
Generation and Compositional Action Recognition. It consists of 1260 video clips and 112 object state combinations captured from multiple viewpoints for 20 objects and 8 cut styles. We also pro-pose two new compositional tasks and benchmarks - (1) Image
Generation: given training images of various objects in various states, the goal is to generate images of unseen combinations of ob-jects and states. (2) Action Recognition: training videos are used to recognize objects along with transition from state1 → state2, to generalize on recognizing unseen object-state transitions. states or enough data to study how object states evolve un-der different conditions. Therefore, measuring the compo-sitional generalizability of these models on different tasks remains an open challenge.
In this paper, we propose a new dataset, Chop &
Learn (ChopNLearn) collected to support studying com-positional generalization, the ability to recognize and gen-erate unseen compositions of objects in different states. To focus on the compositional aspect, we limit our study to a common task in our daily lives – cutting fruits and vegeta-bles. When using different styles of cutting, these objects undergo different transformations and the resulting states are easily recognizable by humans. Our goal is to study how these different styles can be applied to a variety of
refers to objects, Comp.
Table 1. Comparison with other video datasets. This table high-lights the distribution of the objects, states and compositions in different datasets. Obj. is composi-tions of objects and styles, N refers to the number of compositions that have more than 10 samples, and Styles∗ refers to grouping of styles: instead of generic names like cut, chop, etc., we use 3 dis-tinct styles (chop/dice, peel, grate) as styles. MIT-States† is the only image-based dataset, the rest are video-based datasets. All these data numbers are for edible objects and cutting style actions from respective datasets. Our dataset has uniform distribution for each metric in the table, which makes it suitable for learning ob-jects and their states.
Datasets
Total # of
Avg. # of Samples
Samples Obj. Comp. Styles∗ /Obj.
/Comp. /Style
N # of
Views
MIT-States† [25]
Youcook2 [72]
VISOR [8]
COIN [61]
Ego4D [13] 50Salads [59]
ChangeIt [57]
CrossTask [73]
Breakfast [29] 1676 714 301 390 216 904 264 1150 1055 27 160 58 6 12 5 8 7 3 52 313 122 7 12 6 14 8 4
ChopNLearn 1260 20 112 4 3 3 2 3 2 4 2 2 8 48 419 62.07 32.23 166.7 26 2.2 7.3 3 42.9 2.5 5.2 6 195 55 65 8 54.5 18 18.2 6 457 152 182 14 96 46.3 26.4 8 164.3 143.7 575 4 351.7 263.8 527.5 74.2 11.8 185.5 112 1 1 1 1 1 1 1 1 1 4 objects for recognizing unseen object states. More specif-ically, we select twenty objects and seven commonly used styles of cuts (plus whole object) which results in object-state pairs with different granularity and sizes (Figure 1).
We collect videos of these objects being from four different viewpoints, and label different object states in each video.
Each style of cut changes the visual appearance of different objects in different ways. To study and understand object appearance changes, we propose two new benchmark tasks of Compositional Image Generation and Compositional Ac-tion Recognition, with a focus on unseen compositions.
The objective of the first task is to generate an image based on an (object, state) composition that was not seen during training. As shown in Figure 1, during training, a generative model is provided with images of an (apple, whole) as well as an (orange, round slices). At the test time, the model has to synthesize a new unseen compo-sition (apple, round slices). We propose to adapt large-scale text-to-image generative models for this task.
Specifically, by using text prompts to represent the object-state composition, we benchmark several existing methods such as Textual Inversion [12] and DreamBooth [51]. We also propose a new method by introducing new tokens for objects and states and simultaneously fine-tuning language and diffusion models. Lastly, we discuss the challenges and limitations of prior works as well as the proposed generative model with an extensive evaluation.
In the second task, we extend an existing task of Compo-sitional Action Recognition [35]. While the focus of prior work [35] is on long-term activity tracking in videos, we aim to recognize subtle changes in object states which is a crucial first step for activity recognition. By detecting the initial state and final object state compositions, our task allows the model to learn unseen object state changes ro-bustly. We benchmark multiple recent baselines for video tasks on the ChopNLearn dataset.
Finally, we discuss various other applications and tasks that can use our dataset in image and video domains. To summarize, our contributions are threefold:
• We propose a new dataset ChopNLearn, consisting of a large number of images and videos of diverse object-state compositions with multiple camera views.
• We introduce the task of Compositional Image Genera-tion, which goes beyond the common conditional image generation benchmarks, and focuses on generating im-ages for unseen object and state compositions.
• We introduce a new benchmark for the task of Composi-tional Action Recognition, which aims at understanding and learning changes in object states over time and across different viewpoints. 2.