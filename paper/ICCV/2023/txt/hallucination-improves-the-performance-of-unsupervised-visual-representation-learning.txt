Abstract
Contrastive learning models based on Siamese struc-ture have demonstrated remarkable performance in self-supervised learning. Such a success of contrastive learn-ing relies on two conditions, a sufficient number of pos-itive pairs and adequate variations between them.
If the conditions are not met, these frameworks will lack seman-tic contrast and be fragile on overfitting. To address these two issues, we propose Hallucinator that could efficiently generate additional positive samples for further contrast.
The Hallucinator is differentiable and creates new data in the feature space. Thus, it is optimized directly with the pre-training task and introduces nearly negligible compu-tation. Moreover, we reduce the mutual information of hal-lucinated pairs and smooth them through non-linear opera-tions. This process helps avoid over-confident contrastive learning models during the training and achieves more transformation-invariant feature embeddings. Remarkably, we empirically prove that the proposed Hallucinator gener-alizes well to various contrastive learning models, includ-ing MoCoV1&V2, SimCLR and SimSiam. Under the linear classification protocol, a stable accuracy gain is achieved, ranging from 0.3% to 3.0% on CIFAR10&100, Tiny Ima-geNet, STL-10 and ImageNet. The improvement is also ob-served in transferring pre-train encoders to the downstream tasks, including object detection and segmentation. 1.

Introduction
In the recent computer vision community, there has been rapid progress in self-supervised learning (SSL), gradu-ally closing the performance gap with supervised learn-ing [23, 29, 10, 7, 51]. Among the diverse approaches of
SSL, contrastive learning, such as MoCoV1&V2 [29, 11],
SimCLR [10], and SimSiam [12], shows promising results.
Generally, contrastive learning treats each image as one class which will be augmented into two separate views.
These two views form one positive pair and should ide-Figure 1: The motivation of the proposed hallucination methods. Given one pair of images with the same seman-tic meaning, such as a pair of horses, a person can envi-sion further similar pairs by imagining one of the horses in different poses and surroundings. If a contrastive learning model could do such hallucination, it could have additional novel pairs to contrast given the same data. Note that this hallucination process is for illustration only. In the imple-mentation, all the hallucinated samples are computed in the feature space. ally be close if mapped to feature space. With sufficient contrast in the feature space, contrastive learning models show a strong capacity to learn transformation-invariant features that are transferable to various downstream tasks, such as classification, object detection, and segmentation
[18, 30, 39].
To ensure sufficient contrast, researchers from previous work address the issue from two essential practices, either introducing large amounts of positive pairs or adding addi-tional variants&transformation among them. For example,
SimCLR uses a batch size that generates thousands of pos-itives to facilitate the convergence of models [10]. Work
from [51, 9] reduces mutual information of positive pairs using stronger data augmentation, i.e., color distortion and jigsaw transformation. Likewise, the work from [44] intro-duces ContrastiveCrop, and the work from [49] proposes
Un-Mix, respectively, to reduce the similar semantic mean-ing of sample pairs in the original image space. Beyond the data augmentation and image operations, researchers from
[68] propose to apply a linear operation to generate hard positive samples in feature space.
Despite the success of prior approaches, we argue that large batch sizes are not always achievable. Meanwhile, all proposed techniques only focus on improving the original pairs. Given one positive pair of positive samples, humans are born with the amazing ability to come up with additional positives by imagining a sample from different surround-ings and perspectives without much effort, as demonstrated in Figure 1. This process of self-imagination, in turn, will benefit the human neurological system, improving recog-nition capacity [24]. Similarly, if we could empower con-trastive learning models with the ability to hallucinate or imagine an object to a novel view, additional positive pairs could be provided for the learning tasks.
Unfortunately, exploring feasible methods to hallucinate novel positive pairs is challenging. Firstly, while genera-tive models produce realistic images that could form ad-ditional positive views [22, 1, 42, 4], realistic data do not necessarily benefit learning tasks [55]. More importantly, applying these approaches forces us to fall back into a com-putational dilemma to the previous method. In other words, image-level hallucination still suffers from expensive com-putation as we still need to encode the hallucinated images into feature space. Lastly, if the generated positive pairs are similar to each other, training a discriminative model would be too trivial, thus showing poor generalization ca-pacity [44, 51, 68].
Therefore, our key insight is that the sample-generation process should aim for three critical elements: (i) feature-space operation (ii) sufficient variance of positive pairs (iii) a differentiable module optimized directly related to the learning task. To achieve this, we propose Hallucinator to improve the performance of contrastive learning with
Siamese structures. The Hallucinator is plugged in af-ter the encoder to manipulate feature vectors and improve the feature-level batch size for further contrast. To ensure adequate variance is introduced, we propose an asymmet-ric feature extrapolation method inspired by the work from
[68]. More importantly, we present a non-linear hallucina-tion process for the extrapolated samples. Such a process is differentiable (i.e. learnable), therefore essentially boosting
Hallucinator to generate smooth and task-related features.
The proposed Hallucinator delivers extra positives and simultaneously enlarges the variance between newly intro-duced pairs. Moreover, this approach only relies on pos-itive samples. Therefore, it can be easily applied to any
Siamese structure by adding it after the encoders as a plug-and-play module. Without the tedious exploration of hyper-parameters and much additional computation, we empir-ically prove the effectiveness of the proposed Hallucina-tor on popular contrastive learning models, including Mo-CoV1&V2, SimCLR and SimSiam. We notice a stable im-provement ranging from 0.3% to 3.0% under the linear clas-sification protocol, crossing the CIFAR10&100, Tiny Ima-geNet, STL-10 and ImageNet. We also observe that mod-els trained with Hallucinator show better transferability in downstream tasks like object detection and segmentation.
Our contributions can be summarized as follows:
• We investigate a critical yet under-explored aspect of introducing additional positive contrastive learning: pairs with further variation in feature space.
• To the best of our knowledge, this is the first attempt to incorporate the concept of “Hallucination” into con-trastive representation learning. We propose Halluci-nator to realize this idea, which effectively generates smooth and less similar positive feature vectors. The
Hallucinator is simple, effective, and agnostic to con-trastive frameworks.
• We empirically illustrate that the proposed approach significantly benefits the various contrastive learning models within multiple datasets. 2.