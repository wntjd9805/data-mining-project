Abstract 1.

Introduction
Few-shot video action recognition is an effective ap-proach to recognizing new categories with only a few la-beled examples, thereby reducing the challenges associated with collecting and annotating large-scale video datasets.
Existing methods in video action recognition rely on large labeled datasets from the same domain. However, this setup is not realistic as novel categories may come from different data domains that may have different spatial and tempo-ral characteristics. This dissimilarity between the source and target domains can pose a significant challenge, ren-dering traditional few-shot action recognition techniques ineffective. To address this issue, in this work, we pro-pose a novel cross-domain few-shot video action recog-nition method that leverages self-supervised learning and curriculum learning to balance the information from the source and target domains. To be particular, our method employs a masked autoencoder-based self-supervised train-ing objective to learn from both source and target data in a self-supervised manner. Then a progressive curriculum balances learning the discriminative information from the source dataset with the generic information learned from the target domain. Initially, our curriculum utilizes super-vised learning to learn class discriminative features from the source data. As the training progresses, we transition to learning target-domain-specific features. We propose a progressive curriculum to encourage the emergence of rich features in the target domain based on class discrimina-tive supervised features in the source domain. We evaluate our method on several challenging benchmark datasets and demonstrate that our approach outperforms existing cross-domain few-shot learning techniques. Our code is available at https://github.com/Sarinda251/CDFSL-V
Even though deep learning is inspired by the biological brain, in sharp contrast to humans, current deep models rely on large reservoirs of data to learn. The few-shot learning problem [41] is introduced to close this gap, where a learn-ing model should generalize solely based on a handful of training data. In traditional few-shot learning [6], the learn-ing model is initially exposed to an annotated base dataset, to learn generic features for the domain of interest. Then, this model is fine-tuned on a few labeled examples (sup-port samples) of the test dataset and consequently evaluated on unlabeled test examples (query samples). However, this classic pipeline assumes the base and test datasets are from the same domain, thus closely related [35].
To mitigate this shortcoming, cross-domain few-shot learning (CDFSL) was proposed in [12], where the base dataset is from a different domain than the test data. Inter-estingly, it is shown in [25] that standard transfer learning— consisting of pre-training on the base dataset and fine-tuning on test data— can significantly outperform few-shot learning methods in the cross-domain few-shot learning problem. Recently, extra unlabeled test examples were in-corporated in addition to the base dataset in [28, 16] . Their approaches push forward cross-domain few-shot learning performance. In this paper, we follow this recent adapta-tion of CDFSL.
While few-shot learning is widely studied in the com-puter vision community [26], video few-shot learning is less explored [4]. To the best of our knowledge, current methods in cross-domain few-shot learning are solely fo-cused on image data. In this work, for the first time, we study cross-domain few-shot learning in the video domain.
A common scheme in video few-shot learning [45] utilizes an implicit assumption about video data, such as: a common mode of variation, similar temporal dynamics, or class dis-tinctive features. However, in cross-domain few-shot learn-ing, the base dataset can be drastically different from the
Figure 1: On the left, we have the existing benchmark for CDFSL in the image domain. On the right, we present our proposed benchmark for CDFSL in the video domain. Our benchmark includes tasks from diverse target datasets, which require recognizing novel actions from different data distributions (UCF101, HMDB51), strong temporal reasoning (SSV2), atypical action understanding (RareAct), and fine-grained temporal understanding (Diving48). target data, For instance, the RareAct dataset [22] contains atypical actions which significantly deviate from the com-mon actions present in the standard video datasets in terms of spatio-temporal dynamics, and the Diving48 dataset [21] contains temporally fine-grained actions which have very similar spatial layout. Therefore, it is challenging to apply standard video few-shot learning methods to these datasets.
In the context of cross-domain few-shot learning, super-vised pre-training on the source dataset has emerged as a common first step for most techniques [28, 16]. This is be-cause a strong source backbone can significantly contribute to the overall performance of the model [25]. However, simply relying on supervised pre-training may not be suf-ficient, especially when the target domain is substantially different from the source domain. To address this, in this work we propose to perform self-supervised pre-training on both source and target data to learn generic features. To be particular, we use recently proposed masked auto-encoder based [36] feature learning to learn generic features which are highly scalable and show better generalization perfor-mance. Nevertheless, the challenge remains on how to bal-ance the learning of generic features (from source and target domain) and class discriminative features from the source dataset.
To this end, we propose a curriculum learning scheme by designing a progressive curriculum that balances learning the discriminative information from the source dataset with the generic information learned from the target domain. In the initial phase of the training, our curriculum utilizes su-pervised cross-entropy loss to learn class discriminative fea-tures from the source data. As the training progresses, we strive to transition to the target domain through learning dis-criminative features in the target domain. To achieve this, we devise a schedule that increases the weight of a con-sistency loss to help with this transition. We conduct ex-tensive experiments to demonstrate the effectiveness of our proposed approach on various benchmark datasets. Our ex-periments show significant improvements in cross-domain few-shot action recognition performance.
In summary, our work makes the following major contri-butions,
• We propose a new, challenging, and realistic prob-lem called cross-domain few-shot learning in videos (CDFSL-V).
• We propose a novel solution based on self-supervised feature learning and curriculum learning for this chal-lenging problem, which can address the difficulties as-sociated with CDFSL-V by striking a balance between learning generic and class-discriminative features.
• We conduct extensive experimentation on multiple benchmark datasets. Our proposed method outper-forms the existing methods in cross-domain few-shot learning, as well as, strong baselines based on transfer learning. 2.