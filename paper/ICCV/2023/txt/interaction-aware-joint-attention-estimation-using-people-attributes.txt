Abstract
This paper proposes joint attention estimation in a single image. Different from related work in which only the gaze-related attributes of people are independently employed, (i) their locations and actions are also employed as contex-tual cues for weighting their attributes, and (ii) interac-tions among all of these attributes are explicitly modeled in our method. For the interaction modeling, we propose a novel Transformer-based attention network to encode joint attention as low-dimensional features. We introduce a spe-cialized MLP head with positional embedding to the Trans-former so that it predicts pixelwise confidence of joint atten-tion for generating the confidence heatmap. This pixelwise prediction improves the heatmap accuracy by avoiding the ill-posed problem in which the high-dimensional heatmap is predicted from the low-dimensional features. The estimated joint attention is further improved by being integrated with general image-based attention estimation. Our method out-performs SOTA methods quantitatively in comparative ex-periments. Code: https://github.com/chihina/
PJAE. 1.

Introduction
Attention analysis enables various applications, such as customer’s interest estimation [51], analyzing atypical gaze perception in autism spectrum disorder [21, 2], and human-robot interaction [47]. While attention is represented as a point, region, or object in the literature, we represent it as an attention point, AP, because a point can be used in any applications as an elemental representation. The confidence distribution of APs can be expressed in a heatmap image [5, 60, 8, 52] where each pixel value represents the confidence.
Attention estimation has two categories: single attention estimation [45, 46, 4, 3, 5, 9, 52, 18] and joint attention esti-mation [8, 60, 40, 41, 17]. While single attention estimation targets the attention of a person, attention shared by multi-ple people is detected by joint attention estimation.
In single attention estimation, an AP is estimated based on the gaze direction of a target person [42, 33, 10, 36, 26, 25, 14, 24] and the saliency map of a scene im-(a) Previous methods [5, 60, 8, 52] (b) Ours: gaze distributions are omitted for simple visualization
Figure 1. Difference between joint attention estimation methods. (a) Aggregating the gaze-related features of equally-weighted peo-ple without considering their interaction. (b) Aggregating the at-tributes of people weighted by their contextual attributes (i.e., lo-cations and actions) via the interaction among all the attributes. age [27, 38, 23, 56, 59, 24] in general. By simply aggre-gating (e.g., averaging) multi-people APs that are indepen-dently estimated by single attention estimation, a joint AP can be estimated [5, 52]. However, the APs of multiple peo-ple are not independent but jointly correlated in context.
For joint attention estimation with such contextual corre-lation, in [8, 60], only the gaze-related attributes of people (e.g., “Gaze distribution” in Fig. 1 (a)) are employed. Such a straightforward approach has the problems below:
• No contribution weights of people: Some people share attention, but others do not. The latter people should not affect joint attention estimation, while all people are equally weighted in [8, 60].
• No explicit interaction among people attributes: As contextual cues related to joint attention, not only gazes [8, 60] but also other attributes of people, such as their locations and actions, are useful. Their interac-tions are also informative. For example, nearby people doing the same action may share the AP. Such interac-tions among people attributes are neglected in [8, 60].
These problems are resolved by the following novel con-tributions in this paper, as illustrated in Fig. 1 (b):
• Activity awareness: Each person’s activity such as the location and action can be an important clue for joint attention estimation. For example, people shar-ing the AP tend to share their activities as group mem-bers. This assumption motivates us to focus on what and where each person is doing, namely the location and action of each person, to weight the contributions of people for joint attention estimation.
• Interaction awareness:
Interactions among people attributes are explicitly modeled by our Position-embedded Joint Attention Transformer (PJAT), where a self-attention mechanism extracts the features of peo-ple sharing the AP.
• Pixelwise joint attention heatmapping: While the extracted joint-attention features are efficiently but suf-ficiently low-dimensional, it is ill-posed to estimate a high-dimensional heatmap image representing the AP confidences from such low-dimensional features. To avoid such an ill-posed estimation problem, we em-ploy a network with image-coordinate embedding for estimating the AP confidence pixelwise. 2.