Abstract
Video-language pre-training has advanced the perfor-mance of various downstream video-language tasks. How-ever, most previous methods directly inherit or adapt typical image-language pre-training paradigms to video-language pre-training, thus not fully exploiting the unique charac-In this paper, we pro-teristic of video, i.e., temporal. pose a Hierarchical Temporal-Aware video-language pre-training framework, HiTeA, with two novel pre-training tasks for yielding temporal-aware multi-modal representa-tion with cross-modal fine-grained temporal moment infor-mation and temporal contextual relations between video-text multi-modal pairs. First, we propose a cross-modal moment exploration task to explore moments in videos by mining the paired texts, which results in detailed video moment rep-resentation. Then, based on the learned detailed moment representations, the inherent temporal contextual relations are captured by aligning video-text pairs as a whole in dif-ferent time resolutions with multi-modal temporal relation exploration task. Furthermore, we introduce the shuffling test to evaluate the temporal reliance of datasets and video-language pre-training models. We achieve state-of-the-art results on 15 well-established video-language understand-ing and generation tasks, especially on temporal-oriented datasets (e.g., SSv2-Template and SSv2-Label) with 8.6% and 11.1% improvement respectively. HiTeA also demon-strates strong generalization ability when directly trans-ferred to downstream tasks in a zero-shot manner. 1.

Introduction
Vision and language are two primary signals that con-stitute the real-world perception of humanity. With the success of image-language pre-training [8,22,25,47], video-language pre-training [23,26,27,34] has recently received in-creasing attention. Large-scale video-language pre-training helps the model to learn effective multi-modal representa-tion, which has shown significant improvement on a va-riety of video-language downstream tasks, such as video-text retrieval, video question answering and video caption-∗Corresponding Author.
Figure 1: Comparison between existing paradigms and ours for video-language pre-training. (a) Previous methods align video and text within global perspective as the pretext. (b)
We introduce HiTeA by varying video in different temporal views and modeling cross-modal temporal information be-tween moments and texts, as well as the temporal contextual relations between multi-modal pairs. ing [5, 33, 44, 48, 50, 53, 59].
Inspired by the success of image-language pre-training paradigm, various methods [10,11,23,26,27] have been pro-posed to adapt it to video-language pre-training. ClipBERT
[21] and Singularity [20] directly build on representations from image encoders and aggregate them via score aggre-gation function and temporal encoder. Furthermore, MIL-NCE [34] and Frozen [2] switch image encoder to video en-coder for spatio-temporal video representation learning and align the video with corresponding text. In addition, some advanced pre-training tasks are designed through modeling entity [23], reconstructing masked patches [10] and predict-ing frame order [26, 61]. Despite their promising perfor-mance on downstream tasks, they treat video within global perspective illustrated in Figure 1(a), thus failing to consider fine-grained temporal information and temporal contextual relations which are essential to video-language pre-training.
Since untrimmed video contains various temporal details,
directly treating the video globally has two main limitations: (1) Less effective in modeling the fine-grained moment in-formation including atomic actions and moments. As illus-trated in Figure 1(b), we vary time resolutions and generate two views (long & short) for the input video. As a result, the short-view video clip tends to represent the moment in-formation and the long-view video may express more event-level information. For example, the short-view video clip in Figure 1(b) only describes the moment of "lick fingers" rather than "eating ice cream". Such fine-grained moment information is hard to be captured by the long-view video under global event perspective; (2) Ignoring the temporal contextual relations implicitly existed in the video. Know-ing the event expressed by the text, the moment "eating ice cream" can be inferred from the moment "lick fingers" shown by short-view video. However, such implicit tempo-ral contextual relations between the moment and the event are rarely explored in previous works.
To address these problems, we propose a Hierarchical
Temporal-Aware video-language pre-training framework,
HiTeA, for both multi-modal understanding and generation.
Except for the standard pre-training tasks, HiTeA introduces two novel temporal-aware video-language pre-training tasks, named cross-modal moment exploration (CME) and multi-modal temporal relation exploration (MTRE), which not only model the fine-grained temporal moment information but also captures temporal contextual information hierarchi-cally, yielding temporal-aware multi-modal representations for both understanding and generation. Specifically, we first generate the long-view and short-view videos with different time resolutions to build hierarchy of the input video. Then, based on the similarities of words and short-view video, we select the most relevant words as positive and leave the rest of the words as hard negatives. The CME pre-training task is applied to align the positive words and short-view video representations in the same embedding space. Moreover, to capture association between moments and the event for tem-poral contextual modeling, we match different views for the same video. However, directly matching two views visually would be noisy due to the background similarity [39]. To this end, we perform multi-modal pair alignment between video-text pairs via the MTRE pre-training task. More specifically, the short-view video guided by most relevant words and the long-view video guided by text will be aligned, which en-ables the model to extrapolate the contextual information from the short-view with language signal while enhancing temporal reasoning ability. Empowered by above two novel temporal-aware video-language pre-training tasks, HiTeA is capable of modeling temporal-aware multi-modal informa-tion revealed in video-text data including both fine-grained moment information and temporal contextual relations.
In spite of a good performance, recent studies [4, 20] re-veal most video-language downstream datasets are biased to-wards still objects, scenes, etc., while the temporal dynamics are negligible. To evaluate the temporal performance of the video-language pre-training model and temporal reliance of downstream datasets, we introduce temporal shuffling test for these datasets. This enables a comprehensive evalua-tion of temporal modeling capability in the video-language pre-training field. Besides, our method achieves significant improvement on the datasets with heavy temporal reliance.
In summary, our key contributions are the followings:
• We propose a novel hierarchical temporal-aware video-language pre-training framework with both video-language understanding and generation capabilities.
• We introduce temporal-aware pre-training tasks to generate temporal-aware multi-modal representation through modeling fine-grained temporal moment in-formation as well as capturing the temporal contextual relations between moment and event.
• Extensive experiments demonstrate the effectiveness and generalization ability of HiTeA, and it achieves state-of-the-art performance on 15 video-language downstream datasets including video-text retrieval, video question answering, and video captioning, es-pecially on temporal-oriented datasets (e.g., SSv2-Template and SSv2-Label) with 8.6% and 11.1% im-provement respectively. 2.