Abstract
Building an interactive AI assistant that can perceive, reason, and collaborate with humans in the real world has been a long-standing pursuit in the AI community. This work is part of a broader research effort to develop intel-ligent agents that can interactively guide humans through performing tasks in the physical world. As a first step in this direction, we introduce HoloAssist, a large-scale egocentric human interaction dataset, where two people collaboratively complete physical manipulation tasks. The task performer executes the task while wearing a mixed-reality headset that captures seven synchronized data streams. The task instruc-tor watches the performer’s egocentric video in real time and guides them verbally. By augmenting the data with action and conversational annotations and observing the rich be-haviors of various participants, we present key insights into how human assistants correct mistakes, intervene in the task completion procedure, and ground their instructions to the environment. HoloAssist spans 166 hours of data captured by 350 unique instructor-performer pairs. Furthermore, we construct and present benchmarks on mistake detection, in-tervention type prediction, and hand forecasting, along with detailed analysis. We expect HoloAssist will provide an im-portant resource for building AI assistants that can fluidly collaborate with humans in the real world. Data can be downloaded at https://holoassist.github.io/. 1.

Introduction
Recent years have witnessed incredible progress in general-purpose AI agents that assist humans with various open-world tasks, especially in the digital world. AI sys-tems powered by large language models (LLMs) like Chat-GPT [27] can answer users’ questions and assist them with various text-based tasks. However, these AI assistants do not have sufficient first-hand experience in the physical world and thus cannot perceive world states and actively intervene
*Co-first authors; †Work done at Microsoft
Figure 1: HoloAssist features a two-person interactive assis-tive task completion setting. The task performer wears an
AR device and completes the tasks while the captured data is streamed over the network to a remote instructor watching it on the laptop. The instructor provides verbal guidance to the student. HoloAssist includes seven modalities captured live and human annotated text descriptions as the 8th modality. in the task completion procedure.
Building an AI assistant that can perceive, reason and interact in the physical world has attracted attention from researchers across different fields in computer vision [7, 23, 37, 38], human-computer interaction [6, 8, 15, 29], robotics [5, 32], and industrial practitioners. For example,
AR Guides [1], which aims to guide users to complete com-plex tasks, has become popular with the development of augmented reality (AR) devices. However, existing systems often rely on pre-defined instructions or formulate the virtual assistant as a question answering [37, 38] or video under-1
standing problem [7, 23] without real-world interaction.
In another line of work, researchers have developed simu-lation environments like Habitat [21, 36], VirtualHome [26], and AI2-Thor [17] to build AI agents that can interact with the physical world and collaboratively achieve new tasks [42]. Still, a large gap remains in transferring these agents to the real world, and the interaction between agents is largely simplified compared to real-world human interaction.
In this work, we focus on the challenges of developing intelligent agents that share perspectives with humans and interactively guide human users through performing tasks in the physical world. As a first step, we introduce HoloAssist, a large-scale egocentric human interaction dataset to explore and identify the open problems in this direction. As shown in
Figure 1, the task performer wears an AR headset* to capture data while completing the tasks. An instructor watches the real-time egocentric video feed remotely and verbally guides the performer. We have developed and open-sourced a data capture tool [3] using a distributed server-client setup to enable data streaming and multimodal data capture.
HoloAssist contains 166 hours of data captured by 222 diverse participants forming 350 unique instructor-performer pairs and carrying out 20 object-centric manipulation tasks.
The objects range from common electronic devices to rare objects in factories and specialized labs. The tasks are gen-erally challenging for first-time participants, requiring in-structor assistance for successful completion. Seven raw sensor modalities are captured, including RGB, depth, head pose, 3D hand pose, eye gaze, audio, and IMU, to aid in the understanding of human intentions, estimating world states, predicting future actions, and so on. Finally, the dataset is augmented with third-person manual annotations consisting of a text summary, intervention types, mistake annotation, and action segments of the videos as illustrated in Figure 2.
We have observed several characteristics demonstrated by human instructors from HoloAssist. First, instructors are often proactive with precisely timed interventions. Instead of waiting until mistakes happen, instructors provide follow-up instructions when the task performer appears confused.
Second, the verbal guidance from the instructors tends to be concise and grounded in the task performer’s environment.
The instructions are often framed as spatial deictics to aid the task performer in spatial directions and distances in the 3D world. Moreover, instructors often have a good world model estimation and can detect whether mistakes disrupt task completion and then adjust the guidance.
We take a step further and introduce new tasks and bench-marks on mistake detection, intervention type prediction, and 3D hand pose forecasting, which we conjecture are essential modules for an intelligent assistant. Additionally, we bench-mark the dataset on action classification and anticipation tasks and provide empirical results to understand the role of
*We use HoloLens 2 [2] for data capture in this work.
Figure 2: HoloAssist includes action and conversational annotations, in addition to text summaries of the videos, to indicate the mistakes and interventions in task completion. mistake or correct attributes are associated with each fine-grained action. A purpose label is associated with every utterance to indicate the type of verbal intervention. different modalities in various tasks. We hope our dataset, findings, and tooling can inspire and provide rich resources for future work on designing interactive AI assistants and situated AI assistance applications in the real world. 2.