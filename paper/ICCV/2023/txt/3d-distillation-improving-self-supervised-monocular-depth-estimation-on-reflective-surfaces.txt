Abstract
Self-supervised monocular depth estimation (SSMDE) aims at predicting the dense depth maps of monocular im-ages, by learning to minimize a photometric loss using spatially neighboring image pairs during training. While
SSMDE offers a significant scalability advantage over su-pervised approaches, it performs poorly on reflective sur-faces as the photometric constancy assumption of the pho-tometric loss is violated. We note that the appearance of reflective surfaces is view-dependent and often there are views of such surfaces in the training data that are not contaminated by strong specular reflections. Thus, reflec-tive surfaces can be accurately reconstructed by aggregat-ing the predicted depth of these views. Motivated by this observation, we propose 3D distillation: a novel training framework that utilizes the projected depth of reconstructed reflective surfaces to generate reasonably accurate depth pseudo-labels. To identify those surfaces automatically, we employ an uncertainty-guided depth fusion method, com-bining the smoother and more accurate projected depth on reflective surfaces and the detailed predicted depth else-where. In our experiments using the ScanNet and 7-Scenes datasets, we show that 3D distillation not only significantly improves the prediction accuracy, especially on the prob-lematic surfaces, but also that it generalizes well over var-ious underlying network architectures and to new datasets. 1.

Introduction
Monocular depth estimation [37, 7] is the task of pre-dicting the dense depth map of a monocular image. It is a fundamental and challenging problem in computer vi-sion as it bridges the gap between 2D images and the 3D world. Supervised monocular depth estimation requires a large number of images from diverse scenes with ground truth depth. However, creating depth annotations involves
Email: XuepengShi@outlook.com. X. Shi did the work while intern-ing at Qualcomm. (a) (b) (c)
Figure 1: (a) On a reflective surface, predicting the correct surface depth does not minimize the photometric loss [14], due to the disparity between the projection with correct depth A and the observed location B. (b) L→R: Image from the ScanNet test set (scene0781 00) [4], predicted depth of
Monodepth2 [14] and MonoViT [53] which overestimates (c) L→R: Ground truth, pre-the depth of the highlight. dicted depth of [14, 53] with our 3D distillation. expensive hardware and is time-consuming [12, 4, 39]. In contrast, self-supervised monocular depth estimation (SS-MDE) [11, 55, 13, 14] only requires posed images as train-ing data, such as stereo pairs and video sequences, and is therefore important for domains such as autonomous driv-ing and virtual/augmented reality where the scalability of the data acquisition for various environments and camera setups matters. As a consequence, SSMDE has drawn much attention in recent years [43, 42].
Fundamentally, training an SSMDE model is based on the photometric loss [14]: given (i) the relative pose be-tween two frames (source and target), (ii) the camera in-trinsic parameters and (iii) the predicted depth map of the target frame, one can transform the source image into
(a) (b) (c)
Figure 2: (a) Images from the ScanNet train set [4] with annotated highlights. (b) Depth predictions of Monodepth2 [14]. (c) Mesh [32] of the table showing that the reflective surface can still be reconstructed correctly by aggregating the predicted depth from different view directions. The artifacts from the overestimated depths are occluded by the correct mesh surface. the target view direction with interpolation-based warping.
The photometric loss can then be used to guide the train-ing of the underlying depth estimation model. The effec-tiveness of SSMDE in outdoor applications such as au-tonomous driving [12] has been demonstrated in many prior works [11, 55, 13, 34, 14, 15, 16, 22, 45].
On the other hand, applying SSMDE to indoor scenes [4, 39] is challenging due to commonly observed reflective sur-faces such as shiny floors, tables, screens, etc. As illus-trated in Fig. 1a, predicting the correct depth of a reflec-tive surface does not minimize the photometric loss due to view-dependent effects violating the photometric constancy assumption. Specifically, perceiving depth at the mirror im-age of a light source on the reflective surface, appearing as a virtual faraway object, minimizes the said loss. Conse-quently, the network learns to predict overestimated depth for the specular reflection, see Fig. 1b. However, this issue has not been studied enough.
To address this issue, we propose 3D distillation: a gen-eral training framework to improve SSMDE on reflective surfaces. As shown in Fig. 2a and Fig. 2b, we observe that specular highlights are view-dependent, and that there are some view directions in which the surface appearance is not contaminated by them. Thus, reflective surfaces can be ac-curately reconstructed by aggregating the predicted depth of these views, as shown in Fig. 2c. Inspired by this ob-servation, we utilize the projected depth of reconstructed scenes to generate accurate depth pseudo-labels for chal-lenging reflective surfaces. However, while the projected depth is more accurate at reflective surfaces, it is lacking high-frequency details due to volumetric averaging over multiple views. To overcome this over-smoothing prob-lem, we propose a fusion scheme in which the projected and predicted depth are combined under the guidance of an uncertainty map associated to the predicted depth. Our 3D distillation is agnostic to the underlying network architec-tures [14, 29, 53] and significantly improves the depth pre-diction accuracy on reflective surfaces, as shown in Fig. 1c.
We highlight the contributions of this paper as follows: 1. We propose 3D distillation: a novel training frame-work that utilizes multi-view 3D information to im-prove depth prediction accuracy on reflective surfaces without adding computational cost or model parame-ters during inference. 2. We originally fuse the predicted and projected depth for pseudo-label generation, and propose an uncertainty-based approach that accurately identifies specular highlights. 3. To validate the effectiveness, we select a subset of the
ScanNet dataset [4] which is rich in specular reflec-tions and glossy surfaces and thus provide a foundation for benchmarking future works tackling this issue. 4. Through extensive evaluations, we show that 3D dis-tillation significantly improves the depth accuracy of reflective surfaces on ScanNet [4] and 7-Scenes [39], while being agnostic to the underlying networks. 2.