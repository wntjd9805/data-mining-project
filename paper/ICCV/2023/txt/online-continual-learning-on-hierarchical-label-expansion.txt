Abstract
Continual learning (CL) enables models to adapt to new tasks and environments without forgetting previously learned knowledge. While current CL setups have ignored the relationship between labels in the past task and the new task with or without small task overlaps, real-world scenarios often involve hierarchical relationships between old and new tasks, posing another challenge for traditional
CL approaches. To address this challenge, we propose a novel multi-level hierarchical class incremental task con-figuration with an online learning constraint, called hier-archical label expansion (HLE). Our configuration allows a network to first learn coarse-grained classes, with data labels continually expanding to more fine-grained classes in various hierarchy depths. To tackle this new setup, we propose a rehearsal-based method that utilizes hierarchy-aware pseudo-labeling to incorporate hierarchical class in-formation. Additionally, we propose a simple yet effective memory management and sampling strategy that selectively adopts samples of newly encountered classes. Our experi-ments demonstrate that our proposed method can effectively use hierarchy on our HLE setup to improve classification accuracy across all levels of hierarchies, regardless of depth and class imbalance ratio, outperforming prior state-of-the-art works by significant margins while also outperforming them on the conventional disjoint, blurry and i-Blurry CL setups. 1.

Introduction
In real-world continual learning scenarios, new knowl-edge often augments existing understanding, typically fol-lowing a hierarchical path from general to specific classes.
This hierarchical structure is not an anomaly, but rather an inherent part of many disciplines. The schema theory [9; 43] in cognitive psychology and the conceptual clustering the-ory [28] in machine learning both emphasize hierarchical
∗ Equal contribution, † Corresponding authors. organization of knowledge. The COBWEB algorithm [20], a prominent machine learning method, uses hierarchical clustering for grouping related instances into meaningful categories. Hierarchical organization is also observed in bi-ology’s taxonomy theory [8], classifying organisms based on shared traits, and in chemistry [27], where elements are arranged hierarchically according to their atomic proper-ties. However, despite the prevalence of hierarchical rela-tionships in these areas, many previous continual learning works [3; 5; 6; 31] do not fully incorporate these relation-ships. This may be an area that needs more attention, as hierarchical relationships could play a role in knowledge evolution in incremental learning.
Here we introduce a novel CL setup called Hierarchi-cal Label Expansion (HLE), designed to account for hier-archical class relationships in task-free online CL. In HLE, class learning is incremental, with fine-grained classes de-rived from prior coarse-grained ones, effectively mirroring real-world knowledge accumulation. As our proposed ap-proach is designed for online continual learning, where data is seen only once in the data stream, each task’s data is dis-joint. We assess our models’ performance using any-time inference [31] and evaluate classification accuracy for all levels of hierarchy. This demonstrates the potential of our approach to complement existing CL methods and enhance their evaluation. HLE encompasses both single and multi-ple hierarchy depths, as well as balanced and imbalanced class data scenarios. To tackle the CL on HLE, we propose a new CL method that utilizes pseudo-labeling based memory management (PL) and flexible memory sampling (FMS).
This method effectively exploits hierarchy information be-tween class labels in the dataset, resembling how knowl-edge is accumulated in real-world scenarios. Extensive ex-periments demonstrate that our approach outperforms state-of-the-art methods by substantial margins in HLE, while re-maining superior in performance on existing CL setups in-cluding disjoint, blurry [5] and i-Blurry [31].
We summarize our contributions as follows: 1. We propose new online class-incremental, hierarchy-aware, task-free CL setups called HLE, designed to
Figure 1. Comparison sketch between conventional, blurry, and our HLE setups. (a) Conventional task-free online CL setup gradually introduces new classes and classifies data without task identification (b) Blurry task-free online CL setup where classes are divided into major and minor categories at each task, with varying proportions, leads to unclear task boundaries (c) Proposed HLE CL setup features class label expansion where child class labels are added to parent class labels throughout the learning process. simulate how knowledge is accumulated in real-world scenarios. 2. We propose a new online CL method, PL-FMS, that consists of pseudo-labeling (PL) based memory man-agement and flexible memory sampling (FMS) to bet-ter exploit hierarchy information and address the HLE setup. 3. We evaluate our approach on CIFAR100, Stanford-Cars, iNaturalist-19, and a novel dataset named
ImageNet-Hier100, demonstrating that our method outperforms prior state-of-the-art works by significant margins on HLE while still outperforming them on the existing disjoint, blurry and i-Blurry CL setups. 2.