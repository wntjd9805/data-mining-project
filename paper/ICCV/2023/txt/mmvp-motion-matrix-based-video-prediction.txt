Abstract
A central challenge of video prediction lies where the system has to reason the objects’ future motions from image frames while simultaneously maintaining the consistency of their appearances across frames. This work introduces an end-to-end trainable two-stream video prediction frame-work, Motion-Matrix-based Video Prediction (MMVP), to tackle this challenge. Unlike previous methods that usu-ally handle motion prediction and appearance maintenance within the same set of modules, MMVP decouples motion and appearance information by constructing appearance-agnostic motion matrices. The motion matrices represent the temporal similarity of each and every pair of feature patches in the input frames, and are the sole input of the motion prediction module in MMVP. This design improves video prediction in both accuracy and efficiency, and re-duces the model size. Results of extensive experiments demonstrate that MMVP outperforms state-of-the-art sys-tems on public data sets by non-negligible large margins (≈ 1 db in PSNR, UCF Sports) in significantly smaller model sizes (84% the size or smaller). Please refer to this link for the official code and the datasets used in this paper. 1.

Introduction
Video prediction aims at predicting future frames from limited past frames. It is a longstanding yet unsolved prob-lem studied for decades [13, 3]. Advancing research in this area benefits various applications such as video com-pression [29, 53, 28], surveillance systems [57, 59, 7], and robotics [9, 16, 8]. The task can be essentially broken down into two sub-tasks: i) motion prediction and ii) frame syn-thesis. Each sub-task has its unique goal that cannot be simply accomplished by achieving the other one. For the sub-task of motion prediction, systems need to reason the future movements of objects/backgrounds by discovering the motion cues hidden in the past frames. Whereas for
*Equal contributions.
†The work is done during Yiqi Zhong’s internship at Microsoft.
‡Corresponding authors. the sub-task of frame synthesis, systems need to maintain appearance features and generate future frames that keep appearance consistency. These separated goals make video prediction inherently much more difficult than the individ-ual task of normal motion prediction or content synthesis.
Figure 1: Performance comparison on UCF Sports with
STIP [4] and SimVP [12]. The hard subset contains sam-ples where SSIM between the last observed frame and the first future frame is smaller than 0.6, which indicates drastic motion patterns (data is from Tables 2 and 6).
Most existing works in video prediction are based on a single-stream pipeline that conducts motion prediction and appearance feature extraction for frame synthesis within the same set of modules. Their systems [47, 45, 46, 55, 5, 4] usually grow out of advanced network structures for se-quential data analysis, such as recurrent neural networks (RNNs) [31] and transformers [42]. One shared characteris-tic of those methods is that they show excellent capabilities in capturing complex motion patterns but lower capabili-ties in appearance maintenance, yielding “correct” but not
“good” synthesized frames. The reason behind this is that those methods usually contain complicated spatial-temporal feature extraction and state transition operations, which can cause unavoidable appearance information loss.
Figure 2: MMVP is a two-stream video prediction framework. It decouples motion prediction and appearance maintenance, and it reunites motion and appearance features through feature composition operation.
Researchers have proposed several solutions to mitigate appearance information loss; the most common approaches are introducing sophisticated appearance-aware state transi-tion unit [5, 4, 54], or adding frequent feature shortcuts from previous frames [12, 46, 47, 45]. However, the former solu-tion tends to build cumbersome models with a huge number of parameters; and for the latter solution, too much residual information from previous frames can cause a larger perfor-mance drop for hard cases such as videos with fast move-ments and/or moving cameras. Figure 1 shows the compar-ison between STIP [4] (an example of the former solution) and SimVP [12] (an example of the latter solution).
To avoid running into possible trade-offs between mo-tion and appearance in single-stream pipelines, a few works have explored two-stream pipelines [27, 2, 10, 43, 11], de-coupling motion prediction and appearance maintenance.
However, they either require auxiliary sub-networks such as optical flow estimator [27, 52] and key point detector [11] to generate motion representations, which complicates video prediction and reduces the generalizability of systems; or they do not provide an efficient solution to reunite the pre-dicted motion and the appearance features [6, 43].
With these gaps in current research, we introduce a novel two-stream, end-to-end trainable framework for video pre-diction: Motion-Matrix-based Video Prediction (MMVP) (see Figure 2 for the framework overview). As the name indicates, MMVP uses motion matrices as the decoupled motion representation of video frames. The motion ma-trix is a 4D matrix representing the image feature patches of consecutive frames (see Figure 3 I). As motion matrices are the sole input of the matrix predictor (i.e., the motion prediction module in MMVP), MMVP specifies the hidden motion information and makes the matrix predictor only fo-cus on motion-related information. For the reunion of mo-tion and appearance features, MMVP gets inspiration from the image autoencoder. It first embeds the past frames in-dividually through an image encoder. Then it composes the embedding of the future frames using the predicted motion matrices output by the matrix predictor and the past frames’ embeddings through matrix multiplication (see Figure 3 II).
Then, an image decoder decodes the composed embeddings into the predicted future frames.
The advantage of MMVP is three-fold: (i) MMVP de-couples motion and appearance by constructing motion ma-trices, requiring no extra construction modules; (ii) unlike optical flow that describes the one-to-one relationship be-tween pixels, motion matrices describe the many-to-many relationship between feature patches, and are more flex-ible and applicable for real-world data; (iii) MMVP re-unites the appearance and motion prediction results through matrix multiplication, which is interpretable and of little information loss. The advantages make MMVP a much more compact model with significantly fewer parameters yet still matching SOTA methods in performance. We vali-date MMVP on three datasets, UCF Sports [36], KTH [38], and MovingMNIST [41]. Experiments show that MMVP matches or surpasses SOTA methods on all three datasets across metrics. Specifically, compared to STIP [4], MMVP uses 84% fewer parameters (18M vs. 2.8M) but achieves 38% better performance in the LPIPS metric (12.73 vs. 7.88) on the UCF Sports dataset (Table 4). 2.