Abstract
We present DINAR, an approach for creating realistic rigged fullbody avatars from single RGB images. Simi-larly to previous works, our method uses neural textures combined with the SMPL-X body model to achieve photo-realistic quality of avatars while keeping them easy to an-imate and fast to infer. To restore the texture, we use a latent diffusion model and show how such model can be trained in the neural texture space. The use of the diffu-sion model allows us to realistically reconstruct large un-seen regions such as the back of a person given the frontal view. The models in our pipeline are trained using 2D im-ages and videos only.
In the experiments, our approach achieves state-of-the-art rendering quality and good gen-eralization to new poses and viewpoints. In particular, the approach improves state-of-the-art on the SnapshotPeople public benchmark. 1.

Introduction
The use of fullbody avatars in the virtual and aug-mented reality applications [34] is one of the drivers be-hind the recent surge of interest in fullbody photorealistic avatars [42, 4, 40]. Apart from the realism and fidelity of avatars, the ease of acquisition of new personalized avatars is of paramount importance. Towards this end, several works propose methods to restore 3D textured model of a human from a single image [48, 49, 19, 4] but such models require additional efforts to produce rigging for animation.
The use of additional rigging methods significantly compli-cates the process of obtaining an avatar and often restricts the poses that can be handled. At the same time, some of the recent methods use textured parametric models of hu-man body [56, 28] while applying inpainting in the texture space. Current texture-based methods, however, lack photo-realism and rendering quality.
An alternative to using classical RGB textures directly is to use deferred neural rendering [53]. Such approaches make it possible to create human avatars controlled by the parametric model [29, 40]. The resulting avatars are more photo-realistic and easier to animate. However, existing ap-proaches require a video sequence to create an avatar [41].
The StylePeople system [16], which is also based on de-ferred neural rendering and parametric model, provides an opportunity to create avatars from single images, however the quality of rendering for unobserved body parts is low.
We propose a new method to create photo-realistic an-imatable human avatars from a single photo. To make the avatars animatable we leverage neural texture approach [53] along with the SMPL-X parametric body model [40]. We propose a new architecture for generating the neural tex-tures, in which the texture comprises both the RGB part explicitly extracted from the input photograph by warp-ing and additional neural channels obtained by mapping the image to a latent vector space and decoding the result into the texture space. As is customary with neural ren-dering [53, 16, 41], the texture generation is trained in an end-to-end fashion with the rendering network.
To restore the neural texture for unobserved parts of the human body we develop a diffusion model [22]. This ap-proach allows us to obtain photo-realistic human avatars from single images.
In the presence of multiple im-ages, we can merge neural textures corresponding to dif-ferent images while restoring parts that are still missing by diffusion-based inpainting. The use of diffusion for inpainting distinguishes our approach from several previ-ous works [28, 20, 17] including StylePeople [16] that rely on generative adversarial framework [15] to perform in-painting of human body textures. As in other image do-mains [10, 47], we found that the use of diffusion alleviates problems with mode collapse and allows to obtain plausible samples from complex multi-modal distributions. To the best of our knowledge, we are the first to extend the diffu-sion models to the task of generating human body textures.
Figure 1: Animations of one-shot avatars. We generate avatars of previously unseen people from single images and animate the avatars by changing their SMPL-X poses. Our model produces plausible results for new poses and views, even for complex and loose garments like skirts and can handle complex poses (such as hands near the body) gracefully.
To sum up, our contributions are as follows:
• We propose a new approach for modeling human avatars based on neural textures that combine the RGB and the latent components.
• We adapt the diffusion framework for neural textures and demonstrate that it is capable of inpainting such textures.
• We demonstrate the ability of our system to build real-istic animatable avatars from a single photograph.
The proposed approach allows us to obtain a photorealis-tic animatable person’s avatar from a single image. Specif-ically, when the input photograph is taken from the front, we observe that the person’s back is restored in a consistent photo-realistic manner. We demonstrate the effectiveness and accuracy of our approach on real-world images from
SnapshotPeople [3] public benchmark and images of peo-ple in natural poses. 2.