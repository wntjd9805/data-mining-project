Abstract
Current video captioning efforts most focus on describ-ing a single video while the need for captioning videos in groups has increased considerably. In this study, we pro-pose a new task, group video captioning, which aims to in-fer the desired content among a group of target videos and describe it with another group of related reference videos.
This task requires the model to effectively summarize the target videos and accurately describe the distinguishing content compared to the reference videos, and it becomes more difficult as the video length increases. To solve this problem, 1) First, we propose an efficient relational approx-imation (ERA) to identify the shared content among videos while the complexity is linearly related to the number of videos. 2) Then, we introduce a contextual feature refinery with intra-group self-supervision to capture the contextual information and further refine the common properties. 3) In addition, we construct two group video captioning datasets derived from the YouCook2 and the ActivityNet Captions.
The experimental results demonstrate the effectiveness of our method on this new task. 1.

Introduction
Video captioning aimed to understand the scene and de-scribe it in words has recently attracted extensive research attention. Currently, mainstream video captioning works mostly focus on describing individual videos [27, 8, 17, 34].
However, since the amount of online videos has been grow-ing at an exponential rate, the need for captioning the video groups has increased considerably like titling a categorized video folder and query suggestions for text-based video re-trieval. Although [16] has studied group image captioning which boosts many real-world applications, there is no ex-isting work in the literature that addresses the task of group-based video captioning.
Thus, we are inspired by the group image captioning[16] and propose the novel problem of group video caption:
* Equal contribution.
† Corresponding author
Figure 1. An example of context-aware group video captioning.
We aim to generate a description chop the onion that best describes the target group (shown in the green area) with the contextual in-formation from the reference group (shown in the yellow area). given a group of target videos and a group of reference videos, to generate a description that simultaneously iden-tifies both important generalities arising in target videos, as well as, particularities captured from reference videos. A promising application scenario is shown in Figure 1, the search engine returns a group of topically close videos with the query of the onions and the user indicates his/her inter-est in some of the videos (i.e. the target group shown in green area). With the remaining videos (i.e. the reference group shown in yellow area), we can infer the user’s hidden preferences among multiple events in the target videos and suggest a refined search query chop the onions accordingly.
Compared to the conventional setting of single-based video captioning, the challenges of our group-based video captioning are two-fold: 1) identifying which temporal fea-tures correspond to the shared content for videos in the group, and 2) distinguishing the shared content of target videos from all videos in the reference group, i.e. group-level distinctiveness.
For identifying the shared content, the method of group image captioning is not suitable for video since the dispar-ity of solution space size. We argue that the fundamental issue of group-based captioning among group videos lies in modeling the long sequence relevance from the cross-video perspective in an efficient manner. Following this premise, we first investigate the traversal method whose computation complexity is O (mn) for n videos with m frames. Then, we further introduce an Efficient Relational Approximation (ERA) to summarize the shared content in video groups. In particular, we find a new random feature mapping that can be equivalent to the softmax-kernel method and the com-plexity scales linearly O (n) with the number of videos.
To achieve group-level distinctiveness, we propose the contextual feature refinery which can learn to capture the salient feature difference between target and reference videos precisely. Specifically, the contextual feature refin-ery enables cross-group interactions between target and ref-erence video groups through a multi-layer co-attention. To avoid the model mainly extracting information from certain unrelated/noisy content like background, we further intro-duce Intra-Group Contrastive (IGC) learning into the re-finery. The key idea of IGC is leveraging the intra-group self-supervision to learn desirable representations that keep alignment between semantically-related the contextual fea-ture and the shared target group feature.
As the first step in this type of problem, we constructed two new datasets for our task by using the existing dense video captioning dataset YouCook2[37] and ActivityNet
Captions[15]. The reason is that dense video captioning dataset annotations map sentences describing events to the segments in the videos which is easier to be grouped. Spe-cially, we parse the single-segment caption and use the shared verb phrases as the groups’ ground-truth captions.
Our main contributions can be summarized as:
• We propose a novel task of group video captioning that can boost many real-world applications like video re-trieval and classification.
• We introduce a new model for group video captioning with an efficient relational approximation that summa-rizes relevant shared information in the groups. Also, our model proposes a contextual feature refinery to capture discriminative information.
• We constructed two new video datasets specifically for the group captioning problem. Experiments on the two datasets demonstrate that our model outperforms vari-ous baselines on the group video captioning task. 2.