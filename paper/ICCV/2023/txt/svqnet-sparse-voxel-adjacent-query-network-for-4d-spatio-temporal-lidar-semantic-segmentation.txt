Abstract
LiDAR-based semantic perception tasks are critical yet challenging for autonomous driving. Due to the motion of objects and static/dynamic occlusion, temporal information plays an essential role in reinforcing perception by enhanc-ing and completing single-frame knowledge. Previous ap-proaches either directly stack historical frames to the cur-rent frame or build a 4D spatio-temporal neighborhood us-ing KNN, which duplicates computation and hinders real-time performance. Based on our observation that stacking all the historical points would damage performance due to a large amount of redundant and misleading informa-tion, we propose the Sparse Voxel-Adjacent Query Network (SVQNet) for 4D LiDAR semantic segmentation. To take full advantage of the historical frames high-efficiently, we shunt the historical points into two groups with reference to the current points. One is the Voxel-Adjacent Neighborhood carrying local enhancing knowledge. The other is the His-torical Context completing the global knowledge. Then we propose new modules to select and extract the instructive features from the two groups. Our SVQNet achieves state-of-the-art performance in LiDAR semantic segmentation of the SemanticKITTI benchmark and the nuScenes dataset. 1.

Introduction
Serving as a robust 3D perception solution, LiDAR-based perception is under enthusiastic exploration by re-searchers, among which 3D LiDAR semantic segmentation, aiming at assigning a category label to each point in the whole LiDAR scene at the semantic level, is of great signifi-cance in autonomous driving and robotics. Recently, a large number of literature [33, 43, 45, 28, 13, 19] concentrates on semantic segmentation within a single frame. However, the information in a single frame is affected by multiple factors: 1) occlusion problems caused by obstacles or the move-ment of ego-car, leading to incomplete information of the
*Equal contribution
†Corresponding authors
Figure 1. 4D spatio-temporal LiDAR points for a truck. We shunt the historical points into 1) Voxel-Adjacent points that lie in the voxel containing current frame points; 2) The remaining points named Historical Context points whose features will be adaptively fused to complete the missing features in the current frame. occluded objects; and 2) ambiguity between similar point clusters, for example, the fence looks similar to one side of a big truck, which severely degrades the performance of single-frame based LiDAR semantic segmentation.
To eliminate the distortion within the single frame, the use of sequential knowledge has attracted widespread at-tention [24, 44, 42, 1, 39] as the LiDAR continuously trans-mits and receives sensory data. Therefore, 4D Spatio-temporal information (LiDAR video) is increasingly play-ing an essential role in reinforcing perception by enhancing and completing single-frame knowledge. Classic temporal methods [1, 48] directly stack frames in the last few times-tamps by adding additional channel t to the coordinates xyz of each point, which is quite straightforward but super-imposing all historical points without any selection brings redundancy, masking the useful temporal knowledge and weakens the benefits of the time series.
To model spatio-temporal relationship instead of stack-ing all frames, approaches based on KNN or radius neigh-bors query [21, 4, 31, 37] apply point-wise nearest neighbor search methods to extract instructive features across time and space. However, these approaches will not only fail when the target object is moving at high speed but also bear the high complexity of searching algorithms that lead to the
inability to adopt long time-series information. Other ap-proaches based on RNN [15, 10, 18] or memory [9, 23] using a recurrent neural network or sequence-independent storage to memorize the instructive features from historical frames, can model the long sequence knowledge. Neverthe-less, these approaches are unable to align recurrent features under sparse representation and thus adopt a range-image view [8, 38], which are impossible to gain from the sparse representation [12] of point clouds.
To efficiently extract valuable spatio-temporal features in 3D voxel representation, we propose a Sparse Voxel-Adjacent Query Network (SVQNet). Our SVQNet shunts the historical information into two groups based on the ob-servation from Fig. 1: 1) Voxel-Adjacent Neighborhood: historical points around points in current frame can enhance the spatial semantic features from sparse to dense across time to disambiguate current frame semantics; 2) Historical
Context: some occlusion can be completed from multiple frames in a learning manner, by activating valuable histori-cal context according to current voxel features. Unlike pre-vious work [31, 21] requiring the calculation and sorting of distances between current and historical points to find near-est neighbors, the search of Voxel-Adjacent is highly effi-cient, which performs sparse hash query from current points to historical points, under several scales from small to big, acquiring spatio-temporal neighbors from near to far. The sparse hash algorithm allows us to reduce the complexity from quadratic to linear, which further endows us with real-time performance. The proposed SVQNet achieves state-of-the-art performance on SemanticKITTI [2] and nuScenes
[3] datasets while maintaining a real-time runtime. Our main contributions are as followed:
• The Spatio-temporal information is formulated as en-hancing and completing in the first time, with a novel
Spatio-Temporal Information Shunt module to effi-ciently shunt the stream of historical information.
• An efficient Sparse Voxel-Adjacent Query module is proposed to search instructive neighbors in 4D sparse voxel space, and extract knowledge from the Voxel-Adjacent Neighborhood.
• The learnable Context Activator is introduced to acti-vate and extract historical completing information.
• We furthermore introduce a lightweight Temporal Fea-ture Inheritance method to collect features of historical frames and reuse them in the current frame. 2.