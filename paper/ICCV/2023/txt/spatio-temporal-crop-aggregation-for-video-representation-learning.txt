Abstract
We propose Spatio-temporal Crop Aggregation for video representation LEarning (SCALE), a novel method that en-joys high scalability at both training and inference time.
Our model builds long-range video features by learning from sets of video clip-level features extracted with a pre-trained backbone. To train the model, we propose a self-supervised objective consisting of masked clip feature pre-dictions. We apply sparsity to both the input, by extracting a random set of video clips, and to the loss function, by only reconstructing the sparse inputs. Moreover, we use di-mensionality reduction by working in the latent space of a pre-trained backbone applied to single video clips. These techniques make our method not only extremely efficient to train but also highly effective in transfer learning. We demonstrate that our video representation yields state-of-the-art performance with linear, nonlinear, and k-NN prob-ing on common action classification and video understand-ing datasets. 1.

Introduction
Videos provide rich and detailed information about ob-jects and their activities. Their analysis is, however, made challenging not only by the difference in the information provided across space and time but also by the high dimen-sionality of the data [45, 29, 70]. While computational re-sources are expected to scale over time, so is the demand for higher data resolution (both in space and time) and also the need for processing data with even more dimensions, such as videos of volumetric data [10]. Therefore, it is of paramount importance to explore methods that drastically reduce the computational requirements to process videos.
Moreover, the annotation of videos is an extremely costly and time-consuming burden that makes the use of models pre-trained in a self-supervised manner essential [44].
Self-supervised learning (SSL) is a very popular tech-nique to reduce the need for annotation because it can build useful representations from unlabeled data through an arti-ficial goal, also called pseudo- or pretext-task. These repre-sentations can either be evaluated through K-nearest neigh-bor or linear probing [71, 72] or through fine-tuning (i.e., as the initialization parameters of the trained model) [25, 54, 35] on a downstream task, where only a small labeled dataset is available. More remarkably, SSL pre-trained models can outperform models that were pre-trained on an annotated dataset [53, 4].
SSL methods for video representation learning present fundamental scalability challenges [54, 45, 29, 65]. A first major challenge is that training models from scratch for any new pseudo-task is not sustainable. A more viable setting is one where the processing of videos is broken down into multiple steps. First, one pre-processes video clips (spatio-temporal crops) only once via some pre-trained general-purpose model (e.g., trained via self-supervised learning
[18, 23, 26, 44, 7, 37, 41]). In general, the idea is that this (compressed) local representation is stored and used later for other training purposes or retrieval. Then, one can train more efficiently other super-features that aggregate infor-mation from these local representations. Within this set-ting, we address two questions: 1) Can one further improve the performance of pre-computed video features by train-ing a model on top of them? 2) Can such training be made computationally scalable? In this paper, we show that both questions have a positive answer. The answer exploits four fronts: 1. Input Sparsity: Sparsity in the input to the model [54, 25, 3, 19, 1] drastically reduces computational load and memory requirements, leveraging information redun-Inspired by prior dancy in images and videos [16]. work, we extract a sparse set of clips from a video (these are spatio-temporal video crops and not image patches or video tubelets); each clip is fed separately to a neural network so that we obtain a set of video clip representations. 2. Output Sparsity: Another way to reduce the compu-tational cost is to use a sparse reconstruction output instead of a dense one [71, 51, 59, 5]. This is not just a reduction of the number of terms in the loss function, but also in the number of actual outputs of
the model, and thus also a reduction of the compu-tations needed to obtain them. This is in contrast to
MAE-based SSL methods for vision, where the pro-posed pseudo-tasks are based on the reconstruction of the whole input [25, 54, 19, 16], and even if the loss uses a subset of the tokens (the masked ones) for the loss calculation, the remaining tokens are still part of the decoder’s output and computational graph. 3. Dimensionality Reduction:
Inspired by prior work [71, 41, 13], instead of directly processing the raw input data, we work in the latent space. This allows us to further reduce the dimensionality of both input and output data. 4. Use of a Pre-trained Backbone: To reduce training time and further speed up the processing per iteration, we exploit SSL pre-trained models. These pre-trained backbones already learn very strong short-term spatio-temporal features, and our approach is a way to extract longer-term features (see LVU results 2) by building a video representation on top of a set of pre-trained features (one for each video clip).
To integrate all these components, we propose a novel
SSL method that we call Spatio-temporal Crop Aggregation for video representation LEarning (SCALE). Given an input video, SCALE extracts a random set of clips and produces an embedding for each clip through a pre-trained backbone, which is kept frozen. These initial embeddings are then augmented in two ways: 1) each one is refined into a more discriminative feature, and 2) the set of all embeddings is summarized in a global feature. These global features can learn long-term correlations in the whole video by aggregat-ing the short-term information in each clip embedding. The combination of the initial embeddings with their refinement and the global feature is then used in an ensemble for appli-cations on new downstream tasks.
To train SCALE, we introduce two novel pseudo-tasks, which aim to improve the discriminability of the embed-dings of each video clip as well as obtain a global repre-sentation of the video. One task, which we call Masked
Clip Modeling (MCM), is the reconstruction of a video clip embedding as in masked autoencoders [54]. Masked em-beddings are combined with positional encodings so that the model can (spatio-temporally) relate the missing input embeddings to the other available embeddings (similarly to
BERT in Natural Language Processing [12]). A second task is to train the model to output a global feature token (which we refer to as CLS, just for consistency with previ-ous works [7, 12, 14, 55, 9]) for a set of clips via contrastive learning, so that the global feature is invariant to the cho-sen set of clips from the same video, but can discriminate summary features of clips from other videos. Both tasks are trained via contrastive losses.
SCALE consistently improves upon pretrained features, with more evident gains considering the computational cost of other methods. For instance, SCALE outperforms Video-MAE [54] by 0.3% (1600 epochs checkpoint – fine-tuning) and ρBYOL [18] by 0.6% (800 epochs checkpoint – linear probe) on Kinetics400 with 64 V100 GPUs in about 5 mins.
In contrast, VideoMAE takes 27.7 hours to gain 0.5% (800 to 1600 epochs checkpoint), and ρBYOL needs 48 hours to gain 0.4% (200 to 400 epochs checkpoint) with the same resources.
To summarize our contributions, we propose SCALE, a novel and highly scalable video representation method that
• is trained via novel pseudo-tasks on sets of video clips (in contrast to existing methods that work only with pairs of clips at a time [44, 18, 45]);
• results in video feature representations with a sig-in k-NN (re-nificant performance improvement trieval), linear, and nonlinear probing across a wide range of datasets for action classification (UCF [48],
HMDB [34], SSv2 [22], Kinetics400 [33]) and long-form video understanding (LVU [63]);
• achieves consistent transfer learning performance im-provement across diverse SotA pre-trained backbones (architectures, scale, and pre-training tasks). For ex-ample, our nonlinear probed model even outperforms fully fine-tuned SVT [44] on HMDB [34]. 2.