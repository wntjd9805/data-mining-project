Abstract
Light plays an important role in human well-being.
However, most computer vision tasks treat pixels without considering their relationship to physical luminance. To ad-dress this shortcoming, we introduce the Laval Photometric
Indoor HDR Dataset, the first large-scale photometrically calibrated dataset of high dynamic range 360◦ panoramas.
Our key contribution is the calibration of an existing, un-calibrated HDR Dataset. We do so by accurately captur-ing RAW bracketed exposures simultaneously with a profes-sional photometric measurement device (chroma meter) for multiple scenes across a variety of lighting conditions. Us-ing the resulting measurements, we establish the calibration coefficients to be applied to the HDR images. The resulting dataset is a rich representation of indoor scenes which dis-plays a wide range of illuminance and color, and varied types of light sources. We exploit the dataset to introduce three novel tasks, where: per-pixel luminance, per-pixel color and planar illuminance can be predicted from a single input image. Finally, we also capture another smaller pho-tometric dataset with a commercial 360◦ camera, to exper-iment on generalization across cameras. We are optimistic that the release of our datasets and associated code will spark interest in physically accurate light estimation within the community. Dataset and code are available at https:
//lvsn.github.io/beyondthepixel/. 1.

Introduction
Natural light has shaped the way our human visual sys-tem evolved [13], plays a key role in driving our circadian rhythm [16], and affects our mental health [47] and social organization [11]. It has also been shown [54] that human vision relies on stable properties of light, measured in terms of luminance (in cd m−2), in order to perceive object fea-tures such as shape and color.
Natural light is also at the heart of photography and computer vision. However, most if not all computer vi-sion approaches consider pixel values as a 3-channel in-put to be processed without considering the relationship be-tween pixel intensity and luminance. This is understandable since modern digital cameras pursue a goal different from measuring physically accurate perceived brightness: they strive to create visually pleasing photographs. In doing so, their internal image signal processors (ISP) perform a se-ries of operations on the measured light (denoising, contrast enhancement, tonemapping, etc. [34]) in order to produce pixel values which, while visually appealing, no longer cor-respond to the physical properties of the environment.
Modeling camera ISPs and inverting their image for-mation process has been the subject of many works (e.g.
[67, 29, 55]). Here, most approaches aim at recovering an image where pixel values are linearly proportional to the scene radiance (or luminance). Closely related are approaches for capturing high dynamic range (HDR) im-ages [15], or predicting HDR from low dynamic range (LDR) photographs [17, 39, 49, 41, 45]. While linear pixel values can be extremely useful for physics-based vision ap-plications (e.g. [27]), the scale factor to absolute luminance is still unknown. Can we go beyond (linear) pixel values and recover per-pixel luminance from a single image?
In this paper, we propose the Laval Photometric Indoor
HDR Dataset: what we believe to be the first large-scale dataset to help the community answer this question. The novel dataset of physically accurate luminance and colors acquired in a wide variety of indoor scenes. Our key idea is to leverage the camera and RAW captures of an existing dataset of HDR indoor 360◦ panoramas that was previously captured [20]. We contribute by carefully calibrating the camera with a chroma meter to determine the per-channel correction factors to be applied to each panorama. Our anal-ysis shows that the Laval Photometric Indoor HDR Dataset
[0 lx, 7000 lx]) contains a wide range of illuminance (e.g. and color, expressed in correlated color temperature (CCT)
[2000 K, 8000 K]), capturing the diversity of indoor (e.g. environments. We also explore the luminance and color dis-tributions of individual light sources in the dataset, which span several orders of magnitude of average luminance.
We present three novel learning tasks that are enabled by our calibrated dataset. Given a single image as input, we ex-plore how per-pixel luminance, per-pixel color, and planar
illuminance can be estimated. More importantly, we also consider what information must be available for accurate light prediction. Indeed, democratizing the process of cap-turing physical luminance begs several important questions: can luminance be accurately estimated using conventional, uncalibrated cameras? If so, is HDR imagery needed or is a single, well-exposed shot sufficient? Is a generic approach appropriate or do methods need to be finetuned to specific cameras? We provide initial answers to these challenging questions by presenting learning experiments on our novel dataset, as well as on another, smaller photometric dataset captured with an off-the-shelf 360◦ camera. By publicly re-leasing the calibrated datasets and associated code, we hope to spur interest in the community and help it consider the physical light measurements that lie beyond the pixels. 2.