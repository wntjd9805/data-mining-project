Abstract
Recent advancements in 3D hand pose estimation have shown promising results, but its effectiveness has primarily relied on the availability of large-scale annotated datasets, the creation of which is a laborious and costly process.
To alleviate the label-hungry limitation, we propose a self-supervised learning framework, HaMuCo, that learns a single-view hand pose estimator from multi-view pseudo 2D labels. However, one of the main challenges of self-supervised learning is the presence of noisy labels and the
“groupthink” effect from multiple views. To overcome these issues, we introduce a cross-view interaction network that distills the single-view estimator by utilizing the cross-view correlated features and enforcing multi-view consistency to achieve collaborative learning. Both the single-view esti-mator and the cross-view interaction network are trained jointly in an end-to-end manner. Extensive experiments show that our method can achieve state-of-the-art perfor-mance on multi-view self-supervised hand pose estimation.
Furthermore, the proposed cross-view interaction network can also be applied to hand pose estimation from multi-view input and outperforms previous methods under the same settings. 1.

Introduction 3D hand pose estimation is essential in various applica-tion scenarios, from action recognition and sign language translation to AR/VR [19, 20]. Hand pose estimation has achieved a significant improvement in recent years. How-ever, the progress heavily relies on the emergence of many hand pose datasets with accurate 3D annotations. Acquir-ing labeled datasets is quite time-consuming and laborious, exposing a realistic challenge for deep learning models to learn with limited and noisy data.
Self-supervised learning is an emerging solution to the
Project page: https : / / zxz267 . github . io / HaMuCo.
* Corresponding author.
† Equal contribution. challenge posed by manual annotation. Though worth ex-ploring, self-supervised pose estimation with RGB hand im-ages is a challenging and relatively unexplored area with only one pioneering method, S2HAND [9]. S2HAND aims to conduct 3D hand reconstruction from a single RGB im-age with the noisy off-the-shell 2D hand pose estimation results for supervision. Unfortunately, S2HAND faces a predicament where its performance is significantly reliant on the quality of the pseudo label, and inferior labeling may result in reduced performance. Moreover, evaluating the quality of the pseudo label is an ill-posed problem that lacks clear criteria or input, further complicating the issue.
This observation motivates us to leverage multi-view in-formation for enhancing self-supervised learning, as the complementary nature of multi-view observations can help mitigate the ambiguity inherent in pose estimation. Al-though the first 3D hand dataset with synchronized multi-view input (HanCo [62]) was proposed in 2021, to our knowledge, there is no previous work exploring the poten-tial of multi-view for self-supervised hand pose estimation.
Therefore, we turn to studies in the human body pose esti-mation, which share some similarities.
As mentioned in previous work [24], naively enforc-ing multi-view consistency is prone to generate degener-ated solutions, thus they resorted to additional 2D labels of unrelated datasets and proposed a solution under the scope of weakly supervised learning. Other studies, such as EpipolarPose [27] and CanonPose [52], utilized multi-view data with special designs to enhance the supervision and achieved promising results under the scope of self-supervised learning.
In this paper, we push along this direction on hand pose estimation via multi-view collaborative learning. We take one step further by designing a learnable network, which utilizes multi-view information, to tackle 1) noisy pseudo labels and 2) unreliable multi-view “groupthink” issues causing training collapse in the early training stage. For-mally, we name the pipeline HaMuCo, which stands for
Figure 1. Overall pipeline comparison: HaMuCo learns a monocular 3D hand pose estimator from multi-view self-supervision via cross-view feature interaction. Our cross-view interaction network addresses the importance of introducing learnable feature interaction, which is absent from previous methods [27, 52]. At inference time only the gray part is applied.
Hand Multiview Collaborative learning.
The core idea of our approach is to enhance the single-view estimation by means of cross-view feature interac-tion and further integrate multi-view results to supervise the single-view output to achieve self-distillation in an end-to-end fashion. Thus, our framework is built with a single-view hand pose estimator and a cross-view interaction net-work for supervision. The single-view estimator uses the
MANO [45] hand model as the decoder, which provides the hand prior to regularizing irrational anatomy when super-vised by noisy pseudo labels. The cross-view interaction network captures cross-view features and utilizes several consistent losses among different views to guide collabo-rative learning.
We conduct comprehensive experiments on the
HanCo [62] dataset and our approach outperforms previous methods by a considerable margin for self-supervised 3D hand pose estimation. Notably, our results demonstrate competitive performance compared to a state-of-the-art fully supervised approach proposed by Chen et al. [7]. Our proposed framework is highly versatile, as it can be trained with or without calibration, and is capable of incorporating the cross-view interaction network to achieve superior multi-view inference results when multi-view test data is available. Moreover, our model can generalize well to other datasets [29, 46, 64] and in-the-wild images.
In summary, our contributions are the following:
• We propose the first self-supervised learning frame-work for single-view hand pose estimation without any training data annotation and achieve state-of-the-art performance via multi-view collaborative learning.
• We propose a cross-view interaction network to super-vise the single-view estimator by enforcing multi-view consistency and capturing cross-view features for col-laborative learning among multiple views.
• The proposed framework is capable of multi-view inference by incorporating the cross-view interac-tion network and achieves state-of-the-art performance without bells and whistles. 2.