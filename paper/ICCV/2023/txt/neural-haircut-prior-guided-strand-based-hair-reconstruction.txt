Abstract 1.

Introduction
Generating realistic human 3D reconstructions using image or video data is essential for various communica-tion and entertainment applications. While existing meth-ods achieved impressive results for body and facial regions, realistic hair modeling still remains challenging due to its high mechanical complexity. This work proposes an ap-proach capable of accurate hair geometry reconstruction at a strand level from a monocular video or multi-view images captured in uncontrolled lighting conditions. Our method has two stages, with the first stage performing joint recon-struction of coarse hair and bust shapes and hair orien-tation using implicit volumetric representations. The sec-ond stage then estimates a strand-level hair reconstruction by reconciling in a single optimization process the coarse volumetric constraints with hair strand and hairstyle pri-ors learned from the synthetic data. To further increase the reconstruction fidelity, we incorporate image-based losses into the fitting process using a new differentiable renderer.
The combined system, named Neural Haircut, achieves high realism and personalization of the reconstructed hairstyles.
For video results, please refer to our project page†.
∗ Work done at Samsung AI Center
† https://samsunglabs.github.io/NeuralHaircut/
We propose a new image-based modeling method that recovers human hair from multi-view photographs or video frames. Hair reconstruction remains one of the most chal-lenging problems in human 3D modeling because of its highly complex geometry, physics, and reflectance. Nev-ertheless, it is critical for many applications, such as special effects, telepresence, and gaming.
In computer graphics, the dominant representation for hair is 3D polylines, or strands, which can facilitate both realistic rendering and physics simulation [6]. At the same time, modern image- and video-based human reconstruc-tion systems often model hairstyles using data structures that have fewer degrees of freedom and are easier to esti-mate, such as meshes with fixed topology [12, 20] or vol-umetric representations [4, 7, 9, 10, 11, 15, 27, 28, 31, 39, 40, 43, 48, 49, 56, 59, 60]. As a result, these methods often obtain over-smoothed hair geometries and can only model the “outer shell” of the hairstyle without its inner structure.
Accurate strand-based hair reconstruction can be accom-plished via controlled lighting equipment and dense capture setup with synchronized cameras, i.e. using light stages [8].
Recently, impressive results were achieved [30, 33, 44, 50, 51] by relying on uniform or structured lighting and camera calibration to facilitate the reconstruction process. The lat-est work [44] further utilized manual frame-wise annotation
of the hair growth directions to achieve physically plausible reconstructions. However, despite the impressive quality of the results, the sophisticated capture setup and the manual pre-processing requirements make such methods unsuitable for many practical applications. Some learning-based meth-ods for hairstyle modeling [5, 16, 22, 45, 54, 55, 58, 61] incorporate hair priors learned from the strand-based syn-thetic data to ease the acquisition process. However, the ac-curacy of these methods naturally depends on the size of the training dataset. Existing datasets [16, 54] typically consist of only a few hundred samples and are inadequately small for handling the diversity of human hairstyles, leading to the low fidelity of the reconstructions.
In this work, we propose a method for hair modeling that uses only image- or video-based data without any additional manual annotations and works in uncontrolled lighting con-ditions. To achieve that, we have designed a two-stage re-construction pipeline. The first stage, coarse volumetric hair reconstruction, employs implicit volumetric represen-tations and is purely data-driven. The second stage, fine strand-based reconstruction, operates at the level of hair strands and relies heavily on priors learned from a small-scale synthetic dataset.
During the first stage, we reconstruct implicit surface representations [38] for hair and bust (head and shoulders) regions. Additionally, we learn a field of hair growth di-rections, which we call 3D orientations, by matching them through a differentiable projection with hair directions ob-served in the training images or 2D orientation maps. While this field can facilitate a more accurate hair shape fitting, its primary use case is to constrain the optimization of hair strands during the second stage. To calculate the hair ori-entation maps from the input frames, we use a classic ap-proach based on image gradients [37].
The second stage relies on pre-trained priors to ob-tain strand-based reconstructions. We employ an improved parametric model learned from the synthetic data using an auto-encoder [44] to represent individual strands and com-bine it with a new diffusion-based prior [14, 18] to model their joint distribution, i.e. a complete hairstyle. This stage thus reconciles the coarse hair reconstruction obtained in the first stage with the learning-based priors through an op-timization process. Lastly, we improve the fidelity of recon-structed hairstyles via differentiable rendering using a new hair renderer based on soft rasterization [26].
To summarize, our contributions are:
• Human head 3D reconstruction method for bust and hair regions, which includes hair orientations;
• Improved training procedure for the strand prior;
• Latent diffusion-based prior for global hairstyle model-ing, which “interfaces” with the parametric strand prior;
• Differentiable soft hair rasterization technique that leads to more accurate reconstructions than the previous ren-dering methods;
• Strand-fitting process that incorporates all the compo-nents discussed above to produce high-quality recon-structions of human hair at the level of strands.
We validate the efficacy of our method on synthetic [57] and real-world data, for which we use multi-view images from a 3D scanner operating in unconstrained lighting con-ditions [43] and monocular videos from a smartphone. 2.