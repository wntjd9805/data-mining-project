Abstract
Human-centric scene understanding is significant for real-world applications, but it is extremely challenging due to the existence of diverse human poses and ac-tions, complex human-environment interactions, severe oc-clusions in crowds, etc. In this paper, we present a large-scale multi-modal dataset for human-centric scene under-standing, dubbed HuCenLife, which is collected in diverse daily-life scenarios with rich and fine-grained annotations.
Our HuCenLife can benefit many 3D perception tasks, such as segmentation, detection, action recognition, etc., and we also provide benchmarks for these tasks to facili-tate related research.
In addition, we design novel mod-ules for LiDAR-based segmentation and action recognition, which are more applicable for large-scale human-centric
*Equal contribution. â€  Corresponding author. This work was sup-ported by NSFC (No.62206173), Natural Science Foundation of Shang-hai (No.22dz1201900), MoE Key Laboratory of Intelligent Perception and Human-Machine Collaboration (ShanghaiTech University), Shang-hai Frontiers Science Center of Human-centered Artificial Intelligence (ShangHAI), Shanghai Engineering Research Center of Intelligent Vision and Imaging. scenarios and achieve state-of-the-art performance. The dataset and code can be found at https://github. com/4DVLab/HuCenLife.git. 1.

Introduction
Human-centric scene understanding in 3D large-scale scenarios is attracting increasing attention [13, 11, 42, 31], which plays an indispensable role in human-centric ap-plications, including assistive robotics, autonomous driv-ing, surveillance, human-robot cooperation, etc. It is often confronted with substantial difficulties since these human-centric scenarios usually have the attributes of various sub-jects with different poses, fine-grained human-object inter-actions, and challenging localization and recognition with occlusions. Moreover, current state-of-the-art perception methods heavily rely on large-scale datasets to achieve good performance. Therefore, to promote the research of human-centric scene understanding, the collection of large-scale datasets with rich and fine-grained annotations is required urgently, which is difficult but of great significance.
In previous work, many studies target on the scene un-derstanding based on the input of image or video [2, 37, 17, 59], which are not applicable to real-world applications due to the limited 2D visual representations. Afterward, some works pay attention to the static indoor-scene understand-ing [12, 1, 5] based on the pre-scanned RGB-D data, which are not suitable for the research of real-time perception. Re-cently, more and more outdoor multi-modal datasets [6, 49] are released equipped with LiDAR point clouds. They pro-vide detailed annotations under complex outdoor scenes, while they often focus on the vehicle-dominated traffic en-vironment and neglect the more challenging human-centric daily-life scenarios. Although the dataset STCrowd [11] ap-pears lately, it focuses on the detection task of dense pedes-trian scenes, lacking varied human activities and diversi-fied annotations. Consequently, the dataset with rich and fine-grained annotations for human-centric understanding in long-range 3D space is crucial and insufficient.
In this paper, to facilitate the research of human-centric 3D scene understanding, we collect a large-scale multi-modal dataset, namely HuCenLife, by using calibrated and synchronized camera and LiDAR. Specifically, the dataset captures 32 multi-person involved daily-life scenes with rich human activities and human-object interactions. Var-ious indoor and outdoor scenarios are both included. For the annotation, we provide fine-grained labels including in-stance segmentation, 3D bounding box, action categories, and continuous instance IDs, which can benefit various 3D perception tasks, such as point cloud segmentation, detec-tion, action recognition, Human-Object Interaction (HOI) detection, tracking, motion prediction, etc.
In this paper, we provide benchmarks for the former three tasks by exe-cuting current state-of-the-art methods on HuCenLife and give discussions for other downstream tasks.
In particular, considering the specific characteristics of human-centric scenarios, we propose effective modules to improve the performance for point cloud-based segmenta-tion and action recognition in the complex human-centric environments. First, we model human-human interactions and human-object interactions and leverage their mutual relationships to benefit the classification of points and in-stances. Second, to solve the problem of the big scale span of objects in daily-life scenarios, we exploit multi-resolution feature extraction strategy to aggregate global features and local features hierarchically so that small ob-jects can be better attended. We evaluate our methods and conduct extensive experiments on HuCenLife. Several ab-lation studies are also conducted to demonstrate the effec-tiveness of each module and good generalization capability.
Our contributions are summarized as follows: 1. We introduce HuCenLife, the first large-scale multi-modal dataset for human-centric 3D scene understand-ing with rich human-environment interactions and fine-grained annotations. 2. HuCenLife can benefit various human-centric 3D per-ception tasks, including segmentation, detection, ac-tion recognition, HOI, tracking, motion prediction, etc.
We provide baselines for three main tasks to facilitate future research. 3. Several novel modules are designed by incorporating fine-grained interactions and capturing features at var-ious resolutions to promote more accurate perception in human-centric scenes. 2.