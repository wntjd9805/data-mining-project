Abstract
In this paper, we propose a robust 3D detector, named
Cross Modal Transformer (CMT), for end-to-end 3D multi-modal detection. Without explicit view transformation,
CMT takes the image and point clouds tokens as inputs and directly outputs accurate 3D bounding boxes. The spatial alignment of multi-modal tokens is performed by encod-ing the 3D points into multi-modal features. The core de-sign of CMT is quite simple while its performance is im-pressive. It achieves 74.1% NDS (state-of-the-art with sin-gle model) on nuScenes test set while maintaining faster inference speed. Moreover, CMT has a strong robustness even if the LiDAR is missing. Code is released at https:
//github.com/junjie18/CMT. 1.

Introduction
Multi-sensor fusion has shown its great superiority in autonomous driving system [31, 8, 22, 1, 27]. Different sensors usually provide the complementary information for each other. For instance, the camera captures information in a perspective view and the image contains rich semantic features while point clouds provide much more localization and geometry information. Taking full advantage of differ-ent sensors helps reduce the uncertainty and makes accurate and robust prediction.
Sensor data of different modalities usually has large dis-crepancy in distribution, making it hard to merge the multi-modalities. State-of-the-art (SoTA) methods tend to fuse the multi-modality by constructing uniﬁed bird’s-eye-view (BEV) representation [31, 27, 22] or querying from to-kens [1, 8]. For example, BEVFusion [31] explores a uni-ﬁed representation by BEV transformation for BEV feature fusion (see Fig. 1(a)). TransFusion [1] follows a two-stage pipeline and the camera images in second stage provide supplementary information for prediction reﬁnement (see
Fig. 1(b)). However, exploring a truly end-to-end pipeline for multi-sensor fusion remains to be a question.
✉ Corresponding author.
Figure 1: Comparison between BEVFusion, TransFusion, and our proposed CMT. (a) In BEVFusion, the camera fea-tures are transformed into BEV space by view transform.
Two modality features are concatenated in BEV space and the BEV encoder is adopted for fusion. (b) TransFusion ﬁrst generates the queries from the high response regions of Li-DAR features. After that, object queries interact with point cloud features and image features separately. (c) In CMT, the object queries directly interact with multi modality fea-tures simultaneously. Position encoding (PE) is added to the multi-modal features for alignment. ”VT” is the view transformation from image to 3D space.
Recently, the effectiveness of end-to-end object detec-tion with transformer (DETR) [3, 60] has been proved in many perception tasks, such as instance segmentation [13, 15], multi-object tracking [55, 33] and visual 3D detec-tion [47, 29, 30]. The DETR architecture is simple yet ef-fective thanks to the object queries for representing different
)
% (
P
A m 72 71 70 69 68 67 66 65 64 63 62
DeepInteration
CMT (Ours)
CMT
BEVFusion
TransFusion
UVTR
FUTR3D 1 2 3
FUTR3D
TransFusion
BEVFusion
CMT (Ours) 4
Speed (FPS) w/o Cams w/o LiDAR
CMT-C
CMT-L mAP 64.2 67.5 68.5
NDS 68.0 71.3 71.4
Speed (FPS) 2.5 3.2 4.2 72.90  68.10  result drop 68.60  44.70  46.00  5 6 7 0 10 20 30 40
NDS(%) 50 60 70 80
Figure 2: Left: Performance comparison between CMT and existing methods. All speed statistics are measured on a single
Tesla A100 GPU using the best model of ofﬁcial repositories. Right: Performance evaluation of CMT under sensor missing.
During inference, CMT achieves vision-based performance when LiDAR is missing, showing strong robustness. instances and bipartite matching for one-to-one assignment.
Inspired by DETR, we aim to build an elegant end-to-end pipeline for multi-modal fusion in 3D object detection.
In DETR, object queries directly interact with the image to-kens through cross-attention in transformer decoder. For 3D object detection, one intuitive way is to concatenate the im-age and point cloud tokens together for further interaction with object queries. However, the concatenated tokens are disordered and unaware of their corresponding locations in 3D space. Therefore, it is necessary to provide the location prior for multi-modal tokens and object queries.
In this paper, we propose Cross-Modal Transformer (CMT), a simple yet effective end-to-end pipeline for ro-bust 3D object detection (see Fig. 1(c)). First, we propose the Coordinates Encoding Module (CEM), which produces position-aware features, by encoding 3D points set implic-itly into multi-modal tokens. Speciﬁcally, for camera im-ages, 3D points sampled from frustum space are used to in-dicate the probability of 3D positions for each pixel. While for LiDAR, the BEV coordinates are simply encoded into the point cloud tokens. Next, we introduce the position-guided queries. Each query is initialized as a 3D reference point following PETR [29]. We transform the 3D coordi-nates of reference points to both image and LiDAR spaces, to perform the relative coordinates encoding in each space.
The proposed CMT framework brings many advantages compared to existing methods. Firstly, our method is a sim-ple and end-to-end pipeline and can be easily extended. The 3D positions are encoded into the multi-modal features im-plicitly, which avoids introducing the bias caused by ex-plicit cross-view feature alignment. Secondly, our method only contains basic operations, without the feature sampling or complex 2D-to-3D view transformation on multi-modal features. It achieves state-of-the-art performance and shows obvious superiority compared to existing approaches, as shown in the left graph of Fig. 2. Thirdly, the robustness of our CMT is much stronger than other existing approaches.
Extremely, under the condition of LiDAR miss, our CMT with only image tokens can achieve similar performance compared to those vision-based 3D object detectors [29, 26] (see the right graph of Fig. 2).
To summarize, our contributions are:
• we propose a fast and robust 3D detector, which is a truly end-to-end framework without any post-process.
It overcomes the sensor missing problem.
• The 3D positions are encoded into the multi-modal to-kens, without any complex operations, like grid sam-pling and voxel-pooling.
• CMT achieves state-of-the-art 3D detection perfor-mance on nuScenes dataset. It provides a simple base-line for future research. 2.