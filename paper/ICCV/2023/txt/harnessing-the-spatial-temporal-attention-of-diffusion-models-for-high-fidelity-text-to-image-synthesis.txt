Abstract
Diffusion-based models have achieved state-of-the-art performance on text-to-image synthesis tasks. However, one critical limitation of these models is the low ﬁdelity of gen-erated images with respect to the text description, such as missing objects, mismatched attributes, and mislocated ob-jects. One key reason for such inconsistencies is the inac-curate cross-attention to text in both the spatial dimension, which controls at what pixel region an object should appear, and the temporal dimension, which controls how different levels of details are added through the denoising steps. In this paper, we propose a new text-to-image algorithm that adds explicit control over spatial-temporal cross-attention in diffusion models. We ﬁrst utilize a layout predictor to predict the pixel regions for objects mentioned in the text.
We then impose spatial attention control by combining the attention over the entire text description and that over the local description of the particular object in the correspond-ing pixel region of that object. The temporal attention con-trol is further added by allowing the combination weights to change at each denoising step, and the combination weights are optimized to ensure high ﬁdelity between the image and the text. Experiments show that our method generates im-ages with higher ﬁdelity compared to diffusion-model-based baselines without ﬁne-tuning the diffusion model. Our code is publicly available.1 1.

Introduction
Diffusion models [14, 19, 46, 47, 48] have recently rev-olutionized the ﬁeld of image synthesis. Compared with previous generative models such as generative adversar-⇤Equal contribution. 1https://github.com/UCSB-NLP-Chang/
Diffusion-SpaceTime-Attn
ial networks [1, 3, 11, 18] and variational autoencoders
[21, 38, 39], diffusion models have demonstrated superior performance in generating images with higher quality, more diversity, and better control over generated contents. Partic-ularly, text-to-image diffusion models [2, 33, 36, 40, 42] allow generating images conditioned on a text description, which enables generation of creative images due to the ex-pressiveness of natural language.
However, recent studies [10, 31] have revealed that one critical limitation of existing diffusion-model-based text-to-image algorithms is the low ﬁdelity with respect to the text descriptions – the content of the generated image is some-times at odds with the text description, especially when the description is complex. Speciﬁcally, typical errors made by stable diffusion models fall into three categories: missing objects, mismatched attributes, and mislocated objects. For example, in Fig. 1(a), stable diffusion model ignores the air-plane even though it is mentioned in text; in Fig. 1(b), the model confuses “red car” and “black mailbox” and gener-ates a red mailbox; in Fig. 1(c), the model locates teddy bear behind the toilet, despite the description “teddy bear is placed high above the toilet.”
Such inﬁdelity problems suggest that the cross-attention map on the text description may not be accurate. In partic-ular, if we view the generation process of a diffusion model as a sequence of denoising steps, then the cross-attention on text descriptions can be considered as a function of both spatial (pixels) and temporal (denoising steps) information.
Therefore, the inaccuracies of the cross-attention can result from the loose control over both the spatial and temporal dimensions. On one hand, spatial attention controls at what pixels the model should attend to each object and the cor-responding attributes mentioned in the text. If the spatial attention is incorrect, the resulting images will have incor-rect object locations or miss-associated attributes. On the other hand, temporal attention controls when the models should attend to different levels of details in the text. As previous works have revealed, diffusion models tend to fo-cus on generating object outlines at earlier denoising steps and on details at later [50]. Thus loose control over the temporal aspect of attention can easily lead to overlooking certain levels of the object details. In short, to improve the
ﬁdelity of text-to-image synthesis, one would need to ex-plicitly control both spatial and temporal attention to follow an accurate and optimal distribution.
In this paper, we propose a new text-to-image algorithm based on a pre-trained conditional diffusion model with ex-plicit control over the spatial-temporal cross-attention map on text. The proposed algorithm introduces a layout predic-tor and a spatial-temporal attention optimizer. The layout predictor takes the text description as input and generates a spatial layout for each object mentioned in the text. Alter-natively, the layout can also be provided by the user. Then the spatial-temporal attention optimizer imposes direct con-trol over the spatial and temporal aspects of the attention according to the spatial layout. In particular, for the spatial aspect, we parameterize the attention map such that the at-tention outputs in the designated pixel region for an object are a weighted combination of attention over the entire text description and that over the local description that specif-ically describes the corresponding object. In this way, we manage to emphasize the attention over the object descrip-tions. For the temporal aspect, we allow the combination weights to change across time and optimized according to a CLIP objective that measures the agreement between the generated images and the text description. In this way, we allow the attention to focus more on the entire description at the early stage and gradually shift to the detailed local descriptions as the denoising process proceeds. The entire pipeline resembles a typical painting process of a human, where each object’s position is determined beforehand and the focus gradually shifts from global information to the lo-cal details of each object.
We conduct extensive experiments on datasets that con-tain real and template-based captions [6, 26] and our newly created synthetic dataset that contains complex text de-scriptions. Results show that our method generates im-ages that better align with descriptions compared to other stable diffusion-based baselines. As shown in Fig. 1, our method effectively resolves the above-mentioned three er-rors. Particularly, controlling spatial attention locates ob-jects at the desired position, and controlling temporal at-tention promotes the occurrence of objects with associated attributes. Our ﬁndings shed light on ﬁne-grained control of diffusion models in text-to-image generation tasks. 2.