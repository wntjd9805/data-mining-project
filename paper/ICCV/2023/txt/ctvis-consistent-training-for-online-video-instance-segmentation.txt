Abstract
The discrimination of instance embeddings plays a vital role in associating instances across time for online video instance segmentation (VIS). Instance embedding learn-ing is directly supervised by the contrastive loss computed upon the contrastive items (CIs), which are sets of an-chor/positive/negative embeddings. Recent online VIS meth-ods leverage CIs sourced from one reference frame only, which we argue is insufficient for learning highly discrimina-tive embeddings. Intuitively, a possible strategy to enhance
CIs is replicating the inference phase during training. To this end, we propose a simple yet effective training strategy, called Consistent Training for Online VIS (CTVIS), which devotes to aligning the training and inference pipelines in terms of building CIs. Specifically, CTVIS constructs CIs by referring inference the momentum-averaged embedding and the memory bank storage mechanisms, and adding noise to the relevant embeddings. Such an extension allows a reliable comparison between embeddings of current instances and the stable representations of historical instances, thereby conferring an advantage in modeling VIS challenges such as occlusion, re-identification, and deformation. Empirically,
CTVIS outstrips the SOTA VIS models by up to +5.0 points on three VIS benchmarks, including YTVIS19 (55.1% AP),
YTVIS21 (50.1% AP) and OVIS (35.5% AP). Furthermore, we find that pseudo-videos transformed from images can train robust models surpassing fully-supervised ones. 1.

Introduction
Video instance segmentation is a joint vision task involv-ing classifying, segmenting, and tracking interested instances
*KY (email: kaining.ying.cv@gmail.com) and QZ contributed equally to this work. This work was done when KY, QZ, WM, YZ were visiting Zhejiang University.
†Correponding authors.
Figure 1. Comparison of inconsistent and consistent training (Ours). (a) Previous methods typically build contrastive items (CIs) and su-pervise the instance embeddings between key and reference frames.
We call this paradigm inconsistent training, where the interaction with the long-term memory bank during training and the lack of modeling for long video in real inference scenarios is overlooked. (b) The purpose of CTVIS is to align the training and inference pipelines. Specifically, CTVIS constructs training stage CIs by leveraging the memory bank and incorporates noise during the memory bank updating to simulate real-world scenarios, such as
ID switching, that can occur during inference. across videos [27]. It is critical in many video-based applica-tions, such as video surveillance, video editing, autonomous driving, augmented reality, etc. Current mainstream VIS methods [4, 11–13, 15, 24–28] can be categorized into offline and online groups. The former [4, 11, 13, 24, 25] segments and classifies all video frames simultaneously and makes the instance association in a single step. The latter [12, 26–28] takes as input a video in a frame-by-frame fashion, detecting and segmenting objects per frame while associating instances across time. In this paper, we focus on the online branch.
Online methods are typically built upon image-level in-stance segmentation models [5, 8, 22, 34, 35]. Several works
[17, 21, 27] utilize convolution-based instance segmentation models to segment each frame and associate instances by incorporating heuristic clues, such as mask-overlapping ra-tios and the similarity of appearance. However, these hand-designed approaches always fail to tackle complicated cases, which typically include severe target occlusion, deformation and re-identification. Recently, encouraged by the thriving of
Transformer-based [23] architectures in object detection and segmentation [2, 5, 35], a bunch of query-based online frame-works have been proposed [12, 26], which take advantage of the temporal consistency of query embeddings and asso-ciate instances by linking corresponding query embeddings frame by frame. These advances boost the performance of online VIS models, which become de-facto leading VIS per-formance on most benchmarks (especially on challenging ones such as OVIS [21]).
Though the importance of the discrimination of query em-beddings to associate instances has been nominated [12, 26], less research attention has been paid in this vein. Min-VIS [12] simply trains a single-frame segmentor, and the quality of its query embedding is hampered by the segmen-tor originally proposed for image-based instance segmen-tation. As shown in Figure 1(a), recent methods [14, 26] merely supervise instance embedding generation between two temporally adjacent frames with thein contrastive losses computed upon contrastive items. Specifically, for each in-stance at the key frame, if the same instance appears on the reference frame, the embedding of it is selected as the anchor embedding v. Meanwhile, its embedding in the ref-erence frame is taken as the positive embedding k+, and the embeddings of other instances in the reference frame are used as the negative embeddings k−. In convention the set {v, k+, k−} is called contrastive item (CI). This train-ing paradigm is inconsistent with the inference (shown in the right of Figure 1), as it overlooks the interaction with the long-term memory bank to construct contrastive items and lacks modelling for long videos. To bridge this gap, we propose CTVIS (as shown in Figure 1(b)), which intuitively brings in useful tactics from inference, including memory bank, momentum-averaged (MA) embedding and noise train-ing. Specifically, CTVIS samples several frames from a long video to form one training sample. Then we process each sample frame by frame, which can produce abundant CIs.
Moreover, we sample momentum-averaged (MA) embed-dings from the memory bank to create positive and negative embeddings. Furthermore, we introduce noise training for
VIS, incorporating a few noises into the memory bank up-dating procedure to simulate the tracking failure scenarios during the inference process.
We also consider the availability of large-scale training samples, which are especially expensive to annotate and maintain for VIS. To tackle this, we implement and test sev-eral goal-oriented augmentation methods (to align with the distribution of real data) to produce pseudo-videos. Different from the COCO joint training, we only use pseudo-videos to train VIS models.
Without bells and whistles, CTVIS outperforms the state-of-the-art by large margins on all benchmark datasets, in-cluding YTVIS19 [27], YTVIS22 [27], and OVIS [21]. Even trained with pseudo-videos only, CTVIS surpasses fully su-pervised VIS models [11, 25, 26]. Here we summarize our key contributions as
• We propose a simple yet effective training framework (CTVIS) for online VIS. CTVIS promotes the discrimina-tive ability of the instance embedding by interacting with long-term memory banks to build CIs, and by introducing noise into the memory bank updating procedure.
• We propose to create pseudo-VIS training samples by aug-menting still images and their mask annotations. CTVIS models trained with pseudo-data only surpass their fully-supervised opponents already, which suggests that it is a desirable choice, especially when dense temporal mask annotations are limited.
• CTVIS achieves impressive performance on three pub-lic datasets. Meanwhile, extensive ablation validates the method’s effectiveness. 2.