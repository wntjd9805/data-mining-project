Abstract
Input image
We present a method to estimate the depth of ﬁeld effect from a single image. Most existing methods related to this task provide either a per-pixel estimation of blur and/or depth. Instead, we go further and propose to use a lens-based representation that models the depth of ﬁeld using two parameters: the blur factor and focus disparity. Those two parameters, along with the signed defocus representation, result in a more intuitive and linear representation which we solve using a novel weighting network. Furthermore, our method explicitly enforces consistency between the estimated defocus blur, the lens parameters, and the depth map. Fi-nally, we train our deep-learning-based model on a mix of real images with synthetic depth of ﬁeld and fully synthetic images. These improvements result in a more robust and accurate method, as demonstrated by our state-of-the-art re-sults. In particular, our lens parametrization enables several applications, such as 3D staging for AR environments and seamless object compositing. 1.

Introduction
Modern cameras are equipped with sophisticated com-pound lenses whose task is to focus light on the sensor. The lens aperture (or f-number) determines, along with the expo-sure time, the amount of light captured in a photograph. Ad-justing the aperture however has another side effect: a larger aperture (lower f-number) induces a depth-dependent blur, leaving only parts of the image in focus. This depth of ﬁeld (DoF) effect is often sought by photographers to guide the viewers attention to the subject (e.g., macro shots), or create an artistic look (e.g., tilt-shift photography). Even when not exacerbated by an experienced photographer, some amount of depth-dependent defocus blur is bound to be present in images due to non-zero apertures.
The vast majority of computer vision algorithms ignore this DoF blur and make the simplifying (pinhole) assump-tion that the image is entirely in focus. While this may be acceptable in some cases, this may create unwanted effects.
Consider for example the case of virtual 3D object insertion
Estimated disparity
Estimated defocus
Estimated weight
Lens parameters
Naive object insertion
Lens-aware object insertion
Figure 1: Naively inserting 3D objects in images results in unreasonable compositions in the presence of depth of
ﬁeld blur (bottom-left). Our technique (top-right) estimates physically-based lens parameters that enable lens-aware in-sertion (bottom-right), where objects appear more realistic. (ﬁg. 1). It is of paramount importance that the inserted ob-jects be rendered by a virtual camera and lens sharing the same properties as the real ones (ﬁg. 1, bottom-right), as not doing so results in an improbable result (ﬁg. 1, bottom-left).
Blur estimation is a longstanding problem in computer vision, and many techniques estimating defocus blur specif-ically [43, 30, 23, 52, 28] have been proposed in the liter-ature. However, these non-parametric approaches produce per-pixel estimates, which do not allow advanced image editing tasks such as 3D virtual object insertion as in ﬁg. 1.
In this paper, we propose to go beyond per-pixel defocus blur and argue that reasoning about the camera lens param-eters is a more principled approach to understanding the
DoF effect in images. We propose what is, to the best of our knowledge, the ﬁrst method for estimating the camera lens parameters that control the depth of ﬁeld from a single image. Speciﬁcally, we estimate two key lens parameters: the focus disparity and the blur factor (scaled aperture). Our approach ﬁrst relies on pixel-wise depth and disparity es-timation networks. Then, we use a signed disparity blur model, allowing for a more intuitive and linear representa-tion that can recover the desired lens parameters via linear least-squares. Our work goes one step further and proposes a weighting network to guide the attention of the method during the least-squares ﬁtting to increase the accuracy and robustness. We obtain a corrected defocus map by making it explicitly consistent with the estimated lens parameters, con-trary to currently available methods which can technically produce any defocus at any depth, yielding globally inconsis-tent results. Finally, we use the recovered lens parameters to enable novel applications such as 3D virtual object insertion, where objects are automatically blurred by the rendering engine without having to rely on any 2D post-processing. In short, we make the following contributions:
• We present the ﬁrst method for estimating the lens param-eters that control the DoF from a single image: the focus disparity and blur factor. Our method produces globally consistent defocus maps and employs a weighting module to focus its attention on relevant parts of the image;
• We present a comparative analysis of lens parameter es-timation from per-pixel maps, favorably comparing our proposed approach to several strong baselines;
• We demonstrate state-of-the-art performance on pixel-wise disparity and defocus blur estimation on images with strong DoF effects compared to recent techniques;
• Recovering lens parameters from a single image enables, for the ﬁrst time, the realistic insertion of 3D objects in shallow DoF images. 2.