Abstract
Novel view synthesis and 3D modeling using implicit neural ﬁeld representation are shown to be very effective for calibrated multi-view cameras. Such representations are known to beneﬁt from additional geometric and seman-tic supervision. Most existing methods that exploit addi-tional supervision require dense pixel-wise labels or local-ized scene priors. These methods cannot beneﬁt from high-level vague scene priors provided in terms of scenes’ de-scriptions. In this work, we aim to leverage the geometric prior of Manhattan scenes to improve the implicit neural radiance ﬁeld representations. More precisely, we assume that only the knowledge of the indoor scene (under investi-gation) being Manhattan is known – with no additional in-formation whatsoever – with an unknown Manhattan coor-dinate frame. Such high-level prior is used to self-supervise the surface normals derived explicitly in the implicit neu-ral ﬁelds. Our modeling allows us to cluster the derived normals and exploit their orthogonality constraints for self-supervision. Our exhaustive experiments on datasets of di-verse indoor scenes demonstrate the signiﬁcant beneﬁt of the proposed method over the established baselines. The source code will be available at https://github. com/nikola3794/normal-clustering-nerf. 1.

Introduction
Above 80% images ever taken are estimated to in-volve human-made architectural structures, with a substan-tial share of indoor scenes [11]. These scenes often exhibit strong structural regularities, including ﬂat and texture-poor surfaces in some axis-aligned Cartesian coordinate system – also known as Manhattan world [7, 12]. Paradoxically, these regularities may hinder the 3D modeling process if it is unaware of the human-made scene priors.
In fact, several computer vision works have even beneﬁted from the knowledge of the Manhattan world for the task of 3D scene reconstruction [43, 12], camera localization [19], self-calibration [50], and more [37, 35, 29, 57].
For calibrated multi-view cameras, 3D inversion using implicit neural representations [24, 47, 54] is becoming in-creasingly popular due to their remarkable performance and recent efﬁciency developments [55, 30, 25, 51]. Meanwhile, such representations are known to beneﬁt from additional supervision in the form of depth [1, 9, 32], normals [18], semantics [45, 58, 17, 15], local-regularization [46, 44, 26], local planar patches [21], or their combinations [56, 14, 21].
In this context, a notable recent work ManhattanSDF [14] demonstrates the beneﬁt of exploiting the high-level geo-metric prior for structured scenes. More precisely, Manhat-tanSDF [14] uses the known semantic regions to impose the planar geometry prior of ﬂoors and walls under the Manhat-tan scene assumption. During this process, the exact normal of the ﬂoor and partial normals of the walls are assumed to be known, with respect to the camera coordinates.
We aim to improve the 3D neural radiance ﬁeld repre-sentations for calibrated multi-view cameras in indoor Man-hattan scenes, with no further assumptions. In other words, we consider that the structural and semantic information is not available, unlike ManhattanSDF [14]. In addition to the
ﬂoor and walls used in ManhattanSDF [14], we wish to ex-ploit the Manhattan prior of many other indoor scene parts (e.g. tables and wardrobes). More importantly, we consider that the Manhattan coordinate frame is also unknown. Our assumptions (of unknown semantics and Manhattan frame) on one hand make our setting very practical. On the other hand, those assumptions make the problem of exploiting the
Manhattan scene prior for 3D inversion very challenging.
The virtue of the Manhattan world assumption comes from its simplicity, allowing us to intuitively reason about the geometry of a wide range of complex scenes/objects such as cities, buildings, and furniture. However, such reasoning often requires the axis-aligned Cartesian coor-dinate frame, also known as the Manhattan frame (MF), to be known [37]. Unfortunately, recovering the Manhat-tan frame directly from images is not an easy task [7, 37].
Therefore, several methods have been developed until re-cently [10, 4, 36, 16, 13] to recover the Manhattan frame, relying upon the known 3D reconstruction or image primi-Nx ny
Ny nx nz
Nz
N3
{x1, x2, x3} n
{r1, r2, r3} o
N1
N2
Figure 1: Illustration of the proposed method. We compute one surface normal for each ray triplet, using 3D surface points derived from rendered depths (left). The computed normals (from many ray triplets) are clustered to obtain the MF (middle). The non-Manhattan and noisy surfaces are handled by a robust orthogonal normals search, which is later used for self-supervision through the Manhattan prior (right). tives (eg. lines, planes). We wish to exploit the Manhattan prior for improving the 3D representation, without needing to know MF beforehand. Instead, our experiments reveal that knowing the MF beforehand offers no additional bene-ﬁt in Manhattan-prior aware radiance ﬁeld representation.
In this work, we propose a method that jointly learns the Manhattan frame and neural radiance ﬁeld, from cali-brated multi-view in indoor Manhattan scenes, in an end-to-end manner using the recent efﬁcient backbone of Instant-NGP [25]. The proposed method requires no additional in-formation to exploit the Manhattan prior and relies on the explicitly derived normals in the implicit neural ﬁelds. We use batches of three neighboring rays, whose effective sur-face’s local piece-wise planarity is assumed to derive the ex-plicit normals by algebraic means. In pure, complete, and enclosed Manhattan scenes, these normals form six clus-ters corresponding to three orthogonal and other three par-allel counterpart planes. However real scenes consist of non-Manhattan scene parts and missing planes. Therefore, we use a robust method that uses minimal three orthogonal clusters to recover the sought Manhattan frame. As in the literature, we seek a rotation matrix whose entries are di-rectly derived from the orthogonal clusters of normals, to align the Manhattan frame. The recovered MF is then used to encourage the derived normals to be axis-aligned for self-supervision. A graphical illustration of our method is pre-sented in Figure 1. Our extensive experiments demonstrate the robustness and utility of our method in improving the implicit 3D in neural radiance ﬁeld representations.
The major contributions of our paper are listed below: – We address the problem of exploiting the structured-scene knowledge without requiring any dense or localized scene priors, for the ﬁrst time in this paper. – We present a method that successfully exploits the Man-hattan scene prior with an unknown Manhattan frame. The proposed method also recovers the unknown frame. – We demonstrate the robustness and utility of our method on three indoor datasets, where our method improves the established baselines signiﬁcantly. These datasets consist of 200+ scenes, making our method tested in signiﬁcantly more scenes than the state-of-the-art methods. 2.