Abstract
The goal of building a benchmark (suite of datasets) is to provide a unified protocol for fair evaluation and thus facilitate the evolution of a specific area. Nonethe-less, we point out that existing protocols of action recog-nition could yield partial evaluations due to several limita-tions. To comprehensively probe the effectiveness of spa-tiotemporal representation learning, we introduce BEAR, a new BEnchmark on video Action Recognition. BEAR is a collection of 18 video datasets grouped into 5 cate-gories (anomaly, gesture, daily, sports, and instructional), which covers a diverse set of real-world applications. With
BEAR, we thoroughly evaluate 6 common spatiotemporal models pre-trained by both supervised and self-supervised learning. We also report transfer performance via stan-dard finetuning, few-shot finetuning, and unsupervised do-main adaptation. Our observation suggests that the cur-rent state-of-the-art cannot solidly guarantee high perfor-mance on datasets close to real-world applications, and we hope BEAR can serve as a fair and challenging evalua-tion benchmark to gain insights on building next-generation spatiotemporal learners. Our dataset, code, and models are released at: https://github.com/AndongDeng/BEAR 1.

Introduction
Learning good spatiotemporal representations [45, 70, 24, 60, 16, 76] is fundamental for video understanding tasks.
In action recognition, a common evaluation pro-tocol is to first evaluate the model performance on large-scale video datasets such as Kinetics-400 [30], then show its effectiveness of transfer learning to different downstream tasks [5, 17, 38, 2, 41, 72]. Many video datasets [56, 32, 30, 20, 9] have been introduced over the past few years to advance the field. However, there are several major limi-tations: (1) These datasets are similar in terms of domains
*Equal Contribution.
Figure 1: BEAR is a collection of 18 video action recogni-tion datasets grouped into 5 categories (Anomaly, Gesture,
Daily, Sports, and Instructional). It enables various evalua-tion settings, e.g., standard finetuning, few-shot finetuning, unsupervised domain adaptation, and zero-shot learning. and actions. Most of them only contain daily or sports ac-tions because these categories are easy to collect from the web. Yet many important real-world applications, such as anomaly detection and industrial inspection, are rarely in-(2) Each of these datasets has its own character-cluded. istics (e.g. appearance-focused [30], motion-focused [20], fine-grained [52], egocentric [9]). Previous works usually conduct evaluations on a few datasets. However, without evaluating a suite of datasets, we cannot fully diagnose a model and make further improvements. (3) The held-out test set for these datasets either does not exist or is not com-monly adopted. This will affect the transfer performance because models tuned on a test set using hyperparameter op-timization or neural architecture search might achieve good performance but cannot transfer well due to overfitting.
In light of this, we propose a unified and challenging
BEnchmark on video Action Recognition, named BEAR, to better evaluate spatiotemporal representation learning. We define good representations as those that can achieve strong
transfer learning performance on diverse, unseen domains even with limited data. To this end, we build BEAR by collecting a suite of 18 video action recognition datasets grouped into 5 categories (Anomaly [57, 78], Gesture [42],
Daily [53, 10], Sports [28], and Instructional [58]), which cover a diverse set of real applications. The datasets in
BEAR are also diverse in video sources (e.g. YouTube,
CCTV cameras, self-collected) and viewpoints (e.g. ego-centric, 3rd person, drone, and surveillance). In addition, we split each dataset into train and test sets, strictly keeping the test set held out during training in all of our experi-ments. We will also provide an online evaluation server to enable fair comparisons.
With BEAR, one can probe spatiotemporal representa-tion learning methods from a much more diverse perspec-tive and answer many important questions. Does the good performance on commonly-used large-scale datasets trans-late to real applications? Do recent transformer-based mod-els consistently outperform simple 2D models in different domains? How sensitive is the model to domain and view-point change? Could the model achieve good performance when downstream data is limited? In this work, we com-prehensively investigate 6 representative video models pre-trained by both supervised and self-supervised learning in various settings (e.g. full-shot, few-shot, domain adapta-tion). Our study quantifies existing intuition and uncovers several new insights: (1) Simple 2D video models can out-perform recent transformer-based models when equipped with strong backbones. (2) The previous evaluation pro-tocols are constrained to downstream datasets that resem-ble Kinetics-400. However, the high performance of these datasets does not necessarily transfer to other application domains. (3) Viewpoint shift has a dramatic impact on downstream task performance. Even the recent domain adaptation methods cannot address the problem to satis-factory. This suggests we may need to go beyond domain adaptation and shift attention to building more comprehen-sive pre-training datasets. (4) Self-supervised spatiotempo-ral representation learning still lags remarkably behind su-pervised learning. Even the SoTA VideoMAE [60] fails to outperform simple supervised models in diverse domains.
Our goal is to provide a unified and challenging evaluation benchmark to evaluate spatiotemporal representation learn-ing from various perspectives, which hopefully could guide future development in video understanding. 2.