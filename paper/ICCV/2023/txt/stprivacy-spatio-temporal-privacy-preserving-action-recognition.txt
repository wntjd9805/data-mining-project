Abstract
Existing methods of privacy-preserving action recogni-tion (PPAR) mainly focus on frame-level (spatial) privacy removal through 2D CNNs. Unfortunately, they have two major drawbacks. First, they may compromise temporal dynamics in input videos, which are critical for accurate action recognition. Second, they are vulnerable to practi-cal attacking scenarios where attackers probe for privacy from an entire video rather than individual frames. To ad-dress these issues, we propose a novel framework STPrivacy to perform video-level PPAR. For the first time, we intro-duce vision Transformers into PPAR by treating a video as a tubelet sequence, and accordingly design two complemen-tary mechanisms, i.e., sparsification and anonymization, to remove privacy from a spatio-temporal perspective. In spe-cific, our privacy sparsification mechanism applies adap-tive token selection to abandon action-irrelevant tubelets.
Then, our anonymization mechanism implicitly manipulates the remaining action-tubelets to erase privacy in the em-bedding space through adversarial learning. These mech-anisms provide significant advantages in terms of privacy preservation for human eyes and action-privacy trade-off adjustment during deployment. We additionally contribute the first two large-scale PPAR benchmarks, VP-HMDB51 and VP-UCF101, to the community. Extensive evaluations on them, as well as two other tasks, validate the effective-ness and generalization capability of our framework. 1.

Introduction
Action recognition has seen tremendous progress in re-cent years, but the increasing concerns regarding privacy
*Corresponding author. Jia-Wei Liu and Mike Zheng Shou are with
Show Lab, NUS.
Figure 1: Comparison between the existing paradigm for
PPAR and the proposed algorithm. (a) Existing meth-ods for PPAR remove private information from individual frames independently against a frame-level privacy recog-nizer. They not only neglect the temporal dynamics be-tween frames, hurting action recognition performance, but also leave the entire video vulnerable to privacy attacks. (b) Our proposed algorithm addresses these issues by treat-ing the input video as a whole to remove privacy against a video-level privacy recognizer. It promotes action dynam-ics and protects both video-level and frame-level privacy.
The black rectangles in our transformed video represent the abandoned tubelets, and the emotional faces indicate the performance of the corresponding task. leakage have given rise to an emerging research topic privacy-preserving action recognition (PPAR) [38, 37, 6]. It aims to remove private information from videos while en-suring accurate action recognition.
Current studies on PPAR [30, 19, 41, 25, 38, 37, 6] mainly focus on frame-level privacy preservation. As illus-trated in Figure 1 (a), the paradigm typically involves three steps: 1) extracting frames from a video, 2) independently
removing privacy from each frame, and 3) performing pri-vacy recognition and action recognition on the transformed frames and the spliced pseudo video, respectively.
While this paradigm is effective against frame-level pri-vacy attacks, it has two major drawbacks. First, it ne-glects the temporal dynamics between frames, which are crucial for accurate action recognition [35, 11]. This is be-cause it usually relies on a 2D convolutional neural network (CNN) to process each video frame independently, result-ing in a serious discontinuity in object dynamics. Second, the paradigm only protects spatial privacy against frame-level privacy attacks, leaving the entire video vulnerable to potential video-level privacy attacks. A typical example is that it can merely remove part areas of a face to make it dif-ficult to solely identify from each individual frame. But a video-level privacy recognizer can still identify the face by aggregating the highly complementary facial clues from the remaining areas of all frames, owing to the high information redundancy in a video [31, 10]. The essential techniques can be obtained referring to the research on occluded video object recognition [14, 36, 13, 42].
To overcome these drawbacks of frame-level PPAR, we present a novel algorithm, named STPrivacy, which per-forms video-level PPAR from both spatial and temporal per-spectives as illustrated in Figure 1 (b). Inspired by the lat-est vision Transformers (ViTs), STPrivacy treats an input video as a tubelet sequence and captures the temporal dy-namics with the self-attention operations. To enable the pri-vacy removal within our framework, we propose two com-plementary token-wise mechanisms, namely sparsification and anonymization. The privacy sparsification mechanism applies adaptive token selection to directly abandon the pri-vate tubelets that are irrelevant to the action. Then, the pri-vacy anonymization mechanism manipulates the remaining action-tubelets in the embedding space to implicitly erase privacy. For training the proposed network, we employ an adversarial learning objective by feeding the transformed tubelets into an action recognizer and a video-level privacy recognizer. Both intuitively and experimentally, our STPri-vacy is superior in protecting both video-level (Section 4.4) and frame-level (Section 4.7) privacy. Clearly, our frame-work emphasizes temporal dynamics for action recognition and protects privacy in a more strict manner.
In summary, our main contributions are as follows:
• We propose a novel video-level PPAR framework that enhances temporal dynamics for action recognition and protects privacy in a more strict way, compared with existing frame-level methods.
• The proposed STPrivacy introduces ViTs into PPAR for the first time and demonstrates significant advan-tages in high-quality privacy preservation in terms of human eyes (Section 4.5) and convenient adjustment of action-privacy recognition trade-off during deploy-ment (Section 4.6).
• We provide new benchmark datasets VP-HMDB51 and VP-UCF101, which are considerably larger than the existing one (i.e., PA-HMDB [37] containing 515 videos), for evaluating PPAR methods sufficiently.
Our annotations will be made publicly available.
• Extensive experiments demonstrate that STPrivacy significantly outperforms the state-of-the-art (SOTA) methods in terms of action recognition and privacy protection, both quantitatively and qualitatively. In ad-dition, its generalization ability is demonstrated by two related tasks on CelebVHQ [44] and P-HVU [6]. 2.