Abstract
Continual domain shift poses a significant challenge in real-world applications, particularly in situations where la-beled data is not available for new domains. The challenge of acquiring knowledge in this problem setting is referred to as unsupervised continual domain shift learning. Exist-ing methods for domain adaptation and generalization have limitations in addressing this issue, as they focus either on adapting to a specific domain or generalizing to unseen do-mains, but not both.
In this paper, we propose Comple-mentary Domain Adaptation and Generalization (CoDAG), a simple yet effective learning framework that combines domain adaptation and generalization in a complementary manner to achieve three major goals of unsupervised con-tinual domain shift learning: adapting to a current do-main, generalizing to unseen domains, and preventing for-getting of previously seen domains. Our approach is model-agnostic, meaning that it is compatible with any existing do-main adaptation and generalization algorithms. We evalu-ate CoDAG on several benchmark datasets and demonstrate that our model outperforms state-of-the-art models in all datasets and evaluation metrics, highlighting its effective-ness and robustness in handling unsupervised continual do-main shift learning. 1.

Introduction
Machine learning algorithms have found extensive appli-cations in various fields such as image recognition [19, 23], natural language processing [3, 9], and autonomous driv-ing [5, 10]. Typically, these algorithms learn from a train-ing dataset to build a model that performs a target task on new data. The assumption underlying these algorithms is that the training data and the test data are identically and independently distributed (IID) [18], drawn from the same distribution that is characterized by an environment or do-*Corresponding Author
Figure 1. Our proposed Complementary Domain Adaptation and
Generalization (CoDAG) framework for unsupervised continual domain shift learning. main. However, this IID assumption is often not valid in real-world scenarios, as the environment in which the model is applied is more likely to change over time than remain fixed. This implies that the model will encounter new data from various domains over time, and its performance may decline if the data is from a domain that differs significantly from the one it was trained on.
To address this issue, two main approaches have been developed: domain generalization and domain adaptation.
Domain generalization [60] is a method to enhance a model’s ability to generalize to unseen domains by training the model on labeled data from one or more domains, with-out assuming any prior knowledge of the test environment in which the model will be applied. However, collecting a large volume of labeled data from various domains for a particular task can be challenging in practice. Single-source domain generalization techniques [46] have been developed to address this issue, which rely solely on data from a single domain. Although more practical, these techniques gener-ally have lower generalization abilities compared to multi-source domain generalization techniques.
On the other hand, domain adaptation [61] aims to en-hance model performance only on the current target domain and does not prioritize performance on all other domains.
In particular, unsupervised domain adaptation (UDA) [44] techniques leverage unlabeled data to adapt the model to a new target domain. However, domain adaptation meth-ods fundamentally suffer from performance degradation on a new target domain before and during the adaptation pro-cess due to the lack of inherent mechanism to prepare for unseen domains.
In this paper, we tackle a challenging problem that simu-lates real-world scenarios where models face continual do-main shifts and no labeled data is available for new do-mains. We refer to this problem as unsupervised continual domain shift learning. In this setting, the model must con-tinually adapt to new domains (domain adaptation), while maintaining its generalization ability for upcoming and un-seen domains (domain generalization), in an unsupervised manner. However, achieving both objectives simultane-ously is not always feasible since they involve related but distinct goals. For instance, if the current target domain is vastly dissimilar from any other domains, none of the opti-mal solutions for adapting to the current target domain with
DA would necessarily result in optimal generalization for performing well on other domains. Similarly, achieving op-timal generalization for unseen domains through DG may not result in the best solution for the current target domain.
Therefore, to address unsupervised continual domain shift learning, it is necessary to find a solution that resolves this trade-off between domain adaptation and generalization.
To address the trade-off between domain adaptation and generalization, we propose Complementary Domain Adap-tation and Generalization (CoDAG), a learning framework that combines domain adaptation and domain generaliza-tion in a complementary manner. As shown in Fig. 1, our approach involves training two separate models: one for do-main adaptation and the other for domain generalization.
We use the domain adaptation model to adapt to the tar-get domain, generating more accurate and reliable pseudo-labels for training the domain generalization model. In turn, the domain generalization model learns more generalized representations across multiple domains and provides the domain adaptation model with initializing parameters, en-hancing its adaptability to a new domain. As a result, the domain adaptation and generalization models complement each other in our framework, leading to improved perfor-mance for both.
The main contribution of our framework, CoDAG, lies in the complementary manner in which we leverage exist-ing domain adaptation and domain generalization meth-ods to address unsupervised continual domain shift learn-ing, a unique and challenging problem that has not been thoroughly explored. We deliberately apply existing meth-ods to our framework, rather than introducing new ones, to underscore that the effectiveness of our framework is due to its complementary structure, not its individual compo-nents. Indeed, without requiring any models tailored for the present problem, our framework proved its merit by achiev-ing SoTA performance against all baselines, including the one which is explicitly designed for this setting [37].
Finally, it is important to note that that our work is one of the first attempts to explore the potential synergies between domain adaptation and domain generalization methods. We are breaking new ground by bridging the divide between the disparate fields of domain adaptation and generalization, which were primarily studied independently. This paradigm shift represents not just a novel approach, but one with pro-found practical implications.
Our contributions can be summarized as follows:
• We introduce a novel framework that combines do-main adaptation and generalization models in a com-plementary manner, resulting in a synergistic process that enhances overall performance.
• Our method consistently outperforms state-of-the-art models across all datasets and metrics, demonstrating superior robustness with the lowest standard deviation across different orders in almost all cases.
• Our approach does not necessitate the use of models designed for the present problem, allowing seamless integration with existing domain adaptation and gener-alization algorithms for broader applications. 2.