Abstract
Synthesizing high-ﬁdelity videos from real-world multi-view input is challenging due to the complexities of real-world environments and high-dynamic movements. Previ-ous works based on neural radiance ﬁelds have demon-strated high-quality reconstructions of dynamic scenes.
However, training such models on real-world scenes is time-consuming, usually taking days or weeks.
In this paper, we present a novel method named MixVoxels to efﬁciently represent dynamic scenes, enabling fast training and ren-dering speed. The proposed MixVoxels represents the 4D dynamic scenes as a mixture of static and dynamic voxels and processes them with different networks.
In this way, the computation of the required modalities for static vox-els can be processed by a lightweight model, which essen-tially reduces the amount of computation as many daily dynamic scenes are dominated by static backgrounds. To distinguish the two kinds of voxels, we propose a novel variation ﬁeld to estimate the temporal variance of each voxel. For the dynamic representations, we design an in-ner product time query method to efﬁciently query multiple time steps, which is essential to recover the high-dynamic movements. As a result, with 15 minutes of training for dy-namic scenes with inputs of 300-frame videos, MixVoxels achieves better PSNR than previous methods. For render-ing, MixVoxels can render a novel view video with 1K reso-lution at 37 fps. Codes and trained models are available at https://github.com/fengres/mixvoxels. 1.

Introduction
Dynamic scene reconstruction from multi-view videos is a critical and challenging problem, with many poten-tial applications such as interactively free-viewpoint control
*Corresponding author.
Figure 1. Our method enables rapid reconstruction of 4D dynamic scenes. We visualize the rendering results with different train-ing schedules. With only 15 minutes of training, our approach achieves comparable PSNRs to other methods.
Increasing the training time further enhances the ability to recover ﬁne details. for movies, cinematic effects like freeze-frame bullet time, novel view replays for sporting events, and various poten-tial VR/AR applications. Recently, neural radiance ﬁelds
[26] have demonstrated the possibility of rendering photo-realistic novel views for static scenes, with physically mo-tivated 3D density and radiance modelling. Many methods
[19, 20, 49, 13, 10, 31, 28, 29, 44] extend the neural radiance
ﬁelds to dynamic scenes with additional time queries or an explicit deformation ﬁeld. Many of these methods focus on the monocular input video setting on relatively simple dy-namic scenes. To model more complex real-world dynamic scenes, a more practical solution is to use multi-view syn-chronized videos to provide dense spatial-temporal supervi-sions [55, 23, 3, 19].
Recently, Li et al.
[19] propose a real-world dynamic scene dataset including many challenging situations such as objects of high specularity, topology changes, and volumet-ric effects. They address the problem by a hierarchical train-ing scheme and the ray importance sampling strategies. Al-though signiﬁcant improvements have been achieved, some challenges still exist: (1) The training and rendering take a
lot of time and computation resources. (2) Highly dynamic scenes with complex motions are still difﬁcult to track.
In this paper, we focus on the multi-view 3D video syn-thesis problem and present a novel method named MixVox-els to address the above two challenges. The proposed
MixVoxels is based on the explicit voxel-grid represen-tation, which is recently popular due to its fast training and rendering speed on static scenes [50, 40, 8, 27]. We extend the voxel-grid representations to support dynamic scenes and propose an efﬁcient inner product time query-ing method that can query a large number of time steps simultaneously, which is essential to recover the sharp de-tails for highly-dynamic objects. Additionally, we represent dynamic scenes as a mixed static-dynamic voxel-grid repre-sentation. Speciﬁcally, the 3D spaces are split into static and dynamic voxels by our proposed variation ﬁeld. The two components are processed by different models to reduce the redundant computations for the static space. Theoretically, once a dynamic scene consists of some static spaces, the training speed will beneﬁt from the proposed mixed voxels.
For a variety of events that occur in the physical world, the static components of environments are dominated in most cases, and the mixed voxels will speed up the training sig-niﬁcantly in these scenarios. Besides, the separation of vox-els makes the time-variant model focus on the dynamic re-gions, avoiding the time-aware voxels being biased by the static spaces to produce blurred motions. Our empirical validation conﬁrms that the separation enables the model to learn sharp and distinct boundaries in high-dynamic re-gions. This also frees our method from the complex impor-tance sampling strategies. With these designs, our method is capable of reconstructing a dynamic scene consisting of 300 frames within 15 minutes. To summarize, the main con-tributions of this work are:
• We propose a simple yet effective dynamic represen-tation with inner product time querying method that can efﬁciently query multiple times simultaneously, improving the rendering quality for dynamic objects.
• We design an efﬁcient variation ﬁeld to separate static and dynamic spaces and present a mixed voxel-grid representation to accelerate training and rendering.
• We conduct qualitative and quantitative experiments to validate our method. As a result, the proposed
MixVoxels achieves competitive or better rendering qualities with a 5000× training speedup compared to implicit dynamic scene representations. 2.