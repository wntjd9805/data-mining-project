Abstract
Geospatial technologies are becoming increasingly es-sential in our world for a wide range of applications, including agriculture, urban planning, and disaster re-sponse. To help improve the applicability and performance of deep learning models on these geospatial tasks, vari-ous works have begun investigating foundation models for this domain. Researchers have explored two prominent ap-proaches for introducing such models in geospatial appli-cations, but both have drawbacks in terms of limited per-formance benefit or prohibitive training cost. Therefore, in this work, we propose a novel paradigm for building highly effective geospatial foundation models with minimal resource cost and carbon impact. We first construct a com-pact yet diverse dataset from multiple sources to promote feature diversity, which we term GeoPile. Then, we in-vestigate the potential of continual pretraining from large-scale ImageNet-22k models and propose a multi-objective continual pretraining paradigm, which leverages the strong representations of ImageNet while simultaneously provid-ing the freedom to learn valuable in-domain features. Our approach outperforms previous state-of-the-art geospatial pretraining methods in an extensive evaluation on seven downstream datasets covering various tasks such as change detection, classification, multi-label classification, seman-tic segmentation, and super-resolution. Code is available at https://github.com/mmendiet/GFM . 1.

Introduction
The significance of geospatial technologies has pro-gressively increased for various applications worldwide.
Progress in this domain can substantially improve our abil-ity to understand the earth and how we interact with it. With the rising popularity of foundation models in vision and nat-ural language, researchers have begun to investigate apply-*Work done as an intern at Amazon Web Services
Figure 1. Our geospatial foundation model (GFM) achieves favor-able performance on a broad set of tasks in comparison to other state-of-the-art geospatial pretraining methods (SeCo [28], Sat-MAE [9]) and ImageNet supervised pretraining baselines. Leg-end is as follows. Cyan: ImageNet-1k Supervised (ResNet50),
Blue: SeCo [28], Purple: ImageNet-22k Supervised (ViT), Or-ange: SatMAE [9], Gray:
ImageNet-22k Supervised (Swin),
Green: GFM (ours). ing such principles to the geospatial domain in order to en-hance the suitability of deep learning models in downstream tasks [29, 28, 9, 2]. In the literature, various works have ex-plored two prominent approaches for introducing pretrained foundation models in geospatial applications. The first ob-vious approach is to leverage existing foundation models from the natural image domain, like those trained on the
In practice, this large-scale ImageNet-22k dataset [11]. is done by directly finetuning publicly-available ImageNet pretrained models on the downstream tasks. This approach has the advantage of being straight-forward, as ImageNet models can be simply downloaded from many open-source model zoos, and has been shown to be effective [29, 30].
However, due to the domain gap between natural images
and remote sensing, this approach is not optimal for geospa-tial data, and still leaves performance gains on the table.
In recent years, a second approach has gained significant traction, where researchers aim to pretrained models spe-cific to the geospatial domain [28, 2, 9, 38]. These methods typically train a network from scratch on a large corpus of remote sensing imagery to learn in-domain representa-tions transferable to downstream tasks. Unfortunately, this can require a significant amount of data and training time to achieve good performance, especially when employing large state-of-the-art (SOTA) transformer models. For in-stance, the current SOTA in geospatial foundation models,
SatMAE [9], requires 768 hours on a V100 GPU for train-ing a vision transformer [14]. This has substantial cost associated with producing the model, not just in terms of time and computation but also environmentally, with a to-tal estimated carbon footprint of 109.44 kg CO2 equivalent.
Additionally, the final performance of such models are not consistently better across various tasks than simply utilizing publicly-available ImageNet pretrained models (Section 4), despite the high resource expense.
In this work, we propose to investigate a different paradigm for producing more effective geospatial founda-tion models with substantially less resource costs. First, we begin with a discussion on pretraining data selection, and ultimately construct a concise yet diverse collection of data from various sources to promote feature diversity and ef-fective pretraining. Second, rather than following the afore-mentioned typical approaches, we investigate the potential of continual pretraining for the geospatial domain from readily-available ImageNet models. Continual pretraining has been practiced in the NLP domain with success in var-ious works [16, 17, 26]. In this paradigm, existing founda-tion models are further improved for a specific domain or task through a secondary pretraining stage. This new single model can now be fine-tuned on the various downstream tasks in that domain. In principle, we reason that such a paradigm has the potential to boost performance by utiliz-ing large-scale ImageNet representations as a base on which stronger geospatial foundation models can be built. Further-more, such natural image models are constantly being im-proved and released by the general computer vision commu-nity, providing a consistent source of better baseline mod-els. Therefore, an approach that could enable the geospa-tial domain to leverage these improvements with minimal resource needs and carbon footprint paves the way for con-tinual, sustainable benefits for the geospatial community.
However, when we initially experiment with the standard continual pretraining formulation, we find it provides only marginal benefits (Section 3.2). Instead, we discover that utilizing ImageNet representations as an auxiliary distilla-tion objective during pretraining leads to a stronger geospa-tial foundation model. Building upon this principle, we pro-pose a multi-objective continual pretraining paradigm that significantly enhances performance while requiring mini-mal resources. Our approach leverages ImageNet’s power-ful representations to facilitate and expedite learning, while also enabling the acquisition of valuable in-domain features via self-supervised learning on geospatial data. Further-more, our proposed Geospatial Foundation Model (GFM) exhibits strong performance, surpassing previous state-of-the-art (SOTA) methods across a diverse range of down-stream tasks (Section 4). Our contributions are as follows:
• We investigate a novel paradigm for creating highly ef-fective geospatial models with minimal resource costs.
Our methodology begins with data selection and con-struction of a compact yet diverse dataset from multi-ple sources to promote feature diversity and enhance pretraining effectiveness, which we term GeoPile. We further explore the potential of continual pretraining from ImageNet models, but find it is not satisfactory in its standard formulation.
• Therefore, to achieve better performance with minimal resource needs, we propose a multi-objective contin-ual pretraining paradigm. Our design is surprisingly simple yet effective, constructed as a teacher-student strategy with both a distillation objective and self-supervised masked image modeling. This approach allows GFM to leverage the strong representations of
ImageNet to guide and quicken learning, while simul-taneously providing the freedom to learn valuable in-domain features.
• We evaluate our GFM approach, as well as several baseline and SOTA methods, on 7 datasets covering important geospatial applications such as change de-tection, classification, multi-label classification, se-mantic segmentation, and super-resolution. Overall, our GFM performs favorably over previous methods (as shown in Figure 1). 2.