Abstract
Humans can easily understand a single image as depict-ing multiple potential objects permitting interaction. We use this skill to plan our interactions with the world and accelerate understanding new objects without engaging in interaction. In this paper, we would like to endow machines with the similar ability, so that intelligent agents can bet-ter explore the 3D scene or manipulate objects. Our ap-proach is a transformer-based model that predicts the 3D location, physical properties and affordance of objects. To power this model, we collect a dataset with Internet videos, egocentric videos and indoor images to train and validate our approach. Our model yields strong performance on our data, and generalizes well to robotics data. 1.

Introduction
What can you do in Figure 1? This single RGB image conveys a rich, interactive 3D world where you can interact with many objects. For instance, if you grab the chair with two hands, you can move it as a rigid object; the pillow can be picked up freely and squished; and door can be moved, but only rotated. This ability to recognize and interpret po-tential affordances in scenes helps humans plan our interac-tions and more quickly learn to interact with objects. The goal of this work is to give the same ability to computers.
Obtaining such an understanding of potential interac-tions from a single 3D image is beyond the current state of the art in scene understanding because it spans multiple disparate subfields of computer vision. For instance, single image 3D has made substantial progress [50, 47, 68, 18], but primarily focuses on the scene as it exists, as opposed to as it could be. There has been an increasing interest in under-standing articulation [34, 65, 49], but these works primarily focus on articulation as it occurs in a 3D model or carefully collected demonstrations, instead of as it could occur. Fi-nally, while there is long-standing work on enabling robots to learn interaction and potential interaction points [48, 56], these works focus primarily on evaluation in primarily the same environment (e.g. the lab) and do not focus on apply-ing the understanding in entirely new environments.
We propose to bootstrap this interactive understand-ing by developing (1) a problem formulation, (2) a rich dataset of annotations on challenging images, and (3) a transformer-based approach. We frame the problem of rec-ognizing the articulation as a prediction-at-a-query-location problem: given an image and 2D location, our method aims to answer “what can I do here?” in the style of classic point-and-click games like Myst. We frame “what can I do here” via a set of common questions: whether the object can be moved, its extent when moved and location in 3D, rigid-ity, whether there are constrains on its motion, as well as estimates of how one would interact the object. To max-imize the potential for downstream transfer, our questions are chosen to be generic rather than specific to particular hands or end-effectors: knowing where to act or the degrees
of freedom of an object may accelerate reinforcement learn-ing even if one must still learn end-effector-specific skills.
In order to tackle the task, we introduce a transformer-based model. Our approach, described in Section 5 builds on a detection backbone such as Segment-Anything [30] in order to build on the advances and expertise of object de-tection. We extend the backbone with additional heads that predict each of our “what I can I do here” tasks, and which can be trained end-to-end. As an advantage of our formu-lation, we can train the system on sparse annotations; we believe this will be helpful for eventually converting our di-rect supervision to supervision via video.
Powering our approach is a new dataset, described in
Section 4, which we name the 3D Object Interaction dataset (3DOI). In order to maximize the likelihood of generaliz-ing to new environments, the underlying data comes from diverse sources, namely Internet and egocentric videos as well as 3D renderings of scene layouts. We provide annota-tions of our tasks on this data and, due to the source of the data, we also naturally obtain 3D supervision in the form of depth and normals. In total, the dataset has over 50K ob-jects across 10K images, as well as over 31K annotations of non-interactable objects (e.g., floor, wall).
Our experiments in Section 6 test how well our ap-proach recognizes potential interaction, testing on both un-seen data in 3DOI as well as robotics data. We compare with a number of alternatives, including generalizing from data of demonstrations [49, 46] and synthetic data [65], as well alternate network designs. Our approach outper-forms these models and shows strong generalization to the robotics dataset WHIRL [1].
To summarize, we see our primary contributions as: (1) the novel task of detecting 3D object interactions from a sin-gle RGB image; (2) 3D Object Interaction dataset, which is the first large-scale dataset containing objects that can be interacted and their corresponding locations, affordance and physical properties; (3) A transformer-based model to tackle this problem, which has strong performance on the 3DOI dataset and robotics data. 2.