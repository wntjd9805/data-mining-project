Abstract
Sound of 
“dog barking”
Mixed sound of “dog  barking” and sound
“children playing”
Mix of sound “dog barking”  and text caption “in painting  style by Vincent van Gogh”
Text-to-image (T2I) models based on diffusion processes have achieved remarkable success in controllable image generation using user-provided captions. However, the tight coupling between the current text encoder and image de-coder in T2I models makes it challenging to replace or up-grade. Such changes often require massive ﬁne-tuning or even training from scratch with the prohibitive expense. To address this problem, we propose GlueGen, which applies a newly proposed GlueNet model to align features from single-modal or multi-modal encoders with the latent space of an existing T2I model. The approach introduces a new training objective that leverages parallel corpora to align the representation spaces of different encoders. Empirical results show that GlueNet can be trained efﬁciently and en-ables various capabilities beyond previous state-of-the-art models: 1) multilingual language models such as XLM-Roberta can be aligned with existing T2I models, allowing for the generation of high-quality images from captions be-yond English; 2) GlueNet can align multi-modal encoders such as AudioCLIP with the Stable Diffusion model, en-abling sound-to-image generation; 3) it can also upgrade the current text encoder of the latent diffusion model for challenging case generation. By the alignment of various feature representations, the GlueNet allows for ﬂexible and efﬁcient integration of new functionality into existing T2I models and sheds light on X-to-image (X2I) generation.1 1.

Introduction
Text-to-image (T2I) generative models have made great progress in the last few years thanks to algorithmic advances
*This work was done when Can Qin interned at Salesforce AI Research.
Primary contact: qin.ca@northeastern.edu 1Code will be available at: https://github.com/salesforce/GlueGen (cid:2) <text> (cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:5)(cid:7) (cid:8)(cid:9)(cid:10)(cid:7)(cid:11)(cid:12)(cid:3)(cid:5)(cid:9)(cid:13)(cid:14)(cid:15)(cid:15)(cid:4)(cid:16)(cid:14)(cid:17)(cid:18) (a) Single Sound (b) Mixed Sound (c) Sound-Text Mix
Figure 1. Setting of GlueGen. GlueNet is trying to provide an adaptable portal for the Stable Diffusion model to input multi-modal data, such as text, audio, i.e., (a) and (b), or text-audio hy-brid signals, i.e. (c), for X-to-image generation. and the availability of large-scale paired training datasets
[42, 70, 45, 50, 49]. Diffusion-based T2I generative mod-els in particular have achieved remarkable results in terms of image quality [24, 36, 3, 47, 35, 73]. Despite these strong results, controllable generation for these methods is still challenging: generated images are often not faithful to the captions, compositional capabilities are lacking, and prompt engineering is often required to achieve the desired results [37]. Moreover, most large-scale models have only been trained on English text captions, greatly limiting their use across the world.
Recent research has emphasized the crucial role of text encoders in improving Text-to-Image (T2I) models’ perfor-mance, and their ability to comprehend and represent text is considered a bottleneck for image generation [47, 10].
However, the current T2I models’ text encoders are often trained on short image captions, which limits their per-formance on complex prompts and challenges their qual-Furthermore, T2I mod-ity of feature extraction [45]. els’ capacities are limited to generating images from text, and they cannot incorporate multimodal conditions such as sound and audio easily. Nevertheless, replacing the text en-coder in existing T2I models is challenging since the text encoder and image generator’s representation spaces are tightly coupled [45, 42]. This severe domain gap between the new conditions and the existing model impedes the im-age generation’s ﬁnal performance, and training the entire
T2I model from scratch, with higher quality image-caption pairs, would be prohibitively expensive [14].2
As seen in Fig. 1, we propose GlueNet to address the challenge of efﬁciently replacing or upgrading the text encoder in existing diffusion-based T2I models. With
GlueNet, off-the-shelf pre-trained language models and multimodal encoders can be easily aligned with image en-coders of T2I models, greatly enlarging their functionalities
Importantly, this can be achieved without at a low cost. requiring retraining from scratch or even ﬁnetuning, main-taining the representation alignment between the text and image encoders. The proposed method follows an encoder-decoder structure. The encoder of GlueNet ﬁrst aligns the representation space of the new condition encoder with that of the T2I model’s image generator, minimizing both element-wise and global-wise discrepancy. Then, the de-coder of GlueNet maps the aligned condition representa-tions back to the original representation space of the new condition encoder by minimizing the reconstruction loss, preserving rich semantics captured by the pre-trained model during alignment training. Align existing models would inevitably decrease feature discriminability [7, 11], which makes the feature decoder necessary. The entire training of GlueNet requires only a parallel corpus with the same content but different modalities or languages. At inference time, only the encoder of GlueNet is applied on top of the new condition encoder for representation alignment.
To verify the effectiveness of the proposed framework, we conducted three major experiments ranging from single-and multi-modal encoders. Firstly, we upgraded the exist-ing text encoders of the Latent Diffusion Model [45] us-ing a stronger language model, T5-3B [41]. Our model showed competitive improvements in FID score and user study ranking compared to the baselines but it still re-quired ﬁnetuning for the overall performance boost. Sec-ondly, we aligned a multilingual language model, XLM-Roberta-L [9], using our approach, enabling multilingual text-to-image generation. It achieved competitive results of translation-based models under a signiﬁcantly lower train-ing cost. Finally, we demonstrated GlueNet’s capability to bring new functionalities beyond text signals into existing
T2I models. The alignment of the AudioClip [20] encoder enables sound-to-image generation without requiring any 2The cost of training a Stable Diffusion model is around 600K USD. parameter ﬁnetuning of the image generator. This new ca-pability allows the existing Stable Diffusion model to gen-erate high-quality images that correspond to sound signals such as dogs barking and street music. This new capabil-ity goes beyond the traditional T2I generation and opens up new possibilities for creating multimedia content towards
X-to-Image (X2I) generation.
Our contributions can be summarized as follows:
• To the best of our knowledge, this is the ﬁrst work to consider the problem of efﬁciently aligning a pre-trained audio model with a pre-trained T2I diffusion model for sound-to-image generation.
• Extensive experiments on text-to-image generation benchmarks demonstrate the superiority of our model over the baseline LDM method on both image quality and language controllability.
• Our framework also enables text-to-image generation beyond English prompts without the need of multilin-gual image-text pairs for retraining. 2.