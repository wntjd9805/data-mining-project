Abstract
Recently proposed LaMa [25] introduce Fast Fourier
Convolution (FFC) [4] into image inpainting. FFC empow-ers the fully convolutional network to have a global recep-tive ﬁeld in its early layers, and have the ability to produce robust repeating texture. However, LaMa has difﬁculty in generating clear and sharp complex content. In this paper, we analyze the fundamental ﬂaws of using FFC in image inpainting, which are 1) spectrum shifting, 2) unexpected spatial activation, and 3) limited frequency receptive ﬁeld.
Such ﬂaws make FFC-based inpainting framework difﬁcult in generating complicated texture and performing faithful reconstruction. Based on the above analysis, we propose a novel Unbiased Fast Fourier Convolution (UFFC) module.
UFFC is constructed by modifying the vanilla FFC mod-ule with 1) range transform and inverse transform, 2) abso-lute position embedding, 3) dynamic skip connection, and 4) adaptive clip, to overcome the above ﬂaws. UFFC cap-tures frequency information efﬁciently and realize recon-struction without introducing additional artifacts, achiev-ing better inpainting results and more efﬁcient training. In
*Corresponding author. addition, we propose two novel perceptual losses for better generation quality and more robust training. Extensive ex-periments on several benchmark datasets demonstrate the effectiveness of our method, outperforming the state-of-the-art methods in both texture-capturing ability and expres-siveness. 1.

Introduction
Image inpainting (also known as image completion) is a subtask of the low-level vision tasks that aims to recover the masked missing/degraded area by referring to the con-tent of the undegraded area. Traditional non-learning image inpainting methods, such as diffusion [2], PatchMatch [1], etc., use statistical information of the undegraded area to infer the missing content. These methods can produce in-painting results with reasonable structure and texture when the mask area is small, or the undegraded part shows well-deﬁned geometry. However, these methods often perform poorly when being required to recover semantics since they lack image semantic priors.
Compared with the non-learning methods, learning-based models can obtain semantic priors for a certain class 1
of images after training. Therefore, a reasonable inpainting effect can be achieved by generating content that does not exist in the current degraded image. However, in the case of extremely large and continuous masks, learning-based methods are still prone to artifacts. The researchers found that this is because the limited receptive ﬁeld of classic Con-volutional Neural Network (CNN) backbones is hard to cap-ture the global semantics. Hence, many works [33, 30, 28] have been proposed to increase the receptive ﬁeld of image inpainting models. But none of them can achieve the bal-ance between receptive ﬁeld and computational cost. Re-cently, Suvorov et al. proposed LaMa [25], which directly introduces Fast Fourier Convolution (FFC) [4] to image in-painting for obtaining the global receptive ﬁeld with rela-tively small computational cost.
In detail, FFC performs
ChannelFC - batch normalization - ReLU in frequency do-main after fast Fourier transform (refer to Fig. 2). The in-ductive bias of the Fourier transform makes LaMa performs better in inpainting images with ﬁxed pattern texture.
Though LaMa has the ability to capture global pattern, researchers have found that LaMa has difﬁculty in gener-ating clear and sharp complex content. Recall that high-level vision tasks are to convert high-dimensional input into low-dimensional output, thus requiring the model to ﬁlter out irrelevant information while retaining principal com-ponents representing classiﬁcation labels. FFC [4] was
ﬁrst designed for high-level vision tasks (classiﬁcation) and has achieved SOTA performance. On the contrary, low-level vision tasks have to preserve semantic information and achieve accurate pixel-level reconstruction, which is difﬁ-cult for FFC. Therefore, it is inappropriate to directly apply
FFC to low-level vision tasks without any speciﬁc adap-tation. Speciﬁcally, simply ﬁltering out all negative val-ues via ReLU operation by FFC in the frequency domain will damage the statistics of the spectrum, causing artifacts and unexpected extremely large values in the spatial fea-ture after the inverse Fourier transform. Tiny deviations in the frequency feature can accumulate in the spatial fea-ture (and vice versa), often resulting in biases that are sev-eral orders of magnitude larger than normal value, which cause additional artifacts and compressed effective feature values after the normalization layer. In addition, the chan-nelFC (conv1x1) applied on the frequency feature only cal-culate among features with the same frequency, while ignor-ing the relationship between different frequencies, which makes FFC difﬁculty to capture complex content. This goes against the requirement of low-level vision tasks. We sum-marize FFC’s ﬂaws in inpainting as 1) spectrum shifting, 2) unexpected spatial activation, and 3) limited frequency re-ceptive ﬁeld. As can be seen from Fig. 1, though FFC can capture texture patterns compared to the commonly used spatial module (ResBlock), it still inevitably suffers from biased color and artifacts in inpainting.
To address the above issues, we propose a novel Unbi-ased Fast Fourier Convolution (UFFC) module to replace
FFC in LaMa.
In addition to Fourier transform/inverse transform and activation function, UFFC mainly contains learnable range transform/inverse transform, dynamic skip connection, position embedding, and adaptive clip. Those components enable the UFFC module to obtain stronger feature capture capabilities while avoiding the fundamental
ﬂaws of FFC in inpainting. Our contribution can be sum-marized as follows:
- We ﬁnd out the reason why FFC is not suitable to be directly applied to image inpainting and the issues that may result when doing so by analyzing the difference between high/low-level vision task and frequency/spatial domain.
- We propose a novel Unbiased Fast Fourier Convolution (UFFC) module, which can capture frequency information more efﬁciently and accurately than FFC by avoiding fun-damental ﬂaws such as 1) spectrum shifting, 2) unexpected spatial activation, and 3) limited frequency receptive ﬁeld. loss and self-perceptual loss for better generation quality and more robust training.
- We propose MAE [10] perceptual
- Extensive experiments on Places2 [36], CelebA [15], and Paris Streetview [6] show that our method converges faster and generates better inpainting results than LaMa [25] and is competitive with other SOTA inpainting models. Ex-periments on DTD [5] show that our UFFC has a stronger ability to capture textures than FFC. 2.