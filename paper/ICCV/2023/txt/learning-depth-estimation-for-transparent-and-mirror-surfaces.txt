Abstract
RGB
Base
Ours
Inferring the depth of transparent or mirror (ToM) sur-faces represents a hard challenge for either sensors, algo-rithms, or deep networks. We propose a simple pipeline for learning to estimate depth properly for such surfaces with neural networks, without requiring any ground-truth anno-tation. We unveil how to obtain reliable pseudo labels by in-painting ToM objects in images and processing them with a monocular depth estimation model. These labels can be used to fine-tune existing monocular or stereo networks, to let them learn how to deal with ToM surfaces. Experimental results on the Booster dataset show the dramatic improve-ments enabled by our remarkably simple proposal. 1.

Introduction
In our daily lives, we often interact with several objects of various appearances. Among them are those made of transparent or mirror surfaces (ToM), ranging from the glass windows of buildings to the reflective surfaces of cars and appliances. These might represent a hard challenge for an autonomous agent leveraging computer vision to operate in unknown environments. Specifically, among the many tasks involved in Spatial AI, accurately estimating depth infor-mation on these surfaces remains a challenging problem for both computer vision algorithms and deep networks [64], yet necessary for proper interaction with the environment in robotic, autonomous navigation, picking, and other ap-plication fields. This difficulty arises because ToM surfaces introduce misleading visual information about scene geom-etry, which makes depth estimation challenging not only for computer vision systems but even for humans – e.g., we might not distinguish the presence of a glass door in front of us due to its transparency. On the one hand, the defini-tion of depth itself might appear ambiguous in such cases: o n o
M o e r e t
S
Figure 1. Depth estimation on ToM surfaces. Two examples for both monocular (top) and stereo (bottom) images. In the cen-tral column, the depth/disparity maps predicted by DPT [37] and
CREStereo [22] original weights.
In the rightmost column, the depth/disparity maps predicted by the models after being fine-tuned by our strategy without exploiting any ground-truth depth. is depth the distance to the scene behind the glass door or to the door itself? Nonetheless, from a practical point of view, we argue that the actual definition depends on the task it-self – e.g., a mobile robot should definitely be aware of the presence of the glass door. On the other hand, as humans can deal with this through experience, depth sensing tech-niques based on deep learning, e.g., monocular [38, 37] or stereo [26, 22] networks, hold the potential to address this challenge given sufficient training data [64].
*These authors contributed equally to this work.
Unfortunately, light reflection and refraction over ToM
surfaces violate also the working principles of most ac-tive depth sensors, such as Time-of-Flight (ToF) cameras or devices projecting structured-light patterns. This has two practical consequences: i) it makes active sensors unsuited to deal with ToM objects in real-world applications, and ii) prevents the use of these sensors for collecting and anno-tating data to train deep neural networks to deal with ToM objects. As evidence of this, very few datasets featuring transparent objects provide ground-truth depth annotations, which have been obtained through very intensive human intervention [64], graphical engines [40], or based on the availability of CAD models [5] for ToM objects.
In short, accurately perceiving the presence (and depth) of ToM objects represents an open challenge for both sens-ing technologies and deep learning frameworks. Purposely, this paper proposes a simple yet effective strategy for ob-taining training data and, thereby, dramatically boosting the accuracy of learning-based depth estimation frameworks dealing with ToM surfaces. Driven by the observation that
ToM objects alone are responsible for misleading recent monocular networks [38, 37], which would otherwise gen-eralize well to most unseen environments, we argue that replacing them with equivalent, yet opaque objects would allow restoring an environment layout in which such net-works could accurately estimate the depth of the scene. To this end, we mask ToM objects in images by in-painting them with arbitrary uniform colors. Then, we employ a monocular depth network to generate a virtual depth map out of the modified image. By repeating this process on a variety of images featuring ToM objects, we can easily and effectively annotate a dataset and then use it to train the same monocular network used to distill labels, which will now process the not-in-painted images. As a result, the trained monocular network will learn to handle ToM ob-jects, producing consistent depth even in their presence.
Our main contributions can be resumed as follows:
• We propose a simple yet very effective strategy to deal with ToM objects. We trick a monocular depth estima-tion network by replacing ToM objects with virtually textured ones, inducing it to hallucinate their depths.
• We introduce a processing pipeline for fine-tuning a monocular depth estimation network to deal with ToM objects. Our pipeline exploits the network itself to generate virtual depth annotations and requires only segmentation masks delineating ToM objects – either human-made or predicted by other networks [54, 60] – thus getting rid of the need for any depth annotations.
• We show how our strategy can be extended to other depth estimation settings, such as stereo matching.
Our experiments on the Booster dataset [64] prove how monocular and stereo networks dramatically improve their prediction on ToM objects after being fine-tuned according to our methodology.
Fig. 1 highlights some specific regions where monocular (top) and stereo (bottom) models struggle (middle column), and how they learn to handle ToM surfaces thanks to our strategy (rightmost column). 2.