Abstract
Performing holistic 3D scene understanding from a single-view observation, involving generating instance shapes and 3D scene segmentation, is a long-standing chal-lenge. Prevailing works either focus only on geometry or segmentation, or model the task in two folds by separate modules, whose results are merged later to form the final prediction. Inspired by recent advances in 2D vision that unify image segmentation and detection by Transformer-based models, we present Uni-3D, a holistic 3D scene pars-ing/reconstruction system for a single RGB image. Uni-3D features a universal model with query-based represen-tations for predicting segments of both object instances and scene layout. In Uni-3D, we also introduce a single Trans-former for 2D depth-aware panoptic segmentation, which offers queries that serve as strong shape priors in 3D. Uni-3D seamlessly integrates 2D and 3D in its architecture and it outperforms previous methods significantly. 1.

Introduction
Humans have a remarkable ability to infer 3D shapes and scene layouts accurately from limited or single-view obser-vations, which is attributed to the efficient representations that encode the 3D world for 2D projection. In computer vision and graphics, understanding the 3D world from a single-view 2D observation is a longstanding task, which plays an essential role in multiple downstream applications such as autonomous driving, augmented reality, and robotic systems.
Notable breakthroughs have been made recently in ad-dressing this challenge, thanks to the advancements in neu-ral networks and the rapid growth of data quantity. 3D shape reconstruction methods [45, 48, 24, 17] aim to predict 3D models of instances in images and have exhibited impres-sive reconstruction quality. Another category of methods, including [41, 10], performs scene reconstruction by di-rectly recovering the geometric structure of the 3D scene
* indicates equal contribution.
Code: https://github.com/mlpc-ucsd/Uni-3D.
Figure 1. Pipeline comparison between (a) previous work (e.g. [8]) and (b) our proposed universal 3D reconstruction framework (Uni-3D). Uni-3D integrates panoptic segmentation and depth estima-tion in 2D for shared knowledge and unifies thing and stuff in-stances in 3D, which is a universal pipeline for panoptic 3D scene reconstruction. captured by 2D images. The aforementioned approaches demonstrate the capability of deep neural networks to learn shape priors from training data and recover the 3D world from 2D observations during inference. However, they fo-cus solely on 3D shapes and silhouettes of instances or en-tire scenes and do not extract 3D semantic information nec-essary for scene understanding.
Factored3D [42] is a pioneering work that not only re-covers 3D shapes but also performs scene understanding by predicting the layout of instances in scenes. Huang et al. [21] propose a holistic scene understanding system that unifies the estimation of 3D object poses, camera pose, and scene layout, providing a comprehensive understanding of the input scene. Total3D [37] further incorporates mesh re-construction into the 3D scene understanding system.
In summary, these models have approached 3D scene under-standing from three aspects: 1) instance reconstruction; 2) scene geometric structure prediction; and 3) 3D layout. We are interested in an all-in-one system that integrates all three aspects and provides all estimations in a single run. Liu et al. [31] developed a two-stage system that can perform 3D scene parsing and reconstruction for a single in-the-wild image, as well as an end-to-end system for 3D scene recon-struction. The work most relevant to our target is Pano-Re [8]. It tackles the tasks of single-image geometric re-construction, 3D semantic segmentation, and 3D instance segmentation simultaneously, which is named by them as panoptic 3D scene reconstruction. However, it lifts 2D in-stance features back to 3D and embeds 2D instance seg-mentation maps as priors to link the two dimensions, which only handles instances in 2D without stuff information so that the 3D part of its network is only informed by instance priors.
Taking inspiration from the trend of unifying 2D se-mantic and instance segmentation via Transformers such as
Mask2Former [4], we introduce a universal system called
Uni-3D that addresses the task of 3D scene understanding in a holistic manner. Specifically, we first introduce a depth-aware panoptic segmentation architecture based on Trans-formers, which integrates all 2D predictions in a unified paradigm. Benefiting from this, the query embeddings of our 2D Transformers are informative, effectively learning multiple properties of the input scene, such as layout, in-stance silhouettes, category labels, and depth. We then build a query-based architecture for the 3D part, where 3D seg-ments of both object instances and stuff layout are predicted individually with the guidance of corresponding 2D queries.
This query-based design seamlessly integrates learned 2D features and priors into 3D, offering more flexibility and ro-bustness.
Our contribution is summarized as follows:
• We introduce Uni-3D, a universal model that integrates 3D instance and semantic segmentation as a panoptic one by learning segment representations for both in-stances and stuff layout. Each 3D segment is learned with the guidance from its corresponding 2D query which serves as a strong 2D prior.
• In Uni-3D, we develop a 2D Transformer for unifying 2D panoptic segmentation and depth estimation. This approach facilitates interactions between the two tasks, offering shared knowledge from both tasks to features and learnable queries that serve as shape priors for pro-ducing 3D segments.
Uni-3D outperforms previous methods by a large margin both in quantitative and qualitative evaluations. 2.