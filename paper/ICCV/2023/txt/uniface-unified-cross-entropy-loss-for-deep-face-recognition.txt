Abstract
As a widely used loss function in deep face recognition, the softmax loss cannot guarantee that the minimum posi-tive sample-to-class similarity is larger than the maximum negative sample-to-class similarity. As a result, no unified threshold is available to separate positive sample-to-class pairs from negative sample-to-class pairs. To bridge this gap, we design a UCE (Unified Cross-Entropy) loss for face recognition model training, which is built on the vital con-straint that all the positive sample-to-class similarities shall be larger than the negative ones. Our UCE loss can be integrated with margins for a further performance boost.
The face recognition model trained with the proposed UCE loss, UniFace, was intensively evaluated using a number of popular public datasets like MFR, IJB-C, LFW, CFP-FP, AgeDB, and MegaFace. Experimental results show that our approach outperforms SOTA methods like SphereFace,
CosFace, ArcFace, Partial FC, etc. Especially, till the sub-mission of this work (Mar. 8, 2023), the proposed UniFace achieves the highest TAR@MR-All on the academic track of the MFR-ongoing challenge. Code is publicly available. 1.

Introduction
Face recognition, from verification on mobile phones to identification on surveillance streams, plays an important role in our daily life. A general face recognition system contains three core steps: face detection, facial feature ex-traction, and recognition (including one-to-one verification and one-to-all identification). Discriminative facial feature learning is therefore crucial to face recognition systems.
Specifically, a facial feature of a subject should be close to the features belonging to the same identity, while being
†Equal contribution; #Corresponding author. Parts of the work were done when J. Z and X. J were students at Shenzhen University.
Figure 1. From (a), (b), and (c), we illustrate the sample-to-class similarities learned from normalized softmax loss, marginal soft-max loss, and the proposed UCE loss, in which Wi is the class proxy, and x(i) is a face image/feature with identity i. The classi-fication of all three faces is correct with all three losses. However, both normalized and marginal softmax loss can not separate pos-itive from negative sample-to-class pairs with a proper threshold, while with a unified threshold t = cos θt, our UCE loss can. far from the features of the other identities, i.e., the mini-mum feature similarity from positive pairs should ideally be larger than a threshold t and the maximum feature similarity from negative pairs should be smaller than this t. Inspired by the success of deep neural networks on natural image classification tasks, modern facial recognition approaches are mostly based on deep convolutional neural networks.
Such approaches can be broadly split into two categories according to their learning objectives, i.e., 1) sample-to-sample distance and 2) sample-to-class similarity. Sample-to-sample distance-based methods [19, 17] map the face images into a high-compact Euclidean feature space where distances are used to measure the facial feature similarity.
However, the training of such methods is difficult [17], as they require sophisticated sampling strategies for construct-ing efficient negative and positive pairs/tuples. Sample-to-class similarity-based methods [22, 21] usually adopt the softmax loss as the learning objective and tackle face recog-nition as a multi-class classification problem. Some works proposed to combine the softmax loss with extra carefully designed losses to either increase the intra-class similarity
[28] or decrease the inter-class similarity [11, 7, 30]. Such methods, however, introduce additional hyper-parameters which require elaborated hyper-parameter tuning. The other works (such as L-softmax loss [14], SphereFace[13], AM-softmax[23], CosFace [25], and ArcFace [6]) extended the original softmax loss by introducing marginal distance be-tween classes to decrease the intra-class distance and in-crease the inter-class distance.
However, softmax loss only encourages the largest an-gular similarity between an individual training sample and its corresponding positive class proxy, it does not consider the similarities between this positive class proxy and other samples (see Sec. 3.1). In other words, it’s difficult to select a unified threshold t to separate negative sample-to-class pairs from positive ones with both the softmax and marginal softmax loss, as illustrated in Fig. 1. To solve the prob-lem, we propose a Unified Cross-Entropy (UCE) loss which explicitly encourages that all the positive sample-to-class similarities are larger than a threshold t = cos θt, while all negative sample-to-class similarities are smaller than this t.
We elaborate on the proposed UCE loss (Sec. 3.2) and dis-cuss its design principle linked with real applications (Sec. 3.3). We further improve the UCE loss by 1) introducing an enforced margin and 2) proposing two alternative ways to balance its training on a large number of face identities (Sec. 3.4). We name the face model trained with our UCE loss as UniFace and evaluate it on several public large-scale benchmarks (Sec. 4 and 5). The contributions of this work are summarized as follows:
• After investigating the softmax loss, we found that its learned minimum positive sample-to-class similarity is actually not guaranteed to be larger than its maxi-mum negative sample-to-class similarity. To address this problem, we design the UCE loss by supposing a fixed threshold t to constrain the similarity of both positive and negative sample-to-class pairs.
• Though separating positive and negative pairs with a unified threshold t is a prestigious idea, to the best of our knowledge, this paper is the first work that incor-porates the unified t as an automatic learnable parame-ter in a deep face recognition framework. Our UCE loss encourages that all the positive sample-to-class similarities are larger than the negative ones, which matches well with the expectation of real face recogni-tion applications.
• Our UCE loss works well alone and can directly re-place the softmax loss in existing deep face recognition models (Table 1 and 2). We additionally propose two extensions of UCE, i.e., marginal UCE and balanced
UCE losses, to integrate with margins and balancing strategies to improve the performance of UCE loss. It is noticeable that the marginal UCE loss is more robust to hyper-parameters than the softmax loss (Fig. 3(a)).
• The face recognition model trained with the proposed
UCE loss, UniFace, was intensively evaluated using a number of popular public datasets like MFR, IJB-C,
LFW, CFP-FP, AgeDB, and MegaFace. Experimen-tal results show that our approach outperforms SOTA methods such as SphereFace, CosFace, ArcFace, and
Partial FC. 2.