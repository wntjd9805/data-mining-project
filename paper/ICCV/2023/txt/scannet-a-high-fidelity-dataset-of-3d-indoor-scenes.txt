Abstract 1.

Introduction
We present ScanNet++, a large-scale dataset that cou-ples together capture of high-quality and commodity-level geometry and color of indoor scenes. Each scene is cap-tured with a high-end laser scanner at sub-millimeter res-olution, along with registered 33-megapixel images from a
DSLR camera, and RGB-D streams from an iPhone. Scene reconstructions are further annotated with an open vocab-ulary of semantics, with label-ambiguous scenarios explic-itly annotated for comprehensive semantic understanding.
ScanNet++ enables a new real-world benchmark for novel view synthesis, both from high-quality RGB capture, and importantly also from commodity-level images, in addition to a new benchmark for 3D semantic scene understanding that comprehensively encapsulates diverse and ambiguous semantic labeling scenarios. Currently, ScanNet++ con-tains 460 scenes, 280,000 captured DSLR images, and over 3.7M iPhone RGBD frames.
*Equal contribution.
†Project page: https://cy94.github.io/scannetpp/
Reconstruction and understanding of 3D scenes is fun-damental to many applications in computer vision, includ-ing robotics, autonomous driving, mixed reality and con-tent creation, among others. The last several years have seen a revolution in representing and reconstructing 3D scenes with groundbreaking networks such as neural radi-ance fields (NeRFs) [27]. NeRFs optimize complex scene representations from an input set of posed RGB images with a continuous volumetric scene function to enable syn-thesis of novel image views, with recent works achiev-ing improved efficiency, speed, and scene regularization
[40, 11, 48, 25, 23, 1, 2, 5, 29]. Recent works have even extended the photometric-based formulation to further opti-mize scene semantics based on 2D semantic signal from the input RGB images [52, 43, 12, 21, 36].
Notably, such radiance field scene representations focus on individual per-scene optimization, without learning gen-eralized priors for view synthesis. This is due to the lack of large-scale datasets which would support learning such general priors. As shown in Table 1, existing datasets ei-1
Dataset
Num. scenes
LLFF [26]
DTU [15]
BlendedMVS [47]
ScanNet [9]
Matterport3D [4]
Tanks and Temples [20]
ETH3D [35]
ARKitScenes [3]
ScanNet++ (ours) 35 124 113 1503 90‡ 21 25 1004 460
Average scans/scene
-----10.57 2.33 3.16 4.85
Total scans
-----74 42 3179 1858
RGB (MP)
Depth
Commodity DSLR LR HR
✗
✗
✗ ✓
✗
✗
✓ ✗
✓ ✗
✗ ✓
✗ ✓
✓ ✓
✓ ✓ 12 1.9
✗ 1.25 1.3
✗
✗ 3 2.7
✗
✗
✗
✗
✗ 8 24
✗ 33
Dense semantics NVS
✗
✗
✓
✓
✓
✗
✗
✗
✓
✓
✓
✗
✗
✗
✓
✗
✗
✓
Table 1: Comparison of datasets in terms of RGB and geometry. ScanNet++ surpasses existing datasets in terms of resolution, quality, density of semantic annotations, and coverage of laser scans. While the quality of the reconstructed geometry in
ARKitScenes is similar to ours, we additionally capture DSLR data to support the novel view synthesis task (NVS) and provide dense semantic annotations. ther contain a large quantity of scenes that lack high-quality color and geometry capture, or contain a very limited num-ber of scenes with high-quality color and geometry. We propose to bridge this divide with ScanNet++, a large-scale dataset that contains both high-quality color and geometry capture coupled with commodity-level data of indoor envi-ronments. We hope that this inspires future work on gener-alizable novel view synthesis with semantic priors.
ScanNet++ contains 460 scenes covering a total floor area of 15,000m2, with each scene captured by a Faro Fo-cus Premium laser scanner at sub-millimeter resolution with an average distance of 0.9mm between points in a scan,
DSLR camera images at 33-megapixels, and RGB-D video from an iPhone 13 Pro. All sensor modalities are regis-tered together to enable seamless interaction between ge-ometric and color modalities, as well as commodity-level and high-end data capture. Furthermore, as semantic un-derstanding and reconstruction can be seen as interdepen-dent, each captured scene is additionally densely annotated with its semantic instances. Since semantic labeling can be ambiguous in many scenarios, we collect annotations that are both open-vocabulary and explicitly label semantically ambiguous instances, with more than 1000 unique classes annotated.
ScanNet++ thus supports new benchmarks for novel view synthesis and 3D semantic scene understanding, en-abling evaluation against precise real-world ground truth not previously available. This enables comprehensive, quantitative evaluation of state-of-the-art methods with a general and fair evaluation across a diversity of scene sce-narios, opening avenues for new improvement.
For novel view synthesis, we also introduce a new task of view synthesis from commodity sensor data to match that of high-quality DSLR ground truth capture, which we believe will push existing methodologies to their limits. In contrast to existing 3D semantic scene understanding benchmarks, we explicitly take into account label ambiguities for more accurate, comprehensive semantics.
To summarize, our main contributions are:
• We present a new large-scale and high-resolution in-door dataset with 3D reconstructions, high-quality
RGB images, commodity RGB-D video, and seman-tic annotations covering label ambiguities.
• Our dataset enables optimizing and benchmarking novel view synthesis on large-scale real-world scenes from both high-quality DSLR and commodity-level iPhone images. Instead of sampling from the scanning trajectory for testing ground-truth images, we provide a more challenging setting where testing images are captured independently from the scanning trajectory.
• Our 3D semantic data enables training and benchmark-ing a comprehensive view of semantic understanding that handles possible label ambiguities inherent to se-mantic labeling tasks. 2.