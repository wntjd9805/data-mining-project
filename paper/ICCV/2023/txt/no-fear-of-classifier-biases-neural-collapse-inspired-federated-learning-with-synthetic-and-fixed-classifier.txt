Abstract
Data heterogeneity is an inherent challenge that hinders the performance of federated learning (FL). Recent stud-ies have identified the biased classifiers of local models as the key bottleneck. Previous attempts have used classifier calibration after FL training, but this approach falls short in improving the poor feature representations caused by training-time classifier biases. Resolving the classifier bias dilemma in FL requires a full understanding of the mech-anisms behind the classifier. Recent advances in neural collapse have shown that the classifiers and feature pro-totypes under perfect training scenarios collapse into an optimal structure called simplex equiangular tight frame (ETF). Building on this neural collapse insight, we pro-pose a solution to the FL’s classifier bias problem by uti-lizing a synthetic and fixed ETF classifier during training.
The optimal classifier structure enables all clients to learn unified and optimal feature representations even under ex-tremely heterogeneous data. We devise several effective modules to better adapt the ETF structure in FL, achiev-ing both high generalization and personalization. Extensive experiments demonstrate that our method achieves state-of-the-art performances on CIFAR-10, CIFAR-100, and Tiny-ImageNet. The code is available at https://github. com/ZexiLee/ICCV-2023-FedETF. 1.

Introduction
Federated learning (FL) [32, 28, 48, 26] is a distributed training paradigm that enables collaborative training from massive multi-source datasets without transferring the raw data, reserving data ownership [29] while relieving com-munication burdens [32]. FL facilitates broad applications in medical images [2, 7], the internet of things [17, 33],
*Corresponding authors. †Work was done during Xinyi’s visit to West-lake University. mobile services [8, 16], and so on; it shows promising prospects in data collaboration. However, clients in FL training may hold heterogeneous data, in other words, clients’ datasets are in Non-IID distributions1, which causes a huge degradation to the global model’s generalization
[5, 31, 3, 27, 13, 14].
Numerous recent studies have shown that classifier bi-ases in clients’ local models caused by Non-IID data are the primary cause of degradation in FL [31, 52, 21].
It has been discovered that the classifier layer is more biased than other layers [31], and classifier biases will create a vi-cious cycle between biased classifiers and misaligned fea-tures across clients [52]. Figure 1 illustrates the issue of classifier bias in FL, where Non-IID data leads to poor pair-wise cosine similarities among clients’ classifiers and fea-ture prototypes. Furthermore, class-wise classifier vectors of clients are scattered in the embedding space, leading to significant generalization declines.
Previous research has attempted to mitigate classifier bi-ases through classifier retraining via generated virtual fea-tures at the end of FL training [31, 37]. However, these methods fail to address classifier biases during training. Bi-ased classifiers during the training phase lead to inadequate feature extractors and poor representations of generated fea-tures, negatively affecting the retrained classifiers. Our ex-periments have also shown the limitations of classifier re-training methods (Table 1). Therefore, we wonder:
Can we break the classifier bias dilemma during training, improving both the classifiers and feature representations?
To resolve the classifier bias dilemma, it is essential to fully understand the mechanisms behind the classifier. We further wonder: what are the properties of a well-trained (i.e. good) classifier? An emerging discovery called neural collapse [34, 43, 25, 44] has shed light on this matter. It describes the phenomenon that, in the perfect training sce-1We use “data heterogeneity” and “Non-IID data” interchangeably.
Figure 1. How data heterogeneity causes classifier biases in FL. Smaller α corresponds to higher Non-IID. Experiments are conducted on CIFAR-10 with vanilla FEDAVG. Columns from left to right: (1) Non-IID data results in poor generalization, biased classifiers, and misaligned features. (2) Clients’ data distributions. (3) Clients with Non-IID data have smaller pair-wise classifier cosine similarities. (4) t-SNE visualization of clients’ class-wise classifier vectors (represented by colors), which are more scattered in Non-IID data. nario, where the dataset is balanced and sufficient, the fea-ture prototypes and classifier vectors converge to an optimal simplex equiangular tight frame (ETF) with maximal pair-wise angles [34]. Insights from balanced training inspire us to tackle the challenges in Non-IID FL.
Thus, in this paper, we fundamentally solve the FL’s classifier bias problem with a neural-collapse-inspired ap-proach. Knowing the optimal classifier structure, we make the first attempt to introduce a synthetic simplex ETF as a fixed classifier for all clients so that the clients can learn unified and optimal feature representations even under high heterogeneity. We devise FEDETF which incorporates sev-eral effective modules that better adapt the ETF structure in
FL training, reaching strong results on both generalization and personalization.
Specifically, we employ a projection layer that maps the features to a space where neural collapse is more likely to occur. We also implement a balanced feature loss with a learnable temperature to minimize entropy between the fea-tures and the fixed ETF classifier. These techniques enable us to achieve high generalization performance of the global model during FL training. To further improve personaliza-tion, we introduce a novel fine-tuning strategy that adapts the global model locally after FL training. Extensive ex-periments have strongly supported the effectiveness of our method. Our contributions are summarized as follows.
• To the best of our knowledge, this is the first paper that tackles the data heterogeneity problem in FL from the per-spective of neural collapse.
• We devise FEDETF, which takes the simplex ETF as a fixed classifier, and it fundamentally solves the classifier biases brought by Non-IID data, reaching high general-ization of the global model.
• We propose a local fine-tuning strategy in FEDETF to boost personalization in each client after FL training.
• Our method is validated on three vision datasets: CIFAR-10, CIFAR-100, and Tiny-ImageNet. Our proposed method outperforms strong baselines and achieves sota in both generalization and personalization. 2.