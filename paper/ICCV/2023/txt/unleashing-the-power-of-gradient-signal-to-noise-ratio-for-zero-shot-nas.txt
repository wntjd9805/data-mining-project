Abstract
Neural Architecture Search (NAS) aims to automatically find optimal neural network architectures in an efficient way. Zero-Shot NAS is a promising technique that leverages proxies to predict the accuracy of candidate architectures without any training. However, we have observed that most existing proxies do not consistently perform well across dif-ferent search spaces, and are less concerned with general-ization. Recently, the gradient signal-to-noise ratio (GSNR) was shown to be correlated with neural network general-ization performance. In this paper, we not only explicitly give the probability that larger GSNR at network initializa-tion can ensure better generalization, but also theoretically prove that GSNR can ensure better convergence. Then we design the ξ-based gradient signal-to-noise ratio (ξ-GSNR) as a Zero-Shot NAS proxy to predict the network accuracy at initialization. Extensive experiments in different search spaces demonstrate that ξ-GSNR provides superior rank-ing consistency compared to previous proxies. Moreover, ξ-GSNR-based Zero-Shot NAS also achieves outstanding per-formance when directly searching for the optimal architec-ture in various search spaces and datasets. The source code is available at https://github.com/Sunzh1996/Xi-GSNR. 1.

Introduction
Neural architecture search (NAS)
[16] is a technique that automates the design of neural network architectures, easing the burden of human trial and error
[19, 44]. The main challenge of NAS is to evaluate the performance of each candidate architecture in a given search space. Early approaches that trained each architecture separately until convergence were time-consuming and resource-intensive
†Equal contribution. ∗Corresponding authors. (a) GSNR vs. Generalization (b) GSNR vs. Convergence
Figure 1. (a) Networks with larger GSNR tend to have a higher generalization ratio, indicating better generalization. Generaliza-tion Ratio: the ratio between one-step validation loss and training loss reduction. (b) Networks with larger GSNR have lower train-ing loss, thus better convergence.
[3, 68, 42, 41]. To improve the search efficiency, ENAS
[38] proposed to share the same operation weights in a super-network. This reduces the training cost to only one super-network, namely One-Shot NAS [4]. Gradient-based
NAS [32, 64, 8, 57, 14, 11, 9, 58] and Sampling-based NAS
[17, 10, 61, 65, 62] are two mainstream paradigms for train-ing super-networks. However, they still require training be-fore evaluating each candidate architecture, suffering from extra search overhead.
In order to eliminate the need for training in NAS, Zero-Shot NAS [35] was developed to evaluate network perfor-mance without any training, thus significantly promoting search speed. To achieve this, a series of proxies are de-signed to predict the performance of a given candidate ar-chitecture. Some of these proxies are based on empirical inspiration from the pruning-at-initialization literature, us-ing saliency metrics such as SNIP [23], GraSP [54], Syn-Flow [51] and so on. Whereas, recent studies [1, 36] have shown that these proxies perform poorly in correlating with network performance. Other Zero-Shot proxies are theo-retically designed from the deep learning theory of neu-ral networks [35, 30, 46, 7, 66, 28]. For example, some proxies [35, 30] analyzed the relationship between the lin-ear region [40] and network performance, some leveraged
Neural Tangent Kernel (NTK) [21] to assess the predictive performance of a network, while others [66, 28] analyzed the gradients of the sample-wise optimization [2]. Never-theless, most existing proxies cannot consistently perform well across different search spaces, and are less concerned with generalization, motivating us to address this issue.
As revealed in recent studies [47, 53], architectures with faster convergence are preferred by NAS algorithms, but may not guarantee good generalization performance. This insight motivates us to consider both generalization and convergence when designing proxy indicators. The gradient signal-to-noise ratio (GSNR) was proposed in [33], and is defined as the ratio between the squared mean and variance of the parameter gradient. In this paper, we clearly give the probability guarantee of good generalization is positively correlated with higher GSNR. Moreover, we have further provided theoretical evidence of a strict correlation between larger GSNR and better network convergence. To verify our theory, we conduct a toy experiment in NAS-Bench-201 on CIFAR-10. We randomly sample 800 architectures and train them for one epoch. As illustrated in Fig.1, GSNR is positively correlated with the generalization ratio and neg-atively correlated with the training loss. This result sug-gests that GSNR can effectively capture the generalization and convergence at network initialization.
In this paper, we further develop a novel Zero-Shot
NAS proxy called ξ-based gradient signal-to-noise ratio (ξ-GSNR), which introduces a fairly small ξ term to smooth the variance of the parameter gradient. The network per-formance is then predicted by ξ-GSNR proxy at network initialization without any training. A larger ξ-GSNR value indicates better network performance. We conduct exper-iments in various NAS-Benches and observe that the ξ-GSNR proxy exhibits significant ranking consistency with network performance, surpassing existing Zero-Shot NAS proxies. In addition, when directly searching for architec-tures in different search spaces, the accuracy has also been boosted due to the effectiveness of ξ-GSNR proxy.
To this end, we summarize our contributions as follows:
• We provide theoretical proof that gradient signal-to-noise ratio (GSNR), which is the ratio between the squared mean and variance of the parameter gradi-ent, is positively correlated with the generalization and convergence of the network at initialization.
• We develop a novel Zero-Shot NAS proxy called ξ-based gradient signal-to-noise ratio (ξ-GSNR) by in-troducing a fairly small ξ term to smooth the variance of the parameter gradient. ξ-GSNR is more accurate than vanilla GSNR in predicting network performance.
• Our experimental results indicate that ξ-GSNR proxy achieves significant ranking consistency with network accuracy in various NAS-Benches.
In addition, the performance of the searched architecture has also been improved when involving ξ-GSNR proxy during the searching procedure of NAS. 2.