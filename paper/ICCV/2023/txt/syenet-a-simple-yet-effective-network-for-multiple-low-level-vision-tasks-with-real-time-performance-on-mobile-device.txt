Abstract
With the rapid development of AI hardware accelerators, applying deep learning-based algorithms to solve various low-level vision tasks on mobile devices has gradually be-come possible. However, two main problems still need to be solved: task-speciﬁc algorithms make it difﬁcult to inte-grate them into a single neural network architecture, and large amounts of parameters make it difﬁcult to achieve real-time inference. To tackle these problems, we propose a novel network, SYENet, with only 6K parameters, to han-dle multiple low-level vision tasks on mobile devices in a real-time manner. The SYENet consists of two asymmet-rical branches with simple building blocks. To effectively connect the results by asymmetrical branches, a Quadratic
Connection Unit(QCU) is proposed. Furthermore, to im-prove performance, a new Outlier-Aware Loss is proposed to process the image. The proposed method proves its su-perior performance with the best PSNR as compared with other networks in real-time applications such as Image Sig-nal Processing(ISP), Low-Light Enhancement(LLE), and
Super-Resolution(SR) with 2K60FPS throughput on Qual-comm 8 Gen 1 mobile SoC(System-on-Chip). Particularly, for ISP task, SYENet got the highest score in MAI 2022
Learned Smartphone ISP challenge. 1.

Introduction
In recent years, with the thriving development of AI ac-celerators [54, 77], such as Neural Processor Units(NPUs) or Graphic Processor Units(GPUs), AI algorithms can be deployed on mobile devices and achieved great success
∗These authors contributed equally to this work.
†Corresponding author: Ke Xu(xu.kevin@sanechips.com.cn)
[65, 73, 94, 99]. Many mobile SoCs, especially those de-signed for smartphone, tablet, and in-vehicle infotainment systems, require superior visual quality processing, which cannot be achieved without leveraging deep networks such as ISP [43, 46], LLE [7], and SR [8, 11, 14]. However, due to the tight hardware constraints such as power and com-puting resources, deploying these algorithms on mobile de-vices still has several issues as follows.
The ﬁrst issue concerns real-time processing. Usually, these low-level vision tasks require a 2K60FPS or even higher real-time performance to satisfy the viewer’s needs.
Although the State-of-the-Arts(SOTAs) [8, 14, 46, 90] deal-ing with similar tasks have boosted the performance, they increased the numbers of parameters and computational cost drastically, which cannot satisfy real-time inference deployment even on powerful hardware such as server-level processors. Moreover, compared with high-level tasks
[94, 99], where the input images could be resized into a lower resolution such as 128 × 128 or 256 × 256 with-out noticeable effects, low-level vision tasks cannot do the same thing as their preliminary goal is to improve the hu-man visual quality. A more detailed discussion about the constraints of low-level vision tasks is in Appendix G.
The second issue is related to hardware resources on mobile devices such as Qualcomm’s Snapdragon. As compared with server-level Central Processing Unit(CPU) or GPU, mobile SoC usually has limited computing re-sources such as multiplication-and-accumulation units, lim-ited memory bandwidth, and limited power consumption budget. Unfortunately, most low-level vision algorithms are task-speciﬁc [7, 14, 46, 62] and independent to each other, which makes it difﬁcult to merge into a single architec-ture. To make things worse, many advanced operators, such as deformable convolution [106] and 3D-convolution [68], cannot be directly applied on mobile devices, which further
network architecture.
In this paper, we propose a new architecture SYENet, which can solve multiple low-level vision tasks with 2K60FPS on a mobile device such as Qualcomm’s 8 Gen 1.
We ﬁrst decompose the low-level vision into two sub-tasks, which are texture generation and pattern classiﬁcation. We then leverage two asymmetric branches to handle each task and a Quadratic Connection Unit(QCU) to connect the out-puts to enlarge the representational power. Furthermore, the network replaces ordinary convolution with revised re-parameterized convolution to boost the performance with-out increasing inference time, and Channel Attention(CA) is utilized for enhancement by global information. In ad-dition, we propose Outlier-Aware Loss by involving global information and putting more focus on the outliers of the prediction for improving the performance. The proposed network achieves SOTA performance, as compared with other methods on low-level tasks. The comprehensive per-formance evaluation of SR, LLE and ISP tasks are shown in
Table 1, 2, and 3, respectively.
The contributions of this paper can be summarized in three aspects: 1. We propose that asymmetric branches fused with
Quadratic Connections Unit(QCU) is an effective method for solving multiple low-level vision tasks due to its ability to enlarge the representation power with modicum parameter count. Building upon this struc-ture, we introduce SYENet, which incorporates re-vised reparameterized convolutions and channel atten-tion to enhance performance without sacriﬁcing speed. 2. A new loss function termed Outlier-Aware Loss is proposed for better training by leveraging global infor-mation and prioritizing outliers, the poorly predicted pixels. 3. Compared with other studies, our network has a supe-rior performance according to the evaluation metrics in MAI Challenge [40], which reﬂects both the image quality and efﬁciency as shown in Fig. 1. 2.