Abstract
The motivation of the semi-supervised domain adapta-tion (SSDA) is to train a model by leveraging knowledge acquired from the plentiful labeled source combined with extremely scarce labeled target data to achieve the low-est error on the unlabeled target data at the testing time.
However, due to inter-domain and intra-domain discrepan-cies, the improvement of classification accuracy is limited.
To solve these, we propose the Trico-training method that utilizes a multilayer perceptron (MLP) classifier and two graph convolutional network (GCN) classifiers called inter-view GCN and intra-view GCN classifiers. The first co-training strategy exploits a correlation between MLP and inter-view GCN classifiers to minimize the inter-domain dis-crepancy, in which the inter-view GCN classifier provides its pseudo labels to teach the MLP classifier, which en-courages class representation alignment across domains. In contrast, the MLP classifier gives feedback to the inter-view
GCN classifier by using a new concept, ‘pseudo-edge’, for neighbor’s feature aggregation. Doing this increases the data structure mining ability of the inter-view GCN clas-sifier; thus, the quality of generated pseudo labels is im-proved. The second co-training strategy between MLP and intra-view GCN is conducted in a similar way to reduce the intra-domain discrepancy by enhancing the correlation be-tween labeled and unlabeled target data. Due to an im-balance in classification accuracy between inter-view and intra-view GCN classifiers, we propose the third co-training strategy that encourages them to cooperate to address this problem. We verify the effectiveness of the proposed method on three standard SSDA benchmark datasets: Office-31,
Office-Home, and DomainNet. The extended experimental results show that our method surpasses the prior state-of-the-art approaches in SSDA. 1.

Introduction
Recently, semi-supervised domain adaptation (SSDA) task has received much attention because the the
*Corresponding author. target classification accuracy significantly increases thanks to a little labeled target data during training. However, it releases a new issue called intra-domain discrepancy presenting the difference between labeled and unlabeled target data within the target domain. Specifically, only the unlabeled target data having a strong correlation with the labeled target data is attracted for alignment, while the unlabeled target data having a less correlation with the labeled target data can be misaligned. Therefore, SSDA is still a challenging task because of existing inter-domain and intra-domain discrepancies, as represented in Figure 1.
The inter-domain discrepancy occurs due to the different data distribution between source and target domains called domain shift [27]. To alleviate inter-domain discrepancy, the previous works [1, 4, 24, 28, 34] rely much on the ad-versarial learning strategy. In contrast, to solve the intra-domain discrepancy, many approaches [10,11,15,19,25] in-crease the correlation between labeled and unlabeled target representations by using contrastive learning or clustering integrated with the pseudo-labeling strategy. For example,
CDAC [10] combines the pseudo labeling and clustering methods to enhance the relationship between labeled and unlabeled target data, while Con2DA [19] and CLDA [25] select a solution combining pseudo labeling and contrastive learning. However, the classification accuracy of these ap-proaches has opportunities for improvement because their multilayer perceptron (MLP) classifiers often misclassify the unlabeled target data since the inter-domain and intra-domain discrepancies still exist. This is because the quality and quantity of pseudo labels generated by these MLP clas-sifiers are still limited. Indeed, the MLP classifier only has the ability to exploit the semantic information of each indi-vidual image; thus, it can be failed to capture neighbor fea-tures for generalizing data structure for training. To solve this problem, we take advantage of the graph convolutional network (GCN) classifier for the neighbor feature aggrega-tion that effectively mines the data structure. To be specific, we use an inter-view GCN classifier to elaborately exploit label information on the rich source data that provides an inter-view observation on the unlabeled target data. Then,
small amount of labeled target data to minimize the clas-sification error on the unlabeled target data. MME [23] is the most popular method in SSDA using a minimax entropy strategy, in which the labeled source and target samples are integrated to estimate the prototypes. Then, the minimax entropy strategy is used to encourage the estimated pro-totypes toward the unlabeled target samples.
Inspired by this approach, UODA [20] and ASDA [21] introduce a new framework that trains multiple classifier models with dif-ferent minimax entropy strategies for explicit feature align-ment. However, the classification accuracy of these ap-proaches still has room to improve due to the biased pro-totype estimation and intra-domain discrepancy issues. The estimated prototypes are dominated by the rich information of the source data. The intra-domain discrepancy occurs within the target domain, which is firstly concerned and an-alyzed by APE [8], where only unlabeled target samples are aligned with the labeled target samples if they are located nearby these labeled target samples, while other unlabeled target samples located far from the labeled target samples can be misaligned. 2.2. Pseudo labeling on SSDA
Recently, the pseudo-labeling techniques [10, 11, 14–16, 19, 25, 32] have shown a remarkable ability to improve the target classification performance in the SSDA setting.
MAP-F [16], PAC [14], CDAC [10], and MCL [31] use the pseudo labeling and consistency regularization for self-training with a single classifier. Besides, Con2DA [19], and
CLDA [25] show outstanding classification performance on the SSDA task by using contrastive learning integrated with pseudo labeling. Furthermore, DECOTA [32] and MVCL
[15] significantly improve the target classification accuracy with a divide-to-conquer strategy, in which they split the
SSDA task into subtasks; then, they use different mod-els to handle different tasks. Finally, these models teach each other by exchanging their pseudo labels via the pro-posed co-training strategy. However, the quality and quan-tity of generated pseudo labels from these abovementioned approaches have an opportunity for improvement. That is because they use the multilayer perceptron (MLP) classi-fiers to extract pseudo labels, while the MLP classifier only exploits information of each individual image; thus, it can fail to explore the neighborhood structure. To solve this problem, we take advantage of the GCN classifier for the feature aggregation that effectively mines the data structure to increase the number of generated pseudo labels with high reliability.
Figure 1: Illustration of inter-domain and intra-domain dis-crepancies in the SSDA setting. this inter-view GCN classifier generates pseudo labels for supporting the MLP classifier to alleviate the inter-domain discrepancy. Similarly, we use an intra-view GCN classi-fier to mine data structure on the limited labeled target data, which offers an intra-view observation on the unlabeled tar-get data. Then, this intra-view GCN classifier also creates pseudo labels that guide the MLP classifier to mitigate the intra-domain discrepancy by enhancing the correlation be-tween labeled and unlabeled target samples. To increase the quality of pseudo labels generated from inter-view and intra-view GCN classifiers, we introduce a novel concept called ‘pseudo-edge’ created by the MLP classifier to train these GCN classifiers. The mutual interaction among two
GCN classifiers and the MLP classifier can be represented by two co-training strategies such as MLP and inter-view
GCN, and MLP and intra-view GCN.
However, the number of labeled source data is signifi-cantly larger than the labeled target data. Therefore, the im-balance of classification accuracy between inter-view GCN and intra-view GCN classifiers can occur. Finally, to solve this problem, we introduce the third co-training strategy, in which these two GCN classifiers teach each other by ex-changing their pseudo labels. We summarize the contribu-tions of this paper as follows:
• We propose a method Trico-training (TriCT) that in-cludes three co-training strategies to overcome the inter-and-intra-domain discrepancies and the imbal-ance classification accuracy issue in the SSDA task.
• We successfully cooperate between GCN and MLP classifier models with the pseudo labeling technique flexibly to boost the classification performance by in-troducing a novel concept named ‘pseudo-edge’.
• The experimental results of the proposed TriCT on three benchmark datasets, including Office-31, Office-Home, and DomainNet surpass the state-of-the-art ap-proaches. 2.