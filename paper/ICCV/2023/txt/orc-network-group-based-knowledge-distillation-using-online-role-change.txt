Abstract
In knowledge distillation, since a single, omnipo-tent teacher network cannot solve all problems, multiple teacher-based knowledge distillations have been studied re-cently. However, sometimes their improvements are not as good as expected because some immature teachers may transfer the false knowledge to the student. In this paper, to overcome this limitation and take the efficacy of the multiple networks, we divide the multiple networks into teacher and student groups, respectively. That is, the student group is a set of immature networks that require learning the teacher’s knowledge, while the teacher group consists of the selected networks that are capable of teaching successfully. We pro-pose our online role change strategy where the top-ranked networks in the student group are able to promote to the teacher group at every iteration. After training the teacher group using the error samples of the student group to re-fine the teacher group’s knowledge, we transfer the col-laborative knowledge from the teacher group to the stu-dent group successfully. We verify the superiority of the proposed method on CIFAR-10, CIFAR-100, and ImageNet which achieves high performance. We further show the gen-erality of our method with various backbone architectures such as ResNet, WRN, VGG, Mobilenet, and Shufflenet.1 1.

Introduction
Deep learning using convolutional neural networks (CNN) is making significant progress in computer vision tasks (e.g., object detection, classification, segmentation).
For making the efficient network, many trials have been studied from quantization [36, 3, 30, 19] to pruning [13, 23, 9, 10]. After Hinton’s proposal [16] on Knowledge Distil-lation (KD), KD methods have been proposed in various forms [31, 39, 34, 35, 38, 4, 6] but most of them lever-age a pair of a single large teacher and a small student for knowledge transfer. However, there is a clear limitation to improving the performance of the student successfully be-1Our code is available at https://github.com/choijunyong/ORCKD cause the teacher network is egocentric and complacent in spite of its good performance and note that the teacher is in-dependently trained in advance without the consideration of the student’s characteristics. Eventually, the teacher’s knowledge is transferred from the viewpoint the teacher has learned in advance, not the direction in which the student can learn successfully, and the student network easily over-fits in an undesired direction. Another limitation of KD is that when the difference of network sizes between the teacher and student networks is large, a single teacher-based
KD does not properly transfer the teacher’s knowledge to the student as described in [25, 33].
Recently, the multiple teacher-based KD methods [25, 33, 43, 8] have been proposed to solve the issues men-tioned above. Specifically, Teacher Assistant-based KD (TAKD) [25, 33] was proposed as a method of teaching stu-dents using multiple networks with different network sizes and online distillation methods [43, 22, 8] simultaneously learned the knowledge from the multiple networks of the similar capacity from the beginning. Most of all, the funda-mental problem of the multiple network-based KD is that, as shown in Fig. 1 (a), the networks are trained by using even knowledge from immature networks. As a result, the collaborative knowledge that should be used for learning can be contaminated by the false knowledge. On the other hand, we use multiple networks of different sizes to over-come the teacher-student large gap in online KD and divide the networks into a teacher group and a student group to pre-vent false knowledge from being used for learning as shown in Fig. 1 (b).
In this paper, we propose a method to prevent the trans-fer of false knowledge. False knowledge refers to flawed information present in teacher networks, and our objective is to prevent the student networks from learning in the wrong direction by avoiding the assimilation of this false knowledge. We first deploy the multiple networks into the teacher group and the student group according to their roles.
The main role of the teacher group is to educate the student network using collaborative knowledge, and the main role of the student group is to receive useful information from the teacher group and train optimally. What we need to
Figure 1. Network Group-based Knowledge Distillation. (a) Multiple networks are still suffering from the problem of transferring false knowledge from the immature network. (b) Our method divides multiple networks into a teacher group and a student group according to the performance and assigns different roles to them every iteration. In each iteration, the role of the network belonging to each group can be continuously changed according to its changed performance. know at this point is that group members’ role could be changed according to their current performances on-the-fly during training and we call it Online Role Change (ORC) strategy.
In detail, the highest-performing student in the student group is promoted to the teacher group based on its demonstrated teaching ability at every iteration. To prevent transferring false knowledge, ORC consists of three steps:
Intensive teaching, Private teaching, and Group teaching.
In intensive teaching, we create feedback samples for students’ incorrect predictions and train pivot teacher using them so that it can focus on intensively about their incorrect information. In order to narrow the gap with pivot teacher by further improving the teaching ability of promoted temporary teacher, private teaching is conducted by pivot teacher. the teacher
Through the previous two steps, group grows into a more complementary group and group teaching is conducted to guide the student group using their collaborative knowledge. Our major contributions are summarized as follows:
• We propose the novel multiple network-based KD us-ing ORC mechanism that effectively prevents false knowledge transfer by promoting the top-ranked stu-dent network to a temporary teacher model during training.
• We suggest three teaching methods such as intensive, private, and group teachings to achieve the successful
ORC for online KD.
• We promisingly show that the proposed method out-performs various well-known KD methods in CIFAR-10, CIFAR-100, and ImageNet. 2.