Abstract
We present accumulator-aware quantization (A2Q), a novel weight quantization method designed to train quan-tized neural networks (QNNs) to avoid overflow when using low-precision accumulators during inference. A2Q intro-duces a unique formulation inspired by weight normaliza-tion that constrains the ℓ1-norm of model weights according to accumulator bit width bounds that we derive. Thus, in training QNNs for low-precision accumulation, A2Q also inherently promotes unstructured weight sparsity to guar-antee overflow avoidance. We apply our method to deep learning-based computer vision tasks to show that A2Q can train QNNs for low-precision accumulators while main-taining model accuracy competitive with a floating-point baseline.
In our evaluations, we consider the impact of
A2Q on both general-purpose platforms and programmable hardware. However, we primarily target model deployment on FPGAs because they can be programmed to fully ex-ploit custom accumulator bit widths. Our experimentation shows accumulator bit width significantly impacts the re-source efficiency of FPGA-based accelerators. On average across our benchmarks, A2Q offers up to a 2.3x reduction in resource utilization over 32-bit accumulator counterparts with 99.2% of the floating-point model accuracy. 1.

Introduction
Quantization is the process of reducing the range and precision of the numerical representation of data. When applied to the weights and activations of neural networks, integer quantization reduces compute and memory require-ments, usually in exchange for minor reductions in model accuracy [14, 19, 20, 47]. During inference, most of the compute workload is concentrated in operators such as con-volutions and matrix multiplications, whose products are typically accumulated into 32-bit registers that we refer to as accumulators. It has been shown that reducing accumu-lators to 16 bits on CPUs and ASICs can increase inference throughput and bandwidth efficiency by up to 2x [9, 45], and reducing to 8 bits can improve energy efficiency by over 4x [32]. However, exploiting such an optimization is highly non-trivial as doing so incurs a high risk of overflow. Due to wraparound two’s complement arithmetic, this can intro-duce numerical errors that degrade model accuracy [32].
Previous works have sought to either reduce the risk of overflow [25, 37, 45] or mitigate its impact on model ac-curacy [32]. However, such approaches struggle to main-tain accuracy when overflow occurs too frequently [32], and are unable to support applications that require guaranteed arithmetic correctness, such as finite-precision fully homo-morphic encryption computations [28, 40]. Thus, we are motivated to avoid overflow altogether. As the first princi-pled approach to guarantee overflow avoidance, we provide theoretical motivation in Section 3, where we derive com-prehensive accumulator bit width bounds with finer gran-ularity than existing literature.
In Section 4, we present accumulator-aware quantization (A2Q); a novel method de-signed to train quantized neural networks (QNNs) to use low-precision accumulators during inference without any risk of overflow.
In Section 5, we show that our method not only prepares QNNs for low-precision accumulation, but also inherently increases the sparsity of the weights.
While our results have implications for general-purpose platforms such as CPUs and GPUs, we primarily target model deployment on custom FPGA-based inference ac-celerators. FPGAs allow bit-level control over every part of a low-precision inference accelerator and can therefore take advantage of custom data types to a greater extent than general-purpose platforms, which are often restricted to power-of-2 bit widths. In doing so, we show in Section 5 that reducing the bit width of the accumulator can in turn improve the overall trade-off between resource utilization and model accuracy for custom low-precision accelerators.
To the best of our knowledge, we are the first to explore the use of low-precision accumulators to improve the design efficiency of FPGA-based QNN inference accelerators. As such, we integrate A2Q into the open-source Brevitas quan-tization library [35] and FINN compiler [1] to demonstrate an end-to-end flow for training QNNs for low-precision ac-cumulation and generating custom streaming architectures targeted for AMD-Xilinx FPGAs.
2.