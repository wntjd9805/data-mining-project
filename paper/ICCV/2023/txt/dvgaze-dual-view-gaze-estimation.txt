Abstract
Gaze estimation methods estimate gaze from facial ap-pearance with a single camera. However, due to the limited view of a single camera, the captured facial appearance can-not provide complete facial information and thus complicate the gaze estimation problem. Recently, camera devices are rapidly updated. Dual cameras are affordable for users and have been integrated in many devices. This development suggests that we can further improve gaze estimation per-formance with dual-view gaze estimation. In this paper, we propose a dual-view gaze estimation network (DV-Gaze).
DV-Gaze estimates dual-view gaze directions from a pair of images. We first propose a dual-view interactive convolution (DIC) block in DV-Gaze. DIC blocks exchange dual-view information during convolution in multiple feature scales.
It fuses dual-view features along epipolar lines and com-pensates for the original feature with the fused feature. We further propose a dual-view transformer to estimate gaze from dual-view features. Camera poses are encoded to in-dicate the position information in the transformer. We also consider the geometric relation between dual-view gaze di-rections and propose a dual-view gaze consistency loss for
DV-Gaze. DV-Gaze achieves state-of-the-art performance on
ETH-XGaze and EVE datasets. Our experiments also prove the potential of dual-view gaze estimation. We release codes in https://github.com/yihuacheng/DVGaze. 1.

Introduction
Human gaze provides important cues for understanding human cognition [25] and behavior [9]. It has applications in various fields such as salience detection [32, 8, 31], virtual reality [23, 20, 18] and human-computer interaction [11, 7].
Gaze estimation methods estimate human gaze from fa-cial appearance. Conventional gaze estimation methods usu-ally learn person-specific eye models and fit the eye model
*Corresponding Author
This work was supported by National Natural Science Foundation of China (NSFC) under grant 62372019.
Figure 1. We explore dual-view gaze estimation in this work. We propose DV-Gaze to estimate gaze directions from a pair of images.
DV-Gaze contains dual-view interactive convolution blocks which exchange the information of a pair of images during convolution in multiple feature scales, and a transformer to estimate gaze from dual-view feature. to estimate human gaze. These model-based methods need to build specific camera system which contains multiple IR cameras and light sources [13]. Although model-based meth-ods have good accuracy, the complex camera system brings high costs and harms flexibility. Appearance-based gaze estimation methods have low requirements in devices. It only requires a single webcam to capture facial appearance and directly learns a mapping function from the appearance to gaze. The low requirement means appearance-based gaze estimation methods have larger potential than model-based methods. They attract much attention and become a hotspot.
However, the low requirement also brings limitations. In particular, one single webcam has a limited field of view and therefore captures incomplete facial appearance due to facial self-occlusion. The problem complicates gaze estima-tion and brings performance drop. To handle the problem, recent methods usually design efficient feature extraction networks[6, 29] or synthesize more images to cover data space[24, 30].
In this paper, we explore a new direction for gaze es-timation. Recently, camera devices are rapidly developed and the cost of camera is also decreased. Dual cameras are affordable for users [12] and have been applied in many
devices [26]. The developing dual-camera devices make it possible and meaningful to estimate the human gaze with dual cameras. Compared with a single camera, dual cameras provide a larger field of view. Dual-view images can also provide more cues for gaze estimation. These advantages indicate dual-view gaze estimation can further improve gaze estimation accuracy than conventional single-view methods.
We propose a dual-view gaze estimation network (DV-Gaze) in this work. Common solutions usually extracts dual-view features from dual-view images and concatenate the two features for gaze estimation. They only fuse dual-view information in a high level. Our idea is to exchange dual-view information anywhere. We first propose a dual-view interactive convolution (DIC) block. DIC blocks exchange dual-view information during convolution. The block first fuses dual-view features along epipolar lines and add the fused features back to original features for compensation. It then performs feature extraction separately on the two com-pensated feature maps with convolution layers. We stack multiple DIC blocks to exchange dual-view information in multiple feature scales during convolution. We further pro-pose a dual-view transformer. The transformer estimates dual-view gaze directions from dual-view features. We use camera pose to indicate the position information in the trans-former. We also consider the geometric relation between dual-view gaze directions and propose a dual-view gaze consistency loss function.
Overall, we summarize our contributions as following. 1. We explore dual-view gaze estimation in this work.
To the best of our knowledge, our work is the first to explore dual-view gaze direction estimation. 2. We propose DIC blocks for dual-view feature extraction.
The block fuses dual-view features along epipolar lines and use fused features to compensate original features.
Our method exchanges dual-view feature in multiple feature scales during convolution with DIC blocks. 3. We propose a dual-view transformer to estimate gaze from dual-view feature. We use camera pose to indi-cate the position information in the transformer. We also propose a dual-view gaze consistency loss which improves performance in a self-supervised manner. 2.