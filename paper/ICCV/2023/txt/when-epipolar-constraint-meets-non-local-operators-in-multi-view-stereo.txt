Abstract
Learning-based multi-view stereo (MVS) method heav-ily relies on feature matching, which requires distinctive and descriptive representations. An effective solution is to apply non-local feature aggregation, e.g., Transformer.
Albeit useful, these techniques introduce heavy computa-tion overheads for MVS. Each pixel densely attends to the
In contrast, we propose to constrain non-whole image. local feature augmentation within a pair of lines: each point only attends the corresponding pair of epipolar lines.
Our idea takes inspiration from the classic epipolar geom-etry, which shows that one point with different depth hy-potheses will be projected to the epipolar line on the other view. This constraint reduces the 2D search space into the epipolar line in stereo matching. Similarly, this sug-gests that the matching of MVS is to distinguish a series of points lying on the same line. Inspired by this point-to-line search, we devise a line-to-point non-local augmenta-tion strategy. We first devise an optimized searching algo-rithm to split the 2D feature maps into epipolar line pairs.
Then, an Epipolar Transformer (ET) performs non-local feature augmentation among epipolar line pairs. We incor-porate the ET into a learning-based MVS baseline, named
ET-MVSNet. ET-MVSNet achieves state-of-the-art recon-struction performance on both the DTU and Tanks-and-Temples benchmark with high efficiency. Code is available at https://github.com/TQTQliu/ET-MVSNet. 1.

Introduction
As a fundamental topic in computer vision, Multi-view stereo (MVS) aims to reconstruct dense 3D representations from multiple calibrated images given corresponding cam-era parameters. MVS can facilitate many applications such as automatic driving and augmented reality [12]. A criti-cal part of MVS is to match pixels to find the correspond-ing points. This matching process heavily relies on the fea-*Corresponding author
Figure 1. Our methods and results. (a) Inspired by classic epipo-lar geometry in stereo matching, we propose an efficient non-local feature aggregation paradigm that aggregates features on the cor-responding epipolar line pairs. (b) We instantiate the proposed non-local mechanism with a Transformer, termed Epipolar Trans-former (ET), which attains state-of-the-art performance with lim-ited computation overhead. ture representation: corresponding points in different views should be close in the embedding space. Traditional MVS approaches [11, 14, 27, 28] adopt hand-craft feature rep-resentation, which faces challenges on repetitive-pattern, weak-texture, and reflective regions.
To remedy the weak representation of the traditional learning-based methods are proposed
MVS approaches, and show superior performance. Learning-based MVS ap-proaches adopt deep neural networks to encode features, which forms the foundation for the subsequent feature matching and depth generation. Powerful as convolutional neural networks (CNNs) are, they still prefer to aggregate local context [22], which may be less accurate for matching.
One feasible solution is to adopt non-local feature augmen-tation strategy with large receptive field and flexible feature aggregation, which is crucial for robust and descriptive fea-ture representations.
To this end, techniques such as deformable convolu-tion [8], attention [2], and Transformers [29], are intro-duced to the feature encoding of MVS. Albeit useful, these general-purpose non-local operators introduce huge com-putation overhead for MVS, as each point need to densely attend the reference and source images. In this case, dis-tractive or irrelevant features can also be attended which are harmful to the feature matching, e.g., repetitive patterns shown in Fig. 7. This raises a fundamental question: where and how to mine non-local context for MVS? Inspired by the classic epipolar geometry, we propose to constrain the non-local feature aggregation within the epipolar lines, which enjoys both efficiency and descriptive representation.
According to the epipolar geometry theory, the actual point corresponding to a pixel in one view can only be pro-jected on the epipolar line in the other view. Hence, when searching the corresponding points, the 2D search space can be reduced into a 1D line. This reduces the computation and eliminates background interference. Naturally, each source image can form a stereo matching problem with the refer-ence image. As shown in Fig. 2, points with different depth hypotheses will be projected onto the epipolar line in the other image. Therefore, the feature encoding in MVS can be viewed as describing different points on the epipolar line.
Since the points to be distinguished lie on the same line, inspired by this epipolar constraint, we propose a line-to-point feature aggregation strategy. For each point, the in-formation from the corresponding epipolar line is used to describe it. A similar line-based feature aggregation has been adopted by CSwin [10] for vision backbones, which is proved to be efficient and effective. We find that for MVS, with epipolar geometry as a theoretical basis, using epipo-lar lines as the source of non-local features can efficiently achieve high-quality representation, as shown in Fig. 1 (b).
Specifically, we propose an epipolar line-guided non-local mechanism, which only fetches features from the cor-responding epipolar lines. First, we decompose the ref-erence and source feature map into pixel groups; each group shares identical epipolar line pairs. This process is processed with an optimized search algorithm. For each point in the reference image, we calculate its correspond-ing epipolar line in the source image. Then, points with approximate epipolar line parameters are clustered into the same group. With this step, all the pixels in the source and reference features can find their epipolar line pairs. Then, as shown in Fig. 1 (a), we perform non-local feature aggrega-tion within the source epipolar line and across the epipolar line pairs. We also add a local augmentation module to miti-gate the discontinuities of epipolar line partition. We instan-Figure 2. Epipolar Geometry. A pixel with different depth hypotheses in the reference image can be projected to the same epipolar line in the source image, termed point-to-line. The plane that passes through two camera centers and a 3-D point produces an epipolar line when intersecting with an image plane. The two epipolar lines of two images form an epipolar line pair, termed line-to-line. tiate this epipolar line-guided non-local mechanism with a
Transformer model termed Epipolar Transformer (ET).
To demonstrate the effectiveness of the proposed mod-ules, we present a coarse-to-fine framework termed ET-MVSNet, which applies the Epipolar Transformer (ET) to the feature encoding part of recent works [15, 31]. We con-duct extensive experiments to demonstrate the effect of our method. Thanks to Epipolar Transformer (ET), the depth estimation and 3-D representations have greatly improved and achieved state-of-the-art results on both the DTU [1] and the Tanks and Temple [19] benchmark. Besides, com-pared with the prevailing global attention that mines global context, Epipolar Transformer (ET) shows significantly bet-ter performance and efficiency.
Our main contributions can be summarized as follows:
• We propose an epipolar line feature aggregation strat-egy for the representation learning of MVS.
• We devise an optimized epipolar pair search algorithm, and an Epipolar Transformer (ET) to enable non-local feature augmentation on epipolar lines.
• Our method achieves state-of-the-art performance on both the DTU dataset and Tanks-and-Temples bench-mark. 2.