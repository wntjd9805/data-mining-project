Abstract
Semantic segmentation is a crucial task in computer vision that involves segmenting images into semantically meaningful regions at the pixel level. However, existing approaches often rely on expensive human annotations as supervision for model training, limiting their scalability to large, unlabeled datasets. To address this challenge, we present ZeroSeg, a novel method that leverages the exist-ing pretrained vision-language (VL) model (e.g. CLIP vi-sion encoder [39]) to train open-vocabulary zero-shot se-mantic segmentation models. Although acquired exten-sive knowledge of visual concepts, it is non-trivial to ex-ploit knowledge from these VL models to the task of se-mantic segmentation, as they are usually trained at an im-age level. ZeroSeg overcomes this by distilling the visual concepts learned by VL models into a set of segment to-kens, each summarizing a localized region of the target im-age. We evaluate ZeroSeg on multiple popular segmenta-tion benchmarks, including PASCAL VOC 2012, PASCAL
Context, and COCO, in a zero-shot manner Our approach achieves state-of-the-art performance when compared to other zero-shot segmentation methods under the same train-ing data, while also performing competitively compared to strongly supervised methods. Finally, we also demon-strated the effectiveness of ZeroSeg on open-vocabulary segmentation, through both human studies and qualitative visualizations. The code is publicly available at https:
//github.com/facebookresearch/ZeroSeg 1.

Introduction
Semantic segmentation involves dividing an image into distinct regions and assigning each area a corresponding
*Work done during an internship at Meta AI
†Equal mentorships
Figure 1. ZeroSeg overview. ZeroSeg is a zero-shot open-vocabulary method for semantic segmentation. The approach be-gins by dividing the input image into a set of multi-scale views.
Each view is then individually processed by a pretrained CLIP vi-sual encoder model to extract visual concepts. These visual con-cepts are then distilled into our ZeroSeg model via the proposed segment matching loss. After training, our ZeroSeg model can be directly transferred to downstream semantic segmentation tasks in a zero-shot manner (i.e., no training or adaption on target datasets). label, and the open-vocabulary setting targets performing segmentation with an unrestricted vocabulary. This process typically necessitates human-generated annotations, such as per-pixel label supervision [55, 19, 24, 40, 45, 53, 56, 11], or image-level supervision, e.g. human natural language
[20, 16, 48]. However, it can be time-consuming and ex-pensive to obtain these annotations, and thus the resulting model can not be trained on large amounts of data. Re-cently, new developments in the field of vision and lan-1
guage learning [39, 26, 1, 52, 8, 58] have emerged. Al-though some of these approaches have demonstrated im-pressive open-vocabulary image/object classification capa-bilities, their performance for open-vocabulary semantic segmentation has been less promising. Nonetheless, they provide a potential alternative solution to overcome the lim-itations of traditional supervised methods.
To improve the scalability of semantic segmentation for a large or open vocabulary, researchers have explored models that can learn directly from tens of millions of text sam-ples [20, 48, 16]. However, these vision-language (VL) models are prohibitively expensive to train and thus it is best to be able to exploit pretrained VL model weights (e.g.,
CLIP) for downstream segmentation tasks. However, to di-rectly adapt CLIP for per-pixel semantic segmentation is not trivial, since CLIP has only been trained using coarse-grained image-level supervision, even though it has learned extensive visual concepts.
Initial attempts have been made to also leverage pre-trained vision-language models for open-vocabulary se-mantic segmentation, such as those discussed in [50, 33].
However, these previous attempts primarily treated CLIP as a zero-shot segment-level classifier or as a visual backbone for the improved initialization. They usually still need to require expensive per-pixel level labels or extensive image-text pairs for the training. In contrast, our proposed method treats CLIP as a teacher model and distills its knowledge into our newly designed segmentation model, named Ze-roSeg, to facilitate semantic segmentation. This process en-ables the direct transfer of various learned visual concepts into ZeroSeg without the need for any dense pixel-leve su-pervision or other forms of human annotations, thereby nat-urally extending CLIP for open-vocabulary semantic seg-mentation.
One of the main challenges in using a large pretrained vision-language model for per-pixel level supervision is how to effectively group and categorize semantically con-sistent pixels. To tackle this problem, we have incorpo-rated a segments-grouping approach [48] into our ZeroSeg model. This approach automates the grouping of pixels into more significant, arbitrary-shaped segments. With these segments, it then becomes much easier to distill semantic information from the CLIP visual encoder to these localized image regions. As illustrated in Fig. 1, ZeroSeg divides the input image into multiple scaled regions and extracts their semantic features via the CLIP visual encoder. Each of those regional features will be distilled into a set of learn-able segment tokens both locally and globally. The visual segments will finally emerge to match the consistency with the different scales of semantic information from CLIP. Ad-ditionally, to improve the efficiency of training, our model also incorporates a masked autoencoder [21].
To assess the efficacy of our proposed model, we trained
ZeroSeg using only the ImageNet 1k dataset [13], with-out any human label supervision. Our findings reveal that our model is comparable in performance to those that were trained with human-label supervision. Specifically, we achieved a mean intersection over union (mIoU) of 40.8 on PASCAL VOC 2012 [18], a mIoU of 20.6 on PASCAL
Context [34], and a mIoU of 20.4 on the COCO dataset
[29] in a zero-shot manner. These results are compara-ble to models such as GroupViT [48] and MaskCLIP [16], which were pretrained on 26M and 20M image-text pairs, respectively, indicating the efficiency and effectiveness of our approach. Additionally, our model has performed well in a larger-vocabulary (1000 classes) semantic segmentation task. Our work is the first to enable open-vocabulary seman-tic segmentation by only distilling knowledge from the pre-trained CLIP vision encoder without using any pixel-level or text annotations.
Contributions. We make the following contributions:
• We introduce ZeroSeg, a model that enables efficient open-vocabulary semantic segmentation without rely-ing on any pixel-level or text annotations. By dis-tilling knowledge from a pretrained vision-language model, ZeroSeg bypasses the need for training on a large dataset of image-text pairs.
• We introduce segment matching loss and multi-scaled feature distillation loss, which are crucial for enabling open-vocabulary semantic segmentation from CLIP vision encoder distillation only.
• Despite being pretrained on only ImageNet-1k, which has almost 20 times fewer samples than the other baseline models trained on text supervision, ZeroSeg achieves comparable results. As a result, our model provides a significant increase in training efficiency without sacrificing performance. 2.