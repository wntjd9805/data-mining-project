Abstract
Reinterpretable cameras are defined by their post-processing capabilities that exceed traditional imaging. We present “SoDaCam” that provides reinterpretable cam-eras at the granularity of photons, from photon-cubes ac-quired by single-photon devices. Photon-cubes represent the spatio-temporal detections of photons as a sequence of binary frames, at frame-rates as high as 100 kHz. We show that simple transformations of the photon-cube, or photon-cube projections, provide the functionality of nu-merous imaging systems including: exposure bracketing, flutter shutter cameras, video compressive systems, event cameras, and even cameras that move during exposure.
Our photon-cube projections offer the flexibility of being software-defined constructs that are only limited by what is computable, and shot-noise. We exploit this flexibility to provide new capabilities for the emulated cameras. As an added benefit, our projections provide camera-dependent compression of photon-cubes, which we demonstrate using an implementation of our projections on a novel compute architecture that is designed for single-photon imaging. 1.

Introduction
Throughout the history of imaging, sensing technologies and the corresponding processing have developed hand-in-hand. In fact, sensing technologies have, to some extent, de-fined the scope of processing captured data. In the film era, instances of such processing included dodging and burning.
The advent of digital cameras provided processing at the granularity of pixels and paved the way for modern com-puter vision. Light field cameras [34, 78], by sampling the plenoptic function [2], allowed post-capture processing at
∗Corresponding author: Varun Sundar. This research was supported in parts by NSF CAREER award 1943149, NSF award CNS-2107060, and the Swiss National Science Foundation grant 200021 166289. We also thank Paul Mos for providing access to SwissSPAD2 acquisition software. the granularity of light rays, enabling novel functionalities such as refocusing photos after-capture. The logical limit of post-capture processing, given the fundamental quanti-zation of light, would be at the level of individual photons.
What would imaging look like if we could perform compu-tational processing on individual photons?
In this work, we show that photon data captured by a new class of single-photon detectors, called single-photon avalanche diodes (SPADs), makes it possible to emulate a wide range of imaging modalities such as exposure brack-eting [12], video compressive systems [38, 55] and event cameras [52, 60]. A user then has the flexibility to choose one (or even multiple) of these functionalities post-capture (Fig. 1 (top)). SPAD arrays can operate as extremely high
∼100 kHz), producing a tem-frame-rate photon detectors ( poral sequence of binary frames called a photon-cube [16].
We show that computing photon-cube projections, which are simple linear and shift operations, can reinterpret the photon-cube to achieve novel post-capture imaging func-tionalities in a software-defined manner (Fig. 1 (middle)).
As case studies, we emulate three distinct imagers: high-speed video compressive imaging; event cameras which re-spond to dynamic scene content; and motion projections which emulate sensor motion, without any real camera movement. Fig. 1 (bottom) shows the outputs of these cam-eras that are derived from the same photon-cube.
Computing photon-cube projections. One way to ob-tain photon-cube projections is to read the entire photon-cube off the SPAD array and then perform relevant compu-tations off-chip; we adopt this strategy for our experiments in Secs. 6.1 and 6.2. While reasonable for certain applica-tions, reading out photon-cubes requires an exorbitant data-bandwidth, which can be up to 100 Gbps for a 1 MPixel array—well beyond the capacity of existing data peripher-als. Such readout considerations will become center stage as large-format SPAD arrays are fabricated [48, 49].
An alternative is to avoid transferring the entire photon-Figure 1: (top) SoDaCam can emulate a variety of cameras from the photon-cubes acquired by single-photon devices. (middle) Photon-cubes represent the spatio-temporal detection of photons as a sequence of binary frames. Projections of the photon-cube, when computed either on or off-chip, result in reinterpretable and software-defined cameras. We demonstrate the versatility of photon-cube projections on a real dynamic scene: a die falls on a table, bounces, spins in the air and later ricochets off a nearby toy top. (bottom) The cameras emulated by our photon-cube projections can produce: a 12
× high-speed video from a single compressive snapshot, event-stream representations of two time intervals (blue and red depict positive and negative spikes respectively), an image where the die appears stationary, as well as a motion-deblurred image. cube by computing projections near sensor. As a proof-of-concept, we implement photon-cube projections on Ultra-Phase [5], a recently-developed programmable SPAD im-ager with independent processing cores that have dedicated
RAM and instruction memory. We show, in Sec. 6.3, that computing projections on-chip greatly reduces sensor read-out and, as a consequence, power consumption.
Implications: Toward a photon-level software-defined camera. The photon-cube projections introduced in this paper are computational constructs that provide a real-ization of software-defined cameras or SoDaCam. Be-ing software-defined, SoDaCam can emulate multiple cam-eras simultaneously without additional hardware complex-ity. SoDaCam, by going beyond baked in hardware choices,
unlocks hitherto unseen capabilities—such as 2000 FPS video from 25 Hz readout (Fig. 7); event imaging in very low-light conditions (Fig. 9); and motion stacks, which are a stack of images where in each image, objects only in cer-tain velocity ranges appear sharp (Fig. 6).
×
Limitations. The SPAD array [67] used in this work has a relatively low spatial resolution (512 256), and a low
∼10%) owing to the lack of microlenses in the fill-factor ( prototype used. Similarly, the near-sensor processor that we use has limited capabilities compared to off-chip pro-cessors. However, with rapid progress in the development of single-photon cameras [48, 49] and increasing interest in near-sensor processors, we anticipate that many of these shortcomings will be addressed in the upcoming years. 2.