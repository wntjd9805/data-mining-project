Abstract
Most video compression methods aim to improve the de-coded video visual quality, instead of particularly guar-anteeing the semantic-completeness, which deteriorates downstream video analysis tasks, e.g., action recogni-tion.
In this paper, we focus on a novel unsupervised video semantic compression problem, where video seman-tics is compressed in a downstream task-agnostic man-ner. To tackle this problem, we first propose a Semantic-Mining-then-Compensation (SMC) framework to enhance the plain video codec with powerful semantic coding ca-pability. Then, we optimize the framework with only un-labeled video data, by masking out a proportion of the compressed video and reconstructing the masked regions of the original video, which is inspired by recent masked im-age modeling (MIM) methods. Although the MIM scheme learns generalizable semantic features, its inner generative learning paradigm may also facilitate the coding frame-work memorizing non-semantic information with extra bit costs. To suppress this deficiency, we explicitly decrease the non-semantic information entropy of the decoded video fea-tures, by formulating it as a parametrized Gaussian Mixture
Model conditioned on the mined video semantics. Com-prehensive experimental results demonstrate the proposed approach shows remarkable superiority over previous tra-ditional, learnable, and perceptual quality-oriented video codecs, on three video analysis tasks and seven datasets. 1.

Introduction
Video compression has been actively researched over the past few decades. Most methods, including traditional methods [102][14] and learnable ones [83][61], aim at im-proving the reconstructed video quality for human percep-tion, rather than particularly preserving AI task-required semantic information, e.g., human key-points and object shapes. This degrades downstream AI tasks [135][104].
To tackle this problem, lots of research efforts have been devoted to a new problem of Video Coding for
Machine (VCM), i.e., lossy video compression for sup-porting downstream AI
For example, early tasks. works [142][23][8] and standards [46][44][45] additionally transport the manually-designed image descriptors, sup-porting limited tasks with undesirable performance. Later, (cid:66) Corresponding Author some methods [86][53][33] improve the traditional codec with hand-crafted designs to better cope with the specific tasks, e.g., saliency-aware bit allocation for object detection task [53]. Meanwhile, some methods [34][29][28][100][51] compress the feature maps of AI models instead of the im-ages, where the tail task modules shall adapt to the fea-tures by supervised learning. Besides, most scalable cod-ing methods [79][133][126][35][36][103] are optimized by supervised learning or task-relevant feature matching loss functions. Despite these efforts, rare methods are task-agnostic while still exploiting data-driven semantics. Task-agnostic decouples the coding system from downstream tasks and is friendly to data scarcity scenarios. Data-driven objective prompts the system to learn a more generalizable semantic representation than hand-crafted designs.
In this paper, we focus on a novel Unsupervised Video
Semantic Compression task that satisfies the two above re-quirements, where compressed videos readily support vari-ous analysis tasks. Considering plain video codecs are al-ready of powerful visual coding capability, it is natural to build the semantic compression framework upon them for inheriting the advantages. However, it is non-trivial to deal with the semantic information lost during the compression, especially without the guidance of task-specific data labels.
To address these issues, we first propose a simple yet effective Semantic-Mining-then-Compensation (SMC) framework as a baseline method, to improve current plain video codecs with better semantic coding capability.
Specifically, the semantic feature of the original video and the lossy video is extracted by neural networks on the en-coder side, and only the residual part is transported. On the decoder side, the lossy video and its semantics com-pensated by the residual, together synthesize the final de-coded video. As for the self-supervised optimization of the framework, we mask out a large proportion of the decoded video patches, and use the unmasked parts to reconstruct the masked regions of the original video, inspired by re-cent Masked Image Modeling (MIM) methods [56][113].
The reconstruction task facilitates decoded videos to be of rich semantics in terms of both intra-patch appearance and inter-patch interactions. Although the MIM scheme learns generalizable semantic features, its inner generative learn-ing paradigm also facilitates the coding framework mem-orizing non-semantic information [41], which makes the extracted semantic features contain redundant information,
consuming extra bitcost. To suppress this deficiency, we ex-plicitly decrease the non-semantic information entropy of the video, by formulating it as a parameterized Gaussian
Mixture Model conditioned on the mined video semantics.
The alternative semantic learning and non-semantic sup-pressing procedures make the system bootstrapping itself toward more efficient semantic coding. As a result, it shows remarkable performance on a wide range of tasks without leveraging any data labels.
Contributions:
• We focus on a novel unsupervised video semantic compression problem, proposing a concise yet effec-tive baseline framework dubbed SMC.
• Our work is the first one that applies Masked Image
Modeling (MIM) scheme to semantic coding problem, aiming to learn a semantic representation that is appli-cable to various downstream tasks.
• We propose the Non-Semantics Suppressed (NSS) learning strategy to better adapt the general MIM scheme to the compression problem, suppressing the framework from encoding non-semantic information.
• Our approach demonstrates notable superiority over previous traditional, learnable and perceptual codecs, on three video analysis tasks and seven datasets. 2.