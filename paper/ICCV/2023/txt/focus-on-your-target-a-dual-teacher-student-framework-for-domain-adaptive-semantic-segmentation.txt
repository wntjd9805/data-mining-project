Abstract
We study unsupervised domain adaptation (UDA) for se-mantic segmentation. Currently, a popular UDA framework lies in self-training which endows the model with two-fold abilities: (i) learning reliable semantics from the labeled images in the source domain, and (ii) adapting to the target domain via generating pseudo labels on the unlabeled im-ages. We find that, by decreasing/increasing the proportion of training samples from the target domain, the ‘learning ability’ is strengthened/weakened while the ‘adapting abil-ity’ goes in the opposite direction, implying a conflict be-tween these two abilities, especially for a single model. To alleviate the issue, we propose a novel dual teacher-student (DTS) framework and equip it with a bidirectional learn-ing strategy. By increasing the proportion of target-domain data, the second teacher-student model learns to ‘Focus on Your Target’ while the first model is not affected. DTS is easily plugged into existing self-training approaches.
In a standard UDA scenario (training on synthetic, la-beled data and real, unlabeled data), DTS shows consis-tent gains over the baselines and sets new state-of-the-art results of 76.5% and 75.1% mIoUs on GTAv→Cityscapes and SYNTHIA→Cityscapes, respectively. The implementa-tion is available at https://github.com/xinyuehuo/DTS. 1.

Introduction
Deep neural networks [28] have been proven effective in learning from labeled, in-domain visual data, but their abil-ity to adapt to unlabeled data or unknown domains is often not guaranteed. We delve into this topic by studying the un-supervised domain adaptation (UDA) problem for semantic segmentation, where densely (pixel-wise) labeled data are available for the source domain but only unlabeled images are provided for the target domain. The setting is useful
*Corresponding Author.
Figure 1. Segmentation results on a test image from Cityscapes (the model is trained on GTAv). There are two traffic signs with different styles, where Target A is similar to the training data in the source domain while Target B is not. The baseline (DAFormer) gradually improves the accuracy on Target A but achieves the best accuracy on Target B in the midst (at 12K iterations). Our method,
DTS, achieves good accuracy for both targets and the trend is more stable throughout the training process (see the curves). in real-world applications where new domains emerge fre-quently but annotating them is a major burden.
The state-of-the-art UDA approaches for semantic seg-mentation are mostly built upon a self-training framework.
It first learns semantics from labeled data in the source do-main and then generates pseudo labels in the target domain, so that the model adapts to the target domain by fitting the pseudo labels. Conceptually, the two steps of self-training endow the model with two-fold abilities, namely, learning knowledge (semantics) from the source domain and adapt-ing the knowledge to the target domain.
However, these two abilities can conflict with each other.
Figure 1 shows an example of two targets of the same class (traffic sign), where Target A is close to the typical train-ing samples in the source domain, while Target B has a quite different appearance. Using the default setting of
DAFormer [19] (a UDA baseline), the recognition accu-racy of Target A gradually increases while that of Target
B grows first but eventually drops to 0. Interestingly, if one increases the proportion of training samples from the target domain, the recognition quality of Target B will be better while the recognition accuracy of some unexpected classes (e.g., sidewalk) may drop significantly (more details and examples can be found in the Appendix). The conflict sets an obstacle to achieving higher UDA accuracy.
The above analysis prompts us to design a dual teacher-student (DTS) framework. The key is to train two individual models (with different network weights) so that the afore-mentioned conflict is alleviated. DTS can be built upon most existing self-training approaches in three steps: (1) making two copies of the original teacher-student model, (2) increasing the proportion of training data from the tar-get domain in the second group, and (3) applying a bidirec-tional learning strategy so that two models supervise each other via pseudo labels. DTS allows the system to pursue a stronger adapting ability (with the second model) mean-while the learning ability (guaranteed by the first model) is mostly unaffected.
We conduct semantic segmentation experiments with the standard UDA setting. Two synthetic datasets with dense labels, GTAv [38] and SYNTHIA [39], are used as the source domains, and a real dataset with only images,
Cityscapes [6], is used as the target domain. We establish the DTS framework upon three recent self-training base-lines, namely, DAFormer [19], HRDA [20], and MIC [21].
DTS improves the segmentation accuracy in every single trial, demonstrating its generalized ability across baselines and datasets.
In particular, when integrated with MIC, the prior state-of-the-art, DTS reports 76.5% and 67.8% mIoUs on GTAv→Cityscapes and SYNTHIA→Cityscapes, respectively, setting new records for both benchmarks. 2.