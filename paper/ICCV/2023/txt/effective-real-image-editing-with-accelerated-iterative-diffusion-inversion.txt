Abstract
Despite all recent progress, it is still challenging to edit and manipulate natural images with modern generative models.
When using Generative Adversarial Network (GAN), one major hurdle is in the inversion process mapping a real im-age to its corresponding noise vector in the latent space, since it is necessary to be able to reconstruct an image to edit its contents. Likewise for Denoising Diffusion Implicit
Models (DDIM), the linearization assumption in each inver-sion step makes the whole deterministic inversion process unreliable. Existing approaches that have tackled the prob-lem of inversion stability often incur in significant trade-offs in computational efficiency. In this work we propose an
Accelerated Iterative Diffusion Inversion method, dubbed
AIDI, that significantly improves reconstruction accuracy with minimal additional overhead in space and time com-plexity. By using a novel blended guidance technique, we show that effective results can be obtained on a large range of image editing tasks without large classifier-free guidance in inversion. Furthermore, when compared with other diffu-sion inversion based works, our proposed process is shown to be more robust for fast image editing in the 10 and 20 diffusion steps’ regimes. 1.

Introduction
Diffusion models are a class of generative models that learn to generate high-quality images by iteratively applying a de-noising process from a random noisy starting point. They are capable to achieve state-of-the-art (SOTA) image qual-ity since the very early stage of denoising diffusion prob-abilistic models (DDPM) [21] and score-based generative modeling [42]. While the number of sampling steps re-quired for high quality image generation was initially very large, several follow-up studies [34, 7, 29, 30] have reduced significantly the number of steps without degrading the im-age quality, making the widespread use of diffusion models possible. In particular, denoising diffusion implicit models (DDIM) [41] are widely used for their speed and flexibility in deterministic and stochastic generations. Further reduc-ing the number of sampling steps for both image generation or editing is still nevertheless an open research problem.
Diffusion models were initially designed for image gen-eration; for this reason, their usefulness for real image edit-ing is limited without a proper inversion process, similar to the inversion challenge [46] faced by real image editing us-ing a Generative Adversarial Network (GAN) [17]. GAN inversion is limited by the reduced dimensionality in latent space; diffusion inversion is comparably less restricted as the latent space has the same dimensionality as the origi-nal image. Na¨ıve one-step inversion step, i.e. simply per-turbing an image with random noise, was initially used for early image editing works such as SDEdit [31] and Blended
Diffusion [6]. Later, an Euler method based inversion pro-cess that applies deterministic step-by-step noise injection was used for image-to-image translation in DDIB [43] and
DiffusionCLIP [27]. However, as shown in the text-guided image editing tests in Prompt-to-Prompt (PTP) [19], such inversion is not reliable for real image editing because the inversion often leads to failed reconstruction even when no editing is performed. Follow-up works like null-text inver-sion (NTI) [32] and exact diffusion inversion (EDICT) [45] have focused on improving the reconstruction accuracy by introducing auxiliary variables like the learned null-text em-bedding in NTI, or processes like the coupled diffusion pro-cess in EDICT. The reconstruction accuracy improvements come however with regressions in computational complex-ity of the inversion and/or the editing processes.
In this paper, we are the first to look beyond the simple inversion process that is based on the Euler method and in-vestigate a better numerical solver for improved inversion stability. Modeling the inversion process as an ordinary dif-ferential equation (ODE) problem, the implicit backward
Euler method is well suited as its solution results in an ex-act reconstruction using Euler’s method, assuming the same time steps are used. Given that, we propose an Acceler-ated Iterative Diffusion Iteration (AIDI) method that adopts a fixed-point iteration process for each inversion step. Com-bined with Anderson acceleration [5] method which helps convergence of this iteration, it is demonstrated through ex-periments of large test set that it results in significantly im-per image while editing could be applied multiple times, the latency time is split to editing and inversion, represented as inner circle and outer ring respectively. For latency times, ours is only slower in inversion than the original PTP while as fast as PTP and NTI in editing.
In summary, based on pretrained text-to-image diffusion models, we propose a framework for text-based real image editing with the following key advantages:
• We are the first to our knowledge to apply fixed-point iteration and acceleration in diffusion inversion, show-ing significant improvements in reconstruction accu-racy based on a large 5000 image COCO test set. The
LPIPS value for a 20-step reconstruction is reduced to 0.063 compared to 0.148 for the baseline.
• For inversion without classifier-free guidance or low guidance scale, we propose a blended guidance method to apply larger guidance scales for effective editing in relevant areas, while maintaining fidelity elsewhere with low scales.
• Our proposed image editing method is still effective for inversion steps as low 10, where competing ap-proaches exhibit significant artifacts. 2.