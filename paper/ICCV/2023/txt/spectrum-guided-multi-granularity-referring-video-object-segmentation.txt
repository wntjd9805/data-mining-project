Abstract (cid:3031) (a) Previous: decode-and-segment w/ decoded features (cid:1832)(cid:3049)(cid:3039)
Current referring video object segmentation (R-VOS) techniques extract conditional kernels from encoded (low-resolution) vision-language features to segment the decoded high-resolution features. We discovered that this causes sig-niﬁcant feature drift, which the segmentation kernels strug-gle to perceive during the forward computation. This nega-tively affects the ability of segmentation kernels. To address the drift problem, we propose a Spectrum-guided Multi-granularity (SgMg) approach, which performs direct seg-mentation on the encoded features and employs visual de-tails to further optimize the masks.
In addition, we pro-pose Spectrum-guided Cross-modal Fusion (SCF) to per-form intra-frame global interactions in the spectral domain for effective multimodal representation. Finally, we ex-tend SgMg to perform multi-object R-VOS, a new paradigm that enables simultaneous segmentation of multiple referred objects in a video. This not only makes R-VOS faster, but also more practical. Extensive experiments show that
SgMg achieves state-of-the-art performance on four video benchmark datasets, outperforming the nearest competitor by 2.8% points on Ref-YouTube-VOS. Our extended SgMg enables multi-object R-VOS, runs about 3× faster while maintaining satisfactory performance. Code is available at https://github.com/bo-miao/SgMg. 1.

Introduction
Referring video object segmentation (R-VOS) aims at segmenting objects in a video, referred to by linguistic de-scriptions. R-VOS is an emerging task for multimodal rea-soning and promotes a wide range of applications, including language-guided video editing and human-machine inter-action. Different from conventional semi-supervised video object segmentation [44, 7, 41], where the mask annotation for the ﬁrst frame is provided for reference, R-VOS is more challenging due to the need for cross-modal understanding between vision and free-form language expressions.
Early R-VOS techniques [2, 21, 61] perform feature en-coding, cross-modal interaction, and language grounding
Decode t-SNE Visualization (cid:1832)(cid:3049)(cid:3039) (cid:3031) (cid:1832)(cid:3049)(cid:3039)
Feature Drift (cid:3031) (cid:1832)(cid:3049)(cid:3039)
Upsampling
Fusion (cid:1837)(cid:3030) (cid:1832)(cid:3049)(cid:3039) (cid:1843) a turtle to the bottom left (b) SgMg (Ours): segment-and-optimize w/ encoded features (cid:1832)(cid:3049)(cid:3039)
Fusion (cid:1837)(cid:3030)(cid:3043) (cid:1832)(cid:3049)(cid:3039) (cid:1843)
Multi-Granularity 
Optimization
Conditional Segmentation a turtle to the bottom left (cid:1839)(cid:3018) (cid:1839)(cid:3018)
Fusion: Cross-modal Fusion      (cid:1832)(cid:3049)(cid:3039): Encoded Vision-Language Features      (cid:1832)(cid:3049)(cid:3039)
Q: Instance Query     (cid:1837)(cid:3030): Conditional Kernel     (cid:1837)(cid:3030)(cid:3043): Conditional Patch Kernel     (cid:1839)(cid:3018): Predicted Mask (cid:3031) : Decoded Features 
Figure 1. (a) Previous methods [4, 57] apply segmentation ker-nels Kc [50], extracted from encoded features Fvl, to segment the decoded high-resolution features F d vl. (b) We use segmentation kernels Kcp, extracted from encoded features Fvl, to segment the encoded features Fvl directly, and propose multi-granularity opti-mization to recover visual details and produce ﬁne-grained masks. using convolutional neural networks (CNNs). However, the limited ability of CNNs to capture long-range depen-dencies and handle free-form features constrains the model performance. With the advancement of attention mecha-nisms [52, 43, 15, 16, 58], recent methods achieved signiﬁ-cant improvement on R-VOS using cross-attention [49, 23, 27] for multimodal understanding and transformers [6, 56] for spatio-temporal representation. Based on transformers, conditional kernel [50] is then introduced to separate fore-ground from semantic features given its high adaptability to different instances [4, 57]. As illustrated in Fig. 1(a), these methods attend to encoded vision-language features
Fvl using instance queries Q to predict conditional kernels
Kc, and employ Kc as the segmentation head to segment decoded features F d vl. Despite the promising performance,
this paradigm still has some limitations. Firstly, as shown in the t-SNE [51] visualization in Fig. 1(a), although the nonlinear decoding process introduces visual details, this is accompanied by a signiﬁcant feature drift, which increases the difﬁculty of segmentation since Kc is predicted before feature decoding. Secondly, bilinear upsampling of the pre-dicted masks MQ to increase resolution impedes the seg-mentation performance. Thirdly, these methods only sup-port single expression-based segmentation, making R-VOS inefﬁcient when multiple referred objects exist in a video.
In this work, we propose a Spectrum-guided Multi-granularity (SgMg) approach that follows a segment-and-optimize pipeline to address the above problems. As de-picted in Fig. 1(b), SgMg introduces Conditional Patch Ker-nel (CPK) Kcp to directly segment its fully perceived en-coded features Fvl, avoiding the feature drift and its adverse effects. The segmentation is then reﬁned using our pro-posed Multi-granularity Segmentation Optimizer (MSO), which employs low-level visual details to produce full-resolution masks. Within the SgMg framework, we further develop Spectrum-guided Cross-modal Fusion (SCF) that performs intra-frame global interactions in the spectral do-main to facilitate multimodal understanding. Finally, we introduce a new paradigm called multi-object R-VOS to si-multaneously segment multiple referred objects in a video.
To achieve this, we extend SgMg by devising multi-instance fusion and decoupling. Our main contributions are summa-rized as follows:
• We explain how existing R-VOS methods suffer from the feature drift problem. To address this problem, we propose SgMg that follows a segment-and-optimize pipeline and achieves top-ranked overall performance on multiple benchmark datasets.
• We propose Spectrum-guided Cross-modal Fusion to encourage intra-frame global interactions in the spec-tral domain.
• We extend SgMg to perform multi-object R-VOS, a new paradigm that enables simultaneous segmentation of multiple referred objects in a video. Our multi-object variant is more practical and runs 3× faster.
We conduct extensive experiments on multiple bench-including Ref-YouTube-VOS [49], Ref-mark datasets,
DAVIS17 [21], A2D-Sentences and JHMDB-Sentences [20], and achieve state-of-the-art performance on all four. On the largest validation set Ref-YouTube-VOS,
SgMg achieves 65.7 J &F which is 2.8% points higher than that of the closest competitor ReferFormer [57]. On the
A2D-Sentences, SgMg achieves 58.5 mAP which is 3.5% points higher than that of ReferFormer.
[17], 2.