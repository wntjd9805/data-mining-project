Abstract
Self-supervised learning has attracted a lot of attention recently, which is able to learn powerful representations without any manual annotations. However, self-supervised learning needs to develop the ability to continuously learn to cope with a variety of real-world challenges, i.e., Con-tinual Self-Supervised Learning (CSSL). Catastrophic for-getting is a notorious problem in CSSL, where the model tends to forget the learned knowledge.
In practice, sim-ple rehearsal or regularization will bring extra negative ef-fects while alleviating catastrophic forgetting in CSSL, e.g., overﬁtting on the rehearsal samples or hindering the model from encoding fresh information. In order to address catas-trophic forgetting without overﬁtting on the rehearsal sam-ples, we propose Augmentation Stability Rehearsal (ASR) in this paper, which selects the most representative and dis-criminative samples by estimating the augmentation stabil-ity for rehearsal. Meanwhile, we design a matching strategy for ASR to dynamically update the rehearsal buffer. In addi-tion, we further propose Contrastive Continuity on Augmen-tation Stability Rehearsal (C2ASR) based on ASR. We show that C2ASR is an upper bound of the Information Bottleneck (IB) principle, which suggests that C2ASR essentially pre-serves as much information shared among seen task streams as possible to prevent catastrophic forgetting and dismisses the redundant information between previous task streams and current task stream to free up the ability to encode fresh information. Our method obtains a great achievement com-pared with state-of-the-art CSSL methods on a variety of
CSSL benchmarks. 1.

Introduction
Recently, self-supervised learning (SSL) has received much attention from the community due to its great po-∗Corresponding authors. tential [11, 21, 18, 5, 12, 49]. Self-supervised learning is able to learn beneﬁcial representations for a variety of downstream tasks without any manual annotations, where contrastive learning based on dual branch framework is the mainstream, as shown in Figure 1(a). However, data is often presented as streams progressively over time in real-world scenarios.
It’s nearly infeasible for self-supervised learn-ing to collect the whole data streams to train the networks, since the ever-increasing data makes the notoriously costly training of self-supervised learning models even more ex-pensive and sometimes it can’t even access previous data due to privacy protection. Self-supervised learning needs to develop continuity to cope with a variety of real-world challenges, which is also called Continual Self-Supervised
Learning (CSSL) [16], as shown in Figure 1(c).
Continual learning (CL) aims to learn from non-stationary data distributions, as shown in Figure 1(b).
Catastrophic forgetting is a notorious problem in CL, which refers to that the model tends to forget what it has already learned. Many methods [20, 40, 47, 39, 25, 50, 32, 1, 4] are proposed to alleviate it. CSSL also suffers from catastrophic forgetting, and some pioneers start to address this problem.
Rehearsal-based method LUMP [33] utilizes rehearsal sam-ples to augment current task samples by mixup [51], and regularization-based method CaSSLe [16] encourages cur-rent model to maintain a consistency with previous state via a prediction head. However, LUMP which is based on dark experience sampling strategy for rehearsal tends to overﬁt on the rehearsal samples due to the long training epoches of self-supervised learning [16], and CaSSLe introduces too much invariance among task streams, which preserves most information of previous task streams and hinders the model from learning fresh knowledge.
In order to address catastrophic forgetting without over-ﬁtting on the rehearsal samples, we propose Augmentation
Stability Rehearsal (ASR) in this paper, which selects the most representative and discriminative samples by estimat-design a rehearsal selection strategy based on the augmenta-tion stability, i.e., selecting the samples with especially high score (located at the center of the category distribution) and low score (located at the boundary of the category distribu-tion) from the augmentation stability distribution to ﬁll the rehearsal buffer. Meanwhile, since the traditional queue and stack update strategy cannot satisfy the requirement that re-taining the most representative and discriminative samples for the rehearsal buffer, we develop a matching strategy for
ASR to dynamically update the rehearsal buffer.
Generally, the continual self-supervised model needs to encode the information of previous task streams to over-come catastrophic forgetting, as well as the information of current task to be of the ability to continuously learn. How-ever, the whole information of previous task streams is not only redundant for preventing catastrophic forgetting [24], but also hinders the model from encoding fresh informa-In order to balance the prevention of catastrophic tion. forgetting and the ability to continuously learn, we further propose Contrastive Continuity on Augmentation Stability
Rehearsal (C2ASR) based on ASR, which aims to encour-age the feature distribution of current model on rehearsal samples to be consistent with previous states to prevent catastrophic forgetting and the feature distribution of cur-rent model on current task samples to be inconsistent with previous states to free up the ability to continuously learn.
In addition, we show that the proposed C2ASR is an upper bound of the Information Bottleneck (IB) principle [42, 41], which suggests that C2ASR essentially preserves as much information shared among seen task streams as possible to prevent catastrophic forgetting and dismisses the redundant information between previous task streams and current task stream to free up the ability to encode fresh information.
Finally, we introduce the augmentation invariance and sym-metrization strategy [18, 12] into C2ASR to further increase the diversity and stability of contrastive continuity pairs.
We validate the effectiveness of our method on sev-eral popular CSSL benchmarks, e.g., the average accuracy and average forgetting on Split CIFAR-10, Split CIFAR-100 and Split Tiny-ImageNet and the average accuracy on out of distribution (OOD) datasets. Our method achieves the best performance on most evaluation metrics compared with state-of-the-art CSSL methods. 2.