Abstract
Learning a policy with great generalization to unseen environments remains challenging but critical in visual re-inforcement learning. Despite the success of augmenta-tion combination in the supervised learning generalization, naively applying it to visual RL algorithms may damage the training efficiency, suffering from serve performance degradation.
In this paper, we first conduct qualitative analysis and illuminate the main causes: (i) high-variance gradient magnitudes and (ii) gradient conflicts existed in various augmentation methods. To alleviate these issues, we propose a general policy gradient optimization frame-work, named Conflict-aware Gradient Agreement Augmen-tation (CG2A), and better integrate augmentation combina-tion into visual RL algorithms to address the generalization bias. In particular, CG2A develops a Gradient Agreement
Solver to adaptively balance the varying gradient magni-tudes, and introduces a Soft Gradient Surgery strategy to al-leviate the gradient conflicts. Extensive experiments demon-strate that CG2A significantly improves the generalization performance and sample efficiency of visual RL algorithms. 1.

Introduction
With the development of deep learning in various tasks [28, 26, 25, 27, 7, 6, 38, 40, 39, 24], visual Rein-forcement Learning (RL) has achieved impressive success in various fields such as robotic control [11], autonomous driving [17], and game-playing [35]. Previous works usu-ally formulate it as a Partially Observable Markov Deci-sion Process (POMDP) [33], and the agent receives high-dimensional image observations as inputs. As depicted in [15, 14], visual RL generalization refers to the ability
*Corresponding author of a pretrained RL agent to perform well in unseen environ-ments. Due to the dynamic nature of the real world, even minor perturbations in the environment can result in signifi-cant semantic shifts in the visual observations, which makes visual RL generalization challenging.
To improve generalization performance, data augmenta-tion [29] is a widely adopted technique in reinforcement learning. Numerous studies [22, 13] utilize data augmen-tation methods to generate synthetic data and diversify the training environments, yielding considerable performance improvements. However, recent methods [14, 3, 44] mostly select a single augmentation technique to improve the gen-eralization capability, resulting in a poor performance in the environments with observations varying far from the aug-mented images. For instance, ColorJitter [23] is the pre-ferred choice for addressing color variations, but agents trained with such augmentation still hard to cope with in-tricate texture patterns. In other words, the generalization ability heavily relies on the selection of specific data aug-mentation technique, which is so-called generalization bias.
Compared to single data augmentation, Augmentation
Combination (AC) [16] integrates multiple data augmenta-tion methods to enhance the diversity of augmentations and alleviate the generalization bias, which is a more promising pre-processing solution. Unfortunately, there is a dilemma in incorporating AC into visual RL. Although data augmen-tation combination can effectively improve generalization capability in the supervised visual tasks, RL algorithms are quite sensitive to excessive variations, resulting in perfor-mance degradation and training sample inefficiency. There-fore, it is necessary to rethink why visual RL algorithms cannot benefit from AC as much as supervised learning.
From the perspective of gradient optimization, we con-duct numerous qualitative analysis to illustrate the causes of performance degradation and training collapse that occur when employing augmentation combinations during train-ing. There are two primary reasons for this phenomenon: (i)
the utilization of diverse data augmentations leads to high gradient magnitude variations, resulting in biased general-ization; (ii) the gradient conflicts1 [42] existed cross multi-ple augmentation methods hinder the policy optimization.
To balance the gradients with high-variance magnitudes, one effective approach is to customize the weights of the loss terms with manually defined hyper-parameters [14].
However, hyper-parameter tuning relies heavily on expert knowledge, which can be inflexible and computationally expensive when dealing with multiple data augmentations.
Besides, [30] indicates that the widely employed average-based gradient update strategies tend to converge towards the speed-greedy direction, and are ill-posed to effectively handle complex gradient conflicts, leading to local optima and a decrease in sampling efficiency.
Specifically,
To address these issues, we propose a general policy gra-dient optimization framework, named Conflict-aware Gra-dient Agreement Augmentation (CG2A), to integrate aug-mentation combination into the RL framework and im-prove its generalization performance. the
CG2A contains two key components: an adaptive weight assigner called Gradient Agreement Solver (GAS) and a conflict-aware gradient update strategy Soft Gradient
Surgery (SGS). To effectively harmonize high-variance magnitudes gradients, we formulate the hyper-parameter tune as a second-order multi-objective optimization prob-lem and use the GAS to obtain a proximal approximate solution with minimal computational cost. Moreover, ac-cording to [30], although gradient conflicts slow down con-vergence speed, these conflicting gradient components may contain more semantic-irrelevant information that can im-prove invariant learning consistency. Motivated by this hy-pothesis, we propose SGS to improve the gradient update process, which preserves a small amount of conflicting gra-dient components to strike a balance between convergence speed and generalization performance. To validate the ef-fectiveness of CG2A, we conduct extensive experiments on DMControl Generalization Benchmark (DMC-GB) and some robotic manipulation tasks. In summary, our contri-bution encompasses three main manifolds:
• We point out the generalization bias induced by single data augmentation and illustrate the primary causes for performance degradation when naively applying aug-mentation combination in RL algorithms.
• We propose a general policy gradient optimization framework named Conflict-aware Gradient Agreement
Augmentation (CG2A), to efficiently integrate data augmentation combinations into the RL algorithms and significantly improve the generalization perfor-mance in various environments. 1The gradient conflicts mean the gradient directions point away from each other, e.g., appears a negative cosine similarity.
• We devise a Gradient Agreement Solver (GAS) to har-monize multiple gradients with high-variance magni-tudes, and propose a Soft Gradient Surgery (SGS) strategy to alleviate the gradient conflicts existed in various data augmentations.
• Compared to previous state-of-the-art methods, CG2A achieves competitive generalization performance and significantly improves sample efficiency. 2.