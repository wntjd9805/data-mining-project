Abstract
Computer graphics (CG) rendering platforms produce imagery with ever-increasing photo realism. The narrow-ing domain gap between real and synthetic imagery makes it possible to use CG images as training data for deep learn-ing models targeting high-level computer vision tasks, such as autonomous driving and semantic segmentation. CG im-ages, however, are currently not suitable for low-level vision tasks targeting RAW sensor images. This is because RAW images are encoded in sensor-speciﬁc color spaces and in-cur pre-white-balance color casts caused by the sensor’s response to scene illumination. CG images are rendered di-rectly to a device-independent perceptual color space with-out needing white balancing. As a result, it is necessary to apply a mapping procedure to close the domain gap be-tween graphics and RAW images. To this end, we intro-duce a framework to process graphics images to mimic RAW sensor images accurately. Our approach allows a one-to-many mapping, where a single graphics image can be trans-formed to match multiple sensors and multiple scene illumi-nations. In addition, our approach requires only a handful of example RAW-DNG ﬁles from the target sensor as param-eters for the mapping process. We compare our method to alternative strategies and show that our approach produces more realistic RAW images and provides better results on three low-level vision tasks: RAW denoising, illumination estimation, and neural rendering for night photography. Fi-nally, as part of this work, we provide a dataset of 292 real-istic CG images for training low-light imaging models. 1.

Introduction
Access to large datasets of high-quality images remains a critical factor for training deep learning models. For many computer vision tasks, data acquisition is time-consuming, expensive, and error prone. This is particularly true for
*Equal contribution.
†Work done while with the Samsung AI Center Toronto.
‡Work done while an intern at the Samsung AI Center Toronto.
Figure 1. Images (A) and (B) started as RAW-DNG images, one from a Nikon, the other from a Samsung camera. RAW images are shown as insets. Both DNGs have been rendered by Adobe Pho-toshop to sRGB based on the DNG’s metadata. One is a computer graphics image converted to a RAW-DNG using our method; the other is a real sensor RAW image. Can you tell which is CG and which is real? See Sec. 3.3. camera engineers who often require sensor-speciﬁc train-ing data. Any time a new sensor is introduced on a smart-phone or other camera platform, training images speciﬁc to the new sensor must be captured. Computer-generated (CG) imagery has long been viewed as a promising solu-tion to the data acquisition bottleneck. The main challenge with graphics imagery is the domain gap between synthetic and real data. Fortunately, the visual gap is rapidly closing thanks to the improved realism of graphics engines.
CG imagery has been successfully used to augment or even replace real training data for applications such as au-tonomous driving [46, 42], semantic segmentation [16, 44], and facial landmark detection [51]. There are now compa-nies dedicated to generating CG images for training com-puter vision AI models [2, 4, 7]. Graphics platforms, such as Unreal [6], Unity [5], and Blender [1], typically render images directly to a device-independent display-referred color space—namely, standard RGB (sRGB). Moreover, computer graphics are implicitly white-balanced during rendering. Cameras also process RAW sensor images to sRGB. However, unlike graphics, a RAW image represents the sensor’s direct response to physical radiance from the scene. RAW images are in a sensor-speciﬁc color space deﬁned by the spectral sensitivities of the sensor’s color ﬁl-ters. In addition, RAW images exhibit a strong color cast
due to scene illumination. Cameras have dedicated image signal processors (ISPs) that apply a series of operations to convert the RAW sensor image to the ﬁnal sRGB im-age [20]. Among the ISP operations is a white-balance step to remove the color cast due to the scene illumination and a color space transformation step to convert the RAW image from its sensor-speciﬁc color space to the device-independent sRGB color space. For low-level vision tasks, such as illuminant estimation, that train DNN-models di-rectly on RAW images, graphics images cannot be directly used. As a result, a mapping procedure explicitly targeting the conversion of graphics to RAW is needed.
Closely related to the graphics to RAW problem is the topic of RAW reconstruction from camera-rendered sRGB images. These methods aim to recover the RAW sensor im-age from the camera’s output sRGB image. Data-driven methods, such as Cycle ISP [55] and Invertible ISP [52], suffer from the same data acquisition bottleneck we seek to avoid in that they require a large dataset of paired RAW-sRGB images from the target sensor for training. Metadata-assisted RAW reconstruction methods [53, 40, 39] assume that samples from the RAW image are available to recon-struct the RAW image and, therefore, cannot be applied to our task. The method of [10] is the closest to our work.
This approach inverts ISP operations step-by-step to con-vert camera-rendered sRGB images back to plausible RAW images to be used for training image denoisers. However,
[10] does not strive for color accuracy which is critical for other tasks such as illuminant estimation or color rendering.
Graphics images have several advantages over camera-rendered images. Camera sRGB images may have limited bit depth and resolution, and contain demosaicing or ring-ing artifacts, residual noise, and blur due to the ISP’s RAW image processing. Properly rendered graphics images do not suffer from these issues and can produce signiﬁcantly higher-quality training data. Furthermore, data diversity and concerns over privacy are less of an issue with CG.
Contributions We provide a description outlining how graphics images are directly rendered to sRGB and how cameras process RAW images to sRGB. Based on this in-sight, we present a framework that converts CG images to appear as RAW images captured by a target sensor under varied illuminations. In contrast to existing DNN methods that require large datasets of RAW-sRGB pairs for training, our approach requires only a handful of RAW-DNG ﬁles from the target sensor. We use the metadata in the DNG
ﬁles to sample the illumination space of the target sensor and map the graphics images to a RAW color space that ac-curately mimics the target sensor. We can save our synthetic
RAW images with the appropriate metadata back to DNG
ﬁles so that they can be rendered by any standard DNG reader, such as Photoshop (see Fig. 1). We compare our results to competing methods and demonstrate the useful-ness of our approach to DNN-based RAW image denois-ing, illumination estimation, and neural rendering. Finally, we provide a dataset of 292 nighttime graphics images for training neural ISPs targeting night photography based on synthetic RAW images. To our knowledge, this is the ﬁrst work to target RAW image synthesis from CG imagery. 2.