Abstract
We propose an agglomerative Transformer (AGER) that enables Transformer-based human-object interaction (HOI) detectors to flexibly exploit extra instance-level cues in a single-stage and end-to-end manner for the first time.
AGER acquires instance tokens by dynamically clustering patch tokens and aligning cluster centers to instances with textual guidance, thus enjoying two benefits: 1) Integral-ity: each instance token is encouraged to contain all dis-criminative feature regions of an instance, which demon-strates a significant improvement in the extraction of dif-ferent instance-level cues and subsequently leads to a new state-of-the-art performance of HOI detection with 36.75 mAP on HICO-Det. 2) Efficiency: the dynamical clus-tering mechanism allows AGER to generate instance to-kens jointly with the feature learning of the Transformer encoder, eliminating the need of an additional object de-tector or instance decoder in prior methods, thus allow-ing the extraction of desirable extra cues for HOI de-tection in a single-stage and end-to-end pipeline. Con-cretely, AGER reduces GFLOPs by 8.5% and improves FPS by 36%, even compared to a vanilla DETR-like pipeline without extra cue extraction. The code will be available at https://github.com/six6607/AGER.git. 1.

Introduction
Human-object interaction (HOI) detection aims at under-standing human activities at a fine-grained level. It involves both the localization of interacted human-object pairs and the recognition of their interactions, where the latter poses the major challenges as a higher-level vision task [9].
Since interactions describe the relations between dif-ferent instances (i.e., humans and objects), instance-level cues (e.g., human pose and gaze) are unanimously rec-ognized as pivotal to discriminating subtle visual differ-ences between similar relation patterns in interaction recog-nition. However, extracting these instance-level cues intu-itively indicates a multi-stage pipeline, where an off-the-shelf object detector is essential to generate instance pro-posals firstly [11, 44, 23, 7, 50, 58]. Such a paradigm struggles in proposal generation, yielding less competitive
Figure 1: Instance queries vs. instance tokens. Instance queries typically attend to instance parts, while our instance tokens are encouraged to contain integral discriminative regions of instances.
More examples are presented in supplementary materials. performance in model efficiency.
In this work, we seek to explore a single-stage Transformer-based HOI detector to flexibly and efficiently exploit extra instance-level cues, thus continuing their success in HOI detection.
The challenge stems from task-bias, i.e., different tasks have different preferences of discriminative feature re-gions [61]. Gaze tracking, for example, prefers the discrim-inative regions of human heads [40], whereas pose estima-tion favours holistic human body contexts [22]. Therefore, the crux of building a single-stage pipeline lies in a proper design of information carrier, which need to ensure the in-tegrality of instance-level representations (IRs), i.e., con-taining all discriminative regions of an instance to satisfy the diverse region preferences of different tasks. However, most popular Transformer-based detectors deal with local patches, neglecting the integrality of different instances.
Some previous methods partially tackled the above chal-lenge. STIP [58] employs an additional DETR detec-tor to generate instance proposals, which yet suffers from the low efficiency of the multistage pipeline.
Several works [27, 3, 18] propose to use an additional query-based instance decoder to extract instance queries individually.
Despite being ingenious, these queries are task-driven and
In this short paper, we present AGER, above-mentioned challenges learned to highlight only the most distinguishable feature regions preferred by a given task, as verified by the spar-sity of learned attention map [65]. As shown in Fig. 1, the object detection driven human queries in existing methods typically contain only instance extremities, which likewise fails to guarantee the integrality of IRs, limiting its adapt-ability to other tasks (e.g., pose estimation) due to task bias (Sec. 4.2). Although joint multitask learning can partially alleviate the sparsity of instance queries, it introduces unex-pected ambiguities and makes the model fitting harder [52]. for
AGglomerative TransformeER, a new framework that improves prior methods by proposing instance tokens, favorably. handling the
Specifically, we formulate tokenization as a text-guided dynamic clustering process, which progressively agglom-erates semantic-related patch tokens (i.e., belonging to the same instance) to enable the emergence of instance tokens through feature learning. Being decoupled from down-stream tasks, the clustering mechanism encourages instance tokens to ensure the integrality of extracted IRs (Fig. 1) and eliminate task bias, thus allowing a flexible extraction of different instance-level cues for HOI detection. Despite being conceptually simple, instance tokens have some striking impacts. Unlike instance proposals being regular rectangles, the instance tokens are generated over irregu-larly shaped regions that are aligned to different instances with arbitrary shapes (Fig. 1), thus being more expressive.
With this, AGER already outperforms QPIC [38] by 10.6% mAP even without involving any extra cues (Sec. 4.3).
Additionally, compared to instance queries, instance tokens demonstrate a significant precision improvement in cue extraction (Fig. 3), leading to a new state-of-the-art perfor-mance of HOI detection on HICO-Det [2] with 36.75 mAP.
Of particular interest, the dynamical clustering mechanism can be seamlessly integrated with Transformer encoder, dispensing with additional object detectors or instance decoders and showing an impressive efficiency. Concretely, taking as input an image with size of 640 Ã— 640, AGER reduces GFLOPs by 8.5% and improves FPS by 36.0% even compared to QPIC that built on an vanilla DETR-like
Transformer pipeline (Sec. 4.3), and the relative efficiency gaps become more evident as the image resolution grows (Fig 3c). 2.