Abstract
Out-of-distribution (OOD) detection is a desired ability to ensure the reliability and safety of intelligent systems. A scoring function is often designed to measure the degree of any new data being an OOD sample. While most designed scoring functions are based on a single source of informa-tion (e.g., the classifier’s output, logits, or feature vector), recent studies demonstrate that fusion of multiple sources may help better detect OOD data. In this study, after de-tailed analysis of the issue in OOD detection by the con-ventional principal component analysis (PCA), we propose fusing a simple regularized PCA-based reconstruction er-ror with other source of scoring function to further improve
OOD detection performance. In particular, when combined with a strong energy score-based OOD method, the reg-ularized reconstruction error helps achieve new state-of-the-art OOD detection results on multiple standard bench-marks. The code is available at https://github.com/SYSU-MIA-GROUP/pca-based-out-of-distribution-detection. 1.

Introduction
Out-of-distribution (OOD) detection refers to the ability of a model to correctly detect samples that are from a differ-ent distribution compared to that of the in-distribution (ID) training data [13, 35]. It is a crucial ability to ensure the reliability and safety of machine learning systems. For in-stance, in autonomous driving, the system should be able to detect previously unseen scenarios or objects and alert the human operator when it cannot make a safe decision. How-ever, current intelligent systems are often limited in OOD detection.
Multiple approaches have been proposed to improve model’s OOD detection ability [4, 9, 21, 26, 34]. The main
*Authors contributed equally
†Corresponding author
Figure 1. The OOD detection performance (AUROC) on two benchmarks from our method and representative strong baselines.
The ImageNet-1K Benchmark includes the ID set ImageNet-1K and four OOD sets iNaturalist, SUN, Places, and Textures. The
CIFAR-100 Benchmark includes the ID set CIFAR-100 and seven
OOD sets SVHN, Tiny-ImageNet, iSUN, LSUN-Crop, LSUN-Resize, Textures and Places365. objective is to design or derive a scoring function whose output is different between OOD input and ID input. For deep learning classifier models, the scoring function is of-ten designed based on three sources of information, i.e., the output probability of the classifier [7, 8, 15, 38], the logit (i.e., input to the softmax function) at the last classifier layer [6, 10, 16, 31], and the feature vector output from the feature extractor part of the classifier [14, 18, 22, 29]. How-ever, one source of information is often inherently limited.
For example, softmax output and logit often discard some of the information in the feature vector even the discarded in-formation might be helpful for OOD detection, while scor-ing function solely based the feature vector cannot utilize the class-associated weight parameters at the last classifier layer. Based on this observation, one recent study starts to explore combination of multiple sources for further im-proving OOD detection performance [29, 40]. Specifically,
certain discarded information from the feature vector is re-used to generate an extra logit, which is then input to the softmax together with the original logits to obtain the final
OOD score. Such multi-source fusion has achieved state-of-the-art performance on some benchmarks [29].
In this study, following the multi-source fusion idea, we propose a simple yet effective OOD score based on in-depth analysis of the effect of the conventional principal compo-nent analysis (PCA) on OOD detection. Starting from the observation that the PCA-based reconstruction error for fea-ture vectors is often inseparable between ID and OOD data, we provide a detailed theoretical analysis and find that sta-tistically the larger variance in components of feature vec-tors on the ID data is probably the main source of such inseparability. Based on this analysis, a regularized PCA-based reconstruction error is applied to improve the separa-bility between ID and OOD data. Such regularized recon-struction error is post-hoc (i.e., working on any well-trained classifier) and can be flexibly combined with existing OOD detection methods such that multiple sources of informa-tion can be utilized together to better detect OOD data.
When fusing this regularized error with the energy score-based OOD method, new state-of-the-art performance was obtained on multiple standard benchmarks. 2. Preliminaries 2.1. Out-of-distribution detection
For a supervised classification task, denote by Din the training set of C classes which are randomly sampled from the in-distribution Din of the C classes. The goal of OOD detection is to train a classifier G (e.g., a convolutional neu-ral network) which can not only correctly classify any test data sampled from the in-distribution Din, but also detect whether a test data is from the in-distribution Din or from a different and unknown distribution Dout. A decision func-tion F built on the classifier G needs to be designed to help the classifier detect any potential data x from the out-of-distribution Dout, and ideally
F (x; G) = (cid:40) 1 0 if x ∼ Din if x ∼ Dout
. (1) 2.2. OOD detection with energy score
Suppose the classifier G is a convolutional neural net-work which consists of multiple convolutional layers and a fully connected layer, with softmax output as the final clas-sification prediction. Given a test sample x, denote by fc(x) the c-th output of the final linear layer, i.e., the c-th logit corresponding to class c. One type of decision function F is based on the energy score E,
E(x; G) = − log
C (cid:88) c=1 exp(fc(x)) , (2)
In particular, Liu et al. [16] proposed the negative energy score −E(x; G) as the score measurement to detect OOD examples, with a higher score −E(x; G) suggesting that x is more likely from the in-distribution Din and lower score suggesting that x is more likely from the out-of-distribution
Dout. 3. Method
In this section, we first analyze the reason why the re-construction error based on conventional PCA is not help-ful for OOD detection (Section 3.1), and then use a simple normalized PCA-based feature reconstruction error which can be combined with existing OOD scores (mainly energy score) to further improve the separability between ID and
OOD data. 3.1. Issues in PCA-based OOD detection
Suppose a neural network classifier has been well trained with ID data. Intuitively, when applying PCA to the feature vector output from the well-trained feature extractor, one would expect that the reconstruction error is smaller for ID data while larger for OOD data, and thus the PCA-based re-construction error would help separate ID from OOD data.
However, it is observed that such expectation is not true sta-tistically. In the following, we will reformulate the PCA-based reconstruction error and analyze why it is not work-ing in separating ID from OOD data.
Denote by h(x) ∈ RK the feature vector from the penul-timate layer of the well-trained neural network classifier for any input data x. The covariance matrix Σ of feature vec-tors h(x)’s over all the ID training data can be obtained and then decomposed to Σ = UΛU⊺, where U ∈ RK×K is a unitary matrix and constituted by the principal compo-nent directions, with each principal component correspond-ing to one eigenvector of the covariance matrix Σ, and
Λ ∈ RK×K is the diagonal matrix with the diagonal en-tries corresponding to the eigenvalues of Σ in descending order. Since ID data often lie in a subspace of the original feature vector space, the projection of ID data into the sub-space and then reconstruction to the original feature space would lose little information compared to the original fea-ture vector for any ID data. Suppose the subspace is k-dimensional and its base is from the first k principal com-ponents which constitutes the first k columns of U, denoted by Uk ∈ RK×k. k can be automatically determined as for traditional dimensionality reduction by PCA, e.g., choosing the k such that the first k eigenvalues can account for 95% of the variance in distribution of feature vectors for all the
ID training data. Denote by µ the mean feature vector over all ID training data in the original feature vector space, and k ∈ RK×K. Then any original feature vector
M = UkU h(x) can be projected to the subspace and then projected
⊺
Figure 2. Distributions of the reconstruction error on the ID dataset (blue) and each of the four OOD datasets (orange). The dimension of the original feature space and the subspace is respectively 2048 and 256.
Figure 4. Distributions of the angle θ between the centralized fea-ture vector h(x) − µ and its reconstruction M(h(x) − µ) on the
ID dataset ImageNet-1K (blue) and on the OOD dataset iNatural-ist (first subfigure, orange), SUN (second, orange), Places (third, orange), and Textures (fourth, orange), respectively. Model with backbone ResNet50 is trained on the ID dataset ImageNet-1K here and below by default.
Figure 3. The geometry relationship between the centralized fea-ture vector h(x)−µ and the reconstructed centralized feature vec-tor M(h(x) − µ). The reconstructed error is the magnitude of the difference between the two vectors.
Figure 5. Distributions of the magnitude ∥h(x) − µ∥ of the cen-tralized feature vector on the ID dataset (blue) and each of the four
OOD datasets (orange). back to the original feature space as below,
ˆh(x) = M(h(x) − µ) + µ , (3) and the reconstruction error e(x) can be directly obtained as e(x) = ∥h(x) − ˆh(x)∥
= ∥(I − M)(h(x) − µ)∥. (4) (5)
Although it is expected that e(x) would be smaller for ID data and larger for OOD data, surprisingly we observed that the distribution of the reconstruction error on the ID dataset is often not separable from the distribution of the recon-struction error on the OOD dataset (Figure 2). To explore the underlying cause to such inseparability between ID and
OOD data, we reformulate the reconstruction error formula (Eq. 5) based on the geometry relationship between the cen-tralized feature vector h(x) − µ and the reconstructed cen-tralized feature vector M(h(x)−µ). As Figure 3 illustrates, the reconstructed centralized feature vector M(h(x) − µ) coincide with the projection of the centralized feature vector h(x) − µ to the subspace (Uk-space) spanned by the first k principal components (see proof in Supplementary Sec-tion B), and the reconstruction error e(x) is actually the L2 norm of the difference between h(x)−µ and M(h(x)−µ).
Denote by θ(x) the angle between the two vectors h(x)−µ and M(h(x) − µ), then the reconstruction error can be re-formulated as e(x) = ∥h(x) − µ∥ ·
∥(I − M)(h(x) − µ)∥
∥h(x) − µ∥
= ∥h(x) − µ∥ · sin θ . (6) (7)
Since relatively large variations in ID signals are already in the subspace, the variations in ID signals along any direc-tion orthogonal to the subspace would be relatively small.
Figure 6. Positively skewed distribution of each zi on both ID dataset ImageNet-1K (first row) and an OOD dataset iNaturalist (second row). Four zi’s are uniformly sampled for demonstra-tion. Model with backbone ResNet50 is trained on the ID dataset
ImageNet-1K.
In other words, the angle θ is expected to be relatively small for ID data. In contrast, OOD signals are not utilized to de-termine the first k principal components and therefore some of large variations in OOD signals may not be in the sub-space, and the angle θ may be relatively large at least for some OOD data. This has been confirmed on all the four
OOD datasets as shown in Figure 4.
From Eq. (7) and the observations in Figures 2 and 4, it can be inferred that the magnitude ∥h(x) − µ∥ of the centralized feature vector h(x) − µ should be statistically larger for ID data while smaller for OOD data. Again, this is confirmed by the empirical observation on the four OOD datasets as demonstrated in Figure 5. In the following, fur-ther exploration is performed to search for the source of such difference in magnitude ∥h(x) − µ∥ between ID and
OOD data. 3.1.1 Variance analysis
For ease of analysis, let zi(x) denote the i-th component of h(x), µin,i denote the i-th component of µ, and define v(x) = ∥h(x) − µ∥2. Then the expectation of v(x) on the
ID dataset Din and an OOD dataset Dout can be respectively
calculated by
E x∈Din
[v(x)] =
E x∈Dout
[v(x)] =
K (cid:88) i=1
K (cid:88) i=1
[(zi(x) − µin,i)2]
E x∈Din
E x∈Dout
[(zi(x) − µin,i)2] (8) (9)
Figure 7. Histograms of the initial variance estimates of feature vector components respectively on the ID dataset ImageNet-1k (blue) and on each of the four OOD dataset iNaturalist, SUN,
Places, and Textures (orange).
=
K (cid:88) i=1 (cid:8) E x∈Dout
[(zi(x) − µout,i)2] + ∆µi (cid:9) , where µout,i is the average of the i-th component of feature vector output over all the OOD data in Dout, and ∆µi = (µout,i − µin,i)2.
If the distribution of each zi (i.e., zi(x), omitting x for simplicity below) is normal, the variance of zi and conse-quently the expectation of v(x) on either ID or OOD dataset can be unbiasedly estimated based on the collection of ID or
OOD data. However, largely due to the ReLU activation op-erator (only keeping non-negative signals) which is adopted in most CNN models, the distribution of each zi is often positively skewed on both ID and OOD dataset, as demon-strated in Figure 6. In this case, the initially estimated vari-ance of each zi needs to be modified by considering the non-symmetrical property of each zi’s distribution. Following the previous studies [17, 22], the epsilon-skew-norm (ESN) distribution is used to approximate the positively skewed distribution of each zi. Formally, suppose σout,i is the ini-tially estimated standard deviation of zi on the collection of
OOD data, and mout,i is the estimated mode of zi over all the OOD data, then the positively skewed distribution of the zi on the OOD dataset can be modelled by a ESN distribu-tion as below, q(zi) = (cid:40) 1
σout, i 1
σout, i
ϕ(
ϕ( zi−mout,i
σout,i(1+ϵout,i) ) zi−mout,i
σout,i(1−ϵout,i) ) if zi < mout,i if zi ≥ mout,i
, (10) where ϕ(·) represents the p.d.f of standard normal distribu-tion, and the hyper-parameter ϵout,i ∈ (−1, 1) controls the skewness. q(zi) will become the well-known half-normal distributions when ϵout,i → ±1, and reduce to the normal distribution when ϵout,i = 0. In particular, ϵout,i < 0 for
In this case, the re-positively-skewed ESN distribution. lationship between initial estimated variance of zi and the variance can be formulated as [17]
Var(zout,i) =
σ2 out,i
π
[(3π − 8)ϵ2 out,i + π]. (11)
The upper bound of the variance for each zi can be obtained by setting ϵout,i = −1. Similarly on the ID dataset, the vari-ance of zi can be estimated as well, with the parameters in Eqs. (10) and (11) estimated based on the ID dataset, and the lower bound of the variance for each zi can be ob-tained by setting ϵin,i = 0. After replacing Ex∈Dout[(zi −
µout,i)2] (Eq. 9) by the upper bound of Var(zi) on the OOD dataset, and replacing Ex∈Din[(zi(x) − µin,i)2] (Eq. 8) by the lower bound of Var(zi) on the ID dataset, the upper bound of Ex∈Dout[v(x)] (Eq. 9) and the the lower bound of
Ex∈Din[v(x)] (Eq. 8) can be directly obtained. Table 1 (last row) shows that even the lower bound of Ex∈Din [v(x)] on the ID dataset (first column) is clearly larger than the upper bound of Ex∈Dout [v(x)] on each of the four OOD datasets (last four columns).
This supports the above finding that the magnitude
∥h(x) − µ∥ of the centralized feature vector h(x) − µ is statistically larger for ID data than for OOD data. Together with Eq. (11), it also indicates that the initial estimates
{σ2 i=1 on the ID dataset should be statistically larger than the initial estimates {σ2 out,i}K i=1 on the OOD dataset.
This is confirmed by the histogram of the initial variance estimates respectively on the ID dataset and on each OOD dataset (Figure 7). in,i}K
In summary, we find that statistically the larger variance in components (zi’s) of the feature vector h(x) on the ID data, together with the smaller angle between the original feature vector of ID data and its projection in the subspace, is probably the source of the inseparability in PCA-based reconstruction error between ID and OOD data. One may further wonder why feature vector components have rela-tively smaller variances on OOD data, which is beyond the scope of this study and can be investigated as a future work.
Table 1. The lower bound of Eq. 8 on ID data (1st column) and the upper bound of Eq. 9 on OOD data (last 4 columns).
Places
-1 333.76
Textures
-1 388.19
Dataset
ϵ
E[v(x)]
ImageNet1K iNaturalist
SUN
-1 335.06 0 393.22
-1 370.03 3.2. Regularized reconstruction error
While it is still unclear why feature vector components have relatively larger variance on the ID data, it is well-known that a variable whose values are at a larger scale would more likely have a relatively larger variance. If that is the case, we may use the relatively larger norm of fea-ture vector to alleviate the negative effect of the larger vari-ance from ID data. Fortunately, previous studies [28] have shown that statistically the norm of feature vector from ID data is larger than that from OOD data (also see the re-implemented result in Figure 8). With all the above con-Figure 8. Distributions of the norm of feature vector on the ID dataset ImageNet-1K (blue) and each of the four OOD datasets iNaturalist, Plcaces, SUN, and Textures (orange).
Figure 9. Distributions of the regularized reconstruction error on the ID dataset (blue) and each of the four OOD datasets (orange). sideration, we choose to use a simple regularized version of the original reconstruction error e(x) (Eq. 5) as below r(x) = ∥h(x) − ˆh(x)∥/∥h(x)∥. (12)
With the regularized reconstruction error r(x), statistically smaller reconstruction error is observed on the ID dataset compared to that on each OOD dataset (Figure 9). Although the separability in the regularized reconstruction error be-tween ID and OOD data is still weak, the regularized recon-struction error r(x) can be combined with existing OOD scores to potentially further improve the OOD detection performance. For example, the fusion of r(x) with the en-ergy score results in a new OOD score as below
D(x) = (1 − r(x)) log
C (cid:88) c=1 exp(fc(x)) . (13)
The multiplication rather than the addition operator is used for the fusion of two OOD scores to improve the influence of the regularized reconstruction error r(x) and meanwhile avoid using the extra trade-off coefficient between the two scores. It is expected that ID data would result in a higher score D(x) and OOD data lower score. Also, it is worth noting that the proposed r(x) is compatible with multiple existing OOD methods, i.e., it can be fused with not only energy-based OOD scores but also others like MSP [7] for
OOD detection (see Table 3 below). 4. Experiments 4.1. Experimental settings
Our method was extensively evaluated on four OOD detection benchmarks. Each benchmark contains a train-ing ID set, a test ID set, and multiple test OOD sets.
The ImageNet-1K and ImageNet-100 [25] Benchmarks re-spectively use the ImageNet-1K and ImageNet-100 as ID sets, and both use four OOD sets, including iNatural-ist [27], SUN [32], Places [39], and Textures [2]. The
CIFAR-10 and CIFAR-100 Benchmarks respectively use the CIFAR-10 and CIFAR-100 as ID sets, and both use seven OOD sets, including SVHN [19], Tiny-ImageNet [1], iSUN [33], LSUN-Crop [36], LSUN-Resize [36], Textures, and Places365 [39]. There are no overlapped classes be-tween ID set and each OOD set. Please refer to Supplemen-tary Section A for more details of datasets.
On the ImageNet-1K Benchmark, a publicly re-leased pre-trained CNN classifier with certain backbone (ResNet50 [5] or MobileNet-v2 [20]) was directly used for evaluation of our method and each baseline method. On the other three benchmarks, a CNN classifier with certain back-bone was trained from scratch with the associated training
ID set. The stochastic gradient descent optimizer with mo-mentum (0.9) and weight decay (0.0005) was used to train each classifier up to 100 epochs on the ImageNet-100 train-ing set or up to 200 epochs on the CIFAR [12] training datasets. The batch size was set to 128. The initial learning was set to 0.1, and decayed by a factor of 10 at the 50-th, 75-th, and 90-th epoch on ImageNet-100, or at the 100-th and 150-th epoch on CIFAR. On the ImageNet-100 training set, each image was resized to 256 × 256 pixels and then randomly cropped to 224 × 224 pixels. On the CIFAR-10 or CIFAR-100 training set, each training image was padded from 32 × 32 pixels to 36 × 36 pixels and then randomly cropped to 32 × 32 pixels. Random horizontal flipping was adopted together with random cropping on each training im-age. During test, only center cropping together resizing was used on the ImageNet [3] dataset.
Following previous studies [22], ResNet50 and the light-weight MobileNet-v2 were used as the classifier backbone on the ImageNet Benchmarks, ResNet34 and
WideResNet28-10 [31, 40] were used on CIFAR Bench-marks. The feature dimension of the penultimate layer is respectively 2048, 1280, 512, and 640 for ResNet50,
MobileNet-v2, ResNet34, and WideResNet28-10. In all ex-periments, the reduced feature dimension was empirically set to 256, 128, 9 (on CIFAR10) or 49 (on CIFAR100), and 16 (on CIFAR10) or 64 (on CIFAR100) respectively for the four backbones, with the constraint that around 90% of the original feature signal was preserved after dimension reduc-tion on the ID training sets.
Multiple types of competitive OOD detection methods were adoped as baselines for comprehensive evaluation, including the Maximum Softmax Probability (MSP) [7],
ODIN [15], Energy [16], Mahalanobis [14], ViM [29],
DICE [23], ReAct [22] and BATS [40]. All the baselines are post-hoc and can obtain the OOD score based on a pre-trained CNN classifier.
In addition, LogitNorm [31] was used on Benchmark III because it achieves the state-of-the-art performance on the benchmark. For all experiments,
FPR95 (i.e., the false positive rate of OOD examples when the true positive rate of ID examples is 95%) and AUROC
Table 2. Comparison between different methods in OOD detection on the ImageNet-1K Benchmark with two different model backbones.
↑ indicates that larger values are better and ↓ indicates that smaller values are better. All values are percentages.
ID Dataset
Model
Method iNaturalist
FPR95↓ AUROC↑
SUN
FPR95↓ AUROC↑
OOD Datasets
Places
FPR95↓ AUROC↑
Textures
FPR95↓ AUROC↑
Average
FPR95↓ AUROC↑
ImageNet-1K
ResNet50
ImageNet-1K
MobileNet
MSP
ODIN
Mahalanobis
Energy
ViM
BATS
DICE
ReAct
DICE+ReAct
Ours
MSP
ODIN
Mahalanobis
Energy
ViM
BATS
DICE
ReAct
DICE+ReAct
Ours 54.99 47.66 97.00 55.72 68.86 42.26 26.66 20.38 20.08 10.17 64.29 58.54 62.11 59.50 91.83 49.57 43.28 43.07 41.75 35.84 87.74 89.66 52.65 89.95 87.13 92.75 94.49 96.22 96.11 97.97 85.32 87.51 81.00 88.91 77.47 91.50 90.79 92.72 89.84 93.66 70.83 60.15 98.50 59.26 79.62 44.70 36.08 24.20 26.50 18.50 77.02 57.00 47.82 62.65 94.34 57.81 38.86 52.47 39.07 40.35 80.86 84.59 42.41 85.89 81.67 90.22 90.98 94.20 93.83 95.80 77.10 85.83 83.66 84.50 70.24 85.96 90.41 87.26 90.39 90.77 73.99 67.89 98.40 64.92 83.81 55.85 47.63 33.85 38.34 27.31 79.23 59.87 52.09 69.37 93.97 64.48 53.48 59.91 54.41 52.38 79.76 81.78 41.79 82.86 77.80 86.48 87.73 91.58 90.61 93.39 76.27 84.77 83.63 81.19 68.26 82.83 85.67 84.07 84.03 86.76 68.00 50.23 55.80 53.72 14.95 33.24 32.46 47.30 29.36 18.67 73.51 52.07 92.38 58.05 37.62 39.77 33.14 40.20 19.98 18.44 79.61 85.62 85.01 85.99 96.74 93.33 90.46 89.80 92.65 95.95 77.30 85.04 33.06 85.03 92.65 91.17 91.26 90.96 95.86 95.39 66.95 56.48 87.43 58.41 61.81 44.01 35.71 31.43 28.57 18.66 73.51 56.87 63.60 62.39 79.44 52.91 42.19 48.91 38.80 36.75 81.99 85.41 55.47 86.17 85.83 90.69 90.92 92.95 93.30 95.78 79.00 85.79 71.01 84.91 77.15 87.87 89.53 88.75 90.03 91.65 (the area under the receiver operating characteristic curve) in OOD detection were used as metric, with lower FPR95 values and higher AUROC values indicating better OOD de-tection performance. 4.2. Quantitative evaluations
On the ImageNet-1K Benchmark, Table 2 summarizes the OOD detection performance of each method on each
OOD set (together with the ID test set) and the average per-formance over the four OOD sets. By default, our method is built on the state-of-the-art baseline ReAct which uses the energy score for OOD detection. With the ResNet-50 back-bone, our method achieves state-of-the-art performance on three of the four OOD sets, and in average outperforms the best baseline DICE+ReAct by a large margin (95.78% vs. 93.30% on AUROC, and 18.66% vs. 28.57% on FPR95).
Similar finding can be obtained with the MobileNet-v2 backbone (Table 2, lower half), achieving the best AUROC performance on three OOD sets and the state-of-the-art av-erage performance over the four OOD sets.
Our method can be flexibly combined with existing base-lines to further improve their OOD detection performance.
As Table 3 shows, when fusing the proposed regularized reconstruction error term (1 − r) into the OOD scores in ex-isting methods MSP, Energy, BATS, and ReAct, the OOD detection performance is boosted on each OOD set. With the ResNet50 backbone, the average AUROC is improved by 1.87% − 2.83%, and the average FPR95 is successfully reduced by 4.24%−12.77%. Similar result can be observed with the MobileNet-v2 backbone (Table 3, lower half).
Consistently on the ImageNet-100 Benchmark, our method achieves the best performance in average (Table 4, last two columns). On individual OOD sets, our method performs either best or similar to the the best baseline.
Note that for each method, the OOD performance on the
ImageNet-100 Benchmark is slightly worse than that on
ImageNet-1K Benchmark probably because the classifier trained on ImageNet-100 is more confident about its pre-dictions, which causes more confident false positive predic-tions (i.e., OOD samples considered as ID classes) and cor-respondingly lower AUROC values.
On the CIFAR Benchmarks, the regularized reconstruc-tion error also helps achieve state-of-the-art performance in average on the seven OOD sets with both CNN backbones, as shown in Table 5. Note that on the CIFAR-10 Bench-mark, the regularized reconstruction error is combined with the state-of-the-art method LogitNorm by default. In addi-tion, Table 6 consistently confirms that the regularized re-construction error can be flexibly combined with multiple
OOD methods to further improve their original detection performance. 4.3. Sensitivity study
Here we evaluate the sensitivity of our method to the di-mension k of the subspace which is crucial to compute the regularized reconstruction error. Smaller k statistically cor-responds to less percent of the original feature signal pre-served after dimension reduction on the ID training set. As shown in Figure 10, our method works quite stable when varying the value of k in a large range on both benchmarks.
Although the OOD detection performance slowly decreases when k gradually becomes smaller, our method still works
Table 3. Fusion of the regularized reconstruction error with various OOD methods on the ImageNet-1K Benchmark. For each paired values by ‘/’: the left one is from the original baseline and the right one is from the fusion one.
ID Dataset
Model
ImageNet-1K
ResNet50
ImageNet-1K
MobileNet
Method iNaturalist
SUN
OOD Datasets
Places
MSP
Energy
BATS
ReAct
MSP
Energy
BATS
ReAct
FPR95↓ 54.99/51.47 55.72/50.36 42.26/29.66 20.38/10.17 64.29/59.49 59.50/56.92 49.57/50.51 43.07/35.84
AUROC↑ 87.74/88.95 89.95/91.09 92.75/94.49 96.22/97.97 85.32/86.87 88.91/89.62 91.50/90.86 92.72/93.66
FPR95↓ 70.83/67.64 59.26/54.19 44.70/38.11 24.20/18.50 77.02/73.75 62.65/60.07 57.81/55.41 52.47/40.35
AUROC↑ 80.86/82.71 85.89/87.55 91.40/90.03 94.20/95.80 77.10/79.41 84.50/85.80 85.96/87.00 87.26/90.77
FPR95↓ 73.99/71.20 64.92/64.13 55.85/51.70 33.85/27.31 79.23/76.79 69.37/69.23 64.48/66.43 59.91/52.38
AUROC↑ 79.76/ 80.87 82.86/84.00 86.48/87.25 91.58/93.39 76.27/ 77.94 81.19/81.72 82.83/82.60 84.07/86.76
Textures
Average
FPR95↓ 68.00/60.53 53.72/29.33 33.24/13.46 47.30/18.67 73.51/65.71 58.05/34.22 39.77/23.26 40.20/18.44
AUROC↑ 79.61/85.86 85.99/92.59 93.33/97.09 89.80/95.95 77.30/83.46 85.03/91.66 91.17/94.70 90.96/95.39
FPR95↓ 66.95/62.71 58.41/49.50 44.01/33.23 31.43/18.66 73.51/68.93 62.39/55.11 52.91/48.90 48.91/36.75
AUROC↑ 81.99/84.60 86.17/88.81 90.69/92.56 92.95/95.78 79.00/81.92 84.91/87.20 87.87/88.79 88.75/91.65
Table 4. Comparison between different methods in OOD detection on the ImageNet-100 Benchmark.
ID Dataset
Model
Method iNaturalist
FPR95↓ AUROC↑
SUN
FPR95↓ AUROC↑
OOD Datasets
Places
FPR95↓ AUROC↑
Textures
FPR95↓ AUROC↑
Average
FPR95↓ AUROC↑
ImageNet-100
ResNet50
MSP
ODIN
Mahalanobis
Energy
ViM
BATS
DICE
ReAct
DICE+ReAct
Ours 69.28 44.22 98.09 64.60 84.92 43.05 35.08 30.60 29.20 26.60 85.84 92.42 35.06 89.08 81.92 92.65 93.29 94.40 93.83 94.83 70.14 54.71 98.75 62.70 83.18 58.75 36.89 47.55 47.01 41.55 84.20 88.94 34.70 88.03 81.47 87.83 92.53 89.99 89.28 90.96 69.43 57.52 98.66 60.70 81.45 57.72 43.71 47.21 38.40 44.50 84.29 88.01 34.95 87.78 81.55 87.43 90.66 89.53 92.01 90.22 64.27 42.87 44.08 51.38 20.00 38.88 31.84 50.89 31.84 22.18 84.09 89.76 83.07 87.89 96.07 91.97 92.08 87.57 92.08 95.07 68.28 49.83 84.98 59.85 67.39 49.60 36.46 44.06 36.61 33.71 84.60 89.78 44.08 88.20 85.25 89.97 92.11 90.45 91.80 92.77 well even just 70% of the original signals is preserved in the feature subspace. 5.