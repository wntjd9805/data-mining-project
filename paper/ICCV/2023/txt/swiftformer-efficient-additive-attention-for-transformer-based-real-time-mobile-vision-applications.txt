Abstract
Self-attention has become a defacto choice for captur-ing global context in various vision applications. However, its quadratic computational complexity with respect to im-age resolution limits its use in real-time applications, espe-cially for deployment on resource-constrained mobile de-vices. Although hybrid approaches have been proposed to combine the advantages of convolutions and self-attention for a better speed-accuracy trade-off, the expensive matrix multiplication operations in self-attention remain a bottle-neck. In this work, we introduce a novel efficient additive attention mechanism that effectively replaces the quadratic matrix multiplication operations with linear element-wise multiplications. Our design shows that the key-value inter-action can be replaced with a linear layer without sacrific-ing any accuracy. Unlike previous state-of-the-art methods, our efficient formulation of self-attention enables its usage at all stages of the network. Using our proposed efficient additive attention, we build a series of models called “Swift-Former” which achieves state-of-the-art performance in terms of both accuracy and mobile inference speed. Our small variant achieves 78.5% top-1 ImageNet-1K accuracy with only 0.8 ms latency on iPhone 14, which is more ac-curate and 2× faster compared to MobileViT-v2. Our code and models: https://tinyurl.com/5ft8v46w 1.

Introduction
In recent years, transformer models have shown remark-able success in various vision applications such as classi-fication [8, 44, 24, 23, 9], detection [2, 59, 33, 56, 28], and segmentation [4, 40]. However, deploying these models on resource-constrained mobile devices for real-time applica-tions remains challenging due to their inherently complex nature [20, 29]. Specifically, vision transformers (ViTs) rely on global self-attention, which has a quadratic com-Figure 1: Latency vs Accuracy Comparison. Compared to the recent EfficientFormer-L1 [20], our SwiftFormer-L1 achieves an absolute gain of 1.7% in terms of top-1 accu-racy with the same latency and without requiring any neural architecture search. plexity with respect to the input image resolution, mak-ing it impractical for deployment on low-powered mobile devices [31]. As a result, convolutional neural networks (CNNs) are still the preferred choice for real-time deploy-ment on mobile devices, primarily because the convolution operation is computationally efficient [39, 15]. However, a major limitation of CNNs is their reliance on local connec-tions and stationary weights, which can limit their ability to adapt to variable input resolutions and capture long-range dependencies in the data. Therefore, developing more effi-cient and flexible models that combine the strengths of both
CNNs and transformers is critical, particularly for mobile devices with limited computational resources.
*Corresponding author: abdelrahman.youssief@mbzuai.ac.ae
To achieve this goal, several hybrid approaches have
been proposed that use lightweight CNN modules in the high-resolution early stages and self-attention in the low-resolution later stages [55, 29, 20]. This approach effec-tively increases the receptive field of the network and strives to achieve a trade-off between speed and accuracy. Further-more, different efficient variants of computing self-attention have been proposed to reduce the model complexity. These include computing attention across feature dimensions to implicitly model the global context [29], computing atten-tion within local windows [24], pooling spatial features be-fore applying self-attention [9], and sparsely attending to a fixed number of tokens [34], to name a few.
Although these approaches effectively reduce network complexity, they still involve inefficient matrix multiplica-tion operations that significantly impact latency on mobile devices. To address this issue, Mehta et al. [31] propose a separable self-attention mechanism that replaces matrix multiplication operations to element-wise multiplications.
This is achieved by projecting queries to context scores, fol-lowed by element-wise multiplication with keys to calculate context vectors for encoding global context.
In this work, we propose efficient additive attention, which eliminates the need for expensive matrix multipli-cation operations in computing self-attention. Additionally, we propose to compute the global context using only the query-key interactions followed by a linear transformation, without requiring explicit key-value interactions. This sig-nificantly reduces the computational complexity and en-ables us to use the proposed attention block in all stages of the network. Our contributions are as follows:
• We introduce efficient additive attention, a new ap-proach for computing self-attention in vision back-bones that eliminates the need for expensive matrix multiplication operations, significantly reducing the computational complexity of the model.
• Unlike previous methods, our proposed efficient atten-tion design can be used at all stages of the network, enabling more effective contextual information capture and achieving superior speed-accuracy trade-off.
• We build a series of efficient generic classification models called “SwiftFormer”, which utilize our pro-posed efficient additive attention. Our small model achieves 78.5% top-1 ImageNet-1K [7] accuracy while running at only 0.8 ms latency on iPhone 14. More-over, our large model achieves 83.0% accuracy with a latency of only 1.9 ms. Our model achieves state-of-the-art performance, outperforming recent MobileViT-v2 [31] and EfficientFormer [20] by obtaining a better trade-off between accuracy and latency (see Fig. 1). 2.