Abstract
Most state-of-the-art instance segmentation methods rely on large amounts of pixel-precise ground-truth annotations for training, which are expensive to create. Interactive seg-mentation networks help generate such annotations based on an image and the corresponding user interactions such as clicks. Existing methods for this task can only process a single instance at a time and each user interaction re-quires a full forward pass through the entire deep network.
We introduce a more efficient approach, called DynaMITe, in which we represent user interactions as spatio-temporal queries to a Transformer decoder with a potential to seg-ment multiple object instances in a single iteration. Our architecture also alleviates any need to re-compute image features during refinement, and requires fewer interactions for segmenting multiple instances in a single image when compared to other methods. DynaMITe achieves state-of-the-art results on multiple existing interactive segmentation benchmarks, and also on the new multi-instance benchmark that we propose in this paper. 1.

Introduction
Interactive segmentation algorithms enable a user to an-notate the objects of interest within a given image with the help of user interactions such as scribbles and clicks.
Such algorithms have several advantages compared to fully-automatic segmentation methods, since they enable a user to select and iteratively refine the objects of interest. Ex-isting interactive segmentation methods [8, 28, 30, 39, 40] formulate this task as a binary instance segmentation prob-lem, where the single object of interest can be segmented and corrected using user clicks.
Most of these approaches use deep neural networks to generate the image features that are conditioned on the user clicks and previous predictions, and they require the image
*Equal contribution. level features to be re-computed for every user interaction.
While such a design has been proven to be effective, the runtime for processing each interaction is proportional to the size of the feature extractor used, since a forward pass through the network is needed per interaction [8, 28, 30, 39, 40]. Hence, these methods often have to limit their network sizes in order to achieve a good runtime performance and are thus not scalable in this respect.
In addition, the design decision to model interactive seg-mentation as a binary segmentation problem forces existing methods to approach multi-instance segmentation tasks as a sequence of single-instance problems, operating on sep-arate (sometimes cropped and rescaled [8]) image regions.
Consequently, such methods need additional clicks if there are multiple similar foreground instances in an image, since each of those instances has to be processed separately with a disjoint set of user interactions, specifying the foreground and background. This is inefficient, since it is often the case that one object instance has to be considered as background for a different nearby instance, such that a refinement with a negative click becomes necessary for the current object of focus.
In this work, we improve on both of the above issues by proposing a Dynamic Multi-object Interactive segmen-tation Transformer (DynaMITe), a novel multi-instance ap-proach for interactive segmentation that only requires a sin-gle forward pass through the feature extractor and that pro-cesses all relevant objects together, while learning a com-mon background representation. Our approach is based on a novel Transformer-based iterative refinement architecture which determines instance level descriptors directly from the spatio-temporal click sequence. DynaMITe dynami-cally generates queries to the Transformer that are con-ditioned on the backbone features at the click locations.
These queries are updated during the refinement process whenever the network receives a new click, prompting the
Transformer to output a new multi-instance segmentation.
Thus, DynaMITe removes the need to re-compute image-False Positive Area
Corrective Click
Additional Click
Final Segmentation
Figure 1: DynaMITe processes multiple instances at once and models the background jointly. In this example, the false positive region on the camel in the second image is corrected automatically when the user chooses to segment it as foreground.
DynaMITe is also able to correctly segment tiny structures, such as the camel’s leash in the final segmentation mask.
Click-based Refinement Process level features for each user interaction, while making more effective use of user clicks by handling multiple object in-stances together.
The attention-based formulation of learning object repre-sentations from user interactions allows multiple objects to interact with each other and with the common background representations, thus enabling the network to estimate a bet-ter context from the input image. Fig. 1 shows a typical ex-ample, highlighting DynaMITe’s capability to segment all the relevant objects in the input image using few clicks. An advantage of such a network formulation can be directly seen in the third refinement iteration, where a positive click on the unsegmented camel instance automatically removes the false positive region that was spilled over after segment-ing a different camel instance nearby in the previous itera-tion. Existing approaches that perform sequential single-instance segmentation would have to first add a negative click to remove the false positive as part of the individual object refinement in the third iteration, thereby requiring additional annotation effort.
In order to enable quantitative evaluation, we also pro-pose a novel multi-instance interactive segmentation task (MIST) and a corresponding evaluation strategy. Compared to single-instance segmentation, MIST has the added com-plexity of requiring decisions which object to click on next, which is significantly harder than just deciding where to click next in a given single-instance error region. In particu-lar, different users may apply different next-object selection strategies, and it is important that an interactive segmen-tation method is robust to this and always performs well.
Hence, we propose to evaluate against a set of several dif-ferent (but still basic) click sampling heuristics that are in-tended to span the expected variability of user types.
In summary, we propose DynaMITe, a novel segmentation method
Transformer-based which uses a query bootstrapping mechanism to learn object representations from image-level features that are conditioned on the user interactions. We also model the iterative refinement process as temporal update steps for the queries to our Transformer module, which removes the need to re-compute image-level features. We evalu-interactive ate DynaMITe on the standard interactive segmentation benchmarks and show that it performs competitively in the single-instance setting, while outperforming existing state-of-the-art methods on multi-instance tasks. 2.