Abstract
Speech-driven 3D face animation aims to generate real-istic facial expressions that match the speech content and emotion. However, existing methods often neglect emo-tional facial expressions or fail to disentangle them from speech content. To address this issue, this paper proposes an end-to-end neural network to disentangle different emo-tions in speech so as to generate rich 3D facial expressions.
Specifically, we introduce the emotion disentangling en-coder (EDE) to disentangle the emotion and content in the speech by cross-reconstructed speech signals with different emotion labels. Then an emotion-guided feature fusion de-coder is employed to generate a 3D talking face with en-hanced emotion. The decoder is driven by the disentangled identity, emotional, and content embeddings so as to gen-erate controllable personal and emotional styles. Finally, considering the scarcity of the 3D emotional talking face data, we resort to the supervision of facial blendshapes, which enables the reconstruction of plausible 3D faces from 2D emotional data, and contribute a large-scale 3D emo-tional talking face dataset (3D-ETF) to train the network.
Our experiments and user studies demonstrate that our ap-proach outperforms state-of-the-art methods and exhibits more diverse facial movements. We recommend watch-ing the supplementary video: https://ziqiaopeng. github.io/emotalk 1.

Introduction
Dynamic and realistic speech-driven facial animation has garnered growing interest in virtual reality [50, 11, 12], computer gaming [36, 10, 2], and film production [26, 51, 7]. For current commercial products, 3D face blendshape is handcrafted by animators, whereas manual scripts drive
*corresponding authors
Figure 1. Results of EmoTalk. Given audio input expressing dif-ferent emotions, EmoTalk produces realistic 3D facial animation sequences with corresponding emotional expressions as outputs. facial expressions. Such a process demands substantial ex-penses and considerable time and labor. As deep learning techniques are utilized in various scenarios [37], deep end-to-end speech-driven facial animation [20, 8, 39, 9, 6] has been widely studied in industry and academia. Presently, learning-based 3D facial animations can not only produce high-quality animation effects but also facilitate cost reduc-tion during production.
However, current methods mainly focus on improv-ing the synchronization between lip movements and speech [42], neglecting the emotional variation of facial ex-pressions. We argue that emotions are an essential aspect of human communication and expression, and emotion ab-sence in 3D facial animations may cause the uncanny valley effect. It is a crucial issue to recover emotional expressions
for the speech-driven 3D face animation problem. In fact, emotional information is naturally contained in the speech, and extracting emotions is a crucial task for speech under-standing [48]. Nevertheless, as audio content and emotion are entangled, it is hard to extract explicit content and emo-tion from a speech simultaneously. In order to generate rich emotional facial expressions, previous 2D facial animation methods encode the emotions manually and only learn the content feature from the speech [34, 4, 43]. By manipulat-ing the emotion code, the facial decoder could achieve ap-propriate emotional modulation. Manually controlling may generate changeable emotions, but it could result in contra-diction with the emotion in speech. For example, it does not conform to human intuition by inputting angry speech but outputting a happy expression.
To address this issue, we propose a novel speech-driven emotion-enhanced 3D facial animation method (Fig. 1) in this paper, where an emotion disentangling encoder and emotion-guided feature fusion decoder are proposed to con-sist of our key contribution, as illustrated in Fig. 2. For the emotion disentangling encoder, two distinct audio fea-ture extractors [1] are introduced and utilized to extract two separate latent spaces for the content and emotion, respec-tively, which is exploited to decouple emotion and content.
A cross-reconstruction loss is further presented to constrain the learning process to better disentangle the emotion and content from the speech. While for the emotion-guided fea-ture fusion decoder, multiple different types of features are decoded by a Transformer [47] module with periodic po-sitional encoding and emotion-guided multi-head attention, which will output 52 emotion-enhanced blendshape coeffi-cients to represent the final human facial expressions. Ex-tensive experiments show that our method significantly out-performs current state-of-the-art methods in terms of emo-tional expression by disentangling content and emotion.
To train the proposed network, emotional speeches with corresponding 3D facial expressions are required. However, as far as we know, there is no publicly available 3D emo-tional talking face dataset that we can use, posing a serious new challenge. To tackle the issue, a large-scale pseudo-3D emotional talking face dataset, termed the 3D-ETF dataset, is further introduced in our work. To build this dataset and make it more applicable, we first collaborated with sev-eral professional animators to create 52 FLAME head tem-plates [25] that are semantic meaningful. Then, “pseudo” 3D blendshape labels are generated from images of large-scale audio-visual datasets [28, 56] by utilizing a well-established 3D facial blendshape capture system. Finally, the 3D-ETF dataset with both blendshape coefficients [24] and mesh vertices are constructed through blend linear skin-ning. Since its blendshape labels are semantic meaningful, the 3D-ETF dataset is versatile, allowing the facile transfer of facial movements among different virtual characters [35].
In summary, the main contributions of our work are as follows:
• We propose an end-to-end neural network for speech-driven emotion-enhanced 3D facial animation, which achieves various emotional expressions and outper-forms existing state-of-the-art methods.
• We introduce the emotion disentangling encoder, which disentangles the emotion and content in the speech and makes the facial animation aware of clear emotional information.
• We present a large-scale 3D emotional talking face (3D-ETF) dataset including both blendshape coeffi-cients and mesh vertices. We have implemented parameterized transformations for blendshape coeffi-cients and the FLAME model, allowing for efficient conversion between various facial animations. 2.