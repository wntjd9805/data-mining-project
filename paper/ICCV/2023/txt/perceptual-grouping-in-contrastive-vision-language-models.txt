Abstract
Recent advances in zero-shot image recognition suggest that vision-language models learn generic visual representa-tions with a high degree of semantic information that may be arbitrarily probed with natural language phrases. Under-standing an image, however, is not just about understanding what content resides within an image, but importantly, where that content resides. In this work we examine how well vision-language models are able to understand where objects reside within an image and group together visually related parts of the imagery. We demonstrate how contemporary vision and language representation learning models based on con-trastive losses and large web-based data capture limited object localization information. We propose a minimal set of modiﬁcations that results in models that uniquely learn both semantic and spatial information. We measure this perfor-mance in terms of zero-shot image recognition, unsupervised bottom-up and top-down semantic segmentations, as well as robustness analyses. We ﬁnd that the resulting model achieves state-of-the-art results in terms of unsupervised segmentation, and demonstrate that the learned representa-tions are uniquely robust to spurious correlations in datasets designed to probe the causal behavior of vision models. 1.

Introduction
Learning a representation for visual imagery requires resolving not only what resides within an image, but also where that information resides [72]. In many applications, knowledge of where information resides is sometimes more important than a precise description of the content [33, 98].
Hence, our ability to learn more generic and robust visual representations requires learning the geometry of visual se-mantics, and how visual information may be grounded by speciﬁc regions of the visual ﬁeld.
While recent vision-language models trained under weak supervision demonstrate a remarkable ability to learn generic and transferable visual representations [50, 85, 117, 24], they
*Work performed as part of Apple internship.
†Work performed at Apple.
Figure 1: Semantic localization in contrastive VLMs. We mea-sure the ability of vision-language models to predict a label at each spatial position in a zero shot manner based on the similarity of location tokens to the corresponding language tokens on selected examples. CLIP / ALIGN [50, 85] have minimal understanding of the spatial location of individual objects (row 4). Our proposed
CLIPpy (row 3) predicts the label at locations that correspond closely to human annotation for semantic segmentation (row 2).
All predictions were performed with no access to any segmentation data during training or inference. More visualizations in App. B. showcase a profound inability to associate visual content with individual objects (Fig. 1, bottom row). In other words, models trained on large weakly-supervised data have a lim-ited ability to group together visually related content [36].
Because the representations have a poor understanding of where an object resides, they easily conﬂate background with foreground content. Hence, the learned representations are unable to learn the spatial layout of a scene [97, 101], and are susceptible to learning spurious correlations between a semantic label and extraneous content [91, 65].
Recent work [113, 114] attempts to bridge this gap through grouping mechanisms under the same weakly su-pervised training paradigm, but focus more on foreground
objects (neglecting background classes). Another direction is task speciﬁc unsupervised ﬁne-tuning [126, 26] which loses the generic and transferable nature of these representations.
In this work, we explore vision-language models that learn from similar weakly labeled data, but a) retain the generic and transferable nature of features, and b) learns where all (background and foreground) visual content resides within an image. Unlike previous attempts using grouping speciﬁc architectures [113, 114] or dense human annotations
[36, 38, 57], we explore a minimal set of modiﬁcations to existing CLIP models [85] that leads to grouping of visual imagery while retaining their weakly supervised and scalable training procedure. We ﬁnd that two small adjustments – em-ploying speciﬁc pretraining strategies and adjusting spatial feature aggregation – results in models that are equally ef-fective in zero-shot image recognition, but also retain spatial information regarding object locations (see Fig. 1, 3rd row).
The resulting model termed CLIPpy exhibits perceptual grouping – that is, the ability to select and combine re-lated visual signals into semantically meaningful regions
[110, 72, 89]. Endowing models with perceptual grouping – whether in a bottom up (based solely on visual content) or top down (guided by external information, language in this case) manner – in learned representations has been a long standing goal in computer vision [70, 71]. In this work, our key contributions are as follows:
• Identify systematic failure of contrastive vision-language models [85, 50] to properly identify where objects reside within an image, and group semantically related content.
• Design a minimal set of changes to endow these model with perceptual grouping, resulting in state-of-the-art zero-shot segmentation without training on any segmentation data or performing task speciﬁc ﬁne-tuning.
• Emergence of localization ability in our models uniquely leads to robustness to counterfactual manipulations. The degree of robustness matches if not surpasses previous state-of-the-art supervised learning methods employing specialized training methodologies. 2.