Abstract
The fidelity of Generative Adversarial Networks (GAN) inversion is impeded by Out-Of-Domain (OOD) areas (e.g., background, accessories) in the image. Detecting the OOD areas beyond the generation ability of the pre-trained model and blending these regions with the input image can en-hance fidelity. The “invertibility mask” figures out these
OOD areas, and existing methods predict the mask with the reconstruction error. However, the estimated mask is usu-ally inaccurate due to the influence of the reconstruction er-ror in the In-Domain (ID) area. In this paper, we propose a novel framework that enhances the fidelity of human face in-*Corresponding author. version by designing a new module to decompose the input images to ID and OOD partitions with invertibility masks.
Unlike previous works, our invertibility detector is simul-taneously learned with a spatial alignment module. We it-eratively align the generated features to the input geome-try and reduce the reconstruction error in the ID regions.
Thus, the OOD areas are more distinguishable and can be precisely predicted. Then, we improve the fidelity of our re-sults by blending the OOD areas from the input image with the ID GAN inversion results. Our method produces photo-realistic results for real-world human face image inversion and manipulation. Extensive experiments demonstrate our method’s superiority over existing methods in the quality of GAN inversion and attribute manipulation. Our code is available at: AbnerVictor/OOD-GAN-inversion
1.

Introduction
In recent years, there have been efforts to apply gener-ative models, e.g., StyleGAN2 [23], for face image edit-ing [1, 2, 44, 36, 5, 40, 28, 39, 4, 31, 46, 32, 26, 43] and restoration [41, 15, 48, 45]. The basis of these applica-tions is the GAN inversion. The typical inversion strat-egy [31, 39] is to train encoders to encode the face images into the latent of the pre-trained generator and reconstruct the images via the generator. However, such approaches can not result in artifact-free and precise reconstruction due to the inevitable information loss when translating the high-resolution image into the limited GAN latent space.
Some improved methods were proposed to further fine-tune the generator for image-specific reconstruction with-out losing editability [32, 26]. Although these finetuning-based techniques improve the inversion accuracy of iden-tity and style, still, the generator can hardly handle the reconstruction of out-of-domain contents, e.g., the com-plex background, accessories, and hair. Meanwhile, some works [40, 41, 28] strengthen the potential of out-of-domain inversion capability of pre-trained GAN models by mod-ulating the generator features with the input features ex-tracted from source images. Such methods suffer from the fidelity-editability trade-off [40, 33, 38, 15, 41, 45] since the feature modulation operation breaks the GAN priors. The larger latent space increases the reconstruction quality but undermines the editability of the framework.
Furthermore, some recent works [36, 28] propose to dis-entangle the target image into different spatial areas. They refine the regions with low invertibility to improve the inver-sion fidelity. Such low invertibility regions are the parts that cannot be reconstructed well with the generator, so-called
Out-Of-Domain (OOD) areas. E.g., Song et al. [36] esti-mate a manipulation-aware mask with an attribute classi-fier. However, they ignore the geometrical misalignment between the inverted and the original images and apply a deghosting module for result refinement, which leads to undesired artifacts in their results (Fig. 1 (e)). Parmar et al. [28] train an invertibility mask prediction module with perceptual supervision. The invertibility masks are then used as guidance for GAN feature modulation. However, their prediction of invertibility is noisy and inconsistent with the face semantic parts. Therefore, they adopt a seg-mentation model for refinement, but the invertibility in the same semantic area (e.g., occlusions on the face) could be inconsistent. Also, they manually design thresholds to fil-ter out the OOD areas, which is hard to optimize for small objects (Fig. 1 (d)).
In summary, existing invertibility estimation methods mainly adopt the reconstruction error as the reference to judge the OOD regions. However, they ignore that recon-struction errors also come from the In-Domain (ID) areas.
Consequently, their predicted mask is noisy and unreliable.
In this paper, we propose a novel strategy for photo-realistic GAN inversion by decomposing the input images into OOD and ID areas with invertibility masks. We fo-cus on the high-resolution (10242 pixels) GAN inversion on human face images and the downstream applications (e.g., attribute editing). Our basic idea is to reduce the recon-struction error of the ID areas and thus highlight the error of OOD regions. The reconstruction error of the ID area comes from both the textural and geometrical misalignment between the input image and the generated image. Although previous works improve the textural accuracy in the recon-struction by predicting or optimizing a better latent vector w, the geometrical misalignment is rarely discussed, which we believe is also important for invertibility estimation.
Hence, we design an invertibility detector learned with an optical flow prediction module to reduce the influence of ge-ometrical misalignment. The optical flow is computed be-tween the features of the encoder and the generator, which is then applied to warp the generated features to alleviate their misalignment with the input features. Compared with feature modulation [40, 28, 46], such warping will not break the fidelity of the generated textures. Along the training, the reconstruction error of the ID area will be minimized, and the invertibility mask prediction will be gradually focused on the OOD regions. The overall procedure needs no extra labels for the mask or flows.
Based on the invertibility prediction, we design an ef-fective approach to composite the generated content with the out-of-domain input feature for a photo-realistic gener-ation with high fidelity. Our framework consists of three major parts: the encoder, the Spatial Alignment and Mask-ing Module (SAMM), and the generator. First, we extract features from the input image and predict its latent vector with a pre-trained image-to-latent encoder [39, 4]. Second, we feed the latent vector into a pre-trained StyleGAN2 [23] model for content generation, acquiring generated features.
Third, we estimate the optical flow and the invertibility mask between the input and the generated features at multi-ple resolutions. Then, we warp the generated features with the flow, aiming to minimize the reconstruction error of ID regions. Finally, we composite the input image with the generated content according to the invertibility mask.
Since only the spatial operation, i.e., warping, is applied to the generated features, we maintain their editability with existing GAN editing methods. Combined with the artifact-free and precise inversion effects, our method has excellent superiority in reconstruction accuracy and editing fidelity over existing approaches.
In this paper, we adopt Style-GAN2 as the backbone for experiments, and extensive ex-perimental results demonstrate that our method outperforms current state-of-the-art methods with higher reconstruction fidelity and better visual quality.
In summary, our contributions are listed as follows:
1. We propose a novel framework for out-of-domain
GAN inversion on human face images by aligning and blending the generated image with the input image via optical flow and invertibility mask prediction. 2. We investigate the GAN invertibility with a novel Spa-tial Alignment and Masking Module, which is a new solution for invertibility decomposition. 3. Our proposed framework can produce photo-realistic results in both reconstruction and editing tasks. Exper-iments show that our framework outperforms existing methods in reconstruction accuracy and visual fidelity. 2.