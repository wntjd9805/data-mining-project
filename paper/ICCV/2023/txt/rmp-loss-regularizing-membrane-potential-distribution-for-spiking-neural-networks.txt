Abstract
Spiking Neural Networks (SNNs) as one of the biology-inspired models have received much attention recently. It can significantly reduce energy consumption since they quantize the real-valued membrane potentials to 0/1 spikes to transmit information thus the multiplications of activa-tions and weights can be replaced by additions when im-plemented on hardware. However, this quantization mecha-nism will inevitably introduce quantization error, thus caus-ing catastrophic information loss. To address the quanti-zation error problem, we propose a regularizing membrane potential loss (RMP-Loss) to adjust the distribution which is directly related to quantization error to a range close to the spikes. Our method is extremely simple to implement and straightforward to train an SNN. Furthermore, it is shown to consistently outperform previous state-of-the-art meth-ods over different network architectures and datasets. 1.

Introduction
Recently, many efforts have been done to make deep neural networks (DNNs) lightweight, so that they can be deployed in devices where energy consumption is lim-ited. To this end, several approaches have been pro-posed, including network pruning [61], network quantiza-tion [18, 37, 15], knowledge transfer/distillation [46], neu-ral architecture search [65, 39], and spiking neural networks (SNNs) [23, 19, 22, 17, 52, 36, 44, 55, 54, 63, 49, 56, 57].
The SNN provides a special way to reduce energy consump-tion following the working mechanism of the brain neuron.
Its neurons accumulate spikes from previous neurons and present spikes to posterior neurons when the membrane po-tential exceeds the firing threshold. This information trans-mission paradigm will convert the computationally expen-sive multiplication to computationally convenient additions
*Equal contribution.
†Corresponding author, mazhe thu@163.com. thus making SNNs energy-efficient when implemented on hardware. Specialized neuromorphic hardware based on an event-driven processing paradigm is currently under various stages of development, e.g., SpiNNaker [31], TrueNorth [1],
Tianjic [45], and Loihi [9], where SNNs can be efficiently implemented further. Due to the advantage of computa-tional efficiency and rapid development of neuromorphic hardware, the SNN has gained more and more attention.
Although SNNs have been widely studied, their perfor-mance is still not comparable with that of DNNs. This per-formance gap is largely related to the quantization of the real-valued membrane potential to 0/1 spikes for the firing of the SNN in implementation [21]. The excessive infor-mation loss induced by the firing activity forcing all infor-mation only to two values will cause accuracy to decrease.
Although information loss is important for computer vision tasks and the quantization will cause information loss too, the essential role of the activation function is to introduce non-linearity for neural networks [43]. Therefore, how to effectively reduce the information loss of membrane poten-tial quantization is of high research importance. However, as far as we know, few studies have focused on directly solv-ing this problem. The quantization error problem also exists in these methods that convert the ANN model to an SNN model [11, 4, 36]. However, these methods solve the prob-lem by changing the activation function in ANNs or increas-ing timesteps in SNNs, which don’t work for SNN training.
InfLoR-SNN [21] adds a membrane potential redistribution function in the spiking neuron to reduce the quantization er-ror via redistributing the membrane potential. However, it will decrease the biological plausibility of the SNN and in-crease the inference burden. This paper focuses on reducing the quantization error in SNN training directly and aims to introduce no burden for the SNN.
Quantization error is smaller when the membrane poten-tial is close to the spiking threshold or reset values [21].
Hence, to mitigate the information loss, we suggest redis-tributing the membrane potential to where the membrane potential is closer to the 0/1 spike. Then an additional
Figure 1: The overall workflow of the proposed method. We embed a membrane potential regularization loss in the task loss to redistribute the membrane potential in the training phase to reduce the quantization error. loss term aims at regularizing membrane potential is pre-sented, called RMP-Loss, which can encourage the mem-brane potentials to gather around binary spike values during the training phase. The workflow of our method is shown in
Fig. 1. Our main contributions can be concluded as follows:
• To our best knowledge, there have been few works noticing the quantization error in direct training of
SNNs. To mitigate the quantization error, we present the RMP-Loss, which is of benefit to training an SNN model that enjoys a narrow gap between the mem-brane potential and its corresponding 0/1 spike. Fur-thermore, we also provide theoretical proof to clarify why the RMP-Loss can prevent information loss.
• Some existing methods can address information loss too. While achieving comparable performance, more parameters or computation burdens are also introduced in the inference phase. Different from those meth-ods, the RMP-Loss can handle information loss di-rectly without introducing any additional parameters or inference burden.
• Extensive experiments on both static and dynamic datasets show that our method performs better than many state-of-the-art SNN models. 2.