Abstract
Transformer-based methods have swept the benchmarks on 2D and 3D detection on images. Because tokenization before the attention mechanism drops the spatial informa-tion, positional encoding becomes critical for those meth-ods. Recent works found that encodings based on samples of the 3D viewing rays can significantly improve the qual-ity of multi-camera 3D object detection. We hypothesize that 3D point locations can provide more information than rays. Therefore, we introduce 3D point positional encod-ing, 3DPPE, to the 3D detection Transformer decoder. Al-though 3D measurements are not available at the inference time of monocular 3D object detection, 3DPPE uses pre-dicted depth to approximate the real point positions. Our hybrid-depth module combines direct and categorical depth to estimate the refined depth of each pixel. Despite the ap-proximation, 3DPPE achieves 46.0 mAP and 51.4 NDS on the competitive nuScenes dataset, significantly outperform-ing encodings based on ray samples. The code is available at https://github.com/drilistbox/3DPPE. 1.

Introduction 3D object detection is a vital component of autonomous driving perception systems. Particularly, image-based 3D object detection has received increasing attention from both academia and industry due to its lower cost compared to LiDAR-dependent solutions. Despite the fact that au-tonomous driving vehicles are equipped with multiple cam-eras, early attempts at image-based 3D object detection, as seen in previous works [17, 19], focus on monocular detec-tion and combine the detection results from multiple cam-eras. This kind of solution is unable to make use of cor-respondence in the overlapping area of adjacent cameras, and the paradigm to individually detect objects in each view
*These authors contributed equally to this work.
†Corresponding authors.
Figure 1. An illustration of (a) 3D camera-ray positional encoding (PE) and (b) our proposed 3D point PE. The 3D camera-ray PE represents camera-ray information by determining the positions of a set number of discrete points along the direction from the camera optical center to the image plane pixel. This encoding approach is coarse-grained. On the other hand, the 3D point PE provides more precise position information by encoding the location of a single point with an estimated depth. In the figure, four pixels are randomly selected to demonstrate the methods. involves a large computational overhead. Alternatively, a group of recent studies [8, 7, 27, 29] follow the paradigm of
Lift-Splat-Shoot (LSS) [6] to first transform multi-camera images to unified bird-eye-view (BEV) representation in parallel and then perform object detection on the BEV rep-resentation. However, such ill-posed view transformation inevitably causes error accumulation, which further affects the accuracy of 3D object detection.
At the same time, the transformer-based (DETR-like) [1] scheme has also been explored in this field. Typically, the methods following this scheme [3, 25, 26, 28, 32] utilizes a set of learnable 3D object query to iteratively interact
with multi-view 2D features, and further perform 3D ob-ject detection without explicit view transformation. Within the transformer-based methods, there are two general ways to enable the interaction of 3D queries and 2D image fea-tures, i.e., projection-based and position-encoding-based.
The former one projects 3D queries into the 2D image plane [28, 32] for feature sampling, which requires extra deployment efforts. Moreover, such a sampling procedure only extracts local features, failing to make use of global coherence for improving 3D object detection. The other way, as first introduced in PETR [26], integrates the 3D in-formation into 2D image features by positional encoding.
With 3D positional encoding (PE), 2D image features can be directly exploited by 3D queries, without extra projec-tion efforts.
Enhancement of the 3D PE is anticipated to result in more precise 3D object detection. Despite effectiveness, the mechanism and design options of 3D PE in previous methods have not been fully explored. The typical 3D PE is the 3D camera-ray PE, as shown in Figure 1 (a). It en-codes the ray direction starting from the camera’s optical center to the pixel on the image plane. However, the ray direction only provides coarse localization information for the 2D image feature without the depth prior. Moreover, as the object query is embedded from the randomly initialized 3D reference point, the inconsistent embedding space for the reference point and camera-ray PE further hampers the effectiveness of the attention mechanism in the transformer decoder. Thus, reformulating a new 3D positional encoding with depth prior to localize the 2D feature and unify repre-sentation for both image feature and object query is still a legacy issue.
In this work, we explore an alternative 3D PE paradigm to ameliorate the aforementioned problem. Formally, we in-troduce 3D point positional encoding (3DPPE) to improve transformer-based multi-camera 3D object detection. As il-lustrated in Figure 1 (b), 3DPPE improves the camera-ray 3D PE by involving depth prior. Moreover, we find that 3D point PE not merely avoids the defects above, but also can provide better representative similarity (shown in Figure 6).
Specifically, in 3DPPE, we first devise a hybrid-depth mod-ule that combines direct and categorical ones to estimate the refined depth of each pixel. Then, we transform the pixels to 3D points via the camera parameters and predicted depth.
The resulting 3D points are sequentially sent to a position encoder for 3D point PE. Particularly, we exploit a shared position encoder for the transformed 3D points and refer-ence points to develop a unified embedding space.
We conduct extensive experiments to demonstrate the advantages of our proposed 3DPPE on challenging
NuScene benchmarks. With the proposed 3D point po-sitional encoding, our proposed 3DPPE can improve the camera-ray-based encoding by 1.9% mAP and 1.0% NDS. 2.