Abstract
Hierarchical clustering is a natural approach to discover ontologies from data. Yet, existing approaches are ham-pered by their inability to scale to large datasets and the dis-crete encoding of the hierarchy. We introduce scalable Hy-perbolic Hierarchical Clustering (sHHC) which overcomes these limitations by learning continuous hierarchies in hy-perbolic space. Our hierarchical clustering is of high qual-ity and can be obtained in a fraction of the runtime.
Additionally, we demonstrate the strength of sHHC on a downstream cross-modal self-supervision task. By us-ing the discovered hierarchies from sound and vision to construct continuous hierarchical pseudo-labels we can ef-ficiently optimize a network for activity recognition and obtain competitive performance compared to recent self-supervised learning models. Our findings demonstrate the strength of Hyperbolic Hierarchical Clustering and its po-tential for Self-Supervised Learning. 1.

Introduction
Concept hierarchies have been introduced for a variety of tasks including recognition [20, 7], retrieval [6], seg-mentation [30], fine-grained classification [15].
In many datasets, the hierarchy is manually defined [8] in terms of vision [11] or auditory [14] taxonomy. Those hierarchy definitions are subjective and may not suit domain-specific tasks. For example, vision hierarchy does not suit sound tasks and vice versa. In contrast to the abundance of appli-cations of pre-defined hierarchies, models discovering hier-archy from data [39, 31] are scarce and outdated. In this paper, we strive for discovering the audio-visual hierarchy from video data in a self-supervised manner.
By learning the concept hierarchy we can, for ex-ample, use it as a pseudo-labeling scheme for training.
Previous works on clustering-based self-supervised learn-ing [9, 3, 42, 35, 27] have shown the benefits of assign-ing pseudo-labels to guide training. Continuous pseudo-labeling [25, 44, 10], in particular, draws attention as it can further improve self-supervision. Despite the appeal of
Figure 1. Example clustering result. Our model subdivides “play-ing guitar” for the visual modality into sub-categories playing guitar in concert/at home and for the audio modality into play-ing classical/electronic guitar. (Left) The plain clustering in Eu-clidean space Rn with randomly distributed clusters, no subordi-nate relationship between sub-categories, and no corresponding (Right) The proposed hierarchical clustering in super-category. hyperbolic space Dn, similar clusters are grouped in the same fan-shaped zone, enabling us to subdivide sub-categories and aggre-gate super-categories. continuous labels, hierarchical clustering models are nat-urally discrete [18], limiting their power to enhance self-supervision. Existing approaches for encoding discrete hi-erarchy into a continuous form have led to a noticeable in-formation loss [38].
However, hyperbolic geometry [37] allows lossless em-bedding of hierarchical trees [34, 21] into hyperbolic space
Dn. Previous work explored how hyperbolic geometry helps hierarchical clustering [33, 12]. Unfortunately, the application of these techniques to self-supervised learning has been limited by the computational complexity, they can hardly scale to datasets with >∼20K data points. We solve this problem by breaking the hyperbolic clustering down into a two-stage process. The first stage, elaborated in Sec-tion 3.1, aims to generate K evenly distributed clusters. The second stage, described in Section 3.2 then aims to build a hierarchy from K clusters. This two-stage process enables us to efficiently train on large-scale video datasets, thereby
enabling self-supervised learning with hierarchical pseudo-labels.
Our contributions are threefold: Firstly, we discover au-dio hierarchies and video hierarchies from data completely unsupervised. Secondly, we reduce the computational com-plexity of hyperbolic hierarchical clustering to scale, while maintaining the hierarchy quality. Thirdly, our model not only outperforms plain clustering-based competitors but also achieves faster convergence in a self-supervised set-ting. 2.