Abstract
Recent video recognition models utilize Transformer mod-els for long-range spatio-temporal context modeling. Video transformer designs are based on self-attention that can model global context at a high computational cost.
In comparison, convolutional designs for videos offer an ef-ficient alternative but lack long-range dependency mod-eling. Towards achieving the best of both designs, this work proposes Video-FocalNet, an effective and efficient architecture for video recognition that models both local and global contexts. Video-FocalNet is based on a spatio-temporal focal modulation architecture that reverses the interaction and aggregation steps of self-attention for bet-ter efficiency. Further, the aggregation step and the in-teraction step are both implemented using efficient con-volution and element-wise multiplication operations that are computationally less expensive than their self-attention counterparts on video representations. We extensively ex-plore the design space of focal modulation-based spatio-temporal context modeling and demonstrate our parallel spatial and temporal encoding design to be the optimal choice. Video-FocalNets perform favorably well against the state-of-the-art transformer-based models for video recog-nition on five large-scale datasets (Kinetics-400, Kinetics-600, SS-v2, Diving-48, and ActivityNet-1.3) at a lower computational cost. Our code/models are released at https://github.com/TalalWasim/Video-FocalNets. 1.

Introduction
State-of-the-art video recognition methods have been sig-nificantly influenced by Convolutional Neural Networks (CNNs) since the introduction of Alexnet [36].
Initially 2D [30, 48, 55] and later 3D [7, 19, 63] CNNs achieved better performance on both small-scale [37, 57] and large-scale [6, 20, 31] video recognition benchmarks. With their
*Joint first authors. (cid:0) wasimtalal@gmail.com
Figure 1: Accuracy vs Computational Complexity trade-off comparison: We show the performance of Video-FocalNets against recent methods for video action recognition. Accu-racy is compared on the Kinetics-400 [31] dataset against
GFLOPs/view. Our Video-FocalNets perform favorably compared to their counterparts across a range of model sizes (Tiny, Small, and Base). local connectivity and translational equivariance properties,
CNNs have a better inductive bias especially useful for learn-ing on small datasets. However, CNNs are limited in their ability to model long-range dependencies due to their lim-ited receptive field. On the other hand, Vision Transform-ers (ViTs) [13] offer long-range context modeling and have been quite effective for image classification [13, 45, 46] and video recognition [2, 4, 47, 75]. ViTs are based on the self-attention [65] mechanism originally proposed in Natural
Language Processing (NLP) that encodes minimal inductive biases and can model both short- and long-range dependen-cies. This allows ViTs to better generalize to large datasets, as shown by recent results on major video recognition bench-marks [6, 20, 31] where they have out-performed their CNN counterparts. However, ViTs come at a high computational and parameter cost [76].
Video recognition requires both short-range and long-range spatio-temporal dependencies to be accurately mod-eled in order to achieve high performance. However, existing methods demonstrate a trade-off between efficiency and per-formance. While CNNs are more efficient and suited for short-range information modeling, they are limited in their representation learning capabilities for long-range dependen-cies and larger datasets. ViTs resolve these issues but at an increased parametric complexity and high computational cost. The high complexity originates from the dual-step self-attention operation that first performs a query-key inter-action, followed by an aggregation over the context values.
The query-key interaction requires the computationally ex-pensive step of calculating token-to-token attention scores via dot-product since the queries and keys do not contain information about the surrounding context (they are simply linear projections of the input tokens). In this context, this work seeks to optimize efficiency and performance while modeling both local and global contexts in videos.
We present an effective and efficient architecture for video recognition named Video-FocalNet (Fig. 1). Video-FocalNet proposes a spatio-temporal focal modulation architecture that reverses the steps of the self-attention operation for better efficiency. This architecture is inspired by focal mod-ulation [76] for image recognition and extends it to videos by independently aggregating the surrounding spatial and temporal context for each token into spatial and temporal modulators, followed by fusing them with the queries in the interaction step. The aggregation is based on a hierar-chical contextualization step using a stack of depthwise and pointwise convolutions for the spatial and temporal branches, respectively, followed by a gated aggregation that enables modeling both short- and long-range dependencies. The ag-gregation step (based on depthwise/pointwise convolutions) and the interaction step (based on element-wise multipli-cation) are both computationally less expensive than their self-attention counterparts i.e., query-key interactions and query-value aggregation via matrix multiplications.
We extensively explore various design configurations for optimal spatio-temporal context modeling with focal modu-lation. Our analysis shows that the proposed parallel spatial and temporal focal modulation design offers the best perfor-mance and is suitably efficient compared to other sequential designs. We introduce a family of Video-FocalNet architec-tures (tiny, small, and base) based on spatio-temporal focal modulation and demonstrate their favorable performance compared to state-of-the-art transformer-based methods on video recognition at a lower computational cost. Our major contributions are summarized as follows:
• We tackle the challenge of effective spatio-temporal modeling for video recognition. To solve this chal-lenge, we propose a video-focal modulation block that is able to use computationally efficient depthwise and pointwise convolutions through a hierarchical context aggregation design for local-global context modeling.
• We explore various design choices for spatio-temporal focal modulation and propose a parallel design for spa-tial and temporal encoding that optimizes for both per-formance and computation cost as shown in Fig. 5.
• We achieve state-of-the-art performance on three major benchmarks: Kinetics-400 [31], Kinetics-600 [5] and
Something-Something-v2 [20], surpassing comparable methods in literature by 0.6%, 1.2% and 0.6% respec-tively. Also, we outperform previous works on the rela-tively smaller Diving-48 [41] and ActivityNet-1.3 [24] datasets. We achieve an optimal trade-off between ac-curacy and computation cost as shown in Fig. 1. 2.