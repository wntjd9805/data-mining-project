Abstract
When applying a pre-trained 2D-to-3D human pose lift-ing model to a target unseen dataset, large performance degradation is commonly encountered due to domain shift issues. We observe that the degradation is caused by two factors: 1) the large distribution gap over global positions of poses between the source and target datasets due to vari-ant camera parameters and settings, and 2) the deficient di-versity of local structures of poses in training. To this end, we combine global adaptation and local generalization in
PoseDA, a simple yet effective framework of unsupervised domain adaptation for 3D human pose estimation. Specif-ically, global adaptation aims to align global positions of poses from the source domain to the target domain with a proposed global position alignment (GPA) module. And lo-cal generalization is designed to enhance the diversity of 2D-3D pose mapping with a local pose augmentation (LPA) module. These modules bring significant performance im-provement without introducing additional learnable param-eters.
In addition, we propose local pose augmentation (LPA) to enhance the diversity of 3D poses following an adversarial training scheme consisting of 1) a augmenta-tion generator that generates the parameters of pre-defined pose transformations and 2) an anchor discriminator to en-sure the reality and quality of the augmented data. Our ap-proach can be applicable to almost all 2D-3D lifting mod-els. PoseDA achieves 61.3 mm of MPJPE on MPI-INF-3DHP under a cross-dataset evaluation setup, improving upon the previous state-of-the-art method by 10.2%. 1.

Introduction 3D human pose estimation is an essential computer vi-sion task which aims to estimate the coordinates of 3D joints from single-frame images or videos. This task can be further used for several downstream tasks in multiple ob-ject tracking [1, 15, 13, 21], person re-identification [45], action recognition [44], robot [47], human body reconstruc-tion [14], sports application [64], etc. However, large-scale
Figure 1. PoseDA addresses 3D human pose domain adaptation problem through global adaptation and local generalization. The 2D poses from target dataset are used to guide the translation of 3D poses from source dataset. And the local root-relative poses also are augmented to achieve better generalization ability. 3D-annotated datasets are hard to obtain. Existing meth-ods are usually built on an off-the-shelf 2D pose estima-tors [9, 46] following two-stage schemes.
Deep learning methods [37, 41] have achieved great success in the mapping from 2D to 3D in the past decade. Despite their success in in-distribution data, these fully-supervised methods show poor performance in cross-dataset inference [18]. We argue that the real bottleneck lies in the domain gap of 3D pose data rather than the 2D-3D lifting network architecture or training strategy. Ex-isting datasets either lack enough diversity in laborato-rial environments [23] or lack adequate quantity and ac-curacy in the wild [38] due to the complex visual condi-tion [59, 57, 58, 34, 33, 24]. We model the pose domain gap in terms of global position and local pose separately shown in Figure 1. As for the global position, the camera intrinsic and extrinsic parameters are completely different in differ-ent datasets, resulting in performance degradation in cross-dataset evaluation. And for the local pose, the lack of action diversity also limits the generalization ability of the model.
Addressing the domain adaptation or generalization prob-lem is a crucial step for 3D human pose estimation to move from toy experiments to real-world applications. Recent 1
works focus more on enhancing the generalization ability of the 2D-3D lifting networks or set camera view prediction as an auxiliary task [52, 54] to address adaptation problems.
Some methods apply data augmentation in training images through image transformations [42, 39, 40] or human syn-thetics [8, 22, 49]. However, our proposed method does not rely on RGB or temporal information. Specifically, our method first generates transformed pose pairs from source dataset and then use them to train the 2D-3D lifting net-work, thus can fit any off-the-shelf model.
In this paper, we propose PoseDA, an unsupervised do-main adaptation framework for 3D human pose estima-tion. Our method only requires non-sequence 2D poses (not images) and camera intrinsic parameters in target dataset as well as a large-scale 3D-annotated human pose dataset (e.g., Human3.6M [23]). In real-world scenarios, obtaining prior knowledge of the camera intrinsic parameters is often not a concern. This is due to the fact that such informa-tion is readily available from camera specifications or can be inferred from the input images or videos alone [48]. The basic idea behind the proposed method is to combine global adaptation and local generalization to address the issue of unsupervised domain adaptation for 3D human pose esti-mation. Global adaptation aims to align global positions of poses from source domain to target domain, and local gener-alization aims to enhance the diversity of local structures of poses. Therefore, our proposed method applies global posi-tion alignment strictly but only enhances the diversity of lo-cal poses. To be specific, we take a sample from the source dataset and apply transformations in terms of bone angle, bone length, and rotation. We use an augmentation gen-erator to generate the parameters for these transformations and an anchor discriminator to ensure the realisticity and quality of these transformed pose pairs. As for the global position, we apply 2D global position alignment to ensure the alignment in both scales and 2D root positions between the projected 3D poses from the source domain with sam-pled 2D poses from the target domain. This process is solv-able through geometry constraints with no additional learn-able parameters. Finally, we use the transformed pose pairs to fine-tune the pre-trained 2D-3D lifting network and thus boost model performance on the target dataset without any use of 3D annotations. Our contributions are summarized as follows:
• We reduce the domain gap by separately applying global position alignment and local pose augmenta-tion, where two major domain gaps are effectively de-coupled.
• We align the global position through geometry con-straint with no additional learnable parameters, which can boost model performance significantly. And we apply local pose augmentation to enhance the diversity of local structures of 3D poses.
• Our approach is applicable to almost all 2D-3D lifting models. We achieve the state-of-the-art performance in
Human3.6M-3DHP cross-dataset evaluation with 61.3 mm of MPJPE. 2.