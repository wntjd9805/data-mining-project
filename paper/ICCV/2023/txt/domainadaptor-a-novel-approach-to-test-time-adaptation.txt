Abstract
To deal with the domain shift between training and test samples, current methods have primarily focused on learn-ing generalizable features during training and ignore the specificity of unseen samples that are also critical during the test. In this paper, we investigate a more challenging task that aims to adapt a trained CNN model to unseen do-mains during the test. To maximumly mine the information in the test data, we propose a unified method called Do-mainAdaptor for the test-time adaptation, which consists of an AdaMixBN module and a Generalized Entropy Mini-mization (GEM) loss. Specifically, AdaMixBN addresses the domain shift by adaptively fusing training and test statis-tics in the normalization layer via a dynamic mixture co-efficient and a statistic transformation operation. To fur-ther enhance the adaptation ability of AdaMixBN, we de-sign a GEM loss that extends the Entropy Minimization loss to better exploit the information in the test data. Extensive experiments show that DomainAdaptor consistently out-performs the state-of-the-art methods on four benchmarks.
Furthermore, our method brings more remarkable improve-ment against existing methods on the few-data unseen do-main. The code is available at https://github.com/ koncle/DomainAdaptor. 1.

Introduction
To overcome the domain shift (i.e., training source and test target data come from distinct domains, for deep learn-ing models), previous studies (e.g., unsupervised domain adaptation [32] or domain generalization [30]) have mainly focused on designing sophisticated models in the training stage. Despite their efforts, when a large domain gap exists in the test stage, they still inevitably suffer drastic perfor-mance degeneration. Given that abundant information ex-ists in the unlabeled unseen data during the test, which is ne-*Corresponding authors: Yinghuan Shi and Lei Qi.
Figure 1: Illustration of the problem of (1) inaccurate statis-tics estimation in BN and (2) small loss produced by en-tropy minimization for the test-time adaptation methods. glected when solely considering generalization in the train-ing phase, a more practical approach is to adapt a trained model to the unlabeled unseen data by incorporating this information during the test (i.e., test-time adaptation [43]).
Incorporating both source data and unlabeled target data can improve adaptation and enable a model to handle un-seen domains in real-world scenarios. However, this ap-proach is infeasible in practice due to the high computa-tional cost of processing these data during the test. Be-sides, data privacy is paramount in many real-world scenar-ios where only model weights are accessible (e.g., clinical data [19, 41] or commercial data [43]). These restrictions remind us to consider a more practical problem: adapt a trained CNN model to an arbitrarily unseen domain in the test time without access to the source data, namely fully test-time adaptation [43], to expand the generalization abil-ity of a trained model without bearing retraining costs.
Several methods [37, 43, 2] have been developed re-cently for fully test-time adaptation. One of the widely em-ployed adaptation paradigm [43, 31, 10, 1] to exploit the information of unseen samples is to finetune the Batch Nor-malization [16] layers in a trained model with an unsuper-vised loss, which is both simple and computationally effi-cient. However, most of these methods only show their ef-fectiveness on artificially corrupted datasets (e.g., CIFAR-10-C [14]), which is different from the real-world scene with diverse cross-domain styles in the domain generaliza-tion task. For instance, when we adapt the model trained by
photo data to the art painting data using existing methods, there still exist the following two issues, as shown in Fig. 1: (1) The success of current test-time adaptation methods relies on an accurate estimation of the normalization statis-tics, which is hard to achieve by solely employing the test statistics obtained from the limited unseen data with a large domain gap. We argue that the source statistics can help the estimation, which is neglected by previous methods [43]. (2) During adaptation, previous unsupervised losses [3] (e.g., Entropy Minimization (EM) loss) tend to bias the training procedure to the samples that have low confidence by producing large gradients and overlook the highly confi-dent samples that also can help the adaptation procedure.
Considering the above limitations of current methods, we present the DomainAdaptor, a novel approach for adapt-ing a pre-trained CNN model to unseen domains, which comprises an AdaMixBN module and a Generalized En-tropy Minimization (GEM) loss. The AdaMixBN module overcomes inaccurate estimation of test statistics by com-bining training and test statistics and adapting the mixture coefficient based on the current batch. However, directly finetuning AdaMixBN may lead to performance degrada-tion due to the weight mismatch problem caused by the combined source statistics after finetuning. To address this issue, we transform the source statistics into affine param-eters in the normalization layers before finetuning. This not only maintains the effectiveness of AdaMixBN but also eliminates the negative impact of mismatched source statis-tics. Moreover, the traditional Entropy Minimization (EM) loss is not effective for finetuning AdaMixBN due to the sharp probability distribution predicted by the model for confident samples. Hence, we propose a GEM loss that em-phasizes the role of temperature scaling in the traditional
EM loss. GEM loss softens the probability distribution of each sample with temperature, generating large gradients for confident samples and encouraging further learning.
The contributions of our proposed DomainAdaptor can be summarized as follows:
• We propose AdaMixBN to adaptively mix the training and test stats. in the transformed normalization layer, which can trade off the training and test information.
• To better exploit unlabeled test samples, we propose the Generalized Entropy Minimization loss to effec-tively optimize the parameters of AdaMixBN.
• Our proposed method exhibits significant improve-ment over existing approaches on four benchmark datasets for domain generalization. 2.