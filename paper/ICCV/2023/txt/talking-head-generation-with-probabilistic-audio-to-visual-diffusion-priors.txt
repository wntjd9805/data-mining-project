Abstract
We introduce a novel framework for one-shot audio-driven talking head generation. Unlike prior works that require additional driving sources for controlled synthesis in a deterministic manner, we instead sample all holistic lip-irrelevant facial motions (i.e. pose, expression, blink, gaze, etc.) to semantically match the input audio while still maintaining both the photo-realism of audio-lip synchro-nization and overall naturalness. This is achieved by our newly proposed audio-to-visual diffusion prior trained on top of the mapping between audio and non-lip representa-tions. Thanks to the probabilistic nature of the diffusion prior, one big advantage of our framework is it can synthe-*These authors have contributed equally to this work.
†This work was done when Zixin Yin and Deyu Zhou were interns at
XiaoBing.AI.
‡Corresponding author. size diverse facial motion sequences given the same audio clip, which is quite user-friendly for many real applications.
Through comprehensive evaluations of public benchmarks, we conclude that (1) our diffusion prior outperforms auto-regressive prior significantly on all the concerned metrics; (2) our overall system is competitive with prior works in terms of audio-lip synchronization but can effectively sam-ple rich and natural-looking lip-irrelevant facial motions while still semantically harmonized with the audio input. 1.

Introduction
Audio-driven face reenactment and talking head genera-tion have received raising attention due to the broad killer applications in movie production, gaming, virtual digital avatars, and potentially more while we move toward the era
of the metaverse. The past literature tries to advance this area from various perspectives. One line of research fo-cuses on improving the generation quality originating from regular GAN-based methods [64, 65] to pre-trained Style-GAN [3] and to the most recent Neural Radiance Field (NeRF) based methods [13, 42]. Orthogonal to direction, another line of works emphasizes the importance of disen-tangled representation from the audio [32, 21, 66] for con-trolled generations. i.e., [21] can predict the emotion from the audio input while [66] can decouple the speech signal into speaker identity and the phonetic content. A closely re-lated set of works targeting more granular controlled talking head generation by providing additional input signals. For example, PC-AVS [65] requires a separate video to provide the pose signal in addition to the audio clip but does not sup-port the control of expression and blinking. To remedy this,
GC-AVT [27] requires inputting additional driving video for expression in addition to pose and audio driving clip.
The technical challenge behind those approaches is how to faithfully transfer the desired driving signal (i.e., pose, ex-pression, audio) into the results without affecting each other through intrinsic disentanglement. Although encouraging results have been reported, we argue such a setting is not practical for broader applications. It is quite challenging, or at least labor-intensive, for novice users to find the “best” individual driving sources for each controlled dimension to make the final talking head video overall look not only life-like but also coherent from semantic and emotional perspec-tives. Therefore, it is both more practical and generalizable if the setting only requires an audio signal as the driving source and expects a data-driven model to sample reason-able other facial motions irrelevant to lips.
By no means, we are the first to advocate an audio-only driving setup for talking head generation. Audio2head [54] predicts the poses from audio input with an LSTM net-work. [29] employs an autoregressive model by assuming the poses are jointly determined by the audio and past head motions along the sequence, although the results are en-couraging, the model can’t generalize due to their person-specific training strategy. FACIAL [61] could infer the blink, but requires a reference video input rather than a one-shot reference image. There are other related works along this direction, but they either only infer one facial attribute (i.e., emotion in EVP[21], pose from [54]) using ad-hoc methods, or they simply do not support inferring other facial motions [38], or treat it as a block-box mapping model [3] without explicitly respecting the one-to-many mapping na-ture between audio and other visual facial attributes (pose, expression, blink). A more principled solution is desired to consolidate this line of work.
In this paper, we introduce a novel framework that can holistically infer all the non-lip-related facial attributes from the audio input while maintaining accurate synchronization between audio and the corresponding visual lip motions.
This is achieved by two important learning steps in our pipeline. (1) A pre-trained identity-irrelevant facial motion representation can help decouple the lip and non-lip repre-sentations. To promote this learning, we employ a novel orthogonal loss on top of the modified facial reenactment framework [6]. The disentanglement enables to generate one-to-one mapping with lip representations to ensure the synchronization, and generate richer non-lip representations with a one-to-many mapping. (2) A novel audio-to-visual diffusion prior model is introduced to address the proba-bilistic sampling from audio representations to the above-learned non-lip representation. This prior is expected to solve the one-to-many mapping and provide diverse results during the inference stage. The entire pipeline can be eas-ily built up on top of the existing framework, such as PC-AVS[65] without heavily retraining every component. To sum up, we make the following contributions:
• To our best knowledge, we are the first to holistically pre-dict non-lip facial motions based on audio input only, pro-viding good usability for reenactment or dubbing appli-cations without extra driving video sources. Our method addresses the intrinsic one-to-many challenge in a prob-abilistic way, allowing diverse and realistic facial motion generation under the same audio input, as shown in Fig. 1.
• We leverage the pre-trained visual identity-irrelevant fa-cial motion representations. And further, learn disentan-gled lip-related and lip-irrelevant representations through a novel orthogonal loss on top of PC-AVS[65]. A pow-erful diffusion prior model is then introduced to effec-tively infer all lip-irrelevant facial motions for a given au-dio segment in the representation space.
• We systematically evaluate the naturalness and diversity of the results with new metrics, which paves a way for future studies. Meanwhile, results show that our method can produce natural-looking head poses and facial mo-tions without hurting the audio-lip synchronization. Our model will be released. 2.