Abstract
Deep neural networks are susceptible to adversarial ex-amples, posing a signiﬁcant security risk in critical applica-tions. Adversarial Training (AT) is a well-established tech-nique to enhance adversarial robustness, but it often comes at the cost of decreased generalization ability. This paper proposes Robustness Critical Fine-Tuning (RiFT), a novel approach to enhance generalization without compromising adversarial robustness. The core idea of RiFT is to exploit the redundant capacity for robustness by ﬁne-tuning the ad-versarially trained model on its non-robust-critical module.
To do so, we introduce module robust criticality (MRC), a measure that evaluates the signiﬁcance of a given mod-ule to model robustness under worst-case weight perturba-tions. Using this measure, we identify the module with the lowest MRC value as the non-robust-critical module and
ﬁne-tune its weights to obtain ﬁne-tuned weights. Subse-quently, we linearly interpolate between the adversarially trained weights and ﬁne-tuned weights to derive the optimal
ﬁne-tuned model weights. We demonstrate the efﬁcacy of
RiFT on ResNet18, ResNet34, and WideResNet34-10 mod-els trained on CIFAR10, CIFAR100, and Tiny-ImageNet datasets. Our experiments show that RiFT can signiﬁcantly improve both generalization and out-of-distribution robust-ness by around 1.5% while maintaining or even slightly enhancing adversarial robustness. Code is available at https://github.com/Immortalise/RiFT. 1.

Introduction
The pursuit of accurate and trustworthy artiﬁcial intelli-gence systems is a fundamental objective in the deep learn-ing community. Adversarial examples [45, 15], which per-turbs input by a small, human imperceptible noise that
*Corresponding author.
Figure 1. Interpolation results of ﬁne-tuning on different modules of ResNet18 on CIFAR10 dataset. Dots denote different inter-polation points between the ﬁnal ﬁne-tuned weights of RiFT and the initial adversarially trained weights. All ﬁne-tuning meth-ods improve the generalization ability, but only ﬁne-tuning on the non-robust-critical module (layer2.1.conv2 in Figure 3) can preserve robustness. Additionally, ﬁne-tuning on robust-critical module (layer4.1.conv1) causes the worst trade-off between
In the initial interpolation stage, generalization and robustness.
ﬁne-tuning on non-robust-critical modules enhances adversarial robustness by around 0.3%. can cause deep neural networks to make incorrect predic-tions, pose a signiﬁcant threat to the security of AI sys-tems. Notable experimental and theoretical progress has been made in defending against such adversarial examples
[6, 4, 10, 19, 11, 16, 37]. Among various defense methods
[52, 33, 57, 31, 8], adversarial training (AT) [29] has been shown to be one of the most promising approaches [4, 11] to enhance the adversarial robustness. However, compared to standard training, AT severely sacriﬁces generalization on in-distribution data [42, 46, 58, 36, 32] and is exception-ally vulnerable to certain out-of-distribution (OOD) exam-ples [14, 53, 22] such as Contrast, Bright and Fog, resulting in unsatisfactory performance.
Prior studies tend to mitigate the trade-off between gen-eralization and adversarial robustness within the adversar-ial training procedure. For example, some approaches have explored reweighting instances [59], using unlabeled data
[36], or redeﬁning the robust loss function [58, 48, 50, 32].
In this paper, we take a different perspective to address such a trade-off by leveraging the redundant capacity for robust-ness of neural networks after adversarial training. Recent research has demonstrated that deep neural networks can exhibit redundant capacity for generalization due to their complex and opaque nature, where speciﬁc network mod-ules can be deleted, permuted [47], or reset to their initial values [55, 9] with only minor degradation in generalization performance. Hence, it is intuitive to ask: Do adversarially trained models have such redundant capacity? If so, how to leverage it to improve the generalization and OOD robust-ness 1 while maintaining adversarial robustness?
Based on such motivation, we introduce a new con-cept called Module Robust Criticality (MRC) 2 to investi-gate the redundant capacity of adversarially trained models for robustness. MRC aims to quantify the maximum in-crease of robustness loss of a module’s parameters under the constrained weight perturbation. As illustrated in Fig-ure 3, we empirically ﬁnd that certain modules exhibit re-dundant characteristics under such perturbations, resulting in negligible drops in adversarial robustness. We refer to the modules with the lowest MRC value as the non-robust-critical modules. These ﬁndings further inspire us to pro-pose a novel ﬁne-tuning technique called Robust Critical
Fine-Tuning (RiFT), which aims to leverage the redundant capacity of the non-robust-critical module to improve gen-eralization while maintaining adversarial robustness. RiFT consists of three steps: (1) Module robust criticality charac-terization, which calculates the MRC value for each mod-ule and identiﬁes the non-robust-critical module. (2) Non-robust-critical module ﬁne-tuning, which exploits the re-dundant capacity of the non-robust-critical module via ﬁne-tuning its weights with standard examples. (3) Mitigating robustness-generalization trade-off via interpolation, which interpolates between adversarially trained parameters and
ﬁne-tuned parameters to ﬁnd the best weights that maxi-mize the improvement in generalization while preserving adversarial robustness.
Experimental results demonstrate that RiFT signiﬁcantly improves both the generalization performance and OOD robustness by around 2% while maintaining or even im-proving the adversarial robustness of the original models.
Furthermore, we also incorporate RiFT to other adversar-1Here, generalization refers to generalization to in-distribution (ID) samples, and OOD robustness refers to generalization to OOD samples. 2In our paper, a module refers to a layer of the neural network. ial training regimes such as TRADES [58], MART [48],
AT-AWP [50], and SCORE [32], and show that such incor-poration leads to further enhancements. More importantly, our experiments reveal several insights. First, we found that ﬁne-tuning on non-robust-critical modules can effec-tively mitigate the trade-off between adversarial robustness and generalization, showing that these two can both be im-proved (Section 5.3). As illustrated in Figure 1, adver-sarial robustness increases alongside the generalization in the initial interpolation procedure, indicating that the fea-tures learned by ﬁne-tuning can beneﬁt both generaliza-tion and adversarial robustness. This contradicts the pre-vious claim [46] that the features learned by optimal stan-dard and robust classiﬁers are fundamentally different. Sec-ond, the existence of non-robust-critical modules suggests that current adversarial training regimes do not fully uti-lize the capacity of DNNs (Section 5.2). This motivates future work to design more efﬁcient adversarial training ap-proaches using such capacity. Third, while previous study
[25] reported that ﬁne-tuning on pre-train models could dis-tort the learned robust features and result in poor perfor-mance on OOD samples, we ﬁnd that ﬁne-tuning adversar-ially trained models do NOT lead to worse OOD perfor-mance (Section 5.3).
The contribution of this work is summarized as follows: 1. We propose the concept of module robust criticality and verify the existence of redundant capacity for ro-bustness in adversarially trained models. We then pro-pose RiFT to exploit such redundancy to improve the generalization of AT models. 2. Our approach improves both generalization and OOD robustness of AT models. It can also be incorporated with previous AT methods to mitigate the trade-off be-tween generalization and adversarial robustness. 3. The ﬁndings of our experiments shed light on the intri-cate interplay between generalization, adversarial ro-bustness, and OOD robustness. Our work emphasizes the potential of leveraging the redundant capacity in
AT models to improve generalization and robustness further, which may motivate more effective adversar-ial training methods. 2.