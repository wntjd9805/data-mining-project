Abstract
Image colorization is a challenging problem due to multi-modal uncertainty and high ill-posedness. Directly training a deep neural network usually leads to incorrect semantic colors and low color richness. While transformer-based methods can deliver better results, they often rely on manually designed priors, suffer from poor generaliza-tion ability, and introduce color bleeding effects. To ad-dress these issues, we propose DDColor, an end-to-end method with dual decoders for image colorization. Our ap-proach includes a pixel decoder and a query-based color decoder. The former restores the spatial resolution of the image, while the latter utilizes rich visual features to refine color queries, thus avoiding hand-crafted priors. Our two decoders work together to establish correlations between color and multi-scale semantic representations via cross-attention, significantly alleviating the color bleeding effect.
Additionally, a simple yet effective colorfulness loss is in-troduced to enhance the color richness. Extensive experi-ments demonstrate that DDColor achieves superior perfor-mance to existing state-of-the-art works both quantitatively and qualitatively. The codes and models are publicly avail-able at https://github.com/piddnad/DDColor. 1.

Introduction
Image colorization is a classic computer vision task and has great potential in many real-world applications, such as legacy photo restoration [41], video remastering [21] and art creation [35], etc. Given a grayscale image, colorization aims to recover its two missing color channels, which is highly ill-posed and usually suffers from multi-modal un-certainty, e.g., an object may have multiple plausible col-ors. Traditional colorization methods address this problem mainly based on user guidance such as reference images
[44, 22, 14, 27, 9] and color graffiti [25, 48, 35, 32]. Al-though great progress has been made, it remains a challeng-ing research problem.
With the rise of deep learning, automatic colorization has drawn a lot of attention, targeting at producing appropriate colors from complex image semantics (e.g., shape, texture,
and context). Some early methods [8, 10, 49, 39, 1] attempt to predict per-pixel color distributions using convolutional neural networks (CNNs). Unfortunately, these CNN-based methods often yield incorrect or unsaturated colorization results due to the lack of a comprehensive understanding of image semantics (Figure 1 CIC [49], InstColor [39] and
DeOldify [1]). In order to embrace semantic information, some methods [46, 13] resort to generative adversarial net-works (GANs) and utilize their rich representations as gen-erative priors for colorization. However, due to the limited representation space of GAN prior, they fail to handle im-ages with complex structures and semantics, resulting in in-appropriate colorization results or unpleasant artifacts (Fig-ure 1 Wu et al. [46] and BigColor [13]).
With the tremendous success in natural language pro-cessing (NLP), Transformer [42] has been extended to many computer vision tasks. Recently, some works [24, 45, 47] introduce the non-local attention mechanism of transformer to image colorization. Though achieving promising re-sults, these methods either train several independent sub-nets, leading to accumulated error (Figure 1 ColTran [24]), or perform color attention operations on single-scale im-age feature maps, causing visible color bleeding when tack-ling complex image contexts (Figure 1 CT2 [45] and Color-Former [47]). In addition, these methods often rely on hand-crafted dataset-level empirical distribution priors, such as color masks in [45] and semantic-color mappings in [47], which are cumbersome and difficult to generalize.
In this paper, we propose an novel colorization method, namely DDColor, targeting at achieving semantically rea-sonable and visually vivid colorization. Our approach uti-lizes an encoder-decoder structure where the encoder ex-tracts image features and the dual decoders restore spatial resolution. Unlike previous methods that optimize color likelihood resorting to an extra network or manually cal-culated priors, our method uses a query-based transformer as color decoder to learn semantic-aware color queries in an end-to-end way. By using multi-scale image features to learn color queries, our method alleviates color bleed-ing and improves the colorization of complex contexts and small objects significantly (see Figure 1). Over and above this, we present a new colorfulness loss to improve the color richness of generated results.
We validate the performance of our model on pub-lic benchmarks ImageNet [36] and conduct ablations to demonstrate the advantages of our framework. The visu-alization results and evaluation metrics show that our work achieves significant improvements to previous state-of-the-art methods in terms of semantic consistency, color rich-ness, etc. Furthermore, we test our model on two additional datasets (COCO-Stuff [4], and ADE20k [53]) without fine-tuning and achieve best performance among all baselines, demonstrating its generalization ability.
Our key contributions are summarized as follow:
• We propose an end-to-end network with dual decoders for automatic image colorization, which ensures vivid and semantically consistent results.
• Our method includes a novel color decoder that learns color queries from visual features without relying on hand-crafted priors. Additionally, our pixel decoder provides multi-scale semantic representations to guide the optimization of color queries, which effectively re-duces the color bleeding effect.
• Comprehensive experiments demonstrate that our method achieves state-of-the-art performance and ex-hibits good generalization compared to the baselines. 2.