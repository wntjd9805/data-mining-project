Abstract
Interpretability of Deep Learning (DL) is a barrier to trustworthy AI. Despite great efforts made by the Explain-able AI (XAI) community, explanations lack robustness— indistinguishable input perturbations may lead to different
XAI results. Thus, it is vital to assess how robust DL inter-pretability is, given an XAI method. In this paper, we iden-tify several challenges that the state-of-the-art is unable to cope with collectively: i) existing metrics are not compre-hensive; ii) XAI techniques are highly heterogeneous; iii) misinterpretations are normally rare events. To tackle these challenges, we introduce two black-box evaluation meth-ods, concerning the worst-case interpretation discrepancy and a probabilistic notion of how robust in general, respec-tively. Genetic Algorithm (GA) with bespoke fitness function is used to solve constrained optimisation for efficient worst-case evaluation. Subset Simulation (SS), dedicated to esti-mate rare event probabilities, is used for evaluating overall robustness. Experiments show that the accuracy, sensitiv-ity, and efficiency of our methods outperform the state-of-the-arts. Finally, we demonstrate two applications of our methods: ranking robust XAI methods and selecting train-ing schemes to improve both classification and interpreta-tion robustness. 1.

Introduction
A key impediment to the wide adoption of Deep Learn-ing (DL) is its perceived lack of transparency. Explainable
AI (XAI) is a research area that aims at providing the vis-ibility into how a DL model makes decisions, and thus en-ables the use of DL in vision-based safety critical applica-tions, such as autonomous driving [32], and medical image analysis [43]. Typically, XAI techniques visualise which input features are significant to the DL model’s prediction via attribution maps [4, 21]. However, interpretations1 suf-fer from the lack of robustness. Many works have shown 1Despite the subtle difference between interpretability and explain-ability, we use both terms interchangeably as attributes of DL models in that a small perturbation can manipulate the interpretation while keeping model’s prediction unchanged, e.g., [19, 25].
Moreover, there exists the misinterpretation of Adversarial
Examples (AEs) [50], i.e., adversarial inputs are misclassi-fied2 by the DL model, but interpreted highly similarly to the benign counterparts. Fig. 1 illustrates examples of the aforementioned two types of misinterpretations. In this re-gard, it is vital to assess how robust the coupled DL model and XAI method are against input perturbations, which mo-tivates this work.
Figure 1: Two types of misinterpretations after perturbation
To answer the question, the first challenge we recognise is the lack of diverse evaluation metrics from the state-of-the-art. Most of the existing works focus on adversarial at-tack [20] and defence [16, 42] on explanations, which es-sentially answer the binary question of whether there ex-ist any adversarial interpretation in some perturbation dis-tance. On the other hand, evaluation methods mainly study worst-case metrics, e.g., the maximum change in the result-ing explanations when perturbations are made [3] and local this paper. However, as suggested in [31], we use the terms explana-tion/interpretation specifically for individual predictions. 2Without loss of generality, in this paper we assume the DL model is a classifier if with no further clarification.
Lipschitz continuity as the sensitivity to perturbations [48].
However, for systematic evaluation, we also need a notion of how robust in general the model is whenever a misin-terpretation can be found (in line with the insight gained from evaluating classification robustness [45]). We intro-duce two metrics concerning the worst-case interpretation discrepancy and a probabilistic metric to calculate the pro-portion of misinterpretations in the local norm-ball around the original input, that complement each other from differ-ent perspectives.
Second, XAI techniques are so heterogeneous that no ex-isting white-box evaluation methods are generic enough to be applicable to all common ones. That said, black-box methods, that only access inputs and outputs of the cou-pled DL model and XAI tool without requiring any inter-nal information, are promising for all kinds of XAI tech-niques (including perturbation-based ones that are missing from current literature). Based on this insight, we design a
Genetic Algorithm (GA) and a statistical Subset Simulation (SS) approach to estimate the aforementioned two robust-ness metrics, both of which are of black-box nature.
The third challenge we identified is that misinterpreta-tions are normally rare events in a local norm-ball. With-out white-box information like gradients, black-box meth-ods have to leverage auxiliary information to detect such rare events efficiently. To this end, we design bespoke fit-ness functions in the GA (when solving the optimisation) and retrofit the established SS (dedicated to estimating rare event probabilities [6]) for efficient evaluation.
To the best of our knowledge, no state-of-the-art methods can collectively cope with the three pinpointed challenges like ours. To validate this claim, we conduct experiments to study the accuracy, sensitivity, and efficiency of our meth-ods. Additionally, we develop two practical applications of our methods: i) We evaluate a wide range of XAI tech-niques and gain insights that no XAI technique is superior in terms of robustness to both types of adversarial attacks. ii) We discover a strong correlation between classification robustness and interpretation robustness through theoretical analysis (see Appx.) and empirical studies. We also identi-fied the best training scheme to improve both aspects.
In summary, the key contributions of this paper include:
• Two diverse metrics, worst-case interpretation discrep-ancy and probabilistic interpretation robustness, com-plement each other as a versatile approach, allowing for a holistic evaluation of interpretation robustness.
• We introduce new methods based on GA and SS to estimate these two metrics. These methods are black-box and thus applicable to diverse XAI tools, enabling robustness evaluation of perturbation-based XAI tech-niques for the first time. Despite the rare occurrence of misinterpretations, our GA and SS algorithms effi-ciently detect them.
• We demonstrate two practical applications of our methods: ranking robust XAI techniques and selecting training schemes to improve both classification and in-terpretation robustness. 2.