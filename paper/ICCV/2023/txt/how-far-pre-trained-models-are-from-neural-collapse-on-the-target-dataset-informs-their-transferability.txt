Abstract
This paper focuses on model transferability estimation, i.e., assessing the performance of pre-trained models on a downstream task without performing fine-tuning. Moti-vated by neural collapse (NC) [25] that reveals particular feature geometry at the terminal stage of training, we con-sider model transferability as how far the target activations obtained by pre-trained models are from their hypothetical state in the terminal phase of the model fine-tuned on the target domain. We propose a metric that measures this prox-imity based on three phenomena of NC: within-class vari-ability collapse, simplex encoded label interpolation geom-etry structure is formed, and the nearest center classifier becomes optimal on training data. Through experiments on 11 datasets, we confirm none of the three NC proxies are dispensable, which allows us to obtain very competi-tive transferability estimation accuracy with approximately 10× wall-clock time speed up compared to state-of-the-art approaches. 1.

Introduction
The past decade has witnessed a surge in the availabil-ity of off-the-shelf pre-trained models in public reposito-ries. They are adapted and re-tuned to facilitate new tasks, which has become a standard practice [12]. Nevertheless, this approach raises an important practical question of how to effectively select the best pre-trained model from a large model pool to be carried onto a downstream task.
While precise ranking of these pre-trained models on a new task can be obtained by a greedy approach of fine-turning each model and comparing the test accuracy, it is often prohibitive as it requires a vast amount of com-puting resources. Therefore, an early line of research has been developed referred to as transferability estima-tion [22, 41, 24, 29], where the aim is to design efficient methods for ranking the performances of pre-trained mod-els on a downstream task without fine-tuning. Existing
Figure 1: Illustration of the transferability estimation prob-lem. Given a pool of pre-trained models and labeled target data, the objective of transferability estimation is to predict which models can achieve higher performance on the target data after fine-tuning. This task is much more time efficient than fine-tuning the models to obtain ground truth rankings. approaches in transferability estimation typically consider the current status of the pre-trained models (w.r.t the target task) and can be generally categorized into two streams of probabilistic-based [22, 36, 41, 1] and feature distribution-based techniques [24, 29]. Approaches from the former category model the expected conditional probability of tar-get labels given the extracted target features [41, 20] or pre-dictions [36, 22]. The latter category models transferability as the class-wise separation of features, in which Pandey et al. [24] employed the Bhattacharyya coefficient to approx-imate the overlap between class distributions.
From a different viewpoint for transferability estimation, this paper measures the difference between the current sta-tus and the hypothetical terminal status of pre-trained mod-els after fine-tuning. The latter status refers to neural col-lapse (NC) [25, 33] on the target domain, which commonly occurs when the training loss approaches zero. Specifically, there are three distinct characteristics of NC: 1) the within-class feature variability drops towards zero; 2) class means gradually converge to a simplex encoded label interpolation
(SELI) geometry (def. in Sec.3); and 3) the behavior of the final classifier layer is similar to the nearest centroid classi-fier. Meanwhile, it is empirically demonstrated in [25] that stopping training at the terminal phase achieves higher test performance compared to the models that stop at an earlier stage (i.e., zero prediction error).
Motivated by these interesting findings, we formalize a transferability metric based on the observations of NC dis-cussed above. The metric measures how close the geome-try of the target features is to their hypothetical state in the terminal stage of the fine-tuned model. Specifically, we an-alyze the spectral property of the class-wise feature matrix to understand to what extent the current status is from zero within-class variation. Secondly, we assess the gap between the features extracted by the pre-trained model and SELI geometry with the rank of the feature matrix. Thirdly, we calculate the difference between the hypothetical classifier the empiri-layer and the nearest neighbor classifier w.r.t. cal class-conditional distribution. Collectively, these three different measurements allow us to understand how far the pre-trained models are from NC on the target domain. On a series of standard benchmarks, we demonstrate that the metric exhibits a strong correlation with the transferability of supervised and self-supervised pre-trained models and thus is a very useful transferability estimator. Compared with the state-of-the-art, our method gives stronger correla-tions while being computationally efficient. We summarize our main contributions below.
• We propose the neural collapse transferability index (NCTI) for the transferability estimation. NCTI approxi-mately measures the model w.r.t the three prominent char-acteristics of NC, which can be efficiently computed.
• Experimental results on 11 datasets demonstrate strong correlations between NCTI and the transferability of models to the target dataset, which outperforms existing transferability estimation methods.
• We give interesting insights when the NC components are computed on a subset of the pre-training dataset. Our findings suggest a potential for an unsupervised trans-ferability estimation metric, solely based on measuring within-class variability collapse of the source features. 2.