Abstract
Video analysis tasks rely heavily on identifying the pix-els from different frames that correspond to the same visual target. To tackle this problem, recent studies have advo-cated feature learning methods that aim to learn distinc-tive representations to match the pixels, especially in a self-supervised fashion. Unfortunately, these methods have dif-ﬁculties for tiny or even single-pixel visual targets. Pixel-wise video correspondences were traditionally related to optical ﬂows, which however lead to deterministic corre-spondences and lack robustness on real-world videos. We address the problem of learning features for establishing pixel-wise correspondences. Motivated by optical ﬂows as well as the self-supervised feature learning, we propose to use not only labeled synthetic videos but also unlabeled real-world videos for learning ﬁne-grained representations in a holistic framework. We adopt an adversarial learning scheme to enhance the generalization ability of the learned features. Moreover, we design a coarse-to-ﬁne framework to pursue high computational efﬁciency. Our experimental results on a series of correspondence-based tasks demon-strate that the proposed method outperforms state-of-the-art rivals in both accuracy and efﬁciency. 1.

Introduction
One of the most fundamental problems in computer vi-sion is learning visual correspondences across space and time, which has many applications such as 3D reconstruc-tion, physical understanding, and dynamic object model-ing. Due to the factors such as viewpoint change, distrac-tors, and deformations, this task is extremely challenging and can be roughly divided into three categories accord-This work was supported by the Natural Science Foundation of China under Grants 62022075 and 62036005, and by the Fundamental Research
Funds for the Central Universities under Grant WK3490000006. This work was also supported by the advanced computing resources provided by the Supercomputing Center of USTC. (Corresponding author: Dong
Liu.)
C o a r s e
F i n e
Figure 1: We illustrate video correspondences with different granularities, including object-wise, group-wise, and pixel-wise.
In this paper, we concentrate on learning ﬁne-grained features to address the pixel-wise video correspondences. the ﬁrst one is object-wise corre-ing to the granularity: spondences that exist between coarsely localized bounding boxes [36, 39] along the video; the second one is group-wise correspondences, indicating the mapping at group-level, and usually applied to downstream tasks like video object segmentation [5, 25]; the third one is pixel-wise cor-respondences, which describe the pixel-level relation be-tween video frames with the ﬁnest granularity.
Learning dense representations from videos is one ap-proach to ﬁnding the correspondences. Researchers have been exploring different self-supervised methods for learn-ing generalizable representations using unlabeled videos collected in the real world [16, 20, 23, 24, 41, 45]. For exam-ple, Wang et al. [41] propose using an object-level cycle-consistency across time (i.e., forward-backward tracking) as a supervision signal. Jabri et al. [16] further enhance it by combining cycles of time with the similarities between path-level representations. Inspired by contrastive learning,
Xu et al. [45] try to learn spatial and temporal represen-tation through a frame-wise contrastive loss, while Li et al. [23] propose a spatial-then-temporal pretext task to learn better spatiotemporal features. Despite obtaining promising outcomes, current research has predominantly emphasized performing object-level or patch-level similarity learning, making it difﬁcult to accurately recognize pixel-wise differ-ences with the learned features. As a result, there is an in-creasing necessity for learning ﬁne-grained representations in order to tackle this problem. good balance of performance and efﬁciency. In summary, the main contribution of this work lies in:
At the same time, there is another line of work approach-ing video correspondences by deterministically predicting the displacement of each pixel, which is known as optical
ﬂow estimation. Early studies used optimization methods to estimate the motion between two frames [4]. In recent years, approaches use synthetic data with supervised learn-ing for ﬂow estimation [8, 26], using a coarse-to-ﬁne pyra-mid framework to improve the accuracy [33]. RAFT [34] further devises an iterative optimization algorithm to come up with the result of high-resolution ﬂow ﬁelds through it-erative updates, which show a superior ability to ﬁnd ﬁne-grained correspondences. However, in real scenarios, there are often appearance variants, illumination changes, and de-formations between video frames, which leads to the lack of robustness on real-world videos for the optical ﬂow model supervised by labeled synthetic videos.
In this paper, we explore how to learn ﬁne-grained repre-sentations to meet the needs of pixel-wise video correspon-dences. To this end, we ﬁrst investigate how to leverage syn-thetic data for ﬁne-grained feature learning. Speciﬁcally, given a query pixel, the supervision in synthetic videos only supplies the one-to-one mapping, i.e., a motion vector, rep-resenting the deterministic correspondence of the pixel to another pixel in the next frame. Nevertheless, the pixel-wise features evolve slowly over space and time, which indicates a soft distribution of the correspondences. We ﬁnd directly utilizing the synthetic supervision as hard labels results in inferior representations, and the learned features are unable to recognize the pixel-wise differences across different spa-tial locations and periods of time. To address the issue, we propose to use an external pre-trained 2D encoder to derive soft supervision for optimization based on the ﬂow.
Furthermore, we incorporate self-supervised feature learning on unlabeled real data in the overall training to alle-viate the generalization issues in real scenes, which consists of two carefully designed components. Firstly, inspired by the temporal consistency assumption [3], we learn temporal persistent features via self-supervised reconstructive learn-ing, where each query pixel can be reconstructed by lever-aging the information in adjacent frames. Besides, given the synthetic and real data, we perform adversarial training by introducing Gradient Reverse Layer [9] with a discrim-inator for the learned correspondences. We observe such designs can further enhance learned ﬁne-grained features.
Though already getting impressive results, we ﬁnd it takes more time to get the results of the dense matching be-tween ﬁne-grained features. Thus, we make another step to devise a coarse-to-ﬁne framework to address the problem.
We put the complex feature matching on the coarse-grained feature map and then get the ﬁne-grained results through a learnable up-sampling layer. As a result, we achieve a
• We address the problem of establishing pixel-wise video correspondences via a feature learning approach.
• We propose an effective method of learning ﬁne-grained features from both synthetic and unlabeled videos, fol-lowed by a carefully designed framework to address the issue of efﬁciency.
• We validate our method in a series of correspondence-based tasks. Experiment results indicate consistent im-provement over state-of-the-art methods. 2.