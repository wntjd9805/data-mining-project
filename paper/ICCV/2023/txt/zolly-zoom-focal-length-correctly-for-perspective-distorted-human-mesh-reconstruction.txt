Abstract
As it is hard to calibrate single-view RGB images in the wild, existing 3D human mesh reconstruction (3DHMR) methods either use a constant large focal length or estimate one based on the background environment context, which can not tackle the problem of the torso, limb, hand or face distortion caused by perspective camera projection when the camera is close to the human body. The naive focal length assumptions can harm this task with the incorrectly formulated projection matrices. To solve this, we propose
Zolly, the first 3DHMR method focusing on perspective-distorted images. Our approach begins with analysing the reason for perspective distortion, which we find is mainly caused by the relative location of the human body to the camera center. We propose a new camera model and a novel 2D representation, termed distortion image, which describes the 2D dense distortion scale of the human body.
We then estimate the distance from distortion scale features rather than environment context features. Afterwards, We integrate the distortion feature with image features to re-construct the body mesh. To formulate the correct projec-*LY† is the corresponding author. tion matrix and locate the human body position, we simul-taneously use perspective and weak-perspective projection loss. Since existing datasets could not handle this task, we propose the first synthetic dataset PDHuman and extend two real-world datasets tailored for this task, all contain-ing perspective-distorted human images. Extensive exper-iments show that Zolly outperforms existing state-of-the-art methods on both perspective-distorted datasets and the standard benchmark (3DPW). Code and dataset will be re-leased at https://wenjiawang0312.github.io/ projects/zolly/. 1.

Introduction
Human pose and shape estimation from single-view
RGB images is a long-standing research area of com-puter vision, as the reconstructed motion and mesh could empower various human-centered downstream applications like 3D animations, robotics, or AR/VR development. Pre-vious works [19, 26, 23, 24, 41, 23, 27, 20, 55] formulate the problem under the assumption that the reconstructed people are far away from the camera, thus the torso and limb distor-tion caused by the perspective projection can be neglected.
However, perspective distortion in close-up images is common in real-life scenarios, such as photographs of ath-letes/actors in sports events/films or selfies taken for social media. In such images, distortions are usually caused by aerial photography, overhead beat, or large depth variance among torsos and limbs, resulting in depth ambiguity in single-view RGB images, which makes it a big challenge to recover human pose and shape (See Fig. 1).
Previous methods typically assume a large fixed focal length [19, 26, 23, 24] or estimate a focal length [25] using pre-trained networks and calculate the translation from the estimated focal length. These settings are appropriate when people are far from the cameras, where the depth variance of the human body is negligible compared to the distance to the camera. However, these methods are inappropriate for handling scenarios in which human bodies are perspec-tive distorted. Overestimating the focal length could lead to joint angle ambiguity or harm joint rotation learning. Sev-eral methodologies for pose estimation, as proposed by pre-vious works [22, 29], assume a large field-of-view (FoV) angle. However, these methods may not show significant improvement when the focus is solely on non-distorted hu-man images, as they often lack a conditioning for depth variance when the camera zooms in or out. Inaccuracies in estimating the depth variance with respect to translation can adversely impact re-projection loss, leading to erroneous results, as illustrated in Fig. 1. Actually, a correctly esti-mated distance and focal length also help with 2D align-ment, which will be useful in downstream tasks.
To address the challenge of perspective distortion in close-up images, showing respect to Hitchcock’s dolly zoom shot, we introduce Zolly (Zoom fOcal Length cor-rectLY) for perspective-distorted human mesh reconstruc-tion. Our method utilizes 2D human distortion features to estimate the real-world distance to the camera cen-ter, enabling the reconstruction of the 3D human mesh in perspective-distorted images. The framework comprises of two parts: a translation estimation module for estimating the z-axis distance of the human body from the camera cen-ter, and a mesh reconstruction module for reconstructing 3D vertex coordinates in camera space. Additionally, we intro-duce a hybrid loss function that combines both perspective and weak perspective projection to boost performance.
Inspired by the iconic dolly-zoom shot [39] (also known as zolly shot), which creatively combines camera movement and zooming to create a distorted perspective and sense of unease, we propose a translation estimation module for the perspective-distorted 3DHMR task. This module highlights how the relative position of the human body to the camera affects the perspective distortion in images. Based on this insight, we introduce the distorted image as a new repre-sentation to capture the 2D shrinking or dilation scales of each pixel. Our translation network utilizes distortion and
Figure 2: This figure showcases how the distortion scale of a person’s limbs becomes more pronounced when closer to the camera center. Further, when two human bodies are at the same included angle with respect to the human-camera axis, they appear similar in facial direction. However, the horizontal translation may cause distinct distortion types on the left and right sides. We demonstrate that as the human body gets closer to the camera center, the distortion magni-tude increases, leading to a more precise estimation of depth and rotation angles.
IUV images to accurately estimate z-axis translation, over-coming the limitations of traditional methods that rely on environmental information. IUV image could help elimi-nate the 2D shift and scale information in distorted images and represent 2D dense position information. For mesh re-construction, we lift the 2D position feature to the 3D vertex position feature and sample the by-vertex distortion feature to regress 3D vertex coordinates. We use perspective pro-jection to supervise correctly and weak-perspective projec-tion to locate the 2D human body position in the image and help to calculate our focal length.
In summary, our contributions are as follows: (1) We analyze the state-of-the-art (SOTA) 3DHMR methods and propose a novel approach tailored to the perspective-distorted 3DHMR task. (2) We propose a novel learning-based method to tackle the perspective-distorted 3DHMR task without relying on extra camera information. The core of our method is a newly designed representation, termed distortion im-age, and a hybrid projection supervision that make use of both perspective and weak-perspective projection. (3) We build the first large-scale synthetic dataset PDHuman for the perspective-distorted 3DHMR task, with high-quality
SMPL ground truth and camera parameters. To evaluate the performance on real images, we prepare two real-world benchmark datasets, SPEC-MTP [25] and HuMMan [6], which contain perspective-distorted images with well-fitted
SMPL parameters and camera parameters.
shape and pose. To address this issue, several recent works such as BeyondWeak [22], SPEC [25], and CLIFF [29] have proposed different camera system assumptions. SPEC predicts camera parameters (pitch, yaw, and FoV) from a single-view image, but its asymmetric Softargmax-L2 loss tends to overestimate focal length and translation, which is not suitable for distorted images. Moreover, SPEC re-gresses camera parameters through environmental informa-tion, which can sometimes be meaningless when the back-ground lacks geometry information. CLIFF focuses on joint rotation variance caused by horizontal shift but has not con-ditioned the distance from the human body to the cam-era. CLIFF, following BeyondWeak[22], uses the diagonal length of the image as the focal length, which is not a close assumption for distorted problems since the focal length can be easily adjusted during image capture.
Compared to these methods, our framework estimates the z-axis translation from 2D human distortion features, and obtains a more accurate focal length from the estimated translation, leading to much better reconstruction accuracy on distorted images. See a comparison of the camera mod-els in Fig. 3. In the Sup. Mat., we quantitatively demon-strate the bad re-projection influence caused by a wrongly formulated projection matrix. 3. Methodology
In this section, we first review the formulation of pre-vious camera systems and then present our camera sys-tem customized for distorted images in Sec. 3.1. Sec. 3.2 presents our network architecture with two key compo-nents: (i) translation estimation module and (ii) mesh re-construction module. Subsequently, we explain the pro-posed hybrid re-projection loss functions for distorted hu-man mesh reconstruction in Sec. 3.3. 3.1. Preliminary
Camera system analysis. In weak-perspective projection, the inner depth variance is ignored in the human body, which means this projection model views the human body as a planar object without thickness. Thus the projection matrix should be as follows:





 f
 f

 1 x + Tx y + Ty z + Tz f (x + Tx) f (y + Ty)
Tz,
 =

 , z = 0, (1) where f refers to the focal length in NDC (Normalized De-vice Coordinate) space, x, y, z refers to a vertex point on human body mesh and Tx, Ty, Tz refers to pelvis transla-tion. The weak-perspective camera parameters (s, tx, ty), which represent 2D orthographic transformation, could be used to approximate the projection: (cid:20)f (x + Tx)/Tz f (y + Ty)/Tz (cid:21)
= (cid:21) (cid:20)s(x + tx) s(y + ty)
. (2)
Figure 3: Comparison of 4 different camera models in 3DHMR task. For HMR [19], the focal length is fixed as 5000 pixels. Most methods follow this setting. For
SPEC [25], the focal length is estimated by a network Rf pre-trained on other datasets. For CLIFF [29], during train-ing, if without ground-truth focal length, will use the length of diagonal length. For Zolly, we use the estimated z-axis translation z, camera parameter s, and image height h to cal-culate focal length. 2.