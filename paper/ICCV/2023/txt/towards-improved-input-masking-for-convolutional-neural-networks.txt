Abstract
The ability to remove features from the input of machine learning models is very important to understand and inter-pret model predictions. However, this is non-trivial for vi-sion models since masking out parts of the input image typ-ically causes large distribution shifts. This is because the baseline color used for masking (typically grey or black) is out of distribution. Furthermore, the shape of the mask itself can contain unwanted signals which can be used by the model for its predictions. Recently, there has been some progress in mitigating this issue (called missingness bias) in image masking for vision transformers.
In this work, we propose a new masking method for CNNs we call layer masking in which the missingness bias caused by masking is reduced to a large extent. Intuitively, layer masking ap-plies a mask to intermediate activation maps so that the model only processes the unmasked input. We show that our method (i) is able to eliminate or minimize the influ-ence of the mask shape or color on the output of the model, and (ii) is much better than replacing the masked region by black or grey for input perturbation based interpretabil-ity techniques like LIME. Thus, layer masking is much less affected by missingness bias than other masking strategies.
We also demonstrate how the shape of the mask may leak in-formation about the class, thus affecting estimates of model reliance on class-relevant features derived from input mask-ing. Furthermore, we discuss the role of data augmentation techniques for tackling this problem, and argue that they are not sufficient for preventing model reliance on mask shape. 1.

Introduction
While deep learning methods have become extremely successful in solving many computer vision tasks, they are generally opaque, and they do not admit easy debugging of errors. Many novel interpretability methods have been developed in recent years which attempt to analyze the ra-tionale behind a model’s predictions.
In particular, it is natural to analyze the dependence of the model prediction on its input by perturbing parts of its input and observing corresponding changes in the output [22, 16, 38]. Com-mon perturbations include adding Gaussian noise, Gaus-sian blurring, replacing with a baseline color, etc. How-ever, many of these perturbation methods come with cer-tain downsides. Partial perturbations, like Gaussian noise or blurring, attempt to slightly corrupt parts of the image, while still preserving much of the information present in those parts. While this has the advantage of not changing the input distribution drastically, we can only measure the local sensitivity of the model - if the model were to be ro-bust to these perturbations, it would be locally insensitive to perturbations on certain parts of the image but it might still rely heavily on them for its prediction [27, 31]. Full perturbation methods remove the parts completely, and re-place it with a baseline color like black or grey. In discrete domains like natural language, this is often the most pop-ular method, as it is easy to remove words from the input
[17]. In images, however, this creates a large shift in input distribution, leading the model to perform poorly on such inputs [29, 30]. For example, if we randomly mask out 16
× 16 sized patches from the image, ResNets are more likely to predict that the image is a maze or crossword [12]
In recent work [21, 25, 8], it has been observed that vi-sion transformers [6] are highly robust to many kinds of large magnitude input perturbations like occlusions and do-main shifts, maintaining upto 60% accuracy on ImageNet even if 80% of the input is randomly blacked out. Jain et al [12] argue that this property can make interpretability methods based on full perturbation especially effective for transformers. They further propose to simply drop tokens corresponding to masked out input parts instead of black-ing out or greying out image portions, just like dropping
BPE tokens in a transformer-based language model. This would make the transformer model completely insensitive to choice of baseline color and the shape of the mask.
Motivated by the same intuition, we devise a new mask-ing technique for CNNs which mitigates the drawbacks of full perturbation to a large extent, which we call layer masking. Layer masking (as depicted in Fig. 1) works by running the CNN only on the unmasked portion of the im-Figure 1: An outline of layer masking, our proposed method, for a convolutional layer. The image is first masked and then padded using neighbor padding. The convolutional layer then acts on the padded image, and a maxpool of the same kernel size and stride acts on the mask. These are then propagated forward through the CNN. The mask boundary is highlighted in the padded image for illustrative purposes. age, thus avoiding any large distribution shift. This is done by carefully masking and padding the input of each layer to make the model focus only on the unmasked input re-gions. Using this technique, we are able to randomly re-move upto 50 % of the input to a ResNet-50 (in the form of 16 × 16 sized patches) while maintaining the top-1 ac-curacy on ImageNet over 70%. We are also able to mask out objects from images precisely without leaking any in-formation about those objects via the shape of the mask.
In addition, layer masking operates at the pixel level and is thus much more flexible than token dropping for vision transformers which only acts on a patch level. We also find that LIME [22] scores obtained using our masking method are more aligned with the most salient features of the image as compared to simply blacking or greying out the masked portion. 2.