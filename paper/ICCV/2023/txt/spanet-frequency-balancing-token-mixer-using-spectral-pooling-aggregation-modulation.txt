Abstract
Recent studies show that self-attentions behave like low-pass filters (as opposed to convolutions) and enhancing their high-pass filtering capability improves model per-formance. Contrary to this idea, we investigate existing convolution-based models with spectral analysis and ob-serve that improving the low-pass filtering in convolution operations also leads to performance improvement. To ac-count for this observation, we hypothesize that utilizing op-timal token mixers that capture balanced representations of both high- and low-frequency components can enhance the performance of models. We verify this by decompos-ing visual features into the frequency domain and combin-ing them in a balanced manner. To handle this, we re-place the balancing problem with a mask filtering problem in the frequency domain. Then, we introduce a novel token-mixer named SPAM and leverage it to derive a MetaFormer model termed as SPANet. Experimental results show that the proposed method provides a way to achieve this bal-ance, and the balanced representations of both high- and low-frequency components can improve the performance of models on multiple computer vision tasks. Our code is available at https://doranlyong.github.io/projects/spanet/. 1.

Introduction
In recent years, Vision Transformers (ViTs) have achieved remarkable success and have garnered significant attention in the field of computer vision. As a result, nu-merous follow-up models based on the ViT [15] have been proposed, making ViTs a dominant architecture and a vi-able alternative to Convolutional Neural Networks (CNNs) in various computer vision tasks including image classifica-tion [55, 67, 34, 58], object detection [3, 80, 76], segmenta-Figure 1: Fourier spectrum maps of ConvNeXt and other
MetaFormers. The output of the spectrum map from each token mixer is processed for the same input. Depth-wise convolution (DepConv) of ConvNeXt-T [35], Global MSA of ViT-B/16 [15], Local MSA of Swin-T [34], Focal module of FocalNet-T [69], and SPAM of our SPANet-S are shown in order. tion [61, 64, 8], and beyond [4, 75, 41, 62].
The reason for the success of ViT has been explained pri-marily as the use of Multi-Head Self-Attention (MSA) for token mixing [15]. This commonly held belief has led to the development of numerous variations of MSA [16, 20, 63, 79] aimed at improving the performance of ViTs. Yet some recent works have challenged this belief by demon-strating competitive results without utilizing MSAs. Tol-stikhin et al. [52] fully replaced the MSAs with a spatial
Multi-Layer Perceptron (MLP) and achieves comparable re-sults on image classification benchmarks. Subsequent stud-ies [24, 33, 54, 51] have attempted to reduce the perfor-mance gap between MLP-like models and ViTs by utilizing improved data-efficient training and redesigned MLP mod-ules. These endeavors have shown the feasibility of MLP-like models to replace MSAs as token mixers. Moreover, other research lines [29, 38, 39, 46, 21] have explored al-ternative self-attention-based token mixers and reported en-couraging results. For example, GFNet [46] replaces self-attention with Fourier Transform and achieves competitive performance to ViT in image classification tasks.
ViT [15], and Swin Transformer [34]. Collecting all these results together, we then naturally make such a hypothesis: utilizing optimal token mixers that capture balanced repre-sentations of both high- and low-frequency components can enhance the performance of models.
To verify this hypothesis, we employ the Discrete
Fourier Transform (DFT) to decompose visual features into low- and high-frequency components. We then assign weights to tokens corresponding to each frequency band to balance low-frequency and high-frequency components in a way. To accomplish this, we replace the balancing problem with a mask filtering problem in the frequency domain and introduce a novel token-mixer called spectral pooling ag-gregation modulation (SPAM) module, which enables the balance of high- and low-frequency components. Using the SPAM token-mixer, we propose SPANet based on the
MetaFormer architecture [72]. The performance of SPANet is evaluated on three benchmark computer vision tasks: im-age classification, object detection, and segmentation, and it demonstrates improved results compared to the previous state-of-the-art.
Our contributions are summarized as three-fold. (1) We handle the balancing problem of high- and low-frequency components of visual features, and show that it can be re-placed with a mask filtering problem in the frequency do-main. Specifically, we solve this problem by introducing
SPAM. (2) Leveraging SPAM, we propose SPANet, which is based on the MetaFormer architecture [72]. (3) Our pro-posed SPANet is evaluated on multiple vision tasks, includ-ing image classification [14], object detection [32], instance segmentation [32], and semantic segmentation [78]. Our results show that SPANet outperforms state-of-the-art mod-els. 2.