Abstract 3D lane detection from monocular images is a funda-mental yet challenging task in autonomous driving. Re-cent advances primarily rely on structural 3D surrogates (e.g., bird’s eye view) built from front-view image features and camera parameters. However, the depth ambiguity in monocular images inevitably causes misalignment be-tween the constructed surrogate feature map and the origi-nal image, posing a great challenge for accurate lane de-tection. To address the above issue, we present a novel
LATR model, an end-to-end 3D lane detector that uses 3D-aware front-view features without transformed view repre-sentation. Specifically, LATR detects 3D lanes via cross-attention based on query and key-value pairs, constructed using our lane-aware query generator and dynamic 3D ground positional embedding. On the one hand, each query is generated based on 2D lane-aware features and adopts a hybrid embedding to enhance the lane information. On the other hand, 3D space information is injected as posi-tional embedding from an iteratively-updated 3D ground plane. LATR outperforms previous state-of-the-art meth-ods on both synthetic Apollo and realistic OpenLane and
ONCE-3DLanes by large margins (e.g., 11.4 gain in terms of F1 score on OpenLane). Code will be released at https://github.com/JMoonr/LATR. 1.

Introduction 3D Lane Detection is critical for various applications in autonomous driving, such as trajectory planning and lane keeping [48]. Despite the remarkable progress of LiDAR-based methods in other 3D perception tasks [56, 13], re-cent advances in 3D lane detection prefer using a monoc-ular camera since it owns desirable advantages compared to LiDARs. Apart from the low deployment cost, cameras offer a longer perception range compared to other sensors and can produce high-resolution images with rich textures, which are crucial for detecting slim and long-span lanes.
Due to the lack of depth information, detecting 3D lanes
*Corresponding author
Figure 1. (a) Previous methods mainly utilize camera parameters and inverse perspective mapping (IPM) to transform the features into a surrogate space (e.g., BEV), and further perform 3D lane detection through anchors and non-maximum suppression (NMS). (b) We propose the novel LATR, an anchor-free and NMS-free
Transformer architecture, to perform 3D lane detection right on the front view. Using lane-aware queries and dynamic 3D ground positional embedding, our model produces well-aligned 3D fea-tures and achieves superior 3D lane detection. from monocular images is challenging. A straightforward solution is to reconstruct the 3D lane layout based on 2D segmentation results and per-pixel depth estimation, as pro-posed in SALAD [49]. However, this method requires high-quality depth data for training and heavily relies on the precision of the estimated depth. Alternatively, Curve-Former [1] employs polynomials to model the 3D lane from the front view. While it avoids indefinite view transforma-tion, the polynomial form adopted in their design restricts the flexibility of capturing diverse lane shapes. In contrast, current mainstream methods favor the utilization of 3D sur-rogate representations [7, 5, 8, 23, 3, 14]. These surrogate representations are constructed based on front-view image features and camera parameters, with no reliance on depth information. Since lanes inherently reside on the road, most of these methods build the 3D surrogate by projecting the image features into a bird’s eye view (BEV) via inverse perspective mapping (IPM) [31]. However, IPM is strictly based on the flat ground assumption, thereby introducing misalignment between the 3D surrogate and the original im-age in many real driving scenarios (e.g., uphill/downhill and
bumps). This misalignment, entangled with distortions, in-evitably hinders the accurate estimation of the road structure and endangers driving safety. Despite attempts [3] made to relieve this issue by introducing deformable attention [57], the problem of misalignment remains unresolved.
Based on the above observation, we aim to improve 3D lane detection by directly locating 3D lanes from the front view without any intermediate 3D surrogates through lane-aware queries.
Inspired by the 2D object detector
DETR [2], we streamline lane detection as an end-to-end set prediction problem, forming LAne detection TRansformer (LATR). LATR detects 3D lanes from front view images using lane-aware queries and dynamic 3D ground posi-tional embedding. We devise a lane representation scheme to describe lane queries, better capturing the properties of 3D lanes. Besides, we utilize lane-aware features to offer queries rich semantic and spatial priors. Since pure front-view features lack awareness of 3D space, we inject 3D po-sitional information from a hypothetical 3D ground into the front-view features. This hypothetical ground, initialized as a horizontal grid, undergoes iterative optimization to fit the ground truth road. Finally, the lane-aware queries interact with 3D-aware features through a transformer decoder, fol-lowed by MLPs to produce 3D lane predictions.
Our main contributions are the following:
• We propose LATR, an end-to-end 3D lane detection framework based on Transformer. By directly detecting 3D lanes from the front view, without using any 3D sur-rogate representations, LATR offers efficiency and avoids feature misalignment present in prior methods.
• We introduce a lane-aware query generator that uses dy-namically extracted lane-aware features to initialize query embeddings. Moreover, a dynamic positional embedding is proposed to bridge 3D space and 2D images, which derives from a constructed 3D ground that is iteratively updated under supervision.
• We conduct thorough experiments on the benchmark datasets of OpenLane, Apollo, and ONCE-3DLanes. Our proposed LATR outperforms previous SoTA methods by large margins (+11.4 improvement on OpenLane, +4.3 on
Apollo, and +6.26 on ONCE-3DLanes w.r.t. F1 score). 2.