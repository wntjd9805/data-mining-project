Abstract
The prolific performances of Vision Transformers (ViTs) in image tasks have prompted research into adapting the image ViTs for video tasks. However, the substantial gap between image and video impedes the spatiotemporal learning of these image-pretrained models. Though video-specialized models like UniFormer can transfer to the video domain more seamlessly, their unique architectures require prolonged image pretraining, limiting the scalability. Given the emergence of powerful open-source image ViTs, we pro-pose unlocking their potential for video understanding with efficient UniFormer designs. We call the resulting model
UniFormerV2, since it inherits the concise style of the Uni-Former block, while redesigning local and global relation aggregators that seamlessly integrate advantages from both
ViTs and UniFormer. Our UniFormerV2 achieves state-of-the-art performances on 8 popular video benchmarks, in-cluding scene-related Kinetics-400/600/700, heterogeneous
Moments in Time, temporal-related Something-Something
V1/V2, and untrimmed ActivityNet and HACS. It is note-worthy that to the best of our knowledge, UniFormerV2 is the first to elicit 90% top-1 accuracy on Kinetics-400. 1.

Introduction
The triumph of transformer-based language foundation models [16, 51, 5] has resulted in the swift growth of im-age foundation models [18, 24, 50, 73], which have been meticulously trained on massive web datasets with rich su-pervision, such as image-text contrastive learning [50, 30] and mask image modeling [24, 3]. The resulting Vision
Transformers (ViTs) exhibit exceptional generalization ca-pacity for a range of image tasks [43, 12, 53], motivating researchers to explore their applications for video tasks.
*Interns at Shanghai AI Laboratory. â€ Corresponding authors.
Figure 1: Comparison with SOTA methods using open sources. Our UniFormerV2 achieves state-of-the-art per-formances on popular scene-related, temporal-related, het-erogeneous and untrimmed video benchmarks. Compared to VideoMAE [71] which requires thousands of epochs for pre-training, our method directly arms well-prepared image
ViTs with efficient designs for robust video understanding.
In light of the success of adapting 2D convolution neu-ral networks (CNNs) for spatiotemporal learning [63, 59, 39, 31], researchers have proposed a series of plug-and-play modules for ViTs, such as split space-time attention [4], to-ken shift module [23], and motion-enhanced decoder [40].
Thanks to powerful image pretraining [66, 55, 50], these
ViT-based video learners surpass CNNs by a considerable margin on traditional scene-related benchmarks [32, 9, 10], which can be recognized easily by a single frame. However, when faced with complex temporal-related tasks [22], they perform much worse than CNN-based ones [34, 62]. The substantial domain gap between image and video presents a challenge to adapt image ViTs for video understanding.
Another prevalent paradigm is to design specialized ViTs
[42, 37, 35], which can be effortlessly transferred to the video domain via simple technique, i.e., inflating spatial convolution or attention to spatiotemporal ones. In the ad-vanced UniFormer [35], the authors unify convolution and self-attention as Multi-Head Relation Aggregator (MHRA) in a transformer format. By modeling local and global rela-tions respectively in shallow and deep layers, it can not only handle both scene-related and temporal-related tasks effec-tively, but also significantly reduce the computation burden.
However, as a unique architecture, UniFormer lacks image pretraining as a starting point. To obtain a robust visual rep-resentation, it has to go through prolonged pretraining on images before finetuning on videos, which makes it difficult to scale up. Considering the emergence of powerful open-source image ViTs [66, 3, 50], a natural question arises:
Can we unlock the potential of image ViTs for video un-derstanding with an efficient UniFormer design?
In this paper, we propose a simple yet effective paradigm for constructing powerful video networks, by arming the image-pretrained ViTs with efficient UniFormer designs (see Figure 1). We call the resulting model UniFormerV2, since it inherits the concise style of UniFormer but equips local and global UniBlocks with new MHRA. In the local
UniBlock, we incorporate a local temporal MHRA before the spatial ViT block. Thus we can largely reduce temporal redundancy and leverage the well-pretrained ViT block, for learning local spatiotemporal representation effectively. As for the global UniBlock, we introduce a query-based cross
MHRA. Unlike the costly global MHRA in the original
UniFormer, our cross MHRA can summarize all the spa-tiotemporal tokens into a video token, for learning global spatiotemporal representation efficiently. Finally, we reor-ganize local and global UniBlocks as a multi-stage fusion architecture, which can adaptively integrate multi-scale spa-tiotemporal representation to capture complex dynamics.
We apply our paradigm on ViTs that are pretrained on three popular supervision, including supervised learning
[55, 56], contrastive learning [50], and mask image mod-eling [24, 3]. Our results reveal that all enhanced models exhibit superior performance compared to previous ViT-based approaches, showcasing the generic nature of our
UniFormerV2.
In addition, we have constructed a com-pact Kinetics-710 benchmark, combining the action classes of Kinetics-400/600/700, and have removed repeated and leaked videos in the training sets of these benchmarks for enhanced fairness. As a result, the number of training videos has been reduced from 1.14M to 0.66M. After train-ing on K710, our model can simply achieve higher accuracy on K400/600/700 via only 5-epoch finetuning.
To verify the robustness of our approach, we con-duct experiments on 8 large-scale video benchmarks as shown in Figure 1, including scene-related datasets (i.e.,
Kinetics-400/600/700 [32, 9, 10], a heterogeneous dataset that contains complex inter-class and inter-class variation (i.e., Moments in Time [44]), temporal-related datasets (i.e., Something-Something V1/V2 [22]), and untrimmed datasets (i.e., ActivityNet [25] and HACS [78]). Our Uni-FormerV2 based on CLIP-ViT [50] achieves state-of-the-art results on all the benchmarks. It is worth mentioning that our model is the first to elicit a top-1 accuracy of 90.0% on
Kinetics-400, to the best of our knowledge. 2.