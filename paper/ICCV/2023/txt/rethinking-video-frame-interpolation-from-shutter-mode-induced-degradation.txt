Abstract
Image restoration from various motion-related degrada-tions, like blurry effects recorded by a global shutter (GS) and jello effects caused by a rolling shutter (RS), has been extensively studied. It has been recently recognized that such degradations encode temporal information, which can be exploited for video frame interpolation (VFI), a more challenging task than pure restoration. However, these VFI researches are mainly grounded on experiments with syn-thetic data, rather than real data. More fundamentally, un-der the same imaging condition, it remains unknown which degradation will be more effective toward VFI. In this pa-per, we present the first real-world dataset for learning and benchmarking degraded video frame interpolation, named
RD-VFI, and further explore the performance differences of three types of degradations, including GS blur, RS distortion, and an in-between effect caused by the rolling shutter with global reset (RSGR), thanks to our novel quad-axis imaging system. Moreover, we propose a unified Progressive Mutual
Boosting Network (PMBNet) model to interpolate middle frames at arbitrary time for all shutter modes. Its disentan-glement strategy and dual-stream correction enable us to adaptively deal with different degradations for VFI. Experi-mental results demonstrate that our PMBNet is superior to the respective state-of-the-art methods on all shutter modes. 1.

Introduction
When high-speed cameras are not accessible, the perceiv-ing and modeling of fast motion in video understanding can be challenging. One promising computational alternative is to up-convert low-framerate videos through video frame interpolation (VFI). Specifically, given two consecutive in-puts, frame interpolation aims to reconstruct intermediate frames with temporal and spatial coherence, which has been addressed in existing VFI methods [13, 1, 12, 18, 29].
Unfortunately, despite the remarkable success, these ap-†Corresponding author proaches with sharp frames as input are less applicable, since image degradations are almost unavoidable in the presence of fast motion, which are closely related to shutter modes as well. Therefore, VFI with degraded inputs are of greater in-terest in practice, and it is even believed that the degradations encode rich information relating to motion, which might ben-efit VFI. Recent video restoration works have demonstrated this insight in VFI from single [15, 30] or multiple blurred in-puts [14, 33, 3, 41, 28]. Meanwhile, the RS (rolling shutter) counterpart has also been conducted very recently [9, 11].
However, those VFI algorithms with degraded inputs, without exception, are evaluated on synthetic data only, and their performance in the real-world remains unknown. For example, the most prevalent datasets for blur (GoPRO [25],
Adobe240fps [36]) or RS (Fastec-RS [20]) VFI cannot ex-actly mimic real captured degradations due to their oversim-plified operations of blending consecutive frames or copy-ing distinct scanlines from sharp frames. Such a synthesis method can easily lead to unnatural artifacts, as shown in
Fig. 1, which is also mentioned in [32, 43]. Artifacts caused by unrealistic simulation tend to destroy degradations in-duced by motion and shutter modes, and the learned model has inferior generalization. Therefore, a real dataset without such synthesis artifacts is in immediate need. More impor-tantly, VFI with GS blur or RS distortion has been studied independently, yet a unified study of these degradations un-der the same condition can reveal the advantages of different shutter modes for VFI, which is completely missing.
To address the aforementioned issues, we present the first real-world dataset for learning and benchmarking degraded video frame interpolation, which is dubbed RD-VFI. Inspired by the hardware design of [32, 43], we further propose a quad-axis imaging system to capture temporally and geo-metrically aligned high-speed sharp videos and low-speed degraded videos with three kinds of degradations, including
GS blur, RS distortion, and a mixed effect caused by a rolling shutter with global reset (RSGR). RSGR [38] leads to blurry effects with varying magnitude, a special degradation lying between GS blur and RS distortion. Although RSGR video restoration has been explored in [38], its performance for
Figure 1: Samples from the existing synthetic dataset and our RD-VFI dataset. (a) and (b) are samples from Adobe240fps and GoPRO, respectively. The unnatural hops and steps caused by the discontinuity of averaging process can be easily spotted. (c) are samples from
Fastec-RS with horizontal streak artifacts. (d) are from our real-world dataset RD-VFI.
VFI remains unknown. Facilitated by our dataset, system-atic comparisons between various degradations for VFI have been made to reveal their correlations or performance gaps.
Furthermore, we propose a unified model, Progressive
Mutual Boosting Network (PMBNet), to interpolate middle clear frames at arbitrary time instances for all three expo-sure modes. PMBNet decouples the VFI task into correc-tion and interpolation parts and reconnects them by latent variables and flow-guided feature alignment module (FFA) among the whole iterative layers with different scales. The dual-stream correction absorbs the merits of two classical paradigms from deblurring and RS correction, which enable our model to adaptively handle three types of degradation.
Subsequently, interpolated candidate frame will be refined by a temporal and contextual compensation layer (TCL) to alleviate artifacts at the boundaries of dynamic objects and fill holes caused by occlusion.
In short, our contributions are:
• We present the first real-world dataset RD-VFI for video frame interpolation with degradations from three differ-ent shutter modes.
• Rather than a dual-axis system for single degradation, we develop a quad-axis imaging system that simultane-ously captures three degradations and their high-speed ground truth, which allows direct comparison of differ-ent shutter modes for VFI.
• We introduce an original VFI task based on RSGR videos and a generic neural network architecture named
PMBNet to adaptively handle different degradations, including GS blur, RS distortion, and RSGR effects. 2.