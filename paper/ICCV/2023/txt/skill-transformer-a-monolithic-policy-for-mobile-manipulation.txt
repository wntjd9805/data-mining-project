Abstract
We present Skill Transformer, an approach for solving long-horizon robotic tasks by combining conditional se-quence modeling and skill modularity. Conditioned on ego-centric and proprioceptive observations of a robot, Skill
Transformer is trained end-to-end to predict both a high-level skill (e.g., navigation, picking, placing), and a whole-body low-level action (e.g., base and arm motion), using a transformer architecture and demonstration trajectories that solve the full task.
It retains the composability and modularity of the overall task through a skill predictor mod-ule while reasoning about low-level actions and avoiding hand-off errors, common in modular approaches. We test
Skill Transformer on an embodied rearrangement bench-mark and ﬁnd it performs robust task planning and low-level control in new scenarios, achieving a 2.5x higher suc-cess rate than baselines in hard rearrangement problems. 1.

Introduction
A long-standing goal of embodied AI is to build agents that can seamlessly operate in a home to accomplish tasks like preparing a meal, tidying up, or loading the dishwasher.
To successfully carry out such tasks, an agent must be able to perform a wide range of diverse skills, like navigation and object manipulation, as well as high-level decision-making, all while operating with limited egocentric inputs. Our par-ticular focus is the problem of embodied object rearrange-ment [41]. An example in Figure 1 shows that the Fetch mo-bile manipulator must rearrange a bowl from an initial posi-tion to a desired goal position (both circled in orange) in an unfamiliar environment without access to any pre-existing maps, 3D models, or precise object locations. Success-fully completing this task requires executing both low-level motor commands for accurate arm and base movements, as well as high-level decision-making, such as determining whether a drawer needs to be opened before accessing a speciﬁc object.
Long-horizon tasks like object rearrangement require many low-level control steps and naturally break down into distinct phases of skills such as navigation, picking, plac-ing, opening, and closing. Prior works [6, 33, 37] show that it is possible to end-to-end learn a single policy that can perform such diverse tasks and behaviors. Such a mono-lithic policy directly maps observations to low-level actions, reasoning both about which skill to execute and how to execute it. However, scaling end-to-end learning to long-horizon, multi-phase tasks with thousands of low-level steps and egocentric visual observations remains a challenging research question. Other works decompose the task into individual skills and then separately sequence them to solve the full task [40, 16]. While this decomposition makes the problem tractable, this modularity comes at a cost. Se-quencing skills suffers from hand-off errors between skills, where one skill ends in a situation where the next skill is no longer feasible. Moreover, it does not address how to learn the skill sequencing or how to adapt the planned sequence to unknown disturbances.
We present Skill Transformer, an end-to-end trainable transformer policy for multi-skill tasks. Skill Transformer predicts both low-level actions and high-level skills, con-ditioned on a history of high-dimensional egocentric cam-era observations. By combining conditional sequence mod-eling with a skill prediction module, Skill Transformer can model long sequences of both high and low-level ac-tions without breaking the problem into disconnected sub-modules. In Figure 1, Skill Transformer learns a policy that can perform diverse motor skills, as well as high-level rea-soning over which motor skills to execute. We train Skill
Transformer autoregressively with trajectories that solve the full rearrangement task. Skill Transformer is then tested in unseen settings that require a variable number of high-level and low-level actions, overcoming unknown perturbations.
Skill Transformer achieves state-of-the-art performance in rearrangement tasks requiring variable-length high-level planning and delicate low-level actions, such as rearranging an object that can be in an open or closed receptacle, in the
Habitat environment [41]. It achieves 1.45x higher success rates in the full task and up to 2.5x higher in especially chal-lenging situations over modular and end-to-end baselines
Target Object
Goal
Pred Skill:
Place Object
Navigate
Pick Object
Open Drawer
Skill Transformer
Figure 1: A Fetch robot is placed in an unseen house and must rearrange an object from a speciﬁed start to a desired goal location from egocentric visual observations. The robot infers the drawer is closed and opens the drawer to access the object.
Next, it picks the object, navigates to the goal, and places the object. The Skill Transformer is a monolithic “pixels-to-actions” policy for such long-horizon mobile manipulation problems which infers the active skill and the low-level continuous control. and is more robust to environment disturbances, such as a drawer snapping shut after opening it, despite never seeing such disturbances during training. Furthermore, we demon-strate the importance of the design decisions in Skill Trans-former and its ability to scale with larger training dataset sizes. The code is available at https://bit.ly/3qH2QQK. 2.