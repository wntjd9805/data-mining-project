Abstract
LiDAR and cameras are complementary sensors for 3D object detection in autonomous driving. However, it is chal-lenging to explore the unnatural interaction between point clouds and images, and the critical factor is how to conduct feature alignment of heterogeneous modalities. Currently, many methods achieve feature alignment by projection cal-ibration only, without considering the problem of coor-dinate conversion accuracy errors between sensors, lead-ing to sub-optimal performance. In this paper, we present
GraphAlign, a more accurate feature alignment strategy for 3D object detection by graph matching. Specifically, we fuse image features from a semantic segmentation encoder in the image branch and point cloud features from a 3D
Sparse CNN in the LiDAR branch. To save computation, we construct the nearest neighbor relationship by calculating
Euclidean distance within the subspaces that are divided into the point cloud features. Through the projection cali-bration between the image and point cloud, we project the nearest neighbors of point cloud features onto the image features. Then by matching the nearest neighbors with a single point cloud to multiple images, we search for a more appropriate feature alignment.
In addition, we provide a self-attention module to enhance the weights of significant relations to fine-tune the feature alignment between hetero-geneous modalities. Extensive experiments on nuScenes benchmark demonstrate the effectiveness and efficiency of our GraphAlign. 1.

Introduction 3D object detection, a vital computer vision task in au-tonomous driving, relies on deep learning for accurately
*Corresponding author feature alignment strategies:
Figure 1. Comparison of (a)
Projection-based quickly establishes the relationship between modal features but may suffer from misalignment due to sensor er-ror. (b) Attention-based preserves semantic information by learn-ing alignment but has a high computational cost. (c) Our pro-posed GraphAlign uses graph-based feature alignment to match more plausible alignments between modalities with reduced com-putation and improved accuracy. identifying and locating objects [8, 36, 43, 50, 52, 65–67] in 3D space [34, 35, 58, 60, 86]. With the availability of di-verse sensor data, such as cameras and LiDAR, 3D object detection research has made significant progress. However, challenges and difficulties remain due to the inherent limita-tions of each modality. While LiDAR point cloud provides accurate depth information, it lacks semantic information.
Conversely, camera images contain semantic information but lack depth information [34,58]. Therefore, multi-modal 3D object detection has been proposed to leverage the com-plementary advantages of both modalities to improve detec-tion performance.
Despite the potential of multi-modal 3D object detection, the effective fusion of heterogeneous modal features has not been fully explored. In this work, we mainly attribute the current difficulties of training multi-modal detectors to two
aspects. On the one hand, many methods [4,5,11,15–17,22, 27–29, 32, 37, 39, 49, 54, 55, 61, 63, 64, 70, 72, 78–81, 84, 85] rely on establishing deterministic correspondences between points and image pixels to fuse point clouds and image, as shown in Fig. 1 (a). However, accuracy errors resulting from the difference between LiDAR and camera sensors, such as timing synchronization errors, especially the mis-alignment of small objects in long-range feature fusion, can lead to a decrease in detection performance. On the other hand, a few methods [1, 6, 7, 24] employ attention-based solution to accomplish feature alignment rather than pro-jection, as shown in Fig. 1 (b). However, the key issue with using attention-based for point cloud and image fea-ture alignment in multi-modal 3D object detection is that it is too computationally expensive and cannot meet the real-time detection requirements.
In this work, we propose GraphAlign, a graph matching-based feature alignment strategy, to enhance the accuracy of multi-modal 3D object detection, as shown in Fig. 1 (c). GraphAlign comprises two key modules: Graph Fea-ture Alignment (GFA) and Self-Attention Feature Align-ment (SAFA). The GFA module divides the point cloud space into subspaces and generates the K nearest neigh-bor features for each point cloud.
It then transforms the local neighborhood information of the point cloud into im-age neighborhood information via a projection calibration matrix, followed by one-to-many feature fusion between a single point cloud feature and K neighbor image features.
The SAFA module employs a self-attention mechanism to enhance the weights of important relationships in the fused features and selects the most critical feature from K fused features. Our work’s main contributions can be summarized as follows:
• We propose GraphAlign, a feature alignment frame-work based on graph matching, to address the mis-alignment issue in multi-modal 3D object detection.
• We propose Graph Feature Alignment (GFA) and
Self-Attention Feature Alignment (SAFA) modules to achieve accurate alignment of image features and point cloud features, which can further enhance the feature alignment between point cloud and image modalities, leading to improved detection accuracy.
• Experiments are conducted using the KITTI [12] and nuScenes [2] benchmarks, demonstrating that
GraphAlign can boost point cloud detection accuracy, especially for long-range object detection. 2.