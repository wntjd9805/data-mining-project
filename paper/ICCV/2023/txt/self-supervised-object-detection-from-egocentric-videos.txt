Abstract
Understanding the visual world from human perspec-tives has been a long-standing challenge in computer vi-sion. Egocentric videos exhibit high scene complexity and irregular motion flows compared to typical video un-derstanding tasks. With the egocentric domain in mind, we address the problem of self-supervised, class-agnostic object detection, aiming to locate all objects in a given view, without any annotations or pre-trained weights. Our method, self-supervised object detection from egocentric videos (DEVI), generalizes appearance-based methods to learn features end-to-end that are category-specific and in-variant to viewing angle and illumination. Our approach leverages natural human behavior in egocentric percep-tion to sample diverse views of objects for our multi-view and scale-regression losses, and our cluster residual mod-ule learns multi-category patches for complex scene under-standing. DEVI results in gains up to 4.11% AP50, 0.11%
AR1, 1.32% AR10, and 5.03% AR100 on recent egocen-tric datasets, while significantly reducing model complexity.
We also demonstrate competitive performance on out-of-domain datasets without additional training or fine-tuning. 1.

Introduction
The ability to detect objects in complex scenes is essen-tial in smart applications and systems, such as autonomous vehicles [31], precision agriculture [3], 3D reconstruction and mapping [58], episodic memory [36], and remote sens-ing [4]. Broadly stated, the best performing object detec-tion methods require large amounts of densely annotated data, providing bounding boxes for all or most objects in the scene [27, 54, 88]. Such annotations are costly, time consuming to produce, and difficult to scale over large or complex datasets [8]. Recent methods address the costly procedure by using either weak annotations [2, 44, 70, 71], or general self-supervision pre-training [12, 41]. However, such methods lack generalizability to complex scenes, of-ten depending on image-wise features which lack feature granularity, leading to poor object localization and atten-tion coverage. In this work, we aim to both maximize ap-Figure 1. Image-cluster map pairs. DEVI learns category-specific, dense features end-to-end from egocentric videos with-out using any annotations. Our method is able to distinguish different-category objects, while also remaining consistent for same-category objects. Best viewed in color; colors are random. plicable scene complexity and minimize annotation costs by learning a class-agnostic object detector from highly diverse videos without using any annotations.
We take particular interest in egocentric settings, for sev-eral reasons. The first is its complexity: the way humans perceive the world is markedly different from that of many popular datasets (also referred to as “internet images” or exocentric views), resulting in notable new challenges. In-ternet images [22, 27, 54]–until recently the primary fo-cus of most computer vision methods–capture highly cu-rated, object-centric, specific instances in time removed from global context and filtered from noise and undesired frames; many involve professional and/or manual framing with clear composite objectives.
In contrast, egocentric videos typically capture unscripted, “in-the-wild” scenes, replete with dense environments filled with many, diverse objects in varying scales. These significant domain differ-ences result in a weak inductive bias between the internet images domain (e.g. COCO [54], ImageNet [22]) and the egocentric domain (e.g. Ego4D [36], EpicKitchens [18]), making transfer learning highly difficult [52, 72]; methods designed for and trained on non-egocentric datasets struggle when directly applied to egocentric settings, motivating the development of egocentric-specific methods [9, 19, 51, 67].
gories and therefore learn category-specific features, unlike common self-supervised methods which learn image-wise, general features. To the best of our knowledge, we are the first to learn effective self-supervised features from egocen-tric videos. We qualitatively demonstrate the ability of our method to generate category-specific features in Fig. 1.
Our contributions in this work are as follows: 1. We present a self-supervised object detection model from egocentric videos (DEVI) that estimates loca-tions of objects in complex scenes. 2. We propose loss functions inspired by computational appearance methods and tuned to egocentric percep-tion named the multi-view and scale-regression losses. 3. Our object residual module extends existing work on patch representation learning and complex scene understanding to learn category-specific features and precise representation of ambiguous patches without hand-crafted assumptions. 2.