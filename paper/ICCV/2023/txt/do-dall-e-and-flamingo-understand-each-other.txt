Abstract
The ﬁeld of multimodal research focusing on the com-prehension and creation of both images and text has wit-nessed signiﬁcant strides. This progress is exempliﬁed by the emergence of sophisticated models dedicated to image captioning at scale, such as the notable Flamingo model and text-to-image generative models, with DALL-E serving as a prominent example. An interesting question worth ex-ploring in this domain is whether Flamingo and DALL-E understand each other. To study this question, we propose a reconstruction task where Flamingo generates a descrip-tion for a given image and DALL-E uses this description as input to synthesize a new image. We argue that these models understand each other if the generated image is similar to the given image. Speciﬁcally, we study the re-lationship between the quality of the image reconstruction and that of the text generation. We ﬁnd that an optimal description of an image is one that gives rise to a gener-ated image similar to the original one. The ﬁnding mo-tivates us to propose a uniﬁed framework to ﬁnetune the text-to-image and image-to-text models. Concretely, the re-construction part forms a regularization loss to guide the tuning of the models. Extensive experiments on multiple datasets with different image captioning and image gener-ation models validate our ﬁndings and demonstrate the ef-fectiveness of our proposed uniﬁed framework. As DALL-E and Flamingo are not publicly available, we use Stable Dif-fusion and BLIP in the remaining work. Project website: https://dalleflamingo.github.io. 1.

Introduction
Recently, multimodal research that aims to improve ma-chine understanding of images and text has made signif-icant advances [42, 43, 46, 8, 38, 59, 67, 21]. Text-to-image generation models such as DALL-E [44, 43] and
Stable Diffusion (SD) [46] are capable of converting com-plex textual descriptions [47] from real-world scenarios into high-ﬁdelity images [43, 24, 40, 44]. Conversely, image-to-text generation models, e.g., Flamingo [2] and
*Equal contribution. Correspondence to ha.li@campus.lmu.de
Captioning
Text-to-Image
BLIP
A yellow and black bird is on tree branch
A bird perched in a tree branch next to leaves
SD
SD
Text-to-Image
Captioning
A tiny kitten is sitting on wooden floor near shoes
SD
BLIP
BLIP
A small kitten sitting on floor next to a pair of shoes
A small kitten sitting on top of a wooden floor
Figure 1. Illustration of the communication tasks between SD and
BLIP. Top: SD generates an image for each caption created by
BLIP, where an accurately reconstructed image indicates a more precise caption. Bottom: The reverse task involves SD generating image candidates, which are then used by BLIP to produce cap-tions for those images. The best image that represents a text is the one that leads to the best reconstruction of the original text.
BLIP [35], exhibit the ability to comprehend the intri-cate semantics present in images and produce coherent de-scriptions [57, 26, 56, 55, 62, 37, 60]. Despite the close-ness of the image captioning and text-to-image generation tasks, they are often studied in isolation from each other, i.e., the communication between these models is under-explored [38, 30]. This raises an interesting question: do image-to-text generation models and text-to-image genera-tion models possess mutual understanding? Concretely, we investigate this question by letting an image-to-text model,
BLIP, generate a text description for a given image, which subsequently serves as input to a text-to-image model, SD, to synthesize a new image1. We argue that BLIP and SD understand each other if the generated image is similar to the source image. Such mutual understanding may en-hance their respective abilities to comprehend underlying concepts, resulting in superior caption generation and im-age synthesis. Figure 1 illustrates this idea, where the upper caption is a better representation of the input image than the 1The initial idea of this work was motivated by Flamingo and DALL-E.
However, their model weights are unavailable at the time of publication.
lower caption as it leads to a more faithful reconstruction of the original image.
To verify this assumption, we design two reconstruc-tion tasks: image-text-image and text-image-text, shown in Figure 1. For the ﬁrst reconstruction task, we evaluate the similarity between the semantics of the generated im-age and the input image, e.g., by computing the distance of image features extracted with a pretrained CLIP image en-coder [42]. Afterward, we compare the generated text with human-annotated captions to assess the quality of the gen-erated text [53]. Our experiments reveal that the quality of the reconstruction depends on the quality of the generated the best description text. This leads to our ﬁrst ﬁnding: for an image is the description that enables the generative model to recreate the original image. Similarly, we design the reverse task where SD generates an image from a given text, and subsequently, BLIP produces a text from the gen-erated image. We ﬁnd that the best image representation for text is the one that generated the original text. We conjec-ture that through the reconstruction task, information on the input image is well preserved in the textual description and that meaningful description leads to a faithful recovery back to the image modality.
Based on our ﬁndings, we propose a novel ﬁnetuning framework that facilitates communication between text-to-image and image-to-text models, enabling them to talk to each other. Speciﬁcally, in our framework, a generative model not only receives training signals from human labels but also from a reconstruction loss. For a given image or text, one model ﬁrst generates a representation of the input in the other modality and then the other model converts this representation back to the input modality. The reconstruc-tion part forms a regularization loss to guide the ﬁnetun-ing of the ﬁrst model. In this way, they acquire not only human supervision but also self-supervision that the gen-eration should lead to a more accurate reconstruction. For example, the image captioning model should favor captions that not only match the labeled image-text pairs but also those that can lead to reliable reconstructions.
Our work is closely related to inter-agent communica-tion. Language is a major means of exchanging informa-tion between agents. But how can we be sure that the ﬁrst agent has the same understanding of what a cat or a dog is as the second agent? In this paper, we have the ﬁrst agent analyze an image and produce a text describing that image.
The second agent then obtains the text and simulates an im-age based on the text. This latter process can be thought of as an embodiment process [52]. We propose that commu-nication is successful if the image simulated by the second agent is close to the image the ﬁrst agent received as input.
In essence, we test the effectiveness of language, which is the main communication venue of humans.
We conduct experiments leveraging the off-the-shelf in particular recently models [42, 35, 10, 45, 46, 64], developed large-scale pre-trained image captioning mod-els [35, 34] and image generation models [46, 64]. Exten-sive experiments demonstrated the advantages of our pro-posed framework for various generative models, in both training-free and ﬁnetuning settings. Speciﬁcally, in the training-free paradigm, our framework signiﬁcantly en-hanced the caption and image generation, whereas, for ﬁne-tuning, we achieved improved results for both generative models. Our main contributions are summarized as follows:
• Framework: To the best of our knowledge, we are the ﬁrst to explore the communication of standard alone image-to-text and text-to-image generative mod-els through human-interpretable text and image repre-sentations. Contrastively, related work uniﬁes image and text generation implicitly through an embedding space.
• Findings: We ﬁnd that the quality of a caption can be evaluated by assessing the image reconstruction pro-duced by a text-to-image model. The best caption for an image is one that leads to the most accurate recon-struction of the original image. Similarly, the best im-age for a caption is the image that leads to the best reconstruction of the original text.
• Improvements: Based on our ﬁndings, we propose a uniﬁed framework to enhance both the image-to-text and text-to-image models. This involves ﬁnetuning the image-to-text model using a reconstruction loss com-puted by a text-to-image model as regularization, and
ﬁnetuning the text-to-image model using a reconstruc-tion loss computed by an image-to-text model. We an-alyzed and veriﬁed the effectiveness of our framework. 2.