Abstract
Diffusion-based methods can generate realistic images and videos, but they struggle to edit existing objects in a video while preserving their appearance over time. This prevents diffusion models from being applied to natural video editing in practical scenarios. In this paper, we tackle this problem by introducing temporal dependency to exist-ing text-driven diffusion models, which allows them to gen-erate consistent appearance for the edited objects. Specif-ically, we develop a novel inter-frame propagation mech-anism for diffusion video editing, which leverages the con-cept of layered representations to propagate the appearance information from one frame to the next. We then build up a text-driven video editing framework based on this mecha-nism, namely StableVideo, which can achieve consistency-aware video editing. Extensive experiments demonstrate the strong editing capability of our approach. Compared with state-of-the-art video editing methods, our approach shows superior qualitative and quantitative results. Our code is available at this https URL. 1.

Introduction
Recent years have witnessed significant progress in ex-tensive computer vision tasks taken by deep learning. Nev-*The work was done when the author was with MSRA as an intern. ertheless, natural video editing, which aims at manipulat-ing the appearance of target objects and scenes, still faces two essential challenges that are deterministic to the editing quality: the generator equipped with rich prior knowledge that consistently produces high-fidelity edited contents ad-hering faithfully to the original geometry of the target ob-jects, and the propagator that disseminates the edited con-tents throughout the entire video while keeping highly tem-poral consistency.
The flourish of text-driven generative diffusion models pre-trained on large-scale image and language data [34, 14, 53, 16, 41, 5] provides impressive generation quality. Sev-eral diffusion-based methods achieve good performance in image editing [2, 31], but few methods have tried to apply diffusion models in video editing, since it is challenging to modify existing objects while preserving their appearance over the entire video [49, 12, 26]. Dreamix [27] proposes a solution to generate consistent video according to input im-age/video and prompts. However, it focuses more on gener-ating smooth motions, e.g., pose and camera movements, rather than maintaining geometric consistency of the ob-jects across time. Moreover, such video diffusion models often suffer from huge computing complexity which is not friendly for practical applications.
Neural layered atlas (NLA) [24, 23] tries to tackle the temporal continuity problem by decomposing the video into a set of atlas layers, each of which describes one target ob-ject to be edited. For each atlas layer, the positions of the 1
video are mapped into the corresponding 2D positions in it, so that semantically correspondent pixels over the whole video can be represented by the same atlas position.
In-stead of frame-by-frame editing, NLA edits atlas layers to ensure that the modifications can be precisely mapped back to video frames for temporal smoothness. Text2LIVE [1] provides a text-driven appearance manipulation solution of adding additional edit layers on atlases, in which a specific generator for the edit layers is trained. Although it achieves good results with strict structure preserved, it is not able to apply thorough editing. Moreover, the specifically trained generator also limits the richness of the generated contents.
This brings up the question: Could text-driven diffu-sion video editing achieve high temporal consistency? In-tuitively, employing text-driven diffusion models to edit the atlases corresponding to the target objects could reach such goal. However, this gives rise to drawbacks rather than ben-efits. Being the summary of the whole video, atlases always have distorted appearance due to the viewpoint and cam-era movement, which are required to be specifically pre-trained and generated as in [1]. Diffusion models may fail in generating satisfied atlas pixels in many cases, so that the corresponding edited frames will also be contaminated. To answer the question, we present two concepts for utilizing diffusion models in video editing. Firstly, instead of edit-ing the atlases directly, we propose to update the atlases via editing key video frames. Secondly, we introduce tempo-ral dependency constraints for diffusion models to generate objects with consistent appearance across time.
Based on analysis above, we present a novel dif-to perform fusion video editing approach, StableVideo, consistency-aware video editing.
In specific, we propose two effective technologies for this purpose. Firstly, to edit the objects with consistent appearance, we design an inter-frame propagation mechanism on top of the existing diffu-sion model [55], which can generate new objects with co-herent geometry across time. Secondly, to achieve temporal consistency by leveraging NLA, we design an aggregation network to generate the edited atlases from the key frames.
We then build up a text-driven diffusion-based framework, which provides high-quality natural video editing. We con-duct extensive qualitative and quantitative experiments to demonstrate the capability of our approach. Compared with state-of-the-art methods, our approach achieves superior re-sults with much lower complexity.
In summary, we present the following contributions:
• To our best knowledge, we are the first to solve the consistency problem of diffusion video editing by con-sidering the concept of layered atlas approaches, which provides an efficient and effective way for this topic.
• We present a new video editing framework which can manipulate the appearance of the objects with high ge-ometry and appearance consistency across time. Our method can be easily applied to other text-driven dif-fusion models.
• We conduct extensive experiments on a variety of nat-ural videos, which shows superior editing performance compared with state-of-the-art methods. 2.