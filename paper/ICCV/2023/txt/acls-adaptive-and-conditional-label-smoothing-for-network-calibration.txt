Abstract
We address the problem of network calibration adjusting miscalibrated confidences of deep neural networks. Many approaches to network calibration adopt a regularization-based method that exploits a regularization term to smooth the miscalibrated confidences. Although these approaches have shown the effectiveness on calibrating the networks, there is still a lack of understanding on the underlying principles of regularization in terms of network calibra-tion. We present in this paper an in-depth analysis of exist-ing regularization-based methods, providing a better under-standing on how they affect to network calibration. Specif-ically, we have observed that 1) the regularization-based methods can be interpreted as variants of label smoothing, and 2) they do not always behave desirably. Based on the analysis, we introduce a novel loss function, dubbed ACLS, that unifies the merits of existing regularization methods, while avoiding the limitations. We show extensive exper-imental results for image classification and semantic seg-mentation on standard benchmarks, including CIFAR10,
Tiny-ImageNet, ImageNet, and PASCAL VOC, demonstrat-ing the effectiveness of our loss function. 1.

Introduction
Humans have an ability to make well-calibrated deci-sions, such that confidence levels of decisions reflect like-lihoods of underlying events accurately. On the contrary, deep neural networks (DNNs) often struggle to achieve such levels of calibration, which is problematic particu-larly in applications involving high levels of uncertainty and reasoning, including autonomous driving [9, 13, 31] and medical diagnosis [1, 14, 23].
For example, su-pervised approaches to image classification typically ex-ploit one-hot encoded labels and a softmax cross-entropy loss [3, 10, 19, 26] to train the networks. The loss encour-ages minimizing the entropy of network outputs, i.e., prefer-*Corresponding author
Table 1: Comparison of regularization-based methods for network calibration. Expected calibration error (ECE) is computed with 15 bins using ResNet-50 [11] on Tiny-ImageNet [5]. We denote by
△ adaptive or conditional regularizers with negative effects. LS:
Label smoothing.
Method
LS [32]
FLSD [26]
CPC [3]
MDCA [12]
MbLS [19]
CRL [25]
ACLS
Adaptive
Regularization (AR)
--△
△
-△
✓
Conditional
Regularization (CR)
----△
△
✓ (%)
ECE
↓ 3.17 2.91 3.12 2.77 1.64 1.65 1.05 ring the Dirac delta distribution with a peak at the one-hot label, which causes overconfident predictions, while overly penalizing uncertainty in the predictions [19].
Recently, a variety of confidence calibration methods have been introduced to address this problem [3, 6, 8, 10, 12, 19, 22, 25, 26, 27], which can broadly be divided into two categories: Post-hoc methods and regularization-based methods. Post-hoc approaches to network calibration adjust the predictions of a pre-trained model at test time, typically using additional trainable parameters [6, 10, 30]. For exam-ple, a temperature scaling technique [30] multiplies logits by a temperature parameter, resulting in a softened distri-bution of predictions that can help mitigate the overconfi-dence problem. While post-hoc approaches are effective and computationally cheap, they mainly have two limita-tions. First, they require a held-out dataset to tune the addi-tional parameters [12, 26]. Second, post-hoc approaches as-sume that training and test samples are drawn from the same distribution [19], which limits performance under distribu-tion shifts between training and test datasets [12, 16, 26].
Regularization approaches integrate calibration techniques into the training process, e.g., in a form of objective func-tions [3, 12, 19, 25, 26]. These methods regularize net-work outputs implicitly [26, 27] or explicitly [3, 12, 19, 25] by penalizing overconfident or underconfident predictions,
encouraging the distribution of predictions to be uniform.
While regularization approaches have shown the effective-ness, the influence of regularization terms on network cali-bration remains unclear, and only a limited number of stud-ies have explored to delve deeper into these approaches. For instance, the works of [19, 27] have demonstrated that the label smoothing technique [32] is also effective for network calibration. In addition, the focal loss [18], which was ini-tially developed for object detection, has been proven to raise the entropy of network predictions, performing reg-ularization implicitly [19, 26].
We present in this paper a theoretical and empirical analysis of various regularization-based methods, including
LS [32], FLSD [26], CPC [3], MDCA [12], MbLS [19], and CRL [25], to better understand the underlying princi-ples of these approaches. Our analysis on the gradients of objective functions for the regularization-based methods re-veals that 1) these methods can be viewed as variants of the label smoothing technique [32], and differ only in how they determine the degree of label smoothing, and 2) they do not always behave as expected, and often produce unde-sired results in terms of network calibration. We further cat-egorize the regularization-based methods into three groups based on the type of label smoothing: Adaptive regulariza-tion (AR) [3, 12], conditional regularization (CR) [19], and a combination of them [25]. AR adjusts the strength of reg-ularization for (one-hot encoded) training labels adaptively based on output probabilities of a network across classes, which is more beneficial for network calibration than the vanilla label smoothing technique (CPC [3], MDCA [12] vs. LS [32] in Table 1). Ideally, the label of a true class should decrease in accordance with the degree of increase in a corresponding output probability, whereas the labels of false classes should increase proportionally with the de-gree of decrement in each output probability. However, our observation suggests that the regularization-based methods using AR [3, 12] do not consistently behave as in the ideal scenario. Specifically, when the output probability is ex-ceedingly high, the label of a true class rather decreases slightly. CR modifies training labels selectively based on a specific criterion, e.g., using margin-based penalties [19].
Since output probabilities of a network are not always mis-calibrated, CR performs regularization on the miscalibrated ones only, thus showing better calibration capability than the label smoothing technique (MbLS [19] vs. LS [32] in
Table 1). However, MbLS [19] using CR is not likely to reg-ularize the training label of a true class, which leads to the overconfidence problem. A hybrid method, e.g., CRL [25], takes advantages of AR and CR, but it would inherit the limitations of both approaches. We will provide a more de-tailed analysis in Sec. 3.2.
Based on the gradient analysis for the regularization-loss function, based approaches, we introduce a novel dubbed ACLS, for network calibration. ACLS combines the strengths of AR and CR, while mitigating the drawbacks of the regularization-based methods [3, 12, 18, 19, 25, 26, 32], providing better calibration results (Table 1). On the one hand, it determines the degree of label smoothing adap-tively for each class, while avoiding the undesirable as-pects observed in the regularization-based methods using
AR [3, 12]. Specifically, ACLS regularizes the labels of true object classes more strongly, as corresponding output probabilities increase, and vice versa for other classes. On the other hand, it exploits a predefined margin to determine whether to adjust training labels, similar to the regulariza-tion method using CR [19], but also modifies the label of a true class to smooth a corresponding probability, prevent-ing the overconfidence problem. Experimental results on standard benchmarks [5, 7, 15, 17] demonstrate that net-works trained with ACLS outperform the state of the art in terms of expected calibration error (ECE) and adaptive
ECE (AECE). Our main contributions can be summarized as follows:
•
•
•
We present an in-depth analysis of existing loss functions for network calibration [3, 12, 18, 19, 25, 26, 32]. We show that current calibration methods can be viewed as variations of the label smoothing technique [32], and they are limited to prevent overconfidence and/or underconfi-dence problems.
Based on the analysis, we present a new loss function,
ACLS, that retains the advantages of AR and CR, while overcoming the negative effects of existing calibration methods.
We set a new state of the art on standard benchmarks [5, 15, 17], including CIFAR10 [15], Tiny-ImageNet [17],
ImageNet [5], and PASCAL VOC [7], demonstrating the effectiveness of our method with extensive experiments and ablation studies. 2.