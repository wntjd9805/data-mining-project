Abstract
Choose Action w/ Min Distance!
The ﬁeld of visual representation learning has seen explosive growth in the past years, but its beneﬁts in robotics have been surprisingly limited so far. Prior work uses generic visual representations as a basis to learn (task-speciﬁc) robot action policies (e.g., via be-havior cloning). While the visual representations do accelerate learning, they are primarily used to encode visual observations. Thus, action information has to be derived purely from robot data, which is expensive to collect! In this work, we present a scalable alternative where the visual representations can help directly infer robot actions. We observe that vision encoders express relationships between image observations as distances (e.g., via embedding dot product) that could be used to eﬃciently plan robot behavior. We operationalize this insight and develop a simple algorithm for acquiring a distance function and dynamics predictor, by ﬁne-tuning a pre-trained representation on human collected video sequences. The ﬁnal method is able to substan-tially outperform traditional robot learning baselines (e.g., 70% success v.s. 50% for behavior cloning on pick-place) on a suite of diverse real-world manipula-tion tasks. It can also generalize to novel objects, with-out using any robot demonstrations during train time.
For visualizations of the learned policies please check: https://agi-labs.github.io/manipulate-by-seeing/. 1.

Introduction
The lack of suitable, large-scale data-sets is a major bottleneck in robot learning. Due to the physical na-ture of data collection, robotics data-sets are: (a) hard to scale; (b) collected in sterile, non-realistic environ-ments (e.g. robotics lab); (c) too homogeneous (e.g. toy objects with ﬁxed backgrounds/lighting). In con-trast, vision data-sets contain diverse tasks, objects,
∗Denotes equal contribution.
.3
Goal Image
.4
.5
Current State
Figure 1: This paper proposes to solve a range of ma-nipulation tasks (e.g. pushing) by learning a functional distance metric within the embedding space of a pre-trained network. This distance function – in combina-tion with a learned dynamics model – can be used to greedily plan for robot actions that reach a goal state.
Our experiments reveal that the proposed method can outperform SOTA robot learning methods across four diverse manipulation tasks. and settings (e.g. Ego4D [14]). Therefore, recent ap-proaches have explored transferring priors from large scale vision data-sets to robotics settings. What is the right way to accomplish this?
Prior work uses vision data-sets to pre-train repre-sentations [31, 29, 32] that encode image observations as state vectors (i.e. s = R(i)). This visual represen-tation is then simply used as an input for a controller learned from robot data – e.g. a policy π(a s), trained with expert data via Behavior Cloning [35], or a Value function V (s), trained using exploratory roll-outs via
Reinforcement Learning [43]. Is this approach the most eﬃcient way to use pre-trained representations? We ar-gue that pre-trained networks can do more than just represent states, since their latent space already en-codes semantic, task-level information – e.g. by plac-| 1
ing semantically similar states more closely together.
Leveraging this structure to infer actions, could enable us to use signiﬁcantly less robotic data during train time.
Our paper achieves this by ﬁne-tuning a pre-trained representation into: (a) a one-step dynamics module,
F (s, a), that predicts how the robot’s next state given the current state/action; and (b) a “functional distance module”, d(s, g), that calculates how close the robot is to achieving its goal g in the state s. The distance function is learned with limited human demonstration data, using a contrastive learning objective (see Fig. 2).
Both d and F are used in conjunction to greedily plan robot actions (see Fig. 1 for intuition). Our exper-iments demonstrate that this approach works better than policy learning (via Behavior Cloning), because the pre-trained representation itself does the heavy lift-ing (thanks to its structure) and we entirely dodge the challenge of multi-modal, sequential action prediction.
Furthermore, our learned distance function is both sta-ble and easy to train, which allows it to easily scale and generalize to new scenarios.
To summarize, we show a simple approach for ex-ploiting the information hidden in pre-trained visual representations. Our contributions include:
• Developing a simple algorithm for acquiring a dis-tance function and dynamics model by ﬁne-tuning a pre-trained visual representation on minimal (human collected) data.
• Creating an eﬀective manipulation controller that substantially outperforms State-of-the-Art (SOTA) prior methods from the robot learning community (e.g. Behavior Cloning [35], Oﬄine-RL [20], etc.).
• Demonstrating that our approach can handle four realistic manipulation tasks, generalize to new ob-jects and settings, and solve challenging scenarios w/ multi-modal action distributions. 2.