Abstract
Vision Transformers (ViTs) have demonstrated power-ful representation ability in various visual tasks thanks to their intrinsic data-hungry nature. However, we unexpect-edly find that ViTs perform vulnerably when applied to face recognition (FR) scenarios with extremely large datasets.
We investigate the reasons for this phenomenon and dis-cover that the existing data augmentation approach and hard sample mining strategy are incompatible with ViTs-based FR backbone due to the lack of tailored consideration on preserving face structural information and leveraging each local token information. To remedy these problems, this paper proposes a superior FR model called TransFace, which employs a patch-level data augmentation strategy named DPAP and a hard sample mining strategy named
EHSM. Specially, DPAP randomly perturbs the amplitude information of dominant patches to expand sample diver-sity, which effectively alleviates the overfitting problem in
ViTs. EHSM utilizes the information entropy in the lo-cal tokens to dynamically adjust the importance weight of easy and hard samples during training, leading to a more stable prediction. Experiments on several bench-marks demonstrate the superiority of our TransFace. Code and models are available at https://github.com/
DanJun6737/TransFace. 1.

Introduction
Over the past few years, Convolutional Neural Networks (CNNs) [23, 32] have achieved remarkable success in the computer vision community, thanks to the availability of large-scale datasets. Recently, the introduction of Vision
Transformers (ViTs) [12] has caught the attention of the
* Equal Contribution, † Corresponding Author . computer vision community due to their powerful repre-sentation abilities. Unlike CNN models, ViTs lack some convolution-like inductive biases, such as translation equiv-ariance and locality, leading to challenges in the conver-gence of the ViTs backbone. To remedy this problem, pi-oneering works [61, 12, 4, 82] point out the rationale be-hind this derives from its data-hungry nature, indicating that a superior ViTs representation would be supported by large-scale training data. By taking advantage of their in-trinsic data-hungry property, ViTs are commonly used to serve as an alternative backbone on several visual tasks
[44, 12, 61, 58, 15, 17].
However, when considering an extremely data-adequate scenario to satisfy ViTs’ data-hungry property, namely Face
Recognition (FR), we unexpectedly discover that the per-formance of ViTs is almost equal to that of CNNs [80]. To explore why ViTs perform vulnerably in the FR realm, we investigate the training process of ViTs. From a data-centric perspective, we discover that the instance-level data aug-mentation approach and hard sample mining strategy are incompatible for ViTs-based FR backbone due to the lack of tailored consideration on preserving face structural in-formation and leveraging each local token information (il-lustrated in Fig. 1). To handle these drawbacks, we make two following efforts: (i) Identify why and how to devise a patch-level data augmentation strategy on the ViTs-based
FR backbone. (ii) Unveil why and how to mine representa-tive token information.
Patch-level Data Augmentation Strategy. Due to the lack of inductive biases, ViT-based models are hard to train and prone to overfitting [61, 12, 4]. To alleviate the overfitting phenomenon, existing works [61, 82, 70] attempt several data augmentation strategies, such as Random Erasing [81],
Mixup [76], CutMix [75], RandAugment [7] and their vari-ants [4, 19, 61, 38], to construct diverse training samples.
However, these instance-level data augmentation strategies are not suitable for the FR task, because they will inevitably
destroy some key structural information of the face identity (as shown in the top of Fig. 1), which may lead the ViTs to optimize in the incorrect direction. Furthermore, the recent study [82] observes that ViTs are actually prone to over-fitting to certain local patches during training, resulting in severely impaired generalization performance of the model.
For example, in the FR task, the prediction of ViT may be dominated by a few face patches (e.g., eyes and forehead).
Therefore, once these key patches are disturbed (e.g., a su-perstar wearing sunglasses or a hat), the model tends to make spurious decisions. These problems seriously affect the large-scale deployment of ViT-based FR models in real scenarios.
To solve the aforementioned problem, motivated by the structural information preserving property of the Fourier phase spectrum [50, 52, 73, 49], we introduce a patch-level data augmentation strategy named Dominant Patch Ampli-tude Perturbation (DPAP). Without destroying the fidelity and structural information of the face, DPAP can efficiently expand sample diversity. Concretely, DPAP uses a Squeeze-and-Excitation (SE) module [24] to screen out the top-K patches (dominant patches), then randomly mixes their amplitude information, and combines it with the original phase information to generate diverse samples.
Different from previous data augmentation strategies, the proposed DPAP cleverly utilizes the prior knowledge (i.e., the position of dominant patches) provided by the model to augment data, which can more precisely alleviate the over-fitting problem in ViTs. Furthermore, as diverse patches are continuously generated, DPAP also indirectly encour-ages ViTs to utilize other face patches, especially some patches that are easily ignored by the deep network (e.g., ears, mouth, and nose), to make more confident decisions.
Hard Sample Mining Strategy. As demonstrated in Refs.
[36, 26, 3], hard sample mining technology plays an impor-tant role in boosting the model’s final performance via con-tinuously assimilating knowledge from effective/hard sam-ples. Most previous works are specially designed for CNNs, they usually adopt several instance-level indicators of the sample, such as prediction probability [36, 27], prediction loss [14, 56], and latent features [53], to mine hard samples (as shown in the bottom of Fig. 1). However, the recent study [82] has shown that the prediction of ViT is mainly determined by only a few patch tokens, which means that the global token of ViTs may be dominated by a few local tokens. Therefore, directly using such biased indicators to mine hard sample is suboptimal for ViTs, especially when some dominant local tokens are ignored.
To better mine hard samples, inspired by the informa-tion theory [51, 55, 5], we propose a novel hard sample mining strategy named Entropy-guided Hard Sample Min-ing (EHSM). EHSM treats the ViT as an information pro-cessing system, which dynamically adjusts the importance
Figure 1. (Top) Previous data augmentation approaches may de-stroy the fidelity and structural information of face identity when augmenting samples. Our DPAP strategy not only constructs di-verse samples but also effectively preserves the key information of the face. (Bottom) Existing hard sample mining methods usu-ally adopt several instance-level indicators to measure sample dif-ficulty, which is suboptimal for ViTs. Our EHSM strategy lever-ages information entropy from all local tokens to mine hard sam-ples. weight of easy and hard samples in terms of the total amount
It is worth of information contained in the local tokens. mentioning that EHSM has the potential to encourage the
ViT to fully mine fine-grained information contained in each face patch, especially some less-attended face cues (e.g., lip and jaw), which greatly enhances the feature rep-resentation power of each local token (as verified by our experiment in Fig. 2 and Fig. 5). In this way, even if some important image patches are destroyed, the model can also make full use of the remaining face cues to generalize the global token, leading to a more stable prediction.
The following are the main contributions of this paper: (1) A patch-level data augmentation strategy named
DPAP is introduced to effectively alleviate the overfitting problem in ViTs. (2) A novel hard sample mining strategy named EHSM is proposed to enhance the stability of FR model prediction. (3) Experimental results on various popular face bench-marks have shown the superiority of our method, e.g., we achieve 97.61% accuracy on “TAR@FAR=1E-4” of the
IJB-C benchmark using the Glint360K training set. 2.