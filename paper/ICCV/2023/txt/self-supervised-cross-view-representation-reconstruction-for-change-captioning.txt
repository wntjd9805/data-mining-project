Abstract
Change captioning aims to describe the difference be-tween a pair of similar images.
Its key challenge is how to learn a stable difference representation under pseudo changes caused by viewpoint change.
In this paper, we address this by proposing a self-supervised cross-view representation reconstruction (SCORER) network. Con-cretely, we first design a multi-head token-wise matching to model relationships between cross-view features from
Then, by maximizing cross-similar/dissimilar images. view contrastive alignment of two similar images, SCORER learns two view-invariant image representations in a self-supervised way. Based on these, we reconstruct the rep-resentations of unchanged objects by cross-attention, thus learning a stable difference representation for caption gen-eration. Further, we devise a cross-modal backward rea-soning to improve the quality of caption. This module re-versely models a “hallucination” representation with the caption and “before” representation. By pushing it closer to the “after” representation, we enforce the caption to be informative about the difference in a self-supervised man-ner. Extensive experiments show our method achieves the state-of-the-art results on four datasets. The code is avail-able at https://github.com/tuyunbin/SCORER. 1.

Introduction
Change captioning is a new task of vision and language, which requires not only understanding the contents of two similar images, but also describing their difference with nat-*Corresponding authors
Figure 1. The examples of change captioning. (a) is from a surveil-lance scene with underlying illumination change. (b) is from an image editing scene. (c) shows that with both object move and moderate viewpoint change. (d) shows that with both object move and extreme viewpoint change. Changed objects and referents are shown in red and green boxes, respectively. ural language. In real world, this task brings a variety of ap-plications, such as generating elaborated reports about mon-itored facilities [8, 10] and pathological changes [18, 14].
While single-image captioning is already regarded as a very challenging task, change captioning carries additional difficulties. Simply locating inconspicuous differences is one such challenge (Fig. 1 (a) (b)). Further, in a dynamic environment, it is common to acquire two images under dif-ferent viewpoints, which leads to pseudo changes about ob-jects’ scale and location (Fig. 1 (c) (d)). As such, change
captioning needs to characterize the real change while re-sisting pseudo changes. To locate change, the most intu-itive way is to subtract two images [22, 7], but this risks computing difference features with noise if two images are unaligned [31]. Recently, researchers [25] find that same objects from different viewpoints would have similar fea-tures, so they match object features between two images to predict difference features. This paradigm has been fol-lowed by some of the recent works [11, 24, 38, 31, 30].
Despite the progress, current match-based methods suf-fer from learning stable difference features under pseudo changes.
In detail, the matching is directly modeled be-tween two image features, usually by cross-attention. How-ever, the features of corresponding objects might shift un-der pseudo change. This case is more severe under drastic viewpoint changes (Fig. 1 (d)). Such feature shift appearing in most objects would overwhelm the local feature change, thus making it less effective to directly match two images.
For this challenge, we have two new observations. (1)
While the feature difference might be ignored between a pair of similar images, it is hard to be overwhelmed be-tween two images from different pairs. As such, contrastive difference learning between similar/dissimilar images can help the model focus more on the change of feature and re-sist feature shift. (2) Pseudo changes are essentially differ-ent distortions of objects, so they just construct cross-view comparison between two similar images, rather than affect-ing their similarity. Motivated by these, we study cross-view feature matching between similar/dissimilar images, and maximize the alignment of similar ones, so as to learn two view-invariant image representations. Based on these, we can reconstruct the representations of unchanged objects and learn a stable difference representation.
In this paper, we tackle the above challenge with a novel Self-supervised CrOss-view REpresentation
Reconstruction (SCORER) network, which learns a stable difference representation while resisting pseudo changes for caption generation. Concretely, given two similar images, we first devise a multi-head token-wise matching (MTM) to model relationships between cross-view features from similar/dissimilar images, via fully interacting dif-ferent feature subspaces. Then, by maximizing cross-view contrastive alignment of the given image pair, SCORER learns their representations that are invariant to pseudo changes in a self-supervised way. Based on these, SCORER mines their reliable common features by cross-attention, the representations of unchanged so as to reconstruct objects. Next, we fuse the representations into two images to highlight the unchanged objects and implicitly infer the difference. Through this manner, we can obtain the difference representation that not only captures the change, but also conserves referent information, thus generating a high-level linguistic sentence with a transformer decoder.
To improve the quality of sentence, we further design a cross-modal backward reasoning (CBR) module. CBR first reversely produces a “hallucination” representation with the full representations of sentence and “before” image, where the “hallucination” is modeled based on the viewpoint of
“before”. Then, we push it closer to the “after” representa-tion by maximizing their cross-view contrastive alignment.
Through this self-supervised manner, we ensure that the generated sentence is informative about the difference.
Our key contributions are: (1) We propose SCORER to learn two view-invariant image representations for re-constructing the representations of unchanged objects, so as to model a stable difference representation under pseudo changes. (2) We devise MTM to model relationships be-tween cross-view images by fully interacting their differ-ent feature subspaces, which plays a critical role in view-(3) We design CBR to invariant representation learning. improve captioning quality by enforcing the generated cap-(4) Our method tion is informative about the difference. performs favorably against the state-of-the-art methods on four public datasets with different change scenarios. 2.