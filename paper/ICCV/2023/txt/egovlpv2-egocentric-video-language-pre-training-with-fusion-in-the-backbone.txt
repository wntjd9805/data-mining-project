Abstract
Video-language pre-training (VLP) has become increas-ingly important due to its ability to generalize to vari-ous vision and language tasks. However, existing ego-centric VLP frameworks utilize separate video and lan-guage encoders and learn task-specific cross-modal infor-mation only during fine-tuning, limiting the development of a unified system.
In this work, we introduce the sec-ond generation of egocentric video-language pre-training (EgoVLPv2), a significant improvement from the previous generation, by incorporating cross-modal fusion directly into the video and language backbones. EgoVLPv2 learns strong video-text representation during pre-training and reuses the cross-modal attention modules to support dif-ferent downstream tasks in a flexible and efficient man-ner, reducing fine-tuning costs. Moreover, our proposed fusion in the backbone strategy is more lightweight and compute-efficient than stacking additional fusion-specific layers. Extensive experiments on a wide range of VL tasks demonstrate the effectiveness of EgoVLPv2 by achieving con-sistent state-of-the-art performance over strong baselines across all downstream. Our project page can be found at https://shramanpramanick.github.io/EgoVLPv2/. 1.

Introduction
Video-Language Pre-training (VLP) has proven to be the de-facto solution for a variety of video-text tasks, e.g., video-text retrieval [98, 66, 4], VQA [95, 104, 112], zero-shot recognition, [7, 49, 32] and video-text grounding [61, 51].
This is fueled by recent advances in vision [15, 53, 6, 4, 2, 19, 54] and language [84, 14, 52, 102, 74, 12, 73], cou-pled with large-scale data [98, 111, 59, 4, 24, 13]. Existing video-language datasets generally fall under two categories: third-person view and first-person view (egocentric). The noticeable domain gap between them restricts VLP frame-†Part of this work was done during an internship at Meta AI.
EgoMCQ (intra-vid. acc.) 60.9
QFVS (avg F-score) 52.1 49.7 57.2 45.0
EK-100
MIR (mAP) 47.3
EgoNLQ (R@5
IoU@0.3) 23.8 18.8 59.4 61.9
EK-100
MIR (nDCG) 32.7 37.9 32.1 34.1
CharadesEgo (mAP) 65.6 68.2
EgoTaskQA (acc.)
EgoMQ (R@5
IoU@0.3)
EgoVLPv2
EgoVLP [50]
Figure 1: EgoVLPv2 achieves the state-of-the-art per-formance across a broad range of egocentric video under-standing tasks (see Table 1 for details) among similar-sized baselines by incorporating cross-modal attention in the trans-former backbones to learn video-language representation. works pre-trained on third-person videos from performing well on egocentric benchmarks [50]. However, the recent introduction of a massive-scale egocentric dataset Ego4D
[24] helps unlock the full potential of egocentric VLP.
Existing egocentric VLP approaches [50, 110, 60, 3] pre-train separate (dual) video and language encoders and learn task-specific cross-modal information only during fine-tuning, limiting the development of unified egocen-tric VL frameworks. Moreover, they lack strong zero-shot inference ability on multi-modal downstream tasks.
This issue is commonly addressed by stacking dedicated fusion layers on top of the dual video and text encoders
[57, 37, 96, 82, 99, 100, 105], or with a shared video-language architecture [41, 1, 35, 83, 86]. However, these approaches introduce a large number of fusion-specific pa-(a) Dual Encoders (b) Stacked Fusion Layers (c) Shared Encoders (d) Fusion in the Backbone (Ours)
Figure 2: Four categories of VLP frameworks. (a) use separate (dual) video and text backbones, with InfoNCE [64] as the common pretraining objective [50, 110, 3, 60] (b) use cross-modal fusion layers on top of dual encoders, with MLM,
VTM, etc. as common pretraining tasks [57, 37, 96, 82] (c) use a single encoder for different modalities, with similar learning objectives as (b) [41, 1, 35] (d) Fusion in the Backbone (Ours). rameters, and the resulting encoder cannot be directly applied to uni-modal (video-only) tasks.
In this work, we present the second generation of ego-centric VLP (EgoVLPv2), a significant improvement over the previous generation [50] by incorporating cross-modal fusion directly into the video and language backbones. Our approach improves over existing VLP frameworks by: (i) fewer fusion parameters compared to stacked fusion-specific transformer layers or shared encoders, requiring less GPU memory, compute resources, and training time; (ii) the flexi-bility to switch between dual and fusion encoders, by turning on and off cross-attention fusion using a gating mechanism; (iii) being applicable to both uni- and multi-modal tasks.
Inserting cross-modal fusion directly into the backbone helps unify a wide range of dual- and fusion-encoder-based downstream tasks. Specifically, the “switching” ability of
EgoVLPv2 enables us to utilize the same pre-trained en-coders for fast retrieval and grounding tasks, which require dual and fusion encoders, respectively. Moreover, in con-trast to existing egocentric VLP frameworks that learn task-specific fusion parameters during fine-tuning, EgoVLPv2 reuses the pre-trained cross-attention modules across differ-ent tasks, significantly reducing the fine-tuning cost. This enables us to introduce query-focused video summarization as a downstream task, which has recently gained attention in the community [62, 91, 92, 30, 93, 63]. The scarcity of anno-tated data has been a bottleneck to training decent-sized mod-els end-to-end on this task, with the only available egocentric dataset, QFVS [77], providing merely 135 video-query train-ing samples. EgoVLPv2 achieves new state-of-the-art results on QFVS with a decent margin over the baselines.
In summary, our contributions are: (i) We advance a step forward in egocentric VLP by proposing EgoVLPv2, the second generation of EgoVLP [50] with cross-modal fusion in the backbone. Our proposed framework can switch be-tween dual and fusion encoders and requires 45% lesser com-pute (GMACs) than learning additional fusion-specific trans-former layers. (ii) The switching capability of EgoVLPv2 allows us to unify a wide range of dual- and fusion-encoder-based downstream tasks under the same VLP framework and reduce the task-specific fine-tuning cost by employing the same pre-trained cross-attention modules across different video-language tasks. (iii) We demonstrate the effectiveness of EgoVLPv2 on eight egocentric benchmarks and achieve state-of-the-art performance among comparable-sized back-bones. We summarize these results in Figure 1. 2.