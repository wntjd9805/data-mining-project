Abstract
In the field of monocular 3D detection, it is common practice to utilize scene geometric clues to enhance the de-tector’s performance. However, many existing works adopt these clues explicitly such as estimating a depth map and back-projecting it into 3D space. This explicit methodology induces sparsity in 3D representations due to the increased dimensionality from 2D to 3D, and leads to substantial in-formation loss, especially for distant and occluded objects.
To alleviate this issue, we propose MonoNeRD, a novel de-tection framework that can infer dense 3D geometry and occupancy. Specifically, we model scenes with Signed Dis-tance Functions (SDF), facilitating the production of dense 3D representations. We treat these representations as Neu-ral Radiance Fields (NeRF) and then employ volume ren-dering to recover RGB images and depth maps. To the best of our knowledge, this work is the first to introduce volume rendering for M3D, and demonstrates the poten-tial of implicit reconstruction for image-based 3D percep-tion. Extensive experiments conducted on the KITTI-3D benchmark and Waymo Open Dataset demonstrate the ef-fectiveness of MonoNeRD. Codes are available at https:
//github.com/cskkxjk/MonoNeRD. 1.

Introduction
Monocular 3D detection (M3D) is an active research topic in the computer vision community due to its conve-nience, low cost and wide range of applications, includ-ing autonomous driving, robotic navigation and more. The key point of the task is to establish reasonable correspon-dences between 2D images and 3D space. Some works leverage geometrical priors to extract 3D information, such as object poses via 2D-3D constraints. These constraints usually require additional keypoint annotations [10, 21] or
*Work performed during an internship at FABU Inc.
†Corresponding author
Figure 1.
Different intermediate 3D representations. Previ-ous depth-based methods generate 3D representations by back-projecting estimated depth maps to 3D space, whereas our method predicts implicit 3D representations, and obtains depth estimates through volume rendering.
CAD models [5, 34]. Other works convert estimated depth maps into 3D point cloud representations (Pseudo-LiDAR)
[32, 55, 60]. Depth estimates are also used to combine with image features [58, 33] or generate meaningful bird’s-eye-view (BEV) representations [45, 47, 48, 51], then produce 3D object detection results. These methods have made re-markable progress in M3D.
Unfortunately, current 3D representations transformed by explicit depths have some limitations. First, the lifted features obtained from depth estimates or pseudo-LiDAR exhibit an uneven distribution throughout the 3D space.
Specifically, they have high density in close range, but the density decreases as the distance increases. (See top part of Figure 1). Second, the final detection performance heav-ily depends on the accuracy of the depth estimation, which remains challenging to improve. Thus such representations
cannot produce dense reasonable 3D features for M3D.
Recent researches in Neural Radiance Fields (NeRF)
[3, 38, 64] have shown the capability to reconstruct detailed and dense 3D scene geometry and occupancy information from posed images (images with known camera poses). In-spired by NeRF and its follow-ups [20, 25, 59], we re-formulate the intermediate 3D representations in M3D to
NeRF-like 3D representations, which can produce dense reasonable 3D geometry and occupancy. To achieve this goal, we combine extracted 2D backbone features and cor-responding normalized frustum 3D coordinates to construct 3D Position-aware Frustum features (Section 4.1.1). These 3D features are used to create signed distance fields and ra-diance fields (RGB color). The signed distance fields en-code the distance to the closest surface at every location as a scalar value. This allows us to model scenes implicitly by the zero-level set of the signed distance fields. (Sec-tion 3.1). We then adopt volume rendering [35] technique to generate RGB images and depth maps from the signed distance fields and radiance fields (Section 4.1.2), supervis-ing them by original RGB images and LiDAR points (Sec-tion 4.2). While the previous depth-based methods gener-ate 3D representations based on predicted depth maps, our method generates depth maps based on 3D representations (Figure 1). It is worth noting that our approach is capable of generating dense 3D occupancy (i.e., volume density) with-out requiring explicit binary occupancy annotations for in-dividual voxels. Experiments on KITTI [17] 3D benchmark and Waymo Open Dataset [52] show the superiority of our
NeRF-like representations in monocular 3D detection.
Our main contributions are threefold:
• We present a novel detection framework, named
MonoNeRD, that connects Neural Radiance Fields and monocular 3D detection. It leverages NeRF-like continuous 3D representations to enable accurate 3D perception and understanding from a single image.
• We propose to use volume rendering to directly opti-mize 3D representations in detection tasks. To the best of our knowledge, our work is the first to introduce volume rendering for 3D detection tasks.
• Extensive experiments on KITTI [17] 3D detection benchmark and Waymo Open Dataset [52] demon-strate the effectiveness of our method, which is com-petitive with previous state-of-the-art works. This re-search presents the potential of 3D implicit reconstruc-tion for image-based 3D perception. 2.