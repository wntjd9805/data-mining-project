Abstract
Recent studies have substantiated that machine learning algorithms including convolutional neural networks often suffer from unreliable generalizations when there is a sig-nificant gap between the source and target data distribu-tions. To mitigate this issue, a predetermined distribution shift has been addressed independently (e.g., single domain generalization, de-biasing). However, a distribution mis-match cannot be clearly estimated because the target dis-tribution is unknown at training. Therefore, a conservative approach robust on unexpected diverse distributions is more desirable in practice. Our work starts from a motivation to allow adaptive inference once we know the target, since it is accessible only at testing. Instead of assuming and fixing the target distribution at training, our proposed approach allows adjusting the feature space the model refers to at ev-ery prediction, i.e., instance-wise adaptive inference. The extensive evaluation demonstrates our method is effective for generalization on diverse distributions. 1.

Introduction
A fundamental assumption of machine learning is that the training dataset represents the true distribution and the test set also follows the same true distribution. For instance, a face recognition model is trained on a dataset of thousands or millions of face images, assuming that this dataset rep-resents a set of all possible faces of human beings under all conditions like illumination or angle. In this sense, any dataset is nothing but a collection of samples from the real world, and it may not perfectly represent the true distribu-tion, even if it is huge. No dataset is free from this sampling error; rather it is just a matter of degree. Obviously, a model trained on a particular set of samples is affected by the dis-tribution. In some fortunate cases, we may have similar dis-tribution over training samples and over inference set, while distribution shift between source and target data caused by extreme sampling error could give rise to unreliable gener-alization of the model.
Although we are not able to know the true distribution in most cases, there are many practical situations where we
Figure 1. Our motivation. At training, we do not know to which distribution the model would be deployed. Thus, generalization for a predetermined distribution mismatch (e.g., SDG or UBL) is often unfavorable. The image at the bottom illustrates our sce-nario, where an arbitrary distribution could be the target. need to build a model based only on a limited set of training samples and apply it to another target data, which may nei-ther necessarily follow the training distribution, nor the true distribution. For instance, a gender classifier trained only on images of the young may not perform well for the old. For this practical need, previous research has been dedicated to tackling this issue of domain shift. Particularly, several works suggest that the performance of the model trained on the source domain could often be degraded on the out-of-distribution (OOD) target domains. This is called single domain generalization (SDG) [25, 22, 32, 7, 20, 30, 26, 5], mainly tackling an unknown domain shift between the train-ing and test sets.
On the other hand, unbiased learning (UBL) is an-other related research area, aiming at removing bias that potentially a machine learning model may rely on. As an example, a gender classifier may consider ‘age’ as an important factor if trained on (young, female) and (old, male) samples and hence frequently fail to correctly pre-dict (old, female) and (young, male) test samples. Bi-ased attributes could make the model biased and several recent works propose various approaches to mitigate this issue [3, 21, 19, 11, 19, 13, 23].
However, the type of distribution shift can be defined only when we know both the source and target distributions.
As illustrated in Fig. 1, we do not know at training on which 1
Figure 2. Distribution mismatch. Domain generalization and un-biased learning can be seen as two special cases of the distribution mismatch problem. distribution the model will be used for inference. We hy-pothesize that conventional methods may perform well or not depending on the distribution shift because they are de-signed considering one predetermined scenario.
From this perspective, we explore the generalizability of the previous methods on two special cases of distribution shift problems (i.e., biased and domain-limited distributions depicted in Fig. 2) by conducting a motivating experiment to apply state-of-the-art unbiased learning (UBL) models to single domain generalization (SDG) and vice versa. Not surprisingly, we observe they significantly underperform beyond the designated situations they were trained on (See
Sec. 3 for more details). In other words, previous methods tackling either SDG or UBL tend to be over-optimized to each specific case, failing to generalize to the other problem, even if they are indeed the same problem, i.e., domain mis-match. This quantitatively demonstrates that inaccurate es-timation of data distribution and inappropriate model selec-tion during training could significantly drop performance.
In this paper, we aim to design a framework robust on diverse data distribution mismatch problems, especially fo-cusing on convolutional neural networks (CNNs) for image recognition. CNNs learn two manuals: learning to repre-sent a feature space from input images and fitting a classi-fier from the features to the target labels. From this point of view, we hypothesize that the features helpful for image discrimination would be different depending on the data.
This would be because image data distribution implies cor-relations of the attributes in the image space including the label, which can be used as a predictive logic to match the feature space to the label space. In this respect, the model optimized to represent features and select appropriate ones only for a specific source data may not be directly applied to OOD data.
Therefore, we propose instance-wise adaptive inference (IAI) that adjusts the referenced feature space to be more suitable for each test instance at inference. Although we do not know the type of distribution mismatch at training, it can be indirectly estimated by comparing each target sam-ple with the learned representations. Specifically, our oper-ative idea is threefold: (i) to widen the feature space (since if the model has a limited feature space, there may be little opportunity to apply IAI), (ii) to disentangle the widened feature space, and (iii) to adaptively select features among them, considering each test instance at inference.
In consequence, we demonstrate that our proposed method exhibits remarkable generalizability on both SDG and UBL tasks compared to the state-of-the-art methods.
Our experimental results quantitatively verify our hypothe-sis that target data needs different features from source data for better image discrimination, justifying the effectiveness of IAI for robustness.
We summarize our main contributions as follows:
• We present a novel framework for robust learning on arbitrary diverse distributions. This is desirable in sce-narios where a model is deployed in a dynamic envi-ronment where queries often drift while the model can-not be frequently revised.
• We propose a novel method that exploits adaptive inference re-weighting instance-wise on disentangled representations in widened feature space. Extensive evaluations demonstrate our method is robust on di-verse distributions. 2. Problem Statement
Consider an instance set X = {xi|xi ∈ X , i = 1, . . . , N } for a classification problem, where X denotes the input space. The instance set X is generated by col-lecting N samples {x1, ..., xN } from the real world, and this process is not free from the sampling error, resulting in distribution shift, as mentioned at the beginning. In ma-chine learning, a model may not suffer from generalization problem when distribution shift is subtle. However, if the model is trained on a dataset with severe sampling error, its performance could be significantly degraded at inference.
Our problem setting. Let us denote the true, training (source), and testing (target) distributions by p, pS, and pT , respectively. Also, we denote the source and target datasets by DS ∼ pS and DT ∼ pT , respectively, composed of samples from the corresponding distributions. A machine learning model ˆf is trained to minimize the empirical risk
ˆf = argmin f 1
N
N (cid:88) i=1
L(yi, f (xi)), (1) where (xi, yi) ∈ DS for i = 1, ..., N . The ideal goal of our problem is maximizing its performance on true distribution p(x, y); that is, f ∗ = argmin
ˆf
Ep(x,y) (cid:104)
L(y, ˆf (x)) (cid:105)
. (2)
However, since constructing DT with pT ≃ p is im-practical, we aim to make the model f robust on diverse
{DS, DT } pairs where there is significant distribution mis-match between them, denoted by DS ̸≈ DT .
Figure 3. Exploration of previous methods on diverse distributions. (a) no-shift distribution, (b) domain-limited distribution, (c) biased distribution. Blue bars in the graphs mean SDG models and red ones are UB models. Dotted lines are the lowest performance among the corresponding task models to compare the methods of other tasks, meaning the accuracy of BASE, ADA, and UBNet for (a), (b), and (c) respectively.
Domain-limited distribution. A domain O is defined as a tuple of the input space X and a marginal distribution pX over samples X ⊂ X , i.e., O = (X , pX ). The source dis-tribution pS is called domain-limited when the source do-main OS is significantly different from the target domain
OT with XS ̸= XT , pS ̸= pT . In this sense, the domain-limited distribution is a special case of DS ̸≈ DT .
Biased distribution. Biased distribution is another spe-cial case of DS ̸≈ DT . The label space Y can be defined as a set of all possible assignments to each instance x. For (xS, yS) ∈ DS, yS is a particular one selected from Y, e.g., gender of the person in an image. Most y′ ̸= yS ∈ Y may be independent of the yS, but some might be significantly correlated with yS [13]. For the latter y′, if it also has a cor-relation with labels yT of the target dataset DT , y′ can be used as a meaningful factor for prediction. For instance, the mustache is a meaningful indicator that the person is likely a male. However, if y′ is spuriously correlated with the la-bel yS, relying on y′ would cause DS ̸≈ DT , misleading the generalization of the model, considered as a bias. For example, hair length is a well-known bias for gender classi-fication due to its high correlation with the label, although it is biologically independent. 3. Motivating Experiments
Experimental setup. We explore the performance of the state-of-the-art SDG and UBL models on three distribu-tions: no-shift (pS ≃ pT ), domain-limited, and biased one. We set ADA [25], ME-ADA [32], and L2D [30] for
SDG as competing models and LfF [21], LDR [19] and
UBNet [13] for UBL. The experiments are conducted on
CIFAR-10 [17]. We assume the training and test sets in
CIFAR-10 have no-shift distribution, i.e., pS ≃ pT . The domain-limited and biased CIFAR-10 are made by widen-ing the target domain and by intentionally planting bias, re-spectively, following the setup in LfF [21]. Specifically, one of the 10 types of corruption, {fog, snow, frost, bright-ness, contrast, spatter, elastic, jpeg compression, pixelate, saturate}, is applied to the training images per each label; e.g., (airplane, snow), (automobile, frost), and so on. The unbiased validation set is corrupted uniformly randomly for all the labels. For SDG, 12 validation sets are created with
{fog, snow, frost, zoom blur, defocus blur, glass blur, speckle noise, shot noise, impulse noise, jpeg compression, pixelate, spatter}. That is, all the validation image samples are cor-rupted by ‘snow’, meaning the ‘snow’ target domain. We use ResNet18 [10] as the baseline model.
Results and Analysis. Figure 3 shows that all the ap-proaches for SDG and UBNet outperform the base model on the no-shift distribution. Yet, LfF and LDR degrade ac-curacy significantly. All the UBL models generalize worse than SDG models for SDG task, and vice versa. Conse-quently, we observe that conventional methods are limited to addressing various sampling errors. This result implies that most previous works perform well only on a special case of distribution mismatch problems, where L2D is the only one that performs reasonably on most distributions ac-cording to our experiments. 4. Methodology
For generalization on diverse distributions, we propose three ideas: (i) widening the feature space to be referred, (ii) disentangling multiple independent representations from one another in the widened feature space, and (iii) perform-ing inference adaptively to each test example, weighting differently on disentangled features based on distribution mismatch between the source and target data. Towards this, the overall architecture of our method consists of the style generation module G and the classifier [F = {Fa, Fb}; H] (See Fig. 4). Aiming to train [F ; H] to be robust, G aids it to be generalized on diverse distributions by creating diversely stylized inputs. Our detailed model designs are depicted in the following subsections. 4.1.