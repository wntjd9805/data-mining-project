Abstract
Federated learning (FL) is a promising approach for enhancing data privacy preservation, particularly for au-thentication systems. However, limited round communica-tions, scarce representation, and scalability pose signifi-cant challenges to its deployment, hindering its full po-tential. In this paper, we propose ‘ProtoFL’, Prototypical
Representation Distillation based unsupervised Federated
Learning to enhance the representation power of a global model and reduce round communication costs. Addition-ally, we introduce a local one-class classifier based on nor-malizing flows to improve performance with limited data.
Our study represents the first investigation of using FL to improve one-class classification performance. We con-duct extensive experiments on five widely used benchmarks, namely MNIST, CIFAR-10, CIFAR-100, ImageNet-30, and
Keystroke-Dynamics, to demonstrate the superior perfor-mance of our proposed framework over previous methods in the literature. 1.

Introduction
In recent years, there has been a growing concern about privacy, leading people to hesitate when it comes to upload-ing their biological data to central data servers. Moreover, companies that possess personal information from users are strictly bound by the General Data Protection Regulation (GDPR) [44]. To address these privacy issues, Federated
Learning (FL), an emerging distributed data parallel ma-chine learning approach, has been proposed. FL lever-ages the decentralized data available on individual clients to collaboratively train a shared global model on a mediator server without the need to share personal data.
*These authors contributed equally to this work
†Corresponding author (α =0.0 is
Figure 1: Visualization of extreme non-i.i.d. a concentration parameter of Dirichlet distribution) data based FL schema shows that each device has only the data from its target class not shared among all joined devices, and prototypical representation is distributed separately to corresponding joined clients in secure from a central server.
In the context of FL, where data is decentralized across individual clients, one-class classification (OCC) can be used. This is because that OCC can determine whether a new example belongs to the target distribution or not. De-spite not using data from the non-target class, OCC has shown impressive performance [31, 38, 43, 42, 39, 40, 15, 28]. In computer vision applications, OCC is particularly useful for detecting fraud, defects, and unauthorized users.
Recent advancements in biometric authentication have highlighted the importance of FL-based OCC in computer vision (e.g., user-defined embedding-based methods [17, 26] and data-driven methods [2, 24, 32]). However, FL-based OCC methods face significant challenges, including high communication costs, limited representation, and un-stable learning processes. Additionally, centralized server-based methods [31, 42, 43] may not be suitable for FL-based OCC due to limited computing resources on each de-vice, which are a major obstacle for centralized methods that require substantial computing power.
The limitations inherent in FL-based OCC necessitated the development of a novel approach to address these chal-lenges.
In this paper, we aim to attain a more expres-sive global model without the need for frequent re-training processes between the central server and client devices on a large scale. To achieve this, we propose Prototypical representation distillation based unsupervised Federated
Learning (ProtoFL) that distills representation from an off-the-shelf model learned using off-the-shelf datasets, regard-less of individual client data. In contrast to traditional FL-based OCC, our proposed ProtoFL approach does not trans-fer parameters directly to the client, as depicted in Fig. 1.
This resolves the issues of frequent round communication costs and the need for extra data due to one-time prototype representation distillation. ProtoFL provides a novel solu-tion to the existing challenges of FL-based OCC, enabling efficient and effective global model updates.
Subsequently, we suggest a novel approach for estimat-ing the density of a target class in a distributed setting using a flow-based one-class classifier [20]. To achieve this, we conduct the estimation on individual client de-vices, using augmented latent variables for training their distributed models. Our approach leverages two key tech-niques: maximum likelihood estimation with log-likelihood and a probabilistic similarity loss function that includes
KL-divergence. By combining the distillation and one-class classification phases, we can effectively handle com-plex data distributions that are non-independent and non-identically distributed across individual clients. Our two-phase learning framework is inspired by the success of flow-based models in various applications [20, 11, 40, 1], and we demonstrate its effectiveness in handling complex data dis-tributions in the distributed settings.
The experimental findings of our proposed method demonstrate superior classification performance compared to both server-based and client-based methods on both im-age and tabular datasets. Additionally, we have validated the scalability of the learned representation and have shown that the global model learned by the ProtoFL is compatible with existing one-class classifiers as well as our one-class classifier based on the benchmark datasets. Our results indi-cate that our method is a promising approach for large-scale machine learning tasks that require robust and scalable clas-sification capabilities.
Our contributions are described as follows:
• We propose a novel unsupervised federated learning framework that effectively addresses the challenge of insufficient local training data. By leveraging normal-izing flows for local classifier learning and prototypical representation distillation, our approach enables effi-cient and effective global model updates.
• We propose a novel prototype-based representation learning method for distilling normal data represen-tation using an off-the-shelf model and dataset. Our approach demonstrates the scalability of the global model, which can be verified by adding new clients in FL-based OCC.
• We propose new federated and centralized learning methods for one-class classification, which we evalu-ate on five widely-used benchmarks. Our experiments show that our methods achieve superior performance compared to existing approaches. 2.