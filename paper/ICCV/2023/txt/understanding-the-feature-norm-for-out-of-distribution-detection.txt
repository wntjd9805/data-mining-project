Abstract
A neural network trained on a classification dataset of-ten exhibits a higher vector norm of hidden layer fea-tures for in-distribution (ID) samples, while producing rel-atively lower norm values on unseen instances from out-of-distribution (OOD). Despite this intriguing phenomenon being utilized in many applications, the underlying cause has not been thoroughly investigated. In this study, we de-mystify this very phenomenon by scrutinizing the discrimi-native structures concealed in the intermediate layers of a neural network. Our analysis leads to the following dis-coveries: (1) The feature norm is a confidence value of a classifier hidden in the network layer, specifically its max-imum logit. Hence, the feature norm distinguishes OOD from ID in the same manner that a classifier confidence does. (2) The feature norm is class-agnostic, thus it can detect OOD samples across diverse discriminative models. (3) The conventional feature norm fails to capture the deac-tivation tendency of hidden layer neurons, which may lead to misidentification of ID samples as OOD instances. To resolve this drawback, we propose a novel negative-aware norm (NAN) that can capture both the activation and de-activation tendencies of hidden layer neurons. We conduct extensive experiments on NAN, demonstrating its efficacy and compatibility with existing OOD detectors, as well as its capability in label-free environments. 1.

Introduction
Deep learning-based models are increasingly used for safety-critical applications such as autonomous driving and medical diagnosis. Despite the effectiveness of deep models in closed-set environments where all test queries are sam-pled from the same distribution of train data, the deep mod-els are reported fairly vulnerable [33, 16] to outliers from out-of-distribution [19, 51] and make highly confident but invalid predictions thereon [35]. As it is critical to prevent such malfunction in deploying deep models for open envi-ronment applications, the out-of-distribution (OOD) detec-† Corresponding author: Andrew Beng Jin Teoh
Figure 1: (left) As a discriminative model is trained, its hidden layer features exhibit higher vector norm on in-distribution sam-ples (ID) and relatively lower norm on out-of-distribution (OOD) instances. This phenomenon prevails even when the model re-duces the overall feature norm (e.g. by weight decay). (right) As a result, ID samples are separated from OOD instances with respect to the feature norm. To see its underlying cause, we analyze the discriminative structures concealed in the hidden layer. tion problem has attracted massive attention in recent years
[52].
Despite the importance of this field, only a handful of works have been devoted to understanding how the deep network becomes aware of OOD [9, 10, 8, 30, 31]. One par-ticular under-studied signal in OOD detection is the norm of feature vectors residing in the hidden layers of neural net-works. Its known behavior is that a model trained on the ID data exhibits larger values of feature norm over ID samples than the OOD instances [7, 53, 3, 28]. However, the studies are mainly empirical and provide no underlying principle of the feature norm at a fundamental level.
A preliminary attempt at understanding the feature norm has been given in the appendix of [45]. The authors of [45] argue that minimizing the cross entropy (CE) maximizes the feature norm of ID samples. However, the argument is not general. As we observe in Fig. 1, training the weight-decayed model decreases the overall feature norm, but the separation between ID and OOD remains obvious. Hence, we require a new lens to understand the underlying cause of feature norm separation.
In this work, we study why the feature norm separates
ID from OOD. To this end, we both theoretically and em-pirically show that the feature norm is equal to a confi-dence value of a classifier hidden in the corresponding layer.
Based on the existing theory on the classifier confidence
[10], the equality guarantees the detection capability of fea-ture norm.
Furthermore, our analysis indicates that the feature norm is agnostic to the class label space. This suggests that the feature norm can detect OOD using any general dis-criminative model, including self-supervised classifiers. We validate this postulation empirically under several aspects:
Firstly, by considering inter- and intra-class learning inde-pendently, we show that inter-class learning enables the fea-ture norm to separate OOD from the training fold of ID.
The intra-class learning, on the other hand, generalizes the detection capability to the test environment, enabling the feature norm to differentiate OOD from the test fold of ID.
The finding shows that inter- and intra-class learning corre-sponds to memorization and generalization, respectively, in the context of OOD detection. Secondly, we show that the detection capability of feature norm is strongly correlated to the entropy of activation (i.e. diversity of on/off status of neurons). As activation entropy is a class-agnostic charac-teristic, the finding reinforces our postulation.
In addition to that, we observe that the conventional vec-tor norm only captures the activation tendency of hidden layer neurons, but misses the deactivation counterpart. Fail-ing to account for the deactivation tendencies results in the loss of important characteristics specific to ID samples, po-tentially leading to misidentification of such instances as
OOD examples. Motivated by this drawback, we derive a novel negative-aware norm that captures both the activation and deactivation tendencies of hidden layer neurons.
We perform a thorough assessment of the NAN and demonstrate its efficacy across OOD benchmarks. Addi-tionally, we confirm that NAN is compatible with several state-of-the-art OOD detectors. Furthermore, NAN is free of hyperparameters, requires no classification layer, and does not necessitate expensive feature extraction from a bank set. Consequently, NAN can be readily deployed in scenarios where class labels are unavailable. We evaluate
NAN in unsupervised environments using self-supervised models and assess its performance on one-class classifica-tion benchmarks.
The contributions of our work are summarized as fol-lows:
• We demystify the OOD detection capability of the fea-ture norm by showing that the feature norm is a confi-dence value of a classifier hidden in the corresponding layer (Sec. 3).
• We reveal that the feature norm is class-agnostic, hence able to detect OOD using general discriminative mod-els (Sec. 4). We validate this property under several aspects including inter/intra-class learning and activa-tion entropy.
• We put forward a novel negative-aware norm (NAN), which captures both activation and deactivation ten-dencies of hidden layer neurons (Sec. 5). NAN is hyperparameter-free, label-free, and bank-set-free.
NAN can be easily integrated with state-of-the-art
OOD detectors. (Sec. 6) 2.