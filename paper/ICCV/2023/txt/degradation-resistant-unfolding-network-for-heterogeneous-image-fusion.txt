Abstract
Heterogeneous image fusion (HIF) techniques aim to en-hance image quality by merging complementary informa-tion from images captured by different sensors. Among these algorithms, deep unfolding network (DUN)-based methods achieve promising performance but still suffer from two issues: they lack a degradation-resistant-oriented fu-sion model and struggle to adequately consider the struc-tural properties of DUNs, making them vulnerable to degra-dation scenarios. In this paper, we propose a Degradation-Resistant Unfolding Network (DeRUN) for the HIF task to generate high-quality fused images even in degradation sce-narios. Specifically, we introduce a novel HIF model for degradation resistance and derive its optimization proce-dures. Then, we incorporate the optimization unfolding process into the proposed DeRUN for end-to-end training.
To ensure the robustness and efficiency of DeRUN, we em-ploy a joint constraint strategy and a lightweight partial weight sharing module. To train DeRUN, we further pro-pose a gradient direction-based entropy loss with power-ful texture representation capacity. Extensive experiments show that DeRUN significantly outperforms existing meth-ods on four HIF tasks, as well as downstream applications, with cheaper computational and memory costs. 1.

Introduction
Heterogeneous image fusion (HIF) aims to integrate im-ages acquired by different imaging sensors to generate a more robust and informative image. HIF has been investi-gated in various domains, such as infrared and visible image fusion (IVF) [61], medical image fusion (MIF) [45], and biological image fusion (BIF) [41], based on different sen-sors. By merging the complementary information from the
*Corresponding author, † Work done during the internship at Smart
Vision.
Infrared
Visible
IVF-Net [17]
DeRUN
Figure 1: IVF results under degradation scenarios, including low light and heavy smoke. DeRUN generates more visual-appealing results for its degradation resistance capacity. heterogeneous images, the fused image is expected to have better quality than individual ones and thus serves better for decision-making or subsequent processing, especially in degradation scenarios such as extreme weather in IVF, magnetic turbulence in MIF, and phase noise in BIF.
Taking the IVF task as an example, visible images typi-cally contain more details but are easily influenced by poor illumination, fog, and other factors. In contrast, infrared im-ages are resistant to lighting and weather disturbances but usually have poor texture details. Hence, IVF aims to get fused images that preserve texture details from visible im-ages and the robust thermal radiation from infrared images simultaneously. The fused images with enhanced quality and informative components can better resist degradation conditions and thus benefit high-level image analysis.
Existing HIF techniques can be classified into model-based and learning-based methods. Model-based fusion methods focus on iteratively solving the optimization-based fusion framework with manually designed fusion rules [30, 34]. Although guaranteed with strong interpretability, such methods are limited by hand-crafted feature extractors with poor generalizability, thus failing to cope with degradation conditions. Benefiting from the nonlinear capacity of con-volutional neural network (CNN), learning-based fusion so-lutions have strong generalizability by learning an end-to-end fusion mapping [60, 65, 64]. However, such learning-based methods are criticized as black boxes [6] for lacking interpretability, which inevitably suppresses the fusion per-formance, especially in complex degradation scenarios.
Recently, a novel learning-based technique called deep unfolding network (DUN) has been proposed to unify the merits of model-based and deep learning-based methods.
Specifically, DUNs unfold the iterative optimization steps of a model-based solution into a deep neural network for end-to-end training. DM-Fusion [44] and IVF-Net [17] have introduced DUN to the IVF task and achieved promis-ing performance in most cases. However, existing DUN-based HIF techniques suffer from two issues. (i) They lack a fusion model dedicated to alleviating degradation, making it difficult to fully leverage the complementary information in complex degradation scenarios. (ii) These methods fail to entirely consider the structural characteristics of DUNs, thus neglecting to employ the valuable inter-module infor-mation interaction and lacking an efficient feature extrac-tion module among the cascade structure. Consequently, as shown in Fig. 1, existing DUN-based methods struggle to generate visually appealing results in degradation scenarios.
To address the above problem, in this paper, we propose a novel deep unfolding network, Degradation-Resistant Un-folding Network (DeRUN), for the HIF task to generate high-quality fused images even in degradation scenarios (see Fig. 1). DeRUN is derived from a novel fusion model (HIFM) which improves the existing fusion model (Eq. (1)) in salient information emphasis and noise suppression, thus better resisting degradation conditions. Next, we frame the iterative optimization process of the fusion model into a multi-stage network, where each stage consists of three modules: data fusion module (DFM), visual fidelity mod-ule (VFM), and structure preservation module (SPM), with all the connections following the update procedure, thus ac-commodating interpretability and generalizability.
To ensure the robustness and efficiency of DeRUN, we incorporate a joint constraint strategy (JCS) and a partial weight sharing module (PWSM). Specifically, we propose
JCS by imposing physical and denoising constraints, which are derived from DFM and VFM, on SPM to ensure the ro-bustness of DeRUN in salient texture enhancement even in degradation scenarios. Additionally, to achieve efficient and effective feature extraction, we propose a lightweight net-work named PWSM, which can also mitigate the domain discrepancy problem of JCS. We further propose to train
DeRUN with a novel gradient direction-based entropy loss.
This loss is inspired by the group property of gradient di-rections for weak boundary recognition and noise removal, thus encouraging DeRUN to possess the texture representa-tion capacity, even in degradation scenarios.
Our contributions are summarized as follows:
• We propose DeRUN for HIF, which unifies inter-pretability and generalization, to generate high-quality fused images even in degradation scenarios.
• We propose a series of valid techniques to ensure the robustness and efficiency of our DeRUN model, in-cluding the joint constraint strategy (JCS), the partial weight sharing module (PWSM), and a novel gradient direction-based entropy loss dubbed LGDE loss.
• Extensive experiments on four HIF tasks verify the su-periority of our DeRUN to existing methods in terms of image quality, running efficiency, degradation resis-tance, and favorability in downstream applications. 2.