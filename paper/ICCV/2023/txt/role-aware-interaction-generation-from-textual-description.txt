Abstract
This research tackles the problem of generating interac-tion between two human actors corresponding to textual de-scription. We claim that certain interactions, which we call asymmetric interactions, involve a relationship between an actor and a receiver, whose motions significantly differ de-pending on the assigned role. However, existing studies of interaction generation attempt to learn the correspondence between a single label and the motions of both actors com-bined, overlooking differences in individual roles. We con-sider a novel problem of role-aware interaction generation, where roles can be designated before generation. We trans-late the text of the asymmetric interactions into active and passive voice to ensure the textual context is consistent with each role. We propose a model that learns to generate mo-tions of the designated role, which together form a mutually consistent interaction. As the model treats individual mo-tions separately, it can be pretrained to derive knowledge from single-person motion data for more accurate interac-tions. Moreover, we introduce a method inspired by Per-mutation Invariant Training (PIT) that can automatically learn which of the two actions corresponds to an actor or a receiver without additional annotation. We further present cases where existing evaluation metrics fail to accurately assess the quality of generated interactions, and propose a novel metric, Mutual Consistency, to address such short-comings. Experimental results demonstrate the efficacy of our method, as well as the necessity of the proposed met-ric. Our code is available at https://github.com/ line/Human-Interaction-Generation. 1.

Introduction
Modeling human motion is becoming an important ele-ment for creating high-quality 3D animation, with the rising demand in applications such as animating game characters and 3D online avatars. As human motion is complex and has a wide range of variations, animation of human charac-ters has mostly been either hand-crafted or produced based on human actors through means such as 3D motion capture
Figure 1. Examples of role-aware interactions generated by the proposed method, with language as input and colored roles as-signed to corresponding human actors. Top: Asymmetric interac-tion, in which there is a speaker and a listener. Bottom: Symmetric interaction, in which both share the same action of shaking hands. systems, both of which are generally very expensive.
In recent years, generating single human motion from language has made significant progress [37, 9, 51, 59].
Especially with the recent development of diffusion mod-els [16, 47, 20], it has become possible to generate more faithful and diverse motions of a single character from lan-guage. When we consider multiple characters, more com-plex modeling is required compared to animating a single character, as their relative positions and interactions need to be taken into account, in addition to modeling individual motions. Some attempts have been made to combine two human motions and learn the correspondence between a sin-gle label and the pair of actions [29, 11]. However, viewing from the perspective of each actor, this approach is partic-ularly problematic in interactions where there is an initia-tor and a receiver of an action, as there is a contradiction between the action label, which is generally in the active voice, and the receiver’s motion, which should be passive.
In this paper, we propose a role-aware interaction gen-eration model that can designate individual roles while achieving consistency between two human motions by ex-tending a single motion generation method based on the dif-fusion model [59]. We base our proposal on the insight that among interactions between two humans, there are asym-metric interactions, in which there is an actor and a receiver,
and symmetric interactions, where both human actors per-form a common action, as shown in Fig. 1. In the asymmet-ric example, the sentence describing the interaction between two humans encompasses a relationship where one motion should be described as “whispering” in the active voice, and the other as “being whispered to” in the passive voice.
We assign appropriate textual description for each role in the interaction by providing passive and active voice lan-guages to the corresponding motions when the interaction is asymmetric, and the same language description to both motions when the interaction is symmetric. To capture the characteristics of the different roles, we propose a model based on two Transformer units with shared parameters.
We introduce a cross attention module between the two
Transformers, to take into account the relationship between two human positions and motions, and to capture tempo-ral correspondence between motions of the two roles. As the proposal handles individual motions separately, we fur-ther show that the Transformer units can be pretrained on a single-person motion data, which is a powerful prior knowl-edge for generating individual motions faithful to texts.
To train our model, we need to indicate which of the two humans is an actor or a receiver. To avoid introduc-ing additional annotation costs, we propose a method that can automatically learn to differentiate the roles by adopt-ing Permutation Invariant Training (PIT) [22], which is a commonly used technique in multi-talker speech separa-tion tasks. We train a label-based version of our interac-tion generation model to separate the two roles and assign pseudo-labels for each role. Which of the pseudo-labels corresponds to an active or a passive role can be determined by simple inspection of the generated results. The obtained roles are assigned to the training data to generate interac-tions that conform to the textual description of each role.
We evaluate our method on the interaction subset of the
NTU-RGB+D 120 [43] dataset. As the conventional evalu-ation metrics were insufficient to identify the flaws and in-consistency in the generated interactions, we additionally propose a novel metric, Mutual Consistency, to further as-sess the accuracy of interactions. The experimental results, along with the proposed metric, demonstrate that the pro-posed method is able to effectively generate realistic inter-actions between two human characters.
Overall, our contributions are as follows:
• We propose a novel role-aware interaction generation model that can assign individual roles while maintain-ing consistency between two human motions. For the asymmetric interactions, we translate the text into ac-tive and passive voice descriptions for the model input.
• We propose a method inspired by PIT, which automat-ically learns to separate interactions into motions of an actor and a receiver to reduce annotation costs.
• Experimental results, and further evaluation with our metric Mutual Consistency, show that our method can generate interactions in which two human motions are more mutually consistent and faithful to the input text than existing studies that do not consider roles. 2.