Abstract
Text-driven content creation has evolved to be a trans-formative technique that revolutionizes creativity. Here we study the task of text-driven human video generation, where a video sequence is synthesized from texts describing the ap-pearance and motions of a target performer. Compared to general text-driven video generation, human-centric video generation requires maintaining the appearance of synthe-In this sized human while performing complex motions. work, we present Text2Performer to generate vivid human videos with articulated motions from texts. Text2Performer has two novel designs: 1) decomposed human representa-tion and 2) diffusion-based motion sampler. First, we de-compose the VQVAE latent space into human appearance and pose representation in an unsupervised manner by uti-lizing the nature of human videos.
In this way, the ap-pearance is well maintained along the generated frames.
Then, we propose continuous VQ-diffuser to sample a sequence of pose embeddings. Unlike existing VQ-based methods that operate in the discrete space, continuous VQ-diffuser directly outputs the continuous pose embeddings for better motion modeling. Finally, motion-aware masking strategy is designed to mask the pose embeddings spatial-temporally to enhance the temporal coherence. Moreover, to facilitate the task of text-driven human video genera-tion, we contribute a Fashion-Text2Video dataset with man-ually annotated action labels and text descriptions. Ex-tensive experiments demonstrate that Text2Performer gen-erates high-quality human videos (up to 512 × 256 res-olution) with diverse appearances and flexible motions.
Our project page is https://yumingj.github.io/ projects/Text2Performer.html 1.

Introduction
Since its emergence, text-guided image synthesis (e.g. DALLE [37, 38]) has attracted substantial attention.
Recent works [9, 10, 14, 18, 40, 47] have demonstrated fas-cinating performance for the quality of synthesized images and their consistency with the texts. Beyond image genera-tion, text-driven video generation [22, 24, 44, 53] is an ad-vanced topic to explore. Existing works rely on large-scale datasets to drive large models. Although they have achieved surprising performance on general objects, when applied to the generation of some specific tasks, such as generating videos of garment presents for e-commerce websites, they fail to generate plausible results. Take the CogVideo [24] as an example. As shown in Fig. 2, the generated human ob-jects contain incomplete human structures, and the temporal
Figure 2: Results of General Large Text-to-Video Mod-els. We use the pretrained general large Text-to-Video Mod-els [24] to generate videos using the same texts as the right example of Fig. 1. The result fails to generate complete hu-man structures and maintains temporal consistency. consistency is poorly maintained. On the other hand, these methods need billions of training data, which hampers the application to those specific tasks (e.g. human video gener-ation) without a large amount of paired data.
Therefore, it is worthwhile to explore text-to-video gen-eration in human video generation, which has numerous ap-plications [31, 45]. In this paper, we focus on the task of text-driven human video generation. Compared to general text-to-video generation, text-driven human video genera-tion poses several unique challenges: 1) The human struc-ture is articulated. The joint movements of different body components form many complicated out-of-plane motions, e.g., rotations. 2) When performing complicated motions, the appearance of the synthesized human should remain the same. For example, the appearance of a target human after turning around should be consistent with that at the first be-ginning. In sum, to achieve high-fidelity human video gen-eration, consistent human representation and complicated human motions should be carefully modeled.
We propose a novel text-driven human video genera-tion framework Text2Performer to handle consistent hu-man representations and complex out-of-plane motions.
As shown in Fig. 1, given texts describing appearance and motions, Text2Performer is able to generate tempo-rally consistent human videos with complete human struc-tures and unified human appearances. Text2Performer is built upon VQVAE-based frameworks [4, 11, 18, 52]. In
Text2Performer, thanks to the specific nature of human videos which shares the same objects across the frames within one video, VQVAE latent space can be decomposed into appearance and pose representations. With the decom-posed VQ-space, videos are generated by sampling appear-ance representation and a sequence of pose representations separately. This decomposition contributes to the mainte-nance of human identity. Besides, it makes the motion mod-eling more tractable, as the motion sampler does not need to take the appearance information into consideration.
To model complicated human motions, a novel continu-ous VQ-diffuser is proposed to sample a sequence of mean-ingful pose representations. The architecture of the con-tinuous VQ-diffuser is transformer. The key difference to the previous transformer-based samplers [4, 11, 52] is that the continuous VQ-diffuser directly predicts the continu-ous pose embeddings rather than their indices in the code-book. After predicting continuous pose embeddings, we also make use of the rich embeddings stored in the code-book by retrieving the nearest embeddings of the predicted embeddings from the codebook. Predicting continuous em-beddings alleviates the one-to-many prediction issue in pre-vious discrete methods and the use of codebook constrains the prediction space. With this design, more temporally co-herent human motions and more complete structures of hu-man frames are sampled. In addition, we borrow the idea of diffusion models [4, 18, 23, 39, 26] to progressively pre-dict the long sequence of the pose embeddings. We propose a motion-aware masking strategy to sample the pose em-beddings of the first frame and last frame firstly. Then the pose embeddings of the intermediate frames are gradually diffused. The motion-aware masking strategy enhances the completeness of human structures and temporal coherence.
To facilitate the task of text-guided human video genera-tion, we propose the Fashion-Text2Video Dataset. It is built upon the FashionDataset [56], which consists of 600 human videos performing the fashion show. We manually segment the whole video into clips and label the motion types. Each clip is performing one motion. With the manually labeled motion labels, we then pair them with text descriptions.
Our contributions are summarized as follows:
• We present and study the task of text-guided human video generation. Our proposed Text2Performer can be well trained and have generative abilities with only a small amount of data for training.
• We propose to decompose the VQ-space into appear-ance and pose representations. The decomposition is achieved by making use of the nature of human videos, i.e., the motions are performed under one identity (ap-pearance) across the frames. The decomposition of
VQ-space improves the appearance coherence among frames and eases motion modeling.
• We propose the continuous VQ-diffuser to predict continuous pose embeddings with the pose codebook.
The final continuous pose embeddings are iteratively predicted with the guidance of the motion-aware mask-ing scheme. These designs contribute to generated frames of high quality and temporal coherence.
• We construct the Fashion-Text2Video Dataset with hu-man motion labels and text descriptions to facilitate the research on text-driven human video generation. 2.