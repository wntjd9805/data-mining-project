Abstract
We propose RANA, a relightable and articulated neural avatar for the synthesis of humans under arbitrary view-points, body poses, and lighting. We only require a short video clip of the person to create the avatar and assume no knowledge about the lighting environment. We present a novel framework to model humans while disentangling their geometry, texture, and lighting environment from monocu-lar RGB videos. To simplify this otherwise ill-posed task we first estimate the coarse geometry and texture of the per-son via SMPL+D model fitting and then learn an articu-lated neural representation for higher quality image syn-thesis. RANA first generates the normal and albedo maps of the person in any given target body pose and then uses spherical harmonics lighting to generate the shaded im-age in the target lighting environment. We also propose to pre-train RANA using synthetic images and demonstrate that it leads to better disentanglement between geometry and texture while also improving robustness to novel body poses. Finally, we also present a new photo-realistic syn-thetic dataset, Relighting Human, to quantitatively evaluate the performance of the proposed approach. 1.

Introduction
*equal contribution. The work was partially done during AC’s internship at NVIDIA.
Articulated neural avatars of humans have numerous ap-plications across telepresence, animation, and visual con-tent creation. To enable the widespread adoption of these neural avatars, they should be easy to generate and animate under novel poses and viewpoints, able to render in photo-realistic image quality, and easy to relight in novel environ-ments. Existing methods commonly aim to learn such neu-ral avatars using monocular videos [47, 46, 45, 54, 66, 58, 31]. While this allows photo-realistic image quality and an-imation, the synthesized images are always limited to the lighting environment of the training video. Other works directly tackle relighting of human avatars but do not pro-vide control over the body pose [23, 41]. Moreover, these approaches often require multiview images recorded in a
Light Stage for training, which is limited to controlled set-tings only. Some recent methods aim to relight RGB videos of a dynamic human but do not provide control over body pose [16].
In this work, we present the Relightable Articulated Neu-ral Avatar (RANA) method, which allows animation of peo-ple under any novel body pose, viewpoint, and lighting en-vironment. To create an avatar, we only require a short monocular video clip of the person in unconstrained envi-ronment, clothing, and arbitrary body pose. During infer-ence, we only require the target body pose and target light-ing information.
Learning relightable neural avatars of dynamics humans from monocular RGB videos recorded in unknown environ-ments is a challenging problem. First, it requires model-ing the complex human body articulations and geometry.
Second, in order to allow relighting under novel environ-ments, the texture, geometry, and illumination information have to be disentangled, which is an ill-posed problem to solve from RGB videos [8]. To address these challenges, we first extract canonical coarse geometry and texture informa-tion from the training frames using a statistical human shape model SMPL+D [40, 5, 36]. We then propose a novel con-volutional neural network that is trained on synthetic data to remove the shading information from the coarse texture.
We augment the coarse geometry and texture with learn-able latent features and pass them to our proposed neu-ral avatar framework, which generates refined normal and albedo maps of the person under the target body pose us-ing two separate convolutional networks. Given the normal map, albedo map, and lighting information, we generate the final shaded image using spherical harmonics (SH) light-ing [49]. During training, since the environment lighting is unknown, we jointly optimize it together with the person’s appearance and propose novel regularization terms to pre-vent the leaking of lighting into the albedo texture. We also propose to pre-train the avatar using photo-realistic syn-thetic data with ground-truth albedo and normal maps. Dur-ing pretraining, we train a single model for multiple subjects while having separate neural features for each subject. This not only improves the generalizability of the neural avatar to novel body poses but also learns to decouple the texture and geometry information. For a new subject, we only learn a new set of neural features and fine-tune the avatar model to capture fine-grained person-specific details. In our exper-iments, the avatar for a novel subject can be learned within 15k training iterations.
Since no dataset exists to evaluate the renderings of avatars in terms of both novel light and pose synthesis, we also propose a novel photo-realistic synthetic Relight-ing Human Dataset (RHD) with ground truth albedo, nor-mals, and lighting information. We also perform a qualita-tive evaluation of RANA on the People Snapshot dataset [5] to compare with other baselines.
Our contributions can be summarized as follows:
• We present, RANA, a novel framework for learning relightable articulated neural avatars from short uncon-strained monocular videos. The proposed approach is very easy to train and does not require any knowledge about the environment lighting of the training video.
• Our proposed approach can synthesize images of ar-ticulated humans under any arbitrary body pose, view-point, and lighting. It can also be used for relighting videos of dynamic humans.
• We present a new photo-realistic synthetic dataset for quantitative evaluation and to further the research in this direction. 2.