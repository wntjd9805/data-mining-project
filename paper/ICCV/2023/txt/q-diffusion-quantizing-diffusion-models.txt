Abstract
Diffusion models have achieved great success in image synthesis through iterative noise estimation using deep neu-ral networks. However, the slow inference, high memory consumption, and computation intensity of the noise estima-tion model hinder the efficient adoption of diffusion models.
Although post-training quantization (PTQ) is considered a go-to compression method for other tasks, it does not work out-of-the-box on diffusion models. We propose a novel
PTQ method specifically tailored towards the unique multi-timestep pipeline and model architecture of the diffusion models, which compresses the noise estimation network to accelerate the generation process. We identify the key diffi-culty of diffusion model quantization as the changing output distributions of noise estimation networks over multiple time steps and the bimodal activation distribution of the shortcut layers within the noise estimation network. We tackle these challenges with timestep-aware calibration and split short-cut quantization in this work. Experimental results show that our proposed method is able to quantize full-precision unconditional diffusion models into 4-bit while maintaining comparable performance (small FID change of at most 2.34 compared to >100 for traditional PTQ) in a training-free manner. Our approach can also be applied to text-guided image generation, where we can run stable diffusion in 4-bit weights with high generation quality for the first time. 1.

Introduction
Diffusion models have shown great success in gener-ating images with both high diversity and high fidelity
[43, 13, 44, 42, 6, 33, 36, 34]. Recent work [16, 15] has demonstrated superior performance than state-of-the-art GAN models, which suffer from unstable training. As a class of flexible generative models, diffusion models demon-strate their power in various applications such as image super-resolution [37, 18], inpainting [44], shape generation
[3], graph generation [31], image-to-image translation [40], and molecular conformation generation [47].
However, the generation process for diffusion models can be slow due to the need for an iterative noise estimation of 50 to 1,000 time steps [13, 42] using complex neural networks.
While previous state-of-the-art approaches (e.g., GANs) are able to generate multiple images in under 1 second, it nor-mally takes several seconds for a diffusion model to sample a single image. Consequently, speeding up the image gener-ation process becomes an important step toward broadening the applications of diffusion models. Previous work has been solving this problem by finding shorter, more effective sampling trajectories [42, 30, 39, 22, 1, 24], which reduces the number of steps in the denoising process. However, they have largely ignored another important factor: the noise esti-mation model used in each iteration itself is compute- and memory-intensive. This is an orthogonal factor to the repet-itive sampling, which not only slows down the inference speed of diffusion models, but also poses crucial challenges in terms of high memory footprints.
This work explores the quantization [50, 8, 49, 7, 29] of the noise estimation model used in the diffusion model to accelerate the denoising of all time steps. Specifically, we propose exploring post-training quantization (PTQ) on the diffusion model. PTQ has already been well studied in other learning domains like classification and object detection [4, 2, 20, 11, 23], and has been considered a go-to compression method given its minimal requirement for training data and the straightforward deployment on real hardware devices.
However, the iterative computation process of the diffusion model and the model architecture of the noise estimation network brings unique challenges to the PTQ of diffusion models. PTQ4DM [41] presents an inaugural application of PTQ to compress diffusion models down to 8-bit, but it primarily focuses on smaller datasets and lower resolutions.
Our work, evolving concurrently with [41], offers a com-prehensive analysis of the novel challenges of performing
PTQ on diffusion models. Specifically, as visualized in Fig-ure 1(a), we discover that the output distribution of the noise estimation network at each time step can be largely different, and naively applying previous PTQ calibration methods with an arbitrary time step leads to poor performance. Further-more, as illustrated in Figure 1(b), the iterative inference of the noise estimation network leads to an accumulation of quantization error, which poses higher demands on design-Figure 1: Conventional PTQ scenarios and Q-Diffusion differ in (a) calibration dataset creation and (b) model inference workflow. Traditional PTQ approaches sample data randomly [11], synthesize with statistics in model layers [4], or draw from the training set to create calibration dataset [28, 20], which either contains inconsistency with real inputs during the inference time or are not data-free. In contrast, Q-Diffusion constructed calibration datasets with inputs that are an accurate reflection of data seen during the production in a data-free manner. Traditional PTQ inference only needs to go through the quantized model Î¸q one time, while Q-Diffusion needs to address the accumulated quantization errors in the multi-time step inference. ing novel quantization schemes and calibration objectives for the noise estimation network.
To address these challenges, we propose Q-Diffusion, a
PTQ solution to compress the cumbersome noise estimation network in diffusion models in a data-free manner, while maintaining comparable performance to the full precision counterparts. We propose a time step-aware calibration data sampling mechanism from the pretrained diffusion model, which represents the activation distribution of all time steps.
We further tailor the design of the calibration objective and the weight and activation quantizer to the commonly used noise estimation model architecture to reduce quantization error. We perform thorough ablation studies to verify our design choices, and demonstrate good generation results with diffusion models quantized to only 4 bits.
In summary, our contributions are: 1. We propose Q-Diffusion, a data-free PTQ solution for the noise estimation network in diffusion models. 2. We identify the novel challenge of performing PTQ on diffusion models as the activation distribution diversity and the quantization error accumulation across time steps via a thorough analysis. 3. We propose time step-aware calibration data sampling to improve calibration quality, and propose a special-ized quantizer for the noise estimation network. 4. Extensive results show Q-Diffusion enables W4A8
PTQ for both pixel-space and latent-space uncondi-tional diffusion models with an FID increment of only 0.39-2.34 over full precision models. It can also pro-duce qualitatively comparable images when plugged into Stable Diffusion [34] for text-guided synthesis. 2.