Abstract
Temporal action segmentation is crucial for understand-ing long-form videos. Previous works on this task com-monly adopt an iterative refinement paradigm by using multi-stage models. We propose a novel framework via denoising diffusion models, which nonetheless shares the
In this same inherent spirit of such iterative refinement. framework, action predictions are iteratively generated from random noise with input video features as conditions.
To enhance the modeling of three striking characteristics of human actions, including the position prior, the bound-ary ambiguity, and the relational dependency, we devise a unified masking strategy for the conditioning inputs in our framework. Extensive experiments on three bench-mark datasets, i.e., GTEA, 50Salads, and Breakfast, are performed and the proposed method achieves superior or comparable results to state-of-the-art methods, showing the effectiveness of a generative approach for action segmenta-tion. Code is at tinyurl.com/DiffAct. 1.

Introduction
Temporal action segmentation is a key task for under-standing and analyzing human activities in complex long videos, with a wide range of applications from video surveillance [62], video summarization [3] to skill assess-ment [43]. The goal of temporal action segmentation is to take as input an untrimmed video and output an action se-quence indicating the class label for each frame.
This task has witnessed remarkable progresses in recent years with the development of multi-stage models [20, 41, 68, 72]. The core idea of multi-stage models is to stack sev-eral stages where the first stage produces an initial predic-tion and later stages adjust the prediction from the preceding stage. Researchers have actively explored various architec-tures to implement the multi-stage model, such as the MS-TCN [20, 41] relying on dilated temporal convolutional lay-Figure 1. Multi-stage model vs. diffusion model for action seg-mentation. They both follow an iterative refinement paradigm.
Left: Many previous methods utilize a multi-stage framework to refine the initial prediction. Right: We formulate action segmenta-tion as a frame-wise action sequence generation problem and ob-tain the refined prediction by an iterative denoising process. Col-ors in the barcodes represent different actions. ers and the ASFormer [72] with attention mechanisms. The success of multi-stage models could be largely attributed to the underlying iterative refinement paradigm that properly captures temporal dynamics of actions and significantly re-duces over-segmentation errors [15].
In this paper, we propose an action segmentation method following the same philosophy of iterative refinement but in an essentially new generative approach, which incorpo-rates the denoising diffusion model. Favored for its sim-ple training recipe and high generation quality, the diffusion model [13, 54, 14, 26] has become a rapidly emerging cat-egory of generative models. A forward process in the dif-fusion model corrupts the data by gradually adding noise, while a corresponding reverse process removes the noise step by step so that new samples can be generated from the data distribution starting from fully random noise. Such it-erative denoising in the reverse process coincides with the iterative refinement paradigm for action segmentation. This
motivates us to rethink action segmentation from a genera-tive view employing the diffusion model as in Fig. 1, where we can formulate action segmentation as an action sequence generation problem conditioned on the input video. One distinct advantage of such diffusion-based action segmen-tation is that it not only learns the discriminative mapping from video frames to actions but also implicitly captures the prior distribution of human actions through generative modeling. This prior modeling can be further explicitly en-hanced in line with three prominent characteristics of hu-man actions. The first characteristic is the temporal position prior, which means certain actions are more likely to occur at particular time locations in the video. Taking a video of making salads as an example, actions of cutting vegeta-bles tend to appear in the middle of the video, while serving salads onto the plate is mostly located at the end. The sec-ond characteristic is the boundary prior, which reflects that transitions between actions are visually gradual and thus lead to ambiguous features around action boundaries. The third characteristic is the relation prior that represents hu-man actions usually adhere to some intrinsic temporal or-dering, e.g., cutting a cucumber typically follows behind peeling the cucumber. This relation prior differs from the position prior since it focuses on the arrangements relative to other actions. To jointly exploit these priors of human actions, we devise a condition masking strategy, which nat-urally fits into the newly proposed framework.
In our method, dubbed DiffAct, we formulate action seg-mentation as a conditional generation problem of the frame-wise action label sequence, leveraging the input video as the condition. During training, the model is provided with the input video features as well as a degraded temporal action segmentation sequence obtained from the ground truth with varying levels of injected noise. The model then learns to denoise the sequence to restore the original ground truth.
To achieve this, we impose three loss functions between the denoised sequence and the ground-truth sequence, includ-ing a standard cross-entropy loss, a temporal smoothness loss, and a boundary alignment loss. At inference, follow-ing the reversed diffusion process, the model refines a ran-dom sequence in an iterative manner to generate the action prediction sequence. On the other hand, to boost the mod-eling of the three aforementioned priors of human actions, the conditional information in our framework is controlled through a condition masking strategy during training, which encourages the model to reason over information other than visual features, e.g., time locations, action durations, and the temporal context. The masked conditions convert the generative learning of action sequences from a basic con-ditional one to a combination of fully conditional, partially conditional, and unconditional ones to enhance the three ac-tion priors simultaneously.
The effectiveness of our diffusion-based temporal action segmentation is demonstrated by the experiments on three datasets, GTEA [21], 50Salads [59], and Breakfast [34], on which our model performs better or on par compared to state-of-the-art methods. In summary, our contributions are three-fold: 1) temporal action segmentation is formulated as a conditional generation task; 2) a new iterative refine-ment framework is proposed based on the denoising diffu-sion process; 3) a condition masking strategy is designed to further exploit the priors of human actions. 2.