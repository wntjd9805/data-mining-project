Abstract
Recent years have seen a growing interest in Scene
Graph Generation (SGG), a comprehensive visual scene understanding task that aims to predict entity relationships using a relation encoder-decoder pipeline stacked on top of an object encoder-decoder backbone. Unfortunately, cur-rent SGG methods suffer from an information loss regarding the entities’ local-level cues during the relation encoding process. To mitigate this, we introduce the Vision rElation
TransfOrmer (VETO), consisting of a novel local-level en-tity relation encoder. We further observe that many existing
SGG methods claim to be unbiased, but are still biased to-wards either head or tail classes. To overcome this bias, we introduce a Mutually Exclusive ExperT (MEET) learning strategy that captures important relation features without bias towards head or tail classes. Experimental results on the VG and GQA datasets demonstrate that VETO + MEET boosts the predictive performance by up to 47% over the state of the art while being ∼ 10× smaller.1 1.

Introduction
Visual scene understanding has made great strides in re-cent years, extending beyond standard object detection and recognition tasks to tackle more complex problems such as visual question answering [1] and image captioning [11].
One powerful tool for scene understanding is Scene Graph
Generation (SGG), which aims to identify the relationships between entities in a scene [23]. However, despite recent advancements, SGG models still have significant limita-tions when it comes to real-world applications.
Conventional SGG approaches, as shown in Fig. 1 (panel 3), generate global-level entity patches for relation encod-ing. Yet, during the relation encoding process, they lose local-level entity information. As illustrated in Fig. 2a, we humans have a tendency to focus on the critical local-level information necessary to construct relations between 1Code is available at https://github.com/visinf/veto
Figure 1. VETO-MEET vs. Conventional SGG. (1) VETO: En-hancing the information flow from entity features to relationship prediction by using a local-level entity relation encoder that con-ducts relation and modality fusion of local-level entity patches.
The local-level components (green ticks) keep the model light-weight while reducing information loss. A (blue) and B (green) denote example relation classes taken from the corresponding col-ored region in the predicate frequency histogram of the VG dataset
[17]. (2) MEET: Debiased relation decoder that employs out-of-distribution aware mutually exclusive experts (E1–E3). Grey A and B denote an out-of-distribution prediction discarded by the model. (3) Conventional SGG: The projection components (red crosses) yield a computationally expensive model and the global-level entity patches result in a local-level information loss. things in a scene, which is overlooked by current SGG ap-proaches. Moreover, the major parameter count of current
SGG models stems from projections (red-crossed compo-nents in Fig. 1) involved in global-level entity patch genera-tion. Another challenge with existing SGG approaches, de-spite efforts to enhance scene graphs using additional cues like depth maps and knowledge graphs [27, 48], is that they are either resource intensive or limited in exploiting cross-modal information.
Finally but crucially, SGG training setups are challenged by the strong bias of the visual world around us towards a few frequently occurring relationships, leaving a long tail of under-represented relations. This is also the case with
(a) Significance of local-level cues (b) Recall drop of debiased Motifs [49], VCTree [31]
Figure 2. Challenges in SGG. (a) For establishing the attached to relation between Handle and Basket, the attention should be on the corner regions of the object. (b) R@100 drop (%) and mR@100 increase (%) of unbiased SGG methods Motifs and VCTree rela-tive to their vanilla versions. The R@100 metric measures the av-erage recall of all predictions, which is higher for models that over-fit to the head classes, while mR@100 denotes the per-predicate class mean and is higher for models that overfit to the tail classes. benchmark SGG datasets, e.g., Visual Genome (VG) [17], as depicted by the predicate2 class frequency distribution in
Fig. 1. Due to the dominance of few head predicates, con-ventional SGG models [31, 49] are biased towards the head classes. Though several unbiased SGG methods have been proposed [7,30,46] to overcome this issue, they are prone to over-fitting to the tail classes at the expense of head classes (cf . Fig. 2b). Despite recent efforts [7] to fix this bias issue using multi-expert learning strategies, we find that they still over-fit to the tail classes (“GCL” in Fig. 2b). Overall, there are two main problems with present unbiased SGG meth-ods: (1) Conventional methods, including debiased models, can only learn a limited range of predicates. (2) Existing multi-expert SGG models lack adequate exclusivity to en-hance both head and tail classes at the same time.
Consequently, we propose the Vision rElation Trans-fOrmer (VETO). Inspired by Vision Transformers [8] that use image-level patches for classification, VETO generates local-level entity patches for the relation prediction task.
This improves the information flow from entity features to relationship prediction by channeling the attention towards fused local feature cues of subject and object entities (Re-lation Fusion in Fig. 1) and using a local-level entity re-lation encoder, which processes entity features at the sub-region level. To strengthen the encoder further, we infuse geometric cues into VETO using a Modality Fusion com-2We use the terms predicate/relation interchangeably in this paper. ponent (cf . Fig. 1), which unites visual and geometric fea-tures to yield local-level entity patches. Finally, to suc-cessfully debias VETO, we propose a multi-expert learning strategy termed Mutually Exclusive ExperTs (MEET). After splitting the predicate classes into subgroups, we perform in-distribution and out-of-distribution (OOD) sampling for each subgroup. Then we train each expert on every pred-icate class but each expert will be responsible for only a subset of predicates with out-of-distribution prediction han-dling predicates outside its subgroup. In contrast to exist-ing multi-expert methods [7], where expert classifiers are co-dependent to distill knowledge, OOD sampling enables experts to independently interpret every inference sample.
Contributions. Let us summarize: (1) We propose a novel SGG method with a local-level entity relation encod-ing, which is light-weight and reduces the local-level in-(2) To strengthen the encoder formation loss of entities. further, we propose an effective strategy to infuse addi-tional geometric cues. (3) We devise a mutually exclusive multi-expert learning strategy that effectively exploits our relation network design by learning subgroup-specific di-verse feature representations and discriminating from sam-ples outside its subgroups. (4) Our extensive experimenta-tion shows the significance of both VETO and MEET. 2.