Abstract
Domain adaptation of GANs is a problem of ﬁne-tuning
GAN models pretrained on a large dataset (e.g. StyleGAN) to a speciﬁc domain with few samples (e.g. painting faces, sketches, etc.). While there are many methods that tackle this problem in different ways, there are still many impor-tant questions that remain unanswered. In this paper, we provide a systematic and in-depth analysis of the domain adaptation problem of GANs, focusing on the StyleGAN model. We perform a detailed exploration of the most im-portant parts of StyleGAN that are responsible for adapting the generator to a new domain depending on the similarity between the source and target domains. As a result of this study, we propose new efﬁcient and lightweight parameter-izations of StyleGAN for domain adaptation. Particularly, we show that there exist directions in StyleSpace (StyleDo-main directions) that are sufﬁcient for adapting to similar domains. For dissimilar domains, we propose Afﬁne+ and
AfﬁneLight+ parameterizations that allows us to outper-form existing baselines in few-shot adaptation while having signiﬁcantly less training parameters. Finally, we exam-ine StyleDomain directions and discover their many sur-*Equal contribution prising properties that we apply for domain mixing and cross-domain image morphing. Source code can be found at https://github.com/AIRI-Institute/StyleDomain. 1.

Introduction
Recent years GANs [12, 20, 21, 19, 5] have shown im-pressive results in image synthesis and offered many ways to control the generated data. In particular, the state-of-the-art StyleGAN models [20, 21, 19] have many practical ap-plications such as image enhancement [46, 24, 6, 41], image editing [17, 35, 14, 37, 1, 43, 29], image-to-image trans-lation [31, 16, 36, 11] thanks to their high-quality image generation and their latent representation that has rich se-mantics and disentangled controls for localized meaningful image manipulations. However, it comes at a price, as the training of StyleGAN requires a large, high-quality dataset that signiﬁcantly limits its applicability because many real-world domains are represented by few images. The standard approach to deal with this problem is transfer learning, i.e.
ﬁne-tuning the model pretrained on the source domain A to the target domain B.
There are many domain adaptation methods for Style-GAN [18, 39, 50, 52, 23, 45, 28, 31, 4, 11, 55, 44] that
tackle this problem in different ways depending on the num-ber of available images (e.g., one-shot/few-shot) from the target domain B and the similarity between the source A and target B domains (e.g., faces → sketches, artistic por-traits, or faces → dogs, cats). Most of these works implic-itly assume that StyleGAN can be adapted to a new do-main only if we ﬁne-tune almost all its weights, even for similar domains. However, this common wisdom is poorly investigated and veriﬁed and there is a lack of analysis of which parts of StyleGAN are important depending on dif-ferent data regimes and the similarity between domains.
In this work, we aim to provide a systematic and com-prehensive analysis of this question. Our investigation of the properties of the aligned StyleGAN models consists of several parts. First, in Section 3, we identify what parts of the StyleGAN are sufﬁcient for its adaptation depending on the similarity between the source A and target B domains.
We discover that ﬁne-tuning the whole synthesis convolu-tional network is not always necessary. In the case of sim-ilar A and B domains, the afﬁne layers are sufﬁcient for the adaptation. For more distant domains, we should opti-mize more parameters, however not the whole network. It suggests investigating new more efﬁcient and lightweight parameterizations of StyleGAN to utilize them for domain adaptation.
In the second part of our analysis, we propose two new parameterizations of StyleGAN. For similar domains, we consider the latent space that is formed by the output of afﬁne layers, i.e. StyleSpace [43]. We show that we can directly optimize directions in this space that can adapt to similar target domains with the same quality as ﬁne-tuning all weights of StyleGAN (we call such directions as Style-Domain directions). Further, we explore that we can zero out 80% of StyleDomain direction coordinates without a quality degradation that gives even more lightweight param-eterization (StyleSpaceSparse). For more distant domains, we propose a new parameterization Afﬁne+ that consists of afﬁne layers and only one convolutional block from the synthesis network. It reduces the number of trainable pa-rameters by 6 times and achieves the same quality. Then, we further improve Afﬁne+ parameterization by utilizing low-rank decomposition for weights of afﬁne layers and obtain AfﬁneLight+ parameterization. It allows us to opti-mize by two orders less parameters compared to training the whole StyleGAN. These parameterizations show the state-of-the-art performance for few-shot adaptation for dissimi-lar domains outperforming more complicated and expensive baselines.
Additionally, in Section 4, we inspect StyleDomain di-rections and discover their surprising properties. The ﬁrst one is mixability, i.e. we can sum up these directions to ob-tain a new mixed domain (e.g., see Figure 6 as a mix of the
Joker style, Pixar style and the style from the image). The second impressive property is transferability, i.e. the same
StyleDomain directions can be applied to StyleGAN mod-els that were ﬁne-tuned to other domains (e.g., see Figure 5 where we apply directions found for faces to dogs, cats and churches). We apply these ﬁndings to standard computer vision tasks such as image-to-image translation and cross-domain morphing. 2.