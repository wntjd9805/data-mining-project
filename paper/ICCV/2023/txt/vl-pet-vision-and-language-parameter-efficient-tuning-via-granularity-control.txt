Abstract
As the model size of pre-trained language models full fine-tuning becomes pro-(PLMs) grows rapidly, hibitively expensive for model training and storage.
In vision-and-language (VL), parameter-efficient tuning (PET) techniques are proposed to integrate modular modifica-tions (e.g., Adapter and LoRA) into encoder-decoder PLMs.
By tuning a small set of trainable parameters, these tech-niques perform on par with full fine-tuning. However, ex-cessive modular modifications and neglecting the function-ality gap between the encoders and decoders can lead to performance degradation, while existing PET techniques (e.g., VL-Adapter) overlook these critical issues.
In this paper, we propose a Vision-and-Language Parameter-Efficient Tuning (VL-PET) framework to impose effective control over modular modifications via a novel granularity-controlled mechanism. Considering different granularity-controlled matrices generated by this mechanism, a va-riety of model-agnostic VL-PET modules can be instanti-ated from our framework for better efficiency and effective-ness trade-offs. We further propose lightweight PET mod-ule designs to enhance VL alignment and modeling for the encoders and maintain text generation for the decoders.
Extensive experiments conducted on four image-text tasks and four video-text tasks demonstrate the efficiency, ef-fectiveness and transferability of our VL-PET framework.
In particular, our VL-PETlarge with lightweight PET mod-ule designs significantly outperforms VL-Adapter by 2.92% (3.41%) and LoRA by 3.37% (7.03%) with BART-base (T5-base) on image-text tasks. Furthermore, we validate the enhanced effect of employing our VL-PET designs on ex-isting PET techniques, enabling them to achieve signifi-cant performance improvements. Our code is available at https://github.com/HenryHZY/VL-PET.
*Corresponding author.
Figure 1. Relative average performance gain of difference PET techniques w.r.t to full fine-tuning. Experiments are conducted with three seeds on four image-text tasks based on BART-base. 1.

Introduction
Recently, the paradigm of pre-training transformer-based models on large-scale corpus and then fine-tuning them for downstream tasks has achieved great success in various do-mains, such as natural language processing (NLP) [50, 9, 33, 27, 45, 2], computer vision (CV) [10, 35, 36, 34, 13, 1], and vision-and-language (VL) [6, 28, 23, 29, 8, 42, 51].
However, as the model size of pre-trained language models (PLMs) and the number of tasks grow rapidly, fine-tuning the entire parameter set of PLMs (i.e., full fine-tuning) and preserving a task-specific copy of PLMs becomes pro-hibitively expensive for model training and storage.
To mitigate these problems, parameter-efficient tuning (PET) techniques are proposed to save model storage space.
As stated in [12], most PET techniques freeze the whole
PLM backbone and integrate trainable modular modifica-tions (i.e., additional small trainable PET modules, such as
Adapter [15] and LoRA [16]) into PLM. By only tuning a small set of trainable parameters, these techniques achieve performance comparable to full fine-tuning. Despite the significant achievements of PET in NLP [40, 12, 32, 54, 21, 38, 16, 31] and CV [18, 41, 4, 19, 3, 7, 53], the po-tential of PET in VL has not been fully explored and re-quires further VL-specific investigation to bridge the nat-ural modality gap between vision and language.
In VL, most PET techniques follow NLP-specific modular mod-ifications [39, 20, 55, 56, 22, 52, 37], while lacking VL-specific designs. Moreover, these techniques mainly focus on discriminative tasks (e.g., image-text retrieval), limiting the generalization ability of PLMs. Although the state-of-the-art PET approach VL-Adapter [49] has studies chal-lenging VL tasks, including discriminative and generative tasks (e.g., image captioning), it directly migrates those
NLP-specific modular modifications without deep explo-ration about the most appropriate design for VL domains.
To conduct a more thorough investigation into VL-specific PET techniques, we raise and analyze two criti-cal issues neglected by existing PET techniques in VL: (1) Integrating heavy and excessive modular modifica-tions [12, 46] into PLMs can greatly affect the intermediate output of the PLMs, leading to instability and performance degradation. Therefore, it is crucial to take measures to im-pose effective control over these modular modifications to achieve better performance on VL tasks. However, state-of-the-art PET techniques (e.g., VL-Adapter) directly integrate modular modifications into PLMs without effective control. (2) For PLMs used in VL tasks, there exists functionality gap between the encoders and decoders [8]. Specifically, the encoders focus on VL alignment and modeling, while the decoders focus on auto-regressive text generation con-ditioned on the visual-language representations. PLMs rely on the cross-attention modules inside the decoders to bridge the gap between the encoders and decoders. Therefore, it is essential to introduce tailored modular modification designs for each module, thereby enhancing their unique abilities and achieving better performance. However, state-of-the-art PET techniques directly assign identical modular mod-ifications to PLMs without exploring the unique ability of each PLM module, leading to suboptimal performance.
In this paper, we propose a novel Vision-and-Language
Parameter-Efficient Tuning (VL-PET) framework to ad-dress the above issues. We introduce a novel granularity-controlled mechanism to generate a granularity-controlled matrix as effective control over the modular modifica-tions introduced by PET techniques. Considering dif-ferent granularity control levels, a variety of granularity-controlled matrices are generated by the proposed mecha-nism with different trainable parameter complexities. With these granularity-controlled matrices and a novel multi-head modular modification, a variety of model-agnostic
VL-PET modules can be instantiated from our VL-PET framework for better efficiency and effectiveness trade-offs. Furthermore, conventional PET module designs typ-ically integrate modular modifications (i.e., PET modules such as Adapter) into all self-attention, cross-attention, and feed-forward modules of the PLM backbones. Due to the unique abilities of the encoders and decoders, we pro-pose lightweight PET module designs that facilitate suitable modular modifications integration into the encoders and de-coders. For encoders, we integrate our instantiated VL-PET modules into self-attention and feed-forward for better
VL alignment and modeling. For decoders, we only inte-grate our instantiated VL-PET modules into cross-attention to maintain decoder knowledge and enhance text genera-tion. We further assign our instantiated VL-PET module to the value matrix inside the cross-attention, enabling refined and enhanced control over the decoders. Subsequent exper-iments demonstrate that lightweight designs significantly outperform conventional designs with fewer parameters.
Extensive experiments are conducted on four image-text tasks with BART-base [27], including visual question an-swering (VQAv2 [11] and GQA [17]), visual reasoning (NLVR2 [47]) and image captioning (MSCOCO [5]). As shown in Figure 1, all of the proposed VL-PET modules with lightweight PET module designs outperform the state-of-the-art PET techniques. In particular, VL-PETlarge (i.e., one of our instantiated VL-PET modules) significantly out-performs VL-Adapter by 2.92% and LoRA by 3.37%. Fur-thermore, we transfer our model-agnostic VL-PET mod-ules to another larger backbone (i.e., T5-base [45]), where the observed trends of performance improvement remain consistent with those observed in BART-base. Our VL-PETlarge still significantly surpasses VL-Adapter by 3.41% and LoRA by 7.03% in the same image-text tasks. How-ever, state-of-the-art PET techniques do not show similar improvements with this larger PLM, and some techniques even exhibit performance degradation due to their heavy and excessive modular modifications integration. For com-pleteness, we also transfer VL-PET modules to four video-text tasks, including video question answering (TVQA [24] and How2QA [29]) and video captioning (TVC [25] and
YC2C [57]). Comprehensive experiments and thorough ablation studies demonstrate the efficiency, effectiveness and transferability of our VL-PET framework. More-over, we validate the enhanced effect of employing VL-PET designs (e.g., granularity-controlled mechanism and lightweight PET module designs) on existing PET tech-niques (e.g., Compacter [21] and VL-Adapter), enabling them to achieve significant performance improvements. 2.