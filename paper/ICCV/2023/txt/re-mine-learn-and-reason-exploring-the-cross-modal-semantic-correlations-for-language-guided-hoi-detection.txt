Abstract 1.

Introduction
Human-Object Interaction (HOI) detection is a challeng-ing computer vision task that requires visual models to ad-dress the complex interactive relationship between humans and objects and predict <human, action, object> triplets.
Despite the challenges posed by the numerous interaction combinations, they also offer opportunities for multi-modal learning of visual texts. In this paper, we present a systematic and uniﬁed framework (RmLR) that enhances HOI detec-tion by incorporating structured text knowledge. Firstly, we qualitatively and quantitatively analyze the loss of interac-tion information in the two-stage HOI detector and propose a re-mining strategy to generate more comprehensive vi-sual representation. Secondly, we design more ﬁne-grained sentence- and word-level alignment and knowledge transfer strategies to effectively address the many-to-many matching problem between multiple interactions and multiple texts.
These strategies alleviate the matching confusion problem that arises when multiple interactions occur simultaneously, thereby improving the effectiveness of the alignment process.
Finally, HOI reasoning by visual features augmented with textual knowledge substantially improves the understanding of interactions. Experimental results illustrate the effective-ness of our approach, where state-of-the-art performance is achieved on public benchmarks.
*Corresponding author.
Human-object interaction (HOI) detection [16, 6] is an emerging ﬁeld of research that builds upon object detection and requires more advanced high-level visual understanding.
A high-performing HOI detector should not only accurately localize all interacting Human-Object pairs but also recog-nize their speciﬁc interactions, typically represented as an
HOI triplet in the format of <human, action, object> [67].
Previous approaches for achieving HOI detection can be divided into two pipelines: those that treat object detection and interaction recognition as separate stages [62, 6, 13, 14, 33, 20], and those that aim to handle both simultaneously
[15, 26, 66, 35, 7]. Although both paradigms have made signiﬁcant progress, the task remains challenging due to the vast variety of human-object interaction combinations in the real world [58, 59]. For example, the HICO-DET dataset
[6] contains 600 human-object interaction combinations. A common approach is to optimize the model by mapping these various triplet labels into a discrete one-hot labels. However, this method oversimpliﬁes the intricacy of the HOI task and can be cumbersome for model optimization.
In recent years, multi-modal learning has gained signiﬁ-cant attention in the vision-and-language learning domain, where it has achieved state-of-the-art performance on vari-ous tasks [25, 3, 4, 30, 1, 23]. By integrating information from multiple modalities, such as images [49, 47, 50, 48] and text [64], multi-modal learning can provide a more com-prehensive understanding of entities or events. In the ﬁeld of HOI, several recent studies [65, 21, 34, 56, 58, 59] have applied image-and-text models to improve interaction detec-tion performance. For example, HOI-VP [65] used a set of
binary classiﬁers to verify each category and proposed Lan-guage Prior-guided Channel Attention (LPCA) to enhance
HOI recognition. SSRT [21] pre-selected object-action (OA) prediction candidates and encoded them as text features to re-ﬁne the queries’ representation. PhraseHOI [34] employed a pre-trained word embedding model to generate a phrase em-bedding that enhances the discriminative ability and capacity of the common knowledge space.
Although the use of vision-and-language pre-training (VLP) or language knowledge injection has motivated the ex-ploration of HOI image-text correspondences through multi-modal learning, their effectiveness in knowledge transfer remains limited. This is due to the heterogeneity gap [3] that exists between different modalities, which requires cross-modal modeling to reduce the inter-modality gap and explore semantic correlations. Additionally, the problem of multi-interaction to multi-text matching in HOI tasks remains un-solved, which may limit the reliability of cross-modal cor-respondences. Therefore, a systematic and uniﬁed solution is needed to better exploit cross-modal HOI detection and improve the generalization ability of HOI detectors.
In this paper, we propose a systematic approach (RmLR) to improve HOI detection in light of the structured text knowledge in cross-modal learning. Concretely, our HOI framework proceeds from three perspectives: i) we reveal the problem of interaction information loss in the two-stage
HOI detector, and propose the Re-mine strategy to obtain this crucial visual information; ii) more sophisticated cross-modal Learning method to achieve semantic association from sentence- and word-level; iii) Reasoning using textual knowledge-enhanced representations substantially improves the visual model’s understanding of interactions. The main contribution of this paper is summarized as follows:
• We propose a systematic and uniﬁed framework so that the inherent challenges of HOI can be elaborated in both visual and cross-modal settings.
• We qualitatively and quantitatively analyze the problem of interaction information loss in two-stage visual HOI detector, and propose a re-mining strategy to capture these crucial interaction-aware representations.
• We formulate the cross-modal learning in HOI domain as a many-to-many matching problem, where multiple interactions need to be matched with their correspond-ing textual descriptions, and propose appropriate sen-tence and text alignment strategies to promote learning semantically aligned.
• Extensive experiments show that our RmLR equipped with ResNet-50 outperforms previous SOTA by a large margin and achieves an average mAP increase of about
+3.88p and +5.05p on HICO-DET [6] and V-COCO
[16], respectively.
Figure 1. Which pair of human instances is more similar in the HOI detector? According to our analysis using cosine similarity mea-surement for the human tokens of the DETR-based HOI detector
[62], Pair 1 has a similarity score of 0.99, while Pair 2 has a score of only 0.58. These ﬁndings are consistent with numerous similar cases observed in our experiments, highlighting the phenomenon of interaction-related information loss in which the output tokens of the object detector primarily emphasize spatial position, potentially leading to the loss of crucial information related to the interactions. 2.