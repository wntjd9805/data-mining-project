Abstract (a) Confidence Thresholding
Recent advances in semi-supervised semantic segmenta-tion have been heavily reliant on pseudo labeling to com-pensate for limited labeled data, disregarding the valuable relational knowledge among semantic concepts. To bridge this gap, we devise LOGICDIAG, a brand new neural-logic semi-supervised learning framework. Our key insight is that conﬂicts within pseudo labels, identiﬁed through sym-bolic knowledge, can serve as strong yet commonly ignored learning signals. LOGICDIAG resolves such conﬂicts via reasoning with logic-induced diagnoses, enabling the re-covery of (potentially) erroneous pseudo labels, ultimately alleviating the notorious error accumulation problem. We showcase the practical application of LOGICDIAG in the data-hungry segmentation scenario, where we formalize the structured abstraction of semantic concepts as a set of logic rules. Extensive experiments on three standard semi-supervised semantic segmentation benchmarks demonstrate the effectiveness and generality of LOGICDIAG. Moreover,
LOGICDIAG highlights the promising opportunities arising from the systematic integration of symbolic reasoning into the prevalent statistical, neural learning approaches. 1.

Introduction
Deep learning has revolutionized computer vision tasks, yielding remarkable breakthroughs [1–5]. However, such advances are often only possible in the presence of large labeled training datasets, which are challenging to acquire.
Semantic segmentation, in particular, poses difﬁculties due to the need for pixel-level manual labeling [6–13], which is time-consuming and labor-intensive, precluding the ap-plication of such methods, especially in domains like med-ical image analysis. To remedy this issue, there has been a growing interest in semi-supervised semantic segmentation, which aims to train segmentation models using a combina-tion of limited labeled data and a large amount of unlabeled
*Corresponding author: Yi Yang.
Teacher 
Model (b) Logic-induced Diagnostic Reasoning
Teacher 
Model
Student 
Model
Student 
Model
Unlabeled Data (c) Symbolic Knowledge Base
Prediction
Animal
Pseudo Label
Dog
Cat
Vehicle
Optimization
Animal cannot be Vehicle
∀x Animal(x) → ¬Vehicle(x)
Cat is Animal
∀x Cat(x) → Animal(x) (cid:856)(cid:3)(cid:856)(cid:3)(cid:856)
Figure 1: Prevalent data-driven pseudo-labeling methods typically rely on heuristic conﬁdence thresholding (i.e., (a)). We opt to le-verage symbolic knowledge in the form of logic rules (i.e., (c)), to diagnose and resolve potential errors within predictions (i.e., (b)). data [14–20]. Pseudo labeling [21, 22] constitutes one such technique, where the unlabeled data is assigned pseudo la-bels based on model predictions. The model is then itera-tively trained using these pseudo labeled data as if they were labeled examples. Typically, this method is implemented within the teacher-student framework [17] (Fig. 1 (a)).
Despite its prevalence, the pseudo labeling paradigm, a statistical learning approach, is commonly perceived as unreliable, due to the accumulation of erroneous predic-tions [23]. Previous works [22, 24, 25] have attempted to mitigate this issue by rejecting pseudo labels with classi-ﬁcation scores below a heuristic threshold, known as con-ﬁdence thresholding [22]. However, relying solely on this strategy often proves unsatisfactory. First, the conﬁrmation bias that may occur during the early stages of training can be difﬁcult, if not impossible, to be rectiﬁed in subsequent learning [26]. Second, the purely data-driven nature of such threshold-based methods makes them challenging to inter-pret. Third, to achieve optimal performance, the threshold must be manually adjusted for each model and dataset, lim-iting its practical application. Beyond the conﬁnes, sym-bolic reasoning, as a compelling alternative, offers appeal-ing characteristics: it requires little or no data to general-ize systematically. In light of this background, we suggest a promising direction towards an integration of statistical learning and symbolic reasoning to harness the strengths of both paradigms to achieve improved performance.
In this paper, we propose to explicitly compile the rich symbolic knowledge into the prevalent pseudo label based neural training regime (cf.Fig. 1 (b)). Our key insight is that symbolic knowledge can effectively resolve conﬂicts within the pseudo labels, which reveal potential model errors. For instance, leveraging the prior knowledge on compositional-ity, we can naturally identify the inconsistencies like classi-fying a pixel as both a cat and a vehicle (cf. Fig. 1 (c)).
Correcting these errors progressively enhances the accu-racy of pseudo labels, and thus mitigating the conﬁrmation bias in iterative self-training. Building upon this insight, we introduce LOGICDIAG, a new SSL framework that em-ploys logical reasoning to identify such conﬂicts based on the symbolic knowledge expressed in the form of ﬁrst-order logic. The framework then suggests possible diagnoses to rectify the prediction. To further address the challenge of multiple diagnoses, we model the likelihood of each diag-nosis being the actual fault based on a comprehensive mea-sure of both predictive conﬁdence and degree of conﬂicts according to the fuzzy logic. By doing so, our approach in-troduces a holistic neural-logic machine, that consolidates the beneﬁts of powerful declarative languages, transparent internal functionality, and enhanced model performance.
LOGICDIAG is a principled framework that seamlessly integrates with mainstream semi-supervised learning meth-ods. It requires only minor adjustments to the dense classi-ﬁcation head. With LOGICDIAG, we can easily describe di-verse symbolic knowledge using ﬁrst-order logic and inject it into sub-symbolic pipelines. Taking dense segmentation as main battleﬁeld, we capture and evaluate a central cogni-tive ability of human, i.e., structured abstractions of visual concepts [27], by grounding three logic rules onto LOGIC-DIAG: Composition, Decomposition, and Exclusion.
Extensive experiments have validated the effectiveness of our LOGICDIAG, which exhibit solid performance gains (i.e., 1.21%-4.15% mIoU) on three well-established bench-marks (i.e., PASCAL VOC 2012 [28], Cityscapes [29], and
COCO [30]). We particularly observe signiﬁcant improve-ments in the label scarce settings, indicating LOGICDIAG’s superior utilization of unlabeled data. Besides, when em-ployed onto the existing SSL frameworks, e.g., AEL [15],
MKD [14], the performance is consistently advanced. The results demonstrate the strong generality and promising per-formance of LOGICDIAG, that also evidence the great po-tential of the integrated neural-symbolic computing in the fundamental large-scale semi-supervised segmentation. 2.