Abstract high degree of realism.
We present Neural Fields for LiDAR (NFL), a method to optimise a neural field scene representation from LiDAR measurements, with the goal of synthesizing realistic LiDAR scans from novel viewpoints. NFL combines the rendering power of neural fields with a detailed, physically motivated model of the LiDAR sensing process, thus enabling it to ac-curately reproduce key sensor behaviors like beam diver-gence, secondary returns, and ray dropping. We evaluate
NFL on synthetic and real LiDAR scans and show that it outperforms explicit reconstruct-then-simulate methods as well as other NeRF-style methods on LiDAR novel view syn-thesis task. Moreover, we show that the improved realism of the synthesized views narrows the domain gap to real scans and translates to better registration and semantic segmen-tation performance. 1.

Introduction
The goal of novel view synthesis is to generate a view of a 3D scene, from a viewpoint at which no real sensor image has been captured. This offers the possibility to ob-serve real scenes from a virtual, unobserved perspective.
Among other applications, it has tremendous potential for autonomous driving: synthetic novel views may be used to train and test perception algorithms across a wider range of viewing conditions, thus enhancing robustness and gener-alization. Moreover, novel view synthesis becomes critical when the desired viewpoints are not known in advance, e.g., during training of a planning module whose decisions deter-mine future vehicle locations.
Neural radiance fields (NeRFs) have led to unprece-dented visual quality when synthesizing novel camera views [2, 29, 30, 52]. These methods represent the 3D scene in form of continuous density and radiance fields, from which images can be generated through volume rendering, mimicking the image acquisition process. The inductive bias of neural networks imparts NeRFs the ability to inter-polate complex lighting and reflectance behaviours with a
While most prior works focused on synthesizing cam-era views, 3D perception in the autonomous driving context typically relies partly (or even exclusively) on LiDAR mea-surements. Synthesizing realistic LiDAR scans from novel viewpoints thus has a lot of potential for data augmentation and closed-loop testing of autonomous navigation systems.
The problem of synthesizing novel LiDAR views has previously been addressed in two stages [24]. First, ex-tract an explicit surface representation such as surfels or a triangular mesh from the scanned point clouds. Then, simulate LiDAR measurements from a novel viewpoint by casting rays and intersecting them with the surface model.
Like for images, explicit reconstruction (which is not op-timised towards the subsequent synthesis step) suffers from discretization artifacts and introduces noticeable errors [46].
Moreover, the rendering assumes an idealised ray model and neglects the divergence of the LiDAR beams, which causes frequent second returns from distant surfaces.
Here, we instead build on a main insight of NeRF [29]: directly optimizing an implicit scene representation for novel view synthesis can produce more realistic outputs than the reconstruct-then-simulate approach. Specifically, we propose Neural Fields for LiDAR (NFL), a NeRF-style representation for synthesizing novel LiDAR viewpoints.
Several NeRF extensions have utilized range measure-ments as additional supervision, and have shown that con-straining the scene geometry more tightly can yield better (camera) view synthesis [8, 38]. Yet, the output of those methods are synthetic images, not LiDAR scans, conse-quently they have not paid attention to effects specific to
LiDAR sensing: a laser scanner does not directly sense range, rather it measures the returned light energy per ray and determines the range based on the waveform. This in-cludes the possibilities that there are multiple returns1 from the emitted ray, or no return at all.
Our formulation closely adheres to the principles of the
LiDAR measurement process and incorporates them into 1In principle there can be >2 returns, but automotive LiDAR sensors typically record the first two echos.
the neural field framework. Specifically, we (i) devise vol-ume rendering for LiDAR sensors; (ii) incorporate beam divergence and (iii) propose truncated volume rendering to account for secondary returns and improve range predic-tion.
We evaluate our method on both synthetic and real Li-DAR data. To this end, we (iv) develop a LiDAR simulator for synthesizing scenes from 3D assets that serve as a test bed for viewpoints far from the original scan locations, and to study the effect of different scan patterns. Real data from the Waymo [43] dataset is used to evaluate NFL against real scans at held-out viewpoints, including real-world in-tensities, ray drops and secondary returns. Additionally, we (v) propose a novel closed-loop evaluation protocol that leverages real data to evaluate view synthesis in challeng-ing views. As an end-to-end test for downstream tasks, we further evaluate the performance of state-of-the-art segmen-tation and registration networks when trained on real scans and tested on novel views generated by NFL. 2.