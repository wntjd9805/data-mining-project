Abstract
Source-free domain adaptation (SFDA) aims to adapt a source model trained on a fully-labeled source domain to an unlabeled target domain. Large-data pre-trained networks are used to initialize source models during source training, and subsequently discarded. However, source training can cause the model to overfit to source data distribution and lose applicable target domain knowledge. We propose to integrate the pre-trained network into the target adaptation process as it has diversified features important for general-ization and provides an alternate view of features and clas-sification decisions different from the source model. We pro-pose to distil useful target domain information through a co-learning strategy to improve target pseudolabel quality for finetuning the source model. Evaluation on 4 benchmark datasets show that our proposed strategy improves adapta-tion performance and can be successfully integrated with existing SFDA methods. Leveraging modern pre-trained networks that have stronger representation learning ability in the co-learning strategy further boosts performance. 1.

Introduction
Deep neural networks have demonstrated remarkable ability in diverse applications, but their performance typi-cally relies on the assumption that training (source domain) and test (target domain) data distributions are the same.
This assumption can be violated in practice when the source data does not fully represent the entire data distribution due to difficulties of real-world data collection. Target samples distributed differently from source samples (due to factors such as background, illumination and style [13, 20]) are subject to domain shift (also known as covariate shift), and can severely degrade model performance.
Domain adaptation (DA) aims to tackle the domain shift problem by transferring knowledge from a fully-labeled source domain to an unlabeled target domain. The classic setting of unsupervised domain adaptation assumes source and target data are jointly available for training [45]. Mo-tivated by the theoretical bound on target risk derived in
[3], a key strategy is to minimize the discrepancy between source and target features to learn domain-invariant fea-tures [11, 27, 24, 42, 52, 14, 17, 12, 47]. However, access to source data can be impractical due to data privacy con-cerns. Recently, the source-free domain adaptation (SFDA) setting is proposed [28] to split adaptation into two stages: (a) training network with fully labeled source data, and (b) adapting source model with unlabeled target data. An ex-ample use case in a corporate context is when the vendor has collected data to train a source model, and clients seek to address the same task for their own environments, but data sharing for joint training cannot be achieved due to proprietary or privacy reasons. The vendor makes available the source model, which the clients can adapt with available resources. In this work, we assume the role of the clients.
We focus on the classification task where source and tar-get domain share the same label space. The source model is typically obtained by training a selected network on source data with supervised loss. For adaptation, existing SFDA methods generate or estimate source-like representations to align source and target distributions [38, 8], make use of local clustering structures in the target data [49, 48, 50], and learn semantics through self-supervised tasks [46, 29].
[28, 23, 25] use the source model to generate target pseu-dolabels for finetuning, and [19, 5, 29] further select sam-ples with the low-entropy or low-loss criterion. However, model calibration is known to degrade under distribution shift [35]. We observe in Figure 1 that target pseudola-bels produced by source model can be biased, and outputs such as prediction confidence (and consequently entropy and loss) may not reflect accuracy and cannot reliably be used alone to improve pseudolabel quality.
We reconsider the role of pre-trained networks in SFDA.
Due to their common usage, the discussion in this paper is within the scope of ImageNet networks, but can be ap-plied to other large data pre-trained networks. As in Fig-ure 2, ImageNet weights are conventionally used to initial-ize source models and subsequently discarded. However,
(a) Confusion matrix (b) Distribution of confidence
Figure 1: VisDA-C source-trained ResNet-101 produces unreliable pseudolabels on target samples, and is over-confident on a significant number of incorrect predictions.
Figure 2: Overview of conventional and proposed frame-works: We propose incorporating pre-trained networks dur-ing target adaptation.
ImageNet networks have diverse features important for gen-eralization [6], and finetuning on source data can overfit to source distribution and potentially lose pre-existing target information. Furthermore, modern ImageNet networks may have better generalizability following new architectures and training schemes. We seek to answer the questions:
• Can finetuning pre-trained networks on source data lose applicable target domain knowledge?
• Given a source model, whether and how pre-trained networks can help its adaptation?
We propose to integrate ImageNet networks into the tar-get adaptation process, as depicted in Figure 2, to dis-til any effective target domain knowledge they may hold.
They can help to correct source model prediction bias and produce more accurate target pseudolabels to finetune the source model. We design a simple two-branch co-learning strategy where the adaptation and pre-trained model branch iteratively updates to collaboratively generate more accu-rate pseudolabels, which can be readily applied to existing
SFDA methods as well. We provide an overview of our strategy in Figure 3, and summarize our contributions:
• We observe that finetuning pre-trained networks on source data can lose generalizability for target domain;
• Based on the above observation, we propose integrat-ing pre-trained networks into SFDA target adaptation;
• We propose a simple co-learning strategy to distil use-ful target domain information from a pre-trained fea-ture extractor to improve target pseudolabel quality;
• We demonstrate performance improvements by the proposed strategy (including just reusing the ImageNet network in source model initialization) on 4 bench-mark SFDA image classification datasets. 2.