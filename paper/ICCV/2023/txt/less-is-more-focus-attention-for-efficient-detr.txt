Abstract
DETR-like models have significantly boosted the perfor-mance of detectors and even outperformed classical con-volutional models. However, all tokens are treated equally without discrimination brings a redundant computational burden in the traditional encoder structure. The recent spar-sification strategies exploit a subset of informative tokens to reduce attention complexity maintaining performance through the sparse encoder. But these methods tend to rely on unreliable model statistics. Moreover, simply reducing the token population hinders the detection performance to a large extent, limiting the application of these sparse mod-els. We propose Focus-DETR, which focuses attention on more informative tokens for a better trade-off between com-putation efficiency and model accuracy. Specifically, we re-construct the encoder with dual attention, which includes a token scoring mechanism that considers both localization and category semantic information of the objects from multi-scale feature maps. We efficiently abandon the background queries and enhance the semantic interaction of the fine-grained object queries based on the scores. Compared with the state-of-the-art sparse DETR-like detectors under the same setting, our Focus-DETR gets comparable complex-ity while achieving 50.4AP (+2.2) on COCO. The code is available at torch-version† and mindspore-version‡. 1.

Introduction
Object detection is a fundamental task in computer vi-sion that aims to predict the bounding boxes and classes of objects in an image, as shown in Fig. 1 (a), which is of great importance in real-world applications. DETR proposed by
Carion et al.[1] uses learnable queries to probe image fea-tures from the output of Transformer encoders and bipar-tite graph matching to perform set-based box prediction.
*Corresponding author
†https://github.com/huawei-noah/noah-research/tree/master/Focus-DETR
‡https://gitee.com/mindspore/models/tree/master/research/cv/Focus-DETR
Figure 1: Visualization and comparison of tokens selected by
Sparse DETR [26] and our Focus-DETR. (a) is the original images, (b) and (c) represent the foreground selected by models. (d) indi-cates the object tokens with more fine-grained category semantic.
Patches with smaller sizes come from higher-level features.
DETR-like models [18, 36, 14, 32, 21, 26, 2, 30, 37] have made remarkable progress and bridged the gap with the de-tectors based on convolutional neural networks.
Global attention in the DETR improves the detection per-formance but suffers from computational burden and inef-ficiency due to redundant calculation without explicit dis-crimination for all tokens. To tackle this issue, Deformable
DETR [37] reduces the quadratic complexity to linear com-plexity through key sparsification, and it has developed into a mainstream paradigm due to the advantages of leveraging multi-scale features. Herein, we further analyze the com-putational burden and latency of components in these mod-els (Fig. 2). As shown in Fig. 2, we observe that the cal-culation cost of the encoder is 8.8× that of the decoder in
Deformable DETR [37] and 7.0× in DINO [36]. In addi-tion, the latency of the encoder is approximately 4∼8 times that of the decoder in Deformable DETR and DINO, which emphasizes the necessity to improve the efficiency in the
In line with this, previous works have encoder module. generally discussed the feasibility of compressing tokens in the transformer encoder. For instance, PnP-DETR [29] ab-stracts the whole features into fine foreground object feature vectors and a small number of coarse background contextual feature vectors. IMFA [34] searches key points based on the prediction of decoder layer to sample multi-scale features and aggregates sampled features with single-scale features.
Sparse DETR [26] proposes to preserve the 2D spatial struc-ture of the tokens through query sparsity, which makes it ap-plicable to Deformable DETR [37] to utilize multi-scale fea-tures. By leveraging the cross-attention map in the decoder as the token importance score, Sparse DETR achieves per-formance comparable to Deformable DETR only using 30% of queries in the encoder.
Despite all the progress, the current models [29, 26] are still challenged by sub-optimal token selection strategy. As shown in Fig. 1 (b), the selected tokens contain a lot of noise and some necessary object tokens are obviously over-looked.
In particular, Sparse DETR’s supervision of the foreground predictor relies heavily on the decoder’s cross-attention map (DAM), which is calculated based on the decoder’s queries entirely from encoder priors. Prelimi-nary experiments show severe performance decay when the
Sparse DETR is embedded into the models using learnable queries due to weak correlation between DAM and the re-tained foreground tokens. However, state-of-the-art DETR-like models, such as DINO [36], have proven that the se-lected features are preliminary content features without fur-ther refinement and could be ambiguous and misleading to the decoder. In this case, DAM’s supervision is inefficient.
Moreover, in this monotonous sparse encoder, the number of retained foreground tokens remains numerous, and per-forming the query interaction without more fine-grained se-lection is not feasible due to computational cost limitations.
To address these issues, we propose Focus-DETR to al-locate attention to more informative tokens by stacking the localization and category semantic information. Firstly, we design a scoring mechanism to determine the semantic level of tokens. Foreground Token Selector (FTS) aims to aban-don background tokens based on top-down score modula-tions across multi-scale features. We assign {1,0} labels to all tokens from the backbone with reference to the ground truth and predict the foreground probability. The score of the higher-level tokens from multi-scale feature maps mod-ulates the lower-level ones to impose the validity of selec-tion. To introduce semantic information into the token se-lection process, we design a multi-category score predictor.
The foreground and category scores will jointly determine the more fine-grained tokens with strong category seman-tics, as shown in Fig. 1 (d). Based on the reliable scores and selection from different semantic levels, we feed foreground tokens and more fine-grained object tokens to the encoder with dual attention. Thus, the limitation of deformable at-tention in distant information mixing is remedied, and then the semantic information of foreground queries is enhanced
Figure 2: Distribution of calculation cost and latency in the
Transformer part of the DETR-like models, e.g., Deformable
DETR [37], DINO [36] and our Focus-DETR. by fine-grained token updates.
To sum up, Focus-DETR reconstructs the encoder’s cal-culation process with dual attention based on obtaining more accurate foreground information and focusing on fine-grained tokens by gradually introducing semantic informa-tion, and further enhances fine-grained tokens with mini-mal calculation cost. Extensive experiments validate Focus-DETR’s performance. Furthermore, Focus-DETR is gen-eral for DETR-like models that use different query con-struction strategies. For example, our method can achieve 50.4AP (+2.2) on COCO compared to Sparse DETR with a similar computation cost under the same setting. 2.