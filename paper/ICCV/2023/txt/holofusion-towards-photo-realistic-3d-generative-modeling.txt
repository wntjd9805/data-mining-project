Abstract
Diffusion-based image generators can now produce high-quality and diverse samples, but their success has yet to fully translate to 3D generation: existing diffusion meth-ods can either generate low-resolution but 3D consistent outputs, or detailed 2D views of 3D objects but with poten-tial structural defects and lacking view consistency or real-ism. We present HOLOFUSION, a method that combines the best of these approaches to produce high-fidelity, plausible, and diverse 3D samples while learning from a collection of multi-view 2D images only. The method first generates coarse 3D samples using a variant of the recently proposed
HoloDiffusion generator. Then, it independently renders and upsamples a large number of views of the coarse 3D model, super-resolves them to add detail, and distills those into a single, high-fidelity implicit 3D representation, which also ensures view-consistency of the final renders. The super-resolution network is trained as an integral part of
HOLOFUSION, end-to-end, and the final distillation uses a new sampling scheme to capture the space of super-resolved signals. We compare our method against existing baselines, including DreamFusion, Get3D, EG3D, and HoloDiffusion, and achieve, to the best of our knowledge, the most realistic results on the challenging CO3Dv2 dataset. 1.

Introduction
Diffusion models [32, 7, 31] are at the basis of state-of-the-art 2D image generators which can now produce very high-quality and diverse outputs. However, their success has yet to be translated to 3D and there is no generator that can produce 3D assets of a comparable quality.
Recent attempts at extending diffusion to 3D generation have reported mixed success. Some authors have attempted to apply diffusion directly in 3D [18], or still in 2D but us-ing a 3D-aware neural network [42, 1]. This requires solv-ing two problems: first, finding a suitable 3D representa-tion (e.g., triplane features [4], mesh [19], voxels [18]) that scales well with resolution and is amenable to diffusion;
and, second, obtaining a large amount of 3D training data, for example using synthetic models [41, 27], or training the model using only 2D images [18], often via differentiable (volume) rendering [13, 26]. However, the quality of results so far is limited, especially when training on real images.
Other authors have proposed to distill 3D objects from pre-trained 2D image generators. For instance Score Distil-lation Sampling (SDS) [29] can sample 3D objects from a high-quality off-the-shelf 2D diffusion model while requir-ing no (re)training. However, without any 3D guidance, dis-tillation methods often produce implausible results; for ex-ample, they suffer from the ‘Janus effect’, where details of the front of the object are replicated on its back. They also create overly-smooth outputs that average out inconsisten-cies arising from the fact that the signal obtained from the 2D model is analogous to sampling independent views of the object (see Sec. 4.2 for examples). Furthermore, distil-lation methods do not support unconditional sampling, even if the underlying image generator does, as strong language guidance is required to stabilise the 3D reconstruction.
In this work, we propose HOLOFUSION, a method that combines the best of both approaches. We start from
HoloDiffusion [18], a diffusion-based 3D generator. This model can be trained using only a multiview image dataset like [30] and produces outputs that are 3D consistent. How-ever, the output resolution is limited by computation and memory. We augment the base model with a lightweight super-resolution network that upscales the initial renders.
Crucially, the 2D super-resolution model is integrated and trained jointly with the 3D generator, end-to-end.
The super-resolution network outputs detailed views of the 3D object, and the underlying 3D generator ensures that the coarse structure of these views is indeed consis-tent (e.g., avoiding the Janus effect and other structural arti-facts). However, the 2D upscaling still progresses indepen-dently for different views, which means that fine grained details may still be inconsistent between views. We address this issue by distilling a single, coherent, high quality 3D model of the object from the output of the upsampler. For this, we propose a new distillation technique that efficiently combines several putative super-resolved views of the ob-ject into a single, coherent 3D reconstruction.
With this, we are able to train a high-quality 3D genera-tor model purely from real 2D data. This model is capable of generating consistent and detailed 3D objects, which in turn result in view-consistent renderings (see Fig. 1) at a quality not achievable by prior methods.
We evaluate HOLOFUSION on real images (CO3Dv2 dataset [30]) and compare with a variety of compet-ing alternatives (e.g., HoloDiffusion [18], Get3D [9],
EG3D [4], DreamFusion [40]) demonstrating that view-consistent high-quality 3D generation is possible using our simple, effective, easy-to-implement hybrid approach. 2.