Abstract
Controllable person image synthesis aims at rendering a source image based on user-speciﬁed changes in body pose or appearance. Prior art approaches leverage pixel-level denoising diffusion models conditioned on the coarse skele-ton via cross-attention. This leads to two limitations: low efﬁciency and inaccurate condition information. To address both issues, a novel Pose-Constrained Latent Diffusion model (PoCoLD) is introduced. Rather than using the skele-ton as a sparse pose representation, we exploit DensePose which offers much richer body structure information. To ef-fectively capitalize DensePose at a low cost, we propose an efﬁcient pose-constrained attention module that is capable of modeling the complex interplay between appearance and pose. Extensive experiments show that our PoCoLD out-performs the state-of-the-art competitors in image synthe-sis ﬁdelity. Critically, it runs 2
× smaller memory than the latest diffusion-model-based alter-native during inference. Our code and models are available at https://github.com/BrandonHanx/PoCoLD. faster and consumes 3.6
× 1.

Introduction
The task of Controllable Person Image Synthesis (CPIS) is to modify a source image according to the user-speciﬁed changes in body pose or appearance [2, 23, 30]. Underpin-ning a wide variety of applications in virtual and augmented reality, gaming, and fashion [11,12], there has been increas-ing attention in the computer vision community. In partic-ular, modeling the large pose deformations in the 2D ap-pearance caused by 3D pose changes is one of the biggest changes in CPIS [18]. This is further compounded by the inevitable complex self-occlusion of the human body, caus-ing further uncertainties in predicting unobserved appear-ance for the target pose. Consequently, having the genera-tive CPIS model understand the whole image contextually is indispensable in order to achieve plausible synthesis.
Generative Adversarial Networks (GAN) [9, 26] have been the major architecture used in CPIS [25, 30, 31, 35].
Figure 1. Our PoCoLD is a latent diffusion model that can handle person image synthesis with pose or appearance control. It is con-ditioned on the target DensePose map and the appearance of the source image. An efﬁcient pose-constrained attention is proposed to explicitly regularize the denoising learning process.
However, these existing models are challenged by the need of preserving a consistent body structure and garment tex-ture in a single forward pass. Recently, Diffusion Mod-els (DM) [14, 36] have emerged as a favorable alternative to GAN, additionally with more stable optimization and simpler loss function design. Inspired by the massive ad-vance of image generation and editing [6, 29, 32, 33], dif-fusion models have been recently exploited for CPIS with the best-ever results achieved [2]. However, this diffusion-based method comes with a couple of drawbacks: (1) Mis-match between the sparse pose condition (i.e., body skele-ton) and the source image with dense details: When mod-eling their interaction, such information intensity mismatch might lead to ill association, ﬁnally hurting the ﬁnal syn-thesis. (2) Slow inference as the denoising diffusion takes place in the high-resolution pixel space iteratively. For in-stance, prior diffusion-based art [2] needs approximately 10 seconds for one 256 176 image generation on a machine with one V100 GPU.
×
To address the aforementioned limitations, a novel Pose-Constrained Latent Diffusion (PoCoLD) method is pro-posed in this work. Although latent diffusion process [32] is much more efﬁcient than pixel-level diffusion, adopting it for CPIS is non-trivial because once the source image is converted into a latent feature space, the original structural information is prone to be distorted during conditional de-noising. To alleviate this problem, PoCoLD is formulated by using DensePose [10] as the pose control with much denser structural pose and shape information compared to body skeleton. Moreover, we design a pose-constrained at-tention module to effectively and efﬁciently integrate the source image information with the target pose (Fig. 1).
In particular, the pose constraint is derived from both the source and target DensePose maps for calibrating the spar-siﬁed attention prediction. Consequently, the appearance details of the source image with correspondence to local re-gions of the target pose can be better captured, leading to more accurate and realistic synthesis (Fig. 5).
Our contributions are as follows: (1) We propose the
ﬁrst latent diffusion-based method for controllable person image synthesis with condition on DensePose based user control. (2) A new pose-constrained attention module is formulated for effectively and efﬁciently modeling the non-trivial interplay between source appearance and target pose. (3) Extensive experiments on the DeepFashion [20] bench-mark show that our PoCoLD sets new state of the art under the key performance metrics (e.g., SSIM and LPIPS) whilst enjoying signiﬁcantly faster inference than prior diffusion-model-based alternative. A user study is also conducted to further validate the superiority of our model in the quality of generated images. Also, we show that our model can be applied for more tasks (e.g., pose-only conditioned person image synthesis and appearance transfer) without architec-tural change and further optimization. 2.