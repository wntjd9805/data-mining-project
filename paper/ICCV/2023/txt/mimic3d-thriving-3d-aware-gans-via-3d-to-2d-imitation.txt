Abstract
Generating images with both photorealism and multi-view 3D consistency is crucial for 3D-aware GANs, yet existing methods struggle to achieve them simultaneously.
Improving the photorealism via CNN-based 2D super-resolution can break the strict 3D consistency, while keep-ing the 3D consistency by learning high-resolution 3D rep-resentations for direct rendering often compromises im-age quality.
In this paper, we propose a novel learning strategy, namely 3D-to-2D imitation, which enables a 3D-aware GAN to generate high-quality images while main-taining their strict 3D consistency, by letting the images synthesized by the generator’s 3D rendering branch mimic those generated by its 2D super-resolution branch. We also introduce 3D-aware convolutions into the generator for better 3D representation learning, which further im-proves the image generation quality. With the above strate-gies, our method reaches FID scores of 5.4 and 4.3 on 512 res-FFHQ and AFHQ-v2 Cats, respectively, at 512 olution, largely outperforming existing 3D-aware GANs using direct 3D rendering and coming very close to the previous state-of-the-art method that leverages 2D super-resolution. Project website: https://seanchenxy. github.io/Mimic3DWeb.
× 1.

Introduction 3D-aware GANs [35, 9, 6, 3] have experienced rapid development in recent years and shown great potential for large-scale realistic 3D content creation. The core of 3D-aware GANs is to incorporate 3D representation learning and differentiable rendering into image-level adversarial learning [8]. In this way, the generated 3D representations are forced to mimic real image distribution from arbitrary viewing angles, resulting in their faithful reconstruction of the underlying 3D structures of the subjects for free-view image synthesis. Among different 3D representations, neu-ral radiance field (NeRF) [23] has been proven to be ef-fective in the 3D-aware GAN scenario [35, 4], which guar-antees strong 3D consistency when synthesizing multiview
*These authors have contributed equally to this work.
Figure 1. Comparison between different 3D-aware GANs on im-age generation quality and multiview 3D consistency. The image generation quality is evaluated via FID between generated and real images. The 3D consistency is measured by conducting 3D re-construction [43] on generated multiview images and calculating
PSNR between them and the re-rendered reconstruction results.
Our method inherits the high image quality of approaches lever-aging 2D super-resolution meanwhile maintains strict 3D consis-tency by taking the advantage of direct 3D rendering. images via volume rendering [14].
However, NeRF’s volumetric representation also brings high computation costs to GAN training. This hinders the generative models from synthesizing high-resolution im-ages with fine details. Several attempts have been made to facilitate NeRF-based GAN training at high resolution, via sparse representations [36, 6, 45, 52] or patch-wise adver-sarial learning [41], yet the performance is still unsatisfac-tory and lags far behind state-of-the-art 2D GANs [18, 16].
Along another line, instead of using direct NeRF ren-dering, plenty of works [25, 9, 27, 3, 48] introduce 2D super-resolution module to deal with 3D-aware GAN train-ing at high resolution. A typical procedure is to first ren-der a NeRF-like feature field into low-resolution feature maps, then apply a 2D CNN to generate high-resolution images from them. The representative work among this line, namely EG3D [3], utilizes tri-plane representation to effectively model the low-resolution feature field and lever-ages StyleGAN2-like [18] super-resolution block to achieve image synthesis at high-quality. It sets a record for image
Figure 2. Our method enables high-quality image generation at 512 512 resolution without using a 2D super-resolution module.
× quality among 3D-aware GANs and gets very close to that of state-of-the-art 2D GANs. However, a fatal drawback of this line of works is a sacrifice of strict 3D consistency, due to leveraging a black-box 2D CNN for image synthesis.
A question naturally arises —— Is there any way to combine the above two lines to achieve strict 3D consis-tency and high-quality image generation simultaneously?
The answer, as we will show in this paper, is arguably yes.
The key intuition is to let the images synthesized by direct
NeRF rendering to mimic those generated by a 2D super-resolution module, which we name 3D-to-2D imitation.
Specifically, we start from an EG3D backbone that adopts 2D super-resolution to generate high-resolution im-ages from a low-resolution feature field. Based on this ar-chitecture, we add another 3D super-resolution module to generate high-resolution NeRF from the low-resolution fea-ture field and force the images rendered by the former to imitate those generated by the 2D super-resolution branch.
This process can be seen as a multiview reconstruction pro-cess —— images sharing the same latent code from differ-ent views produced by the 2D branch are pseudo multiview data, and the high-resolution NeRF branch represents the 3D scene to be reconstructed. Previous methods [28, 51, 32] have shown that this procedure can obtain reasonable 3D re-construction, even if the multiview data are not strictly 3D consistent. We believe this is partially due to the induc-tive bias (e.g., continuity and sparsity) of the underlying 3D representation. With the above process, the high-resolution
NeRF learns to depict fine details of the 2D-branch images, thus enabling high-quality image rendering. The 3D con-sistency across different views can also be preserved thanks to the intrinsic property of NeRF. Note that if the rendered images try to faithfully reconstruct every detail of the 2D-branch images across different views, it is likely to obtain blurry results due to detail-level 3D inconsistency of the lat-ter. To avoid this problem, we only let the images produced by the two branches be perceptually similar (i.e. by LPIPS loss [49]), and further enforce adversarial loss between the rendered images from the high-resolution NeRF and real images to maintain high-frequency details. In addition, we only render small image patches to conduct the imitative learning to reduce memory costs.
Apart from the above learning strategy, we introduce 3D-aware convolutions to the EG3D backbone to im-prove tri-plane learning, motivated by a recent 3D diffu-sion model [44]. The original EG3D generates tri-plane features to model the low-resolution feature field via a
StyleGAN2-like generator. The generator is forced to learn 2D-unaligned features on the three orthogonal planes via 2D convolutions, which is inefficient. The 3D-aware convo-lution considers associated features in 3D space when per-forming 2D convolution, which improves feature commu-nications and helps to produce more reasonable tri-planes.
Nevertheless, directly applying 3D-aware convolution in all layers in the generator is unaffordable. As a result, we only apply them after the output layers at each resolution in the tri-plane generator. This helps us to further improve the image generation quality with only a minor increase in the total memory consumption.
×
With the above strategies, our generator is able to syn-thesize 3D-consistent images of virtual subjects with high image quality (Fig. 2). It reaches FID scores [12] of 5.4 and 4.3 on FFHQ [17] and AFHQ-v2 Cats [5], respectively, at 512 512 resolution, largely outperforming previous 3D-aware GANs with direct 3D rendering and even surpassing many leveraging 2D super-resolution (Fig. 1). A by-product of our method is a more powerful 2D-branch generator, which reaches an FID of 4.1 on FFHQ, exceeding previous state-of-the-art EG3D. Though our method presented in this paper is mostly based on EG3D backbone, its 3D-to-2D im-itation strategy can be extended to learning other 3D-aware
GANs as well. We believe this would largely close the qual-ity gap between 3D-aware GANs and traditional 2D GANs, and pave a new way for realistic 3D generation. 2.