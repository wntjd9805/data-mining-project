Abstract
Overfitting to the source domain is a common issue in gradient-based training of deep neural networks. To com-pensate for the over-parameterized models, numerous reg-ularization techniques have been introduced such as those based on dropout. While these methods achieve significant improvements on classical benchmarks such as ImageNet, their performance diminishes with the introduction of do-main shift in the test set i.e. when the unseen data comes from a significantly different distribution. In this paper, we move away from the classical approach of Bernoulli sam-pled dropout mask construction and propose to base the selection on gradient-signal-to-noise ratio (GSNR) of net-workâ€™s parameters. Specifically, at each training step, pa-rameters with high GSNR will be discarded. Furthermore, we alleviate the burden of manually searching for the opti-mal dropout ratio by leveraging a meta-learning approach.
We evaluate our method on standard domain generalization benchmarks and achieve competitive results on classifica-tion and face anti-spoofing problems. 1.

Introduction
In recent years, deep neural networks achieved remark-ably good results on several classification tasks, facilitated by regularization methods that successfully reduce over-fitting of large models to the training data. A simple yet powerful technique is Dropout [51], which mutes randomly chosen activations of fully connected layers at each training iteration. A better-suited variant for Convolutional Neural
Networks is Dropblock [14], which masks contiguous regions of feature maps with spatial correlation. While existing regularization techniques achieve great results,
*Work done while Xiang was at NEC Labs America
Figure 1: Different approaches to dropout mask construc-tion. Left to right: (a) classical approach where values are sampled from a Bernoulli distribution [51]; (b) RSC
[19], where parameters with highest gradients are masked
; (c) ours, where parameters with highest gradient-signal-to-noise ratio are masked. The amount of discarded param-eters is controlled by a dropout ratio p which is typically manually chosen, whereas in our approach it is automati-cally learned. their success is based on the underlying assumption that the train and test data follow similar distributions. A more practical scenario, however, is presented in a domain generalization setting, where there is a distribution shift between the train and test set [50, 74]. Here, models equipped with classical regularization techniques often fail to generalize their inference to unseen examples.
The goal of our work is to build a model that is robust to the domain shift and performs equally well on both source and unseen test domains. We build our model on two obser-vations. First, models with high Gradient Signal to Noise
Ratio (GSNR), defined as the ratio of squared mean over variance of parameters gradients on a particular data distri-bution, exhibit a smaller generalization gap i.e. their perfor-mance does not drastically decrease when evaluated on the unseen data [32]. Second, by iteratively dropping the most predictive parameters, the model is forced to learn less dom-inant features which might correspond to domain-invariant features, thus improving performance on unseen domains
[19].
In light of this discussion, we carefully design a dropout strategy to drop parameters with highest gradient-signal-to-noise ratio in each training step which we illustrate in Fig-ure 1. As a result, the overall GSNR of the model improves which leads to a better generalization performance. Further-more, we have observed that different blocks of neural net-work favour different dropout ratios. Therefore, we replace the standard approach of applying a fixed and manually cho-sen dropout probability p by leveraging a learning-to-learn technique [10, 30] to learn the dropout probability for each neural network block. Lastly, we validate our approach through extensive experiments on benchmark domain gen-eralization datasets on classification and face recognition tasks, and show that our approach outperforms all the base-lines including the ones based on a standard dropout strat-egy. More specifically, our GSNR-guided dropout is com-plementary to the the recent method of Representation Self-Challenging (RSC)
[19] which drops out features based on gradient magnitudes with a fixed probability. We per-form extensive experiments on widely used domain gener-alization datasets i.e., DomainNet [40], OfficeHome [54] and PACS [26] and the OCIM benchmark for face anti-spoofing consisting of 4 different datasets: OULU-NPU
[2], CASIA-FASD [70], MSU-MFSD [6], and REPLAY-ATTACK [61]. The results show that our method con-sistently leads to improvements compared to the existing methods.
To summarize, our contributions are three-fold: 1. We introduce a novel dropout strategy based on GSNR, that can be easily incorporated in any standard convo-lutional neural network architecture. 2. We alleviate the problem of choosing optimal dropout ratios through a novel meta-learning framework. 3. We empirically validate our approach on a number of domain generalization benchmark datasets for object classification and face anti-spoofing tasks. 2.