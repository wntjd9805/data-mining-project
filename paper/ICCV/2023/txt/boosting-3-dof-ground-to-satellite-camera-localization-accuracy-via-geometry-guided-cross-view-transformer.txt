Abstract
Image retrieval-based cross-view localization methods often lead to very coarse camera pose estimation, due to the limited sampling density of the database satellite im-ages. In this paper, we propose a method to increase the accuracy of a ground camera’s location and orientation by estimating the relative rotation and translation between the ground-level image and its matched/retrieved satellite im-age. Our approach designs a geometry-guided cross-view transformer that combines the benefits of conventional ge-ometry and learnable cross-view transformers to map the ground-view observations to an overhead view. Given the synthesized overhead view and observed satellite feature maps, we construct a neural pose optimizer with strong global information embedding ability to estimate the rela-tive rotation between them. After aligning their rotations, we develop an uncertainty-guided spatial correlation to generate a probability map of the vehicle locations, from which the relative translation can be determined. Exper-imental results demonstrate that our method significantly outperforms the state-of-the-art. Notably, the likelihood of restricting the vehicle lateral pose to be within 1m of its
Ground Truth (GT) value on the cross-view KITTI dataset has been improved from 35.54% to 76.44%, and the likeli-hood of restricting the vehicle orientation to be within 1◦ of its GT value has been improved from 19.64% to 99.10%. 1.

Introduction
Autonomous robots, such as unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs), are be-coming more and more popular in various fields of appli-cations. One of the most demanding capabilities of au-tonomous vehicles is navigating and executing tasks au-tonomously in complex environments, especially in envi-ronments with poor GPS signals. This motivates recent re-search on vision-based localization.
Ground-to-satellite image-based localization is a vision-(a) Joint 3-DoF pose optimization [29] (b) Ours
Figure 1. Conventional image-retrieval methods for ground-to-satellite localization provide a rough location and orientation esti-mation for the ground camera, denoted by the red dot and arrow in the images. This paper aims to refine this pose under the same ground-to-satellite image-matching context. Compared to joint ro-tation and translation optimization [29], which is susceptible to lo-cal minima (a), this paper introduces a new method that allows dense search at all possible locations. Our method produces a probability map (b) over the continuous search space of vehicle locations, thus achieving high localization accuracy. based localization task that aims to estimate the location and orientation of a ground camera by matching a ground-level image against a large satellite map. The task was originally proposed for city-scale localization and tackled by image retrieval techniques. However, image retrieval techniques can only provide a rough pose approximation of the ground camera, and the sample density of the database image al-ways dictates the estimated pose accuracy.
Recent works have explored increasing the localization accuracy by estimating a relative translation and rotation between the ground and its matching satellite images. Ex-isting works have tried to regress the relative translation by
MLPs [50], or further split the retrieved satellite image to a
N × N grid and match the ground image against the grid to improve the localization accuracy [43]. However, both of them cannot estimate the orientation of ground cameras.
To estimate a 3-DoF camera pose (location and orienta-tion), a deep cross-view optimization scheme has been pro-posed [29]. Nonetheless, optimization-based methods are highly susceptible to local minima, as shown in Fig. 1 (a).
To avoid this issue, this paper presents a new framework that allows searching for vehicle locations densely over the entire solution space.
It estimates the orientation and lo-cation of the ground camera sequentially and is designed based on an observation that neural networks behave differ-ently to rotations and translations on input signals.
Specifically, neural network outputs tend to magnify the rotation difference on input signals, making it a desired choice for rotation estimation. Thus, we propose a neu-ral network-based pose optimizer. It works with an over-head view feature synthesis module, which synthesizes an overhead view feature map from the query ground view im-age, to estimate the relative rotation between the ground and satellite image. However, due to feature aggregation layers (e.g., pooling), a small translation difference in input sig-nals might be absorbed in high-level deep features. This makes the estimated translation by an optimizer constructed by neural networks inaccurate.
On the other hand, when the orientation of the synthe-sized overhead view feature map from the ground view im-age has been aligned with the satellite image, the vehicle location can be obtained by performing a spatial correlation between them. The spatial correlation generates a proba-bility map of vehicle locations over the entire search space, as shown in Fig. 1 (b). This allows a dense and exhaustive search for vehicle locations.
Our overhead view feature synthesis module is designed as a geometry-guided cross-view transformer. It explicitly embeds the deterministic geometric correspondences to the learnable cross-view transformers. Compared to the ground plane homography projection in [29], our method handles the height ambiguity of scene objects and the ground cam-era’s slight tilt and roll angle change.
Our contributions are summarized as follows:
• a rotation and translation decoupled cross-view cam-era localization framework, which achieves state-of-the-art performance on four widely used benchmarks;
• a neural pose optimizer for rotation estimation, which produces highly-accurate rotation estimation results;
• a dense search mechanism for translation estimation, which computes a possibility map of vehicle locations over the entire search space;
• a geometry-guided cross-view transformer for ground-to-overhead view feature synthesis, which combines the wisdom of deterministic geometric and data-driven learnable correspondences. 2.