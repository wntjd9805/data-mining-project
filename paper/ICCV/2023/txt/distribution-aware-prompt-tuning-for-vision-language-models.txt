Abstract
Pre-trained vision-language models (VLMs) have shown impressive performance on various downstream tasks by
In general, utilizing knowledge learned from large data. the performance of VLMs on target tasks can be further im-proved by prompt tuning, which adds context to the input image or text. By leveraging data from target tasks, vari-ous prompt-tuning methods have been studied in the liter-ature. A key to prompt tuning is the feature space align-ment between two modalities via learnable vectors with model parameters fixed. We observed that the alignment becomes more effective when embeddings of each modal-ity are ‘well-arranged’ in the latent space. Inspired by this observation, we proposed distribution-aware prompt tuning (DAPT) for vision-language models, which is simple yet ef-fective. Specifically, the prompts are learned by maximizing inter-dispersion, the distance between classes, as well as minimizing the intra-dispersion measured by the distance between embeddings from the same class. Our extensive ex-periments on 11 benchmark datasets demonstrate that our method significantly improves generalizability. The code is available at https://github.com/mlvlab/DAPT. 1.

Introduction
In recent years, pre-trained vision-language models (VLMs) have shown great success in a wide range of applications in computer vision such as image classifi-cation [28, 32], object detection [7, 11, 43], captioning
[21, 24, 41], and visual question answering (VQA) [9]. No-tably, VLMs have shown promising generalization power and transferability in various downstream tasks. For in-stance, VLMs such as CLIP [28] and ALIGN [15] show outstanding performance in zero-shot and few-shot learn-ing. These models opened the door for zero-shot image classification and zero-shot object detection. To further im-prove the pre-trained models’ zero-shot generalization abil-*Equal contribution.
†Corresponding author. (a) Zero-shot CLIP (b) DAPT t x e
T e g a m
I (c) Zero-shot CLIP (d) DAPT
Figure 1: Points in (a) and (b) denote normalized t-SNE embeddings of text features from OxfordPets [26]. In ad-dition, points on (c) and (d) represent the image features’ t-SNE embeddings from EuroSAT [12]. As shown in (a) and (b), zero-shot CLIP has small distances between text embeddings of class labels, but its image embeddings do not cluster well. However, prompt-tuning with DAPT leads to more evenly spaced text embeddings and better cluster-ings of image embeddings within the same class. ity, prompting has been proposed. For instance, in image classification, CLIP [28] suggests using a context text “A photo of a ” in front of the class label [CLASS] to obtain text embeddings for target classes.
Prompting is an emerging research topic due to several advantages over fine-tuning, which is a conventional ap-proach to utilize pre-trained deep-learning models. For a pre-trained VLM, fine-tuning is often practically challeng-ing due to the large number of model parameters. In ad-dition, fine-tuning the entire VLM often results in overfit-ting due to a small amount of target domain data. Zhou et al. [46] have shown that more powerful context strings (hard prompts) exist. However, manually finding better hard prompts (prompt engineering) is time-consuming and suboptimal. So, after that, a line of works has proposed prompt-tuning that optimizes soft prompts, learnable vec-tors [16, 20, 46]. The learnable vectors are concatenated with other inputs and numerically optimized by backprop-agation while the pre-trained VLM models parameters are fixed.
The prompt tuning can be viewed as the alignment be-tween two latent spaces of text and image. Figure 1 shows that each latent space of CLIP is not suitable for fea-ture alignment. The text embeddings of target classes ob-tained from the original CLIP in Figure 1a are gathered nearby, which potentially leads to misclassification to close classes. In addition, the original CLIP’s visual embeddings in Figure 1c are widely spread, and some regions are over-lapped. To address this problem, we propose a prompt tun-ing method, DAPT1, that optimizes the distribution of em-beddings for each modality for better feature alignment.
DAPT learns vectors (i.e., soft prompts) for both text and image encoders with additional loss terms - inter-dispersion loss and intra-dispersion loss. Specifically, we apply the inter-dispersion loss to the text prompts to spread text em-beddings. On the other hand, intra-dispersion loss is applied to the visual prompts to minimize the variability of image embeddings of the same class.
To verify the effectiveness of DAPT, we conducted ex-periments on few-shot learning and domain generalization tasks with various benchmark datasets. For few-shot learn-ing with one sample (1-shot) to 16 samples (16-shots) per class, the proposed method is evaluated on 11 benchmark datasets. For domain generalization, 4 benchmark datasets were used after few-shot learning on ImageNet [5]. Overall, we achieve a significant improvement over recent baselines for few-shot learning and domain generalization.
In summary, we propose DAPT, a prompt tuning method that is aware of the data distributions to improve the perfor-mance of VLMs in the few-shot learning setup. Unlike the orthodox prompt tuning method, DAPT optimizes text and visual prompts to find the appropriate distribution in each modality. In Section 3, we discuss the details of DAPT and show the various experiments in Section 4. 2.