Abstract
The ability to forecast human-environment collisions from egocentric observations is vital to enable collision avoidance in applications such as VR, AR, and wearable assistive robotics.
In this work, we introduce the chal-lenging problem of predicting collisions in diverse envi-ronments from multi-view egocentric videos captured from body-mounted cameras. Solving this problem requires a generalizable perception system that can classify which hu-man body joints will collide and estimate a collision re-gion heatmap to localize collisions in the environment. To achieve this, we propose a transformer-based model called
COPILOT to perform collision prediction and localiza-tion simultaneously, which accumulates information across multi-view inputs through a novel 4D space-time-viewpoint attention mechanism. To train our model and enable future research on this task, we develop a synthetic data genera-tion framework that produces egocentric videos of virtual humans moving and colliding within diverse 3D environ-ments. This framework is then used to establish a large-scale dataset consisting of 8.6M egocentric RGBD frames.
Extensive experiments show that COPILOT generalizes to unseen synthetic as well as real-world scenes. We further
* Equal contribution demonstrate COPILOT outputs are useful for downstream collision avoidance through simple closed-loop control.
Please visit our project webpage at https://sites. google.com/stanford.edu/copilot. 1.

Introduction
Forecasting potential collisions between a human and their environment from egocentric observations is crucial to applications such as virtual reality (VR), augmented re-ality (AR), and wearable assistive robotics. As users are im-mersed in VR / AR, the effective and non-intrusive preven-tion of collisions with real-world surroundings is critical.
On the other hand, wearable exoskeletons [52, 10] provide physical assistance to users with mobility impairments and therefore must control user movements to avoid dangerous falls by estimating when and where a collision may occur.
To this end, a multitude of works have explored ways to predict and localize future collisions. For example, ex-isting approaches for VR leverage redirected walking tech-niques [42] to guide users in the virtual world such that they avoid collisions with real-world objects [23, 22, 48, 15, 57].
However, these approaches rely on pre-scanned scenes and external tracking systems [23, 48, 15, 57] or resort to on-line SLAM [22, 12], making them cumbersome and tedious to use. For assistive exoskeletons, several methods have
been proposed to classify environment types [28, 27], rec-ognize obstacle parameters [31, 25], predict collision risks
[26, 3], and predict future trajectories [5, 46]. While these works are useful in constrained settings, they impose sev-eral constraints to simplify the problem, such as assuming pre-defined environments [3, 25, 27] or performing only en-vironment classification rather than explicit collision fore-casting [40, 28, 41, 11].
In this work, we address the shortcomings of prior ap-proaches to collision forecasting by introducing a general-izable model that provides detailed perception outputs use-ful for downstream collision avoidance. We first formalize the problem of egocentric collision prediction and localiza-tion from single or multi-view videos captured by cameras mounted on a person. To tackle this problem, we design a multi-task architecture that scales to diverse and complex environments and provides rich collision information. We train our model using high-quality simulation and curate data from a large variety of synthetic environments.
We pose collision forecasting as a problem of classify-ing which body joints will collide in the near future and localizing regions in the input videos where these collisions are likely to occur. The input is a set of multi-view RGB egocentric videos (see Fig. 1) without camera poses, which makes our problem setup general but also more challenging.
To successfully predict when and what part of the human body will collide with the environment, we must accumu-late information about the human’s movement, intent, and their surroundings across the multi-view videos.
To address these challenges, we propose COPILOT, a
COllision PredIction and LOcalization Transformer that uses a novel 4D attention scheme across space, time, and viewpoint to leverage information from multi-view video inputs. By alternating between cross-viewpoint and cross-time attention, the proposed 4D attention fuses informa-tion on both scene geometry and human motion. Notably,
COPILOT classifies collisions and estimates collision re-gions simultaneously in a multi-task fashion, thereby im-proving performance on collision prediction and providing actionable information for collision avoidance as well.
To train and evaluate our model, we develop a data framework and curate a high-quality synthetic dataset containing ∼8.6M egocentric RGBD frames with automatically-annotated collision labels and heatmaps.
Data is generated in simulation [45, 47] using real-world scene scans [8, 60] and a human motion generation model pre-trained with motion capture data [43]. Our data frame-work and dataset feature large scene diversity, realistic hu-man motion, and accurate collision checking. Experiments show that COPILOT achieves over 84% collision predic-tion accuracy on unseen synthetic scenes after training on our large-scale dataset, and qualitatively generalizes to di-verse real-world environments. To mimic the exoskeleton application, we combine our model with a closed-loop con-troller in simulation to demonstrate that we can adjust hu-man motion for improved collision avoidance; this avoids 35% and 29% of collision cases on training and unseen syn-thetic scenes, respectively.
In summary, we make the following contributions: (1) we introduce the challenging task of collision prediction and localization from unposed egocentric videos. (2) We propose a multi-view video transformer-based model that employs a novel 4D space-time-viewpoint attention mech-anism to solve this task effectively. (3) We develop a synthetic data framework to generate a large-scale dataset of egocentric RGBD frames with automatically-annotated collision labels and heatmaps. Our model trained on this dataset is shown to generalize to diverse synthetic and real-world environments through extensive evaluations. The dataset and code will be made available upon acceptance. 2.