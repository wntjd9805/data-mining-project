Abstract
We present ExBluRF, a novel view synthesis method for extreme motion blurred images based on efficient radiance fields optimization. Our approach consists of two main components: 6-DOF camera trajectory-based motion blur formulation and voxel-based radiance fields. From ex-tremely blurred images, we optimize the sharp radiance fields by jointly estimating the camera trajectories that gen-erate the blurry images.
In training, multiple rays along the camera trajectory are accumulated to reconstruct sin-gle blurry color, which is equivalent to the physical mo-tion blur operation. We minimize the photo-consistency loss on blurred image space and obtain the sharp radi-ance fields with camera trajectories that explain the blur of all images. The joint optimization on the blurred im-age space demands painfully increasing computation and resources proportional to the blur size. Our method solves this problem by replacing the MLP-based framework to low-dimensional 6-DOF camera poses and voxel-based ra-diance fields. Compared with the existing works, our ap-proach restores much sharper 3D scenes from challenging motion blurred views with the order of 10× less training time and GPU memory consumption. 1.

Introduction
Neural Radiance Fields (NeRF) have made great progress on novel view synthesis in recent years. A num-ber of follow-up works pay attention to NeRF’s [31] photo-realistic view synthesis performance, and focus on improv-ing training [10, 51] and rendering [13, 40] speed for prac-tical applications. However, enhancing NeRF’s rendering quality from degraded multi-view images is yet to be ex-plored extensively.
Camera motion blur is a representative degradation of images taken under low-light conditions and camera shake.
Optimizing NeRF from the motion blurred images suf-(a) Blurry Views (b) Rendered Novel View
Figure 1: Given a set of extremely blurred multi-view im-ages (a), our method restores sharp radiance fields and ren-ders clearly deblurred novel views (b). fers from the severe shape-radiance ambiguity [61] and produces inaccurate 3D geometry reconstruction and low-quality view synthesis of the scene.
One naive solution for this problem is to apply deep learning based 2D image deblurring [5, 59] to the input images before optimizing NeRF. However, this straightfor-ward approach has two limitations: 1) Pre-training and fine-tuning strategy of the deep neural network is invalid, since
NeRF is per-scene optimized without pairs of sharp images. 2) Independently deblurred multi-view images may yield inconsistent geometry in 3D space, and cannot be recovered with NeRF’s optimization.
The first work that considers the blurry images in NeRF’s optimization is DeblurNeRF [28]. DeblurNeRF incorpo-rates 2D pixel-wise blur kernel estimation [2] in front of
NeRF’s ray marching operation. The end-to-end frame-work of DeblurNeRF restores sharp 3D scenes from mod-erate motion blurred images. However, in extreme motion blur cases, the 2D pixel-wise kernel approach is difficult to converge with plausible deblurring, because the blur ker-nels are spatially varying severely with 6-degree of freedom
(6-DOF) camera motions. Also, naive implementation us-ing multi-layer perceptron (MLP) networks becomes a bot-tleneck of training for the extreme motion blur, since the memory consumption and the computation increase propor-tionally to the blur kernel size as shown in Fig. 2
In this paper, we propose a novel and efficient NeRF framework to synthesize sharp novel views from extreme motion blurred images. Inspired by [24, 36], our framework formulates the latent blur operation of each image by a tra-jectory of camera motion that generates blur. Like the image capturing of conventional cameras, the colors of rays are accumulated while the origin and the direction of the rays change along the estimated trajectory. These accumulated colors are optimized to minimize the photo-consistency loss with the colors of the input blurry image. Each view’s tra-jectory is fully constrained by blur patterns of all the pixels on the image, and the latent sharp 3D scene is optimized to satisfy all multi-view blur observations.
In addition to the proposed motion blur formulation, we adopt voxel-based radiance fields for efficient GPU mem-ory consumption and training time. Inherently, the blurry view reconstruction in training time requires explosive com-putation proportional to the number of sampling along the camera trajectory. We take advantage of volumetric rep-resentation approaches [4, 11, 32, 47, 58] that are acceler-ated by replacing the MLP-based representation to voxel-based representation. Furthermore, our voxel-based radi-ance fields keep constant memory to the increasing number of sampling on camera trajectory as shown in Fig. 2.
We conduct extensive experiments to validate the pro-posed approach on both real and synthetic data with extreme motion blur. In particular, we propose ExBlur, which pro-vides multi-view blurred images and sequences of sharp im-ages simultaneously captured by a dual-camera system. The
ExBlur dataset enables accurate evaluation of novel view synthesis and optimized camera trajectories on real blurred scenes. In addition, we analyze the efficiency of the pro-posed method on memory cost and training speed compared to the previous method [28].
To summarize, our contributions are:
• We propose the blur model formulated by 6-DOF camera motion trajectory in NeRF’s volume rendering framework that restores the sharp latent 3D scene with-out neural networks.
• We adopt the voxel-based radiance fields to realize effi-cient deblurring optimization in terms of memory con-sumption and training time.
• We demonstrate the high-quality deblurring and novel view synthesis of the proposed approach by our
ExBlur dataset that presents blurry-sharp multi-view images with ground truth (GT) motion trajectory.
Figure 2: Training time and GPU memory consumption on
“Camellia” shown in Fig 1. Our method, ExBluRF, signif-icantly improves efficiency on both the training time and memory cost with better deblurring performance. N is the number of samples (kernel size) to reconstruct blurry color. 2.