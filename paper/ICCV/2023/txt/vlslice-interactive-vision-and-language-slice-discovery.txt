Abstract
Recent work in vision-and-language demonstrates that large-scale pretraining can learn generalizable models that are efficiently transferable to downstream tasks. While this may improve dataset-scale aggregate metrics, analyzing performance around hand-crafted subgroups targeting spe-cific bias dimensions reveals systemic undesirable behav-iors. However, this subgroup analysis is frequently stalled by annotation efforts, which require extensive time and re-sources to collect the necessary data. Prior art attempts to automatically discover subgroups to circumvent these constraints but typically leverages model behavior on exist-ing task-specific annotations and rapidly degrades on more complex inputs beyond “tabular” data, none of which study vision-and-language models. This paper presents VLSlice, an interactive system enabling user-guided discovery of co-herent representation-level subgroups with consistent visi-olinguistic behavior, denoted as vision-and-language slices, from unlabeled image sets. We show that VLSlice enables users to quickly generate diverse high-coherency slices in a user study (n=22) and release the tool publicly1. 1.

Introduction
Large-scale vision-and-language models trained on cu-rated [25, 9, 36] and web-scrapped [29, 18, 8] data have led to significant improvements over task-specific models when transferred to downstream tasks in terms of aggre-gate metrics. However, researchers probing these models on hand-curated datasets have revealed problematic behaviors and well-known biases [30, 33] learned during pretraining – e.g. biases with respect to perceived gender, skin tone,2 and occupation. These biases can lead to disparate represen-tational performance for population subgroups, resulting in 1https://github.com/slymane/vlslice 2We use the term ‘skin tone’ rather than ‘race’ as race is a socially constructed identity that can span a range of phenotypic features. poor prediction quality for downstream applications such as image captioning [15, 37] and search [28].
In the standard paradigm for bias analysis in vision-and-language models, researchers query and analyze a set of im-ages that potentially exhibit bias. They often select a sub-ject population of interest, some specific subgroups of those subjects to analyze, and a bias dimension to measure against the model [30, 33]. For example, Ross et al. [30] choose im-ages of people as their subject population, label these based on perceived gender and skin tone categories, and measure model-predicted affinities between the labeled image sub-sets and text describing occupations or (un)pleasantness.
To effectively support analysis of such image sets for re-searchers, the set of images returned for the subject popula-tion subsets should be large, coherent, and representative – i.e. containing enough images to make statistically signif-icant statements, capturing a well-defined visual concept, and covering the full diversity of visual presentation for the selected concept rather than an arbitrary subset. Without these, the biases may simply be noise (large), be obscured by effects from images outside the intended subject group being included (coherent), or be the result of some intersec-tional bias captured in the subset that is not consistent across the whole expression of the visual concept (representative).
Collecting and labeling appropriate image sets that ful-fill these properties can be an arduous task. Despite this, manual annotation of static datasets along predefined sub-group and bias dimensions is the standard practice [19, 17].
This data collection methodology is expensive to perform – effectively limiting broad bias-auditing to high-resource institutions. Further, the one-off nature of this labeling pro-cess limits the scope of testing to pre-identified biases and does not account for how concepts may shift in visual ex-pression or cultural convention over time.
Several methods have been proposed to automatically discover biased “slices” of data which share similar input attributes and exhibit consistent responses from machine learning models [10, 31, 14, 11, 32, 21]. These Slice Dis-covery Methods (SDMs) have typically been deployed in
tabular input settings where individual input dimensions are semantically meaningful. While some recent work has ex-plored extending SDMs to more complex inputs like images
[14, 11, 32, 21], these require task-specific annotations to evaluate the model – making them unsuitable to auditing general vision-and-language alignment models.
To improve this workflow, we propose VLSlice, an inter-active system to discover vision-and-language slices from unlabeled collections of images. VLSlice consists of four primary stages of user-driven interaction with a vision-and-language alignment model of interest as depicted in the sys-tem overview in Fig. 1. A First, users write a query defin-ing a subject population of interest (e.g., “person”, “car”) and bias dimension to measure (e.g., “intelligent”, “fast”) which is submitted to VLSlice to select from a large set of unlabeled images down to a subset of subject-relevant images, then cluster those images by visual similarity and alignment with the bias dimension. B Second, users are displayed the clusters generated by VLSlice and can search, filter, and sort those clusters in (un)directed searches to identify and capture candidate slices (e.g., “people wear-ing suits”, “red cars”). C Next, users interact with VLSlice in a loop viewing recommended similar and counterfactual clusters to their slice to gather more coherent and represen-tative samples. D Finally, users can view a plot that shows the relationship between the slice they formed and the bias term across the entire subject population of interest, validat-ing if biased model behavior is demonstrated in the slice.
We demonstrate that VLSlice enables users to quickly generate diverse high-coherency slices in a between-subjects user study (n=22), contrasting with a control in-terface mimicking a linear unguided image search. From this study, we present both qualitative and quantitative sup-port, and discuss emergent user interaction paradigms. We choose to study CLIP [29] as a representative model of contemporary methods in large-scale pretraining and self-supervision for image-text alignment. 2.