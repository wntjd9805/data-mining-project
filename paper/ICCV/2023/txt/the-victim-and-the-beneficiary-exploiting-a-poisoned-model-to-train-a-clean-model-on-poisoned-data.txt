Abstract
Recently, backdoor attacks have posed a serious secu-rity threat to the training process of deep neural networks (DNNs). The attacked model behaves normally on benign samples but outputs a specific result when the trigger is present. However, compared with the rocketing progress of backdoor attacks, existing defenses are difficult to deal with these threats effectively or require benign samples to work, which may be unavailable in real scenarios. In this paper, we find that the poisoned samples and benign samples can be distinguished with prediction entropy. This inspires us to propose a novel dual-network training framework: The Vic-tim and The Beneficiary (V&B), which exploits a poisoned model to train a clean model without extra benign samples.
Firstly, we sacrifice the Victim network to be a powerful poisoned sample detector by training on suspicious sam-ples. Secondly, we train the Beneficiary network on the credible samples selected by the Victim to inhibit backdoor injection. Thirdly, a semi-supervised suppression strategy is adopted for erasing potential backdoors and improving model performance. Furthermore, to better inhibit missed poisoned samples, we propose a strong data augmentation method, AttentionMix, which works well with our proposed
V&B framework. Extensive experiments on two widely used datasets against 6 state-of-the-art attacks demonstrate that our framework is effective in preventing backdoor injec-tion and robust to various attacks while maintaining the performance on benign samples. Our code is available at https://github.com/Zixuan-Zhu/VaB. 1.

Introduction
Large amounts of training data and powerful computing resources are two key factors for the success of deep neural networks (DNNs). While in practical applications, obtain-ing enough samples for training is laborious, and training is likely to be resource-constrained. As a result, more and
*Corresponding author
Figure 1. The average prediction entropy of benign samples ver-sus poisoned samples crafted by 6 backdoor attacks. We conduct the experiment on CIFAR-10 with ResNet-18 under poisoning rate 10%, where our special training strategy (described in section 3.3) was adopted. more third-party data (e.g. data released on the Internet) and training platforms (e.g. Google Colab) are adopted to re-duce the overhead, which brings new security risks.
Backdoor attacks [5, 11, 24] pose a serious security threat to the training process of DNNs and can be easily executed through data poisoning. Specifically, attackers
[7, 10, 22] inject the designed trigger (e.g. a small patch or random noise) to a few benign samples selected from the training set and change their labels to the attacker-defined target label. These poisoned samples will force models to learn the correlation between the trigger and the target la-bel. In the reference process, the attacked model will be-have normally on benign samples but output the target la-bel when the trigger is present. Deploying attacked models can lead to severe consequences, even life-threatening in some scenarios (e.g. autonomous driving). Hence, a secure training framework is needed when using third-party data or training platforms.
Many researchers have devoted themselves to this topic and proposed feasible defenses. Li et al. [18] utilize local benign samples to erase existing backdoors in DNNs, and
Borgnia et al. [3] employ strong data augmentations to in-hibit backdoor injection during training. Li et al. [17] find that the training loss decreased faster for poisoned samples than benign samples and designed a loss function to sepa-rate them. Finally, they select a fixed portion of samples with the lowest loss to erase potential backdoors. In real scenarios, it is challenging to distinguish poisoned samples from benign ones in this way due to the unknown poison-ing rate and close loss values. In experiments, we find the entropy of model prediction is a more discriminative prop-erty, and a fixed threshold could filter out most poisoned samples, as shown in Figure 1. During training, poisoned samples will be learned faster than benign samples due to the similar feature of their triggers [17]. Hence, the poi-soned network can confidently predict poisoned samples as the target label in early epochs but hesitates about the be-nign samples.
Based on this observation, we propose a novel secure training framework, The Victim and The Beneficiary (V&B), that can directly train clean models on poisoned data with-out resorting to benign samples. Firstly, in warming up stage, the training dataset is recomposed into a suspicious set and a credible set according to the prediction entropy is lower or higher than the predefined threshold. Then the
Victim network is trained iteratively with the suspicious set to obtain a strong poisoned sample detector. Secondly, in the clean training stage, the Beneficiary network is trained with credible samples filtered by the Victim to inhibit back-door injection. In later training epochs, the Victim network is fully exploited to provide its learned knowledge for the
Beneficiary network to help the latter erase potential back-doors through semi-supervised learning. To diminish the threat of missed poisoned samples, we design a strong data augmentation AttentionMix, which mixes the image region with high activation values according to the attention map.
Compared with existing augmentations [27, 30, 31], it has a stronger inhibition effect against stealthy backdoor attacks
[23, 22, 26].
The main contributions of this paper are as follows: (1)
We propose a novel dual-network secure training frame-work, V&B, which can train clean models on the poisoned dataset without resorting to benign samples. (2) We design a powerful data augmentation method called AttentionMix, which has a stronger inhibition effect against stealthy back-(3) Extensive experiments on two popular door attacks. benchmark datasets demonstrate the effectiveness and ro-bustness of our framework. 2.