Abstract
In this paper, we propose to regularize ill-posed inverse problems using a deep hierarchical variational autoencoder (HVAE) as an image prior. The proposed method synthe-sizes the advantages of i) denoiser-based Plug & Play ap-proaches and ii) generative model based approaches to in-verse problems. First, we exploit VAE properties to de-sign an efficient algorithm that benefits from convergence guarantees of Plug-and-Play (PnP) methods. Second, our approach is not restricted to specialized datasets and the proposed PnP-HVAE model is able to solve image restora-tion problems on natural images of any size. Our experi-ments show that the proposed PnP-HVAE method is com-petitive with both SOTA denoiser-based PnP approaches, and other SOTA restoration methods based on genera-tive models. The code for this project is available at https://github.com/jprost76/PnP-HVAE. 1.

Introduction
In this work, we study linear inverse problems y = Ax + ϵ (1)
∈
∼ N
Rm is the degraded observation, x
Rd in which y
∈
Rm×d is an the original signal we wish to retrieve, A
∈ (0, σ2I) is an additive Gaus-observation matrix and ϵ sian noise. Many image restoration tasks can be formulated as (1), including deblurring, super-resolution or inpainting.
With the development of deep learning in computer vi-sion, image restoration have known significant progress.
The most straight-forward way to exploit deep learning for solving image inverse problems is to train a neural network to map degraded images to their clean version in a super-vised fashion. However, this type of approach requires a large amount of training data, and it lacks flexibility, as one network is needed for each different inverse problem.
An alternate approach is to use deep latent variable gen-erative models such as GANs or VAEs and to compute the
Maximum-a-Posterior (MAP) estimator in the latent space: (2)
ˆz = arg max log p (y
G(z)) + log p (z) , z
| where z is the latent variable and G is the generative net-work [4, 34]. In (2) the likelihood p (y
G(z)) is related to
| the forward model (1), and p (z) corresponds to the prior distribution over the latent space. After solving (2), the so-lution of the inverse problem is defined as ˆx = G ( ˆz). The latent optimization methods (2) provide high-quality solu-tions that are guaranteed to be in the range of a generative network. However, this implies highly non-convex prob-lems (2) due to the complexity of the generator and the ob-tained solutions may lack of consistency with the degraded observation [47]. Although the convergence of latent opti-mization algorithms has been studied in the literature, exist-ing convergence guarantees are either restricted to specific settings, or rely on assumptions that are hard to verify.
In this work, we propose an algorithm that exploits the strong prior of a deep generative model while providing re-alistic convergence guarantees. We consider a specific type of deep generative model, the hierarchical variational au-toencoder (HVAE). HVAE gives state-of-the-art results on image generation benchmarks [53, 6, 15, 31], and provides an encoder that will be key in the design of our proposed method.
As the HVAE model differs significantly from the archi-tecture of concurrent models, it is necessary to design algo-rithms adapted to their specific structure. The latent space dimension of HVAE is significantly higher than the image dimension. Hence, constraining the solution to lie in the image of the generator is not sufficient enough to regularize
inverse problems. Indeed, it has been observed that HVAEs can perfectly reconstruct out-of-domain images [14]. Con-sequently, we propose to constrain the latent variable of the solution to lie in the high probability area of the HVAE prior distribution. This can be done efficiently by controlling the variance of the prior over the latent variables.
The common practice of optimizing the latent variables of the generative model with backpropagation is impracti-cal due to the high dimensionality of the hierarchical latent space. Instead, we exploit the HVAE encoder to define an alternating algorithm [11] to optimize the joint distribution over the image and its latent variable
To derive convergence guarantees for our algorithm, we show that it can be reformulated as a Plug-and-Play (PnP) method [54], which alternates between an application of the proximal operator of the data-fidelity term, and a recon-struction by the HVAE. Under this perspective, we give suf-ficient conditions to ensure the convergence of our method, and we provide an explicit characterization of the fixed-point of the iterations. Motivated by the parallel with PnP methods, we name our method PnP-HVAE. 1.1. Contributions and outline
In this work, we introduce PnP-HVAE, a method for regularizing image restoration problems with a hierarchical variational autoencoder. Our approach exploits the expres-siveness of a deep HVAE generative model and its capacity to provide a strong prior on specialized datasets, as well as convergence guarantees of Plug-and-Play methods and their ability to deal with natural images of any size. After a re-view of related works (section 2) and of the background on
HVAEs (section 3), our contributions are the following.
In section 4, we introduce PnP-HVAE, an algorithm to
• solve inverse problems with a HVAE prior. PnP-HVAE op-timizes a joint posterior on image and latent variables with-out backpropagation through the generative network. It can be viewed as a generalization of JPMAP [11] to hierarchical
VAEs, with additional control of the regularization.
In section 5, we demonstrate the convergence of PnP-•
HVAE under hypotheses on the autoencoder reconstruction.
Numerical experiments illustrate that the technical hypothe-ses are empirically met on noisy images with our proposed architecture. We also exhibit the better convergence prop-erties of our alternate algorithm with respect to the use of
Adam for optimizing the joint posterior objective.
In section 6, we demonstrate the effectiveness of PnP-•
HVAE through image restoration experiments and compar-isons on (i) faces images using the pre-trained VDVAE model from [6]; and (ii) natural images using the proposed
PatchVDVAE architecture trained on natural image patches. 2.