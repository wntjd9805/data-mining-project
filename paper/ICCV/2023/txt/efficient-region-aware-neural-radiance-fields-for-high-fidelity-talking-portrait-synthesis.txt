Abstract
This paper presents ER-NeRF, a novel conditional Neu-ral Radiance Fields (NeRF) based architecture for talking portrait synthesis that can concurrently achieve fast con-vergence, real-time rendering, and state-of-the-art perfor-mance with small model size. Our idea is to explicitly ex-ploit the unequal contribution of spatial regions to guide talking portrait modeling. Specifically, to improve the ac-curacy of dynamic head reconstruction, a compact and ex-pressive NeRF-based Tri-Plane Hash Representation is in-troduced by pruning empty spatial regions with three pla-nar hash encoders. For speech audio, we propose a Re-gion Attention Module to generate region-aware condition feature via an attention mechanism. Different from exist-ing methods that utilize an MLP-based encoder to learn the cross-modal relation implicitly, the attention mecha-nism builds an explicit connection between audio features and spatial regions to capture the priors of local motions.
Moreover, a direct and fast Adaptive Pose Encoding is in-troduced to optimize the head-torso separation problem by mapping the complex transformation of the head pose into spatial coordinates. Extensive experiments demonstrate that our method renders better high-fidelity and audio-lips synchronized talking portrait videos, with realistic details and high efficiency compared to previous methods. Code is available at https://github.com/Fictionarry/
ER-NeRF. 1.

Introduction
Audio-driven talking portrait synthesis is an important and challenging issue with several applications such as dig-ital humans, virtual avatars, film-making, and video con-ferencing. Over the past few years, many researchers have tackled the task with deep generative models [10, 28, 36, 52,
*Corresponding author: Xiao Bai (baixiao@buaa.edu.cn).
Figure 1. Instead of learning the implicit audiovisual relation by an MLP-based encoder, we explicitly attend to the cross-modal in-teraction between speech audio and spatial regions. Region aware-ness enables ER-NeRF to render more accurate facial motions. 51, 25, 45]. Recently, Neural Radiance Fields (NeRF) [26] is introduced into audio-driven talking portrait synthesis. It provides a new way to learn a direct mapping from the audio feature to the corresponding visual appearance by a deep multi-layer perceptron (MLP). Since then, several studies condition NeRF on audio signals in an end-to-end way
[19, 24, 29, 42] or by some intermediate representations
[43, 7] to reconstruct a specific talking portrait. Though these vanilla NeRF-based methods have shown great suc-cess in the synthesis quality, the inference speed is far from meeting real-time requirements, which seriously limits their practical applications.
Several recent works on efficient neural representation have demonstrated tremendous speedups over vanilla NeRF by replacing part of the MLP network with sparse feature grids [33, 27, 6, 8, 15, 5, 16]. Instant-NGP [27] introduces a hash-encoded voxel grid for static scene modeling, al-lowing fast speed and high-quality rendering with a com-pact model. RAD-NeRF [35] first applies this technique to talking portrait synthesis and builds a real-time framework with state-of-the-art performance. However, this approach requires a complex MLP-based grid encoder to learn the regional audio-motion mapping implicitly, which limits its convergence and reconstruction quality.
This paper aims to explore a more effective solution for efficient and high-fidelity talking portrait synthesis. Based on previous studies, we notice that different spatial regions contribute unequally to representing talking portraits: (1)
In volumetric rendering, since only the surface regions con-tribute to representing the dynamic head, most other spatial regions are empty and can be pruned with some efficient
NeRF techniques to reduce the training difficulty; (2) As the fact that different facial areas have varying associations with speech audio [24], different spatial regions are inherently related to the audio signal in their own distinct manners and lead to unique audio-driven local motions. Inspired by these observations, we explicitly exploit the unequal contri-bution of spatial regions to guide the talking portrait mod-eling, and present a novel Efficient Region-aware talking portrait NeRF (ER-NeRF) framework for realistic and effi-cient talking portrait synthesis, which achieves high-quality rendering, fast convergence, and real-time inference with small model size.
Our first improvement focuses on the dynamic head representation. Although RAD-NeRF [35] has leveraged
Instant-NGP to represent the talking portrait and achieves a fast inference, its rendering quality and convergence are hampered by hash collisions when modeling the 3D dy-namic talking head. To address this problem, we introduce a
Tri-Plane Hash Representation that factorizes the 3D space into three orthogonal planes via a NeRF-based tri-plane de-composition [6]. During the factorization, all spatial regions are squeezed onto 2D planes, with the corresponding fea-ture grids pruned. Hence, hash collisions only occur in low-dimensional subspaces and are reduced in number. With fewer noises, the network can pay more attention to process-ing audio features,leading to the capability of reconstructing more accurate head structures and finer dynamic motions.
To capture the regional impact of audio signals, we fur-ther explore the relevance between the audio feature and position encoding of the proposed Tri-Plane Hash Repre-Instead of concatenating the raw features and sentation. learning the audiovisual correlation by a large MLP-based encoder, we propose a Region Attention Module that ad-justs the audio feature to best fit certain spatial regions via a cross-modal attention mechanism. Hence, the dynamic parts of the portrait can acquire more appropriate features to model accurate facial movements, while other static por-tions remain unaffected by the changing signals. By gaining regional awareness, high-quality and efficient modeling for local motions can be achieved.
Moreover, a simple but effective Adaptive Pose Encod-ing is proposed in our framework to solve the head-torso separation problem. It maps the complex pose transforma-tion onto spatial coordinates and provides a clearer position relation for torso-NeRF to learn its own pose implicitly.
The main contributions of our work are summarized as follows:
• We introduce an efficient Tri-Plane Hash Representa-tion to facilitate dynamic head reconstruction, which also achieves high-quality rendering, real-time inference and fast convergence with a compact model size.
• We propose a novel Region Attention Module to capture the correlation between the audio condition and spatial regions for accurate facial motion modeling.
• Extensive experiments show that the proposed ER-NeRF renders realistic talking portraits with high efficiency and visual quality, which outperforms state-of-the-art meth-ods on both objective evaluation and human studies. 2.