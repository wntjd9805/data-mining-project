Abstract 3D understanding and rendering of moving humans from monocular videos is a challenging task. Despite recent progress, the task remains difficult in real-world scenar-ios, where obstacles may block the camera view and cause partial occlusions in the captured videos. Existing meth-ods cannot handle such defects due to two reasons. First, the standard rendering strategy relies on point-point map-ping, which could lead to dramatic disparities between the visible and occluded areas of the body. Second, the naive direct regression approach does not consider any feasi-bility criteria (i.e., prior information) for rendering under occlusions. To tackle the above drawbacks, we present
OccNeRF, a neural rendering method that achieves bet-ter rendering of humans in severely occluded scenes. As direct solutions to the two drawbacks, we propose surface-based rendering by integrating geometry and visibility pri-ors. We validate our method on both simulated and real-world occlusions and demonstrate our method’s superi-ority. Project page: https://cs.stanford.edu/
˜xtiange/projects/occnerf/ 1.

Introduction
Rendering 3D human bodies from a sequence of obser-vations is of great interest in various communities, includ-ing robotics [70], motion analysis [16], and healthcare [19].
This task is challenging, since one must recover the com-plete human body with complex textures and poses from sparse partial observations. It is usually cumbersome to ac-quire images of the same human object from multiple cam-era angles simultaneously; hence, capturing a monocular video from a single camera is more common and feasible.
The task of rendering humans from a monocular video is not new. Progress so far mainly focuses on rendering quality [51, 66] and rendering efficiency [49, 28]. How-ever, most existing neural rendering methods assume that the human object is placed in a scene with a clear view of the entire body without any external interference. In con-trast, real-world environments often contain undesired ob-stacles that contaminate training data and impact the ren-*Correspondence to xtiange@stanford.edu
Figure 1. Object obstacles in the scene may cause severe occlu-sions in the rendered/captured videos, imposing additional chal-lenges into the rendering process. Top row: Ideal scene with no defects and clear view of the body; Bottom row: Real-world scene with undesired obstacles and occluded body parts. dering quality (See Figure 1). These real-world occlusions pose significant challenges for training when using only monocular videos, where no other camera angles can be used to provide complementary information. As a result, a direct application of previous neural rendering methods on object-occluded videos leads to subpar performance. Opti-mizing a neural radiance field is difficult under occlusions.
There is often no ground truth associated with the occluded area. Additionally, radiance fields are typically optimized in a scene-specific manner; that is, no external information can and should be used to fill in the occluded areas.
Two major drawbacks of previous methods impair their capabilities to train on object-occluded videos. First, the prior work does not account for local geometry cues in their rendering process. Following the point-based render-ing paradigm as in NeRF [43], most previous methods ren-der color and density values of a ray sample by only look-ing at a single 3D coordinate. However, we explain in sec-tion 3.2 that this basic strategy may lead to dramatically different rendering results even in very close positions. Sec-ond, methods suffer from not properly incorporating priors.
In the monocular video setting, geometry (e.g., SMPL [40]) and visibility priors can describe a complete human geome-try and indicate which body parts are visible to the camera.
In this work, we propose novel methods for dealing with the above drawbacks, allowing us to accurately render oc-cluded humans from monocular video. We first present a surface-based rendering strategy that determines the radi-ance of each 3D ray sample by conditioning it on a wide re-gion of the human body’s surface. A geometry prior is used to discretely parameterize the surface segments. We then collect visibility frequencies on the human body through training frames and formulate them as attention maps for better aggregation of the surface regions. Finally, we design a loss function to encourage the network to output high-density values for positions within the human body.
In summary, our contributions are three-fold: (i) We are the first to study dynamic human rendering under real-world settings with severe occlusions. (ii) We propose novel meth-ods that include surface-based rendering, a reformulation of body part visibility frequency as attention, and a complete-ness loss to enable human rendering from object-occluded monocular videos. (iii) We empirically demonstrate that our methods achieve significant quantitative and qualitative im-provements compared to the previous state-of-the-art, yield-ing the first baseline in this topic. 2.