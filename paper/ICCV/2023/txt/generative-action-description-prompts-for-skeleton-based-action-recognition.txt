Abstract
Skeleton-based action recognition has recently received considerable attention. Current approaches to skeleton-based action recognition are typically formulated as one-hot classification tasks and do not fully exploit the seman-tic relations between actions. For example, “make victory sign” and “thumb up” are two actions of hand gestures, whose major difference lies in the movement of hands. This information is agnostic from the categorical one-hot encod-ing of action classes but could be unveiled from the ac-tion description. Therefore, utilizing action description in training could potentially benefit representation learning.
In this work, we propose a Generative Action-description
Prompts (GAP) approach for skeleton-based action recog-nition. More specifically, we employ a pre-trained large-scale language model as the knowledge engine to automati-cally generate text descriptions for body parts movements of actions, and propose a multi-modal training scheme by uti-lizing the text encoder to generate feature vectors for differ-ent body parts and supervise the skeleton encoder for action representation learning. Experiments show that our pro-posed GAP method achieves noticeable improvements over various baseline models without extra computation cost at inference. GAP achieves new state-of-the-arts on popu-lar skeleton-based action recognition benchmarks, includ-ing NTU RGB+D, NTU RGB+D 120 and NW-UCLA. The source code is available at https://github.com/
MartinXM/GAP. 1.

Introduction
Action recognition has been an active research topic due to its wide range of applications in human-computer inter-action, sports and health analysis, entertainment, etc. In re-cent years, with the emergence of depth sensors, such as
Kinect [44] and RealSense [14], human body joints can be easily acquired. The action recognition approach utilizing
*Corresponding author body joints, i.e., the so-called skeleton-based action recog-nition, has drawn a lot of attentions due to its computation efficiency and robustness to lighting conditions, viewpoint variations and background noise.
Most of the previous methods in skeleton-based action recognition focus on modeling the relation of human joints, following a unimodal training scheme with a sequence of skeleton coordinates as inputs [41, 15, 27, 9, 28, 4, 25, 40,
Inspired by the recent success of multi-30, 36, 35, 22]. modal training with image and language [23, 1], we in-vestigate an interesting question: whether action language description could unveil the action relations and benefit skeleton-based action recognition? Regrettably, due to the absence of a large-scale dataset consisting of skeleton-text pairs, constructing such a dataset would require significant time and financial resources. Consequently, the training scheme outlined in [23, 11, 39] cannot be directly applied to skeleton-based action recognition. As a result, the develop-ment of novel multi-modal training paradigms is necessary to address this issue.
We propose to leverage the generative category-level hu-man action description in the form of language prompts.
The language definition of an action contains rich prior knowledge. For example, different actions focus on the movement of different body parts: “make victory sign” and
“thumb up” describe the gesture of hands; “arm circles” and
“tennis bat swing” describe the movement of arms; “nod head” and “shake head” are the motions of head; “jump up” and “side kick” rely on movements of foot and leg. Some actions describe the interaction of multiple body parts, e.g.,
“put on a hat” and “put on a shoe” involve actions of hand and head, hand and foot, respectively. These prior knowl-edge about actions could provide fine-grained guidance for representation learning. In addition, to resolve the labori-ous work to collect human action prompts, we resort to pre-trained large language model (LLM), e.g. GPT-3 [1] for efficient automatic prompts generation.
In specific, we develop a new training paradigm, which employs generative action prompts for skeleton-based ac-tion recognition. We take advantages of the GPT-3 [1] as 1
Figure 1: Comparison of our proposed Generative Action-description Prompts (GAP) framework (dual encoder) with other skeleton recognition methods (single encoder). Besides classification loss, our proposed method contains additional con-trastive loss. Notice that text encoder is only used at the training stage and GPT-3 is applied for offline action description generation. For every given action query, GPT-3 generates text description of actions with prompt templates, the action de-scription is then employed for multi-modal training. our knowledge engine to generate meaningful text descrip-tions for actions. With elaborately designed text prompts, detailed text descriptions for the whole action and each body part can be produced. In Figure 1, we compare our proposed frameworks (b) and (c) with traditional single en-coder skeleton-based action recognition framework (a). In our framework, a multi-modal training scheme is devel-oped, which contains a skeleton encoder and a text encoder.
The skeleton encoder takes skeleton coordinates as inputs and generates both part feature vectors and global feature representations. The text encoder transforms global action description or body part descriptions into text features for the whole action or each body part. A multi-part contrastive loss (single contrastive loss for (b)) is used to align the text part features and skeleton part features, and the cross-entropy loss is applied on the global features.
Our contributions are summarized as follow:
- As far as we known, this is the first work to use gen-erative prompts for skeleton-based action recognition, which applies a LLM as the knowledge engine and elaborately employs text prompts to generate detailed text descriptions of the whole action and body parts movements for different actions automatically.
- We propose a new multi-modal training paradigm that utilizes generative action prompts to guide skeleton-based action recognition, which enhances the repre-sentation by using knowledge about actions and human body parts. It could improve the model performance without bringing any computation cost at inference.
- With the proposed training paradigm, we achieve state-of-the-art performance on several popular skeleton-based action recognition benchmarks, including NTU
RGB+D, NTU RGB+D 120 and NW-UCLA. 2.