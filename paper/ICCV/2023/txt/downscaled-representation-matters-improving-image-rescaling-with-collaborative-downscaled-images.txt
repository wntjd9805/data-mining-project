Abstract (cid:43)(cid:53)(cid:3)(cid:44)(cid:80)(cid:68)(cid:74)(cid:72) (cid:53)(cid:72)(cid:70)(cid:82)(cid:81)(cid:86)(cid:87)(cid:85)(cid:88)(cid:70)(cid:87)(cid:72)(cid:71)(cid:3)(cid:44)(cid:80)(cid:68)(cid:74)(cid:72) (cid:47)(cid:53)(cid:3)(cid:44)(cid:80)(cid:68)(cid:74)(cid:72)
Deep networks have achieved great success in image rescaling (IR) task that seeks to learn the optimal down-scaled representations, i.e., low-resolution (LR) images, to reconstruct the original high-resolution (HR) images. Com-pared with super-resolution methods that consider a ﬁxed downscaling scheme, e.g., bicubic, IR often achieves sig-niﬁcantly better reconstruction performance thanks to the learned downscaled representations. This highlights the importance of a good downscaled representation. Exist-ing IR methods mainly learn the downscaled representation by jointly optimizing the downscaling and upscaling mod-els. Unlike them, we seek to improve the downscaled rep-resentation through a different and more direct way – di-rectly optimizing the downscaled image itself instead of the down-/upscaling models. Consequently, we propose a Hi-erarchical Collaborative Downscaling (HCD) method that performs gradient descent w.r.t. the reconstruction loss in both HR and LR domains to improve the downscaled rep-resentations, so as to boost IR performance. Extensive ex-periments show that our HCD signiﬁcantly improves the re-construction performance both quantitatively and qualita-tively. Particularly, we improve over popular IR methods by
>0.57 dB PSNR on Set5. Moreover, we also highlight the
ﬂexibility of our HCD since it can generalize well across diverse image rescaling models. The code is available at https://github.com/xubingna/HCD. 1.

Introduction
Image rescaling seeks to downscale the high-resolution (HR) images to visually valid low-resolution (LR) images and then upscale them to recover the original HR images.
In practice, the downscaled images play an important role in saving storage or bandwidth and ﬁtting the screens with
*Authors contributed equally.
†Corresponding author. (cid:37)(cid:76)(cid:70)(cid:88)(cid:69)(cid:76)(cid:70) (cid:43)(cid:38)(cid:41)(cid:79)(cid:82)(cid:90) (cid:43)(cid:38)(cid:39)(cid:11)(cid:50)(cid:88)(cid:85)(cid:86)(cid:12) (cid:44)(cid:80)(cid:68)(cid:74)(cid:72)(cid:3)(cid:53)(cid:72)(cid:70)(cid:82)(cid:81)(cid:86)(cid:87)(cid:85)(cid:88)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81) (cid:51)(cid:54)(cid:49)(cid:53)(cid:29)(cid:21)(cid:24)(cid:17)(cid:28)(cid:23)(cid:71)(cid:37) (cid:51)(cid:54)(cid:49)(cid:53)(cid:29)(cid:22)(cid:19)(cid:17)(cid:21)(cid:20)(cid:71)(cid:37) (cid:51)(cid:54)(cid:49)(cid:53)(cid:29)(cid:22)(cid:20)(cid:17)(cid:21)(cid:25)(cid:71)(cid:37) (cid:72) (cid:74) (cid:68) (cid:80) (cid:44) (cid:3) (cid:53) (cid:47) (cid:72) (cid:74) (cid:68) (cid:80) (cid:44) (cid:3) (cid:53) (cid:43) (cid:54)(cid:90)(cid:76)(cid:81)(cid:44)(cid:53) (cid:43)(cid:38)(cid:41)(cid:79)(cid:82)(cid:90) (cid:43)(cid:38)(cid:39)(cid:3)(cid:11)(cid:50)(cid:88)(cid:85)(cid:86)(cid:12)
Figure 1. Image rescaling pipeline and the comparisons of the downscaled images along with the corresponding reconstructed
HR images (4×). Top: we show the entire process of image rescal-ing. Middle: we visualize the downscaled representations used in different methods. Bottom: we compare the reconstructed HR im-ages. With the improved downscaled representation, our method yields the best result both quantitatively and qualitatively. different resolutions [43], such as image/video restoration and communication [48, 40, 35]. A typical application scenario of IR is to obtain HR images/videos (previously stored in the server) on an edge device, e.g., mobile. To save storage and reduce transmission latency, the original
HR images/videos are usually downscaled to LR and then stored on the server. In some scenarios, these LR images can be directly used by edge devices, such as when the de-vice screen has a low resolution or only as a preview, at the same time, they can also be upscaled to the original reso-(cid:86) (cid:72) (cid:75) (cid:70) (cid:68) (cid:82) (cid:85) (cid:83) (cid:83) (cid:36) (cid:74) (cid:81) (cid:76) (cid:87) (cid:86) (cid:76) (cid:91) (cid:40) (cid:86) (cid:85) (cid:88) (cid:50) (cid:50)(cid:85)(cid:76)(cid:74)(cid:76)(cid:81)(cid:68)(cid:79) (cid:43)(cid:53) (cid:44)(cid:80)(cid:68)(cid:74)(cid:72) (cid:50)(cid:85)(cid:76)(cid:74)(cid:76)(cid:81)(cid:68)(cid:79) (cid:43)(cid:53) (cid:44)(cid:80)(cid:68)(cid:74)(cid:72) (cid:44)(cid:80)(cid:68)(cid:74)(cid:72) (cid:39)(cid:82)(cid:90)(cid:81)(cid:86)(cid:70)(cid:68)(cid:79)(cid:76)(cid:81)(cid:74) (cid:44)(cid:80)(cid:68)(cid:74)(cid:72) (cid:56)(cid:83)(cid:86)(cid:70)(cid:68)(cid:79)(cid:76)(cid:81)(cid:74) (cid:74) (cid:81) (cid:76) (cid:79) (cid:68) (cid:70) (cid:86) (cid:81) (cid:90) (cid:82) (cid:39) (cid:79) (cid:72) (cid:71) (cid:82) (cid:48) (cid:47)(cid:53) (cid:44)(cid:80)(cid:68)(cid:74)(cid:72) (cid:74) (cid:81) (cid:76) (cid:79) (cid:68) (cid:70) (cid:86) (cid:81) (cid:90) (cid:82) (cid:39) (cid:79) (cid:72) (cid:71) (cid:82) (cid:48) (cid:38)(cid:82)(cid:79)(cid:79)(cid:68)(cid:69)(cid:82)(cid:85)(cid:68)(cid:87)(cid:76)(cid:89)(cid:72) (cid:40)(cid:91)(cid:68)(cid:80)(cid:83)(cid:79)(cid:72) (cid:42)(cid:72)(cid:81)(cid:72)(cid:85)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81) (cid:47)(cid:53) (cid:44)(cid:80)(cid:68)(cid:74)(cid:72) (cid:38)(cid:82)(cid:79)(cid:79)(cid:68)(cid:69)(cid:82)(cid:85)(cid:68)(cid:87)(cid:76)(cid:89)(cid:72) (cid:40)(cid:91)(cid:68)(cid:80)(cid:83)(cid:79)(cid:72) (cid:74) (cid:81) (cid:76) (cid:79) (cid:68) (cid:70) (cid:86) (cid:83) (cid:56) (cid:79) (cid:72) (cid:71) (cid:82) (cid:48) (cid:74) (cid:81) (cid:76) (cid:79) (cid:68) (cid:70) (cid:86) (cid:83) (cid:56) (cid:79) (cid:72) (cid:71) (cid:82) (cid:48) (cid:53)(cid:72)(cid:70)(cid:82)(cid:81)(cid:86)(cid:87)(cid:85)(cid:88)(cid:70)(cid:87)(cid:72)(cid:71) (cid:44)(cid:80)(cid:68)(cid:74)(cid:72) (cid:53)(cid:72)(cid:70)(cid:82)(cid:81)(cid:86)(cid:87)(cid:85)(cid:88)(cid:70)(cid:87)(cid:72)(cid:71) (cid:44)(cid:80)(cid:68)(cid:74)(cid:72)
Figure 2. Comparison between existing image rescaling methods and the proposed HCD method. We additionally generate col-laborative LR examples to improve the downscaled representation while keeping the upscaling process unchanged. lution when needed. Interestingly, unlike super-resolution (SR) [38, 28, 3] methods that consider a ﬁxed downscal-ing kernel, e.g., bicubic, IR often yields signiﬁcantly bet-ter reconstruction performance [51, 4, 15] since IR essen-tially learns a better downscaled representation method. As shown in Figure 1, compared with a popular SR method
SwinIR [25], the rescaling method HCFlow [26] greatly im-proves the PSNR score from 25.94 dB to 30.21 dB, which highlights the importance of a good downscaled represen-tation in image reconstruction tasks.
To boost IR performance, existing methods jointly learn the downscaling and upscaling models by minimizing the reconstruction loss [43, 26]. However, in a complete pipeline, in addition to the trained neural network model, the downscaled representation itself is also very important.
When diverse data are fed into a frozen model, we often ob-tain signiﬁcantly different results. For example, in Figure 3, compared to the original LR images, the adversarial exam-ples cause a 0.93 dB drop in PSNR of the reconstructed
HR images, and visually, the lines became blurry. In con-trast, when facing collaborative LR images, i.e., opposite to adversarial examples, not only improve the performance by 1.29 dB but also produce clearer and smoother lines.
Inspired by this, in Figure 2, we propose a collaborative downscaling scheme that focuses on getting a better down-sampled representation (purple box) of images instead of learning the model (blue box), making our approach essen-tially different from existing IR approaches. Furthermore, since LR images are obtained from the original HR image via downscaling, we can also improve the downscaled rep-resentation if we obtain a better representation in the HR domain, i.e., generating collaborative HR examples.
Motivated by this, we propose a Hierarchical Collabo-rative Downscaling (HCD) scheme that optimizes the rep-resentations in both HR and LR domains to obtain a bet-ter downscaled example. Speciﬁcally, we ﬁrst generate a collaborative example in the HR domain and downscaled it to obtain an LR image. Taking the downscaled image as an initialization point, we then generate the collabora-tive LR example to further improve the downscaled rep-resentation. Due to the dependence between HR and LR image (based on the downscaling process), the hierarchical collaborative learning scheme can be formulated by a bi-level optimization problem. More critically, although our method increases the cost of generating downscaled im-ages, we highlight that the increased cost only exists in the downscaling stage on server (can be processed ofﬂine), with no effect on the real-time rescaling on edge devices and making our method applicable to real-world scenarios. As shown in Figure 6, our HCD consistently improves PSNR while maintaining the same upscaling latency across diverse methods. Experiments show that our method signiﬁcantly boosts the reconstruction performance with the help of col-laborative downscaled examples.
Our contributions are summarized as follows:
• We propose a novel collaborative image downscaling method that improves the image rescaling performance from a new perspective – learning a better downscaled representation. We highlight that, in the community of image reconstruction, it is the ﬁrst attempt to di-rectly optimize the downscaled representation instead of learning the downscaling or upscaling models to boost the performance of image rescaling.
• Since the low-resolution (LR) images strongly depend on the corresponding high-resolution (HR) images, we propose a Hierarchical Collaborative Downscaling (HCD) that optimizes the representations in both HR and LR domains to learn a better downscaled represen-tation. We formulate the learning process as a bi-level optimization problem and solve it by alternatively gen-erating collaborative HR and LR examples.
• Experiments on multiple benchmark datasets show that, on top of state-of-the-art image rescaling models, our HCD yields signiﬁcantly better results both quan-titatively and qualitatively. For example, based on a strong baseline HCFlow [26], we obtain a large PSNR improvement of 0.7 dB on Set5 for 4× rescaling. 2.