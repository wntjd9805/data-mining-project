Abstract
The combination of Neural Architecture Search (NAS) and quantization has proven successful in automatically designing low-FLOPs INT8 quantized neural networks (QNN). However, directly applying NAS to design accurate
QNN models that achieve low latency on real-world devices leads to inferior performance. In this work, we identify that the poor INT8 latency is due to the quantization-unfriendly issue: the operator and configuration (e.g., channel width) choices in prior art search spaces lead to diverse quan-tization efficiency and can slow down the INT8 inference speed. To address this challenge, we propose SpaceEvo, an automatic method for designing a dedicated, quantization-friendly search space for each target hardware. The key idea of SpaceEvo is to automatically search hardware-preferred operators and configurations to construct the search space, guided by a metric called Q-T score to quan-tify how quantization-friendly a candidate search space is. We further train a quantized-for-all supernet over our discovered search space, enabling the searched models to be directly deployed without extra retraining or quantiza-tion. Our discovered models, SEQnet, establish new SOTA
INT8 quantized accuracy under various latency constraints, achieving up to 10.1% accuracy improvement on ImageNet than prior art CNNs under the same latency. Extensive ex-periments on real devices show that SpaceEvo consistently outperforms manually-designed search spaces with up to 2.5× faster speed while achieving the same accuracy. 1.

Introduction
INT8 Quantization[27, 19, 10, 3] is a widely used tech-nique for deploying DNNs on edge devices by reducing 4× in model size and memory cost for full-precision (FP32)
*Equal contribution
§Work was done during the internship at Microsoft Research
‡Corresponding author (lzhani@microsoft.com)
Figure 1. INT8 quantized model error distributions (EDF) of dif-ferent NAS search spaces. We propose to search quantization-friendly search space for each hardware, which yields significantly better INT8 model populations than SOTA search spaces. models. However, prior art DNN models achieve only marginal speedup from INT8 quantization (in Fig. 2(a)), the still high latency after quantization making them difficult to deploy in latency-critical scenarios. Designing models that achieve high accuracy and low latency after quantization be-comes the important but challenging problem.
Neural Architecture Search (NAS) is a powerful tool for automating efficient quantized model design [36, 39, 12, 37, 6]. Recently, OQAT [32] and BatchQuant [1] achieve remarkable search efficiency and accuracy by adopting a two-stage paradigm. The first stage trains a weight-shared quantized supernet assembling all candidate architectures in the search space. This allows all the sub-networks (sub-nets) to simultaneously reach comparable quantized accu-racy as when trained from scratch individually. The sec-ond stage uses typical search algorithms to find subnets with best quantized accuracy under different FLOPs constraints.
This approach avoids the need to retrain each subnet for ac-curacy evaluation, greatly improving the search efficiency.
Though promising in optimizing model FLOPs, we iden-tify a significant challenge when directly applying two-stage NAS to low quantized latency scenarios. This is due to the quantization-unfriendly search space issue, where prior art search spaces can unexpectedly impede INT8 la-tency. Consequently, since INT8 quantization yields only a marginal speedup, we are forced to search for small-sized models to fulfill latency criteria, which can unfortunately restrict NAS to find better quantized models for edge de-vices. Then, a question naturally arise: Can we design a quantization-friendly search space, allowing NAS to dis-cover larger and superior models that meet the low INT8 latency requirements?
We start by conducting an in-depth study to understand the factors that determine INT8 quantized latency and how (1) they affect search space design. Our study shows: both operator type and configurations (e.g., channel width) greatly impact the INT8 latency; Improper selections can slow down the INT8 latency. For instance, Squeeze-and-Excitation (SE) [16] and Hardswish [14] are widely-used operators in current search spaces as it improves accuracy with little latency introduced. However, their INT8 infer-ence speeds are slower than FP32 inference on Intel CPU (Fig. 3(a)), because the extra costs (e.g., data transforma-tion between INT32 and INT8) introduced by quantization outweigh the latency reduction by INT8 computation. (2)
The quantization efficiency varies across different devices, and the preferred operator types can be contradictory.
The above study motivates us to design specialized quantization-friendly search spaces for each hardware, rather than relying on a single, large search space as seen in SPOS [12] for all hardware, which provides different op-erator options per layer. This is because two-stage NAS re-quires the search space to adhere to a specific condition for training the supernet, wherein each layer must utilize a fixed operator. Our study indicates significant variations in opti-mal operators across different hardware. Thus, customizing the search space for each hardware is crucial for optimal results. However, designing such specialized quantization-friendly search spaces for various edge devices presents a significant challenge, requiring expertise in both AI and hardware domains, as well as many trial and error attempts to optimize accuracy and INT8 latency for each hardware.
In this paper, we propose SpaceEvo, a novel method for automatically designing specialized quantization-friendly search space for each hardware. The search space is com-prised of hardware-preferred operators and configurations, enabling the search of larger and better models with low
INT8 latency. With the discovered search space, we lever-age two-stage quantization NAS to train a quantized-for-all supernet, and utilize evolution search [4] to find best quan-tized models under various INT8 latency constraints. Our approach addresses three key challenges: (1) What is the definition of a quantization-friendly search space in terms of both quantized accuracy and latency? (2) How to auto-matically design a search space without human expertise? (3) How to handle with the prohibitive cost caused by qual-ity evaluation of a candidate search space?
To address the first challenge, we propose a latency-aware Q-T score to quantify the effectiveness of a candidate search space, which measures the INT8 accuracy-latency quality of top-tier subnets in a search space. The behind intuition is that the goal of NAS is to search top subnets with better accuracy-latency tradeoffs.
Then, we introduce an evolutionary-based search algo-rithm that can effectively search a quantization-friendly search space with highest Q-T score. Searching a search space involves discovering a collection of model population that contains billions of models, which is challenging and easily introduce complexity. To address this challenge, we propose to factorize and encode a search space into a se-quence of elastic stages, which have flexible operator types and configurations. Through this design, the task of search space design is then simplied to find a search space with the optimal elastic stages, so that existing search algorithms can be easily applied. Specifically, we design a stage-wise hy-perspace to include many candidate search spaces and lever-age aging evolution [30] to perform random mutations of elastic stages for search space evolution. The evolution is guided by maximizing the Q-T score.
Finally, estimating the quality score (Q-T score) of a search space involves a costly training process for evalu-ating the accuracy of sub-networks, which presents a sig-nificant obstacle for our evolutionary algorithm. Naively adopting a two-stage NAS approach, training a supernet for each candidate search space [8, 7], is prohibitively expen-sive, taking of thousands GPU hours. To address this is-sue, we draw inspiration from block-wise knowledge dis-tillation [26, 22] and propose a block-wise search space quantization scheme. This scheme trains each elastic stage separately and rapidly estimates a model’s quantized accu-racy by summing block-level loss with a quantized accuracy lookup table, as shown in Fig. 5. This significantly reduces the training and evaluation costs, while providing effective accuracy rankings among search spaces. We summarize our contributions as follows:
• We study the INT8 quantization efficiency on real-world edge devices and find that the choices of operator types and configurations in a quantized model can significantly impact the INT8 latency, leaving a huge room for design optimization of quantized models.
• We propose SpaceEvo to automatically design a hardware-dedicated quantization-friendly search space and leverage two-stage quantization NAS to produce su-perior INT8 models under various latency constraints.
• We present three innovative techniques that enable the first-ever efficient and cost-effective evolution search to explore a search space comprising billions of models.
• Extensive experiments on two real-world edge devices and ImageNet demonstrate that our automatically de-signed search spaces significantly surpass manually-designed search spaces. Our discovered models, SE-Qnet, establish the new state-of-the-art INT8 quantized accuracy-latency tradeoffs. For instance, SEQnet@cpu-A4, achieves 80.0% accuracy on ImageNet, which is 3.3ms faster with 1.8% higher accuracy than FBNetV3-A. Moreover, SpaceEvo delivers superior tiny models, achieving up to 10.1% accuracy improvement over Shuf-fleNetV2x0.5 (41M FLOPs, 4.3ms). 2.