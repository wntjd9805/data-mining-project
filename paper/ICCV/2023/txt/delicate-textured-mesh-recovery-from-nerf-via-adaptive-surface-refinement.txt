Abstract 1.

Introduction
Neural Radiance Fields (NeRF) have constituted a re-markable breakthrough in image-based 3D reconstruction.
However, their implicit volumetric representations differ significantly from the widely-adopted polygonal meshes and lack support from common 3D software and hardware, mak-ing their rendering and manipulation inefficient. To over-come this limitation, we present a novel framework that generates textured surface meshes from images. Our ap-proach begins by efficiently initializing the geometry and view-dependency decomposed appearance with a NeRF.
Subsequently, a coarse mesh is extracted, and an itera-tive surface refinement algorithm is developed to adaptively adjust both vertex positions and face density based on re-projected rendering errors. We jointly refine the appear-ance with geometry and bake it into texture images for real-time rendering. Extensive experiments demonstrate that our method achieves superior mesh quality and competitive ren-dering quality.
The reconstruction of 3D scenes from RGB images is a complex task in computer vision with many real-world applications.
In recent years, Neural Radiance Fields (NeRF) [31, 2, 8, 32] have gained popularity for their im-pressive ability to reconstruct and render large-scale scenes with realistic details. However, NeRF representations often use implicit functions and specialized ray marching algo-rithms for rendering, making them difficult to manipulate and slow to render due to poor hardware support, which limits their use in downstream applications.
In contrast, polygonal meshes are the most commonly used represen-tation in 3D applications and are well-supported by most graphic hardware to accelerate rendering. However, direct reconstruction of meshes can be challenging due to their ir-regularity, and most approaches are limited to object-level reconstructions [33, 9, 10].
Some recent works [33, 6, 11, 54] have focused on com-bining the advantages of both NeRF and mesh representa-tion. MobileNeRF [11] presents a method of optimizing
NeRF on a grid mesh and incorporates rasterization for real-time rendering. However, the resulting mesh is far from the real surface of the reconstructed scene. Besides, the tex-tures are in the feature space instead of the RGB space, which makes editing or manipulation inconvenient. To ob-tain accurate surface meshes, a popular approach is to use
Signed Distance Fields (SDF), which defines an exact sur-face [48, 53, 57]. However, this line of research typically generates over-smoothed geometry that fails to model thin structures. Additionally, meshes obtained through March-ing Cubes [29] produce a large number of redundant ver-tices and faces to keep details. NVdiffrec [33] uses a dif-ferentiable rasterizer [23] to optimize a deformable tetra-hedral grid but is limited to object-level reconstruction and also fails to recover complex topology. The presence of a representation gap makes it challenging to recover accurate surface meshes from volumetric NeRF while maintaining rendering quality. a novel
This paper presents framework called
NeRF2Mesh for extracting delicate textured surface meshes from RGB images, as illustrated in Figure 1. Our key insight is to refine a coarse mesh extracted from
NeRF for joint optimization of geometry and appear-ance. The volumetric NeRF representation is suitable for efficient initialization of geometry and appearance. With a coarse mesh extracted from NeRF, we adjust the vertices’ position and face density based on 2D rendering errors, which in turn contributes to appearance optimization. To enable texture editing, we decompose the appearance into view-independent diffuse and view-dependent specular terms, so the diffuse color can be exported as a standard
RGB image texture. The specular term is exported as a feature texture that produces view-dependent color through a small MLP embedded in the fragment shader. Overall, our framework enables the creation of versatile and practical mesh assets that can be used in a range of scenarios that are challenging for volumetric NeRF.
Our contributions can be summarized as follows:
• We present the NeRF2Mesh framework to reconstruct textured surface meshes from multi-view RGB images, by jointly refining the geometry and appearance of coarse meshes extracted from an appearance decom-posed NeRF.
• We propose an iterative mesh refinement algorithm that enables us to adaptively adjust face density, where complex surfaces are subdivided and simpler surfaces are decimated based on re-projected 2D image errors.
• Our method achieves enhanced surface mesh quality, relatively smaller mesh size, and competitive render-ing quality to recent methods. Furthermore, the result-ing meshes can be real-time rendered and interactively edited with common 3D hardware and software. 2.