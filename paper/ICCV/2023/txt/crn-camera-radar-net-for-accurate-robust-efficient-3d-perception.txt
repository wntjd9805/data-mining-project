Abstract
Autonomous driving requires an accurate and fast 3D perception system that includes 3D object detection, track-ing, and segmentation. Although recent low-cost camera-based approaches have shown promising results, they are susceptible to poor illumination or bad weather conditions and have a large localization error. Hence, fusing cam-era with low-cost radar, which provides precise long-range measurement and operates reliably in all environments, is promising but has not yet been thoroughly investigated. In this paper, we propose Camera Radar Net (CRN), a novel camera-radar fusion framework that generates a semanti-cally rich and spatially accurate bird’s-eye-view (BEV) fea-ture map for various tasks. To overcome the lack of spa-tial information in an image, we transform perspective view image features to BEV with the help of sparse but accu-rate radar points. We further aggregate image and radar feature maps in BEV using multi-modal deformable atten-tion designed to tackle the spatial misalignment between inputs. CRN with real-time setting operates at 20 FPS while achieving comparable performance to LiDAR detec-tors on nuScenes, and even outperforms at a far distance on 100m setting. Moreover, CRN with offline setting yields 62.4% NDS, 57.5% mAP on nuScenes test set and ranks first among all camera and camera-radar 3D object detectors. 1.

Introduction
Accurate and robust 3D perception system is crucial for many applications, such as autonomous driving and mo-bile robot. For efficient 3D perception, obtaining a reli-able bird’s eye view (BEV) feature map from sensor inputs is necessary since various downstream tasks can be oper-ated on BEV space (e.g., object detection & tracking [80],
BEV segmentation [82], HD map generation [64], trajec-tory prediction [17], and motion planning [52]). Another important ingredient for deploying 3D perception to the real world is to build a system that relies less on LiDAR disadvantaged from high-cost, high-maintenance, and low-reliability. Apart from the drawbacks of LiDAR, 3D per-FPS vs. accuracy on nuScenes val set. We show
Figure 1. that fusing radar can significantly boost camera-only method with marginal computational cost. CRN outperforms all methods with much faster speed. See Table 1 and Fig. 6 for more details. ception system is required to identify semantic information on the road (e.g., traffic lights, road sign) that can be eas-ily leveraged by camera. In addition to the need for rich semantic information, detecting distant objects is essential, and this can be benefited from radar.
Recently, camera-based 3D perception in BEV [19, 52, 57] has drawn great attention. Thanks to rich semantic in-formation in dense image pixels, camera approaches can distinguish objects even at a far distance. Despite the advan-tage of cameras, localizing the accurate position of objects from a monocular image is naturally a challenging ill-posed problem. Moreover, cameras can be significantly affected by illumination conditions (e.g., glare, low-contrast, or low-lighting) due to the nature of the passive sensor. To address this, we aim to generate a BEV feature map using a camera with the help of a cost-effective range sensor, radar.
Radar has advantages not only in cost but also in high-reliability, long-range perception (up to 200m for typical automotive radar [8]), robustness in various conditions (e.g., snow, fog, or rain), and providing velocity estimation from a single measurement. However, radar also brings its chal-lenges such as sparsity (typically 180× fewer than LiDAR points per single frame in nuScenes [2]), noisy and am-biguous measurements (false negatives by low resolution,
accuracy, or low radar cross-section, and false positives by multi-path or clutters). As a result, previous camera-radar fusion methods using late fusion strategies that fuse detection-level results [7, 13] fail to fully exploit the com-plementary information, thus having limited performance and operating environment. Despite the huge potential of learning-based fusion, only a few studies [23, 24, 48] ex-plore camera-radar fusion in autonomous driving scenarios.
To put the aforementioned advantages and disadvan-tages of camera and radar in perspective, camera-radar fu-sion should be capable of following properties to fully exploit the complementary characteristics of each sensor.
First, camera features should be accurately transformed into
BEV space in terms of spatial position. Second, the fu-sion method should be able to handle the spatial misalign-ment between feature maps when aggregating two modal-ities. Last but not least, transformation and fusion should be adaptive in order to tackle noisy and ambiguous radar measurements.
To this end, we design a novel two-stage fusion method for BEV feature encoding, Camera Radar Net (CRN). The key idea of the proposed method is to generate semantically rich and spatially accurate BEV feature map by fusing com-plementary characteristics of camera and radar sensors. In particular, we first transform image features in perspective view into BEV not solely relying on estimated depth but us-ing radar, named radar-assisted view transformation (RVT).
Since transformed image features in BEV is not completely accurate, following multi-modal feature aggregation (MFA) layers consecutively encodes the multi-modal feature maps into a unified feature map using an attention mechanism.
We conduct extensive experiments on nuScenes and demon-strate that our proposed method can generate a fine-grained
BEV feature map to set the new state-of-the-art on various tasks while maintaining high efficiency, as shown in Fig 1.
The main contributions of our works are three-fold:
• Accuracy. CRN achieves LiDAR-level performance using camera and radar on 3D object detection, track-ing, and BEV segmentation tasks.
• Robustness.
CRN maintains robust performance even if one of the single sensor inputs is entirely un-available, which allows the fault-tolerant system.
• Efficiency.
CRN requires marginal extra cost for significant performance improvement, which enables long-range perception in real-time. 2.