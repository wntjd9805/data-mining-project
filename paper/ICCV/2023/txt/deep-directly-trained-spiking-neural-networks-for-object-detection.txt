Abstract
Spiking neural networks (SNNs) are brain-inspired energy-efficient models that encode information in spa-tiotemporal dynamics. Recently, deep SNNs trained directly have shown great success in achieving high performance on classification tasks with very few time steps. However, how to design a directly-trained SNN for the regression task of object detection still remains a challenging prob-lem. To address this problem, we propose EMS-YOLO, a novel directly-trained SNN framework for object detec-tion, which is the first trial to train a deep SNN with sur-rogate gradients for object detection rather than ANN-SNN conversion strategies. Specifically, we design a full-spike residual block, EMS-ResNet, which can effectively extend the depth of the directly-trained SNN with low power con-sumption. Furthermore, we theoretically analyze and prove the EMS-ResNet could avoid gradient vanishing or explod-ing. The results demonstrate that our approach outper-forms the state-of-the-art ANN-SNN conversion methods (at least 500 time steps) in extremely fewer time steps (only 4 time steps). It is shown that our model could achieve com-parable performance to the ANN with the same architec-ture while consuming 5.83× less energy on the frame-based
COCO Dataset and the event-based Gen1 Dataset. Our code is available in https://github.com/BICLab/
EMS-YOLO. 1.

Introduction
Object detection is a key and challenging problem in computer vision. It aims to recognize multiple overlapped
*Corresponding author objects and locate them with precise bounding boxes. This task has many applications in various fields, such as au-tonomous driving [3], security surveillance [47], and med-ical imaging [29]. Most existing frameworks (e.g., YOLO series [38], RCNN series [13]) for object detection use ar-tificial neural networks (ANNs), which have high perfor-mance but also high computational complexity and energy consumption. Spiking neural networks (SNNs), known as the third generation of neural networks [32, 40], potentially serves as a more efficient and biologically inspired way to perform object detection. Specifically, SNNs utilize bi-nary signals (spikes) instead of continuous signals for neu-ron communication, which reduces data transfer and stor-age overhead. Furthermore, the SNNs exhibit asynchronous computation and event-driven communication, which could avoid unnecessary computation and synchronization over-head. When deployed on neuromorphic hardware [33, 36],
SNNs show great energy efficiency.
However, most SNNs for object detection are converted from ANNs, which have some limitations. For example,
Spiking-Yolo [23, 22] needs at least 3500 time steps to match the performance of the original ANN. Spike Calibra-tion [26] can reduce the time steps to hundreds, but it still depends on the performance of the original ANN model.
Moreover, most methods for converted SNNs are only ap-plicable for static images, and not suitable for sparse event datas, because their dynamics are designed to approximate the expected activation of the ANN and fail to capture the spatiotemporal information of DVS data [10]. A promising approach is to train SNNs directly with surrogate gradient, which can achieve high performance with few time steps and process both static images and event data efficiently.
Another challenge is to deal with the multi-scale object features in object detection, which demands the network has sufficient representation capability. Correspondingly, deep structure training is needed. Existing models on ob-ject detection are limited to shallow structures [7] or hybrid structures [27, 20] that may not be deployed on some neu-romorphic hardware [8, 2, 30] where only spike operation is allowed. To achieve deep direct training of SNNs, Hu et al. [17] and Fang et al. [12] explored on classification tasks and proposed MS-ResNet and SEW-ResNet respectively to overcome the vanishing/exploding gradient problems and advanced the directly-trained SNNs to depths greater than 100 layers. Unfortunately, multi-scale transformation of channels and dimensions when extracting features of ob-jects of different sizes will cause the problem of increased energy consumption due to non-spike convolutional oper-ations in their networks to be prominent in the object de-tection task. Therefore, the problem of high energy con-sumption caused by the non-spike convolutional operations urgently needs to be addressed.
To tackle these problems, we propose a novel directly trained SNN for object detection based on the YOLO frame-work (EMS-YOLO). Our model is the first to use surro-gate gradients to train a deep and large-scale SNN for ob-ject detection without converting from ANNs. Specially, to deal with the multi-scale object features in object detec-tion, we design a new full-spike energy-efficient residual block, EMS-ResNet, that avoids redundant MAC operations caused by non-spike convolution. Our model can achieve high performance with few time steps and handle both static images and event data efficiently. Compared with other con-verted or hybrid SNNs for object detection, our model has higher performance and lower energy consumption.
Our major contributions of this paper can be summarized as follows:
• We propose EMS-YOLO, a novel directly trained spik-ing neural network for object detection, which could achieve better performance than the advanced ANN-SNN conversion methods while requiring only 4 time steps and inferencing in real time.
• We design an Energy-efficient Membrane-Shortcut
ResNet, EMS-ResNet, that enables full spiking in the network thus reducing power consumption. Moreover, we theoretically analyze that it can be trained deeply since it avoids gradient disappearance or explosion.
• The experiments on COCO and the Gen1 Datasets demonstrate that our models could achieve compara-ble performance to the ANN with the same architec-ture meanwhile reducing 5.83× energy consumption. 2.