Abstract
Hand-object interaction understanding and the barely addressed novel view synthesis are highly desired in the im-mersive communication, whereas it is challenging due to the high deformation of hand and heavy occlusions between hand and object. In this paper, we propose a neural render-ing and pose estimation system for hand-object interaction from sparse views, which can also enable 3D hand-object interaction editing. We share the inspiration from recent scene understanding work that shows a scene specific model built beforehand can significantly improve and unblock vi-sion tasks especially when inputs are sparse, and extend it to the dynamic hand-object interaction scenario and pro-pose to solve the problem in two stages. We first learn the shape and appearance prior knowledge of hands and objects separately with the neural representation at the of-fline stage. During the online stage, we design a rendering-based joint model fitting framework to understand the dy-namic hand-object interaction with the pre-built hand and object models as well as interaction priors, which thereby overcomes penetration and separation issues between hand and object and also enables novel view synthesis.
In or-der to get stable contact during the hand-object interaction process in a sequence, we propose a stable contact loss to make the contact region to be consistent. Experiments demonstrate that our method outperforms the state-of-the-art methods. Code and dataset are available in project web-page https://iscas3dv.github.io/HO-NeRF. 1.

Introduction
Hand-object interaction understanding plays an impor-tant role in immersive contextual teaching applications such as surgical operation and training in the use of machin-ery. Previous works mostly focus on the hand-object in-teraction detection [10], reasoning [26] or pose estima-*indicates corresponding author. tion [17, 16]. However, the barely addressed novel view synthesis of hand-object interaction is also highly desired.
Recently, neural rendering is emerging to facilitate the novel view synthesis simply by learning from a collection of images and produces promising high-quality images. Al-though existing neural rendering approaches perform well on static scenes [31, 2], rigid objects [54, 12] and human models [39, 38, 45], they barely considered scene con-text in interaction (such as contact [63] and model pen-etration [4, 24]).
In the realm of hand, LISA [8] is the only hand neural rendering model, and achieves promis-ing rendering results of bare hands. However, LISA cannot work well for hand-object interaction due to heavy inter-occlusions and it requires dense (about 20) camera views that may refrain it from wide applications. It is even more challenging to use sparse-view images to synthesize novel views [49, 25] and estimate accurate pose for hand-object interaction, which plays a key role in many applications such as Holoportation [36] and manipulation skill learning from human demonstration [42, 1].
In this work, we propose a novel-view synthesis and pose estimation system for hand-object interaction scenes with sparse camera views (Fig. 1). Recent scene understanding work [53] shows a scene specific model built beforehand can significantly improve and unblock vision tasks espe-cially when inputs are sparse, and we extend it from static objects to dynamic hand-object interaction scenes and solve the problem in two stages. We first use sparse-view images as input to train the pose-driven neural rendering models of hand and object during the offline stage. Benefiting from the progress of hand pose tracking [15, 19, 28] and object pose estimation [27], we only need very low cost to build hand model and object model. Then at the online stage, we estimate both hand and object poses using a novel dif-ferentiable rendering-based model fitting under geometric constraints. In this way, we can understand hand-object in-teraction accurately and render novel views effectively.
However, it is non-trivial to fulfill this goal. Firstly, it is difficult to build neural rendering systems from sparse cam-Figure 1: We propose a neural rendering and pose estimation system for hand-object interaction using sparse view images. (a) During offline stage, we learn hand and object models that enable rendering and shape reconstruction. During online stage, we initialize the pose from sparse camera views (b), and then conduct online fitting to improve pose estimation, which enables photo-realistic free viewpoint rendering (c). Our framework also naturally supports hand object interaction editing. era views due to insufficient visual information and depth ambiguity caused by hand-object inter-occlusions. Exist-ing few-shot neural rendering methods [25, 49] fail under sparse camera views (Fig. 7). In order to solve this prob-lem, we establish the fitting process based on the pre-built models which provides strong shape and appearance pri-ors, and it can achieve excellent novel view rendering from sparse views. Secondly, it is difficult to obtain accurate hand and object poses and reasonable interactions only by pho-tometric constraints due to extensive occlusion.
In order to handle this problem, we propose a novel differentiable rendering-based model fitting process under geometric con-straints to refine the poses and enforce the spatial context between hand and object by leveraging the signed distance function (SDF) to reduce penetration and to encourage tight hand-object regions to contact. We propose a stable contact loss to penalize large sliding of the hand-object contact area across temporally consecutive frames. Through the joint model fitting process, we can achieve accurate pose estima-tion for hand-object interactions. Thirdly, our system needs a dataset to include images of hand, object, and hand-object interaction. However, existing datasets such as [5, 13] can-not satisfy this requirement, so we need to collect a real dataset to evaluate our method.
Our main contribution is summarized as follows. First, to the best of our knowledge, we present the first solution to unblock hand-object interaction neural rendering from sparse views. We design a new two-stage approach (i.e. offline model building and online model fitting) to achieve accurate hand-object pose estimation and photo-realistic novel view synthesis. Second, we leverage effective geo-metric constraints to conduct rendering-based model fitting, which can recover reasonable hand-object interaction even under sparse views and inter-occlusions. To reduce the slid-ing that occurs during the interaction, we design a new sta-ble contact loss to enforce the hand-object contacted regions to be consistent for video sequences. Third, we propose a hand-object interaction dataset HandObject for neural ren-dering tasks, including images for hand, object and hand-object interaction scenes. Finally, experiments demonstrate that, with the help of offline and online stages, our method achieves significantly better performance in pose estimation and rendering quality than previous methods. 2.