Abstract
This paper tackles the challenges of self-supervised monocular depth estimation in indoor scenes caused by large rotation between frames and low texture. We ease the learning process by obtaining coarse camera poses from monocular sequences through multi-view geometry to deal with the former. However, we found that limited by the scale ambiguity across different scenes in the train-ing dataset, a na¨ıve introduction of geometric coarse poses cannot play a positive role in performance improvement, which is counter-intuitive. To address this problem, we propose to reﬁne those poses during training through ro-tation and translation/scale optimization. To soften the effect of the low texture, we combine the global reason-ing of vision transformers with an overﬁtting-aware, iter-ative self-distillation mechanism, providing more accurate depth guidance coming from the network itself. Experi-ments on NYUv2, ScanNet, 7scenes, and KITTI datasets support the effectiveness of each component in our frame-work, which sets a new state-of-the-art for indoor self-supervised monocular depth estimation, as well as out-standing generalization ability. Code and models are avail-able at https://github.com/zxcqlf/GasMono 1.

Introduction
Depth estimation from images is one of the fundamen-tal tasks in computer vision and plays a key role in sev-eral higher-level applications [28, 44]. It has a long history and has been intensively studied building upon multi-view geometry [17, 37], exploiting image matching across two or multiple images and their camera positions [31, 35, 41].
The advent of deep learning rejuvenated this ﬁeld and intro-duced new, exciting perspectives. Among many, the possi-bility of learning to estimate depth out of a single image – for long considered the holy grail in computer vision – be-∗ Corresponding author, yangtang@ecust.edu.cn.
Figure 1. Comparison between existing methods [2, 11, 27] and
GasMono. Our framework shows remarkable accuracy on thin objects and global structures. came true [7, 9, 23, 38, 39]. However, this came at the cost of requiring a massive amount of images annotated with ground truth depth, often expensive to collect.
In light of this, multi-view geometry maintained a key role in softening this latter constraint, allowing for the de-velopment of self-supervised monocular depth estimation frameworks [57]. These replace the need for depth labels by exploiting a proxy signal based on image reconstruction of the frame given as input to the network, i.e. the target, starting from one or multiple source images. The only re-quirement consists of collecting raw video sequences [64] or rectiﬁed stereo images [12]. Among the two alternatives, the former [64] results as the cheapest and most ﬂexible, since requiring a single camera only to move around and collect training data, with the relative camera poses between consecutive frames needing to be estimated by employing a dedicated pose network (PoseNet) alongside the depth esti-mation task. A large body of literature built ever more accu-rate self-supervised solutions, mainly focusing on outdoor environments – i.e. with driving context [10, 51] represent-ing the preferred benchmark.
Nonetheless, the indoor setting is equally important for the development of, among others, navigation and assistive technologies, although featuring 1) much more complex ego-motion conﬁgurations and 2) large untextured regions,
making this setting itself challenging for self-supervised depth estimation frameworks [62]. Speciﬁcally, the differ-ent data collection equipment involved in outdoor and in-door scenes – car-mounted vs handheld cameras – leads to motion models largely different in the two cases. As an example, the average rotation between consecutive images in KITTI [10] is 0.25◦, while on NYUv2 [32] dataset it is 2.28◦ [2]. Larger rotations among the images hinder the training process, because of the discontinuous Euler angle representation [65] commonly used for this task. Addition-ally, the lack of texture tampers the image reconstruction process through which supervision is provided to the net-work, with several local minima in the training loss signal.
In this paper, we propose a novel, Geometry-aided self-supervised framework for Monocular depth estimation, dubbed GasMono, speciﬁcally designed to face these chal-lenges. Speciﬁcally, we leverage classic structure-from-motion algorithms such as COLMAP [41] on the training sequences to initialize the pose estimation process, which is then reﬁned to cope with the scale inconsistency occur-ring between the different monocular sequences part of the training set. To deal with the reduced texture characterizing indoor images, we combine recent architectures based on vision transformers [59] with an iterative, self-distillation scheme to obtain stronger supervision and, thus, train Gas-Mono more effectively. Our main contributions are:
• We tackle the challenges of learning camera poses in indoor scenes by exploiting strong priors coming from classic structure-from-motion algorithms [41].
• This, however, is not sufﬁcient: we explore the fac-tors in such an approach making the training process unstable and a further, learning-based reﬁnement strat-egy is proposed to optimize both rotation and transla-tion/scale of the initial poses.
• We explore the effectiveness of transformer architec-ture in improving the depth estimation of low-texture regions for indoor scenes, coupled with an overﬁtting-aware iterative self-distillation method, iteratively dis-tilling pseudo labels from the depth network itself.
• Our GasMono framework is evaluated on a variety of indoor datasets, establishing a new state-of-the-art for indoor, self-supervised monocular depth estimation.
Fig. 1 shows a comparison between existing frameworks and GasMono, which shows more coherent predictions. 2.