Abstract
Fast generation of high-quality 3D digital humans is important to a vast number of applications ranging from entertainment to professional concerns. Recent advances in differentiable rendering have enabled the training of 3D generative models without requiring 3D ground truths.
However, the quality of the generated 3D humans still has much room to improve in terms of both fidelity and diver-sity. In this paper, we present Get3DHuman, a novel 3D human framework that can significantly boost the realism and diversity of the generated outcomes by only using a
*Corresponding author: hanxiaoguang@cuhk.edu.cn limited budget of 3D ground-truth data. Our key obser-vation is that the 3D generator can profit from human-related priors learned through 2D human generators and 3D reconstructors. Specifically, we bridge the latent space of Get3DHuman with that of StyleGAN-Human [13] via a specially-designed prior network, where the input latent code is mapped to the shape and texture feature volumes spanned by the pixel-aligned 3D reconstructor [50]. The outcomes of the prior network are then leveraged as the su-pervisory signals for the main generator network. To ensure effective training, we further propose three tailored losses applied to the generated feature volumes and the intermedi-ate feature maps. Extensive experiments demonstrate that
Get3DHuman greatly outperforms the other state-of-the-art approaches and can support a wide range of applica-tions including shape interpolation, shape re-texturing, and single-view reconstruction through latent inversion. 1.

Introduction
Generating diverse and high-quality virtual humans plays an important role in numerous applications, including
VR/AR, visual effects, game production, etc. The advances in generative models have brought impressive advances to the state-of-the-art of generating 2D virtual avatars, such as images or videos. However, synthesizing 3D humans with high fidelity and large variations remains much under-explored due to the scarcity of 3D human data.
The conventional solution [9, 8, 55, 11] for acquiring a 3D avatar from a real person is typically a time-consuming and cumbersome process, requiring a specialized capture system, substantial manual efforts, and extensive computa-tion. To circumvent the requirement of collecting a large corpus of 3D ground-truth data, recent works utilize the differentiable rendering technique to train a 3D generative model in a 3D-unsupervised manner [6, 14]. Specifically, instead of using direct 3D supervision, adversarial losses are applied to the images rendered from the synthesized 3D content. However, due to the lack of dense multi-view images for each model, these methods can only encourage geometry-to-image consistency in the selected views while failing to produce plausible reconstruction in the unseen re-gions. In addition, the differentiable rendering process is computationally heavy, making the network training highly inefficient.
To resolve the above issues, in this work, we present
Get3DHuman, a novel 3D generator that can faithfully syn-thesize high-fidelity clothed 3D humans with a diversity of shapes and textures. Our key observation is that 3D human generators can benefit from the inductive bias from a 2D human synthesizer and the prior knowledge learned through relevant 3D modeling tasks. In particular, to bypass the lim-ited availability of 3D ground truths, we propose to leverage the generative power of 2D human synthesizers which have shown more promising and stable quality than their counter-part 3D generators. We further lift the rich prior from the 2D generator, i.e. StyleGAN-Human [13], to 3D by using the pixel-aligned reconstruction priors, i.e. the pre-trained PIFu network [49], through single-view human reconstruction.
Thanks to the strong generalization ability of pixel-aligned implicit reconstructor, by feeding it with a myriad of photo-realistic human images generated by StyleGAN-Human, we are able to obtain a vast number of 3D human models with highly diversified body shapes, apparel, poses, and textures.
To further ensure the high quality of the generated shapes, we filter out inferior results via manual inspections.
We further materialize the above idea via a novel prior induction mechanism. Specifically, we first train a prior network to encode the 2D generator prior and the 3D re-construction prior into three supervisory signals. That is, given a random latent code, the prior network would gen-erate normal maps, depth maps, and shape and texture fea-ture volumes of the 3D human corresponding to the input code (see Fig. 2). The input code is sampled from the latent space of StyleGAN-Human while the shape and texture fea-ture volumes are consistent with the PIFu latent, and, hence, can be converted to the predicted human shape via the pre-trained PIFu decoder. We then supervise the training of the proposed 3D generators via three specially-tailored losses.
First, a latent prior loss is introduced to provide direct su-pervision of the generated feature volumes for the shape and texture generation branches. Second, an adversarial loss is applied to the 3D feature volumes instead of the output signed distance field (SDF). This helps reduce the train-ing complexity while ensuring the realism of the generated 3D humans. Lastly, normal maps and depth maps are used for supervising the generation of intermediate feature maps.
Specifically, instead of directly transforming the input code into a 3D feature volume, we first map the code to 2D fea-ture maps and then lift them into 3D feature volume. This additional intermediate supervision helps cast finer-grained geometry details as shown in our experiments (see Fig. 7).
We further utilize a refinement module to improve the qual-ity of our textured mesh as the texture prior is not always satisfactory.
Our method can support a wide range of applications, in-cluding shape generation, interpolation, shape re-texturing, and latent inversion from a single image. We evaluate
Get3DHuman via extensive experiments and demonstrate that it strongly outperforms the state-of-the-art methods, both qualitatively and quantitatively.
To summarize, our main contributions include:
• We propose a novel 3D human generation framework that explicitly incorporates priors from top-tier 2D human generators and 3D reconstruction schemes to achieve high-quality and diverse 3D clothed human generation.
• We present specially-tailored prior induction losses for effective and efficient prior-based supervision.
• We set the new state of the art in the task of shape gen-eration while supporting many applications including shape interpolation, re-texturing, and latent inversion. 2.