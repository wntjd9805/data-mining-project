Abstract dataset, pre-trained model weights, and code are available at https://satlas-pretrain.allen.ai/.
Remote sensing images are useful for a wide variety of planet monitoring applications, from tracking deforestation to tackling illegal fishing. The Earth is extremely diverse— the amount of potential tasks in remote sensing images is massive, and the sizes of features range from several kilometers to just tens of centimeters. However, creating generalizable computer vision methods is a challenge in part due to the lack of a large-scale dataset that captures these diverse features for many tasks.
In this paper, we present SATLASPRETRAIN, a remote sensing dataset that is large in both breadth and scale, combining Sentinel-2 and NAIP images with 302M labels under 137 categories and seven label types. We evaluate eight baselines and a proposed method on SATLASPRETRAIN, and find that there is substantial room for improvement in addressing re-search challenges specific to remote sensing, including pro-cessing image time series that consist of images from very different types of sensors, and taking advantage of long-range spatial context. Moreover, we find that pre-training on SATLASPRETRAIN substantially improves performance on downstream tasks, increasing average accuracy by 18% over ImageNet and 6% over the next best baseline. The 1.

Introduction
Satellite and aerial images provide a diverse range of in-formation about the physical world. In images of urban ar-eas, we can identify unmapped roads and buildings and in-corporate them into digital map datasets, as well as monitor urban expansion. In images of industrial areas, we can cat-alogue solar farms and wind turbines to track the progress of renewable energy deployment. In images of glaciers and forests, we can monitor slow natural changes like glacier loss and deforestation. With the availability of global, regu-larly updated, and public domain sources of remote sensing images like the EU’s Sentinel missions [4], we can monitor the Earth for all of these applications and more at a global-scale, on a monthly or even weekly basis.
Because the immense scale of the Earth makes global manual analysis of remote sensing images cost-prohibitive, automatic computer vision methods are crucial for unlock-ing their full potential. Previous work has proposed apply-ing computer vision for automatically inferring the posi-tions of roads and buildings [10, 13, 33, 37, 60, 61]; moni-toring changes in land cover and land use such as deforesta-tion and urban expansion [46, 47]; predicting vessel posi-tions and types to help tackle illegal fishing [42]; and track-ing the progress and extent of natural disasters like floods, wildfires, and tornadoes [8, 23, 44]. However, in practice, most deployed applications continue to rely on manual or semi-automated rather than fully automated analysis of re-mote sensing images [1] for two reasons. First, accuracy remains a barrier even in major applications like road ex-traction [12], making full automation impractical. Second, there is a long tail of remote sensing applications that re-quire expert annotation but have few labeled examples (e.g., a recent New York Times study manually documented ille-gal airstrips in Brazil using satellite images [9]).
We believe that the lack of a very-large-scale, multi-task remote sensing dataset is a major impediment for progress on automated methods for remote sensing tasks today. First, state-of-the-art architectures such as ViT [26] and CLIP [43] require huge datasets to achieve peak per-formance. However, existing remote sensing datasets for object detection, instance segmentation, and seman-tic segmentation like DOTA [55], iSAID [58], and Deep-Globe [24] contain less than 10K images each, compared to the 328K images in COCO and millions used to train CLIP; the small size of these datasets means we cannot fully take advantage of recent architectures. Second, existing remote sensing benchmarks are fragmented, with individual bench-marks for categories like roads [41], vessels [42], and crop types [28], but no benchmark spanning many categories.
The lack of a large-scale, centralized, and accessible bench-mark prevents transfer learning opportunities across tasks, and makes it difficult for computer vision researchers to en-gage in this domain.
We present SATLASPRETRAIN, a large-scale dataset for improving remote sensing image understanding mod-els. Our goal with SATLASPRETRAIN is to label every-thing that is visible in a satellite image. To this end,
SATLASPRETRAIN combines Sentinel-2 and NAIP images with 302M distinct labels under 137 diverse categories and the label types are points like wind tur-7 label types: bines and water towers; polygons like buildings and air-ports; polylines like roads and rivers; segmentation and regression labels like land cover categories and bathymetry (water depth); properties of objects like the rotor diameter of a wind turbine; and patch classification labels like the presence of smoke in an image. Figure 1 demonstrates the wide range of categories in SATLASPRETRAIN, along with the diverse applications that they serve.
We find that the huge scale of SATLASPRETRAIN en-ables pre-training to substantially improve downstream per-formance. We compare SATLASPRETRAIN pre-training against pre-training on other datasets as well as self-supervised learning methods, and find that it improves av-erage performance across seven downstream tasks by 18% over ImageNet and 6% over the next best baseline. These results show that SATLASPRETRAIN can readily improve accuracy on the numerous niche remote sensing tasks that require costly expert annotation.
Additionally, we believe that SATLASPRETRAIN will en-courage work on computer vision methods that tackle the unique research challenges in the remote sensing domain.
Compared to general-purpose computer vision methods, re-mote sensing models require specialized techniques such as accounting for long-range spatial context, synthesizing in-formation across images over time captured by diverse sen-sors like multispectral images and synthetic aperture radar (SAR), and predicting objects that vary widely in size, from forests spanning many km2 to street lamps. We evalu-ate eight computer vision baselines on SATLASPRETRAIN and find that no single existing method supports all the
SATLASPRETRAIN label types; instead, each baseline can only predict a subset of categories. Thus, inspired by re-cent work that integrate task-specific output heads [21, 29, 35, 36], we develop a unified model called SATLASNET that incorporates seven such heads so that it can learn from every category in the dataset. Compared to training sep-arately on each label type, we find that jointly training
SATLASNET on all categories and then fine-tuning on each label type improves average performance by 7.1%, show-ing that SATLASNET is able to leverage transfer learning opportunities between label types.
In summary, our contributions are: 1. SATLASPRETRAIN, a large-scale remote sensing dataset with 137 categories under seven label types. 2. Demonstrating that pre-training on SATLASPRETRAIN improves average performance on seven downstream datasets by 6%. 3. SATLASNET, a unified model that supports predictions for all label types in SATLASPRETRAIN.
We have released the dataset and code at https:// satlas-pretrain.allen.ai/. We have also re-leased model weights pre-trained on SATLASPRETRAIN which can be fine-tuned for downstream tasks. 2.