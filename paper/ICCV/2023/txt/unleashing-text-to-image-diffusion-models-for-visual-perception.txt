Abstract
Diffusion models (DMs) have become the new trend of generative models and have demonstrated a powerful abil-ity of conditional synthesis. Among those, text-to-image dif-fusion models pre-trained on large-scale image-text pairs are highly controllable by customizable prompts. Unlike the unconditional generative models that focus on low-level attributes and details, text-to-image diffusion models contain more high-level knowledge thanks to the vision-language pre-training. In this paper, we propose VPD (Vi-sual Perception with pre-trained Diffusion models), a new framework that exploits the semantic information of a pre-trained text-to-image diffusion model in visual perception tasks.
Instead of using the pre-trained denoising autoen-coder in a diffusion-based pipeline, we simply use it as a backbone and aim to study how to take full advantage of the learned knowledge. Specifically, we prompt the denois-ing decoder with proper textual inputs and refine the text features with an adapter, leading to a better alignment to the pre-trained stage and making the visual contents inter-act with the text prompts. We also propose to utilize the cross-attention maps between the visual features and the text features to provide explicit guidance. Compared with other pre-training methods, we show that vision-language pre-trained diffusion models can be faster adapted to down-stream visual perception tasks using the proposed VPD.
Extensive experiments on semantic segmentation, referring image segmentation, and depth estimation demonstrate the effectiveness of our method. Notably, VPD attains 0.254
RMSE on NYUv2 depth estimation and 73.3% oIoU on
RefCOCO-val referring image segmentation, establishing new records on these two benchmarks. Code is available at https://github.com/wl-zhao/VPD. 1.

Introduction
Recently, large text-to-image diffusion models [45, 42] have demonstrated phenomenal power in generating di-Figure 1: The main idea of the proposed VPD frame-work. Motivated by the compelling generative semantic of a text-to-image diffusion model, we propose a new frame-work named VPD to exploit the pre-trained knowledge in the denoising UNet to provide semantic guidance for down-stream visual perception tasks. verse and high-fidelity images with high customizabil-ity [45, 19, 38, 6], attracting growing attention from both the research community and the public eye. By leverag-ing large-scale datasets of image-text pairs (e.g., LAION-5B [48]), text-to-image diffusion models exhibit favorable scaling ability. Large-scale text-to-image diffusion mod-els are able to generate high-quality images with rich tex-ture, diverse content and reasonable structures while having compositional and editable semantics. This phenomenon potentially suggests that large text-to-image diffusion mod-els can implicitly learn both high-level and low-level visual concepts from massive image-text pairs. Moreover, recent research [19, 38] also has highlighted the clear correlations between the latent visual features and corresponding words in text prompts in text-to-image diffusion models.
*Equal contribution.
â€ Corresponding author.
The compelling generative semantic and compositional
abilities of text-to-image diffusion models motivate us to think: is it possible to extract the visual knowledge learned by large diffusion models for visual perception tasks?
However, it is non-trivial to solve this problem. Conven-tional visual pre-training methods aim to encode the input image as latent representations and learn the representa-tions with pretext tasks like contrastive learning [18, 10] and masked image modeling [2, 17] or massive annotations in classification and vision-language tasks. The pre-training process makes the learned latent representation naturally suitable for a range of visual perception tasks where seman-tic knowledge is extracted from the raw images. In contrast, text-to-image models are designed to generate high-fidelity images based on textual prompts. They take as input ran-dom noises and text prompts, and aim to produce images through a progressive denoising process [45, 20]. While there is a notable gap between the text-to-image generation task and the conventional visual pre-training mechanisms, the training process of text-to-image models also requires them to capture both low-level knowledge of images (e.g., textures, edge, and structures) and high-level semantic rela-tions between visual and linguistic concepts from diverse and large-scale image-text pairs in an implicit way. Al-though rich representations are learned in large diffusion models, it is still unknown how to extract this knowledge for downstream tasks and whether it can benefit visual per-ception.
In this paper, we study how to leverage the knowledge learned in text-to-image for visual perception. Compared to transferring knowledge from conventional pre-trained mod-els to downstream visual perception tasks, there are two dis-tinct challenges to performing transfer learning on diffusion models: the incompatibility between the diffusion pipeline and visual perception tasks and the architectural differences between UNet [46]-like diffusion models and popular visual backbones. To tackle these challenges, we introduce a new framework called VPD to adapt pre-trained diffusion mod-els for visual perception tasks. Instead of using the step-by-step diffusion pipeline, we propose to simply employ the autoencoder as a backbone model to directly consume the natural images without noise and perform a single extra de-noising step with designed prompts to extract the semantic information. Our framework is based on popular Stable Dif-fusion [45] models, which conduct the denoising process in a learned latent space with a UNet architecture. We extract features from different hierarchies from the UNet decoder to construct visual representations of the input image. To align with the pre-trained stage and facilitate interactions between visual content and text prompts, we prompt the denoising diffusion model with proper textual inputs and refine the text features with an adapter. Additionally, in-spired by previous studies on the relations between prompt words and visual patterns in diffusion models, we propose to utilize the cross-attention maps between the visual and text features to provide explicit guidance. The combined implicit and explicit guidance can be fed to various task-specific decoders to perform visual perception tasks. Our main idea is summarized in Figure 1.
We evaluate our method on three representative vi-sual perception tasks covering: 1) semantic segmenta-tion [60] which requires the understanding of high-level and fine-grained visual concepts, 2) referring image segmen-tation [58, 35] that requires the ability of visual-language modeling, and 3) depth estimation [49] that requires low-level and structural knowledge of images. With the help of the proposed VPD, we show that a vision-language pre-trained diffusion model can be a fast and powerful learner of downstream visual perception tasks. Our method at-tains 73.3% oIoU and 0.254 RMSE on RefCOCO [58] re-ferring image segmentation and NYUv2 [49] depth esti-mation, respectively, establishing new state-of-the-art on these two benchmarks. Equipped with a lightweight Se-mantic FPN [24] decoder, our model achieves 54.6% mIoU on ADE20K [60], outperforming supervisedly pre-trained
ConvNeXt-XL [31] model with comparable computational complexity. We also exhibit that models pre-trained with diffusion tasks can fast obtain 44.7% mIoU on this chal-lenging benchmark with only 4K iteration training, outper-forming existing pre-training methods. We expect our study to offer a new perspective on learning more generic visual representations with generative models and spark further re-search on bridging and unifying the vibrant research fields of image generation and perception. 2.