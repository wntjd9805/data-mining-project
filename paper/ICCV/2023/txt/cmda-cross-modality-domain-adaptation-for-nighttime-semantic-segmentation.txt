Abstract
Most nighttime semantic segmentation studies are based on domain adaptation approaches and image input. How-ever, limited by the low dynamic range of conventional cam-eras, images fail to capture structural details and boundary information in low-light conditions. Event cameras, as a new form of vision sensors, are complementary to conven-tional cameras with their high dynamic range. To this end, we propose a novel unsupervised Cross-Modality Domain
Adaptation (CMDA) framework to leverage multi-modality (Images and Events) information for nighttime semantic seg-mentation, with only labels on daytime images. In CMDA, we design the Image Motion-Extractor to extract motion infor-mation and the Image Content-Extractor to extract content information from images, in order to bridge the gap between different modalities (Images ⇌ Events) and domains (Day
⇌ Night). Besides, we introduce the first image-event night-time semantic segmentation dataset. Extensive experiments on both the public image dataset and the proposed image-event dataset demonstrate the effectiveness of our proposed approach. We open-source our code, models, and dataset at https://github.com/XiaRho/CMDA.
Figure 1. Images captured at different moments in the same lo-cation show that the low dynamic range of frame-based cameras leads to reduced color contrast and detailed edges of objects at night. To overcome this challenge, we introduce event cameras that have a high dynamic range and are capable of capturing more nighttime details. In comparison to the semantic segmentation re-sults obtained from daytime images [36], nighttime images result in misclassification cases [14]. However, our proposed CMDA improves this by introducing event modality for the first time. 1.

Introduction
Semantic segmentation is a crucial aspect of computer vision, which is essential for many applications, such as au-tonomous driving [20, 28], robotics [4, 19, 21], and surveil-lance [18]. While semantic segmentation of daytime scenes has made significant progress [5, 29, 36, 41], challenges remain unsolved for nighttime scenes due to the much-degraded image quality at night, as well as the lack of high-quality annotations. Most existing works [11, 33, 34, 37] em-ployed unsupervised domain adaptation (UDA) for nighttime semantic segmentation to solve the label scarcity problem, which leverage labeled daytime images (Source Domain)
*Corresponding author. and unlabeled nighttime images (Target Domain). However, the low dynamic range of conventional frame-based cameras results in poor image quality at night compared to daytime images, i.e., the decrease in color contrast and details results in a reduction of clarity in nighttime images. This impedes the effective discrimination of object boundaries. Thus, the performance of methods solely relying on nighttime images as input is limited.
To address the limitations of frame-based cameras, we propose to employ event cameras for nighttime semantic segmentation. Event cameras output the spatio-temporal coordinates of pixels whose luminosity changes exceeding a certain threshold value [9, 17]. Their unique operating principle offers a higher dynamic range (140 dB vs. 60 dB)
over frame-based cameras [10], which enhances contrast in low-light scenarios, facilitating more precise segmentation of objects. On the other hand, events are asynchronous and spa-tially sparse, lacking a comprehensive representation of the scene. Hence methods based solely on events are typically inferior to image-based approaches [31, 32]. To this end, we propose the first image-event cross-modality framework,
Cross-Modality Domain Adaptation (CMDA), to leverage both image and event modalities for nighttime semantic seg-mentation in an unsupervised manner. As shown in Figure 1, compared to conventional image-based UDA approaches, our framework achieves substantially improved nighttime semantic segmentation performance with the combination of event modality.
In the proposed CMDA, the key challenges lie in estab-lishing the connection between image and event modalities, as well as minimizing the domain shifts between the repre-sentations of daytime and nighttime images. Specifically:
Challenge 1: Images ⇌ Events. The absence of event modality in the source domain hinders the fusion of im-ages and events. An intuitive idea is to transfer the daytime images into events. However, event cameras record the movement of the scene w.r.t. the camera, which cannot be determined with a single image. Thus, we propose the Im-age Motion-Extractor to extract motion information from adjacent images and bridge the gap between image and event modalities.
Challenge 2: Day ⇌ Night. Images can typically be separated into content and style information [16]. Previ-ous image-based UDA approaches employed a style transfer network [44] to transform daytime images so they look like nighttime [11, 37]. However, the transferred images are often unrealistic and unreliable, due to the significant and heteroge-neous noise at night [40]. In contrast, we eliminate daytime and nighttime style information and preserve only content information based on the proposed Image Content-Extractor, which transfers both daytime and nighttime images to a com-mon content domain.
Then, we construct our network based on the image-based
UDA method DAFormer [14]. Instead of taking only im-ages as input, we combine events with images to perform improved nighttime semantic segmentation, with domain adaptation from labeled daytime images. In addition, as there are no existing benchmark datasets in the community for nighttime image-event semantic segmentation evaluation, we follow the image-based Dark Zurich dataset [25] and manually annotate 150 image-event with fine, pixel-level labels from DSEC dataset [13].
In summary, our contributions are as follows:
• 1) To the best of our knowledge, we introduce the first method to utilize event modality in nighttime semantic segmentation.
• 2) We propose a novel CMDA framework by fusing image and event modalities in an unsupervised manner with only labeled images from the source domain.
• 3) We propose the Image Motion-Extractor and Image
Content-Extractor to bridge the gaps between modali-ties (Images ⇌ Events) and domains (Day ⇌ Night).
• 4) To fill in the missing evaluation criteria for nighttime image-event semantic segmentation, we align images and event modalities in the DSEC dataset [13] and manually annotate 150 image-event with fine, pixel-level labels.
• 5) We show the effectiveness of our CMDA framework, which achieves SOTA results on both the existing night-time images benchmark dataset [25] and our proposed image-event dataset. 2.