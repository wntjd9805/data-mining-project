Abstract
Recently, large-scale pre-trained language-image mod-els like CLIP have shown extraordinary capabilities for un-derstanding spatial contents, but naively transferring such models to video recognition still suffers from unsatisfactory temporal modeling capabilities. Existing methods insert tunable structures into or in parallel with the pre-trained model, which either requires back-propagation through the whole pre-trained model and is thus resource-demanding, or is limited by the temporal reasoning capability of the pre-trained structure. In this work, we present DiST, which disentangles the learning of spatial and temporal aspects of videos. Specifically, DiST uses a dual-encoder struc-ture, where a pre-trained foundation model acts as the spa-tial encoder, and a lightweight network is introduced as the temporal encoder. An integration branch is inserted between the encoders to fuse spatio-temporal information.
The disentangled spatial and temporal learning in DiST is highly efficient because it avoids the back-propagation of massive pre-trained parameters. Meanwhile, we empiri-cally show that disentangled learning with an extra net-work for integration benefits both spatial and temporal un-derstanding. Extensive experiments on five benchmarks show that DiST delivers better performance than existing state-of-the-art methods by convincing gaps. When pre-training on the large-scale Kinetics-710, we achieve 89.7% on Kinetics-400 with a frozen ViT-L model, which verifies the scalability of DiST. Codes and models can be found in https://github.com/alibaba-mmai-research/DiST. 1.

Introduction
Video understanding is a fundamental yet challenging re-search topic in computer vision. Early approaches for this task learn spatio-temporal representations by designing dif-∗Corresponding authors.
Figure 1: Accuracy vs. per-video GFLOPs on SSV2 [19] and K400 [29] with ViT-B/16 [12]. “EVL” [39]: Efficient
Video Learning. “ST-A” [45]: ST-Adapter. “CLIP”: Fully fine-tuning the CLIP pre-trained image encoder. ferent architectures, such as two-stream models [28, 65], 3D networks [59, 16, 29], Transformers [1, 3, 72], and they have achieved impressive progress on some challeng-ing benchmarks [29, 19]. Recently, a new paradigm that transfers the large-scale language-image pre-training mod-els, e.g., CLIP [52], to video understanding tasks [26, 39, 45, 31] has been drawing lots of attention due to its remark-able spatial modeling potential, and it deserves the enhance-ment of the potential for spatio-temporal reasoning.
As in Fig. 2 (a), a popular design for efficient transfer learning is to insert tunable structures between pre-trained
Transformer blocks [45, 37, 18, 8]. Parameter-efficient as it is, it would require back-propagation through massive pa-rameters that are supposed to be frozen, which is inefficient in training. With a large number of video frames, this in-efficiency hinders the scaling of large video-text foundation models under limited GPU memory, as in Fig. 1. To tackle this, the recent work [39] introduces a decoder in parallel with the pre-trained encoder. This indeed increases train-Method
Temp. Spat.
BP-Free (a) ST-Adapter ✗ Strong Strong
✓ Weak Strong
✓ Strong Strong (c) DiST (b) EVL
Figure 2: Comparison with existing efficient fine-tuning approaches for video recognition. (a) ST-Adapter [45]. (b) EVL [39]. (c) Our proposed DiST. “BP-Free” indicates “back-propagation-free” for the encoder. “Temp.” and “Spat.” are “temporal modeling” and “spatial modeling”, respectively. ing efficiency by avoiding back-propagation through pre-trained parameters. However, the major function of the de-coder in such an approach is to collect relevant information from the frozen encoder, which makes the output of the de-coder highly correlated to the spatial information provided by the pre-trained image Transformer.
In this work, we present DiST, a dual-encoder frame-work for efficiently transferring the pre-trained image-text foundation models to video-text ones. DiST shares the mer-its of both frameworks mentioned above by disentangling the spatial and temporal modeling: (i) by connecting all the structures in parallel to the frozen model, DiST avoids the back-propagation through the massive parameters in the pre-trained Transformer; (ii) by introducing a separated en-coder that specifically designed to extract temporal informa-tion for the input video, the temporal modeling capability is enhanced. Further, to simultaneously exploit the spatial se-mantics and the temporal information extracted by the dual-encoder structure, an integration branch is imposed to fuse the features from both spatial and temporal encoders.
We evaluate DiST on three challenging supervised i.e., Kinetics-400 [29], video recognition benchmarks,
Something-Something v2 [19], Epic-Kitchens-100 [10], i.e., UCF101 [55] and and two zero-shot benchmarks,
HMDB51 [23]. Our DiST achieves state-of-the-art per-formance on all datasets with convincing gains to existing approaches. Moreover, under limited resources, DiST en-ables us to pre-train on large-scale video datasets since only the lightweight temporal encoder and integration branch re-quire pre-training. With Kinetics-710 for pre-training, we verify the superior scalability of DiST and achieve better performance than fully fine-tuned ones. 2.