Abstract
A multitude of prevalent pre-trained models mark a major milestone in the development of artificial intelligence, while fine-tuning has been a common practice that enables pre-trained models to figure prominently in a wide array of target datasets. Our empirical results reveal that off-the-shelf fine-tuning techniques are far from adequate to mitigate negative transfer caused by two types of underperforming features in a pre-trained model, including rare features and spuriously correlated features. Rooted in structural causal models of predictions after fine-tuning, we propose a Concept-wise fine-tuning (Concept-Tuning) approach which refines feature representations in the level of patches with each patch en-coding a concept. Concept-Tuning minimizes the negative impacts of rare features and spuriously correlated features by (1) maximizing the mutual information between examples in the same category with regard to a slice of rare features (a patch) and (2) applying front-door adjustment via attention neural networks in channels and feature slices (patches).
The proposed Concept-Tuning consistently and significantly (by up to 4.76%) improves prior state-of-the-art fine-tuning methods on eleven datasets, diverse pre-training strategies (supervised and self-supervised ones), various network ar-chitectures, and sample sizes in a target dataset. 1.

Introduction
Pre-trained models, pre-trained by either conventional su-pervised [26] or resurgent self-supervised strategies [17, 20], undoubtedly constitute a milestone in the artificial intelli-gence community. Such a resounding success stems from the gap between the heavy reliance of deep neural networks on extensive data on one side and the lack of annotated data in many real-world applications on the other side. Pre-trained models suffice to bridge this gap under the aegis of the well-established fine-tuning paradigm [21].
The practice of fine-tuning, unfortunately but unsurpris-ingly, is not always outperforming; the notorious problem of
*Part of the work was done when the author interned at Tencent AI Lab.
†Corresponding Author
Figure 1: Exemplar attentive regions of the model trained (a) from scratch, by (b) linear probing, (c) vanilla fine-tuning, and (d) bi-tuning via Eigen-Grad-CAM [38], where only (a) predicts correctly. negative transfer [8, 51] arises especially when downstream tasks are out of the distribution of pre-training data. There could be cases where the model trained from scratch outper-forms fine-tuning. For example, there exist 3.08% testing images of the downstream dataset CUB [50] on which the model trained from scratch makes correct predictions while fine-tuning the supervised pre-trained model misclassifies.
The issue remains even if we resort to one recent state-of-the-art fine-tuning method named Bi-tuning [61], despite offering a smaller percentage of 2.74%. Beyond such a pre-diction gap between the from-scratch model and fine-tuning models, we do see a gap in attended regions. Fig. 1 shows that the from-scratch model that predicts correctly pays at-tention to the feet, but fine-tuning models that misclassify concentrate on body-related features, possibly mislead by
ImageNet pre-trained models. The devil that accounts for such gaps lies in those underperforming features offered by the pre-trained model, among which we focus on the following two types based on our empirical studies.
Type I (Rare features): These undertrained features by the pre-training dataset force the attention of the fine-tuned model to be diverted to those well-trained features, although the most discriminative features for the downstream dataset are exactly those undertrained ones.
Fig. 2a demonstrates a slice of rare features, i.e., neck/tail features. Concretely, we mask an original image via Gaus-sian blurring with two patches preserved, and retrieve its most similar images. The five most similar images retrieved by neck/tail are unfortunately distant (evidenced by colorful necks instead of the expected dark neck) using either pre-trained features (before fine-tuning) or fine-tuned features
(a) Top-5 similar images retrieved given a masked image. (b) The union of top-k predictive probability scores by different methods, where the star marks the ground-truth class index 3.
Figure 2: (a) Retrieval of the most similar images based on the cosine similarity between features of the masked image and another one. (b) Given the masked image in (a), the model trained from scratch predicts correctly but both fine-tuning and linear probing fail. (after fine-tuning), advocating that the fine-grained neck features are undertrained in the coarse-grained pre-training dataset of ImageNet. As a consequence, both linear prob-ing that only fine-tunes the classification head and vanilla fine-tuning lose the attention of the discriminative neck fea-tures, resulting in incorrect predictions, while training from scratch where all features are initialized to be unanimously uninformative succeeds.
Type II (Spuriously correlated features): The mislead-ing correlations in the pre-training dataset constitute another source that diverts the attention of the fine-tuned model.
In Fig. 3, we show head features and bird feeder features as a pair of spuriously correlated features. For the masked image 1 with only the two patches of head and tail preserved, all three models make correct predictions; nonetheless, in-cluding one more patch describing the bird feeder in the masked image 2 significantly alters the predictions by fine-tuning and linear probing, despite having no influence on that by the from-scratch model. This can be explained by the spurious correlations between head and bird feeder features that exist in another bird class of the pre-training dataset.
This work sets out to develop a fine-tuning strategy that alleviates the negative effects of these two types of under-performing pre-trained features. First, we conclude that maximizing the mutual information between examples in the same class with respect to a particular slice of rare features is contributory to draw the attention to refining rare features to be discriminative. Second, by investigating the causal models of predictions after fine-tuning, we identify that the pre-training dataset as a confounder explains the negative
Figure 3: An examplar pair of spuriously correlated features, where the model trained from scratch predicts consistently after including one more patch in the masked image 2. Un-fortunately, fine-tuning and linear probing predict incorrectly with the introduction of the same patch. transfer by spuriously correlated features. To this end, we propose to deconfound by the principled front-door adjust-ment rule. In light of the key role of a patch implementing a slice of features and the fact that each patch usually encrypts a concept, we dub the proposed approach “Concept-Tuning”.
We summarize our contributions as follows.
• We have identified two specific types of underperform-ing pre-trained features that give rise to negative transfer and revealed the root cause of their negative impacts, upon which more principled fine-tuning techniques can be developed.
• To our best knowledge, Concept-Tuning is the first that fine-tunes concept-level sliced features, theoretically analyzed to offer high invariance to the confounder of the pre-training dataset.
• Concept-Tuning improves the state-of-the-art sample-level fine-tuning methods by a large margin (up to 4.76%) and with consistency in eight classification datasets, seven pre-trained models (supervised and self-supervised), three network architectures, and different sample sizes in downstream tasks. Moreover, Concept-Tuning can extend well to semantic segmentation and domain generalization tasks. 2.