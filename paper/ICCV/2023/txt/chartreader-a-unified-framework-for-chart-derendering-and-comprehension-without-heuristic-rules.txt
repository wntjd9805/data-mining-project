Abstract
Charts are a powerful tool for visually conveying com-plex data, but their comprehension poses a challenge due to the diverse chart types and intricate components. Ex-isting chart comprehension methods suffer from either heuristic rules or an over-reliance on OCR systems, re-sulting in suboptimal performance. To address these is-sues, we present ChartReader, a unified framework that seamlessly integrates chart derendering and comprehension tasks. Our approach includes a transformer-based chart component detection module and an extended pre-trained vision-language model for chart-to-X tasks. By learning the rules of charts automatically from annotated datasets, our approach eliminates the need for manual rule-making, re-ducing effort and enhancing accuracy. We also introduce a data variable replacement technique and extend the in-put and position embeddings of the pre-trained model for cross-task training. We evaluate ChartReader on Chart-to-Table, ChartQA, and Chart-to-Text tasks, demonstrating its superiority over existing methods. Our proposed framework can significantly reduce the manual effort involved in chart analysis, providing a step towards a universal chart under-standing model. Moreover, our approach offers opportuni-ties for plug-and-play integration with mainstream LLMs such as T5 and TaPas, extending their capability to chart comprehension tasks.1 1.

Introduction
The adage, “a picture is worth a thousand words,” under-scores the immense value of charts found on various web-sites and articles, which often depict knowledge that can-not be conveyed through words alone. Chart derendering, which refers to the conversion of charts into tables (i.e.,
Chart-to-Table [11, 41, 42]), is widely viewed as essential in facilitating a range of downstream tasks, such as chart
*Corresponding author 1Code is at https://github.com/zhiqic/ChartReader
Figure 1. Illustration of chart derendering and comprehension tasks. The Chart-to-Table task aims to transform a chart into a machine-readable table, while ChartQA and Chart-to-Text tasks involve answering questions and summarizing the content of the chart, respectively. [Best viewed in color]. question-answering (ChartQA [27, 45, 66]) and chart sum-marization (Chart-to-Text [7, 15, 48]). As shown in Fig-ure 1, the Chart-to-Table task aims to recognize the chart as a machine-readable table, while ChartQA and Chart-to-Text tasks involve answering pre-set questions and summarizing the content of the chart, respectively. The interdependence and mutual value of these research tasks have also been em-phasized in earlier studies [22, 58, 61], underscoring their critical role in chart comprehension research.
Despite the critical role of chart comprehension, existing research has failed to address the three sub-tasks separately, let alone propose a universal solution. As depicted in Fig-ure 2, charts come in various types, each designed to con-vey domain-specific knowledge, and can exhibit intricate components, texture variations, and speckled backgrounds.
Confronted with such complex charts, existing chart com-prehension methods face two main problems.
Firstly, existing chart derendering approaches [4, 11, 18, 41, 42] (i.e., Chart-to-Table) resort to heuristic rules that demand extensive domain knowledge and effort to formu-late. For example, ChartOCR [42], a pioneering method, requires chart classification to identify categories first and then detects different components using various pre-defined heuristic rules. To avoid complicated rule-making, some methods even try only one set of limited chart types, such as
Figure 2. Demonstrates the complexity of charts in EC400K [42], which can vary in type, design, and visual properties. Charts can contain intricate components, texture variations, and speckled backgrounds, posing challenges for chart comprehension. [Best viewed in color]. bar [13, 50] and line [30, 43] charts. These limitations im-pede the ability to extract data for unknown categories. As a result, many latest methods [40, 45] even use tables directly from ground truth to complete the answers and summary tasks. It is evident that these studies are challenging to au-tomate and struggle to extract data from real-world charts.
On the other hand, current chart comprehension meth-ods, such as Chart-to-Text
[7, 15, 48] and ChartQA [29, 36, 44, 45, 54], often heavily rely on off-the-shelf OCR sys-tems or pre-extracted tables from the ground truth. By treat-ing chart derendering as a black box, these methods neglect the visual and structural information of the charts, result-ing in the following issues: (1) Chart-to-Text and ChartQA tasks devolve into text-only quizzes [39, 44], as they can-not extract visual semantics from chart derendering. This explains why OCR-based and end-to-end methods, such as LayoutLM [64], PresSTU [31], PaLI [8], CoCa [65],
Donut [32], and Dessurt [14], have shown suboptimal re-sults in chart understanding. (2) Chart-to-table tasks do not benefit from chart comprehension tasks. Due to the lack of understanding to the visual semantics in charts, existing sys-tems, such as those using OCR systems [39, 42], struggle to accurately convert charts to tables. Overall, we contend that the problems with chart comprehension arise from an over-reliance on predefined rules and a lack of a universal framework to support multi-tasking.
In light of the previous analysis, it seems that a visual-language model is a promising direction for building a uni-versal framework. However, while Pix2Struct [35], a pre-training strategy for visually-situated language, has shown superior performance over OCR-based models [2, 5, 37, 60], it is not suitable for chart derendering. Moreover, despite recognizing this issue, Metcha [40] had to rely on Pix2Struct as a backbone due to the lack of better visual-language models. Nonetheless, neither Pix2Struct nor Metcha address the two issues identified earlier: 1) ex-cessive reliance on heuristic rules in table derendering, and 2) heavy reliance on existing OCR systems despite attempts to incorporate additional chart comprehension tasks.
To overcome these concerns, we introduce ChartReader, a unified framework that seamlessly integrates chart deren-dering and comprehension tasks. Our approach comprises a rule-free chart component detection module and an ex-tended pre-trained vision-language model for chart-to-X (text/table/QA) tasks. Unlike heuristic rule-based meth-ods, our approach leverages a transformer-based approach to detect the location and type of chart components, en-abling automatic rule learning by processing existing an-notated datasets. To enhance cross-task training, we ex-tend the input and position embeddings of the pre-trained model and introduce a data variable replacement technique.
Specifically, we standardize chart-to-X (table/text) tasks as question-answering problems, allowing us to solve multi-ple chart understanding tasks effectively. Additionally, the model generates the data variable instead of the actual value to avoid errors and hallucinations, which improves the con-sistency in multi-task training. Our approach represents a step towards a unified chart understanding model, as vali-dated through experiments. The proposed framework has the potential to reduce the manual effort involved in chart analysis, paving the way for more efficient and accurate chart comprehension.
To summarize, our contributions are: 1) a unified frame-work that seamlessly integrates chart derendering and com-prehension tasks; 2) a rule-free chart component detection module that leverages a transformer-based approach to au-tomatically learn the rules of charts; 3) extending the input and position embeddings of the pre-trained LLMs and em-ploying a data variable replacement technique to improve cross-task training; 4) validating our approach through ex-periments, demonstrating significant improvement over ex-isting methods in chart understanding tasks. 2.