Abstract
Machine learning models are known to be susceptible to adversarial perturbation. One famous attack is the adversar-ial patch, a particularly crafted sticker that makes the model mispredict the object it is placed on. This attack presents a critical threat to cyber-physical systems that rely on cam-eras such as autonomous cars. Despite the signiﬁcance of the problem, conducting research in this setting has been difﬁcult; evaluating attacks and defenses in the real world is exceptionally costly while synthetic data are unrealistic.
In this work, we propose the REAP (REalistic Adversarial
Patch) benchmark, a digital benchmark that enables the eval-uations on real images under real-world conditions. Built on top of the Mapillary Vistas dataset, our benchmark con-tains over 14,000 trafﬁc signs. Each sign is augmented with geometric and lighting transformations for applying a digi-tally generated patch realistically onto the sign. Using our benchmark, we perform the ﬁrst large-scale assessments of adversarial patch attacks under realistic conditions. Our experiments suggest that patch attacks may present a smaller threat than previously believed and that the success rate of an attack on simpler digital simulations is not predictive of its actual effectiveness in practice. Our benchmark is released publicly at https://github.com/wagner-group/ reap-benchmark. 1.

Introduction
Research has shown that machine learning models lack ro-bustness against adversarially chosen perturbations. Szegedy et al. [54] ﬁrst demonstrated that one can engineer perturba-tions that are indiscernible to the human eye yet that cause neural networks to misclassify images with high conﬁdence.
*Equal contribution. 1
Since then, there has been a large body of academic work on understanding the robustness of neural networks to such attacks [18, 39, 55, 8, 27, 57, 36, 7, 22].
One particularly concerning type of attack is the adversar-ial patch attack [6, 17, 24, 53, 10, 23, 33, 43, 52, 66, 21, 61].
These are real-world attacks, where the attacker’s objective is to print out a patch, physically place it in a scene, and cause a vision network processing the scene to malfunction. These attacks are especially concerning because of the potential im-pact on autonomous vehicles. A malicious agent could, for instance, produce a sticker that, when placed on a stop sign, cause a self-driving car to believe it is (say) a speed limit sign, and fail to stop. Indeed, similar attacks have already been demonstrated both in academic settings [31, 16, 50] and on real-world autonomous vehicles [56].
Despite this signiﬁcant risk, research on these attacks has stalled to a certain extent because quantitatively evaluating the signiﬁcance of this threat is challenging. The most accu-rate approach would be to conduct real-world experiments, but they are very expensive, and at present, not practical to do at a large scale. This leaves much to be desired compared to other branches of computer vision research, where the availability of benchmarks such as ImageNet have reduced the barriers to research and spurred tremendous innovation.
Instead, researchers turn to one of two techniques: either, they physically create their attacks and try them out on a small number of real-world examples by physically attach-ing them to objects, or they digitally evaluate patch attacks using digital images containing simulated patches. Both approaches have major drawbacks. Although the former simulates more realistic conditions, the sample size is very small, and typically one cannot draw statistical conclusions from the results [6, 17, 53, 10, 66, 20, 21, 61]. Addition-ally, because of the ad-hoc nature of these evaluations, it is impossible to compare the results across different papers. Ul-timately, such experiments only serve as a proof of concept for the proposed attacks and defenses, but not as a rigorous evaluation of their effectiveness.
In contrast, a digital simulation of attacks/defenses allows quantitative evaluation [24, 33, 65, 46, 37, 62, 58, 44]. How-ever, it is difﬁcult to accurately capture all of the challenges that arise in the real world. Past work often made unrealistic assumptions, such as that the patch is square, axis-aligned, can be located anywhere on the image, fully under the con-trol of the attacker, and ignore noise and variation in lighting and pose (see top row of Fig. 1). Consequently, it is unclear if these evaluations are actually reﬂective of what would happen in real-world scenarios. 1.1. Our Contributions
The REAP Benchmark: We propose REalistic Adversarial
Patch Benchmark (REAP), the ﬁrst large-scale standardized benchmark for security against patch attacks. Motivated by the aforementioned shortcomings of prior evaluations, we design REAP with the following principles in mind: 1. Large-scale evaluation: REAP consists of 14,651 im-ages of road signs drawn from the Mapillary Vistas dataset. This allows us to draw quantitative conclusions about the effectiveness of attacks/defenses on the dataset. 2. Realistic patch rendering: REAP has tooling, which, for every road sign in the dataset, allows us to realistically render any digital patch onto the sign, matching factors such as where to place the patch, the camera angle, light-ing conditions, etc. Importantly, this transformation is fast and differentiable so one can still perform backprop-agation through the rendering process. 3. Realistic image distribution: REAP consists of images taken under realistic conditions, including variation in sizes and distances from the camera as well as various lighting conditions and degrees of occlusion.
Evaluations with REAP: With our new benchmark in hand, we also perform the ﬁrst large-scale evaluations of existing attacks on object detectors. We evaluate existing attacks on three different object detection architectures: Faster R-CNN [48], YOLOF [9], and DINO [64]. We also implement and evaluate a baseline defense adapted from adversarial training [36] to defend against patch attacks on object detec-tion. The conclusions we ﬁnd are: 1. Existing patch attacks are not that effective. Perhaps surprisingly, existing attacks do not succeed on a major-ity of images on our benchmark. This is in contrast to simpler attack models such as `p-bounded perturbations or patch attacks on simpler benchmarks, where the at-tack success rate is near 100%. Moreover, adversarially trained models can almost completely stop the attacks with only a minor performance drop on benign data. 2. Performance on synthetic data is not reﬂective of per-formance on REAP. We ﬁnd that the success rates of attacks on synthetic versions of our benchmark and the full REAP are only poorly correlated. We conclude that performance on simple synthetic benchmarks is not pre-dictive of attack success rate in more realistic conditions. 3. Lighting and patch placement are particularly impor-tant. Finally, we investigate what transforms in the patch rendering are the most important, in terms of the effect on the attack success rate. We ﬁnd that the most signif-icant ﬁrst-order effects are from the lighting transform, as well as the positioning of the patch. In contrast, the perspective transforms—while still important—seem to affect the attack success rate somewhat less.
While we believe these conclusions are already quite inter-esting, they are only the tip of the iceberg of what can be done with REAP. We believe that REAP will help support future research in adversarial patches by enabling a more accurate evaluation of new attacks and defenses.
2.