Abstract
Large Pre-trained Transformers exhibit an intriguing ca-pacity for in-context learning. Without gradient updates, these models can rapidly construct new predictors from demonstrations presented in the inputs. Recent works pro-mote this ability in the vision-language domain by incorpo-rating visual information into large language models that can already make in-context predictions. However, these methods could inherit issues in the language domain, such as template sensitivity and hallucination. Also, the scale of these language models raises a significant demand for computations, making learning and operating these mod-els resource-intensive. To this end, we raise a question:
“How can we enable in-context learning without relying on the intrinsic in-context ability of large language models?”.
To answer it, we propose a succinct and general frame-work, Self-supervised IN-Context learning (SINC), that in-troduces a meta-model to learn on self-supervised prompts consisting of tailored demonstrations. The learned mod-els can be transferred to downstream tasks for making in-context predictions on-the-fly. Extensive experiments show that SINC outperforms gradient-based methods in various vision-language tasks under few-shot settings. Further-more, the designs of SINC help us investigate the benefits of in-context learning across different tasks, and the analysis further reveals the essential components for the emergence of in-context learning in the vision-language domain. 1.

Introduction
Large language models such as GPT-3 [6] are able to per-form in-context learning (ICL): given a prompt consisting of a series of demonstrations and a query data as input, the model can generate the corresponding prediction without any parameter updates. Meanwhile, recent works show that large vision-language (VL) models [2, 67] can also possess such an ability. Specifically, these models can rapidly in-Figure 1. Illustration of SINC. A meta-model is introduced for acquiring in-context ability given features extracted from gen-eral models. During pre-training, the meta-model is learned on prompts constructed in a self-supervised manner. Our designs jointly enable the transfer of in-context ability to the downstream. corporate multimodal information with few demonstrations for tackling a variety of downstream tasks, such as image captioning [9], visual question answering [22], and fast con-cept binding [6]. Behind the success, the shared scheme of these approaches is to incorporate visual information into large language models via proposed modules. In particu-lar, the in-context ability of these VL models would signif-icantly rely on the language side. As such, the issues in the language domain could be inherited, such as template sensitivity [40, 52] and hallucination [27]. Previous stud-ies [19, 2, 6, 56, 67] also indicate that the in-context ability scales with the model sizes and barely emerges in smaller models [6]. This property requires current methods to be built upon large language models for leveraging in-context demonstrations. Although previous works typically freeze language models for training efficiency, the language mod-dictions based on demonstrations, with the transferability across tasks. In particular, we learn the meta-model on tai-lored prompts comprising sequences of data and label repre-sentations. For constructing data-label pairs in the prompts, inspired by the literature of question answering [20, 50, 71], we regard data sharing similar missing semantics as ho-mogeneous and group them as a class. This strategy en-ables us to create diverse labels from unannotated image-text pairs. However, we identify that the predictions from models would be agnostic to the demonstrations if there is no adequate correlation with the query data. Therefore, we leverage the idea from few-shot learning [11, 62] to cre-ate specific prompts to trigger the model for utilizing the demonstrated information. Furthermore, in our observa-tions, downstream tasks would demand the in-context abil-ity to different degrees. We thus propose learning different prompts with a controllable ratio to better benefit and study different tasks. Regarding the formation of representations, on the data side, we propose incorporating pre-trained mod-els from various domains, where the produced features are further aggregated with the proposed multi-source feature fuser (MFF). On the label side, the representations are com-posed of subword embeddings [53] of label descriptions, enabling the generalization to unseen labels. Overall, our representation-level in-context learner could be transferred to different scenarios after pre-training, as shown in Fig. 1.
A comparison of our architecture with prior works [2, 67] is depicted in Fig. 2, wherein prior works either (a) prepend a learnable vision encoder or (b) interleave adapter mod-ules to the large language models for ICL. In contrast, we achieve ICL by introducing the meta-model after the frozen models, enabling us to prepare the representations on sepa-rate devices or in an offline manner. This scheme exempts the frozen models from all the backward processes, thereby significantly alleviating the computation burden. The main contributions of this paper are summarized as follows.
• We propose a novel framework, SINC, that decouples the acquisition of ICL from VL pre-training, enabling
ICL in a more manageable and extensible way with-out relying on the intrinsic in-context ability of large language models.
• We propose to learn a meta-model on self-supervised prompts consisting of tailored demonstrations. The learned models can be transferred to downstream tasks for making in-context predictions on-the-fly.
• Extensive experiments show that SINC outperforms previous gradient-based methods and a strong ICL baseline. The analysis further reveals the properties and essential components for ICL in the VL domain.
Figure 2. Architectural comparison. Previous works (a) [67] and (b) [2] achieve in-context learning for VL tasks with large lan-guage models. Our SINC relieves such a constraint by introducing a meta-model for acquiring the in-context ability. els are still involved in the learning process, creating a non-negligible demand for resources to learn or operate these models [64]. Moreover, the length of demonstrations could readily exceed the practical limitation of most Transformer models [78], especially for vision-language data.
To this end, we pose a challenging research question:
“How can we enable in-context learning without relying on the intrinsic in-context ability of large language models?”
The access to the solution could lie in the understanding of
ICL properties in large language models. Previous studies have shown that the formats of demonstrations can affect performance drastically [37, 40, 79, 81]. Furthermore, [43] shows that randomly assigning labels for the demonstra-tions could barely decrease the performance, while [77] jus-tifies that the phenomenon is not generalized across tasks.
In view of these obscure observations, recent works attempt to study ICL specifically from different perspectives. [1, 14] show that Transformers trained from scratch can implicitly implement the gradient descent algorithm in their forward pass, and the number of attention layers [82] could further relate to the equivalent learning steps. [7, 56], in another way, study the effects of training data for language mod-els. The results reveal that the emergence of ICL could be attributed to certain language properties such as bursti-ness [33, 54]. Overall, these studies suggest that the in-context ability of language models results from multiple factors. Moreover, this ability could be incidental since typ-ical language modeling is not intentionally designed based on these factors. Thus, models could exhibit unexpected behaviors, and the acquisition of in-context ability could be inefficient in terms of model capacity.
To address previous limitations, we present a general framework, named Self-supervised IN-Context learning (SINC). The core idea is to decouple the acquisition of in-context ability from conventional VL pre-training and incentivize it through both architectural and data perspec-tives. Specifically, we introduce a meta-model as the in-context learner that directly operates on the representations produced from frozen models. A self-supervised learning scheme is proposed to enable the meta-model to make pre-2.