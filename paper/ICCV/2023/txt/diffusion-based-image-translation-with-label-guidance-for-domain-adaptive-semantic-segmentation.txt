Abstract
Existing Methods
Ours
Semantic 
Consistency
Translating images from a source domain to a target do-main for learning target models is one of the most com-mon strategies in domain adaptive semantic segmentation (DASS). However, existing methods still struggle to preserve semantically-consistent local details between the original and translated images. In this work, we present an innova-tive approach that addresses this challenge by using source-domain labels as explicit guidance during image transla-tion. Concretely, we formulate cross-domain image transla-tion as a denoising diffusion process and utilize a novel Se-mantic Gradient Guidance (SGG) method to constrain the translation process, conditioning it on the pixel-wise source labels. Additionally, a Progressive Translation Learning (PTL) strategy is devised to enable the SGG method to work reliably across domains with large gaps. Extensive exper-iments demonstrate the superiority of our approach over state-of-the-art methods. 1.

Introduction
Semantic segmentation is a fundamental computer vision task that aims to assign a semantic category to each pixel in an image. In the past decade, fully supervised methods
[47, 59, 29, 84] have achieved remarkable progress in se-mantic segmentation. However, the progress is at the cost of a large number of dense pixel-level annotations, which are signiﬁcantly labor-intensive and time-consuming to ob-tain [3, 47]. Facing this problem, one alternative is to lever-age synthetic datasets, e.g., GTA5 [57] or SYNTHIA [60], where the simulated images and corresponding pixel-level labels can be generated by the computer engine freely. Al-though the annotation cost is much cheaper than manual labeling, segmentation models trained with such synthetic datasets often do not work well on real images due to the large domain discrepancy.
*Corresponding Author
Translation 
Model guide
Translation 
Model (a) Architecture Comparison (cid:3843) (cid:3844) (cid:3845)
Source Semantics (cid:3843) (cid:3844) (cid:3845)
Existing Method (cid:3843) (cid:3844) (cid:3845) (cid:3843) (cid:3844) (cid:3845) (cid:3843) (cid:3844) (cid:3845)
Ours (cid:3843) (cid:3844) (cid:3845)
Road
Side.W Build Wall
Fence
Pole Tr. Light Tr. Sign Veg.
Terrain
Sky
Person
Rider
Car
Truck
Bus
Train Motor
Bike
N/A (b) Results Comparison
Figure 1. (a) Architecture comparison between different image translation methods for DASS. Existing methods mainly translate images based on the input image. Our method further introduces the source semantic label to guide the translation process, making the translated content semantically consistent with the source la-bel (as well as the source image). (b) Results comparison in detail.
First row: the source semantic label. Second row: the translated image of an existing method [16], which is a state-of-the-art image translation method based on GAN. Third row: the translated image of our method. The translated image of the existing method shows unsatisfactory local details (e.g., in case (cid:2) of the second row, the trafﬁc light and its surrounding sky are translated into building).
In contrast, our method preserves details in a ﬁne-grained manner.
The task of domain adaptive semantic segmentation (DASS) aims to address the domain discrepancy problem by transferring knowledge learned from a source domain to a target one with labels provided for the source domain
only. It is a very challenging problem as there can be a large discrepancy (e.g., between synthetic and real domains), re-sulting in great difﬁculties in knowledge transfer. In DASS, one type of methods [24, 7, 61, 5, 83, 12, 48, 68, 49, 66, 72] focuses on aligning the data distributions from different do-mains in the feature space, which obtains promising perfor-mance, while another type of approaches [86, 87, 40, 81, 38, 42] aims to generate pseudo labels for unlabeled tar-get images, also making great achievements. Besides above approaches, image translation [85] is also an effective and important way to handle the DASS problem. It focuses on translating the labeled source images into the target domain, then using the translated images and source labels to train a target-domain segmentation model. In the past few years, lots of DASS studies [54, 23, 6, 44, 16] have been conducted based on image translation. This is because image trans-lation allows for easy visual inspection of the adaptation method’s results, and the translated images can be saved as a dataset for the future training of any model, which is con-venient. Existing image translation methods mainly adopt generative adversarial networks (GANs) [19] to translate images across a large domain gap. However, due to the intractable adversarial training, GAN models can be notori-ously hard to train and prone to show unstable performance
[62, 53]. Furthermore, many studies [51, 11, 55] have also shown that GAN-based methods struggle with preserving local details, leading to semantic inconsistencies between the translated images and corresponding labels, as shown in the second row of Fig. 1 (b). Training the segmentation model on such mismatched image and label pairs will cause sub-optimal domain adaptation performance. Facing these inherent limitations, the development of GAN-based image translation methods is increasingly encountering a bottle-neck. Yet, there is currently little research exploring alter-native approaches to improve image translation for DASS beyond GANs.
Denoising Diffusion Probabilistic Models (DDPMs), also known as diffusion models, have recently emerged as a promising alternative to GANs for various tasks, such as im-age generation [11, 69], image restoration [73, 79], and im-age editing [1, 52], etc. Diffusion models have been shown
In-to have a more stable training process [11, 55, 22]. spired by its strong capability, we propose to exploit the diffusion model for image translation in DASS. Speciﬁ-cally, based on the diffusion technique, we propose a label-guided image translation framework to preserve local de-tails. We observe that existing image translation methods
[23, 54, 6, 44, 41] mainly focus on training a translation model with image data alone, while neglecting the utiliza-tion of source-domain labels during translation, as shown in Fig. 1 (a). Since source labels can explicitly indicate the semantic category of each pixel, introducing them to guide the translation process should improve the capability of the translation model to preserve details. A straightfor-ward idea of incorporating the source labels is to directly train a conditional image translation model, where the trans-lated results are semantically conditioned on the pixel-wise source labels. However, it is non-trivial since the source labels and target images are not paired, which cannot sup-port the standard conditional training. Recent advances
[11, 55, 58] have shown that a pre-trained unconditional diffusion model can become conditional with the help of the gradient guidance [11]. The gradient guidance method can guide the pre-trained unconditional diffusion model to provide desired results by directly affecting the inference process (without any training). In light of this, we propose to ﬁrst train an unconditional diffusion-based image trans-lation model, and then apply gradient guidance to the im-age translation process, making it conditioned on source la-bels. However, we still face two challenges: (1) Traditional gradient guidance methods generally focus on guiding the diffusion model based on image-level labels (i.e., classiﬁ-cation labels), while the DASS problem requires the im-age translation to be conditioned on pixel-level labels (i.e., segmentation labels). (2) The gradient guidance methods typically work within a single domain, whereas the DASS task requires it to guide the image translation across dif-ferent domains. To address the ﬁrst challenge, we pro-pose a novel Semantic Gradient Guidance method (SGG), which can precisely guide the image translation based on
ﬁne-grained segmentation labels. To tackle the second chal-lenge, we carefully design a Progressive Translation Learn-ing strategy (PTL), which is proposed to drive the SGG method to reliably work across a large domain gap. With the help of SGG and PTL, our framework effectively han-dles the image translation for DASS in a ﬁne-grained man-ner, which is shown in the third row of Fig. 1 (b).
In summary, our contributions are three-fold. (i) We pro-pose a novel diffusion-based image translation framework with the guidance of pixel-wise labels, which achieves im-age translation with ﬁne-grained semantic preservation. To the best of our knowledge, this is the ﬁrst study to exploit the diffusion model for DASS. (ii) We devise a Seman-tic Gradient Guidance (SGG) scheme accompanied with a
Progressive Translation Learning (PTL) strategy to guide the translation process across a large domain gap in DASS. (iii) Our method achieves the new state-of-the-art perfor-mance on two widely used DASS settings. Remarkably, our method outperforms existing GAN-based image translation methods signiﬁcantly on various backbones and settings. 2.