Abstract
One of the main challenges in LiDAR-based 3D ob-ject detection is that the sensors often fail to capture the complete spatial information about the objects due to long distance and occlusion. Two-stage detectors with point cloud completion approaches tackle this problem by adding more points to the regions of interest (RoIs) with a pre-trained network. However, these methods generate dense point clouds of objects for all region proposals, assum-ing that objects always exist in the RoIs. This leads to the indiscriminate point generation for incorrect propos-als as well. Motivated by this, we propose Point Gen-eration R-CNN (PG-RCNN), a novel end-to-end detector that generates semantic surface points of foreground ob-jects for accurate detection. Our method uses a jointly trained RoI point generation module to process the contex-tual information of RoIs and estimate the complete shape and displacement of foreground objects. For every gen-erated point, PG-RCNN assigns a semantic feature that indicates the estimated foreground probability. Extensive experiments show that the point clouds generated by our method provide geometrically and semantically rich infor-mation for reﬁning false positive and misaligned propos-als. PG-RCNN achieves competitive performance on the
KITTI benchmark, with signiﬁcantly fewer parameters than state-of-the-art models. The code is available at https:
//github.com/quotation2520/PG-RCNN . 1.

Introduction 3D object detection using LiDAR point clouds is a fun-damental perception task in autonomous driving that has re-ceived signiﬁcant attention in recent years. LiDAR sensors are frequently used in many 3D applications, such as odom-etry and mapping [41, 26, 17], object tracking [27, 30, 29],
*Denote equal contribution (a) (b)
Figure 1. The intermediate outputs of PG-RCNN: (a) ﬁrst stage output with initial bounding box proposals in red, and (b) sec-ond stage outputs with generated points over 0.6 foreground score in yellow and ﬁnal detection outputs in green bounding boxes.
Ground truth bounding boxes are shown in blue. and detection [34, 19, 4], due to their ability to provide ac-curate distance information in various conditions.
LiDAR-based 3D object detectors use either a point-based [15, 16, 22, 14] or a voxel-based [43, 34, 13] net-work to generate bounding boxes for foreground objects.
The two-stage framework with a proposal reﬁnement stage is often adopted in many detectors to enhance the detection accuracy [20, 35]. While researchers have explored differ-ent methods [19, 4] to extract effective reﬁnement features for the regions of interest (RoIs), some of the most recent works with voxel-based backbones [18, 8] revisit the point information within the RoIs at the reﬁnement stage, consid-ering the precise coordinates and density of internal points. the inherent sparsity of LiDAR point clouds poses a challenge in 3D object detection, particu-larly for distant and occluded objects. These objects have fewer collected points, making them difﬁcult to detect and degrading the overall performance of detectors [12, 32]. To address this problem, point cloud completion methods have been explored to assist proposal reﬁnement by adding more points to the RoIs. The methods in [12, 42] enhance the resolution of point clouds by utilizing a point cloud com-pletion network pre-trained from an external dataset [1], but
Nevertheless,
they only take point coordinates pooled from the proposal bounding box into account during the generation process.
As a result, they fail to capture the contextual information of the surroundings and indiscriminately produce dense point clouds for all proposals, including incorrect proposals.
Motivated by this, we propose the Point Generation R-CNN (PG-RCNN), an end-to-end two-stage 3D object de-tection method that can extract geometrically and semanti-cally rich proposal reﬁnement features via semantic surface point generation. Our method includes the RoI point gener-ation (RPG) module that estimates the actual shape and dis-placement of foreground objects, using primitive RoI fea-tures aggregated from the backbone as input. Note that we jointly train the RPG module using auxiliary supervision from given data without introducing any external dataset.
While previous point cloud completion methods only output sets of spatial coordinates, our method goes beyond that by assigning a semantic feature to each generated point, which represents its estimated probability of belonging to the fore-ground. These characteristics allow our novel point gener-ation method to produce more informative point clouds for object detection. Figure 1 shows the intermediate outputs of our method. PG-RCNN generates points with different foreground scores, presenting high-conﬁdence foreground points for true positive proposals. The generation points intuitively express the predicted location and shape of the objects, visualizing the reasoning process of PG-RCNN.
We demonstrate the effectiveness of our method with extensive experiments on the KITTI dataset [5].
PG-RCNN achieves competitive performance with state-of-the-art models while signiﬁcantly reducing the computational cost. Qualitative results show that our approach better serves the purpose of reﬁning false-positive or misaligned proposals compared to previous point cloud completion methods.
In summary, our main contributions are:
• We present PG-RCNN, a novel two-stage 3D object detection method for LiDAR point clouds. In the pro-posal reﬁnement stage, our method generates semantic surface points with foreground probabilities to extract shape-aware, semantically rich reﬁnement features.
• We compare the point generation results of PG-RCNN to a previous point cloud completion approach and show that our method generates more effective points for object detection.
• PG-RCNN achieves competitive performance on the
KITTI benchmark, with a signiﬁcantly smaller number of parameters and inference time than the state-of-the-art models. 2.