Abstract
Image-to-text generation aims to describe images us-ing natural language. Recently, zero-shot image caption-ing based on pre-trained vision-language models (VLMs) and large language models (LLMs) has made significant progress. However, we have observed and empirically demonstrated that these methods are susceptible to modal-ity bias induced by LLMs and tend to generate descrip-tions containing objects (entities) that do not actually ex-ist in the image but frequently appear during training (i.e.,
In this paper, we propose ViECap, object hallucination). a transferable decoding model that leverages entity-aware decoding to generate descriptions in both seen and unseen scenarios. ViECap incorporates entity-aware hard prompts to guide LLMs’ attention toward the visual entities present in the image, enabling coherent caption generation across diverse scenes. With entity-aware hard prompts, ViECap is capable of maintaining performance when transferring from in-domain to out-of-domain scenarios. Extensive ex-periments demonstrate that ViECap sets a new state-of-the-art cross-domain (transferable) captioning and performs competitively in-domain captioning compared to previous
VLMs-based zero-shot methods. Our code is available at: https://github.com/FeiElysia/ViECap 1.

Introduction
Large-scale pre-trained vision-language models (VLMs) like CLIP [37] and ALIGN [22] showcase impressive zero-shot transferability in various discriminative downstream tasks (e.g., classification [37], segmentation [26, 49], and detection [16, 27]). However, effectively adapting these pre-trained VLMs into zero-shot generative tasks (e.g., text and image generation) remains an open question that re-quires further exploration. Recently, some works [43, 42] have leveraged large language models (LLMs), e.g.,
Figure 1. Decoding paradigm for zero-shot image-to-text genera-tion. (a) in-domain (ID) image, (b) out-of-domain (OOD) image.
ID refers to objects appearing in the image included in the train-ing corpus, while OOD indicates that they are not included. ✕ and ✓ refers to the incorrect and correct predictions, respectively.
Late-guidance methods generate descriptions irrelevant to the im-age, e.g., “jump” and “donut”, while early-guidance models often tend to hallucinate objects that are not actually present in the OOD image, e.g., “surfboard”. In contrast, our model utilizes entities as additional prompts to describe novel objects in the image, leading to superior transferability in OOD settings, e.g., “sea turtle”.
GPT [38, 5], to achieve CLIP-based zero-shot image-to-text generation. They follow a late-guidance paradigm where visual information is injected after completing word predic-tion. However, the weak visual guidance in this paradigm often results in modality bias, i.e., the language prior in
LLMs dominates the decoding process and therefore gen-erates descriptions that are unrelated to the corresponding images. Fig. 1(a) shows incorrect predictions made by late-guidance decoding, e.g., even if “jump” and “cobblestone” are unrelated to the image, they finally appear in the pre-dictions due to their close association with predicted words
“skateboarder” and “snowy”. Similarly, another example in Fig. 1(b) shows that “donut” and “rocks” are primarily generated by language prior instead of visual guidance.
∗ Equal contribution † Corresponding author
Early-guidance methods [28, 35, 53] provide explicit
guidance for word generation in LLMs by prefixing visual prompts to the text tokens. Typically, visual prompts are projected from the CLIP image embedding using a learn-able projector. This early-guidance paradigm significantly alleviates the modality bias and boosts the alignments be-tween the image and the generated captions. However, the learnable (soft) visual prompts are prone to overfitting when trained on a limited corpus, leading to poor performance in describing a diverse range of objects (visual entities).
This, in turn, may cause object hallucination in the gener-ated captions. Specifically, when transferring these models to unseen scenarios beyond the training corpus, novel en-tities are often misrecognized as similar entities frequently appearing in the training corpus. As Fig. 1 shows, early-guidance decoding is capable of understanding in-domain (ID) images but tends to hallucinate entities that do not ac-tually exist in out-of-domain (OOD) images (i.e., halluci-nating “sea turtle” with “surfboard”, where “surfboard” frequently appears in the training corpus). Consequently, the transferability of the well-learned CLIP latent space is degraded into current decoding strategies, significantly lim-iting their applicability in real-world scenarios. We further validate the observed modality bias and object hallucina-tion issues when adapting pre-trained VLMs and LLMs for image-to-text generation through experiments in Sec. 3.
To address the observed issues, we propose ViECap, which incorporates entity-aware hard prompts to compen-sate for the degradation of the CLIP latent space caused by learning soft prompts on a specific training corpus. This method is motivated by our observation that the CLIP-based entity classifier can accurately classify both ID and OOD images (e.g., “snowboard” and “sea turtle” in Fig. 1).
The entity-aware hard prompts enable transferable language decoding from the CLIP latent space. Fig. 1 shows that the proposed entity-aware decoding approach is capable of describing both seen and unseen entities in diverse im-ages. Specifically, ViECap builds on early-guidance de-coding methods, e.g., CapDec [35]. Unlike these models, which can only describe entities present in the training cor-pus, our model can generate captions in diverse scenarios.
Following CapDec, we train ViECap only using text data.
The entity-aware hard prompt is the critical design enabling the transferability of our model to diverse captioning sce-narios. The hard prompts, constructed by nouns extracted from texts during training or entities retrieved from images during inference, can prompt the LLMs to attend training-agnostic entities based on open vocabulary retrieval through
CLIP. As we find that a naive integration of entities pushes
ViECap to learn a copy-then-paste shortcut (i.e., directly copying the entities to captions), we introduce a simple yet efficient entity masking strategy when incorporating the entity-aware hard prompts into early-guidance decoding.
We extensively evaluate ViECap on four bench-marks, NoCaps [1], COCO [30, 8], Flickr30k [52], and
FlickrStyle10K [15]. The experimental results demonstrate that ViECap outperforms all other text-only methods and sets a new state-of-the-art in the cross-domain (transferable) setting while remaining competitive with them in the in-domain setting. In out-of-domain scenarios (NoCaps), we achieve a margin of 39.2 and 36.3 improvements, respec-tively, compared to DeCap and CapDec. We even surpass some supervised methods, indicating our model generalizes well to novel entities. In the experiment on FlickrStyle10K,
ViECap effectively generates captions in different styles corresponding to the styles of the training set. Additionally, the data-efficient experiment shows ViECap’s applicability in low-data settings, further highlighting its versatility and effectiveness across various scenarios.
To summarize, our contributions are as follows: 1)
We shed light on the observations and underlying rea-sons behind the degraded generalizability when adapting pre-trained VLMs and LLMs into image-to-text genera-tion, i.e., modality bias and object hallucination, provid-ing timely and valuable insights for pre-trained large-scale model adaptation. 2) We introduce entity-aware decod-ing to improve the transferability of zero-shot captioning.
Specifically, aided by VLMs, we integrate entity-aware hard prompts with entity masking strategy into the decoding pro-cess, guiding LLMs to attend both seen and unseen enti-ties. 3) Extensive experiments show the remarkable zero-shot transferability of ViECap, even in low-data scenarios. 2.