Abstract
We present a novel differentiable rendering framework for joint geometry, material, and lighting estimation from multi-view images. In contrast to previous methods which assume a simplified environment map or co-located flash-lights, in this work, we formulate the lighting of a static scene as one neural incident light field (NeILF) and one outgoing neural radiance field (NeRF). The key insight of the proposed method is the union of the incident and out-going light fields through physically-based rendering and inter-reflections between surfaces, making it possible to dis-entangle the scene geometry, material, and lighting from image observations in a physically-based manner. The pro-posed incident light and inter-reflection framework can be easily applied to other NeRF systems. We show that our method can not only decompose the outgoing radiance into incident lights and surface materials, but also serve as a surface refinement module that further improves the recon-struction detail of the neural surface. We demonstrate on several datasets that the proposed method is able to achieve state-of-the-art results in terms of geometry reconstruction quality, material estimation accuracy, and the fidelity of novel view rendering. 1.

Introduction
Reconstructing 3D scene information from multi-view images is a persistent challenge in the field of computer vi-sion and computer graphics. The Neural Radiance Field (NeRF) approach [13], which uses differentiable volume rendering to jointly optimize scene geometry and appear-ance, offers a novel solution and has demonstrated great re-sults in novel view synthesis and neural surface reconstruc-tion. While NeRF successfully models the outgoing radi-ance of a scene, it fails to disentangle the incident lighting
*Research done while at Apple.
Figure 1. Compared with the baseline that simply runs NeILF [28] following VolSDF [29], the proposed approach is able to jointly optimize the scene geometry and material. The neural surface, material, and final rendering quality are improved. and surface properties from the radiance, preventing its use in downstream applications such as object relighting and material property editing.
Recent works have acknowledged the problem and pro-posed to further decompose the outgoing radiance into the material and environmental lighting. Some of them adopt simplified lighting models to reduce the computational complexity, including co-located flash light [16, 3, 2, 20] and environment map [34, 36, 4, 15, 5]. These models are insufficient to model all kinds of lighting configurations, such as non-distant light sources and inter-reflections be-tween surfaces. Recently, the Neural Incident Light Field (NeILF) [28] instead models arbitrary static lighting con-ditions by encoding spatially-varying incident lighting in a neural network. However, it only provides the modeling ca-pability of, but does not introduce an explicit constraint on the inter-reflection issue. Moreover, NeILF requires an ob-ject mesh as input, whose quality has a strong influence on the estimated material.
In this work, we demonstrate that suitable lighting rep-resentation is the key to joint geometry and material esti-mation from multi-view images. Without loss of generality, the lighting of a typical scene can be represented by the in-cident light to the object surface and the outgoing radiance from the surface. Based on the observation, we propose to model the light fields, geometry, and materials of the scene as four separated fields, namely 1) one outgoing radiance field [13], 2) one incident light field [28], 3) one signed dis-tance field as the scene geometry, and 4) one field of bidi-rectional reflectance distribution function (BRDF) parame-ters as the surface material. The key insight of the proposed method is that the incident light and the outgoing radiance can be naturally unified through physically-based render-ing and inter-reflections between surfaces. We demonstrate that with the union of the outgoing radiance and the inci-dent light field, the proposed method not only can generate high-quality BRDF estimation for object relighting, but also improves the reconstruction detail of the surface geometry.
The proposed approach can be easily applied to different
NeRF systems for material decomposition and surface de-tail refinement.
To better evaluate the quality of estimated material from physically-based rendering, we construct a real-world lin-ear high-dynamic range (HDR) dataset called NeILF-HDR.
With this, we can avoid adding an HDR-LDR conversion module and directly train in HDR color space. We have ex-tensively studied our approach on the DTU dataset [8], the
NeILF synthetic dataset [28], and the NeILF-HDR dataset.
We show that the proposed method is able to achieve state-of-the-art results in geometry reconstruction quality, mate-rial estimation accuracy, and novel view rendering quality.
To summarize, major contributions of this paper include:
• A general light field representation by marrying one incident light field and one outgoing radiance field via
PBR and inter-reflections between surfaces.
• An optimization scheme for joint geometry, material, and lighting estimation. The proposed method can be easily applied to the prevalent NeRF family for mate-rial decomposition and neural surface refinement.
• A real-world linear HDR dataset for the evaluation of material estimation and other neural rendering tasks. 2.