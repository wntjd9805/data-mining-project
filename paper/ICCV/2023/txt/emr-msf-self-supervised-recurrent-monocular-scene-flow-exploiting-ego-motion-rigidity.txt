Abstract
Self-supervised monocular scene flow estimation, aiming to understand both 3D structures and 3D motions from two temporally consecutive monocular images, has received in-creasing attention for its simple and economical sensor setup. However, the accuracy of current methods suffers from the bottleneck of less-efficient network architecture and lack of motion rigidity for regularization. In this pa-per, we propose a superior model named EMR-MSF by borrowing the advantages of network architecture design under the scope of supervised learning. We further im-pose explicit and robust geometric constraints with an elab-orately constructed ego-motion aggregation module where a rigidity soft mask is proposed to filter out dynamic re-gions for stable ego-motion estimation using static regions.
Moreover, we propose a motion consistency loss along with a mask regularization loss to fully exploit static regions.
Several efficient training strategies are integrated includ-ing a gradient detachment technique and an enhanced view synthesis process for better performance. Our proposed method outperforms the previous self-supervised works by a large margin and catches up to the performance of su-pervised methods. On the KITTI scene flow benchmark, our approach improves the SF-all metric of the state-of-the-art self-supervised monocular method by 44% and demon-strates superior performance across sub-tasks including depth and visual odometry, amongst other self-supervised single-task or multi-task methods. 1.

Introduction
Scene flow estimation, which involves estimating both 3D structure and 3D motion of a dynamic scene from its two consecutive observations, has been receiving in-creasing attention due to its significance in areas such as robotics [10], augmented reality [22], and autonomous ve-hicles [35]. Recently, deep learning has demonstrated re-markable progress in the domain of scene flow estimation
Figure 1: Comparison between our method and [20]. (a) input first frame, (b) input second frame, (c) depth of first frame from [20], (d) synthesized optical flow from [20], (e) depth of first frame from our method, (f) synthesized optical flow from our method. Our method generates more regular-ized and detailed predictions as shown in red boxes. based on various input modalities, including stereo images
[3, 24, 32, 41, 51, 40], RGB-D pairs [31, 39, 45, 33], or
Lidar points [28, 18, 54, 56, 38, 55, 12, 7, 11, 52]. These methods, however, either require strict sensor calibrations (e.g., stereo-based), or expensive devices (e.g., RGB-D or
Lidar-based) for achieving satisfactory performance, which restricts their widespread applications.
On the other hand, monocular scene flow estimation methods [5, 57, 58, 62, 30, 26, 20, 21, 2] which only re-quire a monocular camera for obtaining both 3D structure and 3D motion, have been presented as an economical yet effective solution for dynamic 3D perception. The meth-ods [5, 57] combined with supervised learning have yielded promising results, yet the primary challenge facing them has been the limited availability of ground-truth training data. To address this limitation, several multi-task meth-ods [58, 62, 30, 53, 26] have been proposed to jointly learn the depth, 2D optical flow and camera ego-motion networks from monocular sequences in a self-supervised manner, and the scene flow can be calculated from the outputs. Recently,
[20, 21, 2] have shown it feasible to train a single network to directly estimate both depth and 3D scene flow from two
monocular images and outperform the previous multi-task methods. These methods typically build upon a standard optical flow pipeline (e.g., PWC-Net [43] or RAFT [44]) as basis and adapt it for monocular scene flow. Despite the notable progress achieved by these methods, their ac-curacy still lags behind the supervised monocular methods by a large margin.
In this paper, we propose a novel approach for self-supervised monocular scene flow estimation, which out-performs the previous methods significantly as shown in
Fig.1. To introduce explicit 3D geometry-oriented property, we follow the network architecture proposed in the super-vised RGB-D method RAFT-3D [45] that iteratively refines a dense SE3 motion field for scene flow estimation. This improvement of architecture compared to previous meth-ods directly improves the performance to a new level, but we argue that it still lacks the usage of Ego-Motion Rigid-ity (EMR), an important prior that pixels in static regions should have the same SE3 motion as the ego-motion. A novel module named ego-motion aggregation (EMA) is thus proposed to jointly estimate ego-motion as well as a rigid-ity soft mask from the dense SE3 motion field. A new mo-tion consistency loss is elaborately designed for constrain-ing motion estimations in static areas represented by the rigidity soft mask. However, we notice that the network is inclined to select only a small subset of static regions which leads to a rigidity soft mask of low quality. To mitigate this problem, we adopt an efficient mask regularization loss to encourage the network to locate as many static regions as possible. Further performance improvement is attributed to our proposed training strategies including a gradient detach-ment technique and an improved view synthesis process.
Our main contributions are summarized as follows:
• We propose a novel self-supervised monocular scene flow estimation by incorporating 3D geometry-oriented network architecture property and exploiting ego-motion rigidity (EMR-MSF). To the best of our knowledge, we are the first method capable of jointly estimating depth, dense SE3 motion field and ego-motion from monocular images, as well as full scene flow derived from them.
• We introduce a novel ego-motion aggregation (EMA) module accompanied by a rigidity soft mask to pre-cisely locate static regions for robust and accurate ego-motion estimation.
• We propose two new training losses to constrain the motion estimations in static regions, along with two effective training strategies to enhance accuracy as ex-plained in Sec. 3.3.
• We conduct extensive experiments to verify the effec-tiveness of our proposed method, resulting in a 44% accuracy boost in the SF-all metric compared to the previous state-of-the-art method on the task of monoc-ular scene flow estimation, as well as superior results in monocular depth and visual odometry. 2.