Abstract
Deep learning methods have shown outstanding classi-fication accuracy in medical imaging problems, which is largely attributed to the availability of large-scale datasets manually annotated with clean labels. However, given the high cost of such manual annotation, new medical imag-ing classification problems may need to rely on machine-generated noisy labels extracted from radiology reports. In-deed, many Chest X-Ray (CXR) classifiers have been mod-elled from datasets with noisy labels, but their training pro-cedure is in general not robust to noisy-label samples, lead-ing to sub-optimal models. Furthermore, CXR datasets are mostly multi-label, so current multi-class noisy-label learn-ing methods cannot be easily adapted.
In this paper, we propose a new method designed for noisy multi-label CXR learning, which detects and smoothly re-labels noisy sam-ples from the dataset to be used in the training of common multi-label classifiers. The proposed method optimises a bag of multi-label descriptors (BoMD) to promote their sim-ilarity with the semantic descriptors produced by language models from multi-label image annotations. Our experi-ments on noisy multi-label training sets and clean testing sets show that our model has state-of-the-art accuracy and robustness in many CXR multi-label classification bench-marks, including a new benchmark that we propose to sys-tematically assess noisy multi-label methods. Code is avail-able at https://github.com/cyh-0/BoMD. 1.

Introduction
The promising results produced by deep neural networks (DNN) in medical image analysis (MIA) problems [30] is attributed to the availability of large-scale datasets with ac-curately annotated images. Given the high cost of acquir-ing such datasets, the field is considering more affordable automatic annotation processes by Natural Language Pro-cessing (NLP) approaches that extract multiple labels (each
*First two authors contributed equally to this work.
Figure 1: Comparison of multi-class LNL methods [1, 21, 28, 33] and our noisy multi-label approach, BoMD, where the feature extractor returns a single descriptor v per im-age, D is the noisy training set, C is the clean set, and
˜D is the re-labelled training set. BoMD has two compo-nents: 1) learning of a bag of multi-label image descriptors (MID) {v1, v2, v3} to represent the image, and 2) smooth re-labelling of images driven by a graph structure based on the fine-grained relationships between MID descriptors. label representing a disease) from radiology reports [20,57].
However, mistakes made by NLP combined with uncer-tain radiological findings can introduce label noise [44, 45], as can be found in NLP-annotated Chest X-ray (CXR) datasets [20, 57] whose noisy multi-labels can mislead su-pervised training processes. Nevertheless, even without ad-dressing this noisy multi-label problem, current CXR multi-label classifiers [3, 15, 39, 50] show promising results. Al-though these methods show encouraging multi-label clas-sification accuracy, there is still potential for improvement
that can be realised by properly addressing the noisy multi-label learning problem present in CXR datasets [20, 57].
Current learning with noisy label (LNL) approaches fo-cus on leveraging the hidden clean-label information to as-sist the training of DNNs (see Fig. 1). This can be achieved with techniques that clean the label noise [25, 28], robustify loss functions [21, 31, 33], estimate label transition matri-ces [1,12,62,65], smooth training labels [38,59,66], and use graphs to explore the latent structure of data. [21, 60, 61].
These methods have been designed for noisy multi-class problems and do not easily extend to noisy multi-label learning, which is challenging given the potential multi-In addition, ple label mistakes for each training sample. a key characteristic of multi-label classification is the in-herent positive-negative imbalance [51] issue. Such an is-sue may cause sample-selection based methods (e.g., Di-videMix [28]) to select an extremely imbalanced clean set, where the majority of identified clean samples belong to the
’No Findings’ class. Additionally, it could impede the accu-rate estimation of the posterior probabilities for the noisy or intermediate classes [29].” To the best of our knowledge, the state-of-the-art (SOTA) approach that handles noisy multi-label learning is NVUM [31], which is based on an exten-sion of early learning regularisation (ELR) [33]. NVUM shows promising results, but it is challenged by the differ-ent early convergence patterns of multiple labels, which can lead to poor performance for particular label noise condi-tions, as shown in our experiments. Additionally, NVUM is only evaluated on real-world CXR datasets [20, 57] with-out any systematic assessment of robustness to varying label noise conditions, preventing a more complete understand-ing of its functionality.
In this paper, we propose a new solution specifically designed for the noisy multi-label problem by answering the following question: can the detection and correction of noisy multi-labelled samples be facilitated by leverag-ing the semantic information of training labels? available from language models [19, 27, 48]? This question is moti-vated by the successful exploration of language models in computer vision [3, 7, 16, 67], with methods that leverage semantic information to influence the training of visual de-scriptors; an idea that has not been explored in noisy multi-label classification. To answer this question, we introduce the 2-stage Bag of Multi-label Descriptors (BoMD) method (see Fig. 1) to smoothly re-label noisy multi-label image datasets that can then be used for training common multi-label classifiers. The first stage trains a feature extractor to produce a bag of multi-label image descriptors by promot-ing their similarity with the semantic embeddings from lan-guage models. For the second stage, we introduce a novel graph structure, where each image is represented by a sub-graph built from the multi-label image descriptors, learned in the first stage, to smoothly re-label the noisy multi-label images. Compared with graphs built directly from a sin-gle descriptor per image [21], our graph structure with the multi-label image descriptors has the potential to capture more fine-grained image relationships, which is crucial to deal with multi-label annotation. We also propose a new benchmark to systematically assess noisy multi-label meth-ods. In summary, our contributions are: 1. A novel 2-stage learning method to smoothly re-label noisy multi-label datasets of CXR images that can then be used for training a common multi-label classifier; 2. A new bag of multi-label image descriptors learning method that leverages the semantic information avail-able from language models to represent multi-label im-ages and to detect noisy samples; 3. A new graph structure to smoothly re-label noisy multi-label images, with each image being represented by a sub-graph of the learned multi-label image de-scriptors that can capture fine-grained image relation-ships; 4. The first systematic evaluation of noisy multi-label methods that combine the PadChest [6] and Chest X-ray 14 [57] datasets.
We show the effectiveness of our BoMD on a bench-mark that consists of training with two noisy multi-label
CXR datasets and testing on three clean multi-label CXR datasets [31]. Results show that our approach has more accurate classifications than previous multi-label classifiers developed for CXR datasets and noisy-label classifiers. Re-sults on our proposed benchmark show that BoMD is gen-erally more accurate and robust than competing methods under our systematic evaluation. 2.